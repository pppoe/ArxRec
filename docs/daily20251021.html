<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251020.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles", "author": "Peng Wang and Xiang Liu and Peidong Liu", "abstract": "  Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.\n", "link": "http://arxiv.org/abs/2505.21060v2", "date": "2025-10-20", "relevancy": 3.2051, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6461}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles&body=Title%3A%20Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles%0AAuthor%3A%20Peng%20Wang%20and%20Xiang%20Liu%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Stylizing%203D%20scenes%20instantly%20while%20maintaining%20multi-view%20consistency%20and%0Afaithfully%20resembling%20a%20style%20image%20remains%20a%20significant%20challenge.%20Current%0Astate-of-the-art%203D%20stylization%20methods%20typically%20involve%20computationally%0Aintensive%20test-time%20optimization%20to%20transfer%20artistic%20features%20into%20a%0Apretrained%203D%20representation%2C%20often%20requiring%20dense%20posed%20input%20images.%20In%0Acontrast%2C%20leveraging%20recent%20advances%20in%20feed-forward%20reconstruction%20models%2C%20we%0Ademonstrate%20a%20novel%20approach%20to%20achieve%20direct%203D%20stylization%20in%20less%20than%20a%0Asecond%20using%20unposed%20sparse-view%20scene%20images%20and%20an%20arbitrary%20style%20image.%20To%0Aaddress%20the%20inherent%20decoupling%20between%20reconstruction%20and%20stylization%2C%20we%0Aintroduce%20a%20branched%20architecture%20that%20separates%20structure%20modeling%20and%0Aappearance%20shading%2C%20effectively%20preventing%20stylistic%20transfer%20from%20distorting%0Athe%20underlying%203D%20scene%20structure.%20Furthermore%2C%20we%20adapt%20an%20identity%20loss%20to%0Afacilitate%20pre-training%20our%20stylization%20model%20through%20the%20novel%20view%20synthesis%0Atask.%20This%20strategy%20also%20allows%20our%20model%20to%20retain%20its%20original%20reconstruction%0Acapabilities%20while%20being%20fine-tuned%20for%20stylization.%20Comprehensive%20evaluations%2C%0Ausing%20both%20in-domain%20and%20out-of-domain%20datasets%2C%20demonstrate%20that%20our%20approach%0Aproduces%20high-quality%20stylized%203D%20content%20that%20achieve%20a%20superior%20blend%20of%0Astyle%20and%20scene%20appearance%2C%20while%20also%20outperforming%20existing%20methods%20in%20terms%0Aof%20multi-view%20consistency%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyl3R%253A%2520Instant%25203D%2520Stylized%2520Reconstruction%2520for%2520Arbitrary%2520Scenes%2520and%250A%2520%2520Styles%26entry.906535625%3DPeng%2520Wang%2520and%2520Xiang%2520Liu%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Stylizing%25203D%2520scenes%2520instantly%2520while%2520maintaining%2520multi-view%2520consistency%2520and%250Afaithfully%2520resembling%2520a%2520style%2520image%2520remains%2520a%2520significant%2520challenge.%2520Current%250Astate-of-the-art%25203D%2520stylization%2520methods%2520typically%2520involve%2520computationally%250Aintensive%2520test-time%2520optimization%2520to%2520transfer%2520artistic%2520features%2520into%2520a%250Apretrained%25203D%2520representation%252C%2520often%2520requiring%2520dense%2520posed%2520input%2520images.%2520In%250Acontrast%252C%2520leveraging%2520recent%2520advances%2520in%2520feed-forward%2520reconstruction%2520models%252C%2520we%250Ademonstrate%2520a%2520novel%2520approach%2520to%2520achieve%2520direct%25203D%2520stylization%2520in%2520less%2520than%2520a%250Asecond%2520using%2520unposed%2520sparse-view%2520scene%2520images%2520and%2520an%2520arbitrary%2520style%2520image.%2520To%250Aaddress%2520the%2520inherent%2520decoupling%2520between%2520reconstruction%2520and%2520stylization%252C%2520we%250Aintroduce%2520a%2520branched%2520architecture%2520that%2520separates%2520structure%2520modeling%2520and%250Aappearance%2520shading%252C%2520effectively%2520preventing%2520stylistic%2520transfer%2520from%2520distorting%250Athe%2520underlying%25203D%2520scene%2520structure.%2520Furthermore%252C%2520we%2520adapt%2520an%2520identity%2520loss%2520to%250Afacilitate%2520pre-training%2520our%2520stylization%2520model%2520through%2520the%2520novel%2520view%2520synthesis%250Atask.%2520This%2520strategy%2520also%2520allows%2520our%2520model%2520to%2520retain%2520its%2520original%2520reconstruction%250Acapabilities%2520while%2520being%2520fine-tuned%2520for%2520stylization.%2520Comprehensive%2520evaluations%252C%250Ausing%2520both%2520in-domain%2520and%2520out-of-domain%2520datasets%252C%2520demonstrate%2520that%2520our%2520approach%250Aproduces%2520high-quality%2520stylized%25203D%2520content%2520that%2520achieve%2520a%2520superior%2520blend%2520of%250Astyle%2520and%2520scene%2520appearance%252C%2520while%2520also%2520outperforming%2520existing%2520methods%2520in%2520terms%250Aof%2520multi-view%2520consistency%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Styl3R%3A%20Instant%203D%20Stylized%20Reconstruction%20for%20Arbitrary%20Scenes%20and%0A%20%20Styles&entry.906535625=Peng%20Wang%20and%20Xiang%20Liu%20and%20Peidong%20Liu&entry.1292438233=%20%20Stylizing%203D%20scenes%20instantly%20while%20maintaining%20multi-view%20consistency%20and%0Afaithfully%20resembling%20a%20style%20image%20remains%20a%20significant%20challenge.%20Current%0Astate-of-the-art%203D%20stylization%20methods%20typically%20involve%20computationally%0Aintensive%20test-time%20optimization%20to%20transfer%20artistic%20features%20into%20a%0Apretrained%203D%20representation%2C%20often%20requiring%20dense%20posed%20input%20images.%20In%0Acontrast%2C%20leveraging%20recent%20advances%20in%20feed-forward%20reconstruction%20models%2C%20we%0Ademonstrate%20a%20novel%20approach%20to%20achieve%20direct%203D%20stylization%20in%20less%20than%20a%0Asecond%20using%20unposed%20sparse-view%20scene%20images%20and%20an%20arbitrary%20style%20image.%20To%0Aaddress%20the%20inherent%20decoupling%20between%20reconstruction%20and%20stylization%2C%20we%0Aintroduce%20a%20branched%20architecture%20that%20separates%20structure%20modeling%20and%0Aappearance%20shading%2C%20effectively%20preventing%20stylistic%20transfer%20from%20distorting%0Athe%20underlying%203D%20scene%20structure.%20Furthermore%2C%20we%20adapt%20an%20identity%20loss%20to%0Afacilitate%20pre-training%20our%20stylization%20model%20through%20the%20novel%20view%20synthesis%0Atask.%20This%20strategy%20also%20allows%20our%20model%20to%20retain%20its%20original%20reconstruction%0Acapabilities%20while%20being%20fine-tuned%20for%20stylization.%20Comprehensive%20evaluations%2C%0Ausing%20both%20in-domain%20and%20out-of-domain%20datasets%2C%20demonstrate%20that%20our%20approach%0Aproduces%20high-quality%20stylized%203D%20content%20that%20achieve%20a%20superior%20blend%20of%0Astyle%20and%20scene%20appearance%2C%20while%20also%20outperforming%20existing%20methods%20in%20terms%0Aof%20multi-view%20consistency%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21060v2&entry.124074799=Read"},
{"title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception", "author": "Kaichen Zhou and Yuhan Wang and Grace Chen and Xinhai Chang and Gaspard Beaudouin and Fangneng Zhan and Paul Pu Liang and Mengyu Wang", "abstract": "  Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.\n", "link": "http://arxiv.org/abs/2510.17568v1", "date": "2025-10-20", "relevancy": 3.1566, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6329}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6317}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAGE-4D%3A%20Disentangled%20Pose%20and%20Geometry%20Estimation%20for%204D%20Perception&body=Title%3A%20PAGE-4D%3A%20Disentangled%20Pose%20and%20Geometry%20Estimation%20for%204D%20Perception%0AAuthor%3A%20Kaichen%20Zhou%20and%20Yuhan%20Wang%20and%20Grace%20Chen%20and%20Xinhai%20Chang%20and%20Gaspard%20Beaudouin%20and%20Fangneng%20Zhan%20and%20Paul%20Pu%20Liang%20and%20Mengyu%20Wang%0AAbstract%3A%20%20%20Recent%203D%20feed-forward%20models%2C%20such%20as%20the%20Visual%20Geometry%20Grounded%0ATransformer%20%28VGGT%29%2C%20have%20shown%20strong%20capability%20in%20inferring%203D%20attributes%20of%0Astatic%20scenes.%20However%2C%20since%20they%20are%20typically%20trained%20on%20static%20datasets%2C%0Athese%20models%20often%20struggle%20in%20real-world%20scenarios%20involving%20complex%20dynamic%0Aelements%2C%20such%20as%20moving%20humans%20or%20deformable%20objects%20like%20umbrellas.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20PAGE-4D%2C%20a%20feedforward%20model%20that%20extends%0AVGGT%20to%20dynamic%20scenes%2C%20enabling%20camera%20pose%20estimation%2C%20depth%20prediction%2C%20and%0Apoint%20cloud%20reconstruction%20--%20all%20without%20post-processing.%20A%20central%20challenge%0Ain%20multi-task%204D%20reconstruction%20is%20the%20inherent%20conflict%20between%20tasks%3A%0Aaccurate%20camera%20pose%20estimation%20requires%20suppressing%20dynamic%20regions%2C%20while%0Ageometry%20reconstruction%20requires%20modeling%20them.%20To%20resolve%20this%20tension%2C%20we%0Apropose%20a%20dynamics-aware%20aggregator%20that%20disentangles%20static%20and%20dynamic%0Ainformation%20by%20predicting%20a%20dynamics-aware%20mask%20--%20suppressing%20motion%20cues%20for%0Apose%20estimation%20while%20amplifying%20them%20for%20geometry%20reconstruction.%20Extensive%0Aexperiments%20show%20that%20PAGE-4D%20consistently%20outperforms%20the%20original%20VGGT%20in%0Adynamic%20scenarios%2C%20achieving%20superior%20results%20in%20camera%20pose%20estimation%2C%0Amonocular%20and%20video%20depth%20estimation%2C%20and%20dense%20point%20map%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAGE-4D%253A%2520Disentangled%2520Pose%2520and%2520Geometry%2520Estimation%2520for%25204D%2520Perception%26entry.906535625%3DKaichen%2520Zhou%2520and%2520Yuhan%2520Wang%2520and%2520Grace%2520Chen%2520and%2520Xinhai%2520Chang%2520and%2520Gaspard%2520Beaudouin%2520and%2520Fangneng%2520Zhan%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Mengyu%2520Wang%26entry.1292438233%3D%2520%2520Recent%25203D%2520feed-forward%2520models%252C%2520such%2520as%2520the%2520Visual%2520Geometry%2520Grounded%250ATransformer%2520%2528VGGT%2529%252C%2520have%2520shown%2520strong%2520capability%2520in%2520inferring%25203D%2520attributes%2520of%250Astatic%2520scenes.%2520However%252C%2520since%2520they%2520are%2520typically%2520trained%2520on%2520static%2520datasets%252C%250Athese%2520models%2520often%2520struggle%2520in%2520real-world%2520scenarios%2520involving%2520complex%2520dynamic%250Aelements%252C%2520such%2520as%2520moving%2520humans%2520or%2520deformable%2520objects%2520like%2520umbrellas.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520PAGE-4D%252C%2520a%2520feedforward%2520model%2520that%2520extends%250AVGGT%2520to%2520dynamic%2520scenes%252C%2520enabling%2520camera%2520pose%2520estimation%252C%2520depth%2520prediction%252C%2520and%250Apoint%2520cloud%2520reconstruction%2520--%2520all%2520without%2520post-processing.%2520A%2520central%2520challenge%250Ain%2520multi-task%25204D%2520reconstruction%2520is%2520the%2520inherent%2520conflict%2520between%2520tasks%253A%250Aaccurate%2520camera%2520pose%2520estimation%2520requires%2520suppressing%2520dynamic%2520regions%252C%2520while%250Ageometry%2520reconstruction%2520requires%2520modeling%2520them.%2520To%2520resolve%2520this%2520tension%252C%2520we%250Apropose%2520a%2520dynamics-aware%2520aggregator%2520that%2520disentangles%2520static%2520and%2520dynamic%250Ainformation%2520by%2520predicting%2520a%2520dynamics-aware%2520mask%2520--%2520suppressing%2520motion%2520cues%2520for%250Apose%2520estimation%2520while%2520amplifying%2520them%2520for%2520geometry%2520reconstruction.%2520Extensive%250Aexperiments%2520show%2520that%2520PAGE-4D%2520consistently%2520outperforms%2520the%2520original%2520VGGT%2520in%250Adynamic%2520scenarios%252C%2520achieving%2520superior%2520results%2520in%2520camera%2520pose%2520estimation%252C%250Amonocular%2520and%2520video%2520depth%2520estimation%252C%2520and%2520dense%2520point%2520map%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAGE-4D%3A%20Disentangled%20Pose%20and%20Geometry%20Estimation%20for%204D%20Perception&entry.906535625=Kaichen%20Zhou%20and%20Yuhan%20Wang%20and%20Grace%20Chen%20and%20Xinhai%20Chang%20and%20Gaspard%20Beaudouin%20and%20Fangneng%20Zhan%20and%20Paul%20Pu%20Liang%20and%20Mengyu%20Wang&entry.1292438233=%20%20Recent%203D%20feed-forward%20models%2C%20such%20as%20the%20Visual%20Geometry%20Grounded%0ATransformer%20%28VGGT%29%2C%20have%20shown%20strong%20capability%20in%20inferring%203D%20attributes%20of%0Astatic%20scenes.%20However%2C%20since%20they%20are%20typically%20trained%20on%20static%20datasets%2C%0Athese%20models%20often%20struggle%20in%20real-world%20scenarios%20involving%20complex%20dynamic%0Aelements%2C%20such%20as%20moving%20humans%20or%20deformable%20objects%20like%20umbrellas.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20PAGE-4D%2C%20a%20feedforward%20model%20that%20extends%0AVGGT%20to%20dynamic%20scenes%2C%20enabling%20camera%20pose%20estimation%2C%20depth%20prediction%2C%20and%0Apoint%20cloud%20reconstruction%20--%20all%20without%20post-processing.%20A%20central%20challenge%0Ain%20multi-task%204D%20reconstruction%20is%20the%20inherent%20conflict%20between%20tasks%3A%0Aaccurate%20camera%20pose%20estimation%20requires%20suppressing%20dynamic%20regions%2C%20while%0Ageometry%20reconstruction%20requires%20modeling%20them.%20To%20resolve%20this%20tension%2C%20we%0Apropose%20a%20dynamics-aware%20aggregator%20that%20disentangles%20static%20and%20dynamic%0Ainformation%20by%20predicting%20a%20dynamics-aware%20mask%20--%20suppressing%20motion%20cues%20for%0Apose%20estimation%20while%20amplifying%20them%20for%20geometry%20reconstruction.%20Extensive%0Aexperiments%20show%20that%20PAGE-4D%20consistently%20outperforms%20the%20original%20VGGT%20in%0Adynamic%20scenarios%2C%20achieving%20superior%20results%20in%20camera%20pose%20estimation%2C%0Amonocular%20and%20video%20depth%20estimation%2C%20and%20dense%20point%20map%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17568v1&entry.124074799=Read"},
{"title": "Towards 3D Objectness Learning in an Open World", "author": "Taichi Liu and Zhenyu Wang and Ruofeng Liu and Guang Wang and Desheng Zhang", "abstract": "  Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.\n", "link": "http://arxiv.org/abs/2510.17686v1", "date": "2025-10-20", "relevancy": 3.1047, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%203D%20Objectness%20Learning%20in%20an%20Open%20World&body=Title%3A%20Towards%203D%20Objectness%20Learning%20in%20an%20Open%20World%0AAuthor%3A%20Taichi%20Liu%20and%20Zhenyu%20Wang%20and%20Ruofeng%20Liu%20and%20Guang%20Wang%20and%20Desheng%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20object%20detection%20and%20novel%20category%20detection%20have%0Amade%20significant%20progress%2C%20yet%20research%20on%20learning%20generalized%203D%20objectness%0Aremains%20insufficient.%20In%20this%20paper%2C%20we%20delve%20into%20learning%20open-world%203D%0Aobjectness%2C%20which%20focuses%20on%20detecting%20all%20objects%20in%20a%203D%20scene%2C%20including%0Anovel%20objects%20unseen%20during%20training.%20Traditional%20closed-set%203D%20detectors%0Astruggle%20to%20generalize%20to%20open-world%20scenarios%2C%20while%20directly%20incorporating%203D%0Aopen-vocabulary%20models%20for%20open-world%20ability%20struggles%20with%20vocabulary%0Aexpansion%20and%20semantic%20overlap.%20To%20achieve%20generalized%203D%20object%20discovery%2C%20We%0Apropose%20OP3Det%2C%20a%20class-agnostic%20Open-World%20Prompt-free%203D%20Detector%20to%20detect%0Aany%20objects%20within%203D%20scenes%20without%20relying%20on%20hand-crafted%20text%20prompts.%20We%0Aintroduce%20the%20strong%20generalization%20and%20zero-shot%20capabilities%20of%202D%20foundation%0Amodels%2C%20utilizing%20both%202D%20semantic%20priors%20and%203D%20geometric%20priors%20for%0Aclass-agnostic%20proposals%20to%20broaden%203D%20object%20discovery.%20Then%2C%20by%20integrating%0Acomplementary%20information%20from%20point%20cloud%20and%20RGB%20image%20in%20the%20cross-modal%0Amixture%20of%20experts%2C%20OP3Det%20dynamically%20routes%20uni-modal%20and%20multi-modal%0Afeatures%20to%20learn%20generalized%203D%20objectness.%20Extensive%20experiments%20demonstrate%0Athe%20extraordinary%20performance%20of%20OP3Det%2C%20which%20significantly%20surpasses%20existing%0Aopen-world%203D%20detectors%20by%20up%20to%2016.0%25%20in%20AR%20and%20achieves%20a%2013.5%25%20improvement%0Acompared%20to%20closed-world%203D%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%25203D%2520Objectness%2520Learning%2520in%2520an%2520Open%2520World%26entry.906535625%3DTaichi%2520Liu%2520and%2520Zhenyu%2520Wang%2520and%2520Ruofeng%2520Liu%2520and%2520Guang%2520Wang%2520and%2520Desheng%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520object%2520detection%2520and%2520novel%2520category%2520detection%2520have%250Amade%2520significant%2520progress%252C%2520yet%2520research%2520on%2520learning%2520generalized%25203D%2520objectness%250Aremains%2520insufficient.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520learning%2520open-world%25203D%250Aobjectness%252C%2520which%2520focuses%2520on%2520detecting%2520all%2520objects%2520in%2520a%25203D%2520scene%252C%2520including%250Anovel%2520objects%2520unseen%2520during%2520training.%2520Traditional%2520closed-set%25203D%2520detectors%250Astruggle%2520to%2520generalize%2520to%2520open-world%2520scenarios%252C%2520while%2520directly%2520incorporating%25203D%250Aopen-vocabulary%2520models%2520for%2520open-world%2520ability%2520struggles%2520with%2520vocabulary%250Aexpansion%2520and%2520semantic%2520overlap.%2520To%2520achieve%2520generalized%25203D%2520object%2520discovery%252C%2520We%250Apropose%2520OP3Det%252C%2520a%2520class-agnostic%2520Open-World%2520Prompt-free%25203D%2520Detector%2520to%2520detect%250Aany%2520objects%2520within%25203D%2520scenes%2520without%2520relying%2520on%2520hand-crafted%2520text%2520prompts.%2520We%250Aintroduce%2520the%2520strong%2520generalization%2520and%2520zero-shot%2520capabilities%2520of%25202D%2520foundation%250Amodels%252C%2520utilizing%2520both%25202D%2520semantic%2520priors%2520and%25203D%2520geometric%2520priors%2520for%250Aclass-agnostic%2520proposals%2520to%2520broaden%25203D%2520object%2520discovery.%2520Then%252C%2520by%2520integrating%250Acomplementary%2520information%2520from%2520point%2520cloud%2520and%2520RGB%2520image%2520in%2520the%2520cross-modal%250Amixture%2520of%2520experts%252C%2520OP3Det%2520dynamically%2520routes%2520uni-modal%2520and%2520multi-modal%250Afeatures%2520to%2520learn%2520generalized%25203D%2520objectness.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520extraordinary%2520performance%2520of%2520OP3Det%252C%2520which%2520significantly%2520surpasses%2520existing%250Aopen-world%25203D%2520detectors%2520by%2520up%2520to%252016.0%2525%2520in%2520AR%2520and%2520achieves%2520a%252013.5%2525%2520improvement%250Acompared%2520to%2520closed-world%25203D%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%203D%20Objectness%20Learning%20in%20an%20Open%20World&entry.906535625=Taichi%20Liu%20and%20Zhenyu%20Wang%20and%20Ruofeng%20Liu%20and%20Guang%20Wang%20and%20Desheng%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%203D%20object%20detection%20and%20novel%20category%20detection%20have%0Amade%20significant%20progress%2C%20yet%20research%20on%20learning%20generalized%203D%20objectness%0Aremains%20insufficient.%20In%20this%20paper%2C%20we%20delve%20into%20learning%20open-world%203D%0Aobjectness%2C%20which%20focuses%20on%20detecting%20all%20objects%20in%20a%203D%20scene%2C%20including%0Anovel%20objects%20unseen%20during%20training.%20Traditional%20closed-set%203D%20detectors%0Astruggle%20to%20generalize%20to%20open-world%20scenarios%2C%20while%20directly%20incorporating%203D%0Aopen-vocabulary%20models%20for%20open-world%20ability%20struggles%20with%20vocabulary%0Aexpansion%20and%20semantic%20overlap.%20To%20achieve%20generalized%203D%20object%20discovery%2C%20We%0Apropose%20OP3Det%2C%20a%20class-agnostic%20Open-World%20Prompt-free%203D%20Detector%20to%20detect%0Aany%20objects%20within%203D%20scenes%20without%20relying%20on%20hand-crafted%20text%20prompts.%20We%0Aintroduce%20the%20strong%20generalization%20and%20zero-shot%20capabilities%20of%202D%20foundation%0Amodels%2C%20utilizing%20both%202D%20semantic%20priors%20and%203D%20geometric%20priors%20for%0Aclass-agnostic%20proposals%20to%20broaden%203D%20object%20discovery.%20Then%2C%20by%20integrating%0Acomplementary%20information%20from%20point%20cloud%20and%20RGB%20image%20in%20the%20cross-modal%0Amixture%20of%20experts%2C%20OP3Det%20dynamically%20routes%20uni-modal%20and%20multi-modal%0Afeatures%20to%20learn%20generalized%203D%20objectness.%20Extensive%20experiments%20demonstrate%0Athe%20extraordinary%20performance%20of%20OP3Det%2C%20which%20significantly%20surpasses%20existing%0Aopen-world%203D%20detectors%20by%20up%20to%2016.0%25%20in%20AR%20and%20achieves%20a%2013.5%25%20improvement%0Acompared%20to%20closed-world%203D%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17686v1&entry.124074799=Read"},
{"title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?", "author": "Aaron Appelle and Jerome P. Lynch", "abstract": "  Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.\n", "link": "http://arxiv.org/abs/2510.17731v1", "date": "2025-10-20", "relevancy": 3.0134, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6159}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5979}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Image-To-Video%20Models%20Simulate%20Pedestrian%20Dynamics%3F&body=Title%3A%20Can%20Image-To-Video%20Models%20Simulate%20Pedestrian%20Dynamics%3F%0AAuthor%3A%20Aaron%20Appelle%20and%20Jerome%20P.%20Lynch%0AAbstract%3A%20%20%20Recent%20high-performing%20image-to-video%20%28I2V%29%20models%20based%20on%20variants%20of%20the%0Adiffusion%20transformer%20%28DiT%29%20have%20displayed%20remarkable%20inherent%20world-modeling%0Acapabilities%20by%20virtue%20of%20training%20on%20large%20scale%20video%20datasets.%20We%0Ainvestigate%20whether%20these%20models%20can%20generate%20realistic%20pedestrian%20movement%0Apatterns%20in%20crowded%20public%20scenes.%20Our%20framework%20conditions%20I2V%20models%20on%0Akeyframes%20extracted%20from%20pedestrian%20trajectory%20benchmarks%2C%20then%20evaluates%20their%0Atrajectory%20prediction%20performance%20using%20quantitative%20measures%20of%20pedestrian%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Image-To-Video%2520Models%2520Simulate%2520Pedestrian%2520Dynamics%253F%26entry.906535625%3DAaron%2520Appelle%2520and%2520Jerome%2520P.%2520Lynch%26entry.1292438233%3D%2520%2520Recent%2520high-performing%2520image-to-video%2520%2528I2V%2529%2520models%2520based%2520on%2520variants%2520of%2520the%250Adiffusion%2520transformer%2520%2528DiT%2529%2520have%2520displayed%2520remarkable%2520inherent%2520world-modeling%250Acapabilities%2520by%2520virtue%2520of%2520training%2520on%2520large%2520scale%2520video%2520datasets.%2520We%250Ainvestigate%2520whether%2520these%2520models%2520can%2520generate%2520realistic%2520pedestrian%2520movement%250Apatterns%2520in%2520crowded%2520public%2520scenes.%2520Our%2520framework%2520conditions%2520I2V%2520models%2520on%250Akeyframes%2520extracted%2520from%2520pedestrian%2520trajectory%2520benchmarks%252C%2520then%2520evaluates%2520their%250Atrajectory%2520prediction%2520performance%2520using%2520quantitative%2520measures%2520of%2520pedestrian%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Image-To-Video%20Models%20Simulate%20Pedestrian%20Dynamics%3F&entry.906535625=Aaron%20Appelle%20and%20Jerome%20P.%20Lynch&entry.1292438233=%20%20Recent%20high-performing%20image-to-video%20%28I2V%29%20models%20based%20on%20variants%20of%20the%0Adiffusion%20transformer%20%28DiT%29%20have%20displayed%20remarkable%20inherent%20world-modeling%0Acapabilities%20by%20virtue%20of%20training%20on%20large%20scale%20video%20datasets.%20We%0Ainvestigate%20whether%20these%20models%20can%20generate%20realistic%20pedestrian%20movement%0Apatterns%20in%20crowded%20public%20scenes.%20Our%20framework%20conditions%20I2V%20models%20on%0Akeyframes%20extracted%20from%20pedestrian%20trajectory%20benchmarks%2C%20then%20evaluates%20their%0Atrajectory%20prediction%20performance%20using%20quantitative%20measures%20of%20pedestrian%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17731v1&entry.124074799=Read"},
{"title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in\n  Language and Image Input", "author": "Hendric Voss and Stefan Kopp", "abstract": "  Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/\n", "link": "http://arxiv.org/abs/2510.17617v1", "date": "2025-10-20", "relevancy": 3.0082, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.603}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6019}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImaGGen%3A%20Zero-Shot%20Generation%20of%20Co-Speech%20Semantic%20Gestures%20Grounded%20in%0A%20%20Language%20and%20Image%20Input&body=Title%3A%20ImaGGen%3A%20Zero-Shot%20Generation%20of%20Co-Speech%20Semantic%20Gestures%20Grounded%20in%0A%20%20Language%20and%20Image%20Input%0AAuthor%3A%20Hendric%20Voss%20and%20Stefan%20Kopp%0AAbstract%3A%20%20%20Human%20communication%20combines%20speech%20with%20expressive%20nonverbal%20cues%20such%20as%0Ahand%20gestures%20that%20serve%20manifold%20communicative%20functions.%20Yet%2C%20current%0Agenerative%20gesture%20generation%20approaches%20are%20restricted%20to%20simple%2C%20repetitive%0Abeat%20gestures%20that%20accompany%20the%20rhythm%20of%20speaking%20but%20do%20not%20contribute%20to%0Acommunicating%20semantic%20meaning.%20This%20paper%20tackles%20a%20core%20challenge%20in%0Aco-speech%20gesture%20synthesis%3A%20generating%20iconic%20or%20deictic%20gestures%20that%20are%0Asemantically%20coherent%20with%20a%20verbal%20utterance.%20Such%20gestures%20cannot%20be%20derived%0Afrom%20language%20input%20alone%2C%20which%20inherently%20lacks%20the%20visual%20meaning%20that%20is%0Aoften%20carried%20autonomously%20by%20gestures.%20We%20therefore%20introduce%20a%20zero-shot%0Asystem%20that%20generates%20gestures%20from%20a%20given%20language%20input%20and%20additionally%20is%0Ainformed%20by%20imagistic%20input%2C%20without%20manual%20annotation%20or%20human%20intervention.%0AOur%20method%20integrates%20an%20image%20analysis%20pipeline%20that%20extracts%20key%20object%0Aproperties%20such%20as%20shape%2C%20symmetry%2C%20and%20alignment%2C%20together%20with%20a%20semantic%0Amatching%20module%20that%20links%20these%20visual%20details%20to%20spoken%20text.%20An%20inverse%0Akinematics%20engine%20then%20synthesizes%20iconic%20and%20deictic%20gestures%20and%20combines%0Athem%20with%20co-generated%20natural%20beat%20gestures%20for%20coherent%20multimodal%0Acommunication.%20A%20comprehensive%20user%20study%20demonstrates%20the%20effectiveness%20of%20our%0Aapproach.%20In%20scenarios%20where%20speech%20alone%20was%20ambiguous%2C%20gestures%20generated%20by%0Aour%20system%20significantly%20improved%20participants%27%20ability%20to%20identify%20object%0Aproperties%2C%20confirming%20their%20interpretability%20and%20communicative%20value.%20While%0Achallenges%20remain%20in%20representing%20complex%20shapes%2C%20our%20results%20highlight%20the%0Aimportance%20of%20context-aware%20semantic%20gestures%20for%20creating%20expressive%20and%0Acollaborative%20virtual%20agents%20or%20avatars%2C%20marking%20a%20substantial%20step%20forward%0Atowards%20efficient%20and%20robust%2C%20embodied%20human-agent%20interaction.%20More%0Ainformation%20and%20example%20videos%20are%20available%20here%3A%0Ahttps%3A//review-anon-io.github.io/ImaGGen.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImaGGen%253A%2520Zero-Shot%2520Generation%2520of%2520Co-Speech%2520Semantic%2520Gestures%2520Grounded%2520in%250A%2520%2520Language%2520and%2520Image%2520Input%26entry.906535625%3DHendric%2520Voss%2520and%2520Stefan%2520Kopp%26entry.1292438233%3D%2520%2520Human%2520communication%2520combines%2520speech%2520with%2520expressive%2520nonverbal%2520cues%2520such%2520as%250Ahand%2520gestures%2520that%2520serve%2520manifold%2520communicative%2520functions.%2520Yet%252C%2520current%250Agenerative%2520gesture%2520generation%2520approaches%2520are%2520restricted%2520to%2520simple%252C%2520repetitive%250Abeat%2520gestures%2520that%2520accompany%2520the%2520rhythm%2520of%2520speaking%2520but%2520do%2520not%2520contribute%2520to%250Acommunicating%2520semantic%2520meaning.%2520This%2520paper%2520tackles%2520a%2520core%2520challenge%2520in%250Aco-speech%2520gesture%2520synthesis%253A%2520generating%2520iconic%2520or%2520deictic%2520gestures%2520that%2520are%250Asemantically%2520coherent%2520with%2520a%2520verbal%2520utterance.%2520Such%2520gestures%2520cannot%2520be%2520derived%250Afrom%2520language%2520input%2520alone%252C%2520which%2520inherently%2520lacks%2520the%2520visual%2520meaning%2520that%2520is%250Aoften%2520carried%2520autonomously%2520by%2520gestures.%2520We%2520therefore%2520introduce%2520a%2520zero-shot%250Asystem%2520that%2520generates%2520gestures%2520from%2520a%2520given%2520language%2520input%2520and%2520additionally%2520is%250Ainformed%2520by%2520imagistic%2520input%252C%2520without%2520manual%2520annotation%2520or%2520human%2520intervention.%250AOur%2520method%2520integrates%2520an%2520image%2520analysis%2520pipeline%2520that%2520extracts%2520key%2520object%250Aproperties%2520such%2520as%2520shape%252C%2520symmetry%252C%2520and%2520alignment%252C%2520together%2520with%2520a%2520semantic%250Amatching%2520module%2520that%2520links%2520these%2520visual%2520details%2520to%2520spoken%2520text.%2520An%2520inverse%250Akinematics%2520engine%2520then%2520synthesizes%2520iconic%2520and%2520deictic%2520gestures%2520and%2520combines%250Athem%2520with%2520co-generated%2520natural%2520beat%2520gestures%2520for%2520coherent%2520multimodal%250Acommunication.%2520A%2520comprehensive%2520user%2520study%2520demonstrates%2520the%2520effectiveness%2520of%2520our%250Aapproach.%2520In%2520scenarios%2520where%2520speech%2520alone%2520was%2520ambiguous%252C%2520gestures%2520generated%2520by%250Aour%2520system%2520significantly%2520improved%2520participants%2527%2520ability%2520to%2520identify%2520object%250Aproperties%252C%2520confirming%2520their%2520interpretability%2520and%2520communicative%2520value.%2520While%250Achallenges%2520remain%2520in%2520representing%2520complex%2520shapes%252C%2520our%2520results%2520highlight%2520the%250Aimportance%2520of%2520context-aware%2520semantic%2520gestures%2520for%2520creating%2520expressive%2520and%250Acollaborative%2520virtual%2520agents%2520or%2520avatars%252C%2520marking%2520a%2520substantial%2520step%2520forward%250Atowards%2520efficient%2520and%2520robust%252C%2520embodied%2520human-agent%2520interaction.%2520More%250Ainformation%2520and%2520example%2520videos%2520are%2520available%2520here%253A%250Ahttps%253A//review-anon-io.github.io/ImaGGen.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImaGGen%3A%20Zero-Shot%20Generation%20of%20Co-Speech%20Semantic%20Gestures%20Grounded%20in%0A%20%20Language%20and%20Image%20Input&entry.906535625=Hendric%20Voss%20and%20Stefan%20Kopp&entry.1292438233=%20%20Human%20communication%20combines%20speech%20with%20expressive%20nonverbal%20cues%20such%20as%0Ahand%20gestures%20that%20serve%20manifold%20communicative%20functions.%20Yet%2C%20current%0Agenerative%20gesture%20generation%20approaches%20are%20restricted%20to%20simple%2C%20repetitive%0Abeat%20gestures%20that%20accompany%20the%20rhythm%20of%20speaking%20but%20do%20not%20contribute%20to%0Acommunicating%20semantic%20meaning.%20This%20paper%20tackles%20a%20core%20challenge%20in%0Aco-speech%20gesture%20synthesis%3A%20generating%20iconic%20or%20deictic%20gestures%20that%20are%0Asemantically%20coherent%20with%20a%20verbal%20utterance.%20Such%20gestures%20cannot%20be%20derived%0Afrom%20language%20input%20alone%2C%20which%20inherently%20lacks%20the%20visual%20meaning%20that%20is%0Aoften%20carried%20autonomously%20by%20gestures.%20We%20therefore%20introduce%20a%20zero-shot%0Asystem%20that%20generates%20gestures%20from%20a%20given%20language%20input%20and%20additionally%20is%0Ainformed%20by%20imagistic%20input%2C%20without%20manual%20annotation%20or%20human%20intervention.%0AOur%20method%20integrates%20an%20image%20analysis%20pipeline%20that%20extracts%20key%20object%0Aproperties%20such%20as%20shape%2C%20symmetry%2C%20and%20alignment%2C%20together%20with%20a%20semantic%0Amatching%20module%20that%20links%20these%20visual%20details%20to%20spoken%20text.%20An%20inverse%0Akinematics%20engine%20then%20synthesizes%20iconic%20and%20deictic%20gestures%20and%20combines%0Athem%20with%20co-generated%20natural%20beat%20gestures%20for%20coherent%20multimodal%0Acommunication.%20A%20comprehensive%20user%20study%20demonstrates%20the%20effectiveness%20of%20our%0Aapproach.%20In%20scenarios%20where%20speech%20alone%20was%20ambiguous%2C%20gestures%20generated%20by%0Aour%20system%20significantly%20improved%20participants%27%20ability%20to%20identify%20object%0Aproperties%2C%20confirming%20their%20interpretability%20and%20communicative%20value.%20While%0Achallenges%20remain%20in%20representing%20complex%20shapes%2C%20our%20results%20highlight%20the%0Aimportance%20of%20context-aware%20semantic%20gestures%20for%20creating%20expressive%20and%0Acollaborative%20virtual%20agents%20or%20avatars%2C%20marking%20a%20substantial%20step%20forward%0Atowards%20efficient%20and%20robust%2C%20embodied%20human-agent%20interaction.%20More%0Ainformation%20and%20example%20videos%20are%20available%20here%3A%0Ahttps%3A//review-anon-io.github.io/ImaGGen.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17617v1&entry.124074799=Read"},
{"title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop\n  Conditions", "author": "Zhiqiang Teng and Beibei Lin and Tingting Chen and Zifeng Yuan and Xuanyi Li and Xuanyu Zhang and Shunli Zhang", "abstract": "  3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.\n", "link": "http://arxiv.org/abs/2510.17719v1", "date": "2025-10-20", "relevancy": 3.0018, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6423}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5965}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raindrop%20GS%3A%20A%20Benchmark%20for%203D%20Gaussian%20Splatting%20under%20Raindrop%0A%20%20Conditions&body=Title%3A%20Raindrop%20GS%3A%20A%20Benchmark%20for%203D%20Gaussian%20Splatting%20under%20Raindrop%0A%20%20Conditions%0AAuthor%3A%20Zhiqiang%20Teng%20and%20Beibei%20Lin%20and%20Tingting%20Chen%20and%20Zifeng%20Yuan%20and%20Xuanyi%20Li%20and%20Xuanyu%20Zhang%20and%20Shunli%20Zhang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20under%20raindrop%20conditions%20suffers%20from%20severe%0Aocclusions%20and%20optical%20distortions%20caused%20by%20raindrop%20contamination%20on%20the%0Acamera%20lens%2C%20substantially%20degrading%20reconstruction%20quality.%20Existing%0Abenchmarks%20typically%20evaluate%203DGS%20using%20synthetic%20raindrop%20images%20with%20known%0Acamera%20poses%20%28constrained%20images%29%2C%20assuming%20ideal%20conditions.%20However%2C%20in%0Areal-world%20scenarios%2C%20raindrops%20often%20interfere%20with%20accurate%20camera%20pose%0Aestimation%20and%20point%20cloud%20initialization.%20Moreover%2C%20a%20significant%20domain%20gap%0Abetween%20synthetic%20and%20real%20raindrops%20further%20impairs%20generalization.%20To%20tackle%0Athese%20issues%2C%20we%20introduce%20RaindropGS%2C%20a%20comprehensive%20benchmark%20designed%20to%0Aevaluate%20the%20full%203DGS%20pipeline-from%20unconstrained%2C%20raindrop-corrupted%20images%0Ato%20clear%203DGS%20reconstructions.%20Specifically%2C%20the%20whole%20benchmark%20pipeline%0Aconsists%20of%20three%20parts%3A%20data%20preparation%2C%20data%20processing%2C%20and%20raindrop-aware%0A3DGS%20evaluation%2C%20including%20types%20of%20raindrop%20interference%2C%20camera%20pose%0Aestimation%20and%20point%20cloud%20initialization%2C%20single%20image%20rain%20removal%0Acomparison%2C%20and%203D%20Gaussian%20training%20comparison.%20First%2C%20we%20collect%20a%20real-world%0Araindrop%20reconstruction%20dataset%2C%20in%20which%20each%20scene%20contains%20three%20aligned%0Aimage%20sets%3A%20raindrop-focused%2C%20background-focused%2C%20and%20rain-free%20ground%20truth%2C%0Aenabling%20a%20comprehensive%20evaluation%20of%20reconstruction%20quality%20under%20different%0Afocus%20conditions.%20Through%20comprehensive%20experiments%20and%20analyses%2C%20we%20reveal%0Acritical%20insights%20into%20the%20performance%20limitations%20of%20existing%203DGS%20methods%20on%0Aunconstrained%20raindrop%20images%20and%20the%20varying%20impact%20of%20different%20pipeline%0Acomponents%3A%20the%20impact%20of%20camera%20focus%20position%20on%203DGS%20reconstruction%0Aperformance%2C%20and%20the%20interference%20caused%20by%20inaccurate%20pose%20and%20point%20cloud%0Ainitialization%20on%20reconstruction.%20These%20insights%20establish%20clear%20directions%20for%0Adeveloping%20more%20robust%203DGS%20methods%20under%20raindrop%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaindrop%2520GS%253A%2520A%2520Benchmark%2520for%25203D%2520Gaussian%2520Splatting%2520under%2520Raindrop%250A%2520%2520Conditions%26entry.906535625%3DZhiqiang%2520Teng%2520and%2520Beibei%2520Lin%2520and%2520Tingting%2520Chen%2520and%2520Zifeng%2520Yuan%2520and%2520Xuanyi%2520Li%2520and%2520Xuanyu%2520Zhang%2520and%2520Shunli%2520Zhang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520under%2520raindrop%2520conditions%2520suffers%2520from%2520severe%250Aocclusions%2520and%2520optical%2520distortions%2520caused%2520by%2520raindrop%2520contamination%2520on%2520the%250Acamera%2520lens%252C%2520substantially%2520degrading%2520reconstruction%2520quality.%2520Existing%250Abenchmarks%2520typically%2520evaluate%25203DGS%2520using%2520synthetic%2520raindrop%2520images%2520with%2520known%250Acamera%2520poses%2520%2528constrained%2520images%2529%252C%2520assuming%2520ideal%2520conditions.%2520However%252C%2520in%250Areal-world%2520scenarios%252C%2520raindrops%2520often%2520interfere%2520with%2520accurate%2520camera%2520pose%250Aestimation%2520and%2520point%2520cloud%2520initialization.%2520Moreover%252C%2520a%2520significant%2520domain%2520gap%250Abetween%2520synthetic%2520and%2520real%2520raindrops%2520further%2520impairs%2520generalization.%2520To%2520tackle%250Athese%2520issues%252C%2520we%2520introduce%2520RaindropGS%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%250Aevaluate%2520the%2520full%25203DGS%2520pipeline-from%2520unconstrained%252C%2520raindrop-corrupted%2520images%250Ato%2520clear%25203DGS%2520reconstructions.%2520Specifically%252C%2520the%2520whole%2520benchmark%2520pipeline%250Aconsists%2520of%2520three%2520parts%253A%2520data%2520preparation%252C%2520data%2520processing%252C%2520and%2520raindrop-aware%250A3DGS%2520evaluation%252C%2520including%2520types%2520of%2520raindrop%2520interference%252C%2520camera%2520pose%250Aestimation%2520and%2520point%2520cloud%2520initialization%252C%2520single%2520image%2520rain%2520removal%250Acomparison%252C%2520and%25203D%2520Gaussian%2520training%2520comparison.%2520First%252C%2520we%2520collect%2520a%2520real-world%250Araindrop%2520reconstruction%2520dataset%252C%2520in%2520which%2520each%2520scene%2520contains%2520three%2520aligned%250Aimage%2520sets%253A%2520raindrop-focused%252C%2520background-focused%252C%2520and%2520rain-free%2520ground%2520truth%252C%250Aenabling%2520a%2520comprehensive%2520evaluation%2520of%2520reconstruction%2520quality%2520under%2520different%250Afocus%2520conditions.%2520Through%2520comprehensive%2520experiments%2520and%2520analyses%252C%2520we%2520reveal%250Acritical%2520insights%2520into%2520the%2520performance%2520limitations%2520of%2520existing%25203DGS%2520methods%2520on%250Aunconstrained%2520raindrop%2520images%2520and%2520the%2520varying%2520impact%2520of%2520different%2520pipeline%250Acomponents%253A%2520the%2520impact%2520of%2520camera%2520focus%2520position%2520on%25203DGS%2520reconstruction%250Aperformance%252C%2520and%2520the%2520interference%2520caused%2520by%2520inaccurate%2520pose%2520and%2520point%2520cloud%250Ainitialization%2520on%2520reconstruction.%2520These%2520insights%2520establish%2520clear%2520directions%2520for%250Adeveloping%2520more%2520robust%25203DGS%2520methods%2520under%2520raindrop%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raindrop%20GS%3A%20A%20Benchmark%20for%203D%20Gaussian%20Splatting%20under%20Raindrop%0A%20%20Conditions&entry.906535625=Zhiqiang%20Teng%20and%20Beibei%20Lin%20and%20Tingting%20Chen%20and%20Zifeng%20Yuan%20and%20Xuanyi%20Li%20and%20Xuanyu%20Zhang%20and%20Shunli%20Zhang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20under%20raindrop%20conditions%20suffers%20from%20severe%0Aocclusions%20and%20optical%20distortions%20caused%20by%20raindrop%20contamination%20on%20the%0Acamera%20lens%2C%20substantially%20degrading%20reconstruction%20quality.%20Existing%0Abenchmarks%20typically%20evaluate%203DGS%20using%20synthetic%20raindrop%20images%20with%20known%0Acamera%20poses%20%28constrained%20images%29%2C%20assuming%20ideal%20conditions.%20However%2C%20in%0Areal-world%20scenarios%2C%20raindrops%20often%20interfere%20with%20accurate%20camera%20pose%0Aestimation%20and%20point%20cloud%20initialization.%20Moreover%2C%20a%20significant%20domain%20gap%0Abetween%20synthetic%20and%20real%20raindrops%20further%20impairs%20generalization.%20To%20tackle%0Athese%20issues%2C%20we%20introduce%20RaindropGS%2C%20a%20comprehensive%20benchmark%20designed%20to%0Aevaluate%20the%20full%203DGS%20pipeline-from%20unconstrained%2C%20raindrop-corrupted%20images%0Ato%20clear%203DGS%20reconstructions.%20Specifically%2C%20the%20whole%20benchmark%20pipeline%0Aconsists%20of%20three%20parts%3A%20data%20preparation%2C%20data%20processing%2C%20and%20raindrop-aware%0A3DGS%20evaluation%2C%20including%20types%20of%20raindrop%20interference%2C%20camera%20pose%0Aestimation%20and%20point%20cloud%20initialization%2C%20single%20image%20rain%20removal%0Acomparison%2C%20and%203D%20Gaussian%20training%20comparison.%20First%2C%20we%20collect%20a%20real-world%0Araindrop%20reconstruction%20dataset%2C%20in%20which%20each%20scene%20contains%20three%20aligned%0Aimage%20sets%3A%20raindrop-focused%2C%20background-focused%2C%20and%20rain-free%20ground%20truth%2C%0Aenabling%20a%20comprehensive%20evaluation%20of%20reconstruction%20quality%20under%20different%0Afocus%20conditions.%20Through%20comprehensive%20experiments%20and%20analyses%2C%20we%20reveal%0Acritical%20insights%20into%20the%20performance%20limitations%20of%20existing%203DGS%20methods%20on%0Aunconstrained%20raindrop%20images%20and%20the%20varying%20impact%20of%20different%20pipeline%0Acomponents%3A%20the%20impact%20of%20camera%20focus%20position%20on%203DGS%20reconstruction%0Aperformance%2C%20and%20the%20interference%20caused%20by%20inaccurate%20pose%20and%20point%20cloud%0Ainitialization%20on%20reconstruction.%20These%20insights%20establish%20clear%20directions%20for%0Adeveloping%20more%20robust%203DGS%20methods%20under%20raindrop%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17719v1&entry.124074799=Read"},
{"title": "AI-Generated Video Detection via Perceptual Straightening", "author": "Christian Intern\u00f2 and Robert Geirhos and Markus Olhofer and Sunny Liu and Barbara Hammer and David Klindt", "abstract": "  The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.\n", "link": "http://arxiv.org/abs/2507.00583v2", "date": "2025-10-20", "relevancy": 2.9846, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6137}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5885}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Generated%20Video%20Detection%20via%20Perceptual%20Straightening&body=Title%3A%20AI-Generated%20Video%20Detection%20via%20Perceptual%20Straightening%0AAuthor%3A%20Christian%20Intern%C3%B2%20and%20Robert%20Geirhos%20and%20Markus%20Olhofer%20and%20Sunny%20Liu%20and%20Barbara%20Hammer%20and%20David%20Klindt%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20AI%20enables%20highly%20realistic%20synthetic%0Avideos%2C%20posing%20significant%20challenges%20for%20content%20authentication%20and%20raising%0Aurgent%20concerns%20about%20misuse.%20Existing%20detection%20methods%20often%20struggle%20with%0Ageneralization%20and%20capturing%20subtle%20temporal%20inconsistencies.%20We%20propose%0AReStraV%28Representation%20Straightening%20Video%29%2C%20a%20novel%20approach%20to%20distinguish%0Anatural%20from%20AI-generated%20videos.%20Inspired%20by%20the%20%22perceptual%20straightening%22%0Ahypothesis%20--%20which%20suggests%20real-world%20video%20trajectories%20become%20more%20straight%0Ain%20neural%20representation%20domain%20--%20we%20analyze%20deviations%20from%20this%20expected%0Ageometric%20property.%20Using%20a%20pre-trained%20self-supervised%20vision%20transformer%0A%28DINOv2%29%2C%20we%20quantify%20the%20temporal%20curvature%20and%20stepwise%20distance%20in%20the%0Amodel%27s%20representation%20domain.%20We%20aggregate%20statistics%20of%20these%20measures%20for%0Aeach%20video%20and%20train%20a%20classifier.%20Our%20analysis%20shows%20that%20AI-generated%20videos%0Aexhibit%20significantly%20different%20curvature%20and%20distance%20patterns%20compared%20to%0Areal%20videos.%20A%20lightweight%20classifier%20achieves%20state-of-the-art%20detection%0Aperformance%20%28e.g.%2C%2097.17%25%20accuracy%20and%2098.63%25%20AUROC%20on%20the%20VidProM%20benchmark%29%2C%0Asubstantially%20outperforming%20existing%20image-%20and%20video-based%20methods.%20ReStraV%20is%0Acomputationally%20efficient%2C%20it%20is%20offering%20a%20low-cost%20and%20effective%20detection%0Asolution.%20This%20work%20provides%20new%20insights%20into%20using%20neural%20representation%0Ageometry%20for%20AI-generated%20video%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00583v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Generated%2520Video%2520Detection%2520via%2520Perceptual%2520Straightening%26entry.906535625%3DChristian%2520Intern%25C3%25B2%2520and%2520Robert%2520Geirhos%2520and%2520Markus%2520Olhofer%2520and%2520Sunny%2520Liu%2520and%2520Barbara%2520Hammer%2520and%2520David%2520Klindt%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520AI%2520enables%2520highly%2520realistic%2520synthetic%250Avideos%252C%2520posing%2520significant%2520challenges%2520for%2520content%2520authentication%2520and%2520raising%250Aurgent%2520concerns%2520about%2520misuse.%2520Existing%2520detection%2520methods%2520often%2520struggle%2520with%250Ageneralization%2520and%2520capturing%2520subtle%2520temporal%2520inconsistencies.%2520We%2520propose%250AReStraV%2528Representation%2520Straightening%2520Video%2529%252C%2520a%2520novel%2520approach%2520to%2520distinguish%250Anatural%2520from%2520AI-generated%2520videos.%2520Inspired%2520by%2520the%2520%2522perceptual%2520straightening%2522%250Ahypothesis%2520--%2520which%2520suggests%2520real-world%2520video%2520trajectories%2520become%2520more%2520straight%250Ain%2520neural%2520representation%2520domain%2520--%2520we%2520analyze%2520deviations%2520from%2520this%2520expected%250Ageometric%2520property.%2520Using%2520a%2520pre-trained%2520self-supervised%2520vision%2520transformer%250A%2528DINOv2%2529%252C%2520we%2520quantify%2520the%2520temporal%2520curvature%2520and%2520stepwise%2520distance%2520in%2520the%250Amodel%2527s%2520representation%2520domain.%2520We%2520aggregate%2520statistics%2520of%2520these%2520measures%2520for%250Aeach%2520video%2520and%2520train%2520a%2520classifier.%2520Our%2520analysis%2520shows%2520that%2520AI-generated%2520videos%250Aexhibit%2520significantly%2520different%2520curvature%2520and%2520distance%2520patterns%2520compared%2520to%250Areal%2520videos.%2520A%2520lightweight%2520classifier%2520achieves%2520state-of-the-art%2520detection%250Aperformance%2520%2528e.g.%252C%252097.17%2525%2520accuracy%2520and%252098.63%2525%2520AUROC%2520on%2520the%2520VidProM%2520benchmark%2529%252C%250Asubstantially%2520outperforming%2520existing%2520image-%2520and%2520video-based%2520methods.%2520ReStraV%2520is%250Acomputationally%2520efficient%252C%2520it%2520is%2520offering%2520a%2520low-cost%2520and%2520effective%2520detection%250Asolution.%2520This%2520work%2520provides%2520new%2520insights%2520into%2520using%2520neural%2520representation%250Ageometry%2520for%2520AI-generated%2520video%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00583v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Generated%20Video%20Detection%20via%20Perceptual%20Straightening&entry.906535625=Christian%20Intern%C3%B2%20and%20Robert%20Geirhos%20and%20Markus%20Olhofer%20and%20Sunny%20Liu%20and%20Barbara%20Hammer%20and%20David%20Klindt&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20AI%20enables%20highly%20realistic%20synthetic%0Avideos%2C%20posing%20significant%20challenges%20for%20content%20authentication%20and%20raising%0Aurgent%20concerns%20about%20misuse.%20Existing%20detection%20methods%20often%20struggle%20with%0Ageneralization%20and%20capturing%20subtle%20temporal%20inconsistencies.%20We%20propose%0AReStraV%28Representation%20Straightening%20Video%29%2C%20a%20novel%20approach%20to%20distinguish%0Anatural%20from%20AI-generated%20videos.%20Inspired%20by%20the%20%22perceptual%20straightening%22%0Ahypothesis%20--%20which%20suggests%20real-world%20video%20trajectories%20become%20more%20straight%0Ain%20neural%20representation%20domain%20--%20we%20analyze%20deviations%20from%20this%20expected%0Ageometric%20property.%20Using%20a%20pre-trained%20self-supervised%20vision%20transformer%0A%28DINOv2%29%2C%20we%20quantify%20the%20temporal%20curvature%20and%20stepwise%20distance%20in%20the%0Amodel%27s%20representation%20domain.%20We%20aggregate%20statistics%20of%20these%20measures%20for%0Aeach%20video%20and%20train%20a%20classifier.%20Our%20analysis%20shows%20that%20AI-generated%20videos%0Aexhibit%20significantly%20different%20curvature%20and%20distance%20patterns%20compared%20to%0Areal%20videos.%20A%20lightweight%20classifier%20achieves%20state-of-the-art%20detection%0Aperformance%20%28e.g.%2C%2097.17%25%20accuracy%20and%2098.63%25%20AUROC%20on%20the%20VidProM%20benchmark%29%2C%0Asubstantially%20outperforming%20existing%20image-%20and%20video-based%20methods.%20ReStraV%20is%0Acomputationally%20efficient%2C%20it%20is%20offering%20a%20low-cost%20and%20effective%20detection%0Asolution.%20This%20work%20provides%20new%20insights%20into%20using%20neural%20representation%0Ageometry%20for%20AI-generated%20video%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00583v2&entry.124074799=Read"},
{"title": "Seeing but Not Believing: Probing the Disconnect Between Visual\n  Attention and Answer Correctness in VLMs", "author": "Zhining Liu and Ziyi Chen and Hui Liu and Chen Luo and Xianfeng Tang and Suhang Wang and Joy Zeng and Zhenwei Dai and Zhan Shi and Tianxin Wei and Benoit Dumoulin and Hanghang Tong", "abstract": "  Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.\n", "link": "http://arxiv.org/abs/2510.17771v1", "date": "2025-10-20", "relevancy": 2.9721, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6014}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20but%20Not%20Believing%3A%20Probing%20the%20Disconnect%20Between%20Visual%0A%20%20Attention%20and%20Answer%20Correctness%20in%20VLMs&body=Title%3A%20Seeing%20but%20Not%20Believing%3A%20Probing%20the%20Disconnect%20Between%20Visual%0A%20%20Attention%20and%20Answer%20Correctness%20in%20VLMs%0AAuthor%3A%20Zhining%20Liu%20and%20Ziyi%20Chen%20and%20Hui%20Liu%20and%20Chen%20Luo%20and%20Xianfeng%20Tang%20and%20Suhang%20Wang%20and%20Joy%20Zeng%20and%20Zhenwei%20Dai%20and%20Zhan%20Shi%20and%20Tianxin%20Wei%20and%20Benoit%20Dumoulin%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20achieve%20strong%20results%20on%20multimodal%20tasks%20such%0Aas%20visual%20question%20answering%2C%20yet%20they%20can%20still%20fail%20even%20when%20the%20correct%0Avisual%20evidence%20is%20present.%20In%20this%20work%2C%20we%20systematically%20investigate%20whether%0Athese%20failures%20arise%20from%20not%20perceiving%20the%20evidence%20or%20from%20not%20leveraging%20it%0Aeffectively.%20By%20examining%20layer-wise%20attention%20dynamics%2C%20we%20find%20that%20shallow%0Alayers%20focus%20primarily%20on%20text%2C%20while%20deeper%20layers%20sparsely%20but%20reliably%0Aattend%20to%20localized%20evidence%20regions.%20Surprisingly%2C%20VLMs%20often%20perceive%20the%0Avisual%20evidence%20when%20outputting%20incorrect%20answers%2C%20a%20phenomenon%20we%20term%0A%60%60seeing%20but%20not%20believing%27%27%20that%20widely%20exists%20in%20major%20VLM%20families.%20Building%0Aon%20this%2C%20we%20introduce%20an%20inference-time%20intervention%20that%20highlights%20deep-layer%0Aevidence%20regions%20through%20selective%20attention-based%20masking.%20It%20requires%20no%0Atraining%20and%20consistently%20improves%20accuracy%20across%20multiple%20families%2C%20including%0ALLaVA%2C%20Qwen%2C%20Gemma%2C%20and%20InternVL.%20These%20results%20show%20that%20VLMs%20encode%20reliable%0Aevidence%20internally%20but%20under-utilize%20it%2C%20making%20such%20signals%20explicit%20can%0Abridge%20the%20gap%20between%20perception%20and%20reasoning%2C%20advancing%20the%20diagnostic%0Aunderstanding%20and%20reliability%20of%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520but%2520Not%2520Believing%253A%2520Probing%2520the%2520Disconnect%2520Between%2520Visual%250A%2520%2520Attention%2520and%2520Answer%2520Correctness%2520in%2520VLMs%26entry.906535625%3DZhining%2520Liu%2520and%2520Ziyi%2520Chen%2520and%2520Hui%2520Liu%2520and%2520Chen%2520Luo%2520and%2520Xianfeng%2520Tang%2520and%2520Suhang%2520Wang%2520and%2520Joy%2520Zeng%2520and%2520Zhenwei%2520Dai%2520and%2520Zhan%2520Shi%2520and%2520Tianxin%2520Wei%2520and%2520Benoit%2520Dumoulin%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520achieve%2520strong%2520results%2520on%2520multimodal%2520tasks%2520such%250Aas%2520visual%2520question%2520answering%252C%2520yet%2520they%2520can%2520still%2520fail%2520even%2520when%2520the%2520correct%250Avisual%2520evidence%2520is%2520present.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520whether%250Athese%2520failures%2520arise%2520from%2520not%2520perceiving%2520the%2520evidence%2520or%2520from%2520not%2520leveraging%2520it%250Aeffectively.%2520By%2520examining%2520layer-wise%2520attention%2520dynamics%252C%2520we%2520find%2520that%2520shallow%250Alayers%2520focus%2520primarily%2520on%2520text%252C%2520while%2520deeper%2520layers%2520sparsely%2520but%2520reliably%250Aattend%2520to%2520localized%2520evidence%2520regions.%2520Surprisingly%252C%2520VLMs%2520often%2520perceive%2520the%250Avisual%2520evidence%2520when%2520outputting%2520incorrect%2520answers%252C%2520a%2520phenomenon%2520we%2520term%250A%2560%2560seeing%2520but%2520not%2520believing%2527%2527%2520that%2520widely%2520exists%2520in%2520major%2520VLM%2520families.%2520Building%250Aon%2520this%252C%2520we%2520introduce%2520an%2520inference-time%2520intervention%2520that%2520highlights%2520deep-layer%250Aevidence%2520regions%2520through%2520selective%2520attention-based%2520masking.%2520It%2520requires%2520no%250Atraining%2520and%2520consistently%2520improves%2520accuracy%2520across%2520multiple%2520families%252C%2520including%250ALLaVA%252C%2520Qwen%252C%2520Gemma%252C%2520and%2520InternVL.%2520These%2520results%2520show%2520that%2520VLMs%2520encode%2520reliable%250Aevidence%2520internally%2520but%2520under-utilize%2520it%252C%2520making%2520such%2520signals%2520explicit%2520can%250Abridge%2520the%2520gap%2520between%2520perception%2520and%2520reasoning%252C%2520advancing%2520the%2520diagnostic%250Aunderstanding%2520and%2520reliability%2520of%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20but%20Not%20Believing%3A%20Probing%20the%20Disconnect%20Between%20Visual%0A%20%20Attention%20and%20Answer%20Correctness%20in%20VLMs&entry.906535625=Zhining%20Liu%20and%20Ziyi%20Chen%20and%20Hui%20Liu%20and%20Chen%20Luo%20and%20Xianfeng%20Tang%20and%20Suhang%20Wang%20and%20Joy%20Zeng%20and%20Zhenwei%20Dai%20and%20Zhan%20Shi%20and%20Tianxin%20Wei%20and%20Benoit%20Dumoulin%20and%20Hanghang%20Tong&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20achieve%20strong%20results%20on%20multimodal%20tasks%20such%0Aas%20visual%20question%20answering%2C%20yet%20they%20can%20still%20fail%20even%20when%20the%20correct%0Avisual%20evidence%20is%20present.%20In%20this%20work%2C%20we%20systematically%20investigate%20whether%0Athese%20failures%20arise%20from%20not%20perceiving%20the%20evidence%20or%20from%20not%20leveraging%20it%0Aeffectively.%20By%20examining%20layer-wise%20attention%20dynamics%2C%20we%20find%20that%20shallow%0Alayers%20focus%20primarily%20on%20text%2C%20while%20deeper%20layers%20sparsely%20but%20reliably%0Aattend%20to%20localized%20evidence%20regions.%20Surprisingly%2C%20VLMs%20often%20perceive%20the%0Avisual%20evidence%20when%20outputting%20incorrect%20answers%2C%20a%20phenomenon%20we%20term%0A%60%60seeing%20but%20not%20believing%27%27%20that%20widely%20exists%20in%20major%20VLM%20families.%20Building%0Aon%20this%2C%20we%20introduce%20an%20inference-time%20intervention%20that%20highlights%20deep-layer%0Aevidence%20regions%20through%20selective%20attention-based%20masking.%20It%20requires%20no%0Atraining%20and%20consistently%20improves%20accuracy%20across%20multiple%20families%2C%20including%0ALLaVA%2C%20Qwen%2C%20Gemma%2C%20and%20InternVL.%20These%20results%20show%20that%20VLMs%20encode%20reliable%0Aevidence%20internally%20but%20under-utilize%20it%2C%20making%20such%20signals%20explicit%20can%0Abridge%20the%20gap%20between%20perception%20and%20reasoning%2C%20advancing%20the%20diagnostic%0Aunderstanding%20and%20reliability%20of%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17771v1&entry.124074799=Read"},
{"title": "NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose\n  Estimation", "author": "Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li", "abstract": "  The widespread application of 3D human pose estimation (HPE) is limited by\nresource-constrained edge devices, requiring more efficient models. A key\napproach to enhancing efficiency involves designing networks based on the\nstructural characteristics of input data. However, effectively utilizing the\nstructural priors in human skeletal inputs remains challenging. To address\nthis, we leverage both explicit and implicit spatio-temporal priors of the\nhuman body through innovative model design and a pre-training proxy task.\nFirst, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE\nnetwork with stacked Hierarchical Mixers to capture explicit features.\nSpecifically, the spatial Hierarchical Mixer efficiently learns the human\nphysical topology across multiple semantic levels, while the temporal\nHierarchical Mixer with discrete cosine transform and low-pass filtering\ncaptures local instantaneous movements and global action coherence. Moreover,\nEfficient Temporal-Spatial Tokenization (ETST) is introduced to enhance\nspatio-temporal interaction and reduce computational complexity significantly.\nSecond, PoseCLR is proposed as a general pre-training method based on\ncontrastive learning for 3D HPE, aimed at extracting implicit representations\nof human topology. By aligning 2D poses from diverse viewpoints in the proxy\ntask, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing\nthe high-dimensional features of the human body, leading to further performance\nimprovements. Extensive experiments verify that NanoHTNet with PoseCLR\noutperforms other state-of-the-art methods in efficiency, making it ideal for\ndeployment on edge devices like the Jetson Nano. Code and models are available\nat https://github.com/vefalun/NanoHTNet.\n", "link": "http://arxiv.org/abs/2501.15763v2", "date": "2025-10-20", "relevancy": 2.8753, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6055}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5729}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NanoHTNet%3A%20Nano%20Human%20Topology%20Network%20for%20Efficient%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20NanoHTNet%3A%20Nano%20Human%20Topology%20Network%20for%20Efficient%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Jialun%20Cai%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Shuheng%20Zhou%20and%20Wenhao%20Li%0AAbstract%3A%20%20%20The%20widespread%20application%20of%203D%20human%20pose%20estimation%20%28HPE%29%20is%20limited%20by%0Aresource-constrained%20edge%20devices%2C%20requiring%20more%20efficient%20models.%20A%20key%0Aapproach%20to%20enhancing%20efficiency%20involves%20designing%20networks%20based%20on%20the%0Astructural%20characteristics%20of%20input%20data.%20However%2C%20effectively%20utilizing%20the%0Astructural%20priors%20in%20human%20skeletal%20inputs%20remains%20challenging.%20To%20address%0Athis%2C%20we%20leverage%20both%20explicit%20and%20implicit%20spatio-temporal%20priors%20of%20the%0Ahuman%20body%20through%20innovative%20model%20design%20and%20a%20pre-training%20proxy%20task.%0AFirst%2C%20we%20propose%20a%20Nano%20Human%20Topology%20Network%20%28NanoHTNet%29%2C%20a%20tiny%203D%20HPE%0Anetwork%20with%20stacked%20Hierarchical%20Mixers%20to%20capture%20explicit%20features.%0ASpecifically%2C%20the%20spatial%20Hierarchical%20Mixer%20efficiently%20learns%20the%20human%0Aphysical%20topology%20across%20multiple%20semantic%20levels%2C%20while%20the%20temporal%0AHierarchical%20Mixer%20with%20discrete%20cosine%20transform%20and%20low-pass%20filtering%0Acaptures%20local%20instantaneous%20movements%20and%20global%20action%20coherence.%20Moreover%2C%0AEfficient%20Temporal-Spatial%20Tokenization%20%28ETST%29%20is%20introduced%20to%20enhance%0Aspatio-temporal%20interaction%20and%20reduce%20computational%20complexity%20significantly.%0ASecond%2C%20PoseCLR%20is%20proposed%20as%20a%20general%20pre-training%20method%20based%20on%0Acontrastive%20learning%20for%203D%20HPE%2C%20aimed%20at%20extracting%20implicit%20representations%0Aof%20human%20topology.%20By%20aligning%202D%20poses%20from%20diverse%20viewpoints%20in%20the%20proxy%0Atask%2C%20PoseCLR%20aids%203D%20HPE%20encoders%20like%20NanoHTNet%20in%20more%20effectively%20capturing%0Athe%20high-dimensional%20features%20of%20the%20human%20body%2C%20leading%20to%20further%20performance%0Aimprovements.%20Extensive%20experiments%20verify%20that%20NanoHTNet%20with%20PoseCLR%0Aoutperforms%20other%20state-of-the-art%20methods%20in%20efficiency%2C%20making%20it%20ideal%20for%0Adeployment%20on%20edge%20devices%20like%20the%20Jetson%20Nano.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/vefalun/NanoHTNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNanoHTNet%253A%2520Nano%2520Human%2520Topology%2520Network%2520for%2520Efficient%25203D%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DJialun%2520Cai%2520and%2520Mengyuan%2520Liu%2520and%2520Hong%2520Liu%2520and%2520Shuheng%2520Zhou%2520and%2520Wenhao%2520Li%26entry.1292438233%3D%2520%2520The%2520widespread%2520application%2520of%25203D%2520human%2520pose%2520estimation%2520%2528HPE%2529%2520is%2520limited%2520by%250Aresource-constrained%2520edge%2520devices%252C%2520requiring%2520more%2520efficient%2520models.%2520A%2520key%250Aapproach%2520to%2520enhancing%2520efficiency%2520involves%2520designing%2520networks%2520based%2520on%2520the%250Astructural%2520characteristics%2520of%2520input%2520data.%2520However%252C%2520effectively%2520utilizing%2520the%250Astructural%2520priors%2520in%2520human%2520skeletal%2520inputs%2520remains%2520challenging.%2520To%2520address%250Athis%252C%2520we%2520leverage%2520both%2520explicit%2520and%2520implicit%2520spatio-temporal%2520priors%2520of%2520the%250Ahuman%2520body%2520through%2520innovative%2520model%2520design%2520and%2520a%2520pre-training%2520proxy%2520task.%250AFirst%252C%2520we%2520propose%2520a%2520Nano%2520Human%2520Topology%2520Network%2520%2528NanoHTNet%2529%252C%2520a%2520tiny%25203D%2520HPE%250Anetwork%2520with%2520stacked%2520Hierarchical%2520Mixers%2520to%2520capture%2520explicit%2520features.%250ASpecifically%252C%2520the%2520spatial%2520Hierarchical%2520Mixer%2520efficiently%2520learns%2520the%2520human%250Aphysical%2520topology%2520across%2520multiple%2520semantic%2520levels%252C%2520while%2520the%2520temporal%250AHierarchical%2520Mixer%2520with%2520discrete%2520cosine%2520transform%2520and%2520low-pass%2520filtering%250Acaptures%2520local%2520instantaneous%2520movements%2520and%2520global%2520action%2520coherence.%2520Moreover%252C%250AEfficient%2520Temporal-Spatial%2520Tokenization%2520%2528ETST%2529%2520is%2520introduced%2520to%2520enhance%250Aspatio-temporal%2520interaction%2520and%2520reduce%2520computational%2520complexity%2520significantly.%250ASecond%252C%2520PoseCLR%2520is%2520proposed%2520as%2520a%2520general%2520pre-training%2520method%2520based%2520on%250Acontrastive%2520learning%2520for%25203D%2520HPE%252C%2520aimed%2520at%2520extracting%2520implicit%2520representations%250Aof%2520human%2520topology.%2520By%2520aligning%25202D%2520poses%2520from%2520diverse%2520viewpoints%2520in%2520the%2520proxy%250Atask%252C%2520PoseCLR%2520aids%25203D%2520HPE%2520encoders%2520like%2520NanoHTNet%2520in%2520more%2520effectively%2520capturing%250Athe%2520high-dimensional%2520features%2520of%2520the%2520human%2520body%252C%2520leading%2520to%2520further%2520performance%250Aimprovements.%2520Extensive%2520experiments%2520verify%2520that%2520NanoHTNet%2520with%2520PoseCLR%250Aoutperforms%2520other%2520state-of-the-art%2520methods%2520in%2520efficiency%252C%2520making%2520it%2520ideal%2520for%250Adeployment%2520on%2520edge%2520devices%2520like%2520the%2520Jetson%2520Nano.%2520Code%2520and%2520models%2520are%2520available%250Aat%2520https%253A//github.com/vefalun/NanoHTNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NanoHTNet%3A%20Nano%20Human%20Topology%20Network%20for%20Efficient%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Jialun%20Cai%20and%20Mengyuan%20Liu%20and%20Hong%20Liu%20and%20Shuheng%20Zhou%20and%20Wenhao%20Li&entry.1292438233=%20%20The%20widespread%20application%20of%203D%20human%20pose%20estimation%20%28HPE%29%20is%20limited%20by%0Aresource-constrained%20edge%20devices%2C%20requiring%20more%20efficient%20models.%20A%20key%0Aapproach%20to%20enhancing%20efficiency%20involves%20designing%20networks%20based%20on%20the%0Astructural%20characteristics%20of%20input%20data.%20However%2C%20effectively%20utilizing%20the%0Astructural%20priors%20in%20human%20skeletal%20inputs%20remains%20challenging.%20To%20address%0Athis%2C%20we%20leverage%20both%20explicit%20and%20implicit%20spatio-temporal%20priors%20of%20the%0Ahuman%20body%20through%20innovative%20model%20design%20and%20a%20pre-training%20proxy%20task.%0AFirst%2C%20we%20propose%20a%20Nano%20Human%20Topology%20Network%20%28NanoHTNet%29%2C%20a%20tiny%203D%20HPE%0Anetwork%20with%20stacked%20Hierarchical%20Mixers%20to%20capture%20explicit%20features.%0ASpecifically%2C%20the%20spatial%20Hierarchical%20Mixer%20efficiently%20learns%20the%20human%0Aphysical%20topology%20across%20multiple%20semantic%20levels%2C%20while%20the%20temporal%0AHierarchical%20Mixer%20with%20discrete%20cosine%20transform%20and%20low-pass%20filtering%0Acaptures%20local%20instantaneous%20movements%20and%20global%20action%20coherence.%20Moreover%2C%0AEfficient%20Temporal-Spatial%20Tokenization%20%28ETST%29%20is%20introduced%20to%20enhance%0Aspatio-temporal%20interaction%20and%20reduce%20computational%20complexity%20significantly.%0ASecond%2C%20PoseCLR%20is%20proposed%20as%20a%20general%20pre-training%20method%20based%20on%0Acontrastive%20learning%20for%203D%20HPE%2C%20aimed%20at%20extracting%20implicit%20representations%0Aof%20human%20topology.%20By%20aligning%202D%20poses%20from%20diverse%20viewpoints%20in%20the%20proxy%0Atask%2C%20PoseCLR%20aids%203D%20HPE%20encoders%20like%20NanoHTNet%20in%20more%20effectively%20capturing%0Athe%20high-dimensional%20features%20of%20the%20human%20body%2C%20leading%20to%20further%20performance%0Aimprovements.%20Extensive%20experiments%20verify%20that%20NanoHTNet%20with%20PoseCLR%0Aoutperforms%20other%20state-of-the-art%20methods%20in%20efficiency%2C%20making%20it%20ideal%20for%0Adeployment%20on%20edge%20devices%20like%20the%20Jetson%20Nano.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/vefalun/NanoHTNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15763v2&entry.124074799=Read"},
{"title": "When Words Smile: Generating Diverse Emotional Facial Expressions from\n  Text", "author": "Haidong Xu and Meishan Zhang and Hao Ju and Zhedong Zheng and Erik Cambria and Min Zhang and Hao Fei", "abstract": "  Enabling digital humans to express rich emotions has significant applications\nin dialogue systems, gaming, and other interactive scenarios. While recent\nadvances in talking head synthesis have achieved impressive results in lip\nsynchronization, they tend to overlook the rich and dynamic nature of facial\nexpressions. To fill this critical gap, we introduce an end-to-end\ntext-to-expression model that explicitly focuses on emotional dynamics. Our\nmodel learns expressive facial variations in a continuous latent space and\ngenerates expressions that are diverse, fluid, and emotionally coherent. To\nsupport this task, we introduce EmoAva, a large-scale and high-quality dataset\ncontaining 15,000 text-3D expression pairs. Extensive experiments on both\nexisting datasets and EmoAva demonstrate that our method significantly\noutperforms baselines across multiple evaluation metrics, marking a significant\nadvancement in the field.\n", "link": "http://arxiv.org/abs/2412.02508v4", "date": "2025-10-20", "relevancy": 2.8404, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5853}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5615}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Words%20Smile%3A%20Generating%20Diverse%20Emotional%20Facial%20Expressions%20from%0A%20%20Text&body=Title%3A%20When%20Words%20Smile%3A%20Generating%20Diverse%20Emotional%20Facial%20Expressions%20from%0A%20%20Text%0AAuthor%3A%20Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20Enabling%20digital%20humans%20to%20express%20rich%20emotions%20has%20significant%20applications%0Ain%20dialogue%20systems%2C%20gaming%2C%20and%20other%20interactive%20scenarios.%20While%20recent%0Aadvances%20in%20talking%20head%20synthesis%20have%20achieved%20impressive%20results%20in%20lip%0Asynchronization%2C%20they%20tend%20to%20overlook%20the%20rich%20and%20dynamic%20nature%20of%20facial%0Aexpressions.%20To%20fill%20this%20critical%20gap%2C%20we%20introduce%20an%20end-to-end%0Atext-to-expression%20model%20that%20explicitly%20focuses%20on%20emotional%20dynamics.%20Our%0Amodel%20learns%20expressive%20facial%20variations%20in%20a%20continuous%20latent%20space%20and%0Agenerates%20expressions%20that%20are%20diverse%2C%20fluid%2C%20and%20emotionally%20coherent.%20To%0Asupport%20this%20task%2C%20we%20introduce%20EmoAva%2C%20a%20large-scale%20and%20high-quality%20dataset%0Acontaining%2015%2C000%20text-3D%20expression%20pairs.%20Extensive%20experiments%20on%20both%0Aexisting%20datasets%20and%20EmoAva%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20baselines%20across%20multiple%20evaluation%20metrics%2C%20marking%20a%20significant%0Aadvancement%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02508v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Words%2520Smile%253A%2520Generating%2520Diverse%2520Emotional%2520Facial%2520Expressions%2520from%250A%2520%2520Text%26entry.906535625%3DHaidong%2520Xu%2520and%2520Meishan%2520Zhang%2520and%2520Hao%2520Ju%2520and%2520Zhedong%2520Zheng%2520and%2520Erik%2520Cambria%2520and%2520Min%2520Zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520Enabling%2520digital%2520humans%2520to%2520express%2520rich%2520emotions%2520has%2520significant%2520applications%250Ain%2520dialogue%2520systems%252C%2520gaming%252C%2520and%2520other%2520interactive%2520scenarios.%2520While%2520recent%250Aadvances%2520in%2520talking%2520head%2520synthesis%2520have%2520achieved%2520impressive%2520results%2520in%2520lip%250Asynchronization%252C%2520they%2520tend%2520to%2520overlook%2520the%2520rich%2520and%2520dynamic%2520nature%2520of%2520facial%250Aexpressions.%2520To%2520fill%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520an%2520end-to-end%250Atext-to-expression%2520model%2520that%2520explicitly%2520focuses%2520on%2520emotional%2520dynamics.%2520Our%250Amodel%2520learns%2520expressive%2520facial%2520variations%2520in%2520a%2520continuous%2520latent%2520space%2520and%250Agenerates%2520expressions%2520that%2520are%2520diverse%252C%2520fluid%252C%2520and%2520emotionally%2520coherent.%2520To%250Asupport%2520this%2520task%252C%2520we%2520introduce%2520EmoAva%252C%2520a%2520large-scale%2520and%2520high-quality%2520dataset%250Acontaining%252015%252C000%2520text-3D%2520expression%2520pairs.%2520Extensive%2520experiments%2520on%2520both%250Aexisting%2520datasets%2520and%2520EmoAva%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520baselines%2520across%2520multiple%2520evaluation%2520metrics%252C%2520marking%2520a%2520significant%250Aadvancement%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02508v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Words%20Smile%3A%20Generating%20Diverse%20Emotional%20Facial%20Expressions%20from%0A%20%20Text&entry.906535625=Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei&entry.1292438233=%20%20Enabling%20digital%20humans%20to%20express%20rich%20emotions%20has%20significant%20applications%0Ain%20dialogue%20systems%2C%20gaming%2C%20and%20other%20interactive%20scenarios.%20While%20recent%0Aadvances%20in%20talking%20head%20synthesis%20have%20achieved%20impressive%20results%20in%20lip%0Asynchronization%2C%20they%20tend%20to%20overlook%20the%20rich%20and%20dynamic%20nature%20of%20facial%0Aexpressions.%20To%20fill%20this%20critical%20gap%2C%20we%20introduce%20an%20end-to-end%0Atext-to-expression%20model%20that%20explicitly%20focuses%20on%20emotional%20dynamics.%20Our%0Amodel%20learns%20expressive%20facial%20variations%20in%20a%20continuous%20latent%20space%20and%0Agenerates%20expressions%20that%20are%20diverse%2C%20fluid%2C%20and%20emotionally%20coherent.%20To%0Asupport%20this%20task%2C%20we%20introduce%20EmoAva%2C%20a%20large-scale%20and%20high-quality%20dataset%0Acontaining%2015%2C000%20text-3D%20expression%20pairs.%20Extensive%20experiments%20on%20both%0Aexisting%20datasets%20and%20EmoAva%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20baselines%20across%20multiple%20evaluation%20metrics%2C%20marking%20a%20significant%0Aadvancement%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02508v4&entry.124074799=Read"},
{"title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues", "author": "Yaning Pan and Zekun Wang and Qianqian Xie and Yongqian Wen and Yuanxing Zhang and Guohui Zhang and Haoxuan Hu and Zhiyu Pan and Yibing Huang and Zhidong Gan and Yonghong Lin and An Ping and Tianhao Peng and Jiaheng Liu", "abstract": "  The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.\n", "link": "http://arxiv.org/abs/2510.17722v1", "date": "2025-10-20", "relevancy": 2.8183, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%0A%20%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues&body=Title%3A%20MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%0A%20%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues%0AAuthor%3A%20Yaning%20Pan%20and%20Zekun%20Wang%20and%20Qianqian%20Xie%20and%20Yongqian%20Wen%20and%20Yuanxing%20Zhang%20and%20Guohui%20Zhang%20and%20Haoxuan%20Hu%20and%20Zhiyu%20Pan%20and%20Yibing%20Huang%20and%20Zhidong%20Gan%20and%20Yonghong%20Lin%20and%20An%20Ping%20and%20Tianhao%20Peng%20and%20Jiaheng%20Liu%0AAbstract%3A%20%20%20The%20recent%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Asignificantly%20advanced%20AI%27s%20ability%20to%20understand%20visual%20modalities.%20However%2C%0Aexisting%20evaluation%20benchmarks%20remain%20limited%20to%20single-turn%20question%0Aanswering%2C%20overlooking%20the%20complexity%20of%20multi-turn%20dialogues%20in%20real-world%0Ascenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MT-Video-Bench%2C%20a%20holistic%20video%0Aunderstanding%20benchmark%20for%20evaluating%20MLLMs%20in%20multi-turn%20dialogues.%0ASpecifically%2C%20our%20MT-Video-Bench%20mainly%20assesses%20six%20core%20competencies%20that%0Afocus%20on%20perceptivity%20and%20interactivity%2C%20encompassing%20987%20meticulously%20curated%0Amulti-turn%20dialogues%20from%20diverse%20domains.%20These%20capabilities%20are%20rigorously%0Aaligned%20with%20real-world%20applications%2C%20such%20as%20interactive%20sports%20analysis%20and%0Amulti-turn%20video-based%20intelligent%20tutoring.%20With%20MT-Video-Bench%2C%20we%0Aextensively%20evaluate%20various%20state-of-the-art%20open-source%20and%20closed-source%0AMLLMs%2C%20revealing%20their%20significant%20performance%20discrepancies%20and%20limitations%20in%0Ahandling%20multi-turn%20video%20dialogues.%20The%20benchmark%20will%20be%20publicly%20available%0Ato%20foster%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Video-Bench%253A%2520A%2520Holistic%2520Video%2520Understanding%2520Benchmark%2520for%2520Evaluating%250A%2520%2520Multimodal%2520LLMs%2520in%2520Multi-Turn%2520Dialogues%26entry.906535625%3DYaning%2520Pan%2520and%2520Zekun%2520Wang%2520and%2520Qianqian%2520Xie%2520and%2520Yongqian%2520Wen%2520and%2520Yuanxing%2520Zhang%2520and%2520Guohui%2520Zhang%2520and%2520Haoxuan%2520Hu%2520and%2520Zhiyu%2520Pan%2520and%2520Yibing%2520Huang%2520and%2520Zhidong%2520Gan%2520and%2520Yonghong%2520Lin%2520and%2520An%2520Ping%2520and%2520Tianhao%2520Peng%2520and%2520Jiaheng%2520Liu%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%250Asignificantly%2520advanced%2520AI%2527s%2520ability%2520to%2520understand%2520visual%2520modalities.%2520However%252C%250Aexisting%2520evaluation%2520benchmarks%2520remain%2520limited%2520to%2520single-turn%2520question%250Aanswering%252C%2520overlooking%2520the%2520complexity%2520of%2520multi-turn%2520dialogues%2520in%2520real-world%250Ascenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MT-Video-Bench%252C%2520a%2520holistic%2520video%250Aunderstanding%2520benchmark%2520for%2520evaluating%2520MLLMs%2520in%2520multi-turn%2520dialogues.%250ASpecifically%252C%2520our%2520MT-Video-Bench%2520mainly%2520assesses%2520six%2520core%2520competencies%2520that%250Afocus%2520on%2520perceptivity%2520and%2520interactivity%252C%2520encompassing%2520987%2520meticulously%2520curated%250Amulti-turn%2520dialogues%2520from%2520diverse%2520domains.%2520These%2520capabilities%2520are%2520rigorously%250Aaligned%2520with%2520real-world%2520applications%252C%2520such%2520as%2520interactive%2520sports%2520analysis%2520and%250Amulti-turn%2520video-based%2520intelligent%2520tutoring.%2520With%2520MT-Video-Bench%252C%2520we%250Aextensively%2520evaluate%2520various%2520state-of-the-art%2520open-source%2520and%2520closed-source%250AMLLMs%252C%2520revealing%2520their%2520significant%2520performance%2520discrepancies%2520and%2520limitations%2520in%250Ahandling%2520multi-turn%2520video%2520dialogues.%2520The%2520benchmark%2520will%2520be%2520publicly%2520available%250Ato%2520foster%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Video-Bench%3A%20A%20Holistic%20Video%20Understanding%20Benchmark%20for%20Evaluating%0A%20%20Multimodal%20LLMs%20in%20Multi-Turn%20Dialogues&entry.906535625=Yaning%20Pan%20and%20Zekun%20Wang%20and%20Qianqian%20Xie%20and%20Yongqian%20Wen%20and%20Yuanxing%20Zhang%20and%20Guohui%20Zhang%20and%20Haoxuan%20Hu%20and%20Zhiyu%20Pan%20and%20Yibing%20Huang%20and%20Zhidong%20Gan%20and%20Yonghong%20Lin%20and%20An%20Ping%20and%20Tianhao%20Peng%20and%20Jiaheng%20Liu&entry.1292438233=%20%20The%20recent%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Asignificantly%20advanced%20AI%27s%20ability%20to%20understand%20visual%20modalities.%20However%2C%0Aexisting%20evaluation%20benchmarks%20remain%20limited%20to%20single-turn%20question%0Aanswering%2C%20overlooking%20the%20complexity%20of%20multi-turn%20dialogues%20in%20real-world%0Ascenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20MT-Video-Bench%2C%20a%20holistic%20video%0Aunderstanding%20benchmark%20for%20evaluating%20MLLMs%20in%20multi-turn%20dialogues.%0ASpecifically%2C%20our%20MT-Video-Bench%20mainly%20assesses%20six%20core%20competencies%20that%0Afocus%20on%20perceptivity%20and%20interactivity%2C%20encompassing%20987%20meticulously%20curated%0Amulti-turn%20dialogues%20from%20diverse%20domains.%20These%20capabilities%20are%20rigorously%0Aaligned%20with%20real-world%20applications%2C%20such%20as%20interactive%20sports%20analysis%20and%0Amulti-turn%20video-based%20intelligent%20tutoring.%20With%20MT-Video-Bench%2C%20we%0Aextensively%20evaluate%20various%20state-of-the-art%20open-source%20and%20closed-source%0AMLLMs%2C%20revealing%20their%20significant%20performance%20discrepancies%20and%20limitations%20in%0Ahandling%20multi-turn%20video%20dialogues.%20The%20benchmark%20will%20be%20publicly%20available%0Ato%20foster%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17722v1&entry.124074799=Read"},
{"title": "DitHub: A Modular Framework for Incremental Open-Vocabulary Object\n  Detection", "author": "Chiara Cappellino and Gianluca Mancusi and Matteo Mosconi and Angelo Porrello and Simone Calderara and Rita Cucchiara", "abstract": "  Open-Vocabulary object detectors can generalize to an unrestricted set of\ncategories through simple textual prompting. However, adapting these models to\nrare classes or reinforcing their abilities on multiple specialized domains\nremains essential. While recent methods rely on monolithic adaptation\nstrategies with a single set of weights, we embrace modular deep learning. We\nintroduce DitHub, a framework designed to build and maintain a library of\nefficient adaptation modules. Inspired by Version Control Systems, DitHub\nmanages expert modules as branches that can be fetched and merged as needed.\nThis modular approach allows us to conduct an in-depth exploration of the\ncompositional properties of adaptation modules, marking the first such study in\nObject Detection. Our method achieves state-of-the-art performance on the\nODinW-13 benchmark and ODinW-O, a newly introduced benchmark designed to assess\nclass reappearance. For more details, visit our project page:\nhttps://aimagelab.github.io/DitHub/\n", "link": "http://arxiv.org/abs/2503.09271v3", "date": "2025-10-20", "relevancy": 2.8022, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DitHub%3A%20A%20Modular%20Framework%20for%20Incremental%20Open-Vocabulary%20Object%0A%20%20Detection&body=Title%3A%20DitHub%3A%20A%20Modular%20Framework%20for%20Incremental%20Open-Vocabulary%20Object%0A%20%20Detection%0AAuthor%3A%20Chiara%20Cappellino%20and%20Gianluca%20Mancusi%20and%20Matteo%20Mosconi%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Open-Vocabulary%20object%20detectors%20can%20generalize%20to%20an%20unrestricted%20set%20of%0Acategories%20through%20simple%20textual%20prompting.%20However%2C%20adapting%20these%20models%20to%0Arare%20classes%20or%20reinforcing%20their%20abilities%20on%20multiple%20specialized%20domains%0Aremains%20essential.%20While%20recent%20methods%20rely%20on%20monolithic%20adaptation%0Astrategies%20with%20a%20single%20set%20of%20weights%2C%20we%20embrace%20modular%20deep%20learning.%20We%0Aintroduce%20DitHub%2C%20a%20framework%20designed%20to%20build%20and%20maintain%20a%20library%20of%0Aefficient%20adaptation%20modules.%20Inspired%20by%20Version%20Control%20Systems%2C%20DitHub%0Amanages%20expert%20modules%20as%20branches%20that%20can%20be%20fetched%20and%20merged%20as%20needed.%0AThis%20modular%20approach%20allows%20us%20to%20conduct%20an%20in-depth%20exploration%20of%20the%0Acompositional%20properties%20of%20adaptation%20modules%2C%20marking%20the%20first%20such%20study%20in%0AObject%20Detection.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20the%0AODinW-13%20benchmark%20and%20ODinW-O%2C%20a%20newly%20introduced%20benchmark%20designed%20to%20assess%0Aclass%20reappearance.%20For%20more%20details%2C%20visit%20our%20project%20page%3A%0Ahttps%3A//aimagelab.github.io/DitHub/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09271v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDitHub%253A%2520A%2520Modular%2520Framework%2520for%2520Incremental%2520Open-Vocabulary%2520Object%250A%2520%2520Detection%26entry.906535625%3DChiara%2520Cappellino%2520and%2520Gianluca%2520Mancusi%2520and%2520Matteo%2520Mosconi%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520object%2520detectors%2520can%2520generalize%2520to%2520an%2520unrestricted%2520set%2520of%250Acategories%2520through%2520simple%2520textual%2520prompting.%2520However%252C%2520adapting%2520these%2520models%2520to%250Arare%2520classes%2520or%2520reinforcing%2520their%2520abilities%2520on%2520multiple%2520specialized%2520domains%250Aremains%2520essential.%2520While%2520recent%2520methods%2520rely%2520on%2520monolithic%2520adaptation%250Astrategies%2520with%2520a%2520single%2520set%2520of%2520weights%252C%2520we%2520embrace%2520modular%2520deep%2520learning.%2520We%250Aintroduce%2520DitHub%252C%2520a%2520framework%2520designed%2520to%2520build%2520and%2520maintain%2520a%2520library%2520of%250Aefficient%2520adaptation%2520modules.%2520Inspired%2520by%2520Version%2520Control%2520Systems%252C%2520DitHub%250Amanages%2520expert%2520modules%2520as%2520branches%2520that%2520can%2520be%2520fetched%2520and%2520merged%2520as%2520needed.%250AThis%2520modular%2520approach%2520allows%2520us%2520to%2520conduct%2520an%2520in-depth%2520exploration%2520of%2520the%250Acompositional%2520properties%2520of%2520adaptation%2520modules%252C%2520marking%2520the%2520first%2520such%2520study%2520in%250AObject%2520Detection.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250AODinW-13%2520benchmark%2520and%2520ODinW-O%252C%2520a%2520newly%2520introduced%2520benchmark%2520designed%2520to%2520assess%250Aclass%2520reappearance.%2520For%2520more%2520details%252C%2520visit%2520our%2520project%2520page%253A%250Ahttps%253A//aimagelab.github.io/DitHub/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09271v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DitHub%3A%20A%20Modular%20Framework%20for%20Incremental%20Open-Vocabulary%20Object%0A%20%20Detection&entry.906535625=Chiara%20Cappellino%20and%20Gianluca%20Mancusi%20and%20Matteo%20Mosconi%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Open-Vocabulary%20object%20detectors%20can%20generalize%20to%20an%20unrestricted%20set%20of%0Acategories%20through%20simple%20textual%20prompting.%20However%2C%20adapting%20these%20models%20to%0Arare%20classes%20or%20reinforcing%20their%20abilities%20on%20multiple%20specialized%20domains%0Aremains%20essential.%20While%20recent%20methods%20rely%20on%20monolithic%20adaptation%0Astrategies%20with%20a%20single%20set%20of%20weights%2C%20we%20embrace%20modular%20deep%20learning.%20We%0Aintroduce%20DitHub%2C%20a%20framework%20designed%20to%20build%20and%20maintain%20a%20library%20of%0Aefficient%20adaptation%20modules.%20Inspired%20by%20Version%20Control%20Systems%2C%20DitHub%0Amanages%20expert%20modules%20as%20branches%20that%20can%20be%20fetched%20and%20merged%20as%20needed.%0AThis%20modular%20approach%20allows%20us%20to%20conduct%20an%20in-depth%20exploration%20of%20the%0Acompositional%20properties%20of%20adaptation%20modules%2C%20marking%20the%20first%20such%20study%20in%0AObject%20Detection.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20the%0AODinW-13%20benchmark%20and%20ODinW-O%2C%20a%20newly%20introduced%20benchmark%20designed%20to%20assess%0Aclass%20reappearance.%20For%20more%20details%2C%20visit%20our%20project%20page%3A%0Ahttps%3A//aimagelab.github.io/DitHub/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09271v3&entry.124074799=Read"},
{"title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant\n  Structures with Gaussian Splats", "author": "Simeon Adebola and Chung Min Kim and Justin Kerr and Shuangyu Xie and Prithvi Akella and Jose Luis Susa Rincon and Eugen Solowjow and Ken Goldberg", "abstract": "  Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.\n", "link": "http://arxiv.org/abs/2510.17783v1", "date": "2025-10-20", "relevancy": 2.7295, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5664}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5475}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Botany-Bot%3A%20Digital%20Twin%20Monitoring%20of%20Occluded%20and%20Underleaf%20Plant%0A%20%20Structures%20with%20Gaussian%20Splats&body=Title%3A%20Botany-Bot%3A%20Digital%20Twin%20Monitoring%20of%20Occluded%20and%20Underleaf%20Plant%0A%20%20Structures%20with%20Gaussian%20Splats%0AAuthor%3A%20Simeon%20Adebola%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Shuangyu%20Xie%20and%20Prithvi%20Akella%20and%20Jose%20Luis%20Susa%20Rincon%20and%20Eugen%20Solowjow%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Commercial%20plant%20phenotyping%20systems%20using%20fixed%20cameras%20cannot%20perceive%20many%0Aplant%20details%20due%20to%20leaf%20occlusion.%20In%20this%20paper%2C%20we%20present%20Botany-Bot%2C%20a%0Asystem%20for%20building%20detailed%20%22annotated%20digital%20twins%22%20of%20living%20plants%20using%0Atwo%20stereo%20cameras%2C%20a%20digital%20turntable%20inside%20a%20lightbox%2C%20an%20industrial%20robot%0Aarm%2C%20and%203D%20segmentated%20Gaussian%20Splat%20models.%20We%20also%20present%20robot%20algorithms%0Afor%20manipulating%20leaves%20to%20take%20high-resolution%20indexable%20images%20of%20occluded%0Adetails%20such%20as%20stem%20buds%20and%20the%20underside/topside%20of%20leaves.%20Results%20from%0Aexperiments%20suggest%20that%20Botany-Bot%20can%20segment%20leaves%20with%2090.8%25%20accuracy%2C%0Adetect%20leaves%20with%2086.2%25%20accuracy%2C%20lift/push%20leaves%20with%2077.9%25%20accuracy%2C%20and%0Atake%20detailed%20overside/underside%20images%20with%2077.3%25%20accuracy.%20Code%2C%20videos%2C%20and%0Adatasets%20are%20available%20at%20https%3A//berkeleyautomation.github.io/Botany-Bot/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBotany-Bot%253A%2520Digital%2520Twin%2520Monitoring%2520of%2520Occluded%2520and%2520Underleaf%2520Plant%250A%2520%2520Structures%2520with%2520Gaussian%2520Splats%26entry.906535625%3DSimeon%2520Adebola%2520and%2520Chung%2520Min%2520Kim%2520and%2520Justin%2520Kerr%2520and%2520Shuangyu%2520Xie%2520and%2520Prithvi%2520Akella%2520and%2520Jose%2520Luis%2520Susa%2520Rincon%2520and%2520Eugen%2520Solowjow%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Commercial%2520plant%2520phenotyping%2520systems%2520using%2520fixed%2520cameras%2520cannot%2520perceive%2520many%250Aplant%2520details%2520due%2520to%2520leaf%2520occlusion.%2520In%2520this%2520paper%252C%2520we%2520present%2520Botany-Bot%252C%2520a%250Asystem%2520for%2520building%2520detailed%2520%2522annotated%2520digital%2520twins%2522%2520of%2520living%2520plants%2520using%250Atwo%2520stereo%2520cameras%252C%2520a%2520digital%2520turntable%2520inside%2520a%2520lightbox%252C%2520an%2520industrial%2520robot%250Aarm%252C%2520and%25203D%2520segmentated%2520Gaussian%2520Splat%2520models.%2520We%2520also%2520present%2520robot%2520algorithms%250Afor%2520manipulating%2520leaves%2520to%2520take%2520high-resolution%2520indexable%2520images%2520of%2520occluded%250Adetails%2520such%2520as%2520stem%2520buds%2520and%2520the%2520underside/topside%2520of%2520leaves.%2520Results%2520from%250Aexperiments%2520suggest%2520that%2520Botany-Bot%2520can%2520segment%2520leaves%2520with%252090.8%2525%2520accuracy%252C%250Adetect%2520leaves%2520with%252086.2%2525%2520accuracy%252C%2520lift/push%2520leaves%2520with%252077.9%2525%2520accuracy%252C%2520and%250Atake%2520detailed%2520overside/underside%2520images%2520with%252077.3%2525%2520accuracy.%2520Code%252C%2520videos%252C%2520and%250Adatasets%2520are%2520available%2520at%2520https%253A//berkeleyautomation.github.io/Botany-Bot/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Botany-Bot%3A%20Digital%20Twin%20Monitoring%20of%20Occluded%20and%20Underleaf%20Plant%0A%20%20Structures%20with%20Gaussian%20Splats&entry.906535625=Simeon%20Adebola%20and%20Chung%20Min%20Kim%20and%20Justin%20Kerr%20and%20Shuangyu%20Xie%20and%20Prithvi%20Akella%20and%20Jose%20Luis%20Susa%20Rincon%20and%20Eugen%20Solowjow%20and%20Ken%20Goldberg&entry.1292438233=%20%20Commercial%20plant%20phenotyping%20systems%20using%20fixed%20cameras%20cannot%20perceive%20many%0Aplant%20details%20due%20to%20leaf%20occlusion.%20In%20this%20paper%2C%20we%20present%20Botany-Bot%2C%20a%0Asystem%20for%20building%20detailed%20%22annotated%20digital%20twins%22%20of%20living%20plants%20using%0Atwo%20stereo%20cameras%2C%20a%20digital%20turntable%20inside%20a%20lightbox%2C%20an%20industrial%20robot%0Aarm%2C%20and%203D%20segmentated%20Gaussian%20Splat%20models.%20We%20also%20present%20robot%20algorithms%0Afor%20manipulating%20leaves%20to%20take%20high-resolution%20indexable%20images%20of%20occluded%0Adetails%20such%20as%20stem%20buds%20and%20the%20underside/topside%20of%20leaves.%20Results%20from%0Aexperiments%20suggest%20that%20Botany-Bot%20can%20segment%20leaves%20with%2090.8%25%20accuracy%2C%0Adetect%20leaves%20with%2086.2%25%20accuracy%2C%20lift/push%20leaves%20with%2077.9%25%20accuracy%2C%20and%0Atake%20detailed%20overside/underside%20images%20with%2077.3%25%20accuracy.%20Code%2C%20videos%2C%20and%0Adatasets%20are%20available%20at%20https%3A//berkeleyautomation.github.io/Botany-Bot/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17783v1&entry.124074799=Read"},
{"title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation\n  Reasoning and Aligning", "author": "Min Cao and Xinyu Zhou and Ding Jiang and Bo Du and Mang Ye and Min Zhang", "abstract": "  Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.\n", "link": "http://arxiv.org/abs/2510.17685v1", "date": "2025-10-20", "relevancy": 2.6983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6053}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Text-to-Image%20Person%20Retrieval%20via%20Bidirectional%20Relation%0A%20%20Reasoning%20and%20Aligning&body=Title%3A%20Multilingual%20Text-to-Image%20Person%20Retrieval%20via%20Bidirectional%20Relation%0A%20%20Reasoning%20and%20Aligning%0AAuthor%3A%20Min%20Cao%20and%20Xinyu%20Zhou%20and%20Ding%20Jiang%20and%20Bo%20Du%20and%20Mang%20Ye%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Text-to-image%20person%20retrieval%20%28TIPR%29%20aims%20to%20identify%20the%20target%20person%0Ausing%20textual%20descriptions%2C%20facing%20challenge%20in%20modality%20heterogeneity.%20Prior%0Aworks%20have%20attempted%20to%20address%20it%20by%20developing%20cross-modal%20global%20or%20local%0Aalignment%20strategies.%20However%2C%20global%20methods%20typically%20overlook%20fine-grained%0Across-modal%20differences%2C%20whereas%20local%20methods%20require%20prior%20information%20to%0Aexplore%20explicit%20part%20alignments.%20Additionally%2C%20current%20methods%20are%0AEnglish-centric%2C%20restricting%20their%20application%20in%20multilingual%20contexts.%20To%0Aalleviate%20these%20issues%2C%20we%20pioneer%20a%20multilingual%20TIPR%20task%20by%20developing%20a%0Amultilingual%20TIPR%20benchmark%2C%20for%20which%20we%20leverage%20large%20language%20models%20for%0Ainitial%20translations%20and%20refine%20them%20by%20integrating%20domain-specific%20knowledge.%0ACorrespondingly%2C%20we%20propose%20Bi-IRRA%3A%20a%20Bidirectional%20Implicit%20Relation%0AReasoning%20and%20Aligning%20framework%20to%20learn%20alignment%20across%20languages%20and%0Amodalities.%20Within%20Bi-IRRA%2C%20a%20bidirectional%20implicit%20relation%20reasoning%20module%0Aenables%20bidirectional%20prediction%20of%20masked%20image%20and%20text%2C%20implicitly%20enhancing%0Athe%20modeling%20of%20local%20relations%20across%20languages%20and%20modalities%2C%20a%0Amulti-dimensional%20global%20alignment%20module%20is%20integrated%20to%20bridge%20the%20modality%0Aheterogeneity.%20The%20proposed%20method%20achieves%20new%20state-of-the-art%20results%20on%20all%0Amultilingual%20TIPR%20datasets.%20Data%20and%20code%20are%20presented%20in%0Ahttps%3A//github.com/Flame-Chasers/Bi-IRRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Text-to-Image%2520Person%2520Retrieval%2520via%2520Bidirectional%2520Relation%250A%2520%2520Reasoning%2520and%2520Aligning%26entry.906535625%3DMin%2520Cao%2520and%2520Xinyu%2520Zhou%2520and%2520Ding%2520Jiang%2520and%2520Bo%2520Du%2520and%2520Mang%2520Ye%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Text-to-image%2520person%2520retrieval%2520%2528TIPR%2529%2520aims%2520to%2520identify%2520the%2520target%2520person%250Ausing%2520textual%2520descriptions%252C%2520facing%2520challenge%2520in%2520modality%2520heterogeneity.%2520Prior%250Aworks%2520have%2520attempted%2520to%2520address%2520it%2520by%2520developing%2520cross-modal%2520global%2520or%2520local%250Aalignment%2520strategies.%2520However%252C%2520global%2520methods%2520typically%2520overlook%2520fine-grained%250Across-modal%2520differences%252C%2520whereas%2520local%2520methods%2520require%2520prior%2520information%2520to%250Aexplore%2520explicit%2520part%2520alignments.%2520Additionally%252C%2520current%2520methods%2520are%250AEnglish-centric%252C%2520restricting%2520their%2520application%2520in%2520multilingual%2520contexts.%2520To%250Aalleviate%2520these%2520issues%252C%2520we%2520pioneer%2520a%2520multilingual%2520TIPR%2520task%2520by%2520developing%2520a%250Amultilingual%2520TIPR%2520benchmark%252C%2520for%2520which%2520we%2520leverage%2520large%2520language%2520models%2520for%250Ainitial%2520translations%2520and%2520refine%2520them%2520by%2520integrating%2520domain-specific%2520knowledge.%250ACorrespondingly%252C%2520we%2520propose%2520Bi-IRRA%253A%2520a%2520Bidirectional%2520Implicit%2520Relation%250AReasoning%2520and%2520Aligning%2520framework%2520to%2520learn%2520alignment%2520across%2520languages%2520and%250Amodalities.%2520Within%2520Bi-IRRA%252C%2520a%2520bidirectional%2520implicit%2520relation%2520reasoning%2520module%250Aenables%2520bidirectional%2520prediction%2520of%2520masked%2520image%2520and%2520text%252C%2520implicitly%2520enhancing%250Athe%2520modeling%2520of%2520local%2520relations%2520across%2520languages%2520and%2520modalities%252C%2520a%250Amulti-dimensional%2520global%2520alignment%2520module%2520is%2520integrated%2520to%2520bridge%2520the%2520modality%250Aheterogeneity.%2520The%2520proposed%2520method%2520achieves%2520new%2520state-of-the-art%2520results%2520on%2520all%250Amultilingual%2520TIPR%2520datasets.%2520Data%2520and%2520code%2520are%2520presented%2520in%250Ahttps%253A//github.com/Flame-Chasers/Bi-IRRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Text-to-Image%20Person%20Retrieval%20via%20Bidirectional%20Relation%0A%20%20Reasoning%20and%20Aligning&entry.906535625=Min%20Cao%20and%20Xinyu%20Zhou%20and%20Ding%20Jiang%20and%20Bo%20Du%20and%20Mang%20Ye%20and%20Min%20Zhang&entry.1292438233=%20%20Text-to-image%20person%20retrieval%20%28TIPR%29%20aims%20to%20identify%20the%20target%20person%0Ausing%20textual%20descriptions%2C%20facing%20challenge%20in%20modality%20heterogeneity.%20Prior%0Aworks%20have%20attempted%20to%20address%20it%20by%20developing%20cross-modal%20global%20or%20local%0Aalignment%20strategies.%20However%2C%20global%20methods%20typically%20overlook%20fine-grained%0Across-modal%20differences%2C%20whereas%20local%20methods%20require%20prior%20information%20to%0Aexplore%20explicit%20part%20alignments.%20Additionally%2C%20current%20methods%20are%0AEnglish-centric%2C%20restricting%20their%20application%20in%20multilingual%20contexts.%20To%0Aalleviate%20these%20issues%2C%20we%20pioneer%20a%20multilingual%20TIPR%20task%20by%20developing%20a%0Amultilingual%20TIPR%20benchmark%2C%20for%20which%20we%20leverage%20large%20language%20models%20for%0Ainitial%20translations%20and%20refine%20them%20by%20integrating%20domain-specific%20knowledge.%0ACorrespondingly%2C%20we%20propose%20Bi-IRRA%3A%20a%20Bidirectional%20Implicit%20Relation%0AReasoning%20and%20Aligning%20framework%20to%20learn%20alignment%20across%20languages%20and%0Amodalities.%20Within%20Bi-IRRA%2C%20a%20bidirectional%20implicit%20relation%20reasoning%20module%0Aenables%20bidirectional%20prediction%20of%20masked%20image%20and%20text%2C%20implicitly%20enhancing%0Athe%20modeling%20of%20local%20relations%20across%20languages%20and%20modalities%2C%20a%0Amulti-dimensional%20global%20alignment%20module%20is%20integrated%20to%20bridge%20the%20modality%0Aheterogeneity.%20The%20proposed%20method%20achieves%20new%20state-of-the-art%20results%20on%20all%0Amultilingual%20TIPR%20datasets.%20Data%20and%20code%20are%20presented%20in%0Ahttps%3A//github.com/Flame-Chasers/Bi-IRRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17685v1&entry.124074799=Read"},
{"title": "SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model", "author": "Chun Xie and Yuichi Yoshii and Itaru Kitahara", "abstract": "  X-ray imaging is a rapid and cost-effective tool for visualizing internal\nhuman anatomy. While multi-view X-ray imaging provides complementary\ninformation that enhances diagnosis, intervention, and education, acquiring\nimages from multiple angles increases radiation exposure and complicates\nclinical workflows. To address these challenges, we propose a novel\nview-conditioned diffusion model for synthesizing multi-view X-ray images from\na single view. Unlike prior methods, which are limited in angular range,\nresolution, and image quality, our approach leverages the Diffusion Transformer\nto preserve fine details and employs a weak-to-strong training strategy for\nstable high-resolution image generation. Experimental results demonstrate that\nour method generates higher-resolution outputs with improved control over\nviewing angles. This capability has significant implications not only for\nclinical applications but also for medical education and data extension,\nenabling the creation of diverse, high-quality datasets for training and\nanalysis. Our code is available at https://github.com/xiechun298/SV-DRR.\n", "link": "http://arxiv.org/abs/2507.05148v3", "date": "2025-10-20", "relevancy": 2.6615, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.67}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.67}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model&body=Title%3A%20SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model%0AAuthor%3A%20Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara%0AAbstract%3A%20%20%20X-ray%20imaging%20is%20a%20rapid%20and%20cost-effective%20tool%20for%20visualizing%20internal%0Ahuman%20anatomy.%20While%20multi-view%20X-ray%20imaging%20provides%20complementary%0Ainformation%20that%20enhances%20diagnosis%2C%20intervention%2C%20and%20education%2C%20acquiring%0Aimages%20from%20multiple%20angles%20increases%20radiation%20exposure%20and%20complicates%0Aclinical%20workflows.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aview-conditioned%20diffusion%20model%20for%20synthesizing%20multi-view%20X-ray%20images%20from%0Aa%20single%20view.%20Unlike%20prior%20methods%2C%20which%20are%20limited%20in%20angular%20range%2C%0Aresolution%2C%20and%20image%20quality%2C%20our%20approach%20leverages%20the%20Diffusion%20Transformer%0Ato%20preserve%20fine%20details%20and%20employs%20a%20weak-to-strong%20training%20strategy%20for%0Astable%20high-resolution%20image%20generation.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20generates%20higher-resolution%20outputs%20with%20improved%20control%20over%0Aviewing%20angles.%20This%20capability%20has%20significant%20implications%20not%20only%20for%0Aclinical%20applications%20but%20also%20for%20medical%20education%20and%20data%20extension%2C%0Aenabling%20the%20creation%20of%20diverse%2C%20high-quality%20datasets%20for%20training%20and%0Aanalysis.%20Our%20code%20is%20available%20at%20https%3A//github.com/xiechun298/SV-DRR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05148v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSV-DRR%253A%2520High-Fidelity%2520Novel%2520View%2520X-Ray%2520Synthesis%2520Using%2520Diffusion%2520Model%26entry.906535625%3DChun%2520Xie%2520and%2520Yuichi%2520Yoshii%2520and%2520Itaru%2520Kitahara%26entry.1292438233%3D%2520%2520X-ray%2520imaging%2520is%2520a%2520rapid%2520and%2520cost-effective%2520tool%2520for%2520visualizing%2520internal%250Ahuman%2520anatomy.%2520While%2520multi-view%2520X-ray%2520imaging%2520provides%2520complementary%250Ainformation%2520that%2520enhances%2520diagnosis%252C%2520intervention%252C%2520and%2520education%252C%2520acquiring%250Aimages%2520from%2520multiple%2520angles%2520increases%2520radiation%2520exposure%2520and%2520complicates%250Aclinical%2520workflows.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aview-conditioned%2520diffusion%2520model%2520for%2520synthesizing%2520multi-view%2520X-ray%2520images%2520from%250Aa%2520single%2520view.%2520Unlike%2520prior%2520methods%252C%2520which%2520are%2520limited%2520in%2520angular%2520range%252C%250Aresolution%252C%2520and%2520image%2520quality%252C%2520our%2520approach%2520leverages%2520the%2520Diffusion%2520Transformer%250Ato%2520preserve%2520fine%2520details%2520and%2520employs%2520a%2520weak-to-strong%2520training%2520strategy%2520for%250Astable%2520high-resolution%2520image%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520generates%2520higher-resolution%2520outputs%2520with%2520improved%2520control%2520over%250Aviewing%2520angles.%2520This%2520capability%2520has%2520significant%2520implications%2520not%2520only%2520for%250Aclinical%2520applications%2520but%2520also%2520for%2520medical%2520education%2520and%2520data%2520extension%252C%250Aenabling%2520the%2520creation%2520of%2520diverse%252C%2520high-quality%2520datasets%2520for%2520training%2520and%250Aanalysis.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/xiechun298/SV-DRR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05148v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model&entry.906535625=Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara&entry.1292438233=%20%20X-ray%20imaging%20is%20a%20rapid%20and%20cost-effective%20tool%20for%20visualizing%20internal%0Ahuman%20anatomy.%20While%20multi-view%20X-ray%20imaging%20provides%20complementary%0Ainformation%20that%20enhances%20diagnosis%2C%20intervention%2C%20and%20education%2C%20acquiring%0Aimages%20from%20multiple%20angles%20increases%20radiation%20exposure%20and%20complicates%0Aclinical%20workflows.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aview-conditioned%20diffusion%20model%20for%20synthesizing%20multi-view%20X-ray%20images%20from%0Aa%20single%20view.%20Unlike%20prior%20methods%2C%20which%20are%20limited%20in%20angular%20range%2C%0Aresolution%2C%20and%20image%20quality%2C%20our%20approach%20leverages%20the%20Diffusion%20Transformer%0Ato%20preserve%20fine%20details%20and%20employs%20a%20weak-to-strong%20training%20strategy%20for%0Astable%20high-resolution%20image%20generation.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20generates%20higher-resolution%20outputs%20with%20improved%20control%20over%0Aviewing%20angles.%20This%20capability%20has%20significant%20implications%20not%20only%20for%0Aclinical%20applications%20but%20also%20for%20medical%20education%20and%20data%20extension%2C%0Aenabling%20the%20creation%20of%20diverse%2C%20high-quality%20datasets%20for%20training%20and%0Aanalysis.%20Our%20code%20is%20available%20at%20https%3A//github.com/xiechun298/SV-DRR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05148v3&entry.124074799=Read"},
{"title": "Morpheus: Benchmarking Physical Reasoning of Video Generative Models\n  with Real Physical Experiments", "author": "Chenyu Zhang and Daniil Cherniavskii and Antonios Tragoudaras and Antonios Vozikis and Thijmen Nijdam and Derck W. E. Prinzhorn and Mark Bodracska and Nicu Sebe and Andrii Zadaianchuk and Efstratios Gavves", "abstract": "  Recent advances in image and video generation raise hopes that these models\npossess world modeling capabilities, the ability to generate realistic,\nphysically plausible videos. This could revolutionize applications in robotics,\nautonomous driving, and scientific simulation. However, before treating these\nmodels as world models, we must ask: Do they adhere to physical conservation\nlaws? To answer this, we introduce Morpheus, a benchmark for evaluating video\ngeneration models on physical reasoning. It features 80 real-world videos\ncapturing physical phenomena, guided by conservation laws. Since artificial\ngenerations lack ground truth, we assess physical plausibility using\nphysics-informed metrics evaluated with respect to infallible conservation laws\nknown per physical setting, leveraging advances in physics-informed neural\nnetworks and vision-language foundation models. Our findings reveal that even\nwith advanced prompting and video conditioning, current models struggle to\nencode physical principles despite generating aesthetically pleasing videos.\nAll data, leaderboard, and code are open-sourced at our project page.\n", "link": "http://arxiv.org/abs/2504.02918v2", "date": "2025-10-20", "relevancy": 2.6511, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.7266}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.62}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morpheus%3A%20Benchmarking%20Physical%20Reasoning%20of%20Video%20Generative%20Models%0A%20%20with%20Real%20Physical%20Experiments&body=Title%3A%20Morpheus%3A%20Benchmarking%20Physical%20Reasoning%20of%20Video%20Generative%20Models%0A%20%20with%20Real%20Physical%20Experiments%0AAuthor%3A%20Chenyu%20Zhang%20and%20Daniil%20Cherniavskii%20and%20Antonios%20Tragoudaras%20and%20Antonios%20Vozikis%20and%20Thijmen%20Nijdam%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Mark%20Bodracska%20and%20Nicu%20Sebe%20and%20Andrii%20Zadaianchuk%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20Recent%20advances%20in%20image%20and%20video%20generation%20raise%20hopes%20that%20these%20models%0Apossess%20world%20modeling%20capabilities%2C%20the%20ability%20to%20generate%20realistic%2C%0Aphysically%20plausible%20videos.%20This%20could%20revolutionize%20applications%20in%20robotics%2C%0Aautonomous%20driving%2C%20and%20scientific%20simulation.%20However%2C%20before%20treating%20these%0Amodels%20as%20world%20models%2C%20we%20must%20ask%3A%20Do%20they%20adhere%20to%20physical%20conservation%0Alaws%3F%20To%20answer%20this%2C%20we%20introduce%20Morpheus%2C%20a%20benchmark%20for%20evaluating%20video%0Ageneration%20models%20on%20physical%20reasoning.%20It%20features%2080%20real-world%20videos%0Acapturing%20physical%20phenomena%2C%20guided%20by%20conservation%20laws.%20Since%20artificial%0Agenerations%20lack%20ground%20truth%2C%20we%20assess%20physical%20plausibility%20using%0Aphysics-informed%20metrics%20evaluated%20with%20respect%20to%20infallible%20conservation%20laws%0Aknown%20per%20physical%20setting%2C%20leveraging%20advances%20in%20physics-informed%20neural%0Anetworks%20and%20vision-language%20foundation%20models.%20Our%20findings%20reveal%20that%20even%0Awith%20advanced%20prompting%20and%20video%20conditioning%2C%20current%20models%20struggle%20to%0Aencode%20physical%20principles%20despite%20generating%20aesthetically%20pleasing%20videos.%0AAll%20data%2C%20leaderboard%2C%20and%20code%20are%20open-sourced%20at%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorpheus%253A%2520Benchmarking%2520Physical%2520Reasoning%2520of%2520Video%2520Generative%2520Models%250A%2520%2520with%2520Real%2520Physical%2520Experiments%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Daniil%2520Cherniavskii%2520and%2520Antonios%2520Tragoudaras%2520and%2520Antonios%2520Vozikis%2520and%2520Thijmen%2520Nijdam%2520and%2520Derck%2520W.%2520E.%2520Prinzhorn%2520and%2520Mark%2520Bodracska%2520and%2520Nicu%2520Sebe%2520and%2520Andrii%2520Zadaianchuk%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520image%2520and%2520video%2520generation%2520raise%2520hopes%2520that%2520these%2520models%250Apossess%2520world%2520modeling%2520capabilities%252C%2520the%2520ability%2520to%2520generate%2520realistic%252C%250Aphysically%2520plausible%2520videos.%2520This%2520could%2520revolutionize%2520applications%2520in%2520robotics%252C%250Aautonomous%2520driving%252C%2520and%2520scientific%2520simulation.%2520However%252C%2520before%2520treating%2520these%250Amodels%2520as%2520world%2520models%252C%2520we%2520must%2520ask%253A%2520Do%2520they%2520adhere%2520to%2520physical%2520conservation%250Alaws%253F%2520To%2520answer%2520this%252C%2520we%2520introduce%2520Morpheus%252C%2520a%2520benchmark%2520for%2520evaluating%2520video%250Ageneration%2520models%2520on%2520physical%2520reasoning.%2520It%2520features%252080%2520real-world%2520videos%250Acapturing%2520physical%2520phenomena%252C%2520guided%2520by%2520conservation%2520laws.%2520Since%2520artificial%250Agenerations%2520lack%2520ground%2520truth%252C%2520we%2520assess%2520physical%2520plausibility%2520using%250Aphysics-informed%2520metrics%2520evaluated%2520with%2520respect%2520to%2520infallible%2520conservation%2520laws%250Aknown%2520per%2520physical%2520setting%252C%2520leveraging%2520advances%2520in%2520physics-informed%2520neural%250Anetworks%2520and%2520vision-language%2520foundation%2520models.%2520Our%2520findings%2520reveal%2520that%2520even%250Awith%2520advanced%2520prompting%2520and%2520video%2520conditioning%252C%2520current%2520models%2520struggle%2520to%250Aencode%2520physical%2520principles%2520despite%2520generating%2520aesthetically%2520pleasing%2520videos.%250AAll%2520data%252C%2520leaderboard%252C%2520and%2520code%2520are%2520open-sourced%2520at%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morpheus%3A%20Benchmarking%20Physical%20Reasoning%20of%20Video%20Generative%20Models%0A%20%20with%20Real%20Physical%20Experiments&entry.906535625=Chenyu%20Zhang%20and%20Daniil%20Cherniavskii%20and%20Antonios%20Tragoudaras%20and%20Antonios%20Vozikis%20and%20Thijmen%20Nijdam%20and%20Derck%20W.%20E.%20Prinzhorn%20and%20Mark%20Bodracska%20and%20Nicu%20Sebe%20and%20Andrii%20Zadaianchuk%20and%20Efstratios%20Gavves&entry.1292438233=%20%20Recent%20advances%20in%20image%20and%20video%20generation%20raise%20hopes%20that%20these%20models%0Apossess%20world%20modeling%20capabilities%2C%20the%20ability%20to%20generate%20realistic%2C%0Aphysically%20plausible%20videos.%20This%20could%20revolutionize%20applications%20in%20robotics%2C%0Aautonomous%20driving%2C%20and%20scientific%20simulation.%20However%2C%20before%20treating%20these%0Amodels%20as%20world%20models%2C%20we%20must%20ask%3A%20Do%20they%20adhere%20to%20physical%20conservation%0Alaws%3F%20To%20answer%20this%2C%20we%20introduce%20Morpheus%2C%20a%20benchmark%20for%20evaluating%20video%0Ageneration%20models%20on%20physical%20reasoning.%20It%20features%2080%20real-world%20videos%0Acapturing%20physical%20phenomena%2C%20guided%20by%20conservation%20laws.%20Since%20artificial%0Agenerations%20lack%20ground%20truth%2C%20we%20assess%20physical%20plausibility%20using%0Aphysics-informed%20metrics%20evaluated%20with%20respect%20to%20infallible%20conservation%20laws%0Aknown%20per%20physical%20setting%2C%20leveraging%20advances%20in%20physics-informed%20neural%0Anetworks%20and%20vision-language%20foundation%20models.%20Our%20findings%20reveal%20that%20even%0Awith%20advanced%20prompting%20and%20video%20conditioning%2C%20current%20models%20struggle%20to%0Aencode%20physical%20principles%20despite%20generating%20aesthetically%20pleasing%20videos.%0AAll%20data%2C%20leaderboard%2C%20and%20code%20are%20open-sourced%20at%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02918v2&entry.124074799=Read"},
{"title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure\n  Model Segmentation", "author": "Siqi Chen and Shanyue Guan", "abstract": "  The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.\n", "link": "http://arxiv.org/abs/2510.17609v1", "date": "2025-10-20", "relevancy": 2.634, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5198}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20BIM%20and%20UAV-based%20photogrammetry%20for%20Automated%203D%20Structure%0A%20%20Model%20Segmentation&body=Title%3A%20Integrating%20BIM%20and%20UAV-based%20photogrammetry%20for%20Automated%203D%20Structure%0A%20%20Model%20Segmentation%0AAuthor%3A%20Siqi%20Chen%20and%20Shanyue%20Guan%0AAbstract%3A%20%20%20The%20advancement%20of%20UAV%20technology%20has%20enabled%20efficient%2C%20non-contact%0Astructural%20health%20monitoring.%20Combined%20with%20photogrammetry%2C%20UAVs%20can%20capture%0Ahigh-resolution%20scans%20and%20reconstruct%20detailed%203D%20models%20of%20infrastructure.%0AHowever%2C%20a%20key%20challenge%20remains%20in%20segmenting%20specific%20structural%20components%0Afrom%20these%20models-a%20process%20traditionally%20reliant%20on%20time-consuming%20and%0Aerror-prone%20manual%20labeling.%20To%20address%20this%20issue%2C%20we%20propose%20a%20machine%0Alearning-based%20framework%20for%20automated%20segmentation%20of%203D%20point%20clouds.%20Our%0Aapproach%20uses%20the%20complementary%20strengths%20of%20real-world%20UAV-scanned%20point%0Aclouds%20and%20synthetic%20data%20generated%20from%20Building%20Information%20Modeling%20%28BIM%29%20to%0Aovercome%20the%20limitations%20associated%20with%20manual%20labeling.%20Validation%20on%20a%0Arailroad%20track%20dataset%20demonstrated%20high%20accuracy%20in%20identifying%20and%20segmenting%0Amajor%20components%20such%20as%20rails%20and%20crossties.%20Moreover%2C%20by%20using%20smaller-scale%0Adatasets%20supplemented%20with%20BIM%20data%2C%20the%20framework%20significantly%20reduced%0Atraining%20time%20while%20maintaining%20reasonable%20segmentation%20accuracy.%20This%0Aautomated%20approach%20improves%20the%20precision%20and%20efficiency%20of%203D%20infrastructure%0Amodel%20segmentation%20and%20advances%20the%20integration%20of%20UAV%20and%20BIM%20technologies%20in%0Astructural%20health%20monitoring%20and%20infrastructure%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520BIM%2520and%2520UAV-based%2520photogrammetry%2520for%2520Automated%25203D%2520Structure%250A%2520%2520Model%2520Segmentation%26entry.906535625%3DSiqi%2520Chen%2520and%2520Shanyue%2520Guan%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520UAV%2520technology%2520has%2520enabled%2520efficient%252C%2520non-contact%250Astructural%2520health%2520monitoring.%2520Combined%2520with%2520photogrammetry%252C%2520UAVs%2520can%2520capture%250Ahigh-resolution%2520scans%2520and%2520reconstruct%2520detailed%25203D%2520models%2520of%2520infrastructure.%250AHowever%252C%2520a%2520key%2520challenge%2520remains%2520in%2520segmenting%2520specific%2520structural%2520components%250Afrom%2520these%2520models-a%2520process%2520traditionally%2520reliant%2520on%2520time-consuming%2520and%250Aerror-prone%2520manual%2520labeling.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520machine%250Alearning-based%2520framework%2520for%2520automated%2520segmentation%2520of%25203D%2520point%2520clouds.%2520Our%250Aapproach%2520uses%2520the%2520complementary%2520strengths%2520of%2520real-world%2520UAV-scanned%2520point%250Aclouds%2520and%2520synthetic%2520data%2520generated%2520from%2520Building%2520Information%2520Modeling%2520%2528BIM%2529%2520to%250Aovercome%2520the%2520limitations%2520associated%2520with%2520manual%2520labeling.%2520Validation%2520on%2520a%250Arailroad%2520track%2520dataset%2520demonstrated%2520high%2520accuracy%2520in%2520identifying%2520and%2520segmenting%250Amajor%2520components%2520such%2520as%2520rails%2520and%2520crossties.%2520Moreover%252C%2520by%2520using%2520smaller-scale%250Adatasets%2520supplemented%2520with%2520BIM%2520data%252C%2520the%2520framework%2520significantly%2520reduced%250Atraining%2520time%2520while%2520maintaining%2520reasonable%2520segmentation%2520accuracy.%2520This%250Aautomated%2520approach%2520improves%2520the%2520precision%2520and%2520efficiency%2520of%25203D%2520infrastructure%250Amodel%2520segmentation%2520and%2520advances%2520the%2520integration%2520of%2520UAV%2520and%2520BIM%2520technologies%2520in%250Astructural%2520health%2520monitoring%2520and%2520infrastructure%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20BIM%20and%20UAV-based%20photogrammetry%20for%20Automated%203D%20Structure%0A%20%20Model%20Segmentation&entry.906535625=Siqi%20Chen%20and%20Shanyue%20Guan&entry.1292438233=%20%20The%20advancement%20of%20UAV%20technology%20has%20enabled%20efficient%2C%20non-contact%0Astructural%20health%20monitoring.%20Combined%20with%20photogrammetry%2C%20UAVs%20can%20capture%0Ahigh-resolution%20scans%20and%20reconstruct%20detailed%203D%20models%20of%20infrastructure.%0AHowever%2C%20a%20key%20challenge%20remains%20in%20segmenting%20specific%20structural%20components%0Afrom%20these%20models-a%20process%20traditionally%20reliant%20on%20time-consuming%20and%0Aerror-prone%20manual%20labeling.%20To%20address%20this%20issue%2C%20we%20propose%20a%20machine%0Alearning-based%20framework%20for%20automated%20segmentation%20of%203D%20point%20clouds.%20Our%0Aapproach%20uses%20the%20complementary%20strengths%20of%20real-world%20UAV-scanned%20point%0Aclouds%20and%20synthetic%20data%20generated%20from%20Building%20Information%20Modeling%20%28BIM%29%20to%0Aovercome%20the%20limitations%20associated%20with%20manual%20labeling.%20Validation%20on%20a%0Arailroad%20track%20dataset%20demonstrated%20high%20accuracy%20in%20identifying%20and%20segmenting%0Amajor%20components%20such%20as%20rails%20and%20crossties.%20Moreover%2C%20by%20using%20smaller-scale%0Adatasets%20supplemented%20with%20BIM%20data%2C%20the%20framework%20significantly%20reduced%0Atraining%20time%20while%20maintaining%20reasonable%20segmentation%20accuracy.%20This%0Aautomated%20approach%20improves%20the%20precision%20and%20efficiency%20of%203D%20infrastructure%0Amodel%20segmentation%20and%20advances%20the%20integration%20of%20UAV%20and%20BIM%20technologies%20in%0Astructural%20health%20monitoring%20and%20infrastructure%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17609v1&entry.124074799=Read"},
{"title": "UniCTokens: Boosting Personalized Understanding and Generation via\n  Unified Concept Tokens", "author": "Ruichuan An and Sihan Yang and Renrui Zhang and Zijun Shen and Ming Lu and Gaole Dai and Hao Liang and Ziyu Guo and Shilin Yan and Yulin Luo and Bocheng Zou and Chaoqun Yang and Wentao Zhang", "abstract": "  Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation \\textit{\\textbf{personalized\nattribute-reasoning generation}}. To address the limitation, we present\nUniCTokens, a novel framework that effectively integrates personalized\ninformation into a unified vision language model (VLM) for understanding and\ngeneration. UniCTokens trains a set of unified concept tokens to leverage\ncomplementary semantics, boosting two personalized tasks. Moreover, we propose\na progressive training strategy with three stages: understanding warm-up,\nbootstrapping generation from understanding, and deepening understanding from\ngeneration to enhance mutual benefits between both tasks. To quantitatively\nevaluate the unified VLM personalization, we present UnifyBench, the first\nbenchmark for assessing concept understanding, concept generation, and\nattribute-reasoning generation. Experimental results on UnifyBench indicate\nthat UniCTokens shows competitive performance compared to leading methods in\nconcept understanding, concept generation, and achieving state-of-the-art\nresults in personalized attribute-reasoning generation. Our research\ndemonstrates that enhanced understanding improves generation, and the\ngeneration process can yield valuable insights into understanding. Our code and\ndataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.\n", "link": "http://arxiv.org/abs/2505.14671v3", "date": "2025-10-20", "relevancy": 2.6318, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens&body=Title%3A%20UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens%0AAuthor%3A%20Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Zijun%20Shen%20and%20Ming%20Lu%20and%20Gaole%20Dai%20and%20Hao%20Liang%20and%20Ziyu%20Guo%20and%20Shilin%20Yan%20and%20Yulin%20Luo%20and%20Bocheng%20Zou%20and%20Chaoqun%20Yang%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Personalized%20models%20have%20demonstrated%20remarkable%20success%20in%20understanding%20and%0Agenerating%20concepts%20provided%20by%20users.%20However%2C%20existing%20methods%20use%20separate%0Aconcept%20tokens%20for%20understanding%20and%20generation%2C%20treating%20these%20tasks%20in%0Aisolation.%20This%20may%20result%20in%20limitations%20for%20generating%20images%20with%20complex%0Aprompts.%20For%20example%2C%20given%20the%20concept%20%24%5Clangle%20bo%5Crangle%24%2C%20generating%0A%22%24%5Clangle%20bo%5Crangle%24%20wearing%20its%20hat%22%20without%20additional%20textual%20descriptions%0Aof%20its%20hat.%20We%20call%20this%20kind%20of%20generation%20%5Ctextit%7B%5Ctextbf%7Bpersonalized%0Aattribute-reasoning%20generation%7D%7D.%20To%20address%20the%20limitation%2C%20we%20present%0AUniCTokens%2C%20a%20novel%20framework%20that%20effectively%20integrates%20personalized%0Ainformation%20into%20a%20unified%20vision%20language%20model%20%28VLM%29%20for%20understanding%20and%0Ageneration.%20UniCTokens%20trains%20a%20set%20of%20unified%20concept%20tokens%20to%20leverage%0Acomplementary%20semantics%2C%20boosting%20two%20personalized%20tasks.%20Moreover%2C%20we%20propose%0Aa%20progressive%20training%20strategy%20with%20three%20stages%3A%20understanding%20warm-up%2C%0Abootstrapping%20generation%20from%20understanding%2C%20and%20deepening%20understanding%20from%0Ageneration%20to%20enhance%20mutual%20benefits%20between%20both%20tasks.%20To%20quantitatively%0Aevaluate%20the%20unified%20VLM%20personalization%2C%20we%20present%20UnifyBench%2C%20the%20first%0Abenchmark%20for%20assessing%20concept%20understanding%2C%20concept%20generation%2C%20and%0Aattribute-reasoning%20generation.%20Experimental%20results%20on%20UnifyBench%20indicate%0Athat%20UniCTokens%20shows%20competitive%20performance%20compared%20to%20leading%20methods%20in%0Aconcept%20understanding%2C%20concept%20generation%2C%20and%20achieving%20state-of-the-art%0Aresults%20in%20personalized%20attribute-reasoning%20generation.%20Our%20research%0Ademonstrates%20that%20enhanced%20understanding%20improves%20generation%2C%20and%20the%0Ageneration%20process%20can%20yield%20valuable%20insights%20into%20understanding.%20Our%20code%20and%0Adataset%20will%20be%20released%20at%3A%0A%5Chref%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14671v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCTokens%253A%2520Boosting%2520Personalized%2520Understanding%2520and%2520Generation%2520via%250A%2520%2520Unified%2520Concept%2520Tokens%26entry.906535625%3DRuichuan%2520An%2520and%2520Sihan%2520Yang%2520and%2520Renrui%2520Zhang%2520and%2520Zijun%2520Shen%2520and%2520Ming%2520Lu%2520and%2520Gaole%2520Dai%2520and%2520Hao%2520Liang%2520and%2520Ziyu%2520Guo%2520and%2520Shilin%2520Yan%2520and%2520Yulin%2520Luo%2520and%2520Bocheng%2520Zou%2520and%2520Chaoqun%2520Yang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Personalized%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520understanding%2520and%250Agenerating%2520concepts%2520provided%2520by%2520users.%2520However%252C%2520existing%2520methods%2520use%2520separate%250Aconcept%2520tokens%2520for%2520understanding%2520and%2520generation%252C%2520treating%2520these%2520tasks%2520in%250Aisolation.%2520This%2520may%2520result%2520in%2520limitations%2520for%2520generating%2520images%2520with%2520complex%250Aprompts.%2520For%2520example%252C%2520given%2520the%2520concept%2520%2524%255Clangle%2520bo%255Crangle%2524%252C%2520generating%250A%2522%2524%255Clangle%2520bo%255Crangle%2524%2520wearing%2520its%2520hat%2522%2520without%2520additional%2520textual%2520descriptions%250Aof%2520its%2520hat.%2520We%2520call%2520this%2520kind%2520of%2520generation%2520%255Ctextit%257B%255Ctextbf%257Bpersonalized%250Aattribute-reasoning%2520generation%257D%257D.%2520To%2520address%2520the%2520limitation%252C%2520we%2520present%250AUniCTokens%252C%2520a%2520novel%2520framework%2520that%2520effectively%2520integrates%2520personalized%250Ainformation%2520into%2520a%2520unified%2520vision%2520language%2520model%2520%2528VLM%2529%2520for%2520understanding%2520and%250Ageneration.%2520UniCTokens%2520trains%2520a%2520set%2520of%2520unified%2520concept%2520tokens%2520to%2520leverage%250Acomplementary%2520semantics%252C%2520boosting%2520two%2520personalized%2520tasks.%2520Moreover%252C%2520we%2520propose%250Aa%2520progressive%2520training%2520strategy%2520with%2520three%2520stages%253A%2520understanding%2520warm-up%252C%250Abootstrapping%2520generation%2520from%2520understanding%252C%2520and%2520deepening%2520understanding%2520from%250Ageneration%2520to%2520enhance%2520mutual%2520benefits%2520between%2520both%2520tasks.%2520To%2520quantitatively%250Aevaluate%2520the%2520unified%2520VLM%2520personalization%252C%2520we%2520present%2520UnifyBench%252C%2520the%2520first%250Abenchmark%2520for%2520assessing%2520concept%2520understanding%252C%2520concept%2520generation%252C%2520and%250Aattribute-reasoning%2520generation.%2520Experimental%2520results%2520on%2520UnifyBench%2520indicate%250Athat%2520UniCTokens%2520shows%2520competitive%2520performance%2520compared%2520to%2520leading%2520methods%2520in%250Aconcept%2520understanding%252C%2520concept%2520generation%252C%2520and%2520achieving%2520state-of-the-art%250Aresults%2520in%2520personalized%2520attribute-reasoning%2520generation.%2520Our%2520research%250Ademonstrates%2520that%2520enhanced%2520understanding%2520improves%2520generation%252C%2520and%2520the%250Ageneration%2520process%2520can%2520yield%2520valuable%2520insights%2520into%2520understanding.%2520Our%2520code%2520and%250Adataset%2520will%2520be%2520released%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/arctanxarc/UniCTokens%257D%257Bhttps%253A//github.com/arctanxarc/UniCTokens%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14671v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens&entry.906535625=Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Zijun%20Shen%20and%20Ming%20Lu%20and%20Gaole%20Dai%20and%20Hao%20Liang%20and%20Ziyu%20Guo%20and%20Shilin%20Yan%20and%20Yulin%20Luo%20and%20Bocheng%20Zou%20and%20Chaoqun%20Yang%20and%20Wentao%20Zhang&entry.1292438233=%20%20Personalized%20models%20have%20demonstrated%20remarkable%20success%20in%20understanding%20and%0Agenerating%20concepts%20provided%20by%20users.%20However%2C%20existing%20methods%20use%20separate%0Aconcept%20tokens%20for%20understanding%20and%20generation%2C%20treating%20these%20tasks%20in%0Aisolation.%20This%20may%20result%20in%20limitations%20for%20generating%20images%20with%20complex%0Aprompts.%20For%20example%2C%20given%20the%20concept%20%24%5Clangle%20bo%5Crangle%24%2C%20generating%0A%22%24%5Clangle%20bo%5Crangle%24%20wearing%20its%20hat%22%20without%20additional%20textual%20descriptions%0Aof%20its%20hat.%20We%20call%20this%20kind%20of%20generation%20%5Ctextit%7B%5Ctextbf%7Bpersonalized%0Aattribute-reasoning%20generation%7D%7D.%20To%20address%20the%20limitation%2C%20we%20present%0AUniCTokens%2C%20a%20novel%20framework%20that%20effectively%20integrates%20personalized%0Ainformation%20into%20a%20unified%20vision%20language%20model%20%28VLM%29%20for%20understanding%20and%0Ageneration.%20UniCTokens%20trains%20a%20set%20of%20unified%20concept%20tokens%20to%20leverage%0Acomplementary%20semantics%2C%20boosting%20two%20personalized%20tasks.%20Moreover%2C%20we%20propose%0Aa%20progressive%20training%20strategy%20with%20three%20stages%3A%20understanding%20warm-up%2C%0Abootstrapping%20generation%20from%20understanding%2C%20and%20deepening%20understanding%20from%0Ageneration%20to%20enhance%20mutual%20benefits%20between%20both%20tasks.%20To%20quantitatively%0Aevaluate%20the%20unified%20VLM%20personalization%2C%20we%20present%20UnifyBench%2C%20the%20first%0Abenchmark%20for%20assessing%20concept%20understanding%2C%20concept%20generation%2C%20and%0Aattribute-reasoning%20generation.%20Experimental%20results%20on%20UnifyBench%20indicate%0Athat%20UniCTokens%20shows%20competitive%20performance%20compared%20to%20leading%20methods%20in%0Aconcept%20understanding%2C%20concept%20generation%2C%20and%20achieving%20state-of-the-art%0Aresults%20in%20personalized%20attribute-reasoning%20generation.%20Our%20research%0Ademonstrates%20that%20enhanced%20understanding%20improves%20generation%2C%20and%20the%0Ageneration%20process%20can%20yield%20valuable%20insights%20into%20understanding.%20Our%20code%20and%0Adataset%20will%20be%20released%20at%3A%0A%5Chref%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14671v3&entry.124074799=Read"},
{"title": "Unbiased Gradient Low-Rank Projection", "author": "Rui Pan and Yang Luo and Yuxing Liu and Yang You and Tong Zhang", "abstract": "  Memory-efficient optimization is critical for training increasingly large\nlanguage models (LLMs). A popular strategy involves gradient low-rank\nprojection, storing only the projected optimizer states, with GaLore being a\nrepresentative example. However, a significant drawback of many such methods is\ntheir lack of convergence guarantees, as various low-rank projection approaches\nintroduce inherent biases relative to the original optimization algorithms,\nwhich contribute to performance gaps compared to full-parameter training.\nAiming to tackle this problem, this paper investigates the layerwise sampling\ntechnique for debiasing low-rank projection mechanisms. In particular, an\ninstantiation of the paradigm gives rise to a novel and unbiased low-rank\noptimization method built upon GaLore's mechanism and the Muon algorithm, named\nGaLore Unbiased with Muon (GUM). We theoretically prove our method matches the\nconvergence guarantees of the base Muon algorithm while preserving the memory\nefficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and\npretraining also demonstrate non-trivial improvements over GaLore and even\nbetter performance than full-parameter training. Further investigation shows\nthat the improvement of this technique comes from a more uniform distribution\nof knowledge inside layers, leading to more efficient utilization of the model\nparameter space and better memorization.\n", "link": "http://arxiv.org/abs/2510.17802v1", "date": "2025-10-20", "relevancy": 2.6098, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5371}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5276}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unbiased%20Gradient%20Low-Rank%20Projection&body=Title%3A%20Unbiased%20Gradient%20Low-Rank%20Projection%0AAuthor%3A%20Rui%20Pan%20and%20Yang%20Luo%20and%20Yuxing%20Liu%20and%20Yang%20You%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Memory-efficient%20optimization%20is%20critical%20for%20training%20increasingly%20large%0Alanguage%20models%20%28LLMs%29.%20A%20popular%20strategy%20involves%20gradient%20low-rank%0Aprojection%2C%20storing%20only%20the%20projected%20optimizer%20states%2C%20with%20GaLore%20being%20a%0Arepresentative%20example.%20However%2C%20a%20significant%20drawback%20of%20many%20such%20methods%20is%0Atheir%20lack%20of%20convergence%20guarantees%2C%20as%20various%20low-rank%20projection%20approaches%0Aintroduce%20inherent%20biases%20relative%20to%20the%20original%20optimization%20algorithms%2C%0Awhich%20contribute%20to%20performance%20gaps%20compared%20to%20full-parameter%20training.%0AAiming%20to%20tackle%20this%20problem%2C%20this%20paper%20investigates%20the%20layerwise%20sampling%0Atechnique%20for%20debiasing%20low-rank%20projection%20mechanisms.%20In%20particular%2C%20an%0Ainstantiation%20of%20the%20paradigm%20gives%20rise%20to%20a%20novel%20and%20unbiased%20low-rank%0Aoptimization%20method%20built%20upon%20GaLore%27s%20mechanism%20and%20the%20Muon%20algorithm%2C%20named%0AGaLore%20Unbiased%20with%20Muon%20%28GUM%29.%20We%20theoretically%20prove%20our%20method%20matches%20the%0Aconvergence%20guarantees%20of%20the%20base%20Muon%20algorithm%20while%20preserving%20the%20memory%0Aefficiency%20of%20low-rank%20techniques.%20Empirical%20experiments%20on%20LLM%20fine-tuning%20and%0Apretraining%20also%20demonstrate%20non-trivial%20improvements%20over%20GaLore%20and%20even%0Abetter%20performance%20than%20full-parameter%20training.%20Further%20investigation%20shows%0Athat%20the%20improvement%20of%20this%20technique%20comes%20from%20a%20more%20uniform%20distribution%0Aof%20knowledge%20inside%20layers%2C%20leading%20to%20more%20efficient%20utilization%20of%20the%20model%0Aparameter%20space%20and%20better%20memorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnbiased%2520Gradient%2520Low-Rank%2520Projection%26entry.906535625%3DRui%2520Pan%2520and%2520Yang%2520Luo%2520and%2520Yuxing%2520Liu%2520and%2520Yang%2520You%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Memory-efficient%2520optimization%2520is%2520critical%2520for%2520training%2520increasingly%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520A%2520popular%2520strategy%2520involves%2520gradient%2520low-rank%250Aprojection%252C%2520storing%2520only%2520the%2520projected%2520optimizer%2520states%252C%2520with%2520GaLore%2520being%2520a%250Arepresentative%2520example.%2520However%252C%2520a%2520significant%2520drawback%2520of%2520many%2520such%2520methods%2520is%250Atheir%2520lack%2520of%2520convergence%2520guarantees%252C%2520as%2520various%2520low-rank%2520projection%2520approaches%250Aintroduce%2520inherent%2520biases%2520relative%2520to%2520the%2520original%2520optimization%2520algorithms%252C%250Awhich%2520contribute%2520to%2520performance%2520gaps%2520compared%2520to%2520full-parameter%2520training.%250AAiming%2520to%2520tackle%2520this%2520problem%252C%2520this%2520paper%2520investigates%2520the%2520layerwise%2520sampling%250Atechnique%2520for%2520debiasing%2520low-rank%2520projection%2520mechanisms.%2520In%2520particular%252C%2520an%250Ainstantiation%2520of%2520the%2520paradigm%2520gives%2520rise%2520to%2520a%2520novel%2520and%2520unbiased%2520low-rank%250Aoptimization%2520method%2520built%2520upon%2520GaLore%2527s%2520mechanism%2520and%2520the%2520Muon%2520algorithm%252C%2520named%250AGaLore%2520Unbiased%2520with%2520Muon%2520%2528GUM%2529.%2520We%2520theoretically%2520prove%2520our%2520method%2520matches%2520the%250Aconvergence%2520guarantees%2520of%2520the%2520base%2520Muon%2520algorithm%2520while%2520preserving%2520the%2520memory%250Aefficiency%2520of%2520low-rank%2520techniques.%2520Empirical%2520experiments%2520on%2520LLM%2520fine-tuning%2520and%250Apretraining%2520also%2520demonstrate%2520non-trivial%2520improvements%2520over%2520GaLore%2520and%2520even%250Abetter%2520performance%2520than%2520full-parameter%2520training.%2520Further%2520investigation%2520shows%250Athat%2520the%2520improvement%2520of%2520this%2520technique%2520comes%2520from%2520a%2520more%2520uniform%2520distribution%250Aof%2520knowledge%2520inside%2520layers%252C%2520leading%2520to%2520more%2520efficient%2520utilization%2520of%2520the%2520model%250Aparameter%2520space%2520and%2520better%2520memorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbiased%20Gradient%20Low-Rank%20Projection&entry.906535625=Rui%20Pan%20and%20Yang%20Luo%20and%20Yuxing%20Liu%20and%20Yang%20You%20and%20Tong%20Zhang&entry.1292438233=%20%20Memory-efficient%20optimization%20is%20critical%20for%20training%20increasingly%20large%0Alanguage%20models%20%28LLMs%29.%20A%20popular%20strategy%20involves%20gradient%20low-rank%0Aprojection%2C%20storing%20only%20the%20projected%20optimizer%20states%2C%20with%20GaLore%20being%20a%0Arepresentative%20example.%20However%2C%20a%20significant%20drawback%20of%20many%20such%20methods%20is%0Atheir%20lack%20of%20convergence%20guarantees%2C%20as%20various%20low-rank%20projection%20approaches%0Aintroduce%20inherent%20biases%20relative%20to%20the%20original%20optimization%20algorithms%2C%0Awhich%20contribute%20to%20performance%20gaps%20compared%20to%20full-parameter%20training.%0AAiming%20to%20tackle%20this%20problem%2C%20this%20paper%20investigates%20the%20layerwise%20sampling%0Atechnique%20for%20debiasing%20low-rank%20projection%20mechanisms.%20In%20particular%2C%20an%0Ainstantiation%20of%20the%20paradigm%20gives%20rise%20to%20a%20novel%20and%20unbiased%20low-rank%0Aoptimization%20method%20built%20upon%20GaLore%27s%20mechanism%20and%20the%20Muon%20algorithm%2C%20named%0AGaLore%20Unbiased%20with%20Muon%20%28GUM%29.%20We%20theoretically%20prove%20our%20method%20matches%20the%0Aconvergence%20guarantees%20of%20the%20base%20Muon%20algorithm%20while%20preserving%20the%20memory%0Aefficiency%20of%20low-rank%20techniques.%20Empirical%20experiments%20on%20LLM%20fine-tuning%20and%0Apretraining%20also%20demonstrate%20non-trivial%20improvements%20over%20GaLore%20and%20even%0Abetter%20performance%20than%20full-parameter%20training.%20Further%20investigation%20shows%0Athat%20the%20improvement%20of%20this%20technique%20comes%20from%20a%20more%20uniform%20distribution%0Aof%20knowledge%20inside%20layers%2C%20leading%20to%20more%20efficient%20utilization%20of%20the%20model%0Aparameter%20space%20and%20better%20memorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17802v1&entry.124074799=Read"},
{"title": "Reasoning Distillation and Structural Alignment for Improved Code\n  Generation", "author": "Amir Jalilifard and Anderson de Rezende Rocha and Marcos Medeiros Raimundo", "abstract": "  Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.\n", "link": "http://arxiv.org/abs/2510.17598v1", "date": "2025-10-20", "relevancy": 2.5859, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Distillation%20and%20Structural%20Alignment%20for%20Improved%20Code%0A%20%20Generation&body=Title%3A%20Reasoning%20Distillation%20and%20Structural%20Alignment%20for%20Improved%20Code%0A%20%20Generation%0AAuthor%3A%20Amir%20Jalilifard%20and%20Anderson%20de%20Rezende%20Rocha%20and%20Marcos%20Medeiros%20Raimundo%0AAbstract%3A%20%20%20Effective%20code%20generation%20with%20language%20models%20hinges%20on%20two%20critical%0Afactors%3A%20accurately%20understanding%20the%20intent%20of%20the%20prompt%20and%20generating%20code%0Athat%20applies%20algorithmic%20reasoning%20to%20produce%20correct%20solutions%20capable%20of%0Apassing%20diverse%20test%20cases%20while%20adhering%20to%20the%20syntax%20of%20the%20target%0Aprogramming%20language.%20Unlike%20other%20language%20tasks%2C%20code%20generation%20requires%0Amore%20than%20accurate%20token%20prediction%3B%20it%20demands%20comprehension%20of%20solution-level%0Aand%20structural%20relationships%20rather%20than%20merely%20generating%20the%20most%20likely%0Atokens.%20very%20large%20language%20model%20%28VLLM%29%20are%20capable%20of%20generating%20detailed%0Asteps%20toward%20the%20correct%20solution%20of%20complex%20tasks%20where%20reasoning%20is%20crucial%0Ain%20solving%20the%20problem.%20Such%20reasoning%20capabilities%20may%20be%20absent%20in%20smaller%0Alanguage%20models.%20Therefore%2C%20in%20this%20work%2C%20we%20distill%20the%20reasoning%20capabilities%0Aof%20a%20VLLM%20into%20a%20smaller%2C%20more%20efficient%20model%20that%20is%20faster%20and%20cheaper%20to%0Adeploy.%20Our%20approach%20trains%20the%20model%20to%20emulate%20the%20reasoning%20and%0Aproblem-solving%20abilities%20of%20the%20VLLM%20by%20learning%20to%20identify%20correct%20solution%0Apathways%20and%20establishing%20a%20structural%20correspondence%20between%20problem%0Adefinitions%20and%20potential%20solutions%20through%20a%20novel%20method%20of%20structure-aware%0Aloss%20optimization.%20This%20enables%20the%20model%20to%20transcend%20token-level%20generation%0Aand%20to%20deeply%20grasp%20the%20overarching%20structure%20of%20solutions%20for%20given%20problems.%0AExperimental%20results%20show%20that%20our%20fine-tuned%20model%2C%20developed%20through%20a%20cheap%0Aand%20simple%20to%20implement%20process%2C%20significantly%20outperforms%20our%20baseline%20model%0Ain%20terms%20of%20pass%401%2C%20average%20data%20flow%2C%20and%20average%20syntax%20match%20metrics%20across%0Athe%20MBPP%2C%20MBPP%20Plus%2C%20and%20HumanEval%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Distillation%2520and%2520Structural%2520Alignment%2520for%2520Improved%2520Code%250A%2520%2520Generation%26entry.906535625%3DAmir%2520Jalilifard%2520and%2520Anderson%2520de%2520Rezende%2520Rocha%2520and%2520Marcos%2520Medeiros%2520Raimundo%26entry.1292438233%3D%2520%2520Effective%2520code%2520generation%2520with%2520language%2520models%2520hinges%2520on%2520two%2520critical%250Afactors%253A%2520accurately%2520understanding%2520the%2520intent%2520of%2520the%2520prompt%2520and%2520generating%2520code%250Athat%2520applies%2520algorithmic%2520reasoning%2520to%2520produce%2520correct%2520solutions%2520capable%2520of%250Apassing%2520diverse%2520test%2520cases%2520while%2520adhering%2520to%2520the%2520syntax%2520of%2520the%2520target%250Aprogramming%2520language.%2520Unlike%2520other%2520language%2520tasks%252C%2520code%2520generation%2520requires%250Amore%2520than%2520accurate%2520token%2520prediction%253B%2520it%2520demands%2520comprehension%2520of%2520solution-level%250Aand%2520structural%2520relationships%2520rather%2520than%2520merely%2520generating%2520the%2520most%2520likely%250Atokens.%2520very%2520large%2520language%2520model%2520%2528VLLM%2529%2520are%2520capable%2520of%2520generating%2520detailed%250Asteps%2520toward%2520the%2520correct%2520solution%2520of%2520complex%2520tasks%2520where%2520reasoning%2520is%2520crucial%250Ain%2520solving%2520the%2520problem.%2520Such%2520reasoning%2520capabilities%2520may%2520be%2520absent%2520in%2520smaller%250Alanguage%2520models.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520distill%2520the%2520reasoning%2520capabilities%250Aof%2520a%2520VLLM%2520into%2520a%2520smaller%252C%2520more%2520efficient%2520model%2520that%2520is%2520faster%2520and%2520cheaper%2520to%250Adeploy.%2520Our%2520approach%2520trains%2520the%2520model%2520to%2520emulate%2520the%2520reasoning%2520and%250Aproblem-solving%2520abilities%2520of%2520the%2520VLLM%2520by%2520learning%2520to%2520identify%2520correct%2520solution%250Apathways%2520and%2520establishing%2520a%2520structural%2520correspondence%2520between%2520problem%250Adefinitions%2520and%2520potential%2520solutions%2520through%2520a%2520novel%2520method%2520of%2520structure-aware%250Aloss%2520optimization.%2520This%2520enables%2520the%2520model%2520to%2520transcend%2520token-level%2520generation%250Aand%2520to%2520deeply%2520grasp%2520the%2520overarching%2520structure%2520of%2520solutions%2520for%2520given%2520problems.%250AExperimental%2520results%2520show%2520that%2520our%2520fine-tuned%2520model%252C%2520developed%2520through%2520a%2520cheap%250Aand%2520simple%2520to%2520implement%2520process%252C%2520significantly%2520outperforms%2520our%2520baseline%2520model%250Ain%2520terms%2520of%2520pass%25401%252C%2520average%2520data%2520flow%252C%2520and%2520average%2520syntax%2520match%2520metrics%2520across%250Athe%2520MBPP%252C%2520MBPP%2520Plus%252C%2520and%2520HumanEval%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Distillation%20and%20Structural%20Alignment%20for%20Improved%20Code%0A%20%20Generation&entry.906535625=Amir%20Jalilifard%20and%20Anderson%20de%20Rezende%20Rocha%20and%20Marcos%20Medeiros%20Raimundo&entry.1292438233=%20%20Effective%20code%20generation%20with%20language%20models%20hinges%20on%20two%20critical%0Afactors%3A%20accurately%20understanding%20the%20intent%20of%20the%20prompt%20and%20generating%20code%0Athat%20applies%20algorithmic%20reasoning%20to%20produce%20correct%20solutions%20capable%20of%0Apassing%20diverse%20test%20cases%20while%20adhering%20to%20the%20syntax%20of%20the%20target%0Aprogramming%20language.%20Unlike%20other%20language%20tasks%2C%20code%20generation%20requires%0Amore%20than%20accurate%20token%20prediction%3B%20it%20demands%20comprehension%20of%20solution-level%0Aand%20structural%20relationships%20rather%20than%20merely%20generating%20the%20most%20likely%0Atokens.%20very%20large%20language%20model%20%28VLLM%29%20are%20capable%20of%20generating%20detailed%0Asteps%20toward%20the%20correct%20solution%20of%20complex%20tasks%20where%20reasoning%20is%20crucial%0Ain%20solving%20the%20problem.%20Such%20reasoning%20capabilities%20may%20be%20absent%20in%20smaller%0Alanguage%20models.%20Therefore%2C%20in%20this%20work%2C%20we%20distill%20the%20reasoning%20capabilities%0Aof%20a%20VLLM%20into%20a%20smaller%2C%20more%20efficient%20model%20that%20is%20faster%20and%20cheaper%20to%0Adeploy.%20Our%20approach%20trains%20the%20model%20to%20emulate%20the%20reasoning%20and%0Aproblem-solving%20abilities%20of%20the%20VLLM%20by%20learning%20to%20identify%20correct%20solution%0Apathways%20and%20establishing%20a%20structural%20correspondence%20between%20problem%0Adefinitions%20and%20potential%20solutions%20through%20a%20novel%20method%20of%20structure-aware%0Aloss%20optimization.%20This%20enables%20the%20model%20to%20transcend%20token-level%20generation%0Aand%20to%20deeply%20grasp%20the%20overarching%20structure%20of%20solutions%20for%20given%20problems.%0AExperimental%20results%20show%20that%20our%20fine-tuned%20model%2C%20developed%20through%20a%20cheap%0Aand%20simple%20to%20implement%20process%2C%20significantly%20outperforms%20our%20baseline%20model%0Ain%20terms%20of%20pass%401%2C%20average%20data%20flow%2C%20and%20average%20syntax%20match%20metrics%20across%0Athe%20MBPP%2C%20MBPP%20Plus%2C%20and%20HumanEval%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17598v1&entry.124074799=Read"},
{"title": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution", "author": "Ritik Shah and Marco F. Duarte", "abstract": "  High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.\n", "link": "http://arxiv.org/abs/2507.13339v2", "date": "2025-10-20", "relevancy": 2.5705, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5137}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution&body=Title%3A%20SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution%0AAuthor%3A%20Ritik%20Shah%20and%20Marco%20F.%20Duarte%0AAbstract%3A%20%20%20High-spatial-resolution%20hyperspectral%20images%20%28HSI%29%20are%20essential%20for%0Aapplications%20such%20as%20remote%20sensing%20and%20medical%20imaging%2C%20yet%20HSI%20sensors%0Ainherently%20trade%20spatial%20detail%20for%20spectral%20richness.%20Fusing%0Ahigh-spatial-resolution%20multispectral%20images%20%28HR-MSI%29%20with%0Alow-spatial-resolution%20hyperspectral%20images%20%28LR-HSI%29%20is%20a%20promising%20route%20to%0Arecover%20fine%20spatial%20structures%20without%20sacrificing%20spectral%20fidelity.%20Most%0Astate-of-the-art%20methods%20for%20HSI-MSI%20fusion%20demand%20point%20spread%20function%20%28PSF%29%0Acalibration%20or%20ground%20truth%20high%20resolution%20HSI%20%28HR-HSI%29%2C%20both%20of%20which%20are%0Aimpractical%20to%20obtain%20in%20real%20world%20settings.%20We%20present%20SpectraLift%2C%20a%20fully%0Aself-supervised%20framework%20that%20fuses%20LR-HSI%20and%20HR-MSI%20inputs%20using%20only%20the%0AMSI%27s%20Spectral%20Response%20Function%20%28SRF%29.%20SpectraLift%20trains%20a%20lightweight%0Aper-pixel%20multi-layer%20perceptron%20%28MLP%29%20network%20using%20%28%24i%24%29~a%20synthetic%0Alow-spatial-resolution%20multispectral%20image%20%28LR-MSI%29%20obtained%20by%20applying%20the%0ASRF%20to%20the%20LR-HSI%20as%20input%2C%20%28%24ii%24%29~the%20LR-HSI%20as%20the%20output%2C%20and%20%28%24iii%24%29~an%0A%24%5Cell_1%24%20spectral%20reconstruction%20loss%20between%20the%20estimated%20and%20true%20LR-HSI%20as%0Athe%20optimization%20objective.%20At%20inference%2C%20SpectraLift%20uses%20the%20trained%20network%0Ato%20map%20the%20HR-MSI%20pixel-wise%20into%20a%20HR-HSI%20estimate.%20SpectraLift%20converges%20in%0Aminutes%2C%20is%20agnostic%20to%20spatial%20blur%20and%20resolution%2C%20and%20outperforms%0Astate-of-the-art%20methods%20on%20PSNR%2C%20SAM%2C%20SSIM%2C%20and%20RMSE%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectraLift%253A%2520Physics-Guided%2520Spectral-Inversion%2520Network%2520for%250A%2520%2520Self-Supervised%2520Hyperspectral%2520Image%2520Super-Resolution%26entry.906535625%3DRitik%2520Shah%2520and%2520Marco%2520F.%2520Duarte%26entry.1292438233%3D%2520%2520High-spatial-resolution%2520hyperspectral%2520images%2520%2528HSI%2529%2520are%2520essential%2520for%250Aapplications%2520such%2520as%2520remote%2520sensing%2520and%2520medical%2520imaging%252C%2520yet%2520HSI%2520sensors%250Ainherently%2520trade%2520spatial%2520detail%2520for%2520spectral%2520richness.%2520Fusing%250Ahigh-spatial-resolution%2520multispectral%2520images%2520%2528HR-MSI%2529%2520with%250Alow-spatial-resolution%2520hyperspectral%2520images%2520%2528LR-HSI%2529%2520is%2520a%2520promising%2520route%2520to%250Arecover%2520fine%2520spatial%2520structures%2520without%2520sacrificing%2520spectral%2520fidelity.%2520Most%250Astate-of-the-art%2520methods%2520for%2520HSI-MSI%2520fusion%2520demand%2520point%2520spread%2520function%2520%2528PSF%2529%250Acalibration%2520or%2520ground%2520truth%2520high%2520resolution%2520HSI%2520%2528HR-HSI%2529%252C%2520both%2520of%2520which%2520are%250Aimpractical%2520to%2520obtain%2520in%2520real%2520world%2520settings.%2520We%2520present%2520SpectraLift%252C%2520a%2520fully%250Aself-supervised%2520framework%2520that%2520fuses%2520LR-HSI%2520and%2520HR-MSI%2520inputs%2520using%2520only%2520the%250AMSI%2527s%2520Spectral%2520Response%2520Function%2520%2528SRF%2529.%2520SpectraLift%2520trains%2520a%2520lightweight%250Aper-pixel%2520multi-layer%2520perceptron%2520%2528MLP%2529%2520network%2520using%2520%2528%2524i%2524%2529~a%2520synthetic%250Alow-spatial-resolution%2520multispectral%2520image%2520%2528LR-MSI%2529%2520obtained%2520by%2520applying%2520the%250ASRF%2520to%2520the%2520LR-HSI%2520as%2520input%252C%2520%2528%2524ii%2524%2529~the%2520LR-HSI%2520as%2520the%2520output%252C%2520and%2520%2528%2524iii%2524%2529~an%250A%2524%255Cell_1%2524%2520spectral%2520reconstruction%2520loss%2520between%2520the%2520estimated%2520and%2520true%2520LR-HSI%2520as%250Athe%2520optimization%2520objective.%2520At%2520inference%252C%2520SpectraLift%2520uses%2520the%2520trained%2520network%250Ato%2520map%2520the%2520HR-MSI%2520pixel-wise%2520into%2520a%2520HR-HSI%2520estimate.%2520SpectraLift%2520converges%2520in%250Aminutes%252C%2520is%2520agnostic%2520to%2520spatial%2520blur%2520and%2520resolution%252C%2520and%2520outperforms%250Astate-of-the-art%2520methods%2520on%2520PSNR%252C%2520SAM%252C%2520SSIM%252C%2520and%2520RMSE%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectraLift%3A%20Physics-Guided%20Spectral-Inversion%20Network%20for%0A%20%20Self-Supervised%20Hyperspectral%20Image%20Super-Resolution&entry.906535625=Ritik%20Shah%20and%20Marco%20F.%20Duarte&entry.1292438233=%20%20High-spatial-resolution%20hyperspectral%20images%20%28HSI%29%20are%20essential%20for%0Aapplications%20such%20as%20remote%20sensing%20and%20medical%20imaging%2C%20yet%20HSI%20sensors%0Ainherently%20trade%20spatial%20detail%20for%20spectral%20richness.%20Fusing%0Ahigh-spatial-resolution%20multispectral%20images%20%28HR-MSI%29%20with%0Alow-spatial-resolution%20hyperspectral%20images%20%28LR-HSI%29%20is%20a%20promising%20route%20to%0Arecover%20fine%20spatial%20structures%20without%20sacrificing%20spectral%20fidelity.%20Most%0Astate-of-the-art%20methods%20for%20HSI-MSI%20fusion%20demand%20point%20spread%20function%20%28PSF%29%0Acalibration%20or%20ground%20truth%20high%20resolution%20HSI%20%28HR-HSI%29%2C%20both%20of%20which%20are%0Aimpractical%20to%20obtain%20in%20real%20world%20settings.%20We%20present%20SpectraLift%2C%20a%20fully%0Aself-supervised%20framework%20that%20fuses%20LR-HSI%20and%20HR-MSI%20inputs%20using%20only%20the%0AMSI%27s%20Spectral%20Response%20Function%20%28SRF%29.%20SpectraLift%20trains%20a%20lightweight%0Aper-pixel%20multi-layer%20perceptron%20%28MLP%29%20network%20using%20%28%24i%24%29~a%20synthetic%0Alow-spatial-resolution%20multispectral%20image%20%28LR-MSI%29%20obtained%20by%20applying%20the%0ASRF%20to%20the%20LR-HSI%20as%20input%2C%20%28%24ii%24%29~the%20LR-HSI%20as%20the%20output%2C%20and%20%28%24iii%24%29~an%0A%24%5Cell_1%24%20spectral%20reconstruction%20loss%20between%20the%20estimated%20and%20true%20LR-HSI%20as%0Athe%20optimization%20objective.%20At%20inference%2C%20SpectraLift%20uses%20the%20trained%20network%0Ato%20map%20the%20HR-MSI%20pixel-wise%20into%20a%20HR-HSI%20estimate.%20SpectraLift%20converges%20in%0Aminutes%2C%20is%20agnostic%20to%20spatial%20blur%20and%20resolution%2C%20and%20outperforms%0Astate-of-the-art%20methods%20on%20PSNR%2C%20SAM%2C%20SSIM%2C%20and%20RMSE%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13339v2&entry.124074799=Read"},
{"title": "A Synthetic Data-Driven Radiology Foundation Model for Pan-tumor\n  Clinical Diagnosis", "author": "Wenhui Lei and Hanyu Chen and Zitian Zhang and Luyang Luo and Qiong Xiao and Yannian Gu and Peng Gao and Yankai Jiang and Ci Wang and Guangtao Wu and Tongjia Xu and Yingjie Zhang and Pranav Rajpurkar and Xiaofan Zhang and Shaoting Zhang and Zhenning Wang", "abstract": "  AI-assisted imaging made substantial advances in tumor diagnosis and\nmanagement. However, a major barrier to developing robust oncology foundation\nmodels is the scarcity of large-scale, high-quality annotated datasets, which\nare limited by privacy restrictions and the high cost of manual labeling. To\naddress this gap, we present PASTA, a pan-tumor radiology foundation model\nbuilt on PASTA-Gen, a synthetic data framework that generated 30,000 3D CT\nscans with pixel-level lesion masks and structured reports of tumors across ten\norgan systems. Leveraging this resource, PASTA achieves state-of-the-art\nperformance on 45 of 46 oncology tasks, including non-contrast CT tumor\nscreening, lesion segmentation, structured reporting, tumor staging, survival\nprediction, and MRI-modality transfer. To assess clinical applicability, we\ndeveloped PASTA-AID, a clinical decision support system, and ran a\nretrospective simulated clinical trial across two scenarios. For pan-tumor\nscreening on plain CT with fixed reading time, PASTA-AID increased\nradiologists' throughput by 11.1-25.1% and improved sensitivity by 17.0-31.4%\nand precision by 10.5-24.9%; additionally, in a diagnosis-aid workflow, it\nreduced segmentation time by up to 78.2% and reporting time by up to 36.5%.\nBeyond gains in accuracy and efficiency, PASTA-AID narrowed the expertise gap,\nenabling less-experienced radiologists to approach expert-level performance.\nTogether, this work establishes an end-to-end, synthetic data-driven pipeline\nspanning data generation, model development, and clinical validation, thereby\ndemonstrating substantial potential for pan-tumor research and clinical\ntranslation.\n", "link": "http://arxiv.org/abs/2502.06171v2", "date": "2025-10-20", "relevancy": 2.5589, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5222}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Synthetic%20Data-Driven%20Radiology%20Foundation%20Model%20for%20Pan-tumor%0A%20%20Clinical%20Diagnosis&body=Title%3A%20A%20Synthetic%20Data-Driven%20Radiology%20Foundation%20Model%20for%20Pan-tumor%0A%20%20Clinical%20Diagnosis%0AAuthor%3A%20Wenhui%20Lei%20and%20Hanyu%20Chen%20and%20Zitian%20Zhang%20and%20Luyang%20Luo%20and%20Qiong%20Xiao%20and%20Yannian%20Gu%20and%20Peng%20Gao%20and%20Yankai%20Jiang%20and%20Ci%20Wang%20and%20Guangtao%20Wu%20and%20Tongjia%20Xu%20and%20Yingjie%20Zhang%20and%20Pranav%20Rajpurkar%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang%20and%20Zhenning%20Wang%0AAbstract%3A%20%20%20AI-assisted%20imaging%20made%20substantial%20advances%20in%20tumor%20diagnosis%20and%0Amanagement.%20However%2C%20a%20major%20barrier%20to%20developing%20robust%20oncology%20foundation%0Amodels%20is%20the%20scarcity%20of%20large-scale%2C%20high-quality%20annotated%20datasets%2C%20which%0Aare%20limited%20by%20privacy%20restrictions%20and%20the%20high%20cost%20of%20manual%20labeling.%20To%0Aaddress%20this%20gap%2C%20we%20present%20PASTA%2C%20a%20pan-tumor%20radiology%20foundation%20model%0Abuilt%20on%20PASTA-Gen%2C%20a%20synthetic%20data%20framework%20that%20generated%2030%2C000%203D%20CT%0Ascans%20with%20pixel-level%20lesion%20masks%20and%20structured%20reports%20of%20tumors%20across%20ten%0Aorgan%20systems.%20Leveraging%20this%20resource%2C%20PASTA%20achieves%20state-of-the-art%0Aperformance%20on%2045%20of%2046%20oncology%20tasks%2C%20including%20non-contrast%20CT%20tumor%0Ascreening%2C%20lesion%20segmentation%2C%20structured%20reporting%2C%20tumor%20staging%2C%20survival%0Aprediction%2C%20and%20MRI-modality%20transfer.%20To%20assess%20clinical%20applicability%2C%20we%0Adeveloped%20PASTA-AID%2C%20a%20clinical%20decision%20support%20system%2C%20and%20ran%20a%0Aretrospective%20simulated%20clinical%20trial%20across%20two%20scenarios.%20For%20pan-tumor%0Ascreening%20on%20plain%20CT%20with%20fixed%20reading%20time%2C%20PASTA-AID%20increased%0Aradiologists%27%20throughput%20by%2011.1-25.1%25%20and%20improved%20sensitivity%20by%2017.0-31.4%25%0Aand%20precision%20by%2010.5-24.9%25%3B%20additionally%2C%20in%20a%20diagnosis-aid%20workflow%2C%20it%0Areduced%20segmentation%20time%20by%20up%20to%2078.2%25%20and%20reporting%20time%20by%20up%20to%2036.5%25.%0ABeyond%20gains%20in%20accuracy%20and%20efficiency%2C%20PASTA-AID%20narrowed%20the%20expertise%20gap%2C%0Aenabling%20less-experienced%20radiologists%20to%20approach%20expert-level%20performance.%0ATogether%2C%20this%20work%20establishes%20an%20end-to-end%2C%20synthetic%20data-driven%20pipeline%0Aspanning%20data%20generation%2C%20model%20development%2C%20and%20clinical%20validation%2C%20thereby%0Ademonstrating%20substantial%20potential%20for%20pan-tumor%20research%20and%20clinical%0Atranslation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Synthetic%2520Data-Driven%2520Radiology%2520Foundation%2520Model%2520for%2520Pan-tumor%250A%2520%2520Clinical%2520Diagnosis%26entry.906535625%3DWenhui%2520Lei%2520and%2520Hanyu%2520Chen%2520and%2520Zitian%2520Zhang%2520and%2520Luyang%2520Luo%2520and%2520Qiong%2520Xiao%2520and%2520Yannian%2520Gu%2520and%2520Peng%2520Gao%2520and%2520Yankai%2520Jiang%2520and%2520Ci%2520Wang%2520and%2520Guangtao%2520Wu%2520and%2520Tongjia%2520Xu%2520and%2520Yingjie%2520Zhang%2520and%2520Pranav%2520Rajpurkar%2520and%2520Xiaofan%2520Zhang%2520and%2520Shaoting%2520Zhang%2520and%2520Zhenning%2520Wang%26entry.1292438233%3D%2520%2520AI-assisted%2520imaging%2520made%2520substantial%2520advances%2520in%2520tumor%2520diagnosis%2520and%250Amanagement.%2520However%252C%2520a%2520major%2520barrier%2520to%2520developing%2520robust%2520oncology%2520foundation%250Amodels%2520is%2520the%2520scarcity%2520of%2520large-scale%252C%2520high-quality%2520annotated%2520datasets%252C%2520which%250Aare%2520limited%2520by%2520privacy%2520restrictions%2520and%2520the%2520high%2520cost%2520of%2520manual%2520labeling.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520present%2520PASTA%252C%2520a%2520pan-tumor%2520radiology%2520foundation%2520model%250Abuilt%2520on%2520PASTA-Gen%252C%2520a%2520synthetic%2520data%2520framework%2520that%2520generated%252030%252C000%25203D%2520CT%250Ascans%2520with%2520pixel-level%2520lesion%2520masks%2520and%2520structured%2520reports%2520of%2520tumors%2520across%2520ten%250Aorgan%2520systems.%2520Leveraging%2520this%2520resource%252C%2520PASTA%2520achieves%2520state-of-the-art%250Aperformance%2520on%252045%2520of%252046%2520oncology%2520tasks%252C%2520including%2520non-contrast%2520CT%2520tumor%250Ascreening%252C%2520lesion%2520segmentation%252C%2520structured%2520reporting%252C%2520tumor%2520staging%252C%2520survival%250Aprediction%252C%2520and%2520MRI-modality%2520transfer.%2520To%2520assess%2520clinical%2520applicability%252C%2520we%250Adeveloped%2520PASTA-AID%252C%2520a%2520clinical%2520decision%2520support%2520system%252C%2520and%2520ran%2520a%250Aretrospective%2520simulated%2520clinical%2520trial%2520across%2520two%2520scenarios.%2520For%2520pan-tumor%250Ascreening%2520on%2520plain%2520CT%2520with%2520fixed%2520reading%2520time%252C%2520PASTA-AID%2520increased%250Aradiologists%2527%2520throughput%2520by%252011.1-25.1%2525%2520and%2520improved%2520sensitivity%2520by%252017.0-31.4%2525%250Aand%2520precision%2520by%252010.5-24.9%2525%253B%2520additionally%252C%2520in%2520a%2520diagnosis-aid%2520workflow%252C%2520it%250Areduced%2520segmentation%2520time%2520by%2520up%2520to%252078.2%2525%2520and%2520reporting%2520time%2520by%2520up%2520to%252036.5%2525.%250ABeyond%2520gains%2520in%2520accuracy%2520and%2520efficiency%252C%2520PASTA-AID%2520narrowed%2520the%2520expertise%2520gap%252C%250Aenabling%2520less-experienced%2520radiologists%2520to%2520approach%2520expert-level%2520performance.%250ATogether%252C%2520this%2520work%2520establishes%2520an%2520end-to-end%252C%2520synthetic%2520data-driven%2520pipeline%250Aspanning%2520data%2520generation%252C%2520model%2520development%252C%2520and%2520clinical%2520validation%252C%2520thereby%250Ademonstrating%2520substantial%2520potential%2520for%2520pan-tumor%2520research%2520and%2520clinical%250Atranslation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Synthetic%20Data-Driven%20Radiology%20Foundation%20Model%20for%20Pan-tumor%0A%20%20Clinical%20Diagnosis&entry.906535625=Wenhui%20Lei%20and%20Hanyu%20Chen%20and%20Zitian%20Zhang%20and%20Luyang%20Luo%20and%20Qiong%20Xiao%20and%20Yannian%20Gu%20and%20Peng%20Gao%20and%20Yankai%20Jiang%20and%20Ci%20Wang%20and%20Guangtao%20Wu%20and%20Tongjia%20Xu%20and%20Yingjie%20Zhang%20and%20Pranav%20Rajpurkar%20and%20Xiaofan%20Zhang%20and%20Shaoting%20Zhang%20and%20Zhenning%20Wang&entry.1292438233=%20%20AI-assisted%20imaging%20made%20substantial%20advances%20in%20tumor%20diagnosis%20and%0Amanagement.%20However%2C%20a%20major%20barrier%20to%20developing%20robust%20oncology%20foundation%0Amodels%20is%20the%20scarcity%20of%20large-scale%2C%20high-quality%20annotated%20datasets%2C%20which%0Aare%20limited%20by%20privacy%20restrictions%20and%20the%20high%20cost%20of%20manual%20labeling.%20To%0Aaddress%20this%20gap%2C%20we%20present%20PASTA%2C%20a%20pan-tumor%20radiology%20foundation%20model%0Abuilt%20on%20PASTA-Gen%2C%20a%20synthetic%20data%20framework%20that%20generated%2030%2C000%203D%20CT%0Ascans%20with%20pixel-level%20lesion%20masks%20and%20structured%20reports%20of%20tumors%20across%20ten%0Aorgan%20systems.%20Leveraging%20this%20resource%2C%20PASTA%20achieves%20state-of-the-art%0Aperformance%20on%2045%20of%2046%20oncology%20tasks%2C%20including%20non-contrast%20CT%20tumor%0Ascreening%2C%20lesion%20segmentation%2C%20structured%20reporting%2C%20tumor%20staging%2C%20survival%0Aprediction%2C%20and%20MRI-modality%20transfer.%20To%20assess%20clinical%20applicability%2C%20we%0Adeveloped%20PASTA-AID%2C%20a%20clinical%20decision%20support%20system%2C%20and%20ran%20a%0Aretrospective%20simulated%20clinical%20trial%20across%20two%20scenarios.%20For%20pan-tumor%0Ascreening%20on%20plain%20CT%20with%20fixed%20reading%20time%2C%20PASTA-AID%20increased%0Aradiologists%27%20throughput%20by%2011.1-25.1%25%20and%20improved%20sensitivity%20by%2017.0-31.4%25%0Aand%20precision%20by%2010.5-24.9%25%3B%20additionally%2C%20in%20a%20diagnosis-aid%20workflow%2C%20it%0Areduced%20segmentation%20time%20by%20up%20to%2078.2%25%20and%20reporting%20time%20by%20up%20to%2036.5%25.%0ABeyond%20gains%20in%20accuracy%20and%20efficiency%2C%20PASTA-AID%20narrowed%20the%20expertise%20gap%2C%0Aenabling%20less-experienced%20radiologists%20to%20approach%20expert-level%20performance.%0ATogether%2C%20this%20work%20establishes%20an%20end-to-end%2C%20synthetic%20data-driven%20pipeline%0Aspanning%20data%20generation%2C%20model%20development%2C%20and%20clinical%20validation%2C%20thereby%0Ademonstrating%20substantial%20potential%20for%20pan-tumor%20research%20and%20clinical%0Atranslation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06171v2&entry.124074799=Read"},
{"title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code\n  Summarization and Clone Detection", "author": "Guang Yang and Yujie Zhu", "abstract": "  Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.\n", "link": "http://arxiv.org/abs/2510.17591v1", "date": "2025-10-20", "relevancy": 2.5577, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGAdapter%3A%20Hypergraph-based%20Adapters%20in%20Language%20Models%20for%20Code%0A%20%20Summarization%20and%20Clone%20Detection&body=Title%3A%20HGAdapter%3A%20Hypergraph-based%20Adapters%20in%20Language%20Models%20for%20Code%0A%20%20Summarization%20and%20Clone%20Detection%0AAuthor%3A%20Guang%20Yang%20and%20Yujie%20Zhu%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20%28PLMs%29%20are%20increasingly%20being%20applied%20to%0Acode-related%20tasks.%20Although%20PLMs%20have%20achieved%20good%20results%2C%20they%20do%20not%20take%0Ainto%20account%20potential%20high-order%20data%20correlations%20within%20the%20code.%20We%20propose%0Athree%20types%20of%20high-order%20correlations%20in%20code%20tokens%2C%20i.e.%20abstract%20syntax%0Atree%20family%20correlation%2C%20lexical%20correlation%2C%20and%20line%20correlation.%20We%20design%20a%0Atokens%20and%20hyperedges%20generator%20to%20capture%20these%20high-order%20data%20correlations.%0AWe%20improve%20the%20architecture%20of%20hypergraph%20neural%20networks%20and%20combine%20it%20with%0Aadapter%20tuning%20to%20propose%20a%20novel%20hypergraph-based%20adapter%20%28HGAdapter%29%20to%0Afine-tune%20PLMs.%20HGAdapter%20can%20encode%20high-order%20data%20correlations%20and%20is%0Aallowed%20to%20be%20inserted%20into%20various%20PLMs%20to%20enhance%20performance.%20Experiments%0Awere%20conducted%20on%20several%20public%20datasets%2C%20including%20six%20languages%20of%20code%0Asummarization%20and%20code%20clone%20detection%20tasks.%20Our%20methods%20improved%20the%0Aperformance%20of%20PLMs%20in%20datasets%20to%20varying%20degrees.%20Experimental%20results%0Avalidate%20the%20introduction%20of%20high-order%20data%20correlations%20that%20contribute%20to%0Aimproved%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGAdapter%253A%2520Hypergraph-based%2520Adapters%2520in%2520Language%2520Models%2520for%2520Code%250A%2520%2520Summarization%2520and%2520Clone%2520Detection%26entry.906535625%3DGuang%2520Yang%2520and%2520Yujie%2520Zhu%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520%2528PLMs%2529%2520are%2520increasingly%2520being%2520applied%2520to%250Acode-related%2520tasks.%2520Although%2520PLMs%2520have%2520achieved%2520good%2520results%252C%2520they%2520do%2520not%2520take%250Ainto%2520account%2520potential%2520high-order%2520data%2520correlations%2520within%2520the%2520code.%2520We%2520propose%250Athree%2520types%2520of%2520high-order%2520correlations%2520in%2520code%2520tokens%252C%2520i.e.%2520abstract%2520syntax%250Atree%2520family%2520correlation%252C%2520lexical%2520correlation%252C%2520and%2520line%2520correlation.%2520We%2520design%2520a%250Atokens%2520and%2520hyperedges%2520generator%2520to%2520capture%2520these%2520high-order%2520data%2520correlations.%250AWe%2520improve%2520the%2520architecture%2520of%2520hypergraph%2520neural%2520networks%2520and%2520combine%2520it%2520with%250Aadapter%2520tuning%2520to%2520propose%2520a%2520novel%2520hypergraph-based%2520adapter%2520%2528HGAdapter%2529%2520to%250Afine-tune%2520PLMs.%2520HGAdapter%2520can%2520encode%2520high-order%2520data%2520correlations%2520and%2520is%250Aallowed%2520to%2520be%2520inserted%2520into%2520various%2520PLMs%2520to%2520enhance%2520performance.%2520Experiments%250Awere%2520conducted%2520on%2520several%2520public%2520datasets%252C%2520including%2520six%2520languages%2520of%2520code%250Asummarization%2520and%2520code%2520clone%2520detection%2520tasks.%2520Our%2520methods%2520improved%2520the%250Aperformance%2520of%2520PLMs%2520in%2520datasets%2520to%2520varying%2520degrees.%2520Experimental%2520results%250Avalidate%2520the%2520introduction%2520of%2520high-order%2520data%2520correlations%2520that%2520contribute%2520to%250Aimproved%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGAdapter%3A%20Hypergraph-based%20Adapters%20in%20Language%20Models%20for%20Code%0A%20%20Summarization%20and%20Clone%20Detection&entry.906535625=Guang%20Yang%20and%20Yujie%20Zhu&entry.1292438233=%20%20Pre-trained%20language%20models%20%28PLMs%29%20are%20increasingly%20being%20applied%20to%0Acode-related%20tasks.%20Although%20PLMs%20have%20achieved%20good%20results%2C%20they%20do%20not%20take%0Ainto%20account%20potential%20high-order%20data%20correlations%20within%20the%20code.%20We%20propose%0Athree%20types%20of%20high-order%20correlations%20in%20code%20tokens%2C%20i.e.%20abstract%20syntax%0Atree%20family%20correlation%2C%20lexical%20correlation%2C%20and%20line%20correlation.%20We%20design%20a%0Atokens%20and%20hyperedges%20generator%20to%20capture%20these%20high-order%20data%20correlations.%0AWe%20improve%20the%20architecture%20of%20hypergraph%20neural%20networks%20and%20combine%20it%20with%0Aadapter%20tuning%20to%20propose%20a%20novel%20hypergraph-based%20adapter%20%28HGAdapter%29%20to%0Afine-tune%20PLMs.%20HGAdapter%20can%20encode%20high-order%20data%20correlations%20and%20is%0Aallowed%20to%20be%20inserted%20into%20various%20PLMs%20to%20enhance%20performance.%20Experiments%0Awere%20conducted%20on%20several%20public%20datasets%2C%20including%20six%20languages%20of%20code%0Asummarization%20and%20code%20clone%20detection%20tasks.%20Our%20methods%20improved%20the%0Aperformance%20of%20PLMs%20in%20datasets%20to%20varying%20degrees.%20Experimental%20results%0Avalidate%20the%20introduction%20of%20high-order%20data%20correlations%20that%20contribute%20to%0Aimproved%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17591v1&entry.124074799=Read"},
{"title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for\n  End-to-End Weakly-Supervised Road Crack Detection", "author": "Nachuan Ma and Zhengfei Song and Qiang Hu and Xiaoyu Tang and Chengxi Zhang and Rui Fan and Lihua Xie", "abstract": "  Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.\n", "link": "http://arxiv.org/abs/2510.17566v1", "date": "2025-10-20", "relevancy": 2.539, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5365}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WP-CrackNet%3A%20A%20Collaborative%20Adversarial%20Learning%20Framework%20for%0A%20%20End-to-End%20Weakly-Supervised%20Road%20Crack%20Detection&body=Title%3A%20WP-CrackNet%3A%20A%20Collaborative%20Adversarial%20Learning%20Framework%20for%0A%20%20End-to-End%20Weakly-Supervised%20Road%20Crack%20Detection%0AAuthor%3A%20Nachuan%20Ma%20and%20Zhengfei%20Song%20and%20Qiang%20Hu%20and%20Xiaoyu%20Tang%20and%20Chengxi%20Zhang%20and%20Rui%20Fan%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Road%20crack%20detection%20is%20essential%20for%20intelligent%20infrastructure%20maintenance%0Ain%20smart%20cities.%20To%20reduce%20reliance%20on%20costly%20pixel-level%20annotations%2C%20we%0Apropose%20WP-CrackNet%2C%20an%20end-to-end%20weakly-supervised%20method%20that%20trains%20with%0Aonly%20image-level%20labels%20for%20pixel-wise%20crack%20detection.%20WP-CrackNet%20integrates%0Athree%20components%3A%20a%20classifier%20generating%20class%20activation%20maps%20%28CAMs%29%2C%20a%0Areconstructor%20measuring%20feature%20inferability%2C%20and%20a%20detector%20producing%0Apixel-wise%20road%20crack%20detection%20results.%20During%20training%2C%20the%20classifier%20and%0Areconstructor%20alternate%20in%20adversarial%20learning%20to%20encourage%20crack%20CAMs%20to%0Acover%20complete%20crack%20regions%2C%20while%20the%20detector%20learns%20from%20pseudo%20labels%0Aderived%20from%20post-processed%20crack%20CAMs.%20This%20mutual%20feedback%20among%20the%20three%0Acomponents%20improves%20learning%20stability%20and%20detection%20accuracy.%20To%20further%20boost%0Adetection%20performance%2C%20we%20design%20a%20path-aware%20attention%20module%20%28PAAM%29%20that%0Afuses%20high-level%20semantics%20from%20the%20classifier%20with%20low-level%20structural%20cues%0Afrom%20the%20reconstructor%20by%20modeling%20spatial%20and%20channel-wise%20dependencies.%0AAdditionally%2C%20a%20center-enhanced%20CAM%20consistency%20module%20%28CECCM%29%20is%20proposed%20to%0Arefine%20crack%20CAMs%20using%20center%20Gaussian%20weighting%20and%20consistency%20constraints%2C%0Aenabling%20better%20pseudo-label%20generation.%20We%20create%20three%20image-level%20datasets%0Aand%20extensive%20experiments%20show%20that%20WP-CrackNet%20achieves%20comparable%20results%20to%0Asupervised%20methods%20and%20outperforms%20existing%20weakly-supervised%20methods%2C%0Asignificantly%20advancing%20scalable%20road%20inspection.%20The%20source%20code%20package%20and%0Adatasets%20are%20available%20at%20https%3A//mias.group/WP-CrackNet/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWP-CrackNet%253A%2520A%2520Collaborative%2520Adversarial%2520Learning%2520Framework%2520for%250A%2520%2520End-to-End%2520Weakly-Supervised%2520Road%2520Crack%2520Detection%26entry.906535625%3DNachuan%2520Ma%2520and%2520Zhengfei%2520Song%2520and%2520Qiang%2520Hu%2520and%2520Xiaoyu%2520Tang%2520and%2520Chengxi%2520Zhang%2520and%2520Rui%2520Fan%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Road%2520crack%2520detection%2520is%2520essential%2520for%2520intelligent%2520infrastructure%2520maintenance%250Ain%2520smart%2520cities.%2520To%2520reduce%2520reliance%2520on%2520costly%2520pixel-level%2520annotations%252C%2520we%250Apropose%2520WP-CrackNet%252C%2520an%2520end-to-end%2520weakly-supervised%2520method%2520that%2520trains%2520with%250Aonly%2520image-level%2520labels%2520for%2520pixel-wise%2520crack%2520detection.%2520WP-CrackNet%2520integrates%250Athree%2520components%253A%2520a%2520classifier%2520generating%2520class%2520activation%2520maps%2520%2528CAMs%2529%252C%2520a%250Areconstructor%2520measuring%2520feature%2520inferability%252C%2520and%2520a%2520detector%2520producing%250Apixel-wise%2520road%2520crack%2520detection%2520results.%2520During%2520training%252C%2520the%2520classifier%2520and%250Areconstructor%2520alternate%2520in%2520adversarial%2520learning%2520to%2520encourage%2520crack%2520CAMs%2520to%250Acover%2520complete%2520crack%2520regions%252C%2520while%2520the%2520detector%2520learns%2520from%2520pseudo%2520labels%250Aderived%2520from%2520post-processed%2520crack%2520CAMs.%2520This%2520mutual%2520feedback%2520among%2520the%2520three%250Acomponents%2520improves%2520learning%2520stability%2520and%2520detection%2520accuracy.%2520To%2520further%2520boost%250Adetection%2520performance%252C%2520we%2520design%2520a%2520path-aware%2520attention%2520module%2520%2528PAAM%2529%2520that%250Afuses%2520high-level%2520semantics%2520from%2520the%2520classifier%2520with%2520low-level%2520structural%2520cues%250Afrom%2520the%2520reconstructor%2520by%2520modeling%2520spatial%2520and%2520channel-wise%2520dependencies.%250AAdditionally%252C%2520a%2520center-enhanced%2520CAM%2520consistency%2520module%2520%2528CECCM%2529%2520is%2520proposed%2520to%250Arefine%2520crack%2520CAMs%2520using%2520center%2520Gaussian%2520weighting%2520and%2520consistency%2520constraints%252C%250Aenabling%2520better%2520pseudo-label%2520generation.%2520We%2520create%2520three%2520image-level%2520datasets%250Aand%2520extensive%2520experiments%2520show%2520that%2520WP-CrackNet%2520achieves%2520comparable%2520results%2520to%250Asupervised%2520methods%2520and%2520outperforms%2520existing%2520weakly-supervised%2520methods%252C%250Asignificantly%2520advancing%2520scalable%2520road%2520inspection.%2520The%2520source%2520code%2520package%2520and%250Adatasets%2520are%2520available%2520at%2520https%253A//mias.group/WP-CrackNet/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WP-CrackNet%3A%20A%20Collaborative%20Adversarial%20Learning%20Framework%20for%0A%20%20End-to-End%20Weakly-Supervised%20Road%20Crack%20Detection&entry.906535625=Nachuan%20Ma%20and%20Zhengfei%20Song%20and%20Qiang%20Hu%20and%20Xiaoyu%20Tang%20and%20Chengxi%20Zhang%20and%20Rui%20Fan%20and%20Lihua%20Xie&entry.1292438233=%20%20Road%20crack%20detection%20is%20essential%20for%20intelligent%20infrastructure%20maintenance%0Ain%20smart%20cities.%20To%20reduce%20reliance%20on%20costly%20pixel-level%20annotations%2C%20we%0Apropose%20WP-CrackNet%2C%20an%20end-to-end%20weakly-supervised%20method%20that%20trains%20with%0Aonly%20image-level%20labels%20for%20pixel-wise%20crack%20detection.%20WP-CrackNet%20integrates%0Athree%20components%3A%20a%20classifier%20generating%20class%20activation%20maps%20%28CAMs%29%2C%20a%0Areconstructor%20measuring%20feature%20inferability%2C%20and%20a%20detector%20producing%0Apixel-wise%20road%20crack%20detection%20results.%20During%20training%2C%20the%20classifier%20and%0Areconstructor%20alternate%20in%20adversarial%20learning%20to%20encourage%20crack%20CAMs%20to%0Acover%20complete%20crack%20regions%2C%20while%20the%20detector%20learns%20from%20pseudo%20labels%0Aderived%20from%20post-processed%20crack%20CAMs.%20This%20mutual%20feedback%20among%20the%20three%0Acomponents%20improves%20learning%20stability%20and%20detection%20accuracy.%20To%20further%20boost%0Adetection%20performance%2C%20we%20design%20a%20path-aware%20attention%20module%20%28PAAM%29%20that%0Afuses%20high-level%20semantics%20from%20the%20classifier%20with%20low-level%20structural%20cues%0Afrom%20the%20reconstructor%20by%20modeling%20spatial%20and%20channel-wise%20dependencies.%0AAdditionally%2C%20a%20center-enhanced%20CAM%20consistency%20module%20%28CECCM%29%20is%20proposed%20to%0Arefine%20crack%20CAMs%20using%20center%20Gaussian%20weighting%20and%20consistency%20constraints%2C%0Aenabling%20better%20pseudo-label%20generation.%20We%20create%20three%20image-level%20datasets%0Aand%20extensive%20experiments%20show%20that%20WP-CrackNet%20achieves%20comparable%20results%20to%0Asupervised%20methods%20and%20outperforms%20existing%20weakly-supervised%20methods%2C%0Asignificantly%20advancing%20scalable%20road%20inspection.%20The%20source%20code%20package%20and%0Adatasets%20are%20available%20at%20https%3A//mias.group/WP-CrackNet/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17566v1&entry.124074799=Read"},
{"title": "Frugal Federated Learning for Violence Detection: A Comparison of\n  LoRA-Tuned VLMs and Personalized CNNs", "author": "S\u00e9bastien Thuau and Siba Haidar and Ayush Bajracharya and Rachid Chelouah", "abstract": "  We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.\n", "link": "http://arxiv.org/abs/2510.17651v1", "date": "2025-10-20", "relevancy": 2.5275, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frugal%20Federated%20Learning%20for%20Violence%20Detection%3A%20A%20Comparison%20of%0A%20%20LoRA-Tuned%20VLMs%20and%20Personalized%20CNNs&body=Title%3A%20Frugal%20Federated%20Learning%20for%20Violence%20Detection%3A%20A%20Comparison%20of%0A%20%20LoRA-Tuned%20VLMs%20and%20Personalized%20CNNs%0AAuthor%3A%20S%C3%A9bastien%20Thuau%20and%20Siba%20Haidar%20and%20Ayush%20Bajracharya%20and%20Rachid%20Chelouah%0AAbstract%3A%20%20%20We%20examine%20frugal%20federated%20learning%20approaches%20to%20violence%20detection%20by%0Acomparing%20two%20complementary%20strategies%3A%20%28i%29%20zero-shot%20and%20federated%20fine-tuning%0Aof%20vision-language%20models%20%28VLMs%29%2C%20and%20%28ii%29%20personalized%20training%20of%20a%20compact%0A3D%20convolutional%20neural%20network%20%28CNN3D%29.%20Using%20LLaVA-7B%20and%20a%2065.8M%20parameter%0ACNN3D%20as%20representative%20cases%2C%20we%20evaluate%20accuracy%2C%20calibration%2C%20and%20energy%0Ausage%20under%20realistic%20non-IID%20settings.%20Both%20approaches%20exceed%2090%25%20accuracy.%0ACNN3D%20slightly%20outperforms%20Low-Rank%20Adaptation%28LoRA%29-tuned%20VLMs%20in%20ROC%20AUC%20and%0Alog%20loss%2C%20while%20using%20less%20energy.%20VLMs%20remain%20favorable%20for%20contextual%0Areasoning%20and%20multimodal%20inference.%20We%20quantify%20energy%20and%20CO%24_2%24%20emissions%0Aacross%20training%20and%20inference%2C%20and%20analyze%20sustainability%20trade-offs%20for%0Adeployment.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20comparative%20study%20of%20LoRA-tuned%0Avision-language%20models%20and%20personalized%20CNNs%20for%20federated%20violence%20detection%2C%0Awith%20an%20emphasis%20on%20energy%20efficiency%20and%20environmental%20metrics.%20These%20findings%0Asupport%20a%20hybrid%20model%3A%20lightweight%20CNNs%20for%20routine%20classification%2C%20with%0Aselective%20VLM%20activation%20for%20complex%20or%20descriptive%20scenarios.%20The%20resulting%0Aframework%20offers%20a%20reproducible%20baseline%20for%20responsible%2C%20resource-aware%20AI%20in%0Avideo%20surveillance%2C%20with%20extensions%20toward%20real-time%2C%20multimodal%2C%20and%0Alifecycle-aware%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrugal%2520Federated%2520Learning%2520for%2520Violence%2520Detection%253A%2520A%2520Comparison%2520of%250A%2520%2520LoRA-Tuned%2520VLMs%2520and%2520Personalized%2520CNNs%26entry.906535625%3DS%25C3%25A9bastien%2520Thuau%2520and%2520Siba%2520Haidar%2520and%2520Ayush%2520Bajracharya%2520and%2520Rachid%2520Chelouah%26entry.1292438233%3D%2520%2520We%2520examine%2520frugal%2520federated%2520learning%2520approaches%2520to%2520violence%2520detection%2520by%250Acomparing%2520two%2520complementary%2520strategies%253A%2520%2528i%2529%2520zero-shot%2520and%2520federated%2520fine-tuning%250Aof%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520and%2520%2528ii%2529%2520personalized%2520training%2520of%2520a%2520compact%250A3D%2520convolutional%2520neural%2520network%2520%2528CNN3D%2529.%2520Using%2520LLaVA-7B%2520and%2520a%252065.8M%2520parameter%250ACNN3D%2520as%2520representative%2520cases%252C%2520we%2520evaluate%2520accuracy%252C%2520calibration%252C%2520and%2520energy%250Ausage%2520under%2520realistic%2520non-IID%2520settings.%2520Both%2520approaches%2520exceed%252090%2525%2520accuracy.%250ACNN3D%2520slightly%2520outperforms%2520Low-Rank%2520Adaptation%2528LoRA%2529-tuned%2520VLMs%2520in%2520ROC%2520AUC%2520and%250Alog%2520loss%252C%2520while%2520using%2520less%2520energy.%2520VLMs%2520remain%2520favorable%2520for%2520contextual%250Areasoning%2520and%2520multimodal%2520inference.%2520We%2520quantify%2520energy%2520and%2520CO%2524_2%2524%2520emissions%250Aacross%2520training%2520and%2520inference%252C%2520and%2520analyze%2520sustainability%2520trade-offs%2520for%250Adeployment.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520comparative%2520study%2520of%2520LoRA-tuned%250Avision-language%2520models%2520and%2520personalized%2520CNNs%2520for%2520federated%2520violence%2520detection%252C%250Awith%2520an%2520emphasis%2520on%2520energy%2520efficiency%2520and%2520environmental%2520metrics.%2520These%2520findings%250Asupport%2520a%2520hybrid%2520model%253A%2520lightweight%2520CNNs%2520for%2520routine%2520classification%252C%2520with%250Aselective%2520VLM%2520activation%2520for%2520complex%2520or%2520descriptive%2520scenarios.%2520The%2520resulting%250Aframework%2520offers%2520a%2520reproducible%2520baseline%2520for%2520responsible%252C%2520resource-aware%2520AI%2520in%250Avideo%2520surveillance%252C%2520with%2520extensions%2520toward%2520real-time%252C%2520multimodal%252C%2520and%250Alifecycle-aware%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frugal%20Federated%20Learning%20for%20Violence%20Detection%3A%20A%20Comparison%20of%0A%20%20LoRA-Tuned%20VLMs%20and%20Personalized%20CNNs&entry.906535625=S%C3%A9bastien%20Thuau%20and%20Siba%20Haidar%20and%20Ayush%20Bajracharya%20and%20Rachid%20Chelouah&entry.1292438233=%20%20We%20examine%20frugal%20federated%20learning%20approaches%20to%20violence%20detection%20by%0Acomparing%20two%20complementary%20strategies%3A%20%28i%29%20zero-shot%20and%20federated%20fine-tuning%0Aof%20vision-language%20models%20%28VLMs%29%2C%20and%20%28ii%29%20personalized%20training%20of%20a%20compact%0A3D%20convolutional%20neural%20network%20%28CNN3D%29.%20Using%20LLaVA-7B%20and%20a%2065.8M%20parameter%0ACNN3D%20as%20representative%20cases%2C%20we%20evaluate%20accuracy%2C%20calibration%2C%20and%20energy%0Ausage%20under%20realistic%20non-IID%20settings.%20Both%20approaches%20exceed%2090%25%20accuracy.%0ACNN3D%20slightly%20outperforms%20Low-Rank%20Adaptation%28LoRA%29-tuned%20VLMs%20in%20ROC%20AUC%20and%0Alog%20loss%2C%20while%20using%20less%20energy.%20VLMs%20remain%20favorable%20for%20contextual%0Areasoning%20and%20multimodal%20inference.%20We%20quantify%20energy%20and%20CO%24_2%24%20emissions%0Aacross%20training%20and%20inference%2C%20and%20analyze%20sustainability%20trade-offs%20for%0Adeployment.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20comparative%20study%20of%20LoRA-tuned%0Avision-language%20models%20and%20personalized%20CNNs%20for%20federated%20violence%20detection%2C%0Awith%20an%20emphasis%20on%20energy%20efficiency%20and%20environmental%20metrics.%20These%20findings%0Asupport%20a%20hybrid%20model%3A%20lightweight%20CNNs%20for%20routine%20classification%2C%20with%0Aselective%20VLM%20activation%20for%20complex%20or%20descriptive%20scenarios.%20The%20resulting%0Aframework%20offers%20a%20reproducible%20baseline%20for%20responsible%2C%20resource-aware%20AI%20in%0Avideo%20surveillance%2C%20with%20extensions%20toward%20real-time%2C%20multimodal%2C%20and%0Alifecycle-aware%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17651v1&entry.124074799=Read"},
{"title": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models", "author": "Samer Al-Hamadani", "abstract": "  Object detection traditionally relies on costly manual annotation. We present\nthe first comprehensive cost-effectiveness analysis comparing supervised YOLO\nand zero-shot vision-language models (Gemini Flash 2.5 and GPT-4). Evaluated on\n5,000 stratified COCO images and 500 diverse product images, combined with\nTotal Cost of Ownership modeling, we derive break-even thresholds for\narchitecture selection. Results show supervised YOLO attains 91.2% accuracy\nversus 68.5% for Gemini and 71.3% for GPT-4 on standard categories; the\nannotation expense for a 100-category system is $10,800, and the accuracy\nadvantage only pays off beyond 55 million inferences (151,000 images/day for\none year). On diverse product categories Gemini achieves 52.3% and GPT-4 55.1%,\nwhile supervised YOLO cannot detect untrained classes.\nCost-per-correct-detection favors Gemini ($0.00050) and GPT-4 ($0.00067) over\nYOLO ($0.143) at 100,000 inferences. We provide decision frameworks showing\nthat optimal architecture choice depends on inference volume, category\nstability, budget, and accuracy requirements.\n", "link": "http://arxiv.org/abs/2510.11302v2", "date": "2025-10-20", "relevancy": 2.4737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Supervised%20Training%20Pay%20Off%3F%20The%20Hidden%20Economics%20of%20Object%0A%20%20Detection%20in%20the%20Era%20of%20Vision-Language%20Models&body=Title%3A%20When%20Does%20Supervised%20Training%20Pay%20Off%3F%20The%20Hidden%20Economics%20of%20Object%0A%20%20Detection%20in%20the%20Era%20of%20Vision-Language%20Models%0AAuthor%3A%20Samer%20Al-Hamadani%0AAbstract%3A%20%20%20Object%20detection%20traditionally%20relies%20on%20costly%20manual%20annotation.%20We%20present%0Athe%20first%20comprehensive%20cost-effectiveness%20analysis%20comparing%20supervised%20YOLO%0Aand%20zero-shot%20vision-language%20models%20%28Gemini%20Flash%202.5%20and%20GPT-4%29.%20Evaluated%20on%0A5%2C000%20stratified%20COCO%20images%20and%20500%20diverse%20product%20images%2C%20combined%20with%0ATotal%20Cost%20of%20Ownership%20modeling%2C%20we%20derive%20break-even%20thresholds%20for%0Aarchitecture%20selection.%20Results%20show%20supervised%20YOLO%20attains%2091.2%25%20accuracy%0Aversus%2068.5%25%20for%20Gemini%20and%2071.3%25%20for%20GPT-4%20on%20standard%20categories%3B%20the%0Aannotation%20expense%20for%20a%20100-category%20system%20is%20%2410%2C800%2C%20and%20the%20accuracy%0Aadvantage%20only%20pays%20off%20beyond%2055%20million%20inferences%20%28151%2C000%20images/day%20for%0Aone%20year%29.%20On%20diverse%20product%20categories%20Gemini%20achieves%2052.3%25%20and%20GPT-4%2055.1%25%2C%0Awhile%20supervised%20YOLO%20cannot%20detect%20untrained%20classes.%0ACost-per-correct-detection%20favors%20Gemini%20%28%240.00050%29%20and%20GPT-4%20%28%240.00067%29%20over%0AYOLO%20%28%240.143%29%20at%20100%2C000%20inferences.%20We%20provide%20decision%20frameworks%20showing%0Athat%20optimal%20architecture%20choice%20depends%20on%20inference%20volume%2C%20category%0Astability%2C%20budget%2C%20and%20accuracy%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Supervised%2520Training%2520Pay%2520Off%253F%2520The%2520Hidden%2520Economics%2520of%2520Object%250A%2520%2520Detection%2520in%2520the%2520Era%2520of%2520Vision-Language%2520Models%26entry.906535625%3DSamer%2520Al-Hamadani%26entry.1292438233%3D%2520%2520Object%2520detection%2520traditionally%2520relies%2520on%2520costly%2520manual%2520annotation.%2520We%2520present%250Athe%2520first%2520comprehensive%2520cost-effectiveness%2520analysis%2520comparing%2520supervised%2520YOLO%250Aand%2520zero-shot%2520vision-language%2520models%2520%2528Gemini%2520Flash%25202.5%2520and%2520GPT-4%2529.%2520Evaluated%2520on%250A5%252C000%2520stratified%2520COCO%2520images%2520and%2520500%2520diverse%2520product%2520images%252C%2520combined%2520with%250ATotal%2520Cost%2520of%2520Ownership%2520modeling%252C%2520we%2520derive%2520break-even%2520thresholds%2520for%250Aarchitecture%2520selection.%2520Results%2520show%2520supervised%2520YOLO%2520attains%252091.2%2525%2520accuracy%250Aversus%252068.5%2525%2520for%2520Gemini%2520and%252071.3%2525%2520for%2520GPT-4%2520on%2520standard%2520categories%253B%2520the%250Aannotation%2520expense%2520for%2520a%2520100-category%2520system%2520is%2520%252410%252C800%252C%2520and%2520the%2520accuracy%250Aadvantage%2520only%2520pays%2520off%2520beyond%252055%2520million%2520inferences%2520%2528151%252C000%2520images/day%2520for%250Aone%2520year%2529.%2520On%2520diverse%2520product%2520categories%2520Gemini%2520achieves%252052.3%2525%2520and%2520GPT-4%252055.1%2525%252C%250Awhile%2520supervised%2520YOLO%2520cannot%2520detect%2520untrained%2520classes.%250ACost-per-correct-detection%2520favors%2520Gemini%2520%2528%25240.00050%2529%2520and%2520GPT-4%2520%2528%25240.00067%2529%2520over%250AYOLO%2520%2528%25240.143%2529%2520at%2520100%252C000%2520inferences.%2520We%2520provide%2520decision%2520frameworks%2520showing%250Athat%2520optimal%2520architecture%2520choice%2520depends%2520on%2520inference%2520volume%252C%2520category%250Astability%252C%2520budget%252C%2520and%2520accuracy%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Supervised%20Training%20Pay%20Off%3F%20The%20Hidden%20Economics%20of%20Object%0A%20%20Detection%20in%20the%20Era%20of%20Vision-Language%20Models&entry.906535625=Samer%20Al-Hamadani&entry.1292438233=%20%20Object%20detection%20traditionally%20relies%20on%20costly%20manual%20annotation.%20We%20present%0Athe%20first%20comprehensive%20cost-effectiveness%20analysis%20comparing%20supervised%20YOLO%0Aand%20zero-shot%20vision-language%20models%20%28Gemini%20Flash%202.5%20and%20GPT-4%29.%20Evaluated%20on%0A5%2C000%20stratified%20COCO%20images%20and%20500%20diverse%20product%20images%2C%20combined%20with%0ATotal%20Cost%20of%20Ownership%20modeling%2C%20we%20derive%20break-even%20thresholds%20for%0Aarchitecture%20selection.%20Results%20show%20supervised%20YOLO%20attains%2091.2%25%20accuracy%0Aversus%2068.5%25%20for%20Gemini%20and%2071.3%25%20for%20GPT-4%20on%20standard%20categories%3B%20the%0Aannotation%20expense%20for%20a%20100-category%20system%20is%20%2410%2C800%2C%20and%20the%20accuracy%0Aadvantage%20only%20pays%20off%20beyond%2055%20million%20inferences%20%28151%2C000%20images/day%20for%0Aone%20year%29.%20On%20diverse%20product%20categories%20Gemini%20achieves%2052.3%25%20and%20GPT-4%2055.1%25%2C%0Awhile%20supervised%20YOLO%20cannot%20detect%20untrained%20classes.%0ACost-per-correct-detection%20favors%20Gemini%20%28%240.00050%29%20and%20GPT-4%20%28%240.00067%29%20over%0AYOLO%20%28%240.143%29%20at%20100%2C000%20inferences.%20We%20provide%20decision%20frameworks%20showing%0Athat%20optimal%20architecture%20choice%20depends%20on%20inference%20volume%2C%20category%0Astability%2C%20budget%2C%20and%20accuracy%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11302v2&entry.124074799=Read"},
{"title": "Atlas-based Manifold Representations for Interpretable Riemannian\n  Machine Learning", "author": "Ryan A. Robinett and Sophia A. Madejski and Kyle Ruark and Samantha J. Riesenfeld and Lorenzo Orecchia", "abstract": "  Despite the popularity of the manifold hypothesis, current manifold-learning\nmethods do not support machine learning directly on the latent $d$-dimensional\ndata manifold, as they primarily aim to perform dimensionality reduction into\n$\\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$\napproaches $d$.\n  On the other hand, methods that directly learn the latent manifold as a\ndifferentiable atlas have been relatively underexplored.\n  In this paper, we aim to give a proof of concept of the effectiveness and\npotential of atlas-based methods. To this end, we implement a generic data\nstructure to maintain a differentiable atlas that enables Riemannian\noptimization over the manifold. We complement this with an unsupervised\nheuristic that learns a differentiable atlas from point cloud data. We\nexperimentally demonstrate that this approach has advantages in terms of\nefficiency and accuracy in selected settings. Moreover, in a supervised\nclassification task over the Klein bottle and in RNA velocity analysis of\nhematopoietic data, we showcase the improved interpretability and robustness of\nour approach.\n", "link": "http://arxiv.org/abs/2510.17772v1", "date": "2025-10-20", "relevancy": 2.4696, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atlas-based%20Manifold%20Representations%20for%20Interpretable%20Riemannian%0A%20%20Machine%20Learning&body=Title%3A%20Atlas-based%20Manifold%20Representations%20for%20Interpretable%20Riemannian%0A%20%20Machine%20Learning%0AAuthor%3A%20Ryan%20A.%20Robinett%20and%20Sophia%20A.%20Madejski%20and%20Kyle%20Ruark%20and%20Samantha%20J.%20Riesenfeld%20and%20Lorenzo%20Orecchia%0AAbstract%3A%20%20%20Despite%20the%20popularity%20of%20the%20manifold%20hypothesis%2C%20current%20manifold-learning%0Amethods%20do%20not%20support%20machine%20learning%20directly%20on%20the%20latent%20%24d%24-dimensional%0Adata%20manifold%2C%20as%20they%20primarily%20aim%20to%20perform%20dimensionality%20reduction%20into%0A%24%5Cmathbb%7BR%7D%5ED%24%2C%20losing%20key%20manifold%20features%20when%20the%20embedding%20dimension%20%24D%24%0Aapproaches%20%24d%24.%0A%20%20On%20the%20other%20hand%2C%20methods%20that%20directly%20learn%20the%20latent%20manifold%20as%20a%0Adifferentiable%20atlas%20have%20been%20relatively%20underexplored.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20give%20a%20proof%20of%20concept%20of%20the%20effectiveness%20and%0Apotential%20of%20atlas-based%20methods.%20To%20this%20end%2C%20we%20implement%20a%20generic%20data%0Astructure%20to%20maintain%20a%20differentiable%20atlas%20that%20enables%20Riemannian%0Aoptimization%20over%20the%20manifold.%20We%20complement%20this%20with%20an%20unsupervised%0Aheuristic%20that%20learns%20a%20differentiable%20atlas%20from%20point%20cloud%20data.%20We%0Aexperimentally%20demonstrate%20that%20this%20approach%20has%20advantages%20in%20terms%20of%0Aefficiency%20and%20accuracy%20in%20selected%20settings.%20Moreover%2C%20in%20a%20supervised%0Aclassification%20task%20over%20the%20Klein%20bottle%20and%20in%20RNA%20velocity%20analysis%20of%0Ahematopoietic%20data%2C%20we%20showcase%20the%20improved%20interpretability%20and%20robustness%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtlas-based%2520Manifold%2520Representations%2520for%2520Interpretable%2520Riemannian%250A%2520%2520Machine%2520Learning%26entry.906535625%3DRyan%2520A.%2520Robinett%2520and%2520Sophia%2520A.%2520Madejski%2520and%2520Kyle%2520Ruark%2520and%2520Samantha%2520J.%2520Riesenfeld%2520and%2520Lorenzo%2520Orecchia%26entry.1292438233%3D%2520%2520Despite%2520the%2520popularity%2520of%2520the%2520manifold%2520hypothesis%252C%2520current%2520manifold-learning%250Amethods%2520do%2520not%2520support%2520machine%2520learning%2520directly%2520on%2520the%2520latent%2520%2524d%2524-dimensional%250Adata%2520manifold%252C%2520as%2520they%2520primarily%2520aim%2520to%2520perform%2520dimensionality%2520reduction%2520into%250A%2524%255Cmathbb%257BR%257D%255ED%2524%252C%2520losing%2520key%2520manifold%2520features%2520when%2520the%2520embedding%2520dimension%2520%2524D%2524%250Aapproaches%2520%2524d%2524.%250A%2520%2520On%2520the%2520other%2520hand%252C%2520methods%2520that%2520directly%2520learn%2520the%2520latent%2520manifold%2520as%2520a%250Adifferentiable%2520atlas%2520have%2520been%2520relatively%2520underexplored.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520give%2520a%2520proof%2520of%2520concept%2520of%2520the%2520effectiveness%2520and%250Apotential%2520of%2520atlas-based%2520methods.%2520To%2520this%2520end%252C%2520we%2520implement%2520a%2520generic%2520data%250Astructure%2520to%2520maintain%2520a%2520differentiable%2520atlas%2520that%2520enables%2520Riemannian%250Aoptimization%2520over%2520the%2520manifold.%2520We%2520complement%2520this%2520with%2520an%2520unsupervised%250Aheuristic%2520that%2520learns%2520a%2520differentiable%2520atlas%2520from%2520point%2520cloud%2520data.%2520We%250Aexperimentally%2520demonstrate%2520that%2520this%2520approach%2520has%2520advantages%2520in%2520terms%2520of%250Aefficiency%2520and%2520accuracy%2520in%2520selected%2520settings.%2520Moreover%252C%2520in%2520a%2520supervised%250Aclassification%2520task%2520over%2520the%2520Klein%2520bottle%2520and%2520in%2520RNA%2520velocity%2520analysis%2520of%250Ahematopoietic%2520data%252C%2520we%2520showcase%2520the%2520improved%2520interpretability%2520and%2520robustness%2520of%250Aour%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atlas-based%20Manifold%20Representations%20for%20Interpretable%20Riemannian%0A%20%20Machine%20Learning&entry.906535625=Ryan%20A.%20Robinett%20and%20Sophia%20A.%20Madejski%20and%20Kyle%20Ruark%20and%20Samantha%20J.%20Riesenfeld%20and%20Lorenzo%20Orecchia&entry.1292438233=%20%20Despite%20the%20popularity%20of%20the%20manifold%20hypothesis%2C%20current%20manifold-learning%0Amethods%20do%20not%20support%20machine%20learning%20directly%20on%20the%20latent%20%24d%24-dimensional%0Adata%20manifold%2C%20as%20they%20primarily%20aim%20to%20perform%20dimensionality%20reduction%20into%0A%24%5Cmathbb%7BR%7D%5ED%24%2C%20losing%20key%20manifold%20features%20when%20the%20embedding%20dimension%20%24D%24%0Aapproaches%20%24d%24.%0A%20%20On%20the%20other%20hand%2C%20methods%20that%20directly%20learn%20the%20latent%20manifold%20as%20a%0Adifferentiable%20atlas%20have%20been%20relatively%20underexplored.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20give%20a%20proof%20of%20concept%20of%20the%20effectiveness%20and%0Apotential%20of%20atlas-based%20methods.%20To%20this%20end%2C%20we%20implement%20a%20generic%20data%0Astructure%20to%20maintain%20a%20differentiable%20atlas%20that%20enables%20Riemannian%0Aoptimization%20over%20the%20manifold.%20We%20complement%20this%20with%20an%20unsupervised%0Aheuristic%20that%20learns%20a%20differentiable%20atlas%20from%20point%20cloud%20data.%20We%0Aexperimentally%20demonstrate%20that%20this%20approach%20has%20advantages%20in%20terms%20of%0Aefficiency%20and%20accuracy%20in%20selected%20settings.%20Moreover%2C%20in%20a%20supervised%0Aclassification%20task%20over%20the%20Klein%20bottle%20and%20in%20RNA%20velocity%20analysis%20of%0Ahematopoietic%20data%2C%20we%20showcase%20the%20improved%20interpretability%20and%20robustness%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17772v1&entry.124074799=Read"},
{"title": "Signature Forgery Detection: Improving Cross-Dataset Generalization", "author": "Matheus Ramos Parracho", "abstract": "  Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.\n", "link": "http://arxiv.org/abs/2510.17724v1", "date": "2025-10-20", "relevancy": 2.436, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4931}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4893}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signature%20Forgery%20Detection%3A%20Improving%20Cross-Dataset%20Generalization&body=Title%3A%20Signature%20Forgery%20Detection%3A%20Improving%20Cross-Dataset%20Generalization%0AAuthor%3A%20Matheus%20Ramos%20Parracho%0AAbstract%3A%20%20%20Automated%20signature%20verification%20is%20a%20critical%20biometric%20technique%20used%20in%0Abanking%2C%20identity%20authentication%2C%20and%20legal%20documentation.%20Despite%20the%20notable%0Aprogress%20achieved%20by%20deep%20learning%20methods%2C%20most%20approaches%20in%20offline%0Asignature%20verification%20still%20struggle%20to%20generalize%20across%20datasets%2C%20as%0Avariations%20in%20handwriting%20styles%20and%20acquisition%20protocols%20often%20degrade%0Aperformance.%20This%20study%20investigates%20feature%20learning%20strategies%20for%20signature%0Aforgery%20detection%2C%20focusing%20on%20improving%20cross-dataset%20generalization%20--%20that%0Ais%2C%20model%20robustness%20when%20trained%20on%20one%20dataset%20and%20tested%20on%20another.%20Using%0Athree%20public%20benchmarks%20--%20CEDAR%2C%20ICDAR%2C%20and%20GPDS%20Synthetic%20--%20two%20experimental%0Apipelines%20were%20developed%3A%20one%20based%20on%20raw%20signature%20images%20and%20another%0Aemploying%20a%20preprocessing%20method%20referred%20to%20as%20shell%20preprocessing.%20Several%0Abehavioral%20patterns%20were%20identified%20and%20analyzed%3B%20however%2C%20no%20definitive%0Asuperiority%20between%20the%20two%20approaches%20was%20established.%20The%20results%20show%20that%0Athe%20raw-image%20model%20achieved%20higher%20performance%20across%20benchmarks%2C%20while%20the%0Ashell-based%20model%20demonstrated%20promising%20potential%20for%20future%20refinement%20toward%0Arobust%2C%20cross-domain%20signature%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignature%2520Forgery%2520Detection%253A%2520Improving%2520Cross-Dataset%2520Generalization%26entry.906535625%3DMatheus%2520Ramos%2520Parracho%26entry.1292438233%3D%2520%2520Automated%2520signature%2520verification%2520is%2520a%2520critical%2520biometric%2520technique%2520used%2520in%250Abanking%252C%2520identity%2520authentication%252C%2520and%2520legal%2520documentation.%2520Despite%2520the%2520notable%250Aprogress%2520achieved%2520by%2520deep%2520learning%2520methods%252C%2520most%2520approaches%2520in%2520offline%250Asignature%2520verification%2520still%2520struggle%2520to%2520generalize%2520across%2520datasets%252C%2520as%250Avariations%2520in%2520handwriting%2520styles%2520and%2520acquisition%2520protocols%2520often%2520degrade%250Aperformance.%2520This%2520study%2520investigates%2520feature%2520learning%2520strategies%2520for%2520signature%250Aforgery%2520detection%252C%2520focusing%2520on%2520improving%2520cross-dataset%2520generalization%2520--%2520that%250Ais%252C%2520model%2520robustness%2520when%2520trained%2520on%2520one%2520dataset%2520and%2520tested%2520on%2520another.%2520Using%250Athree%2520public%2520benchmarks%2520--%2520CEDAR%252C%2520ICDAR%252C%2520and%2520GPDS%2520Synthetic%2520--%2520two%2520experimental%250Apipelines%2520were%2520developed%253A%2520one%2520based%2520on%2520raw%2520signature%2520images%2520and%2520another%250Aemploying%2520a%2520preprocessing%2520method%2520referred%2520to%2520as%2520shell%2520preprocessing.%2520Several%250Abehavioral%2520patterns%2520were%2520identified%2520and%2520analyzed%253B%2520however%252C%2520no%2520definitive%250Asuperiority%2520between%2520the%2520two%2520approaches%2520was%2520established.%2520The%2520results%2520show%2520that%250Athe%2520raw-image%2520model%2520achieved%2520higher%2520performance%2520across%2520benchmarks%252C%2520while%2520the%250Ashell-based%2520model%2520demonstrated%2520promising%2520potential%2520for%2520future%2520refinement%2520toward%250Arobust%252C%2520cross-domain%2520signature%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signature%20Forgery%20Detection%3A%20Improving%20Cross-Dataset%20Generalization&entry.906535625=Matheus%20Ramos%20Parracho&entry.1292438233=%20%20Automated%20signature%20verification%20is%20a%20critical%20biometric%20technique%20used%20in%0Abanking%2C%20identity%20authentication%2C%20and%20legal%20documentation.%20Despite%20the%20notable%0Aprogress%20achieved%20by%20deep%20learning%20methods%2C%20most%20approaches%20in%20offline%0Asignature%20verification%20still%20struggle%20to%20generalize%20across%20datasets%2C%20as%0Avariations%20in%20handwriting%20styles%20and%20acquisition%20protocols%20often%20degrade%0Aperformance.%20This%20study%20investigates%20feature%20learning%20strategies%20for%20signature%0Aforgery%20detection%2C%20focusing%20on%20improving%20cross-dataset%20generalization%20--%20that%0Ais%2C%20model%20robustness%20when%20trained%20on%20one%20dataset%20and%20tested%20on%20another.%20Using%0Athree%20public%20benchmarks%20--%20CEDAR%2C%20ICDAR%2C%20and%20GPDS%20Synthetic%20--%20two%20experimental%0Apipelines%20were%20developed%3A%20one%20based%20on%20raw%20signature%20images%20and%20another%0Aemploying%20a%20preprocessing%20method%20referred%20to%20as%20shell%20preprocessing.%20Several%0Abehavioral%20patterns%20were%20identified%20and%20analyzed%3B%20however%2C%20no%20definitive%0Asuperiority%20between%20the%20two%20approaches%20was%20established.%20The%20results%20show%20that%0Athe%20raw-image%20model%20achieved%20higher%20performance%20across%20benchmarks%2C%20while%20the%0Ashell-based%20model%20demonstrated%20promising%20potential%20for%20future%20refinement%20toward%0Arobust%2C%20cross-domain%20signature%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17724v1&entry.124074799=Read"},
{"title": "Plasma Shape Control via Zero-shot Generative Reinforcement Learning", "author": "Niannian Wu and Rongpeng Li and Zongyu Yang and Yong Xiao and Ning Wei and Yihang Chen and Bo Li and Zhifeng Zhao and Wulyu Zhong", "abstract": "  Traditional PID controllers have limited adaptability for plasma shape\ncontrol, and task-specific reinforcement learning (RL) methods suffer from\nlimited generalization and the need for repetitive retraining. To overcome\nthese challenges, this paper proposes a novel framework for developing a\nversatile, zero-shot control policy from a large-scale offline dataset of\nhistorical PID-controlled discharges. Our approach synergistically combines\nGenerative Adversarial Imitation Learning (GAIL) with Hilbert space\nrepresentation learning to achieve dual objectives: mimicking the stable\noperational style of the PID data and constructing a geometrically structured\nlatent space for efficient, goal-directed control. The resulting foundation\npolicy can be deployed for diverse trajectory tracking tasks in a zero-shot\nmanner without any task-specific fine-tuning. Evaluations on the HL-3 tokamak\nsimulator demonstrate that the policy excels at precisely and stably tracking\nreference trajectories for key shape parameters across a range of plasma\nscenarios. This work presents a viable pathway toward developing highly\nflexible and data-efficient intelligent control systems for future fusion\nreactors.\n", "link": "http://arxiv.org/abs/2510.17531v1", "date": "2025-10-20", "relevancy": 2.4303, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5081}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4764}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plasma%20Shape%20Control%20via%20Zero-shot%20Generative%20Reinforcement%20Learning&body=Title%3A%20Plasma%20Shape%20Control%20via%20Zero-shot%20Generative%20Reinforcement%20Learning%0AAuthor%3A%20Niannian%20Wu%20and%20Rongpeng%20Li%20and%20Zongyu%20Yang%20and%20Yong%20Xiao%20and%20Ning%20Wei%20and%20Yihang%20Chen%20and%20Bo%20Li%20and%20Zhifeng%20Zhao%20and%20Wulyu%20Zhong%0AAbstract%3A%20%20%20Traditional%20PID%20controllers%20have%20limited%20adaptability%20for%20plasma%20shape%0Acontrol%2C%20and%20task-specific%20reinforcement%20learning%20%28RL%29%20methods%20suffer%20from%0Alimited%20generalization%20and%20the%20need%20for%20repetitive%20retraining.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20for%20developing%20a%0Aversatile%2C%20zero-shot%20control%20policy%20from%20a%20large-scale%20offline%20dataset%20of%0Ahistorical%20PID-controlled%20discharges.%20Our%20approach%20synergistically%20combines%0AGenerative%20Adversarial%20Imitation%20Learning%20%28GAIL%29%20with%20Hilbert%20space%0Arepresentation%20learning%20to%20achieve%20dual%20objectives%3A%20mimicking%20the%20stable%0Aoperational%20style%20of%20the%20PID%20data%20and%20constructing%20a%20geometrically%20structured%0Alatent%20space%20for%20efficient%2C%20goal-directed%20control.%20The%20resulting%20foundation%0Apolicy%20can%20be%20deployed%20for%20diverse%20trajectory%20tracking%20tasks%20in%20a%20zero-shot%0Amanner%20without%20any%20task-specific%20fine-tuning.%20Evaluations%20on%20the%20HL-3%20tokamak%0Asimulator%20demonstrate%20that%20the%20policy%20excels%20at%20precisely%20and%20stably%20tracking%0Areference%20trajectories%20for%20key%20shape%20parameters%20across%20a%20range%20of%20plasma%0Ascenarios.%20This%20work%20presents%20a%20viable%20pathway%20toward%20developing%20highly%0Aflexible%20and%20data-efficient%20intelligent%20control%20systems%20for%20future%20fusion%0Areactors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlasma%2520Shape%2520Control%2520via%2520Zero-shot%2520Generative%2520Reinforcement%2520Learning%26entry.906535625%3DNiannian%2520Wu%2520and%2520Rongpeng%2520Li%2520and%2520Zongyu%2520Yang%2520and%2520Yong%2520Xiao%2520and%2520Ning%2520Wei%2520and%2520Yihang%2520Chen%2520and%2520Bo%2520Li%2520and%2520Zhifeng%2520Zhao%2520and%2520Wulyu%2520Zhong%26entry.1292438233%3D%2520%2520Traditional%2520PID%2520controllers%2520have%2520limited%2520adaptability%2520for%2520plasma%2520shape%250Acontrol%252C%2520and%2520task-specific%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520suffer%2520from%250Alimited%2520generalization%2520and%2520the%2520need%2520for%2520repetitive%2520retraining.%2520To%2520overcome%250Athese%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%2520for%2520developing%2520a%250Aversatile%252C%2520zero-shot%2520control%2520policy%2520from%2520a%2520large-scale%2520offline%2520dataset%2520of%250Ahistorical%2520PID-controlled%2520discharges.%2520Our%2520approach%2520synergistically%2520combines%250AGenerative%2520Adversarial%2520Imitation%2520Learning%2520%2528GAIL%2529%2520with%2520Hilbert%2520space%250Arepresentation%2520learning%2520to%2520achieve%2520dual%2520objectives%253A%2520mimicking%2520the%2520stable%250Aoperational%2520style%2520of%2520the%2520PID%2520data%2520and%2520constructing%2520a%2520geometrically%2520structured%250Alatent%2520space%2520for%2520efficient%252C%2520goal-directed%2520control.%2520The%2520resulting%2520foundation%250Apolicy%2520can%2520be%2520deployed%2520for%2520diverse%2520trajectory%2520tracking%2520tasks%2520in%2520a%2520zero-shot%250Amanner%2520without%2520any%2520task-specific%2520fine-tuning.%2520Evaluations%2520on%2520the%2520HL-3%2520tokamak%250Asimulator%2520demonstrate%2520that%2520the%2520policy%2520excels%2520at%2520precisely%2520and%2520stably%2520tracking%250Areference%2520trajectories%2520for%2520key%2520shape%2520parameters%2520across%2520a%2520range%2520of%2520plasma%250Ascenarios.%2520This%2520work%2520presents%2520a%2520viable%2520pathway%2520toward%2520developing%2520highly%250Aflexible%2520and%2520data-efficient%2520intelligent%2520control%2520systems%2520for%2520future%2520fusion%250Areactors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plasma%20Shape%20Control%20via%20Zero-shot%20Generative%20Reinforcement%20Learning&entry.906535625=Niannian%20Wu%20and%20Rongpeng%20Li%20and%20Zongyu%20Yang%20and%20Yong%20Xiao%20and%20Ning%20Wei%20and%20Yihang%20Chen%20and%20Bo%20Li%20and%20Zhifeng%20Zhao%20and%20Wulyu%20Zhong&entry.1292438233=%20%20Traditional%20PID%20controllers%20have%20limited%20adaptability%20for%20plasma%20shape%0Acontrol%2C%20and%20task-specific%20reinforcement%20learning%20%28RL%29%20methods%20suffer%20from%0Alimited%20generalization%20and%20the%20need%20for%20repetitive%20retraining.%20To%20overcome%0Athese%20challenges%2C%20this%20paper%20proposes%20a%20novel%20framework%20for%20developing%20a%0Aversatile%2C%20zero-shot%20control%20policy%20from%20a%20large-scale%20offline%20dataset%20of%0Ahistorical%20PID-controlled%20discharges.%20Our%20approach%20synergistically%20combines%0AGenerative%20Adversarial%20Imitation%20Learning%20%28GAIL%29%20with%20Hilbert%20space%0Arepresentation%20learning%20to%20achieve%20dual%20objectives%3A%20mimicking%20the%20stable%0Aoperational%20style%20of%20the%20PID%20data%20and%20constructing%20a%20geometrically%20structured%0Alatent%20space%20for%20efficient%2C%20goal-directed%20control.%20The%20resulting%20foundation%0Apolicy%20can%20be%20deployed%20for%20diverse%20trajectory%20tracking%20tasks%20in%20a%20zero-shot%0Amanner%20without%20any%20task-specific%20fine-tuning.%20Evaluations%20on%20the%20HL-3%20tokamak%0Asimulator%20demonstrate%20that%20the%20policy%20excels%20at%20precisely%20and%20stably%20tracking%0Areference%20trajectories%20for%20key%20shape%20parameters%20across%20a%20range%20of%20plasma%0Ascenarios.%20This%20work%20presents%20a%20viable%20pathway%20toward%20developing%20highly%0Aflexible%20and%20data-efficient%20intelligent%20control%20systems%20for%20future%20fusion%0Areactors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17531v1&entry.124074799=Read"},
{"title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning", "author": "Maggie Huan and Yuetai Li and Tuney Zheng and Xiaoyu Xu and Seungone Kim and Minxin Du and Radha Poovendran and Graham Neubig and Xiang Yue", "abstract": "  Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.\n", "link": "http://arxiv.org/abs/2507.00432v2", "date": "2025-10-20", "relevancy": 2.4086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Math%20Reasoning%20Improve%20General%20LLM%20Capabilities%3F%20Understanding%0A%20%20Transferability%20of%20LLM%20Reasoning&body=Title%3A%20Does%20Math%20Reasoning%20Improve%20General%20LLM%20Capabilities%3F%20Understanding%0A%20%20Transferability%20of%20LLM%20Reasoning%0AAuthor%3A%20Maggie%20Huan%20and%20Yuetai%20Li%20and%20Tuney%20Zheng%20and%20Xiaoyu%20Xu%20and%20Seungone%20Kim%20and%20Minxin%20Du%20and%20Radha%20Poovendran%20and%20Graham%20Neubig%20and%20Xiang%20Yue%0AAbstract%3A%20%20%20Math%20reasoning%20has%20become%20the%20poster%20child%20of%20progress%20in%20large%20language%0Amodels%20%28LLMs%29%2C%20with%20new%20models%20rapidly%20surpassing%20human-level%20performance%20on%0Abenchmarks%20like%20MATH%20and%20AIME.%20But%20as%20math%20leaderboards%20improve%20week%20by%20week%2C%0Ait%20is%20worth%20asking%3A%20do%20these%20gains%20reflect%20broader%20problem-solving%20ability%20or%0Ajust%20narrow%20overfitting%3F%20To%20answer%20this%20question%2C%20we%20evaluate%20over%2020%0Aopen-weight%20reasoning-tuned%20models%20across%20a%20broad%20suite%20of%20tasks%2C%20including%0Amath%2C%20scientific%20QA%2C%20agent%20planning%2C%20coding%2C%20and%20standard%0Ainstruction-following.%20We%20surprisingly%20find%20that%20most%20models%20that%20succeed%20in%0Amath%20fail%20to%20transfer%20their%20gains%20to%20other%20domains.%20To%20rigorously%20study%20this%0Aphenomenon%2C%20we%20conduct%20controlled%20experiments%20on%20Qwen3-14B%20models%20using%0Amath-only%20data%20but%20different%20tuning%20methods.%20We%20find%20that%20reinforcement%0Alearning%20%28RL%29-tuned%20models%20generalize%20well%20across%20domains%2C%20while%20supervised%0Afine-tuning%20%28SFT%29-tuned%20models%20often%20forget%20general%20capabilities.%20Latent-space%0Arepresentation%20and%20token-space%20distribution%20shift%20analyses%20reveal%20that%20SFT%0Ainduces%20substantial%20representation%20and%20output%20drift%2C%20while%20RL%20preserves%0Ageneral-domain%20structure.%20Our%20results%20suggest%20a%20need%20to%20rethink%20standard%0Apost-training%20recipes%2C%20particularly%20the%20reliance%20on%20SFT-distilled%20data%20for%0Aadvancing%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.00432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Math%2520Reasoning%2520Improve%2520General%2520LLM%2520Capabilities%253F%2520Understanding%250A%2520%2520Transferability%2520of%2520LLM%2520Reasoning%26entry.906535625%3DMaggie%2520Huan%2520and%2520Yuetai%2520Li%2520and%2520Tuney%2520Zheng%2520and%2520Xiaoyu%2520Xu%2520and%2520Seungone%2520Kim%2520and%2520Minxin%2520Du%2520and%2520Radha%2520Poovendran%2520and%2520Graham%2520Neubig%2520and%2520Xiang%2520Yue%26entry.1292438233%3D%2520%2520Math%2520reasoning%2520has%2520become%2520the%2520poster%2520child%2520of%2520progress%2520in%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520with%2520new%2520models%2520rapidly%2520surpassing%2520human-level%2520performance%2520on%250Abenchmarks%2520like%2520MATH%2520and%2520AIME.%2520But%2520as%2520math%2520leaderboards%2520improve%2520week%2520by%2520week%252C%250Ait%2520is%2520worth%2520asking%253A%2520do%2520these%2520gains%2520reflect%2520broader%2520problem-solving%2520ability%2520or%250Ajust%2520narrow%2520overfitting%253F%2520To%2520answer%2520this%2520question%252C%2520we%2520evaluate%2520over%252020%250Aopen-weight%2520reasoning-tuned%2520models%2520across%2520a%2520broad%2520suite%2520of%2520tasks%252C%2520including%250Amath%252C%2520scientific%2520QA%252C%2520agent%2520planning%252C%2520coding%252C%2520and%2520standard%250Ainstruction-following.%2520We%2520surprisingly%2520find%2520that%2520most%2520models%2520that%2520succeed%2520in%250Amath%2520fail%2520to%2520transfer%2520their%2520gains%2520to%2520other%2520domains.%2520To%2520rigorously%2520study%2520this%250Aphenomenon%252C%2520we%2520conduct%2520controlled%2520experiments%2520on%2520Qwen3-14B%2520models%2520using%250Amath-only%2520data%2520but%2520different%2520tuning%2520methods.%2520We%2520find%2520that%2520reinforcement%250Alearning%2520%2528RL%2529-tuned%2520models%2520generalize%2520well%2520across%2520domains%252C%2520while%2520supervised%250Afine-tuning%2520%2528SFT%2529-tuned%2520models%2520often%2520forget%2520general%2520capabilities.%2520Latent-space%250Arepresentation%2520and%2520token-space%2520distribution%2520shift%2520analyses%2520reveal%2520that%2520SFT%250Ainduces%2520substantial%2520representation%2520and%2520output%2520drift%252C%2520while%2520RL%2520preserves%250Ageneral-domain%2520structure.%2520Our%2520results%2520suggest%2520a%2520need%2520to%2520rethink%2520standard%250Apost-training%2520recipes%252C%2520particularly%2520the%2520reliance%2520on%2520SFT-distilled%2520data%2520for%250Aadvancing%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.00432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Math%20Reasoning%20Improve%20General%20LLM%20Capabilities%3F%20Understanding%0A%20%20Transferability%20of%20LLM%20Reasoning&entry.906535625=Maggie%20Huan%20and%20Yuetai%20Li%20and%20Tuney%20Zheng%20and%20Xiaoyu%20Xu%20and%20Seungone%20Kim%20and%20Minxin%20Du%20and%20Radha%20Poovendran%20and%20Graham%20Neubig%20and%20Xiang%20Yue&entry.1292438233=%20%20Math%20reasoning%20has%20become%20the%20poster%20child%20of%20progress%20in%20large%20language%0Amodels%20%28LLMs%29%2C%20with%20new%20models%20rapidly%20surpassing%20human-level%20performance%20on%0Abenchmarks%20like%20MATH%20and%20AIME.%20But%20as%20math%20leaderboards%20improve%20week%20by%20week%2C%0Ait%20is%20worth%20asking%3A%20do%20these%20gains%20reflect%20broader%20problem-solving%20ability%20or%0Ajust%20narrow%20overfitting%3F%20To%20answer%20this%20question%2C%20we%20evaluate%20over%2020%0Aopen-weight%20reasoning-tuned%20models%20across%20a%20broad%20suite%20of%20tasks%2C%20including%0Amath%2C%20scientific%20QA%2C%20agent%20planning%2C%20coding%2C%20and%20standard%0Ainstruction-following.%20We%20surprisingly%20find%20that%20most%20models%20that%20succeed%20in%0Amath%20fail%20to%20transfer%20their%20gains%20to%20other%20domains.%20To%20rigorously%20study%20this%0Aphenomenon%2C%20we%20conduct%20controlled%20experiments%20on%20Qwen3-14B%20models%20using%0Amath-only%20data%20but%20different%20tuning%20methods.%20We%20find%20that%20reinforcement%0Alearning%20%28RL%29-tuned%20models%20generalize%20well%20across%20domains%2C%20while%20supervised%0Afine-tuning%20%28SFT%29-tuned%20models%20often%20forget%20general%20capabilities.%20Latent-space%0Arepresentation%20and%20token-space%20distribution%20shift%20analyses%20reveal%20that%20SFT%0Ainduces%20substantial%20representation%20and%20output%20drift%2C%20while%20RL%20preserves%0Ageneral-domain%20structure.%20Our%20results%20suggest%20a%20need%20to%20rethink%20standard%0Apost-training%20recipes%2C%20particularly%20the%20reliance%20on%20SFT-distilled%20data%20for%0Aadvancing%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.00432v2&entry.124074799=Read"},
{"title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity\n  Recognition", "author": "Nanda Kumar Rengarajan and Jun Yan and Chun Wang", "abstract": "  Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.\n", "link": "http://arxiv.org/abs/2510.17720v1", "date": "2025-10-20", "relevancy": 2.3619, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PANER%3A%20A%20Paraphrase-Augmented%20Framework%20for%20Low-Resource%20Named%20Entity%0A%20%20Recognition&body=Title%3A%20PANER%3A%20A%20Paraphrase-Augmented%20Framework%20for%20Low-Resource%20Named%20Entity%0A%20%20Recognition%0AAuthor%3A%20Nanda%20Kumar%20Rengarajan%20and%20Jun%20Yan%20and%20Chun%20Wang%0AAbstract%3A%20%20%20Named%20Entity%20Recognition%20%28NER%29%20is%20a%20critical%20task%20that%20requires%20substantial%0Aannotated%20data%2C%20making%20it%20challenging%20in%20low-resource%20scenarios%20where%20label%0Aacquisition%20is%20expensive.%20While%20zero-shot%20and%20instruction-tuned%20approaches%20have%0Amade%20progress%2C%20they%20often%20fail%20to%20generalize%20to%20domain-specific%20entities%20and%20do%0Anot%20effectively%20utilize%20limited%20available%20data.%20We%20present%20a%20lightweight%0Afew-shot%20NER%20framework%20that%20addresses%20these%20challenges%20through%20two%20key%0Ainnovations%3A%20%281%29%20a%20new%20instruction%20tuning%20template%20with%20a%20simplified%20output%0Aformat%20that%20combines%20principles%20from%20prior%20IT%20approaches%20to%20leverage%20the%20large%0Acontext%20window%20of%20recent%20state-of-the-art%20LLMs%3B%20%282%29%20introducing%20a%20strategic%0Adata%20augmentation%20technique%20that%20preserves%20entity%20information%20while%0Aparaphrasing%20the%20surrounding%20context%2C%20thereby%20expanding%20our%20training%20data%0Awithout%20compromising%20semantic%20relationships.%20Experiments%20on%20benchmark%20datasets%0Ashow%20that%20our%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%20models%0Aon%20few-shot%20and%20zero-shot%20tasks%2C%20with%20our%20few-shot%20approach%20attaining%20an%0Aaverage%20F1%20score%20of%2080.1%20on%20the%20CrossNER%20datasets.%20Models%20trained%20with%20our%0Aparaphrasing%20approach%20show%20consistent%20improvements%20in%20F1%20scores%20of%20up%20to%2017%0Apoints%20over%20baseline%20versions%2C%20offering%20a%20promising%20solution%20for%20groups%20with%0Alimited%20NER%20training%20data%20and%20compute%20power.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPANER%253A%2520A%2520Paraphrase-Augmented%2520Framework%2520for%2520Low-Resource%2520Named%2520Entity%250A%2520%2520Recognition%26entry.906535625%3DNanda%2520Kumar%2520Rengarajan%2520and%2520Jun%2520Yan%2520and%2520Chun%2520Wang%26entry.1292438233%3D%2520%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520is%2520a%2520critical%2520task%2520that%2520requires%2520substantial%250Aannotated%2520data%252C%2520making%2520it%2520challenging%2520in%2520low-resource%2520scenarios%2520where%2520label%250Aacquisition%2520is%2520expensive.%2520While%2520zero-shot%2520and%2520instruction-tuned%2520approaches%2520have%250Amade%2520progress%252C%2520they%2520often%2520fail%2520to%2520generalize%2520to%2520domain-specific%2520entities%2520and%2520do%250Anot%2520effectively%2520utilize%2520limited%2520available%2520data.%2520We%2520present%2520a%2520lightweight%250Afew-shot%2520NER%2520framework%2520that%2520addresses%2520these%2520challenges%2520through%2520two%2520key%250Ainnovations%253A%2520%25281%2529%2520a%2520new%2520instruction%2520tuning%2520template%2520with%2520a%2520simplified%2520output%250Aformat%2520that%2520combines%2520principles%2520from%2520prior%2520IT%2520approaches%2520to%2520leverage%2520the%2520large%250Acontext%2520window%2520of%2520recent%2520state-of-the-art%2520LLMs%253B%2520%25282%2529%2520introducing%2520a%2520strategic%250Adata%2520augmentation%2520technique%2520that%2520preserves%2520entity%2520information%2520while%250Aparaphrasing%2520the%2520surrounding%2520context%252C%2520thereby%2520expanding%2520our%2520training%2520data%250Awithout%2520compromising%2520semantic%2520relationships.%2520Experiments%2520on%2520benchmark%2520datasets%250Ashow%2520that%2520our%2520method%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520models%250Aon%2520few-shot%2520and%2520zero-shot%2520tasks%252C%2520with%2520our%2520few-shot%2520approach%2520attaining%2520an%250Aaverage%2520F1%2520score%2520of%252080.1%2520on%2520the%2520CrossNER%2520datasets.%2520Models%2520trained%2520with%2520our%250Aparaphrasing%2520approach%2520show%2520consistent%2520improvements%2520in%2520F1%2520scores%2520of%2520up%2520to%252017%250Apoints%2520over%2520baseline%2520versions%252C%2520offering%2520a%2520promising%2520solution%2520for%2520groups%2520with%250Alimited%2520NER%2520training%2520data%2520and%2520compute%2520power.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PANER%3A%20A%20Paraphrase-Augmented%20Framework%20for%20Low-Resource%20Named%20Entity%0A%20%20Recognition&entry.906535625=Nanda%20Kumar%20Rengarajan%20and%20Jun%20Yan%20and%20Chun%20Wang&entry.1292438233=%20%20Named%20Entity%20Recognition%20%28NER%29%20is%20a%20critical%20task%20that%20requires%20substantial%0Aannotated%20data%2C%20making%20it%20challenging%20in%20low-resource%20scenarios%20where%20label%0Aacquisition%20is%20expensive.%20While%20zero-shot%20and%20instruction-tuned%20approaches%20have%0Amade%20progress%2C%20they%20often%20fail%20to%20generalize%20to%20domain-specific%20entities%20and%20do%0Anot%20effectively%20utilize%20limited%20available%20data.%20We%20present%20a%20lightweight%0Afew-shot%20NER%20framework%20that%20addresses%20these%20challenges%20through%20two%20key%0Ainnovations%3A%20%281%29%20a%20new%20instruction%20tuning%20template%20with%20a%20simplified%20output%0Aformat%20that%20combines%20principles%20from%20prior%20IT%20approaches%20to%20leverage%20the%20large%0Acontext%20window%20of%20recent%20state-of-the-art%20LLMs%3B%20%282%29%20introducing%20a%20strategic%0Adata%20augmentation%20technique%20that%20preserves%20entity%20information%20while%0Aparaphrasing%20the%20surrounding%20context%2C%20thereby%20expanding%20our%20training%20data%0Awithout%20compromising%20semantic%20relationships.%20Experiments%20on%20benchmark%20datasets%0Ashow%20that%20our%20method%20achieves%20performance%20comparable%20to%20state-of-the-art%20models%0Aon%20few-shot%20and%20zero-shot%20tasks%2C%20with%20our%20few-shot%20approach%20attaining%20an%0Aaverage%20F1%20score%20of%2080.1%20on%20the%20CrossNER%20datasets.%20Models%20trained%20with%20our%0Aparaphrasing%20approach%20show%20consistent%20improvements%20in%20F1%20scores%20of%20up%20to%2017%0Apoints%20over%20baseline%20versions%2C%20offering%20a%20promising%20solution%20for%20groups%20with%0Alimited%20NER%20training%20data%20and%20compute%20power.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17720v1&entry.124074799=Read"},
{"title": "Creative synthesis of kinematic mechanisms", "author": "Jiong Lin and Jialong Ning and Judah Goldfeder and Hod Lipson", "abstract": "  In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework.\n", "link": "http://arxiv.org/abs/2510.03308v2", "date": "2025-10-20", "relevancy": 2.3419, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6032}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5888}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creative%20synthesis%20of%20kinematic%20mechanisms&body=Title%3A%20Creative%20synthesis%20of%20kinematic%20mechanisms%0AAuthor%3A%20Jiong%20Lin%20and%20Jialong%20Ning%20and%20Judah%20Goldfeder%20and%20Hod%20Lipson%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20formulate%20the%20problem%20of%20kinematic%20synthesis%20for%20planar%0Alinkages%20as%20a%20cross-domain%20image%20generation%20task.%20We%20develop%20a%20planar%20linkages%0Adataset%20using%20RGB%20image%20representations%2C%20covering%20a%20range%20of%20mechanisms%3A%20from%0Asimple%20types%20such%20as%20crank-rocker%20and%20crank-slider%20to%20more%20complex%20eight-bar%0Alinkages%20like%20Jansen%27s%20mechanism.%20A%20shared-latent%20variational%20autoencoder%20%28VAE%29%0Ais%20employed%20to%20explore%20the%20potential%20of%20image%20generative%20models%20for%0Asynthesizing%20unseen%20motion%20curves%20and%20simulating%20novel%20kinematics.%20By%20encoding%0Athe%20drawing%20speed%20of%20trajectory%20points%20as%20color%20gradients%2C%20the%20same%0Aarchitecture%20also%20supports%20kinematic%20synthesis%20conditioned%20on%20both%20trajectory%0Ashape%20and%20velocity%20profiles.%20We%20validate%20our%20method%20on%20three%20datasets%20of%0Aincreasing%20complexity%3A%20a%20standard%20four-bar%20linkage%20set%2C%20a%20mixed%20set%20of%20four-bar%0Aand%20crank-slider%20mechanisms%2C%20and%20a%20complex%20set%20including%20multi-loop%20mechanisms.%0APreliminary%20results%20demonstrate%20the%20effectiveness%20of%20image-based%0Arepresentations%20for%20generative%20mechanical%20design%2C%20showing%20that%20mechanisms%20with%0Arevolute%20and%20prismatic%20joints%2C%20and%20potentially%20cams%20and%20gears%2C%20can%20be%0Arepresented%20and%20synthesized%20within%20a%20unified%20image%20generation%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.03308v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreative%2520synthesis%2520of%2520kinematic%2520mechanisms%26entry.906535625%3DJiong%2520Lin%2520and%2520Jialong%2520Ning%2520and%2520Judah%2520Goldfeder%2520and%2520Hod%2520Lipson%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520formulate%2520the%2520problem%2520of%2520kinematic%2520synthesis%2520for%2520planar%250Alinkages%2520as%2520a%2520cross-domain%2520image%2520generation%2520task.%2520We%2520develop%2520a%2520planar%2520linkages%250Adataset%2520using%2520RGB%2520image%2520representations%252C%2520covering%2520a%2520range%2520of%2520mechanisms%253A%2520from%250Asimple%2520types%2520such%2520as%2520crank-rocker%2520and%2520crank-slider%2520to%2520more%2520complex%2520eight-bar%250Alinkages%2520like%2520Jansen%2527s%2520mechanism.%2520A%2520shared-latent%2520variational%2520autoencoder%2520%2528VAE%2529%250Ais%2520employed%2520to%2520explore%2520the%2520potential%2520of%2520image%2520generative%2520models%2520for%250Asynthesizing%2520unseen%2520motion%2520curves%2520and%2520simulating%2520novel%2520kinematics.%2520By%2520encoding%250Athe%2520drawing%2520speed%2520of%2520trajectory%2520points%2520as%2520color%2520gradients%252C%2520the%2520same%250Aarchitecture%2520also%2520supports%2520kinematic%2520synthesis%2520conditioned%2520on%2520both%2520trajectory%250Ashape%2520and%2520velocity%2520profiles.%2520We%2520validate%2520our%2520method%2520on%2520three%2520datasets%2520of%250Aincreasing%2520complexity%253A%2520a%2520standard%2520four-bar%2520linkage%2520set%252C%2520a%2520mixed%2520set%2520of%2520four-bar%250Aand%2520crank-slider%2520mechanisms%252C%2520and%2520a%2520complex%2520set%2520including%2520multi-loop%2520mechanisms.%250APreliminary%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520image-based%250Arepresentations%2520for%2520generative%2520mechanical%2520design%252C%2520showing%2520that%2520mechanisms%2520with%250Arevolute%2520and%2520prismatic%2520joints%252C%2520and%2520potentially%2520cams%2520and%2520gears%252C%2520can%2520be%250Arepresented%2520and%2520synthesized%2520within%2520a%2520unified%2520image%2520generation%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03308v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creative%20synthesis%20of%20kinematic%20mechanisms&entry.906535625=Jiong%20Lin%20and%20Jialong%20Ning%20and%20Judah%20Goldfeder%20and%20Hod%20Lipson&entry.1292438233=%20%20In%20this%20paper%2C%20we%20formulate%20the%20problem%20of%20kinematic%20synthesis%20for%20planar%0Alinkages%20as%20a%20cross-domain%20image%20generation%20task.%20We%20develop%20a%20planar%20linkages%0Adataset%20using%20RGB%20image%20representations%2C%20covering%20a%20range%20of%20mechanisms%3A%20from%0Asimple%20types%20such%20as%20crank-rocker%20and%20crank-slider%20to%20more%20complex%20eight-bar%0Alinkages%20like%20Jansen%27s%20mechanism.%20A%20shared-latent%20variational%20autoencoder%20%28VAE%29%0Ais%20employed%20to%20explore%20the%20potential%20of%20image%20generative%20models%20for%0Asynthesizing%20unseen%20motion%20curves%20and%20simulating%20novel%20kinematics.%20By%20encoding%0Athe%20drawing%20speed%20of%20trajectory%20points%20as%20color%20gradients%2C%20the%20same%0Aarchitecture%20also%20supports%20kinematic%20synthesis%20conditioned%20on%20both%20trajectory%0Ashape%20and%20velocity%20profiles.%20We%20validate%20our%20method%20on%20three%20datasets%20of%0Aincreasing%20complexity%3A%20a%20standard%20four-bar%20linkage%20set%2C%20a%20mixed%20set%20of%20four-bar%0Aand%20crank-slider%20mechanisms%2C%20and%20a%20complex%20set%20including%20multi-loop%20mechanisms.%0APreliminary%20results%20demonstrate%20the%20effectiveness%20of%20image-based%0Arepresentations%20for%20generative%20mechanical%20design%2C%20showing%20that%20mechanisms%20with%0Arevolute%20and%20prismatic%20joints%2C%20and%20potentially%20cams%20and%20gears%2C%20can%20be%0Arepresented%20and%20synthesized%20within%20a%20unified%20image%20generation%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.03308v2&entry.124074799=Read"},
{"title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D\n  Modeling", "author": "Shuyuan Zhang and Chenhan Jiang and Zuoou Li and Jiankang Deng", "abstract": "  3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.\n", "link": "http://arxiv.org/abs/2510.17603v1", "date": "2025-10-20", "relevancy": 2.3328, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6176}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.603}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeCraft%3A%20LLM%20Agents%20for%20Structured%2C%20Textured%20and%20Interactive%203D%0A%20%20Modeling&body=Title%3A%20ShapeCraft%3A%20LLM%20Agents%20for%20Structured%2C%20Textured%20and%20Interactive%203D%0A%20%20Modeling%0AAuthor%3A%20Shuyuan%20Zhang%20and%20Chenhan%20Jiang%20and%20Zuoou%20Li%20and%20Jiankang%20Deng%0AAbstract%3A%20%20%203D%20generation%20from%20natural%20language%20offers%20significant%20potential%20to%20reduce%0Aexpert%20manual%20modeling%20efforts%20and%20enhance%20accessibility%20to%203D%20assets.%20However%2C%0Aexisting%20methods%20often%20yield%20unstructured%20meshes%20and%20exhibit%20poor%0Ainteractivity%2C%20making%20them%20impractical%20for%20artistic%20workflows.%20To%20address%20these%0Alimitations%2C%20we%20represent%203D%20assets%20as%20shape%20programs%20and%20introduce%20ShapeCraft%2C%0Aa%20novel%20multi-agent%20framework%20for%20text-to-3D%20generation.%20At%20its%20core%2C%20we%0Apropose%20a%20Graph-based%20Procedural%20Shape%20%28GPS%29%20representation%20that%20decomposes%0Acomplex%20natural%20language%20into%20a%20structured%20graph%20of%20sub-tasks%2C%20thereby%0Afacilitating%20accurate%20LLM%20comprehension%20and%20interpretation%20of%20spatial%0Arelationships%20and%20semantic%20shape%20details.%20Specifically%2C%20LLM%20agents%0Ahierarchically%20parse%20user%20input%20to%20initialize%20GPS%2C%20then%20iteratively%20refine%0Aprocedural%20modeling%20and%20painting%20to%20produce%20structured%2C%20textured%2C%20and%0Ainteractive%203D%20assets.%20Qualitative%20and%20quantitative%20experiments%20demonstrate%0AShapeCraft%27s%20superior%20performance%20in%20generating%20geometrically%20accurate%20and%0Asemantically%20rich%203D%20assets%20compared%20to%20existing%20LLM-based%20agents.%20We%20further%0Ashow%20the%20versatility%20of%20ShapeCraft%20through%20examples%20of%20animated%20and%0Auser-customized%20editing%2C%20highlighting%20its%20potential%20for%20broader%20interactive%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeCraft%253A%2520LLM%2520Agents%2520for%2520Structured%252C%2520Textured%2520and%2520Interactive%25203D%250A%2520%2520Modeling%26entry.906535625%3DShuyuan%2520Zhang%2520and%2520Chenhan%2520Jiang%2520and%2520Zuoou%2520Li%2520and%2520Jiankang%2520Deng%26entry.1292438233%3D%2520%25203D%2520generation%2520from%2520natural%2520language%2520offers%2520significant%2520potential%2520to%2520reduce%250Aexpert%2520manual%2520modeling%2520efforts%2520and%2520enhance%2520accessibility%2520to%25203D%2520assets.%2520However%252C%250Aexisting%2520methods%2520often%2520yield%2520unstructured%2520meshes%2520and%2520exhibit%2520poor%250Ainteractivity%252C%2520making%2520them%2520impractical%2520for%2520artistic%2520workflows.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520represent%25203D%2520assets%2520as%2520shape%2520programs%2520and%2520introduce%2520ShapeCraft%252C%250Aa%2520novel%2520multi-agent%2520framework%2520for%2520text-to-3D%2520generation.%2520At%2520its%2520core%252C%2520we%250Apropose%2520a%2520Graph-based%2520Procedural%2520Shape%2520%2528GPS%2529%2520representation%2520that%2520decomposes%250Acomplex%2520natural%2520language%2520into%2520a%2520structured%2520graph%2520of%2520sub-tasks%252C%2520thereby%250Afacilitating%2520accurate%2520LLM%2520comprehension%2520and%2520interpretation%2520of%2520spatial%250Arelationships%2520and%2520semantic%2520shape%2520details.%2520Specifically%252C%2520LLM%2520agents%250Ahierarchically%2520parse%2520user%2520input%2520to%2520initialize%2520GPS%252C%2520then%2520iteratively%2520refine%250Aprocedural%2520modeling%2520and%2520painting%2520to%2520produce%2520structured%252C%2520textured%252C%2520and%250Ainteractive%25203D%2520assets.%2520Qualitative%2520and%2520quantitative%2520experiments%2520demonstrate%250AShapeCraft%2527s%2520superior%2520performance%2520in%2520generating%2520geometrically%2520accurate%2520and%250Asemantically%2520rich%25203D%2520assets%2520compared%2520to%2520existing%2520LLM-based%2520agents.%2520We%2520further%250Ashow%2520the%2520versatility%2520of%2520ShapeCraft%2520through%2520examples%2520of%2520animated%2520and%250Auser-customized%2520editing%252C%2520highlighting%2520its%2520potential%2520for%2520broader%2520interactive%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeCraft%3A%20LLM%20Agents%20for%20Structured%2C%20Textured%20and%20Interactive%203D%0A%20%20Modeling&entry.906535625=Shuyuan%20Zhang%20and%20Chenhan%20Jiang%20and%20Zuoou%20Li%20and%20Jiankang%20Deng&entry.1292438233=%20%203D%20generation%20from%20natural%20language%20offers%20significant%20potential%20to%20reduce%0Aexpert%20manual%20modeling%20efforts%20and%20enhance%20accessibility%20to%203D%20assets.%20However%2C%0Aexisting%20methods%20often%20yield%20unstructured%20meshes%20and%20exhibit%20poor%0Ainteractivity%2C%20making%20them%20impractical%20for%20artistic%20workflows.%20To%20address%20these%0Alimitations%2C%20we%20represent%203D%20assets%20as%20shape%20programs%20and%20introduce%20ShapeCraft%2C%0Aa%20novel%20multi-agent%20framework%20for%20text-to-3D%20generation.%20At%20its%20core%2C%20we%0Apropose%20a%20Graph-based%20Procedural%20Shape%20%28GPS%29%20representation%20that%20decomposes%0Acomplex%20natural%20language%20into%20a%20structured%20graph%20of%20sub-tasks%2C%20thereby%0Afacilitating%20accurate%20LLM%20comprehension%20and%20interpretation%20of%20spatial%0Arelationships%20and%20semantic%20shape%20details.%20Specifically%2C%20LLM%20agents%0Ahierarchically%20parse%20user%20input%20to%20initialize%20GPS%2C%20then%20iteratively%20refine%0Aprocedural%20modeling%20and%20painting%20to%20produce%20structured%2C%20textured%2C%20and%0Ainteractive%203D%20assets.%20Qualitative%20and%20quantitative%20experiments%20demonstrate%0AShapeCraft%27s%20superior%20performance%20in%20generating%20geometrically%20accurate%20and%0Asemantically%20rich%203D%20assets%20compared%20to%20existing%20LLM-based%20agents.%20We%20further%0Ashow%20the%20versatility%20of%20ShapeCraft%20through%20examples%20of%20animated%20and%0Auser-customized%20editing%2C%20highlighting%20its%20potential%20for%20broader%20interactive%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17603v1&entry.124074799=Read"},
{"title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion\n  Language Models", "author": "Haidong Xu and Guangwei Xu and Zhedong Zheng and Xiatian Zhu and Wei Ji and Xiangtai Li and Ruijie Guo and Meishan Zhang and Min zhang and Hao Fei", "abstract": "  This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input. All the resources are available at\nhttps://walkermitty.github.io/VimoRAG/\n", "link": "http://arxiv.org/abs/2508.12081v2", "date": "2025-10-20", "relevancy": 2.3257, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6027}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5797}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VimoRAG%3A%20Video-based%20Retrieval-augmented%203D%20Motion%20Generation%20for%20Motion%0A%20%20Language%20Models&body=Title%3A%20VimoRAG%3A%20Video-based%20Retrieval-augmented%203D%20Motion%20Generation%20for%20Motion%0A%20%20Language%20Models%0AAuthor%3A%20Haidong%20Xu%20and%20Guangwei%20Xu%20and%20Zhedong%20Zheng%20and%20Xiatian%20Zhu%20and%20Wei%20Ji%20and%20Xiangtai%20Li%20and%20Ruijie%20Guo%20and%20Meishan%20Zhang%20and%20Min%20zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20This%20paper%20introduces%20VimoRAG%2C%20a%20novel%20video-based%20retrieval-augmented%20motion%0Ageneration%20framework%20for%20motion%20large%20language%20models%20%28LLMs%29.%20As%20motion%20LLMs%0Aface%20severe%20out-of-domain/out-of-vocabulary%20issues%20due%20to%20limited%20annotated%0Adata%2C%20VimoRAG%20leverages%20large-scale%20in-the-wild%20video%20databases%20to%20enhance%203D%0Amotion%20generation%20by%20retrieving%20relevant%202D%20human%20motion%20signals.%20While%0Avideo-based%20motion%20RAG%20is%20nontrivial%2C%20we%20address%20two%20key%20bottlenecks%3A%20%281%29%0Adeveloping%20an%20effective%20motion-centered%20video%20retrieval%20model%20that%0Adistinguishes%20human%20poses%20and%20actions%2C%20and%20%282%29%20mitigating%20the%20issue%20of%20error%0Apropagation%20caused%20by%20suboptimal%20retrieval%20results.%20We%20design%20the%20Gemini%20Motion%0AVideo%20Retriever%20mechanism%20and%20the%20Motion-centric%20Dual-alignment%20DPO%20Trainer%2C%0Aenabling%20effective%20retrieval%20and%20generation%20processes.%20Experimental%20results%0Ashow%20that%20VimoRAG%20significantly%20boosts%20the%20performance%20of%20motion%20LLMs%0Aconstrained%20to%20text-only%20input.%20All%20the%20resources%20are%20available%20at%0Ahttps%3A//walkermitty.github.io/VimoRAG/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVimoRAG%253A%2520Video-based%2520Retrieval-augmented%25203D%2520Motion%2520Generation%2520for%2520Motion%250A%2520%2520Language%2520Models%26entry.906535625%3DHaidong%2520Xu%2520and%2520Guangwei%2520Xu%2520and%2520Zhedong%2520Zheng%2520and%2520Xiatian%2520Zhu%2520and%2520Wei%2520Ji%2520and%2520Xiangtai%2520Li%2520and%2520Ruijie%2520Guo%2520and%2520Meishan%2520Zhang%2520and%2520Min%2520zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520VimoRAG%252C%2520a%2520novel%2520video-based%2520retrieval-augmented%2520motion%250Ageneration%2520framework%2520for%2520motion%2520large%2520language%2520models%2520%2528LLMs%2529.%2520As%2520motion%2520LLMs%250Aface%2520severe%2520out-of-domain/out-of-vocabulary%2520issues%2520due%2520to%2520limited%2520annotated%250Adata%252C%2520VimoRAG%2520leverages%2520large-scale%2520in-the-wild%2520video%2520databases%2520to%2520enhance%25203D%250Amotion%2520generation%2520by%2520retrieving%2520relevant%25202D%2520human%2520motion%2520signals.%2520While%250Avideo-based%2520motion%2520RAG%2520is%2520nontrivial%252C%2520we%2520address%2520two%2520key%2520bottlenecks%253A%2520%25281%2529%250Adeveloping%2520an%2520effective%2520motion-centered%2520video%2520retrieval%2520model%2520that%250Adistinguishes%2520human%2520poses%2520and%2520actions%252C%2520and%2520%25282%2529%2520mitigating%2520the%2520issue%2520of%2520error%250Apropagation%2520caused%2520by%2520suboptimal%2520retrieval%2520results.%2520We%2520design%2520the%2520Gemini%2520Motion%250AVideo%2520Retriever%2520mechanism%2520and%2520the%2520Motion-centric%2520Dual-alignment%2520DPO%2520Trainer%252C%250Aenabling%2520effective%2520retrieval%2520and%2520generation%2520processes.%2520Experimental%2520results%250Ashow%2520that%2520VimoRAG%2520significantly%2520boosts%2520the%2520performance%2520of%2520motion%2520LLMs%250Aconstrained%2520to%2520text-only%2520input.%2520All%2520the%2520resources%2520are%2520available%2520at%250Ahttps%253A//walkermitty.github.io/VimoRAG/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VimoRAG%3A%20Video-based%20Retrieval-augmented%203D%20Motion%20Generation%20for%20Motion%0A%20%20Language%20Models&entry.906535625=Haidong%20Xu%20and%20Guangwei%20Xu%20and%20Zhedong%20Zheng%20and%20Xiatian%20Zhu%20and%20Wei%20Ji%20and%20Xiangtai%20Li%20and%20Ruijie%20Guo%20and%20Meishan%20Zhang%20and%20Min%20zhang%20and%20Hao%20Fei&entry.1292438233=%20%20This%20paper%20introduces%20VimoRAG%2C%20a%20novel%20video-based%20retrieval-augmented%20motion%0Ageneration%20framework%20for%20motion%20large%20language%20models%20%28LLMs%29.%20As%20motion%20LLMs%0Aface%20severe%20out-of-domain/out-of-vocabulary%20issues%20due%20to%20limited%20annotated%0Adata%2C%20VimoRAG%20leverages%20large-scale%20in-the-wild%20video%20databases%20to%20enhance%203D%0Amotion%20generation%20by%20retrieving%20relevant%202D%20human%20motion%20signals.%20While%0Avideo-based%20motion%20RAG%20is%20nontrivial%2C%20we%20address%20two%20key%20bottlenecks%3A%20%281%29%0Adeveloping%20an%20effective%20motion-centered%20video%20retrieval%20model%20that%0Adistinguishes%20human%20poses%20and%20actions%2C%20and%20%282%29%20mitigating%20the%20issue%20of%20error%0Apropagation%20caused%20by%20suboptimal%20retrieval%20results.%20We%20design%20the%20Gemini%20Motion%0AVideo%20Retriever%20mechanism%20and%20the%20Motion-centric%20Dual-alignment%20DPO%20Trainer%2C%0Aenabling%20effective%20retrieval%20and%20generation%20processes.%20Experimental%20results%0Ashow%20that%20VimoRAG%20significantly%20boosts%20the%20performance%20of%20motion%20LLMs%0Aconstrained%20to%20text-only%20input.%20All%20the%20resources%20are%20available%20at%0Ahttps%3A//walkermitty.github.io/VimoRAG/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12081v2&entry.124074799=Read"},
{"title": "Efficient Tensor Completion Algorithms for Highly Oscillatory Operators", "author": "Navjot Singh and Edgar Solomonik and Xiaoye Sherry Li and Yang Liu", "abstract": "  This paper presents low-complexity tensor completion algorithms and their\nefficient implementation to reconstruct highly oscillatory operators\ndiscretized as $n\\times n$ matrices. The underlying tensor decomposition is\nbased on the reshaping of the input matrix and its butterfly decomposition into\nan order $\\mathcal{O} (\\log n)$ tensor. The reshaping of the input matrix into\na tensor allows for representation of the butterfly decomposition as a tensor\ndecomposition with dense tensors. This leads to efficient utilization of the\nexisting software infrastructure for dense and sparse tensor computations. We\npropose two tensor completion algorithms in the butterfly format, using\nalternating least squares and gradient-based optimization, as well as a novel\nstrategy that uses low-rank matrix completion to efficiently generate an\ninitial guess for the proposed algorithms. To demonstrate the efficiency and\napplicability of our proposed algorithms, we perform three numerical\nexperiments using simulated oscillatory operators in seismic applications. In\nthese experiments, we use $\\mathcal {O} (n \\log n)$ observed entries in the\ninput matrix and demonstrate an $\\mathcal{O}(n\\log^3 n)$ computational cost of\nthe proposed algorithms, leading to a speedup of orders of magnitudes per\niteration for large matrices compared to the low-rank matrix and quantized\ntensor-train completion. Moreover, the proposed butterfly completion\nalgorithms, equipped with the novel initial guess generation strategy, achieve\nreconstruction errors that are smaller by an order of magnitude, enabling\naccurate recovery of the underlying structure compared to the state-of-the-art\ncompletion algorithms.\n", "link": "http://arxiv.org/abs/2510.17734v1", "date": "2025-10-20", "relevancy": 2.2726, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4541}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Tensor%20Completion%20Algorithms%20for%20Highly%20Oscillatory%20Operators&body=Title%3A%20Efficient%20Tensor%20Completion%20Algorithms%20for%20Highly%20Oscillatory%20Operators%0AAuthor%3A%20Navjot%20Singh%20and%20Edgar%20Solomonik%20and%20Xiaoye%20Sherry%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20This%20paper%20presents%20low-complexity%20tensor%20completion%20algorithms%20and%20their%0Aefficient%20implementation%20to%20reconstruct%20highly%20oscillatory%20operators%0Adiscretized%20as%20%24n%5Ctimes%20n%24%20matrices.%20The%20underlying%20tensor%20decomposition%20is%0Abased%20on%20the%20reshaping%20of%20the%20input%20matrix%20and%20its%20butterfly%20decomposition%20into%0Aan%20order%20%24%5Cmathcal%7BO%7D%20%28%5Clog%20n%29%24%20tensor.%20The%20reshaping%20of%20the%20input%20matrix%20into%0Aa%20tensor%20allows%20for%20representation%20of%20the%20butterfly%20decomposition%20as%20a%20tensor%0Adecomposition%20with%20dense%20tensors.%20This%20leads%20to%20efficient%20utilization%20of%20the%0Aexisting%20software%20infrastructure%20for%20dense%20and%20sparse%20tensor%20computations.%20We%0Apropose%20two%20tensor%20completion%20algorithms%20in%20the%20butterfly%20format%2C%20using%0Aalternating%20least%20squares%20and%20gradient-based%20optimization%2C%20as%20well%20as%20a%20novel%0Astrategy%20that%20uses%20low-rank%20matrix%20completion%20to%20efficiently%20generate%20an%0Ainitial%20guess%20for%20the%20proposed%20algorithms.%20To%20demonstrate%20the%20efficiency%20and%0Aapplicability%20of%20our%20proposed%20algorithms%2C%20we%20perform%20three%20numerical%0Aexperiments%20using%20simulated%20oscillatory%20operators%20in%20seismic%20applications.%20In%0Athese%20experiments%2C%20we%20use%20%24%5Cmathcal%20%7BO%7D%20%28n%20%5Clog%20n%29%24%20observed%20entries%20in%20the%0Ainput%20matrix%20and%20demonstrate%20an%20%24%5Cmathcal%7BO%7D%28n%5Clog%5E3%20n%29%24%20computational%20cost%20of%0Athe%20proposed%20algorithms%2C%20leading%20to%20a%20speedup%20of%20orders%20of%20magnitudes%20per%0Aiteration%20for%20large%20matrices%20compared%20to%20the%20low-rank%20matrix%20and%20quantized%0Atensor-train%20completion.%20Moreover%2C%20the%20proposed%20butterfly%20completion%0Aalgorithms%2C%20equipped%20with%20the%20novel%20initial%20guess%20generation%20strategy%2C%20achieve%0Areconstruction%20errors%20that%20are%20smaller%20by%20an%20order%20of%20magnitude%2C%20enabling%0Aaccurate%20recovery%20of%20the%20underlying%20structure%20compared%20to%20the%20state-of-the-art%0Acompletion%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Tensor%2520Completion%2520Algorithms%2520for%2520Highly%2520Oscillatory%2520Operators%26entry.906535625%3DNavjot%2520Singh%2520and%2520Edgar%2520Solomonik%2520and%2520Xiaoye%2520Sherry%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520low-complexity%2520tensor%2520completion%2520algorithms%2520and%2520their%250Aefficient%2520implementation%2520to%2520reconstruct%2520highly%2520oscillatory%2520operators%250Adiscretized%2520as%2520%2524n%255Ctimes%2520n%2524%2520matrices.%2520The%2520underlying%2520tensor%2520decomposition%2520is%250Abased%2520on%2520the%2520reshaping%2520of%2520the%2520input%2520matrix%2520and%2520its%2520butterfly%2520decomposition%2520into%250Aan%2520order%2520%2524%255Cmathcal%257BO%257D%2520%2528%255Clog%2520n%2529%2524%2520tensor.%2520The%2520reshaping%2520of%2520the%2520input%2520matrix%2520into%250Aa%2520tensor%2520allows%2520for%2520representation%2520of%2520the%2520butterfly%2520decomposition%2520as%2520a%2520tensor%250Adecomposition%2520with%2520dense%2520tensors.%2520This%2520leads%2520to%2520efficient%2520utilization%2520of%2520the%250Aexisting%2520software%2520infrastructure%2520for%2520dense%2520and%2520sparse%2520tensor%2520computations.%2520We%250Apropose%2520two%2520tensor%2520completion%2520algorithms%2520in%2520the%2520butterfly%2520format%252C%2520using%250Aalternating%2520least%2520squares%2520and%2520gradient-based%2520optimization%252C%2520as%2520well%2520as%2520a%2520novel%250Astrategy%2520that%2520uses%2520low-rank%2520matrix%2520completion%2520to%2520efficiently%2520generate%2520an%250Ainitial%2520guess%2520for%2520the%2520proposed%2520algorithms.%2520To%2520demonstrate%2520the%2520efficiency%2520and%250Aapplicability%2520of%2520our%2520proposed%2520algorithms%252C%2520we%2520perform%2520three%2520numerical%250Aexperiments%2520using%2520simulated%2520oscillatory%2520operators%2520in%2520seismic%2520applications.%2520In%250Athese%2520experiments%252C%2520we%2520use%2520%2524%255Cmathcal%2520%257BO%257D%2520%2528n%2520%255Clog%2520n%2529%2524%2520observed%2520entries%2520in%2520the%250Ainput%2520matrix%2520and%2520demonstrate%2520an%2520%2524%255Cmathcal%257BO%257D%2528n%255Clog%255E3%2520n%2529%2524%2520computational%2520cost%2520of%250Athe%2520proposed%2520algorithms%252C%2520leading%2520to%2520a%2520speedup%2520of%2520orders%2520of%2520magnitudes%2520per%250Aiteration%2520for%2520large%2520matrices%2520compared%2520to%2520the%2520low-rank%2520matrix%2520and%2520quantized%250Atensor-train%2520completion.%2520Moreover%252C%2520the%2520proposed%2520butterfly%2520completion%250Aalgorithms%252C%2520equipped%2520with%2520the%2520novel%2520initial%2520guess%2520generation%2520strategy%252C%2520achieve%250Areconstruction%2520errors%2520that%2520are%2520smaller%2520by%2520an%2520order%2520of%2520magnitude%252C%2520enabling%250Aaccurate%2520recovery%2520of%2520the%2520underlying%2520structure%2520compared%2520to%2520the%2520state-of-the-art%250Acompletion%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Tensor%20Completion%20Algorithms%20for%20Highly%20Oscillatory%20Operators&entry.906535625=Navjot%20Singh%20and%20Edgar%20Solomonik%20and%20Xiaoye%20Sherry%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20This%20paper%20presents%20low-complexity%20tensor%20completion%20algorithms%20and%20their%0Aefficient%20implementation%20to%20reconstruct%20highly%20oscillatory%20operators%0Adiscretized%20as%20%24n%5Ctimes%20n%24%20matrices.%20The%20underlying%20tensor%20decomposition%20is%0Abased%20on%20the%20reshaping%20of%20the%20input%20matrix%20and%20its%20butterfly%20decomposition%20into%0Aan%20order%20%24%5Cmathcal%7BO%7D%20%28%5Clog%20n%29%24%20tensor.%20The%20reshaping%20of%20the%20input%20matrix%20into%0Aa%20tensor%20allows%20for%20representation%20of%20the%20butterfly%20decomposition%20as%20a%20tensor%0Adecomposition%20with%20dense%20tensors.%20This%20leads%20to%20efficient%20utilization%20of%20the%0Aexisting%20software%20infrastructure%20for%20dense%20and%20sparse%20tensor%20computations.%20We%0Apropose%20two%20tensor%20completion%20algorithms%20in%20the%20butterfly%20format%2C%20using%0Aalternating%20least%20squares%20and%20gradient-based%20optimization%2C%20as%20well%20as%20a%20novel%0Astrategy%20that%20uses%20low-rank%20matrix%20completion%20to%20efficiently%20generate%20an%0Ainitial%20guess%20for%20the%20proposed%20algorithms.%20To%20demonstrate%20the%20efficiency%20and%0Aapplicability%20of%20our%20proposed%20algorithms%2C%20we%20perform%20three%20numerical%0Aexperiments%20using%20simulated%20oscillatory%20operators%20in%20seismic%20applications.%20In%0Athese%20experiments%2C%20we%20use%20%24%5Cmathcal%20%7BO%7D%20%28n%20%5Clog%20n%29%24%20observed%20entries%20in%20the%0Ainput%20matrix%20and%20demonstrate%20an%20%24%5Cmathcal%7BO%7D%28n%5Clog%5E3%20n%29%24%20computational%20cost%20of%0Athe%20proposed%20algorithms%2C%20leading%20to%20a%20speedup%20of%20orders%20of%20magnitudes%20per%0Aiteration%20for%20large%20matrices%20compared%20to%20the%20low-rank%20matrix%20and%20quantized%0Atensor-train%20completion.%20Moreover%2C%20the%20proposed%20butterfly%20completion%0Aalgorithms%2C%20equipped%20with%20the%20novel%20initial%20guess%20generation%20strategy%2C%20achieve%0Areconstruction%20errors%20that%20are%20smaller%20by%20an%20order%20of%20magnitude%2C%20enabling%0Aaccurate%20recovery%20of%20the%20underlying%20structure%20compared%20to%20the%20state-of-the-art%0Acompletion%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17734v1&entry.124074799=Read"},
{"title": "NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft\n  Pose Estimation", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  On-orbit operations require the estimation of the relative 6D pose, i.e.,\nposition and orientation, between a chaser spacecraft and its target. While\ndata-driven spacecraft pose estimation methods have been developed, their\nadoption in real missions is hampered by the lack of understanding of their\ndecision process. This paper presents a method to visualize the 3D visual cues\non which a given pose estimator relies. For this purpose, we train a NeRF-based\nimage generator using the gradients back-propagated through the pose estimation\nnetwork. This enforces the generator to render the main 3D features exploited\nby the spacecraft pose estimation network. Experiments demonstrate that our\nmethod recovers the relevant 3D cues. Furthermore, they offer additional\ninsights on the relationship between the pose estimation network supervision\nand its implicit representation of the target spacecraft.\n", "link": "http://arxiv.org/abs/2509.14890v2", "date": "2025-10-20", "relevancy": 2.2705, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5975}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation&body=Title%3A%20NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20On-orbit%20operations%20require%20the%20estimation%20of%20the%20relative%206D%20pose%2C%20i.e.%2C%0Aposition%20and%20orientation%2C%20between%20a%20chaser%20spacecraft%20and%20its%20target.%20While%0Adata-driven%20spacecraft%20pose%20estimation%20methods%20have%20been%20developed%2C%20their%0Aadoption%20in%20real%20missions%20is%20hampered%20by%20the%20lack%20of%20understanding%20of%20their%0Adecision%20process.%20This%20paper%20presents%20a%20method%20to%20visualize%20the%203D%20visual%20cues%0Aon%20which%20a%20given%20pose%20estimator%20relies.%20For%20this%20purpose%2C%20we%20train%20a%20NeRF-based%0Aimage%20generator%20using%20the%20gradients%20back-propagated%20through%20the%20pose%20estimation%0Anetwork.%20This%20enforces%20the%20generator%20to%20render%20the%20main%203D%20features%20exploited%0Aby%20the%20spacecraft%20pose%20estimation%20network.%20Experiments%20demonstrate%20that%20our%0Amethod%20recovers%20the%20relevant%203D%20cues.%20Furthermore%2C%20they%20offer%20additional%0Ainsights%20on%20the%20relationship%20between%20the%20pose%20estimation%20network%20supervision%0Aand%20its%20implicit%20representation%20of%20the%20target%20spacecraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-based%2520Visualization%2520of%25203D%2520Cues%2520Supporting%2520Data-Driven%2520Spacecraft%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520On-orbit%2520operations%2520require%2520the%2520estimation%2520of%2520the%2520relative%25206D%2520pose%252C%2520i.e.%252C%250Aposition%2520and%2520orientation%252C%2520between%2520a%2520chaser%2520spacecraft%2520and%2520its%2520target.%2520While%250Adata-driven%2520spacecraft%2520pose%2520estimation%2520methods%2520have%2520been%2520developed%252C%2520their%250Aadoption%2520in%2520real%2520missions%2520is%2520hampered%2520by%2520the%2520lack%2520of%2520understanding%2520of%2520their%250Adecision%2520process.%2520This%2520paper%2520presents%2520a%2520method%2520to%2520visualize%2520the%25203D%2520visual%2520cues%250Aon%2520which%2520a%2520given%2520pose%2520estimator%2520relies.%2520For%2520this%2520purpose%252C%2520we%2520train%2520a%2520NeRF-based%250Aimage%2520generator%2520using%2520the%2520gradients%2520back-propagated%2520through%2520the%2520pose%2520estimation%250Anetwork.%2520This%2520enforces%2520the%2520generator%2520to%2520render%2520the%2520main%25203D%2520features%2520exploited%250Aby%2520the%2520spacecraft%2520pose%2520estimation%2520network.%2520Experiments%2520demonstrate%2520that%2520our%250Amethod%2520recovers%2520the%2520relevant%25203D%2520cues.%2520Furthermore%252C%2520they%2520offer%2520additional%250Ainsights%2520on%2520the%2520relationship%2520between%2520the%2520pose%2520estimation%2520network%2520supervision%250Aand%2520its%2520implicit%2520representation%2520of%2520the%2520target%2520spacecraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20On-orbit%20operations%20require%20the%20estimation%20of%20the%20relative%206D%20pose%2C%20i.e.%2C%0Aposition%20and%20orientation%2C%20between%20a%20chaser%20spacecraft%20and%20its%20target.%20While%0Adata-driven%20spacecraft%20pose%20estimation%20methods%20have%20been%20developed%2C%20their%0Aadoption%20in%20real%20missions%20is%20hampered%20by%20the%20lack%20of%20understanding%20of%20their%0Adecision%20process.%20This%20paper%20presents%20a%20method%20to%20visualize%20the%203D%20visual%20cues%0Aon%20which%20a%20given%20pose%20estimator%20relies.%20For%20this%20purpose%2C%20we%20train%20a%20NeRF-based%0Aimage%20generator%20using%20the%20gradients%20back-propagated%20through%20the%20pose%20estimation%0Anetwork.%20This%20enforces%20the%20generator%20to%20render%20the%20main%203D%20features%20exploited%0Aby%20the%20spacecraft%20pose%20estimation%20network.%20Experiments%20demonstrate%20that%20our%0Amethod%20recovers%20the%20relevant%203D%20cues.%20Furthermore%2C%20they%20offer%20additional%0Ainsights%20on%20the%20relationship%20between%20the%20pose%20estimation%20network%20supervision%0Aand%20its%20implicit%20representation%20of%20the%20target%20spacecraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14890v2&entry.124074799=Read"},
{"title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation\n  for Visual Place Recognition", "author": "Timur Ismagilov and Shakaiba Majeed and Michael Milford and Tan Viet Tuyen Nguyen and Sarvapali D. Ramchurn and Shoaib Ehsan", "abstract": "  We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.\n", "link": "http://arxiv.org/abs/2510.17739v1", "date": "2025-10-20", "relevancy": 2.2687, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Multi-Condition%20Representation%20Modelling%20via%20Matrix%20Factorisation%0A%20%20for%20Visual%20Place%20Recognition&body=Title%3A%20Joint%20Multi-Condition%20Representation%20Modelling%20via%20Matrix%20Factorisation%0A%20%20for%20Visual%20Place%20Recognition%0AAuthor%3A%20Timur%20Ismagilov%20and%20Shakaiba%20Majeed%20and%20Michael%20Milford%20and%20Tan%20Viet%20Tuyen%20Nguyen%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan%0AAbstract%3A%20%20%20We%20address%20multi-reference%20visual%20place%20recognition%20%28VPR%29%2C%20where%20reference%0Asets%20captured%20under%20varying%20conditions%20are%20used%20to%20improve%20localisation%0Aperformance.%20While%20deep%20learning%20with%20large-scale%20training%20improves%20robustness%2C%0Aincreasing%20data%20diversity%20and%20model%20complexity%20incur%20extensive%20computational%0Acost%20during%20training%20and%20deployment.%20Descriptor-level%20fusion%20via%20voting%20or%0Aaggregation%20avoids%20training%2C%20but%20often%20targets%20multi-sensor%20setups%20or%20relies%20on%0Aheuristics%20with%20limited%20gains%20under%20appearance%20and%20viewpoint%20change.%20We%20propose%0Aa%20training-free%2C%20descriptor-agnostic%20approach%20that%20jointly%20models%20places%20using%0Amultiple%20reference%20descriptors%20via%20matrix%20decomposition%20into%20basis%0Arepresentations%2C%20enabling%20projection-based%20residual%20matching.%20We%20also%20introduce%0ASotonMV%2C%20a%20structured%20benchmark%20for%20multi-viewpoint%20VPR.%20On%20multi-appearance%0Adata%2C%20our%20method%20improves%20Recall%401%20by%20up%20to%20~18%25%20over%20single-reference%20and%0Aoutperforms%20multi-reference%20baselines%20across%20appearance%20and%20viewpoint%20changes%2C%0Awith%20gains%20of%20~5%25%20on%20unstructured%20data%2C%20demonstrating%20strong%20generalisation%0Awhile%20remaining%20lightweight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Multi-Condition%2520Representation%2520Modelling%2520via%2520Matrix%2520Factorisation%250A%2520%2520for%2520Visual%2520Place%2520Recognition%26entry.906535625%3DTimur%2520Ismagilov%2520and%2520Shakaiba%2520Majeed%2520and%2520Michael%2520Milford%2520and%2520Tan%2520Viet%2520Tuyen%2520Nguyen%2520and%2520Sarvapali%2520D.%2520Ramchurn%2520and%2520Shoaib%2520Ehsan%26entry.1292438233%3D%2520%2520We%2520address%2520multi-reference%2520visual%2520place%2520recognition%2520%2528VPR%2529%252C%2520where%2520reference%250Asets%2520captured%2520under%2520varying%2520conditions%2520are%2520used%2520to%2520improve%2520localisation%250Aperformance.%2520While%2520deep%2520learning%2520with%2520large-scale%2520training%2520improves%2520robustness%252C%250Aincreasing%2520data%2520diversity%2520and%2520model%2520complexity%2520incur%2520extensive%2520computational%250Acost%2520during%2520training%2520and%2520deployment.%2520Descriptor-level%2520fusion%2520via%2520voting%2520or%250Aaggregation%2520avoids%2520training%252C%2520but%2520often%2520targets%2520multi-sensor%2520setups%2520or%2520relies%2520on%250Aheuristics%2520with%2520limited%2520gains%2520under%2520appearance%2520and%2520viewpoint%2520change.%2520We%2520propose%250Aa%2520training-free%252C%2520descriptor-agnostic%2520approach%2520that%2520jointly%2520models%2520places%2520using%250Amultiple%2520reference%2520descriptors%2520via%2520matrix%2520decomposition%2520into%2520basis%250Arepresentations%252C%2520enabling%2520projection-based%2520residual%2520matching.%2520We%2520also%2520introduce%250ASotonMV%252C%2520a%2520structured%2520benchmark%2520for%2520multi-viewpoint%2520VPR.%2520On%2520multi-appearance%250Adata%252C%2520our%2520method%2520improves%2520Recall%25401%2520by%2520up%2520to%2520~18%2525%2520over%2520single-reference%2520and%250Aoutperforms%2520multi-reference%2520baselines%2520across%2520appearance%2520and%2520viewpoint%2520changes%252C%250Awith%2520gains%2520of%2520~5%2525%2520on%2520unstructured%2520data%252C%2520demonstrating%2520strong%2520generalisation%250Awhile%2520remaining%2520lightweight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Multi-Condition%20Representation%20Modelling%20via%20Matrix%20Factorisation%0A%20%20for%20Visual%20Place%20Recognition&entry.906535625=Timur%20Ismagilov%20and%20Shakaiba%20Majeed%20and%20Michael%20Milford%20and%20Tan%20Viet%20Tuyen%20Nguyen%20and%20Sarvapali%20D.%20Ramchurn%20and%20Shoaib%20Ehsan&entry.1292438233=%20%20We%20address%20multi-reference%20visual%20place%20recognition%20%28VPR%29%2C%20where%20reference%0Asets%20captured%20under%20varying%20conditions%20are%20used%20to%20improve%20localisation%0Aperformance.%20While%20deep%20learning%20with%20large-scale%20training%20improves%20robustness%2C%0Aincreasing%20data%20diversity%20and%20model%20complexity%20incur%20extensive%20computational%0Acost%20during%20training%20and%20deployment.%20Descriptor-level%20fusion%20via%20voting%20or%0Aaggregation%20avoids%20training%2C%20but%20often%20targets%20multi-sensor%20setups%20or%20relies%20on%0Aheuristics%20with%20limited%20gains%20under%20appearance%20and%20viewpoint%20change.%20We%20propose%0Aa%20training-free%2C%20descriptor-agnostic%20approach%20that%20jointly%20models%20places%20using%0Amultiple%20reference%20descriptors%20via%20matrix%20decomposition%20into%20basis%0Arepresentations%2C%20enabling%20projection-based%20residual%20matching.%20We%20also%20introduce%0ASotonMV%2C%20a%20structured%20benchmark%20for%20multi-viewpoint%20VPR.%20On%20multi-appearance%0Adata%2C%20our%20method%20improves%20Recall%401%20by%20up%20to%20~18%25%20over%20single-reference%20and%0Aoutperforms%20multi-reference%20baselines%20across%20appearance%20and%20viewpoint%20changes%2C%0Awith%20gains%20of%20~5%25%20on%20unstructured%20data%2C%20demonstrating%20strong%20generalisation%0Awhile%20remaining%20lightweight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17739v1&entry.124074799=Read"},
{"title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation\n  in Large Language Models", "author": "Dayan Pan and Zhaoyang Fu and Jingyuan Wang and Xiao Han and Yue Zhu and Xiangyu Zhao", "abstract": "  Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.\n", "link": "http://arxiv.org/abs/2510.17705v1", "date": "2025-10-20", "relevancy": 2.2685, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6193}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Attention%20Modulation%3A%20Towards%20Efficient%20Multi-Task%20Adaptation%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20Contextual%20Attention%20Modulation%3A%20Towards%20Efficient%20Multi-Task%20Adaptation%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Dayan%20Pan%20and%20Zhaoyang%20Fu%20and%20Jingyuan%20Wang%20and%20Xiao%20Han%20and%20Yue%20Zhu%20and%20Xiangyu%20Zhao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20possess%20remarkable%20generalization%20capabilities%0Abut%20struggle%20with%20multi-task%20adaptation%2C%20particularly%20in%20balancing%20knowledge%0Aretention%20with%20task-specific%20specialization.%20Conventional%20fine-tuning%20methods%0Asuffer%20from%20catastrophic%20forgetting%20and%20substantial%20resource%20consumption%2C%20while%0Aexisting%20parameter-efficient%20methods%20perform%20suboptimally%20in%20complex%20multi-task%0Ascenarios.%20To%20address%20this%2C%20we%20propose%20Contextual%20Attention%20Modulation%20%28CAM%29%2C%20a%0Anovel%20mechanism%20that%20dynamically%20modulates%20the%20representations%20of%0Aself-attention%20modules%20in%20LLMs.%20CAM%20enhances%20task-specific%20features%20while%0Apreserving%20general%20knowledge%2C%20thereby%20facilitating%20more%20effective%20and%20efficient%0Aadaptation.%20For%20effective%20multi-task%20adaptation%2C%20CAM%20is%20integrated%20into%20our%0AHybrid%20Contextual%20Attention%20Modulation%20%28HyCAM%29%20framework%2C%20which%20combines%20a%0Ashared%2C%20full-parameter%20CAM%20module%20with%20multiple%20specialized%2C%20lightweight%20CAM%0Amodules%2C%20enhanced%20by%20a%20dynamic%20routing%20strategy%20for%20adaptive%20knowledge%20fusion.%0AExtensive%20experiments%20on%20heterogeneous%20tasks%2C%20including%20question%20answering%2C%0Acode%20generation%2C%20and%20logical%20reasoning%2C%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20approaches%2C%20achieving%20an%20average%20performance%0Aimprovement%20of%203.65%25.%20The%20implemented%20code%20and%20data%20are%20available%20to%20ease%0Areproducibility%20at%20https%3A//github.com/Applied-Machine-Learning-Lab/HyCAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Attention%2520Modulation%253A%2520Towards%2520Efficient%2520Multi-Task%2520Adaptation%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDayan%2520Pan%2520and%2520Zhaoyang%2520Fu%2520and%2520Jingyuan%2520Wang%2520and%2520Xiao%2520Han%2520and%2520Yue%2520Zhu%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520possess%2520remarkable%2520generalization%2520capabilities%250Abut%2520struggle%2520with%2520multi-task%2520adaptation%252C%2520particularly%2520in%2520balancing%2520knowledge%250Aretention%2520with%2520task-specific%2520specialization.%2520Conventional%2520fine-tuning%2520methods%250Asuffer%2520from%2520catastrophic%2520forgetting%2520and%2520substantial%2520resource%2520consumption%252C%2520while%250Aexisting%2520parameter-efficient%2520methods%2520perform%2520suboptimally%2520in%2520complex%2520multi-task%250Ascenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520Contextual%2520Attention%2520Modulation%2520%2528CAM%2529%252C%2520a%250Anovel%2520mechanism%2520that%2520dynamically%2520modulates%2520the%2520representations%2520of%250Aself-attention%2520modules%2520in%2520LLMs.%2520CAM%2520enhances%2520task-specific%2520features%2520while%250Apreserving%2520general%2520knowledge%252C%2520thereby%2520facilitating%2520more%2520effective%2520and%2520efficient%250Aadaptation.%2520For%2520effective%2520multi-task%2520adaptation%252C%2520CAM%2520is%2520integrated%2520into%2520our%250AHybrid%2520Contextual%2520Attention%2520Modulation%2520%2528HyCAM%2529%2520framework%252C%2520which%2520combines%2520a%250Ashared%252C%2520full-parameter%2520CAM%2520module%2520with%2520multiple%2520specialized%252C%2520lightweight%2520CAM%250Amodules%252C%2520enhanced%2520by%2520a%2520dynamic%2520routing%2520strategy%2520for%2520adaptive%2520knowledge%2520fusion.%250AExtensive%2520experiments%2520on%2520heterogeneous%2520tasks%252C%2520including%2520question%2520answering%252C%250Acode%2520generation%252C%2520and%2520logical%2520reasoning%252C%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520existing%2520approaches%252C%2520achieving%2520an%2520average%2520performance%250Aimprovement%2520of%25203.65%2525.%2520The%2520implemented%2520code%2520and%2520data%2520are%2520available%2520to%2520ease%250Areproducibility%2520at%2520https%253A//github.com/Applied-Machine-Learning-Lab/HyCAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Attention%20Modulation%3A%20Towards%20Efficient%20Multi-Task%20Adaptation%0A%20%20in%20Large%20Language%20Models&entry.906535625=Dayan%20Pan%20and%20Zhaoyang%20Fu%20and%20Jingyuan%20Wang%20and%20Xiao%20Han%20and%20Yue%20Zhu%20and%20Xiangyu%20Zhao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20possess%20remarkable%20generalization%20capabilities%0Abut%20struggle%20with%20multi-task%20adaptation%2C%20particularly%20in%20balancing%20knowledge%0Aretention%20with%20task-specific%20specialization.%20Conventional%20fine-tuning%20methods%0Asuffer%20from%20catastrophic%20forgetting%20and%20substantial%20resource%20consumption%2C%20while%0Aexisting%20parameter-efficient%20methods%20perform%20suboptimally%20in%20complex%20multi-task%0Ascenarios.%20To%20address%20this%2C%20we%20propose%20Contextual%20Attention%20Modulation%20%28CAM%29%2C%20a%0Anovel%20mechanism%20that%20dynamically%20modulates%20the%20representations%20of%0Aself-attention%20modules%20in%20LLMs.%20CAM%20enhances%20task-specific%20features%20while%0Apreserving%20general%20knowledge%2C%20thereby%20facilitating%20more%20effective%20and%20efficient%0Aadaptation.%20For%20effective%20multi-task%20adaptation%2C%20CAM%20is%20integrated%20into%20our%0AHybrid%20Contextual%20Attention%20Modulation%20%28HyCAM%29%20framework%2C%20which%20combines%20a%0Ashared%2C%20full-parameter%20CAM%20module%20with%20multiple%20specialized%2C%20lightweight%20CAM%0Amodules%2C%20enhanced%20by%20a%20dynamic%20routing%20strategy%20for%20adaptive%20knowledge%20fusion.%0AExtensive%20experiments%20on%20heterogeneous%20tasks%2C%20including%20question%20answering%2C%0Acode%20generation%2C%20and%20logical%20reasoning%2C%20demonstrate%20that%20our%20approach%0Asignificantly%20outperforms%20existing%20approaches%2C%20achieving%20an%20average%20performance%0Aimprovement%20of%203.65%25.%20The%20implemented%20code%20and%20data%20are%20available%20to%20ease%0Areproducibility%20at%20https%3A//github.com/Applied-Machine-Learning-Lab/HyCAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17705v1&entry.124074799=Read"},
{"title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents", "author": "Yilei Jiang and Yaozhi Zheng and Yuxuan Wan and Jiaming Han and Qunzhong Wang and Michael R. Lyu and Xiangyu Yue", "abstract": "  Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While multimodal large language models (MLLMs)\ncan translate images to code, they often fail on complex UIs, struggling to\nunify visual perception, layout planning, and code synthesis within a single\nmonolithic model, which leads to frequent perception and planning errors. To\naddress this, we propose ScreenCoder, a modular multi-agent framework that\ndecomposes the task into three interpretable stages: grounding, planning, and\ngeneration. By assigning these distinct responsibilities to specialized agents,\nour framework achieves significantly higher robustness and fidelity than\nend-to-end approaches. Furthermore, ScreenCoder serves as a scalable data\nengine, enabling us to generate high-quality image-code pairs. We use this data\nto fine-tune open-source MLLM via a dual-stage pipeline of supervised\nfine-tuning and reinforcement learning, demonstrating substantial gains in its\nUI generation capabilities. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance in layout accuracy, structural coherence,\nand code correctness. Our code is made publicly available at\nhttps://github.com/leigest519/ScreenCoder.\n", "link": "http://arxiv.org/abs/2507.22827v2", "date": "2025-10-20", "relevancy": 2.2614, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5947}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5454}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents&body=Title%3A%20ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents%0AAuthor%3A%20Yilei%20Jiang%20and%20Yaozhi%20Zheng%20and%20Yuxuan%20Wan%20and%20Jiaming%20Han%20and%20Qunzhong%20Wang%20and%20Michael%20R.%20Lyu%20and%20Xiangyu%20Yue%0AAbstract%3A%20%20%20Automating%20the%20transformation%20of%20user%20interface%20%28UI%29%20designs%20into%20front-end%0Acode%20holds%20significant%20promise%20for%20accelerating%20software%20development%20and%0Ademocratizing%20design%20workflows.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%0Acan%20translate%20images%20to%20code%2C%20they%20often%20fail%20on%20complex%20UIs%2C%20struggling%20to%0Aunify%20visual%20perception%2C%20layout%20planning%2C%20and%20code%20synthesis%20within%20a%20single%0Amonolithic%20model%2C%20which%20leads%20to%20frequent%20perception%20and%20planning%20errors.%20To%0Aaddress%20this%2C%20we%20propose%20ScreenCoder%2C%20a%20modular%20multi-agent%20framework%20that%0Adecomposes%20the%20task%20into%20three%20interpretable%20stages%3A%20grounding%2C%20planning%2C%20and%0Ageneration.%20By%20assigning%20these%20distinct%20responsibilities%20to%20specialized%20agents%2C%0Aour%20framework%20achieves%20significantly%20higher%20robustness%20and%20fidelity%20than%0Aend-to-end%20approaches.%20Furthermore%2C%20ScreenCoder%20serves%20as%20a%20scalable%20data%0Aengine%2C%20enabling%20us%20to%20generate%20high-quality%20image-code%20pairs.%20We%20use%20this%20data%0Ato%20fine-tune%20open-source%20MLLM%20via%20a%20dual-stage%20pipeline%20of%20supervised%0Afine-tuning%20and%20reinforcement%20learning%2C%20demonstrating%20substantial%20gains%20in%20its%0AUI%20generation%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance%20in%20layout%20accuracy%2C%20structural%20coherence%2C%0Aand%20code%20correctness.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/leigest519/ScreenCoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22827v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScreenCoder%253A%2520Advancing%2520Visual-to-Code%2520Generation%2520for%2520Front-End%250A%2520%2520Automation%2520via%2520Modular%2520Multimodal%2520Agents%26entry.906535625%3DYilei%2520Jiang%2520and%2520Yaozhi%2520Zheng%2520and%2520Yuxuan%2520Wan%2520and%2520Jiaming%2520Han%2520and%2520Qunzhong%2520Wang%2520and%2520Michael%2520R.%2520Lyu%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3D%2520%2520Automating%2520the%2520transformation%2520of%2520user%2520interface%2520%2528UI%2529%2520designs%2520into%2520front-end%250Acode%2520holds%2520significant%2520promise%2520for%2520accelerating%2520software%2520development%2520and%250Ademocratizing%2520design%2520workflows.%2520While%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Acan%2520translate%2520images%2520to%2520code%252C%2520they%2520often%2520fail%2520on%2520complex%2520UIs%252C%2520struggling%2520to%250Aunify%2520visual%2520perception%252C%2520layout%2520planning%252C%2520and%2520code%2520synthesis%2520within%2520a%2520single%250Amonolithic%2520model%252C%2520which%2520leads%2520to%2520frequent%2520perception%2520and%2520planning%2520errors.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520ScreenCoder%252C%2520a%2520modular%2520multi-agent%2520framework%2520that%250Adecomposes%2520the%2520task%2520into%2520three%2520interpretable%2520stages%253A%2520grounding%252C%2520planning%252C%2520and%250Ageneration.%2520By%2520assigning%2520these%2520distinct%2520responsibilities%2520to%2520specialized%2520agents%252C%250Aour%2520framework%2520achieves%2520significantly%2520higher%2520robustness%2520and%2520fidelity%2520than%250Aend-to-end%2520approaches.%2520Furthermore%252C%2520ScreenCoder%2520serves%2520as%2520a%2520scalable%2520data%250Aengine%252C%2520enabling%2520us%2520to%2520generate%2520high-quality%2520image-code%2520pairs.%2520We%2520use%2520this%2520data%250Ato%2520fine-tune%2520open-source%2520MLLM%2520via%2520a%2520dual-stage%2520pipeline%2520of%2520supervised%250Afine-tuning%2520and%2520reinforcement%2520learning%252C%2520demonstrating%2520substantial%2520gains%2520in%2520its%250AUI%2520generation%2520capabilities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%250Aachieves%2520state-of-the-art%2520performance%2520in%2520layout%2520accuracy%252C%2520structural%2520coherence%252C%250Aand%2520code%2520correctness.%2520Our%2520code%2520is%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/leigest519/ScreenCoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22827v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScreenCoder%3A%20Advancing%20Visual-to-Code%20Generation%20for%20Front-End%0A%20%20Automation%20via%20Modular%20Multimodal%20Agents&entry.906535625=Yilei%20Jiang%20and%20Yaozhi%20Zheng%20and%20Yuxuan%20Wan%20and%20Jiaming%20Han%20and%20Qunzhong%20Wang%20and%20Michael%20R.%20Lyu%20and%20Xiangyu%20Yue&entry.1292438233=%20%20Automating%20the%20transformation%20of%20user%20interface%20%28UI%29%20designs%20into%20front-end%0Acode%20holds%20significant%20promise%20for%20accelerating%20software%20development%20and%0Ademocratizing%20design%20workflows.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%0Acan%20translate%20images%20to%20code%2C%20they%20often%20fail%20on%20complex%20UIs%2C%20struggling%20to%0Aunify%20visual%20perception%2C%20layout%20planning%2C%20and%20code%20synthesis%20within%20a%20single%0Amonolithic%20model%2C%20which%20leads%20to%20frequent%20perception%20and%20planning%20errors.%20To%0Aaddress%20this%2C%20we%20propose%20ScreenCoder%2C%20a%20modular%20multi-agent%20framework%20that%0Adecomposes%20the%20task%20into%20three%20interpretable%20stages%3A%20grounding%2C%20planning%2C%20and%0Ageneration.%20By%20assigning%20these%20distinct%20responsibilities%20to%20specialized%20agents%2C%0Aour%20framework%20achieves%20significantly%20higher%20robustness%20and%20fidelity%20than%0Aend-to-end%20approaches.%20Furthermore%2C%20ScreenCoder%20serves%20as%20a%20scalable%20data%0Aengine%2C%20enabling%20us%20to%20generate%20high-quality%20image-code%20pairs.%20We%20use%20this%20data%0Ato%20fine-tune%20open-source%20MLLM%20via%20a%20dual-stage%20pipeline%20of%20supervised%0Afine-tuning%20and%20reinforcement%20learning%2C%20demonstrating%20substantial%20gains%20in%20its%0AUI%20generation%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance%20in%20layout%20accuracy%2C%20structural%20coherence%2C%0Aand%20code%20correctness.%20Our%20code%20is%20made%20publicly%20available%20at%0Ahttps%3A//github.com/leigest519/ScreenCoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22827v2&entry.124074799=Read"},
{"title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads", "author": "Ling Liu and Jun Tian and Li Yi", "abstract": "  4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.\n", "link": "http://arxiv.org/abs/2510.17664v1", "date": "2025-10-20", "relevancy": 2.2415, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5679}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DSegStreamer%3A%20Streaming%204D%20Panoptic%20Segmentation%20via%20Dual%20Threads&body=Title%3A%204DSegStreamer%3A%20Streaming%204D%20Panoptic%20Segmentation%20via%20Dual%20Threads%0AAuthor%3A%20Ling%20Liu%20and%20Jun%20Tian%20and%20Li%20Yi%0AAbstract%3A%20%20%204D%20panoptic%20segmentation%20in%20a%20streaming%20setting%20is%20critical%20for%20highly%0Adynamic%20environments%2C%20such%20as%20evacuating%20dense%20crowds%20and%20autonomous%20driving%20in%0Acomplex%20scenarios%2C%20where%20real-time%2C%20fine-grained%20perception%20within%20a%0Aconstrained%20time%20budget%20is%20essential.%20In%20this%20paper%2C%20we%20introduce%0A4DSegStreamer%2C%20a%20novel%20framework%20that%20employs%20a%20Dual-Thread%20System%20to%0Aefficiently%20process%20streaming%20frames.%20The%20framework%20is%20general%20and%20can%20be%0Aseamlessly%20integrated%20into%20existing%203D%20and%204D%20segmentation%20methods%20to%20enable%0Areal-time%20capability.%20It%20also%20demonstrates%20superior%20robustness%20compared%20to%0Aexisting%20streaming%20perception%20approaches%2C%20particularly%20under%20high%20FPS%0Aconditions.%20The%20system%20consists%20of%20a%20predictive%20thread%20and%20an%20inference%20thread.%0AThe%20predictive%20thread%20leverages%20historical%20motion%20and%20geometric%20information%20to%0Aextract%20features%20and%20forecast%20future%20dynamics.%20The%20inference%20thread%20ensures%0Atimely%20prediction%20for%20incoming%20frames%20by%20aligning%20with%20the%20latest%20memory%20and%0Acompensating%20for%20ego-motion%20and%20dynamic%20object%20movements.%20We%20evaluate%0A4DSegStreamer%20on%20the%20indoor%20HOI4D%20dataset%20and%20the%20outdoor%20SemanticKITTI%20and%0AnuScenes%20datasets.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20particularly%20in%20accurately%20predicting%20dynamic%20objects%20in%20complex%0Ascenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DSegStreamer%253A%2520Streaming%25204D%2520Panoptic%2520Segmentation%2520via%2520Dual%2520Threads%26entry.906535625%3DLing%2520Liu%2520and%2520Jun%2520Tian%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%25204D%2520panoptic%2520segmentation%2520in%2520a%2520streaming%2520setting%2520is%2520critical%2520for%2520highly%250Adynamic%2520environments%252C%2520such%2520as%2520evacuating%2520dense%2520crowds%2520and%2520autonomous%2520driving%2520in%250Acomplex%2520scenarios%252C%2520where%2520real-time%252C%2520fine-grained%2520perception%2520within%2520a%250Aconstrained%2520time%2520budget%2520is%2520essential.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A4DSegStreamer%252C%2520a%2520novel%2520framework%2520that%2520employs%2520a%2520Dual-Thread%2520System%2520to%250Aefficiently%2520process%2520streaming%2520frames.%2520The%2520framework%2520is%2520general%2520and%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520existing%25203D%2520and%25204D%2520segmentation%2520methods%2520to%2520enable%250Areal-time%2520capability.%2520It%2520also%2520demonstrates%2520superior%2520robustness%2520compared%2520to%250Aexisting%2520streaming%2520perception%2520approaches%252C%2520particularly%2520under%2520high%2520FPS%250Aconditions.%2520The%2520system%2520consists%2520of%2520a%2520predictive%2520thread%2520and%2520an%2520inference%2520thread.%250AThe%2520predictive%2520thread%2520leverages%2520historical%2520motion%2520and%2520geometric%2520information%2520to%250Aextract%2520features%2520and%2520forecast%2520future%2520dynamics.%2520The%2520inference%2520thread%2520ensures%250Atimely%2520prediction%2520for%2520incoming%2520frames%2520by%2520aligning%2520with%2520the%2520latest%2520memory%2520and%250Acompensating%2520for%2520ego-motion%2520and%2520dynamic%2520object%2520movements.%2520We%2520evaluate%250A4DSegStreamer%2520on%2520the%2520indoor%2520HOI4D%2520dataset%2520and%2520the%2520outdoor%2520SemanticKITTI%2520and%250AnuScenes%2520datasets.%2520Comprehensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520approach%252C%2520particularly%2520in%2520accurately%2520predicting%2520dynamic%2520objects%2520in%2520complex%250Ascenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DSegStreamer%3A%20Streaming%204D%20Panoptic%20Segmentation%20via%20Dual%20Threads&entry.906535625=Ling%20Liu%20and%20Jun%20Tian%20and%20Li%20Yi&entry.1292438233=%20%204D%20panoptic%20segmentation%20in%20a%20streaming%20setting%20is%20critical%20for%20highly%0Adynamic%20environments%2C%20such%20as%20evacuating%20dense%20crowds%20and%20autonomous%20driving%20in%0Acomplex%20scenarios%2C%20where%20real-time%2C%20fine-grained%20perception%20within%20a%0Aconstrained%20time%20budget%20is%20essential.%20In%20this%20paper%2C%20we%20introduce%0A4DSegStreamer%2C%20a%20novel%20framework%20that%20employs%20a%20Dual-Thread%20System%20to%0Aefficiently%20process%20streaming%20frames.%20The%20framework%20is%20general%20and%20can%20be%0Aseamlessly%20integrated%20into%20existing%203D%20and%204D%20segmentation%20methods%20to%20enable%0Areal-time%20capability.%20It%20also%20demonstrates%20superior%20robustness%20compared%20to%0Aexisting%20streaming%20perception%20approaches%2C%20particularly%20under%20high%20FPS%0Aconditions.%20The%20system%20consists%20of%20a%20predictive%20thread%20and%20an%20inference%20thread.%0AThe%20predictive%20thread%20leverages%20historical%20motion%20and%20geometric%20information%20to%0Aextract%20features%20and%20forecast%20future%20dynamics.%20The%20inference%20thread%20ensures%0Atimely%20prediction%20for%20incoming%20frames%20by%20aligning%20with%20the%20latest%20memory%20and%0Acompensating%20for%20ego-motion%20and%20dynamic%20object%20movements.%20We%20evaluate%0A4DSegStreamer%20on%20the%20indoor%20HOI4D%20dataset%20and%20the%20outdoor%20SemanticKITTI%20and%0AnuScenes%20datasets.%20Comprehensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20approach%2C%20particularly%20in%20accurately%20predicting%20dynamic%20objects%20in%20complex%0Ascenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17664v1&entry.124074799=Read"},
{"title": "Grounded Reinforcement Learning for Visual Reasoning", "author": "Gabriel Sarch and Snigdha Saha and Naitik Khandelwal and Ayush Jain and Michael J. Tarr and Aviral Kumar and Katerina Fragkiadaki", "abstract": "  While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.\n", "link": "http://arxiv.org/abs/2505.23678v2", "date": "2025-10-20", "relevancy": 2.2374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20Reinforcement%20Learning%20for%20Visual%20Reasoning&body=Title%3A%20Grounded%20Reinforcement%20Learning%20for%20Visual%20Reasoning%0AAuthor%3A%20Gabriel%20Sarch%20and%20Snigdha%20Saha%20and%20Naitik%20Khandelwal%20and%20Ayush%20Jain%20and%20Michael%20J.%20Tarr%20and%20Aviral%20Kumar%20and%20Katerina%20Fragkiadaki%0AAbstract%3A%20%20%20While%20reinforcement%20learning%20%28RL%29%20over%20chains%20of%20thought%20has%20significantly%0Aadvanced%20language%20models%20in%20tasks%20such%20as%20mathematics%20and%20coding%2C%20visual%0Areasoning%20introduces%20added%20complexity%20by%20requiring%20models%20to%20direct%20visual%0Aattention%2C%20interpret%20perceptual%20inputs%2C%20and%20ground%20abstract%20reasoning%20in%0Aspatial%20evidence.%20We%20introduce%20ViGoRL%20%28Visually%20Grounded%20Reinforcement%0ALearning%29%2C%20a%20vision-language%20model%20trained%20with%20RL%20to%20explicitly%20anchor%20each%0Areasoning%20step%20to%20specific%20visual%20coordinates.%20Inspired%20by%20human%20visual%0Adecision-making%2C%20ViGoRL%20learns%20to%20produce%20spatially%20grounded%20reasoning%20traces%2C%0Aguiding%20visual%20attention%20to%20task-relevant%20regions%20at%20each%20step.%20When%0Afine-grained%20exploration%20is%20required%2C%20our%20novel%20multi-turn%20RL%20framework%20enables%0Athe%20model%20to%20dynamically%20zoom%20into%20predicted%20coordinates%20as%20reasoning%20unfolds.%0AAcross%20a%20diverse%20set%20of%20visual%20reasoning%20benchmarks--including%20SAT-2%20and%20BLINK%0Afor%20spatial%20reasoning%2C%20V%2Abench%20for%20visual%20search%2C%20and%20ScreenSpot%20and%0AVisualWebArena%20for%20web-based%20grounding--ViGoRL%20consistently%20outperforms%20both%0Asupervised%20fine-tuning%20and%20conventional%20RL%20baselines%20that%20lack%20explicit%0Agrounding%20mechanisms.%20Incorporating%20multi-turn%20RL%20with%20zoomed-in%20visual%0Afeedback%20significantly%20improves%20ViGoRL%27s%20performance%20on%20localizing%20small%20GUI%0Aelements%20and%20visual%20search%2C%20achieving%2086.4%25%20on%20V%2ABench.%20Additionally%2C%20we%20find%0Athat%20grounding%20amplifies%20other%20visual%20behaviors%20such%20as%20region%20exploration%2C%0Agrounded%20subgoal%20setting%2C%20and%20visual%20verification.%20Finally%2C%20human%20evaluations%0Ashow%20that%20the%20model%27s%20visual%20references%20are%20not%20only%20spatially%20accurate%20but%0Aalso%20helpful%20for%20understanding%20model%20reasoning%20steps.%20Our%20results%20show%20that%0Avisually%20grounded%20RL%20is%20a%20strong%20paradigm%20for%20imbuing%20models%20with%0Ageneral-purpose%20visual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520Reinforcement%2520Learning%2520for%2520Visual%2520Reasoning%26entry.906535625%3DGabriel%2520Sarch%2520and%2520Snigdha%2520Saha%2520and%2520Naitik%2520Khandelwal%2520and%2520Ayush%2520Jain%2520and%2520Michael%2520J.%2520Tarr%2520and%2520Aviral%2520Kumar%2520and%2520Katerina%2520Fragkiadaki%26entry.1292438233%3D%2520%2520While%2520reinforcement%2520learning%2520%2528RL%2529%2520over%2520chains%2520of%2520thought%2520has%2520significantly%250Aadvanced%2520language%2520models%2520in%2520tasks%2520such%2520as%2520mathematics%2520and%2520coding%252C%2520visual%250Areasoning%2520introduces%2520added%2520complexity%2520by%2520requiring%2520models%2520to%2520direct%2520visual%250Aattention%252C%2520interpret%2520perceptual%2520inputs%252C%2520and%2520ground%2520abstract%2520reasoning%2520in%250Aspatial%2520evidence.%2520We%2520introduce%2520ViGoRL%2520%2528Visually%2520Grounded%2520Reinforcement%250ALearning%2529%252C%2520a%2520vision-language%2520model%2520trained%2520with%2520RL%2520to%2520explicitly%2520anchor%2520each%250Areasoning%2520step%2520to%2520specific%2520visual%2520coordinates.%2520Inspired%2520by%2520human%2520visual%250Adecision-making%252C%2520ViGoRL%2520learns%2520to%2520produce%2520spatially%2520grounded%2520reasoning%2520traces%252C%250Aguiding%2520visual%2520attention%2520to%2520task-relevant%2520regions%2520at%2520each%2520step.%2520When%250Afine-grained%2520exploration%2520is%2520required%252C%2520our%2520novel%2520multi-turn%2520RL%2520framework%2520enables%250Athe%2520model%2520to%2520dynamically%2520zoom%2520into%2520predicted%2520coordinates%2520as%2520reasoning%2520unfolds.%250AAcross%2520a%2520diverse%2520set%2520of%2520visual%2520reasoning%2520benchmarks--including%2520SAT-2%2520and%2520BLINK%250Afor%2520spatial%2520reasoning%252C%2520V%252Abench%2520for%2520visual%2520search%252C%2520and%2520ScreenSpot%2520and%250AVisualWebArena%2520for%2520web-based%2520grounding--ViGoRL%2520consistently%2520outperforms%2520both%250Asupervised%2520fine-tuning%2520and%2520conventional%2520RL%2520baselines%2520that%2520lack%2520explicit%250Agrounding%2520mechanisms.%2520Incorporating%2520multi-turn%2520RL%2520with%2520zoomed-in%2520visual%250Afeedback%2520significantly%2520improves%2520ViGoRL%2527s%2520performance%2520on%2520localizing%2520small%2520GUI%250Aelements%2520and%2520visual%2520search%252C%2520achieving%252086.4%2525%2520on%2520V%252ABench.%2520Additionally%252C%2520we%2520find%250Athat%2520grounding%2520amplifies%2520other%2520visual%2520behaviors%2520such%2520as%2520region%2520exploration%252C%250Agrounded%2520subgoal%2520setting%252C%2520and%2520visual%2520verification.%2520Finally%252C%2520human%2520evaluations%250Ashow%2520that%2520the%2520model%2527s%2520visual%2520references%2520are%2520not%2520only%2520spatially%2520accurate%2520but%250Aalso%2520helpful%2520for%2520understanding%2520model%2520reasoning%2520steps.%2520Our%2520results%2520show%2520that%250Avisually%2520grounded%2520RL%2520is%2520a%2520strong%2520paradigm%2520for%2520imbuing%2520models%2520with%250Ageneral-purpose%2520visual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20Reinforcement%20Learning%20for%20Visual%20Reasoning&entry.906535625=Gabriel%20Sarch%20and%20Snigdha%20Saha%20and%20Naitik%20Khandelwal%20and%20Ayush%20Jain%20and%20Michael%20J.%20Tarr%20and%20Aviral%20Kumar%20and%20Katerina%20Fragkiadaki&entry.1292438233=%20%20While%20reinforcement%20learning%20%28RL%29%20over%20chains%20of%20thought%20has%20significantly%0Aadvanced%20language%20models%20in%20tasks%20such%20as%20mathematics%20and%20coding%2C%20visual%0Areasoning%20introduces%20added%20complexity%20by%20requiring%20models%20to%20direct%20visual%0Aattention%2C%20interpret%20perceptual%20inputs%2C%20and%20ground%20abstract%20reasoning%20in%0Aspatial%20evidence.%20We%20introduce%20ViGoRL%20%28Visually%20Grounded%20Reinforcement%0ALearning%29%2C%20a%20vision-language%20model%20trained%20with%20RL%20to%20explicitly%20anchor%20each%0Areasoning%20step%20to%20specific%20visual%20coordinates.%20Inspired%20by%20human%20visual%0Adecision-making%2C%20ViGoRL%20learns%20to%20produce%20spatially%20grounded%20reasoning%20traces%2C%0Aguiding%20visual%20attention%20to%20task-relevant%20regions%20at%20each%20step.%20When%0Afine-grained%20exploration%20is%20required%2C%20our%20novel%20multi-turn%20RL%20framework%20enables%0Athe%20model%20to%20dynamically%20zoom%20into%20predicted%20coordinates%20as%20reasoning%20unfolds.%0AAcross%20a%20diverse%20set%20of%20visual%20reasoning%20benchmarks--including%20SAT-2%20and%20BLINK%0Afor%20spatial%20reasoning%2C%20V%2Abench%20for%20visual%20search%2C%20and%20ScreenSpot%20and%0AVisualWebArena%20for%20web-based%20grounding--ViGoRL%20consistently%20outperforms%20both%0Asupervised%20fine-tuning%20and%20conventional%20RL%20baselines%20that%20lack%20explicit%0Agrounding%20mechanisms.%20Incorporating%20multi-turn%20RL%20with%20zoomed-in%20visual%0Afeedback%20significantly%20improves%20ViGoRL%27s%20performance%20on%20localizing%20small%20GUI%0Aelements%20and%20visual%20search%2C%20achieving%2086.4%25%20on%20V%2ABench.%20Additionally%2C%20we%20find%0Athat%20grounding%20amplifies%20other%20visual%20behaviors%20such%20as%20region%20exploration%2C%0Agrounded%20subgoal%20setting%2C%20and%20visual%20verification.%20Finally%2C%20human%20evaluations%0Ashow%20that%20the%20model%27s%20visual%20references%20are%20not%20only%20spatially%20accurate%20but%0Aalso%20helpful%20for%20understanding%20model%20reasoning%20steps.%20Our%20results%20show%20that%0Avisually%20grounded%20RL%20is%20a%20strong%20paradigm%20for%20imbuing%20models%20with%0Ageneral-purpose%20visual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23678v2&entry.124074799=Read"},
{"title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling\n  for Robotic Manipulation", "author": "Yuquan Xue and Guanxing Lu and Zhenyu Wu and Chuanrui Zhang and Bofang Jia and Zhengyi Gu and Yansong Tang and Ziwei Wang", "abstract": "  Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models.\n", "link": "http://arxiv.org/abs/2510.17640v1", "date": "2025-10-20", "relevancy": 2.2367, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6332}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5541}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RESample%3A%20A%20Robust%20Data%20Augmentation%20Framework%20via%20Exploratory%20Sampling%0A%20%20for%20Robotic%20Manipulation&body=Title%3A%20RESample%3A%20A%20Robust%20Data%20Augmentation%20Framework%20via%20Exploratory%20Sampling%0A%20%20for%20Robotic%20Manipulation%0AAuthor%3A%20Yuquan%20Xue%20and%20Guanxing%20Lu%20and%20Zhenyu%20Wu%20and%20Chuanrui%20Zhang%20and%20Bofang%20Jia%20and%20Zhengyi%20Gu%20and%20Yansong%20Tang%20and%20Ziwei%20Wang%0AAbstract%3A%20%20%20Vision-Language-Action%20models%20%28VLAs%29%20have%20demonstrated%20remarkable%20performance%0Aon%20complex%20robotic%20manipulation%20tasks%20through%20imitation%20learning.%20However%2C%0Aexisting%20imitation%20learning%20datasets%20contain%20only%20successful%20trajectories%20and%0Alack%20failure%20or%20recovery%20data%2C%20especially%20for%20out-of-distribution%20%28OOD%29%20states%0Awhere%20the%20robot%20deviates%20from%20the%20main%20policy%20due%20to%20minor%20perturbations%20or%0Aerrors%2C%20leading%20VLA%20models%20to%20struggle%20with%20states%20deviating%20from%20the%20training%0Adistribution.%20To%20this%20end%2C%20we%20propose%20an%20automated%20OOD%20data%20augmentation%0Aframework%20named%20RESample%20through%20exploratory%20sampling.%20Specifically%2C%20we%20first%0Aleverage%20offline%20reinforcement%20learning%20to%20obtain%20an%20action-value%20network%20that%0Aaccurately%20identifies%20sub-optimal%20actions%20under%20the%20current%20manipulation%0Apolicy.%20We%20further%20sample%20potential%20OOD%20states%20from%20trajectories%20via%20rollout%2C%0Aand%20design%20an%20exploratory%20sampling%20mechanism%20that%20adaptively%20incorporates%20these%0Aaction%20proxies%20into%20the%20training%20dataset%20to%20ensure%20efficiency.%20Subsequently%2C%0Aour%20framework%20explicitly%20encourages%20the%20VLAs%20to%20recover%20from%20OOD%20states%20and%0Aenhances%20their%20robustness%20against%20distributional%20shifts.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20LIBERO%20benchmark%20as%20well%20as%20real-world%20robotic%20manipulation%0Atasks%2C%20demonstrating%20that%20RESample%20consistently%20improves%20the%20stability%20and%0Ageneralization%20ability%20of%20VLA%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRESample%253A%2520A%2520Robust%2520Data%2520Augmentation%2520Framework%2520via%2520Exploratory%2520Sampling%250A%2520%2520for%2520Robotic%2520Manipulation%26entry.906535625%3DYuquan%2520Xue%2520and%2520Guanxing%2520Lu%2520and%2520Zhenyu%2520Wu%2520and%2520Chuanrui%2520Zhang%2520and%2520Bofang%2520Jia%2520and%2520Zhengyi%2520Gu%2520and%2520Yansong%2520Tang%2520and%2520Ziwei%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520models%2520%2528VLAs%2529%2520have%2520demonstrated%2520remarkable%2520performance%250Aon%2520complex%2520robotic%2520manipulation%2520tasks%2520through%2520imitation%2520learning.%2520However%252C%250Aexisting%2520imitation%2520learning%2520datasets%2520contain%2520only%2520successful%2520trajectories%2520and%250Alack%2520failure%2520or%2520recovery%2520data%252C%2520especially%2520for%2520out-of-distribution%2520%2528OOD%2529%2520states%250Awhere%2520the%2520robot%2520deviates%2520from%2520the%2520main%2520policy%2520due%2520to%2520minor%2520perturbations%2520or%250Aerrors%252C%2520leading%2520VLA%2520models%2520to%2520struggle%2520with%2520states%2520deviating%2520from%2520the%2520training%250Adistribution.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520automated%2520OOD%2520data%2520augmentation%250Aframework%2520named%2520RESample%2520through%2520exploratory%2520sampling.%2520Specifically%252C%2520we%2520first%250Aleverage%2520offline%2520reinforcement%2520learning%2520to%2520obtain%2520an%2520action-value%2520network%2520that%250Aaccurately%2520identifies%2520sub-optimal%2520actions%2520under%2520the%2520current%2520manipulation%250Apolicy.%2520We%2520further%2520sample%2520potential%2520OOD%2520states%2520from%2520trajectories%2520via%2520rollout%252C%250Aand%2520design%2520an%2520exploratory%2520sampling%2520mechanism%2520that%2520adaptively%2520incorporates%2520these%250Aaction%2520proxies%2520into%2520the%2520training%2520dataset%2520to%2520ensure%2520efficiency.%2520Subsequently%252C%250Aour%2520framework%2520explicitly%2520encourages%2520the%2520VLAs%2520to%2520recover%2520from%2520OOD%2520states%2520and%250Aenhances%2520their%2520robustness%2520against%2520distributional%2520shifts.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520the%2520LIBERO%2520benchmark%2520as%2520well%2520as%2520real-world%2520robotic%2520manipulation%250Atasks%252C%2520demonstrating%2520that%2520RESample%2520consistently%2520improves%2520the%2520stability%2520and%250Ageneralization%2520ability%2520of%2520VLA%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RESample%3A%20A%20Robust%20Data%20Augmentation%20Framework%20via%20Exploratory%20Sampling%0A%20%20for%20Robotic%20Manipulation&entry.906535625=Yuquan%20Xue%20and%20Guanxing%20Lu%20and%20Zhenyu%20Wu%20and%20Chuanrui%20Zhang%20and%20Bofang%20Jia%20and%20Zhengyi%20Gu%20and%20Yansong%20Tang%20and%20Ziwei%20Wang&entry.1292438233=%20%20Vision-Language-Action%20models%20%28VLAs%29%20have%20demonstrated%20remarkable%20performance%0Aon%20complex%20robotic%20manipulation%20tasks%20through%20imitation%20learning.%20However%2C%0Aexisting%20imitation%20learning%20datasets%20contain%20only%20successful%20trajectories%20and%0Alack%20failure%20or%20recovery%20data%2C%20especially%20for%20out-of-distribution%20%28OOD%29%20states%0Awhere%20the%20robot%20deviates%20from%20the%20main%20policy%20due%20to%20minor%20perturbations%20or%0Aerrors%2C%20leading%20VLA%20models%20to%20struggle%20with%20states%20deviating%20from%20the%20training%0Adistribution.%20To%20this%20end%2C%20we%20propose%20an%20automated%20OOD%20data%20augmentation%0Aframework%20named%20RESample%20through%20exploratory%20sampling.%20Specifically%2C%20we%20first%0Aleverage%20offline%20reinforcement%20learning%20to%20obtain%20an%20action-value%20network%20that%0Aaccurately%20identifies%20sub-optimal%20actions%20under%20the%20current%20manipulation%0Apolicy.%20We%20further%20sample%20potential%20OOD%20states%20from%20trajectories%20via%20rollout%2C%0Aand%20design%20an%20exploratory%20sampling%20mechanism%20that%20adaptively%20incorporates%20these%0Aaction%20proxies%20into%20the%20training%20dataset%20to%20ensure%20efficiency.%20Subsequently%2C%0Aour%20framework%20explicitly%20encourages%20the%20VLAs%20to%20recover%20from%20OOD%20states%20and%0Aenhances%20their%20robustness%20against%20distributional%20shifts.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20LIBERO%20benchmark%20as%20well%20as%20real-world%20robotic%20manipulation%0Atasks%2C%20demonstrating%20that%20RESample%20consistently%20improves%20the%20stability%20and%0Ageneralization%20ability%20of%20VLA%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17640v1&entry.124074799=Read"},
{"title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "author": "Samir Khaki and Junxian Guo and Jiaming Tang and Shang Yang and Yukang Chen and Konstantinos N. Plataniotis and Yao Lu and Song Han and Zhijian Liu", "abstract": "  Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.\n", "link": "http://arxiv.org/abs/2510.17777v1", "date": "2025-10-20", "relevancy": 2.2285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseVILA%3A%20Decoupling%20Visual%20Sparsity%20for%20Efficient%20VLM%20Inference&body=Title%3A%20SparseVILA%3A%20Decoupling%20Visual%20Sparsity%20for%20Efficient%20VLM%20Inference%0AAuthor%3A%20Samir%20Khaki%20and%20Junxian%20Guo%20and%20Jiaming%20Tang%20and%20Shang%20Yang%20and%20Yukang%20Chen%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Lu%20and%20Song%20Han%20and%20Zhijian%20Liu%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20rapidly%20advanced%20in%20integrating%20visual%20and%0Atextual%20reasoning%2C%20powering%20applications%20across%20high-resolution%20image%0Aunderstanding%2C%20long-video%20analysis%2C%20and%20multi-turn%20conversation.%20However%2C%20their%0Ascalability%20remains%20limited%20by%20the%20growing%20number%20of%20visual%20tokens%20that%0Adominate%20inference%20latency.%20We%20present%20SparseVILA%2C%20a%20new%20paradigm%20for%20efficient%0AVLM%20inference%20that%20decouples%20visual%20sparsity%20across%20the%20prefilling%20and%20decoding%0Astages.%20SparseVILA%20distributes%20sparsity%20across%20stages%20by%20pruning%20redundant%0Avisual%20tokens%20during%20prefill%20and%20retrieving%20only%20query-relevant%20tokens%20during%0Adecoding.%20This%20decoupled%20design%20matches%20leading%20prefill%20pruning%20methods%20while%0Apreserving%20multi-turn%20fidelity%20by%20retaining%20most%20of%20the%20visual%20cache%20so%20that%0Aquery-aware%20tokens%20can%20be%20retrieved%20at%20each%20conversation%20round.%20Built%20on%20an%0AAWQ-optimized%20inference%20pipeline%2C%20SparseVILA%20achieves%20up%20to%204.0%20times%20faster%0Aprefilling%2C%202.5%20times%20faster%20decoding%2C%20and%20an%20overall%202.6%20times%20end-to-end%0Aspeedup%20on%20long-context%20video%20tasks%20--%20while%20improving%20accuracy%20on%0Adocument-understanding%20and%20reasoning%20tasks.%20By%20decoupling%20query-agnostic%0Apruning%20and%20query-aware%20retrieval%2C%20SparseVILA%20establishes%20a%20new%20direction%20for%0Aefficient%20multimodal%20inference%2C%20offering%20a%20training-free%2C%20architecture-agnostic%0Aframework%20for%20accelerating%20large%20VLMs%20without%20sacrificing%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseVILA%253A%2520Decoupling%2520Visual%2520Sparsity%2520for%2520Efficient%2520VLM%2520Inference%26entry.906535625%3DSamir%2520Khaki%2520and%2520Junxian%2520Guo%2520and%2520Jiaming%2520Tang%2520and%2520Shang%2520Yang%2520and%2520Yukang%2520Chen%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%2520and%2520Zhijian%2520Liu%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520rapidly%2520advanced%2520in%2520integrating%2520visual%2520and%250Atextual%2520reasoning%252C%2520powering%2520applications%2520across%2520high-resolution%2520image%250Aunderstanding%252C%2520long-video%2520analysis%252C%2520and%2520multi-turn%2520conversation.%2520However%252C%2520their%250Ascalability%2520remains%2520limited%2520by%2520the%2520growing%2520number%2520of%2520visual%2520tokens%2520that%250Adominate%2520inference%2520latency.%2520We%2520present%2520SparseVILA%252C%2520a%2520new%2520paradigm%2520for%2520efficient%250AVLM%2520inference%2520that%2520decouples%2520visual%2520sparsity%2520across%2520the%2520prefilling%2520and%2520decoding%250Astages.%2520SparseVILA%2520distributes%2520sparsity%2520across%2520stages%2520by%2520pruning%2520redundant%250Avisual%2520tokens%2520during%2520prefill%2520and%2520retrieving%2520only%2520query-relevant%2520tokens%2520during%250Adecoding.%2520This%2520decoupled%2520design%2520matches%2520leading%2520prefill%2520pruning%2520methods%2520while%250Apreserving%2520multi-turn%2520fidelity%2520by%2520retaining%2520most%2520of%2520the%2520visual%2520cache%2520so%2520that%250Aquery-aware%2520tokens%2520can%2520be%2520retrieved%2520at%2520each%2520conversation%2520round.%2520Built%2520on%2520an%250AAWQ-optimized%2520inference%2520pipeline%252C%2520SparseVILA%2520achieves%2520up%2520to%25204.0%2520times%2520faster%250Aprefilling%252C%25202.5%2520times%2520faster%2520decoding%252C%2520and%2520an%2520overall%25202.6%2520times%2520end-to-end%250Aspeedup%2520on%2520long-context%2520video%2520tasks%2520--%2520while%2520improving%2520accuracy%2520on%250Adocument-understanding%2520and%2520reasoning%2520tasks.%2520By%2520decoupling%2520query-agnostic%250Apruning%2520and%2520query-aware%2520retrieval%252C%2520SparseVILA%2520establishes%2520a%2520new%2520direction%2520for%250Aefficient%2520multimodal%2520inference%252C%2520offering%2520a%2520training-free%252C%2520architecture-agnostic%250Aframework%2520for%2520accelerating%2520large%2520VLMs%2520without%2520sacrificing%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseVILA%3A%20Decoupling%20Visual%20Sparsity%20for%20Efficient%20VLM%20Inference&entry.906535625=Samir%20Khaki%20and%20Junxian%20Guo%20and%20Jiaming%20Tang%20and%20Shang%20Yang%20and%20Yukang%20Chen%20and%20Konstantinos%20N.%20Plataniotis%20and%20Yao%20Lu%20and%20Song%20Han%20and%20Zhijian%20Liu&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20have%20rapidly%20advanced%20in%20integrating%20visual%20and%0Atextual%20reasoning%2C%20powering%20applications%20across%20high-resolution%20image%0Aunderstanding%2C%20long-video%20analysis%2C%20and%20multi-turn%20conversation.%20However%2C%20their%0Ascalability%20remains%20limited%20by%20the%20growing%20number%20of%20visual%20tokens%20that%0Adominate%20inference%20latency.%20We%20present%20SparseVILA%2C%20a%20new%20paradigm%20for%20efficient%0AVLM%20inference%20that%20decouples%20visual%20sparsity%20across%20the%20prefilling%20and%20decoding%0Astages.%20SparseVILA%20distributes%20sparsity%20across%20stages%20by%20pruning%20redundant%0Avisual%20tokens%20during%20prefill%20and%20retrieving%20only%20query-relevant%20tokens%20during%0Adecoding.%20This%20decoupled%20design%20matches%20leading%20prefill%20pruning%20methods%20while%0Apreserving%20multi-turn%20fidelity%20by%20retaining%20most%20of%20the%20visual%20cache%20so%20that%0Aquery-aware%20tokens%20can%20be%20retrieved%20at%20each%20conversation%20round.%20Built%20on%20an%0AAWQ-optimized%20inference%20pipeline%2C%20SparseVILA%20achieves%20up%20to%204.0%20times%20faster%0Aprefilling%2C%202.5%20times%20faster%20decoding%2C%20and%20an%20overall%202.6%20times%20end-to-end%0Aspeedup%20on%20long-context%20video%20tasks%20--%20while%20improving%20accuracy%20on%0Adocument-understanding%20and%20reasoning%20tasks.%20By%20decoupling%20query-agnostic%0Apruning%20and%20query-aware%20retrieval%2C%20SparseVILA%20establishes%20a%20new%20direction%20for%0Aefficient%20multimodal%20inference%2C%20offering%20a%20training-free%2C%20architecture-agnostic%0Aframework%20for%20accelerating%20large%20VLMs%20without%20sacrificing%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17777v1&entry.124074799=Read"},
{"title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration", "author": "Yehonathan Refael and Amit Aides and Aviad Barzilai and George Leifman and Genady Beryozkin and Vered Silverman and Bolous Jaber and Tomer Shekel", "abstract": "  Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.\n", "link": "http://arxiv.org/abs/2510.17670v1", "date": "2025-10-20", "relevancy": 2.2182, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-the-Fly%20OVD%20Adaptation%20with%20FLAME%3A%20Few-shot%20Localization%20via%20Active%0A%20%20Marginal-Samples%20Exploration&body=Title%3A%20On-the-Fly%20OVD%20Adaptation%20with%20FLAME%3A%20Few-shot%20Localization%20via%20Active%0A%20%20Marginal-Samples%20Exploration%0AAuthor%3A%20Yehonathan%20Refael%20and%20Amit%20Aides%20and%20Aviad%20Barzilai%20and%20George%20Leifman%20and%20Genady%20Beryozkin%20and%20Vered%20Silverman%20and%20Bolous%20Jaber%20and%20Tomer%20Shekel%0AAbstract%3A%20%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20models%20offer%20remarkable%20flexibility%20by%0Adetecting%20objects%20from%20arbitrary%20text%20queries.%20However%2C%20their%20zero-shot%0Aperformance%20in%20specialized%20domains%20like%20Remote%20Sensing%20%28RS%29%20is%20often%0Acompromised%20by%20the%20inherent%20ambiguity%20of%20natural%20language%2C%20limiting%20critical%0Adownstream%20applications.%20For%20instance%2C%20an%20OVD%20model%20may%20struggle%20to%20distinguish%0Abetween%20fine-grained%20classes%20such%20as%20%22fishing%20boat%22%20and%20%22yacht%22%20since%20their%0Aembeddings%20are%20similar%20and%20often%20inseparable.%20This%20can%20hamper%20specific%20user%0Agoals%2C%20such%20as%20monitoring%20illegal%20fishing%2C%20by%20producing%20irrelevant%20detections.%0ATo%20address%20this%2C%20we%20propose%20a%20cascaded%20approach%20that%20couples%20the%20broad%0Ageneralization%20of%20a%20large%20pre-trained%20OVD%20model%20with%20a%20lightweight%20few-shot%0Aclassifier.%20Our%20method%20first%20employs%20the%20zero-shot%20model%20to%20generate%0Ahigh-recall%20object%20proposals.%20These%20proposals%20are%20then%20refined%20for%20high%0Aprecision%20by%20a%20compact%20classifier%20trained%20in%20real-time%20on%20only%20a%20handful%20of%0Auser-annotated%20examples%20-%20drastically%20reducing%20the%20high%20costs%20of%20RS%20imagery%0Aannotation.The%20core%20of%20our%20framework%20is%20FLAME%2C%20a%20one-step%20active%20learning%0Astrategy%20that%20selects%20the%20most%20informative%20samples%20for%20training.%20FLAME%0Aidentifies%2C%20on%20the%20fly%2C%20uncertain%20marginal%20candidates%20near%20the%20decision%0Aboundary%20using%20density%20estimation%2C%20followed%20by%20clustering%20to%20ensure%20sample%0Adiversity.%20This%20efficient%20sampling%20technique%20achieves%20high%20accuracy%20without%0Acostly%20full-model%20fine-tuning%20and%20enables%20instant%20adaptation%2C%20within%20less%20then%0Aa%20minute%2C%20which%20is%20significantly%20faster%20than%20state-of-the-art%20alternatives.Our%0Amethod%20consistently%20surpasses%20state-of-the-art%20performance%20on%20RS%20benchmarks%2C%0Aestablishing%20a%20practical%20and%20resource-efficient%20framework%20for%20adapting%0Afoundation%20models%20to%20specific%20user%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-the-Fly%2520OVD%2520Adaptation%2520with%2520FLAME%253A%2520Few-shot%2520Localization%2520via%2520Active%250A%2520%2520Marginal-Samples%2520Exploration%26entry.906535625%3DYehonathan%2520Refael%2520and%2520Amit%2520Aides%2520and%2520Aviad%2520Barzilai%2520and%2520George%2520Leifman%2520and%2520Genady%2520Beryozkin%2520and%2520Vered%2520Silverman%2520and%2520Bolous%2520Jaber%2520and%2520Tomer%2520Shekel%26entry.1292438233%3D%2520%2520Open-vocabulary%2520object%2520detection%2520%2528OVD%2529%2520models%2520offer%2520remarkable%2520flexibility%2520by%250Adetecting%2520objects%2520from%2520arbitrary%2520text%2520queries.%2520However%252C%2520their%2520zero-shot%250Aperformance%2520in%2520specialized%2520domains%2520like%2520Remote%2520Sensing%2520%2528RS%2529%2520is%2520often%250Acompromised%2520by%2520the%2520inherent%2520ambiguity%2520of%2520natural%2520language%252C%2520limiting%2520critical%250Adownstream%2520applications.%2520For%2520instance%252C%2520an%2520OVD%2520model%2520may%2520struggle%2520to%2520distinguish%250Abetween%2520fine-grained%2520classes%2520such%2520as%2520%2522fishing%2520boat%2522%2520and%2520%2522yacht%2522%2520since%2520their%250Aembeddings%2520are%2520similar%2520and%2520often%2520inseparable.%2520This%2520can%2520hamper%2520specific%2520user%250Agoals%252C%2520such%2520as%2520monitoring%2520illegal%2520fishing%252C%2520by%2520producing%2520irrelevant%2520detections.%250ATo%2520address%2520this%252C%2520we%2520propose%2520a%2520cascaded%2520approach%2520that%2520couples%2520the%2520broad%250Ageneralization%2520of%2520a%2520large%2520pre-trained%2520OVD%2520model%2520with%2520a%2520lightweight%2520few-shot%250Aclassifier.%2520Our%2520method%2520first%2520employs%2520the%2520zero-shot%2520model%2520to%2520generate%250Ahigh-recall%2520object%2520proposals.%2520These%2520proposals%2520are%2520then%2520refined%2520for%2520high%250Aprecision%2520by%2520a%2520compact%2520classifier%2520trained%2520in%2520real-time%2520on%2520only%2520a%2520handful%2520of%250Auser-annotated%2520examples%2520-%2520drastically%2520reducing%2520the%2520high%2520costs%2520of%2520RS%2520imagery%250Aannotation.The%2520core%2520of%2520our%2520framework%2520is%2520FLAME%252C%2520a%2520one-step%2520active%2520learning%250Astrategy%2520that%2520selects%2520the%2520most%2520informative%2520samples%2520for%2520training.%2520FLAME%250Aidentifies%252C%2520on%2520the%2520fly%252C%2520uncertain%2520marginal%2520candidates%2520near%2520the%2520decision%250Aboundary%2520using%2520density%2520estimation%252C%2520followed%2520by%2520clustering%2520to%2520ensure%2520sample%250Adiversity.%2520This%2520efficient%2520sampling%2520technique%2520achieves%2520high%2520accuracy%2520without%250Acostly%2520full-model%2520fine-tuning%2520and%2520enables%2520instant%2520adaptation%252C%2520within%2520less%2520then%250Aa%2520minute%252C%2520which%2520is%2520significantly%2520faster%2520than%2520state-of-the-art%2520alternatives.Our%250Amethod%2520consistently%2520surpasses%2520state-of-the-art%2520performance%2520on%2520RS%2520benchmarks%252C%250Aestablishing%2520a%2520practical%2520and%2520resource-efficient%2520framework%2520for%2520adapting%250Afoundation%2520models%2520to%2520specific%2520user%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-the-Fly%20OVD%20Adaptation%20with%20FLAME%3A%20Few-shot%20Localization%20via%20Active%0A%20%20Marginal-Samples%20Exploration&entry.906535625=Yehonathan%20Refael%20and%20Amit%20Aides%20and%20Aviad%20Barzilai%20and%20George%20Leifman%20and%20Genady%20Beryozkin%20and%20Vered%20Silverman%20and%20Bolous%20Jaber%20and%20Tomer%20Shekel&entry.1292438233=%20%20Open-vocabulary%20object%20detection%20%28OVD%29%20models%20offer%20remarkable%20flexibility%20by%0Adetecting%20objects%20from%20arbitrary%20text%20queries.%20However%2C%20their%20zero-shot%0Aperformance%20in%20specialized%20domains%20like%20Remote%20Sensing%20%28RS%29%20is%20often%0Acompromised%20by%20the%20inherent%20ambiguity%20of%20natural%20language%2C%20limiting%20critical%0Adownstream%20applications.%20For%20instance%2C%20an%20OVD%20model%20may%20struggle%20to%20distinguish%0Abetween%20fine-grained%20classes%20such%20as%20%22fishing%20boat%22%20and%20%22yacht%22%20since%20their%0Aembeddings%20are%20similar%20and%20often%20inseparable.%20This%20can%20hamper%20specific%20user%0Agoals%2C%20such%20as%20monitoring%20illegal%20fishing%2C%20by%20producing%20irrelevant%20detections.%0ATo%20address%20this%2C%20we%20propose%20a%20cascaded%20approach%20that%20couples%20the%20broad%0Ageneralization%20of%20a%20large%20pre-trained%20OVD%20model%20with%20a%20lightweight%20few-shot%0Aclassifier.%20Our%20method%20first%20employs%20the%20zero-shot%20model%20to%20generate%0Ahigh-recall%20object%20proposals.%20These%20proposals%20are%20then%20refined%20for%20high%0Aprecision%20by%20a%20compact%20classifier%20trained%20in%20real-time%20on%20only%20a%20handful%20of%0Auser-annotated%20examples%20-%20drastically%20reducing%20the%20high%20costs%20of%20RS%20imagery%0Aannotation.The%20core%20of%20our%20framework%20is%20FLAME%2C%20a%20one-step%20active%20learning%0Astrategy%20that%20selects%20the%20most%20informative%20samples%20for%20training.%20FLAME%0Aidentifies%2C%20on%20the%20fly%2C%20uncertain%20marginal%20candidates%20near%20the%20decision%0Aboundary%20using%20density%20estimation%2C%20followed%20by%20clustering%20to%20ensure%20sample%0Adiversity.%20This%20efficient%20sampling%20technique%20achieves%20high%20accuracy%20without%0Acostly%20full-model%20fine-tuning%20and%20enables%20instant%20adaptation%2C%20within%20less%20then%0Aa%20minute%2C%20which%20is%20significantly%20faster%20than%20state-of-the-art%20alternatives.Our%0Amethod%20consistently%20surpasses%20state-of-the-art%20performance%20on%20RS%20benchmarks%2C%0Aestablishing%20a%20practical%20and%20resource-efficient%20framework%20for%20adapting%0Afoundation%20models%20to%20specific%20user%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17670v1&entry.124074799=Read"},
{"title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image\n  Segmentation Foundation Model", "author": "Xinwei Zhang and Hu Chen and Zhe Yuan and Sukun Tian and Peng Feng", "abstract": "  Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.\n", "link": "http://arxiv.org/abs/2510.17684v1", "date": "2025-10-20", "relevancy": 2.21, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Communication%20Mixture-of-Experts%20Boosted-Medical%20Image%0A%20%20Segmentation%20Foundation%20Model&body=Title%3A%20Intelligent%20Communication%20Mixture-of-Experts%20Boosted-Medical%20Image%0A%20%20Segmentation%20Foundation%20Model%0AAuthor%3A%20Xinwei%20Zhang%20and%20Hu%20Chen%20and%20Zhe%20Yuan%20and%20Sukun%20Tian%20and%20Peng%20Feng%0AAbstract%3A%20%20%20Foundation%20models%20for%20medical%20image%20segmentation%20have%20achieved%20remarkable%0Aperformance.%20Adaptive%20fine-tuning%20of%20natural%20image%20segmentation%20foundation%0Amodels%20is%20crucial%20for%20medical%20image%20segmentation%20tasks.%20However%2C%20some%0Alimitations%20exist%20in%20existing%20fine-tuning%20methods%3A%201%29%20insufficient%0Arepresentation%20of%20high-level%20features%20and%202%29%20the%20fine-tuning%20process%20disrupts%0Athe%20structural%20integrity%20of%20pretrained%20weights.%20Inspired%20by%20these%20critical%0Aproblems%2C%20we%20propose%20an%20intelligent%20communication%20mixture-of-experts%0Aboosted-medical%20image%20segmentation%20foundation%20model%2C%20named%20IC-MoE%2C%20with%20twofold%0Aideas%3A%201%29%20We%20construct%20basic%20experts%2C%20semantic%20experts%2C%20and%20adaptive%20experts.%0AMoreover%2C%20we%20implement%20a%20pixel%20probability%20adaptive%20voting%20strategy%2C%20which%0Aenables%20expert%20selection%20and%20fusion%20through%20label%20consistency%20and%20load%0Abalancing.%20This%20approach%20preliminarily%20enhances%20the%20representation%20capability%0Aof%20high-level%20features%20while%20preserving%20the%20structural%20integrity%20of%20pretrained%0Aweights.%202%29%20We%20propose%20a%20semantic-guided%20contrastive%20learning%20method%20to%20address%0Athe%20issue%20of%20weak%20supervision%20in%20contrastive%20learning.%20This%20method%20further%0Aenhances%20the%20representation%20capability%20of%20high-level%20features%20while%20preserving%0Athe%20structural%20integrity%20of%20pretrained%20weights.%20Extensive%20experiments%20across%0Athree%20public%20medical%20image%20segmentation%20datasets%20demonstrate%20that%20the%20IC-MoE%0Aoutperforms%20other%20SOTA%20models.%20Consequently%2C%20the%20proposed%20IC-MoE%20effectively%0Asupplements%20foundational%20medical%20image%20segmentation%20models%20with%20high-level%0Afeatures%20and%20pretrained%20structural%20integrity.%20We%20also%20validate%20the%20superior%0Ageneralizability%20of%20the%20IC-MoE%20across%20diverse%20medical%20image%20segmentation%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Communication%2520Mixture-of-Experts%2520Boosted-Medical%2520Image%250A%2520%2520Segmentation%2520Foundation%2520Model%26entry.906535625%3DXinwei%2520Zhang%2520and%2520Hu%2520Chen%2520and%2520Zhe%2520Yuan%2520and%2520Sukun%2520Tian%2520and%2520Peng%2520Feng%26entry.1292438233%3D%2520%2520Foundation%2520models%2520for%2520medical%2520image%2520segmentation%2520have%2520achieved%2520remarkable%250Aperformance.%2520Adaptive%2520fine-tuning%2520of%2520natural%2520image%2520segmentation%2520foundation%250Amodels%2520is%2520crucial%2520for%2520medical%2520image%2520segmentation%2520tasks.%2520However%252C%2520some%250Alimitations%2520exist%2520in%2520existing%2520fine-tuning%2520methods%253A%25201%2529%2520insufficient%250Arepresentation%2520of%2520high-level%2520features%2520and%25202%2529%2520the%2520fine-tuning%2520process%2520disrupts%250Athe%2520structural%2520integrity%2520of%2520pretrained%2520weights.%2520Inspired%2520by%2520these%2520critical%250Aproblems%252C%2520we%2520propose%2520an%2520intelligent%2520communication%2520mixture-of-experts%250Aboosted-medical%2520image%2520segmentation%2520foundation%2520model%252C%2520named%2520IC-MoE%252C%2520with%2520twofold%250Aideas%253A%25201%2529%2520We%2520construct%2520basic%2520experts%252C%2520semantic%2520experts%252C%2520and%2520adaptive%2520experts.%250AMoreover%252C%2520we%2520implement%2520a%2520pixel%2520probability%2520adaptive%2520voting%2520strategy%252C%2520which%250Aenables%2520expert%2520selection%2520and%2520fusion%2520through%2520label%2520consistency%2520and%2520load%250Abalancing.%2520This%2520approach%2520preliminarily%2520enhances%2520the%2520representation%2520capability%250Aof%2520high-level%2520features%2520while%2520preserving%2520the%2520structural%2520integrity%2520of%2520pretrained%250Aweights.%25202%2529%2520We%2520propose%2520a%2520semantic-guided%2520contrastive%2520learning%2520method%2520to%2520address%250Athe%2520issue%2520of%2520weak%2520supervision%2520in%2520contrastive%2520learning.%2520This%2520method%2520further%250Aenhances%2520the%2520representation%2520capability%2520of%2520high-level%2520features%2520while%2520preserving%250Athe%2520structural%2520integrity%2520of%2520pretrained%2520weights.%2520Extensive%2520experiments%2520across%250Athree%2520public%2520medical%2520image%2520segmentation%2520datasets%2520demonstrate%2520that%2520the%2520IC-MoE%250Aoutperforms%2520other%2520SOTA%2520models.%2520Consequently%252C%2520the%2520proposed%2520IC-MoE%2520effectively%250Asupplements%2520foundational%2520medical%2520image%2520segmentation%2520models%2520with%2520high-level%250Afeatures%2520and%2520pretrained%2520structural%2520integrity.%2520We%2520also%2520validate%2520the%2520superior%250Ageneralizability%2520of%2520the%2520IC-MoE%2520across%2520diverse%2520medical%2520image%2520segmentation%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Communication%20Mixture-of-Experts%20Boosted-Medical%20Image%0A%20%20Segmentation%20Foundation%20Model&entry.906535625=Xinwei%20Zhang%20and%20Hu%20Chen%20and%20Zhe%20Yuan%20and%20Sukun%20Tian%20and%20Peng%20Feng&entry.1292438233=%20%20Foundation%20models%20for%20medical%20image%20segmentation%20have%20achieved%20remarkable%0Aperformance.%20Adaptive%20fine-tuning%20of%20natural%20image%20segmentation%20foundation%0Amodels%20is%20crucial%20for%20medical%20image%20segmentation%20tasks.%20However%2C%20some%0Alimitations%20exist%20in%20existing%20fine-tuning%20methods%3A%201%29%20insufficient%0Arepresentation%20of%20high-level%20features%20and%202%29%20the%20fine-tuning%20process%20disrupts%0Athe%20structural%20integrity%20of%20pretrained%20weights.%20Inspired%20by%20these%20critical%0Aproblems%2C%20we%20propose%20an%20intelligent%20communication%20mixture-of-experts%0Aboosted-medical%20image%20segmentation%20foundation%20model%2C%20named%20IC-MoE%2C%20with%20twofold%0Aideas%3A%201%29%20We%20construct%20basic%20experts%2C%20semantic%20experts%2C%20and%20adaptive%20experts.%0AMoreover%2C%20we%20implement%20a%20pixel%20probability%20adaptive%20voting%20strategy%2C%20which%0Aenables%20expert%20selection%20and%20fusion%20through%20label%20consistency%20and%20load%0Abalancing.%20This%20approach%20preliminarily%20enhances%20the%20representation%20capability%0Aof%20high-level%20features%20while%20preserving%20the%20structural%20integrity%20of%20pretrained%0Aweights.%202%29%20We%20propose%20a%20semantic-guided%20contrastive%20learning%20method%20to%20address%0Athe%20issue%20of%20weak%20supervision%20in%20contrastive%20learning.%20This%20method%20further%0Aenhances%20the%20representation%20capability%20of%20high-level%20features%20while%20preserving%0Athe%20structural%20integrity%20of%20pretrained%20weights.%20Extensive%20experiments%20across%0Athree%20public%20medical%20image%20segmentation%20datasets%20demonstrate%20that%20the%20IC-MoE%0Aoutperforms%20other%20SOTA%20models.%20Consequently%2C%20the%20proposed%20IC-MoE%20effectively%0Asupplements%20foundational%20medical%20image%20segmentation%20models%20with%20high-level%0Afeatures%20and%20pretrained%20structural%20integrity.%20We%20also%20validate%20the%20superior%0Ageneralizability%20of%20the%20IC-MoE%20across%20diverse%20medical%20image%20segmentation%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17684v1&entry.124074799=Read"},
{"title": "Glyph: Scaling Context Windows via Visual-Text Compression", "author": "Jiale Cheng and Yusen Liu and Xinyu Zhang and Yulin Fei and Wenyi Hong and Ruiliang Lyu and Weihan Wang and Zhe Su and Xiaotao Gu and Xiao Liu and Yushi Bai and Jie Tang and Hongning Wang and Minlie Huang", "abstract": "  Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.\n", "link": "http://arxiv.org/abs/2510.17800v2", "date": "2025-10-21", "relevancy": 2.2066, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&body=Title%3A%20Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression%0AAuthor%3A%20Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17800v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlyph%253A%2520Scaling%2520Context%2520Windows%2520via%2520Visual-Text%2520Compression%26entry.906535625%3DJiale%2520Cheng%2520and%2520Yusen%2520Liu%2520and%2520Xinyu%2520Zhang%2520and%2520Yulin%2520Fei%2520and%2520Wenyi%2520Hong%2520and%2520Ruiliang%2520Lyu%2520and%2520Weihan%2520Wang%2520and%2520Zhe%2520Su%2520and%2520Xiaotao%2520Gu%2520and%2520Xiao%2520Liu%2520and%2520Yushi%2520Bai%2520and%2520Jie%2520Tang%2520and%2520Hongning%2520Wang%2520and%2520Minlie%2520Huang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520rely%2520on%2520long-context%2520modeling%2520for%250Atasks%2520such%2520as%2520document%2520understanding%252C%2520code%2520analysis%252C%2520and%2520multi-step%2520reasoning.%250AHowever%252C%2520scaling%2520context%2520windows%2520to%2520the%2520million-token%2520level%2520brings%2520prohibitive%250Acomputational%2520and%2520memory%2520costs%252C%2520limiting%2520the%2520practicality%2520of%2520long-context%2520LLMs.%250AIn%2520this%2520work%252C%2520we%2520take%2520a%2520different%2520perspective-visual%2520context%2520scaling-to%2520tackle%250Athis%2520challenge.%2520Instead%2520of%2520extending%2520token-based%2520sequences%252C%2520we%2520propose%2520Glyph%252C%2520a%250Aframework%2520that%2520renders%2520long%2520texts%2520into%2520images%2520and%2520processes%2520them%2520with%250Avision-language%2520models%2520%2528VLMs%2529.%2520This%2520approach%2520substantially%2520compresses%2520textual%250Ainput%2520while%2520preserving%2520semantic%2520information%252C%2520and%2520we%2520further%2520design%2520an%250ALLM-driven%2520genetic%2520search%2520to%2520identify%2520optimal%2520visual%2520rendering%2520configurations%250Afor%2520balancing%2520accuracy%2520and%2520compression.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520achieves%25203-4x%2520token%2520compression%2520while%2520maintaining%250Aaccuracy%2520comparable%2520to%2520leading%2520LLMs%2520such%2520as%2520Qwen3-8B%2520on%2520various%2520long-context%250Abenchmarks.%2520This%2520compression%2520also%2520leads%2520to%2520around%25204x%2520faster%2520prefilling%2520and%250Adecoding%252C%2520and%2520approximately%25202x%2520faster%2520SFT%2520training.%2520Furthermore%252C%2520under%2520extreme%250Acompression%252C%2520a%2520128K-context%2520VLM%2520could%2520scale%2520to%2520handle%25201M-token-level%2520text%250Atasks.%2520In%2520addition%252C%2520the%2520rendered%2520text%2520data%2520benefits%2520real-world%2520multimodal%250Atasks%252C%2520such%2520as%2520document%2520understanding.%2520Our%2520code%2520and%2520model%2520are%2520released%2520at%250Ahttps%253A//github.com/thu-coai/Glyph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17800v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glyph%3A%20Scaling%20Context%20Windows%20via%20Visual-Text%20Compression&entry.906535625=Jiale%20Cheng%20and%20Yusen%20Liu%20and%20Xinyu%20Zhang%20and%20Yulin%20Fei%20and%20Wenyi%20Hong%20and%20Ruiliang%20Lyu%20and%20Weihan%20Wang%20and%20Zhe%20Su%20and%20Xiaotao%20Gu%20and%20Xiao%20Liu%20and%20Yushi%20Bai%20and%20Jie%20Tang%20and%20Hongning%20Wang%20and%20Minlie%20Huang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20rely%20on%20long-context%20modeling%20for%0Atasks%20such%20as%20document%20understanding%2C%20code%20analysis%2C%20and%20multi-step%20reasoning.%0AHowever%2C%20scaling%20context%20windows%20to%20the%20million-token%20level%20brings%20prohibitive%0Acomputational%20and%20memory%20costs%2C%20limiting%20the%20practicality%20of%20long-context%20LLMs.%0AIn%20this%20work%2C%20we%20take%20a%20different%20perspective-visual%20context%20scaling-to%20tackle%0Athis%20challenge.%20Instead%20of%20extending%20token-based%20sequences%2C%20we%20propose%20Glyph%2C%20a%0Aframework%20that%20renders%20long%20texts%20into%20images%20and%20processes%20them%20with%0Avision-language%20models%20%28VLMs%29.%20This%20approach%20substantially%20compresses%20textual%0Ainput%20while%20preserving%20semantic%20information%2C%20and%20we%20further%20design%20an%0ALLM-driven%20genetic%20search%20to%20identify%20optimal%20visual%20rendering%20configurations%0Afor%20balancing%20accuracy%20and%20compression.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%203-4x%20token%20compression%20while%20maintaining%0Aaccuracy%20comparable%20to%20leading%20LLMs%20such%20as%20Qwen3-8B%20on%20various%20long-context%0Abenchmarks.%20This%20compression%20also%20leads%20to%20around%204x%20faster%20prefilling%20and%0Adecoding%2C%20and%20approximately%202x%20faster%20SFT%20training.%20Furthermore%2C%20under%20extreme%0Acompression%2C%20a%20128K-context%20VLM%20could%20scale%20to%20handle%201M-token-level%20text%0Atasks.%20In%20addition%2C%20the%20rendered%20text%20data%20benefits%20real-world%20multimodal%0Atasks%2C%20such%20as%20document%20understanding.%20Our%20code%20and%20model%20are%20released%20at%0Ahttps%3A//github.com/thu-coai/Glyph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17800v2&entry.124074799=Read"},
{"title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language\n  Models", "author": "Qilin Liao and Anamika Lochab and Ruqi Zhang", "abstract": "  Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.\n", "link": "http://arxiv.org/abs/2510.17759v1", "date": "2025-10-20", "relevancy": 2.179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERA-V%3A%20Variational%20Inference%20Framework%20for%20Jailbreaking%20Vision-Language%0A%20%20Models&body=Title%3A%20VERA-V%3A%20Variational%20Inference%20Framework%20for%20Jailbreaking%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Qilin%20Liao%20and%20Anamika%20Lochab%20and%20Ruqi%20Zhang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20extend%20large%20language%20models%20with%20visual%0Areasoning%2C%20but%20their%20multimodal%20design%20also%20introduces%20new%2C%20underexplored%0Avulnerabilities.%20Existing%20multimodal%20red-teaming%20methods%20largely%20rely%20on%0Abrittle%20templates%2C%20focus%20on%20single-attack%20settings%2C%20and%20expose%20only%20a%20narrow%0Asubset%20of%20vulnerabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20VERA-V%2C%20a%0Avariational%20inference%20framework%20that%20recasts%20multimodal%20jailbreak%20discovery%20as%0Alearning%20a%20joint%20posterior%20distribution%20over%20paired%20text-image%20prompts.%20This%0Aprobabilistic%20view%20enables%20the%20generation%20of%20stealthy%2C%20coupled%20adversarial%0Ainputs%20that%20bypass%20model%20guardrails.%20We%20train%20a%20lightweight%20attacker%20to%0Aapproximate%20the%20posterior%2C%20allowing%20efficient%20sampling%20of%20diverse%20jailbreaks%0Aand%20providing%20distributional%20insights%20into%20vulnerabilities.%20VERA-V%20further%0Aintegrates%20three%20complementary%20strategies%3A%20%28i%29%20typography-based%20text%20prompts%0Athat%20embed%20harmful%20cues%2C%20%28ii%29%20diffusion-based%20image%20synthesis%20that%20introduces%0Aadversarial%20signals%2C%20and%20%28iii%29%20structured%20distractors%20to%20fragment%20VLM%0Aattention.%20Experiments%20on%20HarmBench%20and%20HADES%20benchmarks%20show%20that%20VERA-V%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20on%20both%20open-source%20and%0Afrontier%20VLMs%2C%20achieving%20up%20to%2053.75%25%20higher%20attack%20success%20rate%20%28ASR%29%20over%20the%0Abest%20baseline%20on%20GPT-4o.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERA-V%253A%2520Variational%2520Inference%2520Framework%2520for%2520Jailbreaking%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DQilin%2520Liao%2520and%2520Anamika%2520Lochab%2520and%2520Ruqi%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520extend%2520large%2520language%2520models%2520with%2520visual%250Areasoning%252C%2520but%2520their%2520multimodal%2520design%2520also%2520introduces%2520new%252C%2520underexplored%250Avulnerabilities.%2520Existing%2520multimodal%2520red-teaming%2520methods%2520largely%2520rely%2520on%250Abrittle%2520templates%252C%2520focus%2520on%2520single-attack%2520settings%252C%2520and%2520expose%2520only%2520a%2520narrow%250Asubset%2520of%2520vulnerabilities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520VERA-V%252C%2520a%250Avariational%2520inference%2520framework%2520that%2520recasts%2520multimodal%2520jailbreak%2520discovery%2520as%250Alearning%2520a%2520joint%2520posterior%2520distribution%2520over%2520paired%2520text-image%2520prompts.%2520This%250Aprobabilistic%2520view%2520enables%2520the%2520generation%2520of%2520stealthy%252C%2520coupled%2520adversarial%250Ainputs%2520that%2520bypass%2520model%2520guardrails.%2520We%2520train%2520a%2520lightweight%2520attacker%2520to%250Aapproximate%2520the%2520posterior%252C%2520allowing%2520efficient%2520sampling%2520of%2520diverse%2520jailbreaks%250Aand%2520providing%2520distributional%2520insights%2520into%2520vulnerabilities.%2520VERA-V%2520further%250Aintegrates%2520three%2520complementary%2520strategies%253A%2520%2528i%2529%2520typography-based%2520text%2520prompts%250Athat%2520embed%2520harmful%2520cues%252C%2520%2528ii%2529%2520diffusion-based%2520image%2520synthesis%2520that%2520introduces%250Aadversarial%2520signals%252C%2520and%2520%2528iii%2529%2520structured%2520distractors%2520to%2520fragment%2520VLM%250Aattention.%2520Experiments%2520on%2520HarmBench%2520and%2520HADES%2520benchmarks%2520show%2520that%2520VERA-V%250Aconsistently%2520outperforms%2520state-of-the-art%2520baselines%2520on%2520both%2520open-source%2520and%250Afrontier%2520VLMs%252C%2520achieving%2520up%2520to%252053.75%2525%2520higher%2520attack%2520success%2520rate%2520%2528ASR%2529%2520over%2520the%250Abest%2520baseline%2520on%2520GPT-4o.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERA-V%3A%20Variational%20Inference%20Framework%20for%20Jailbreaking%20Vision-Language%0A%20%20Models&entry.906535625=Qilin%20Liao%20and%20Anamika%20Lochab%20and%20Ruqi%20Zhang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20extend%20large%20language%20models%20with%20visual%0Areasoning%2C%20but%20their%20multimodal%20design%20also%20introduces%20new%2C%20underexplored%0Avulnerabilities.%20Existing%20multimodal%20red-teaming%20methods%20largely%20rely%20on%0Abrittle%20templates%2C%20focus%20on%20single-attack%20settings%2C%20and%20expose%20only%20a%20narrow%0Asubset%20of%20vulnerabilities.%20To%20address%20these%20limitations%2C%20we%20introduce%20VERA-V%2C%20a%0Avariational%20inference%20framework%20that%20recasts%20multimodal%20jailbreak%20discovery%20as%0Alearning%20a%20joint%20posterior%20distribution%20over%20paired%20text-image%20prompts.%20This%0Aprobabilistic%20view%20enables%20the%20generation%20of%20stealthy%2C%20coupled%20adversarial%0Ainputs%20that%20bypass%20model%20guardrails.%20We%20train%20a%20lightweight%20attacker%20to%0Aapproximate%20the%20posterior%2C%20allowing%20efficient%20sampling%20of%20diverse%20jailbreaks%0Aand%20providing%20distributional%20insights%20into%20vulnerabilities.%20VERA-V%20further%0Aintegrates%20three%20complementary%20strategies%3A%20%28i%29%20typography-based%20text%20prompts%0Athat%20embed%20harmful%20cues%2C%20%28ii%29%20diffusion-based%20image%20synthesis%20that%20introduces%0Aadversarial%20signals%2C%20and%20%28iii%29%20structured%20distractors%20to%20fragment%20VLM%0Aattention.%20Experiments%20on%20HarmBench%20and%20HADES%20benchmarks%20show%20that%20VERA-V%0Aconsistently%20outperforms%20state-of-the-art%20baselines%20on%20both%20open-source%20and%0Afrontier%20VLMs%2C%20achieving%20up%20to%2053.75%25%20higher%20attack%20success%20rate%20%28ASR%29%20over%20the%0Abest%20baseline%20on%20GPT-4o.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17759v1&entry.124074799=Read"},
{"title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation", "author": "Fr\u00e9d\u00e9ric LIN and Biruk Abere Ambaw and Adrian Popescu and Hejer Ammar and Romaric Audigier and Herv\u00e9 Le Borgne", "abstract": "  AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.\n", "link": "http://arxiv.org/abs/2510.17626v2", "date": "2025-10-21", "relevancy": 2.1763, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation&body=Title%3A%20CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation%0AAuthor%3A%20Fr%C3%A9d%C3%A9ric%20LIN%20and%20Biruk%20Abere%20Ambaw%20and%20Adrian%20Popescu%20and%20Hejer%20Ammar%20and%20Romaric%20Audigier%20and%20Herv%C3%A9%20Le%20Borgne%0AAbstract%3A%20%20%20AI%20systems%20must%20adapt%20to%20evolving%20visual%20environments%2C%20especially%20in%20domains%0Awhere%20object%20appearances%20change%20over%20time.%20We%20introduce%20Car%20Models%20in%20Time%0A%28CaMiT%29%2C%20a%20fine-grained%20dataset%20capturing%20the%20temporal%20evolution%20of%20car%20models%2C%0Aa%20representative%20class%20of%20technological%20artifacts.%20CaMiT%20includes%20787K%20labeled%0Asamples%20of%20190%20car%20models%20%282007-2023%29%20and%205.1M%20unlabeled%20samples%20%282005-2023%29%2C%0Asupporting%20both%20supervised%20and%20self-supervised%20learning.%20Static%20pretraining%20on%0Ain-domain%20data%20achieves%20competitive%20performance%20with%20large-scale%20generalist%0Amodels%20while%20being%20more%20resource-efficient%2C%20yet%20accuracy%20declines%20when%20models%0Aare%20tested%20across%20years.%20To%20address%20this%2C%20we%20propose%20a%20time-incremental%0Aclassification%20setting%2C%20a%20realistic%20continual%20learning%20scenario%20with%20emerging%2C%0Aevolving%2C%20and%20disappearing%20classes.%20We%20evaluate%20two%20strategies%3A%0Atime-incremental%20pretraining%2C%20which%20updates%20the%20backbone%2C%20and%20time-incremental%0Aclassifier%20learning%2C%20which%20updates%20only%20the%20final%20layer%2C%20both%20improving%0Atemporal%20robustness.%20Finally%2C%20we%20explore%20time-aware%20image%20generation%20that%0Aleverages%20temporal%20metadata%20during%20training%2C%20yielding%20more%20realistic%20outputs.%0ACaMiT%20offers%20a%20rich%20benchmark%20for%20studying%20temporal%20adaptation%20in%20fine-grained%0Avisual%20recognition%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaMiT%253A%2520A%2520Time-Aware%2520Car%2520Model%2520Dataset%2520for%2520Classification%2520and%2520Generation%26entry.906535625%3DFr%25C3%25A9d%25C3%25A9ric%2520LIN%2520and%2520Biruk%2520Abere%2520Ambaw%2520and%2520Adrian%2520Popescu%2520and%2520Hejer%2520Ammar%2520and%2520Romaric%2520Audigier%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%26entry.1292438233%3D%2520%2520AI%2520systems%2520must%2520adapt%2520to%2520evolving%2520visual%2520environments%252C%2520especially%2520in%2520domains%250Awhere%2520object%2520appearances%2520change%2520over%2520time.%2520We%2520introduce%2520Car%2520Models%2520in%2520Time%250A%2528CaMiT%2529%252C%2520a%2520fine-grained%2520dataset%2520capturing%2520the%2520temporal%2520evolution%2520of%2520car%2520models%252C%250Aa%2520representative%2520class%2520of%2520technological%2520artifacts.%2520CaMiT%2520includes%2520787K%2520labeled%250Asamples%2520of%2520190%2520car%2520models%2520%25282007-2023%2529%2520and%25205.1M%2520unlabeled%2520samples%2520%25282005-2023%2529%252C%250Asupporting%2520both%2520supervised%2520and%2520self-supervised%2520learning.%2520Static%2520pretraining%2520on%250Ain-domain%2520data%2520achieves%2520competitive%2520performance%2520with%2520large-scale%2520generalist%250Amodels%2520while%2520being%2520more%2520resource-efficient%252C%2520yet%2520accuracy%2520declines%2520when%2520models%250Aare%2520tested%2520across%2520years.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520time-incremental%250Aclassification%2520setting%252C%2520a%2520realistic%2520continual%2520learning%2520scenario%2520with%2520emerging%252C%250Aevolving%252C%2520and%2520disappearing%2520classes.%2520We%2520evaluate%2520two%2520strategies%253A%250Atime-incremental%2520pretraining%252C%2520which%2520updates%2520the%2520backbone%252C%2520and%2520time-incremental%250Aclassifier%2520learning%252C%2520which%2520updates%2520only%2520the%2520final%2520layer%252C%2520both%2520improving%250Atemporal%2520robustness.%2520Finally%252C%2520we%2520explore%2520time-aware%2520image%2520generation%2520that%250Aleverages%2520temporal%2520metadata%2520during%2520training%252C%2520yielding%2520more%2520realistic%2520outputs.%250ACaMiT%2520offers%2520a%2520rich%2520benchmark%2520for%2520studying%2520temporal%2520adaptation%2520in%2520fine-grained%250Avisual%2520recognition%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaMiT%3A%20A%20Time-Aware%20Car%20Model%20Dataset%20for%20Classification%20and%20Generation&entry.906535625=Fr%C3%A9d%C3%A9ric%20LIN%20and%20Biruk%20Abere%20Ambaw%20and%20Adrian%20Popescu%20and%20Hejer%20Ammar%20and%20Romaric%20Audigier%20and%20Herv%C3%A9%20Le%20Borgne&entry.1292438233=%20%20AI%20systems%20must%20adapt%20to%20evolving%20visual%20environments%2C%20especially%20in%20domains%0Awhere%20object%20appearances%20change%20over%20time.%20We%20introduce%20Car%20Models%20in%20Time%0A%28CaMiT%29%2C%20a%20fine-grained%20dataset%20capturing%20the%20temporal%20evolution%20of%20car%20models%2C%0Aa%20representative%20class%20of%20technological%20artifacts.%20CaMiT%20includes%20787K%20labeled%0Asamples%20of%20190%20car%20models%20%282007-2023%29%20and%205.1M%20unlabeled%20samples%20%282005-2023%29%2C%0Asupporting%20both%20supervised%20and%20self-supervised%20learning.%20Static%20pretraining%20on%0Ain-domain%20data%20achieves%20competitive%20performance%20with%20large-scale%20generalist%0Amodels%20while%20being%20more%20resource-efficient%2C%20yet%20accuracy%20declines%20when%20models%0Aare%20tested%20across%20years.%20To%20address%20this%2C%20we%20propose%20a%20time-incremental%0Aclassification%20setting%2C%20a%20realistic%20continual%20learning%20scenario%20with%20emerging%2C%0Aevolving%2C%20and%20disappearing%20classes.%20We%20evaluate%20two%20strategies%3A%0Atime-incremental%20pretraining%2C%20which%20updates%20the%20backbone%2C%20and%20time-incremental%0Aclassifier%20learning%2C%20which%20updates%20only%20the%20final%20layer%2C%20both%20improving%0Atemporal%20robustness.%20Finally%2C%20we%20explore%20time-aware%20image%20generation%20that%0Aleverages%20temporal%20metadata%20during%20training%2C%20yielding%20more%20realistic%20outputs.%0ACaMiT%20offers%20a%20rich%20benchmark%20for%20studying%20temporal%20adaptation%20in%20fine-grained%0Avisual%20recognition%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17626v2&entry.124074799=Read"},
{"title": "Navigating the Latent Space Dynamics of Neural Models", "author": "Marco Fumero and Luca Moschella and Emanuele Rodol\u00e0 and Francesco Locatello", "abstract": "  Neural networks transform high-dimensional data into compact, structured\nrepresentations, often modeled as elements of a lower dimensional latent space.\nIn this paper, we present an alternative interpretation of neural models as\ndynamical systems acting on the latent manifold. Specifically, we show that\nautoencoder models implicitly define a latent vector field on the manifold,\nderived by iteratively applying the encoding-decoding map, without any\nadditional training. We observe that standard training procedures introduce\ninductive biases that lead to the emergence of attractor points within this\nvector field. Drawing on this insight, we propose to leverage the vector field\nas a representation for the network, providing a novel tool to analyze the\nproperties of the model and the data. This representation enables to: (i)\nanalyze the generalization and memorization regimes of neural models, even\nthroughout training; (ii) extract prior knowledge encoded in the network's\nparameters from the attractors, without requiring any input data; (iii)\nidentify out-of-distribution samples from their trajectories in the vector\nfield. We further validate our approach on vision foundation models, showcasing\nthe applicability and effectiveness of our method in real-world scenarios.\n", "link": "http://arxiv.org/abs/2505.22785v3", "date": "2025-10-20", "relevancy": 2.1754, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5543}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.548}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20the%20Latent%20Space%20Dynamics%20of%20Neural%20Models&body=Title%3A%20Navigating%20the%20Latent%20Space%20Dynamics%20of%20Neural%20Models%0AAuthor%3A%20Marco%20Fumero%20and%20Luca%20Moschella%20and%20Emanuele%20Rodol%C3%A0%20and%20Francesco%20Locatello%0AAbstract%3A%20%20%20Neural%20networks%20transform%20high-dimensional%20data%20into%20compact%2C%20structured%0Arepresentations%2C%20often%20modeled%20as%20elements%20of%20a%20lower%20dimensional%20latent%20space.%0AIn%20this%20paper%2C%20we%20present%20an%20alternative%20interpretation%20of%20neural%20models%20as%0Adynamical%20systems%20acting%20on%20the%20latent%20manifold.%20Specifically%2C%20we%20show%20that%0Aautoencoder%20models%20implicitly%20define%20a%20latent%20vector%20field%20on%20the%20manifold%2C%0Aderived%20by%20iteratively%20applying%20the%20encoding-decoding%20map%2C%20without%20any%0Aadditional%20training.%20We%20observe%20that%20standard%20training%20procedures%20introduce%0Ainductive%20biases%20that%20lead%20to%20the%20emergence%20of%20attractor%20points%20within%20this%0Avector%20field.%20Drawing%20on%20this%20insight%2C%20we%20propose%20to%20leverage%20the%20vector%20field%0Aas%20a%20representation%20for%20the%20network%2C%20providing%20a%20novel%20tool%20to%20analyze%20the%0Aproperties%20of%20the%20model%20and%20the%20data.%20This%20representation%20enables%20to%3A%20%28i%29%0Aanalyze%20the%20generalization%20and%20memorization%20regimes%20of%20neural%20models%2C%20even%0Athroughout%20training%3B%20%28ii%29%20extract%20prior%20knowledge%20encoded%20in%20the%20network%27s%0Aparameters%20from%20the%20attractors%2C%20without%20requiring%20any%20input%20data%3B%20%28iii%29%0Aidentify%20out-of-distribution%20samples%20from%20their%20trajectories%20in%20the%20vector%0Afield.%20We%20further%20validate%20our%20approach%20on%20vision%20foundation%20models%2C%20showcasing%0Athe%20applicability%20and%20effectiveness%20of%20our%20method%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22785v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520the%2520Latent%2520Space%2520Dynamics%2520of%2520Neural%2520Models%26entry.906535625%3DMarco%2520Fumero%2520and%2520Luca%2520Moschella%2520and%2520Emanuele%2520Rodol%25C3%25A0%2520and%2520Francesco%2520Locatello%26entry.1292438233%3D%2520%2520Neural%2520networks%2520transform%2520high-dimensional%2520data%2520into%2520compact%252C%2520structured%250Arepresentations%252C%2520often%2520modeled%2520as%2520elements%2520of%2520a%2520lower%2520dimensional%2520latent%2520space.%250AIn%2520this%2520paper%252C%2520we%2520present%2520an%2520alternative%2520interpretation%2520of%2520neural%2520models%2520as%250Adynamical%2520systems%2520acting%2520on%2520the%2520latent%2520manifold.%2520Specifically%252C%2520we%2520show%2520that%250Aautoencoder%2520models%2520implicitly%2520define%2520a%2520latent%2520vector%2520field%2520on%2520the%2520manifold%252C%250Aderived%2520by%2520iteratively%2520applying%2520the%2520encoding-decoding%2520map%252C%2520without%2520any%250Aadditional%2520training.%2520We%2520observe%2520that%2520standard%2520training%2520procedures%2520introduce%250Ainductive%2520biases%2520that%2520lead%2520to%2520the%2520emergence%2520of%2520attractor%2520points%2520within%2520this%250Avector%2520field.%2520Drawing%2520on%2520this%2520insight%252C%2520we%2520propose%2520to%2520leverage%2520the%2520vector%2520field%250Aas%2520a%2520representation%2520for%2520the%2520network%252C%2520providing%2520a%2520novel%2520tool%2520to%2520analyze%2520the%250Aproperties%2520of%2520the%2520model%2520and%2520the%2520data.%2520This%2520representation%2520enables%2520to%253A%2520%2528i%2529%250Aanalyze%2520the%2520generalization%2520and%2520memorization%2520regimes%2520of%2520neural%2520models%252C%2520even%250Athroughout%2520training%253B%2520%2528ii%2529%2520extract%2520prior%2520knowledge%2520encoded%2520in%2520the%2520network%2527s%250Aparameters%2520from%2520the%2520attractors%252C%2520without%2520requiring%2520any%2520input%2520data%253B%2520%2528iii%2529%250Aidentify%2520out-of-distribution%2520samples%2520from%2520their%2520trajectories%2520in%2520the%2520vector%250Afield.%2520We%2520further%2520validate%2520our%2520approach%2520on%2520vision%2520foundation%2520models%252C%2520showcasing%250Athe%2520applicability%2520and%2520effectiveness%2520of%2520our%2520method%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22785v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20the%20Latent%20Space%20Dynamics%20of%20Neural%20Models&entry.906535625=Marco%20Fumero%20and%20Luca%20Moschella%20and%20Emanuele%20Rodol%C3%A0%20and%20Francesco%20Locatello&entry.1292438233=%20%20Neural%20networks%20transform%20high-dimensional%20data%20into%20compact%2C%20structured%0Arepresentations%2C%20often%20modeled%20as%20elements%20of%20a%20lower%20dimensional%20latent%20space.%0AIn%20this%20paper%2C%20we%20present%20an%20alternative%20interpretation%20of%20neural%20models%20as%0Adynamical%20systems%20acting%20on%20the%20latent%20manifold.%20Specifically%2C%20we%20show%20that%0Aautoencoder%20models%20implicitly%20define%20a%20latent%20vector%20field%20on%20the%20manifold%2C%0Aderived%20by%20iteratively%20applying%20the%20encoding-decoding%20map%2C%20without%20any%0Aadditional%20training.%20We%20observe%20that%20standard%20training%20procedures%20introduce%0Ainductive%20biases%20that%20lead%20to%20the%20emergence%20of%20attractor%20points%20within%20this%0Avector%20field.%20Drawing%20on%20this%20insight%2C%20we%20propose%20to%20leverage%20the%20vector%20field%0Aas%20a%20representation%20for%20the%20network%2C%20providing%20a%20novel%20tool%20to%20analyze%20the%0Aproperties%20of%20the%20model%20and%20the%20data.%20This%20representation%20enables%20to%3A%20%28i%29%0Aanalyze%20the%20generalization%20and%20memorization%20regimes%20of%20neural%20models%2C%20even%0Athroughout%20training%3B%20%28ii%29%20extract%20prior%20knowledge%20encoded%20in%20the%20network%27s%0Aparameters%20from%20the%20attractors%2C%20without%20requiring%20any%20input%20data%3B%20%28iii%29%0Aidentify%20out-of-distribution%20samples%20from%20their%20trajectories%20in%20the%20vector%0Afield.%20We%20further%20validate%20our%20approach%20on%20vision%20foundation%20models%2C%20showcasing%0Athe%20applicability%20and%20effectiveness%20of%20our%20method%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22785v3&entry.124074799=Read"},
{"title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data\n  Augmentation for Robust Lung Ultrasound Classification", "author": "Athanasios Angelakis and Amne Mousa and Micah L. A. Heldeweg and Laurens A. Biesheuvel and Mark A. Haaksma and Jasper M. Smit and Pieter R. Tuinman and Paul W. G. Elbers", "abstract": "  Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging.\n", "link": "http://arxiv.org/abs/2510.17650v1", "date": "2025-10-20", "relevancy": 2.167, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.535}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZACH-ViT%3A%20A%20Zero-Token%20Vision%20Transformer%20with%20ShuffleStrides%20Data%0A%20%20Augmentation%20for%20Robust%20Lung%20Ultrasound%20Classification&body=Title%3A%20ZACH-ViT%3A%20A%20Zero-Token%20Vision%20Transformer%20with%20ShuffleStrides%20Data%0A%20%20Augmentation%20for%20Robust%20Lung%20Ultrasound%20Classification%0AAuthor%3A%20Athanasios%20Angelakis%20and%20Amne%20Mousa%20and%20Micah%20L.%20A.%20Heldeweg%20and%20Laurens%20A.%20Biesheuvel%20and%20Mark%20A.%20Haaksma%20and%20Jasper%20M.%20Smit%20and%20Pieter%20R.%20Tuinman%20and%20Paul%20W.%20G.%20Elbers%0AAbstract%3A%20%20%20Differentiating%20cardiogenic%20pulmonary%20oedema%20%28CPE%29%20from%20non-cardiogenic%20and%0Astructurally%20normal%20lungs%20in%20lung%20ultrasound%20%28LUS%29%20videos%20remains%20challenging%0Adue%20to%20the%20high%20visual%20variability%20of%20non-cardiogenic%20inflammatory%20patterns%0A%28NCIP/ARDS-like%29%2C%20interstitial%20lung%20disease%2C%20and%20healthy%20lungs.%20This%0Aheterogeneity%20complicates%20automated%20classification%20as%20overlapping%20B-lines%20and%0Apleural%20artefacts%20are%20common.%20We%20introduce%20ZACH-ViT%20%28Zero-token%20Adaptive%0ACompact%20Hierarchical%20Vision%20Transformer%29%2C%20a%200.25%20M-parameter%20Vision%20Transformer%0Avariant%20that%20removes%20both%20positional%20embeddings%20and%20the%20%5BCLS%5D%20token%2C%20making%20it%0Afully%20permutation-invariant%20and%20suitable%20for%20unordered%20medical%20image%20data.%20To%0Aenhance%20generalization%2C%20we%20propose%20ShuffleStrides%20Data%20Augmentation%20%28SSDA%29%2C%0Awhich%20permutes%20probe-view%20sequences%20and%20frame%20orders%20while%20preserving%0Aanatomical%20validity.%20ZACH-ViT%20was%20evaluated%20on%20380%20LUS%20videos%20from%2095%0Acritically%20ill%20patients%20against%20nine%20state-of-the-art%20baselines.%20Despite%20the%0Aheterogeneity%20of%20the%20non-cardiogenic%20group%2C%20ZACH-ViT%20achieved%20the%20highest%0Avalidation%20and%20test%20ROC-AUC%20%280.80%20and%200.79%29%20with%20balanced%20sensitivity%20%280.60%29%0Aand%20specificity%20%280.91%29%2C%20while%20all%20competing%20models%20collapsed%20to%20trivial%0Aclassification.%20It%20trains%201.35x%20faster%20than%20Minimal%20ViT%20%280.62M%20parameters%29%20with%0A2.5x%20fewer%20parameters%2C%20supporting%20real-time%20clinical%20deployment.%20These%20results%0Ashow%20that%20aligning%20architectural%20design%20with%20data%20structure%20can%20outperform%0Ascale%20in%20small-data%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZACH-ViT%253A%2520A%2520Zero-Token%2520Vision%2520Transformer%2520with%2520ShuffleStrides%2520Data%250A%2520%2520Augmentation%2520for%2520Robust%2520Lung%2520Ultrasound%2520Classification%26entry.906535625%3DAthanasios%2520Angelakis%2520and%2520Amne%2520Mousa%2520and%2520Micah%2520L.%2520A.%2520Heldeweg%2520and%2520Laurens%2520A.%2520Biesheuvel%2520and%2520Mark%2520A.%2520Haaksma%2520and%2520Jasper%2520M.%2520Smit%2520and%2520Pieter%2520R.%2520Tuinman%2520and%2520Paul%2520W.%2520G.%2520Elbers%26entry.1292438233%3D%2520%2520Differentiating%2520cardiogenic%2520pulmonary%2520oedema%2520%2528CPE%2529%2520from%2520non-cardiogenic%2520and%250Astructurally%2520normal%2520lungs%2520in%2520lung%2520ultrasound%2520%2528LUS%2529%2520videos%2520remains%2520challenging%250Adue%2520to%2520the%2520high%2520visual%2520variability%2520of%2520non-cardiogenic%2520inflammatory%2520patterns%250A%2528NCIP/ARDS-like%2529%252C%2520interstitial%2520lung%2520disease%252C%2520and%2520healthy%2520lungs.%2520This%250Aheterogeneity%2520complicates%2520automated%2520classification%2520as%2520overlapping%2520B-lines%2520and%250Apleural%2520artefacts%2520are%2520common.%2520We%2520introduce%2520ZACH-ViT%2520%2528Zero-token%2520Adaptive%250ACompact%2520Hierarchical%2520Vision%2520Transformer%2529%252C%2520a%25200.25%2520M-parameter%2520Vision%2520Transformer%250Avariant%2520that%2520removes%2520both%2520positional%2520embeddings%2520and%2520the%2520%255BCLS%255D%2520token%252C%2520making%2520it%250Afully%2520permutation-invariant%2520and%2520suitable%2520for%2520unordered%2520medical%2520image%2520data.%2520To%250Aenhance%2520generalization%252C%2520we%2520propose%2520ShuffleStrides%2520Data%2520Augmentation%2520%2528SSDA%2529%252C%250Awhich%2520permutes%2520probe-view%2520sequences%2520and%2520frame%2520orders%2520while%2520preserving%250Aanatomical%2520validity.%2520ZACH-ViT%2520was%2520evaluated%2520on%2520380%2520LUS%2520videos%2520from%252095%250Acritically%2520ill%2520patients%2520against%2520nine%2520state-of-the-art%2520baselines.%2520Despite%2520the%250Aheterogeneity%2520of%2520the%2520non-cardiogenic%2520group%252C%2520ZACH-ViT%2520achieved%2520the%2520highest%250Avalidation%2520and%2520test%2520ROC-AUC%2520%25280.80%2520and%25200.79%2529%2520with%2520balanced%2520sensitivity%2520%25280.60%2529%250Aand%2520specificity%2520%25280.91%2529%252C%2520while%2520all%2520competing%2520models%2520collapsed%2520to%2520trivial%250Aclassification.%2520It%2520trains%25201.35x%2520faster%2520than%2520Minimal%2520ViT%2520%25280.62M%2520parameters%2529%2520with%250A2.5x%2520fewer%2520parameters%252C%2520supporting%2520real-time%2520clinical%2520deployment.%2520These%2520results%250Ashow%2520that%2520aligning%2520architectural%2520design%2520with%2520data%2520structure%2520can%2520outperform%250Ascale%2520in%2520small-data%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZACH-ViT%3A%20A%20Zero-Token%20Vision%20Transformer%20with%20ShuffleStrides%20Data%0A%20%20Augmentation%20for%20Robust%20Lung%20Ultrasound%20Classification&entry.906535625=Athanasios%20Angelakis%20and%20Amne%20Mousa%20and%20Micah%20L.%20A.%20Heldeweg%20and%20Laurens%20A.%20Biesheuvel%20and%20Mark%20A.%20Haaksma%20and%20Jasper%20M.%20Smit%20and%20Pieter%20R.%20Tuinman%20and%20Paul%20W.%20G.%20Elbers&entry.1292438233=%20%20Differentiating%20cardiogenic%20pulmonary%20oedema%20%28CPE%29%20from%20non-cardiogenic%20and%0Astructurally%20normal%20lungs%20in%20lung%20ultrasound%20%28LUS%29%20videos%20remains%20challenging%0Adue%20to%20the%20high%20visual%20variability%20of%20non-cardiogenic%20inflammatory%20patterns%0A%28NCIP/ARDS-like%29%2C%20interstitial%20lung%20disease%2C%20and%20healthy%20lungs.%20This%0Aheterogeneity%20complicates%20automated%20classification%20as%20overlapping%20B-lines%20and%0Apleural%20artefacts%20are%20common.%20We%20introduce%20ZACH-ViT%20%28Zero-token%20Adaptive%0ACompact%20Hierarchical%20Vision%20Transformer%29%2C%20a%200.25%20M-parameter%20Vision%20Transformer%0Avariant%20that%20removes%20both%20positional%20embeddings%20and%20the%20%5BCLS%5D%20token%2C%20making%20it%0Afully%20permutation-invariant%20and%20suitable%20for%20unordered%20medical%20image%20data.%20To%0Aenhance%20generalization%2C%20we%20propose%20ShuffleStrides%20Data%20Augmentation%20%28SSDA%29%2C%0Awhich%20permutes%20probe-view%20sequences%20and%20frame%20orders%20while%20preserving%0Aanatomical%20validity.%20ZACH-ViT%20was%20evaluated%20on%20380%20LUS%20videos%20from%2095%0Acritically%20ill%20patients%20against%20nine%20state-of-the-art%20baselines.%20Despite%20the%0Aheterogeneity%20of%20the%20non-cardiogenic%20group%2C%20ZACH-ViT%20achieved%20the%20highest%0Avalidation%20and%20test%20ROC-AUC%20%280.80%20and%200.79%29%20with%20balanced%20sensitivity%20%280.60%29%0Aand%20specificity%20%280.91%29%2C%20while%20all%20competing%20models%20collapsed%20to%20trivial%0Aclassification.%20It%20trains%201.35x%20faster%20than%20Minimal%20ViT%20%280.62M%20parameters%29%20with%0A2.5x%20fewer%20parameters%2C%20supporting%20real-time%20clinical%20deployment.%20These%20results%0Ashow%20that%20aligning%20architectural%20design%20with%20data%20structure%20can%20outperform%0Ascale%20in%20small-data%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17650v1&entry.124074799=Read"},
{"title": "Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring", "author": "Shawn M. Gibford and Mohammad Reza Boskabadi and Christopher J. Savoie and Seyed Soheil Mansouri", "abstract": "  Data scarcity and sparsity in bio-manufacturing poses challenges for accurate\nmodel\n  development, process monitoring, and optimization. We aim to replicate and\ncapture\n  the complex dynamics of industrial bioprocesses by proposing the use of a\nQuantum\n  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)\nto\n  generate synthetic time series data for industrially relevant processes. The\n  generator within our GAN is comprised of a Parameterized Quantum Circuit\n(PQC). This\n  methodology offers potential advantages in process monitoring, modeling,\n  forecasting, and optimization, enabling more efficient bioprocess management\nby\n  reducing the dependence on scarce experimental data. Our results demonstrate\n  acceptable performance in capturing the temporal dynamics of real bioprocess\ndata.\n  We focus on Optical Density, a key measurement for Dry Biomass estimation.\nThe data\n  generated showed high fidelity to the actual historical experimental data.\nThis\n  intersection of quantum computing and machine learning has opened new\nfrontiers in\n  data analysis and generation, particularly in computationally intensive\nfields, for\n  use cases such as increasing prediction accuracy for soft sensor design or\nfor use\n  in predictive control.\n", "link": "http://arxiv.org/abs/2510.17688v1", "date": "2025-10-20", "relevancy": 2.1663, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.562}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5318}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Synthetic%20Data%20Generation%20for%20Industrial%20Bioprocess%20Monitoring&body=Title%3A%20Quantum%20Synthetic%20Data%20Generation%20for%20Industrial%20Bioprocess%20Monitoring%0AAuthor%3A%20Shawn%20M.%20Gibford%20and%20Mohammad%20Reza%20Boskabadi%20and%20Christopher%20J.%20Savoie%20and%20Seyed%20Soheil%20Mansouri%0AAbstract%3A%20%20%20Data%20scarcity%20and%20sparsity%20in%20bio-manufacturing%20poses%20challenges%20for%20accurate%0Amodel%0A%20%20development%2C%20process%20monitoring%2C%20and%20optimization.%20We%20aim%20to%20replicate%20and%0Acapture%0A%20%20the%20complex%20dynamics%20of%20industrial%20bioprocesses%20by%20proposing%20the%20use%20of%20a%0AQuantum%0A%20%20Wasserstein%20Generative%20Adversarial%20Network%20with%20Gradient%20Penalty%20%28QWGAN-GP%29%0Ato%0A%20%20generate%20synthetic%20time%20series%20data%20for%20industrially%20relevant%20processes.%20The%0A%20%20generator%20within%20our%20GAN%20is%20comprised%20of%20a%20Parameterized%20Quantum%20Circuit%0A%28PQC%29.%20This%0A%20%20methodology%20offers%20potential%20advantages%20in%20process%20monitoring%2C%20modeling%2C%0A%20%20forecasting%2C%20and%20optimization%2C%20enabling%20more%20efficient%20bioprocess%20management%0Aby%0A%20%20reducing%20the%20dependence%20on%20scarce%20experimental%20data.%20Our%20results%20demonstrate%0A%20%20acceptable%20performance%20in%20capturing%20the%20temporal%20dynamics%20of%20real%20bioprocess%0Adata.%0A%20%20We%20focus%20on%20Optical%20Density%2C%20a%20key%20measurement%20for%20Dry%20Biomass%20estimation.%0AThe%20data%0A%20%20generated%20showed%20high%20fidelity%20to%20the%20actual%20historical%20experimental%20data.%0AThis%0A%20%20intersection%20of%20quantum%20computing%20and%20machine%20learning%20has%20opened%20new%0Afrontiers%20in%0A%20%20data%20analysis%20and%20generation%2C%20particularly%20in%20computationally%20intensive%0Afields%2C%20for%0A%20%20use%20cases%20such%20as%20increasing%20prediction%20accuracy%20for%20soft%20sensor%20design%20or%0Afor%20use%0A%20%20in%20predictive%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Synthetic%2520Data%2520Generation%2520for%2520Industrial%2520Bioprocess%2520Monitoring%26entry.906535625%3DShawn%2520M.%2520Gibford%2520and%2520Mohammad%2520Reza%2520Boskabadi%2520and%2520Christopher%2520J.%2520Savoie%2520and%2520Seyed%2520Soheil%2520Mansouri%26entry.1292438233%3D%2520%2520Data%2520scarcity%2520and%2520sparsity%2520in%2520bio-manufacturing%2520poses%2520challenges%2520for%2520accurate%250Amodel%250A%2520%2520development%252C%2520process%2520monitoring%252C%2520and%2520optimization.%2520We%2520aim%2520to%2520replicate%2520and%250Acapture%250A%2520%2520the%2520complex%2520dynamics%2520of%2520industrial%2520bioprocesses%2520by%2520proposing%2520the%2520use%2520of%2520a%250AQuantum%250A%2520%2520Wasserstein%2520Generative%2520Adversarial%2520Network%2520with%2520Gradient%2520Penalty%2520%2528QWGAN-GP%2529%250Ato%250A%2520%2520generate%2520synthetic%2520time%2520series%2520data%2520for%2520industrially%2520relevant%2520processes.%2520The%250A%2520%2520generator%2520within%2520our%2520GAN%2520is%2520comprised%2520of%2520a%2520Parameterized%2520Quantum%2520Circuit%250A%2528PQC%2529.%2520This%250A%2520%2520methodology%2520offers%2520potential%2520advantages%2520in%2520process%2520monitoring%252C%2520modeling%252C%250A%2520%2520forecasting%252C%2520and%2520optimization%252C%2520enabling%2520more%2520efficient%2520bioprocess%2520management%250Aby%250A%2520%2520reducing%2520the%2520dependence%2520on%2520scarce%2520experimental%2520data.%2520Our%2520results%2520demonstrate%250A%2520%2520acceptable%2520performance%2520in%2520capturing%2520the%2520temporal%2520dynamics%2520of%2520real%2520bioprocess%250Adata.%250A%2520%2520We%2520focus%2520on%2520Optical%2520Density%252C%2520a%2520key%2520measurement%2520for%2520Dry%2520Biomass%2520estimation.%250AThe%2520data%250A%2520%2520generated%2520showed%2520high%2520fidelity%2520to%2520the%2520actual%2520historical%2520experimental%2520data.%250AThis%250A%2520%2520intersection%2520of%2520quantum%2520computing%2520and%2520machine%2520learning%2520has%2520opened%2520new%250Afrontiers%2520in%250A%2520%2520data%2520analysis%2520and%2520generation%252C%2520particularly%2520in%2520computationally%2520intensive%250Afields%252C%2520for%250A%2520%2520use%2520cases%2520such%2520as%2520increasing%2520prediction%2520accuracy%2520for%2520soft%2520sensor%2520design%2520or%250Afor%2520use%250A%2520%2520in%2520predictive%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Synthetic%20Data%20Generation%20for%20Industrial%20Bioprocess%20Monitoring&entry.906535625=Shawn%20M.%20Gibford%20and%20Mohammad%20Reza%20Boskabadi%20and%20Christopher%20J.%20Savoie%20and%20Seyed%20Soheil%20Mansouri&entry.1292438233=%20%20Data%20scarcity%20and%20sparsity%20in%20bio-manufacturing%20poses%20challenges%20for%20accurate%0Amodel%0A%20%20development%2C%20process%20monitoring%2C%20and%20optimization.%20We%20aim%20to%20replicate%20and%0Acapture%0A%20%20the%20complex%20dynamics%20of%20industrial%20bioprocesses%20by%20proposing%20the%20use%20of%20a%0AQuantum%0A%20%20Wasserstein%20Generative%20Adversarial%20Network%20with%20Gradient%20Penalty%20%28QWGAN-GP%29%0Ato%0A%20%20generate%20synthetic%20time%20series%20data%20for%20industrially%20relevant%20processes.%20The%0A%20%20generator%20within%20our%20GAN%20is%20comprised%20of%20a%20Parameterized%20Quantum%20Circuit%0A%28PQC%29.%20This%0A%20%20methodology%20offers%20potential%20advantages%20in%20process%20monitoring%2C%20modeling%2C%0A%20%20forecasting%2C%20and%20optimization%2C%20enabling%20more%20efficient%20bioprocess%20management%0Aby%0A%20%20reducing%20the%20dependence%20on%20scarce%20experimental%20data.%20Our%20results%20demonstrate%0A%20%20acceptable%20performance%20in%20capturing%20the%20temporal%20dynamics%20of%20real%20bioprocess%0Adata.%0A%20%20We%20focus%20on%20Optical%20Density%2C%20a%20key%20measurement%20for%20Dry%20Biomass%20estimation.%0AThe%20data%0A%20%20generated%20showed%20high%20fidelity%20to%20the%20actual%20historical%20experimental%20data.%0AThis%0A%20%20intersection%20of%20quantum%20computing%20and%20machine%20learning%20has%20opened%20new%0Afrontiers%20in%0A%20%20data%20analysis%20and%20generation%2C%20particularly%20in%20computationally%20intensive%0Afields%2C%20for%0A%20%20use%20cases%20such%20as%20increasing%20prediction%20accuracy%20for%20soft%20sensor%20design%20or%0Afor%20use%0A%20%20in%20predictive%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17688v1&entry.124074799=Read"},
{"title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for\n  Longitudinal MRI Segmentation", "author": "Yovin Yahathugoda and Davide Prezzi and Piyalitt Ittichaiwong and Vicky Goh and Sebastien Ourselin and Michela Antonelli", "abstract": "  Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.\n", "link": "http://arxiv.org/abs/2510.17529v1", "date": "2025-10-20", "relevancy": 2.1533, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaX-Net%3A%20Dual-Input%20Mamba-Enhanced%20Cross-Attention%20Network%20for%0A%20%20Longitudinal%20MRI%20Segmentation&body=Title%3A%20MambaX-Net%3A%20Dual-Input%20Mamba-Enhanced%20Cross-Attention%20Network%20for%0A%20%20Longitudinal%20MRI%20Segmentation%0AAuthor%3A%20Yovin%20Yahathugoda%20and%20Davide%20Prezzi%20and%20Piyalitt%20Ittichaiwong%20and%20Vicky%20Goh%20and%20Sebastien%20Ourselin%20and%20Michela%20Antonelli%0AAbstract%3A%20%20%20Active%20Surveillance%20%28AS%29%20is%20a%20treatment%20option%20for%20managing%20low%20and%0Aintermediate-risk%20prostate%20cancer%20%28PCa%29%2C%20aiming%20to%20avoid%20overtreatment%20while%0Amonitoring%20disease%20progression%20through%20serial%20MRI%20and%20clinical%20follow-up.%0AAccurate%20prostate%20segmentation%20is%20an%20important%20preliminary%20step%20for%20automating%0Athis%20process%2C%20enabling%20automated%20detection%20and%20diagnosis%20of%20PCa.%20However%2C%0Aexisting%20deep-learning%20segmentation%20models%20are%20often%20trained%20on%0Asingle-time-point%20and%20expertly%20annotated%20datasets%2C%20making%20them%20unsuitable%20for%0Alongitudinal%20AS%20analysis%2C%20where%20multiple%20time%20points%20and%20a%20scarcity%20of%20expert%0Alabels%20hinder%20their%20effective%20fine-tuning.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MambaX-Net%2C%20a%20novel%20semi-supervised%2C%20dual-scan%203D%20segmentation%0Aarchitecture%20that%20computes%20the%20segmentation%20for%20time%20point%20t%20by%20leveraging%20the%0AMRI%20and%20the%20corresponding%20segmentation%20mask%20from%20the%20previous%20time%20point.%20We%0Aintroduce%20two%20new%20components%3A%20%28i%29%20a%20Mamba-enhanced%20Cross-Attention%20Module%2C%0Awhich%20integrates%20the%20Mamba%20block%20into%20cross%20attention%20to%20efficiently%20capture%0Atemporal%20evolution%20and%20long-range%20spatial%20dependencies%2C%20and%20%28ii%29%20a%20Shape%0AExtractor%20Module%20that%20encodes%20the%20previous%20segmentation%20mask%20into%20a%20latent%0Aanatomical%20representation%20for%20refined%20zone%20delination.%20Moreover%2C%20we%20introduce%20a%0Asemi-supervised%20self-training%20strategy%20that%20leverages%20pseudo-labels%20generated%0Afrom%20a%20pre-trained%20nnU-Net%2C%20enabling%20effective%20learning%20without%20expert%0Aannotations.%20MambaX-Net%20was%20evaluated%20on%20a%20longitudinal%20AS%20dataset%2C%20and%20results%0Ashowed%20that%20it%20significantly%20outperforms%20state-of-the-art%20U-Net%20and%0ATransformer-based%20models%2C%20achieving%20superior%20prostate%20zone%20segmentation%20even%0Awhen%20trained%20on%20limited%20and%20noisy%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaX-Net%253A%2520Dual-Input%2520Mamba-Enhanced%2520Cross-Attention%2520Network%2520for%250A%2520%2520Longitudinal%2520MRI%2520Segmentation%26entry.906535625%3DYovin%2520Yahathugoda%2520and%2520Davide%2520Prezzi%2520and%2520Piyalitt%2520Ittichaiwong%2520and%2520Vicky%2520Goh%2520and%2520Sebastien%2520Ourselin%2520and%2520Michela%2520Antonelli%26entry.1292438233%3D%2520%2520Active%2520Surveillance%2520%2528AS%2529%2520is%2520a%2520treatment%2520option%2520for%2520managing%2520low%2520and%250Aintermediate-risk%2520prostate%2520cancer%2520%2528PCa%2529%252C%2520aiming%2520to%2520avoid%2520overtreatment%2520while%250Amonitoring%2520disease%2520progression%2520through%2520serial%2520MRI%2520and%2520clinical%2520follow-up.%250AAccurate%2520prostate%2520segmentation%2520is%2520an%2520important%2520preliminary%2520step%2520for%2520automating%250Athis%2520process%252C%2520enabling%2520automated%2520detection%2520and%2520diagnosis%2520of%2520PCa.%2520However%252C%250Aexisting%2520deep-learning%2520segmentation%2520models%2520are%2520often%2520trained%2520on%250Asingle-time-point%2520and%2520expertly%2520annotated%2520datasets%252C%2520making%2520them%2520unsuitable%2520for%250Alongitudinal%2520AS%2520analysis%252C%2520where%2520multiple%2520time%2520points%2520and%2520a%2520scarcity%2520of%2520expert%250Alabels%2520hinder%2520their%2520effective%2520fine-tuning.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520MambaX-Net%252C%2520a%2520novel%2520semi-supervised%252C%2520dual-scan%25203D%2520segmentation%250Aarchitecture%2520that%2520computes%2520the%2520segmentation%2520for%2520time%2520point%2520t%2520by%2520leveraging%2520the%250AMRI%2520and%2520the%2520corresponding%2520segmentation%2520mask%2520from%2520the%2520previous%2520time%2520point.%2520We%250Aintroduce%2520two%2520new%2520components%253A%2520%2528i%2529%2520a%2520Mamba-enhanced%2520Cross-Attention%2520Module%252C%250Awhich%2520integrates%2520the%2520Mamba%2520block%2520into%2520cross%2520attention%2520to%2520efficiently%2520capture%250Atemporal%2520evolution%2520and%2520long-range%2520spatial%2520dependencies%252C%2520and%2520%2528ii%2529%2520a%2520Shape%250AExtractor%2520Module%2520that%2520encodes%2520the%2520previous%2520segmentation%2520mask%2520into%2520a%2520latent%250Aanatomical%2520representation%2520for%2520refined%2520zone%2520delination.%2520Moreover%252C%2520we%2520introduce%2520a%250Asemi-supervised%2520self-training%2520strategy%2520that%2520leverages%2520pseudo-labels%2520generated%250Afrom%2520a%2520pre-trained%2520nnU-Net%252C%2520enabling%2520effective%2520learning%2520without%2520expert%250Aannotations.%2520MambaX-Net%2520was%2520evaluated%2520on%2520a%2520longitudinal%2520AS%2520dataset%252C%2520and%2520results%250Ashowed%2520that%2520it%2520significantly%2520outperforms%2520state-of-the-art%2520U-Net%2520and%250ATransformer-based%2520models%252C%2520achieving%2520superior%2520prostate%2520zone%2520segmentation%2520even%250Awhen%2520trained%2520on%2520limited%2520and%2520noisy%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaX-Net%3A%20Dual-Input%20Mamba-Enhanced%20Cross-Attention%20Network%20for%0A%20%20Longitudinal%20MRI%20Segmentation&entry.906535625=Yovin%20Yahathugoda%20and%20Davide%20Prezzi%20and%20Piyalitt%20Ittichaiwong%20and%20Vicky%20Goh%20and%20Sebastien%20Ourselin%20and%20Michela%20Antonelli&entry.1292438233=%20%20Active%20Surveillance%20%28AS%29%20is%20a%20treatment%20option%20for%20managing%20low%20and%0Aintermediate-risk%20prostate%20cancer%20%28PCa%29%2C%20aiming%20to%20avoid%20overtreatment%20while%0Amonitoring%20disease%20progression%20through%20serial%20MRI%20and%20clinical%20follow-up.%0AAccurate%20prostate%20segmentation%20is%20an%20important%20preliminary%20step%20for%20automating%0Athis%20process%2C%20enabling%20automated%20detection%20and%20diagnosis%20of%20PCa.%20However%2C%0Aexisting%20deep-learning%20segmentation%20models%20are%20often%20trained%20on%0Asingle-time-point%20and%20expertly%20annotated%20datasets%2C%20making%20them%20unsuitable%20for%0Alongitudinal%20AS%20analysis%2C%20where%20multiple%20time%20points%20and%20a%20scarcity%20of%20expert%0Alabels%20hinder%20their%20effective%20fine-tuning.%20To%20address%20these%20challenges%2C%20we%0Apropose%20MambaX-Net%2C%20a%20novel%20semi-supervised%2C%20dual-scan%203D%20segmentation%0Aarchitecture%20that%20computes%20the%20segmentation%20for%20time%20point%20t%20by%20leveraging%20the%0AMRI%20and%20the%20corresponding%20segmentation%20mask%20from%20the%20previous%20time%20point.%20We%0Aintroduce%20two%20new%20components%3A%20%28i%29%20a%20Mamba-enhanced%20Cross-Attention%20Module%2C%0Awhich%20integrates%20the%20Mamba%20block%20into%20cross%20attention%20to%20efficiently%20capture%0Atemporal%20evolution%20and%20long-range%20spatial%20dependencies%2C%20and%20%28ii%29%20a%20Shape%0AExtractor%20Module%20that%20encodes%20the%20previous%20segmentation%20mask%20into%20a%20latent%0Aanatomical%20representation%20for%20refined%20zone%20delination.%20Moreover%2C%20we%20introduce%20a%0Asemi-supervised%20self-training%20strategy%20that%20leverages%20pseudo-labels%20generated%0Afrom%20a%20pre-trained%20nnU-Net%2C%20enabling%20effective%20learning%20without%20expert%0Aannotations.%20MambaX-Net%20was%20evaluated%20on%20a%20longitudinal%20AS%20dataset%2C%20and%20results%0Ashowed%20that%20it%20significantly%20outperforms%20state-of-the-art%20U-Net%20and%0ATransformer-based%20models%2C%20achieving%20superior%20prostate%20zone%20segmentation%20even%0Awhen%20trained%20on%20limited%20and%20noisy%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17529v1&entry.124074799=Read"},
{"title": "Speech Foundation Models Generalize to Time Series Tasks from Wearable\n  Sensor Data", "author": "Jaya Narain and Zakaria Aldeneh and Shirley Ren", "abstract": "  Both speech and sensor time series data encode information in both the time-\nand frequency- domains, like spectral powers and waveform shapelets. We show\nthat speech foundation models learn representations that generalize beyond the\nspeech domain and achieve state-of-the-art performance on diverse time-series\ntasks from wearable sensors. Probes trained on features extracted from HuBERT\nand wav2vec 2.0 outperform those extracted from self-supervised models trained\ndirectly on modality-specific datasets for mood classification, arrhythmia\ndetection, and activity classification tasks. We find that the convolutional\nfeature encoders of speech models are particularly relevant for wearable sensor\napplications. The proposed approach enhances performance on data-scarce\ntime-series tasks using simple probing methods. This work takes a step toward\ndeveloping generalized time-series models that unify speech and sensor\nmodalities.\n", "link": "http://arxiv.org/abs/2509.00221v2", "date": "2025-10-20", "relevancy": 2.1409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech%20Foundation%20Models%20Generalize%20to%20Time%20Series%20Tasks%20from%20Wearable%0A%20%20Sensor%20Data&body=Title%3A%20Speech%20Foundation%20Models%20Generalize%20to%20Time%20Series%20Tasks%20from%20Wearable%0A%20%20Sensor%20Data%0AAuthor%3A%20Jaya%20Narain%20and%20Zakaria%20Aldeneh%20and%20Shirley%20Ren%0AAbstract%3A%20%20%20Both%20speech%20and%20sensor%20time%20series%20data%20encode%20information%20in%20both%20the%20time-%0Aand%20frequency-%20domains%2C%20like%20spectral%20powers%20and%20waveform%20shapelets.%20We%20show%0Athat%20speech%20foundation%20models%20learn%20representations%20that%20generalize%20beyond%20the%0Aspeech%20domain%20and%20achieve%20state-of-the-art%20performance%20on%20diverse%20time-series%0Atasks%20from%20wearable%20sensors.%20Probes%20trained%20on%20features%20extracted%20from%20HuBERT%0Aand%20wav2vec%202.0%20outperform%20those%20extracted%20from%20self-supervised%20models%20trained%0Adirectly%20on%20modality-specific%20datasets%20for%20mood%20classification%2C%20arrhythmia%0Adetection%2C%20and%20activity%20classification%20tasks.%20We%20find%20that%20the%20convolutional%0Afeature%20encoders%20of%20speech%20models%20are%20particularly%20relevant%20for%20wearable%20sensor%0Aapplications.%20The%20proposed%20approach%20enhances%20performance%20on%20data-scarce%0Atime-series%20tasks%20using%20simple%20probing%20methods.%20This%20work%20takes%20a%20step%20toward%0Adeveloping%20generalized%20time-series%20models%20that%20unify%20speech%20and%20sensor%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.00221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech%2520Foundation%2520Models%2520Generalize%2520to%2520Time%2520Series%2520Tasks%2520from%2520Wearable%250A%2520%2520Sensor%2520Data%26entry.906535625%3DJaya%2520Narain%2520and%2520Zakaria%2520Aldeneh%2520and%2520Shirley%2520Ren%26entry.1292438233%3D%2520%2520Both%2520speech%2520and%2520sensor%2520time%2520series%2520data%2520encode%2520information%2520in%2520both%2520the%2520time-%250Aand%2520frequency-%2520domains%252C%2520like%2520spectral%2520powers%2520and%2520waveform%2520shapelets.%2520We%2520show%250Athat%2520speech%2520foundation%2520models%2520learn%2520representations%2520that%2520generalize%2520beyond%2520the%250Aspeech%2520domain%2520and%2520achieve%2520state-of-the-art%2520performance%2520on%2520diverse%2520time-series%250Atasks%2520from%2520wearable%2520sensors.%2520Probes%2520trained%2520on%2520features%2520extracted%2520from%2520HuBERT%250Aand%2520wav2vec%25202.0%2520outperform%2520those%2520extracted%2520from%2520self-supervised%2520models%2520trained%250Adirectly%2520on%2520modality-specific%2520datasets%2520for%2520mood%2520classification%252C%2520arrhythmia%250Adetection%252C%2520and%2520activity%2520classification%2520tasks.%2520We%2520find%2520that%2520the%2520convolutional%250Afeature%2520encoders%2520of%2520speech%2520models%2520are%2520particularly%2520relevant%2520for%2520wearable%2520sensor%250Aapplications.%2520The%2520proposed%2520approach%2520enhances%2520performance%2520on%2520data-scarce%250Atime-series%2520tasks%2520using%2520simple%2520probing%2520methods.%2520This%2520work%2520takes%2520a%2520step%2520toward%250Adeveloping%2520generalized%2520time-series%2520models%2520that%2520unify%2520speech%2520and%2520sensor%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech%20Foundation%20Models%20Generalize%20to%20Time%20Series%20Tasks%20from%20Wearable%0A%20%20Sensor%20Data&entry.906535625=Jaya%20Narain%20and%20Zakaria%20Aldeneh%20and%20Shirley%20Ren&entry.1292438233=%20%20Both%20speech%20and%20sensor%20time%20series%20data%20encode%20information%20in%20both%20the%20time-%0Aand%20frequency-%20domains%2C%20like%20spectral%20powers%20and%20waveform%20shapelets.%20We%20show%0Athat%20speech%20foundation%20models%20learn%20representations%20that%20generalize%20beyond%20the%0Aspeech%20domain%20and%20achieve%20state-of-the-art%20performance%20on%20diverse%20time-series%0Atasks%20from%20wearable%20sensors.%20Probes%20trained%20on%20features%20extracted%20from%20HuBERT%0Aand%20wav2vec%202.0%20outperform%20those%20extracted%20from%20self-supervised%20models%20trained%0Adirectly%20on%20modality-specific%20datasets%20for%20mood%20classification%2C%20arrhythmia%0Adetection%2C%20and%20activity%20classification%20tasks.%20We%20find%20that%20the%20convolutional%0Afeature%20encoders%20of%20speech%20models%20are%20particularly%20relevant%20for%20wearable%20sensor%0Aapplications.%20The%20proposed%20approach%20enhances%20performance%20on%20data-scarce%0Atime-series%20tasks%20using%20simple%20probing%20methods.%20This%20work%20takes%20a%20step%20toward%0Adeveloping%20generalized%20time-series%20models%20that%20unify%20speech%20and%20sensor%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.00221v2&entry.124074799=Read"},
{"title": "Expose Camouflage in the Water: Underwater Camouflaged Instance\n  Segmentation and Dataset", "author": "Chuhong Wang and Hua Li and Chongyi Li and Huazhong Liu and Xiongxin Tang and Sam Kwong", "abstract": "  With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2510.17585v1", "date": "2025-10-20", "relevancy": 2.13, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expose%20Camouflage%20in%20the%20Water%3A%20Underwater%20Camouflaged%20Instance%0A%20%20Segmentation%20and%20Dataset&body=Title%3A%20Expose%20Camouflage%20in%20the%20Water%3A%20Underwater%20Camouflaged%20Instance%0A%20%20Segmentation%20and%20Dataset%0AAuthor%3A%20Chuhong%20Wang%20and%20Hua%20Li%20and%20Chongyi%20Li%20and%20Huazhong%20Liu%20and%20Xiongxin%20Tang%20and%20Sam%20Kwong%0AAbstract%3A%20%20%20With%20the%20development%20of%20underwater%20exploration%20and%20marine%20protection%2C%0Aunderwater%20vision%20tasks%20are%20widespread.%20Due%20to%20the%20degraded%20underwater%0Aenvironment%2C%20characterized%20by%20color%20distortion%2C%20low%20contrast%2C%20and%20blurring%2C%0Acamouflaged%20instance%20segmentation%20%28CIS%29%20faces%20greater%20challenges%20in%20accurately%0Asegmenting%20objects%20that%20blend%20closely%20with%20their%20surroundings.%20Traditional%0Acamouflaged%20instance%20segmentation%20methods%2C%20trained%20on%20terrestrial-dominated%0Adatasets%20with%20limited%20underwater%20samples%2C%20may%20exhibit%20inadequate%20performance%20in%0Aunderwater%20scenes.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20underwater%0Acamouflaged%20instance%20segmentation%20%28UCIS%29%20dataset%2C%20abbreviated%20as%20UCIS4K%2C%20which%0Acomprises%203%2C953%20images%20of%20camouflaged%20marine%20organisms%20with%20instance-level%0Aannotations.%20In%20addition%2C%20we%20propose%20an%20Underwater%20Camouflaged%20Instance%0ASegmentation%20network%20based%20on%20Segment%20Anything%20Model%20%28UCIS-SAM%29.%20Our%20UCIS-SAM%0Aincludes%20three%20key%20modules.%20First%2C%20the%20Channel%20Balance%20Optimization%20Module%0A%28CBOM%29%20enhances%20channel%20characteristics%20to%20improve%20underwater%20feature%20learning%2C%0Aeffectively%20addressing%20the%20model%27s%20limited%20understanding%20of%20underwater%0Aenvironments.%20Second%2C%20the%20Frequency%20Domain%20True%20Integration%20Module%20%28FDTIM%29%20is%0Aproposed%20to%20emphasize%20intrinsic%20object%20features%20and%20reduce%20interference%20from%0Acamouflage%20patterns%2C%20enhancing%20the%20segmentation%20performance%20of%20camouflaged%0Aobjects%20blending%20with%20their%20surroundings.%20Finally%2C%20the%20Multi-scale%20Feature%0AFrequency%20Aggregation%20Module%20%28MFFAM%29%20is%20designed%20to%20strengthen%20the%20boundaries%0Aof%20low-contrast%20camouflaged%20instances%20across%20multiple%20frequency%20bands%2C%0Aimproving%20the%20model%27s%20ability%20to%20achieve%20more%20precise%20segmentation%20of%0Acamouflaged%20objects.%20Extensive%20experiments%20on%20the%20proposed%20UCIS4K%20and%20public%0Abenchmarks%20show%20that%20our%20UCIS-SAM%20outperforms%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpose%2520Camouflage%2520in%2520the%2520Water%253A%2520Underwater%2520Camouflaged%2520Instance%250A%2520%2520Segmentation%2520and%2520Dataset%26entry.906535625%3DChuhong%2520Wang%2520and%2520Hua%2520Li%2520and%2520Chongyi%2520Li%2520and%2520Huazhong%2520Liu%2520and%2520Xiongxin%2520Tang%2520and%2520Sam%2520Kwong%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520underwater%2520exploration%2520and%2520marine%2520protection%252C%250Aunderwater%2520vision%2520tasks%2520are%2520widespread.%2520Due%2520to%2520the%2520degraded%2520underwater%250Aenvironment%252C%2520characterized%2520by%2520color%2520distortion%252C%2520low%2520contrast%252C%2520and%2520blurring%252C%250Acamouflaged%2520instance%2520segmentation%2520%2528CIS%2529%2520faces%2520greater%2520challenges%2520in%2520accurately%250Asegmenting%2520objects%2520that%2520blend%2520closely%2520with%2520their%2520surroundings.%2520Traditional%250Acamouflaged%2520instance%2520segmentation%2520methods%252C%2520trained%2520on%2520terrestrial-dominated%250Adatasets%2520with%2520limited%2520underwater%2520samples%252C%2520may%2520exhibit%2520inadequate%2520performance%2520in%250Aunderwater%2520scenes.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520first%2520underwater%250Acamouflaged%2520instance%2520segmentation%2520%2528UCIS%2529%2520dataset%252C%2520abbreviated%2520as%2520UCIS4K%252C%2520which%250Acomprises%25203%252C953%2520images%2520of%2520camouflaged%2520marine%2520organisms%2520with%2520instance-level%250Aannotations.%2520In%2520addition%252C%2520we%2520propose%2520an%2520Underwater%2520Camouflaged%2520Instance%250ASegmentation%2520network%2520based%2520on%2520Segment%2520Anything%2520Model%2520%2528UCIS-SAM%2529.%2520Our%2520UCIS-SAM%250Aincludes%2520three%2520key%2520modules.%2520First%252C%2520the%2520Channel%2520Balance%2520Optimization%2520Module%250A%2528CBOM%2529%2520enhances%2520channel%2520characteristics%2520to%2520improve%2520underwater%2520feature%2520learning%252C%250Aeffectively%2520addressing%2520the%2520model%2527s%2520limited%2520understanding%2520of%2520underwater%250Aenvironments.%2520Second%252C%2520the%2520Frequency%2520Domain%2520True%2520Integration%2520Module%2520%2528FDTIM%2529%2520is%250Aproposed%2520to%2520emphasize%2520intrinsic%2520object%2520features%2520and%2520reduce%2520interference%2520from%250Acamouflage%2520patterns%252C%2520enhancing%2520the%2520segmentation%2520performance%2520of%2520camouflaged%250Aobjects%2520blending%2520with%2520their%2520surroundings.%2520Finally%252C%2520the%2520Multi-scale%2520Feature%250AFrequency%2520Aggregation%2520Module%2520%2528MFFAM%2529%2520is%2520designed%2520to%2520strengthen%2520the%2520boundaries%250Aof%2520low-contrast%2520camouflaged%2520instances%2520across%2520multiple%2520frequency%2520bands%252C%250Aimproving%2520the%2520model%2527s%2520ability%2520to%2520achieve%2520more%2520precise%2520segmentation%2520of%250Acamouflaged%2520objects.%2520Extensive%2520experiments%2520on%2520the%2520proposed%2520UCIS4K%2520and%2520public%250Abenchmarks%2520show%2520that%2520our%2520UCIS-SAM%2520outperforms%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expose%20Camouflage%20in%20the%20Water%3A%20Underwater%20Camouflaged%20Instance%0A%20%20Segmentation%20and%20Dataset&entry.906535625=Chuhong%20Wang%20and%20Hua%20Li%20and%20Chongyi%20Li%20and%20Huazhong%20Liu%20and%20Xiongxin%20Tang%20and%20Sam%20Kwong&entry.1292438233=%20%20With%20the%20development%20of%20underwater%20exploration%20and%20marine%20protection%2C%0Aunderwater%20vision%20tasks%20are%20widespread.%20Due%20to%20the%20degraded%20underwater%0Aenvironment%2C%20characterized%20by%20color%20distortion%2C%20low%20contrast%2C%20and%20blurring%2C%0Acamouflaged%20instance%20segmentation%20%28CIS%29%20faces%20greater%20challenges%20in%20accurately%0Asegmenting%20objects%20that%20blend%20closely%20with%20their%20surroundings.%20Traditional%0Acamouflaged%20instance%20segmentation%20methods%2C%20trained%20on%20terrestrial-dominated%0Adatasets%20with%20limited%20underwater%20samples%2C%20may%20exhibit%20inadequate%20performance%20in%0Aunderwater%20scenes.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20first%20underwater%0Acamouflaged%20instance%20segmentation%20%28UCIS%29%20dataset%2C%20abbreviated%20as%20UCIS4K%2C%20which%0Acomprises%203%2C953%20images%20of%20camouflaged%20marine%20organisms%20with%20instance-level%0Aannotations.%20In%20addition%2C%20we%20propose%20an%20Underwater%20Camouflaged%20Instance%0ASegmentation%20network%20based%20on%20Segment%20Anything%20Model%20%28UCIS-SAM%29.%20Our%20UCIS-SAM%0Aincludes%20three%20key%20modules.%20First%2C%20the%20Channel%20Balance%20Optimization%20Module%0A%28CBOM%29%20enhances%20channel%20characteristics%20to%20improve%20underwater%20feature%20learning%2C%0Aeffectively%20addressing%20the%20model%27s%20limited%20understanding%20of%20underwater%0Aenvironments.%20Second%2C%20the%20Frequency%20Domain%20True%20Integration%20Module%20%28FDTIM%29%20is%0Aproposed%20to%20emphasize%20intrinsic%20object%20features%20and%20reduce%20interference%20from%0Acamouflage%20patterns%2C%20enhancing%20the%20segmentation%20performance%20of%20camouflaged%0Aobjects%20blending%20with%20their%20surroundings.%20Finally%2C%20the%20Multi-scale%20Feature%0AFrequency%20Aggregation%20Module%20%28MFFAM%29%20is%20designed%20to%20strengthen%20the%20boundaries%0Aof%20low-contrast%20camouflaged%20instances%20across%20multiple%20frequency%20bands%2C%0Aimproving%20the%20model%27s%20ability%20to%20achieve%20more%20precise%20segmentation%20of%0Acamouflaged%20objects.%20Extensive%20experiments%20on%20the%20proposed%20UCIS4K%20and%20public%0Abenchmarks%20show%20that%20our%20UCIS-SAM%20outperforms%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17585v1&entry.124074799=Read"},
{"title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory\n  Pre-training Model", "author": "Yichen Liu and Yan Lin and Shengnan Guo and Zeyu Zhou and Youfang Lin and Huaiyu Wan", "abstract": "  Vehicle GPS trajectories record how vehicles move over time, storing valuable\ntravel semantics, including movement patterns and travel purposes. Learning\ntravel semantics effectively and efficiently is crucial for real-world\napplications of trajectory data, which is hindered by two major challenges.\nFirst, travel purposes are tied to the functions of the roads and\npoints-of-interest (POIs) involved in a trip. Such information is encoded in\ntextual addresses and descriptions and introduces heavy computational burden to\nmodeling. Second, real-world trajectories often contain redundant points, which\nharm both computational efficiency and trajectory embedding quality. To address\nthese challenges, we propose TrajMamba, a novel approach for efficient and\nsemantically rich vehicle trajectory learning. TrajMamba introduces a\nTraj-Mamba Encoder that captures movement patterns by jointly modeling both GPS\nand road perspectives of trajectories, enabling robust representations of\ncontinuous travel behaviors. It also incorporates a Travel Purpose-aware\nPre-training procedure to integrate travel purposes into the learned embeddings\nwithout introducing extra overhead to embedding calculation. To reduce\nredundancy in trajectories, TrajMamba features a Knowledge Distillation\nPre-training scheme to identify key trajectory points through a learnable mask\ngenerator and obtain effective compressed trajectory embeddings. Extensive\nexperiments on two real-world datasets and three downstream tasks show that\nTrajMamba outperforms state-of-the-art baselines in both efficiency and\naccuracy.\n", "link": "http://arxiv.org/abs/2510.17545v2", "date": "2025-10-21", "relevancy": 2.1174, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrajMamba%3A%20An%20Efficient%20and%20Semantic-rich%20Vehicle%20Trajectory%0A%20%20Pre-training%20Model&body=Title%3A%20TrajMamba%3A%20An%20Efficient%20and%20Semantic-rich%20Vehicle%20Trajectory%0A%20%20Pre-training%20Model%0AAuthor%3A%20Yichen%20Liu%20and%20Yan%20Lin%20and%20Shengnan%20Guo%20and%20Zeyu%20Zhou%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20Vehicle%20GPS%20trajectories%20record%20how%20vehicles%20move%20over%20time%2C%20storing%20valuable%0Atravel%20semantics%2C%20including%20movement%20patterns%20and%20travel%20purposes.%20Learning%0Atravel%20semantics%20effectively%20and%20efficiently%20is%20crucial%20for%20real-world%0Aapplications%20of%20trajectory%20data%2C%20which%20is%20hindered%20by%20two%20major%20challenges.%0AFirst%2C%20travel%20purposes%20are%20tied%20to%20the%20functions%20of%20the%20roads%20and%0Apoints-of-interest%20%28POIs%29%20involved%20in%20a%20trip.%20Such%20information%20is%20encoded%20in%0Atextual%20addresses%20and%20descriptions%20and%20introduces%20heavy%20computational%20burden%20to%0Amodeling.%20Second%2C%20real-world%20trajectories%20often%20contain%20redundant%20points%2C%20which%0Aharm%20both%20computational%20efficiency%20and%20trajectory%20embedding%20quality.%20To%20address%0Athese%20challenges%2C%20we%20propose%20TrajMamba%2C%20a%20novel%20approach%20for%20efficient%20and%0Asemantically%20rich%20vehicle%20trajectory%20learning.%20TrajMamba%20introduces%20a%0ATraj-Mamba%20Encoder%20that%20captures%20movement%20patterns%20by%20jointly%20modeling%20both%20GPS%0Aand%20road%20perspectives%20of%20trajectories%2C%20enabling%20robust%20representations%20of%0Acontinuous%20travel%20behaviors.%20It%20also%20incorporates%20a%20Travel%20Purpose-aware%0APre-training%20procedure%20to%20integrate%20travel%20purposes%20into%20the%20learned%20embeddings%0Awithout%20introducing%20extra%20overhead%20to%20embedding%20calculation.%20To%20reduce%0Aredundancy%20in%20trajectories%2C%20TrajMamba%20features%20a%20Knowledge%20Distillation%0APre-training%20scheme%20to%20identify%20key%20trajectory%20points%20through%20a%20learnable%20mask%0Agenerator%20and%20obtain%20effective%20compressed%20trajectory%20embeddings.%20Extensive%0Aexperiments%20on%20two%20real-world%20datasets%20and%20three%20downstream%20tasks%20show%20that%0ATrajMamba%20outperforms%20state-of-the-art%20baselines%20in%20both%20efficiency%20and%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajMamba%253A%2520An%2520Efficient%2520and%2520Semantic-rich%2520Vehicle%2520Trajectory%250A%2520%2520Pre-training%2520Model%26entry.906535625%3DYichen%2520Liu%2520and%2520Yan%2520Lin%2520and%2520Shengnan%2520Guo%2520and%2520Zeyu%2520Zhou%2520and%2520Youfang%2520Lin%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520Vehicle%2520GPS%2520trajectories%2520record%2520how%2520vehicles%2520move%2520over%2520time%252C%2520storing%2520valuable%250Atravel%2520semantics%252C%2520including%2520movement%2520patterns%2520and%2520travel%2520purposes.%2520Learning%250Atravel%2520semantics%2520effectively%2520and%2520efficiently%2520is%2520crucial%2520for%2520real-world%250Aapplications%2520of%2520trajectory%2520data%252C%2520which%2520is%2520hindered%2520by%2520two%2520major%2520challenges.%250AFirst%252C%2520travel%2520purposes%2520are%2520tied%2520to%2520the%2520functions%2520of%2520the%2520roads%2520and%250Apoints-of-interest%2520%2528POIs%2529%2520involved%2520in%2520a%2520trip.%2520Such%2520information%2520is%2520encoded%2520in%250Atextual%2520addresses%2520and%2520descriptions%2520and%2520introduces%2520heavy%2520computational%2520burden%2520to%250Amodeling.%2520Second%252C%2520real-world%2520trajectories%2520often%2520contain%2520redundant%2520points%252C%2520which%250Aharm%2520both%2520computational%2520efficiency%2520and%2520trajectory%2520embedding%2520quality.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520TrajMamba%252C%2520a%2520novel%2520approach%2520for%2520efficient%2520and%250Asemantically%2520rich%2520vehicle%2520trajectory%2520learning.%2520TrajMamba%2520introduces%2520a%250ATraj-Mamba%2520Encoder%2520that%2520captures%2520movement%2520patterns%2520by%2520jointly%2520modeling%2520both%2520GPS%250Aand%2520road%2520perspectives%2520of%2520trajectories%252C%2520enabling%2520robust%2520representations%2520of%250Acontinuous%2520travel%2520behaviors.%2520It%2520also%2520incorporates%2520a%2520Travel%2520Purpose-aware%250APre-training%2520procedure%2520to%2520integrate%2520travel%2520purposes%2520into%2520the%2520learned%2520embeddings%250Awithout%2520introducing%2520extra%2520overhead%2520to%2520embedding%2520calculation.%2520To%2520reduce%250Aredundancy%2520in%2520trajectories%252C%2520TrajMamba%2520features%2520a%2520Knowledge%2520Distillation%250APre-training%2520scheme%2520to%2520identify%2520key%2520trajectory%2520points%2520through%2520a%2520learnable%2520mask%250Agenerator%2520and%2520obtain%2520effective%2520compressed%2520trajectory%2520embeddings.%2520Extensive%250Aexperiments%2520on%2520two%2520real-world%2520datasets%2520and%2520three%2520downstream%2520tasks%2520show%2520that%250ATrajMamba%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520efficiency%2520and%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrajMamba%3A%20An%20Efficient%20and%20Semantic-rich%20Vehicle%20Trajectory%0A%20%20Pre-training%20Model&entry.906535625=Yichen%20Liu%20and%20Yan%20Lin%20and%20Shengnan%20Guo%20and%20Zeyu%20Zhou%20and%20Youfang%20Lin%20and%20Huaiyu%20Wan&entry.1292438233=%20%20Vehicle%20GPS%20trajectories%20record%20how%20vehicles%20move%20over%20time%2C%20storing%20valuable%0Atravel%20semantics%2C%20including%20movement%20patterns%20and%20travel%20purposes.%20Learning%0Atravel%20semantics%20effectively%20and%20efficiently%20is%20crucial%20for%20real-world%0Aapplications%20of%20trajectory%20data%2C%20which%20is%20hindered%20by%20two%20major%20challenges.%0AFirst%2C%20travel%20purposes%20are%20tied%20to%20the%20functions%20of%20the%20roads%20and%0Apoints-of-interest%20%28POIs%29%20involved%20in%20a%20trip.%20Such%20information%20is%20encoded%20in%0Atextual%20addresses%20and%20descriptions%20and%20introduces%20heavy%20computational%20burden%20to%0Amodeling.%20Second%2C%20real-world%20trajectories%20often%20contain%20redundant%20points%2C%20which%0Aharm%20both%20computational%20efficiency%20and%20trajectory%20embedding%20quality.%20To%20address%0Athese%20challenges%2C%20we%20propose%20TrajMamba%2C%20a%20novel%20approach%20for%20efficient%20and%0Asemantically%20rich%20vehicle%20trajectory%20learning.%20TrajMamba%20introduces%20a%0ATraj-Mamba%20Encoder%20that%20captures%20movement%20patterns%20by%20jointly%20modeling%20both%20GPS%0Aand%20road%20perspectives%20of%20trajectories%2C%20enabling%20robust%20representations%20of%0Acontinuous%20travel%20behaviors.%20It%20also%20incorporates%20a%20Travel%20Purpose-aware%0APre-training%20procedure%20to%20integrate%20travel%20purposes%20into%20the%20learned%20embeddings%0Awithout%20introducing%20extra%20overhead%20to%20embedding%20calculation.%20To%20reduce%0Aredundancy%20in%20trajectories%2C%20TrajMamba%20features%20a%20Knowledge%20Distillation%0APre-training%20scheme%20to%20identify%20key%20trajectory%20points%20through%20a%20learnable%20mask%0Agenerator%20and%20obtain%20effective%20compressed%20trajectory%20embeddings.%20Extensive%0Aexperiments%20on%20two%20real-world%20datasets%20and%20three%20downstream%20tasks%20show%20that%0ATrajMamba%20outperforms%20state-of-the-art%20baselines%20in%20both%20efficiency%20and%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17545v2&entry.124074799=Read"},
{"title": "Principled Feature Disentanglement for High-Fidelity Unified Brain MRI\n  Synthesis", "author": "Jihoon Cho and Jonghye Woo and Jinah Park", "abstract": "  Multisequence Magnetic Resonance Imaging (MRI) provides a more reliable\ndiagnosis in clinical applications through complementary information across\nsequences. However, in practice, the absence of certain MR sequences is a\ncommon problem that can lead to inconsistent analysis results. In this work, we\npropose a novel unified framework for synthesizing multisequence MR images,\ncalled hybrid-fusion GAN (HF-GAN). The fundamental mechanism of this work is\nprincipled feature disentanglement, which aligns the design of the architecture\nwith the complexity of the features. A powerful many-to-one stream is\nconstructed for the extraction of complex complementary features, while\nutilizing parallel, one-to-one streams to process modality-specific\ninformation. These disentangled features are dynamically integrated into a\ncommon latent space by a channel attention-based fusion module (CAFF) and then\ntransformed via a modality infuser to generate the target sequence. We\nvalidated our framework on public datasets of both healthy and pathological\nbrain MRI. Quantitative and qualitative results show that HF-GAN achieves\nstate-of-the-art performance, with our 2D slice-based framework notably\noutperforming a leading 3D volumetric model. Furthermore, the utilization of\nHF-GAN for data imputation substantially improves the performance of the\ndownstream brain tumor segmentation task, demonstrating its clinical relevance.\n", "link": "http://arxiv.org/abs/2406.14954v2", "date": "2025-10-20", "relevancy": 2.0994, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5263}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5256}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Principled%20Feature%20Disentanglement%20for%20High-Fidelity%20Unified%20Brain%20MRI%0A%20%20Synthesis&body=Title%3A%20Principled%20Feature%20Disentanglement%20for%20High-Fidelity%20Unified%20Brain%20MRI%0A%20%20Synthesis%0AAuthor%3A%20Jihoon%20Cho%20and%20Jonghye%20Woo%20and%20Jinah%20Park%0AAbstract%3A%20%20%20Multisequence%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20a%20more%20reliable%0Adiagnosis%20in%20clinical%20applications%20through%20complementary%20information%20across%0Asequences.%20However%2C%20in%20practice%2C%20the%20absence%20of%20certain%20MR%20sequences%20is%20a%0Acommon%20problem%20that%20can%20lead%20to%20inconsistent%20analysis%20results.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20unified%20framework%20for%20synthesizing%20multisequence%20MR%20images%2C%0Acalled%20hybrid-fusion%20GAN%20%28HF-GAN%29.%20The%20fundamental%20mechanism%20of%20this%20work%20is%0Aprincipled%20feature%20disentanglement%2C%20which%20aligns%20the%20design%20of%20the%20architecture%0Awith%20the%20complexity%20of%20the%20features.%20A%20powerful%20many-to-one%20stream%20is%0Aconstructed%20for%20the%20extraction%20of%20complex%20complementary%20features%2C%20while%0Autilizing%20parallel%2C%20one-to-one%20streams%20to%20process%20modality-specific%0Ainformation.%20These%20disentangled%20features%20are%20dynamically%20integrated%20into%20a%0Acommon%20latent%20space%20by%20a%20channel%20attention-based%20fusion%20module%20%28CAFF%29%20and%20then%0Atransformed%20via%20a%20modality%20infuser%20to%20generate%20the%20target%20sequence.%20We%0Avalidated%20our%20framework%20on%20public%20datasets%20of%20both%20healthy%20and%20pathological%0Abrain%20MRI.%20Quantitative%20and%20qualitative%20results%20show%20that%20HF-GAN%20achieves%0Astate-of-the-art%20performance%2C%20with%20our%202D%20slice-based%20framework%20notably%0Aoutperforming%20a%20leading%203D%20volumetric%20model.%20Furthermore%2C%20the%20utilization%20of%0AHF-GAN%20for%20data%20imputation%20substantially%20improves%20the%20performance%20of%20the%0Adownstream%20brain%20tumor%20segmentation%20task%2C%20demonstrating%20its%20clinical%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrincipled%2520Feature%2520Disentanglement%2520for%2520High-Fidelity%2520Unified%2520Brain%2520MRI%250A%2520%2520Synthesis%26entry.906535625%3DJihoon%2520Cho%2520and%2520Jonghye%2520Woo%2520and%2520Jinah%2520Park%26entry.1292438233%3D%2520%2520Multisequence%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520provides%2520a%2520more%2520reliable%250Adiagnosis%2520in%2520clinical%2520applications%2520through%2520complementary%2520information%2520across%250Asequences.%2520However%252C%2520in%2520practice%252C%2520the%2520absence%2520of%2520certain%2520MR%2520sequences%2520is%2520a%250Acommon%2520problem%2520that%2520can%2520lead%2520to%2520inconsistent%2520analysis%2520results.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520unified%2520framework%2520for%2520synthesizing%2520multisequence%2520MR%2520images%252C%250Acalled%2520hybrid-fusion%2520GAN%2520%2528HF-GAN%2529.%2520The%2520fundamental%2520mechanism%2520of%2520this%2520work%2520is%250Aprincipled%2520feature%2520disentanglement%252C%2520which%2520aligns%2520the%2520design%2520of%2520the%2520architecture%250Awith%2520the%2520complexity%2520of%2520the%2520features.%2520A%2520powerful%2520many-to-one%2520stream%2520is%250Aconstructed%2520for%2520the%2520extraction%2520of%2520complex%2520complementary%2520features%252C%2520while%250Autilizing%2520parallel%252C%2520one-to-one%2520streams%2520to%2520process%2520modality-specific%250Ainformation.%2520These%2520disentangled%2520features%2520are%2520dynamically%2520integrated%2520into%2520a%250Acommon%2520latent%2520space%2520by%2520a%2520channel%2520attention-based%2520fusion%2520module%2520%2528CAFF%2529%2520and%2520then%250Atransformed%2520via%2520a%2520modality%2520infuser%2520to%2520generate%2520the%2520target%2520sequence.%2520We%250Avalidated%2520our%2520framework%2520on%2520public%2520datasets%2520of%2520both%2520healthy%2520and%2520pathological%250Abrain%2520MRI.%2520Quantitative%2520and%2520qualitative%2520results%2520show%2520that%2520HF-GAN%2520achieves%250Astate-of-the-art%2520performance%252C%2520with%2520our%25202D%2520slice-based%2520framework%2520notably%250Aoutperforming%2520a%2520leading%25203D%2520volumetric%2520model.%2520Furthermore%252C%2520the%2520utilization%2520of%250AHF-GAN%2520for%2520data%2520imputation%2520substantially%2520improves%2520the%2520performance%2520of%2520the%250Adownstream%2520brain%2520tumor%2520segmentation%2520task%252C%2520demonstrating%2520its%2520clinical%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Principled%20Feature%20Disentanglement%20for%20High-Fidelity%20Unified%20Brain%20MRI%0A%20%20Synthesis&entry.906535625=Jihoon%20Cho%20and%20Jonghye%20Woo%20and%20Jinah%20Park&entry.1292438233=%20%20Multisequence%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20a%20more%20reliable%0Adiagnosis%20in%20clinical%20applications%20through%20complementary%20information%20across%0Asequences.%20However%2C%20in%20practice%2C%20the%20absence%20of%20certain%20MR%20sequences%20is%20a%0Acommon%20problem%20that%20can%20lead%20to%20inconsistent%20analysis%20results.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20unified%20framework%20for%20synthesizing%20multisequence%20MR%20images%2C%0Acalled%20hybrid-fusion%20GAN%20%28HF-GAN%29.%20The%20fundamental%20mechanism%20of%20this%20work%20is%0Aprincipled%20feature%20disentanglement%2C%20which%20aligns%20the%20design%20of%20the%20architecture%0Awith%20the%20complexity%20of%20the%20features.%20A%20powerful%20many-to-one%20stream%20is%0Aconstructed%20for%20the%20extraction%20of%20complex%20complementary%20features%2C%20while%0Autilizing%20parallel%2C%20one-to-one%20streams%20to%20process%20modality-specific%0Ainformation.%20These%20disentangled%20features%20are%20dynamically%20integrated%20into%20a%0Acommon%20latent%20space%20by%20a%20channel%20attention-based%20fusion%20module%20%28CAFF%29%20and%20then%0Atransformed%20via%20a%20modality%20infuser%20to%20generate%20the%20target%20sequence.%20We%0Avalidated%20our%20framework%20on%20public%20datasets%20of%20both%20healthy%20and%20pathological%0Abrain%20MRI.%20Quantitative%20and%20qualitative%20results%20show%20that%20HF-GAN%20achieves%0Astate-of-the-art%20performance%2C%20with%20our%202D%20slice-based%20framework%20notably%0Aoutperforming%20a%20leading%203D%20volumetric%20model.%20Furthermore%2C%20the%20utilization%20of%0AHF-GAN%20for%20data%20imputation%20substantially%20improves%20the%20performance%20of%20the%0Adownstream%20brain%20tumor%20segmentation%20task%2C%20demonstrating%20its%20clinical%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14954v2&entry.124074799=Read"},
{"title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with\n  Denoising Models", "author": "Vincenzo Carletti and Pasquale Foggia and Carlo Mazzocca and Giuseppe Parrella and Mario Vento", "abstract": "  Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric.\n", "link": "http://arxiv.org/abs/2510.17621v1", "date": "2025-10-20", "relevancy": 2.0955, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5305}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5209}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUIDE%3A%20Enhancing%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning%20with%0A%20%20Denoising%20Models&body=Title%3A%20GUIDE%3A%20Enhancing%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning%20with%0A%20%20Denoising%20Models%0AAuthor%3A%20Vincenzo%20Carletti%20and%20Pasquale%20Foggia%20and%20Carlo%20Mazzocca%20and%20Giuseppe%20Parrella%20and%20Mario%20Vento%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20Machine%20Learning%0A%28ML%29%20models%20across%20multiple%20clients%20while%20preserving%20their%20privacy.%20Rather%20than%0Asharing%20raw%20data%2C%20federated%20clients%20transmit%20locally%20computed%20updates%20to%20train%0Athe%20global%20model.%20Although%20this%20paradigm%20should%20provide%20stronger%20privacy%0Aguarantees%20than%20centralized%20ML%2C%20client%20updates%20remain%20vulnerable%20to%20privacy%0Aleakage.%20Adversaries%20can%20exploit%20them%20to%20infer%20sensitive%20properties%20about%20the%0Atraining%20data%20or%20even%20to%20reconstruct%20the%20original%20inputs%20via%20Gradient%20Inversion%0AAttacks%20%28GIAs%29.%20Under%20the%20honest-butcurious%20threat%20model%2C%20GIAs%20attempt%20to%0Areconstruct%20training%20data%20by%20reversing%20intermediate%20updates%20using%0Aoptimizationbased%20techniques.%20We%20observe%20that%20these%20approaches%20usually%0Areconstruct%20noisy%20approximations%20of%20the%20original%20inputs%2C%20whose%20quality%20can%20be%0Aenhanced%20with%20specialized%20denoising%20models.%20This%20paper%20presents%20Gradient%20Update%0AInversion%20with%20DEnoising%20%28GUIDE%29%2C%20a%20novel%20methodology%20that%20leverages%20diffusion%0Amodels%20as%20denoising%20tools%20to%20improve%20image%20reconstruction%20attacks%20in%20FL.%20GUIDE%0Acan%20be%20integrated%20into%20any%20GIAs%20that%20exploits%20surrogate%20datasets%2C%20a%20widely%0Aadopted%20assumption%20in%20GIAs%20literature.%20We%20comprehensively%20evaluate%20our%20approach%0Ain%20two%20attack%20scenarios%20that%20use%20different%20FL%20algorithms%2C%20models%2C%20and%20datasets.%0AOur%20results%20demonstrate%20that%20GUIDE%20integrates%20seamlessly%20with%20two%20state-ofthe-%0Aart%20GIAs%2C%20substantially%20improving%20reconstruction%20quality%20across%20multiple%0Ametrics.%20Specifically%2C%20GUIDE%20achieves%20up%20to%2046%25%20higher%20perceptual%20similarity%2C%0Aas%20measured%20by%20the%20DreamSim%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUIDE%253A%2520Enhancing%2520Gradient%2520Inversion%2520Attacks%2520in%2520Federated%2520Learning%2520with%250A%2520%2520Denoising%2520Models%26entry.906535625%3DVincenzo%2520Carletti%2520and%2520Pasquale%2520Foggia%2520and%2520Carlo%2520Mazzocca%2520and%2520Giuseppe%2520Parrella%2520and%2520Mario%2520Vento%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520training%2520of%2520Machine%2520Learning%250A%2528ML%2529%2520models%2520across%2520multiple%2520clients%2520while%2520preserving%2520their%2520privacy.%2520Rather%2520than%250Asharing%2520raw%2520data%252C%2520federated%2520clients%2520transmit%2520locally%2520computed%2520updates%2520to%2520train%250Athe%2520global%2520model.%2520Although%2520this%2520paradigm%2520should%2520provide%2520stronger%2520privacy%250Aguarantees%2520than%2520centralized%2520ML%252C%2520client%2520updates%2520remain%2520vulnerable%2520to%2520privacy%250Aleakage.%2520Adversaries%2520can%2520exploit%2520them%2520to%2520infer%2520sensitive%2520properties%2520about%2520the%250Atraining%2520data%2520or%2520even%2520to%2520reconstruct%2520the%2520original%2520inputs%2520via%2520Gradient%2520Inversion%250AAttacks%2520%2528GIAs%2529.%2520Under%2520the%2520honest-butcurious%2520threat%2520model%252C%2520GIAs%2520attempt%2520to%250Areconstruct%2520training%2520data%2520by%2520reversing%2520intermediate%2520updates%2520using%250Aoptimizationbased%2520techniques.%2520We%2520observe%2520that%2520these%2520approaches%2520usually%250Areconstruct%2520noisy%2520approximations%2520of%2520the%2520original%2520inputs%252C%2520whose%2520quality%2520can%2520be%250Aenhanced%2520with%2520specialized%2520denoising%2520models.%2520This%2520paper%2520presents%2520Gradient%2520Update%250AInversion%2520with%2520DEnoising%2520%2528GUIDE%2529%252C%2520a%2520novel%2520methodology%2520that%2520leverages%2520diffusion%250Amodels%2520as%2520denoising%2520tools%2520to%2520improve%2520image%2520reconstruction%2520attacks%2520in%2520FL.%2520GUIDE%250Acan%2520be%2520integrated%2520into%2520any%2520GIAs%2520that%2520exploits%2520surrogate%2520datasets%252C%2520a%2520widely%250Aadopted%2520assumption%2520in%2520GIAs%2520literature.%2520We%2520comprehensively%2520evaluate%2520our%2520approach%250Ain%2520two%2520attack%2520scenarios%2520that%2520use%2520different%2520FL%2520algorithms%252C%2520models%252C%2520and%2520datasets.%250AOur%2520results%2520demonstrate%2520that%2520GUIDE%2520integrates%2520seamlessly%2520with%2520two%2520state-ofthe-%250Aart%2520GIAs%252C%2520substantially%2520improving%2520reconstruction%2520quality%2520across%2520multiple%250Ametrics.%2520Specifically%252C%2520GUIDE%2520achieves%2520up%2520to%252046%2525%2520higher%2520perceptual%2520similarity%252C%250Aas%2520measured%2520by%2520the%2520DreamSim%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE%3A%20Enhancing%20Gradient%20Inversion%20Attacks%20in%20Federated%20Learning%20with%0A%20%20Denoising%20Models&entry.906535625=Vincenzo%20Carletti%20and%20Pasquale%20Foggia%20and%20Carlo%20Mazzocca%20and%20Giuseppe%20Parrella%20and%20Mario%20Vento&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20training%20of%20Machine%20Learning%0A%28ML%29%20models%20across%20multiple%20clients%20while%20preserving%20their%20privacy.%20Rather%20than%0Asharing%20raw%20data%2C%20federated%20clients%20transmit%20locally%20computed%20updates%20to%20train%0Athe%20global%20model.%20Although%20this%20paradigm%20should%20provide%20stronger%20privacy%0Aguarantees%20than%20centralized%20ML%2C%20client%20updates%20remain%20vulnerable%20to%20privacy%0Aleakage.%20Adversaries%20can%20exploit%20them%20to%20infer%20sensitive%20properties%20about%20the%0Atraining%20data%20or%20even%20to%20reconstruct%20the%20original%20inputs%20via%20Gradient%20Inversion%0AAttacks%20%28GIAs%29.%20Under%20the%20honest-butcurious%20threat%20model%2C%20GIAs%20attempt%20to%0Areconstruct%20training%20data%20by%20reversing%20intermediate%20updates%20using%0Aoptimizationbased%20techniques.%20We%20observe%20that%20these%20approaches%20usually%0Areconstruct%20noisy%20approximations%20of%20the%20original%20inputs%2C%20whose%20quality%20can%20be%0Aenhanced%20with%20specialized%20denoising%20models.%20This%20paper%20presents%20Gradient%20Update%0AInversion%20with%20DEnoising%20%28GUIDE%29%2C%20a%20novel%20methodology%20that%20leverages%20diffusion%0Amodels%20as%20denoising%20tools%20to%20improve%20image%20reconstruction%20attacks%20in%20FL.%20GUIDE%0Acan%20be%20integrated%20into%20any%20GIAs%20that%20exploits%20surrogate%20datasets%2C%20a%20widely%0Aadopted%20assumption%20in%20GIAs%20literature.%20We%20comprehensively%20evaluate%20our%20approach%0Ain%20two%20attack%20scenarios%20that%20use%20different%20FL%20algorithms%2C%20models%2C%20and%20datasets.%0AOur%20results%20demonstrate%20that%20GUIDE%20integrates%20seamlessly%20with%20two%20state-ofthe-%0Aart%20GIAs%2C%20substantially%20improving%20reconstruction%20quality%20across%20multiple%0Ametrics.%20Specifically%2C%20GUIDE%20achieves%20up%20to%2046%25%20higher%20perceptual%20similarity%2C%0Aas%20measured%20by%20the%20DreamSim%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17621v1&entry.124074799=Read"},
{"title": "Test-Time Training for Speech Enhancement", "author": "Avishkar Behera and Riya Ann Easow and Venkatesh Parvathala and K. Sri Rama Murty", "abstract": "  This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing.\n", "link": "http://arxiv.org/abs/2508.01847v2", "date": "2025-10-20", "relevancy": 2.0891, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5422}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5124}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Training%20for%20Speech%20Enhancement&body=Title%3A%20Test-Time%20Training%20for%20Speech%20Enhancement%0AAuthor%3A%20Avishkar%20Behera%20and%20Riya%20Ann%20Easow%20and%20Venkatesh%20Parvathala%20and%20K.%20Sri%20Rama%20Murty%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20application%20of%20Test-Time%20Training%20%28TTT%29%20for%0ASpeech%20Enhancement%2C%20addressing%20the%20challenges%20posed%20by%20unpredictable%20noise%0Aconditions%20and%20domain%20shifts.%20This%20method%20combines%20a%20main%20speech%20enhancement%0Atask%20with%20a%20self-supervised%20auxiliary%20task%20in%20a%20Y-shaped%20architecture.%20The%0Amodel%20dynamically%20adapts%20to%20new%20domains%20during%20inference%20time%20by%20optimizing%20the%0Aproposed%20self-supervised%20tasks%20like%20noise-augmented%20signal%20reconstruction%20or%0Amasked%20spectrogram%20prediction%2C%20bypassing%20the%20need%20for%20labeled%20data.%20We%20further%0Aintroduce%20various%20TTT%20strategies%20offering%20a%20trade-off%20between%20adaptation%20and%0Aefficiency.%20Evaluations%20across%20synthetic%20and%20real-world%20datasets%20show%0Aconsistent%20improvements%20across%20speech%20quality%20metrics%2C%20outperforming%20the%0Abaseline%20model.%20This%20work%20highlights%20the%20effectiveness%20of%20TTT%20in%20speech%0Aenhancement%2C%20providing%20insights%20for%20future%20research%20in%20adaptive%20and%20robust%0Aspeech%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Training%2520for%2520Speech%2520Enhancement%26entry.906535625%3DAvishkar%2520Behera%2520and%2520Riya%2520Ann%2520Easow%2520and%2520Venkatesh%2520Parvathala%2520and%2520K.%2520Sri%2520Rama%2520Murty%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520application%2520of%2520Test-Time%2520Training%2520%2528TTT%2529%2520for%250ASpeech%2520Enhancement%252C%2520addressing%2520the%2520challenges%2520posed%2520by%2520unpredictable%2520noise%250Aconditions%2520and%2520domain%2520shifts.%2520This%2520method%2520combines%2520a%2520main%2520speech%2520enhancement%250Atask%2520with%2520a%2520self-supervised%2520auxiliary%2520task%2520in%2520a%2520Y-shaped%2520architecture.%2520The%250Amodel%2520dynamically%2520adapts%2520to%2520new%2520domains%2520during%2520inference%2520time%2520by%2520optimizing%2520the%250Aproposed%2520self-supervised%2520tasks%2520like%2520noise-augmented%2520signal%2520reconstruction%2520or%250Amasked%2520spectrogram%2520prediction%252C%2520bypassing%2520the%2520need%2520for%2520labeled%2520data.%2520We%2520further%250Aintroduce%2520various%2520TTT%2520strategies%2520offering%2520a%2520trade-off%2520between%2520adaptation%2520and%250Aefficiency.%2520Evaluations%2520across%2520synthetic%2520and%2520real-world%2520datasets%2520show%250Aconsistent%2520improvements%2520across%2520speech%2520quality%2520metrics%252C%2520outperforming%2520the%250Abaseline%2520model.%2520This%2520work%2520highlights%2520the%2520effectiveness%2520of%2520TTT%2520in%2520speech%250Aenhancement%252C%2520providing%2520insights%2520for%2520future%2520research%2520in%2520adaptive%2520and%2520robust%250Aspeech%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Training%20for%20Speech%20Enhancement&entry.906535625=Avishkar%20Behera%20and%20Riya%20Ann%20Easow%20and%20Venkatesh%20Parvathala%20and%20K.%20Sri%20Rama%20Murty&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20application%20of%20Test-Time%20Training%20%28TTT%29%20for%0ASpeech%20Enhancement%2C%20addressing%20the%20challenges%20posed%20by%20unpredictable%20noise%0Aconditions%20and%20domain%20shifts.%20This%20method%20combines%20a%20main%20speech%20enhancement%0Atask%20with%20a%20self-supervised%20auxiliary%20task%20in%20a%20Y-shaped%20architecture.%20The%0Amodel%20dynamically%20adapts%20to%20new%20domains%20during%20inference%20time%20by%20optimizing%20the%0Aproposed%20self-supervised%20tasks%20like%20noise-augmented%20signal%20reconstruction%20or%0Amasked%20spectrogram%20prediction%2C%20bypassing%20the%20need%20for%20labeled%20data.%20We%20further%0Aintroduce%20various%20TTT%20strategies%20offering%20a%20trade-off%20between%20adaptation%20and%0Aefficiency.%20Evaluations%20across%20synthetic%20and%20real-world%20datasets%20show%0Aconsistent%20improvements%20across%20speech%20quality%20metrics%2C%20outperforming%20the%0Abaseline%20model.%20This%20work%20highlights%20the%20effectiveness%20of%20TTT%20in%20speech%0Aenhancement%2C%20providing%20insights%20for%20future%20research%20in%20adaptive%20and%20robust%0Aspeech%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01847v2&entry.124074799=Read"},
{"title": "Conveying Meaning through Gestures: An Investigation into Semantic\n  Co-Speech Gesture Generation", "author": "Hendric Voss and Lisa Michelle Bohnenkamp and Stefan Kopp", "abstract": "  This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization.\n", "link": "http://arxiv.org/abs/2510.17599v1", "date": "2025-10-20", "relevancy": 2.0827, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5319}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5175}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conveying%20Meaning%20through%20Gestures%3A%20An%20Investigation%20into%20Semantic%0A%20%20Co-Speech%20Gesture%20Generation&body=Title%3A%20Conveying%20Meaning%20through%20Gestures%3A%20An%20Investigation%20into%20Semantic%0A%20%20Co-Speech%20Gesture%20Generation%0AAuthor%3A%20Hendric%20Voss%20and%20Lisa%20Michelle%20Bohnenkamp%20and%20Stefan%20Kopp%0AAbstract%3A%20%20%20This%20study%20explores%20two%20frameworks%20for%20co-speech%20gesture%20generation%2C%20AQ-GT%0Aand%20its%20semantically-augmented%20variant%20AQ-GT-a%2C%20to%20evaluate%20their%20ability%20to%0Aconvey%20meaning%20through%20gestures%20and%20how%20humans%20perceive%20the%20resulting%0Amovements.%20Using%20sentences%20from%20the%20SAGA%20spatial%20communication%20corpus%2C%0Acontextually%20similar%20sentences%2C%20and%20novel%20movement-focused%20sentences%2C%20we%0Aconducted%20a%20user-centered%20evaluation%20of%20concept%20recognition%20and%20human-likeness.%0AResults%20revealed%20a%20nuanced%20relationship%20between%20semantic%20annotations%20and%0Aperformance.%20The%20original%20AQ-GT%20framework%2C%20lacking%20explicit%20semantic%20input%2C%20was%0Asurprisingly%20more%20effective%20at%20conveying%20concepts%20within%20its%20training%20domain.%0AConversely%2C%20the%20AQ-GT-a%20framework%20demonstrated%20better%20generalization%2C%0Aparticularly%20for%20representing%20shape%20and%20size%20in%20novel%20contexts.%20While%0Aparticipants%20rated%20gestures%20from%20AQ-GT-a%20as%20more%20expressive%20and%20helpful%2C%20they%0Adid%20not%20perceive%20them%20as%20more%20human-like.%20These%20findings%20suggest%20that%20explicit%0Asemantic%20enrichment%20does%20not%20guarantee%20improved%20gesture%20generation%20and%20that%20its%0Aeffectiveness%20is%20highly%20dependent%20on%20the%20context%2C%20indicating%20a%20potential%0Atrade-off%20between%20specialization%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConveying%2520Meaning%2520through%2520Gestures%253A%2520An%2520Investigation%2520into%2520Semantic%250A%2520%2520Co-Speech%2520Gesture%2520Generation%26entry.906535625%3DHendric%2520Voss%2520and%2520Lisa%2520Michelle%2520Bohnenkamp%2520and%2520Stefan%2520Kopp%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520two%2520frameworks%2520for%2520co-speech%2520gesture%2520generation%252C%2520AQ-GT%250Aand%2520its%2520semantically-augmented%2520variant%2520AQ-GT-a%252C%2520to%2520evaluate%2520their%2520ability%2520to%250Aconvey%2520meaning%2520through%2520gestures%2520and%2520how%2520humans%2520perceive%2520the%2520resulting%250Amovements.%2520Using%2520sentences%2520from%2520the%2520SAGA%2520spatial%2520communication%2520corpus%252C%250Acontextually%2520similar%2520sentences%252C%2520and%2520novel%2520movement-focused%2520sentences%252C%2520we%250Aconducted%2520a%2520user-centered%2520evaluation%2520of%2520concept%2520recognition%2520and%2520human-likeness.%250AResults%2520revealed%2520a%2520nuanced%2520relationship%2520between%2520semantic%2520annotations%2520and%250Aperformance.%2520The%2520original%2520AQ-GT%2520framework%252C%2520lacking%2520explicit%2520semantic%2520input%252C%2520was%250Asurprisingly%2520more%2520effective%2520at%2520conveying%2520concepts%2520within%2520its%2520training%2520domain.%250AConversely%252C%2520the%2520AQ-GT-a%2520framework%2520demonstrated%2520better%2520generalization%252C%250Aparticularly%2520for%2520representing%2520shape%2520and%2520size%2520in%2520novel%2520contexts.%2520While%250Aparticipants%2520rated%2520gestures%2520from%2520AQ-GT-a%2520as%2520more%2520expressive%2520and%2520helpful%252C%2520they%250Adid%2520not%2520perceive%2520them%2520as%2520more%2520human-like.%2520These%2520findings%2520suggest%2520that%2520explicit%250Asemantic%2520enrichment%2520does%2520not%2520guarantee%2520improved%2520gesture%2520generation%2520and%2520that%2520its%250Aeffectiveness%2520is%2520highly%2520dependent%2520on%2520the%2520context%252C%2520indicating%2520a%2520potential%250Atrade-off%2520between%2520specialization%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conveying%20Meaning%20through%20Gestures%3A%20An%20Investigation%20into%20Semantic%0A%20%20Co-Speech%20Gesture%20Generation&entry.906535625=Hendric%20Voss%20and%20Lisa%20Michelle%20Bohnenkamp%20and%20Stefan%20Kopp&entry.1292438233=%20%20This%20study%20explores%20two%20frameworks%20for%20co-speech%20gesture%20generation%2C%20AQ-GT%0Aand%20its%20semantically-augmented%20variant%20AQ-GT-a%2C%20to%20evaluate%20their%20ability%20to%0Aconvey%20meaning%20through%20gestures%20and%20how%20humans%20perceive%20the%20resulting%0Amovements.%20Using%20sentences%20from%20the%20SAGA%20spatial%20communication%20corpus%2C%0Acontextually%20similar%20sentences%2C%20and%20novel%20movement-focused%20sentences%2C%20we%0Aconducted%20a%20user-centered%20evaluation%20of%20concept%20recognition%20and%20human-likeness.%0AResults%20revealed%20a%20nuanced%20relationship%20between%20semantic%20annotations%20and%0Aperformance.%20The%20original%20AQ-GT%20framework%2C%20lacking%20explicit%20semantic%20input%2C%20was%0Asurprisingly%20more%20effective%20at%20conveying%20concepts%20within%20its%20training%20domain.%0AConversely%2C%20the%20AQ-GT-a%20framework%20demonstrated%20better%20generalization%2C%0Aparticularly%20for%20representing%20shape%20and%20size%20in%20novel%20contexts.%20While%0Aparticipants%20rated%20gestures%20from%20AQ-GT-a%20as%20more%20expressive%20and%20helpful%2C%20they%0Adid%20not%20perceive%20them%20as%20more%20human-like.%20These%20findings%20suggest%20that%20explicit%0Asemantic%20enrichment%20does%20not%20guarantee%20improved%20gesture%20generation%20and%20that%20its%0Aeffectiveness%20is%20highly%20dependent%20on%20the%20context%2C%20indicating%20a%20potential%0Atrade-off%20between%20specialization%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17599v1&entry.124074799=Read"},
{"title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in\n  Historic Landscapes Using High-Resolution DEM Derivatives", "author": "Zexian Huang and Mashnoon Islam and Brian Armstrong and Kourosh Khoshelham and Martin Tomko", "abstract": "  Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.\n", "link": "http://arxiv.org/abs/2510.17644v1", "date": "2025-10-20", "relevancy": 2.0631, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5104}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Pre-training%20for%20Mapping%20of%20Archaeological%20Stone%20Wall%20in%0A%20%20Historic%20Landscapes%20Using%20High-Resolution%20DEM%20Derivatives&body=Title%3A%20Self-supervised%20Pre-training%20for%20Mapping%20of%20Archaeological%20Stone%20Wall%20in%0A%20%20Historic%20Landscapes%20Using%20High-Resolution%20DEM%20Derivatives%0AAuthor%3A%20Zexian%20Huang%20and%20Mashnoon%20Islam%20and%20Brian%20Armstrong%20and%20Kourosh%20Khoshelham%20and%20Martin%20Tomko%0AAbstract%3A%20%20%20Dry-stone%20walls%20hold%20significant%20heritage%20and%20environmental%20value.%20Mapping%0Athese%20structures%20is%20essential%20for%20ecosystem%20preservation%20and%20wildfire%0Amanagement%20in%20Australia.%20Yet%2C%20many%20walls%20remain%20unidentified%20due%20to%20their%0Ainaccessibility%20and%20the%20high%20cost%20of%20manual%20mapping.%20Deep%20learning-based%0Asegmentation%20offers%20a%20scalable%20solution%2C%20but%20two%20major%20challenges%20persist%3A%20%281%29%0Avisual%20occlusion%20of%20low-lying%20walls%20by%20dense%20vegetation%2C%20and%20%282%29%20limited%0Alabeled%20data%20for%20supervised%20training.%20We%20propose%20DINO-CV%2C%20a%20segmentation%0Aframework%20for%20automatic%20mapping%20of%20low-lying%20dry-stone%20walls%20using%0Ahigh-resolution%20Airborne%20LiDAR-derived%20digital%20elevation%20models%20%28DEMs%29.%20DEMs%0Aovercome%20visual%20occlusion%20by%20capturing%20terrain%20structures%20hidden%20beneath%0Avegetation%2C%20enabling%20analysis%20of%20structural%20rather%20than%20spectral%20cues.%20DINO-CV%0Aintroduces%20a%20self-supervised%20cross-view%20pre-training%20strategy%20based%20on%0Aknowledge%20distillation%20to%20mitigate%20data%20scarcity.%20It%20learns%20invariant%20visual%0Aand%20geometric%20representations%20across%20multiple%20DEM%20derivatives%2C%20supporting%0Avarious%20vision%20backbones%20including%20ResNet%2C%20Wide%20ResNet%2C%20and%20Vision%0ATransformers.%20Applied%20to%20the%20UNESCO%20World%20Heritage%20cultural%20landscape%20of%20Budj%0ABim%2C%20Victoria%2C%20the%20method%20identifies%20one%20of%20Australia%27s%20densest%20collections%20of%0Acolonial%20dry-stone%20walls%20beyond%20Indigenous%20heritage%20contexts.%20DINO-CV%20achieves%0Aa%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2068.6%25%20on%20test%20areas%20and%20maintains%0A63.8%25%20mIoU%20when%20fine-tuned%20with%20only%2010%25%20labeled%20data.%20These%20results%0Ademonstrate%20the%20potential%20of%20self-supervised%20learning%20on%20high-resolution%20DEM%0Aderivatives%20for%20automated%20dry-stone%20wall%20mapping%20in%20vegetated%20and%20heritage-rich%0Aenvironments%20with%20scarce%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Pre-training%2520for%2520Mapping%2520of%2520Archaeological%2520Stone%2520Wall%2520in%250A%2520%2520Historic%2520Landscapes%2520Using%2520High-Resolution%2520DEM%2520Derivatives%26entry.906535625%3DZexian%2520Huang%2520and%2520Mashnoon%2520Islam%2520and%2520Brian%2520Armstrong%2520and%2520Kourosh%2520Khoshelham%2520and%2520Martin%2520Tomko%26entry.1292438233%3D%2520%2520Dry-stone%2520walls%2520hold%2520significant%2520heritage%2520and%2520environmental%2520value.%2520Mapping%250Athese%2520structures%2520is%2520essential%2520for%2520ecosystem%2520preservation%2520and%2520wildfire%250Amanagement%2520in%2520Australia.%2520Yet%252C%2520many%2520walls%2520remain%2520unidentified%2520due%2520to%2520their%250Ainaccessibility%2520and%2520the%2520high%2520cost%2520of%2520manual%2520mapping.%2520Deep%2520learning-based%250Asegmentation%2520offers%2520a%2520scalable%2520solution%252C%2520but%2520two%2520major%2520challenges%2520persist%253A%2520%25281%2529%250Avisual%2520occlusion%2520of%2520low-lying%2520walls%2520by%2520dense%2520vegetation%252C%2520and%2520%25282%2529%2520limited%250Alabeled%2520data%2520for%2520supervised%2520training.%2520We%2520propose%2520DINO-CV%252C%2520a%2520segmentation%250Aframework%2520for%2520automatic%2520mapping%2520of%2520low-lying%2520dry-stone%2520walls%2520using%250Ahigh-resolution%2520Airborne%2520LiDAR-derived%2520digital%2520elevation%2520models%2520%2528DEMs%2529.%2520DEMs%250Aovercome%2520visual%2520occlusion%2520by%2520capturing%2520terrain%2520structures%2520hidden%2520beneath%250Avegetation%252C%2520enabling%2520analysis%2520of%2520structural%2520rather%2520than%2520spectral%2520cues.%2520DINO-CV%250Aintroduces%2520a%2520self-supervised%2520cross-view%2520pre-training%2520strategy%2520based%2520on%250Aknowledge%2520distillation%2520to%2520mitigate%2520data%2520scarcity.%2520It%2520learns%2520invariant%2520visual%250Aand%2520geometric%2520representations%2520across%2520multiple%2520DEM%2520derivatives%252C%2520supporting%250Avarious%2520vision%2520backbones%2520including%2520ResNet%252C%2520Wide%2520ResNet%252C%2520and%2520Vision%250ATransformers.%2520Applied%2520to%2520the%2520UNESCO%2520World%2520Heritage%2520cultural%2520landscape%2520of%2520Budj%250ABim%252C%2520Victoria%252C%2520the%2520method%2520identifies%2520one%2520of%2520Australia%2527s%2520densest%2520collections%2520of%250Acolonial%2520dry-stone%2520walls%2520beyond%2520Indigenous%2520heritage%2520contexts.%2520DINO-CV%2520achieves%250Aa%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252068.6%2525%2520on%2520test%2520areas%2520and%2520maintains%250A63.8%2525%2520mIoU%2520when%2520fine-tuned%2520with%2520only%252010%2525%2520labeled%2520data.%2520These%2520results%250Ademonstrate%2520the%2520potential%2520of%2520self-supervised%2520learning%2520on%2520high-resolution%2520DEM%250Aderivatives%2520for%2520automated%2520dry-stone%2520wall%2520mapping%2520in%2520vegetated%2520and%2520heritage-rich%250Aenvironments%2520with%2520scarce%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Pre-training%20for%20Mapping%20of%20Archaeological%20Stone%20Wall%20in%0A%20%20Historic%20Landscapes%20Using%20High-Resolution%20DEM%20Derivatives&entry.906535625=Zexian%20Huang%20and%20Mashnoon%20Islam%20and%20Brian%20Armstrong%20and%20Kourosh%20Khoshelham%20and%20Martin%20Tomko&entry.1292438233=%20%20Dry-stone%20walls%20hold%20significant%20heritage%20and%20environmental%20value.%20Mapping%0Athese%20structures%20is%20essential%20for%20ecosystem%20preservation%20and%20wildfire%0Amanagement%20in%20Australia.%20Yet%2C%20many%20walls%20remain%20unidentified%20due%20to%20their%0Ainaccessibility%20and%20the%20high%20cost%20of%20manual%20mapping.%20Deep%20learning-based%0Asegmentation%20offers%20a%20scalable%20solution%2C%20but%20two%20major%20challenges%20persist%3A%20%281%29%0Avisual%20occlusion%20of%20low-lying%20walls%20by%20dense%20vegetation%2C%20and%20%282%29%20limited%0Alabeled%20data%20for%20supervised%20training.%20We%20propose%20DINO-CV%2C%20a%20segmentation%0Aframework%20for%20automatic%20mapping%20of%20low-lying%20dry-stone%20walls%20using%0Ahigh-resolution%20Airborne%20LiDAR-derived%20digital%20elevation%20models%20%28DEMs%29.%20DEMs%0Aovercome%20visual%20occlusion%20by%20capturing%20terrain%20structures%20hidden%20beneath%0Avegetation%2C%20enabling%20analysis%20of%20structural%20rather%20than%20spectral%20cues.%20DINO-CV%0Aintroduces%20a%20self-supervised%20cross-view%20pre-training%20strategy%20based%20on%0Aknowledge%20distillation%20to%20mitigate%20data%20scarcity.%20It%20learns%20invariant%20visual%0Aand%20geometric%20representations%20across%20multiple%20DEM%20derivatives%2C%20supporting%0Avarious%20vision%20backbones%20including%20ResNet%2C%20Wide%20ResNet%2C%20and%20Vision%0ATransformers.%20Applied%20to%20the%20UNESCO%20World%20Heritage%20cultural%20landscape%20of%20Budj%0ABim%2C%20Victoria%2C%20the%20method%20identifies%20one%20of%20Australia%27s%20densest%20collections%20of%0Acolonial%20dry-stone%20walls%20beyond%20Indigenous%20heritage%20contexts.%20DINO-CV%20achieves%0Aa%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2068.6%25%20on%20test%20areas%20and%20maintains%0A63.8%25%20mIoU%20when%20fine-tuned%20with%20only%2010%25%20labeled%20data.%20These%20results%0Ademonstrate%20the%20potential%20of%20self-supervised%20learning%20on%20high-resolution%20DEM%0Aderivatives%20for%20automated%20dry-stone%20wall%20mapping%20in%20vegetated%20and%20heritage-rich%0Aenvironments%20with%20scarce%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17644v1&entry.124074799=Read"},
{"title": "AcademicEval: Live Long-Context LLM Benchmark", "author": "Haozhen Zhang and Tao Feng and Pengrui Han and Jiaxuan You", "abstract": "  Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval\n", "link": "http://arxiv.org/abs/2510.17725v1", "date": "2025-10-20", "relevancy": 2.0467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AcademicEval%3A%20Live%20Long-Context%20LLM%20Benchmark&body=Title%3A%20AcademicEval%3A%20Live%20Long-Context%20LLM%20Benchmark%0AAuthor%3A%20Haozhen%20Zhang%20and%20Tao%20Feng%20and%20Pengrui%20Han%20and%20Jiaxuan%20You%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20remarkable%20performance%20in%0Along-context%20understanding.%20However%2C%20current%20long-context%20LLM%20benchmarks%20are%0Alimited%20by%20rigid%20context%20length%2C%20labor-intensive%20annotation%2C%20and%20the%20pressing%0Achallenge%20of%20label%20leakage%20issues%20during%20LLM%20training.%20Therefore%2C%20we%20propose%0A%5Ctextsc%7BAcademicEval%7D%2C%20a%20live%20benchmark%20for%20evaluating%20LLMs%20over%20long-context%0Ageneration%20tasks.%20%5Ctextsc%7BAcademicEval%7D%20adopts%20papers%20on%20arXiv%20to%20introduce%0Aseveral%20academic%20writing%20tasks%20with%20long-context%20inputs%2C%20%5Ctextit%7Bi.e.%7D%2C%0A%5Ctextsc%7BTitle%7D%2C%20%5Ctextsc%7BAbstract%7D%2C%20%5Ctextsc%7BIntroduction%7D%2C%20and%20%5Ctextsc%7BRelated%0AWork%7D%2C%20which%20cover%20a%20wide%20range%20of%20abstraction%20levels%20and%20require%20no%20manual%0Alabeling.%20Moreover%2C%20%5Ctextsc%7BAcademicEval%7D%20integrates%20high-quality%20and%0Aexpert-curated%20few-shot%20demonstrations%20from%20a%20collected%20co-author%20graph%20to%0Aenable%20flexible%20context%20length.%20Especially%2C%20%5Ctextsc%7BAcademicEval%7D%20features%20an%0Aefficient%20live%20evaluation%2C%20ensuring%20no%20label%20leakage.%20We%20conduct%20a%20holistic%0Aevaluation%20on%20%5Ctextsc%7BAcademicEval%7D%2C%20and%20the%20results%20illustrate%20that%20LLMs%0Aperform%20poorly%20on%20tasks%20with%20hierarchical%20abstraction%20levels%20and%20tend%20to%0Astruggle%20with%20long%20few-shot%20demonstrations%2C%20highlighting%20the%20challenge%20of%20our%0Abenchmark.%20Through%20experimental%20analysis%2C%20we%20also%20reveal%20some%20insights%20for%0Aenhancing%20LLMs%27%20long-context%20modeling%20capabilities.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ulab-uiuc/AcademicEval%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAcademicEval%253A%2520Live%2520Long-Context%2520LLM%2520Benchmark%26entry.906535625%3DHaozhen%2520Zhang%2520and%2520Tao%2520Feng%2520and%2520Pengrui%2520Han%2520and%2520Jiaxuan%2520You%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520achieved%2520remarkable%2520performance%2520in%250Along-context%2520understanding.%2520However%252C%2520current%2520long-context%2520LLM%2520benchmarks%2520are%250Alimited%2520by%2520rigid%2520context%2520length%252C%2520labor-intensive%2520annotation%252C%2520and%2520the%2520pressing%250Achallenge%2520of%2520label%2520leakage%2520issues%2520during%2520LLM%2520training.%2520Therefore%252C%2520we%2520propose%250A%255Ctextsc%257BAcademicEval%257D%252C%2520a%2520live%2520benchmark%2520for%2520evaluating%2520LLMs%2520over%2520long-context%250Ageneration%2520tasks.%2520%255Ctextsc%257BAcademicEval%257D%2520adopts%2520papers%2520on%2520arXiv%2520to%2520introduce%250Aseveral%2520academic%2520writing%2520tasks%2520with%2520long-context%2520inputs%252C%2520%255Ctextit%257Bi.e.%257D%252C%250A%255Ctextsc%257BTitle%257D%252C%2520%255Ctextsc%257BAbstract%257D%252C%2520%255Ctextsc%257BIntroduction%257D%252C%2520and%2520%255Ctextsc%257BRelated%250AWork%257D%252C%2520which%2520cover%2520a%2520wide%2520range%2520of%2520abstraction%2520levels%2520and%2520require%2520no%2520manual%250Alabeling.%2520Moreover%252C%2520%255Ctextsc%257BAcademicEval%257D%2520integrates%2520high-quality%2520and%250Aexpert-curated%2520few-shot%2520demonstrations%2520from%2520a%2520collected%2520co-author%2520graph%2520to%250Aenable%2520flexible%2520context%2520length.%2520Especially%252C%2520%255Ctextsc%257BAcademicEval%257D%2520features%2520an%250Aefficient%2520live%2520evaluation%252C%2520ensuring%2520no%2520label%2520leakage.%2520We%2520conduct%2520a%2520holistic%250Aevaluation%2520on%2520%255Ctextsc%257BAcademicEval%257D%252C%2520and%2520the%2520results%2520illustrate%2520that%2520LLMs%250Aperform%2520poorly%2520on%2520tasks%2520with%2520hierarchical%2520abstraction%2520levels%2520and%2520tend%2520to%250Astruggle%2520with%2520long%2520few-shot%2520demonstrations%252C%2520highlighting%2520the%2520challenge%2520of%2520our%250Abenchmark.%2520Through%2520experimental%2520analysis%252C%2520we%2520also%2520reveal%2520some%2520insights%2520for%250Aenhancing%2520LLMs%2527%2520long-context%2520modeling%2520capabilities.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/ulab-uiuc/AcademicEval%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AcademicEval%3A%20Live%20Long-Context%20LLM%20Benchmark&entry.906535625=Haozhen%20Zhang%20and%20Tao%20Feng%20and%20Pengrui%20Han%20and%20Jiaxuan%20You&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20achieved%20remarkable%20performance%20in%0Along-context%20understanding.%20However%2C%20current%20long-context%20LLM%20benchmarks%20are%0Alimited%20by%20rigid%20context%20length%2C%20labor-intensive%20annotation%2C%20and%20the%20pressing%0Achallenge%20of%20label%20leakage%20issues%20during%20LLM%20training.%20Therefore%2C%20we%20propose%0A%5Ctextsc%7BAcademicEval%7D%2C%20a%20live%20benchmark%20for%20evaluating%20LLMs%20over%20long-context%0Ageneration%20tasks.%20%5Ctextsc%7BAcademicEval%7D%20adopts%20papers%20on%20arXiv%20to%20introduce%0Aseveral%20academic%20writing%20tasks%20with%20long-context%20inputs%2C%20%5Ctextit%7Bi.e.%7D%2C%0A%5Ctextsc%7BTitle%7D%2C%20%5Ctextsc%7BAbstract%7D%2C%20%5Ctextsc%7BIntroduction%7D%2C%20and%20%5Ctextsc%7BRelated%0AWork%7D%2C%20which%20cover%20a%20wide%20range%20of%20abstraction%20levels%20and%20require%20no%20manual%0Alabeling.%20Moreover%2C%20%5Ctextsc%7BAcademicEval%7D%20integrates%20high-quality%20and%0Aexpert-curated%20few-shot%20demonstrations%20from%20a%20collected%20co-author%20graph%20to%0Aenable%20flexible%20context%20length.%20Especially%2C%20%5Ctextsc%7BAcademicEval%7D%20features%20an%0Aefficient%20live%20evaluation%2C%20ensuring%20no%20label%20leakage.%20We%20conduct%20a%20holistic%0Aevaluation%20on%20%5Ctextsc%7BAcademicEval%7D%2C%20and%20the%20results%20illustrate%20that%20LLMs%0Aperform%20poorly%20on%20tasks%20with%20hierarchical%20abstraction%20levels%20and%20tend%20to%0Astruggle%20with%20long%20few-shot%20demonstrations%2C%20highlighting%20the%20challenge%20of%20our%0Abenchmark.%20Through%20experimental%20analysis%2C%20we%20also%20reveal%20some%20insights%20for%0Aenhancing%20LLMs%27%20long-context%20modeling%20capabilities.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ulab-uiuc/AcademicEval%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17725v1&entry.124074799=Read"},
{"title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal\n  Representation", "author": "Yanlong Chen and Mattia Orlandi and Pierangelo Maria Rapa and Simone Benatti and Luca Benini and Yawei Li", "abstract": "  Physiological signals are often corrupted by motion artifacts, baseline\ndrift, and other low-SNR disturbances, which pose significant challenges for\nanalysis. Additionally, these signals exhibit strong non-stationarity, with\nsharp peaks and abrupt changes that evolve continuously, making them difficult\nto represent using traditional time-domain or filtering methods. To address\nthese issues, a novel wavelet-based approach for physiological signal analysis\nis presented, aiming to capture multi-scale time-frequency features in various\nphysiological signals. Leveraging this technique, two large-scale pretrained\nmodels specific to EMG and ECG are introduced for the first time, achieving\nsuperior performance and setting new baselines in downstream tasks.\nAdditionally, a unified multi-modal framework is constructed by integrating\npretrained EEG model, where each modality is guided through its dedicated\nbranch and fused via learnable weighted fusion. This design effectively\naddresses challenges such as low signal-to-noise ratio, high inter-subject\nvariability, and device mismatch, outperforming existing methods on multi-modal\ntasks. The proposed wavelet-based architecture lays a solid foundation for\nanalysis of diverse physiological signals, while the multi-modal design points\nto next-generation physiological signal processing with potential impact on\nwearable health monitoring, clinical diagnostics, and broader biomedical\napplications. Code and data are available at:\ngithub.com/ForeverBlue816/PhysioWave\n", "link": "http://arxiv.org/abs/2506.10351v4", "date": "2025-10-20", "relevancy": 2.0467, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5324}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5144}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysioWave%3A%20A%20Multi-Scale%20Wavelet-Transformer%20for%20Physiological%20Signal%0A%20%20Representation&body=Title%3A%20PhysioWave%3A%20A%20Multi-Scale%20Wavelet-Transformer%20for%20Physiological%20Signal%0A%20%20Representation%0AAuthor%3A%20Yanlong%20Chen%20and%20Mattia%20Orlandi%20and%20Pierangelo%20Maria%20Rapa%20and%20Simone%20Benatti%20and%20Luca%20Benini%20and%20Yawei%20Li%0AAbstract%3A%20%20%20Physiological%20signals%20are%20often%20corrupted%20by%20motion%20artifacts%2C%20baseline%0Adrift%2C%20and%20other%20low-SNR%20disturbances%2C%20which%20pose%20significant%20challenges%20for%0Aanalysis.%20Additionally%2C%20these%20signals%20exhibit%20strong%20non-stationarity%2C%20with%0Asharp%20peaks%20and%20abrupt%20changes%20that%20evolve%20continuously%2C%20making%20them%20difficult%0Ato%20represent%20using%20traditional%20time-domain%20or%20filtering%20methods.%20To%20address%0Athese%20issues%2C%20a%20novel%20wavelet-based%20approach%20for%20physiological%20signal%20analysis%0Ais%20presented%2C%20aiming%20to%20capture%20multi-scale%20time-frequency%20features%20in%20various%0Aphysiological%20signals.%20Leveraging%20this%20technique%2C%20two%20large-scale%20pretrained%0Amodels%20specific%20to%20EMG%20and%20ECG%20are%20introduced%20for%20the%20first%20time%2C%20achieving%0Asuperior%20performance%20and%20setting%20new%20baselines%20in%20downstream%20tasks.%0AAdditionally%2C%20a%20unified%20multi-modal%20framework%20is%20constructed%20by%20integrating%0Apretrained%20EEG%20model%2C%20where%20each%20modality%20is%20guided%20through%20its%20dedicated%0Abranch%20and%20fused%20via%20learnable%20weighted%20fusion.%20This%20design%20effectively%0Aaddresses%20challenges%20such%20as%20low%20signal-to-noise%20ratio%2C%20high%20inter-subject%0Avariability%2C%20and%20device%20mismatch%2C%20outperforming%20existing%20methods%20on%20multi-modal%0Atasks.%20The%20proposed%20wavelet-based%20architecture%20lays%20a%20solid%20foundation%20for%0Aanalysis%20of%20diverse%20physiological%20signals%2C%20while%20the%20multi-modal%20design%20points%0Ato%20next-generation%20physiological%20signal%20processing%20with%20potential%20impact%20on%0Awearable%20health%20monitoring%2C%20clinical%20diagnostics%2C%20and%20broader%20biomedical%0Aapplications.%20Code%20and%20data%20are%20available%20at%3A%0Agithub.com/ForeverBlue816/PhysioWave%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10351v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysioWave%253A%2520A%2520Multi-Scale%2520Wavelet-Transformer%2520for%2520Physiological%2520Signal%250A%2520%2520Representation%26entry.906535625%3DYanlong%2520Chen%2520and%2520Mattia%2520Orlandi%2520and%2520Pierangelo%2520Maria%2520Rapa%2520and%2520Simone%2520Benatti%2520and%2520Luca%2520Benini%2520and%2520Yawei%2520Li%26entry.1292438233%3D%2520%2520Physiological%2520signals%2520are%2520often%2520corrupted%2520by%2520motion%2520artifacts%252C%2520baseline%250Adrift%252C%2520and%2520other%2520low-SNR%2520disturbances%252C%2520which%2520pose%2520significant%2520challenges%2520for%250Aanalysis.%2520Additionally%252C%2520these%2520signals%2520exhibit%2520strong%2520non-stationarity%252C%2520with%250Asharp%2520peaks%2520and%2520abrupt%2520changes%2520that%2520evolve%2520continuously%252C%2520making%2520them%2520difficult%250Ato%2520represent%2520using%2520traditional%2520time-domain%2520or%2520filtering%2520methods.%2520To%2520address%250Athese%2520issues%252C%2520a%2520novel%2520wavelet-based%2520approach%2520for%2520physiological%2520signal%2520analysis%250Ais%2520presented%252C%2520aiming%2520to%2520capture%2520multi-scale%2520time-frequency%2520features%2520in%2520various%250Aphysiological%2520signals.%2520Leveraging%2520this%2520technique%252C%2520two%2520large-scale%2520pretrained%250Amodels%2520specific%2520to%2520EMG%2520and%2520ECG%2520are%2520introduced%2520for%2520the%2520first%2520time%252C%2520achieving%250Asuperior%2520performance%2520and%2520setting%2520new%2520baselines%2520in%2520downstream%2520tasks.%250AAdditionally%252C%2520a%2520unified%2520multi-modal%2520framework%2520is%2520constructed%2520by%2520integrating%250Apretrained%2520EEG%2520model%252C%2520where%2520each%2520modality%2520is%2520guided%2520through%2520its%2520dedicated%250Abranch%2520and%2520fused%2520via%2520learnable%2520weighted%2520fusion.%2520This%2520design%2520effectively%250Aaddresses%2520challenges%2520such%2520as%2520low%2520signal-to-noise%2520ratio%252C%2520high%2520inter-subject%250Avariability%252C%2520and%2520device%2520mismatch%252C%2520outperforming%2520existing%2520methods%2520on%2520multi-modal%250Atasks.%2520The%2520proposed%2520wavelet-based%2520architecture%2520lays%2520a%2520solid%2520foundation%2520for%250Aanalysis%2520of%2520diverse%2520physiological%2520signals%252C%2520while%2520the%2520multi-modal%2520design%2520points%250Ato%2520next-generation%2520physiological%2520signal%2520processing%2520with%2520potential%2520impact%2520on%250Awearable%2520health%2520monitoring%252C%2520clinical%2520diagnostics%252C%2520and%2520broader%2520biomedical%250Aapplications.%2520Code%2520and%2520data%2520are%2520available%2520at%253A%250Agithub.com/ForeverBlue816/PhysioWave%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10351v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysioWave%3A%20A%20Multi-Scale%20Wavelet-Transformer%20for%20Physiological%20Signal%0A%20%20Representation&entry.906535625=Yanlong%20Chen%20and%20Mattia%20Orlandi%20and%20Pierangelo%20Maria%20Rapa%20and%20Simone%20Benatti%20and%20Luca%20Benini%20and%20Yawei%20Li&entry.1292438233=%20%20Physiological%20signals%20are%20often%20corrupted%20by%20motion%20artifacts%2C%20baseline%0Adrift%2C%20and%20other%20low-SNR%20disturbances%2C%20which%20pose%20significant%20challenges%20for%0Aanalysis.%20Additionally%2C%20these%20signals%20exhibit%20strong%20non-stationarity%2C%20with%0Asharp%20peaks%20and%20abrupt%20changes%20that%20evolve%20continuously%2C%20making%20them%20difficult%0Ato%20represent%20using%20traditional%20time-domain%20or%20filtering%20methods.%20To%20address%0Athese%20issues%2C%20a%20novel%20wavelet-based%20approach%20for%20physiological%20signal%20analysis%0Ais%20presented%2C%20aiming%20to%20capture%20multi-scale%20time-frequency%20features%20in%20various%0Aphysiological%20signals.%20Leveraging%20this%20technique%2C%20two%20large-scale%20pretrained%0Amodels%20specific%20to%20EMG%20and%20ECG%20are%20introduced%20for%20the%20first%20time%2C%20achieving%0Asuperior%20performance%20and%20setting%20new%20baselines%20in%20downstream%20tasks.%0AAdditionally%2C%20a%20unified%20multi-modal%20framework%20is%20constructed%20by%20integrating%0Apretrained%20EEG%20model%2C%20where%20each%20modality%20is%20guided%20through%20its%20dedicated%0Abranch%20and%20fused%20via%20learnable%20weighted%20fusion.%20This%20design%20effectively%0Aaddresses%20challenges%20such%20as%20low%20signal-to-noise%20ratio%2C%20high%20inter-subject%0Avariability%2C%20and%20device%20mismatch%2C%20outperforming%20existing%20methods%20on%20multi-modal%0Atasks.%20The%20proposed%20wavelet-based%20architecture%20lays%20a%20solid%20foundation%20for%0Aanalysis%20of%20diverse%20physiological%20signals%2C%20while%20the%20multi-modal%20design%20points%0Ato%20next-generation%20physiological%20signal%20processing%20with%20potential%20impact%20on%0Awearable%20health%20monitoring%2C%20clinical%20diagnostics%2C%20and%20broader%20biomedical%0Aapplications.%20Code%20and%20data%20are%20available%20at%3A%0Agithub.com/ForeverBlue816/PhysioWave%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10351v4&entry.124074799=Read"},
{"title": "Executable Knowledge Graphs for Replicating AI Research", "author": "Yujie Luo and Zhuoyun Yu and Xuehai Wang and Yuqi Zhu and Ningyu Zhang and Lanning Wei and Lun Du and Da Zheng and Huajun Chen", "abstract": "  Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.\n", "link": "http://arxiv.org/abs/2510.17795v1", "date": "2025-10-20", "relevancy": 2.0393, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5443}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5072}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Executable%20Knowledge%20Graphs%20for%20Replicating%20AI%20Research&body=Title%3A%20Executable%20Knowledge%20Graphs%20for%20Replicating%20AI%20Research%0AAuthor%3A%20Yujie%20Luo%20and%20Zhuoyun%20Yu%20and%20Xuehai%20Wang%20and%20Yuqi%20Zhu%20and%20Ningyu%20Zhang%20and%20Lanning%20Wei%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Replicating%20AI%20research%20is%20a%20crucial%20yet%20challenging%20task%20for%20large%20language%0Amodel%20%28LLM%29%20agents.%20Existing%20approaches%20often%20struggle%20to%20generate%20executable%0Acode%2C%20primarily%20due%20to%20insufficient%20background%20knowledge%20and%20the%20limitations%20of%0Aretrieval-augmented%20generation%20%28RAG%29%20methods%2C%20which%20fail%20to%20capture%20latent%0Atechnical%20details%20hidden%20in%20referenced%20papers.%20Furthermore%2C%20previous%20approaches%0Atend%20to%20overlook%20valuable%20implementation-level%20code%20signals%20and%20lack%20structured%0Aknowledge%20representations%20that%20support%20multi-granular%20retrieval%20and%20reuse.%20To%0Aovercome%20these%20challenges%2C%20we%20propose%20Executable%20Knowledge%20Graphs%20%28xKG%29%2C%20a%0Amodular%20and%20pluggable%20knowledge%20base%20that%20automatically%20integrates%20technical%0Ainsights%2C%20code%20snippets%2C%20and%20domain-specific%20knowledge%20extracted%20from%0Ascientific%20literature.%20When%20integrated%20into%20three%20agent%20frameworks%20with%20two%0Adifferent%20LLMs%2C%20xKG%20shows%20substantial%20performance%20gains%20%2810.9%25%20with%20o3-mini%29%20on%0APaperBench%2C%20demonstrating%20its%20effectiveness%20as%20a%20general%20and%20extensible%0Asolution%20for%20automated%20AI%20research%20replication.%20Code%20will%20released%20at%0Ahttps%3A//github.com/zjunlp/xKG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExecutable%2520Knowledge%2520Graphs%2520for%2520Replicating%2520AI%2520Research%26entry.906535625%3DYujie%2520Luo%2520and%2520Zhuoyun%2520Yu%2520and%2520Xuehai%2520Wang%2520and%2520Yuqi%2520Zhu%2520and%2520Ningyu%2520Zhang%2520and%2520Lanning%2520Wei%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Replicating%2520AI%2520research%2520is%2520a%2520crucial%2520yet%2520challenging%2520task%2520for%2520large%2520language%250Amodel%2520%2528LLM%2529%2520agents.%2520Existing%2520approaches%2520often%2520struggle%2520to%2520generate%2520executable%250Acode%252C%2520primarily%2520due%2520to%2520insufficient%2520background%2520knowledge%2520and%2520the%2520limitations%2520of%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520methods%252C%2520which%2520fail%2520to%2520capture%2520latent%250Atechnical%2520details%2520hidden%2520in%2520referenced%2520papers.%2520Furthermore%252C%2520previous%2520approaches%250Atend%2520to%2520overlook%2520valuable%2520implementation-level%2520code%2520signals%2520and%2520lack%2520structured%250Aknowledge%2520representations%2520that%2520support%2520multi-granular%2520retrieval%2520and%2520reuse.%2520To%250Aovercome%2520these%2520challenges%252C%2520we%2520propose%2520Executable%2520Knowledge%2520Graphs%2520%2528xKG%2529%252C%2520a%250Amodular%2520and%2520pluggable%2520knowledge%2520base%2520that%2520automatically%2520integrates%2520technical%250Ainsights%252C%2520code%2520snippets%252C%2520and%2520domain-specific%2520knowledge%2520extracted%2520from%250Ascientific%2520literature.%2520When%2520integrated%2520into%2520three%2520agent%2520frameworks%2520with%2520two%250Adifferent%2520LLMs%252C%2520xKG%2520shows%2520substantial%2520performance%2520gains%2520%252810.9%2525%2520with%2520o3-mini%2529%2520on%250APaperBench%252C%2520demonstrating%2520its%2520effectiveness%2520as%2520a%2520general%2520and%2520extensible%250Asolution%2520for%2520automated%2520AI%2520research%2520replication.%2520Code%2520will%2520released%2520at%250Ahttps%253A//github.com/zjunlp/xKG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Executable%20Knowledge%20Graphs%20for%20Replicating%20AI%20Research&entry.906535625=Yujie%20Luo%20and%20Zhuoyun%20Yu%20and%20Xuehai%20Wang%20and%20Yuqi%20Zhu%20and%20Ningyu%20Zhang%20and%20Lanning%20Wei%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen&entry.1292438233=%20%20Replicating%20AI%20research%20is%20a%20crucial%20yet%20challenging%20task%20for%20large%20language%0Amodel%20%28LLM%29%20agents.%20Existing%20approaches%20often%20struggle%20to%20generate%20executable%0Acode%2C%20primarily%20due%20to%20insufficient%20background%20knowledge%20and%20the%20limitations%20of%0Aretrieval-augmented%20generation%20%28RAG%29%20methods%2C%20which%20fail%20to%20capture%20latent%0Atechnical%20details%20hidden%20in%20referenced%20papers.%20Furthermore%2C%20previous%20approaches%0Atend%20to%20overlook%20valuable%20implementation-level%20code%20signals%20and%20lack%20structured%0Aknowledge%20representations%20that%20support%20multi-granular%20retrieval%20and%20reuse.%20To%0Aovercome%20these%20challenges%2C%20we%20propose%20Executable%20Knowledge%20Graphs%20%28xKG%29%2C%20a%0Amodular%20and%20pluggable%20knowledge%20base%20that%20automatically%20integrates%20technical%0Ainsights%2C%20code%20snippets%2C%20and%20domain-specific%20knowledge%20extracted%20from%0Ascientific%20literature.%20When%20integrated%20into%20three%20agent%20frameworks%20with%20two%0Adifferent%20LLMs%2C%20xKG%20shows%20substantial%20performance%20gains%20%2810.9%25%20with%20o3-mini%29%20on%0APaperBench%2C%20demonstrating%20its%20effectiveness%20as%20a%20general%20and%20extensible%0Asolution%20for%20automated%20AI%20research%20replication.%20Code%20will%20released%20at%0Ahttps%3A//github.com/zjunlp/xKG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17795v1&entry.124074799=Read"},
{"title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts\n  Algorithm", "author": "Hao Qiao and Yan Wang and Shuo Yang and Xiaoyao Yu and Jian kuang and Xiaoji Niu", "abstract": "  With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.\n", "link": "http://arxiv.org/abs/2510.17604v1", "date": "2025-10-20", "relevancy": 2.0176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Inertial%20Odometry%20for%20Cycling%20Based%20on%20Mixture%20of%20Experts%0A%20%20Algorithm&body=Title%3A%20Learned%20Inertial%20Odometry%20for%20Cycling%20Based%20on%20Mixture%20of%20Experts%0A%20%20Algorithm%0AAuthor%3A%20Hao%20Qiao%20and%20Yan%20Wang%20and%20Shuo%20Yang%20and%20Xiaoyao%20Yu%20and%20Jian%20kuang%20and%20Xiaoji%20Niu%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20bike%20sharing%20and%20the%20increasing%20diversity%20of%20cycling%0Aapplications%2C%20accurate%20bicycle%20localization%20has%20become%20essential.%20traditional%0AGNSS-based%20methods%20suffer%20from%20multipath%20effects%2C%20while%20existing%20inertial%0Anavigation%20approaches%20rely%20on%20precise%20modeling%20and%20show%20limited%20robustness.%0ATight%20Learned%20Inertial%20Odometry%20%28TLIO%29%20achieves%20low%20position%20drift%20by%20combining%0Araw%20IMU%20data%20with%20predicted%20displacements%20by%20neural%20networks%2C%20but%20its%20high%0Acomputational%20cost%20restricts%20deployment%20on%20mobile%20devices.%20To%20overcome%20this%2C%20we%0Aextend%20TLIO%20to%20bicycle%20localization%20and%20introduce%20an%20improved%20Mixture-of%0AExperts%20%28MoE%29%20model%20that%20reduces%20both%20training%20and%20inference%20costs.%20Experiments%0Ashow%20that%2C%20compared%20to%20the%20state-of-the-art%20LLIO%20framework%2C%20our%20method%20achieves%0Acomparable%20accuracy%20while%20reducing%20parameters%20by%2064.7%25%20and%20computational%20cost%0Aby%2081.8%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Inertial%2520Odometry%2520for%2520Cycling%2520Based%2520on%2520Mixture%2520of%2520Experts%250A%2520%2520Algorithm%26entry.906535625%3DHao%2520Qiao%2520and%2520Yan%2520Wang%2520and%2520Shuo%2520Yang%2520and%2520Xiaoyao%2520Yu%2520and%2520Jian%2520kuang%2520and%2520Xiaoji%2520Niu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520of%2520bike%2520sharing%2520and%2520the%2520increasing%2520diversity%2520of%2520cycling%250Aapplications%252C%2520accurate%2520bicycle%2520localization%2520has%2520become%2520essential.%2520traditional%250AGNSS-based%2520methods%2520suffer%2520from%2520multipath%2520effects%252C%2520while%2520existing%2520inertial%250Anavigation%2520approaches%2520rely%2520on%2520precise%2520modeling%2520and%2520show%2520limited%2520robustness.%250ATight%2520Learned%2520Inertial%2520Odometry%2520%2528TLIO%2529%2520achieves%2520low%2520position%2520drift%2520by%2520combining%250Araw%2520IMU%2520data%2520with%2520predicted%2520displacements%2520by%2520neural%2520networks%252C%2520but%2520its%2520high%250Acomputational%2520cost%2520restricts%2520deployment%2520on%2520mobile%2520devices.%2520To%2520overcome%2520this%252C%2520we%250Aextend%2520TLIO%2520to%2520bicycle%2520localization%2520and%2520introduce%2520an%2520improved%2520Mixture-of%250AExperts%2520%2528MoE%2529%2520model%2520that%2520reduces%2520both%2520training%2520and%2520inference%2520costs.%2520Experiments%250Ashow%2520that%252C%2520compared%2520to%2520the%2520state-of-the-art%2520LLIO%2520framework%252C%2520our%2520method%2520achieves%250Acomparable%2520accuracy%2520while%2520reducing%2520parameters%2520by%252064.7%2525%2520and%2520computational%2520cost%250Aby%252081.8%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Inertial%20Odometry%20for%20Cycling%20Based%20on%20Mixture%20of%20Experts%0A%20%20Algorithm&entry.906535625=Hao%20Qiao%20and%20Yan%20Wang%20and%20Shuo%20Yang%20and%20Xiaoyao%20Yu%20and%20Jian%20kuang%20and%20Xiaoji%20Niu&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20bike%20sharing%20and%20the%20increasing%20diversity%20of%20cycling%0Aapplications%2C%20accurate%20bicycle%20localization%20has%20become%20essential.%20traditional%0AGNSS-based%20methods%20suffer%20from%20multipath%20effects%2C%20while%20existing%20inertial%0Anavigation%20approaches%20rely%20on%20precise%20modeling%20and%20show%20limited%20robustness.%0ATight%20Learned%20Inertial%20Odometry%20%28TLIO%29%20achieves%20low%20position%20drift%20by%20combining%0Araw%20IMU%20data%20with%20predicted%20displacements%20by%20neural%20networks%2C%20but%20its%20high%0Acomputational%20cost%20restricts%20deployment%20on%20mobile%20devices.%20To%20overcome%20this%2C%20we%0Aextend%20TLIO%20to%20bicycle%20localization%20and%20introduce%20an%20improved%20Mixture-of%0AExperts%20%28MoE%29%20model%20that%20reduces%20both%20training%20and%20inference%20costs.%20Experiments%0Ashow%20that%2C%20compared%20to%20the%20state-of-the-art%20LLIO%20framework%2C%20our%20method%20achieves%0Acomparable%20accuracy%20while%20reducing%20parameters%20by%2064.7%25%20and%20computational%20cost%0Aby%2081.8%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17604v1&entry.124074799=Read"},
{"title": "Market-Driven Subset Selection for Budgeted Training", "author": "Ashish Jha and Valentin Leplat and AH Phan", "abstract": "  Training large language models on massive datasets is computationally\nexpensive, yet empirical evidence suggests that substantial portions of\ntraining examples contribute minimally to final performance. Data subset\nselection addresses this inefficiency by identifying small, high-utility\nsubsets under resource constraints. However, example utility is inherently\nmulti-faceted, encompassing uncertainty, distributional rarity, and diversity\nsignals that are heterogeneous and typically combined through ad hoc weighted\nsums lacking theoretical grounding. We propose a market-based framework that\ntreats each training example as a tradeable contract and employs the\nLogarithmic Market Scoring Rule to aggregate multiple utility signals into\ncoherent prices. Heterogeneous signals act as traders, a single liquidity\nparameter controls concentration versus smoothing, and topic-wise normalization\nensures calibrated aggregation. Token budgets are handled explicitly through a\nprice-per-token decision rule with an interpretable length-bias parameter. We\nestablish theoretical connections to maximum-entropy aggregation and provide\nutility recovery guarantees under noisy but monotone signals. On GSM8K\nmathematical reasoning under strict 60k-token budgets, our selector achieves\nparity with strong single-signal baselines while exhibiting lower variance and\nincurring less than 0.1 GPU-hour overhead. On AGNews classification at 5-25\\%\nretention rates, the market formulation delivers competitive accuracy with\nimproved stability. Our framework unifies multi-signal data curation under\nfixed computational budgets for prompt-level reasoning and classification\ntasks.\n", "link": "http://arxiv.org/abs/2510.02456v2", "date": "2025-10-20", "relevancy": 1.9829, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4961}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Market-Driven%20Subset%20Selection%20for%20Budgeted%20Training&body=Title%3A%20Market-Driven%20Subset%20Selection%20for%20Budgeted%20Training%0AAuthor%3A%20Ashish%20Jha%20and%20Valentin%20Leplat%20and%20AH%20Phan%0AAbstract%3A%20%20%20Training%20large%20language%20models%20on%20massive%20datasets%20is%20computationally%0Aexpensive%2C%20yet%20empirical%20evidence%20suggests%20that%20substantial%20portions%20of%0Atraining%20examples%20contribute%20minimally%20to%20final%20performance.%20Data%20subset%0Aselection%20addresses%20this%20inefficiency%20by%20identifying%20small%2C%20high-utility%0Asubsets%20under%20resource%20constraints.%20However%2C%20example%20utility%20is%20inherently%0Amulti-faceted%2C%20encompassing%20uncertainty%2C%20distributional%20rarity%2C%20and%20diversity%0Asignals%20that%20are%20heterogeneous%20and%20typically%20combined%20through%20ad%20hoc%20weighted%0Asums%20lacking%20theoretical%20grounding.%20We%20propose%20a%20market-based%20framework%20that%0Atreats%20each%20training%20example%20as%20a%20tradeable%20contract%20and%20employs%20the%0ALogarithmic%20Market%20Scoring%20Rule%20to%20aggregate%20multiple%20utility%20signals%20into%0Acoherent%20prices.%20Heterogeneous%20signals%20act%20as%20traders%2C%20a%20single%20liquidity%0Aparameter%20controls%20concentration%20versus%20smoothing%2C%20and%20topic-wise%20normalization%0Aensures%20calibrated%20aggregation.%20Token%20budgets%20are%20handled%20explicitly%20through%20a%0Aprice-per-token%20decision%20rule%20with%20an%20interpretable%20length-bias%20parameter.%20We%0Aestablish%20theoretical%20connections%20to%20maximum-entropy%20aggregation%20and%20provide%0Autility%20recovery%20guarantees%20under%20noisy%20but%20monotone%20signals.%20On%20GSM8K%0Amathematical%20reasoning%20under%20strict%2060k-token%20budgets%2C%20our%20selector%20achieves%0Aparity%20with%20strong%20single-signal%20baselines%20while%20exhibiting%20lower%20variance%20and%0Aincurring%20less%20than%200.1%20GPU-hour%20overhead.%20On%20AGNews%20classification%20at%205-25%5C%25%0Aretention%20rates%2C%20the%20market%20formulation%20delivers%20competitive%20accuracy%20with%0Aimproved%20stability.%20Our%20framework%20unifies%20multi-signal%20data%20curation%20under%0Afixed%20computational%20budgets%20for%20prompt-level%20reasoning%20and%20classification%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02456v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarket-Driven%2520Subset%2520Selection%2520for%2520Budgeted%2520Training%26entry.906535625%3DAshish%2520Jha%2520and%2520Valentin%2520Leplat%2520and%2520AH%2520Phan%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520on%2520massive%2520datasets%2520is%2520computationally%250Aexpensive%252C%2520yet%2520empirical%2520evidence%2520suggests%2520that%2520substantial%2520portions%2520of%250Atraining%2520examples%2520contribute%2520minimally%2520to%2520final%2520performance.%2520Data%2520subset%250Aselection%2520addresses%2520this%2520inefficiency%2520by%2520identifying%2520small%252C%2520high-utility%250Asubsets%2520under%2520resource%2520constraints.%2520However%252C%2520example%2520utility%2520is%2520inherently%250Amulti-faceted%252C%2520encompassing%2520uncertainty%252C%2520distributional%2520rarity%252C%2520and%2520diversity%250Asignals%2520that%2520are%2520heterogeneous%2520and%2520typically%2520combined%2520through%2520ad%2520hoc%2520weighted%250Asums%2520lacking%2520theoretical%2520grounding.%2520We%2520propose%2520a%2520market-based%2520framework%2520that%250Atreats%2520each%2520training%2520example%2520as%2520a%2520tradeable%2520contract%2520and%2520employs%2520the%250ALogarithmic%2520Market%2520Scoring%2520Rule%2520to%2520aggregate%2520multiple%2520utility%2520signals%2520into%250Acoherent%2520prices.%2520Heterogeneous%2520signals%2520act%2520as%2520traders%252C%2520a%2520single%2520liquidity%250Aparameter%2520controls%2520concentration%2520versus%2520smoothing%252C%2520and%2520topic-wise%2520normalization%250Aensures%2520calibrated%2520aggregation.%2520Token%2520budgets%2520are%2520handled%2520explicitly%2520through%2520a%250Aprice-per-token%2520decision%2520rule%2520with%2520an%2520interpretable%2520length-bias%2520parameter.%2520We%250Aestablish%2520theoretical%2520connections%2520to%2520maximum-entropy%2520aggregation%2520and%2520provide%250Autility%2520recovery%2520guarantees%2520under%2520noisy%2520but%2520monotone%2520signals.%2520On%2520GSM8K%250Amathematical%2520reasoning%2520under%2520strict%252060k-token%2520budgets%252C%2520our%2520selector%2520achieves%250Aparity%2520with%2520strong%2520single-signal%2520baselines%2520while%2520exhibiting%2520lower%2520variance%2520and%250Aincurring%2520less%2520than%25200.1%2520GPU-hour%2520overhead.%2520On%2520AGNews%2520classification%2520at%25205-25%255C%2525%250Aretention%2520rates%252C%2520the%2520market%2520formulation%2520delivers%2520competitive%2520accuracy%2520with%250Aimproved%2520stability.%2520Our%2520framework%2520unifies%2520multi-signal%2520data%2520curation%2520under%250Afixed%2520computational%2520budgets%2520for%2520prompt-level%2520reasoning%2520and%2520classification%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02456v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Market-Driven%20Subset%20Selection%20for%20Budgeted%20Training&entry.906535625=Ashish%20Jha%20and%20Valentin%20Leplat%20and%20AH%20Phan&entry.1292438233=%20%20Training%20large%20language%20models%20on%20massive%20datasets%20is%20computationally%0Aexpensive%2C%20yet%20empirical%20evidence%20suggests%20that%20substantial%20portions%20of%0Atraining%20examples%20contribute%20minimally%20to%20final%20performance.%20Data%20subset%0Aselection%20addresses%20this%20inefficiency%20by%20identifying%20small%2C%20high-utility%0Asubsets%20under%20resource%20constraints.%20However%2C%20example%20utility%20is%20inherently%0Amulti-faceted%2C%20encompassing%20uncertainty%2C%20distributional%20rarity%2C%20and%20diversity%0Asignals%20that%20are%20heterogeneous%20and%20typically%20combined%20through%20ad%20hoc%20weighted%0Asums%20lacking%20theoretical%20grounding.%20We%20propose%20a%20market-based%20framework%20that%0Atreats%20each%20training%20example%20as%20a%20tradeable%20contract%20and%20employs%20the%0ALogarithmic%20Market%20Scoring%20Rule%20to%20aggregate%20multiple%20utility%20signals%20into%0Acoherent%20prices.%20Heterogeneous%20signals%20act%20as%20traders%2C%20a%20single%20liquidity%0Aparameter%20controls%20concentration%20versus%20smoothing%2C%20and%20topic-wise%20normalization%0Aensures%20calibrated%20aggregation.%20Token%20budgets%20are%20handled%20explicitly%20through%20a%0Aprice-per-token%20decision%20rule%20with%20an%20interpretable%20length-bias%20parameter.%20We%0Aestablish%20theoretical%20connections%20to%20maximum-entropy%20aggregation%20and%20provide%0Autility%20recovery%20guarantees%20under%20noisy%20but%20monotone%20signals.%20On%20GSM8K%0Amathematical%20reasoning%20under%20strict%2060k-token%20budgets%2C%20our%20selector%20achieves%0Aparity%20with%20strong%20single-signal%20baselines%20while%20exhibiting%20lower%20variance%20and%0Aincurring%20less%20than%200.1%20GPU-hour%20overhead.%20On%20AGNews%20classification%20at%205-25%5C%25%0Aretention%20rates%2C%20the%20market%20formulation%20delivers%20competitive%20accuracy%20with%0Aimproved%20stability.%20Our%20framework%20unifies%20multi-signal%20data%20curation%20under%0Afixed%20computational%20budgets%20for%20prompt-level%20reasoning%20and%20classification%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02456v2&entry.124074799=Read"},
{"title": "Identifiable Latent Bandits: Leveraging observational data for\n  personalized decision-making", "author": "Ahmet Zahid Balc\u0131o\u011flu and Newton Mwai and Emil Carlsson and Fredrik D. Johansson", "abstract": "  Sequential decision-making algorithms such as multi-armed bandits can find\noptimal personalized decisions, but are notoriously sample-hungry. In\npersonalized medicine, for example, training a bandit from scratch for every\npatient is typically infeasible, as the number of trials required is much\nlarger than the number of decision points for a single patient. To combat this,\nlatent bandits offer rapid exploration and personalization beyond what context\nvariables alone can offer, provided that a latent variable model of problem\ninstances can be learned consistently. However, existing works give no guidance\nas to how such a model can be found. In this work, we propose an identifiable\nlatent bandit framework that leads to optimal decision-making with a shorter\nexploration time than classical bandits by learning from historical records of\ndecisions and outcomes. Our method is based on nonlinear independent component\nanalysis that provably identifies representations from observational data\nsufficient to infer optimal actions in new bandit instances. We verify this\nstrategy in simulated and semi-synthetic environments, showing substantial\nimprovement over online and offline learning baselines when identifying\nconditions are satisfied.\n", "link": "http://arxiv.org/abs/2407.16239v5", "date": "2025-10-20", "relevancy": 1.9828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.516}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4956}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20Latent%20Bandits%3A%20Leveraging%20observational%20data%20for%0A%20%20personalized%20decision-making&body=Title%3A%20Identifiable%20Latent%20Bandits%3A%20Leveraging%20observational%20data%20for%0A%20%20personalized%20decision-making%0AAuthor%3A%20Ahmet%20Zahid%20Balc%C4%B1o%C4%9Flu%20and%20Newton%20Mwai%20and%20Emil%20Carlsson%20and%20Fredrik%20D.%20Johansson%0AAbstract%3A%20%20%20Sequential%20decision-making%20algorithms%20such%20as%20multi-armed%20bandits%20can%20find%0Aoptimal%20personalized%20decisions%2C%20but%20are%20notoriously%20sample-hungry.%20In%0Apersonalized%20medicine%2C%20for%20example%2C%20training%20a%20bandit%20from%20scratch%20for%20every%0Apatient%20is%20typically%20infeasible%2C%20as%20the%20number%20of%20trials%20required%20is%20much%0Alarger%20than%20the%20number%20of%20decision%20points%20for%20a%20single%20patient.%20To%20combat%20this%2C%0Alatent%20bandits%20offer%20rapid%20exploration%20and%20personalization%20beyond%20what%20context%0Avariables%20alone%20can%20offer%2C%20provided%20that%20a%20latent%20variable%20model%20of%20problem%0Ainstances%20can%20be%20learned%20consistently.%20However%2C%20existing%20works%20give%20no%20guidance%0Aas%20to%20how%20such%20a%20model%20can%20be%20found.%20In%20this%20work%2C%20we%20propose%20an%20identifiable%0Alatent%20bandit%20framework%20that%20leads%20to%20optimal%20decision-making%20with%20a%20shorter%0Aexploration%20time%20than%20classical%20bandits%20by%20learning%20from%20historical%20records%20of%0Adecisions%20and%20outcomes.%20Our%20method%20is%20based%20on%20nonlinear%20independent%20component%0Aanalysis%20that%20provably%20identifies%20representations%20from%20observational%20data%0Asufficient%20to%20infer%20optimal%20actions%20in%20new%20bandit%20instances.%20We%20verify%20this%0Astrategy%20in%20simulated%20and%20semi-synthetic%20environments%2C%20showing%20substantial%0Aimprovement%20over%20online%20and%20offline%20learning%20baselines%20when%20identifying%0Aconditions%20are%20satisfied.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16239v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520Latent%2520Bandits%253A%2520Leveraging%2520observational%2520data%2520for%250A%2520%2520personalized%2520decision-making%26entry.906535625%3DAhmet%2520Zahid%2520Balc%25C4%25B1o%25C4%259Flu%2520and%2520Newton%2520Mwai%2520and%2520Emil%2520Carlsson%2520and%2520Fredrik%2520D.%2520Johansson%26entry.1292438233%3D%2520%2520Sequential%2520decision-making%2520algorithms%2520such%2520as%2520multi-armed%2520bandits%2520can%2520find%250Aoptimal%2520personalized%2520decisions%252C%2520but%2520are%2520notoriously%2520sample-hungry.%2520In%250Apersonalized%2520medicine%252C%2520for%2520example%252C%2520training%2520a%2520bandit%2520from%2520scratch%2520for%2520every%250Apatient%2520is%2520typically%2520infeasible%252C%2520as%2520the%2520number%2520of%2520trials%2520required%2520is%2520much%250Alarger%2520than%2520the%2520number%2520of%2520decision%2520points%2520for%2520a%2520single%2520patient.%2520To%2520combat%2520this%252C%250Alatent%2520bandits%2520offer%2520rapid%2520exploration%2520and%2520personalization%2520beyond%2520what%2520context%250Avariables%2520alone%2520can%2520offer%252C%2520provided%2520that%2520a%2520latent%2520variable%2520model%2520of%2520problem%250Ainstances%2520can%2520be%2520learned%2520consistently.%2520However%252C%2520existing%2520works%2520give%2520no%2520guidance%250Aas%2520to%2520how%2520such%2520a%2520model%2520can%2520be%2520found.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520identifiable%250Alatent%2520bandit%2520framework%2520that%2520leads%2520to%2520optimal%2520decision-making%2520with%2520a%2520shorter%250Aexploration%2520time%2520than%2520classical%2520bandits%2520by%2520learning%2520from%2520historical%2520records%2520of%250Adecisions%2520and%2520outcomes.%2520Our%2520method%2520is%2520based%2520on%2520nonlinear%2520independent%2520component%250Aanalysis%2520that%2520provably%2520identifies%2520representations%2520from%2520observational%2520data%250Asufficient%2520to%2520infer%2520optimal%2520actions%2520in%2520new%2520bandit%2520instances.%2520We%2520verify%2520this%250Astrategy%2520in%2520simulated%2520and%2520semi-synthetic%2520environments%252C%2520showing%2520substantial%250Aimprovement%2520over%2520online%2520and%2520offline%2520learning%2520baselines%2520when%2520identifying%250Aconditions%2520are%2520satisfied.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16239v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20Latent%20Bandits%3A%20Leveraging%20observational%20data%20for%0A%20%20personalized%20decision-making&entry.906535625=Ahmet%20Zahid%20Balc%C4%B1o%C4%9Flu%20and%20Newton%20Mwai%20and%20Emil%20Carlsson%20and%20Fredrik%20D.%20Johansson&entry.1292438233=%20%20Sequential%20decision-making%20algorithms%20such%20as%20multi-armed%20bandits%20can%20find%0Aoptimal%20personalized%20decisions%2C%20but%20are%20notoriously%20sample-hungry.%20In%0Apersonalized%20medicine%2C%20for%20example%2C%20training%20a%20bandit%20from%20scratch%20for%20every%0Apatient%20is%20typically%20infeasible%2C%20as%20the%20number%20of%20trials%20required%20is%20much%0Alarger%20than%20the%20number%20of%20decision%20points%20for%20a%20single%20patient.%20To%20combat%20this%2C%0Alatent%20bandits%20offer%20rapid%20exploration%20and%20personalization%20beyond%20what%20context%0Avariables%20alone%20can%20offer%2C%20provided%20that%20a%20latent%20variable%20model%20of%20problem%0Ainstances%20can%20be%20learned%20consistently.%20However%2C%20existing%20works%20give%20no%20guidance%0Aas%20to%20how%20such%20a%20model%20can%20be%20found.%20In%20this%20work%2C%20we%20propose%20an%20identifiable%0Alatent%20bandit%20framework%20that%20leads%20to%20optimal%20decision-making%20with%20a%20shorter%0Aexploration%20time%20than%20classical%20bandits%20by%20learning%20from%20historical%20records%20of%0Adecisions%20and%20outcomes.%20Our%20method%20is%20based%20on%20nonlinear%20independent%20component%0Aanalysis%20that%20provably%20identifies%20representations%20from%20observational%20data%0Asufficient%20to%20infer%20optimal%20actions%20in%20new%20bandit%20instances.%20We%20verify%20this%0Astrategy%20in%20simulated%20and%20semi-synthetic%20environments%2C%20showing%20substantial%0Aimprovement%20over%20online%20and%20offline%20learning%20baselines%20when%20identifying%0Aconditions%20are%20satisfied.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16239v5&entry.124074799=Read"},
{"title": "Mapping Post-Training Forgetting in Language Models at Scale", "author": "Jackson Harmon and Andreas Hochlehnert and Matthias Bethge and Ameya Prabhu", "abstract": "  Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.\n", "link": "http://arxiv.org/abs/2510.17776v1", "date": "2025-10-20", "relevancy": 1.9794, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20Post-Training%20Forgetting%20in%20Language%20Models%20at%20Scale&body=Title%3A%20Mapping%20Post-Training%20Forgetting%20in%20Language%20Models%20at%20Scale%0AAuthor%3A%20Jackson%20Harmon%20and%20Andreas%20Hochlehnert%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu%0AAbstract%3A%20%20%20Scaled%20post-training%20now%20drives%20many%20of%20the%20largest%20capability%20gains%20in%0Alanguage%20models%20%28LMs%29%2C%20yet%20its%20effect%20on%20pretrained%20knowledge%20remains%20poorly%0Aunderstood.%20Not%20all%20forgetting%20is%20equal%3A%20Forgetting%20one%20fact%20%28e.g.%2C%20a%20U.S.%0Apresident%20or%20an%20API%20call%29%20does%20not%20%22average%20out%22%20by%20recalling%20another.%20Hence%2C%0Awe%20propose%20a%20sample-wise%20paradigm%20to%20measure%20what%20is%20forgotten%20and%20when%0Abackward%20transfer%20occurs.%20Our%20metric%20counts%201-%3E0%20transitions%20%28correct%20before%0Apost-training%2C%20incorrect%20after%29%20to%20quantify%20forgetting%20and%200-%3E1%20transitions%20to%0Aquantify%20backward%20transfer.%20Traditional%20task%20averages%20conflate%20these%20effects%0Aand%20obscure%20large%20changes.%20For%20multiple-choice%20benchmarks%2C%20we%20add%0Achance-adjusted%20variants%20that%20subtract%20the%20expected%20contribution%20of%20random%0Aguessing%20from%20pre-%20and%20post-training%20accuracies.%20We%20apply%20this%20framework%20across%0Apost-training%20stages%2C%20model%20sizes%2C%20and%20data%20scales.%20Our%20large-scale%20analysis%0Ashows%20that%3A%20%281%29%20Domain-continual%20pretraining%20induces%20moderate%20forgetting%20with%0Alow-to-moderate%20backward%20transfer%3B%20%282%29%20RL/SFT%20post-training%20applied%20to%20base%0Amodels%20and%20Instruction%20tuning%20yields%20moderate-to-large%20backward%20transfer%20on%0Amath%20and%20logic%20with%20overall%20low-to-moderate%20forgetting%3B%20%283%29%20Applying%20RL/SFT%20to%0Ainstruction-tuned%20models%20is%20sensitive%20on%20data%20scale%3A%20at%20small%20scales%2C%20both%0Aforgetting%20and%20backward%20transfer%20are%20small%3B%20at%20larger%20scales%2C%20effects%20are%20mixed%0Aand%20warrant%20further%20study%20with%20better%20controls%3B%20%284%29%20Model%20merging%20does%20not%0Areliably%20mitigate%20forgetting.%20Overall%2C%20our%20framework%20offers%20a%20practical%0Ayardstick%20for%20mapping%20how%20post-training%20alters%20pretrained%20knowledge%20at%20scale%20--%0Aenabling%20progress%20towards%20generally%20capable%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520Post-Training%2520Forgetting%2520in%2520Language%2520Models%2520at%2520Scale%26entry.906535625%3DJackson%2520Harmon%2520and%2520Andreas%2520Hochlehnert%2520and%2520Matthias%2520Bethge%2520and%2520Ameya%2520Prabhu%26entry.1292438233%3D%2520%2520Scaled%2520post-training%2520now%2520drives%2520many%2520of%2520the%2520largest%2520capability%2520gains%2520in%250Alanguage%2520models%2520%2528LMs%2529%252C%2520yet%2520its%2520effect%2520on%2520pretrained%2520knowledge%2520remains%2520poorly%250Aunderstood.%2520Not%2520all%2520forgetting%2520is%2520equal%253A%2520Forgetting%2520one%2520fact%2520%2528e.g.%252C%2520a%2520U.S.%250Apresident%2520or%2520an%2520API%2520call%2529%2520does%2520not%2520%2522average%2520out%2522%2520by%2520recalling%2520another.%2520Hence%252C%250Awe%2520propose%2520a%2520sample-wise%2520paradigm%2520to%2520measure%2520what%2520is%2520forgotten%2520and%2520when%250Abackward%2520transfer%2520occurs.%2520Our%2520metric%2520counts%25201-%253E0%2520transitions%2520%2528correct%2520before%250Apost-training%252C%2520incorrect%2520after%2529%2520to%2520quantify%2520forgetting%2520and%25200-%253E1%2520transitions%2520to%250Aquantify%2520backward%2520transfer.%2520Traditional%2520task%2520averages%2520conflate%2520these%2520effects%250Aand%2520obscure%2520large%2520changes.%2520For%2520multiple-choice%2520benchmarks%252C%2520we%2520add%250Achance-adjusted%2520variants%2520that%2520subtract%2520the%2520expected%2520contribution%2520of%2520random%250Aguessing%2520from%2520pre-%2520and%2520post-training%2520accuracies.%2520We%2520apply%2520this%2520framework%2520across%250Apost-training%2520stages%252C%2520model%2520sizes%252C%2520and%2520data%2520scales.%2520Our%2520large-scale%2520analysis%250Ashows%2520that%253A%2520%25281%2529%2520Domain-continual%2520pretraining%2520induces%2520moderate%2520forgetting%2520with%250Alow-to-moderate%2520backward%2520transfer%253B%2520%25282%2529%2520RL/SFT%2520post-training%2520applied%2520to%2520base%250Amodels%2520and%2520Instruction%2520tuning%2520yields%2520moderate-to-large%2520backward%2520transfer%2520on%250Amath%2520and%2520logic%2520with%2520overall%2520low-to-moderate%2520forgetting%253B%2520%25283%2529%2520Applying%2520RL/SFT%2520to%250Ainstruction-tuned%2520models%2520is%2520sensitive%2520on%2520data%2520scale%253A%2520at%2520small%2520scales%252C%2520both%250Aforgetting%2520and%2520backward%2520transfer%2520are%2520small%253B%2520at%2520larger%2520scales%252C%2520effects%2520are%2520mixed%250Aand%2520warrant%2520further%2520study%2520with%2520better%2520controls%253B%2520%25284%2529%2520Model%2520merging%2520does%2520not%250Areliably%2520mitigate%2520forgetting.%2520Overall%252C%2520our%2520framework%2520offers%2520a%2520practical%250Ayardstick%2520for%2520mapping%2520how%2520post-training%2520alters%2520pretrained%2520knowledge%2520at%2520scale%2520--%250Aenabling%2520progress%2520towards%2520generally%2520capable%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20Post-Training%20Forgetting%20in%20Language%20Models%20at%20Scale&entry.906535625=Jackson%20Harmon%20and%20Andreas%20Hochlehnert%20and%20Matthias%20Bethge%20and%20Ameya%20Prabhu&entry.1292438233=%20%20Scaled%20post-training%20now%20drives%20many%20of%20the%20largest%20capability%20gains%20in%0Alanguage%20models%20%28LMs%29%2C%20yet%20its%20effect%20on%20pretrained%20knowledge%20remains%20poorly%0Aunderstood.%20Not%20all%20forgetting%20is%20equal%3A%20Forgetting%20one%20fact%20%28e.g.%2C%20a%20U.S.%0Apresident%20or%20an%20API%20call%29%20does%20not%20%22average%20out%22%20by%20recalling%20another.%20Hence%2C%0Awe%20propose%20a%20sample-wise%20paradigm%20to%20measure%20what%20is%20forgotten%20and%20when%0Abackward%20transfer%20occurs.%20Our%20metric%20counts%201-%3E0%20transitions%20%28correct%20before%0Apost-training%2C%20incorrect%20after%29%20to%20quantify%20forgetting%20and%200-%3E1%20transitions%20to%0Aquantify%20backward%20transfer.%20Traditional%20task%20averages%20conflate%20these%20effects%0Aand%20obscure%20large%20changes.%20For%20multiple-choice%20benchmarks%2C%20we%20add%0Achance-adjusted%20variants%20that%20subtract%20the%20expected%20contribution%20of%20random%0Aguessing%20from%20pre-%20and%20post-training%20accuracies.%20We%20apply%20this%20framework%20across%0Apost-training%20stages%2C%20model%20sizes%2C%20and%20data%20scales.%20Our%20large-scale%20analysis%0Ashows%20that%3A%20%281%29%20Domain-continual%20pretraining%20induces%20moderate%20forgetting%20with%0Alow-to-moderate%20backward%20transfer%3B%20%282%29%20RL/SFT%20post-training%20applied%20to%20base%0Amodels%20and%20Instruction%20tuning%20yields%20moderate-to-large%20backward%20transfer%20on%0Amath%20and%20logic%20with%20overall%20low-to-moderate%20forgetting%3B%20%283%29%20Applying%20RL/SFT%20to%0Ainstruction-tuned%20models%20is%20sensitive%20on%20data%20scale%3A%20at%20small%20scales%2C%20both%0Aforgetting%20and%20backward%20transfer%20are%20small%3B%20at%20larger%20scales%2C%20effects%20are%20mixed%0Aand%20warrant%20further%20study%20with%20better%20controls%3B%20%284%29%20Model%20merging%20does%20not%0Areliably%20mitigate%20forgetting.%20Overall%2C%20our%20framework%20offers%20a%20practical%0Ayardstick%20for%20mapping%20how%20post-training%20alters%20pretrained%20knowledge%20at%20scale%20--%0Aenabling%20progress%20towards%20generally%20capable%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17776v1&entry.124074799=Read"},
{"title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards", "author": "Zafir Stojanovski and Oliver Stanley and Joe Sharratt and Richard Jones and Abdulhakeem Adefioye and Jean Kaddour and Andreas K\u00f6pf", "abstract": "  We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.\n", "link": "http://arxiv.org/abs/2505.24760v2", "date": "2025-10-20", "relevancy": 1.9761, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5091}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REASONING%20GYM%3A%20Reasoning%20Environments%20for%20Reinforcement%20Learning%20with%0A%20%20Verifiable%20Rewards&body=Title%3A%20REASONING%20GYM%3A%20Reasoning%20Environments%20for%20Reinforcement%20Learning%20with%0A%20%20Verifiable%20Rewards%0AAuthor%3A%20Zafir%20Stojanovski%20and%20Oliver%20Stanley%20and%20Joe%20Sharratt%20and%20Richard%20Jones%20and%20Abdulhakeem%20Adefioye%20and%20Jean%20Kaddour%20and%20Andreas%20K%C3%B6pf%0AAbstract%3A%20%20%20We%20introduce%20Reasoning%20Gym%20%28RG%29%2C%20a%20library%20of%20reasoning%20environments%20for%0Areinforcement%20learning%20with%20verifiable%20rewards.%20It%20provides%20over%20100%20data%0Agenerators%20and%20verifiers%20spanning%20multiple%20domains%20including%20algebra%2C%0Aarithmetic%2C%20computation%2C%20cognition%2C%20geometry%2C%20graph%20theory%2C%20logic%2C%20and%20various%0Acommon%20games.%20Its%20key%20innovation%20is%20the%20ability%20to%20generate%20virtually%20infinite%0Atraining%20data%20with%20adjustable%20complexity%2C%20unlike%20most%20previous%20reasoning%0Adatasets%2C%20which%20are%20typically%20fixed.%20This%20procedural%20generation%20approach%20allows%0Afor%20continuous%20evaluation%20across%20varying%20difficulty%20levels.%20Our%20experimental%0Aresults%20demonstrate%20the%20efficacy%20of%20RG%20in%20both%20evaluating%20and%20reinforcement%0Alearning%20of%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREASONING%2520GYM%253A%2520Reasoning%2520Environments%2520for%2520Reinforcement%2520Learning%2520with%250A%2520%2520Verifiable%2520Rewards%26entry.906535625%3DZafir%2520Stojanovski%2520and%2520Oliver%2520Stanley%2520and%2520Joe%2520Sharratt%2520and%2520Richard%2520Jones%2520and%2520Abdulhakeem%2520Adefioye%2520and%2520Jean%2520Kaddour%2520and%2520Andreas%2520K%25C3%25B6pf%26entry.1292438233%3D%2520%2520We%2520introduce%2520Reasoning%2520Gym%2520%2528RG%2529%252C%2520a%2520library%2520of%2520reasoning%2520environments%2520for%250Areinforcement%2520learning%2520with%2520verifiable%2520rewards.%2520It%2520provides%2520over%2520100%2520data%250Agenerators%2520and%2520verifiers%2520spanning%2520multiple%2520domains%2520including%2520algebra%252C%250Aarithmetic%252C%2520computation%252C%2520cognition%252C%2520geometry%252C%2520graph%2520theory%252C%2520logic%252C%2520and%2520various%250Acommon%2520games.%2520Its%2520key%2520innovation%2520is%2520the%2520ability%2520to%2520generate%2520virtually%2520infinite%250Atraining%2520data%2520with%2520adjustable%2520complexity%252C%2520unlike%2520most%2520previous%2520reasoning%250Adatasets%252C%2520which%2520are%2520typically%2520fixed.%2520This%2520procedural%2520generation%2520approach%2520allows%250Afor%2520continuous%2520evaluation%2520across%2520varying%2520difficulty%2520levels.%2520Our%2520experimental%250Aresults%2520demonstrate%2520the%2520efficacy%2520of%2520RG%2520in%2520both%2520evaluating%2520and%2520reinforcement%250Alearning%2520of%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REASONING%20GYM%3A%20Reasoning%20Environments%20for%20Reinforcement%20Learning%20with%0A%20%20Verifiable%20Rewards&entry.906535625=Zafir%20Stojanovski%20and%20Oliver%20Stanley%20and%20Joe%20Sharratt%20and%20Richard%20Jones%20and%20Abdulhakeem%20Adefioye%20and%20Jean%20Kaddour%20and%20Andreas%20K%C3%B6pf&entry.1292438233=%20%20We%20introduce%20Reasoning%20Gym%20%28RG%29%2C%20a%20library%20of%20reasoning%20environments%20for%0Areinforcement%20learning%20with%20verifiable%20rewards.%20It%20provides%20over%20100%20data%0Agenerators%20and%20verifiers%20spanning%20multiple%20domains%20including%20algebra%2C%0Aarithmetic%2C%20computation%2C%20cognition%2C%20geometry%2C%20graph%20theory%2C%20logic%2C%20and%20various%0Acommon%20games.%20Its%20key%20innovation%20is%20the%20ability%20to%20generate%20virtually%20infinite%0Atraining%20data%20with%20adjustable%20complexity%2C%20unlike%20most%20previous%20reasoning%0Adatasets%2C%20which%20are%20typically%20fixed.%20This%20procedural%20generation%20approach%20allows%0Afor%20continuous%20evaluation%20across%20varying%20difficulty%20levels.%20Our%20experimental%0Aresults%20demonstrate%20the%20efficacy%20of%20RG%20in%20both%20evaluating%20and%20reinforcement%0Alearning%20of%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24760v2&entry.124074799=Read"},
{"title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference\n  Optimization", "author": "Yongcheng Zeng and Xinyu Cui and Xuanfa Jin and Qirui Mi and Guoqing Liu and Zexu Sun and Mengyue Yang and Dong Li and Weiyu Ma and Ning Yang and Jian Zhao and Jianye Hao and Haifeng Zhang and Jun Wang", "abstract": "  Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.\n", "link": "http://arxiv.org/abs/2502.05605v5", "date": "2025-10-20", "relevancy": 1.976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolving%20LLMs%27%20Self-Refinement%20Capability%20via%20Iterative%20Preference%0A%20%20Optimization&body=Title%3A%20Evolving%20LLMs%27%20Self-Refinement%20Capability%20via%20Iterative%20Preference%0A%20%20Optimization%0AAuthor%3A%20Yongcheng%20Zeng%20and%20Xinyu%20Cui%20and%20Xuanfa%20Jin%20and%20Qirui%20Mi%20and%20Guoqing%20Liu%20and%20Zexu%20Sun%20and%20Mengyue%20Yang%20and%20Dong%20Li%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Jian%20Zhao%20and%20Jianye%20Hao%20and%20Haifeng%20Zhang%20and%20Jun%20Wang%0AAbstract%3A%20%20%20Self-Refinement%20refers%20to%20a%20model%27s%20ability%20to%20revise%20its%20own%20responses%20to%0Aproduce%20improved%20outputs.%20This%20capability%20can%20also%20serve%20as%20a%20fundamental%0Amechanism%20for%20Self-Improvement%2C%20for%20example%2C%20by%20reconstructing%20datasets%20with%0Arefined%20results%20to%20enhance%20intrinsic%20model%20performance.%20However%2C%20our%0Acomprehensive%20experiments%20reveal%20that%20large%20language%20models%20%28LLMs%29%20show%20no%0Aclear%20evidence%20of%20inherent%20Self-Refinement%20and%20may%20even%20experience%20response%0Aquality%20degradation%20after%20Self-Refinement.%20To%20address%20this%20issue%2C%20we%20propose%0AEVOLVE%2C%20a%20simple%20and%20effective%20framework%20for%20eliciting%20and%20tracking%20the%0Aevolution%20of%20Self-Refinement%20through%20iterative%20training.%20We%20first%20explore%0Aoptimization%20methods%20during%20training%20to%20activate%20the%20model%27s%20Self-Refinement%0Acapability.%20Then%2C%20at%20inference%2C%20we%20investigate%20various%20generation%20strategies%20to%0Afurther%20enhance%20and%20utilize%20Self-Refinement%20while%20supplying%20the%20necessary%20data%0Afor%20training.%20Through%20synergistic%20optimization%20of%20training%20and%20inference%0Astages%2C%20we%20continually%20evolve%20the%20model%27s%20Self-Refinement%20ability%2C%20enabling%20it%0Ato%20better%20refine%20its%20own%20responses.%20Moreover%2C%20we%20demonstrate%20the%20potential%20of%0Aleveraging%20Self-Refinement%20to%20achieve%20broader%20Self-Improvement%20of%20intrinsic%0Amodel%20abilities.%20Experiments%20show%20that%20the%20evolved%20Self-Refinement%20ability%0Aenables%20the%20Llama-3.1-8B%20base%20model%20to%20surpass%20GPT-4o%2C%20achieving%2062.3%25%0Alength-controlled%20and%2063.3%25%20raw%20win%20rates%20on%20AlpacaEval%202%2C%20and%2050.3%25%20on%0AArena-Hard.%20It%20also%20generalizes%20effectively%20to%20out-of-domain%20reasoning%20tasks%2C%0Aimproving%20performance%20on%20mathematical%20reasoning%20benchmarks%20such%20as%20GSM8K%20and%0AMATH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05605v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolving%2520LLMs%2527%2520Self-Refinement%2520Capability%2520via%2520Iterative%2520Preference%250A%2520%2520Optimization%26entry.906535625%3DYongcheng%2520Zeng%2520and%2520Xinyu%2520Cui%2520and%2520Xuanfa%2520Jin%2520and%2520Qirui%2520Mi%2520and%2520Guoqing%2520Liu%2520and%2520Zexu%2520Sun%2520and%2520Mengyue%2520Yang%2520and%2520Dong%2520Li%2520and%2520Weiyu%2520Ma%2520and%2520Ning%2520Yang%2520and%2520Jian%2520Zhao%2520and%2520Jianye%2520Hao%2520and%2520Haifeng%2520Zhang%2520and%2520Jun%2520Wang%26entry.1292438233%3D%2520%2520Self-Refinement%2520refers%2520to%2520a%2520model%2527s%2520ability%2520to%2520revise%2520its%2520own%2520responses%2520to%250Aproduce%2520improved%2520outputs.%2520This%2520capability%2520can%2520also%2520serve%2520as%2520a%2520fundamental%250Amechanism%2520for%2520Self-Improvement%252C%2520for%2520example%252C%2520by%2520reconstructing%2520datasets%2520with%250Arefined%2520results%2520to%2520enhance%2520intrinsic%2520model%2520performance.%2520However%252C%2520our%250Acomprehensive%2520experiments%2520reveal%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520show%2520no%250Aclear%2520evidence%2520of%2520inherent%2520Self-Refinement%2520and%2520may%2520even%2520experience%2520response%250Aquality%2520degradation%2520after%2520Self-Refinement.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AEVOLVE%252C%2520a%2520simple%2520and%2520effective%2520framework%2520for%2520eliciting%2520and%2520tracking%2520the%250Aevolution%2520of%2520Self-Refinement%2520through%2520iterative%2520training.%2520We%2520first%2520explore%250Aoptimization%2520methods%2520during%2520training%2520to%2520activate%2520the%2520model%2527s%2520Self-Refinement%250Acapability.%2520Then%252C%2520at%2520inference%252C%2520we%2520investigate%2520various%2520generation%2520strategies%2520to%250Afurther%2520enhance%2520and%2520utilize%2520Self-Refinement%2520while%2520supplying%2520the%2520necessary%2520data%250Afor%2520training.%2520Through%2520synergistic%2520optimization%2520of%2520training%2520and%2520inference%250Astages%252C%2520we%2520continually%2520evolve%2520the%2520model%2527s%2520Self-Refinement%2520ability%252C%2520enabling%2520it%250Ato%2520better%2520refine%2520its%2520own%2520responses.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520potential%2520of%250Aleveraging%2520Self-Refinement%2520to%2520achieve%2520broader%2520Self-Improvement%2520of%2520intrinsic%250Amodel%2520abilities.%2520Experiments%2520show%2520that%2520the%2520evolved%2520Self-Refinement%2520ability%250Aenables%2520the%2520Llama-3.1-8B%2520base%2520model%2520to%2520surpass%2520GPT-4o%252C%2520achieving%252062.3%2525%250Alength-controlled%2520and%252063.3%2525%2520raw%2520win%2520rates%2520on%2520AlpacaEval%25202%252C%2520and%252050.3%2525%2520on%250AArena-Hard.%2520It%2520also%2520generalizes%2520effectively%2520to%2520out-of-domain%2520reasoning%2520tasks%252C%250Aimproving%2520performance%2520on%2520mathematical%2520reasoning%2520benchmarks%2520such%2520as%2520GSM8K%2520and%250AMATH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05605v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolving%20LLMs%27%20Self-Refinement%20Capability%20via%20Iterative%20Preference%0A%20%20Optimization&entry.906535625=Yongcheng%20Zeng%20and%20Xinyu%20Cui%20and%20Xuanfa%20Jin%20and%20Qirui%20Mi%20and%20Guoqing%20Liu%20and%20Zexu%20Sun%20and%20Mengyue%20Yang%20and%20Dong%20Li%20and%20Weiyu%20Ma%20and%20Ning%20Yang%20and%20Jian%20Zhao%20and%20Jianye%20Hao%20and%20Haifeng%20Zhang%20and%20Jun%20Wang&entry.1292438233=%20%20Self-Refinement%20refers%20to%20a%20model%27s%20ability%20to%20revise%20its%20own%20responses%20to%0Aproduce%20improved%20outputs.%20This%20capability%20can%20also%20serve%20as%20a%20fundamental%0Amechanism%20for%20Self-Improvement%2C%20for%20example%2C%20by%20reconstructing%20datasets%20with%0Arefined%20results%20to%20enhance%20intrinsic%20model%20performance.%20However%2C%20our%0Acomprehensive%20experiments%20reveal%20that%20large%20language%20models%20%28LLMs%29%20show%20no%0Aclear%20evidence%20of%20inherent%20Self-Refinement%20and%20may%20even%20experience%20response%0Aquality%20degradation%20after%20Self-Refinement.%20To%20address%20this%20issue%2C%20we%20propose%0AEVOLVE%2C%20a%20simple%20and%20effective%20framework%20for%20eliciting%20and%20tracking%20the%0Aevolution%20of%20Self-Refinement%20through%20iterative%20training.%20We%20first%20explore%0Aoptimization%20methods%20during%20training%20to%20activate%20the%20model%27s%20Self-Refinement%0Acapability.%20Then%2C%20at%20inference%2C%20we%20investigate%20various%20generation%20strategies%20to%0Afurther%20enhance%20and%20utilize%20Self-Refinement%20while%20supplying%20the%20necessary%20data%0Afor%20training.%20Through%20synergistic%20optimization%20of%20training%20and%20inference%0Astages%2C%20we%20continually%20evolve%20the%20model%27s%20Self-Refinement%20ability%2C%20enabling%20it%0Ato%20better%20refine%20its%20own%20responses.%20Moreover%2C%20we%20demonstrate%20the%20potential%20of%0Aleveraging%20Self-Refinement%20to%20achieve%20broader%20Self-Improvement%20of%20intrinsic%0Amodel%20abilities.%20Experiments%20show%20that%20the%20evolved%20Self-Refinement%20ability%0Aenables%20the%20Llama-3.1-8B%20base%20model%20to%20surpass%20GPT-4o%2C%20achieving%2062.3%25%0Alength-controlled%20and%2063.3%25%20raw%20win%20rates%20on%20AlpacaEval%202%2C%20and%2050.3%25%20on%0AArena-Hard.%20It%20also%20generalizes%20effectively%20to%20out-of-domain%20reasoning%20tasks%2C%0Aimproving%20performance%20on%20mathematical%20reasoning%20benchmarks%20such%20as%20GSM8K%20and%0AMATH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05605v5&entry.124074799=Read"},
{"title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and\n  Interpretable Survival Prediction", "author": "Raghu Vamshi Hemadri and Geetha Krishna Guruju and Kristi Topollai and Anna Ewa Choromanska", "abstract": "  Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.\n", "link": "http://arxiv.org/abs/2510.17532v1", "date": "2025-10-20", "relevancy": 1.9728, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OncoReason%3A%20Structuring%20Clinical%20Reasoning%20in%20LLMs%20for%20Robust%20and%0A%20%20Interpretable%20Survival%20Prediction&body=Title%3A%20OncoReason%3A%20Structuring%20Clinical%20Reasoning%20in%20LLMs%20for%20Robust%20and%0A%20%20Interpretable%20Survival%20Prediction%0AAuthor%3A%20Raghu%20Vamshi%20Hemadri%20and%20Geetha%20Krishna%20Guruju%20and%20Kristi%20Topollai%20and%20Anna%20Ewa%20Choromanska%0AAbstract%3A%20%20%20Predicting%20cancer%20treatment%20outcomes%20requires%20models%20that%20are%20both%20accurate%0Aand%20interpretable%2C%20particularly%20in%20the%20presence%20of%20heterogeneous%20clinical%20data.%0AWhile%20large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20performance%20in%20biomedical%0ANLP%2C%20they%20often%20lack%20structured%20reasoning%20capabilities%20critical%20for%20high-stakes%0Adecision%20support.%20We%20present%20a%20unified%2C%20multi-task%20learning%20framework%20that%0Aaligns%20autoregressive%20LLMs%20with%20clinical%20reasoning%20for%20outcome%20prediction%20on%0Athe%20MSK-CHORD%20dataset.%20Our%20models%20are%20trained%20to%20jointly%20perform%20binary%0Asurvival%20classification%2C%20continuous%20survival%20time%20regression%2C%20and%20natural%0Alanguage%20rationale%20generation.%20We%20evaluate%20three%20alignment%20strategies%3A%20%281%29%0Astandard%20supervised%20fine-tuning%20%28SFT%29%2C%20%282%29%20SFT%20with%20Chain-of-Thought%20%28CoT%29%0Aprompting%20to%20elicit%20step-by-step%20reasoning%2C%20and%20%283%29%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%2C%20a%20reinforcement%20learning%20method%20that%20aligns%20model%20outputs%0Ato%20expert-derived%20reasoning%20trajectories.%20Experiments%20with%20LLaMa3-8B%20and%0AMed42-8B%20backbones%20demonstrate%20that%20CoT%20prompting%20improves%20F1%20by%20%2B6.0%20and%0Areduces%20MAE%20by%2012%25%2C%20while%20GRPO%20achieves%20state-of-the-art%20interpretability%20and%0Apredictive%20performance%20across%20BLEU%2C%20ROUGE%2C%20and%20BERTScore.%20We%20further%20show%20that%0Aexisting%20biomedical%20LLMs%20often%20fail%20to%20produce%20valid%20reasoning%20traces%20due%20to%0Aarchitectural%20constraints.%20Our%20findings%20underscore%20the%20importance%20of%0Areasoning-aware%20alignment%20in%20multi-task%20clinical%20modeling%20and%20set%20a%20new%0Abenchmark%20for%20interpretable%2C%20trustworthy%20LLMs%20in%20precision%20oncology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOncoReason%253A%2520Structuring%2520Clinical%2520Reasoning%2520in%2520LLMs%2520for%2520Robust%2520and%250A%2520%2520Interpretable%2520Survival%2520Prediction%26entry.906535625%3DRaghu%2520Vamshi%2520Hemadri%2520and%2520Geetha%2520Krishna%2520Guruju%2520and%2520Kristi%2520Topollai%2520and%2520Anna%2520Ewa%2520Choromanska%26entry.1292438233%3D%2520%2520Predicting%2520cancer%2520treatment%2520outcomes%2520requires%2520models%2520that%2520are%2520both%2520accurate%250Aand%2520interpretable%252C%2520particularly%2520in%2520the%2520presence%2520of%2520heterogeneous%2520clinical%2520data.%250AWhile%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520performance%2520in%2520biomedical%250ANLP%252C%2520they%2520often%2520lack%2520structured%2520reasoning%2520capabilities%2520critical%2520for%2520high-stakes%250Adecision%2520support.%2520We%2520present%2520a%2520unified%252C%2520multi-task%2520learning%2520framework%2520that%250Aaligns%2520autoregressive%2520LLMs%2520with%2520clinical%2520reasoning%2520for%2520outcome%2520prediction%2520on%250Athe%2520MSK-CHORD%2520dataset.%2520Our%2520models%2520are%2520trained%2520to%2520jointly%2520perform%2520binary%250Asurvival%2520classification%252C%2520continuous%2520survival%2520time%2520regression%252C%2520and%2520natural%250Alanguage%2520rationale%2520generation.%2520We%2520evaluate%2520three%2520alignment%2520strategies%253A%2520%25281%2529%250Astandard%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520%25282%2529%2520SFT%2520with%2520Chain-of-Thought%2520%2528CoT%2529%250Aprompting%2520to%2520elicit%2520step-by-step%2520reasoning%252C%2520and%2520%25283%2529%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528GRPO%2529%252C%2520a%2520reinforcement%2520learning%2520method%2520that%2520aligns%2520model%2520outputs%250Ato%2520expert-derived%2520reasoning%2520trajectories.%2520Experiments%2520with%2520LLaMa3-8B%2520and%250AMed42-8B%2520backbones%2520demonstrate%2520that%2520CoT%2520prompting%2520improves%2520F1%2520by%2520%252B6.0%2520and%250Areduces%2520MAE%2520by%252012%2525%252C%2520while%2520GRPO%2520achieves%2520state-of-the-art%2520interpretability%2520and%250Apredictive%2520performance%2520across%2520BLEU%252C%2520ROUGE%252C%2520and%2520BERTScore.%2520We%2520further%2520show%2520that%250Aexisting%2520biomedical%2520LLMs%2520often%2520fail%2520to%2520produce%2520valid%2520reasoning%2520traces%2520due%2520to%250Aarchitectural%2520constraints.%2520Our%2520findings%2520underscore%2520the%2520importance%2520of%250Areasoning-aware%2520alignment%2520in%2520multi-task%2520clinical%2520modeling%2520and%2520set%2520a%2520new%250Abenchmark%2520for%2520interpretable%252C%2520trustworthy%2520LLMs%2520in%2520precision%2520oncology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OncoReason%3A%20Structuring%20Clinical%20Reasoning%20in%20LLMs%20for%20Robust%20and%0A%20%20Interpretable%20Survival%20Prediction&entry.906535625=Raghu%20Vamshi%20Hemadri%20and%20Geetha%20Krishna%20Guruju%20and%20Kristi%20Topollai%20and%20Anna%20Ewa%20Choromanska&entry.1292438233=%20%20Predicting%20cancer%20treatment%20outcomes%20requires%20models%20that%20are%20both%20accurate%0Aand%20interpretable%2C%20particularly%20in%20the%20presence%20of%20heterogeneous%20clinical%20data.%0AWhile%20large%20language%20models%20%28LLMs%29%20have%20shown%20strong%20performance%20in%20biomedical%0ANLP%2C%20they%20often%20lack%20structured%20reasoning%20capabilities%20critical%20for%20high-stakes%0Adecision%20support.%20We%20present%20a%20unified%2C%20multi-task%20learning%20framework%20that%0Aaligns%20autoregressive%20LLMs%20with%20clinical%20reasoning%20for%20outcome%20prediction%20on%0Athe%20MSK-CHORD%20dataset.%20Our%20models%20are%20trained%20to%20jointly%20perform%20binary%0Asurvival%20classification%2C%20continuous%20survival%20time%20regression%2C%20and%20natural%0Alanguage%20rationale%20generation.%20We%20evaluate%20three%20alignment%20strategies%3A%20%281%29%0Astandard%20supervised%20fine-tuning%20%28SFT%29%2C%20%282%29%20SFT%20with%20Chain-of-Thought%20%28CoT%29%0Aprompting%20to%20elicit%20step-by-step%20reasoning%2C%20and%20%283%29%20Group%20Relative%20Policy%0AOptimization%20%28GRPO%29%2C%20a%20reinforcement%20learning%20method%20that%20aligns%20model%20outputs%0Ato%20expert-derived%20reasoning%20trajectories.%20Experiments%20with%20LLaMa3-8B%20and%0AMed42-8B%20backbones%20demonstrate%20that%20CoT%20prompting%20improves%20F1%20by%20%2B6.0%20and%0Areduces%20MAE%20by%2012%25%2C%20while%20GRPO%20achieves%20state-of-the-art%20interpretability%20and%0Apredictive%20performance%20across%20BLEU%2C%20ROUGE%2C%20and%20BERTScore.%20We%20further%20show%20that%0Aexisting%20biomedical%20LLMs%20often%20fail%20to%20produce%20valid%20reasoning%20traces%20due%20to%0Aarchitectural%20constraints.%20Our%20findings%20underscore%20the%20importance%20of%0Areasoning-aware%20alignment%20in%20multi-task%20clinical%20modeling%20and%20set%20a%20new%0Abenchmark%20for%20interpretable%2C%20trustworthy%20LLMs%20in%20precision%20oncology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17532v1&entry.124074799=Read"},
{"title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning", "author": "Lindsay Spoor and \u00c1lvaro Serra-G\u00f3mez and Aske Plaat and Thomas Moerland", "abstract": "  In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.\n", "link": "http://arxiv.org/abs/2510.17564v1", "date": "2025-10-20", "relevancy": 1.9618, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.51}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Lagrangian%20Methods%20in%20Safe%20Reinforcement%20Learning&body=Title%3A%20An%20Empirical%20Study%20of%20Lagrangian%20Methods%20in%20Safe%20Reinforcement%20Learning%0AAuthor%3A%20Lindsay%20Spoor%20and%20%C3%81lvaro%20Serra-G%C3%B3mez%20and%20Aske%20Plaat%20and%20Thomas%20Moerland%0AAbstract%3A%20%20%20In%20safety-critical%20domains%20such%20as%20robotics%2C%20navigation%20and%20power%20systems%2C%0Aconstrained%20optimization%20problems%20arise%20where%20maximizing%20performance%20must%20be%0Acarefully%20balanced%20with%20associated%20constraints.%20Safe%20reinforcement%20learning%0Aprovides%20a%20framework%20to%20address%20these%20challenges%2C%20with%20Lagrangian%20methods%20being%0Aa%20popular%20choice.%20However%2C%20the%20effectiveness%20of%20Lagrangian%20methods%20crucially%0Adepends%20on%20the%20choice%20of%20the%20Lagrange%20multiplier%20%24%5Clambda%24%2C%20which%20governs%20the%0Atrade-off%20between%20return%20and%20constraint%20cost.%20A%20common%20approach%20is%20to%20update%0Athe%20multiplier%20automatically%20during%20training.%20Although%20this%20is%20standard%20in%0Apractice%2C%20there%20remains%20limited%20empirical%20evidence%20on%20the%20robustness%20of%20an%0Aautomated%20update%20and%20its%20influence%20on%20overall%20performance.%20Therefore%2C%20we%0Aanalyze%20%28i%29%20optimality%20and%20%28ii%29%20stability%20of%20Lagrange%20multipliers%20in%20safe%0Areinforcement%20learning%20across%20a%20range%20of%20tasks.%20We%20provide%20%24%5Clambda%24-profiles%0Athat%20give%20a%20complete%20visualization%20of%20the%20trade-off%20between%20return%20and%0Aconstraint%20cost%20of%20the%20optimization%20problem.%20These%20profiles%20show%20the%20highly%0Asensitive%20nature%20of%20%24%5Clambda%24%20and%20moreover%20confirm%20the%20lack%20of%20general%0Aintuition%20for%20choosing%20the%20optimal%20value%20%24%5Clambda%5E%2A%24.%20Our%20findings%20additionally%0Ashow%20that%20automated%20multiplier%20updates%20are%20able%20to%20recover%20and%20sometimes%20even%0Aexceed%20the%20optimal%20performance%20found%20at%20%24%5Clambda%5E%2A%24%20due%20to%20the%20vast%20difference%0Ain%20their%20learning%20trajectories.%20Furthermore%2C%20we%20show%20that%20automated%20multiplier%0Aupdates%20exhibit%20oscillatory%20behavior%20during%20training%2C%20which%20can%20be%20mitigated%0Athrough%20PID-controlled%20updates.%20However%2C%20this%20method%20requires%20careful%20tuning%20to%0Aachieve%20consistently%20better%20performance%20across%20tasks.%20This%20highlights%20the%20need%0Afor%20further%20research%20on%20stabilizing%20Lagrangian%20methods%20in%20safe%20reinforcement%0Alearning.%20The%20code%20used%20to%20reproduce%20our%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/lindsayspoor/Lagrangian_SafeRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Lagrangian%2520Methods%2520in%2520Safe%2520Reinforcement%2520Learning%26entry.906535625%3DLindsay%2520Spoor%2520and%2520%25C3%2581lvaro%2520Serra-G%25C3%25B3mez%2520and%2520Aske%2520Plaat%2520and%2520Thomas%2520Moerland%26entry.1292438233%3D%2520%2520In%2520safety-critical%2520domains%2520such%2520as%2520robotics%252C%2520navigation%2520and%2520power%2520systems%252C%250Aconstrained%2520optimization%2520problems%2520arise%2520where%2520maximizing%2520performance%2520must%2520be%250Acarefully%2520balanced%2520with%2520associated%2520constraints.%2520Safe%2520reinforcement%2520learning%250Aprovides%2520a%2520framework%2520to%2520address%2520these%2520challenges%252C%2520with%2520Lagrangian%2520methods%2520being%250Aa%2520popular%2520choice.%2520However%252C%2520the%2520effectiveness%2520of%2520Lagrangian%2520methods%2520crucially%250Adepends%2520on%2520the%2520choice%2520of%2520the%2520Lagrange%2520multiplier%2520%2524%255Clambda%2524%252C%2520which%2520governs%2520the%250Atrade-off%2520between%2520return%2520and%2520constraint%2520cost.%2520A%2520common%2520approach%2520is%2520to%2520update%250Athe%2520multiplier%2520automatically%2520during%2520training.%2520Although%2520this%2520is%2520standard%2520in%250Apractice%252C%2520there%2520remains%2520limited%2520empirical%2520evidence%2520on%2520the%2520robustness%2520of%2520an%250Aautomated%2520update%2520and%2520its%2520influence%2520on%2520overall%2520performance.%2520Therefore%252C%2520we%250Aanalyze%2520%2528i%2529%2520optimality%2520and%2520%2528ii%2529%2520stability%2520of%2520Lagrange%2520multipliers%2520in%2520safe%250Areinforcement%2520learning%2520across%2520a%2520range%2520of%2520tasks.%2520We%2520provide%2520%2524%255Clambda%2524-profiles%250Athat%2520give%2520a%2520complete%2520visualization%2520of%2520the%2520trade-off%2520between%2520return%2520and%250Aconstraint%2520cost%2520of%2520the%2520optimization%2520problem.%2520These%2520profiles%2520show%2520the%2520highly%250Asensitive%2520nature%2520of%2520%2524%255Clambda%2524%2520and%2520moreover%2520confirm%2520the%2520lack%2520of%2520general%250Aintuition%2520for%2520choosing%2520the%2520optimal%2520value%2520%2524%255Clambda%255E%252A%2524.%2520Our%2520findings%2520additionally%250Ashow%2520that%2520automated%2520multiplier%2520updates%2520are%2520able%2520to%2520recover%2520and%2520sometimes%2520even%250Aexceed%2520the%2520optimal%2520performance%2520found%2520at%2520%2524%255Clambda%255E%252A%2524%2520due%2520to%2520the%2520vast%2520difference%250Ain%2520their%2520learning%2520trajectories.%2520Furthermore%252C%2520we%2520show%2520that%2520automated%2520multiplier%250Aupdates%2520exhibit%2520oscillatory%2520behavior%2520during%2520training%252C%2520which%2520can%2520be%2520mitigated%250Athrough%2520PID-controlled%2520updates.%2520However%252C%2520this%2520method%2520requires%2520careful%2520tuning%2520to%250Aachieve%2520consistently%2520better%2520performance%2520across%2520tasks.%2520This%2520highlights%2520the%2520need%250Afor%2520further%2520research%2520on%2520stabilizing%2520Lagrangian%2520methods%2520in%2520safe%2520reinforcement%250Alearning.%2520The%2520code%2520used%2520to%2520reproduce%2520our%2520results%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/lindsayspoor/Lagrangian_SafeRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Lagrangian%20Methods%20in%20Safe%20Reinforcement%20Learning&entry.906535625=Lindsay%20Spoor%20and%20%C3%81lvaro%20Serra-G%C3%B3mez%20and%20Aske%20Plaat%20and%20Thomas%20Moerland&entry.1292438233=%20%20In%20safety-critical%20domains%20such%20as%20robotics%2C%20navigation%20and%20power%20systems%2C%0Aconstrained%20optimization%20problems%20arise%20where%20maximizing%20performance%20must%20be%0Acarefully%20balanced%20with%20associated%20constraints.%20Safe%20reinforcement%20learning%0Aprovides%20a%20framework%20to%20address%20these%20challenges%2C%20with%20Lagrangian%20methods%20being%0Aa%20popular%20choice.%20However%2C%20the%20effectiveness%20of%20Lagrangian%20methods%20crucially%0Adepends%20on%20the%20choice%20of%20the%20Lagrange%20multiplier%20%24%5Clambda%24%2C%20which%20governs%20the%0Atrade-off%20between%20return%20and%20constraint%20cost.%20A%20common%20approach%20is%20to%20update%0Athe%20multiplier%20automatically%20during%20training.%20Although%20this%20is%20standard%20in%0Apractice%2C%20there%20remains%20limited%20empirical%20evidence%20on%20the%20robustness%20of%20an%0Aautomated%20update%20and%20its%20influence%20on%20overall%20performance.%20Therefore%2C%20we%0Aanalyze%20%28i%29%20optimality%20and%20%28ii%29%20stability%20of%20Lagrange%20multipliers%20in%20safe%0Areinforcement%20learning%20across%20a%20range%20of%20tasks.%20We%20provide%20%24%5Clambda%24-profiles%0Athat%20give%20a%20complete%20visualization%20of%20the%20trade-off%20between%20return%20and%0Aconstraint%20cost%20of%20the%20optimization%20problem.%20These%20profiles%20show%20the%20highly%0Asensitive%20nature%20of%20%24%5Clambda%24%20and%20moreover%20confirm%20the%20lack%20of%20general%0Aintuition%20for%20choosing%20the%20optimal%20value%20%24%5Clambda%5E%2A%24.%20Our%20findings%20additionally%0Ashow%20that%20automated%20multiplier%20updates%20are%20able%20to%20recover%20and%20sometimes%20even%0Aexceed%20the%20optimal%20performance%20found%20at%20%24%5Clambda%5E%2A%24%20due%20to%20the%20vast%20difference%0Ain%20their%20learning%20trajectories.%20Furthermore%2C%20we%20show%20that%20automated%20multiplier%0Aupdates%20exhibit%20oscillatory%20behavior%20during%20training%2C%20which%20can%20be%20mitigated%0Athrough%20PID-controlled%20updates.%20However%2C%20this%20method%20requires%20careful%20tuning%20to%0Aachieve%20consistently%20better%20performance%20across%20tasks.%20This%20highlights%20the%20need%0Afor%20further%20research%20on%20stabilizing%20Lagrangian%20methods%20in%20safe%20reinforcement%0Alearning.%20The%20code%20used%20to%20reproduce%20our%20results%20can%20be%20found%20at%0Ahttps%3A//github.com/lindsayspoor/Lagrangian_SafeRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17564v1&entry.124074799=Read"},
{"title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems", "author": "Meiru Zhang and Philipp Borchert and Milan Gritta and Gerasimos Lampouras", "abstract": "  Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.\n", "link": "http://arxiv.org/abs/2510.10815v3", "date": "2025-10-20", "relevancy": 1.9612, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4907}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRIFT%3A%20Decompose%2C%20Retrieve%2C%20Illustrate%2C%20then%20Formalize%20Theorems&body=Title%3A%20DRIFT%3A%20Decompose%2C%20Retrieve%2C%20Illustrate%2C%20then%20Formalize%20Theorems%0AAuthor%3A%20Meiru%20Zhang%20and%20Philipp%20Borchert%20and%20Milan%20Gritta%20and%20Gerasimos%20Lampouras%0AAbstract%3A%20%20%20Automating%20the%20formalization%20of%20mathematical%20statements%20for%20theorem%20proving%0Aremains%20a%20major%20challenge%20for%20Large%20Language%20Models%20%28LLMs%29.%20LLMs%20struggle%20to%0Aidentify%20and%20utilize%20the%20prerequisite%20mathematical%20knowledge%20and%20its%0Acorresponding%20formal%20representation%20in%20languages%20like%20Lean.%20Current%0Aretrieval-augmented%20autoformalization%20methods%20query%20external%20libraries%20using%0Athe%20informal%20statement%20directly%2C%20but%20overlook%20a%20fundamental%20limitation%3A%0Ainformal%20mathematical%20statements%20are%20often%20complex%20and%20offer%20limited%20context%20on%0Athe%20underlying%20math%20concepts.%20To%20address%20this%2C%20we%20introduce%20DRIFT%2C%20a%20novel%0Aframework%20that%20enables%20LLMs%20to%20decompose%20informal%20mathematical%20statements%20into%0Asmaller%2C%20more%20tractable%20%27%27sub-components%27%27.%20This%20facilitates%20targeted%20retrieval%0Aof%20premises%20from%20mathematical%20libraries%20such%20as%20Mathlib.%20Additionally%2C%20DRIFT%0Aretrieves%20illustrative%20theorems%20to%20help%20models%20use%20premises%20more%20effectively%20in%0Aformalization%20tasks.%20We%20evaluate%20DRIFT%20across%20diverse%20benchmarks%20%28ProofNet%2C%0AConNF%2C%20and%20MiniF2F-test%29%20and%20find%20that%20it%20consistently%20improves%20premise%0Aretrieval%2C%20nearly%20doubling%20the%20F1%20score%20compared%20to%20the%20DPR%20baseline%20on%0AProofNet.%20Notably%2C%20DRIFT%20demonstrates%20strong%20performance%20on%20the%0Aout-of-distribution%20ConNF%20benchmark%2C%20with%20BEq%2B%4010%20improvements%20of%2037.14%25%20and%0A42.25%25%20using%20GPT-4.1%20and%20DeepSeek-V3.1%2C%20respectively.%20Our%20analysis%20shows%20that%0Aretrieval%20effectiveness%20in%20mathematical%20autoformalization%20depends%20heavily%20on%0Amodel-specific%20knowledge%20boundaries%2C%20highlighting%20the%20need%20for%20adaptive%0Aretrieval%20strategies%20aligned%20with%20each%20model%27s%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.10815v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRIFT%253A%2520Decompose%252C%2520Retrieve%252C%2520Illustrate%252C%2520then%2520Formalize%2520Theorems%26entry.906535625%3DMeiru%2520Zhang%2520and%2520Philipp%2520Borchert%2520and%2520Milan%2520Gritta%2520and%2520Gerasimos%2520Lampouras%26entry.1292438233%3D%2520%2520Automating%2520the%2520formalization%2520of%2520mathematical%2520statements%2520for%2520theorem%2520proving%250Aremains%2520a%2520major%2520challenge%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520LLMs%2520struggle%2520to%250Aidentify%2520and%2520utilize%2520the%2520prerequisite%2520mathematical%2520knowledge%2520and%2520its%250Acorresponding%2520formal%2520representation%2520in%2520languages%2520like%2520Lean.%2520Current%250Aretrieval-augmented%2520autoformalization%2520methods%2520query%2520external%2520libraries%2520using%250Athe%2520informal%2520statement%2520directly%252C%2520but%2520overlook%2520a%2520fundamental%2520limitation%253A%250Ainformal%2520mathematical%2520statements%2520are%2520often%2520complex%2520and%2520offer%2520limited%2520context%2520on%250Athe%2520underlying%2520math%2520concepts.%2520To%2520address%2520this%252C%2520we%2520introduce%2520DRIFT%252C%2520a%2520novel%250Aframework%2520that%2520enables%2520LLMs%2520to%2520decompose%2520informal%2520mathematical%2520statements%2520into%250Asmaller%252C%2520more%2520tractable%2520%2527%2527sub-components%2527%2527.%2520This%2520facilitates%2520targeted%2520retrieval%250Aof%2520premises%2520from%2520mathematical%2520libraries%2520such%2520as%2520Mathlib.%2520Additionally%252C%2520DRIFT%250Aretrieves%2520illustrative%2520theorems%2520to%2520help%2520models%2520use%2520premises%2520more%2520effectively%2520in%250Aformalization%2520tasks.%2520We%2520evaluate%2520DRIFT%2520across%2520diverse%2520benchmarks%2520%2528ProofNet%252C%250AConNF%252C%2520and%2520MiniF2F-test%2529%2520and%2520find%2520that%2520it%2520consistently%2520improves%2520premise%250Aretrieval%252C%2520nearly%2520doubling%2520the%2520F1%2520score%2520compared%2520to%2520the%2520DPR%2520baseline%2520on%250AProofNet.%2520Notably%252C%2520DRIFT%2520demonstrates%2520strong%2520performance%2520on%2520the%250Aout-of-distribution%2520ConNF%2520benchmark%252C%2520with%2520BEq%252B%254010%2520improvements%2520of%252037.14%2525%2520and%250A42.25%2525%2520using%2520GPT-4.1%2520and%2520DeepSeek-V3.1%252C%2520respectively.%2520Our%2520analysis%2520shows%2520that%250Aretrieval%2520effectiveness%2520in%2520mathematical%2520autoformalization%2520depends%2520heavily%2520on%250Amodel-specific%2520knowledge%2520boundaries%252C%2520highlighting%2520the%2520need%2520for%2520adaptive%250Aretrieval%2520strategies%2520aligned%2520with%2520each%2520model%2527s%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10815v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRIFT%3A%20Decompose%2C%20Retrieve%2C%20Illustrate%2C%20then%20Formalize%20Theorems&entry.906535625=Meiru%20Zhang%20and%20Philipp%20Borchert%20and%20Milan%20Gritta%20and%20Gerasimos%20Lampouras&entry.1292438233=%20%20Automating%20the%20formalization%20of%20mathematical%20statements%20for%20theorem%20proving%0Aremains%20a%20major%20challenge%20for%20Large%20Language%20Models%20%28LLMs%29.%20LLMs%20struggle%20to%0Aidentify%20and%20utilize%20the%20prerequisite%20mathematical%20knowledge%20and%20its%0Acorresponding%20formal%20representation%20in%20languages%20like%20Lean.%20Current%0Aretrieval-augmented%20autoformalization%20methods%20query%20external%20libraries%20using%0Athe%20informal%20statement%20directly%2C%20but%20overlook%20a%20fundamental%20limitation%3A%0Ainformal%20mathematical%20statements%20are%20often%20complex%20and%20offer%20limited%20context%20on%0Athe%20underlying%20math%20concepts.%20To%20address%20this%2C%20we%20introduce%20DRIFT%2C%20a%20novel%0Aframework%20that%20enables%20LLMs%20to%20decompose%20informal%20mathematical%20statements%20into%0Asmaller%2C%20more%20tractable%20%27%27sub-components%27%27.%20This%20facilitates%20targeted%20retrieval%0Aof%20premises%20from%20mathematical%20libraries%20such%20as%20Mathlib.%20Additionally%2C%20DRIFT%0Aretrieves%20illustrative%20theorems%20to%20help%20models%20use%20premises%20more%20effectively%20in%0Aformalization%20tasks.%20We%20evaluate%20DRIFT%20across%20diverse%20benchmarks%20%28ProofNet%2C%0AConNF%2C%20and%20MiniF2F-test%29%20and%20find%20that%20it%20consistently%20improves%20premise%0Aretrieval%2C%20nearly%20doubling%20the%20F1%20score%20compared%20to%20the%20DPR%20baseline%20on%0AProofNet.%20Notably%2C%20DRIFT%20demonstrates%20strong%20performance%20on%20the%0Aout-of-distribution%20ConNF%20benchmark%2C%20with%20BEq%2B%4010%20improvements%20of%2037.14%25%20and%0A42.25%25%20using%20GPT-4.1%20and%20DeepSeek-V3.1%2C%20respectively.%20Our%20analysis%20shows%20that%0Aretrieval%20effectiveness%20in%20mathematical%20autoformalization%20depends%20heavily%20on%0Amodel-specific%20knowledge%20boundaries%2C%20highlighting%20the%20need%20for%20adaptive%0Aretrieval%20strategies%20aligned%20with%20each%20model%27s%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.10815v3&entry.124074799=Read"},
{"title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control\n  for Advanced Role-Playing LLMs", "author": "Xilong Cheng and Yunxiao Qin and Yuting Tan and Zhengnan Li and Ye Wang and Hongjiang Xiao and Yuan Zhang", "abstract": "  Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.\n", "link": "http://arxiv.org/abs/2505.12814v2", "date": "2025-10-20", "relevancy": 1.9609, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4916}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PsyMem%3A%20Fine-grained%20psychological%20alignment%20and%20Explicit%20Memory%20Control%0A%20%20for%20Advanced%20Role-Playing%20LLMs&body=Title%3A%20PsyMem%3A%20Fine-grained%20psychological%20alignment%20and%20Explicit%20Memory%20Control%0A%20%20for%20Advanced%20Role-Playing%20LLMs%0AAuthor%3A%20Xilong%20Cheng%20and%20Yunxiao%20Qin%20and%20Yuting%20Tan%20and%20Zhengnan%20Li%20and%20Ye%20Wang%20and%20Hongjiang%20Xiao%20and%20Yuan%20Zhang%0AAbstract%3A%20%20%20Existing%20LLM-based%20role-playing%20methods%20often%20rely%20on%20superficial%20textual%0Adescriptions%20or%20simplistic%20metrics%2C%20inadequately%20modeling%20both%20intrinsic%20and%0Aextrinsic%20character%20dimensions.%20Additionally%2C%20they%20typically%20simulate%20character%0Amemory%20with%20implicit%20model%20knowledge%20or%20basic%20retrieval%20augment%20generation%0Awithout%20explicit%20memory%20alignment%2C%20compromising%20memory%20consistency.%20The%20two%0Aissues%20weaken%20reliability%20of%20role-playing%20LLMs%20in%20several%20applications%2C%20such%20as%0Atrustworthy%20social%20simulation.%20To%20address%20these%20limitations%2C%20we%20propose%20PsyMem%2C%0Aa%20novel%20framework%20integrating%20fine-grained%20psychological%20attributes%20and%0Aexplicit%20memory%20control%20for%20role-playing.%20PsyMem%20supplements%20textual%0Adescriptions%20with%2026%20psychological%20indicators%20to%20detailed%20model%20character.%0AAdditionally%2C%20PsyMem%20implements%20memory%20alignment%20training%2C%20explicitly%20trains%0Athe%20model%20to%20align%20character%27s%20response%20with%20memory%2C%20thereby%20enabling%20dynamic%0Amemory-controlled%20responding%20during%20inference.%20By%20training%20Qwen2.5-7B-Instruct%0Aon%20our%20specially%20designed%20dataset%20%28including%205%2C414%20characters%20and%2038%2C962%0Adialogues%20extracted%20from%20novels%29%2C%20the%20resulting%20model%2C%20termed%20as%20PsyMem-Qwen%2C%0Aoutperforms%20baseline%20models%20in%20role-playing%2C%20achieving%20the%20best%20performance%20in%0Ahuman-likeness%20and%20character%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPsyMem%253A%2520Fine-grained%2520psychological%2520alignment%2520and%2520Explicit%2520Memory%2520Control%250A%2520%2520for%2520Advanced%2520Role-Playing%2520LLMs%26entry.906535625%3DXilong%2520Cheng%2520and%2520Yunxiao%2520Qin%2520and%2520Yuting%2520Tan%2520and%2520Zhengnan%2520Li%2520and%2520Ye%2520Wang%2520and%2520Hongjiang%2520Xiao%2520and%2520Yuan%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520LLM-based%2520role-playing%2520methods%2520often%2520rely%2520on%2520superficial%2520textual%250Adescriptions%2520or%2520simplistic%2520metrics%252C%2520inadequately%2520modeling%2520both%2520intrinsic%2520and%250Aextrinsic%2520character%2520dimensions.%2520Additionally%252C%2520they%2520typically%2520simulate%2520character%250Amemory%2520with%2520implicit%2520model%2520knowledge%2520or%2520basic%2520retrieval%2520augment%2520generation%250Awithout%2520explicit%2520memory%2520alignment%252C%2520compromising%2520memory%2520consistency.%2520The%2520two%250Aissues%2520weaken%2520reliability%2520of%2520role-playing%2520LLMs%2520in%2520several%2520applications%252C%2520such%2520as%250Atrustworthy%2520social%2520simulation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520PsyMem%252C%250Aa%2520novel%2520framework%2520integrating%2520fine-grained%2520psychological%2520attributes%2520and%250Aexplicit%2520memory%2520control%2520for%2520role-playing.%2520PsyMem%2520supplements%2520textual%250Adescriptions%2520with%252026%2520psychological%2520indicators%2520to%2520detailed%2520model%2520character.%250AAdditionally%252C%2520PsyMem%2520implements%2520memory%2520alignment%2520training%252C%2520explicitly%2520trains%250Athe%2520model%2520to%2520align%2520character%2527s%2520response%2520with%2520memory%252C%2520thereby%2520enabling%2520dynamic%250Amemory-controlled%2520responding%2520during%2520inference.%2520By%2520training%2520Qwen2.5-7B-Instruct%250Aon%2520our%2520specially%2520designed%2520dataset%2520%2528including%25205%252C414%2520characters%2520and%252038%252C962%250Adialogues%2520extracted%2520from%2520novels%2529%252C%2520the%2520resulting%2520model%252C%2520termed%2520as%2520PsyMem-Qwen%252C%250Aoutperforms%2520baseline%2520models%2520in%2520role-playing%252C%2520achieving%2520the%2520best%2520performance%2520in%250Ahuman-likeness%2520and%2520character%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PsyMem%3A%20Fine-grained%20psychological%20alignment%20and%20Explicit%20Memory%20Control%0A%20%20for%20Advanced%20Role-Playing%20LLMs&entry.906535625=Xilong%20Cheng%20and%20Yunxiao%20Qin%20and%20Yuting%20Tan%20and%20Zhengnan%20Li%20and%20Ye%20Wang%20and%20Hongjiang%20Xiao%20and%20Yuan%20Zhang&entry.1292438233=%20%20Existing%20LLM-based%20role-playing%20methods%20often%20rely%20on%20superficial%20textual%0Adescriptions%20or%20simplistic%20metrics%2C%20inadequately%20modeling%20both%20intrinsic%20and%0Aextrinsic%20character%20dimensions.%20Additionally%2C%20they%20typically%20simulate%20character%0Amemory%20with%20implicit%20model%20knowledge%20or%20basic%20retrieval%20augment%20generation%0Awithout%20explicit%20memory%20alignment%2C%20compromising%20memory%20consistency.%20The%20two%0Aissues%20weaken%20reliability%20of%20role-playing%20LLMs%20in%20several%20applications%2C%20such%20as%0Atrustworthy%20social%20simulation.%20To%20address%20these%20limitations%2C%20we%20propose%20PsyMem%2C%0Aa%20novel%20framework%20integrating%20fine-grained%20psychological%20attributes%20and%0Aexplicit%20memory%20control%20for%20role-playing.%20PsyMem%20supplements%20textual%0Adescriptions%20with%2026%20psychological%20indicators%20to%20detailed%20model%20character.%0AAdditionally%2C%20PsyMem%20implements%20memory%20alignment%20training%2C%20explicitly%20trains%0Athe%20model%20to%20align%20character%27s%20response%20with%20memory%2C%20thereby%20enabling%20dynamic%0Amemory-controlled%20responding%20during%20inference.%20By%20training%20Qwen2.5-7B-Instruct%0Aon%20our%20specially%20designed%20dataset%20%28including%205%2C414%20characters%20and%2038%2C962%0Adialogues%20extracted%20from%20novels%29%2C%20the%20resulting%20model%2C%20termed%20as%20PsyMem-Qwen%2C%0Aoutperforms%20baseline%20models%20in%20role-playing%2C%20achieving%20the%20best%20performance%20in%0Ahuman-likeness%20and%20character%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12814v2&entry.124074799=Read"},
{"title": "Auto-Prompt Generation is Not Robust: Prompt Optimization Driven by\n  Pseudo Gradient", "author": "Zeru Shi and Zhenting Wang and Yongye Su and Weidi Luo and Hang Gao and Fan Yang and Ruixiang Tang and Yongfeng Zhang", "abstract": "  While automatic prompt generation methods have recently received significant\nattention, their robustness remains poorly understood. In this paper, we\nintroduce PertBench, a comprehensive benchmark dataset that includes a wide\nrange of input perturbations, designed to systematically evaluate the\nrobustness of current auto-prompting techniques. Our analysis reveals\nsubstantial vulnerabilities in existing prompt generation strategies, where\neven minor modifications to the prompt can lead to significant differences in\nmodel output. To address this issue, we propose PGO, a gradient-free prompt\ngeneration framework that leverages perturbation types as pseudo-gradient\nsignals to guide LLMs in producing more robust prompts. In contrast to existing\nmethods that assess prompt quality only on clean, well-structured inputs, our\napproach explicitly emphasizes robustness under noisy and perturbed conditions.\nExtensive experiments across diverse tasks and multiple LLMs show PGO\nconsistently outperforms previous methods in maintaining performance under\ninput perturbations.\n", "link": "http://arxiv.org/abs/2412.18196v3", "date": "2025-10-20", "relevancy": 1.9468, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4965}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4959}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Prompt%20Generation%20is%20Not%20Robust%3A%20Prompt%20Optimization%20Driven%20by%0A%20%20Pseudo%20Gradient&body=Title%3A%20Auto-Prompt%20Generation%20is%20Not%20Robust%3A%20Prompt%20Optimization%20Driven%20by%0A%20%20Pseudo%20Gradient%0AAuthor%3A%20Zeru%20Shi%20and%20Zhenting%20Wang%20and%20Yongye%20Su%20and%20Weidi%20Luo%20and%20Hang%20Gao%20and%20Fan%20Yang%20and%20Ruixiang%20Tang%20and%20Yongfeng%20Zhang%0AAbstract%3A%20%20%20While%20automatic%20prompt%20generation%20methods%20have%20recently%20received%20significant%0Aattention%2C%20their%20robustness%20remains%20poorly%20understood.%20In%20this%20paper%2C%20we%0Aintroduce%20PertBench%2C%20a%20comprehensive%20benchmark%20dataset%20that%20includes%20a%20wide%0Arange%20of%20input%20perturbations%2C%20designed%20to%20systematically%20evaluate%20the%0Arobustness%20of%20current%20auto-prompting%20techniques.%20Our%20analysis%20reveals%0Asubstantial%20vulnerabilities%20in%20existing%20prompt%20generation%20strategies%2C%20where%0Aeven%20minor%20modifications%20to%20the%20prompt%20can%20lead%20to%20significant%20differences%20in%0Amodel%20output.%20To%20address%20this%20issue%2C%20we%20propose%20PGO%2C%20a%20gradient-free%20prompt%0Ageneration%20framework%20that%20leverages%20perturbation%20types%20as%20pseudo-gradient%0Asignals%20to%20guide%20LLMs%20in%20producing%20more%20robust%20prompts.%20In%20contrast%20to%20existing%0Amethods%20that%20assess%20prompt%20quality%20only%20on%20clean%2C%20well-structured%20inputs%2C%20our%0Aapproach%20explicitly%20emphasizes%20robustness%20under%20noisy%20and%20perturbed%20conditions.%0AExtensive%20experiments%20across%20diverse%20tasks%20and%20multiple%20LLMs%20show%20PGO%0Aconsistently%20outperforms%20previous%20methods%20in%20maintaining%20performance%20under%0Ainput%20perturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Prompt%2520Generation%2520is%2520Not%2520Robust%253A%2520Prompt%2520Optimization%2520Driven%2520by%250A%2520%2520Pseudo%2520Gradient%26entry.906535625%3DZeru%2520Shi%2520and%2520Zhenting%2520Wang%2520and%2520Yongye%2520Su%2520and%2520Weidi%2520Luo%2520and%2520Hang%2520Gao%2520and%2520Fan%2520Yang%2520and%2520Ruixiang%2520Tang%2520and%2520Yongfeng%2520Zhang%26entry.1292438233%3D%2520%2520While%2520automatic%2520prompt%2520generation%2520methods%2520have%2520recently%2520received%2520significant%250Aattention%252C%2520their%2520robustness%2520remains%2520poorly%2520understood.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520PertBench%252C%2520a%2520comprehensive%2520benchmark%2520dataset%2520that%2520includes%2520a%2520wide%250Arange%2520of%2520input%2520perturbations%252C%2520designed%2520to%2520systematically%2520evaluate%2520the%250Arobustness%2520of%2520current%2520auto-prompting%2520techniques.%2520Our%2520analysis%2520reveals%250Asubstantial%2520vulnerabilities%2520in%2520existing%2520prompt%2520generation%2520strategies%252C%2520where%250Aeven%2520minor%2520modifications%2520to%2520the%2520prompt%2520can%2520lead%2520to%2520significant%2520differences%2520in%250Amodel%2520output.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520PGO%252C%2520a%2520gradient-free%2520prompt%250Ageneration%2520framework%2520that%2520leverages%2520perturbation%2520types%2520as%2520pseudo-gradient%250Asignals%2520to%2520guide%2520LLMs%2520in%2520producing%2520more%2520robust%2520prompts.%2520In%2520contrast%2520to%2520existing%250Amethods%2520that%2520assess%2520prompt%2520quality%2520only%2520on%2520clean%252C%2520well-structured%2520inputs%252C%2520our%250Aapproach%2520explicitly%2520emphasizes%2520robustness%2520under%2520noisy%2520and%2520perturbed%2520conditions.%250AExtensive%2520experiments%2520across%2520diverse%2520tasks%2520and%2520multiple%2520LLMs%2520show%2520PGO%250Aconsistently%2520outperforms%2520previous%2520methods%2520in%2520maintaining%2520performance%2520under%250Ainput%2520perturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Prompt%20Generation%20is%20Not%20Robust%3A%20Prompt%20Optimization%20Driven%20by%0A%20%20Pseudo%20Gradient&entry.906535625=Zeru%20Shi%20and%20Zhenting%20Wang%20and%20Yongye%20Su%20and%20Weidi%20Luo%20and%20Hang%20Gao%20and%20Fan%20Yang%20and%20Ruixiang%20Tang%20and%20Yongfeng%20Zhang&entry.1292438233=%20%20While%20automatic%20prompt%20generation%20methods%20have%20recently%20received%20significant%0Aattention%2C%20their%20robustness%20remains%20poorly%20understood.%20In%20this%20paper%2C%20we%0Aintroduce%20PertBench%2C%20a%20comprehensive%20benchmark%20dataset%20that%20includes%20a%20wide%0Arange%20of%20input%20perturbations%2C%20designed%20to%20systematically%20evaluate%20the%0Arobustness%20of%20current%20auto-prompting%20techniques.%20Our%20analysis%20reveals%0Asubstantial%20vulnerabilities%20in%20existing%20prompt%20generation%20strategies%2C%20where%0Aeven%20minor%20modifications%20to%20the%20prompt%20can%20lead%20to%20significant%20differences%20in%0Amodel%20output.%20To%20address%20this%20issue%2C%20we%20propose%20PGO%2C%20a%20gradient-free%20prompt%0Ageneration%20framework%20that%20leverages%20perturbation%20types%20as%20pseudo-gradient%0Asignals%20to%20guide%20LLMs%20in%20producing%20more%20robust%20prompts.%20In%20contrast%20to%20existing%0Amethods%20that%20assess%20prompt%20quality%20only%20on%20clean%2C%20well-structured%20inputs%2C%20our%0Aapproach%20explicitly%20emphasizes%20robustness%20under%20noisy%20and%20perturbed%20conditions.%0AExtensive%20experiments%20across%20diverse%20tasks%20and%20multiple%20LLMs%20show%20PGO%0Aconsistently%20outperforms%20previous%20methods%20in%20maintaining%20performance%20under%0Ainput%20perturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18196v3&entry.124074799=Read"},
{"title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and\n  Reward-Trend Guided Adaptive Exploration", "author": "Praphul Singh and Corey Barrett and Sumana Srivasta and Irfan Bulu and Sri Gadde and Krishnaram Kenthapadi", "abstract": "  Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.\n", "link": "http://arxiv.org/abs/2510.17614v1", "date": "2025-10-20", "relevancy": 1.9361, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5146}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OG-Rank%3A%20Learning%20to%20Rank%20Fast%20and%20Slow%20with%20Uncertainty%20and%0A%20%20Reward-Trend%20Guided%20Adaptive%20Exploration&body=Title%3A%20OG-Rank%3A%20Learning%20to%20Rank%20Fast%20and%20Slow%20with%20Uncertainty%20and%0A%20%20Reward-Trend%20Guided%20Adaptive%20Exploration%0AAuthor%3A%20Praphul%20Singh%20and%20Corey%20Barrett%20and%20Sumana%20Srivasta%20and%20Irfan%20Bulu%20and%20Sri%20Gadde%20and%20Krishnaram%20Kenthapadi%0AAbstract%3A%20%20%20Clinicians%20need%20ranking%20systems%20that%20work%20in%20real%20time%20and%20still%20justify%0Atheir%20choices.%20Motivated%20by%20the%20need%20for%20a%20low-latency%2C%20decoder-based%20reranker%2C%0Awe%20present%20OG-Rank%2C%20a%20single-decoder%20approach%20that%20pairs%20a%20pooled%20first-token%0Ascoring%20signal%20with%20an%20uncertainty-gated%20explanation%20step.%20The%20model%20scores%20all%0Acandidates%20in%20one%20pass%20and%20generates%20a%20brief%2C%20structured%20rationale%20only%20when%0Athe%20list%20is%20genuinely%20ambiguous%2C%20keeping%20latency%20predictable.%20Trained%20with%20a%0Acurriculum%20that%20concentrates%20effort%20on%20hard%20cases%2C%20OG-Rank%20delivers%20strong%0Aeffectiveness%20on%20encounter-scoped%20order%20selection%20%28fast%20path%3A%20Recall%401~0.45%2C%0AnDCG%4020~0.625%29%20and%20improves%20further%20when%20the%20gate%20activates%20%28Recall%401~0.56%2C%0AnDCG%4020~0.699%20at%20a%2045%5C%25%20gate%20rate%29%2C%20while%20compact%20backbones%20show%20similar%20gains%0Aunder%20the%20same%20policy.%20Encoder%20baselines%20trail%20in%20both%20effectiveness%20and%0Aflexibility.%20The%20result%20is%20a%20practical%20recipe%3A%20rank%20fast%20by%20default%20and%20explain%0Awhen%20it%20helps%2C%20a%20pattern%20that%20applies%20broadly%20to%20decision%20tasks%20where%20selective%0Ageneration%20buys%20accuracy%20at%20acceptable%20cost.%20The%20single-policy%20design%0Asimplifies%20deployment%20and%20budget%20planning%2C%20and%20the%20curriculum%20principle%20%28spend%0Amore%20on%20the%20hard%20cases%2C%20less%20on%20the%20easy%20ones%29%20readily%20transfers%20beyond%0Aclinical%20order%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOG-Rank%253A%2520Learning%2520to%2520Rank%2520Fast%2520and%2520Slow%2520with%2520Uncertainty%2520and%250A%2520%2520Reward-Trend%2520Guided%2520Adaptive%2520Exploration%26entry.906535625%3DPraphul%2520Singh%2520and%2520Corey%2520Barrett%2520and%2520Sumana%2520Srivasta%2520and%2520Irfan%2520Bulu%2520and%2520Sri%2520Gadde%2520and%2520Krishnaram%2520Kenthapadi%26entry.1292438233%3D%2520%2520Clinicians%2520need%2520ranking%2520systems%2520that%2520work%2520in%2520real%2520time%2520and%2520still%2520justify%250Atheir%2520choices.%2520Motivated%2520by%2520the%2520need%2520for%2520a%2520low-latency%252C%2520decoder-based%2520reranker%252C%250Awe%2520present%2520OG-Rank%252C%2520a%2520single-decoder%2520approach%2520that%2520pairs%2520a%2520pooled%2520first-token%250Ascoring%2520signal%2520with%2520an%2520uncertainty-gated%2520explanation%2520step.%2520The%2520model%2520scores%2520all%250Acandidates%2520in%2520one%2520pass%2520and%2520generates%2520a%2520brief%252C%2520structured%2520rationale%2520only%2520when%250Athe%2520list%2520is%2520genuinely%2520ambiguous%252C%2520keeping%2520latency%2520predictable.%2520Trained%2520with%2520a%250Acurriculum%2520that%2520concentrates%2520effort%2520on%2520hard%2520cases%252C%2520OG-Rank%2520delivers%2520strong%250Aeffectiveness%2520on%2520encounter-scoped%2520order%2520selection%2520%2528fast%2520path%253A%2520Recall%25401~0.45%252C%250AnDCG%254020~0.625%2529%2520and%2520improves%2520further%2520when%2520the%2520gate%2520activates%2520%2528Recall%25401~0.56%252C%250AnDCG%254020~0.699%2520at%2520a%252045%255C%2525%2520gate%2520rate%2529%252C%2520while%2520compact%2520backbones%2520show%2520similar%2520gains%250Aunder%2520the%2520same%2520policy.%2520Encoder%2520baselines%2520trail%2520in%2520both%2520effectiveness%2520and%250Aflexibility.%2520The%2520result%2520is%2520a%2520practical%2520recipe%253A%2520rank%2520fast%2520by%2520default%2520and%2520explain%250Awhen%2520it%2520helps%252C%2520a%2520pattern%2520that%2520applies%2520broadly%2520to%2520decision%2520tasks%2520where%2520selective%250Ageneration%2520buys%2520accuracy%2520at%2520acceptable%2520cost.%2520The%2520single-policy%2520design%250Asimplifies%2520deployment%2520and%2520budget%2520planning%252C%2520and%2520the%2520curriculum%2520principle%2520%2528spend%250Amore%2520on%2520the%2520hard%2520cases%252C%2520less%2520on%2520the%2520easy%2520ones%2529%2520readily%2520transfers%2520beyond%250Aclinical%2520order%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OG-Rank%3A%20Learning%20to%20Rank%20Fast%20and%20Slow%20with%20Uncertainty%20and%0A%20%20Reward-Trend%20Guided%20Adaptive%20Exploration&entry.906535625=Praphul%20Singh%20and%20Corey%20Barrett%20and%20Sumana%20Srivasta%20and%20Irfan%20Bulu%20and%20Sri%20Gadde%20and%20Krishnaram%20Kenthapadi&entry.1292438233=%20%20Clinicians%20need%20ranking%20systems%20that%20work%20in%20real%20time%20and%20still%20justify%0Atheir%20choices.%20Motivated%20by%20the%20need%20for%20a%20low-latency%2C%20decoder-based%20reranker%2C%0Awe%20present%20OG-Rank%2C%20a%20single-decoder%20approach%20that%20pairs%20a%20pooled%20first-token%0Ascoring%20signal%20with%20an%20uncertainty-gated%20explanation%20step.%20The%20model%20scores%20all%0Acandidates%20in%20one%20pass%20and%20generates%20a%20brief%2C%20structured%20rationale%20only%20when%0Athe%20list%20is%20genuinely%20ambiguous%2C%20keeping%20latency%20predictable.%20Trained%20with%20a%0Acurriculum%20that%20concentrates%20effort%20on%20hard%20cases%2C%20OG-Rank%20delivers%20strong%0Aeffectiveness%20on%20encounter-scoped%20order%20selection%20%28fast%20path%3A%20Recall%401~0.45%2C%0AnDCG%4020~0.625%29%20and%20improves%20further%20when%20the%20gate%20activates%20%28Recall%401~0.56%2C%0AnDCG%4020~0.699%20at%20a%2045%5C%25%20gate%20rate%29%2C%20while%20compact%20backbones%20show%20similar%20gains%0Aunder%20the%20same%20policy.%20Encoder%20baselines%20trail%20in%20both%20effectiveness%20and%0Aflexibility.%20The%20result%20is%20a%20practical%20recipe%3A%20rank%20fast%20by%20default%20and%20explain%0Awhen%20it%20helps%2C%20a%20pattern%20that%20applies%20broadly%20to%20decision%20tasks%20where%20selective%0Ageneration%20buys%20accuracy%20at%20acceptable%20cost.%20The%20single-policy%20design%0Asimplifies%20deployment%20and%20budget%20planning%2C%20and%20the%20curriculum%20principle%20%28spend%0Amore%20on%20the%20hard%20cases%2C%20less%20on%20the%20easy%20ones%29%20readily%20transfers%20beyond%0Aclinical%20order%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17614v1&entry.124074799=Read"},
{"title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for\n  Suicide Prediction", "author": "Vaishnavi Visweswaraiah and Tanvi Banerjee and William Romine", "abstract": "  Suicide prediction is the key for prevention, but real data with sufficient\npositive samples is rare and causes extreme class imbalance. We utilized\nmachine learning (ML) to build the model and deep learning (DL) techniques,\nlike Generative Adversarial Networks (GAN), to generate synthetic data samples\nto enhance the dataset. The initial dataset contained 656 samples, with only\nfour positive cases, prompting the need for data augmentation. A variety of\nmachine learning models, ranging from interpretable data models to black box\nalgorithmic models, were used. On real test data, Logistic Regression (LR)\nachieved a weighted precision of 0.99, a weighted recall of 0.85, and a\nweighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,\nrespectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.\nLR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and\nmisclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &\n0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)\nwith 0 false positives (specificity: 1.0). These results highlight the models'\neffectiveness, with GAN playing a key role in generating synthetic data to\nsupport suicide prevention modeling efforts.\n", "link": "http://arxiv.org/abs/2510.17661v1", "date": "2025-10-20", "relevancy": 1.9347, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4973}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4761}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handling%20Extreme%20Class%20Imbalance%3A%20Using%20GANs%20in%20Data%20Augmentation%20for%0A%20%20Suicide%20Prediction&body=Title%3A%20Handling%20Extreme%20Class%20Imbalance%3A%20Using%20GANs%20in%20Data%20Augmentation%20for%0A%20%20Suicide%20Prediction%0AAuthor%3A%20Vaishnavi%20Visweswaraiah%20and%20Tanvi%20Banerjee%20and%20William%20Romine%0AAbstract%3A%20%20%20Suicide%20prediction%20is%20the%20key%20for%20prevention%2C%20but%20real%20data%20with%20sufficient%0Apositive%20samples%20is%20rare%20and%20causes%20extreme%20class%20imbalance.%20We%20utilized%0Amachine%20learning%20%28ML%29%20to%20build%20the%20model%20and%20deep%20learning%20%28DL%29%20techniques%2C%0Alike%20Generative%20Adversarial%20Networks%20%28GAN%29%2C%20to%20generate%20synthetic%20data%20samples%0Ato%20enhance%20the%20dataset.%20The%20initial%20dataset%20contained%20656%20samples%2C%20with%20only%0Afour%20positive%20cases%2C%20prompting%20the%20need%20for%20data%20augmentation.%20A%20variety%20of%0Amachine%20learning%20models%2C%20ranging%20from%20interpretable%20data%20models%20to%20black%20box%0Aalgorithmic%20models%2C%20were%20used.%20On%20real%20test%20data%2C%20Logistic%20Regression%20%28LR%29%0Aachieved%20a%20weighted%20precision%20of%200.99%2C%20a%20weighted%20recall%20of%200.85%2C%20and%20a%0Aweighted%20F1%20score%20of%200.91%3B%20Random%20Forest%20%28RF%29%20showed%200.98%2C%200.99%2C%20and%200.99%2C%0Arespectively%3B%20and%20Support%20Vector%20Machine%20%28SVM%29%20achieved%200.99%2C%200.76%2C%20and%200.86.%0ALR%20and%20SVM%20correctly%20identified%20one%20suicide%20attempt%20case%20%28sensitivity%3A1.0%29%20and%0Amisclassified%20LR%2820%29%20and%20SVM%20%2831%29%20non-attempts%20as%20attempts%20%28specificity%3A%200.85%20%26%0A0.76%2C%20respectively%29.%20RF%20identified%200%20suicide%20attempt%20cases%20%28sensitivity%3A%200.0%29%0Awith%200%20false%20positives%20%28specificity%3A%201.0%29.%20These%20results%20highlight%20the%20models%27%0Aeffectiveness%2C%20with%20GAN%20playing%20a%20key%20role%20in%20generating%20synthetic%20data%20to%0Asupport%20suicide%20prevention%20modeling%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandling%2520Extreme%2520Class%2520Imbalance%253A%2520Using%2520GANs%2520in%2520Data%2520Augmentation%2520for%250A%2520%2520Suicide%2520Prediction%26entry.906535625%3DVaishnavi%2520Visweswaraiah%2520and%2520Tanvi%2520Banerjee%2520and%2520William%2520Romine%26entry.1292438233%3D%2520%2520Suicide%2520prediction%2520is%2520the%2520key%2520for%2520prevention%252C%2520but%2520real%2520data%2520with%2520sufficient%250Apositive%2520samples%2520is%2520rare%2520and%2520causes%2520extreme%2520class%2520imbalance.%2520We%2520utilized%250Amachine%2520learning%2520%2528ML%2529%2520to%2520build%2520the%2520model%2520and%2520deep%2520learning%2520%2528DL%2529%2520techniques%252C%250Alike%2520Generative%2520Adversarial%2520Networks%2520%2528GAN%2529%252C%2520to%2520generate%2520synthetic%2520data%2520samples%250Ato%2520enhance%2520the%2520dataset.%2520The%2520initial%2520dataset%2520contained%2520656%2520samples%252C%2520with%2520only%250Afour%2520positive%2520cases%252C%2520prompting%2520the%2520need%2520for%2520data%2520augmentation.%2520A%2520variety%2520of%250Amachine%2520learning%2520models%252C%2520ranging%2520from%2520interpretable%2520data%2520models%2520to%2520black%2520box%250Aalgorithmic%2520models%252C%2520were%2520used.%2520On%2520real%2520test%2520data%252C%2520Logistic%2520Regression%2520%2528LR%2529%250Aachieved%2520a%2520weighted%2520precision%2520of%25200.99%252C%2520a%2520weighted%2520recall%2520of%25200.85%252C%2520and%2520a%250Aweighted%2520F1%2520score%2520of%25200.91%253B%2520Random%2520Forest%2520%2528RF%2529%2520showed%25200.98%252C%25200.99%252C%2520and%25200.99%252C%250Arespectively%253B%2520and%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%2520achieved%25200.99%252C%25200.76%252C%2520and%25200.86.%250ALR%2520and%2520SVM%2520correctly%2520identified%2520one%2520suicide%2520attempt%2520case%2520%2528sensitivity%253A1.0%2529%2520and%250Amisclassified%2520LR%252820%2529%2520and%2520SVM%2520%252831%2529%2520non-attempts%2520as%2520attempts%2520%2528specificity%253A%25200.85%2520%2526%250A0.76%252C%2520respectively%2529.%2520RF%2520identified%25200%2520suicide%2520attempt%2520cases%2520%2528sensitivity%253A%25200.0%2529%250Awith%25200%2520false%2520positives%2520%2528specificity%253A%25201.0%2529.%2520These%2520results%2520highlight%2520the%2520models%2527%250Aeffectiveness%252C%2520with%2520GAN%2520playing%2520a%2520key%2520role%2520in%2520generating%2520synthetic%2520data%2520to%250Asupport%2520suicide%2520prevention%2520modeling%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handling%20Extreme%20Class%20Imbalance%3A%20Using%20GANs%20in%20Data%20Augmentation%20for%0A%20%20Suicide%20Prediction&entry.906535625=Vaishnavi%20Visweswaraiah%20and%20Tanvi%20Banerjee%20and%20William%20Romine&entry.1292438233=%20%20Suicide%20prediction%20is%20the%20key%20for%20prevention%2C%20but%20real%20data%20with%20sufficient%0Apositive%20samples%20is%20rare%20and%20causes%20extreme%20class%20imbalance.%20We%20utilized%0Amachine%20learning%20%28ML%29%20to%20build%20the%20model%20and%20deep%20learning%20%28DL%29%20techniques%2C%0Alike%20Generative%20Adversarial%20Networks%20%28GAN%29%2C%20to%20generate%20synthetic%20data%20samples%0Ato%20enhance%20the%20dataset.%20The%20initial%20dataset%20contained%20656%20samples%2C%20with%20only%0Afour%20positive%20cases%2C%20prompting%20the%20need%20for%20data%20augmentation.%20A%20variety%20of%0Amachine%20learning%20models%2C%20ranging%20from%20interpretable%20data%20models%20to%20black%20box%0Aalgorithmic%20models%2C%20were%20used.%20On%20real%20test%20data%2C%20Logistic%20Regression%20%28LR%29%0Aachieved%20a%20weighted%20precision%20of%200.99%2C%20a%20weighted%20recall%20of%200.85%2C%20and%20a%0Aweighted%20F1%20score%20of%200.91%3B%20Random%20Forest%20%28RF%29%20showed%200.98%2C%200.99%2C%20and%200.99%2C%0Arespectively%3B%20and%20Support%20Vector%20Machine%20%28SVM%29%20achieved%200.99%2C%200.76%2C%20and%200.86.%0ALR%20and%20SVM%20correctly%20identified%20one%20suicide%20attempt%20case%20%28sensitivity%3A1.0%29%20and%0Amisclassified%20LR%2820%29%20and%20SVM%20%2831%29%20non-attempts%20as%20attempts%20%28specificity%3A%200.85%20%26%0A0.76%2C%20respectively%29.%20RF%20identified%200%20suicide%20attempt%20cases%20%28sensitivity%3A%200.0%29%0Awith%200%20false%20positives%20%28specificity%3A%201.0%29.%20These%20results%20highlight%20the%20models%27%0Aeffectiveness%2C%20with%20GAN%20playing%20a%20key%20role%20in%20generating%20synthetic%20data%20to%0Asupport%20suicide%20prevention%20modeling%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17661v1&entry.124074799=Read"},
{"title": "PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large\n  Language Models", "author": "Min Zeng", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: \\textit{Can LLMs be effectively applied to\npsychological counseling?} To determine whether an LLM can effectively take on\nthe role of a psychological counselor, the first step is to assess whether it\nmeets the qualifications required for such a role, namely the ability to pass\nthe U.S. National Counselor Certification Exam (NCE). This is because, just as\na human counselor must pass a certification exam to practice, an LLM must\ndemonstrate sufficient psychological knowledge to meet the standards required\nfor such a role. To address this, we introduce PsychCounsel-Bench, a benchmark\ngrounded in U.S.national counselor examinations, a licensure test for\nprofessional counselors that requires about 70\\% accuracy to pass.\nPsychCounsel-Bench comprises approximately 2,252 carefully curated\nsingle-choice questions, crafted to require deep understanding and broad enough\nto cover various sub-disciplines of psychology. This benchmark provides a\ncomprehensive assessment of an LLM's ability to function as a counselor. Our\nevaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and\nGemma3-27B achieve well above the passing threshold, while smaller open-source\nmodels (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results\nsuggest that only frontier LLMs are currently capable of meeting counseling\nexam standards, highlighting both the promise and the challenges of developing\npsychology-oriented LLMs. We release the proposed dataset for public use:\nhttps://github.com/cloversjtu/PsychCounsel-Bench\n", "link": "http://arxiv.org/abs/2510.01611v3", "date": "2025-10-20", "relevancy": 1.9242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PsychCounsel-Bench%3A%20Evaluating%20the%20Psychology%20Intelligence%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20PsychCounsel-Bench%3A%20Evaluating%20the%20Psychology%20Intelligence%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Min%20Zeng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20across%20a%0Awide%20range%20of%20industries%2C%20primarily%20due%20to%20their%20impressive%20generative%0Aabilities.%20Yet%2C%20their%20potential%20in%20applications%20requiring%20cognitive%20abilities%2C%0Asuch%20as%20psychological%20counseling%2C%20remains%20largely%20untapped.%20This%20paper%0Ainvestigates%20the%20key%20question%3A%20%5Ctextit%7BCan%20LLMs%20be%20effectively%20applied%20to%0Apsychological%20counseling%3F%7D%20To%20determine%20whether%20an%20LLM%20can%20effectively%20take%20on%0Athe%20role%20of%20a%20psychological%20counselor%2C%20the%20first%20step%20is%20to%20assess%20whether%20it%0Ameets%20the%20qualifications%20required%20for%20such%20a%20role%2C%20namely%20the%20ability%20to%20pass%0Athe%20U.S.%20National%20Counselor%20Certification%20Exam%20%28NCE%29.%20This%20is%20because%2C%20just%20as%0Aa%20human%20counselor%20must%20pass%20a%20certification%20exam%20to%20practice%2C%20an%20LLM%20must%0Ademonstrate%20sufficient%20psychological%20knowledge%20to%20meet%20the%20standards%20required%0Afor%20such%20a%20role.%20To%20address%20this%2C%20we%20introduce%20PsychCounsel-Bench%2C%20a%20benchmark%0Agrounded%20in%20U.S.national%20counselor%20examinations%2C%20a%20licensure%20test%20for%0Aprofessional%20counselors%20that%20requires%20about%2070%5C%25%20accuracy%20to%20pass.%0APsychCounsel-Bench%20comprises%20approximately%202%2C252%20carefully%20curated%0Asingle-choice%20questions%2C%20crafted%20to%20require%20deep%20understanding%20and%20broad%20enough%0Ato%20cover%20various%20sub-disciplines%20of%20psychology.%20This%20benchmark%20provides%20a%0Acomprehensive%20assessment%20of%20an%20LLM%27s%20ability%20to%20function%20as%20a%20counselor.%20Our%0Aevaluation%20shows%20that%20advanced%20models%20such%20as%20GPT-4o%2C%20Llama3.3-70B%2C%20and%0AGemma3-27B%20achieve%20well%20above%20the%20passing%20threshold%2C%20while%20smaller%20open-source%0Amodels%20%28e.g.%2C%20Qwen2.5-7B%2C%20Mistral-7B%29%20remain%20far%20below%20it.%20These%20results%0Asuggest%20that%20only%20frontier%20LLMs%20are%20currently%20capable%20of%20meeting%20counseling%0Aexam%20standards%2C%20highlighting%20both%20the%20promise%20and%20the%20challenges%20of%20developing%0Apsychology-oriented%20LLMs.%20We%20release%20the%20proposed%20dataset%20for%20public%20use%3A%0Ahttps%3A//github.com/cloversjtu/PsychCounsel-Bench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.01611v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPsychCounsel-Bench%253A%2520Evaluating%2520the%2520Psychology%2520Intelligence%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMin%2520Zeng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520across%2520a%250Awide%2520range%2520of%2520industries%252C%2520primarily%2520due%2520to%2520their%2520impressive%2520generative%250Aabilities.%2520Yet%252C%2520their%2520potential%2520in%2520applications%2520requiring%2520cognitive%2520abilities%252C%250Asuch%2520as%2520psychological%2520counseling%252C%2520remains%2520largely%2520untapped.%2520This%2520paper%250Ainvestigates%2520the%2520key%2520question%253A%2520%255Ctextit%257BCan%2520LLMs%2520be%2520effectively%2520applied%2520to%250Apsychological%2520counseling%253F%257D%2520To%2520determine%2520whether%2520an%2520LLM%2520can%2520effectively%2520take%2520on%250Athe%2520role%2520of%2520a%2520psychological%2520counselor%252C%2520the%2520first%2520step%2520is%2520to%2520assess%2520whether%2520it%250Ameets%2520the%2520qualifications%2520required%2520for%2520such%2520a%2520role%252C%2520namely%2520the%2520ability%2520to%2520pass%250Athe%2520U.S.%2520National%2520Counselor%2520Certification%2520Exam%2520%2528NCE%2529.%2520This%2520is%2520because%252C%2520just%2520as%250Aa%2520human%2520counselor%2520must%2520pass%2520a%2520certification%2520exam%2520to%2520practice%252C%2520an%2520LLM%2520must%250Ademonstrate%2520sufficient%2520psychological%2520knowledge%2520to%2520meet%2520the%2520standards%2520required%250Afor%2520such%2520a%2520role.%2520To%2520address%2520this%252C%2520we%2520introduce%2520PsychCounsel-Bench%252C%2520a%2520benchmark%250Agrounded%2520in%2520U.S.national%2520counselor%2520examinations%252C%2520a%2520licensure%2520test%2520for%250Aprofessional%2520counselors%2520that%2520requires%2520about%252070%255C%2525%2520accuracy%2520to%2520pass.%250APsychCounsel-Bench%2520comprises%2520approximately%25202%252C252%2520carefully%2520curated%250Asingle-choice%2520questions%252C%2520crafted%2520to%2520require%2520deep%2520understanding%2520and%2520broad%2520enough%250Ato%2520cover%2520various%2520sub-disciplines%2520of%2520psychology.%2520This%2520benchmark%2520provides%2520a%250Acomprehensive%2520assessment%2520of%2520an%2520LLM%2527s%2520ability%2520to%2520function%2520as%2520a%2520counselor.%2520Our%250Aevaluation%2520shows%2520that%2520advanced%2520models%2520such%2520as%2520GPT-4o%252C%2520Llama3.3-70B%252C%2520and%250AGemma3-27B%2520achieve%2520well%2520above%2520the%2520passing%2520threshold%252C%2520while%2520smaller%2520open-source%250Amodels%2520%2528e.g.%252C%2520Qwen2.5-7B%252C%2520Mistral-7B%2529%2520remain%2520far%2520below%2520it.%2520These%2520results%250Asuggest%2520that%2520only%2520frontier%2520LLMs%2520are%2520currently%2520capable%2520of%2520meeting%2520counseling%250Aexam%2520standards%252C%2520highlighting%2520both%2520the%2520promise%2520and%2520the%2520challenges%2520of%2520developing%250Apsychology-oriented%2520LLMs.%2520We%2520release%2520the%2520proposed%2520dataset%2520for%2520public%2520use%253A%250Ahttps%253A//github.com/cloversjtu/PsychCounsel-Bench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01611v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PsychCounsel-Bench%3A%20Evaluating%20the%20Psychology%20Intelligence%20of%20Large%0A%20%20Language%20Models&entry.906535625=Min%20Zeng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20across%20a%0Awide%20range%20of%20industries%2C%20primarily%20due%20to%20their%20impressive%20generative%0Aabilities.%20Yet%2C%20their%20potential%20in%20applications%20requiring%20cognitive%20abilities%2C%0Asuch%20as%20psychological%20counseling%2C%20remains%20largely%20untapped.%20This%20paper%0Ainvestigates%20the%20key%20question%3A%20%5Ctextit%7BCan%20LLMs%20be%20effectively%20applied%20to%0Apsychological%20counseling%3F%7D%20To%20determine%20whether%20an%20LLM%20can%20effectively%20take%20on%0Athe%20role%20of%20a%20psychological%20counselor%2C%20the%20first%20step%20is%20to%20assess%20whether%20it%0Ameets%20the%20qualifications%20required%20for%20such%20a%20role%2C%20namely%20the%20ability%20to%20pass%0Athe%20U.S.%20National%20Counselor%20Certification%20Exam%20%28NCE%29.%20This%20is%20because%2C%20just%20as%0Aa%20human%20counselor%20must%20pass%20a%20certification%20exam%20to%20practice%2C%20an%20LLM%20must%0Ademonstrate%20sufficient%20psychological%20knowledge%20to%20meet%20the%20standards%20required%0Afor%20such%20a%20role.%20To%20address%20this%2C%20we%20introduce%20PsychCounsel-Bench%2C%20a%20benchmark%0Agrounded%20in%20U.S.national%20counselor%20examinations%2C%20a%20licensure%20test%20for%0Aprofessional%20counselors%20that%20requires%20about%2070%5C%25%20accuracy%20to%20pass.%0APsychCounsel-Bench%20comprises%20approximately%202%2C252%20carefully%20curated%0Asingle-choice%20questions%2C%20crafted%20to%20require%20deep%20understanding%20and%20broad%20enough%0Ato%20cover%20various%20sub-disciplines%20of%20psychology.%20This%20benchmark%20provides%20a%0Acomprehensive%20assessment%20of%20an%20LLM%27s%20ability%20to%20function%20as%20a%20counselor.%20Our%0Aevaluation%20shows%20that%20advanced%20models%20such%20as%20GPT-4o%2C%20Llama3.3-70B%2C%20and%0AGemma3-27B%20achieve%20well%20above%20the%20passing%20threshold%2C%20while%20smaller%20open-source%0Amodels%20%28e.g.%2C%20Qwen2.5-7B%2C%20Mistral-7B%29%20remain%20far%20below%20it.%20These%20results%0Asuggest%20that%20only%20frontier%20LLMs%20are%20currently%20capable%20of%20meeting%20counseling%0Aexam%20standards%2C%20highlighting%20both%20the%20promise%20and%20the%20challenges%20of%20developing%0Apsychology-oriented%20LLMs.%20We%20release%20the%20proposed%20dataset%20for%20public%20use%3A%0Ahttps%3A//github.com/cloversjtu/PsychCounsel-Bench%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.01611v3&entry.124074799=Read"},
{"title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation", "author": "Jane Luo and Xin Zhang and Steven Liu and Jie Wu and Jianfeng Liu and Yiming Huang and Yangyu Huang and Chengyu Yin and Ying Xin and Yuefeng Zhan and Hao Sun and Qi Chen and Scarlett Li and Mao Yang", "abstract": "  Large language models excel at generating individual functions or single\nfiles of code, yet generating complete repositories from scratch remains a\nfundamental challenge. This capability is key to building coherent software\nsystems from high-level specifications and realizing the full potential of\nautomated code generation. The process requires planning at two levels:\ndeciding what features and modules to build (proposal stage) and defining their\nimplementation details (implementation stage). Current approaches rely on\nnatural language planning, which often produces unclear specifications,\nmisaligned components, and brittle designs due to its inherent ambiguity and\nlack of structure. To address these limitations, we introduce the Repository\nPlanning Graph (RPG), a structured representation that encodes capabilities,\nfile structures, data flows, and functions in a unified graph. By replacing\nfree-form natural language with an explicit blueprint, RPG enables consistent\nlong-horizon planning for repository generation. Building on RPG, we develop\nZeroRepo, a graph-driven framework that operates in three stages:\nproposal-level planning, implementation-level construction, and graph-guided\ncode generation with test validation. To evaluate, we construct RepoCraft, a\nbenchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo\nproduces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\\times$\nlarger than the strongest baseline (Claude Code), and 68$\\times$ larger than\nother baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving\nover Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG\nmodels complex dependencies, enables more sophisticated planning through\nnear-linear scaling, and improves agent understanding of repositories, thus\naccelerating localization.\n", "link": "http://arxiv.org/abs/2509.16198v5", "date": "2025-10-20", "relevancy": 1.9201, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5127}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4881}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RPG%3A%20A%20Repository%20Planning%20Graph%20for%20Unified%20and%20Scalable%20Codebase%0A%20%20Generation&body=Title%3A%20RPG%3A%20A%20Repository%20Planning%20Graph%20for%20Unified%20and%20Scalable%20Codebase%0A%20%20Generation%0AAuthor%3A%20Jane%20Luo%20and%20Xin%20Zhang%20and%20Steven%20Liu%20and%20Jie%20Wu%20and%20Jianfeng%20Liu%20and%20Yiming%20Huang%20and%20Yangyu%20Huang%20and%20Chengyu%20Yin%20and%20Ying%20Xin%20and%20Yuefeng%20Zhan%20and%20Hao%20Sun%20and%20Qi%20Chen%20and%20Scarlett%20Li%20and%20Mao%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20excel%20at%20generating%20individual%20functions%20or%20single%0Afiles%20of%20code%2C%20yet%20generating%20complete%20repositories%20from%20scratch%20remains%20a%0Afundamental%20challenge.%20This%20capability%20is%20key%20to%20building%20coherent%20software%0Asystems%20from%20high-level%20specifications%20and%20realizing%20the%20full%20potential%20of%0Aautomated%20code%20generation.%20The%20process%20requires%20planning%20at%20two%20levels%3A%0Adeciding%20what%20features%20and%20modules%20to%20build%20%28proposal%20stage%29%20and%20defining%20their%0Aimplementation%20details%20%28implementation%20stage%29.%20Current%20approaches%20rely%20on%0Anatural%20language%20planning%2C%20which%20often%20produces%20unclear%20specifications%2C%0Amisaligned%20components%2C%20and%20brittle%20designs%20due%20to%20its%20inherent%20ambiguity%20and%0Alack%20of%20structure.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Repository%0APlanning%20Graph%20%28RPG%29%2C%20a%20structured%20representation%20that%20encodes%20capabilities%2C%0Afile%20structures%2C%20data%20flows%2C%20and%20functions%20in%20a%20unified%20graph.%20By%20replacing%0Afree-form%20natural%20language%20with%20an%20explicit%20blueprint%2C%20RPG%20enables%20consistent%0Along-horizon%20planning%20for%20repository%20generation.%20Building%20on%20RPG%2C%20we%20develop%0AZeroRepo%2C%20a%20graph-driven%20framework%20that%20operates%20in%20three%20stages%3A%0Aproposal-level%20planning%2C%20implementation-level%20construction%2C%20and%20graph-guided%0Acode%20generation%20with%20test%20validation.%20To%20evaluate%2C%20we%20construct%20RepoCraft%2C%20a%0Abenchmark%20of%20six%20real-world%20projects%20with%201%2C052%20tasks.%20On%20RepoCraft%2C%20ZeroRepo%0Aproduces%20nearly%2036K%20Code%20Lines%20and%20445K%20Code%20Tokens%2C%20on%20average%203.9%24%5Ctimes%24%0Alarger%20than%20the%20strongest%20baseline%20%28Claude%20Code%29%2C%20and%2068%24%5Ctimes%24%20larger%20than%0Aother%20baselines.%20It%20achieves%2081.5%25%20coverage%20and%2069.7%25%20test%20accuracy%2C%20improving%0Aover%20Claude%20Code%20by%2027.3%20and%2035.8%20points.%20Further%20analysis%20shows%20that%20RPG%0Amodels%20complex%20dependencies%2C%20enables%20more%20sophisticated%20planning%20through%0Anear-linear%20scaling%2C%20and%20improves%20agent%20understanding%20of%20repositories%2C%20thus%0Aaccelerating%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16198v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRPG%253A%2520A%2520Repository%2520Planning%2520Graph%2520for%2520Unified%2520and%2520Scalable%2520Codebase%250A%2520%2520Generation%26entry.906535625%3DJane%2520Luo%2520and%2520Xin%2520Zhang%2520and%2520Steven%2520Liu%2520and%2520Jie%2520Wu%2520and%2520Jianfeng%2520Liu%2520and%2520Yiming%2520Huang%2520and%2520Yangyu%2520Huang%2520and%2520Chengyu%2520Yin%2520and%2520Ying%2520Xin%2520and%2520Yuefeng%2520Zhan%2520and%2520Hao%2520Sun%2520and%2520Qi%2520Chen%2520and%2520Scarlett%2520Li%2520and%2520Mao%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520excel%2520at%2520generating%2520individual%2520functions%2520or%2520single%250Afiles%2520of%2520code%252C%2520yet%2520generating%2520complete%2520repositories%2520from%2520scratch%2520remains%2520a%250Afundamental%2520challenge.%2520This%2520capability%2520is%2520key%2520to%2520building%2520coherent%2520software%250Asystems%2520from%2520high-level%2520specifications%2520and%2520realizing%2520the%2520full%2520potential%2520of%250Aautomated%2520code%2520generation.%2520The%2520process%2520requires%2520planning%2520at%2520two%2520levels%253A%250Adeciding%2520what%2520features%2520and%2520modules%2520to%2520build%2520%2528proposal%2520stage%2529%2520and%2520defining%2520their%250Aimplementation%2520details%2520%2528implementation%2520stage%2529.%2520Current%2520approaches%2520rely%2520on%250Anatural%2520language%2520planning%252C%2520which%2520often%2520produces%2520unclear%2520specifications%252C%250Amisaligned%2520components%252C%2520and%2520brittle%2520designs%2520due%2520to%2520its%2520inherent%2520ambiguity%2520and%250Alack%2520of%2520structure.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Repository%250APlanning%2520Graph%2520%2528RPG%2529%252C%2520a%2520structured%2520representation%2520that%2520encodes%2520capabilities%252C%250Afile%2520structures%252C%2520data%2520flows%252C%2520and%2520functions%2520in%2520a%2520unified%2520graph.%2520By%2520replacing%250Afree-form%2520natural%2520language%2520with%2520an%2520explicit%2520blueprint%252C%2520RPG%2520enables%2520consistent%250Along-horizon%2520planning%2520for%2520repository%2520generation.%2520Building%2520on%2520RPG%252C%2520we%2520develop%250AZeroRepo%252C%2520a%2520graph-driven%2520framework%2520that%2520operates%2520in%2520three%2520stages%253A%250Aproposal-level%2520planning%252C%2520implementation-level%2520construction%252C%2520and%2520graph-guided%250Acode%2520generation%2520with%2520test%2520validation.%2520To%2520evaluate%252C%2520we%2520construct%2520RepoCraft%252C%2520a%250Abenchmark%2520of%2520six%2520real-world%2520projects%2520with%25201%252C052%2520tasks.%2520On%2520RepoCraft%252C%2520ZeroRepo%250Aproduces%2520nearly%252036K%2520Code%2520Lines%2520and%2520445K%2520Code%2520Tokens%252C%2520on%2520average%25203.9%2524%255Ctimes%2524%250Alarger%2520than%2520the%2520strongest%2520baseline%2520%2528Claude%2520Code%2529%252C%2520and%252068%2524%255Ctimes%2524%2520larger%2520than%250Aother%2520baselines.%2520It%2520achieves%252081.5%2525%2520coverage%2520and%252069.7%2525%2520test%2520accuracy%252C%2520improving%250Aover%2520Claude%2520Code%2520by%252027.3%2520and%252035.8%2520points.%2520Further%2520analysis%2520shows%2520that%2520RPG%250Amodels%2520complex%2520dependencies%252C%2520enables%2520more%2520sophisticated%2520planning%2520through%250Anear-linear%2520scaling%252C%2520and%2520improves%2520agent%2520understanding%2520of%2520repositories%252C%2520thus%250Aaccelerating%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16198v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RPG%3A%20A%20Repository%20Planning%20Graph%20for%20Unified%20and%20Scalable%20Codebase%0A%20%20Generation&entry.906535625=Jane%20Luo%20and%20Xin%20Zhang%20and%20Steven%20Liu%20and%20Jie%20Wu%20and%20Jianfeng%20Liu%20and%20Yiming%20Huang%20and%20Yangyu%20Huang%20and%20Chengyu%20Yin%20and%20Ying%20Xin%20and%20Yuefeng%20Zhan%20and%20Hao%20Sun%20and%20Qi%20Chen%20and%20Scarlett%20Li%20and%20Mao%20Yang&entry.1292438233=%20%20Large%20language%20models%20excel%20at%20generating%20individual%20functions%20or%20single%0Afiles%20of%20code%2C%20yet%20generating%20complete%20repositories%20from%20scratch%20remains%20a%0Afundamental%20challenge.%20This%20capability%20is%20key%20to%20building%20coherent%20software%0Asystems%20from%20high-level%20specifications%20and%20realizing%20the%20full%20potential%20of%0Aautomated%20code%20generation.%20The%20process%20requires%20planning%20at%20two%20levels%3A%0Adeciding%20what%20features%20and%20modules%20to%20build%20%28proposal%20stage%29%20and%20defining%20their%0Aimplementation%20details%20%28implementation%20stage%29.%20Current%20approaches%20rely%20on%0Anatural%20language%20planning%2C%20which%20often%20produces%20unclear%20specifications%2C%0Amisaligned%20components%2C%20and%20brittle%20designs%20due%20to%20its%20inherent%20ambiguity%20and%0Alack%20of%20structure.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20Repository%0APlanning%20Graph%20%28RPG%29%2C%20a%20structured%20representation%20that%20encodes%20capabilities%2C%0Afile%20structures%2C%20data%20flows%2C%20and%20functions%20in%20a%20unified%20graph.%20By%20replacing%0Afree-form%20natural%20language%20with%20an%20explicit%20blueprint%2C%20RPG%20enables%20consistent%0Along-horizon%20planning%20for%20repository%20generation.%20Building%20on%20RPG%2C%20we%20develop%0AZeroRepo%2C%20a%20graph-driven%20framework%20that%20operates%20in%20three%20stages%3A%0Aproposal-level%20planning%2C%20implementation-level%20construction%2C%20and%20graph-guided%0Acode%20generation%20with%20test%20validation.%20To%20evaluate%2C%20we%20construct%20RepoCraft%2C%20a%0Abenchmark%20of%20six%20real-world%20projects%20with%201%2C052%20tasks.%20On%20RepoCraft%2C%20ZeroRepo%0Aproduces%20nearly%2036K%20Code%20Lines%20and%20445K%20Code%20Tokens%2C%20on%20average%203.9%24%5Ctimes%24%0Alarger%20than%20the%20strongest%20baseline%20%28Claude%20Code%29%2C%20and%2068%24%5Ctimes%24%20larger%20than%0Aother%20baselines.%20It%20achieves%2081.5%25%20coverage%20and%2069.7%25%20test%20accuracy%2C%20improving%0Aover%20Claude%20Code%20by%2027.3%20and%2035.8%20points.%20Further%20analysis%20shows%20that%20RPG%0Amodels%20complex%20dependencies%2C%20enables%20more%20sophisticated%20planning%20through%0Anear-linear%20scaling%2C%20and%20improves%20agent%20understanding%20of%20repositories%2C%20thus%0Aaccelerating%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16198v5&entry.124074799=Read"},
{"title": "CLIMB: Class-imbalanced Learning Benchmark on Tabular Data", "author": "Zhining Liu and Zihao Li and Ze Yang and Tianxin Wei and Jian Kang and Yada Zhu and Hendrik Hamann and Jingrui He and Hanghang Tong", "abstract": "  Class-imbalanced learning (CIL) on tabular data is important in many\nreal-world applications where the minority class holds the critical but rare\noutcomes. In this paper, we present CLIMB, a comprehensive benchmark for\nclass-imbalanced learning on tabular data. CLIMB includes 73 real-world\ndatasets across diverse domains and imbalance levels, along with unified\nimplementations of 29 representative CIL algorithms. Built on a high-quality\nopen-source Python package with unified API designs, detailed documentation,\nand rigorous code quality controls, CLIMB supports easy implementation and\ncomparison between different CIL algorithms. Through extensive experiments, we\nprovide practical insights on method accuracy and efficiency, highlighting the\nlimitations of naive rebalancing, the effectiveness of ensembles, and the\nimportance of data quality. Our code, documentation, and examples are available\nat https://github.com/ZhiningLiu1998/imbalanced-ensemble.\n", "link": "http://arxiv.org/abs/2505.17451v2", "date": "2025-10-20", "relevancy": 1.9131, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5218}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4641}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIMB%3A%20Class-imbalanced%20Learning%20Benchmark%20on%20Tabular%20Data&body=Title%3A%20CLIMB%3A%20Class-imbalanced%20Learning%20Benchmark%20on%20Tabular%20Data%0AAuthor%3A%20Zhining%20Liu%20and%20Zihao%20Li%20and%20Ze%20Yang%20and%20Tianxin%20Wei%20and%20Jian%20Kang%20and%20Yada%20Zhu%20and%20Hendrik%20Hamann%20and%20Jingrui%20He%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Class-imbalanced%20learning%20%28CIL%29%20on%20tabular%20data%20is%20important%20in%20many%0Areal-world%20applications%20where%20the%20minority%20class%20holds%20the%20critical%20but%20rare%0Aoutcomes.%20In%20this%20paper%2C%20we%20present%20CLIMB%2C%20a%20comprehensive%20benchmark%20for%0Aclass-imbalanced%20learning%20on%20tabular%20data.%20CLIMB%20includes%2073%20real-world%0Adatasets%20across%20diverse%20domains%20and%20imbalance%20levels%2C%20along%20with%20unified%0Aimplementations%20of%2029%20representative%20CIL%20algorithms.%20Built%20on%20a%20high-quality%0Aopen-source%20Python%20package%20with%20unified%20API%20designs%2C%20detailed%20documentation%2C%0Aand%20rigorous%20code%20quality%20controls%2C%20CLIMB%20supports%20easy%20implementation%20and%0Acomparison%20between%20different%20CIL%20algorithms.%20Through%20extensive%20experiments%2C%20we%0Aprovide%20practical%20insights%20on%20method%20accuracy%20and%20efficiency%2C%20highlighting%20the%0Alimitations%20of%20naive%20rebalancing%2C%20the%20effectiveness%20of%20ensembles%2C%20and%20the%0Aimportance%20of%20data%20quality.%20Our%20code%2C%20documentation%2C%20and%20examples%20are%20available%0Aat%20https%3A//github.com/ZhiningLiu1998/imbalanced-ensemble.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIMB%253A%2520Class-imbalanced%2520Learning%2520Benchmark%2520on%2520Tabular%2520Data%26entry.906535625%3DZhining%2520Liu%2520and%2520Zihao%2520Li%2520and%2520Ze%2520Yang%2520and%2520Tianxin%2520Wei%2520and%2520Jian%2520Kang%2520and%2520Yada%2520Zhu%2520and%2520Hendrik%2520Hamann%2520and%2520Jingrui%2520He%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Class-imbalanced%2520learning%2520%2528CIL%2529%2520on%2520tabular%2520data%2520is%2520important%2520in%2520many%250Areal-world%2520applications%2520where%2520the%2520minority%2520class%2520holds%2520the%2520critical%2520but%2520rare%250Aoutcomes.%2520In%2520this%2520paper%252C%2520we%2520present%2520CLIMB%252C%2520a%2520comprehensive%2520benchmark%2520for%250Aclass-imbalanced%2520learning%2520on%2520tabular%2520data.%2520CLIMB%2520includes%252073%2520real-world%250Adatasets%2520across%2520diverse%2520domains%2520and%2520imbalance%2520levels%252C%2520along%2520with%2520unified%250Aimplementations%2520of%252029%2520representative%2520CIL%2520algorithms.%2520Built%2520on%2520a%2520high-quality%250Aopen-source%2520Python%2520package%2520with%2520unified%2520API%2520designs%252C%2520detailed%2520documentation%252C%250Aand%2520rigorous%2520code%2520quality%2520controls%252C%2520CLIMB%2520supports%2520easy%2520implementation%2520and%250Acomparison%2520between%2520different%2520CIL%2520algorithms.%2520Through%2520extensive%2520experiments%252C%2520we%250Aprovide%2520practical%2520insights%2520on%2520method%2520accuracy%2520and%2520efficiency%252C%2520highlighting%2520the%250Alimitations%2520of%2520naive%2520rebalancing%252C%2520the%2520effectiveness%2520of%2520ensembles%252C%2520and%2520the%250Aimportance%2520of%2520data%2520quality.%2520Our%2520code%252C%2520documentation%252C%2520and%2520examples%2520are%2520available%250Aat%2520https%253A//github.com/ZhiningLiu1998/imbalanced-ensemble.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIMB%3A%20Class-imbalanced%20Learning%20Benchmark%20on%20Tabular%20Data&entry.906535625=Zhining%20Liu%20and%20Zihao%20Li%20and%20Ze%20Yang%20and%20Tianxin%20Wei%20and%20Jian%20Kang%20and%20Yada%20Zhu%20and%20Hendrik%20Hamann%20and%20Jingrui%20He%20and%20Hanghang%20Tong&entry.1292438233=%20%20Class-imbalanced%20learning%20%28CIL%29%20on%20tabular%20data%20is%20important%20in%20many%0Areal-world%20applications%20where%20the%20minority%20class%20holds%20the%20critical%20but%20rare%0Aoutcomes.%20In%20this%20paper%2C%20we%20present%20CLIMB%2C%20a%20comprehensive%20benchmark%20for%0Aclass-imbalanced%20learning%20on%20tabular%20data.%20CLIMB%20includes%2073%20real-world%0Adatasets%20across%20diverse%20domains%20and%20imbalance%20levels%2C%20along%20with%20unified%0Aimplementations%20of%2029%20representative%20CIL%20algorithms.%20Built%20on%20a%20high-quality%0Aopen-source%20Python%20package%20with%20unified%20API%20designs%2C%20detailed%20documentation%2C%0Aand%20rigorous%20code%20quality%20controls%2C%20CLIMB%20supports%20easy%20implementation%20and%0Acomparison%20between%20different%20CIL%20algorithms.%20Through%20extensive%20experiments%2C%20we%0Aprovide%20practical%20insights%20on%20method%20accuracy%20and%20efficiency%2C%20highlighting%20the%0Alimitations%20of%20naive%20rebalancing%2C%20the%20effectiveness%20of%20ensembles%2C%20and%20the%0Aimportance%20of%20data%20quality.%20Our%20code%2C%20documentation%2C%20and%20examples%20are%20available%0Aat%20https%3A//github.com/ZhiningLiu1998/imbalanced-ensemble.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17451v2&entry.124074799=Read"},
{"title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection\n  through Chunk-Based Analysis of Hand-Drawn Patterns", "author": "Mhd Adnan Albani and Riad Sonbol", "abstract": "  Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.\n", "link": "http://arxiv.org/abs/2510.17703v1", "date": "2025-10-20", "relevancy": 1.9066, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.477}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Cross-Patient%20Generalization%20in%20Parkinson%27s%20Disease%20Detection%0A%20%20through%20Chunk-Based%20Analysis%20of%20Hand-Drawn%20Patterns&body=Title%3A%20Improving%20Cross-Patient%20Generalization%20in%20Parkinson%27s%20Disease%20Detection%0A%20%20through%20Chunk-Based%20Analysis%20of%20Hand-Drawn%20Patterns%0AAuthor%3A%20Mhd%20Adnan%20Albani%20and%20Riad%20Sonbol%0AAbstract%3A%20%20%20Parkinson%27s%20disease%20%28PD%29%20is%20a%20neurodegenerative%20disease%20affecting%20about%201%25%20of%0Apeople%20over%20the%20age%20of%2060%2C%20causing%20motor%20impairments%20that%20impede%20hand%0Acoordination%20activities%20such%20as%20writing%20and%20drawing.%20Many%20approaches%20have%20tried%0Ato%20support%20early%20detection%20of%20Parkinson%27s%20disease%20based%20on%20hand-drawn%20images%3B%0Ahowever%2C%20we%20identified%20two%20major%20limitations%20in%20the%20related%20works%3A%20%281%29%20the%20lack%0Aof%20sufficient%20datasets%2C%20%282%29%20the%20robustness%20when%20dealing%20with%20unseen%20patient%0Adata.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%20to%20detect%20Parkinson%27s%20disease%0Athat%20consists%20of%20two%20stages%3A%20The%20first%20stage%20classifies%20based%20on%20their%20drawing%0Atype%28circle%2C%20meander%2C%20spiral%29%2C%20and%20the%20second%20stage%20extracts%20the%20required%0Afeatures%20from%20the%20images%20and%20detects%20Parkinson%27s%20disease.%20We%20overcame%20the%0Aprevious%20two%20limitations%20by%20applying%20a%20chunking%20strategy%20where%20we%20divide%20each%0Aimage%20into%202x2%20chunks.%20Each%20chunk%20is%20processed%20separately%20when%20extracting%0Afeatures%20and%20recognizing%20Parkinson%27s%20disease%20indicators.%20To%20make%20the%20final%0Aclassification%2C%20an%20ensemble%20method%20is%20used%20to%20merge%20the%20decisions%20made%20from%0Aeach%20chunk.%20Our%20evaluation%20shows%20that%20our%20proposed%20approach%20outperforms%20the%20top%0Aperforming%20state-of-the-art%20approaches%2C%20in%20particular%20on%20unseen%20patients.%20On%0Athe%20NewHandPD%20dataset%20our%20approach%2C%20it%20achieved%2097.08%25%20accuracy%20for%20seen%0Apatients%20and%2094.91%25%20for%20unseen%20patients%2C%20our%20proposed%20approach%20maintained%20a%20gap%0Aof%20only%202.17%20percentage%20points%2C%20compared%20to%20the%204.76-point%20drop%20observed%20in%0Aprior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Cross-Patient%2520Generalization%2520in%2520Parkinson%2527s%2520Disease%2520Detection%250A%2520%2520through%2520Chunk-Based%2520Analysis%2520of%2520Hand-Drawn%2520Patterns%26entry.906535625%3DMhd%2520Adnan%2520Albani%2520and%2520Riad%2520Sonbol%26entry.1292438233%3D%2520%2520Parkinson%2527s%2520disease%2520%2528PD%2529%2520is%2520a%2520neurodegenerative%2520disease%2520affecting%2520about%25201%2525%2520of%250Apeople%2520over%2520the%2520age%2520of%252060%252C%2520causing%2520motor%2520impairments%2520that%2520impede%2520hand%250Acoordination%2520activities%2520such%2520as%2520writing%2520and%2520drawing.%2520Many%2520approaches%2520have%2520tried%250Ato%2520support%2520early%2520detection%2520of%2520Parkinson%2527s%2520disease%2520based%2520on%2520hand-drawn%2520images%253B%250Ahowever%252C%2520we%2520identified%2520two%2520major%2520limitations%2520in%2520the%2520related%2520works%253A%2520%25281%2529%2520the%2520lack%250Aof%2520sufficient%2520datasets%252C%2520%25282%2529%2520the%2520robustness%2520when%2520dealing%2520with%2520unseen%2520patient%250Adata.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520approach%2520to%2520detect%2520Parkinson%2527s%2520disease%250Athat%2520consists%2520of%2520two%2520stages%253A%2520The%2520first%2520stage%2520classifies%2520based%2520on%2520their%2520drawing%250Atype%2528circle%252C%2520meander%252C%2520spiral%2529%252C%2520and%2520the%2520second%2520stage%2520extracts%2520the%2520required%250Afeatures%2520from%2520the%2520images%2520and%2520detects%2520Parkinson%2527s%2520disease.%2520We%2520overcame%2520the%250Aprevious%2520two%2520limitations%2520by%2520applying%2520a%2520chunking%2520strategy%2520where%2520we%2520divide%2520each%250Aimage%2520into%25202x2%2520chunks.%2520Each%2520chunk%2520is%2520processed%2520separately%2520when%2520extracting%250Afeatures%2520and%2520recognizing%2520Parkinson%2527s%2520disease%2520indicators.%2520To%2520make%2520the%2520final%250Aclassification%252C%2520an%2520ensemble%2520method%2520is%2520used%2520to%2520merge%2520the%2520decisions%2520made%2520from%250Aeach%2520chunk.%2520Our%2520evaluation%2520shows%2520that%2520our%2520proposed%2520approach%2520outperforms%2520the%2520top%250Aperforming%2520state-of-the-art%2520approaches%252C%2520in%2520particular%2520on%2520unseen%2520patients.%2520On%250Athe%2520NewHandPD%2520dataset%2520our%2520approach%252C%2520it%2520achieved%252097.08%2525%2520accuracy%2520for%2520seen%250Apatients%2520and%252094.91%2525%2520for%2520unseen%2520patients%252C%2520our%2520proposed%2520approach%2520maintained%2520a%2520gap%250Aof%2520only%25202.17%2520percentage%2520points%252C%2520compared%2520to%2520the%25204.76-point%2520drop%2520observed%2520in%250Aprior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Cross-Patient%20Generalization%20in%20Parkinson%27s%20Disease%20Detection%0A%20%20through%20Chunk-Based%20Analysis%20of%20Hand-Drawn%20Patterns&entry.906535625=Mhd%20Adnan%20Albani%20and%20Riad%20Sonbol&entry.1292438233=%20%20Parkinson%27s%20disease%20%28PD%29%20is%20a%20neurodegenerative%20disease%20affecting%20about%201%25%20of%0Apeople%20over%20the%20age%20of%2060%2C%20causing%20motor%20impairments%20that%20impede%20hand%0Acoordination%20activities%20such%20as%20writing%20and%20drawing.%20Many%20approaches%20have%20tried%0Ato%20support%20early%20detection%20of%20Parkinson%27s%20disease%20based%20on%20hand-drawn%20images%3B%0Ahowever%2C%20we%20identified%20two%20major%20limitations%20in%20the%20related%20works%3A%20%281%29%20the%20lack%0Aof%20sufficient%20datasets%2C%20%282%29%20the%20robustness%20when%20dealing%20with%20unseen%20patient%0Adata.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%20to%20detect%20Parkinson%27s%20disease%0Athat%20consists%20of%20two%20stages%3A%20The%20first%20stage%20classifies%20based%20on%20their%20drawing%0Atype%28circle%2C%20meander%2C%20spiral%29%2C%20and%20the%20second%20stage%20extracts%20the%20required%0Afeatures%20from%20the%20images%20and%20detects%20Parkinson%27s%20disease.%20We%20overcame%20the%0Aprevious%20two%20limitations%20by%20applying%20a%20chunking%20strategy%20where%20we%20divide%20each%0Aimage%20into%202x2%20chunks.%20Each%20chunk%20is%20processed%20separately%20when%20extracting%0Afeatures%20and%20recognizing%20Parkinson%27s%20disease%20indicators.%20To%20make%20the%20final%0Aclassification%2C%20an%20ensemble%20method%20is%20used%20to%20merge%20the%20decisions%20made%20from%0Aeach%20chunk.%20Our%20evaluation%20shows%20that%20our%20proposed%20approach%20outperforms%20the%20top%0Aperforming%20state-of-the-art%20approaches%2C%20in%20particular%20on%20unseen%20patients.%20On%0Athe%20NewHandPD%20dataset%20our%20approach%2C%20it%20achieved%2097.08%25%20accuracy%20for%20seen%0Apatients%20and%2094.91%25%20for%20unseen%20patients%2C%20our%20proposed%20approach%20maintained%20a%20gap%0Aof%20only%202.17%20percentage%20points%2C%20compared%20to%20the%204.76-point%20drop%20observed%20in%0Aprior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17703v1&entry.124074799=Read"},
{"title": "Parameter Efficient Fine-tuning via Explained Variance Adaptation", "author": "Fabian Paischer and Lukas Hauzenberger and Thomas Schmied and Benedikt Alkin and Marc Peter Deisenroth and Sepp Hochreiter", "abstract": "  Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned for a specific downstream task. The most common fine-tuning method\nis to update pretrained weights via low-rank adaptation (LoRA). Existing\ninitialization strategies for LoRA often rely on singular value decompositions\n(SVD) of gradients or weight matrices. However, they do not provably maximize\nthe expected gradient signal, which is critical for fast adaptation. To this\nend, we introduce Explained Variance Adaptation (EVA), an initialization scheme\nthat uses the directions capturing the most activation variance, provably\nmaximizing the expected gradient signal and accelerating fine-tuning. EVA\nperforms incremental SVD on minibatches of activation vectors and selects the\nright-singular vectors for initialization once they converged. Further, by\nselecting the directions that capture the most activation-variance for a given\nrank budget, EVA accommodates adaptive ranks that reduce the number of\ntrainable parameters. We apply EVA to a variety of fine-tuning tasks as\nlanguage generation and understanding, image classification, and reinforcement\nlearning. EVA exhibits faster convergence than competitors and achieves the\nhighest average score across a multitude of tasks per domain while reducing the\nnumber of trainable parameters through rank redistribution. In summary, EVA\nestablishes a new Pareto frontier compared to existing LoRA initialization\nschemes in both accuracy and efficiency.\n", "link": "http://arxiv.org/abs/2410.07170v5", "date": "2025-10-20", "relevancy": 1.9043, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4803}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter%20Efficient%20Fine-tuning%20via%20Explained%20Variance%20Adaptation&body=Title%3A%20Parameter%20Efficient%20Fine-tuning%20via%20Explained%20Variance%20Adaptation%0AAuthor%3A%20Fabian%20Paischer%20and%20Lukas%20Hauzenberger%20and%20Thomas%20Schmied%20and%20Benedikt%20Alkin%20and%20Marc%20Peter%20Deisenroth%20and%20Sepp%20Hochreiter%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20are%20pre-trained%20on%20large-scale%20datasets%20and%20then%0Afine-tuned%20for%20a%20specific%20downstream%20task.%20The%20most%20common%20fine-tuning%20method%0Ais%20to%20update%20pretrained%20weights%20via%20low-rank%20adaptation%20%28LoRA%29.%20Existing%0Ainitialization%20strategies%20for%20LoRA%20often%20rely%20on%20singular%20value%20decompositions%0A%28SVD%29%20of%20gradients%20or%20weight%20matrices.%20However%2C%20they%20do%20not%20provably%20maximize%0Athe%20expected%20gradient%20signal%2C%20which%20is%20critical%20for%20fast%20adaptation.%20To%20this%0Aend%2C%20we%20introduce%20Explained%20Variance%20Adaptation%20%28EVA%29%2C%20an%20initialization%20scheme%0Athat%20uses%20the%20directions%20capturing%20the%20most%20activation%20variance%2C%20provably%0Amaximizing%20the%20expected%20gradient%20signal%20and%20accelerating%20fine-tuning.%20EVA%0Aperforms%20incremental%20SVD%20on%20minibatches%20of%20activation%20vectors%20and%20selects%20the%0Aright-singular%20vectors%20for%20initialization%20once%20they%20converged.%20Further%2C%20by%0Aselecting%20the%20directions%20that%20capture%20the%20most%20activation-variance%20for%20a%20given%0Arank%20budget%2C%20EVA%20accommodates%20adaptive%20ranks%20that%20reduce%20the%20number%20of%0Atrainable%20parameters.%20We%20apply%20EVA%20to%20a%20variety%20of%20fine-tuning%20tasks%20as%0Alanguage%20generation%20and%20understanding%2C%20image%20classification%2C%20and%20reinforcement%0Alearning.%20EVA%20exhibits%20faster%20convergence%20than%20competitors%20and%20achieves%20the%0Ahighest%20average%20score%20across%20a%20multitude%20of%20tasks%20per%20domain%20while%20reducing%20the%0Anumber%20of%20trainable%20parameters%20through%20rank%20redistribution.%20In%20summary%2C%20EVA%0Aestablishes%20a%20new%20Pareto%20frontier%20compared%20to%20existing%20LoRA%20initialization%0Aschemes%20in%20both%20accuracy%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07170v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter%2520Efficient%2520Fine-tuning%2520via%2520Explained%2520Variance%2520Adaptation%26entry.906535625%3DFabian%2520Paischer%2520and%2520Lukas%2520Hauzenberger%2520and%2520Thomas%2520Schmied%2520and%2520Benedikt%2520Alkin%2520and%2520Marc%2520Peter%2520Deisenroth%2520and%2520Sepp%2520Hochreiter%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520are%2520pre-trained%2520on%2520large-scale%2520datasets%2520and%2520then%250Afine-tuned%2520for%2520a%2520specific%2520downstream%2520task.%2520The%2520most%2520common%2520fine-tuning%2520method%250Ais%2520to%2520update%2520pretrained%2520weights%2520via%2520low-rank%2520adaptation%2520%2528LoRA%2529.%2520Existing%250Ainitialization%2520strategies%2520for%2520LoRA%2520often%2520rely%2520on%2520singular%2520value%2520decompositions%250A%2528SVD%2529%2520of%2520gradients%2520or%2520weight%2520matrices.%2520However%252C%2520they%2520do%2520not%2520provably%2520maximize%250Athe%2520expected%2520gradient%2520signal%252C%2520which%2520is%2520critical%2520for%2520fast%2520adaptation.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520Explained%2520Variance%2520Adaptation%2520%2528EVA%2529%252C%2520an%2520initialization%2520scheme%250Athat%2520uses%2520the%2520directions%2520capturing%2520the%2520most%2520activation%2520variance%252C%2520provably%250Amaximizing%2520the%2520expected%2520gradient%2520signal%2520and%2520accelerating%2520fine-tuning.%2520EVA%250Aperforms%2520incremental%2520SVD%2520on%2520minibatches%2520of%2520activation%2520vectors%2520and%2520selects%2520the%250Aright-singular%2520vectors%2520for%2520initialization%2520once%2520they%2520converged.%2520Further%252C%2520by%250Aselecting%2520the%2520directions%2520that%2520capture%2520the%2520most%2520activation-variance%2520for%2520a%2520given%250Arank%2520budget%252C%2520EVA%2520accommodates%2520adaptive%2520ranks%2520that%2520reduce%2520the%2520number%2520of%250Atrainable%2520parameters.%2520We%2520apply%2520EVA%2520to%2520a%2520variety%2520of%2520fine-tuning%2520tasks%2520as%250Alanguage%2520generation%2520and%2520understanding%252C%2520image%2520classification%252C%2520and%2520reinforcement%250Alearning.%2520EVA%2520exhibits%2520faster%2520convergence%2520than%2520competitors%2520and%2520achieves%2520the%250Ahighest%2520average%2520score%2520across%2520a%2520multitude%2520of%2520tasks%2520per%2520domain%2520while%2520reducing%2520the%250Anumber%2520of%2520trainable%2520parameters%2520through%2520rank%2520redistribution.%2520In%2520summary%252C%2520EVA%250Aestablishes%2520a%2520new%2520Pareto%2520frontier%2520compared%2520to%2520existing%2520LoRA%2520initialization%250Aschemes%2520in%2520both%2520accuracy%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07170v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Efficient%20Fine-tuning%20via%20Explained%20Variance%20Adaptation&entry.906535625=Fabian%20Paischer%20and%20Lukas%20Hauzenberger%20and%20Thomas%20Schmied%20and%20Benedikt%20Alkin%20and%20Marc%20Peter%20Deisenroth%20and%20Sepp%20Hochreiter&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20are%20pre-trained%20on%20large-scale%20datasets%20and%20then%0Afine-tuned%20for%20a%20specific%20downstream%20task.%20The%20most%20common%20fine-tuning%20method%0Ais%20to%20update%20pretrained%20weights%20via%20low-rank%20adaptation%20%28LoRA%29.%20Existing%0Ainitialization%20strategies%20for%20LoRA%20often%20rely%20on%20singular%20value%20decompositions%0A%28SVD%29%20of%20gradients%20or%20weight%20matrices.%20However%2C%20they%20do%20not%20provably%20maximize%0Athe%20expected%20gradient%20signal%2C%20which%20is%20critical%20for%20fast%20adaptation.%20To%20this%0Aend%2C%20we%20introduce%20Explained%20Variance%20Adaptation%20%28EVA%29%2C%20an%20initialization%20scheme%0Athat%20uses%20the%20directions%20capturing%20the%20most%20activation%20variance%2C%20provably%0Amaximizing%20the%20expected%20gradient%20signal%20and%20accelerating%20fine-tuning.%20EVA%0Aperforms%20incremental%20SVD%20on%20minibatches%20of%20activation%20vectors%20and%20selects%20the%0Aright-singular%20vectors%20for%20initialization%20once%20they%20converged.%20Further%2C%20by%0Aselecting%20the%20directions%20that%20capture%20the%20most%20activation-variance%20for%20a%20given%0Arank%20budget%2C%20EVA%20accommodates%20adaptive%20ranks%20that%20reduce%20the%20number%20of%0Atrainable%20parameters.%20We%20apply%20EVA%20to%20a%20variety%20of%20fine-tuning%20tasks%20as%0Alanguage%20generation%20and%20understanding%2C%20image%20classification%2C%20and%20reinforcement%0Alearning.%20EVA%20exhibits%20faster%20convergence%20than%20competitors%20and%20achieves%20the%0Ahighest%20average%20score%20across%20a%20multitude%20of%20tasks%20per%20domain%20while%20reducing%20the%0Anumber%20of%20trainable%20parameters%20through%20rank%20redistribution.%20In%20summary%2C%20EVA%0Aestablishes%20a%20new%20Pareto%20frontier%20compared%20to%20existing%20LoRA%20initialization%0Aschemes%20in%20both%20accuracy%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07170v5&entry.124074799=Read"},
{"title": "What should a neuron aim for? Designing local objective functions based\n  on information theory", "author": "Andreas C. Schneider and Valentin Neuhaus and David A. Ehrlich and Abdullah Makkeh and Alexander S. Ecker and Viola Priesemann and Michael Wibral", "abstract": "  In modern deep neural networks, the learning dynamics of the individual\nneurons is often obscure, as the networks are trained via global optimization.\nConversely, biological systems build on self-organized, local learning,\nachieving robustness and efficiency with limited global information. We here\nshow how self-organization between individual artificial neurons can be\nachieved by designing abstract bio-inspired local learning goals. These goals\nare parameterized using a recent extension of information theory, Partial\nInformation Decomposition (PID), which decomposes the information that a set of\ninformation sources holds about an outcome into unique, redundant and\nsynergistic contributions. Our framework enables neurons to locally shape the\nintegration of information from various input classes, i.e. feedforward,\nfeedback, and lateral, by selecting which of the three inputs should contribute\nuniquely, redundantly or synergistically to the output. This selection is\nexpressed as a weighted sum of PID terms, which, for a given problem, can be\ndirectly derived from intuitive reasoning or via numerical optimization,\noffering a window into understanding task-relevant local information\nprocessing. Achieving neuron-level interpretability while enabling strong\nperformance using local learning, our work advances a principled\ninformation-theoretic foundation for local learning strategies.\n", "link": "http://arxiv.org/abs/2412.02482v5", "date": "2025-10-20", "relevancy": 1.9025, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4724}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory&body=Title%3A%20What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory%0AAuthor%3A%20Andreas%20C.%20Schneider%20and%20Valentin%20Neuhaus%20and%20David%20A.%20Ehrlich%20and%20Abdullah%20Makkeh%20and%20Alexander%20S.%20Ecker%20and%20Viola%20Priesemann%20and%20Michael%20Wibral%0AAbstract%3A%20%20%20In%20modern%20deep%20neural%20networks%2C%20the%20learning%20dynamics%20of%20the%20individual%0Aneurons%20is%20often%20obscure%2C%20as%20the%20networks%20are%20trained%20via%20global%20optimization.%0AConversely%2C%20biological%20systems%20build%20on%20self-organized%2C%20local%20learning%2C%0Aachieving%20robustness%20and%20efficiency%20with%20limited%20global%20information.%20We%20here%0Ashow%20how%20self-organization%20between%20individual%20artificial%20neurons%20can%20be%0Aachieved%20by%20designing%20abstract%20bio-inspired%20local%20learning%20goals.%20These%20goals%0Aare%20parameterized%20using%20a%20recent%20extension%20of%20information%20theory%2C%20Partial%0AInformation%20Decomposition%20%28PID%29%2C%20which%20decomposes%20the%20information%20that%20a%20set%20of%0Ainformation%20sources%20holds%20about%20an%20outcome%20into%20unique%2C%20redundant%20and%0Asynergistic%20contributions.%20Our%20framework%20enables%20neurons%20to%20locally%20shape%20the%0Aintegration%20of%20information%20from%20various%20input%20classes%2C%20i.e.%20feedforward%2C%0Afeedback%2C%20and%20lateral%2C%20by%20selecting%20which%20of%20the%20three%20inputs%20should%20contribute%0Auniquely%2C%20redundantly%20or%20synergistically%20to%20the%20output.%20This%20selection%20is%0Aexpressed%20as%20a%20weighted%20sum%20of%20PID%20terms%2C%20which%2C%20for%20a%20given%20problem%2C%20can%20be%0Adirectly%20derived%20from%20intuitive%20reasoning%20or%20via%20numerical%20optimization%2C%0Aoffering%20a%20window%20into%20understanding%20task-relevant%20local%20information%0Aprocessing.%20Achieving%20neuron-level%20interpretability%20while%20enabling%20strong%0Aperformance%20using%20local%20learning%2C%20our%20work%20advances%20a%20principled%0Ainformation-theoretic%20foundation%20for%20local%20learning%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02482v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520should%2520a%2520neuron%2520aim%2520for%253F%2520Designing%2520local%2520objective%2520functions%2520based%250A%2520%2520on%2520information%2520theory%26entry.906535625%3DAndreas%2520C.%2520Schneider%2520and%2520Valentin%2520Neuhaus%2520and%2520David%2520A.%2520Ehrlich%2520and%2520Abdullah%2520Makkeh%2520and%2520Alexander%2520S.%2520Ecker%2520and%2520Viola%2520Priesemann%2520and%2520Michael%2520Wibral%26entry.1292438233%3D%2520%2520In%2520modern%2520deep%2520neural%2520networks%252C%2520the%2520learning%2520dynamics%2520of%2520the%2520individual%250Aneurons%2520is%2520often%2520obscure%252C%2520as%2520the%2520networks%2520are%2520trained%2520via%2520global%2520optimization.%250AConversely%252C%2520biological%2520systems%2520build%2520on%2520self-organized%252C%2520local%2520learning%252C%250Aachieving%2520robustness%2520and%2520efficiency%2520with%2520limited%2520global%2520information.%2520We%2520here%250Ashow%2520how%2520self-organization%2520between%2520individual%2520artificial%2520neurons%2520can%2520be%250Aachieved%2520by%2520designing%2520abstract%2520bio-inspired%2520local%2520learning%2520goals.%2520These%2520goals%250Aare%2520parameterized%2520using%2520a%2520recent%2520extension%2520of%2520information%2520theory%252C%2520Partial%250AInformation%2520Decomposition%2520%2528PID%2529%252C%2520which%2520decomposes%2520the%2520information%2520that%2520a%2520set%2520of%250Ainformation%2520sources%2520holds%2520about%2520an%2520outcome%2520into%2520unique%252C%2520redundant%2520and%250Asynergistic%2520contributions.%2520Our%2520framework%2520enables%2520neurons%2520to%2520locally%2520shape%2520the%250Aintegration%2520of%2520information%2520from%2520various%2520input%2520classes%252C%2520i.e.%2520feedforward%252C%250Afeedback%252C%2520and%2520lateral%252C%2520by%2520selecting%2520which%2520of%2520the%2520three%2520inputs%2520should%2520contribute%250Auniquely%252C%2520redundantly%2520or%2520synergistically%2520to%2520the%2520output.%2520This%2520selection%2520is%250Aexpressed%2520as%2520a%2520weighted%2520sum%2520of%2520PID%2520terms%252C%2520which%252C%2520for%2520a%2520given%2520problem%252C%2520can%2520be%250Adirectly%2520derived%2520from%2520intuitive%2520reasoning%2520or%2520via%2520numerical%2520optimization%252C%250Aoffering%2520a%2520window%2520into%2520understanding%2520task-relevant%2520local%2520information%250Aprocessing.%2520Achieving%2520neuron-level%2520interpretability%2520while%2520enabling%2520strong%250Aperformance%2520using%2520local%2520learning%252C%2520our%2520work%2520advances%2520a%2520principled%250Ainformation-theoretic%2520foundation%2520for%2520local%2520learning%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02482v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20should%20a%20neuron%20aim%20for%3F%20Designing%20local%20objective%20functions%20based%0A%20%20on%20information%20theory&entry.906535625=Andreas%20C.%20Schneider%20and%20Valentin%20Neuhaus%20and%20David%20A.%20Ehrlich%20and%20Abdullah%20Makkeh%20and%20Alexander%20S.%20Ecker%20and%20Viola%20Priesemann%20and%20Michael%20Wibral&entry.1292438233=%20%20In%20modern%20deep%20neural%20networks%2C%20the%20learning%20dynamics%20of%20the%20individual%0Aneurons%20is%20often%20obscure%2C%20as%20the%20networks%20are%20trained%20via%20global%20optimization.%0AConversely%2C%20biological%20systems%20build%20on%20self-organized%2C%20local%20learning%2C%0Aachieving%20robustness%20and%20efficiency%20with%20limited%20global%20information.%20We%20here%0Ashow%20how%20self-organization%20between%20individual%20artificial%20neurons%20can%20be%0Aachieved%20by%20designing%20abstract%20bio-inspired%20local%20learning%20goals.%20These%20goals%0Aare%20parameterized%20using%20a%20recent%20extension%20of%20information%20theory%2C%20Partial%0AInformation%20Decomposition%20%28PID%29%2C%20which%20decomposes%20the%20information%20that%20a%20set%20of%0Ainformation%20sources%20holds%20about%20an%20outcome%20into%20unique%2C%20redundant%20and%0Asynergistic%20contributions.%20Our%20framework%20enables%20neurons%20to%20locally%20shape%20the%0Aintegration%20of%20information%20from%20various%20input%20classes%2C%20i.e.%20feedforward%2C%0Afeedback%2C%20and%20lateral%2C%20by%20selecting%20which%20of%20the%20three%20inputs%20should%20contribute%0Auniquely%2C%20redundantly%20or%20synergistically%20to%20the%20output.%20This%20selection%20is%0Aexpressed%20as%20a%20weighted%20sum%20of%20PID%20terms%2C%20which%2C%20for%20a%20given%20problem%2C%20can%20be%0Adirectly%20derived%20from%20intuitive%20reasoning%20or%20via%20numerical%20optimization%2C%0Aoffering%20a%20window%20into%20understanding%20task-relevant%20local%20information%0Aprocessing.%20Achieving%20neuron-level%20interpretability%20while%20enabling%20strong%0Aperformance%20using%20local%20learning%2C%20our%20work%20advances%20a%20principled%0Ainformation-theoretic%20foundation%20for%20local%20learning%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02482v5&entry.124074799=Read"},
{"title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet\n  Arena", "author": "Qingchuan Yang and Simon Mahns and Sida Li and Anri Gu and Jibang Wu and Haifeng Xu", "abstract": "  Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.\n", "link": "http://arxiv.org/abs/2510.17638v1", "date": "2025-10-20", "relevancy": 1.8949, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-as-a-Prophet%3A%20Understanding%20Predictive%20Intelligence%20with%20Prophet%0A%20%20Arena&body=Title%3A%20LLM-as-a-Prophet%3A%20Understanding%20Predictive%20Intelligence%20with%20Prophet%0A%20%20Arena%0AAuthor%3A%20Qingchuan%20Yang%20and%20Simon%20Mahns%20and%20Sida%20Li%20and%20Anri%20Gu%20and%20Jibang%20Wu%20and%20Haifeng%20Xu%0AAbstract%3A%20%20%20Forecasting%20is%20not%20only%20a%20fundamental%20intellectual%20pursuit%20but%20also%20is%20of%0Asignificant%20importance%20to%20societal%20systems%20such%20as%20finance%20and%20economics.%20With%0Athe%20rapid%20advances%20of%20large%20language%20models%20%28LLMs%29%20trained%20on%20Internet-scale%0Adata%2C%20it%20raises%20the%20promise%20of%20employing%20LLMs%20to%20forecast%20real-world%20future%0Aevents%2C%20an%20emerging%20paradigm%20we%20call%20%22LLM-as-a-Prophet%22.%20This%20paper%0Asystematically%20investigates%20such%20predictive%20intelligence%20of%20LLMs.%20To%20this%20end%2C%0Awe%20build%20Prophet%20Arena%2C%20a%20general%20evaluation%20benchmark%20that%20continuously%0Acollects%20live%20forecasting%20tasks%20and%20decomposes%20each%20task%20into%20distinct%20pipeline%0Astages%2C%20in%20order%20to%20support%20our%20controlled%20and%20large-scale%20experimentation.%20Our%0Acomprehensive%20evaluation%20reveals%20that%20many%20LLMs%20already%20exhibit%20impressive%0Aforecasting%20capabilities%2C%20reflected%20in%2C%20e.g.%2C%20their%20small%20calibration%20errors%2C%0Aconsistent%20prediction%20confidence%20and%20promising%20market%20returns.%20However%2C%20we%20also%0Auncover%20key%20bottlenecks%20towards%20achieving%20superior%20predictive%20intelligence%20via%0ALLM-as-a-Prophet%2C%20such%20as%20LLMs%27%20inaccurate%20event%20recalls%2C%20misunderstanding%20of%0Adata%20sources%20and%20slower%20information%20aggregation%20compared%20to%20markets%20when%0Aresolution%20nears.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-as-a-Prophet%253A%2520Understanding%2520Predictive%2520Intelligence%2520with%2520Prophet%250A%2520%2520Arena%26entry.906535625%3DQingchuan%2520Yang%2520and%2520Simon%2520Mahns%2520and%2520Sida%2520Li%2520and%2520Anri%2520Gu%2520and%2520Jibang%2520Wu%2520and%2520Haifeng%2520Xu%26entry.1292438233%3D%2520%2520Forecasting%2520is%2520not%2520only%2520a%2520fundamental%2520intellectual%2520pursuit%2520but%2520also%2520is%2520of%250Asignificant%2520importance%2520to%2520societal%2520systems%2520such%2520as%2520finance%2520and%2520economics.%2520With%250Athe%2520rapid%2520advances%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520trained%2520on%2520Internet-scale%250Adata%252C%2520it%2520raises%2520the%2520promise%2520of%2520employing%2520LLMs%2520to%2520forecast%2520real-world%2520future%250Aevents%252C%2520an%2520emerging%2520paradigm%2520we%2520call%2520%2522LLM-as-a-Prophet%2522.%2520This%2520paper%250Asystematically%2520investigates%2520such%2520predictive%2520intelligence%2520of%2520LLMs.%2520To%2520this%2520end%252C%250Awe%2520build%2520Prophet%2520Arena%252C%2520a%2520general%2520evaluation%2520benchmark%2520that%2520continuously%250Acollects%2520live%2520forecasting%2520tasks%2520and%2520decomposes%2520each%2520task%2520into%2520distinct%2520pipeline%250Astages%252C%2520in%2520order%2520to%2520support%2520our%2520controlled%2520and%2520large-scale%2520experimentation.%2520Our%250Acomprehensive%2520evaluation%2520reveals%2520that%2520many%2520LLMs%2520already%2520exhibit%2520impressive%250Aforecasting%2520capabilities%252C%2520reflected%2520in%252C%2520e.g.%252C%2520their%2520small%2520calibration%2520errors%252C%250Aconsistent%2520prediction%2520confidence%2520and%2520promising%2520market%2520returns.%2520However%252C%2520we%2520also%250Auncover%2520key%2520bottlenecks%2520towards%2520achieving%2520superior%2520predictive%2520intelligence%2520via%250ALLM-as-a-Prophet%252C%2520such%2520as%2520LLMs%2527%2520inaccurate%2520event%2520recalls%252C%2520misunderstanding%2520of%250Adata%2520sources%2520and%2520slower%2520information%2520aggregation%2520compared%2520to%2520markets%2520when%250Aresolution%2520nears.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-as-a-Prophet%3A%20Understanding%20Predictive%20Intelligence%20with%20Prophet%0A%20%20Arena&entry.906535625=Qingchuan%20Yang%20and%20Simon%20Mahns%20and%20Sida%20Li%20and%20Anri%20Gu%20and%20Jibang%20Wu%20and%20Haifeng%20Xu&entry.1292438233=%20%20Forecasting%20is%20not%20only%20a%20fundamental%20intellectual%20pursuit%20but%20also%20is%20of%0Asignificant%20importance%20to%20societal%20systems%20such%20as%20finance%20and%20economics.%20With%0Athe%20rapid%20advances%20of%20large%20language%20models%20%28LLMs%29%20trained%20on%20Internet-scale%0Adata%2C%20it%20raises%20the%20promise%20of%20employing%20LLMs%20to%20forecast%20real-world%20future%0Aevents%2C%20an%20emerging%20paradigm%20we%20call%20%22LLM-as-a-Prophet%22.%20This%20paper%0Asystematically%20investigates%20such%20predictive%20intelligence%20of%20LLMs.%20To%20this%20end%2C%0Awe%20build%20Prophet%20Arena%2C%20a%20general%20evaluation%20benchmark%20that%20continuously%0Acollects%20live%20forecasting%20tasks%20and%20decomposes%20each%20task%20into%20distinct%20pipeline%0Astages%2C%20in%20order%20to%20support%20our%20controlled%20and%20large-scale%20experimentation.%20Our%0Acomprehensive%20evaluation%20reveals%20that%20many%20LLMs%20already%20exhibit%20impressive%0Aforecasting%20capabilities%2C%20reflected%20in%2C%20e.g.%2C%20their%20small%20calibration%20errors%2C%0Aconsistent%20prediction%20confidence%20and%20promising%20market%20returns.%20However%2C%20we%20also%0Auncover%20key%20bottlenecks%20towards%20achieving%20superior%20predictive%20intelligence%20via%0ALLM-as-a-Prophet%2C%20such%20as%20LLMs%27%20inaccurate%20event%20recalls%2C%20misunderstanding%20of%0Adata%20sources%20and%20slower%20information%20aggregation%20compared%20to%20markets%20when%0Aresolution%20nears.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17638v1&entry.124074799=Read"},
{"title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "author": "Katarzyna Kobalczyk and Zhiyuan Jerry Lin and Benjamin Letham and Zhuokai Zhao and Maximilian Balandat and Eytan Bakshy", "abstract": "  For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.\n", "link": "http://arxiv.org/abs/2510.17671v1", "date": "2025-10-20", "relevancy": 1.8877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LILO%3A%20Bayesian%20Optimization%20with%20Interactive%20Natural%20Language%20Feedback&body=Title%3A%20LILO%3A%20Bayesian%20Optimization%20with%20Interactive%20Natural%20Language%20Feedback%0AAuthor%3A%20Katarzyna%20Kobalczyk%20and%20Zhiyuan%20Jerry%20Lin%20and%20Benjamin%20Letham%20and%20Zhuokai%20Zhao%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy%0AAbstract%3A%20%20%20For%20many%20real-world%20applications%2C%20feedback%20is%20essential%20in%20translating%0Acomplex%2C%20nuanced%2C%20or%20subjective%20goals%20into%20quantifiable%20optimization%0Aobjectives.%20We%20propose%20a%20language-in-the-loop%20framework%20that%20uses%20a%20large%0Alanguage%20model%20%28LLM%29%20to%20convert%20unstructured%20feedback%20in%20the%20form%20of%20natural%0Alanguage%20into%20scalar%20utilities%20to%20conduct%20BO%20over%20a%20numeric%20search%20space.%0AUnlike%20preferential%20BO%2C%20which%20only%20accepts%20restricted%20feedback%20formats%20and%0Arequires%20customized%20models%20for%20each%20domain-specific%20problem%2C%20our%20approach%0Aleverages%20LLMs%20to%20turn%20varied%20types%20of%20textual%20feedback%20into%20consistent%20utility%0Asignals%20and%20to%20easily%20include%20flexible%20user%20priors%20without%20manual%20kernel%0Adesign.%20At%20the%20same%20time%2C%20our%20method%20maintains%20the%20sample%20efficiency%20and%0Aprincipled%20uncertainty%20quantification%20of%20BO.%20We%20show%20that%20this%20hybrid%20method%0Anot%20only%20provides%20a%20more%20natural%20interface%20to%20the%20decision%20maker%20but%20also%0Aoutperforms%20conventional%20BO%20baselines%20and%20LLM-only%20optimizers%2C%20particularly%20in%0Afeedback-limited%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLILO%253A%2520Bayesian%2520Optimization%2520with%2520Interactive%2520Natural%2520Language%2520Feedback%26entry.906535625%3DKatarzyna%2520Kobalczyk%2520and%2520Zhiyuan%2520Jerry%2520Lin%2520and%2520Benjamin%2520Letham%2520and%2520Zhuokai%2520Zhao%2520and%2520Maximilian%2520Balandat%2520and%2520Eytan%2520Bakshy%26entry.1292438233%3D%2520%2520For%2520many%2520real-world%2520applications%252C%2520feedback%2520is%2520essential%2520in%2520translating%250Acomplex%252C%2520nuanced%252C%2520or%2520subjective%2520goals%2520into%2520quantifiable%2520optimization%250Aobjectives.%2520We%2520propose%2520a%2520language-in-the-loop%2520framework%2520that%2520uses%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520to%2520convert%2520unstructured%2520feedback%2520in%2520the%2520form%2520of%2520natural%250Alanguage%2520into%2520scalar%2520utilities%2520to%2520conduct%2520BO%2520over%2520a%2520numeric%2520search%2520space.%250AUnlike%2520preferential%2520BO%252C%2520which%2520only%2520accepts%2520restricted%2520feedback%2520formats%2520and%250Arequires%2520customized%2520models%2520for%2520each%2520domain-specific%2520problem%252C%2520our%2520approach%250Aleverages%2520LLMs%2520to%2520turn%2520varied%2520types%2520of%2520textual%2520feedback%2520into%2520consistent%2520utility%250Asignals%2520and%2520to%2520easily%2520include%2520flexible%2520user%2520priors%2520without%2520manual%2520kernel%250Adesign.%2520At%2520the%2520same%2520time%252C%2520our%2520method%2520maintains%2520the%2520sample%2520efficiency%2520and%250Aprincipled%2520uncertainty%2520quantification%2520of%2520BO.%2520We%2520show%2520that%2520this%2520hybrid%2520method%250Anot%2520only%2520provides%2520a%2520more%2520natural%2520interface%2520to%2520the%2520decision%2520maker%2520but%2520also%250Aoutperforms%2520conventional%2520BO%2520baselines%2520and%2520LLM-only%2520optimizers%252C%2520particularly%2520in%250Afeedback-limited%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LILO%3A%20Bayesian%20Optimization%20with%20Interactive%20Natural%20Language%20Feedback&entry.906535625=Katarzyna%20Kobalczyk%20and%20Zhiyuan%20Jerry%20Lin%20and%20Benjamin%20Letham%20and%20Zhuokai%20Zhao%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy&entry.1292438233=%20%20For%20many%20real-world%20applications%2C%20feedback%20is%20essential%20in%20translating%0Acomplex%2C%20nuanced%2C%20or%20subjective%20goals%20into%20quantifiable%20optimization%0Aobjectives.%20We%20propose%20a%20language-in-the-loop%20framework%20that%20uses%20a%20large%0Alanguage%20model%20%28LLM%29%20to%20convert%20unstructured%20feedback%20in%20the%20form%20of%20natural%0Alanguage%20into%20scalar%20utilities%20to%20conduct%20BO%20over%20a%20numeric%20search%20space.%0AUnlike%20preferential%20BO%2C%20which%20only%20accepts%20restricted%20feedback%20formats%20and%0Arequires%20customized%20models%20for%20each%20domain-specific%20problem%2C%20our%20approach%0Aleverages%20LLMs%20to%20turn%20varied%20types%20of%20textual%20feedback%20into%20consistent%20utility%0Asignals%20and%20to%20easily%20include%20flexible%20user%20priors%20without%20manual%20kernel%0Adesign.%20At%20the%20same%20time%2C%20our%20method%20maintains%20the%20sample%20efficiency%20and%0Aprincipled%20uncertainty%20quantification%20of%20BO.%20We%20show%20that%20this%20hybrid%20method%0Anot%20only%20provides%20a%20more%20natural%20interface%20to%20the%20decision%20maker%20but%20also%0Aoutperforms%20conventional%20BO%20baselines%20and%20LLM-only%20optimizers%2C%20particularly%20in%0Afeedback-limited%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17671v1&entry.124074799=Read"},
{"title": "Closing the Sim2Real Performance Gap in RL", "author": "Akhil S Anand and Shambhuraj Sawant and Jasper Hoffmann and Dirk Reinhardt and Sebastien Gros", "abstract": "  Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.\n", "link": "http://arxiv.org/abs/2510.17709v1", "date": "2025-10-20", "relevancy": 1.8769, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL&body=Title%3A%20Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL%0AAuthor%3A%20Akhil%20S%20Anand%20and%20Shambhuraj%20Sawant%20and%20Jasper%20Hoffmann%20and%20Dirk%20Reinhardt%20and%20Sebastien%20Gros%0AAbstract%3A%20%20%20Sim2Real%20aims%20at%20training%20policies%20in%20high-fidelity%20simulation%20environments%0Aand%20effectively%20transferring%20them%20to%20the%20real%20world.%20Despite%20the%20developments%0Aof%20accurate%20simulators%20and%20Sim2Real%20RL%20approaches%2C%20the%20policies%20trained%20purely%0Ain%20simulation%20often%20suffer%20significant%20performance%20drops%20when%20deployed%20in%20real%0Aenvironments.%20This%20drop%20is%20referred%20to%20as%20the%20Sim2Real%20performance%20gap.%20Current%0ASim2Real%20RL%20methods%20optimize%20the%20simulator%20accuracy%20and%20variability%20as%20proxies%0Afor%20real-world%20performance.%20However%2C%20these%20metrics%20do%20not%20necessarily%20correlate%0Awith%20the%20real-world%20performance%20of%20the%20policy%20as%20established%20theoretically%20and%0Aempirically%20in%20the%20literature.%20We%20propose%20a%20novel%20framework%20to%20address%20this%0Aissue%20by%20directly%20adapting%20the%20simulator%20parameters%20based%20on%20real-world%0Aperformance.%20We%20frame%20this%20problem%20as%20a%20bi-level%20RL%20framework%3A%20the%20inner-level%0ARL%20trains%20a%20policy%20purely%20in%20simulation%2C%20and%20the%20outer-level%20RL%20adapts%20the%0Asimulation%20model%20and%20in-sim%20reward%20parameters%20to%20maximize%20real-world%0Aperformance%20of%20the%20in-sim%20policy.%20We%20derive%20and%20validate%20in%20simple%20examples%20the%0Amathematical%20tools%20needed%20to%20develop%20bi-level%20RL%20algorithms%20that%20close%20the%0ASim2Real%20performance%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Sim2Real%2520Performance%2520Gap%2520in%2520RL%26entry.906535625%3DAkhil%2520S%2520Anand%2520and%2520Shambhuraj%2520Sawant%2520and%2520Jasper%2520Hoffmann%2520and%2520Dirk%2520Reinhardt%2520and%2520Sebastien%2520Gros%26entry.1292438233%3D%2520%2520Sim2Real%2520aims%2520at%2520training%2520policies%2520in%2520high-fidelity%2520simulation%2520environments%250Aand%2520effectively%2520transferring%2520them%2520to%2520the%2520real%2520world.%2520Despite%2520the%2520developments%250Aof%2520accurate%2520simulators%2520and%2520Sim2Real%2520RL%2520approaches%252C%2520the%2520policies%2520trained%2520purely%250Ain%2520simulation%2520often%2520suffer%2520significant%2520performance%2520drops%2520when%2520deployed%2520in%2520real%250Aenvironments.%2520This%2520drop%2520is%2520referred%2520to%2520as%2520the%2520Sim2Real%2520performance%2520gap.%2520Current%250ASim2Real%2520RL%2520methods%2520optimize%2520the%2520simulator%2520accuracy%2520and%2520variability%2520as%2520proxies%250Afor%2520real-world%2520performance.%2520However%252C%2520these%2520metrics%2520do%2520not%2520necessarily%2520correlate%250Awith%2520the%2520real-world%2520performance%2520of%2520the%2520policy%2520as%2520established%2520theoretically%2520and%250Aempirically%2520in%2520the%2520literature.%2520We%2520propose%2520a%2520novel%2520framework%2520to%2520address%2520this%250Aissue%2520by%2520directly%2520adapting%2520the%2520simulator%2520parameters%2520based%2520on%2520real-world%250Aperformance.%2520We%2520frame%2520this%2520problem%2520as%2520a%2520bi-level%2520RL%2520framework%253A%2520the%2520inner-level%250ARL%2520trains%2520a%2520policy%2520purely%2520in%2520simulation%252C%2520and%2520the%2520outer-level%2520RL%2520adapts%2520the%250Asimulation%2520model%2520and%2520in-sim%2520reward%2520parameters%2520to%2520maximize%2520real-world%250Aperformance%2520of%2520the%2520in-sim%2520policy.%2520We%2520derive%2520and%2520validate%2520in%2520simple%2520examples%2520the%250Amathematical%2520tools%2520needed%2520to%2520develop%2520bi-level%2520RL%2520algorithms%2520that%2520close%2520the%250ASim2Real%2520performance%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL&entry.906535625=Akhil%20S%20Anand%20and%20Shambhuraj%20Sawant%20and%20Jasper%20Hoffmann%20and%20Dirk%20Reinhardt%20and%20Sebastien%20Gros&entry.1292438233=%20%20Sim2Real%20aims%20at%20training%20policies%20in%20high-fidelity%20simulation%20environments%0Aand%20effectively%20transferring%20them%20to%20the%20real%20world.%20Despite%20the%20developments%0Aof%20accurate%20simulators%20and%20Sim2Real%20RL%20approaches%2C%20the%20policies%20trained%20purely%0Ain%20simulation%20often%20suffer%20significant%20performance%20drops%20when%20deployed%20in%20real%0Aenvironments.%20This%20drop%20is%20referred%20to%20as%20the%20Sim2Real%20performance%20gap.%20Current%0ASim2Real%20RL%20methods%20optimize%20the%20simulator%20accuracy%20and%20variability%20as%20proxies%0Afor%20real-world%20performance.%20However%2C%20these%20metrics%20do%20not%20necessarily%20correlate%0Awith%20the%20real-world%20performance%20of%20the%20policy%20as%20established%20theoretically%20and%0Aempirically%20in%20the%20literature.%20We%20propose%20a%20novel%20framework%20to%20address%20this%0Aissue%20by%20directly%20adapting%20the%20simulator%20parameters%20based%20on%20real-world%0Aperformance.%20We%20frame%20this%20problem%20as%20a%20bi-level%20RL%20framework%3A%20the%20inner-level%0ARL%20trains%20a%20policy%20purely%20in%20simulation%2C%20and%20the%20outer-level%20RL%20adapts%20the%0Asimulation%20model%20and%20in-sim%20reward%20parameters%20to%20maximize%20real-world%0Aperformance%20of%20the%20in-sim%20policy.%20We%20derive%20and%20validate%20in%20simple%20examples%20the%0Amathematical%20tools%20needed%20to%20develop%20bi-level%20RL%20algorithms%20that%20close%20the%0ASim2Real%20performance%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17709v1&entry.124074799=Read"},
{"title": "Closing the Sim2Real Performance Gap in RL", "author": "Akhil S Anand and Shambhuraj Sawant and Jasper Hoffmann and Dirk Reinhardt and Sebastien Gros", "abstract": "  Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.\n", "link": "http://arxiv.org/abs/2510.17709v1", "date": "2025-10-20", "relevancy": 1.8769, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4902}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4713}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL&body=Title%3A%20Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL%0AAuthor%3A%20Akhil%20S%20Anand%20and%20Shambhuraj%20Sawant%20and%20Jasper%20Hoffmann%20and%20Dirk%20Reinhardt%20and%20Sebastien%20Gros%0AAbstract%3A%20%20%20Sim2Real%20aims%20at%20training%20policies%20in%20high-fidelity%20simulation%20environments%0Aand%20effectively%20transferring%20them%20to%20the%20real%20world.%20Despite%20the%20developments%0Aof%20accurate%20simulators%20and%20Sim2Real%20RL%20approaches%2C%20the%20policies%20trained%20purely%0Ain%20simulation%20often%20suffer%20significant%20performance%20drops%20when%20deployed%20in%20real%0Aenvironments.%20This%20drop%20is%20referred%20to%20as%20the%20Sim2Real%20performance%20gap.%20Current%0ASim2Real%20RL%20methods%20optimize%20the%20simulator%20accuracy%20and%20variability%20as%20proxies%0Afor%20real-world%20performance.%20However%2C%20these%20metrics%20do%20not%20necessarily%20correlate%0Awith%20the%20real-world%20performance%20of%20the%20policy%20as%20established%20theoretically%20and%0Aempirically%20in%20the%20literature.%20We%20propose%20a%20novel%20framework%20to%20address%20this%0Aissue%20by%20directly%20adapting%20the%20simulator%20parameters%20based%20on%20real-world%0Aperformance.%20We%20frame%20this%20problem%20as%20a%20bi-level%20RL%20framework%3A%20the%20inner-level%0ARL%20trains%20a%20policy%20purely%20in%20simulation%2C%20and%20the%20outer-level%20RL%20adapts%20the%0Asimulation%20model%20and%20in-sim%20reward%20parameters%20to%20maximize%20real-world%0Aperformance%20of%20the%20in-sim%20policy.%20We%20derive%20and%20validate%20in%20simple%20examples%20the%0Amathematical%20tools%20needed%20to%20develop%20bi-level%20RL%20algorithms%20that%20close%20the%0ASim2Real%20performance%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Sim2Real%2520Performance%2520Gap%2520in%2520RL%26entry.906535625%3DAkhil%2520S%2520Anand%2520and%2520Shambhuraj%2520Sawant%2520and%2520Jasper%2520Hoffmann%2520and%2520Dirk%2520Reinhardt%2520and%2520Sebastien%2520Gros%26entry.1292438233%3D%2520%2520Sim2Real%2520aims%2520at%2520training%2520policies%2520in%2520high-fidelity%2520simulation%2520environments%250Aand%2520effectively%2520transferring%2520them%2520to%2520the%2520real%2520world.%2520Despite%2520the%2520developments%250Aof%2520accurate%2520simulators%2520and%2520Sim2Real%2520RL%2520approaches%252C%2520the%2520policies%2520trained%2520purely%250Ain%2520simulation%2520often%2520suffer%2520significant%2520performance%2520drops%2520when%2520deployed%2520in%2520real%250Aenvironments.%2520This%2520drop%2520is%2520referred%2520to%2520as%2520the%2520Sim2Real%2520performance%2520gap.%2520Current%250ASim2Real%2520RL%2520methods%2520optimize%2520the%2520simulator%2520accuracy%2520and%2520variability%2520as%2520proxies%250Afor%2520real-world%2520performance.%2520However%252C%2520these%2520metrics%2520do%2520not%2520necessarily%2520correlate%250Awith%2520the%2520real-world%2520performance%2520of%2520the%2520policy%2520as%2520established%2520theoretically%2520and%250Aempirically%2520in%2520the%2520literature.%2520We%2520propose%2520a%2520novel%2520framework%2520to%2520address%2520this%250Aissue%2520by%2520directly%2520adapting%2520the%2520simulator%2520parameters%2520based%2520on%2520real-world%250Aperformance.%2520We%2520frame%2520this%2520problem%2520as%2520a%2520bi-level%2520RL%2520framework%253A%2520the%2520inner-level%250ARL%2520trains%2520a%2520policy%2520purely%2520in%2520simulation%252C%2520and%2520the%2520outer-level%2520RL%2520adapts%2520the%250Asimulation%2520model%2520and%2520in-sim%2520reward%2520parameters%2520to%2520maximize%2520real-world%250Aperformance%2520of%2520the%2520in-sim%2520policy.%2520We%2520derive%2520and%2520validate%2520in%2520simple%2520examples%2520the%250Amathematical%2520tools%2520needed%2520to%2520develop%2520bi-level%2520RL%2520algorithms%2520that%2520close%2520the%250ASim2Real%2520performance%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Sim2Real%20Performance%20Gap%20in%20RL&entry.906535625=Akhil%20S%20Anand%20and%20Shambhuraj%20Sawant%20and%20Jasper%20Hoffmann%20and%20Dirk%20Reinhardt%20and%20Sebastien%20Gros&entry.1292438233=%20%20Sim2Real%20aims%20at%20training%20policies%20in%20high-fidelity%20simulation%20environments%0Aand%20effectively%20transferring%20them%20to%20the%20real%20world.%20Despite%20the%20developments%0Aof%20accurate%20simulators%20and%20Sim2Real%20RL%20approaches%2C%20the%20policies%20trained%20purely%0Ain%20simulation%20often%20suffer%20significant%20performance%20drops%20when%20deployed%20in%20real%0Aenvironments.%20This%20drop%20is%20referred%20to%20as%20the%20Sim2Real%20performance%20gap.%20Current%0ASim2Real%20RL%20methods%20optimize%20the%20simulator%20accuracy%20and%20variability%20as%20proxies%0Afor%20real-world%20performance.%20However%2C%20these%20metrics%20do%20not%20necessarily%20correlate%0Awith%20the%20real-world%20performance%20of%20the%20policy%20as%20established%20theoretically%20and%0Aempirically%20in%20the%20literature.%20We%20propose%20a%20novel%20framework%20to%20address%20this%0Aissue%20by%20directly%20adapting%20the%20simulator%20parameters%20based%20on%20real-world%0Aperformance.%20We%20frame%20this%20problem%20as%20a%20bi-level%20RL%20framework%3A%20the%20inner-level%0ARL%20trains%20a%20policy%20purely%20in%20simulation%2C%20and%20the%20outer-level%20RL%20adapts%20the%0Asimulation%20model%20and%20in-sim%20reward%20parameters%20to%20maximize%20real-world%0Aperformance%20of%20the%20in-sim%20policy.%20We%20derive%20and%20validate%20in%20simple%20examples%20the%0Amathematical%20tools%20needed%20to%20develop%20bi-level%20RL%20algorithms%20that%20close%20the%0ASim2Real%20performance%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17709v1&entry.124074799=Read"},
{"title": "Solving Oscillator Ordinary Differential Equations in the Time Domain\n  with High Performance via Soft-constrained Physics-informed Neural Network\n  with Small Data", "author": "Kai-liang Lu", "abstract": "  In many scientific and engineering (e.g., physical, biochemical, medical)\npractices, data generated through expensive experiments or large-scale\nsimulations, are often sparse and noisy. Physics-informed neural network (PINN)\nincorporates physical information and knowledge into network topology or\ncomputational processes as model priors, with the unique advantage of achieving\nstrong generalization with small data. This study aims to investigate the\nperformance characteristics of the soft-constrained PINN method to solving\ntypical linear and nonlinear ordinary differential equations (ODEs) such as\nprimer, Van der Pol and Duffing oscillators, especially the effectiveness,\nefficiency, and robustness to noise with minimal data. It is verified that the\nsoft-constrained PINN significantly reduces the need for labeled data. With the\naid of appropriate collocation points no need to be labeled, it can predict and\nalso extrapolate with minimal data. First-order and second-order ODEs, no\nmatter linear or nonlinear oscillators, require only one and two training data\n(containing initial values) respectively, just like classical analytic or\nRunge-Kutta methods, and with equivalent precision and comparable efficiency\n(fast training in seconds for scalar ODEs). Furthermore, it can conveniently\nimpose a physical law (e.g., conservation of energy) constraint by adding a\nregularization term to the total loss function, improving the performance to\ndeal with various complexities such as nonlinearity like Duffing. The\nDeepXDE-based PINN implementation is light code and can be efficiently trained\non both GPU and CPU platforms. The mathematical and computational framework of\nthis alternative and feasible PINN method to ODEs, can be easily extended to\nPDEs, etc., and is becoming a favorable catalyst for the era of Digital Twins.\n", "link": "http://arxiv.org/abs/2408.11077v5", "date": "2025-10-20", "relevancy": 1.8709, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4703}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4696}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Oscillator%20Ordinary%20Differential%20Equations%20in%20the%20Time%20Domain%0A%20%20with%20High%20Performance%20via%20Soft-constrained%20Physics-informed%20Neural%20Network%0A%20%20with%20Small%20Data&body=Title%3A%20Solving%20Oscillator%20Ordinary%20Differential%20Equations%20in%20the%20Time%20Domain%0A%20%20with%20High%20Performance%20via%20Soft-constrained%20Physics-informed%20Neural%20Network%0A%20%20with%20Small%20Data%0AAuthor%3A%20Kai-liang%20Lu%0AAbstract%3A%20%20%20In%20many%20scientific%20and%20engineering%20%28e.g.%2C%20physical%2C%20biochemical%2C%20medical%29%0Apractices%2C%20data%20generated%20through%20expensive%20experiments%20or%20large-scale%0Asimulations%2C%20are%20often%20sparse%20and%20noisy.%20Physics-informed%20neural%20network%20%28PINN%29%0Aincorporates%20physical%20information%20and%20knowledge%20into%20network%20topology%20or%0Acomputational%20processes%20as%20model%20priors%2C%20with%20the%20unique%20advantage%20of%20achieving%0Astrong%20generalization%20with%20small%20data.%20This%20study%20aims%20to%20investigate%20the%0Aperformance%20characteristics%20of%20the%20soft-constrained%20PINN%20method%20to%20solving%0Atypical%20linear%20and%20nonlinear%20ordinary%20differential%20equations%20%28ODEs%29%20such%20as%0Aprimer%2C%20Van%20der%20Pol%20and%20Duffing%20oscillators%2C%20especially%20the%20effectiveness%2C%0Aefficiency%2C%20and%20robustness%20to%20noise%20with%20minimal%20data.%20It%20is%20verified%20that%20the%0Asoft-constrained%20PINN%20significantly%20reduces%20the%20need%20for%20labeled%20data.%20With%20the%0Aaid%20of%20appropriate%20collocation%20points%20no%20need%20to%20be%20labeled%2C%20it%20can%20predict%20and%0Aalso%20extrapolate%20with%20minimal%20data.%20First-order%20and%20second-order%20ODEs%2C%20no%0Amatter%20linear%20or%20nonlinear%20oscillators%2C%20require%20only%20one%20and%20two%20training%20data%0A%28containing%20initial%20values%29%20respectively%2C%20just%20like%20classical%20analytic%20or%0ARunge-Kutta%20methods%2C%20and%20with%20equivalent%20precision%20and%20comparable%20efficiency%0A%28fast%20training%20in%20seconds%20for%20scalar%20ODEs%29.%20Furthermore%2C%20it%20can%20conveniently%0Aimpose%20a%20physical%20law%20%28e.g.%2C%20conservation%20of%20energy%29%20constraint%20by%20adding%20a%0Aregularization%20term%20to%20the%20total%20loss%20function%2C%20improving%20the%20performance%20to%0Adeal%20with%20various%20complexities%20such%20as%20nonlinearity%20like%20Duffing.%20The%0ADeepXDE-based%20PINN%20implementation%20is%20light%20code%20and%20can%20be%20efficiently%20trained%0Aon%20both%20GPU%20and%20CPU%20platforms.%20The%20mathematical%20and%20computational%20framework%20of%0Athis%20alternative%20and%20feasible%20PINN%20method%20to%20ODEs%2C%20can%20be%20easily%20extended%20to%0APDEs%2C%20etc.%2C%20and%20is%20becoming%20a%20favorable%20catalyst%20for%20the%20era%20of%20Digital%20Twins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11077v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Oscillator%2520Ordinary%2520Differential%2520Equations%2520in%2520the%2520Time%2520Domain%250A%2520%2520with%2520High%2520Performance%2520via%2520Soft-constrained%2520Physics-informed%2520Neural%2520Network%250A%2520%2520with%2520Small%2520Data%26entry.906535625%3DKai-liang%2520Lu%26entry.1292438233%3D%2520%2520In%2520many%2520scientific%2520and%2520engineering%2520%2528e.g.%252C%2520physical%252C%2520biochemical%252C%2520medical%2529%250Apractices%252C%2520data%2520generated%2520through%2520expensive%2520experiments%2520or%2520large-scale%250Asimulations%252C%2520are%2520often%2520sparse%2520and%2520noisy.%2520Physics-informed%2520neural%2520network%2520%2528PINN%2529%250Aincorporates%2520physical%2520information%2520and%2520knowledge%2520into%2520network%2520topology%2520or%250Acomputational%2520processes%2520as%2520model%2520priors%252C%2520with%2520the%2520unique%2520advantage%2520of%2520achieving%250Astrong%2520generalization%2520with%2520small%2520data.%2520This%2520study%2520aims%2520to%2520investigate%2520the%250Aperformance%2520characteristics%2520of%2520the%2520soft-constrained%2520PINN%2520method%2520to%2520solving%250Atypical%2520linear%2520and%2520nonlinear%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%2520such%2520as%250Aprimer%252C%2520Van%2520der%2520Pol%2520and%2520Duffing%2520oscillators%252C%2520especially%2520the%2520effectiveness%252C%250Aefficiency%252C%2520and%2520robustness%2520to%2520noise%2520with%2520minimal%2520data.%2520It%2520is%2520verified%2520that%2520the%250Asoft-constrained%2520PINN%2520significantly%2520reduces%2520the%2520need%2520for%2520labeled%2520data.%2520With%2520the%250Aaid%2520of%2520appropriate%2520collocation%2520points%2520no%2520need%2520to%2520be%2520labeled%252C%2520it%2520can%2520predict%2520and%250Aalso%2520extrapolate%2520with%2520minimal%2520data.%2520First-order%2520and%2520second-order%2520ODEs%252C%2520no%250Amatter%2520linear%2520or%2520nonlinear%2520oscillators%252C%2520require%2520only%2520one%2520and%2520two%2520training%2520data%250A%2528containing%2520initial%2520values%2529%2520respectively%252C%2520just%2520like%2520classical%2520analytic%2520or%250ARunge-Kutta%2520methods%252C%2520and%2520with%2520equivalent%2520precision%2520and%2520comparable%2520efficiency%250A%2528fast%2520training%2520in%2520seconds%2520for%2520scalar%2520ODEs%2529.%2520Furthermore%252C%2520it%2520can%2520conveniently%250Aimpose%2520a%2520physical%2520law%2520%2528e.g.%252C%2520conservation%2520of%2520energy%2529%2520constraint%2520by%2520adding%2520a%250Aregularization%2520term%2520to%2520the%2520total%2520loss%2520function%252C%2520improving%2520the%2520performance%2520to%250Adeal%2520with%2520various%2520complexities%2520such%2520as%2520nonlinearity%2520like%2520Duffing.%2520The%250ADeepXDE-based%2520PINN%2520implementation%2520is%2520light%2520code%2520and%2520can%2520be%2520efficiently%2520trained%250Aon%2520both%2520GPU%2520and%2520CPU%2520platforms.%2520The%2520mathematical%2520and%2520computational%2520framework%2520of%250Athis%2520alternative%2520and%2520feasible%2520PINN%2520method%2520to%2520ODEs%252C%2520can%2520be%2520easily%2520extended%2520to%250APDEs%252C%2520etc.%252C%2520and%2520is%2520becoming%2520a%2520favorable%2520catalyst%2520for%2520the%2520era%2520of%2520Digital%2520Twins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11077v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Oscillator%20Ordinary%20Differential%20Equations%20in%20the%20Time%20Domain%0A%20%20with%20High%20Performance%20via%20Soft-constrained%20Physics-informed%20Neural%20Network%0A%20%20with%20Small%20Data&entry.906535625=Kai-liang%20Lu&entry.1292438233=%20%20In%20many%20scientific%20and%20engineering%20%28e.g.%2C%20physical%2C%20biochemical%2C%20medical%29%0Apractices%2C%20data%20generated%20through%20expensive%20experiments%20or%20large-scale%0Asimulations%2C%20are%20often%20sparse%20and%20noisy.%20Physics-informed%20neural%20network%20%28PINN%29%0Aincorporates%20physical%20information%20and%20knowledge%20into%20network%20topology%20or%0Acomputational%20processes%20as%20model%20priors%2C%20with%20the%20unique%20advantage%20of%20achieving%0Astrong%20generalization%20with%20small%20data.%20This%20study%20aims%20to%20investigate%20the%0Aperformance%20characteristics%20of%20the%20soft-constrained%20PINN%20method%20to%20solving%0Atypical%20linear%20and%20nonlinear%20ordinary%20differential%20equations%20%28ODEs%29%20such%20as%0Aprimer%2C%20Van%20der%20Pol%20and%20Duffing%20oscillators%2C%20especially%20the%20effectiveness%2C%0Aefficiency%2C%20and%20robustness%20to%20noise%20with%20minimal%20data.%20It%20is%20verified%20that%20the%0Asoft-constrained%20PINN%20significantly%20reduces%20the%20need%20for%20labeled%20data.%20With%20the%0Aaid%20of%20appropriate%20collocation%20points%20no%20need%20to%20be%20labeled%2C%20it%20can%20predict%20and%0Aalso%20extrapolate%20with%20minimal%20data.%20First-order%20and%20second-order%20ODEs%2C%20no%0Amatter%20linear%20or%20nonlinear%20oscillators%2C%20require%20only%20one%20and%20two%20training%20data%0A%28containing%20initial%20values%29%20respectively%2C%20just%20like%20classical%20analytic%20or%0ARunge-Kutta%20methods%2C%20and%20with%20equivalent%20precision%20and%20comparable%20efficiency%0A%28fast%20training%20in%20seconds%20for%20scalar%20ODEs%29.%20Furthermore%2C%20it%20can%20conveniently%0Aimpose%20a%20physical%20law%20%28e.g.%2C%20conservation%20of%20energy%29%20constraint%20by%20adding%20a%0Aregularization%20term%20to%20the%20total%20loss%20function%2C%20improving%20the%20performance%20to%0Adeal%20with%20various%20complexities%20such%20as%20nonlinearity%20like%20Duffing.%20The%0ADeepXDE-based%20PINN%20implementation%20is%20light%20code%20and%20can%20be%20efficiently%20trained%0Aon%20both%20GPU%20and%20CPU%20platforms.%20The%20mathematical%20and%20computational%20framework%20of%0Athis%20alternative%20and%20feasible%20PINN%20method%20to%20ODEs%2C%20can%20be%20easily%20extended%20to%0APDEs%2C%20etc.%2C%20and%20is%20becoming%20a%20favorable%20catalyst%20for%20the%20era%20of%20Digital%20Twins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11077v5&entry.124074799=Read"},
{"title": "Weak-to-Strong Generalization Even in Random Feature Networks, Provably", "author": "Marko Medvedev and Kaifeng Lyu and Dingli Yu and Sanjeev Arora and Zhiyuan Li and Nathan Srebro", "abstract": "  Weak-to-Strong Generalization (Burns et al., 2024) is the phenomenon whereby\na strong student, say GPT-4, learns a task from a weak teacher, say GPT-2, and\nends up significantly outperforming the teacher. We show that this phenomenon\ndoes not require a strong learner like GPT-4. We consider student and teacher\nthat are random feature models, described by two-layer networks with a random\nand fixed bottom layer and a trained top layer. A \"weak\" teacher, with a small\nnumber of units (i.e. random features), is trained on the population, and a\n\"strong\" student, with a much larger number of units (i.e. random features), is\ntrained only on labels generated by the weak teacher. We demonstrate, prove,\nand understand how the student can outperform the teacher, even though trained\nonly on data labeled by the teacher. We also explain how such weak-to-strong\ngeneralization is enabled by early stopping. Importantly, we also show the\nquantitative limits of weak-to-strong generalization in this model.\n", "link": "http://arxiv.org/abs/2503.02877v2", "date": "2025-10-20", "relevancy": 1.8549, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4673}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4669}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weak-to-Strong%20Generalization%20Even%20in%20Random%20Feature%20Networks%2C%20Provably&body=Title%3A%20Weak-to-Strong%20Generalization%20Even%20in%20Random%20Feature%20Networks%2C%20Provably%0AAuthor%3A%20Marko%20Medvedev%20and%20Kaifeng%20Lyu%20and%20Dingli%20Yu%20and%20Sanjeev%20Arora%20and%20Zhiyuan%20Li%20and%20Nathan%20Srebro%0AAbstract%3A%20%20%20Weak-to-Strong%20Generalization%20%28Burns%20et%20al.%2C%202024%29%20is%20the%20phenomenon%20whereby%0Aa%20strong%20student%2C%20say%20GPT-4%2C%20learns%20a%20task%20from%20a%20weak%20teacher%2C%20say%20GPT-2%2C%20and%0Aends%20up%20significantly%20outperforming%20the%20teacher.%20We%20show%20that%20this%20phenomenon%0Adoes%20not%20require%20a%20strong%20learner%20like%20GPT-4.%20We%20consider%20student%20and%20teacher%0Athat%20are%20random%20feature%20models%2C%20described%20by%20two-layer%20networks%20with%20a%20random%0Aand%20fixed%20bottom%20layer%20and%20a%20trained%20top%20layer.%20A%20%22weak%22%20teacher%2C%20with%20a%20small%0Anumber%20of%20units%20%28i.e.%20random%20features%29%2C%20is%20trained%20on%20the%20population%2C%20and%20a%0A%22strong%22%20student%2C%20with%20a%20much%20larger%20number%20of%20units%20%28i.e.%20random%20features%29%2C%20is%0Atrained%20only%20on%20labels%20generated%20by%20the%20weak%20teacher.%20We%20demonstrate%2C%20prove%2C%0Aand%20understand%20how%20the%20student%20can%20outperform%20the%20teacher%2C%20even%20though%20trained%0Aonly%20on%20data%20labeled%20by%20the%20teacher.%20We%20also%20explain%20how%20such%20weak-to-strong%0Ageneralization%20is%20enabled%20by%20early%20stopping.%20Importantly%2C%20we%20also%20show%20the%0Aquantitative%20limits%20of%20weak-to-strong%20generalization%20in%20this%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeak-to-Strong%2520Generalization%2520Even%2520in%2520Random%2520Feature%2520Networks%252C%2520Provably%26entry.906535625%3DMarko%2520Medvedev%2520and%2520Kaifeng%2520Lyu%2520and%2520Dingli%2520Yu%2520and%2520Sanjeev%2520Arora%2520and%2520Zhiyuan%2520Li%2520and%2520Nathan%2520Srebro%26entry.1292438233%3D%2520%2520Weak-to-Strong%2520Generalization%2520%2528Burns%2520et%2520al.%252C%25202024%2529%2520is%2520the%2520phenomenon%2520whereby%250Aa%2520strong%2520student%252C%2520say%2520GPT-4%252C%2520learns%2520a%2520task%2520from%2520a%2520weak%2520teacher%252C%2520say%2520GPT-2%252C%2520and%250Aends%2520up%2520significantly%2520outperforming%2520the%2520teacher.%2520We%2520show%2520that%2520this%2520phenomenon%250Adoes%2520not%2520require%2520a%2520strong%2520learner%2520like%2520GPT-4.%2520We%2520consider%2520student%2520and%2520teacher%250Athat%2520are%2520random%2520feature%2520models%252C%2520described%2520by%2520two-layer%2520networks%2520with%2520a%2520random%250Aand%2520fixed%2520bottom%2520layer%2520and%2520a%2520trained%2520top%2520layer.%2520A%2520%2522weak%2522%2520teacher%252C%2520with%2520a%2520small%250Anumber%2520of%2520units%2520%2528i.e.%2520random%2520features%2529%252C%2520is%2520trained%2520on%2520the%2520population%252C%2520and%2520a%250A%2522strong%2522%2520student%252C%2520with%2520a%2520much%2520larger%2520number%2520of%2520units%2520%2528i.e.%2520random%2520features%2529%252C%2520is%250Atrained%2520only%2520on%2520labels%2520generated%2520by%2520the%2520weak%2520teacher.%2520We%2520demonstrate%252C%2520prove%252C%250Aand%2520understand%2520how%2520the%2520student%2520can%2520outperform%2520the%2520teacher%252C%2520even%2520though%2520trained%250Aonly%2520on%2520data%2520labeled%2520by%2520the%2520teacher.%2520We%2520also%2520explain%2520how%2520such%2520weak-to-strong%250Ageneralization%2520is%2520enabled%2520by%2520early%2520stopping.%2520Importantly%252C%2520we%2520also%2520show%2520the%250Aquantitative%2520limits%2520of%2520weak-to-strong%2520generalization%2520in%2520this%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weak-to-Strong%20Generalization%20Even%20in%20Random%20Feature%20Networks%2C%20Provably&entry.906535625=Marko%20Medvedev%20and%20Kaifeng%20Lyu%20and%20Dingli%20Yu%20and%20Sanjeev%20Arora%20and%20Zhiyuan%20Li%20and%20Nathan%20Srebro&entry.1292438233=%20%20Weak-to-Strong%20Generalization%20%28Burns%20et%20al.%2C%202024%29%20is%20the%20phenomenon%20whereby%0Aa%20strong%20student%2C%20say%20GPT-4%2C%20learns%20a%20task%20from%20a%20weak%20teacher%2C%20say%20GPT-2%2C%20and%0Aends%20up%20significantly%20outperforming%20the%20teacher.%20We%20show%20that%20this%20phenomenon%0Adoes%20not%20require%20a%20strong%20learner%20like%20GPT-4.%20We%20consider%20student%20and%20teacher%0Athat%20are%20random%20feature%20models%2C%20described%20by%20two-layer%20networks%20with%20a%20random%0Aand%20fixed%20bottom%20layer%20and%20a%20trained%20top%20layer.%20A%20%22weak%22%20teacher%2C%20with%20a%20small%0Anumber%20of%20units%20%28i.e.%20random%20features%29%2C%20is%20trained%20on%20the%20population%2C%20and%20a%0A%22strong%22%20student%2C%20with%20a%20much%20larger%20number%20of%20units%20%28i.e.%20random%20features%29%2C%20is%0Atrained%20only%20on%20labels%20generated%20by%20the%20weak%20teacher.%20We%20demonstrate%2C%20prove%2C%0Aand%20understand%20how%20the%20student%20can%20outperform%20the%20teacher%2C%20even%20though%20trained%0Aonly%20on%20data%20labeled%20by%20the%20teacher.%20We%20also%20explain%20how%20such%20weak-to-strong%0Ageneralization%20is%20enabled%20by%20early%20stopping.%20Importantly%2C%20we%20also%20show%20the%0Aquantitative%20limits%20of%20weak-to-strong%20generalization%20in%20this%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02877v2&entry.124074799=Read"},
{"title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "author": "Ege Beyazit and KL Navaneet and Prashant Mathur and Roi Blanco and Vidit Bansal and Karim Bouyarmane", "abstract": "  Black-box Large Language Models (LLMs) provide practical and accessible\nalternatives to other machine learning methods, as they require minimal labeled\ndata and machine learning expertise to develop solutions for various decision\nmaking problems. However, for applications that need operating with constraints\non specific metrics (e.g., precision $\\geq$ 95%), decision making with\nblack-box LLMs remains unfavorable, due to their low numerical output\ncardinalities. This results in limited control over their operating points,\npreventing fine-grained adjustment of their decision making behavior. In this\npaper, we study using black-box LLMs as classifiers, focusing on efficiently\nimproving their operational granularity without performance loss. Specifically,\nwe first investigate the reasons behind their low-cardinality numerical outputs\nand show that they are biased towards generating rounded but informative\nverbalized probabilities. Then, we experiment with standard prompt engineering,\nuncertainty estimation and confidence elicitation techniques, and observe that\nthey do not effectively improve operational granularity without sacrificing\nperformance or increasing inference cost. Finally, we propose efficient\napproaches to significantly increase the number and diversity of available\noperating points. Our proposed approaches provide finer-grained operating\npoints and achieve comparable to or better performance than the benchmark\nmethods across 11 datasets and 3 LLMs.\n", "link": "http://arxiv.org/abs/2510.17727v2", "date": "2025-10-21", "relevancy": 1.8519, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4717}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Fine-Grained%20Operating%20Points%20for%20Black-Box%20LLMs&body=Title%3A%20Enabling%20Fine-Grained%20Operating%20Points%20for%20Black-Box%20LLMs%0AAuthor%3A%20Ege%20Beyazit%20and%20KL%20Navaneet%20and%20Prashant%20Mathur%20and%20Roi%20Blanco%20and%20Vidit%20Bansal%20and%20Karim%20Bouyarmane%0AAbstract%3A%20%20%20Black-box%20Large%20Language%20Models%20%28LLMs%29%20provide%20practical%20and%20accessible%0Aalternatives%20to%20other%20machine%20learning%20methods%2C%20as%20they%20require%20minimal%20labeled%0Adata%20and%20machine%20learning%20expertise%20to%20develop%20solutions%20for%20various%20decision%0Amaking%20problems.%20However%2C%20for%20applications%20that%20need%20operating%20with%20constraints%0Aon%20specific%20metrics%20%28e.g.%2C%20precision%20%24%5Cgeq%24%2095%25%29%2C%20decision%20making%20with%0Ablack-box%20LLMs%20remains%20unfavorable%2C%20due%20to%20their%20low%20numerical%20output%0Acardinalities.%20This%20results%20in%20limited%20control%20over%20their%20operating%20points%2C%0Apreventing%20fine-grained%20adjustment%20of%20their%20decision%20making%20behavior.%20In%20this%0Apaper%2C%20we%20study%20using%20black-box%20LLMs%20as%20classifiers%2C%20focusing%20on%20efficiently%0Aimproving%20their%20operational%20granularity%20without%20performance%20loss.%20Specifically%2C%0Awe%20first%20investigate%20the%20reasons%20behind%20their%20low-cardinality%20numerical%20outputs%0Aand%20show%20that%20they%20are%20biased%20towards%20generating%20rounded%20but%20informative%0Averbalized%20probabilities.%20Then%2C%20we%20experiment%20with%20standard%20prompt%20engineering%2C%0Auncertainty%20estimation%20and%20confidence%20elicitation%20techniques%2C%20and%20observe%20that%0Athey%20do%20not%20effectively%20improve%20operational%20granularity%20without%20sacrificing%0Aperformance%20or%20increasing%20inference%20cost.%20Finally%2C%20we%20propose%20efficient%0Aapproaches%20to%20significantly%20increase%20the%20number%20and%20diversity%20of%20available%0Aoperating%20points.%20Our%20proposed%20approaches%20provide%20finer-grained%20operating%0Apoints%20and%20achieve%20comparable%20to%20or%20better%20performance%20than%20the%20benchmark%0Amethods%20across%2011%20datasets%20and%203%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Fine-Grained%2520Operating%2520Points%2520for%2520Black-Box%2520LLMs%26entry.906535625%3DEge%2520Beyazit%2520and%2520KL%2520Navaneet%2520and%2520Prashant%2520Mathur%2520and%2520Roi%2520Blanco%2520and%2520Vidit%2520Bansal%2520and%2520Karim%2520Bouyarmane%26entry.1292438233%3D%2520%2520Black-box%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520provide%2520practical%2520and%2520accessible%250Aalternatives%2520to%2520other%2520machine%2520learning%2520methods%252C%2520as%2520they%2520require%2520minimal%2520labeled%250Adata%2520and%2520machine%2520learning%2520expertise%2520to%2520develop%2520solutions%2520for%2520various%2520decision%250Amaking%2520problems.%2520However%252C%2520for%2520applications%2520that%2520need%2520operating%2520with%2520constraints%250Aon%2520specific%2520metrics%2520%2528e.g.%252C%2520precision%2520%2524%255Cgeq%2524%252095%2525%2529%252C%2520decision%2520making%2520with%250Ablack-box%2520LLMs%2520remains%2520unfavorable%252C%2520due%2520to%2520their%2520low%2520numerical%2520output%250Acardinalities.%2520This%2520results%2520in%2520limited%2520control%2520over%2520their%2520operating%2520points%252C%250Apreventing%2520fine-grained%2520adjustment%2520of%2520their%2520decision%2520making%2520behavior.%2520In%2520this%250Apaper%252C%2520we%2520study%2520using%2520black-box%2520LLMs%2520as%2520classifiers%252C%2520focusing%2520on%2520efficiently%250Aimproving%2520their%2520operational%2520granularity%2520without%2520performance%2520loss.%2520Specifically%252C%250Awe%2520first%2520investigate%2520the%2520reasons%2520behind%2520their%2520low-cardinality%2520numerical%2520outputs%250Aand%2520show%2520that%2520they%2520are%2520biased%2520towards%2520generating%2520rounded%2520but%2520informative%250Averbalized%2520probabilities.%2520Then%252C%2520we%2520experiment%2520with%2520standard%2520prompt%2520engineering%252C%250Auncertainty%2520estimation%2520and%2520confidence%2520elicitation%2520techniques%252C%2520and%2520observe%2520that%250Athey%2520do%2520not%2520effectively%2520improve%2520operational%2520granularity%2520without%2520sacrificing%250Aperformance%2520or%2520increasing%2520inference%2520cost.%2520Finally%252C%2520we%2520propose%2520efficient%250Aapproaches%2520to%2520significantly%2520increase%2520the%2520number%2520and%2520diversity%2520of%2520available%250Aoperating%2520points.%2520Our%2520proposed%2520approaches%2520provide%2520finer-grained%2520operating%250Apoints%2520and%2520achieve%2520comparable%2520to%2520or%2520better%2520performance%2520than%2520the%2520benchmark%250Amethods%2520across%252011%2520datasets%2520and%25203%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Fine-Grained%20Operating%20Points%20for%20Black-Box%20LLMs&entry.906535625=Ege%20Beyazit%20and%20KL%20Navaneet%20and%20Prashant%20Mathur%20and%20Roi%20Blanco%20and%20Vidit%20Bansal%20and%20Karim%20Bouyarmane&entry.1292438233=%20%20Black-box%20Large%20Language%20Models%20%28LLMs%29%20provide%20practical%20and%20accessible%0Aalternatives%20to%20other%20machine%20learning%20methods%2C%20as%20they%20require%20minimal%20labeled%0Adata%20and%20machine%20learning%20expertise%20to%20develop%20solutions%20for%20various%20decision%0Amaking%20problems.%20However%2C%20for%20applications%20that%20need%20operating%20with%20constraints%0Aon%20specific%20metrics%20%28e.g.%2C%20precision%20%24%5Cgeq%24%2095%25%29%2C%20decision%20making%20with%0Ablack-box%20LLMs%20remains%20unfavorable%2C%20due%20to%20their%20low%20numerical%20output%0Acardinalities.%20This%20results%20in%20limited%20control%20over%20their%20operating%20points%2C%0Apreventing%20fine-grained%20adjustment%20of%20their%20decision%20making%20behavior.%20In%20this%0Apaper%2C%20we%20study%20using%20black-box%20LLMs%20as%20classifiers%2C%20focusing%20on%20efficiently%0Aimproving%20their%20operational%20granularity%20without%20performance%20loss.%20Specifically%2C%0Awe%20first%20investigate%20the%20reasons%20behind%20their%20low-cardinality%20numerical%20outputs%0Aand%20show%20that%20they%20are%20biased%20towards%20generating%20rounded%20but%20informative%0Averbalized%20probabilities.%20Then%2C%20we%20experiment%20with%20standard%20prompt%20engineering%2C%0Auncertainty%20estimation%20and%20confidence%20elicitation%20techniques%2C%20and%20observe%20that%0Athey%20do%20not%20effectively%20improve%20operational%20granularity%20without%20sacrificing%0Aperformance%20or%20increasing%20inference%20cost.%20Finally%2C%20we%20propose%20efficient%0Aapproaches%20to%20significantly%20increase%20the%20number%20and%20diversity%20of%20available%0Aoperating%20points.%20Our%20proposed%20approaches%20provide%20finer-grained%20operating%0Apoints%20and%20achieve%20comparable%20to%20or%20better%20performance%20than%20the%20benchmark%0Amethods%20across%2011%20datasets%20and%203%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17727v2&entry.124074799=Read"},
{"title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large\n  Language Models as Embodied Brain", "author": "Yulin Luo and Chun-Kai Fan and Menghang Dong and Jiayu Shi and Mengdi Zhao and Bo-Wen Zhang and Cheng Chi and Jiaming Liu and Gaole Dai and Rongyu Zhang and Ruichuan An and Kun Wu and Zhengping Che and Shaoxuan Xie and Guocai Yao and Zhongxia Zhao and Pengwei Wang and Guang Liu and Zhongyuan Wang and Tiejun Huang and Shanghang Zhang", "abstract": "  Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io.\n", "link": "http://arxiv.org/abs/2510.17801v1", "date": "2025-10-20", "relevancy": 1.8494, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.648}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robobench%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multimodal%20Large%0A%20%20Language%20Models%20as%20Embodied%20Brain&body=Title%3A%20Robobench%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multimodal%20Large%0A%20%20Language%20Models%20as%20Embodied%20Brain%0AAuthor%3A%20Yulin%20Luo%20and%20Chun-Kai%20Fan%20and%20Menghang%20Dong%20and%20Jiayu%20Shi%20and%20Mengdi%20Zhao%20and%20Bo-Wen%20Zhang%20and%20Cheng%20Chi%20and%20Jiaming%20Liu%20and%20Gaole%20Dai%20and%20Rongyu%20Zhang%20and%20Ruichuan%20An%20and%20Kun%20Wu%20and%20Zhengping%20Che%20and%20Shaoxuan%20Xie%20and%20Guocai%20Yao%20and%20Zhongxia%20Zhao%20and%20Pengwei%20Wang%20and%20Guang%20Liu%20and%20Zhongyuan%20Wang%20and%20Tiejun%20Huang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Building%20robots%20that%20can%20perceive%2C%20reason%2C%20and%20act%20in%20dynamic%2C%20unstructured%0Aenvironments%20remains%20a%20core%20challenge.%20Recent%20embodied%20systems%20often%20adopt%20a%0Adual-system%20paradigm%2C%20where%20System%202%20handles%20high-level%20reasoning%20while%20System%0A1%20executes%20low-level%20control.%20In%20this%20work%2C%20we%20refer%20to%20System%202%20as%20the%0Aembodied%20brain%2C%20emphasizing%20its%20role%20as%20the%20cognitive%20core%20for%20reasoning%20and%0Adecision-making%20in%20manipulation%20tasks.%20Given%20this%20role%2C%20systematic%20evaluation%0Aof%20the%20embodied%20brain%20is%20essential.%20Yet%20existing%20benchmarks%20emphasize%20execution%0Asuccess%2C%20or%20when%20targeting%20high-level%20reasoning%2C%20suffer%20from%20incomplete%0Adimensions%20and%20limited%20task%20realism%2C%20offering%20only%20a%20partial%20picture%20of%0Acognitive%20capability.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RoboBench%2C%20a%20benchmark%0Athat%20systematically%20evaluates%20multimodal%20large%20language%20models%20%28MLLMs%29%20as%0Aembodied%20brains.%20Motivated%20by%20the%20critical%20roles%20across%20the%20full%20manipulation%0Apipeline%2C%20RoboBench%20defines%20five%20dimensions-instruction%20comprehension%2C%0Aperception%20reasoning%2C%20generalized%20planning%2C%20affordance%20prediction%2C%20and%20failure%0Aanalysis-spanning%2014%20capabilities%2C%2025%20tasks%2C%20and%206092%20QA%20pairs.%20To%20ensure%0Arealism%2C%20we%20curate%20datasets%20across%20diverse%20embodiments%2C%20attribute-rich%20objects%2C%0Aand%20multi-view%20scenes%2C%20drawing%20from%20large-scale%20real%20robotic%20data.%20For%0Aplanning%2C%20RoboBench%20introduces%20an%20evaluation%20framework%2C%0AMLLM-as-world-simulator.%20It%20evaluate%20embodied%20feasibility%20by%20simulating%20whether%0Apredicted%20plans%20can%20achieve%20critical%20object-state%20changes.%20Experiments%20on%2014%0AMLLMs%20reveal%20fundamental%20limitations%3A%20difficulties%20with%20implicit%20instruction%0Acomprehension%2C%20spatiotemporal%20reasoning%2C%20cross-scenario%20planning%2C%20fine-grained%0Aaffordance%20understanding%2C%20and%20execution%20failure%20diagnosis.%20RoboBench%20provides%20a%0Acomprehensive%20scaffold%20to%20quantify%20high-level%20cognition%2C%20and%20guide%20the%0Adevelopment%20of%20next-generation%20embodied%20MLLMs.%20The%20project%20page%20is%20in%0Ahttps%3A//robo-bench.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobobench%253A%2520A%2520Comprehensive%2520Evaluation%2520Benchmark%2520for%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%2520as%2520Embodied%2520Brain%26entry.906535625%3DYulin%2520Luo%2520and%2520Chun-Kai%2520Fan%2520and%2520Menghang%2520Dong%2520and%2520Jiayu%2520Shi%2520and%2520Mengdi%2520Zhao%2520and%2520Bo-Wen%2520Zhang%2520and%2520Cheng%2520Chi%2520and%2520Jiaming%2520Liu%2520and%2520Gaole%2520Dai%2520and%2520Rongyu%2520Zhang%2520and%2520Ruichuan%2520An%2520and%2520Kun%2520Wu%2520and%2520Zhengping%2520Che%2520and%2520Shaoxuan%2520Xie%2520and%2520Guocai%2520Yao%2520and%2520Zhongxia%2520Zhao%2520and%2520Pengwei%2520Wang%2520and%2520Guang%2520Liu%2520and%2520Zhongyuan%2520Wang%2520and%2520Tiejun%2520Huang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Building%2520robots%2520that%2520can%2520perceive%252C%2520reason%252C%2520and%2520act%2520in%2520dynamic%252C%2520unstructured%250Aenvironments%2520remains%2520a%2520core%2520challenge.%2520Recent%2520embodied%2520systems%2520often%2520adopt%2520a%250Adual-system%2520paradigm%252C%2520where%2520System%25202%2520handles%2520high-level%2520reasoning%2520while%2520System%250A1%2520executes%2520low-level%2520control.%2520In%2520this%2520work%252C%2520we%2520refer%2520to%2520System%25202%2520as%2520the%250Aembodied%2520brain%252C%2520emphasizing%2520its%2520role%2520as%2520the%2520cognitive%2520core%2520for%2520reasoning%2520and%250Adecision-making%2520in%2520manipulation%2520tasks.%2520Given%2520this%2520role%252C%2520systematic%2520evaluation%250Aof%2520the%2520embodied%2520brain%2520is%2520essential.%2520Yet%2520existing%2520benchmarks%2520emphasize%2520execution%250Asuccess%252C%2520or%2520when%2520targeting%2520high-level%2520reasoning%252C%2520suffer%2520from%2520incomplete%250Adimensions%2520and%2520limited%2520task%2520realism%252C%2520offering%2520only%2520a%2520partial%2520picture%2520of%250Acognitive%2520capability.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520RoboBench%252C%2520a%2520benchmark%250Athat%2520systematically%2520evaluates%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520as%250Aembodied%2520brains.%2520Motivated%2520by%2520the%2520critical%2520roles%2520across%2520the%2520full%2520manipulation%250Apipeline%252C%2520RoboBench%2520defines%2520five%2520dimensions-instruction%2520comprehension%252C%250Aperception%2520reasoning%252C%2520generalized%2520planning%252C%2520affordance%2520prediction%252C%2520and%2520failure%250Aanalysis-spanning%252014%2520capabilities%252C%252025%2520tasks%252C%2520and%25206092%2520QA%2520pairs.%2520To%2520ensure%250Arealism%252C%2520we%2520curate%2520datasets%2520across%2520diverse%2520embodiments%252C%2520attribute-rich%2520objects%252C%250Aand%2520multi-view%2520scenes%252C%2520drawing%2520from%2520large-scale%2520real%2520robotic%2520data.%2520For%250Aplanning%252C%2520RoboBench%2520introduces%2520an%2520evaluation%2520framework%252C%250AMLLM-as-world-simulator.%2520It%2520evaluate%2520embodied%2520feasibility%2520by%2520simulating%2520whether%250Apredicted%2520plans%2520can%2520achieve%2520critical%2520object-state%2520changes.%2520Experiments%2520on%252014%250AMLLMs%2520reveal%2520fundamental%2520limitations%253A%2520difficulties%2520with%2520implicit%2520instruction%250Acomprehension%252C%2520spatiotemporal%2520reasoning%252C%2520cross-scenario%2520planning%252C%2520fine-grained%250Aaffordance%2520understanding%252C%2520and%2520execution%2520failure%2520diagnosis.%2520RoboBench%2520provides%2520a%250Acomprehensive%2520scaffold%2520to%2520quantify%2520high-level%2520cognition%252C%2520and%2520guide%2520the%250Adevelopment%2520of%2520next-generation%2520embodied%2520MLLMs.%2520The%2520project%2520page%2520is%2520in%250Ahttps%253A//robo-bench.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robobench%3A%20A%20Comprehensive%20Evaluation%20Benchmark%20for%20Multimodal%20Large%0A%20%20Language%20Models%20as%20Embodied%20Brain&entry.906535625=Yulin%20Luo%20and%20Chun-Kai%20Fan%20and%20Menghang%20Dong%20and%20Jiayu%20Shi%20and%20Mengdi%20Zhao%20and%20Bo-Wen%20Zhang%20and%20Cheng%20Chi%20and%20Jiaming%20Liu%20and%20Gaole%20Dai%20and%20Rongyu%20Zhang%20and%20Ruichuan%20An%20and%20Kun%20Wu%20and%20Zhengping%20Che%20and%20Shaoxuan%20Xie%20and%20Guocai%20Yao%20and%20Zhongxia%20Zhao%20and%20Pengwei%20Wang%20and%20Guang%20Liu%20and%20Zhongyuan%20Wang%20and%20Tiejun%20Huang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Building%20robots%20that%20can%20perceive%2C%20reason%2C%20and%20act%20in%20dynamic%2C%20unstructured%0Aenvironments%20remains%20a%20core%20challenge.%20Recent%20embodied%20systems%20often%20adopt%20a%0Adual-system%20paradigm%2C%20where%20System%202%20handles%20high-level%20reasoning%20while%20System%0A1%20executes%20low-level%20control.%20In%20this%20work%2C%20we%20refer%20to%20System%202%20as%20the%0Aembodied%20brain%2C%20emphasizing%20its%20role%20as%20the%20cognitive%20core%20for%20reasoning%20and%0Adecision-making%20in%20manipulation%20tasks.%20Given%20this%20role%2C%20systematic%20evaluation%0Aof%20the%20embodied%20brain%20is%20essential.%20Yet%20existing%20benchmarks%20emphasize%20execution%0Asuccess%2C%20or%20when%20targeting%20high-level%20reasoning%2C%20suffer%20from%20incomplete%0Adimensions%20and%20limited%20task%20realism%2C%20offering%20only%20a%20partial%20picture%20of%0Acognitive%20capability.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RoboBench%2C%20a%20benchmark%0Athat%20systematically%20evaluates%20multimodal%20large%20language%20models%20%28MLLMs%29%20as%0Aembodied%20brains.%20Motivated%20by%20the%20critical%20roles%20across%20the%20full%20manipulation%0Apipeline%2C%20RoboBench%20defines%20five%20dimensions-instruction%20comprehension%2C%0Aperception%20reasoning%2C%20generalized%20planning%2C%20affordance%20prediction%2C%20and%20failure%0Aanalysis-spanning%2014%20capabilities%2C%2025%20tasks%2C%20and%206092%20QA%20pairs.%20To%20ensure%0Arealism%2C%20we%20curate%20datasets%20across%20diverse%20embodiments%2C%20attribute-rich%20objects%2C%0Aand%20multi-view%20scenes%2C%20drawing%20from%20large-scale%20real%20robotic%20data.%20For%0Aplanning%2C%20RoboBench%20introduces%20an%20evaluation%20framework%2C%0AMLLM-as-world-simulator.%20It%20evaluate%20embodied%20feasibility%20by%20simulating%20whether%0Apredicted%20plans%20can%20achieve%20critical%20object-state%20changes.%20Experiments%20on%2014%0AMLLMs%20reveal%20fundamental%20limitations%3A%20difficulties%20with%20implicit%20instruction%0Acomprehension%2C%20spatiotemporal%20reasoning%2C%20cross-scenario%20planning%2C%20fine-grained%0Aaffordance%20understanding%2C%20and%20execution%20failure%20diagnosis.%20RoboBench%20provides%20a%0Acomprehensive%20scaffold%20to%20quantify%20high-level%20cognition%2C%20and%20guide%20the%0Adevelopment%20of%20next-generation%20embodied%20MLLMs.%20The%20project%20page%20is%20in%0Ahttps%3A//robo-bench.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17801v1&entry.124074799=Read"},
{"title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum\n  Unsupervised Anomaly Detection", "author": "Jia Guo and Shuai Lu and Lei Fan and Zelin Li and Donglin Di and Yang Song and Weihang Zhang and Wenbing Zhu and Hong Yan and Fang Chen and Huiqi Li and Hongen Liao", "abstract": "  Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.\n", "link": "http://arxiv.org/abs/2510.17611v1", "date": "2025-10-20", "relevancy": 1.6554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5643}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Dinomaly2%20Detect%20Them%20All%3A%20A%20Unified%20Framework%20for%20Full-Spectrum%0A%20%20Unsupervised%20Anomaly%20Detection&body=Title%3A%20One%20Dinomaly2%20Detect%20Them%20All%3A%20A%20Unified%20Framework%20for%20Full-Spectrum%0A%20%20Unsupervised%20Anomaly%20Detection%0AAuthor%3A%20Jia%20Guo%20and%20Shuai%20Lu%20and%20Lei%20Fan%20and%20Zelin%20Li%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Weihang%20Zhang%20and%20Wenbing%20Zhu%20and%20Hong%20Yan%20and%20Fang%20Chen%20and%20Huiqi%20Li%20and%20Hongen%20Liao%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20has%20evolved%20from%20building%20specialized%0Asingle-class%20models%20to%20unified%20multi-class%20models%2C%20yet%20existing%20multi-class%0Amodels%20significantly%20underperform%20the%20most%20advanced%20one-for-one%20counterparts.%0AMoreover%2C%20the%20field%20has%20fragmented%20into%20specialized%20methods%20tailored%20to%0Aspecific%20scenarios%20%28multi-class%2C%203D%2C%20few-shot%2C%20etc.%29%2C%20creating%20deployment%0Abarriers%20and%20highlighting%20the%20need%20for%20a%20unified%20solution.%20In%20this%20paper%2C%20we%0Apresent%20Dinomaly2%2C%20the%20first%20unified%20framework%20for%20full-spectrum%20image%20UAD%2C%0Awhich%20bridges%20the%20performance%20gap%20in%20multi-class%20models%20while%20seamlessly%0Aextending%20across%20diverse%20data%20modalities%20and%20task%20settings.%20Guided%20by%20the%20%22less%0Ais%20more%22%20philosophy%2C%20we%20demonstrate%20that%20the%20orchestration%20of%20five%20simple%0Aelement%20achieves%20superior%20performance%20in%20a%20standard%20reconstruction-based%0Aframework.%20This%20methodological%20minimalism%20enables%20natural%20extension%20across%0Adiverse%20tasks%20without%20modification%2C%20establishing%20that%20simplicity%20is%20the%0Afoundation%20of%20true%20universality.%20Extensive%20experiments%20on%2012%20UAD%20benchmarks%0Ademonstrate%20Dinomaly2%27s%20full-spectrum%20superiority%20across%20multiple%20modalities%0A%282D%2C%20multi-view%2C%20RGB-3D%2C%20RGB-IR%29%2C%20task%20settings%20%28single-class%2C%20multi-class%2C%0Ainference-unified%20multi-class%2C%20few-shot%29%20and%20application%20domains%20%28industrial%2C%0Abiological%2C%20outdoor%29.%20For%20example%2C%20our%20multi-class%20model%20achieves%20unprecedented%0A99.9%25%20and%2099.3%25%20image-level%20%28I-%29%20AUROC%20on%20MVTec-AD%20and%20VisA%20respectively.%20For%0Amulti-view%20and%20multi-modal%20inspection%2C%20Dinomaly2%20demonstrates%20state-of-the-art%0Aperformance%20with%20minimum%20adaptations.%20Moreover%2C%20using%20only%208%20normal%20examples%0Aper%20class%2C%20our%20method%20surpasses%20previous%20full-shot%20models%2C%20achieving%2098.7%25%20and%0A97.4%25%20I-AUROC%20on%20MVTec-AD%20and%20VisA.%20The%20combination%20of%20minimalistic%20design%2C%0Acomputational%20scalability%2C%20and%20universal%20applicability%20positions%20Dinomaly2%20as%20a%0Aunified%20solution%20for%20the%20full%20spectrum%20of%20real-world%20anomaly%20detection%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Dinomaly2%2520Detect%2520Them%2520All%253A%2520A%2520Unified%2520Framework%2520for%2520Full-Spectrum%250A%2520%2520Unsupervised%2520Anomaly%2520Detection%26entry.906535625%3DJia%2520Guo%2520and%2520Shuai%2520Lu%2520and%2520Lei%2520Fan%2520and%2520Zelin%2520Li%2520and%2520Donglin%2520Di%2520and%2520Yang%2520Song%2520and%2520Weihang%2520Zhang%2520and%2520Wenbing%2520Zhu%2520and%2520Hong%2520Yan%2520and%2520Fang%2520Chen%2520and%2520Huiqi%2520Li%2520and%2520Hongen%2520Liao%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520%2528UAD%2529%2520has%2520evolved%2520from%2520building%2520specialized%250Asingle-class%2520models%2520to%2520unified%2520multi-class%2520models%252C%2520yet%2520existing%2520multi-class%250Amodels%2520significantly%2520underperform%2520the%2520most%2520advanced%2520one-for-one%2520counterparts.%250AMoreover%252C%2520the%2520field%2520has%2520fragmented%2520into%2520specialized%2520methods%2520tailored%2520to%250Aspecific%2520scenarios%2520%2528multi-class%252C%25203D%252C%2520few-shot%252C%2520etc.%2529%252C%2520creating%2520deployment%250Abarriers%2520and%2520highlighting%2520the%2520need%2520for%2520a%2520unified%2520solution.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520Dinomaly2%252C%2520the%2520first%2520unified%2520framework%2520for%2520full-spectrum%2520image%2520UAD%252C%250Awhich%2520bridges%2520the%2520performance%2520gap%2520in%2520multi-class%2520models%2520while%2520seamlessly%250Aextending%2520across%2520diverse%2520data%2520modalities%2520and%2520task%2520settings.%2520Guided%2520by%2520the%2520%2522less%250Ais%2520more%2522%2520philosophy%252C%2520we%2520demonstrate%2520that%2520the%2520orchestration%2520of%2520five%2520simple%250Aelement%2520achieves%2520superior%2520performance%2520in%2520a%2520standard%2520reconstruction-based%250Aframework.%2520This%2520methodological%2520minimalism%2520enables%2520natural%2520extension%2520across%250Adiverse%2520tasks%2520without%2520modification%252C%2520establishing%2520that%2520simplicity%2520is%2520the%250Afoundation%2520of%2520true%2520universality.%2520Extensive%2520experiments%2520on%252012%2520UAD%2520benchmarks%250Ademonstrate%2520Dinomaly2%2527s%2520full-spectrum%2520superiority%2520across%2520multiple%2520modalities%250A%25282D%252C%2520multi-view%252C%2520RGB-3D%252C%2520RGB-IR%2529%252C%2520task%2520settings%2520%2528single-class%252C%2520multi-class%252C%250Ainference-unified%2520multi-class%252C%2520few-shot%2529%2520and%2520application%2520domains%2520%2528industrial%252C%250Abiological%252C%2520outdoor%2529.%2520For%2520example%252C%2520our%2520multi-class%2520model%2520achieves%2520unprecedented%250A99.9%2525%2520and%252099.3%2525%2520image-level%2520%2528I-%2529%2520AUROC%2520on%2520MVTec-AD%2520and%2520VisA%2520respectively.%2520For%250Amulti-view%2520and%2520multi-modal%2520inspection%252C%2520Dinomaly2%2520demonstrates%2520state-of-the-art%250Aperformance%2520with%2520minimum%2520adaptations.%2520Moreover%252C%2520using%2520only%25208%2520normal%2520examples%250Aper%2520class%252C%2520our%2520method%2520surpasses%2520previous%2520full-shot%2520models%252C%2520achieving%252098.7%2525%2520and%250A97.4%2525%2520I-AUROC%2520on%2520MVTec-AD%2520and%2520VisA.%2520The%2520combination%2520of%2520minimalistic%2520design%252C%250Acomputational%2520scalability%252C%2520and%2520universal%2520applicability%2520positions%2520Dinomaly2%2520as%2520a%250Aunified%2520solution%2520for%2520the%2520full%2520spectrum%2520of%2520real-world%2520anomaly%2520detection%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Dinomaly2%20Detect%20Them%20All%3A%20A%20Unified%20Framework%20for%20Full-Spectrum%0A%20%20Unsupervised%20Anomaly%20Detection&entry.906535625=Jia%20Guo%20and%20Shuai%20Lu%20and%20Lei%20Fan%20and%20Zelin%20Li%20and%20Donglin%20Di%20and%20Yang%20Song%20and%20Weihang%20Zhang%20and%20Wenbing%20Zhu%20and%20Hong%20Yan%20and%20Fang%20Chen%20and%20Huiqi%20Li%20and%20Hongen%20Liao&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20%28UAD%29%20has%20evolved%20from%20building%20specialized%0Asingle-class%20models%20to%20unified%20multi-class%20models%2C%20yet%20existing%20multi-class%0Amodels%20significantly%20underperform%20the%20most%20advanced%20one-for-one%20counterparts.%0AMoreover%2C%20the%20field%20has%20fragmented%20into%20specialized%20methods%20tailored%20to%0Aspecific%20scenarios%20%28multi-class%2C%203D%2C%20few-shot%2C%20etc.%29%2C%20creating%20deployment%0Abarriers%20and%20highlighting%20the%20need%20for%20a%20unified%20solution.%20In%20this%20paper%2C%20we%0Apresent%20Dinomaly2%2C%20the%20first%20unified%20framework%20for%20full-spectrum%20image%20UAD%2C%0Awhich%20bridges%20the%20performance%20gap%20in%20multi-class%20models%20while%20seamlessly%0Aextending%20across%20diverse%20data%20modalities%20and%20task%20settings.%20Guided%20by%20the%20%22less%0Ais%20more%22%20philosophy%2C%20we%20demonstrate%20that%20the%20orchestration%20of%20five%20simple%0Aelement%20achieves%20superior%20performance%20in%20a%20standard%20reconstruction-based%0Aframework.%20This%20methodological%20minimalism%20enables%20natural%20extension%20across%0Adiverse%20tasks%20without%20modification%2C%20establishing%20that%20simplicity%20is%20the%0Afoundation%20of%20true%20universality.%20Extensive%20experiments%20on%2012%20UAD%20benchmarks%0Ademonstrate%20Dinomaly2%27s%20full-spectrum%20superiority%20across%20multiple%20modalities%0A%282D%2C%20multi-view%2C%20RGB-3D%2C%20RGB-IR%29%2C%20task%20settings%20%28single-class%2C%20multi-class%2C%0Ainference-unified%20multi-class%2C%20few-shot%29%20and%20application%20domains%20%28industrial%2C%0Abiological%2C%20outdoor%29.%20For%20example%2C%20our%20multi-class%20model%20achieves%20unprecedented%0A99.9%25%20and%2099.3%25%20image-level%20%28I-%29%20AUROC%20on%20MVTec-AD%20and%20VisA%20respectively.%20For%0Amulti-view%20and%20multi-modal%20inspection%2C%20Dinomaly2%20demonstrates%20state-of-the-art%0Aperformance%20with%20minimum%20adaptations.%20Moreover%2C%20using%20only%208%20normal%20examples%0Aper%20class%2C%20our%20method%20surpasses%20previous%20full-shot%20models%2C%20achieving%2098.7%25%20and%0A97.4%25%20I-AUROC%20on%20MVTec-AD%20and%20VisA.%20The%20combination%20of%20minimalistic%20design%2C%0Acomputational%20scalability%2C%20and%20universal%20applicability%20positions%20Dinomaly2%20as%20a%0Aunified%20solution%20for%20the%20full%20spectrum%20of%20real-world%20anomaly%20detection%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17611v1&entry.124074799=Read"},
{"title": "Invertible ResNets for Inverse Imaging Problems: Competitive Performance\n  with Provable Regularization Properties", "author": "Clemens Arndt and Judith Nickel", "abstract": "  Learning-based methods have demonstrated remarkable performance in solving\ninverse problems, particularly in image reconstruction tasks. Despite their\nsuccess, these approaches often lack theoretical guarantees, which are crucial\nin sensitive applications such as medical imaging. Recent works by Arndt et al\naddressed this gap by analyzing a data-driven reconstruction method based on\ninvertible residual networks (iResNets). They revealed that, under reasonable\nassumptions, this approach constitutes a convergent regularization scheme.\nHowever, the performance of the reconstruction method was only validated on\nacademic toy problems and small-scale iResNet architectures. In this work, we\naddress this gap by evaluating the performance of iResNets on two real-world\nimaging tasks: a linear blurring operator and a nonlinear diffusion operator.\nTo do so, we compare the performance of iResNets against state-of-the-art\nneural networks, revealing their competitiveness at the expense of longer\ntraining times. Moreover, we numerically demonstrate the advantages of the\niResNet's inherent stability and invertibility by showcasing increased\nrobustness across various scenarios as well as interpretability of the learned\noperator, thereby reducing the black-box nature of the reconstruction scheme.\n", "link": "http://arxiv.org/abs/2409.13482v3", "date": "2025-10-20", "relevancy": 1.6311, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5634}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5389}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties&body=Title%3A%20Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties%0AAuthor%3A%20Clemens%20Arndt%20and%20Judith%20Nickel%0AAbstract%3A%20%20%20Learning-based%20methods%20have%20demonstrated%20remarkable%20performance%20in%20solving%0Ainverse%20problems%2C%20particularly%20in%20image%20reconstruction%20tasks.%20Despite%20their%0Asuccess%2C%20these%20approaches%20often%20lack%20theoretical%20guarantees%2C%20which%20are%20crucial%0Ain%20sensitive%20applications%20such%20as%20medical%20imaging.%20Recent%20works%20by%20Arndt%20et%20al%0Aaddressed%20this%20gap%20by%20analyzing%20a%20data-driven%20reconstruction%20method%20based%20on%0Ainvertible%20residual%20networks%20%28iResNets%29.%20They%20revealed%20that%2C%20under%20reasonable%0Aassumptions%2C%20this%20approach%20constitutes%20a%20convergent%20regularization%20scheme.%0AHowever%2C%20the%20performance%20of%20the%20reconstruction%20method%20was%20only%20validated%20on%0Aacademic%20toy%20problems%20and%20small-scale%20iResNet%20architectures.%20In%20this%20work%2C%20we%0Aaddress%20this%20gap%20by%20evaluating%20the%20performance%20of%20iResNets%20on%20two%20real-world%0Aimaging%20tasks%3A%20a%20linear%20blurring%20operator%20and%20a%20nonlinear%20diffusion%20operator.%0ATo%20do%20so%2C%20we%20compare%20the%20performance%20of%20iResNets%20against%20state-of-the-art%0Aneural%20networks%2C%20revealing%20their%20competitiveness%20at%20the%20expense%20of%20longer%0Atraining%20times.%20Moreover%2C%20we%20numerically%20demonstrate%20the%20advantages%20of%20the%0AiResNet%27s%20inherent%20stability%20and%20invertibility%20by%20showcasing%20increased%0Arobustness%20across%20various%20scenarios%20as%20well%20as%20interpretability%20of%20the%20learned%0Aoperator%2C%20thereby%20reducing%20the%20black-box%20nature%20of%20the%20reconstruction%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13482v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvertible%2520ResNets%2520for%2520Inverse%2520Imaging%2520Problems%253A%2520Competitive%2520Performance%250A%2520%2520with%2520Provable%2520Regularization%2520Properties%26entry.906535625%3DClemens%2520Arndt%2520and%2520Judith%2520Nickel%26entry.1292438233%3D%2520%2520Learning-based%2520methods%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520solving%250Ainverse%2520problems%252C%2520particularly%2520in%2520image%2520reconstruction%2520tasks.%2520Despite%2520their%250Asuccess%252C%2520these%2520approaches%2520often%2520lack%2520theoretical%2520guarantees%252C%2520which%2520are%2520crucial%250Ain%2520sensitive%2520applications%2520such%2520as%2520medical%2520imaging.%2520Recent%2520works%2520by%2520Arndt%2520et%2520al%250Aaddressed%2520this%2520gap%2520by%2520analyzing%2520a%2520data-driven%2520reconstruction%2520method%2520based%2520on%250Ainvertible%2520residual%2520networks%2520%2528iResNets%2529.%2520They%2520revealed%2520that%252C%2520under%2520reasonable%250Aassumptions%252C%2520this%2520approach%2520constitutes%2520a%2520convergent%2520regularization%2520scheme.%250AHowever%252C%2520the%2520performance%2520of%2520the%2520reconstruction%2520method%2520was%2520only%2520validated%2520on%250Aacademic%2520toy%2520problems%2520and%2520small-scale%2520iResNet%2520architectures.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520this%2520gap%2520by%2520evaluating%2520the%2520performance%2520of%2520iResNets%2520on%2520two%2520real-world%250Aimaging%2520tasks%253A%2520a%2520linear%2520blurring%2520operator%2520and%2520a%2520nonlinear%2520diffusion%2520operator.%250ATo%2520do%2520so%252C%2520we%2520compare%2520the%2520performance%2520of%2520iResNets%2520against%2520state-of-the-art%250Aneural%2520networks%252C%2520revealing%2520their%2520competitiveness%2520at%2520the%2520expense%2520of%2520longer%250Atraining%2520times.%2520Moreover%252C%2520we%2520numerically%2520demonstrate%2520the%2520advantages%2520of%2520the%250AiResNet%2527s%2520inherent%2520stability%2520and%2520invertibility%2520by%2520showcasing%2520increased%250Arobustness%2520across%2520various%2520scenarios%2520as%2520well%2520as%2520interpretability%2520of%2520the%2520learned%250Aoperator%252C%2520thereby%2520reducing%2520the%2520black-box%2520nature%2520of%2520the%2520reconstruction%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13482v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invertible%20ResNets%20for%20Inverse%20Imaging%20Problems%3A%20Competitive%20Performance%0A%20%20with%20Provable%20Regularization%20Properties&entry.906535625=Clemens%20Arndt%20and%20Judith%20Nickel&entry.1292438233=%20%20Learning-based%20methods%20have%20demonstrated%20remarkable%20performance%20in%20solving%0Ainverse%20problems%2C%20particularly%20in%20image%20reconstruction%20tasks.%20Despite%20their%0Asuccess%2C%20these%20approaches%20often%20lack%20theoretical%20guarantees%2C%20which%20are%20crucial%0Ain%20sensitive%20applications%20such%20as%20medical%20imaging.%20Recent%20works%20by%20Arndt%20et%20al%0Aaddressed%20this%20gap%20by%20analyzing%20a%20data-driven%20reconstruction%20method%20based%20on%0Ainvertible%20residual%20networks%20%28iResNets%29.%20They%20revealed%20that%2C%20under%20reasonable%0Aassumptions%2C%20this%20approach%20constitutes%20a%20convergent%20regularization%20scheme.%0AHowever%2C%20the%20performance%20of%20the%20reconstruction%20method%20was%20only%20validated%20on%0Aacademic%20toy%20problems%20and%20small-scale%20iResNet%20architectures.%20In%20this%20work%2C%20we%0Aaddress%20this%20gap%20by%20evaluating%20the%20performance%20of%20iResNets%20on%20two%20real-world%0Aimaging%20tasks%3A%20a%20linear%20blurring%20operator%20and%20a%20nonlinear%20diffusion%20operator.%0ATo%20do%20so%2C%20we%20compare%20the%20performance%20of%20iResNets%20against%20state-of-the-art%0Aneural%20networks%2C%20revealing%20their%20competitiveness%20at%20the%20expense%20of%20longer%0Atraining%20times.%20Moreover%2C%20we%20numerically%20demonstrate%20the%20advantages%20of%20the%0AiResNet%27s%20inherent%20stability%20and%20invertibility%20by%20showcasing%20increased%0Arobustness%20across%20various%20scenarios%20as%20well%20as%20interpretability%20of%20the%20learned%0Aoperator%2C%20thereby%20reducing%20the%20black-box%20nature%20of%20the%20reconstruction%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13482v3&entry.124074799=Read"},
{"title": "Learning Counterfactual Distributions via Kernel Nearest Neighbors", "author": "Kyuseong Choi and Jacob Feitelberg and Caleb Chin and Anish Agarwal and Raaz Dwivedi", "abstract": "  Consider a setting with multiple units (e.g., individuals, cohorts,\ngeographic locations) and outcomes (e.g., treatments, times, items), where the\ngoal is to learn a multivariate distribution for each unit-outcome entry, such\nas the distribution of a user's weekly spend and engagement under a specific\nmobile app version. A common challenge is the prevalence of missing not at\nrandom data, where observations are available only for certain unit-outcome\ncombinations and the observation availability can be correlated with the\nproperties of distributions themselves, i.e., there is unobserved confounding.\nAn additional challenge is that for any observed unit-outcome entry, we only\nhave a finite number of samples from the underlying distribution. We tackle\nthese two challenges by casting the problem into a novel distributional matrix\ncompletion framework and introduce a kernel based distributional generalization\nof nearest neighbors to estimate the underlying distributions. By leveraging\nmaximum mean discrepancies and a suitable factor model on the kernel mean\nembeddings of the underlying distributions, we establish consistent recovery of\nthe underlying distributions even when data is missing not at random and\npositivity constraints are violated. Furthermore, we demonstrate that our\nnearest neighbors approach is robust to heteroscedastic noise, provided we have\naccess to two or more measurements for the observed unit-outcome entries, a\nrobustness not present in prior works on nearest neighbors with single\nmeasurements.\n", "link": "http://arxiv.org/abs/2410.13381v3", "date": "2025-10-20", "relevancy": 1.3611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4565}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4529}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Counterfactual%20Distributions%20via%20Kernel%20Nearest%20Neighbors&body=Title%3A%20Learning%20Counterfactual%20Distributions%20via%20Kernel%20Nearest%20Neighbors%0AAuthor%3A%20Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Caleb%20Chin%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi%0AAbstract%3A%20%20%20Consider%20a%20setting%20with%20multiple%20units%20%28e.g.%2C%20individuals%2C%20cohorts%2C%0Ageographic%20locations%29%20and%20outcomes%20%28e.g.%2C%20treatments%2C%20times%2C%20items%29%2C%20where%20the%0Agoal%20is%20to%20learn%20a%20multivariate%20distribution%20for%20each%20unit-outcome%20entry%2C%20such%0Aas%20the%20distribution%20of%20a%20user%27s%20weekly%20spend%20and%20engagement%20under%20a%20specific%0Amobile%20app%20version.%20A%20common%20challenge%20is%20the%20prevalence%20of%20missing%20not%20at%0Arandom%20data%2C%20where%20observations%20are%20available%20only%20for%20certain%20unit-outcome%0Acombinations%20and%20the%20observation%20availability%20can%20be%20correlated%20with%20the%0Aproperties%20of%20distributions%20themselves%2C%20i.e.%2C%20there%20is%20unobserved%20confounding.%0AAn%20additional%20challenge%20is%20that%20for%20any%20observed%20unit-outcome%20entry%2C%20we%20only%0Ahave%20a%20finite%20number%20of%20samples%20from%20the%20underlying%20distribution.%20We%20tackle%0Athese%20two%20challenges%20by%20casting%20the%20problem%20into%20a%20novel%20distributional%20matrix%0Acompletion%20framework%20and%20introduce%20a%20kernel%20based%20distributional%20generalization%0Aof%20nearest%20neighbors%20to%20estimate%20the%20underlying%20distributions.%20By%20leveraging%0Amaximum%20mean%20discrepancies%20and%20a%20suitable%20factor%20model%20on%20the%20kernel%20mean%0Aembeddings%20of%20the%20underlying%20distributions%2C%20we%20establish%20consistent%20recovery%20of%0Athe%20underlying%20distributions%20even%20when%20data%20is%20missing%20not%20at%20random%20and%0Apositivity%20constraints%20are%20violated.%20Furthermore%2C%20we%20demonstrate%20that%20our%0Anearest%20neighbors%20approach%20is%20robust%20to%20heteroscedastic%20noise%2C%20provided%20we%20have%0Aaccess%20to%20two%20or%20more%20measurements%20for%20the%20observed%20unit-outcome%20entries%2C%20a%0Arobustness%20not%20present%20in%20prior%20works%20on%20nearest%20neighbors%20with%20single%0Ameasurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13381v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Counterfactual%2520Distributions%2520via%2520Kernel%2520Nearest%2520Neighbors%26entry.906535625%3DKyuseong%2520Choi%2520and%2520Jacob%2520Feitelberg%2520and%2520Caleb%2520Chin%2520and%2520Anish%2520Agarwal%2520and%2520Raaz%2520Dwivedi%26entry.1292438233%3D%2520%2520Consider%2520a%2520setting%2520with%2520multiple%2520units%2520%2528e.g.%252C%2520individuals%252C%2520cohorts%252C%250Ageographic%2520locations%2529%2520and%2520outcomes%2520%2528e.g.%252C%2520treatments%252C%2520times%252C%2520items%2529%252C%2520where%2520the%250Agoal%2520is%2520to%2520learn%2520a%2520multivariate%2520distribution%2520for%2520each%2520unit-outcome%2520entry%252C%2520such%250Aas%2520the%2520distribution%2520of%2520a%2520user%2527s%2520weekly%2520spend%2520and%2520engagement%2520under%2520a%2520specific%250Amobile%2520app%2520version.%2520A%2520common%2520challenge%2520is%2520the%2520prevalence%2520of%2520missing%2520not%2520at%250Arandom%2520data%252C%2520where%2520observations%2520are%2520available%2520only%2520for%2520certain%2520unit-outcome%250Acombinations%2520and%2520the%2520observation%2520availability%2520can%2520be%2520correlated%2520with%2520the%250Aproperties%2520of%2520distributions%2520themselves%252C%2520i.e.%252C%2520there%2520is%2520unobserved%2520confounding.%250AAn%2520additional%2520challenge%2520is%2520that%2520for%2520any%2520observed%2520unit-outcome%2520entry%252C%2520we%2520only%250Ahave%2520a%2520finite%2520number%2520of%2520samples%2520from%2520the%2520underlying%2520distribution.%2520We%2520tackle%250Athese%2520two%2520challenges%2520by%2520casting%2520the%2520problem%2520into%2520a%2520novel%2520distributional%2520matrix%250Acompletion%2520framework%2520and%2520introduce%2520a%2520kernel%2520based%2520distributional%2520generalization%250Aof%2520nearest%2520neighbors%2520to%2520estimate%2520the%2520underlying%2520distributions.%2520By%2520leveraging%250Amaximum%2520mean%2520discrepancies%2520and%2520a%2520suitable%2520factor%2520model%2520on%2520the%2520kernel%2520mean%250Aembeddings%2520of%2520the%2520underlying%2520distributions%252C%2520we%2520establish%2520consistent%2520recovery%2520of%250Athe%2520underlying%2520distributions%2520even%2520when%2520data%2520is%2520missing%2520not%2520at%2520random%2520and%250Apositivity%2520constraints%2520are%2520violated.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520our%250Anearest%2520neighbors%2520approach%2520is%2520robust%2520to%2520heteroscedastic%2520noise%252C%2520provided%2520we%2520have%250Aaccess%2520to%2520two%2520or%2520more%2520measurements%2520for%2520the%2520observed%2520unit-outcome%2520entries%252C%2520a%250Arobustness%2520not%2520present%2520in%2520prior%2520works%2520on%2520nearest%2520neighbors%2520with%2520single%250Ameasurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13381v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Counterfactual%20Distributions%20via%20Kernel%20Nearest%20Neighbors&entry.906535625=Kyuseong%20Choi%20and%20Jacob%20Feitelberg%20and%20Caleb%20Chin%20and%20Anish%20Agarwal%20and%20Raaz%20Dwivedi&entry.1292438233=%20%20Consider%20a%20setting%20with%20multiple%20units%20%28e.g.%2C%20individuals%2C%20cohorts%2C%0Ageographic%20locations%29%20and%20outcomes%20%28e.g.%2C%20treatments%2C%20times%2C%20items%29%2C%20where%20the%0Agoal%20is%20to%20learn%20a%20multivariate%20distribution%20for%20each%20unit-outcome%20entry%2C%20such%0Aas%20the%20distribution%20of%20a%20user%27s%20weekly%20spend%20and%20engagement%20under%20a%20specific%0Amobile%20app%20version.%20A%20common%20challenge%20is%20the%20prevalence%20of%20missing%20not%20at%0Arandom%20data%2C%20where%20observations%20are%20available%20only%20for%20certain%20unit-outcome%0Acombinations%20and%20the%20observation%20availability%20can%20be%20correlated%20with%20the%0Aproperties%20of%20distributions%20themselves%2C%20i.e.%2C%20there%20is%20unobserved%20confounding.%0AAn%20additional%20challenge%20is%20that%20for%20any%20observed%20unit-outcome%20entry%2C%20we%20only%0Ahave%20a%20finite%20number%20of%20samples%20from%20the%20underlying%20distribution.%20We%20tackle%0Athese%20two%20challenges%20by%20casting%20the%20problem%20into%20a%20novel%20distributional%20matrix%0Acompletion%20framework%20and%20introduce%20a%20kernel%20based%20distributional%20generalization%0Aof%20nearest%20neighbors%20to%20estimate%20the%20underlying%20distributions.%20By%20leveraging%0Amaximum%20mean%20discrepancies%20and%20a%20suitable%20factor%20model%20on%20the%20kernel%20mean%0Aembeddings%20of%20the%20underlying%20distributions%2C%20we%20establish%20consistent%20recovery%20of%0Athe%20underlying%20distributions%20even%20when%20data%20is%20missing%20not%20at%20random%20and%0Apositivity%20constraints%20are%20violated.%20Furthermore%2C%20we%20demonstrate%20that%20our%0Anearest%20neighbors%20approach%20is%20robust%20to%20heteroscedastic%20noise%2C%20provided%20we%20have%0Aaccess%20to%20two%20or%20more%20measurements%20for%20the%20observed%20unit-outcome%20entries%2C%20a%0Arobustness%20not%20present%20in%20prior%20works%20on%20nearest%20neighbors%20with%20single%0Ameasurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13381v3&entry.124074799=Read"},
{"title": "Denoising the Future: Top-p Distributions for Moving Through Time", "author": "Florian Andreas Marwitz and Ralf M\u00f6ller and Magnus Bender and Marcel Gehrke", "abstract": "  Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09.\n", "link": "http://arxiv.org/abs/2506.07578v3", "date": "2025-10-21", "relevancy": 1.5392, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.53}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5149}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20the%20Future%3A%20Top-p%20Distributions%20for%20Moving%20Through%20Time&body=Title%3A%20Denoising%20the%20Future%3A%20Top-p%20Distributions%20for%20Moving%20Through%20Time%0AAuthor%3A%20Florian%20Andreas%20Marwitz%20and%20Ralf%20M%C3%B6ller%20and%20Magnus%20Bender%20and%20Marcel%20Gehrke%0AAbstract%3A%20%20%20Inference%20in%20dynamic%20probabilistic%20models%20is%20a%20complex%20task%20involving%0Aexpensive%20operations.%20In%20particular%2C%20for%20Hidden%20Markov%20Models%2C%20the%20whole%20state%0Aspace%20has%20to%20be%20enumerated%20for%20advancing%20in%20time.%20Even%20states%20with%20negligible%0Aprobabilities%20are%20considered%2C%20resulting%20in%20computational%20inefficiency%20and%0Aincreased%20noise%20due%20to%20the%20propagation%20of%20unlikely%20probability%20mass.%20We%20propose%0Ato%20denoise%20the%20future%20and%20speed%20up%20inference%20by%20using%20only%20the%20top-p%20states%2C%0Ai.e.%2C%20the%20most%20probable%20states%20with%20accumulated%20probability%20p.%20We%20show%20that%20the%0Aerror%20introduced%20by%20using%20only%20the%20top-p%20states%20is%20bound%20by%20p%20and%20the%20so-called%0Aminimal%20mixing%20rate%20of%20the%20underlying%20model.%20Moreover%2C%20in%20our%20empirical%0Aevaluation%2C%20we%20show%20that%20we%20can%20expect%20speedups%20of%20at%20least%20an%20order%20of%0Amagnitude%2C%20while%20the%20error%20in%20terms%20of%20total%20variation%20distance%20is%20below%200.09.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07578v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520the%2520Future%253A%2520Top-p%2520Distributions%2520for%2520Moving%2520Through%2520Time%26entry.906535625%3DFlorian%2520Andreas%2520Marwitz%2520and%2520Ralf%2520M%25C3%25B6ller%2520and%2520Magnus%2520Bender%2520and%2520Marcel%2520Gehrke%26entry.1292438233%3D%2520%2520Inference%2520in%2520dynamic%2520probabilistic%2520models%2520is%2520a%2520complex%2520task%2520involving%250Aexpensive%2520operations.%2520In%2520particular%252C%2520for%2520Hidden%2520Markov%2520Models%252C%2520the%2520whole%2520state%250Aspace%2520has%2520to%2520be%2520enumerated%2520for%2520advancing%2520in%2520time.%2520Even%2520states%2520with%2520negligible%250Aprobabilities%2520are%2520considered%252C%2520resulting%2520in%2520computational%2520inefficiency%2520and%250Aincreased%2520noise%2520due%2520to%2520the%2520propagation%2520of%2520unlikely%2520probability%2520mass.%2520We%2520propose%250Ato%2520denoise%2520the%2520future%2520and%2520speed%2520up%2520inference%2520by%2520using%2520only%2520the%2520top-p%2520states%252C%250Ai.e.%252C%2520the%2520most%2520probable%2520states%2520with%2520accumulated%2520probability%2520p.%2520We%2520show%2520that%2520the%250Aerror%2520introduced%2520by%2520using%2520only%2520the%2520top-p%2520states%2520is%2520bound%2520by%2520p%2520and%2520the%2520so-called%250Aminimal%2520mixing%2520rate%2520of%2520the%2520underlying%2520model.%2520Moreover%252C%2520in%2520our%2520empirical%250Aevaluation%252C%2520we%2520show%2520that%2520we%2520can%2520expect%2520speedups%2520of%2520at%2520least%2520an%2520order%2520of%250Amagnitude%252C%2520while%2520the%2520error%2520in%2520terms%2520of%2520total%2520variation%2520distance%2520is%2520below%25200.09.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07578v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20the%20Future%3A%20Top-p%20Distributions%20for%20Moving%20Through%20Time&entry.906535625=Florian%20Andreas%20Marwitz%20and%20Ralf%20M%C3%B6ller%20and%20Magnus%20Bender%20and%20Marcel%20Gehrke&entry.1292438233=%20%20Inference%20in%20dynamic%20probabilistic%20models%20is%20a%20complex%20task%20involving%0Aexpensive%20operations.%20In%20particular%2C%20for%20Hidden%20Markov%20Models%2C%20the%20whole%20state%0Aspace%20has%20to%20be%20enumerated%20for%20advancing%20in%20time.%20Even%20states%20with%20negligible%0Aprobabilities%20are%20considered%2C%20resulting%20in%20computational%20inefficiency%20and%0Aincreased%20noise%20due%20to%20the%20propagation%20of%20unlikely%20probability%20mass.%20We%20propose%0Ato%20denoise%20the%20future%20and%20speed%20up%20inference%20by%20using%20only%20the%20top-p%20states%2C%0Ai.e.%2C%20the%20most%20probable%20states%20with%20accumulated%20probability%20p.%20We%20show%20that%20the%0Aerror%20introduced%20by%20using%20only%20the%20top-p%20states%20is%20bound%20by%20p%20and%20the%20so-called%0Aminimal%20mixing%20rate%20of%20the%20underlying%20model.%20Moreover%2C%20in%20our%20empirical%0Aevaluation%2C%20we%20show%20that%20we%20can%20expect%20speedups%20of%20at%20least%20an%20order%20of%0Amagnitude%2C%20while%20the%20error%20in%20terms%20of%20total%20variation%20distance%20is%20below%200.09.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07578v3&entry.124074799=Read"},
{"title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver", "author": "Aleksandr Oganov and Ilya Bykov and Eva Neudachina and Mishan Aliev and Alexander Tolmachev and Alexander Sidorov and Aleksandr Zuev and Andrey Okhotin and Denis Rakitin and Aibek Alanov", "abstract": "  While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.\n", "link": "http://arxiv.org/abs/2510.17699v1", "date": "2025-10-20", "relevancy": 1.5912, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5505}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5318}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAS%3A%20Improving%20Discretization%20of%20Diffusion%20ODEs%20via%20Generalized%0A%20%20Adversarial%20Solver&body=Title%3A%20GAS%3A%20Improving%20Discretization%20of%20Diffusion%20ODEs%20via%20Generalized%0A%20%20Adversarial%20Solver%0AAuthor%3A%20Aleksandr%20Oganov%20and%20Ilya%20Bykov%20and%20Eva%20Neudachina%20and%20Mishan%20Aliev%20and%20Alexander%20Tolmachev%20and%20Alexander%20Sidorov%20and%20Aleksandr%20Zuev%20and%20Andrey%20Okhotin%20and%20Denis%20Rakitin%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20While%20diffusion%20models%20achieve%20state-of-the-art%20generation%20quality%2C%20they%0Astill%20suffer%20from%20computationally%20expensive%20sampling.%20Recent%20works%20address%20this%0Aissue%20with%20gradient-based%20optimization%20methods%20that%20distill%20a%20few-step%20ODE%0Adiffusion%20solver%20from%20the%20full%20sampling%20process%2C%20reducing%20the%20number%20of%0Afunction%20evaluations%20from%20dozens%20to%20just%20a%20few.%20However%2C%20these%20approaches%20often%0Arely%20on%20intricate%20training%20techniques%20and%20do%20not%20explicitly%20focus%20on%20preserving%0Afine-grained%20details.%20In%20this%20paper%2C%20we%20introduce%20the%20Generalized%20Solver%3A%20a%0Asimple%20parameterization%20of%20the%20ODE%20sampler%20that%20does%20not%20require%20additional%0Atraining%20tricks%20and%20improves%20quality%20over%20existing%20approaches.%20We%20further%0Acombine%20the%20original%20distillation%20loss%20with%20adversarial%20training%2C%20which%0Amitigates%20artifacts%20and%20enhances%20detail%20fidelity.%20We%20call%20the%20resulting%20method%0Athe%20Generalized%20Adversarial%20Solver%20and%20demonstrate%20its%20superior%20performance%0Acompared%20to%20existing%20solver%20training%20methods%20under%20similar%20resource%0Aconstraints.%20Code%20is%20available%20at%20https%3A//github.com/3145tttt/GAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAS%253A%2520Improving%2520Discretization%2520of%2520Diffusion%2520ODEs%2520via%2520Generalized%250A%2520%2520Adversarial%2520Solver%26entry.906535625%3DAleksandr%2520Oganov%2520and%2520Ilya%2520Bykov%2520and%2520Eva%2520Neudachina%2520and%2520Mishan%2520Aliev%2520and%2520Alexander%2520Tolmachev%2520and%2520Alexander%2520Sidorov%2520and%2520Aleksandr%2520Zuev%2520and%2520Andrey%2520Okhotin%2520and%2520Denis%2520Rakitin%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520While%2520diffusion%2520models%2520achieve%2520state-of-the-art%2520generation%2520quality%252C%2520they%250Astill%2520suffer%2520from%2520computationally%2520expensive%2520sampling.%2520Recent%2520works%2520address%2520this%250Aissue%2520with%2520gradient-based%2520optimization%2520methods%2520that%2520distill%2520a%2520few-step%2520ODE%250Adiffusion%2520solver%2520from%2520the%2520full%2520sampling%2520process%252C%2520reducing%2520the%2520number%2520of%250Afunction%2520evaluations%2520from%2520dozens%2520to%2520just%2520a%2520few.%2520However%252C%2520these%2520approaches%2520often%250Arely%2520on%2520intricate%2520training%2520techniques%2520and%2520do%2520not%2520explicitly%2520focus%2520on%2520preserving%250Afine-grained%2520details.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Generalized%2520Solver%253A%2520a%250Asimple%2520parameterization%2520of%2520the%2520ODE%2520sampler%2520that%2520does%2520not%2520require%2520additional%250Atraining%2520tricks%2520and%2520improves%2520quality%2520over%2520existing%2520approaches.%2520We%2520further%250Acombine%2520the%2520original%2520distillation%2520loss%2520with%2520adversarial%2520training%252C%2520which%250Amitigates%2520artifacts%2520and%2520enhances%2520detail%2520fidelity.%2520We%2520call%2520the%2520resulting%2520method%250Athe%2520Generalized%2520Adversarial%2520Solver%2520and%2520demonstrate%2520its%2520superior%2520performance%250Acompared%2520to%2520existing%2520solver%2520training%2520methods%2520under%2520similar%2520resource%250Aconstraints.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/3145tttt/GAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAS%3A%20Improving%20Discretization%20of%20Diffusion%20ODEs%20via%20Generalized%0A%20%20Adversarial%20Solver&entry.906535625=Aleksandr%20Oganov%20and%20Ilya%20Bykov%20and%20Eva%20Neudachina%20and%20Mishan%20Aliev%20and%20Alexander%20Tolmachev%20and%20Alexander%20Sidorov%20and%20Aleksandr%20Zuev%20and%20Andrey%20Okhotin%20and%20Denis%20Rakitin%20and%20Aibek%20Alanov&entry.1292438233=%20%20While%20diffusion%20models%20achieve%20state-of-the-art%20generation%20quality%2C%20they%0Astill%20suffer%20from%20computationally%20expensive%20sampling.%20Recent%20works%20address%20this%0Aissue%20with%20gradient-based%20optimization%20methods%20that%20distill%20a%20few-step%20ODE%0Adiffusion%20solver%20from%20the%20full%20sampling%20process%2C%20reducing%20the%20number%20of%0Afunction%20evaluations%20from%20dozens%20to%20just%20a%20few.%20However%2C%20these%20approaches%20often%0Arely%20on%20intricate%20training%20techniques%20and%20do%20not%20explicitly%20focus%20on%20preserving%0Afine-grained%20details.%20In%20this%20paper%2C%20we%20introduce%20the%20Generalized%20Solver%3A%20a%0Asimple%20parameterization%20of%20the%20ODE%20sampler%20that%20does%20not%20require%20additional%0Atraining%20tricks%20and%20improves%20quality%20over%20existing%20approaches.%20We%20further%0Acombine%20the%20original%20distillation%20loss%20with%20adversarial%20training%2C%20which%0Amitigates%20artifacts%20and%20enhances%20detail%20fidelity.%20We%20call%20the%20resulting%20method%0Athe%20Generalized%20Adversarial%20Solver%20and%20demonstrate%20its%20superior%20performance%0Acompared%20to%20existing%20solver%20training%20methods%20under%20similar%20resource%0Aconstraints.%20Code%20is%20available%20at%20https%3A//github.com/3145tttt/GAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17699v1&entry.124074799=Read"},
{"title": "Reflections from Research Roundtables at the Conference on Health,\n  Inference, and Learning (CHIL) 2025", "author": "Emily Alsentzer and Marie-Laure Charpignon and Bill Chen and Niharika D'Souza and Jason Fries and Yixing Jiang and Aparajita Kashyap and Chanwoo Kim and Simon Lee and Aishwarya Mandyam and Ashery Christopher Mbilinyi and Nikita Mehandru and Nitish Nagesh and Brighton Nuwagira and Emma Pierson and Arvind Pillai and Akane Sano and Tanveer Syeda-Mahmood and Shashank Yadav and Elias Adhanom and Muhammad Umar Afza and Amelia Archer and Suhana Bedi and Vasiliki Bikia and Trenton Chang and George H. Chen and Winston Chen and Erica Chiang and Edward Choi and Octavia Ciora and Paz Dozie-Nnamah and Shaza Elsharief and Matthew Engelhard and Ali Eshragh and Jean Feng and Josh Fessel and Scott Fleming and Kei Sen Fong and Thomas Frost and Soham Gadgil and Judy Gichoya and Leeor Hershkovich and Sujeong Im and Bhavya Jain and Vincent Jeanselme and Furong Jia and Qixuan Jin and Yuxuan Jin and Daniel Kapash and Geetika Kapoor and Behdokht Kiafar and Matthias Kleiner and Stefan Kraft and Annika Kumar and Daeun Kyung and Zhongyuan Liang and Joanna Lin and Qianchu Liu and Chang Liu and Hongzhou Luan and Chris Lunt and Leopoldo Jul\u00edan Lechuga L\u00f3pez and Matthew B. A. McDermott and Shahriar Noroozizadeh and Connor O'Brien and YongKyung Oh and Mixail Ota and Stephen Pfohl and Meagan Pi and Tanmoy Sarkar Pias and Emma Rocheteau and Avishaan Sethi and Toru Shirakawa and Anita Silver and Neha Simha and Kamile Stankeviciute and Max Sunog and Peter Szolovits and Shengpu Tang and Jialu Tang and Aaron Tierney and John Valdovinos and Byron Wallace and Will Ke Wang and Peter Washington and Jeremy Weiss and Daniel Wolfe and Emily Wong and Hye Sun Yun and Xiaoman Zhang and Xiao Yu Cindy Zhang and Hayoung Jeong and Kaveri A. Thakoor", "abstract": "  The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),\nhosted by the Association for Health Learning and Inference (AHLI), was held in\nperson on June 25-27, 2025, at the University of California, Berkeley, in\nBerkeley, California, USA. As part of this year's program, we hosted Research\nRoundtables to catalyze collaborative, small-group dialogue around critical,\ntimely topics at the intersection of machine learning and healthcare. Each\nroundtable was moderated by a team of senior and junior chairs who fostered\nopen exchange, intellectual curiosity, and inclusive engagement. The sessions\nemphasized rigorous discussion of key challenges, exploration of emerging\nopportunities, and collective ideation toward actionable directions in the\nfield. In total, eight roundtables were held by 19 roundtable chairs on topics\nof \"Explainability, Interpretability, and Transparency,\" \"Uncertainty, Bias,\nand Fairness,\" \"Causality,\" \"Domain Adaptation,\" \"Foundation Models,\" \"Learning\nfrom Small Medical Data,\" \"Multimodal Methods,\" and \"Scalable, Translational\nHealthcare Solutions.\"\n", "link": "http://arxiv.org/abs/2510.15217v2", "date": "2025-10-20", "relevancy": 1.5851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3989}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflections%20from%20Research%20Roundtables%20at%20the%20Conference%20on%20Health%2C%0A%20%20Inference%2C%20and%20Learning%20%28CHIL%29%202025&body=Title%3A%20Reflections%20from%20Research%20Roundtables%20at%20the%20Conference%20on%20Health%2C%0A%20%20Inference%2C%20and%20Learning%20%28CHIL%29%202025%0AAuthor%3A%20Emily%20Alsentzer%20and%20Marie-Laure%20Charpignon%20and%20Bill%20Chen%20and%20Niharika%20D%27Souza%20and%20Jason%20Fries%20and%20Yixing%20Jiang%20and%20Aparajita%20Kashyap%20and%20Chanwoo%20Kim%20and%20Simon%20Lee%20and%20Aishwarya%20Mandyam%20and%20Ashery%20Christopher%20Mbilinyi%20and%20Nikita%20Mehandru%20and%20Nitish%20Nagesh%20and%20Brighton%20Nuwagira%20and%20Emma%20Pierson%20and%20Arvind%20Pillai%20and%20Akane%20Sano%20and%20Tanveer%20Syeda-Mahmood%20and%20Shashank%20Yadav%20and%20Elias%20Adhanom%20and%20Muhammad%20Umar%20Afza%20and%20Amelia%20Archer%20and%20Suhana%20Bedi%20and%20Vasiliki%20Bikia%20and%20Trenton%20Chang%20and%20George%20H.%20Chen%20and%20Winston%20Chen%20and%20Erica%20Chiang%20and%20Edward%20Choi%20and%20Octavia%20Ciora%20and%20Paz%20Dozie-Nnamah%20and%20Shaza%20Elsharief%20and%20Matthew%20Engelhard%20and%20Ali%20Eshragh%20and%20Jean%20Feng%20and%20Josh%20Fessel%20and%20Scott%20Fleming%20and%20Kei%20Sen%20Fong%20and%20Thomas%20Frost%20and%20Soham%20Gadgil%20and%20Judy%20Gichoya%20and%20Leeor%20Hershkovich%20and%20Sujeong%20Im%20and%20Bhavya%20Jain%20and%20Vincent%20Jeanselme%20and%20Furong%20Jia%20and%20Qixuan%20Jin%20and%20Yuxuan%20Jin%20and%20Daniel%20Kapash%20and%20Geetika%20Kapoor%20and%20Behdokht%20Kiafar%20and%20Matthias%20Kleiner%20and%20Stefan%20Kraft%20and%20Annika%20Kumar%20and%20Daeun%20Kyung%20and%20Zhongyuan%20Liang%20and%20Joanna%20Lin%20and%20Qianchu%20Liu%20and%20Chang%20Liu%20and%20Hongzhou%20Luan%20and%20Chris%20Lunt%20and%20Leopoldo%20Jul%C3%ADan%20Lechuga%20L%C3%B3pez%20and%20Matthew%20B.%20A.%20McDermott%20and%20Shahriar%20Noroozizadeh%20and%20Connor%20O%27Brien%20and%20YongKyung%20Oh%20and%20Mixail%20Ota%20and%20Stephen%20Pfohl%20and%20Meagan%20Pi%20and%20Tanmoy%20Sarkar%20Pias%20and%20Emma%20Rocheteau%20and%20Avishaan%20Sethi%20and%20Toru%20Shirakawa%20and%20Anita%20Silver%20and%20Neha%20Simha%20and%20Kamile%20Stankeviciute%20and%20Max%20Sunog%20and%20Peter%20Szolovits%20and%20Shengpu%20Tang%20and%20Jialu%20Tang%20and%20Aaron%20Tierney%20and%20John%20Valdovinos%20and%20Byron%20Wallace%20and%20Will%20Ke%20Wang%20and%20Peter%20Washington%20and%20Jeremy%20Weiss%20and%20Daniel%20Wolfe%20and%20Emily%20Wong%20and%20Hye%20Sun%20Yun%20and%20Xiaoman%20Zhang%20and%20Xiao%20Yu%20Cindy%20Zhang%20and%20Hayoung%20Jeong%20and%20Kaveri%20A.%20Thakoor%0AAbstract%3A%20%20%20The%206th%20Annual%20Conference%20on%20Health%2C%20Inference%2C%20and%20Learning%20%28CHIL%202025%29%2C%0Ahosted%20by%20the%20Association%20for%20Health%20Learning%20and%20Inference%20%28AHLI%29%2C%20was%20held%20in%0Aperson%20on%20June%2025-27%2C%202025%2C%20at%20the%20University%20of%20California%2C%20Berkeley%2C%20in%0ABerkeley%2C%20California%2C%20USA.%20As%20part%20of%20this%20year%27s%20program%2C%20we%20hosted%20Research%0ARoundtables%20to%20catalyze%20collaborative%2C%20small-group%20dialogue%20around%20critical%2C%0Atimely%20topics%20at%20the%20intersection%20of%20machine%20learning%20and%20healthcare.%20Each%0Aroundtable%20was%20moderated%20by%20a%20team%20of%20senior%20and%20junior%20chairs%20who%20fostered%0Aopen%20exchange%2C%20intellectual%20curiosity%2C%20and%20inclusive%20engagement.%20The%20sessions%0Aemphasized%20rigorous%20discussion%20of%20key%20challenges%2C%20exploration%20of%20emerging%0Aopportunities%2C%20and%20collective%20ideation%20toward%20actionable%20directions%20in%20the%0Afield.%20In%20total%2C%20eight%20roundtables%20were%20held%20by%2019%20roundtable%20chairs%20on%20topics%0Aof%20%22Explainability%2C%20Interpretability%2C%20and%20Transparency%2C%22%20%22Uncertainty%2C%20Bias%2C%0Aand%20Fairness%2C%22%20%22Causality%2C%22%20%22Domain%20Adaptation%2C%22%20%22Foundation%20Models%2C%22%20%22Learning%0Afrom%20Small%20Medical%20Data%2C%22%20%22Multimodal%20Methods%2C%22%20and%20%22Scalable%2C%20Translational%0AHealthcare%20Solutions.%22%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.15217v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflections%2520from%2520Research%2520Roundtables%2520at%2520the%2520Conference%2520on%2520Health%252C%250A%2520%2520Inference%252C%2520and%2520Learning%2520%2528CHIL%2529%25202025%26entry.906535625%3DEmily%2520Alsentzer%2520and%2520Marie-Laure%2520Charpignon%2520and%2520Bill%2520Chen%2520and%2520Niharika%2520D%2527Souza%2520and%2520Jason%2520Fries%2520and%2520Yixing%2520Jiang%2520and%2520Aparajita%2520Kashyap%2520and%2520Chanwoo%2520Kim%2520and%2520Simon%2520Lee%2520and%2520Aishwarya%2520Mandyam%2520and%2520Ashery%2520Christopher%2520Mbilinyi%2520and%2520Nikita%2520Mehandru%2520and%2520Nitish%2520Nagesh%2520and%2520Brighton%2520Nuwagira%2520and%2520Emma%2520Pierson%2520and%2520Arvind%2520Pillai%2520and%2520Akane%2520Sano%2520and%2520Tanveer%2520Syeda-Mahmood%2520and%2520Shashank%2520Yadav%2520and%2520Elias%2520Adhanom%2520and%2520Muhammad%2520Umar%2520Afza%2520and%2520Amelia%2520Archer%2520and%2520Suhana%2520Bedi%2520and%2520Vasiliki%2520Bikia%2520and%2520Trenton%2520Chang%2520and%2520George%2520H.%2520Chen%2520and%2520Winston%2520Chen%2520and%2520Erica%2520Chiang%2520and%2520Edward%2520Choi%2520and%2520Octavia%2520Ciora%2520and%2520Paz%2520Dozie-Nnamah%2520and%2520Shaza%2520Elsharief%2520and%2520Matthew%2520Engelhard%2520and%2520Ali%2520Eshragh%2520and%2520Jean%2520Feng%2520and%2520Josh%2520Fessel%2520and%2520Scott%2520Fleming%2520and%2520Kei%2520Sen%2520Fong%2520and%2520Thomas%2520Frost%2520and%2520Soham%2520Gadgil%2520and%2520Judy%2520Gichoya%2520and%2520Leeor%2520Hershkovich%2520and%2520Sujeong%2520Im%2520and%2520Bhavya%2520Jain%2520and%2520Vincent%2520Jeanselme%2520and%2520Furong%2520Jia%2520and%2520Qixuan%2520Jin%2520and%2520Yuxuan%2520Jin%2520and%2520Daniel%2520Kapash%2520and%2520Geetika%2520Kapoor%2520and%2520Behdokht%2520Kiafar%2520and%2520Matthias%2520Kleiner%2520and%2520Stefan%2520Kraft%2520and%2520Annika%2520Kumar%2520and%2520Daeun%2520Kyung%2520and%2520Zhongyuan%2520Liang%2520and%2520Joanna%2520Lin%2520and%2520Qianchu%2520Liu%2520and%2520Chang%2520Liu%2520and%2520Hongzhou%2520Luan%2520and%2520Chris%2520Lunt%2520and%2520Leopoldo%2520Jul%25C3%25ADan%2520Lechuga%2520L%25C3%25B3pez%2520and%2520Matthew%2520B.%2520A.%2520McDermott%2520and%2520Shahriar%2520Noroozizadeh%2520and%2520Connor%2520O%2527Brien%2520and%2520YongKyung%2520Oh%2520and%2520Mixail%2520Ota%2520and%2520Stephen%2520Pfohl%2520and%2520Meagan%2520Pi%2520and%2520Tanmoy%2520Sarkar%2520Pias%2520and%2520Emma%2520Rocheteau%2520and%2520Avishaan%2520Sethi%2520and%2520Toru%2520Shirakawa%2520and%2520Anita%2520Silver%2520and%2520Neha%2520Simha%2520and%2520Kamile%2520Stankeviciute%2520and%2520Max%2520Sunog%2520and%2520Peter%2520Szolovits%2520and%2520Shengpu%2520Tang%2520and%2520Jialu%2520Tang%2520and%2520Aaron%2520Tierney%2520and%2520John%2520Valdovinos%2520and%2520Byron%2520Wallace%2520and%2520Will%2520Ke%2520Wang%2520and%2520Peter%2520Washington%2520and%2520Jeremy%2520Weiss%2520and%2520Daniel%2520Wolfe%2520and%2520Emily%2520Wong%2520and%2520Hye%2520Sun%2520Yun%2520and%2520Xiaoman%2520Zhang%2520and%2520Xiao%2520Yu%2520Cindy%2520Zhang%2520and%2520Hayoung%2520Jeong%2520and%2520Kaveri%2520A.%2520Thakoor%26entry.1292438233%3D%2520%2520The%25206th%2520Annual%2520Conference%2520on%2520Health%252C%2520Inference%252C%2520and%2520Learning%2520%2528CHIL%25202025%2529%252C%250Ahosted%2520by%2520the%2520Association%2520for%2520Health%2520Learning%2520and%2520Inference%2520%2528AHLI%2529%252C%2520was%2520held%2520in%250Aperson%2520on%2520June%252025-27%252C%25202025%252C%2520at%2520the%2520University%2520of%2520California%252C%2520Berkeley%252C%2520in%250ABerkeley%252C%2520California%252C%2520USA.%2520As%2520part%2520of%2520this%2520year%2527s%2520program%252C%2520we%2520hosted%2520Research%250ARoundtables%2520to%2520catalyze%2520collaborative%252C%2520small-group%2520dialogue%2520around%2520critical%252C%250Atimely%2520topics%2520at%2520the%2520intersection%2520of%2520machine%2520learning%2520and%2520healthcare.%2520Each%250Aroundtable%2520was%2520moderated%2520by%2520a%2520team%2520of%2520senior%2520and%2520junior%2520chairs%2520who%2520fostered%250Aopen%2520exchange%252C%2520intellectual%2520curiosity%252C%2520and%2520inclusive%2520engagement.%2520The%2520sessions%250Aemphasized%2520rigorous%2520discussion%2520of%2520key%2520challenges%252C%2520exploration%2520of%2520emerging%250Aopportunities%252C%2520and%2520collective%2520ideation%2520toward%2520actionable%2520directions%2520in%2520the%250Afield.%2520In%2520total%252C%2520eight%2520roundtables%2520were%2520held%2520by%252019%2520roundtable%2520chairs%2520on%2520topics%250Aof%2520%2522Explainability%252C%2520Interpretability%252C%2520and%2520Transparency%252C%2522%2520%2522Uncertainty%252C%2520Bias%252C%250Aand%2520Fairness%252C%2522%2520%2522Causality%252C%2522%2520%2522Domain%2520Adaptation%252C%2522%2520%2522Foundation%2520Models%252C%2522%2520%2522Learning%250Afrom%2520Small%2520Medical%2520Data%252C%2522%2520%2522Multimodal%2520Methods%252C%2522%2520and%2520%2522Scalable%252C%2520Translational%250AHealthcare%2520Solutions.%2522%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15217v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflections%20from%20Research%20Roundtables%20at%20the%20Conference%20on%20Health%2C%0A%20%20Inference%2C%20and%20Learning%20%28CHIL%29%202025&entry.906535625=Emily%20Alsentzer%20and%20Marie-Laure%20Charpignon%20and%20Bill%20Chen%20and%20Niharika%20D%27Souza%20and%20Jason%20Fries%20and%20Yixing%20Jiang%20and%20Aparajita%20Kashyap%20and%20Chanwoo%20Kim%20and%20Simon%20Lee%20and%20Aishwarya%20Mandyam%20and%20Ashery%20Christopher%20Mbilinyi%20and%20Nikita%20Mehandru%20and%20Nitish%20Nagesh%20and%20Brighton%20Nuwagira%20and%20Emma%20Pierson%20and%20Arvind%20Pillai%20and%20Akane%20Sano%20and%20Tanveer%20Syeda-Mahmood%20and%20Shashank%20Yadav%20and%20Elias%20Adhanom%20and%20Muhammad%20Umar%20Afza%20and%20Amelia%20Archer%20and%20Suhana%20Bedi%20and%20Vasiliki%20Bikia%20and%20Trenton%20Chang%20and%20George%20H.%20Chen%20and%20Winston%20Chen%20and%20Erica%20Chiang%20and%20Edward%20Choi%20and%20Octavia%20Ciora%20and%20Paz%20Dozie-Nnamah%20and%20Shaza%20Elsharief%20and%20Matthew%20Engelhard%20and%20Ali%20Eshragh%20and%20Jean%20Feng%20and%20Josh%20Fessel%20and%20Scott%20Fleming%20and%20Kei%20Sen%20Fong%20and%20Thomas%20Frost%20and%20Soham%20Gadgil%20and%20Judy%20Gichoya%20and%20Leeor%20Hershkovich%20and%20Sujeong%20Im%20and%20Bhavya%20Jain%20and%20Vincent%20Jeanselme%20and%20Furong%20Jia%20and%20Qixuan%20Jin%20and%20Yuxuan%20Jin%20and%20Daniel%20Kapash%20and%20Geetika%20Kapoor%20and%20Behdokht%20Kiafar%20and%20Matthias%20Kleiner%20and%20Stefan%20Kraft%20and%20Annika%20Kumar%20and%20Daeun%20Kyung%20and%20Zhongyuan%20Liang%20and%20Joanna%20Lin%20and%20Qianchu%20Liu%20and%20Chang%20Liu%20and%20Hongzhou%20Luan%20and%20Chris%20Lunt%20and%20Leopoldo%20Jul%C3%ADan%20Lechuga%20L%C3%B3pez%20and%20Matthew%20B.%20A.%20McDermott%20and%20Shahriar%20Noroozizadeh%20and%20Connor%20O%27Brien%20and%20YongKyung%20Oh%20and%20Mixail%20Ota%20and%20Stephen%20Pfohl%20and%20Meagan%20Pi%20and%20Tanmoy%20Sarkar%20Pias%20and%20Emma%20Rocheteau%20and%20Avishaan%20Sethi%20and%20Toru%20Shirakawa%20and%20Anita%20Silver%20and%20Neha%20Simha%20and%20Kamile%20Stankeviciute%20and%20Max%20Sunog%20and%20Peter%20Szolovits%20and%20Shengpu%20Tang%20and%20Jialu%20Tang%20and%20Aaron%20Tierney%20and%20John%20Valdovinos%20and%20Byron%20Wallace%20and%20Will%20Ke%20Wang%20and%20Peter%20Washington%20and%20Jeremy%20Weiss%20and%20Daniel%20Wolfe%20and%20Emily%20Wong%20and%20Hye%20Sun%20Yun%20and%20Xiaoman%20Zhang%20and%20Xiao%20Yu%20Cindy%20Zhang%20and%20Hayoung%20Jeong%20and%20Kaveri%20A.%20Thakoor&entry.1292438233=%20%20The%206th%20Annual%20Conference%20on%20Health%2C%20Inference%2C%20and%20Learning%20%28CHIL%202025%29%2C%0Ahosted%20by%20the%20Association%20for%20Health%20Learning%20and%20Inference%20%28AHLI%29%2C%20was%20held%20in%0Aperson%20on%20June%2025-27%2C%202025%2C%20at%20the%20University%20of%20California%2C%20Berkeley%2C%20in%0ABerkeley%2C%20California%2C%20USA.%20As%20part%20of%20this%20year%27s%20program%2C%20we%20hosted%20Research%0ARoundtables%20to%20catalyze%20collaborative%2C%20small-group%20dialogue%20around%20critical%2C%0Atimely%20topics%20at%20the%20intersection%20of%20machine%20learning%20and%20healthcare.%20Each%0Aroundtable%20was%20moderated%20by%20a%20team%20of%20senior%20and%20junior%20chairs%20who%20fostered%0Aopen%20exchange%2C%20intellectual%20curiosity%2C%20and%20inclusive%20engagement.%20The%20sessions%0Aemphasized%20rigorous%20discussion%20of%20key%20challenges%2C%20exploration%20of%20emerging%0Aopportunities%2C%20and%20collective%20ideation%20toward%20actionable%20directions%20in%20the%0Afield.%20In%20total%2C%20eight%20roundtables%20were%20held%20by%2019%20roundtable%20chairs%20on%20topics%0Aof%20%22Explainability%2C%20Interpretability%2C%20and%20Transparency%2C%22%20%22Uncertainty%2C%20Bias%2C%0Aand%20Fairness%2C%22%20%22Causality%2C%22%20%22Domain%20Adaptation%2C%22%20%22Foundation%20Models%2C%22%20%22Learning%0Afrom%20Small%20Medical%20Data%2C%22%20%22Multimodal%20Methods%2C%22%20and%20%22Scalable%2C%20Translational%0AHealthcare%20Solutions.%22%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.15217v2&entry.124074799=Read"},
{"title": "LeapFactual: Reliable Visual Counterfactual Explanation Using\n  Conditional Flow Matching", "author": "Zhuo Cao and Xuan Zhao and Lena Krieger and Hanno Scharr and Ira Assent", "abstract": "  The growing integration of machine learning (ML) and artificial intelligence\n(AI) models into high-stakes domains such as healthcare and scientific research\ncalls for models that are not only accurate but also interpretable. Among the\nexisting explainable methods, counterfactual explanations offer\ninterpretability by identifying minimal changes to inputs that would alter a\nmodel's prediction, thus providing deeper insights. However, current\ncounterfactual generation methods suffer from critical limitations, including\ngradient vanishing, discontinuous latent spaces, and an overreliance on the\nalignment between learned and true decision boundaries. To overcome these\nlimitations, we propose LeapFactual, a novel counterfactual explanation\nalgorithm based on conditional flow matching. LeapFactual generates reliable\nand informative counterfactuals, even when true and learned decision boundaries\ndiverge. Following a model-agnostic approach, LeapFactual is not limited to\nmodels with differentiable loss functions. It can even handle human-in-the-loop\nsystems, expanding the scope of counterfactual explanations to domains that\nrequire the participation of human annotators, such as citizen science. We\nprovide extensive experiments on benchmark and real-world datasets showing that\nLeapFactual generates accurate and in-distribution counterfactual explanations\nthat offer actionable insights. We observe, for instance, that our reliable\ncounterfactual samples with labels aligning to ground truth can be beneficially\nused as new training data to enhance the model. The proposed method is broadly\napplicable and enhances both scientific knowledge discovery and non-expert\ninterpretability.\n", "link": "http://arxiv.org/abs/2510.14623v2", "date": "2025-10-20", "relevancy": 1.5555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5213}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5178}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeapFactual%3A%20Reliable%20Visual%20Counterfactual%20Explanation%20Using%0A%20%20Conditional%20Flow%20Matching&body=Title%3A%20LeapFactual%3A%20Reliable%20Visual%20Counterfactual%20Explanation%20Using%0A%20%20Conditional%20Flow%20Matching%0AAuthor%3A%20Zhuo%20Cao%20and%20Xuan%20Zhao%20and%20Lena%20Krieger%20and%20Hanno%20Scharr%20and%20Ira%20Assent%0AAbstract%3A%20%20%20The%20growing%20integration%20of%20machine%20learning%20%28ML%29%20and%20artificial%20intelligence%0A%28AI%29%20models%20into%20high-stakes%20domains%20such%20as%20healthcare%20and%20scientific%20research%0Acalls%20for%20models%20that%20are%20not%20only%20accurate%20but%20also%20interpretable.%20Among%20the%0Aexisting%20explainable%20methods%2C%20counterfactual%20explanations%20offer%0Ainterpretability%20by%20identifying%20minimal%20changes%20to%20inputs%20that%20would%20alter%20a%0Amodel%27s%20prediction%2C%20thus%20providing%20deeper%20insights.%20However%2C%20current%0Acounterfactual%20generation%20methods%20suffer%20from%20critical%20limitations%2C%20including%0Agradient%20vanishing%2C%20discontinuous%20latent%20spaces%2C%20and%20an%20overreliance%20on%20the%0Aalignment%20between%20learned%20and%20true%20decision%20boundaries.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20LeapFactual%2C%20a%20novel%20counterfactual%20explanation%0Aalgorithm%20based%20on%20conditional%20flow%20matching.%20LeapFactual%20generates%20reliable%0Aand%20informative%20counterfactuals%2C%20even%20when%20true%20and%20learned%20decision%20boundaries%0Adiverge.%20Following%20a%20model-agnostic%20approach%2C%20LeapFactual%20is%20not%20limited%20to%0Amodels%20with%20differentiable%20loss%20functions.%20It%20can%20even%20handle%20human-in-the-loop%0Asystems%2C%20expanding%20the%20scope%20of%20counterfactual%20explanations%20to%20domains%20that%0Arequire%20the%20participation%20of%20human%20annotators%2C%20such%20as%20citizen%20science.%20We%0Aprovide%20extensive%20experiments%20on%20benchmark%20and%20real-world%20datasets%20showing%20that%0ALeapFactual%20generates%20accurate%20and%20in-distribution%20counterfactual%20explanations%0Athat%20offer%20actionable%20insights.%20We%20observe%2C%20for%20instance%2C%20that%20our%20reliable%0Acounterfactual%20samples%20with%20labels%20aligning%20to%20ground%20truth%20can%20be%20beneficially%0Aused%20as%20new%20training%20data%20to%20enhance%20the%20model.%20The%20proposed%20method%20is%20broadly%0Aapplicable%20and%20enhances%20both%20scientific%20knowledge%20discovery%20and%20non-expert%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.14623v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeapFactual%253A%2520Reliable%2520Visual%2520Counterfactual%2520Explanation%2520Using%250A%2520%2520Conditional%2520Flow%2520Matching%26entry.906535625%3DZhuo%2520Cao%2520and%2520Xuan%2520Zhao%2520and%2520Lena%2520Krieger%2520and%2520Hanno%2520Scharr%2520and%2520Ira%2520Assent%26entry.1292438233%3D%2520%2520The%2520growing%2520integration%2520of%2520machine%2520learning%2520%2528ML%2529%2520and%2520artificial%2520intelligence%250A%2528AI%2529%2520models%2520into%2520high-stakes%2520domains%2520such%2520as%2520healthcare%2520and%2520scientific%2520research%250Acalls%2520for%2520models%2520that%2520are%2520not%2520only%2520accurate%2520but%2520also%2520interpretable.%2520Among%2520the%250Aexisting%2520explainable%2520methods%252C%2520counterfactual%2520explanations%2520offer%250Ainterpretability%2520by%2520identifying%2520minimal%2520changes%2520to%2520inputs%2520that%2520would%2520alter%2520a%250Amodel%2527s%2520prediction%252C%2520thus%2520providing%2520deeper%2520insights.%2520However%252C%2520current%250Acounterfactual%2520generation%2520methods%2520suffer%2520from%2520critical%2520limitations%252C%2520including%250Agradient%2520vanishing%252C%2520discontinuous%2520latent%2520spaces%252C%2520and%2520an%2520overreliance%2520on%2520the%250Aalignment%2520between%2520learned%2520and%2520true%2520decision%2520boundaries.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520LeapFactual%252C%2520a%2520novel%2520counterfactual%2520explanation%250Aalgorithm%2520based%2520on%2520conditional%2520flow%2520matching.%2520LeapFactual%2520generates%2520reliable%250Aand%2520informative%2520counterfactuals%252C%2520even%2520when%2520true%2520and%2520learned%2520decision%2520boundaries%250Adiverge.%2520Following%2520a%2520model-agnostic%2520approach%252C%2520LeapFactual%2520is%2520not%2520limited%2520to%250Amodels%2520with%2520differentiable%2520loss%2520functions.%2520It%2520can%2520even%2520handle%2520human-in-the-loop%250Asystems%252C%2520expanding%2520the%2520scope%2520of%2520counterfactual%2520explanations%2520to%2520domains%2520that%250Arequire%2520the%2520participation%2520of%2520human%2520annotators%252C%2520such%2520as%2520citizen%2520science.%2520We%250Aprovide%2520extensive%2520experiments%2520on%2520benchmark%2520and%2520real-world%2520datasets%2520showing%2520that%250ALeapFactual%2520generates%2520accurate%2520and%2520in-distribution%2520counterfactual%2520explanations%250Athat%2520offer%2520actionable%2520insights.%2520We%2520observe%252C%2520for%2520instance%252C%2520that%2520our%2520reliable%250Acounterfactual%2520samples%2520with%2520labels%2520aligning%2520to%2520ground%2520truth%2520can%2520be%2520beneficially%250Aused%2520as%2520new%2520training%2520data%2520to%2520enhance%2520the%2520model.%2520The%2520proposed%2520method%2520is%2520broadly%250Aapplicable%2520and%2520enhances%2520both%2520scientific%2520knowledge%2520discovery%2520and%2520non-expert%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14623v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeapFactual%3A%20Reliable%20Visual%20Counterfactual%20Explanation%20Using%0A%20%20Conditional%20Flow%20Matching&entry.906535625=Zhuo%20Cao%20and%20Xuan%20Zhao%20and%20Lena%20Krieger%20and%20Hanno%20Scharr%20and%20Ira%20Assent&entry.1292438233=%20%20The%20growing%20integration%20of%20machine%20learning%20%28ML%29%20and%20artificial%20intelligence%0A%28AI%29%20models%20into%20high-stakes%20domains%20such%20as%20healthcare%20and%20scientific%20research%0Acalls%20for%20models%20that%20are%20not%20only%20accurate%20but%20also%20interpretable.%20Among%20the%0Aexisting%20explainable%20methods%2C%20counterfactual%20explanations%20offer%0Ainterpretability%20by%20identifying%20minimal%20changes%20to%20inputs%20that%20would%20alter%20a%0Amodel%27s%20prediction%2C%20thus%20providing%20deeper%20insights.%20However%2C%20current%0Acounterfactual%20generation%20methods%20suffer%20from%20critical%20limitations%2C%20including%0Agradient%20vanishing%2C%20discontinuous%20latent%20spaces%2C%20and%20an%20overreliance%20on%20the%0Aalignment%20between%20learned%20and%20true%20decision%20boundaries.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20LeapFactual%2C%20a%20novel%20counterfactual%20explanation%0Aalgorithm%20based%20on%20conditional%20flow%20matching.%20LeapFactual%20generates%20reliable%0Aand%20informative%20counterfactuals%2C%20even%20when%20true%20and%20learned%20decision%20boundaries%0Adiverge.%20Following%20a%20model-agnostic%20approach%2C%20LeapFactual%20is%20not%20limited%20to%0Amodels%20with%20differentiable%20loss%20functions.%20It%20can%20even%20handle%20human-in-the-loop%0Asystems%2C%20expanding%20the%20scope%20of%20counterfactual%20explanations%20to%20domains%20that%0Arequire%20the%20participation%20of%20human%20annotators%2C%20such%20as%20citizen%20science.%20We%0Aprovide%20extensive%20experiments%20on%20benchmark%20and%20real-world%20datasets%20showing%20that%0ALeapFactual%20generates%20accurate%20and%20in-distribution%20counterfactual%20explanations%0Athat%20offer%20actionable%20insights.%20We%20observe%2C%20for%20instance%2C%20that%20our%20reliable%0Acounterfactual%20samples%20with%20labels%20aligning%20to%20ground%20truth%20can%20be%20beneficially%0Aused%20as%20new%20training%20data%20to%20enhance%20the%20model.%20The%20proposed%20method%20is%20broadly%0Aapplicable%20and%20enhances%20both%20scientific%20knowledge%20discovery%20and%20non-expert%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.14623v2&entry.124074799=Read"},
{"title": "Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment", "author": "Jiayi Huang and Sangwoo Park and Nicola Paoletti and Osvaldo Simeone", "abstract": "  Edge intelligence enables low-latency inference via compact on-device models,\nbut assuring reliability remains challenging. We study edge-cloud cascades that\nmust preserve conditional coverage: whenever the edge returns a prediction set,\nit should contain the true label with a user-specified probability, as if\nproduced by the cloud model. We formalize conditional coverage with respect to\nthe cloud predictive distribution, and introduce a conformal alignment-based\n(CAb) cascading mechanism that certifies this property with user control over\nthe risk level. Our method casts escalation from edge to cloud models as a\nmultiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)\nto select which inputs can be safely handled at the edge. The proposed CAb\nmodel cascading method yields statistical guarantees on the average fraction of\nedge decisions that satisfy cloud-level conditional coverage. The procedure\napplies to arbitrary edge prediction sets, including variants of conformal\nprediction (CP), and exposes a tunable trade-off among coverage, deferral rate,\nand set size. Experiments on CIFAR-100 image classification and the TeleQnA\nquestion-answering (QA) benchmark show that the proposed CAb cascade maintains\nthe target conditional coverage for edge predictions while substantially\nreducing offloading to the cloud and incurring modest increases in\nprediction-set size.\n", "link": "http://arxiv.org/abs/2510.17543v1", "date": "2025-10-20", "relevancy": 1.403, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4698}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Inference%20in%20Edge-Cloud%20Model%20Cascades%20via%20Conformal%20Alignment&body=Title%3A%20Reliable%20Inference%20in%20Edge-Cloud%20Model%20Cascades%20via%20Conformal%20Alignment%0AAuthor%3A%20Jiayi%20Huang%20and%20Sangwoo%20Park%20and%20Nicola%20Paoletti%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20Edge%20intelligence%20enables%20low-latency%20inference%20via%20compact%20on-device%20models%2C%0Abut%20assuring%20reliability%20remains%20challenging.%20We%20study%20edge-cloud%20cascades%20that%0Amust%20preserve%20conditional%20coverage%3A%20whenever%20the%20edge%20returns%20a%20prediction%20set%2C%0Ait%20should%20contain%20the%20true%20label%20with%20a%20user-specified%20probability%2C%20as%20if%0Aproduced%20by%20the%20cloud%20model.%20We%20formalize%20conditional%20coverage%20with%20respect%20to%0Athe%20cloud%20predictive%20distribution%2C%20and%20introduce%20a%20conformal%20alignment-based%0A%28CAb%29%20cascading%20mechanism%20that%20certifies%20this%20property%20with%20user%20control%20over%0Athe%20risk%20level.%20Our%20method%20casts%20escalation%20from%20edge%20to%20cloud%20models%20as%20a%0Amultiple-hypothesis%20testing%20%28MHT%29%20problem%2C%20tailoring%20conformal%20alignment%20%28CA%29%0Ato%20select%20which%20inputs%20can%20be%20safely%20handled%20at%20the%20edge.%20The%20proposed%20CAb%0Amodel%20cascading%20method%20yields%20statistical%20guarantees%20on%20the%20average%20fraction%20of%0Aedge%20decisions%20that%20satisfy%20cloud-level%20conditional%20coverage.%20The%20procedure%0Aapplies%20to%20arbitrary%20edge%20prediction%20sets%2C%20including%20variants%20of%20conformal%0Aprediction%20%28CP%29%2C%20and%20exposes%20a%20tunable%20trade-off%20among%20coverage%2C%20deferral%20rate%2C%0Aand%20set%20size.%20Experiments%20on%20CIFAR-100%20image%20classification%20and%20the%20TeleQnA%0Aquestion-answering%20%28QA%29%20benchmark%20show%20that%20the%20proposed%20CAb%20cascade%20maintains%0Athe%20target%20conditional%20coverage%20for%20edge%20predictions%20while%20substantially%0Areducing%20offloading%20to%20the%20cloud%20and%20incurring%20modest%20increases%20in%0Aprediction-set%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.17543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Inference%2520in%2520Edge-Cloud%2520Model%2520Cascades%2520via%2520Conformal%2520Alignment%26entry.906535625%3DJiayi%2520Huang%2520and%2520Sangwoo%2520Park%2520and%2520Nicola%2520Paoletti%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520Edge%2520intelligence%2520enables%2520low-latency%2520inference%2520via%2520compact%2520on-device%2520models%252C%250Abut%2520assuring%2520reliability%2520remains%2520challenging.%2520We%2520study%2520edge-cloud%2520cascades%2520that%250Amust%2520preserve%2520conditional%2520coverage%253A%2520whenever%2520the%2520edge%2520returns%2520a%2520prediction%2520set%252C%250Ait%2520should%2520contain%2520the%2520true%2520label%2520with%2520a%2520user-specified%2520probability%252C%2520as%2520if%250Aproduced%2520by%2520the%2520cloud%2520model.%2520We%2520formalize%2520conditional%2520coverage%2520with%2520respect%2520to%250Athe%2520cloud%2520predictive%2520distribution%252C%2520and%2520introduce%2520a%2520conformal%2520alignment-based%250A%2528CAb%2529%2520cascading%2520mechanism%2520that%2520certifies%2520this%2520property%2520with%2520user%2520control%2520over%250Athe%2520risk%2520level.%2520Our%2520method%2520casts%2520escalation%2520from%2520edge%2520to%2520cloud%2520models%2520as%2520a%250Amultiple-hypothesis%2520testing%2520%2528MHT%2529%2520problem%252C%2520tailoring%2520conformal%2520alignment%2520%2528CA%2529%250Ato%2520select%2520which%2520inputs%2520can%2520be%2520safely%2520handled%2520at%2520the%2520edge.%2520The%2520proposed%2520CAb%250Amodel%2520cascading%2520method%2520yields%2520statistical%2520guarantees%2520on%2520the%2520average%2520fraction%2520of%250Aedge%2520decisions%2520that%2520satisfy%2520cloud-level%2520conditional%2520coverage.%2520The%2520procedure%250Aapplies%2520to%2520arbitrary%2520edge%2520prediction%2520sets%252C%2520including%2520variants%2520of%2520conformal%250Aprediction%2520%2528CP%2529%252C%2520and%2520exposes%2520a%2520tunable%2520trade-off%2520among%2520coverage%252C%2520deferral%2520rate%252C%250Aand%2520set%2520size.%2520Experiments%2520on%2520CIFAR-100%2520image%2520classification%2520and%2520the%2520TeleQnA%250Aquestion-answering%2520%2528QA%2529%2520benchmark%2520show%2520that%2520the%2520proposed%2520CAb%2520cascade%2520maintains%250Athe%2520target%2520conditional%2520coverage%2520for%2520edge%2520predictions%2520while%2520substantially%250Areducing%2520offloading%2520to%2520the%2520cloud%2520and%2520incurring%2520modest%2520increases%2520in%250Aprediction-set%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.17543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Inference%20in%20Edge-Cloud%20Model%20Cascades%20via%20Conformal%20Alignment&entry.906535625=Jiayi%20Huang%20and%20Sangwoo%20Park%20and%20Nicola%20Paoletti%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20Edge%20intelligence%20enables%20low-latency%20inference%20via%20compact%20on-device%20models%2C%0Abut%20assuring%20reliability%20remains%20challenging.%20We%20study%20edge-cloud%20cascades%20that%0Amust%20preserve%20conditional%20coverage%3A%20whenever%20the%20edge%20returns%20a%20prediction%20set%2C%0Ait%20should%20contain%20the%20true%20label%20with%20a%20user-specified%20probability%2C%20as%20if%0Aproduced%20by%20the%20cloud%20model.%20We%20formalize%20conditional%20coverage%20with%20respect%20to%0Athe%20cloud%20predictive%20distribution%2C%20and%20introduce%20a%20conformal%20alignment-based%0A%28CAb%29%20cascading%20mechanism%20that%20certifies%20this%20property%20with%20user%20control%20over%0Athe%20risk%20level.%20Our%20method%20casts%20escalation%20from%20edge%20to%20cloud%20models%20as%20a%0Amultiple-hypothesis%20testing%20%28MHT%29%20problem%2C%20tailoring%20conformal%20alignment%20%28CA%29%0Ato%20select%20which%20inputs%20can%20be%20safely%20handled%20at%20the%20edge.%20The%20proposed%20CAb%0Amodel%20cascading%20method%20yields%20statistical%20guarantees%20on%20the%20average%20fraction%20of%0Aedge%20decisions%20that%20satisfy%20cloud-level%20conditional%20coverage.%20The%20procedure%0Aapplies%20to%20arbitrary%20edge%20prediction%20sets%2C%20including%20variants%20of%20conformal%0Aprediction%20%28CP%29%2C%20and%20exposes%20a%20tunable%20trade-off%20among%20coverage%2C%20deferral%20rate%2C%0Aand%20set%20size.%20Experiments%20on%20CIFAR-100%20image%20classification%20and%20the%20TeleQnA%0Aquestion-answering%20%28QA%29%20benchmark%20show%20that%20the%20proposed%20CAb%20cascade%20maintains%0Athe%20target%20conditional%20coverage%20for%20edge%20predictions%20while%20substantially%0Areducing%20offloading%20to%20the%20cloud%20and%20incurring%20modest%20increases%20in%0Aprediction-set%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.17543v1&entry.124074799=Read"},
{"title": "A Generic Framework for Conformal Fairness", "author": "Aditya T. Vadlamani and Anutam Srinivasan and Pranav Maneriker and Ali Payani and Srinivasan Parthasarathy", "abstract": "  Conformal Prediction (CP) is a popular method for uncertainty quantification\nwith machine learning models. While conformal prediction provides probabilistic\nguarantees regarding the coverage of the true label, these guarantees are\nagnostic to the presence of sensitive attributes within the dataset. In this\nwork, we formalize \\textit{Conformal Fairness}, a notion of fairness using\nconformal predictors, and provide a theoretically well-founded algorithm and\nassociated framework to control for the gaps in coverage between different\nsensitive groups. Our framework leverages the exchangeability assumption\n(implicit to CP) rather than the typical IID assumption, allowing us to apply\nthe notion of Conformal Fairness to data types and tasks that are not IID, such\nas graph data. Experiments were conducted on graph and tabular datasets to\ndemonstrate that the algorithm can control fairness-related gaps in addition to\ncoverage aligned with theoretical expectations.\n", "link": "http://arxiv.org/abs/2505.16115v2", "date": "2025-10-20", "relevancy": 1.5145, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5124}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generic%20Framework%20for%20Conformal%20Fairness&body=Title%3A%20A%20Generic%20Framework%20for%20Conformal%20Fairness%0AAuthor%3A%20Aditya%20T.%20Vadlamani%20and%20Anutam%20Srinivasan%20and%20Pranav%20Maneriker%20and%20Ali%20Payani%20and%20Srinivasan%20Parthasarathy%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20method%20for%20uncertainty%20quantification%0Awith%20machine%20learning%20models.%20While%20conformal%20prediction%20provides%20probabilistic%0Aguarantees%20regarding%20the%20coverage%20of%20the%20true%20label%2C%20these%20guarantees%20are%0Aagnostic%20to%20the%20presence%20of%20sensitive%20attributes%20within%20the%20dataset.%20In%20this%0Awork%2C%20we%20formalize%20%5Ctextit%7BConformal%20Fairness%7D%2C%20a%20notion%20of%20fairness%20using%0Aconformal%20predictors%2C%20and%20provide%20a%20theoretically%20well-founded%20algorithm%20and%0Aassociated%20framework%20to%20control%20for%20the%20gaps%20in%20coverage%20between%20different%0Asensitive%20groups.%20Our%20framework%20leverages%20the%20exchangeability%20assumption%0A%28implicit%20to%20CP%29%20rather%20than%20the%20typical%20IID%20assumption%2C%20allowing%20us%20to%20apply%0Athe%20notion%20of%20Conformal%20Fairness%20to%20data%20types%20and%20tasks%20that%20are%20not%20IID%2C%20such%0Aas%20graph%20data.%20Experiments%20were%20conducted%20on%20graph%20and%20tabular%20datasets%20to%0Ademonstrate%20that%20the%20algorithm%20can%20control%20fairness-related%20gaps%20in%20addition%20to%0Acoverage%20aligned%20with%20theoretical%20expectations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generic%2520Framework%2520for%2520Conformal%2520Fairness%26entry.906535625%3DAditya%2520T.%2520Vadlamani%2520and%2520Anutam%2520Srinivasan%2520and%2520Pranav%2520Maneriker%2520and%2520Ali%2520Payani%2520and%2520Srinivasan%2520Parthasarathy%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520popular%2520method%2520for%2520uncertainty%2520quantification%250Awith%2520machine%2520learning%2520models.%2520While%2520conformal%2520prediction%2520provides%2520probabilistic%250Aguarantees%2520regarding%2520the%2520coverage%2520of%2520the%2520true%2520label%252C%2520these%2520guarantees%2520are%250Aagnostic%2520to%2520the%2520presence%2520of%2520sensitive%2520attributes%2520within%2520the%2520dataset.%2520In%2520this%250Awork%252C%2520we%2520formalize%2520%255Ctextit%257BConformal%2520Fairness%257D%252C%2520a%2520notion%2520of%2520fairness%2520using%250Aconformal%2520predictors%252C%2520and%2520provide%2520a%2520theoretically%2520well-founded%2520algorithm%2520and%250Aassociated%2520framework%2520to%2520control%2520for%2520the%2520gaps%2520in%2520coverage%2520between%2520different%250Asensitive%2520groups.%2520Our%2520framework%2520leverages%2520the%2520exchangeability%2520assumption%250A%2528implicit%2520to%2520CP%2529%2520rather%2520than%2520the%2520typical%2520IID%2520assumption%252C%2520allowing%2520us%2520to%2520apply%250Athe%2520notion%2520of%2520Conformal%2520Fairness%2520to%2520data%2520types%2520and%2520tasks%2520that%2520are%2520not%2520IID%252C%2520such%250Aas%2520graph%2520data.%2520Experiments%2520were%2520conducted%2520on%2520graph%2520and%2520tabular%2520datasets%2520to%250Ademonstrate%2520that%2520the%2520algorithm%2520can%2520control%2520fairness-related%2520gaps%2520in%2520addition%2520to%250Acoverage%2520aligned%2520with%2520theoretical%2520expectations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generic%20Framework%20for%20Conformal%20Fairness&entry.906535625=Aditya%20T.%20Vadlamani%20and%20Anutam%20Srinivasan%20and%20Pranav%20Maneriker%20and%20Ali%20Payani%20and%20Srinivasan%20Parthasarathy&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20method%20for%20uncertainty%20quantification%0Awith%20machine%20learning%20models.%20While%20conformal%20prediction%20provides%20probabilistic%0Aguarantees%20regarding%20the%20coverage%20of%20the%20true%20label%2C%20these%20guarantees%20are%0Aagnostic%20to%20the%20presence%20of%20sensitive%20attributes%20within%20the%20dataset.%20In%20this%0Awork%2C%20we%20formalize%20%5Ctextit%7BConformal%20Fairness%7D%2C%20a%20notion%20of%20fairness%20using%0Aconformal%20predictors%2C%20and%20provide%20a%20theoretically%20well-founded%20algorithm%20and%0Aassociated%20framework%20to%20control%20for%20the%20gaps%20in%20coverage%20between%20different%0Asensitive%20groups.%20Our%20framework%20leverages%20the%20exchangeability%20assumption%0A%28implicit%20to%20CP%29%20rather%20than%20the%20typical%20IID%20assumption%2C%20allowing%20us%20to%20apply%0Athe%20notion%20of%20Conformal%20Fairness%20to%20data%20types%20and%20tasks%20that%20are%20not%20IID%2C%20such%0Aas%20graph%20data.%20Experiments%20were%20conducted%20on%20graph%20and%20tabular%20datasets%20to%0Ademonstrate%20that%20the%20algorithm%20can%20control%20fairness-related%20gaps%20in%20addition%20to%0Acoverage%20aligned%20with%20theoretical%20expectations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16115v2&entry.124074799=Read"},
{"title": "Predicting Patient Recovery or Mortality Using Deep Neural Decision Tree\n  and Forest", "author": "Mohammad Dehghani and Mohadeseh Zarei Ghobadi and Mobin Mohammadi and Diyana Tehrany Dehkordy", "abstract": "  Objective: Identifying patients at high risk of mortality is crucial for\nemergency physicians to allocate hospital resources effectively, particularly\nin regions with limited medical services. This need becomes even more pressing\nduring global health crises that lead to significant morbidity and mortality.\nThis study aimed to present the usability deep neural decision forest and deep\nneural decision tree to predict mortality among Coronavirus disease 2019\n(COVID-19) patients. To this end, We used patient data encompassing Coronavirus\ndisease 2019 diagnosis, demographics, health indicators, and occupational risk\nfactors to analyze disease severity and outcomes. The dataset was partitioned\nusing a stratified sampling method, ensuring that 80% was allocated for\ntraining and 20% for testing. Nine machine learning and deep learning methods\nwere employed to build predictive models. The models were evaluated across all\nstages to determine their effectiveness in predicting patient outcomes.\nResults: Among the models, the deep neural decision forest consistently\noutperformed others. Results indicated that using only clinical data yielded an\naccuracy of 80% by deep neural decision forest, demonstrating it as a reliable\npredictor of patient mortality. Moreover, the results suggest that clinical\ndata alone may be the most accurate diagnostic tool for predicting mortality.\n", "link": "http://arxiv.org/abs/2311.13925v4", "date": "2025-10-20", "relevancy": 1.2597, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4413}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Patient%20Recovery%20or%20Mortality%20Using%20Deep%20Neural%20Decision%20Tree%0A%20%20and%20Forest&body=Title%3A%20Predicting%20Patient%20Recovery%20or%20Mortality%20Using%20Deep%20Neural%20Decision%20Tree%0A%20%20and%20Forest%0AAuthor%3A%20Mohammad%20Dehghani%20and%20Mohadeseh%20Zarei%20Ghobadi%20and%20Mobin%20Mohammadi%20and%20Diyana%20Tehrany%20Dehkordy%0AAbstract%3A%20%20%20Objective%3A%20Identifying%20patients%20at%20high%20risk%20of%20mortality%20is%20crucial%20for%0Aemergency%20physicians%20to%20allocate%20hospital%20resources%20effectively%2C%20particularly%0Ain%20regions%20with%20limited%20medical%20services.%20This%20need%20becomes%20even%20more%20pressing%0Aduring%20global%20health%20crises%20that%20lead%20to%20significant%20morbidity%20and%20mortality.%0AThis%20study%20aimed%20to%20present%20the%20usability%20deep%20neural%20decision%20forest%20and%20deep%0Aneural%20decision%20tree%20to%20predict%20mortality%20among%20Coronavirus%20disease%202019%0A%28COVID-19%29%20patients.%20To%20this%20end%2C%20We%20used%20patient%20data%20encompassing%20Coronavirus%0Adisease%202019%20diagnosis%2C%20demographics%2C%20health%20indicators%2C%20and%20occupational%20risk%0Afactors%20to%20analyze%20disease%20severity%20and%20outcomes.%20The%20dataset%20was%20partitioned%0Ausing%20a%20stratified%20sampling%20method%2C%20ensuring%20that%2080%25%20was%20allocated%20for%0Atraining%20and%2020%25%20for%20testing.%20Nine%20machine%20learning%20and%20deep%20learning%20methods%0Awere%20employed%20to%20build%20predictive%20models.%20The%20models%20were%20evaluated%20across%20all%0Astages%20to%20determine%20their%20effectiveness%20in%20predicting%20patient%20outcomes.%0AResults%3A%20Among%20the%20models%2C%20the%20deep%20neural%20decision%20forest%20consistently%0Aoutperformed%20others.%20Results%20indicated%20that%20using%20only%20clinical%20data%20yielded%20an%0Aaccuracy%20of%2080%25%20by%20deep%20neural%20decision%20forest%2C%20demonstrating%20it%20as%20a%20reliable%0Apredictor%20of%20patient%20mortality.%20Moreover%2C%20the%20results%20suggest%20that%20clinical%0Adata%20alone%20may%20be%20the%20most%20accurate%20diagnostic%20tool%20for%20predicting%20mortality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13925v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Patient%2520Recovery%2520or%2520Mortality%2520Using%2520Deep%2520Neural%2520Decision%2520Tree%250A%2520%2520and%2520Forest%26entry.906535625%3DMohammad%2520Dehghani%2520and%2520Mohadeseh%2520Zarei%2520Ghobadi%2520and%2520Mobin%2520Mohammadi%2520and%2520Diyana%2520Tehrany%2520Dehkordy%26entry.1292438233%3D%2520%2520Objective%253A%2520Identifying%2520patients%2520at%2520high%2520risk%2520of%2520mortality%2520is%2520crucial%2520for%250Aemergency%2520physicians%2520to%2520allocate%2520hospital%2520resources%2520effectively%252C%2520particularly%250Ain%2520regions%2520with%2520limited%2520medical%2520services.%2520This%2520need%2520becomes%2520even%2520more%2520pressing%250Aduring%2520global%2520health%2520crises%2520that%2520lead%2520to%2520significant%2520morbidity%2520and%2520mortality.%250AThis%2520study%2520aimed%2520to%2520present%2520the%2520usability%2520deep%2520neural%2520decision%2520forest%2520and%2520deep%250Aneural%2520decision%2520tree%2520to%2520predict%2520mortality%2520among%2520Coronavirus%2520disease%25202019%250A%2528COVID-19%2529%2520patients.%2520To%2520this%2520end%252C%2520We%2520used%2520patient%2520data%2520encompassing%2520Coronavirus%250Adisease%25202019%2520diagnosis%252C%2520demographics%252C%2520health%2520indicators%252C%2520and%2520occupational%2520risk%250Afactors%2520to%2520analyze%2520disease%2520severity%2520and%2520outcomes.%2520The%2520dataset%2520was%2520partitioned%250Ausing%2520a%2520stratified%2520sampling%2520method%252C%2520ensuring%2520that%252080%2525%2520was%2520allocated%2520for%250Atraining%2520and%252020%2525%2520for%2520testing.%2520Nine%2520machine%2520learning%2520and%2520deep%2520learning%2520methods%250Awere%2520employed%2520to%2520build%2520predictive%2520models.%2520The%2520models%2520were%2520evaluated%2520across%2520all%250Astages%2520to%2520determine%2520their%2520effectiveness%2520in%2520predicting%2520patient%2520outcomes.%250AResults%253A%2520Among%2520the%2520models%252C%2520the%2520deep%2520neural%2520decision%2520forest%2520consistently%250Aoutperformed%2520others.%2520Results%2520indicated%2520that%2520using%2520only%2520clinical%2520data%2520yielded%2520an%250Aaccuracy%2520of%252080%2525%2520by%2520deep%2520neural%2520decision%2520forest%252C%2520demonstrating%2520it%2520as%2520a%2520reliable%250Apredictor%2520of%2520patient%2520mortality.%2520Moreover%252C%2520the%2520results%2520suggest%2520that%2520clinical%250Adata%2520alone%2520may%2520be%2520the%2520most%2520accurate%2520diagnostic%2520tool%2520for%2520predicting%2520mortality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13925v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Patient%20Recovery%20or%20Mortality%20Using%20Deep%20Neural%20Decision%20Tree%0A%20%20and%20Forest&entry.906535625=Mohammad%20Dehghani%20and%20Mohadeseh%20Zarei%20Ghobadi%20and%20Mobin%20Mohammadi%20and%20Diyana%20Tehrany%20Dehkordy&entry.1292438233=%20%20Objective%3A%20Identifying%20patients%20at%20high%20risk%20of%20mortality%20is%20crucial%20for%0Aemergency%20physicians%20to%20allocate%20hospital%20resources%20effectively%2C%20particularly%0Ain%20regions%20with%20limited%20medical%20services.%20This%20need%20becomes%20even%20more%20pressing%0Aduring%20global%20health%20crises%20that%20lead%20to%20significant%20morbidity%20and%20mortality.%0AThis%20study%20aimed%20to%20present%20the%20usability%20deep%20neural%20decision%20forest%20and%20deep%0Aneural%20decision%20tree%20to%20predict%20mortality%20among%20Coronavirus%20disease%202019%0A%28COVID-19%29%20patients.%20To%20this%20end%2C%20We%20used%20patient%20data%20encompassing%20Coronavirus%0Adisease%202019%20diagnosis%2C%20demographics%2C%20health%20indicators%2C%20and%20occupational%20risk%0Afactors%20to%20analyze%20disease%20severity%20and%20outcomes.%20The%20dataset%20was%20partitioned%0Ausing%20a%20stratified%20sampling%20method%2C%20ensuring%20that%2080%25%20was%20allocated%20for%0Atraining%20and%2020%25%20for%20testing.%20Nine%20machine%20learning%20and%20deep%20learning%20methods%0Awere%20employed%20to%20build%20predictive%20models.%20The%20models%20were%20evaluated%20across%20all%0Astages%20to%20determine%20their%20effectiveness%20in%20predicting%20patient%20outcomes.%0AResults%3A%20Among%20the%20models%2C%20the%20deep%20neural%20decision%20forest%20consistently%0Aoutperformed%20others.%20Results%20indicated%20that%20using%20only%20clinical%20data%20yielded%20an%0Aaccuracy%20of%2080%25%20by%20deep%20neural%20decision%20forest%2C%20demonstrating%20it%20as%20a%20reliable%0Apredictor%20of%20patient%20mortality.%20Moreover%2C%20the%20results%20suggest%20that%20clinical%0Adata%20alone%20may%20be%20the%20most%20accurate%20diagnostic%20tool%20for%20predicting%20mortality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13925v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


