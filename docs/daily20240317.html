<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding", "author": "Chengyao Wang and Li Jiang and Xiaoyang Wu and Zhuotao Tian and Bohao Peng and Hengshuang Zhao and Jiaya Jia", "abstract": "  Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.\n", "link": "http://arxiv.org/abs/2403.09639v1", "date": "2024-03-14", "relevancy": 2.965, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding&body=Title%3A%20GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding%0AAuthor%3A%20Chengyao%20Wang%20and%20Li%20Jiang%20and%20Xiaoyang%20Wu%20and%20Zhuotao%20Tian%20and%20Bohao%20Peng%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Self-supervised%203D%20representation%20learning%20aims%20to%20learn%20effective%0Arepresentations%20from%20large-scale%20unlabeled%20point%20clouds.%20Most%20existing%0Aapproaches%20adopt%20point%20discrimination%20as%20the%20pretext%20task%2C%20which%20assigns%0Amatched%20points%20in%20two%20distinct%20views%20as%20positive%20pairs%20and%20unmatched%20points%20as%0Anegative%20pairs.%20However%2C%20this%20approach%20often%20results%20in%20semantically%20identical%0Apoints%20having%20dissimilar%20representations%2C%20leading%20to%20a%20high%20number%20of%20false%0Anegatives%20and%20introducing%20a%20%22semantic%20conflict%22%20problem.%20To%20address%20this%20issue%2C%0Awe%20propose%20GroupContrast%2C%20a%20novel%20approach%20that%20combines%20segment%20grouping%20and%0Asemantic-aware%20contrastive%20learning.%20Segment%20grouping%20partitions%20points%20into%0Asemantically%20meaningful%20regions%2C%20which%20enhances%20semantic%20coherence%20and%20provides%0Asemantic%20guidance%20for%20the%20subsequent%20contrastive%20representation%20learning.%0ASemantic-aware%20contrastive%20learning%20augments%20the%20semantic%20information%20extracted%0Afrom%20segment%20grouping%20and%20helps%20to%20alleviate%20the%20issue%20of%20%22semantic%20conflict%22.%0AWe%20conducted%20extensive%20experiments%20on%20multiple%203D%20scene%20understanding%20tasks.%0AThe%20results%20demonstrate%20that%20GroupContrast%20learns%20semantically%20meaningful%0Arepresentations%20and%20achieves%20promising%20transfer%20learning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding&entry.906535625=Chengyao%20Wang%20and%20Li%20Jiang%20and%20Xiaoyang%20Wu%20and%20Zhuotao%20Tian%20and%20Bohao%20Peng%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia&entry.1292438233=%20%20Self-supervised%203D%20representation%20learning%20aims%20to%20learn%20effective%0Arepresentations%20from%20large-scale%20unlabeled%20point%20clouds.%20Most%20existing%0Aapproaches%20adopt%20point%20discrimination%20as%20the%20pretext%20task%2C%20which%20assigns%0Amatched%20points%20in%20two%20distinct%20views%20as%20positive%20pairs%20and%20unmatched%20points%20as%0Anegative%20pairs.%20However%2C%20this%20approach%20often%20results%20in%20semantically%20identical%0Apoints%20having%20dissimilar%20representations%2C%20leading%20to%20a%20high%20number%20of%20false%0Anegatives%20and%20introducing%20a%20%22semantic%20conflict%22%20problem.%20To%20address%20this%20issue%2C%0Awe%20propose%20GroupContrast%2C%20a%20novel%20approach%20that%20combines%20segment%20grouping%20and%0Asemantic-aware%20contrastive%20learning.%20Segment%20grouping%20partitions%20points%20into%0Asemantically%20meaningful%20regions%2C%20which%20enhances%20semantic%20coherence%20and%20provides%0Asemantic%20guidance%20for%20the%20subsequent%20contrastive%20representation%20learning.%0ASemantic-aware%20contrastive%20learning%20augments%20the%20semantic%20information%20extracted%0Afrom%20segment%20grouping%20and%20helps%20to%20alleviate%20the%20issue%20of%20%22semantic%20conflict%22.%0AWe%20conducted%20extensive%20experiments%20on%20multiple%203D%20scene%20understanding%20tasks.%0AThe%20results%20demonstrate%20that%20GroupContrast%20learns%20semantically%20meaningful%0Arepresentations%20and%20achieves%20promising%20transfer%20learning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09639v1&entry.124074799=Read"},
{"title": "Anomaly Detection by Adapting a pre-trained Vision Language Model", "author": "Yuxuan Cai and Xinwei He and Dingkang Liang and Ao Tong and Xiang Bai", "abstract": "  Recently, large vision and language models have shown their success when\nadapting them to many downstream tasks. In this paper, we present a unified\nframework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP\nmodel. To this end, we make two important improvements: 1) To acquire unified\nanomaly detection across industrial images of multiple categories, we introduce\nthe learnable prompt and propose to associate it with abnormal patterns through\nself-supervised learning. 2) To fully exploit the representation power of CLIP,\nwe introduce an anomaly region refinement strategy to refine the localization\nquality. During testing, the anomalies are localized by directly calculating\nthe similarity between the representation of the learnable prompt and the\nimage. Comprehensive experiments demonstrate the superiority of our framework,\ne.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and\nVisA for anomaly detection and localization. In addition, the proposed method\nalso achieves encouraging performance with marginal training data, which is\nmore challenging.\n", "link": "http://arxiv.org/abs/2403.09493v1", "date": "2024-03-14", "relevancy": 2.8603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6252}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model&body=Title%3A%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model%0AAuthor%3A%20Yuxuan%20Cai%20and%20Xinwei%20He%20and%20Dingkang%20Liang%20and%20Ao%20Tong%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Recently%2C%20large%20vision%20and%20language%20models%20have%20shown%20their%20success%20when%0Aadapting%20them%20to%20many%20downstream%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20unified%0Aframework%20named%20CLIP-ADA%20for%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20CLIP%0Amodel.%20To%20this%20end%2C%20we%20make%20two%20important%20improvements%3A%201%29%20To%20acquire%20unified%0Aanomaly%20detection%20across%20industrial%20images%20of%20multiple%20categories%2C%20we%20introduce%0Athe%20learnable%20prompt%20and%20propose%20to%20associate%20it%20with%20abnormal%20patterns%20through%0Aself-supervised%20learning.%202%29%20To%20fully%20exploit%20the%20representation%20power%20of%20CLIP%2C%0Awe%20introduce%20an%20anomaly%20region%20refinement%20strategy%20to%20refine%20the%20localization%0Aquality.%20During%20testing%2C%20the%20anomalies%20are%20localized%20by%20directly%20calculating%0Athe%20similarity%20between%20the%20representation%20of%20the%20learnable%20prompt%20and%20the%0Aimage.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20framework%2C%0Ae.g.%2C%20we%20achieve%20the%20state-of-the-art%2097.5/55.6%20and%2089.3/33.1%20on%20MVTec-AD%20and%0AVisA%20for%20anomaly%20detection%20and%20localization.%20In%20addition%2C%20the%20proposed%20method%0Aalso%20achieves%20encouraging%20performance%20with%20marginal%20training%20data%2C%20which%20is%0Amore%20challenging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09493v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model&entry.906535625=Yuxuan%20Cai%20and%20Xinwei%20He%20and%20Dingkang%20Liang%20and%20Ao%20Tong%20and%20Xiang%20Bai&entry.1292438233=%20%20Recently%2C%20large%20vision%20and%20language%20models%20have%20shown%20their%20success%20when%0Aadapting%20them%20to%20many%20downstream%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20unified%0Aframework%20named%20CLIP-ADA%20for%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20CLIP%0Amodel.%20To%20this%20end%2C%20we%20make%20two%20important%20improvements%3A%201%29%20To%20acquire%20unified%0Aanomaly%20detection%20across%20industrial%20images%20of%20multiple%20categories%2C%20we%20introduce%0Athe%20learnable%20prompt%20and%20propose%20to%20associate%20it%20with%20abnormal%20patterns%20through%0Aself-supervised%20learning.%202%29%20To%20fully%20exploit%20the%20representation%20power%20of%20CLIP%2C%0Awe%20introduce%20an%20anomaly%20region%20refinement%20strategy%20to%20refine%20the%20localization%0Aquality.%20During%20testing%2C%20the%20anomalies%20are%20localized%20by%20directly%20calculating%0Athe%20similarity%20between%20the%20representation%20of%20the%20learnable%20prompt%20and%20the%0Aimage.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20framework%2C%0Ae.g.%2C%20we%20achieve%20the%20state-of-the-art%2097.5/55.6%20and%2089.3/33.1%20on%20MVTec-AD%20and%0AVisA%20for%20anomaly%20detection%20and%20localization.%20In%20addition%2C%20the%20proposed%20method%0Aalso%20achieves%20encouraging%20performance%20with%20marginal%20training%20data%2C%20which%20is%0Amore%20challenging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09493v1&entry.124074799=Read"},
{"title": "PosSAM: Panoptic Open-vocabulary Segment Anything", "author": "Vibashan VS and Shubhankar Borse and Hyojin Park and Debasmit Das and Vishal Patel and Munawar Hayat and Fatih Porikli", "abstract": "  In this paper, we introduce an open-vocabulary panoptic segmentation model\nthat effectively unifies the strengths of the Segment Anything Model (SAM) with\nthe vision-language CLIP model in an end-to-end framework. While SAM excels in\ngenerating spatially-aware masks, it's decoder falls short in recognizing\nobject class information and tends to oversegment without additional guidance.\nExisting approaches address this limitation by using multi-stage techniques and\nemploying separate models to generate class-aware prompts, such as bounding\nboxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model\nwhich leverages SAM's spatially rich features to produce instance-aware masks\nand harnesses CLIP's semantically discriminative features for effective\ninstance classification. Specifically, we address the limitations of SAM and\npropose a novel Local Discriminative Pooling (LDP) module leveraging\nclass-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary\nclassification. Furthermore, we introduce a Mask-Aware Selective Ensembling\n(MASE) algorithm that adaptively enhances the quality of generated masks and\nboosts the performance of open-vocabulary classification during inference for\neach image. We conducted extensive experiments to demonstrate our methods\nstrong generalization properties across multiple datasets, achieving\nstate-of-the-art performance with substantial improvements over SOTA\nopen-vocabulary panoptic segmentation methods. In both COCO to ADE20K and\nADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art\nmethods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website:\nhttps://vibashan.github.io/possam-web/.\n", "link": "http://arxiv.org/abs/2403.09620v1", "date": "2024-03-14", "relevancy": 2.8479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5852}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5263}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything&body=Title%3A%20PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything%0AAuthor%3A%20Vibashan%20VS%20and%20Shubhankar%20Borse%20and%20Hyojin%20Park%20and%20Debasmit%20Das%20and%20Vishal%20Patel%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20open-vocabulary%20panoptic%20segmentation%20model%0Athat%20effectively%20unifies%20the%20strengths%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%0Athe%20vision-language%20CLIP%20model%20in%20an%20end-to-end%20framework.%20While%20SAM%20excels%20in%0Agenerating%20spatially-aware%20masks%2C%20it%27s%20decoder%20falls%20short%20in%20recognizing%0Aobject%20class%20information%20and%20tends%20to%20oversegment%20without%20additional%20guidance.%0AExisting%20approaches%20address%20this%20limitation%20by%20using%20multi-stage%20techniques%20and%0Aemploying%20separate%20models%20to%20generate%20class-aware%20prompts%2C%20such%20as%20bounding%0Aboxes%20or%20segmentation%20masks.%20Our%20proposed%20method%2C%20PosSAM%20is%20an%20end-to-end%20model%0Awhich%20leverages%20SAM%27s%20spatially%20rich%20features%20to%20produce%20instance-aware%20masks%0Aand%20harnesses%20CLIP%27s%20semantically%20discriminative%20features%20for%20effective%0Ainstance%20classification.%20Specifically%2C%20we%20address%20the%20limitations%20of%20SAM%20and%0Apropose%20a%20novel%20Local%20Discriminative%20Pooling%20%28LDP%29%20module%20leveraging%0Aclass-agnostic%20SAM%20and%20class-aware%20CLIP%20features%20for%20unbiased%20open-vocabulary%0Aclassification.%20Furthermore%2C%20we%20introduce%20a%20Mask-Aware%20Selective%20Ensembling%0A%28MASE%29%20algorithm%20that%20adaptively%20enhances%20the%20quality%20of%20generated%20masks%20and%0Aboosts%20the%20performance%20of%20open-vocabulary%20classification%20during%20inference%20for%0Aeach%20image.%20We%20conducted%20extensive%20experiments%20to%20demonstrate%20our%20methods%0Astrong%20generalization%20properties%20across%20multiple%20datasets%2C%20achieving%0Astate-of-the-art%20performance%20with%20substantial%20improvements%20over%20SOTA%0Aopen-vocabulary%20panoptic%20segmentation%20methods.%20In%20both%20COCO%20to%20ADE20K%20and%0AADE20K%20to%20COCO%20settings%2C%20PosSAM%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20by%20a%20large%20margin%2C%202.4%20PQ%20and%204.6%20PQ%2C%20respectively.%20Project%20Website%3A%0Ahttps%3A//vibashan.github.io/possam-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything&entry.906535625=Vibashan%20VS%20and%20Shubhankar%20Borse%20and%20Hyojin%20Park%20and%20Debasmit%20Das%20and%20Vishal%20Patel%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20open-vocabulary%20panoptic%20segmentation%20model%0Athat%20effectively%20unifies%20the%20strengths%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%0Athe%20vision-language%20CLIP%20model%20in%20an%20end-to-end%20framework.%20While%20SAM%20excels%20in%0Agenerating%20spatially-aware%20masks%2C%20it%27s%20decoder%20falls%20short%20in%20recognizing%0Aobject%20class%20information%20and%20tends%20to%20oversegment%20without%20additional%20guidance.%0AExisting%20approaches%20address%20this%20limitation%20by%20using%20multi-stage%20techniques%20and%0Aemploying%20separate%20models%20to%20generate%20class-aware%20prompts%2C%20such%20as%20bounding%0Aboxes%20or%20segmentation%20masks.%20Our%20proposed%20method%2C%20PosSAM%20is%20an%20end-to-end%20model%0Awhich%20leverages%20SAM%27s%20spatially%20rich%20features%20to%20produce%20instance-aware%20masks%0Aand%20harnesses%20CLIP%27s%20semantically%20discriminative%20features%20for%20effective%0Ainstance%20classification.%20Specifically%2C%20we%20address%20the%20limitations%20of%20SAM%20and%0Apropose%20a%20novel%20Local%20Discriminative%20Pooling%20%28LDP%29%20module%20leveraging%0Aclass-agnostic%20SAM%20and%20class-aware%20CLIP%20features%20for%20unbiased%20open-vocabulary%0Aclassification.%20Furthermore%2C%20we%20introduce%20a%20Mask-Aware%20Selective%20Ensembling%0A%28MASE%29%20algorithm%20that%20adaptively%20enhances%20the%20quality%20of%20generated%20masks%20and%0Aboosts%20the%20performance%20of%20open-vocabulary%20classification%20during%20inference%20for%0Aeach%20image.%20We%20conducted%20extensive%20experiments%20to%20demonstrate%20our%20methods%0Astrong%20generalization%20properties%20across%20multiple%20datasets%2C%20achieving%0Astate-of-the-art%20performance%20with%20substantial%20improvements%20over%20SOTA%0Aopen-vocabulary%20panoptic%20segmentation%20methods.%20In%20both%20COCO%20to%20ADE20K%20and%0AADE20K%20to%20COCO%20settings%2C%20PosSAM%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20by%20a%20large%20margin%2C%202.4%20PQ%20and%204.6%20PQ%2C%20respectively.%20Project%20Website%3A%0Ahttps%3A//vibashan.github.io/possam-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09620v1&entry.124074799=Read"},
{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "author": "Qingqiu Li and Xiaohan Yan and Jilan Xu and Runtian Yuan and Yuejie Zhang and Rui Feng and Quanli Shen and Xiaobo Zhang and Shujun Wang", "abstract": "  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.09294v1", "date": "2024-03-14", "relevancy": 2.8002, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5971}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training&body=Title%3A%20Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training%0AAuthor%3A%20Qingqiu%20Li%20and%20Xiaohan%20Yan%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Shujun%20Wang%0AAbstract%3A%20%20%20Learning%20medical%20visual%20representations%20through%20vision-language%20pre-training%0Ahas%20reached%20remarkable%20progress.%20Despite%20the%20promising%20performance%2C%20it%20still%0Afaces%20challenges%2C%20i.e.%2C%20local%20alignment%20lacks%20interpretability%20and%20clinical%0Arelevance%2C%20and%20the%20insufficient%20internal%20and%20external%20representation%20learning%0Aof%20image-report%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Anatomical%0AStructure-Guided%20%28ASG%29%20framework.%20Specifically%2C%20we%20parse%20raw%20reports%20into%0Atriplets%20%3Canatomical%20region%2C%20finding%2C%20existence%3E%2C%20and%20fully%20utilize%20each%0Aelement%20as%20supervision%20to%20enhance%20representation%20learning.%20For%20anatomical%0Aregion%2C%20we%20design%20an%20automatic%20anatomical%20region-sentence%20alignment%20paradigm%20in%0Acollaboration%20with%20radiologists%2C%20considering%20them%20as%20the%20minimum%20semantic%20units%0Ato%20explore%20fine-grained%20local%20alignment.%20For%20finding%20and%20existence%2C%20we%20regard%0Athem%20as%20image%20tags%2C%20applying%20an%20image-tag%20recognition%20decoder%20to%20associate%0Aimage%20features%20with%20their%20respective%20tags%20within%20each%20sample%20and%20constructing%0Asoft%20labels%20for%20contrastive%20learning%20to%20improve%20the%20semantic%20association%20of%0Adifferent%20image-report%20pairs.%20We%20evaluate%20the%20proposed%20ASG%20framework%20on%20two%0Adownstream%20tasks%2C%20including%20five%20public%20benchmarks.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09294v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training&entry.906535625=Qingqiu%20Li%20and%20Xiaohan%20Yan%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Shujun%20Wang&entry.1292438233=%20%20Learning%20medical%20visual%20representations%20through%20vision-language%20pre-training%0Ahas%20reached%20remarkable%20progress.%20Despite%20the%20promising%20performance%2C%20it%20still%0Afaces%20challenges%2C%20i.e.%2C%20local%20alignment%20lacks%20interpretability%20and%20clinical%0Arelevance%2C%20and%20the%20insufficient%20internal%20and%20external%20representation%20learning%0Aof%20image-report%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Anatomical%0AStructure-Guided%20%28ASG%29%20framework.%20Specifically%2C%20we%20parse%20raw%20reports%20into%0Atriplets%20%3Canatomical%20region%2C%20finding%2C%20existence%3E%2C%20and%20fully%20utilize%20each%0Aelement%20as%20supervision%20to%20enhance%20representation%20learning.%20For%20anatomical%0Aregion%2C%20we%20design%20an%20automatic%20anatomical%20region-sentence%20alignment%20paradigm%20in%0Acollaboration%20with%20radiologists%2C%20considering%20them%20as%20the%20minimum%20semantic%20units%0Ato%20explore%20fine-grained%20local%20alignment.%20For%20finding%20and%20existence%2C%20we%20regard%0Athem%20as%20image%20tags%2C%20applying%20an%20image-tag%20recognition%20decoder%20to%20associate%0Aimage%20features%20with%20their%20respective%20tags%20within%20each%20sample%20and%20constructing%0Asoft%20labels%20for%20contrastive%20learning%20to%20improve%20the%20semantic%20association%20of%0Adifferent%20image-report%20pairs.%20We%20evaluate%20the%20proposed%20ASG%20framework%20on%20two%0Adownstream%20tasks%2C%20including%20five%20public%20benchmarks.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09294v1&entry.124074799=Read"},
{"title": "Probabilistic Contrastive Learning for Long-Tailed Visual Recognition", "author": "Chaoqun Du and Yulin Wang and Shiji Song and Gao Huang", "abstract": "  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n", "link": "http://arxiv.org/abs/2403.06726v2", "date": "2024-03-14", "relevancy": 2.7962, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6068}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&body=Title%3A%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition%0AAuthor%3A%20Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06726v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&entry.906535625=Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06726v2&entry.124074799=Read"},
{"title": "Perspective-Equivariant Imaging: an Unsupervised Framework for\n  Multispectral Pansharpening", "author": "Andrew Wang and Mike Davies", "abstract": "  Ill-posed image reconstruction problems appear in many scenarios such as\nremote sensing, where obtaining high quality images is crucial for\nenvironmental monitoring, disaster management and urban planning. Deep learning\nhas seen great success in overcoming the limitations of traditional methods.\nHowever, these inverse problems rarely come with ground truth data,\nhighlighting the importance of unsupervised learning from partial and noisy\nmeasurements alone. We propose perspective-equivariant imaging (EI), a\nframework that leverages perspective variability in optical camera-based\nimaging systems, such as satellites or handheld cameras, to recover information\nlost in ill-posed optical camera imaging problems. This extends previous EI\nwork to include a much richer non-linear class of group transforms and is shown\nto be an excellent prior for satellite and urban image data, where\nperspective-EI achieves state-of-the-art results in multispectral\npansharpening, outperforming other unsupervised methods in the literature. Code\nat https://andrewwango.github.io/perspective-equivariant-imaging\n", "link": "http://arxiv.org/abs/2403.09327v1", "date": "2024-03-14", "relevancy": 2.7863, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5754}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5518}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5445}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening&body=Title%3A%20Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening%0AAuthor%3A%20Andrew%20Wang%20and%20Mike%20Davies%0AAbstract%3A%20%20%20Ill-posed%20image%20reconstruction%20problems%20appear%20in%20many%20scenarios%20such%20as%0Aremote%20sensing%2C%20where%20obtaining%20high%20quality%20images%20is%20crucial%20for%0Aenvironmental%20monitoring%2C%20disaster%20management%20and%20urban%20planning.%20Deep%20learning%0Ahas%20seen%20great%20success%20in%20overcoming%20the%20limitations%20of%20traditional%20methods.%0AHowever%2C%20these%20inverse%20problems%20rarely%20come%20with%20ground%20truth%20data%2C%0Ahighlighting%20the%20importance%20of%20unsupervised%20learning%20from%20partial%20and%20noisy%0Ameasurements%20alone.%20We%20propose%20perspective-equivariant%20imaging%20%28EI%29%2C%20a%0Aframework%20that%20leverages%20perspective%20variability%20in%20optical%20camera-based%0Aimaging%20systems%2C%20such%20as%20satellites%20or%20handheld%20cameras%2C%20to%20recover%20information%0Alost%20in%20ill-posed%20optical%20camera%20imaging%20problems.%20This%20extends%20previous%20EI%0Awork%20to%20include%20a%20much%20richer%20non-linear%20class%20of%20group%20transforms%20and%20is%20shown%0Ato%20be%20an%20excellent%20prior%20for%20satellite%20and%20urban%20image%20data%2C%20where%0Aperspective-EI%20achieves%20state-of-the-art%20results%20in%20multispectral%0Apansharpening%2C%20outperforming%20other%20unsupervised%20methods%20in%20the%20literature.%20Code%0Aat%20https%3A//andrewwango.github.io/perspective-equivariant-imaging%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09327v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening&entry.906535625=Andrew%20Wang%20and%20Mike%20Davies&entry.1292438233=%20%20Ill-posed%20image%20reconstruction%20problems%20appear%20in%20many%20scenarios%20such%20as%0Aremote%20sensing%2C%20where%20obtaining%20high%20quality%20images%20is%20crucial%20for%0Aenvironmental%20monitoring%2C%20disaster%20management%20and%20urban%20planning.%20Deep%20learning%0Ahas%20seen%20great%20success%20in%20overcoming%20the%20limitations%20of%20traditional%20methods.%0AHowever%2C%20these%20inverse%20problems%20rarely%20come%20with%20ground%20truth%20data%2C%0Ahighlighting%20the%20importance%20of%20unsupervised%20learning%20from%20partial%20and%20noisy%0Ameasurements%20alone.%20We%20propose%20perspective-equivariant%20imaging%20%28EI%29%2C%20a%0Aframework%20that%20leverages%20perspective%20variability%20in%20optical%20camera-based%0Aimaging%20systems%2C%20such%20as%20satellites%20or%20handheld%20cameras%2C%20to%20recover%20information%0Alost%20in%20ill-posed%20optical%20camera%20imaging%20problems.%20This%20extends%20previous%20EI%0Awork%20to%20include%20a%20much%20richer%20non-linear%20class%20of%20group%20transforms%20and%20is%20shown%0Ato%20be%20an%20excellent%20prior%20for%20satellite%20and%20urban%20image%20data%2C%20where%0Aperspective-EI%20achieves%20state-of-the-art%20results%20in%20multispectral%0Apansharpening%2C%20outperforming%20other%20unsupervised%20methods%20in%20the%20literature.%20Code%0Aat%20https%3A//andrewwango.github.io/perspective-equivariant-imaging%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09327v1&entry.124074799=Read"},
{"title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for\n  Hierarchical Understanding", "author": "Peng Xia and Xingtong Yu and Ming Hu and Lie Ju and Zhiyong Wang and Peibo Duan and Zongyuan Ge", "abstract": "  Object categories are typically organized into a multi-granularity taxonomic\nhierarchy. When classifying categories at different hierarchy levels,\ntraditional uni-modal approaches focus primarily on image features, revealing\nlimitations in complex scenarios. Recent studies integrating Vision-Language\nModels (VLMs) with class hierarchies have shown promise, yet they fall short of\nfully exploiting the hierarchical relationships. These efforts are constrained\nby their inability to perform effectively across varied granularity of\ncategories. To tackle this issue, we propose a novel framework (HGCLIP) that\neffectively combines CLIP with a deeper exploitation of the Hierarchical class\nstructure via Graph representation learning. We explore constructing the class\nhierarchy into a graph, with its nodes representing the textual or image\nfeatures of each category. After passing through a graph encoder, the textual\nfeatures incorporate hierarchical structure information, while the image\nfeatures emphasize class-aware features derived from prototypes through the\nattention mechanism. Our approach demonstrates significant improvements on 11\ndiverse visual recognition benchmarks. Our codes are fully available at\nhttps://github.com/richard-peng-xia/HGCLIP.\n", "link": "http://arxiv.org/abs/2311.14064v2", "date": "2024-03-14", "relevancy": 2.7639, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5618}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding&body=Title%3A%20HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding%0AAuthor%3A%20Peng%20Xia%20and%20Xingtong%20Yu%20and%20Ming%20Hu%20and%20Lie%20Ju%20and%20Zhiyong%20Wang%20and%20Peibo%20Duan%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20Object%20categories%20are%20typically%20organized%20into%20a%20multi-granularity%20taxonomic%0Ahierarchy.%20When%20classifying%20categories%20at%20different%20hierarchy%20levels%2C%0Atraditional%20uni-modal%20approaches%20focus%20primarily%20on%20image%20features%2C%20revealing%0Alimitations%20in%20complex%20scenarios.%20Recent%20studies%20integrating%20Vision-Language%0AModels%20%28VLMs%29%20with%20class%20hierarchies%20have%20shown%20promise%2C%20yet%20they%20fall%20short%20of%0Afully%20exploiting%20the%20hierarchical%20relationships.%20These%20efforts%20are%20constrained%0Aby%20their%20inability%20to%20perform%20effectively%20across%20varied%20granularity%20of%0Acategories.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20%28HGCLIP%29%20that%0Aeffectively%20combines%20CLIP%20with%20a%20deeper%20exploitation%20of%20the%20Hierarchical%20class%0Astructure%20via%20Graph%20representation%20learning.%20We%20explore%20constructing%20the%20class%0Ahierarchy%20into%20a%20graph%2C%20with%20its%20nodes%20representing%20the%20textual%20or%20image%0Afeatures%20of%20each%20category.%20After%20passing%20through%20a%20graph%20encoder%2C%20the%20textual%0Afeatures%20incorporate%20hierarchical%20structure%20information%2C%20while%20the%20image%0Afeatures%20emphasize%20class-aware%20features%20derived%20from%20prototypes%20through%20the%0Aattention%20mechanism.%20Our%20approach%20demonstrates%20significant%20improvements%20on%2011%0Adiverse%20visual%20recognition%20benchmarks.%20Our%20codes%20are%20fully%20available%20at%0Ahttps%3A//github.com/richard-peng-xia/HGCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14064v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding&entry.906535625=Peng%20Xia%20and%20Xingtong%20Yu%20and%20Ming%20Hu%20and%20Lie%20Ju%20and%20Zhiyong%20Wang%20and%20Peibo%20Duan%20and%20Zongyuan%20Ge&entry.1292438233=%20%20Object%20categories%20are%20typically%20organized%20into%20a%20multi-granularity%20taxonomic%0Ahierarchy.%20When%20classifying%20categories%20at%20different%20hierarchy%20levels%2C%0Atraditional%20uni-modal%20approaches%20focus%20primarily%20on%20image%20features%2C%20revealing%0Alimitations%20in%20complex%20scenarios.%20Recent%20studies%20integrating%20Vision-Language%0AModels%20%28VLMs%29%20with%20class%20hierarchies%20have%20shown%20promise%2C%20yet%20they%20fall%20short%20of%0Afully%20exploiting%20the%20hierarchical%20relationships.%20These%20efforts%20are%20constrained%0Aby%20their%20inability%20to%20perform%20effectively%20across%20varied%20granularity%20of%0Acategories.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20%28HGCLIP%29%20that%0Aeffectively%20combines%20CLIP%20with%20a%20deeper%20exploitation%20of%20the%20Hierarchical%20class%0Astructure%20via%20Graph%20representation%20learning.%20We%20explore%20constructing%20the%20class%0Ahierarchy%20into%20a%20graph%2C%20with%20its%20nodes%20representing%20the%20textual%20or%20image%0Afeatures%20of%20each%20category.%20After%20passing%20through%20a%20graph%20encoder%2C%20the%20textual%0Afeatures%20incorporate%20hierarchical%20structure%20information%2C%20while%20the%20image%0Afeatures%20emphasize%20class-aware%20features%20derived%20from%20prototypes%20through%20the%0Aattention%20mechanism.%20Our%20approach%20demonstrates%20significant%20improvements%20on%2011%0Adiverse%20visual%20recognition%20benchmarks.%20Our%20codes%20are%20fully%20available%20at%0Ahttps%3A//github.com/richard-peng-xia/HGCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14064v2&entry.124074799=Read"},
{"title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap\n  for Domain-Adaptive Object Detection", "author": "Dinh Phat Do and Taehoon Kim and Jaemin Na and Jiwon Kim and Keonho Lee and Kyunghwan Cho and Wonjun Hwang", "abstract": "  Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .\n", "link": "http://arxiv.org/abs/2403.09359v1", "date": "2024-03-14", "relevancy": 2.7576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5943}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5552}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.505}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection&body=Title%3A%20D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection%0AAuthor%3A%20Dinh%20Phat%20Do%20and%20Taehoon%20Kim%20and%20Jaemin%20Na%20and%20Jiwon%20Kim%20and%20Keonho%20Lee%20and%20Kyunghwan%20Cho%20and%20Wonjun%20Hwang%0AAbstract%3A%20%20%20Domain%20adaptation%20for%20object%20detection%20typically%20entails%20transferring%0Aknowledge%20from%20one%20visible%20domain%20to%20another%20visible%20domain.%20However%2C%20there%20are%0Alimited%20studies%20on%20adapting%20from%20the%20visible%20to%20the%20thermal%20domain%2C%20because%20the%0Adomain%20gap%20between%20the%20visible%20and%20thermal%20domains%20is%20much%20larger%20than%0Aexpected%2C%20and%20traditional%20domain%20adaptation%20can%20not%20successfully%20facilitate%0Alearning%20in%20this%20situation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0ADistinctive%20Dual-Domain%20Teacher%20%28D3T%29%20framework%20that%20employs%20distinct%20training%0Aparadigms%20for%20each%20domain.%20Specifically%2C%20we%20segregate%20the%20source%20and%20target%0Atraining%20sets%20for%20building%20dual-teachers%20and%20successively%20deploy%20exponential%0Amoving%20average%20to%20the%20student%20model%20to%20individual%20teachers%20of%20each%20domain.%20The%0Aframework%20further%20incorporates%20a%20zigzag%20learning%20method%20between%20dual%20teachers%2C%0Afacilitating%20a%20gradual%20transition%20from%20the%20visible%20to%20thermal%20domains%20during%0Atraining.%20We%20validate%20the%20superiority%20of%20our%20method%20through%20newly%20designed%0Aexperimental%20protocols%20with%20well-known%20thermal%20datasets%2C%20i.e.%2C%20FLIR%20and%20KAIST.%0ASource%20code%20is%20available%20at%20https%3A//github.com/EdwardDo69/D3T%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09359v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection&entry.906535625=Dinh%20Phat%20Do%20and%20Taehoon%20Kim%20and%20Jaemin%20Na%20and%20Jiwon%20Kim%20and%20Keonho%20Lee%20and%20Kyunghwan%20Cho%20and%20Wonjun%20Hwang&entry.1292438233=%20%20Domain%20adaptation%20for%20object%20detection%20typically%20entails%20transferring%0Aknowledge%20from%20one%20visible%20domain%20to%20another%20visible%20domain.%20However%2C%20there%20are%0Alimited%20studies%20on%20adapting%20from%20the%20visible%20to%20the%20thermal%20domain%2C%20because%20the%0Adomain%20gap%20between%20the%20visible%20and%20thermal%20domains%20is%20much%20larger%20than%0Aexpected%2C%20and%20traditional%20domain%20adaptation%20can%20not%20successfully%20facilitate%0Alearning%20in%20this%20situation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0ADistinctive%20Dual-Domain%20Teacher%20%28D3T%29%20framework%20that%20employs%20distinct%20training%0Aparadigms%20for%20each%20domain.%20Specifically%2C%20we%20segregate%20the%20source%20and%20target%0Atraining%20sets%20for%20building%20dual-teachers%20and%20successively%20deploy%20exponential%0Amoving%20average%20to%20the%20student%20model%20to%20individual%20teachers%20of%20each%20domain.%20The%0Aframework%20further%20incorporates%20a%20zigzag%20learning%20method%20between%20dual%20teachers%2C%0Afacilitating%20a%20gradual%20transition%20from%20the%20visible%20to%20thermal%20domains%20during%0Atraining.%20We%20validate%20the%20superiority%20of%20our%20method%20through%20newly%20designed%0Aexperimental%20protocols%20with%20well-known%20thermal%20datasets%2C%20i.e.%2C%20FLIR%20and%20KAIST.%0ASource%20code%20is%20available%20at%20https%3A//github.com/EdwardDo69/D3T%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09359v1&entry.124074799=Read"},
{"title": "TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions", "author": "Maytus Piriyajitakonkij and Mingfei Sun and Mengmi Zhang and Wei Pan", "abstract": "  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n", "link": "http://arxiv.org/abs/2403.01977v2", "date": "2024-03-14", "relevancy": 2.7311, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions&body=Title%3A%20TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions%0AAuthor%3A%20Maytus%20Piriyajitakonkij%20and%20Mingfei%20Sun%20and%20Mengmi%20Zhang%20and%20Wei%20Pan%0AAbstract%3A%20%20%20Robot%20navigation%20under%20visual%20corruption%20presents%20a%20formidable%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20a%20Test-time%20Adaptation%20%28TTA%29%20method%2C%20named%20as%20TTA-Nav%2C%0Afor%20point-goal%20navigation%20under%20visual%20corruptions.%20Our%20%22plug-and-play%22%20method%0Aincorporates%20a%20top-down%20decoder%20to%20a%20pre-trained%20navigation%20model.%20Firstly%2C%20the%0Apre-trained%20navigation%20model%20gets%20a%20corrupted%20image%20and%20extracts%20features.%0ASecondly%2C%20the%20top-down%20decoder%20produces%20the%20reconstruction%20given%20the%20high-level%0Afeatures%20extracted%20by%20the%20pre-trained%20model.%20Then%2C%20it%20feeds%20the%20reconstruction%0Aof%20a%20corrupted%20image%20back%20to%20the%20pre-trained%20model.%20Finally%2C%20the%20pre-trained%0Amodel%20does%20forward%20pass%20again%20to%20output%20action.%20Despite%20being%20trained%20solely%20on%0Aclean%20images%2C%20the%20top-down%20decoder%20can%20reconstruct%20cleaner%20images%20from%0Acorrupted%20ones%20without%20the%20need%20for%20gradient-based%20adaptation.%20The%20pre-trained%0Anavigation%20model%20with%20our%20top-down%20decoder%20significantly%20enhances%20navigation%0Aperformance%20across%20almost%20all%20visual%20corruptions%20in%20our%20benchmarks.%20Our%20method%0Aimproves%20the%20success%20rate%20of%20point-goal%20navigation%20from%20the%20state-of-the-art%0Aresult%20of%2046%25%20to%2094%25%20on%20the%20most%20severe%20corruption.%20This%20suggests%20its%20potential%0Afor%20broader%20application%20in%20robotic%20visual%20navigation.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/tta-nav%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01977v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions&entry.906535625=Maytus%20Piriyajitakonkij%20and%20Mingfei%20Sun%20and%20Mengmi%20Zhang%20and%20Wei%20Pan&entry.1292438233=%20%20Robot%20navigation%20under%20visual%20corruption%20presents%20a%20formidable%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20a%20Test-time%20Adaptation%20%28TTA%29%20method%2C%20named%20as%20TTA-Nav%2C%0Afor%20point-goal%20navigation%20under%20visual%20corruptions.%20Our%20%22plug-and-play%22%20method%0Aincorporates%20a%20top-down%20decoder%20to%20a%20pre-trained%20navigation%20model.%20Firstly%2C%20the%0Apre-trained%20navigation%20model%20gets%20a%20corrupted%20image%20and%20extracts%20features.%0ASecondly%2C%20the%20top-down%20decoder%20produces%20the%20reconstruction%20given%20the%20high-level%0Afeatures%20extracted%20by%20the%20pre-trained%20model.%20Then%2C%20it%20feeds%20the%20reconstruction%0Aof%20a%20corrupted%20image%20back%20to%20the%20pre-trained%20model.%20Finally%2C%20the%20pre-trained%0Amodel%20does%20forward%20pass%20again%20to%20output%20action.%20Despite%20being%20trained%20solely%20on%0Aclean%20images%2C%20the%20top-down%20decoder%20can%20reconstruct%20cleaner%20images%20from%0Acorrupted%20ones%20without%20the%20need%20for%20gradient-based%20adaptation.%20The%20pre-trained%0Anavigation%20model%20with%20our%20top-down%20decoder%20significantly%20enhances%20navigation%0Aperformance%20across%20almost%20all%20visual%20corruptions%20in%20our%20benchmarks.%20Our%20method%0Aimproves%20the%20success%20rate%20of%20point-goal%20navigation%20from%20the%20state-of-the-art%0Aresult%20of%2046%25%20to%2094%25%20on%20the%20most%20severe%20corruption.%20This%20suggests%20its%20potential%0Afor%20broader%20application%20in%20robotic%20visual%20navigation.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/tta-nav%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01977v2&entry.124074799=Read"},
{"title": "LocalMamba: Visual State Space Model with Windowed Selective Scan", "author": "Tao Huang and Xiaohuan Pei and Shan You and Fei Wang and Chen Qian and Chang Xu", "abstract": "  Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.\n", "link": "http://arxiv.org/abs/2403.09338v1", "date": "2024-03-14", "relevancy": 2.7301, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan&body=Title%3A%20LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan%0AAuthor%3A%20Tao%20Huang%20and%20Xiaohuan%20Pei%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20state%20space%20models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asignificant%20progress%20in%20modeling%20long%20sequences%20for%20tasks%20like%20language%0Aunderstanding.%20Yet%2C%20their%20application%20in%20vision%20tasks%20has%20not%20markedly%0Asurpassed%20the%20performance%20of%20traditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Vision%20Transformers%20%28ViTs%29.%20This%20paper%20posits%20that%20the%20key%20to%20enhancing%0AVision%20Mamba%20%28ViM%29%20lies%20in%20optimizing%20scan%20directions%20for%20sequence%20modeling.%0ATraditional%20ViM%20approaches%2C%20which%20flatten%20spatial%20tokens%2C%20overlook%20the%0Apreservation%20of%20local%202D%20dependencies%2C%20thereby%20elongating%20the%20distance%20between%0Aadjacent%20tokens.%20We%20introduce%20a%20novel%20local%20scanning%20strategy%20that%20divides%0Aimages%20into%20distinct%20windows%2C%20effectively%20capturing%20local%20dependencies%20while%0Amaintaining%20a%20global%20perspective.%20Additionally%2C%20acknowledging%20the%20varying%0Apreferences%20for%20scan%20patterns%20across%20different%20network%20layers%2C%20we%20propose%20a%0Adynamic%20method%20to%20independently%20search%20for%20the%20optimal%20scan%20choices%20for%20each%0Alayer%2C%20substantially%20improving%20performance.%20Extensive%20experiments%20across%20both%0Aplain%20and%20hierarchical%20models%20underscore%20our%20approach%27s%20superiority%20in%0Aeffectively%20capturing%20image%20representations.%20For%20example%2C%20our%20model%0Asignificantly%20outperforms%20Vim-Ti%20by%203.1%25%20on%20ImageNet%20with%20the%20same%201.5G%20FLOPs.%0ACode%20is%20available%20at%3A%20https%3A//github.com/hunto/LocalMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09338v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan&entry.906535625=Tao%20Huang%20and%20Xiaohuan%20Pei%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Chang%20Xu&entry.1292438233=%20%20Recent%20advancements%20in%20state%20space%20models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asignificant%20progress%20in%20modeling%20long%20sequences%20for%20tasks%20like%20language%0Aunderstanding.%20Yet%2C%20their%20application%20in%20vision%20tasks%20has%20not%20markedly%0Asurpassed%20the%20performance%20of%20traditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Vision%20Transformers%20%28ViTs%29.%20This%20paper%20posits%20that%20the%20key%20to%20enhancing%0AVision%20Mamba%20%28ViM%29%20lies%20in%20optimizing%20scan%20directions%20for%20sequence%20modeling.%0ATraditional%20ViM%20approaches%2C%20which%20flatten%20spatial%20tokens%2C%20overlook%20the%0Apreservation%20of%20local%202D%20dependencies%2C%20thereby%20elongating%20the%20distance%20between%0Aadjacent%20tokens.%20We%20introduce%20a%20novel%20local%20scanning%20strategy%20that%20divides%0Aimages%20into%20distinct%20windows%2C%20effectively%20capturing%20local%20dependencies%20while%0Amaintaining%20a%20global%20perspective.%20Additionally%2C%20acknowledging%20the%20varying%0Apreferences%20for%20scan%20patterns%20across%20different%20network%20layers%2C%20we%20propose%20a%0Adynamic%20method%20to%20independently%20search%20for%20the%20optimal%20scan%20choices%20for%20each%0Alayer%2C%20substantially%20improving%20performance.%20Extensive%20experiments%20across%20both%0Aplain%20and%20hierarchical%20models%20underscore%20our%20approach%27s%20superiority%20in%0Aeffectively%20capturing%20image%20representations.%20For%20example%2C%20our%20model%0Asignificantly%20outperforms%20Vim-Ti%20by%203.1%25%20on%20ImageNet%20with%20the%20same%201.5G%20FLOPs.%0ACode%20is%20available%20at%3A%20https%3A//github.com/hunto/LocalMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09338v1&entry.124074799=Read"},
{"title": "DyRA: Portable Dynamic Resolution Adjustment Network for Existing\n  Detectors", "author": "Daeun Seo and Hoeseok Yang and Hyungshin Kim", "abstract": "  Achieving constant accuracy in object detection is challenging due to the\ninherent variability of object sizes. One effective approach to this problem\ninvolves optimizing input resolution, referred to as a multi-resolution\nstrategy. Previous approaches to resolution optimization have often been based\non pre-defined resolutions with manual selection. However, there is a lack of\nstudy on run-time resolution optimization for existing architectures. This\npaper introduces DyRA, a dynamic resolution adjustment network providing an\nimage-specific scale factor for existing detectors. This network is co-trained\nwith detectors utilizing specially designed loss functions, namely\nParetoScaleLoss and BalanceLoss. ParetoScaleLoss determines an adaptive scale\nfactor for robustness, while BalanceLoss optimizes overall scale factors\naccording to the localization performance of the detector. The loss function is\ndevised to minimize the accuracy drop across contrasting objectives of\ndifferent-sized objects for scaling. Our proposed network can improve accuracy\nacross various models, including RetinaNet, Faster-RCNN, FCOS, DINO, and\nH-Deformable-DETR. The code is available at\nhttps://github.com/DaEunFullGrace/DyRA.git.\n", "link": "http://arxiv.org/abs/2311.17098v3", "date": "2024-03-14", "relevancy": 2.7099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5231}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors&body=Title%3A%20DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors%0AAuthor%3A%20Daeun%20Seo%20and%20Hoeseok%20Yang%20and%20Hyungshin%20Kim%0AAbstract%3A%20%20%20Achieving%20constant%20accuracy%20in%20object%20detection%20is%20challenging%20due%20to%20the%0Ainherent%20variability%20of%20object%20sizes.%20One%20effective%20approach%20to%20this%20problem%0Ainvolves%20optimizing%20input%20resolution%2C%20referred%20to%20as%20a%20multi-resolution%0Astrategy.%20Previous%20approaches%20to%20resolution%20optimization%20have%20often%20been%20based%0Aon%20pre-defined%20resolutions%20with%20manual%20selection.%20However%2C%20there%20is%20a%20lack%20of%0Astudy%20on%20run-time%20resolution%20optimization%20for%20existing%20architectures.%20This%0Apaper%20introduces%20DyRA%2C%20a%20dynamic%20resolution%20adjustment%20network%20providing%20an%0Aimage-specific%20scale%20factor%20for%20existing%20detectors.%20This%20network%20is%20co-trained%0Awith%20detectors%20utilizing%20specially%20designed%20loss%20functions%2C%20namely%0AParetoScaleLoss%20and%20BalanceLoss.%20ParetoScaleLoss%20determines%20an%20adaptive%20scale%0Afactor%20for%20robustness%2C%20while%20BalanceLoss%20optimizes%20overall%20scale%20factors%0Aaccording%20to%20the%20localization%20performance%20of%20the%20detector.%20The%20loss%20function%20is%0Adevised%20to%20minimize%20the%20accuracy%20drop%20across%20contrasting%20objectives%20of%0Adifferent-sized%20objects%20for%20scaling.%20Our%20proposed%20network%20can%20improve%20accuracy%0Aacross%20various%20models%2C%20including%20RetinaNet%2C%20Faster-RCNN%2C%20FCOS%2C%20DINO%2C%20and%0AH-Deformable-DETR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DaEunFullGrace/DyRA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17098v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors&entry.906535625=Daeun%20Seo%20and%20Hoeseok%20Yang%20and%20Hyungshin%20Kim&entry.1292438233=%20%20Achieving%20constant%20accuracy%20in%20object%20detection%20is%20challenging%20due%20to%20the%0Ainherent%20variability%20of%20object%20sizes.%20One%20effective%20approach%20to%20this%20problem%0Ainvolves%20optimizing%20input%20resolution%2C%20referred%20to%20as%20a%20multi-resolution%0Astrategy.%20Previous%20approaches%20to%20resolution%20optimization%20have%20often%20been%20based%0Aon%20pre-defined%20resolutions%20with%20manual%20selection.%20However%2C%20there%20is%20a%20lack%20of%0Astudy%20on%20run-time%20resolution%20optimization%20for%20existing%20architectures.%20This%0Apaper%20introduces%20DyRA%2C%20a%20dynamic%20resolution%20adjustment%20network%20providing%20an%0Aimage-specific%20scale%20factor%20for%20existing%20detectors.%20This%20network%20is%20co-trained%0Awith%20detectors%20utilizing%20specially%20designed%20loss%20functions%2C%20namely%0AParetoScaleLoss%20and%20BalanceLoss.%20ParetoScaleLoss%20determines%20an%20adaptive%20scale%0Afactor%20for%20robustness%2C%20while%20BalanceLoss%20optimizes%20overall%20scale%20factors%0Aaccording%20to%20the%20localization%20performance%20of%20the%20detector.%20The%20loss%20function%20is%0Adevised%20to%20minimize%20the%20accuracy%20drop%20across%20contrasting%20objectives%20of%0Adifferent-sized%20objects%20for%20scaling.%20Our%20proposed%20network%20can%20improve%20accuracy%0Aacross%20various%20models%2C%20including%20RetinaNet%2C%20Faster-RCNN%2C%20FCOS%2C%20DINO%2C%20and%0AH-Deformable-DETR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DaEunFullGrace/DyRA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17098v3&entry.124074799=Read"},
{"title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly\n  Detection", "author": "Qihang Zhou and Guansong Pang and Yu Tian and Shibo He and Jiming Chen", "abstract": "  Zero-shot anomaly detection (ZSAD) requires detection models trained using\nauxiliary data to detect anomalies without any training sample in a target\ndataset. It is a crucial task when training data is not accessible due to\nvarious concerns, eg, data privacy, yet it is challenging since the models need\nto generalize to anomalies across different domains where the appearance of\nforeground objects, abnormal regions, and background features, such as\ndefects/tumors on different products/organs, can vary significantly. Recently\nlarge pre-trained vision-language models (VLMs), such as CLIP, have\ndemonstrated strong zero-shot recognition ability in various vision tasks,\nincluding anomaly detection. However, their ZSAD performance is weak since the\nVLMs focus more on modeling the class semantics of the foreground objects\nrather than the abnormality/normality in the images. In this paper we introduce\na novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across\ndifferent domains. The key insight of AnomalyCLIP is to learn object-agnostic\ntext prompts that capture generic normality and abnormality in an image\nregardless of its foreground objects. This allows our model to focus on the\nabnormal image regions rather than the object semantics, enabling generalized\nnormality and abnormality recognition on diverse types of objects. Large-scale\nexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIP\nachieves superior zero-shot performance of detecting and segmenting anomalies\nin datasets of highly diverse class semantics from various defect inspection\nand medical imaging domains. Code will be made available at\nhttps://github.com/zqhang/AnomalyCLIP.\n", "link": "http://arxiv.org/abs/2310.18961v7", "date": "2024-03-14", "relevancy": 2.678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5801}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection&body=Title%3A%20AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Qihang%20Zhou%20and%20Guansong%20Pang%20and%20Yu%20Tian%20and%20Shibo%20He%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20requires%20detection%20models%20trained%20using%0Aauxiliary%20data%20to%20detect%20anomalies%20without%20any%20training%20sample%20in%20a%20target%0Adataset.%20It%20is%20a%20crucial%20task%20when%20training%20data%20is%20not%20accessible%20due%20to%0Avarious%20concerns%2C%20eg%2C%20data%20privacy%2C%20yet%20it%20is%20challenging%20since%20the%20models%20need%0Ato%20generalize%20to%20anomalies%20across%20different%20domains%20where%20the%20appearance%20of%0Aforeground%20objects%2C%20abnormal%20regions%2C%20and%20background%20features%2C%20such%20as%0Adefects/tumors%20on%20different%20products/organs%2C%20can%20vary%20significantly.%20Recently%0Alarge%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Ademonstrated%20strong%20zero-shot%20recognition%20ability%20in%20various%20vision%20tasks%2C%0Aincluding%20anomaly%20detection.%20However%2C%20their%20ZSAD%20performance%20is%20weak%20since%20the%0AVLMs%20focus%20more%20on%20modeling%20the%20class%20semantics%20of%20the%20foreground%20objects%0Arather%20than%20the%20abnormality/normality%20in%20the%20images.%20In%20this%20paper%20we%20introduce%0Aa%20novel%20approach%2C%20namely%20AnomalyCLIP%2C%20to%20adapt%20CLIP%20for%20accurate%20ZSAD%20across%0Adifferent%20domains.%20The%20key%20insight%20of%20AnomalyCLIP%20is%20to%20learn%20object-agnostic%0Atext%20prompts%20that%20capture%20generic%20normality%20and%20abnormality%20in%20an%20image%0Aregardless%20of%20its%20foreground%20objects.%20This%20allows%20our%20model%20to%20focus%20on%20the%0Aabnormal%20image%20regions%20rather%20than%20the%20object%20semantics%2C%20enabling%20generalized%0Anormality%20and%20abnormality%20recognition%20on%20diverse%20types%20of%20objects.%20Large-scale%0Aexperiments%20on%2017%20real-world%20anomaly%20detection%20datasets%20show%20that%20AnomalyCLIP%0Aachieves%20superior%20zero-shot%20performance%20of%20detecting%20and%20segmenting%20anomalies%0Ain%20datasets%20of%20highly%20diverse%20class%20semantics%20from%20various%20defect%20inspection%0Aand%20medical%20imaging%20domains.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/zqhang/AnomalyCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18961v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection&entry.906535625=Qihang%20Zhou%20and%20Guansong%20Pang%20and%20Yu%20Tian%20and%20Shibo%20He%20and%20Jiming%20Chen&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20requires%20detection%20models%20trained%20using%0Aauxiliary%20data%20to%20detect%20anomalies%20without%20any%20training%20sample%20in%20a%20target%0Adataset.%20It%20is%20a%20crucial%20task%20when%20training%20data%20is%20not%20accessible%20due%20to%0Avarious%20concerns%2C%20eg%2C%20data%20privacy%2C%20yet%20it%20is%20challenging%20since%20the%20models%20need%0Ato%20generalize%20to%20anomalies%20across%20different%20domains%20where%20the%20appearance%20of%0Aforeground%20objects%2C%20abnormal%20regions%2C%20and%20background%20features%2C%20such%20as%0Adefects/tumors%20on%20different%20products/organs%2C%20can%20vary%20significantly.%20Recently%0Alarge%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Ademonstrated%20strong%20zero-shot%20recognition%20ability%20in%20various%20vision%20tasks%2C%0Aincluding%20anomaly%20detection.%20However%2C%20their%20ZSAD%20performance%20is%20weak%20since%20the%0AVLMs%20focus%20more%20on%20modeling%20the%20class%20semantics%20of%20the%20foreground%20objects%0Arather%20than%20the%20abnormality/normality%20in%20the%20images.%20In%20this%20paper%20we%20introduce%0Aa%20novel%20approach%2C%20namely%20AnomalyCLIP%2C%20to%20adapt%20CLIP%20for%20accurate%20ZSAD%20across%0Adifferent%20domains.%20The%20key%20insight%20of%20AnomalyCLIP%20is%20to%20learn%20object-agnostic%0Atext%20prompts%20that%20capture%20generic%20normality%20and%20abnormality%20in%20an%20image%0Aregardless%20of%20its%20foreground%20objects.%20This%20allows%20our%20model%20to%20focus%20on%20the%0Aabnormal%20image%20regions%20rather%20than%20the%20object%20semantics%2C%20enabling%20generalized%0Anormality%20and%20abnormality%20recognition%20on%20diverse%20types%20of%20objects.%20Large-scale%0Aexperiments%20on%2017%20real-world%20anomaly%20detection%20datasets%20show%20that%20AnomalyCLIP%0Aachieves%20superior%20zero-shot%20performance%20of%20detecting%20and%20segmenting%20anomalies%0Ain%20datasets%20of%20highly%20diverse%20class%20semantics%20from%20various%20defect%20inspection%0Aand%20medical%20imaging%20domains.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/zqhang/AnomalyCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18961v7&entry.124074799=Read"},
{"title": "DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local\n  Climate Zone Classification", "author": "Qianqian Wu and Xianping Ma and Jialu Sui and Man-On Pun", "abstract": "  Recent advancements in remote sensing (RS) technologies have shown their\npotential in accurately classifying local climate zones (LCZs). However,\ntraditional scene-level methods using convolutional neural networks (CNNs)\noften struggle to integrate prior knowledge of ground objects effectively.\nMoreover, commonly utilized data sources like Sentinel-2 encounter difficulties\nin capturing detailed ground object information. To tackle these challenges, we\npropose a data fusion method that integrates ground object priors extracted\nfrom high-resolution Google imagery with Sentinel-2 multispectral imagery. The\nproposed method introduces a novel Dual-stream Fusion framework for LCZ\nclassification (DF4LCZ), integrating instance-based location features from\nGoogle imagery with the scene-level spatial-spectral features extracted from\nSentinel-2 imagery. The framework incorporates a Graph Convolutional Network\n(GCN) module empowered by the Segment Anything Model (SAM) to enhance feature\nextraction from Google imagery. Simultaneously, the framework employs a 3D-CNN\narchitecture to learn the spectral-spatial features of Sentinel-2 imagery.\nExperiments are conducted on a multi-source remote sensing image dataset\nspecifically designed for LCZ classification, validating the effectiveness of\nthe proposed DF4LCZ. The related code and dataset are available at\nhttps://github.com/ctrlovefly/DF4LCZ.\n", "link": "http://arxiv.org/abs/2403.09367v1", "date": "2024-03-14", "relevancy": 2.67, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification&body=Title%3A%20DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification%0AAuthor%3A%20Qianqian%20Wu%20and%20Xianping%20Ma%20and%20Jialu%20Sui%20and%20Man-On%20Pun%0AAbstract%3A%20%20%20Recent%20advancements%20in%20remote%20sensing%20%28RS%29%20technologies%20have%20shown%20their%0Apotential%20in%20accurately%20classifying%20local%20climate%20zones%20%28LCZs%29.%20However%2C%0Atraditional%20scene-level%20methods%20using%20convolutional%20neural%20networks%20%28CNNs%29%0Aoften%20struggle%20to%20integrate%20prior%20knowledge%20of%20ground%20objects%20effectively.%0AMoreover%2C%20commonly%20utilized%20data%20sources%20like%20Sentinel-2%20encounter%20difficulties%0Ain%20capturing%20detailed%20ground%20object%20information.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20data%20fusion%20method%20that%20integrates%20ground%20object%20priors%20extracted%0Afrom%20high-resolution%20Google%20imagery%20with%20Sentinel-2%20multispectral%20imagery.%20The%0Aproposed%20method%20introduces%20a%20novel%20Dual-stream%20Fusion%20framework%20for%20LCZ%0Aclassification%20%28DF4LCZ%29%2C%20integrating%20instance-based%20location%20features%20from%0AGoogle%20imagery%20with%20the%20scene-level%20spatial-spectral%20features%20extracted%20from%0ASentinel-2%20imagery.%20The%20framework%20incorporates%20a%20Graph%20Convolutional%20Network%0A%28GCN%29%20module%20empowered%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20feature%0Aextraction%20from%20Google%20imagery.%20Simultaneously%2C%20the%20framework%20employs%20a%203D-CNN%0Aarchitecture%20to%20learn%20the%20spectral-spatial%20features%20of%20Sentinel-2%20imagery.%0AExperiments%20are%20conducted%20on%20a%20multi-source%20remote%20sensing%20image%20dataset%0Aspecifically%20designed%20for%20LCZ%20classification%2C%20validating%20the%20effectiveness%20of%0Athe%20proposed%20DF4LCZ.%20The%20related%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ctrlovefly/DF4LCZ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09367v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification&entry.906535625=Qianqian%20Wu%20and%20Xianping%20Ma%20and%20Jialu%20Sui%20and%20Man-On%20Pun&entry.1292438233=%20%20Recent%20advancements%20in%20remote%20sensing%20%28RS%29%20technologies%20have%20shown%20their%0Apotential%20in%20accurately%20classifying%20local%20climate%20zones%20%28LCZs%29.%20However%2C%0Atraditional%20scene-level%20methods%20using%20convolutional%20neural%20networks%20%28CNNs%29%0Aoften%20struggle%20to%20integrate%20prior%20knowledge%20of%20ground%20objects%20effectively.%0AMoreover%2C%20commonly%20utilized%20data%20sources%20like%20Sentinel-2%20encounter%20difficulties%0Ain%20capturing%20detailed%20ground%20object%20information.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20data%20fusion%20method%20that%20integrates%20ground%20object%20priors%20extracted%0Afrom%20high-resolution%20Google%20imagery%20with%20Sentinel-2%20multispectral%20imagery.%20The%0Aproposed%20method%20introduces%20a%20novel%20Dual-stream%20Fusion%20framework%20for%20LCZ%0Aclassification%20%28DF4LCZ%29%2C%20integrating%20instance-based%20location%20features%20from%0AGoogle%20imagery%20with%20the%20scene-level%20spatial-spectral%20features%20extracted%20from%0ASentinel-2%20imagery.%20The%20framework%20incorporates%20a%20Graph%20Convolutional%20Network%0A%28GCN%29%20module%20empowered%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20feature%0Aextraction%20from%20Google%20imagery.%20Simultaneously%2C%20the%20framework%20employs%20a%203D-CNN%0Aarchitecture%20to%20learn%20the%20spectral-spatial%20features%20of%20Sentinel-2%20imagery.%0AExperiments%20are%20conducted%20on%20a%20multi-source%20remote%20sensing%20image%20dataset%0Aspecifically%20designed%20for%20LCZ%20classification%2C%20validating%20the%20effectiveness%20of%0Athe%20proposed%20DF4LCZ.%20The%20related%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ctrlovefly/DF4LCZ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09367v1&entry.124074799=Read"},
{"title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatial Relation Matching", "author": "Meng Chu and Zhedong Zheng and Wei Ji and Tingyu Wang and Tat-Seng Chua", "abstract": "  Navigating drones through natural language commands remains challenging due\nto the dearth of accessible multi-modal datasets and the stringent precision\nrequirements for aligning visual and textual data. To address this pressing\nneed, we introduce GeoText-1652, a new natural language-guided geo-localization\nbenchmark. This dataset is systematically constructed through an interactive\nhuman-computer process leveraging Large Language Model (LLM) driven annotation\ntechniques in conjunction with pre-trained vision models. GeoText-1652 extends\nthe established University-1652 image dataset with spatial-aware text\nannotations, thereby establishing one-to-one correspondences between image,\ntext, and bounding box elements. We further introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains a competitive recall rate\ncomparing other prevailing cross-modality methods. This underscores the\npromising potential of our approach in elevating drone control and navigation\nthrough the seamless integration of natural language commands in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2311.12751v2", "date": "2024-03-14", "relevancy": 2.6635, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5414}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching&body=Title%3A%20Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching%0AAuthor%3A%20Meng%20Chu%20and%20Zhedong%20Zheng%20and%20Wei%20Ji%20and%20Tingyu%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Navigating%20drones%20through%20natural%20language%20commands%20remains%20challenging%20due%0Ato%20the%20dearth%20of%20accessible%20multi-modal%20datasets%20and%20the%20stringent%20precision%0Arequirements%20for%20aligning%20visual%20and%20textual%20data.%20To%20address%20this%20pressing%0Aneed%2C%20we%20introduce%20GeoText-1652%2C%20a%20new%20natural%20language-guided%20geo-localization%0Abenchmark.%20This%20dataset%20is%20systematically%20constructed%20through%20an%20interactive%0Ahuman-computer%20process%20leveraging%20Large%20Language%20Model%20%28LLM%29%20driven%20annotation%0Atechniques%20in%20conjunction%20with%20pre-trained%20vision%20models.%20GeoText-1652%20extends%0Athe%20established%20University-1652%20image%20dataset%20with%20spatial-aware%20text%0Aannotations%2C%20thereby%20establishing%20one-to-one%20correspondences%20between%20image%2C%0Atext%2C%20and%20bounding%20box%20elements.%20We%20further%20introduce%20a%20new%20optimization%0Aobjective%20to%20leverage%20fine-grained%20spatial%20associations%2C%20called%20blending%0Aspatial%20matching%2C%20for%20region-level%20spatial%20relation%20matching.%20Extensive%0Aexperiments%20reveal%20that%20our%20approach%20maintains%20a%20competitive%20recall%20rate%0Acomparing%20other%20prevailing%20cross-modality%20methods.%20This%20underscores%20the%0Apromising%20potential%20of%20our%20approach%20in%20elevating%20drone%20control%20and%20navigation%0Athrough%20the%20seamless%20integration%20of%20natural%20language%20commands%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12751v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching&entry.906535625=Meng%20Chu%20and%20Zhedong%20Zheng%20and%20Wei%20Ji%20and%20Tingyu%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Navigating%20drones%20through%20natural%20language%20commands%20remains%20challenging%20due%0Ato%20the%20dearth%20of%20accessible%20multi-modal%20datasets%20and%20the%20stringent%20precision%0Arequirements%20for%20aligning%20visual%20and%20textual%20data.%20To%20address%20this%20pressing%0Aneed%2C%20we%20introduce%20GeoText-1652%2C%20a%20new%20natural%20language-guided%20geo-localization%0Abenchmark.%20This%20dataset%20is%20systematically%20constructed%20through%20an%20interactive%0Ahuman-computer%20process%20leveraging%20Large%20Language%20Model%20%28LLM%29%20driven%20annotation%0Atechniques%20in%20conjunction%20with%20pre-trained%20vision%20models.%20GeoText-1652%20extends%0Athe%20established%20University-1652%20image%20dataset%20with%20spatial-aware%20text%0Aannotations%2C%20thereby%20establishing%20one-to-one%20correspondences%20between%20image%2C%0Atext%2C%20and%20bounding%20box%20elements.%20We%20further%20introduce%20a%20new%20optimization%0Aobjective%20to%20leverage%20fine-grained%20spatial%20associations%2C%20called%20blending%0Aspatial%20matching%2C%20for%20region-level%20spatial%20relation%20matching.%20Extensive%0Aexperiments%20reveal%20that%20our%20approach%20maintains%20a%20competitive%20recall%20rate%0Acomparing%20other%20prevailing%20cross-modality%20methods.%20This%20underscores%20the%0Apromising%20potential%20of%20our%20approach%20in%20elevating%20drone%20control%20and%20navigation%0Athrough%20the%20seamless%20integration%20of%20natural%20language%20commands%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12751v2&entry.124074799=Read"},
{"title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety\n  Filters of Text-to-Image Models", "author": "Yimo Deng and Huangxun Chen", "abstract": "  Text-to-image (TTI) models offer many innovative services but also raise\nethical concerns due to their potential to generate unethical images. Most\npublic TTI services employ safety filters to prevent unintended images. In this\nwork, we introduce the Divide-and-Conquer Attack to circumvent the safety\nfilters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our\nattack leverages LLMs as text transformation agents to create adversarial\nprompts. We design attack helper prompts that effectively guide LLMs to break\ndown an unethical drawing intent into multiple benign descriptions of\nindividual image elements, allowing them to bypass safety filters while still\ngenerating unethical images. Because the latent harmful meaning only becomes\napparent when all individual elements are drawn together. Our evaluation\ndemonstrates that our attack successfully circumvents multiple strong\nclosed-box safety filters. The comprehensive success rate of DACA bypassing the\nsafety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while\nthe success rate for bypassing Midjourney V6 exceeds 75%. Our findings have\nmore severe security implications than methods of manual crafting or iterative\nTTI model querying due to lower attack barrier, enhanced interpretability , and\nbetter adaptation to defense. Our prototype is available at:\nhttps://github.com/researchcode001/Divide-and-Conquer-Attack\n", "link": "http://arxiv.org/abs/2312.07130v3", "date": "2024-03-14", "relevancy": 2.6596, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5173}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models&body=Title%3A%20Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models%0AAuthor%3A%20Yimo%20Deng%20and%20Huangxun%20Chen%0AAbstract%3A%20%20%20Text-to-image%20%28TTI%29%20models%20offer%20many%20innovative%20services%20but%20also%20raise%0Aethical%20concerns%20due%20to%20their%20potential%20to%20generate%20unethical%20images.%20Most%0Apublic%20TTI%20services%20employ%20safety%20filters%20to%20prevent%20unintended%20images.%20In%20this%0Awork%2C%20we%20introduce%20the%20Divide-and-Conquer%20Attack%20to%20circumvent%20the%20safety%0Afilters%20of%20state-of%20the-art%20TTI%20models%2C%20including%20DALL-E%203%20and%20Midjourney.%20Our%0Aattack%20leverages%20LLMs%20as%20text%20transformation%20agents%20to%20create%20adversarial%0Aprompts.%20We%20design%20attack%20helper%20prompts%20that%20effectively%20guide%20LLMs%20to%20break%0Adown%20an%20unethical%20drawing%20intent%20into%20multiple%20benign%20descriptions%20of%0Aindividual%20image%20elements%2C%20allowing%20them%20to%20bypass%20safety%20filters%20while%20still%0Agenerating%20unethical%20images.%20Because%20the%20latent%20harmful%20meaning%20only%20becomes%0Aapparent%20when%20all%20individual%20elements%20are%20drawn%20together.%20Our%20evaluation%0Ademonstrates%20that%20our%20attack%20successfully%20circumvents%20multiple%20strong%0Aclosed-box%20safety%20filters.%20The%20comprehensive%20success%20rate%20of%20DACA%20bypassing%20the%0Asafety%20filters%20of%20the%20state-of-the-art%20TTI%20engine%20DALL-E%203%20is%20above%2085%25%2C%20while%0Athe%20success%20rate%20for%20bypassing%20Midjourney%20V6%20exceeds%2075%25.%20Our%20findings%20have%0Amore%20severe%20security%20implications%20than%20methods%20of%20manual%20crafting%20or%20iterative%0ATTI%20model%20querying%20due%20to%20lower%20attack%20barrier%2C%20enhanced%20interpretability%20%2C%20and%0Abetter%20adaptation%20to%20defense.%20Our%20prototype%20is%20available%20at%3A%0Ahttps%3A//github.com/researchcode001/Divide-and-Conquer-Attack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07130v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models&entry.906535625=Yimo%20Deng%20and%20Huangxun%20Chen&entry.1292438233=%20%20Text-to-image%20%28TTI%29%20models%20offer%20many%20innovative%20services%20but%20also%20raise%0Aethical%20concerns%20due%20to%20their%20potential%20to%20generate%20unethical%20images.%20Most%0Apublic%20TTI%20services%20employ%20safety%20filters%20to%20prevent%20unintended%20images.%20In%20this%0Awork%2C%20we%20introduce%20the%20Divide-and-Conquer%20Attack%20to%20circumvent%20the%20safety%0Afilters%20of%20state-of%20the-art%20TTI%20models%2C%20including%20DALL-E%203%20and%20Midjourney.%20Our%0Aattack%20leverages%20LLMs%20as%20text%20transformation%20agents%20to%20create%20adversarial%0Aprompts.%20We%20design%20attack%20helper%20prompts%20that%20effectively%20guide%20LLMs%20to%20break%0Adown%20an%20unethical%20drawing%20intent%20into%20multiple%20benign%20descriptions%20of%0Aindividual%20image%20elements%2C%20allowing%20them%20to%20bypass%20safety%20filters%20while%20still%0Agenerating%20unethical%20images.%20Because%20the%20latent%20harmful%20meaning%20only%20becomes%0Aapparent%20when%20all%20individual%20elements%20are%20drawn%20together.%20Our%20evaluation%0Ademonstrates%20that%20our%20attack%20successfully%20circumvents%20multiple%20strong%0Aclosed-box%20safety%20filters.%20The%20comprehensive%20success%20rate%20of%20DACA%20bypassing%20the%0Asafety%20filters%20of%20the%20state-of-the-art%20TTI%20engine%20DALL-E%203%20is%20above%2085%25%2C%20while%0Athe%20success%20rate%20for%20bypassing%20Midjourney%20V6%20exceeds%2075%25.%20Our%20findings%20have%0Amore%20severe%20security%20implications%20than%20methods%20of%20manual%20crafting%20or%20iterative%0ATTI%20model%20querying%20due%20to%20lower%20attack%20barrier%2C%20enhanced%20interpretability%20%2C%20and%0Abetter%20adaptation%20to%20defense.%20Our%20prototype%20is%20available%20at%3A%0Ahttps%3A//github.com/researchcode001/Divide-and-Conquer-Attack%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07130v3&entry.124074799=Read"},
{"title": "SSR-Encoder: Encoding Selective Subject Representation for\n  Subject-Driven Generation", "author": "Yuxuan Zhang and Yiren Song and Jiaming Liu and Rui Wang and Jinpeng Yu and Hao Tang and Huaxia Li and Xu Tang and Yao Hu and Han Pan and Zhongliang Jing", "abstract": "  Recent advancements in subject-driven image generation have led to zero-shot\ngeneration, yet precise selection and focus on crucial subject representations\nremain challenging. Addressing this, we introduce the SSR-Encoder, a novel\narchitecture designed for selectively capturing any subject from single or\nmultiple reference images. It responds to various query modalities including\ntext and masks, without necessitating test-time fine-tuning. The SSR-Encoder\ncombines a Token-to-Patch Aligner that aligns query inputs with image patches\nand a Detail-Preserving Subject Encoder for extracting and preserving fine\nfeatures of the subjects, thereby generating subject embeddings. These\nembeddings, used in conjunction with original text embeddings, condition the\ngeneration process. Characterized by its model generalizability and efficiency,\nthe SSR-Encoder adapts to a range of custom models and control modules.\nEnhanced by the Embedding Consistency Regularization Loss for improved\ntraining, our extensive experiments demonstrate its effectiveness in versatile\nand high-quality image generation, indicating its broad applicability. Project\npage: https://ssr-encoder.github.io\n", "link": "http://arxiv.org/abs/2312.16272v2", "date": "2024-03-14", "relevancy": 2.6277, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5164}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation&body=Title%3A%20SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation%0AAuthor%3A%20Yuxuan%20Zhang%20and%20Yiren%20Song%20and%20Jiaming%20Liu%20and%20Rui%20Wang%20and%20Jinpeng%20Yu%20and%20Hao%20Tang%20and%20Huaxia%20Li%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Han%20Pan%20and%20Zhongliang%20Jing%0AAbstract%3A%20%20%20Recent%20advancements%20in%20subject-driven%20image%20generation%20have%20led%20to%20zero-shot%0Ageneration%2C%20yet%20precise%20selection%20and%20focus%20on%20crucial%20subject%20representations%0Aremain%20challenging.%20Addressing%20this%2C%20we%20introduce%20the%20SSR-Encoder%2C%20a%20novel%0Aarchitecture%20designed%20for%20selectively%20capturing%20any%20subject%20from%20single%20or%0Amultiple%20reference%20images.%20It%20responds%20to%20various%20query%20modalities%20including%0Atext%20and%20masks%2C%20without%20necessitating%20test-time%20fine-tuning.%20The%20SSR-Encoder%0Acombines%20a%20Token-to-Patch%20Aligner%20that%20aligns%20query%20inputs%20with%20image%20patches%0Aand%20a%20Detail-Preserving%20Subject%20Encoder%20for%20extracting%20and%20preserving%20fine%0Afeatures%20of%20the%20subjects%2C%20thereby%20generating%20subject%20embeddings.%20These%0Aembeddings%2C%20used%20in%20conjunction%20with%20original%20text%20embeddings%2C%20condition%20the%0Ageneration%20process.%20Characterized%20by%20its%20model%20generalizability%20and%20efficiency%2C%0Athe%20SSR-Encoder%20adapts%20to%20a%20range%20of%20custom%20models%20and%20control%20modules.%0AEnhanced%20by%20the%20Embedding%20Consistency%20Regularization%20Loss%20for%20improved%0Atraining%2C%20our%20extensive%20experiments%20demonstrate%20its%20effectiveness%20in%20versatile%0Aand%20high-quality%20image%20generation%2C%20indicating%20its%20broad%20applicability.%20Project%0Apage%3A%20https%3A//ssr-encoder.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16272v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation&entry.906535625=Yuxuan%20Zhang%20and%20Yiren%20Song%20and%20Jiaming%20Liu%20and%20Rui%20Wang%20and%20Jinpeng%20Yu%20and%20Hao%20Tang%20and%20Huaxia%20Li%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Han%20Pan%20and%20Zhongliang%20Jing&entry.1292438233=%20%20Recent%20advancements%20in%20subject-driven%20image%20generation%20have%20led%20to%20zero-shot%0Ageneration%2C%20yet%20precise%20selection%20and%20focus%20on%20crucial%20subject%20representations%0Aremain%20challenging.%20Addressing%20this%2C%20we%20introduce%20the%20SSR-Encoder%2C%20a%20novel%0Aarchitecture%20designed%20for%20selectively%20capturing%20any%20subject%20from%20single%20or%0Amultiple%20reference%20images.%20It%20responds%20to%20various%20query%20modalities%20including%0Atext%20and%20masks%2C%20without%20necessitating%20test-time%20fine-tuning.%20The%20SSR-Encoder%0Acombines%20a%20Token-to-Patch%20Aligner%20that%20aligns%20query%20inputs%20with%20image%20patches%0Aand%20a%20Detail-Preserving%20Subject%20Encoder%20for%20extracting%20and%20preserving%20fine%0Afeatures%20of%20the%20subjects%2C%20thereby%20generating%20subject%20embeddings.%20These%0Aembeddings%2C%20used%20in%20conjunction%20with%20original%20text%20embeddings%2C%20condition%20the%0Ageneration%20process.%20Characterized%20by%20its%20model%20generalizability%20and%20efficiency%2C%0Athe%20SSR-Encoder%20adapts%20to%20a%20range%20of%20custom%20models%20and%20control%20modules.%0AEnhanced%20by%20the%20Embedding%20Consistency%20Regularization%20Loss%20for%20improved%0Atraining%2C%20our%20extensive%20experiments%20demonstrate%20its%20effectiveness%20in%20versatile%0Aand%20high-quality%20image%20generation%2C%20indicating%20its%20broad%20applicability.%20Project%0Apage%3A%20https%3A//ssr-encoder.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16272v2&entry.124074799=Read"},
{"title": "Adversarial Training with OCR Modality Perturbation for Scene-Text\n  Visual Question Answering", "author": "Zhixuan Shen and Haonan Luo and Sijia Li and Tianrui Li", "abstract": "  Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.\n", "link": "http://arxiv.org/abs/2403.09288v1", "date": "2024-03-14", "relevancy": 2.6011, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5007}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5002}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering&body=Title%3A%20Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Zhixuan%20Shen%20and%20Haonan%20Luo%20and%20Sijia%20Li%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Scene-Text%20Visual%20Question%20Answering%20%28ST-VQA%29%20aims%20to%20understand%20scene%20text%0Ain%20images%20and%20answer%20questions%20related%20to%20the%20text%20content.%20Most%20existing%0Amethods%20heavily%20rely%20on%20the%20accuracy%20of%20Optical%20Character%20Recognition%20%28OCR%29%0Asystems%2C%20and%20aggressive%20fine-tuning%20based%20on%20limited%20spatial%20location%0Ainformation%20and%20erroneous%20OCR%20text%20information%20often%20leads%20to%20inevitable%0Aoverfitting.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20adversarial%20training%0Aarchitecture%20with%20spatial%20awareness%20capabilities.%20Specifically%2C%20we%20introduce%20an%0AAdversarial%20OCR%20Enhancement%20%28AOE%29%20module%2C%20which%20leverages%20adversarial%20training%0Ain%20the%20embedding%20space%20of%20OCR%20modality%20to%20enhance%20fault-tolerant%20representation%0Aof%20OCR%20texts%2C%20thereby%20reducing%20noise%20caused%20by%20OCR%20errors.%20Simultaneously%2C%20We%0Aadd%20a%20Spatial-Aware%20Self-Attention%20%28SASA%29%20mechanism%20to%20help%20the%20model%20better%0Acapture%20the%20spatial%20relationships%20among%20OCR%20tokens.%20Various%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20significant%20performance%20improvements%20on%0Aboth%20the%20ST-VQA%20and%20TextVQA%20datasets%20and%20provides%20a%20novel%20paradigm%20for%0Amultimodal%20adversarial%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09288v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering&entry.906535625=Zhixuan%20Shen%20and%20Haonan%20Luo%20and%20Sijia%20Li%20and%20Tianrui%20Li&entry.1292438233=%20%20Scene-Text%20Visual%20Question%20Answering%20%28ST-VQA%29%20aims%20to%20understand%20scene%20text%0Ain%20images%20and%20answer%20questions%20related%20to%20the%20text%20content.%20Most%20existing%0Amethods%20heavily%20rely%20on%20the%20accuracy%20of%20Optical%20Character%20Recognition%20%28OCR%29%0Asystems%2C%20and%20aggressive%20fine-tuning%20based%20on%20limited%20spatial%20location%0Ainformation%20and%20erroneous%20OCR%20text%20information%20often%20leads%20to%20inevitable%0Aoverfitting.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20adversarial%20training%0Aarchitecture%20with%20spatial%20awareness%20capabilities.%20Specifically%2C%20we%20introduce%20an%0AAdversarial%20OCR%20Enhancement%20%28AOE%29%20module%2C%20which%20leverages%20adversarial%20training%0Ain%20the%20embedding%20space%20of%20OCR%20modality%20to%20enhance%20fault-tolerant%20representation%0Aof%20OCR%20texts%2C%20thereby%20reducing%20noise%20caused%20by%20OCR%20errors.%20Simultaneously%2C%20We%0Aadd%20a%20Spatial-Aware%20Self-Attention%20%28SASA%29%20mechanism%20to%20help%20the%20model%20better%0Acapture%20the%20spatial%20relationships%20among%20OCR%20tokens.%20Various%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20significant%20performance%20improvements%20on%0Aboth%20the%20ST-VQA%20and%20TextVQA%20datasets%20and%20provides%20a%20novel%20paradigm%20for%0Amultimodal%20adversarial%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09288v1&entry.124074799=Read"},
{"title": "Faceptor: A Generalist Model for Face Perception", "author": "Lixiong Qin and Mei Wang and Xuannan Liu and Yuhang Zhang and Wei Deng and Xiaoshuai Song and Weiran Xu and Weihong Deng", "abstract": "  With the comprehensive research conducted on various face analysis tasks,\nthere is a growing interest among researchers to develop a unified approach to\nface perception. Existing methods mainly discuss unified representation and\ntraining, which lack task extensibility and application efficiency. To tackle\nthis issue, we focus on the unified model structure, exploring a face\ngeneralist model. As an intuitive design, Naive Faceptor enables tasks with the\nsame output shape and granularity to share the structural design of the\nstandardized output head, achieving improved task extensibility. Furthermore,\nFaceptor is proposed to adopt a well-designed single-encoder dual-decoder\narchitecture, allowing task-specific queries to represent new-coming semantics.\nThis design enhances the unification of model structure while improving\napplication efficiency in terms of storage overhead. Additionally, we introduce\nLayer-Attention into Faceptor, enabling the model to adaptively select features\nfrom optimal layers to perform the desired tasks. Through joint training on 13\nface perception datasets, Faceptor achieves exceptional performance in facial\nlandmark localization, face parsing, age estimation, expression recognition,\nbinary attribute classification, and face recognition, achieving or surpassing\nspecialized methods in most tasks. Our training framework can also be applied\nto auxiliary supervised learning, significantly improving performance in\ndata-sparse tasks such as age estimation and expression recognition. The code\nand models will be made publicly available at\nhttps://github.com/lxq1000/Faceptor.\n", "link": "http://arxiv.org/abs/2403.09500v1", "date": "2024-03-14", "relevancy": 2.5818, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception&body=Title%3A%20Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception%0AAuthor%3A%20Lixiong%20Qin%20and%20Mei%20Wang%20and%20Xuannan%20Liu%20and%20Yuhang%20Zhang%20and%20Wei%20Deng%20and%20Xiaoshuai%20Song%20and%20Weiran%20Xu%20and%20Weihong%20Deng%0AAbstract%3A%20%20%20With%20the%20comprehensive%20research%20conducted%20on%20various%20face%20analysis%20tasks%2C%0Athere%20is%20a%20growing%20interest%20among%20researchers%20to%20develop%20a%20unified%20approach%20to%0Aface%20perception.%20Existing%20methods%20mainly%20discuss%20unified%20representation%20and%0Atraining%2C%20which%20lack%20task%20extensibility%20and%20application%20efficiency.%20To%20tackle%0Athis%20issue%2C%20we%20focus%20on%20the%20unified%20model%20structure%2C%20exploring%20a%20face%0Ageneralist%20model.%20As%20an%20intuitive%20design%2C%20Naive%20Faceptor%20enables%20tasks%20with%20the%0Asame%20output%20shape%20and%20granularity%20to%20share%20the%20structural%20design%20of%20the%0Astandardized%20output%20head%2C%20achieving%20improved%20task%20extensibility.%20Furthermore%2C%0AFaceptor%20is%20proposed%20to%20adopt%20a%20well-designed%20single-encoder%20dual-decoder%0Aarchitecture%2C%20allowing%20task-specific%20queries%20to%20represent%20new-coming%20semantics.%0AThis%20design%20enhances%20the%20unification%20of%20model%20structure%20while%20improving%0Aapplication%20efficiency%20in%20terms%20of%20storage%20overhead.%20Additionally%2C%20we%20introduce%0ALayer-Attention%20into%20Faceptor%2C%20enabling%20the%20model%20to%20adaptively%20select%20features%0Afrom%20optimal%20layers%20to%20perform%20the%20desired%20tasks.%20Through%20joint%20training%20on%2013%0Aface%20perception%20datasets%2C%20Faceptor%20achieves%20exceptional%20performance%20in%20facial%0Alandmark%20localization%2C%20face%20parsing%2C%20age%20estimation%2C%20expression%20recognition%2C%0Abinary%20attribute%20classification%2C%20and%20face%20recognition%2C%20achieving%20or%20surpassing%0Aspecialized%20methods%20in%20most%20tasks.%20Our%20training%20framework%20can%20also%20be%20applied%0Ato%20auxiliary%20supervised%20learning%2C%20significantly%20improving%20performance%20in%0Adata-sparse%20tasks%20such%20as%20age%20estimation%20and%20expression%20recognition.%20The%20code%0Aand%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lxq1000/Faceptor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09500v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception&entry.906535625=Lixiong%20Qin%20and%20Mei%20Wang%20and%20Xuannan%20Liu%20and%20Yuhang%20Zhang%20and%20Wei%20Deng%20and%20Xiaoshuai%20Song%20and%20Weiran%20Xu%20and%20Weihong%20Deng&entry.1292438233=%20%20With%20the%20comprehensive%20research%20conducted%20on%20various%20face%20analysis%20tasks%2C%0Athere%20is%20a%20growing%20interest%20among%20researchers%20to%20develop%20a%20unified%20approach%20to%0Aface%20perception.%20Existing%20methods%20mainly%20discuss%20unified%20representation%20and%0Atraining%2C%20which%20lack%20task%20extensibility%20and%20application%20efficiency.%20To%20tackle%0Athis%20issue%2C%20we%20focus%20on%20the%20unified%20model%20structure%2C%20exploring%20a%20face%0Ageneralist%20model.%20As%20an%20intuitive%20design%2C%20Naive%20Faceptor%20enables%20tasks%20with%20the%0Asame%20output%20shape%20and%20granularity%20to%20share%20the%20structural%20design%20of%20the%0Astandardized%20output%20head%2C%20achieving%20improved%20task%20extensibility.%20Furthermore%2C%0AFaceptor%20is%20proposed%20to%20adopt%20a%20well-designed%20single-encoder%20dual-decoder%0Aarchitecture%2C%20allowing%20task-specific%20queries%20to%20represent%20new-coming%20semantics.%0AThis%20design%20enhances%20the%20unification%20of%20model%20structure%20while%20improving%0Aapplication%20efficiency%20in%20terms%20of%20storage%20overhead.%20Additionally%2C%20we%20introduce%0ALayer-Attention%20into%20Faceptor%2C%20enabling%20the%20model%20to%20adaptively%20select%20features%0Afrom%20optimal%20layers%20to%20perform%20the%20desired%20tasks.%20Through%20joint%20training%20on%2013%0Aface%20perception%20datasets%2C%20Faceptor%20achieves%20exceptional%20performance%20in%20facial%0Alandmark%20localization%2C%20face%20parsing%2C%20age%20estimation%2C%20expression%20recognition%2C%0Abinary%20attribute%20classification%2C%20and%20face%20recognition%2C%20achieving%20or%20surpassing%0Aspecialized%20methods%20in%20most%20tasks.%20Our%20training%20framework%20can%20also%20be%20applied%0Ato%20auxiliary%20supervised%20learning%2C%20significantly%20improving%20performance%20in%0Adata-sparse%20tasks%20such%20as%20age%20estimation%20and%20expression%20recognition.%20The%20code%0Aand%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lxq1000/Faceptor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09500v1&entry.124074799=Read"},
{"title": "DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs", "author": "Jiawen Zhu and Huayi Tang and Zhi-Qi Cheng and Jun-Yan He and Bin Luo and Shihao Qiu and Shengming Li and Huchuan Lu", "abstract": "  Existing nighttime unmanned aerial vehicle (UAV) trackers follow an\n\"Enhance-then-Track\" architecture - first using a light enhancer to brighten\nthe nighttime video, then employing a daytime tracker to locate the object.\nThis separate enhancement and tracking fails to build an end-to-end trainable\nvision system. To address this, we propose a novel architecture called Darkness\nClue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by\nefficiently learning to generate darkness clue prompts. Without a separate\nenhancer, DCPT directly encodes anti-dark capabilities into prompts using a\ndarkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing\nand undermining projections for darkness clues. It then injects these learned\nvisual prompts into a daytime tracker with fixed parameters across transformer\nlayers. Moreover, a gated feature aggregation mechanism enables adaptive fusion\nbetween prompts and between prompts and the base model. Extensive experiments\nshow state-of-the-art performance for DCPT on multiple dark scenario\nbenchmarks. The unified end-to-end learning of enhancement and tracking in DCPT\nenables a more trainable system. The darkness clue prompting efficiently\ninjects anti-dark knowledge without extra modules. Code is available at\nhttps://github.com/bearyi26/DCPT.\n", "link": "http://arxiv.org/abs/2309.10491v4", "date": "2024-03-14", "relevancy": 2.5812, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5282}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4927}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs&body=Title%3A%20DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs%0AAuthor%3A%20Jiawen%20Zhu%20and%20Huayi%20Tang%20and%20Zhi-Qi%20Cheng%20and%20Jun-Yan%20He%20and%20Bin%20Luo%20and%20Shihao%20Qiu%20and%20Shengming%20Li%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Existing%20nighttime%20unmanned%20aerial%20vehicle%20%28UAV%29%20trackers%20follow%20an%0A%22Enhance-then-Track%22%20architecture%20-%20first%20using%20a%20light%20enhancer%20to%20brighten%0Athe%20nighttime%20video%2C%20then%20employing%20a%20daytime%20tracker%20to%20locate%20the%20object.%0AThis%20separate%20enhancement%20and%20tracking%20fails%20to%20build%20an%20end-to-end%20trainable%0Avision%20system.%20To%20address%20this%2C%20we%20propose%20a%20novel%20architecture%20called%20Darkness%0AClue-Prompted%20Tracking%20%28DCPT%29%20that%20achieves%20robust%20UAV%20tracking%20at%20night%20by%0Aefficiently%20learning%20to%20generate%20darkness%20clue%20prompts.%20Without%20a%20separate%0Aenhancer%2C%20DCPT%20directly%20encodes%20anti-dark%20capabilities%20into%20prompts%20using%20a%0Adarkness%20clue%20prompter%20%28DCP%29.%20Specifically%2C%20DCP%20iteratively%20learns%20emphasizing%0Aand%20undermining%20projections%20for%20darkness%20clues.%20It%20then%20injects%20these%20learned%0Avisual%20prompts%20into%20a%20daytime%20tracker%20with%20fixed%20parameters%20across%20transformer%0Alayers.%20Moreover%2C%20a%20gated%20feature%20aggregation%20mechanism%20enables%20adaptive%20fusion%0Abetween%20prompts%20and%20between%20prompts%20and%20the%20base%20model.%20Extensive%20experiments%0Ashow%20state-of-the-art%20performance%20for%20DCPT%20on%20multiple%20dark%20scenario%0Abenchmarks.%20The%20unified%20end-to-end%20learning%20of%20enhancement%20and%20tracking%20in%20DCPT%0Aenables%20a%20more%20trainable%20system.%20The%20darkness%20clue%20prompting%20efficiently%0Ainjects%20anti-dark%20knowledge%20without%20extra%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bearyi26/DCPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10491v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs&entry.906535625=Jiawen%20Zhu%20and%20Huayi%20Tang%20and%20Zhi-Qi%20Cheng%20and%20Jun-Yan%20He%20and%20Bin%20Luo%20and%20Shihao%20Qiu%20and%20Shengming%20Li%20and%20Huchuan%20Lu&entry.1292438233=%20%20Existing%20nighttime%20unmanned%20aerial%20vehicle%20%28UAV%29%20trackers%20follow%20an%0A%22Enhance-then-Track%22%20architecture%20-%20first%20using%20a%20light%20enhancer%20to%20brighten%0Athe%20nighttime%20video%2C%20then%20employing%20a%20daytime%20tracker%20to%20locate%20the%20object.%0AThis%20separate%20enhancement%20and%20tracking%20fails%20to%20build%20an%20end-to-end%20trainable%0Avision%20system.%20To%20address%20this%2C%20we%20propose%20a%20novel%20architecture%20called%20Darkness%0AClue-Prompted%20Tracking%20%28DCPT%29%20that%20achieves%20robust%20UAV%20tracking%20at%20night%20by%0Aefficiently%20learning%20to%20generate%20darkness%20clue%20prompts.%20Without%20a%20separate%0Aenhancer%2C%20DCPT%20directly%20encodes%20anti-dark%20capabilities%20into%20prompts%20using%20a%0Adarkness%20clue%20prompter%20%28DCP%29.%20Specifically%2C%20DCP%20iteratively%20learns%20emphasizing%0Aand%20undermining%20projections%20for%20darkness%20clues.%20It%20then%20injects%20these%20learned%0Avisual%20prompts%20into%20a%20daytime%20tracker%20with%20fixed%20parameters%20across%20transformer%0Alayers.%20Moreover%2C%20a%20gated%20feature%20aggregation%20mechanism%20enables%20adaptive%20fusion%0Abetween%20prompts%20and%20between%20prompts%20and%20the%20base%20model.%20Extensive%20experiments%0Ashow%20state-of-the-art%20performance%20for%20DCPT%20on%20multiple%20dark%20scenario%0Abenchmarks.%20The%20unified%20end-to-end%20learning%20of%20enhancement%20and%20tracking%20in%20DCPT%0Aenables%20a%20more%20trainable%20system.%20The%20darkness%20clue%20prompting%20efficiently%0Ainjects%20anti-dark%20knowledge%20without%20extra%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bearyi26/DCPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10491v4&entry.124074799=Read"},
{"title": "Rethinking Class-incremental Learning in the Era of Large Pre-trained\n  Models via Test-Time Adaptation", "author": "Imad Eddine Marouf and Subhankar Roy and Enzo Tartaglione and St\u00e9phane Lathuili\u00e8re", "abstract": "  Class-incremental learning (CIL) is a challenging task that involves\nsequentially learning to categorize classes from new tasks without forgetting\npreviously learned information. The advent of large pre-trained models (PTMs)\nhas fast-tracked the progress in CIL due to the highly transferable PTM\nrepresentations, where tuning a small set of parameters leads to\nstate-of-the-art performance when compared with the traditional CIL methods\nthat are trained from scratch. However, repeated fine-tuning on each task\ndestroys the rich representations of the PTMs and further leads to forgetting\nprevious tasks. To strike a balance between the stability and plasticity of\nPTMs for CIL, we propose a novel perspective of eliminating training on every\nnew task and instead train PTM only on the first task, and then refine its\nrepresentation at inference time using test-time adaptation (TTA). Concretely,\nwe propose Test-Time Adaptation for Class-Incremental Learning (TTACIL) that\nfirst fine-tunes PTMs using Adapters on the first task, then adjusts Layer Norm\nparameters of the PTM on each test instance for learning task-specific\nfeatures, and finally resets them back to the adapted model to preserve\nstability. As a consequence, our TTACIL does not undergo any forgetting, while\nbenefiting each task with the rich PTM features. Additionally, by design, our\nTTACIL is robust to common data corruptions. Our method outperforms several\nstate-of-the-art CIL methods when evaluated on multiple CIL benchmarks under\nboth clean and corrupted data. Code is available at:\nhttps://github.com/IemProg/TTACIL.\n", "link": "http://arxiv.org/abs/2310.11482v2", "date": "2024-03-14", "relevancy": 2.5263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5697}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4714}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation&body=Title%3A%20Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation%0AAuthor%3A%20Imad%20Eddine%20Marouf%20and%20Subhankar%20Roy%20and%20Enzo%20Tartaglione%20and%20St%C3%A9phane%20Lathuili%C3%A8re%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20is%20a%20challenging%20task%20that%20involves%0Asequentially%20learning%20to%20categorize%20classes%20from%20new%20tasks%20without%20forgetting%0Apreviously%20learned%20information.%20The%20advent%20of%20large%20pre-trained%20models%20%28PTMs%29%0Ahas%20fast-tracked%20the%20progress%20in%20CIL%20due%20to%20the%20highly%20transferable%20PTM%0Arepresentations%2C%20where%20tuning%20a%20small%20set%20of%20parameters%20leads%20to%0Astate-of-the-art%20performance%20when%20compared%20with%20the%20traditional%20CIL%20methods%0Athat%20are%20trained%20from%20scratch.%20However%2C%20repeated%20fine-tuning%20on%20each%20task%0Adestroys%20the%20rich%20representations%20of%20the%20PTMs%20and%20further%20leads%20to%20forgetting%0Aprevious%20tasks.%20To%20strike%20a%20balance%20between%20the%20stability%20and%20plasticity%20of%0APTMs%20for%20CIL%2C%20we%20propose%20a%20novel%20perspective%20of%20eliminating%20training%20on%20every%0Anew%20task%20and%20instead%20train%20PTM%20only%20on%20the%20first%20task%2C%20and%20then%20refine%20its%0Arepresentation%20at%20inference%20time%20using%20test-time%20adaptation%20%28TTA%29.%20Concretely%2C%0Awe%20propose%20Test-Time%20Adaptation%20for%20Class-Incremental%20Learning%20%28TTACIL%29%20that%0Afirst%20fine-tunes%20PTMs%20using%20Adapters%20on%20the%20first%20task%2C%20then%20adjusts%20Layer%20Norm%0Aparameters%20of%20the%20PTM%20on%20each%20test%20instance%20for%20learning%20task-specific%0Afeatures%2C%20and%20finally%20resets%20them%20back%20to%20the%20adapted%20model%20to%20preserve%0Astability.%20As%20a%20consequence%2C%20our%20TTACIL%20does%20not%20undergo%20any%20forgetting%2C%20while%0Abenefiting%20each%20task%20with%20the%20rich%20PTM%20features.%20Additionally%2C%20by%20design%2C%20our%0ATTACIL%20is%20robust%20to%20common%20data%20corruptions.%20Our%20method%20outperforms%20several%0Astate-of-the-art%20CIL%20methods%20when%20evaluated%20on%20multiple%20CIL%20benchmarks%20under%0Aboth%20clean%20and%20corrupted%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IemProg/TTACIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11482v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation&entry.906535625=Imad%20Eddine%20Marouf%20and%20Subhankar%20Roy%20and%20Enzo%20Tartaglione%20and%20St%C3%A9phane%20Lathuili%C3%A8re&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20is%20a%20challenging%20task%20that%20involves%0Asequentially%20learning%20to%20categorize%20classes%20from%20new%20tasks%20without%20forgetting%0Apreviously%20learned%20information.%20The%20advent%20of%20large%20pre-trained%20models%20%28PTMs%29%0Ahas%20fast-tracked%20the%20progress%20in%20CIL%20due%20to%20the%20highly%20transferable%20PTM%0Arepresentations%2C%20where%20tuning%20a%20small%20set%20of%20parameters%20leads%20to%0Astate-of-the-art%20performance%20when%20compared%20with%20the%20traditional%20CIL%20methods%0Athat%20are%20trained%20from%20scratch.%20However%2C%20repeated%20fine-tuning%20on%20each%20task%0Adestroys%20the%20rich%20representations%20of%20the%20PTMs%20and%20further%20leads%20to%20forgetting%0Aprevious%20tasks.%20To%20strike%20a%20balance%20between%20the%20stability%20and%20plasticity%20of%0APTMs%20for%20CIL%2C%20we%20propose%20a%20novel%20perspective%20of%20eliminating%20training%20on%20every%0Anew%20task%20and%20instead%20train%20PTM%20only%20on%20the%20first%20task%2C%20and%20then%20refine%20its%0Arepresentation%20at%20inference%20time%20using%20test-time%20adaptation%20%28TTA%29.%20Concretely%2C%0Awe%20propose%20Test-Time%20Adaptation%20for%20Class-Incremental%20Learning%20%28TTACIL%29%20that%0Afirst%20fine-tunes%20PTMs%20using%20Adapters%20on%20the%20first%20task%2C%20then%20adjusts%20Layer%20Norm%0Aparameters%20of%20the%20PTM%20on%20each%20test%20instance%20for%20learning%20task-specific%0Afeatures%2C%20and%20finally%20resets%20them%20back%20to%20the%20adapted%20model%20to%20preserve%0Astability.%20As%20a%20consequence%2C%20our%20TTACIL%20does%20not%20undergo%20any%20forgetting%2C%20while%0Abenefiting%20each%20task%20with%20the%20rich%20PTM%20features.%20Additionally%2C%20by%20design%2C%20our%0ATTACIL%20is%20robust%20to%20common%20data%20corruptions.%20Our%20method%20outperforms%20several%0Astate-of-the-art%20CIL%20methods%20when%20evaluated%20on%20multiple%20CIL%20benchmarks%20under%0Aboth%20clean%20and%20corrupted%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IemProg/TTACIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11482v2&entry.124074799=Read"},
{"title": "Adversarial Fine-tuning of Compressed Neural Networks for Joint\n  Improvement of Robustness and Efficiency", "author": "Hallgrimur Thorsteinsson and Valdemar J Henriksen and Tong Chen and Raghavendra Selvan", "abstract": "  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n", "link": "http://arxiv.org/abs/2403.09441v1", "date": "2024-03-14", "relevancy": 2.5002, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4996}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4963}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency&body=Title%3A%20Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency%0AAuthor%3A%20Hallgrimur%20Thorsteinsson%20and%20Valdemar%20J%20Henriksen%20and%20Tong%20Chen%20and%20Raghavendra%20Selvan%0AAbstract%3A%20%20%20As%20deep%20learning%20%28DL%29%20models%20are%20increasingly%20being%20integrated%20into%20our%0Aeveryday%20lives%2C%20ensuring%20their%20safety%20by%20making%20them%20robust%20against%20adversarial%0Aattacks%20has%20become%20increasingly%20critical.%20DL%20models%20have%20been%20found%20to%20be%0Asusceptible%20to%20adversarial%20attacks%20which%20can%20be%20achieved%20by%20introducing%20small%2C%0Atargeted%20perturbations%20to%20disrupt%20the%20input%20data.%20Adversarial%20training%20has%20been%0Apresented%20as%20a%20mitigation%20strategy%20which%20can%20result%20in%20more%20robust%20models.%20This%0Aadversarial%20robustness%20comes%20with%20additional%20computational%20costs%20required%20to%0Adesign%20adversarial%20attacks%20during%20training.%20The%20two%20objectives%20--%20adversarial%0Arobustness%20and%20computational%20efficiency%20--%20then%20appear%20to%20be%20in%20conflict%20of%0Aeach%20other.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20two%20different%20model%0Acompression%20methods%20--%20structured%20weight%20pruning%20and%20quantization%20--%20on%0Aadversarial%20robustness.%20We%20specifically%20explore%20the%20effects%20of%20fine-tuning%20on%0Acompressed%20models%2C%20and%20present%20the%20trade-off%20between%20standard%20fine-tuning%20and%0Aadversarial%20fine-tuning.%20Our%20results%20show%20that%20compression%20does%20not%20inherently%0Alead%20to%20loss%20in%20model%20robustness%20and%20adversarial%20fine-tuning%20of%20a%20compressed%0Amodel%20can%20yield%20large%20improvement%20to%20the%20robustness%20performance%20of%20models.%20We%0Apresent%20experiments%20on%20two%20benchmark%20datasets%20showing%20that%20adversarial%0Afine-tuning%20of%20compressed%20models%20can%20achieve%20robustness%20performance%20comparable%0Ato%20adversarially%20trained%20models%2C%20while%20also%20improving%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09441v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency&entry.906535625=Hallgrimur%20Thorsteinsson%20and%20Valdemar%20J%20Henriksen%20and%20Tong%20Chen%20and%20Raghavendra%20Selvan&entry.1292438233=%20%20As%20deep%20learning%20%28DL%29%20models%20are%20increasingly%20being%20integrated%20into%20our%0Aeveryday%20lives%2C%20ensuring%20their%20safety%20by%20making%20them%20robust%20against%20adversarial%0Aattacks%20has%20become%20increasingly%20critical.%20DL%20models%20have%20been%20found%20to%20be%0Asusceptible%20to%20adversarial%20attacks%20which%20can%20be%20achieved%20by%20introducing%20small%2C%0Atargeted%20perturbations%20to%20disrupt%20the%20input%20data.%20Adversarial%20training%20has%20been%0Apresented%20as%20a%20mitigation%20strategy%20which%20can%20result%20in%20more%20robust%20models.%20This%0Aadversarial%20robustness%20comes%20with%20additional%20computational%20costs%20required%20to%0Adesign%20adversarial%20attacks%20during%20training.%20The%20two%20objectives%20--%20adversarial%0Arobustness%20and%20computational%20efficiency%20--%20then%20appear%20to%20be%20in%20conflict%20of%0Aeach%20other.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20two%20different%20model%0Acompression%20methods%20--%20structured%20weight%20pruning%20and%20quantization%20--%20on%0Aadversarial%20robustness.%20We%20specifically%20explore%20the%20effects%20of%20fine-tuning%20on%0Acompressed%20models%2C%20and%20present%20the%20trade-off%20between%20standard%20fine-tuning%20and%0Aadversarial%20fine-tuning.%20Our%20results%20show%20that%20compression%20does%20not%20inherently%0Alead%20to%20loss%20in%20model%20robustness%20and%20adversarial%20fine-tuning%20of%20a%20compressed%0Amodel%20can%20yield%20large%20improvement%20to%20the%20robustness%20performance%20of%20models.%20We%0Apresent%20experiments%20on%20two%20benchmark%20datasets%20showing%20that%20adversarial%0Afine-tuning%20of%20compressed%20models%20can%20achieve%20robustness%20performance%20comparable%0Ato%20adversarially%20trained%20models%2C%20while%20also%20improving%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09441v1&entry.124074799=Read"},
{"title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes", "author": "Avyav Kumar Singh and Ekaterina Shutova and Helen Yannakoudakis", "abstract": "  Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2210.17437v3", "date": "2024-03-14", "relevancy": 2.4971, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes&body=Title%3A%20Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes%0AAuthor%3A%20Avyav%20Kumar%20Singh%20and%20Ekaterina%20Shutova%20and%20Helen%20Yannakoudakis%0AAbstract%3A%20%20%20Existing%20approaches%20to%20few-shot%20learning%20in%20NLP%20rely%20on%20large%20language%20models%0Aand%20fine-tuning%20of%20these%20to%20generalise%20on%20out-of-distribution%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20%22extreme%22%20few-shot%20learning%2C%0Awherein%20models%20are%20exposed%20to%20as%20little%20as%204%20examples%20per%20class%2C%20based%20on%0Asoft-label%20prototypes%20that%20collectively%20capture%20the%20distribution%20of%20different%0Aclasses%20across%20the%20input%20domain%20space.%20Inspired%20by%20previous%20work%20%28Sucholutsky%0Aet%20al.%2C%202021%29%20on%20univariate%20or%20simple%20multivariate%20%28synthetic%29%20data%2C%20we%20propose%0Aa%20novel%20approach%20that%20is%20effective%20on%20large%2C%20high-dimensional%20and%20real-world%0Adatasets.%20We%20learn%20soft-label%20prototypes%20within%20a%20neural%20framework%20%28DeepSLP%29%0Aand%20we%20experimentally%20demonstrate%20that%20it%20achieves%20superior%20performance%20on%0A31/48%20tested%20tasks%20and%20few-shot%20settings%20while%20closely%20matching%20the%20performance%0Aof%20strong%20baselines%20on%20the%20rest.%20We%20focus%20on%20learning%20previously%20unseen%20NLP%0Atasks%20from%20very%20few%20examples%20%284%2C%208%2C%2016%29%20per%20label%20and%20present%20an%20in-depth%0Aanalysis%20of%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.17437v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes&entry.906535625=Avyav%20Kumar%20Singh%20and%20Ekaterina%20Shutova%20and%20Helen%20Yannakoudakis&entry.1292438233=%20%20Existing%20approaches%20to%20few-shot%20learning%20in%20NLP%20rely%20on%20large%20language%20models%0Aand%20fine-tuning%20of%20these%20to%20generalise%20on%20out-of-distribution%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20%22extreme%22%20few-shot%20learning%2C%0Awherein%20models%20are%20exposed%20to%20as%20little%20as%204%20examples%20per%20class%2C%20based%20on%0Asoft-label%20prototypes%20that%20collectively%20capture%20the%20distribution%20of%20different%0Aclasses%20across%20the%20input%20domain%20space.%20Inspired%20by%20previous%20work%20%28Sucholutsky%0Aet%20al.%2C%202021%29%20on%20univariate%20or%20simple%20multivariate%20%28synthetic%29%20data%2C%20we%20propose%0Aa%20novel%20approach%20that%20is%20effective%20on%20large%2C%20high-dimensional%20and%20real-world%0Adatasets.%20We%20learn%20soft-label%20prototypes%20within%20a%20neural%20framework%20%28DeepSLP%29%0Aand%20we%20experimentally%20demonstrate%20that%20it%20achieves%20superior%20performance%20on%0A31/48%20tested%20tasks%20and%20few-shot%20settings%20while%20closely%20matching%20the%20performance%0Aof%20strong%20baselines%20on%20the%20rest.%20We%20focus%20on%20learning%20previously%20unseen%20NLP%0Atasks%20from%20very%20few%20examples%20%284%2C%208%2C%2016%29%20per%20label%20and%20present%20an%20in-depth%0Aanalysis%20of%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.17437v3&entry.124074799=Read"},
{"title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting", "author": "Jaewoo Jung and Jisang Han and Honggyu An and Jiwon Kang and Seonghoon Park and Seungryong Kim", "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When trained with randomly initialized\npoint clouds, 3DGS fails to maintain its ability to produce high-quality\nimages, undergoing large performance drops of 4-5 dB in PSNR. Through extensive\nanalysis of SfM initialization in the frequency domain and analysis of a 1D\nregression task with multiple 1D Gaussians, we propose a novel optimization\nstrategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D\nGaussian Splatting), that successfully trains 3D Gaussians from random point\nclouds. We show the effectiveness of our strategy through quantitative and\nqualitative comparisons on multiple datasets, largely improving the performance\nin all settings. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n", "link": "http://arxiv.org/abs/2403.09413v1", "date": "2024-03-14", "relevancy": 2.4952, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&body=Title%3A%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20trained%20with%20randomly%20initialized%0Apoint%20clouds%2C%203DGS%20fails%20to%20maintain%20its%20ability%20to%20produce%20high-quality%0Aimages%2C%20undergoing%20large%20performance%20drops%20of%204-5%20dB%20in%20PSNR.%20Through%20extensive%0Aanalysis%20of%20SfM%20initialization%20in%20the%20frequency%20domain%20and%20analysis%20of%20a%201D%0Aregression%20task%20with%20multiple%201D%20Gaussians%2C%20we%20propose%20a%20novel%20optimization%0Astrategy%20dubbed%20RAIN-GS%20%28Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%0AGaussian%20Splatting%29%2C%20that%20successfully%20trains%203D%20Gaussians%20from%20random%20point%0Aclouds.%20We%20show%20the%20effectiveness%20of%20our%20strategy%20through%20quantitative%20and%0Aqualitative%20comparisons%20on%20multiple%20datasets%2C%20largely%20improving%20the%20performance%0Ain%20all%20settings.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09413v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&entry.906535625=Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20trained%20with%20randomly%20initialized%0Apoint%20clouds%2C%203DGS%20fails%20to%20maintain%20its%20ability%20to%20produce%20high-quality%0Aimages%2C%20undergoing%20large%20performance%20drops%20of%204-5%20dB%20in%20PSNR.%20Through%20extensive%0Aanalysis%20of%20SfM%20initialization%20in%20the%20frequency%20domain%20and%20analysis%20of%20a%201D%0Aregression%20task%20with%20multiple%201D%20Gaussians%2C%20we%20propose%20a%20novel%20optimization%0Astrategy%20dubbed%20RAIN-GS%20%28Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%0AGaussian%20Splatting%29%2C%20that%20successfully%20trains%203D%20Gaussians%20from%20random%20point%0Aclouds.%20We%20show%20the%20effectiveness%20of%20our%20strategy%20through%20quantitative%20and%0Aqualitative%20comparisons%20on%20multiple%20datasets%2C%20largely%20improving%20the%20performance%0Ain%20all%20settings.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09413v1&entry.124074799=Read"},
{"title": "Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training", "author": "Yanlai Yang and Matt Jones and Michael C. Mozer and Mengye Ren", "abstract": "  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n", "link": "http://arxiv.org/abs/2403.09613v1", "date": "2024-03-14", "relevancy": 2.4908, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training&body=Title%3A%20Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training%0AAuthor%3A%20Yanlai%20Yang%20and%20Matt%20Jones%20and%20Michael%20C.%20Mozer%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20We%20explore%20the%20training%20dynamics%20of%20neural%20networks%20in%20a%20structured%20non-IID%0Asetting%20where%20documents%20are%20presented%20cyclically%20in%20a%20fixed%2C%20repeated%20sequence.%0ATypically%2C%20networks%20suffer%20from%20catastrophic%20interference%20when%20training%20on%20a%0Asequence%20of%20documents%3B%20however%2C%20we%20discover%20a%20curious%20and%20remarkable%20property%0Aof%20LLMs%20fine-tuned%20sequentially%20in%20this%20setting%3A%20they%20exhibit%20anticipatory%0Abehavior%2C%20recovering%20from%20the%20forgetting%20on%20documents%20before%20encountering%20them%0Aagain.%20The%20behavior%20emerges%20and%20becomes%20more%20robust%20as%20the%20architecture%20scales%0Aup%20its%20number%20of%20parameters.%20Through%20comprehensive%20experiments%20and%0Avisualizations%2C%20we%20uncover%20new%20insights%20into%20training%20over-parameterized%0Anetworks%20in%20structured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training&entry.906535625=Yanlai%20Yang%20and%20Matt%20Jones%20and%20Michael%20C.%20Mozer%20and%20Mengye%20Ren&entry.1292438233=%20%20We%20explore%20the%20training%20dynamics%20of%20neural%20networks%20in%20a%20structured%20non-IID%0Asetting%20where%20documents%20are%20presented%20cyclically%20in%20a%20fixed%2C%20repeated%20sequence.%0ATypically%2C%20networks%20suffer%20from%20catastrophic%20interference%20when%20training%20on%20a%0Asequence%20of%20documents%3B%20however%2C%20we%20discover%20a%20curious%20and%20remarkable%20property%0Aof%20LLMs%20fine-tuned%20sequentially%20in%20this%20setting%3A%20they%20exhibit%20anticipatory%0Abehavior%2C%20recovering%20from%20the%20forgetting%20on%20documents%20before%20encountering%20them%0Aagain.%20The%20behavior%20emerges%20and%20becomes%20more%20robust%20as%20the%20architecture%20scales%0Aup%20its%20number%20of%20parameters.%20Through%20comprehensive%20experiments%20and%0Avisualizations%2C%20we%20uncover%20new%20insights%20into%20training%20over-parameterized%0Anetworks%20in%20structured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09613v1&entry.124074799=Read"},
{"title": "CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise\n  Classification", "author": "Yiming Ma and Victor Sanchez and Tanaya Guha", "abstract": "  The CLIP (Contrastive Language-Image Pretraining) model has exhibited\noutstanding performance in recognition problems, such as zero-shot image\nclassification and object detection. However, its ability to count remains\nunderstudied due to the inherent challenges of transforming counting--a\nregression task--into a recognition task. In this paper, we investigate CLIP's\npotential in counting, focusing specifically on estimating crowd sizes.\nExisting classification-based crowd-counting methods have encountered issues,\nincluding inappropriate discretization strategies, which impede the application\nof CLIP and result in suboptimal performance. To address these challenges, we\npropose the Enhanced Blockwise Classification (EBC) framework. In contrast to\nprevious methods, EBC relies on integer-valued bins that facilitate the\nlearning of robust decision boundaries. Within our model-agnostic EBC\nframework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting\nmodel capable of generating density maps. Comprehensive evaluations across\ndiverse crowd-counting datasets demonstrate the state-of-the-art performance of\nour methods. Particularly, EBC can improve existing models by up to 76.9%.\nMoreover, our CLIP-EBC model surpasses current crowd-counting methods,\nachieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part\nB datasets, respectively. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.09281v1", "date": "2024-03-14", "relevancy": 2.4908, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5146}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&body=Title%3A%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification%0AAuthor%3A%20Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha%0AAbstract%3A%20%20%20The%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20has%20exhibited%0Aoutstanding%20performance%20in%20recognition%20problems%2C%20such%20as%20zero-shot%20image%0Aclassification%20and%20object%20detection.%20However%2C%20its%20ability%20to%20count%20remains%0Aunderstudied%20due%20to%20the%20inherent%20challenges%20of%20transforming%20counting--a%0Aregression%20task--into%20a%20recognition%20task.%20In%20this%20paper%2C%20we%20investigate%20CLIP%27s%0Apotential%20in%20counting%2C%20focusing%20specifically%20on%20estimating%20crowd%20sizes.%0AExisting%20classification-based%20crowd-counting%20methods%20have%20encountered%20issues%2C%0Aincluding%20inappropriate%20discretization%20strategies%2C%20which%20impede%20the%20application%0Aof%20CLIP%20and%20result%20in%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Enhanced%20Blockwise%20Classification%20%28EBC%29%20framework.%20In%20contrast%20to%0Aprevious%20methods%2C%20EBC%20relies%20on%20integer-valued%20bins%20that%20facilitate%20the%0Alearning%20of%20robust%20decision%20boundaries.%20Within%20our%20model-agnostic%20EBC%0Aframework%2C%20we%20introduce%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20crowd-counting%0Amodel%20capable%20of%20generating%20density%20maps.%20Comprehensive%20evaluations%20across%0Adiverse%20crowd-counting%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20methods.%20Particularly%2C%20EBC%20can%20improve%20existing%20models%20by%20up%20to%2076.9%25.%0AMoreover%2C%20our%20CLIP-EBC%20model%20surpasses%20current%20crowd-counting%20methods%2C%0Aachieving%20mean%20absolute%20errors%20of%2055.0%20and%206.3%20on%20ShanghaiTech%20part%20A%20and%20part%0AB%20datasets%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09281v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&entry.906535625=Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha&entry.1292438233=%20%20The%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20has%20exhibited%0Aoutstanding%20performance%20in%20recognition%20problems%2C%20such%20as%20zero-shot%20image%0Aclassification%20and%20object%20detection.%20However%2C%20its%20ability%20to%20count%20remains%0Aunderstudied%20due%20to%20the%20inherent%20challenges%20of%20transforming%20counting--a%0Aregression%20task--into%20a%20recognition%20task.%20In%20this%20paper%2C%20we%20investigate%20CLIP%27s%0Apotential%20in%20counting%2C%20focusing%20specifically%20on%20estimating%20crowd%20sizes.%0AExisting%20classification-based%20crowd-counting%20methods%20have%20encountered%20issues%2C%0Aincluding%20inappropriate%20discretization%20strategies%2C%20which%20impede%20the%20application%0Aof%20CLIP%20and%20result%20in%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Enhanced%20Blockwise%20Classification%20%28EBC%29%20framework.%20In%20contrast%20to%0Aprevious%20methods%2C%20EBC%20relies%20on%20integer-valued%20bins%20that%20facilitate%20the%0Alearning%20of%20robust%20decision%20boundaries.%20Within%20our%20model-agnostic%20EBC%0Aframework%2C%20we%20introduce%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20crowd-counting%0Amodel%20capable%20of%20generating%20density%20maps.%20Comprehensive%20evaluations%20across%0Adiverse%20crowd-counting%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20methods.%20Particularly%2C%20EBC%20can%20improve%20existing%20models%20by%20up%20to%2076.9%25.%0AMoreover%2C%20our%20CLIP-EBC%20model%20surpasses%20current%20crowd-counting%20methods%2C%0Aachieving%20mean%20absolute%20errors%20of%2055.0%20and%206.3%20on%20ShanghaiTech%20part%20A%20and%20part%0AB%20datasets%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09281v1&entry.124074799=Read"},
{"title": "Explorations in Texture Learning", "author": "Blaine Hoak and Patrick McDaniel", "abstract": "  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n", "link": "http://arxiv.org/abs/2403.09543v1", "date": "2024-03-14", "relevancy": 2.4816, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4957}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4636}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explorations%20in%20Texture%20Learning&body=Title%3A%20Explorations%20in%20Texture%20Learning%0AAuthor%3A%20Blaine%20Hoak%20and%20Patrick%20McDaniel%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20%5Ctextit%7Btexture%20learning%7D%3A%20the%20identification%20of%0Atextures%20learned%20by%20object%20classification%20models%2C%20and%20the%20extent%20to%20which%20they%0Arely%20on%20these%20textures.%20We%20build%20texture-object%20associations%20that%20uncover%20new%0Ainsights%20about%20the%20relationships%20between%20texture%20and%20object%20classes%20in%20CNNs%20and%0Afind%20three%20classes%20of%20results%3A%20associations%20that%20are%20strong%20and%20expected%2C%0Astrong%20and%20not%20expected%2C%20and%20expected%20but%20not%20present.%20Our%20analysis%0Ademonstrates%20that%20investigations%20in%20texture%20learning%20enable%20new%20methods%20for%0Ainterpretability%20and%20have%20the%20potential%20to%20uncover%20unexpected%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09543v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorations%20in%20Texture%20Learning&entry.906535625=Blaine%20Hoak%20and%20Patrick%20McDaniel&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20%5Ctextit%7Btexture%20learning%7D%3A%20the%20identification%20of%0Atextures%20learned%20by%20object%20classification%20models%2C%20and%20the%20extent%20to%20which%20they%0Arely%20on%20these%20textures.%20We%20build%20texture-object%20associations%20that%20uncover%20new%0Ainsights%20about%20the%20relationships%20between%20texture%20and%20object%20classes%20in%20CNNs%20and%0Afind%20three%20classes%20of%20results%3A%20associations%20that%20are%20strong%20and%20expected%2C%0Astrong%20and%20not%20expected%2C%20and%20expected%20but%20not%20present.%20Our%20analysis%0Ademonstrates%20that%20investigations%20in%20texture%20learning%20enable%20new%20methods%20for%0Ainterpretability%20and%20have%20the%20potential%20to%20uncover%20unexpected%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09543v1&entry.124074799=Read"},
{"title": "WSI-SAM: Multi-resolution Segment Anything Model (SAM) for\n  histopathology whole-slide images", "author": "Hong Liu and Haosen Yang and Paul J. van Diest and Josien P. W. Pluim and Mitko Veta", "abstract": "  The Segment Anything Model (SAM) marks a significant advancement in\nsegmentation models, offering powerful zero-shot capabilities and dynamic\nprompting. However, existing medical SAMs are not suitable for the multi-scale\nnature of whole-slide images (WSIs), restricting their effectiveness. To\nresolve this drawback, we present WSI-SAM, enhancing SAM with precise object\nsegmentation capabilities for histopathology images using multi-resolution\npatches, while preserving its original prompt-driven design, efficiency, and\nzero-shot adaptability. To fully exploit pretrained knowledge while minimizing\ntraining overhead, we keep SAM frozen, only introducing minimal additional\nparameters and computation. In particular, we introduce High-Resolution (HR)\ntoken, Low-Resolution (LR) token and dual mask decoder. This decoder integrates\nthe original SAM mask decoder with a lightweight fusion module that integrates\nfeatures at multiple scales. Instead of predicting a mask independently, we\nintegrate HR and LR token at intermediate layer to jointly learn features of\nthe same object across multiple resolutions. Experiments show that our WSI-SAM\noutperforms state-of-the-art SAM and its variants. In particular, our model\noutperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ\n(DCIS) segmentation tasks and breast cancer metastasis segmentation task\n(CAMELYON16 dataset). The code will be available at\nhttps://github.com/HongLiuuuuu/WSI-SAM.\n", "link": "http://arxiv.org/abs/2403.09257v1", "date": "2024-03-14", "relevancy": 2.4647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images&body=Title%3A%20WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images%0AAuthor%3A%20Hong%20Liu%20and%20Haosen%20Yang%20and%20Paul%20J.%20van%20Diest%20and%20Josien%20P.%20W.%20Pluim%20and%20Mitko%20Veta%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20advancement%20in%0Asegmentation%20models%2C%20offering%20powerful%20zero-shot%20capabilities%20and%20dynamic%0Aprompting.%20However%2C%20existing%20medical%20SAMs%20are%20not%20suitable%20for%20the%20multi-scale%0Anature%20of%20whole-slide%20images%20%28WSIs%29%2C%20restricting%20their%20effectiveness.%20To%0Aresolve%20this%20drawback%2C%20we%20present%20WSI-SAM%2C%20enhancing%20SAM%20with%20precise%20object%0Asegmentation%20capabilities%20for%20histopathology%20images%20using%20multi-resolution%0Apatches%2C%20while%20preserving%20its%20original%20prompt-driven%20design%2C%20efficiency%2C%20and%0Azero-shot%20adaptability.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%0Atraining%20overhead%2C%20we%20keep%20SAM%20frozen%2C%20only%20introducing%20minimal%20additional%0Aparameters%20and%20computation.%20In%20particular%2C%20we%20introduce%20High-Resolution%20%28HR%29%0Atoken%2C%20Low-Resolution%20%28LR%29%20token%20and%20dual%20mask%20decoder.%20This%20decoder%20integrates%0Athe%20original%20SAM%20mask%20decoder%20with%20a%20lightweight%20fusion%20module%20that%20integrates%0Afeatures%20at%20multiple%20scales.%20Instead%20of%20predicting%20a%20mask%20independently%2C%20we%0Aintegrate%20HR%20and%20LR%20token%20at%20intermediate%20layer%20to%20jointly%20learn%20features%20of%0Athe%20same%20object%20across%20multiple%20resolutions.%20Experiments%20show%20that%20our%20WSI-SAM%0Aoutperforms%20state-of-the-art%20SAM%20and%20its%20variants.%20In%20particular%2C%20our%20model%0Aoutperforms%20SAM%20by%204.1%20and%202.5%20percent%20points%20on%20a%20ductal%20carcinoma%20in%20situ%0A%28DCIS%29%20segmentation%20tasks%20and%20breast%20cancer%20metastasis%20segmentation%20task%0A%28CAMELYON16%20dataset%29.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HongLiuuuuu/WSI-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09257v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images&entry.906535625=Hong%20Liu%20and%20Haosen%20Yang%20and%20Paul%20J.%20van%20Diest%20and%20Josien%20P.%20W.%20Pluim%20and%20Mitko%20Veta&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20advancement%20in%0Asegmentation%20models%2C%20offering%20powerful%20zero-shot%20capabilities%20and%20dynamic%0Aprompting.%20However%2C%20existing%20medical%20SAMs%20are%20not%20suitable%20for%20the%20multi-scale%0Anature%20of%20whole-slide%20images%20%28WSIs%29%2C%20restricting%20their%20effectiveness.%20To%0Aresolve%20this%20drawback%2C%20we%20present%20WSI-SAM%2C%20enhancing%20SAM%20with%20precise%20object%0Asegmentation%20capabilities%20for%20histopathology%20images%20using%20multi-resolution%0Apatches%2C%20while%20preserving%20its%20original%20prompt-driven%20design%2C%20efficiency%2C%20and%0Azero-shot%20adaptability.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%0Atraining%20overhead%2C%20we%20keep%20SAM%20frozen%2C%20only%20introducing%20minimal%20additional%0Aparameters%20and%20computation.%20In%20particular%2C%20we%20introduce%20High-Resolution%20%28HR%29%0Atoken%2C%20Low-Resolution%20%28LR%29%20token%20and%20dual%20mask%20decoder.%20This%20decoder%20integrates%0Athe%20original%20SAM%20mask%20decoder%20with%20a%20lightweight%20fusion%20module%20that%20integrates%0Afeatures%20at%20multiple%20scales.%20Instead%20of%20predicting%20a%20mask%20independently%2C%20we%0Aintegrate%20HR%20and%20LR%20token%20at%20intermediate%20layer%20to%20jointly%20learn%20features%20of%0Athe%20same%20object%20across%20multiple%20resolutions.%20Experiments%20show%20that%20our%20WSI-SAM%0Aoutperforms%20state-of-the-art%20SAM%20and%20its%20variants.%20In%20particular%2C%20our%20model%0Aoutperforms%20SAM%20by%204.1%20and%202.5%20percent%20points%20on%20a%20ductal%20carcinoma%20in%20situ%0A%28DCIS%29%20segmentation%20tasks%20and%20breast%20cancer%20metastasis%20segmentation%20task%0A%28CAMELYON16%20dataset%29.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HongLiuuuuu/WSI-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09257v1&entry.124074799=Read"},
{"title": "Collision-Free Robot Navigation in Crowded Environments using Learning\n  based Convex Model Predictive Control", "author": "Zhuanglei Wen and Mingze Dong and Xiai Chen", "abstract": "  Navigating robots safely and efficiently in crowded and complex environments\nremains a significant challenge. However, due to the dynamic and intricate\nnature of these settings, planning efficient and collision-free paths for\nrobots to track is particularly difficult. In this paper, we uniquely bridge\nthe robot's perception, decision-making and control processes by utilizing the\nconvex obstacle-free region computed from 2D LiDAR data. The overall pipeline\nis threefold: (1) We proposes a robot navigation framework that utilizes deep\nreinforcement learning (DRL), conceptualizing the observation as the convex\nobstacle-free region, a departure from general reliance on raw sensor inputs.\n(2) We design the action space, derived from the intersection of the robot's\nkinematic limits and the convex region, to enable efficient sampling of\ninherently collision-free reference points. These actions assists in guiding\nthe robot to move towards the goal and interact with other obstacles during\nnavigation. (3) We employ model predictive control (MPC) to track the\ntrajectory formed by the reference points while satisfying constraints imposed\nby the convex obstacle-free region and the robot's kinodynamic limits. The\neffectiveness of proposed improvements has been validated through two sets of\nablation studies and a comparative experiment against the Timed Elastic Band\n(TEB), demonstrating improved navigation performance in crowded and complex\nenvironments.\n", "link": "http://arxiv.org/abs/2403.01450v2", "date": "2024-03-14", "relevancy": 2.4566, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6234}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6015}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control&body=Title%3A%20Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control%0AAuthor%3A%20Zhuanglei%20Wen%20and%20Mingze%20Dong%20and%20Xiai%20Chen%0AAbstract%3A%20%20%20Navigating%20robots%20safely%20and%20efficiently%20in%20crowded%20and%20complex%20environments%0Aremains%20a%20significant%20challenge.%20However%2C%20due%20to%20the%20dynamic%20and%20intricate%0Anature%20of%20these%20settings%2C%20planning%20efficient%20and%20collision-free%20paths%20for%0Arobots%20to%20track%20is%20particularly%20difficult.%20In%20this%20paper%2C%20we%20uniquely%20bridge%0Athe%20robot%27s%20perception%2C%20decision-making%20and%20control%20processes%20by%20utilizing%20the%0Aconvex%20obstacle-free%20region%20computed%20from%202D%20LiDAR%20data.%20The%20overall%20pipeline%0Ais%20threefold%3A%20%281%29%20We%20proposes%20a%20robot%20navigation%20framework%20that%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%2C%20conceptualizing%20the%20observation%20as%20the%20convex%0Aobstacle-free%20region%2C%20a%20departure%20from%20general%20reliance%20on%20raw%20sensor%20inputs.%0A%282%29%20We%20design%20the%20action%20space%2C%20derived%20from%20the%20intersection%20of%20the%20robot%27s%0Akinematic%20limits%20and%20the%20convex%20region%2C%20to%20enable%20efficient%20sampling%20of%0Ainherently%20collision-free%20reference%20points.%20These%20actions%20assists%20in%20guiding%0Athe%20robot%20to%20move%20towards%20the%20goal%20and%20interact%20with%20other%20obstacles%20during%0Anavigation.%20%283%29%20We%20employ%20model%20predictive%20control%20%28MPC%29%20to%20track%20the%0Atrajectory%20formed%20by%20the%20reference%20points%20while%20satisfying%20constraints%20imposed%0Aby%20the%20convex%20obstacle-free%20region%20and%20the%20robot%27s%20kinodynamic%20limits.%20The%0Aeffectiveness%20of%20proposed%20improvements%20has%20been%20validated%20through%20two%20sets%20of%0Aablation%20studies%20and%20a%20comparative%20experiment%20against%20the%20Timed%20Elastic%20Band%0A%28TEB%29%2C%20demonstrating%20improved%20navigation%20performance%20in%20crowded%20and%20complex%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01450v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control&entry.906535625=Zhuanglei%20Wen%20and%20Mingze%20Dong%20and%20Xiai%20Chen&entry.1292438233=%20%20Navigating%20robots%20safely%20and%20efficiently%20in%20crowded%20and%20complex%20environments%0Aremains%20a%20significant%20challenge.%20However%2C%20due%20to%20the%20dynamic%20and%20intricate%0Anature%20of%20these%20settings%2C%20planning%20efficient%20and%20collision-free%20paths%20for%0Arobots%20to%20track%20is%20particularly%20difficult.%20In%20this%20paper%2C%20we%20uniquely%20bridge%0Athe%20robot%27s%20perception%2C%20decision-making%20and%20control%20processes%20by%20utilizing%20the%0Aconvex%20obstacle-free%20region%20computed%20from%202D%20LiDAR%20data.%20The%20overall%20pipeline%0Ais%20threefold%3A%20%281%29%20We%20proposes%20a%20robot%20navigation%20framework%20that%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%2C%20conceptualizing%20the%20observation%20as%20the%20convex%0Aobstacle-free%20region%2C%20a%20departure%20from%20general%20reliance%20on%20raw%20sensor%20inputs.%0A%282%29%20We%20design%20the%20action%20space%2C%20derived%20from%20the%20intersection%20of%20the%20robot%27s%0Akinematic%20limits%20and%20the%20convex%20region%2C%20to%20enable%20efficient%20sampling%20of%0Ainherently%20collision-free%20reference%20points.%20These%20actions%20assists%20in%20guiding%0Athe%20robot%20to%20move%20towards%20the%20goal%20and%20interact%20with%20other%20obstacles%20during%0Anavigation.%20%283%29%20We%20employ%20model%20predictive%20control%20%28MPC%29%20to%20track%20the%0Atrajectory%20formed%20by%20the%20reference%20points%20while%20satisfying%20constraints%20imposed%0Aby%20the%20convex%20obstacle-free%20region%20and%20the%20robot%27s%20kinodynamic%20limits.%20The%0Aeffectiveness%20of%20proposed%20improvements%20has%20been%20validated%20through%20two%20sets%20of%0Aablation%20studies%20and%20a%20comparative%20experiment%20against%20the%20Timed%20Elastic%20Band%0A%28TEB%29%2C%20demonstrating%20improved%20navigation%20performance%20in%20crowded%20and%20complex%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01450v2&entry.124074799=Read"},
{"title": "Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and\n  Personalized Stylization", "author": "Tao Yang and Rongyuan Wu and Peiran Ren and Xuansong Xie and Lei Zhang", "abstract": "  Diffusion models have demonstrated impressive performance in various image\ngeneration, editing, enhancement and translation tasks. In particular, the\npre-trained text-to-image stable diffusion models provide a potential solution\nto the challenging realistic image super-resolution (Real-ISR) and image\nstylization problems with their strong generative priors. However, the existing\nmethods along this line often fail to keep faithful pixel-wise image\nstructures. If extra skip connections are used to reproduce details, additional\ntraining in image space will be required, limiting the application to tasks in\nlatent space such as image stylization. In this work, we propose a pixel-aware\nstable diffusion (PASD) network to achieve robust Real-ISR and personalized\nimage stylization. Specifically, a pixel-aware cross attention module is\nintroduced to enable diffusion models perceiving image local structures in\npixel-wise level, while a degradation removal module is used to extract\ndegradation insensitive features to guide the diffusion process together with\nimage high level information. An adjustable noise schedule is introduced to\nfurther improve the image restoration results. By simply replacing the base\ndiffusion model with a stylized one, PASD can generate diverse stylized images\nwithout collecting pairwise training data, and by shifting the base model with\nan aesthetic one, PASD can bring old photos back to life. Extensive experiments\nin a variety of image enhancement and stylization tasks demonstrate the\neffectiveness of our proposed PASD approach. Our source codes are available at\n\\url{https://github.com/yangxy/PASD/}.\n", "link": "http://arxiv.org/abs/2308.14469v3", "date": "2024-03-14", "relevancy": 2.4405, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.58}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&body=Title%3A%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization%0AAuthor%3A%20Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20are%20used%20to%20reproduce%20details%2C%20additional%0Atraining%20in%20image%20space%20will%20be%20required%2C%20limiting%20the%20application%20to%20tasks%20in%0Alatent%20space%20such%20as%20image%20stylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%0Astable%20diffusion%20%28PASD%29%20network%20to%20achieve%20robust%20Real-ISR%20and%20personalized%0Aimage%20stylization.%20Specifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%0Aintroduced%20to%20enable%20diffusion%20models%20perceiving%20image%20local%20structures%20in%0Apixel-wise%20level%2C%20while%20a%20degradation%20removal%20module%20is%20used%20to%20extract%0Adegradation%20insensitive%20features%20to%20guide%20the%20diffusion%20process%20together%20with%0Aimage%20high%20level%20information.%20An%20adjustable%20noise%20schedule%20is%20introduced%20to%0Afurther%20improve%20the%20image%20restoration%20results.%20By%20simply%20replacing%20the%20base%0Adiffusion%20model%20with%20a%20stylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%0Awithout%20collecting%20pairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%0Aan%20aesthetic%20one%2C%20PASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%0Ain%20a%20variety%20of%20image%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14469v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&entry.906535625=Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20are%20used%20to%20reproduce%20details%2C%20additional%0Atraining%20in%20image%20space%20will%20be%20required%2C%20limiting%20the%20application%20to%20tasks%20in%0Alatent%20space%20such%20as%20image%20stylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%0Astable%20diffusion%20%28PASD%29%20network%20to%20achieve%20robust%20Real-ISR%20and%20personalized%0Aimage%20stylization.%20Specifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%0Aintroduced%20to%20enable%20diffusion%20models%20perceiving%20image%20local%20structures%20in%0Apixel-wise%20level%2C%20while%20a%20degradation%20removal%20module%20is%20used%20to%20extract%0Adegradation%20insensitive%20features%20to%20guide%20the%20diffusion%20process%20together%20with%0Aimage%20high%20level%20information.%20An%20adjustable%20noise%20schedule%20is%20introduced%20to%0Afurther%20improve%20the%20image%20restoration%20results.%20By%20simply%20replacing%20the%20base%0Adiffusion%20model%20with%20a%20stylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%0Awithout%20collecting%20pairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%0Aan%20aesthetic%20one%2C%20PASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%0Ain%20a%20variety%20of%20image%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14469v3&entry.124074799=Read"},
{"title": "A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary\n  Analysis", "author": "Linxi Zhao and Jiankai Tang and Dongyu Chen and Xiaohong Liu and Yong Zhou and Yuanchun Shi and Guangyu Wang and Yuntao Wang", "abstract": "  Nailfold capillaroscopy is widely used in assessing health conditions,\nhighlighting the pressing need for an automated nailfold capillary analysis\nsystem. In this study, we present a pioneering effort in constructing a\ncomprehensive nailfold capillary dataset-321 images, 219 videos from 68\nsubjects, with clinic reports and expert annotations-that serves as a crucial\nresource for training deep-learning models. Leveraging this dataset, we\nfinetuned three deep learning models with expert annotations as supervised\nlabels and integrated them into a novel end-to-end nailfold capillary analysis\npipeline. This pipeline excels in automatically detecting and measuring a wide\nrange of size factors, morphological features, and dynamic aspects of nailfold\ncapillaries. We compared our outcomes with clinical reports. Experiment results\nshowed that our automated pipeline achieves an average of sub-pixel level\nprecision in measurements and 89.9% accuracy in identifying morphological\nabnormalities. These results underscore its potential for advancing\nquantitative medical research and enabling pervasive computing in healthcare.\nOur data and code are available at\nhttps://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.\n", "link": "http://arxiv.org/abs/2312.05930v2", "date": "2024-03-14", "relevancy": 2.4363, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis&body=Title%3A%20A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis%0AAuthor%3A%20Linxi%20Zhao%20and%20Jiankai%20Tang%20and%20Dongyu%20Chen%20and%20Xiaohong%20Liu%20and%20Yong%20Zhou%20and%20Yuanchun%20Shi%20and%20Guangyu%20Wang%20and%20Yuntao%20Wang%0AAbstract%3A%20%20%20Nailfold%20capillaroscopy%20is%20widely%20used%20in%20assessing%20health%20conditions%2C%0Ahighlighting%20the%20pressing%20need%20for%20an%20automated%20nailfold%20capillary%20analysis%0Asystem.%20In%20this%20study%2C%20we%20present%20a%20pioneering%20effort%20in%20constructing%20a%0Acomprehensive%20nailfold%20capillary%20dataset-321%20images%2C%20219%20videos%20from%2068%0Asubjects%2C%20with%20clinic%20reports%20and%20expert%20annotations-that%20serves%20as%20a%20crucial%0Aresource%20for%20training%20deep-learning%20models.%20Leveraging%20this%20dataset%2C%20we%0Afinetuned%20three%20deep%20learning%20models%20with%20expert%20annotations%20as%20supervised%0Alabels%20and%20integrated%20them%20into%20a%20novel%20end-to-end%20nailfold%20capillary%20analysis%0Apipeline.%20This%20pipeline%20excels%20in%20automatically%20detecting%20and%20measuring%20a%20wide%0Arange%20of%20size%20factors%2C%20morphological%20features%2C%20and%20dynamic%20aspects%20of%20nailfold%0Acapillaries.%20We%20compared%20our%20outcomes%20with%20clinical%20reports.%20Experiment%20results%0Ashowed%20that%20our%20automated%20pipeline%20achieves%20an%20average%20of%20sub-pixel%20level%0Aprecision%20in%20measurements%20and%2089.9%25%20accuracy%20in%20identifying%20morphological%0Aabnormalities.%20These%20results%20underscore%20its%20potential%20for%20advancing%0Aquantitative%20medical%20research%20and%20enabling%20pervasive%20computing%20in%20healthcare.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05930v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis&entry.906535625=Linxi%20Zhao%20and%20Jiankai%20Tang%20and%20Dongyu%20Chen%20and%20Xiaohong%20Liu%20and%20Yong%20Zhou%20and%20Yuanchun%20Shi%20and%20Guangyu%20Wang%20and%20Yuntao%20Wang&entry.1292438233=%20%20Nailfold%20capillaroscopy%20is%20widely%20used%20in%20assessing%20health%20conditions%2C%0Ahighlighting%20the%20pressing%20need%20for%20an%20automated%20nailfold%20capillary%20analysis%0Asystem.%20In%20this%20study%2C%20we%20present%20a%20pioneering%20effort%20in%20constructing%20a%0Acomprehensive%20nailfold%20capillary%20dataset-321%20images%2C%20219%20videos%20from%2068%0Asubjects%2C%20with%20clinic%20reports%20and%20expert%20annotations-that%20serves%20as%20a%20crucial%0Aresource%20for%20training%20deep-learning%20models.%20Leveraging%20this%20dataset%2C%20we%0Afinetuned%20three%20deep%20learning%20models%20with%20expert%20annotations%20as%20supervised%0Alabels%20and%20integrated%20them%20into%20a%20novel%20end-to-end%20nailfold%20capillary%20analysis%0Apipeline.%20This%20pipeline%20excels%20in%20automatically%20detecting%20and%20measuring%20a%20wide%0Arange%20of%20size%20factors%2C%20morphological%20features%2C%20and%20dynamic%20aspects%20of%20nailfold%0Acapillaries.%20We%20compared%20our%20outcomes%20with%20clinical%20reports.%20Experiment%20results%0Ashowed%20that%20our%20automated%20pipeline%20achieves%20an%20average%20of%20sub-pixel%20level%0Aprecision%20in%20measurements%20and%2089.9%25%20accuracy%20in%20identifying%20morphological%0Aabnormalities.%20These%20results%20underscore%20its%20potential%20for%20advancing%0Aquantitative%20medical%20research%20and%20enabling%20pervasive%20computing%20in%20healthcare.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05930v2&entry.124074799=Read"},
{"title": "Learning to optimize with convergence guarantees using nonlinear system\n  theory", "author": "Andrea Martin and Luca Furieri", "abstract": "  The increasing reliance on numerical methods for controlling dynamical\nsystems and training machine learning models underscores the need to devise\nalgorithms that dependably and efficiently navigate complex optimization\nlandscapes. Classical gradient descent methods offer strong theoretical\nguarantees for convex problems; however, they demand meticulous hyperparameter\ntuning for non-convex ones. The emerging paradigm of learning to optimize (L2O)\nautomates the discovery of algorithms with optimized performance leveraging\nlearning models and data - yet, it lacks a theoretical framework to analyze\nconvergence and robustness of the learned algorithms. In this paper, we fill\nthis gap by harnessing nonlinear system theory. Specifically, we propose an\nunconstrained parametrization of all convergent algorithms for smooth\nnon-convex objective functions. Notably, our framework is directly compatible\nwith automatic differentiation tools, ensuring convergence by design while\nlearning to optimize.\n", "link": "http://arxiv.org/abs/2403.09389v1", "date": "2024-03-14", "relevancy": 2.4081, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4885}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory&body=Title%3A%20Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory%0AAuthor%3A%20Andrea%20Martin%20and%20Luca%20Furieri%0AAbstract%3A%20%20%20The%20increasing%20reliance%20on%20numerical%20methods%20for%20controlling%20dynamical%0Asystems%20and%20training%20machine%20learning%20models%20underscores%20the%20need%20to%20devise%0Aalgorithms%20that%20dependably%20and%20efficiently%20navigate%20complex%20optimization%0Alandscapes.%20Classical%20gradient%20descent%20methods%20offer%20strong%20theoretical%0Aguarantees%20for%20convex%20problems%3B%20however%2C%20they%20demand%20meticulous%20hyperparameter%0Atuning%20for%20non-convex%20ones.%20The%20emerging%20paradigm%20of%20learning%20to%20optimize%20%28L2O%29%0Aautomates%20the%20discovery%20of%20algorithms%20with%20optimized%20performance%20leveraging%0Alearning%20models%20and%20data%20-%20yet%2C%20it%20lacks%20a%20theoretical%20framework%20to%20analyze%0Aconvergence%20and%20robustness%20of%20the%20learned%20algorithms.%20In%20this%20paper%2C%20we%20fill%0Athis%20gap%20by%20harnessing%20nonlinear%20system%20theory.%20Specifically%2C%20we%20propose%20an%0Aunconstrained%20parametrization%20of%20all%20convergent%20algorithms%20for%20smooth%0Anon-convex%20objective%20functions.%20Notably%2C%20our%20framework%20is%20directly%20compatible%0Awith%20automatic%20differentiation%20tools%2C%20ensuring%20convergence%20by%20design%20while%0Alearning%20to%20optimize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09389v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory&entry.906535625=Andrea%20Martin%20and%20Luca%20Furieri&entry.1292438233=%20%20The%20increasing%20reliance%20on%20numerical%20methods%20for%20controlling%20dynamical%0Asystems%20and%20training%20machine%20learning%20models%20underscores%20the%20need%20to%20devise%0Aalgorithms%20that%20dependably%20and%20efficiently%20navigate%20complex%20optimization%0Alandscapes.%20Classical%20gradient%20descent%20methods%20offer%20strong%20theoretical%0Aguarantees%20for%20convex%20problems%3B%20however%2C%20they%20demand%20meticulous%20hyperparameter%0Atuning%20for%20non-convex%20ones.%20The%20emerging%20paradigm%20of%20learning%20to%20optimize%20%28L2O%29%0Aautomates%20the%20discovery%20of%20algorithms%20with%20optimized%20performance%20leveraging%0Alearning%20models%20and%20data%20-%20yet%2C%20it%20lacks%20a%20theoretical%20framework%20to%20analyze%0Aconvergence%20and%20robustness%20of%20the%20learned%20algorithms.%20In%20this%20paper%2C%20we%20fill%0Athis%20gap%20by%20harnessing%20nonlinear%20system%20theory.%20Specifically%2C%20we%20propose%20an%0Aunconstrained%20parametrization%20of%20all%20convergent%20algorithms%20for%20smooth%0Anon-convex%20objective%20functions.%20Notably%2C%20our%20framework%20is%20directly%20compatible%0Awith%20automatic%20differentiation%20tools%2C%20ensuring%20convergence%20by%20design%20while%0Alearning%20to%20optimize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09389v1&entry.124074799=Read"},
{"title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding", "author": "Chris Kelly and Luhui Hu and Jiayin Hu and Yu Tian and Deshun Yang and Bang Yang and Cindy Yang and Zihao Li and Zaoshan Huang and Yuexian Zou", "abstract": "  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n", "link": "http://arxiv.org/abs/2403.09530v1", "date": "2024-03-14", "relevancy": 2.4034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6291}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&body=Title%3A%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding%0AAuthor%3A%20Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09530v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&entry.906535625=Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou&entry.1292438233=%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09530v1&entry.124074799=Read"},
{"title": "The NeRFect Match: Exploring NeRF Features for Visual Localization", "author": "Qunjie Zhou and Maxim Maximov and Or Litany and Laura Leal-Taix\u00e9", "abstract": "  In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene\nrepresentation for visual localization. Recently, NeRF has been employed to\nenhance pose regression and scene coordinate regression models by augmenting\nthe training database, providing auxiliary supervision through rendered images,\nor serving as an iterative refinement module. We extend its recognized\nadvantages -- its ability to provide a compact scene representation with\nrealistic appearances and accurate geometry -- by exploring the potential of\nNeRF's internal features in establishing precise 2D-3D matches for\nlocalization. To this end, we conduct a comprehensive examination of NeRF's\nimplicit knowledge, acquired through view synthesis, for matching under various\nconditions. This includes exploring different matching network architectures,\nextracting encoder features at multiple layers, and varying training\nconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D\nmatching function that capitalizes on the internal knowledge of NeRF learned\nvia view synthesis. Our evaluation of NeRFMatch on standard localization\nbenchmarks, within a structure-based pipeline, sets a new state-of-the-art for\nlocalization performance on Cambridge Landmarks.\n", "link": "http://arxiv.org/abs/2403.09577v1", "date": "2024-03-14", "relevancy": 2.4021, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&body=Title%3A%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization%0AAuthor%3A%20Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09577v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&entry.906535625=Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09577v1&entry.124074799=Read"},
{"title": "A Hierarchical Fused Quantum Fuzzy Neural Network for Image\n  Classification", "author": "Sheng-Yao Wu and Run-Ze Li and Yan-Qi Song and Su-Juan Qin and Qiao-Yan Wen and Fei Gao", "abstract": "  Neural network is a powerful learning paradigm for data feature learning in\nthe era of big data. However, most neural network models are deterministic\nmodels that ignore the uncertainty of data. Fuzzy neural networks are proposed\nto address this problem. FDNN is a hierarchical deep neural network that\nderives information from both fuzzy and neural representations, the\nrepresentations are then fused to form representation to be classified. FDNN\nperform well on uncertain data classification tasks. In this paper, we proposed\na novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from\nclassical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership\nfunctions in fuzzy neural network. We conducted simulated experiment on two\ntypes of datasets (Dirty-MNIST and 15-Scene), the results show that the\nproposed model can outperform several existing methods. In addition, we\ndemonstrate the robustness of the proposed quantum circuit.\n", "link": "http://arxiv.org/abs/2403.09318v1", "date": "2024-03-14", "relevancy": 2.4008, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4942}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4657}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification&body=Title%3A%20A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification%0AAuthor%3A%20Sheng-Yao%20Wu%20and%20Run-Ze%20Li%20and%20Yan-Qi%20Song%20and%20Su-Juan%20Qin%20and%20Qiao-Yan%20Wen%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Neural%20network%20is%20a%20powerful%20learning%20paradigm%20for%20data%20feature%20learning%20in%0Athe%20era%20of%20big%20data.%20However%2C%20most%20neural%20network%20models%20are%20deterministic%0Amodels%20that%20ignore%20the%20uncertainty%20of%20data.%20Fuzzy%20neural%20networks%20are%20proposed%0Ato%20address%20this%20problem.%20FDNN%20is%20a%20hierarchical%20deep%20neural%20network%20that%0Aderives%20information%20from%20both%20fuzzy%20and%20neural%20representations%2C%20the%0Arepresentations%20are%20then%20fused%20to%20form%20representation%20to%20be%20classified.%20FDNN%0Aperform%20well%20on%20uncertain%20data%20classification%20tasks.%20In%20this%20paper%2C%20we%20proposed%0Aa%20novel%20hierarchical%20fused%20quantum%20fuzzy%20neural%20network%20%28HQFNN%29.%20Different%20from%0Aclassical%20FDNN%2C%20HQFNN%20uses%20quantum%20neural%20networks%20to%20learn%20fuzzy%20membership%0Afunctions%20in%20fuzzy%20neural%20network.%20We%20conducted%20simulated%20experiment%20on%20two%0Atypes%20of%20datasets%20%28Dirty-MNIST%20and%2015-Scene%29%2C%20the%20results%20show%20that%20the%0Aproposed%20model%20can%20outperform%20several%20existing%20methods.%20In%20addition%2C%20we%0Ademonstrate%20the%20robustness%20of%20the%20proposed%20quantum%20circuit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09318v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification&entry.906535625=Sheng-Yao%20Wu%20and%20Run-Ze%20Li%20and%20Yan-Qi%20Song%20and%20Su-Juan%20Qin%20and%20Qiao-Yan%20Wen%20and%20Fei%20Gao&entry.1292438233=%20%20Neural%20network%20is%20a%20powerful%20learning%20paradigm%20for%20data%20feature%20learning%20in%0Athe%20era%20of%20big%20data.%20However%2C%20most%20neural%20network%20models%20are%20deterministic%0Amodels%20that%20ignore%20the%20uncertainty%20of%20data.%20Fuzzy%20neural%20networks%20are%20proposed%0Ato%20address%20this%20problem.%20FDNN%20is%20a%20hierarchical%20deep%20neural%20network%20that%0Aderives%20information%20from%20both%20fuzzy%20and%20neural%20representations%2C%20the%0Arepresentations%20are%20then%20fused%20to%20form%20representation%20to%20be%20classified.%20FDNN%0Aperform%20well%20on%20uncertain%20data%20classification%20tasks.%20In%20this%20paper%2C%20we%20proposed%0Aa%20novel%20hierarchical%20fused%20quantum%20fuzzy%20neural%20network%20%28HQFNN%29.%20Different%20from%0Aclassical%20FDNN%2C%20HQFNN%20uses%20quantum%20neural%20networks%20to%20learn%20fuzzy%20membership%0Afunctions%20in%20fuzzy%20neural%20network.%20We%20conducted%20simulated%20experiment%20on%20two%0Atypes%20of%20datasets%20%28Dirty-MNIST%20and%2015-Scene%29%2C%20the%20results%20show%20that%20the%0Aproposed%20model%20can%20outperform%20several%20existing%20methods.%20In%20addition%2C%20we%0Ademonstrate%20the%20robustness%20of%20the%20proposed%20quantum%20circuit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09318v1&entry.124074799=Read"},
{"title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism", "author": "Megha Srivastava and Simran Arora and Dan Boneh", "abstract": "  The increasing compute demands of AI systems has led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning, poses challenges. Existing works\non verifiable training largely fall into two classes: proof-based systems,\nwhich struggle to scale due to requiring cryptographic techniques, and\n\"optimistic\" methods that consider a trusted third-party auditor who replicates\nthe training process. A key challenge with the latter is that hardware\nnondeterminism between GPU types during training prevents an auditor from\nreplicating the training process exactly, and such schemes are therefore\nnon-robust. We propose a method that combines training in a higher precision\nthan the target model, rounding after intermediate computation steps, and\nstoring rounding decisions based on an adaptive thresholding procedure, to\nsuccessfully control for nondeterminism. Across three different NVIDIA GPUs\n(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32\nprecision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2\n(117M) models. Our verifiable training scheme significantly decreases the\nstorage and time costs compared to proof-based systems.\n", "link": "http://arxiv.org/abs/2403.09603v1", "date": "2024-03-14", "relevancy": 2.3846, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4657}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4578}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism&body=Title%3A%20Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism%0AAuthor%3A%20Megha%20Srivastava%20and%20Simran%20Arora%20and%20Dan%20Boneh%0AAbstract%3A%20%20%20The%20increasing%20compute%20demands%20of%20AI%20systems%20has%20led%20to%20the%20emergence%20of%0Aservices%20that%20train%20models%20on%20behalf%20of%20clients%20lacking%20necessary%20resources.%0AHowever%2C%20ensuring%20correctness%20of%20training%20and%20guarding%20against%20potential%0Atraining-time%20attacks%2C%20such%20as%20data%20poisoning%2C%20poses%20challenges.%20Existing%20works%0Aon%20verifiable%20training%20largely%20fall%20into%20two%20classes%3A%20proof-based%20systems%2C%0Awhich%20struggle%20to%20scale%20due%20to%20requiring%20cryptographic%20techniques%2C%20and%0A%22optimistic%22%20methods%20that%20consider%20a%20trusted%20third-party%20auditor%20who%20replicates%0Athe%20training%20process.%20A%20key%20challenge%20with%20the%20latter%20is%20that%20hardware%0Anondeterminism%20between%20GPU%20types%20during%20training%20prevents%20an%20auditor%20from%0Areplicating%20the%20training%20process%20exactly%2C%20and%20such%20schemes%20are%20therefore%0Anon-robust.%20We%20propose%20a%20method%20that%20combines%20training%20in%20a%20higher%20precision%0Athan%20the%20target%20model%2C%20rounding%20after%20intermediate%20computation%20steps%2C%20and%0Astoring%20rounding%20decisions%20based%20on%20an%20adaptive%20thresholding%20procedure%2C%20to%0Asuccessfully%20control%20for%20nondeterminism.%20Across%20three%20different%20NVIDIA%20GPUs%0A%28A40%2C%20Titan%20XP%2C%20RTX%202080%20Ti%29%2C%20we%20achieve%20exact%20training%20replication%20at%20FP32%0Aprecision%20for%20both%20full-training%20and%20fine-tuning%20of%20ResNet-50%20%2823M%29%20and%20GPT-2%0A%28117M%29%20models.%20Our%20verifiable%20training%20scheme%20significantly%20decreases%20the%0Astorage%20and%20time%20costs%20compared%20to%20proof-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09603v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism&entry.906535625=Megha%20Srivastava%20and%20Simran%20Arora%20and%20Dan%20Boneh&entry.1292438233=%20%20The%20increasing%20compute%20demands%20of%20AI%20systems%20has%20led%20to%20the%20emergence%20of%0Aservices%20that%20train%20models%20on%20behalf%20of%20clients%20lacking%20necessary%20resources.%0AHowever%2C%20ensuring%20correctness%20of%20training%20and%20guarding%20against%20potential%0Atraining-time%20attacks%2C%20such%20as%20data%20poisoning%2C%20poses%20challenges.%20Existing%20works%0Aon%20verifiable%20training%20largely%20fall%20into%20two%20classes%3A%20proof-based%20systems%2C%0Awhich%20struggle%20to%20scale%20due%20to%20requiring%20cryptographic%20techniques%2C%20and%0A%22optimistic%22%20methods%20that%20consider%20a%20trusted%20third-party%20auditor%20who%20replicates%0Athe%20training%20process.%20A%20key%20challenge%20with%20the%20latter%20is%20that%20hardware%0Anondeterminism%20between%20GPU%20types%20during%20training%20prevents%20an%20auditor%20from%0Areplicating%20the%20training%20process%20exactly%2C%20and%20such%20schemes%20are%20therefore%0Anon-robust.%20We%20propose%20a%20method%20that%20combines%20training%20in%20a%20higher%20precision%0Athan%20the%20target%20model%2C%20rounding%20after%20intermediate%20computation%20steps%2C%20and%0Astoring%20rounding%20decisions%20based%20on%20an%20adaptive%20thresholding%20procedure%2C%20to%0Asuccessfully%20control%20for%20nondeterminism.%20Across%20three%20different%20NVIDIA%20GPUs%0A%28A40%2C%20Titan%20XP%2C%20RTX%202080%20Ti%29%2C%20we%20achieve%20exact%20training%20replication%20at%20FP32%0Aprecision%20for%20both%20full-training%20and%20fine-tuning%20of%20ResNet-50%20%2823M%29%20and%20GPT-2%0A%28117M%29%20models.%20Our%20verifiable%20training%20scheme%20significantly%20decreases%20the%0Astorage%20and%20time%20costs%20compared%20to%20proof-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09603v1&entry.124074799=Read"},
{"title": "GiT: Towards Generalist Vision Transformer through Universal Language\n  Interface", "author": "Haiyang Wang and Hao Tang and Li Jiang and Shaoshuai Shi and Muhammad Ferjad Naeem and Hongsheng Li and Bernt Schiele and Liwei Wang", "abstract": "  This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.\n", "link": "http://arxiv.org/abs/2403.09394v1", "date": "2024-03-14", "relevancy": 2.3567, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5941}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5896}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5758}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GiT%3A%20Towards%20Generalist%20Vision%20Transformer%20through%20Universal%20Language%0A%20%20Interface&body=Title%3A%20GiT%3A%20Towards%20Generalist%20Vision%20Transformer%20through%20Universal%20Language%0A%20%20Interface%0AAuthor%3A%20Haiyang%20Wang%20and%20Hao%20Tang%20and%20Li%20Jiang%20and%20Shaoshuai%20Shi%20and%20Muhammad%20Ferjad%20Naeem%20and%20Hongsheng%20Li%20and%20Bernt%20Schiele%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20simple%2C%20yet%20effective%20framework%2C%20called%20GiT%2C%0Asimultaneously%20applicable%20for%20various%20vision%20tasks%20only%20with%20a%20vanilla%20ViT.%0AMotivated%20by%20the%20universality%20of%20the%20Multi-layer%20Transformer%20architecture%20%28e.g%2C%0AGPT%29%20widely%20used%20in%20large%20language%20models%20%28LLMs%29%2C%20we%20seek%20to%20broaden%20its%20scope%0Ato%20serve%20as%20a%20powerful%20vision%20foundation%20model%20%28VFM%29.%20However%2C%20unlike%20language%0Amodeling%2C%20visual%20tasks%20typically%20require%20specific%20modules%2C%20such%20as%20bounding%20box%0Aheads%20for%20detection%20and%20pixel%20decoders%20for%20segmentation%2C%20greatly%20hindering%20the%0Aapplication%20of%20powerful%20multi-layer%20transformers%20in%20the%20vision%20domain.%20To%20solve%0Athis%2C%20we%20design%20a%20universal%20language%20interface%20that%20empowers%20the%20successful%0Aauto-regressive%20decoding%20to%20adeptly%20unify%20various%20visual%20tasks%2C%20from%0Aimage-level%20understanding%20%28e.g.%2C%20captioning%29%2C%20over%20sparse%20perception%20%28e.g.%2C%0Adetection%29%2C%20to%20dense%20prediction%20%28e.g.%2C%20segmentation%29.%20Based%20on%20the%20above%0Adesigns%2C%20the%20entire%20model%20is%20composed%20solely%20of%20a%20ViT%2C%20without%20any%20specific%0Aadditions%2C%20offering%20a%20remarkable%20architectural%20simplification.%20GiT%20is%20a%0Amulti-task%20visual%20model%2C%20jointly%20trained%20across%20five%20representative%20benchmarks%0Awithout%20task-specific%20fine-tuning.%20Interestingly%2C%20our%20GiT%20builds%20a%20new%0Abenchmark%20in%20generalist%20performance%2C%20and%20fosters%20mutual%20enhancement%20across%0Atasks%2C%20leading%20to%20significant%20improvements%20compared%20to%20isolated%20training.%20This%0Areflects%20a%20similar%20impact%20observed%20in%20LLMs.%20Further%20enriching%20training%20with%2027%0Adatasets%2C%20GiT%20achieves%20strong%20zero-shot%20results%20over%20various%20tasks.%20Due%20to%20its%0Asimple%20design%2C%20this%20paradigm%20holds%20promise%20for%20narrowing%20the%20architectural%20gap%0Abetween%20vision%20and%20language.%20Code%20and%20models%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Haiyang-W/GiT%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09394v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GiT%3A%20Towards%20Generalist%20Vision%20Transformer%20through%20Universal%20Language%0A%20%20Interface&entry.906535625=Haiyang%20Wang%20and%20Hao%20Tang%20and%20Li%20Jiang%20and%20Shaoshuai%20Shi%20and%20Muhammad%20Ferjad%20Naeem%20and%20Hongsheng%20Li%20and%20Bernt%20Schiele%20and%20Liwei%20Wang&entry.1292438233=%20%20This%20paper%20proposes%20a%20simple%2C%20yet%20effective%20framework%2C%20called%20GiT%2C%0Asimultaneously%20applicable%20for%20various%20vision%20tasks%20only%20with%20a%20vanilla%20ViT.%0AMotivated%20by%20the%20universality%20of%20the%20Multi-layer%20Transformer%20architecture%20%28e.g%2C%0AGPT%29%20widely%20used%20in%20large%20language%20models%20%28LLMs%29%2C%20we%20seek%20to%20broaden%20its%20scope%0Ato%20serve%20as%20a%20powerful%20vision%20foundation%20model%20%28VFM%29.%20However%2C%20unlike%20language%0Amodeling%2C%20visual%20tasks%20typically%20require%20specific%20modules%2C%20such%20as%20bounding%20box%0Aheads%20for%20detection%20and%20pixel%20decoders%20for%20segmentation%2C%20greatly%20hindering%20the%0Aapplication%20of%20powerful%20multi-layer%20transformers%20in%20the%20vision%20domain.%20To%20solve%0Athis%2C%20we%20design%20a%20universal%20language%20interface%20that%20empowers%20the%20successful%0Aauto-regressive%20decoding%20to%20adeptly%20unify%20various%20visual%20tasks%2C%20from%0Aimage-level%20understanding%20%28e.g.%2C%20captioning%29%2C%20over%20sparse%20perception%20%28e.g.%2C%0Adetection%29%2C%20to%20dense%20prediction%20%28e.g.%2C%20segmentation%29.%20Based%20on%20the%20above%0Adesigns%2C%20the%20entire%20model%20is%20composed%20solely%20of%20a%20ViT%2C%20without%20any%20specific%0Aadditions%2C%20offering%20a%20remarkable%20architectural%20simplification.%20GiT%20is%20a%0Amulti-task%20visual%20model%2C%20jointly%20trained%20across%20five%20representative%20benchmarks%0Awithout%20task-specific%20fine-tuning.%20Interestingly%2C%20our%20GiT%20builds%20a%20new%0Abenchmark%20in%20generalist%20performance%2C%20and%20fosters%20mutual%20enhancement%20across%0Atasks%2C%20leading%20to%20significant%20improvements%20compared%20to%20isolated%20training.%20This%0Areflects%20a%20similar%20impact%20observed%20in%20LLMs.%20Further%20enriching%20training%20with%2027%0Adatasets%2C%20GiT%20achieves%20strong%20zero-shot%20results%20over%20various%20tasks.%20Due%20to%20its%0Asimple%20design%2C%20this%20paradigm%20holds%20promise%20for%20narrowing%20the%20architectural%20gap%0Abetween%20vision%20and%20language.%20Code%20and%20models%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Haiyang-W/GiT%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09394v1&entry.124074799=Read"},
{"title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal\n  Correlation", "author": "Devavrat Tomar and Guillaume Vray and Jean-Philippe Thiran and Behzad Bozorgtabar", "abstract": "  Recent test-time adaptation methods heavily rely on nuanced adjustments of\nbatch normalization (BN) parameters. However, one critical assumption often\ngoes overlooked: that of independently and identically distributed (i.i.d.)\ntest batches with respect to unknown labels. This oversight leads to skewed BN\nstatistics and undermines the reliability of the model under non-i.i.d.\nscenarios. To tackle this challenge, this paper presents a novel method termed\n'Un-Mixing Test-Time Normalization Statistics' (UnMix-TNS). Our method\nre-calibrates the statistics for each instance within a test batch by mixing it\nwith multiple distinct statistics components, thus inherently simulating the\ni.i.d. scenario. The core of this method hinges on a distinctive online\nunmixing procedure that continuously updates these statistics components by\nincorporating the most similar instances from new test batches. Remarkably\ngeneric in its design, UnMix-TNS seamlessly integrates with a wide range of\nleading test-time adaptation methods and pre-trained architectures equipped\nwith BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS\nunder varied scenarios-ranging from single to continual and mixed domain\nshifts, particularly excelling with temporally correlated test data and\ncorrupted non-i.i.d. real-world streams. This adaptability is maintained even\nwith very small batch sizes or single instances. Our results highlight\nUnMix-TNS's capacity to markedly enhance stability and performance across\nvarious benchmarks. Our code is publicly available at\nhttps://github.com/devavratTomar/unmixtns.\n", "link": "http://arxiv.org/abs/2401.08328v2", "date": "2024-03-14", "relevancy": 2.3544, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4781}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4687}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4658}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Un-Mixing%20Test-Time%20Normalization%20Statistics%3A%20Combatting%20Label%20Temporal%0A%20%20Correlation&body=Title%3A%20Un-Mixing%20Test-Time%20Normalization%20Statistics%3A%20Combatting%20Label%20Temporal%0A%20%20Correlation%0AAuthor%3A%20Devavrat%20Tomar%20and%20Guillaume%20Vray%20and%20Jean-Philippe%20Thiran%20and%20Behzad%20Bozorgtabar%0AAbstract%3A%20%20%20Recent%20test-time%20adaptation%20methods%20heavily%20rely%20on%20nuanced%20adjustments%20of%0Abatch%20normalization%20%28BN%29%20parameters.%20However%2C%20one%20critical%20assumption%20often%0Agoes%20overlooked%3A%20that%20of%20independently%20and%20identically%20distributed%20%28i.i.d.%29%0Atest%20batches%20with%20respect%20to%20unknown%20labels.%20This%20oversight%20leads%20to%20skewed%20BN%0Astatistics%20and%20undermines%20the%20reliability%20of%20the%20model%20under%20non-i.i.d.%0Ascenarios.%20To%20tackle%20this%20challenge%2C%20this%20paper%20presents%20a%20novel%20method%20termed%0A%27Un-Mixing%20Test-Time%20Normalization%20Statistics%27%20%28UnMix-TNS%29.%20Our%20method%0Are-calibrates%20the%20statistics%20for%20each%20instance%20within%20a%20test%20batch%20by%20mixing%20it%0Awith%20multiple%20distinct%20statistics%20components%2C%20thus%20inherently%20simulating%20the%0Ai.i.d.%20scenario.%20The%20core%20of%20this%20method%20hinges%20on%20a%20distinctive%20online%0Aunmixing%20procedure%20that%20continuously%20updates%20these%20statistics%20components%20by%0Aincorporating%20the%20most%20similar%20instances%20from%20new%20test%20batches.%20Remarkably%0Ageneric%20in%20its%20design%2C%20UnMix-TNS%20seamlessly%20integrates%20with%20a%20wide%20range%20of%0Aleading%20test-time%20adaptation%20methods%20and%20pre-trained%20architectures%20equipped%0Awith%20BN%20layers.%20Empirical%20evaluations%20corroborate%20the%20robustness%20of%20UnMix-TNS%0Aunder%20varied%20scenarios-ranging%20from%20single%20to%20continual%20and%20mixed%20domain%0Ashifts%2C%20particularly%20excelling%20with%20temporally%20correlated%20test%20data%20and%0Acorrupted%20non-i.i.d.%20real-world%20streams.%20This%20adaptability%20is%20maintained%20even%0Awith%20very%20small%20batch%20sizes%20or%20single%20instances.%20Our%20results%20highlight%0AUnMix-TNS%27s%20capacity%20to%20markedly%20enhance%20stability%20and%20performance%20across%0Avarious%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/devavratTomar/unmixtns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08328v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Un-Mixing%20Test-Time%20Normalization%20Statistics%3A%20Combatting%20Label%20Temporal%0A%20%20Correlation&entry.906535625=Devavrat%20Tomar%20and%20Guillaume%20Vray%20and%20Jean-Philippe%20Thiran%20and%20Behzad%20Bozorgtabar&entry.1292438233=%20%20Recent%20test-time%20adaptation%20methods%20heavily%20rely%20on%20nuanced%20adjustments%20of%0Abatch%20normalization%20%28BN%29%20parameters.%20However%2C%20one%20critical%20assumption%20often%0Agoes%20overlooked%3A%20that%20of%20independently%20and%20identically%20distributed%20%28i.i.d.%29%0Atest%20batches%20with%20respect%20to%20unknown%20labels.%20This%20oversight%20leads%20to%20skewed%20BN%0Astatistics%20and%20undermines%20the%20reliability%20of%20the%20model%20under%20non-i.i.d.%0Ascenarios.%20To%20tackle%20this%20challenge%2C%20this%20paper%20presents%20a%20novel%20method%20termed%0A%27Un-Mixing%20Test-Time%20Normalization%20Statistics%27%20%28UnMix-TNS%29.%20Our%20method%0Are-calibrates%20the%20statistics%20for%20each%20instance%20within%20a%20test%20batch%20by%20mixing%20it%0Awith%20multiple%20distinct%20statistics%20components%2C%20thus%20inherently%20simulating%20the%0Ai.i.d.%20scenario.%20The%20core%20of%20this%20method%20hinges%20on%20a%20distinctive%20online%0Aunmixing%20procedure%20that%20continuously%20updates%20these%20statistics%20components%20by%0Aincorporating%20the%20most%20similar%20instances%20from%20new%20test%20batches.%20Remarkably%0Ageneric%20in%20its%20design%2C%20UnMix-TNS%20seamlessly%20integrates%20with%20a%20wide%20range%20of%0Aleading%20test-time%20adaptation%20methods%20and%20pre-trained%20architectures%20equipped%0Awith%20BN%20layers.%20Empirical%20evaluations%20corroborate%20the%20robustness%20of%20UnMix-TNS%0Aunder%20varied%20scenarios-ranging%20from%20single%20to%20continual%20and%20mixed%20domain%0Ashifts%2C%20particularly%20excelling%20with%20temporally%20correlated%20test%20data%20and%0Acorrupted%20non-i.i.d.%20real-world%20streams.%20This%20adaptability%20is%20maintained%20even%0Awith%20very%20small%20batch%20sizes%20or%20single%20instances.%20Our%20results%20highlight%0AUnMix-TNS%27s%20capacity%20to%20markedly%20enhance%20stability%20and%20performance%20across%0Avarious%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/devavratTomar/unmixtns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08328v2&entry.124074799=Read"},
{"title": "Improving Distant 3D Object Detection Using 2D Box Supervision", "author": "Zetong Yang and Zhiding Yu and Chris Choy and Renhao Wang and Anima Anandkumar and Jose M. Alvarez", "abstract": "  Improving the detection of distant 3d objects is an important yet challenging\ntask. For camera-based 3D perception, the annotation of 3d bounding relies\nheavily on LiDAR for accurate depth information. As such, the distance of\nannotation is often limited due to the sparsity of LiDAR points on distant\nobjects, which hampers the capability of existing detectors for long-range\nscenarios. We address this challenge by considering only 2D box supervision for\ndistant objects since they are easy to annotate. We propose LR3D, a framework\nthat learns to recover the missing depth of distant objects. LR3D adopts an\nimplicit projection head to learn the generation of mapping between 2D boxes\nand depth using the 3D supervision on close objects. This mapping allows the\ndepth estimation of distant objects conditioned on their 2D boxes, making\nlong-range 3D detection with 2D supervision feasible. Experiments show that\nwithout distant 3D annotations, LR3D allows camera-based methods to detect\ndistant objects (over 200m) with comparable accuracy to full 3D supervision.\nOur framework is general, and could widely benefit 3D detection methods to a\nlarge extent.\n", "link": "http://arxiv.org/abs/2403.09230v1", "date": "2024-03-14", "relevancy": 2.3495, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6236}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5899}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Distant%203D%20Object%20Detection%20Using%202D%20Box%20Supervision&body=Title%3A%20Improving%20Distant%203D%20Object%20Detection%20Using%202D%20Box%20Supervision%0AAuthor%3A%20Zetong%20Yang%20and%20Zhiding%20Yu%20and%20Chris%20Choy%20and%20Renhao%20Wang%20and%20Anima%20Anandkumar%20and%20Jose%20M.%20Alvarez%0AAbstract%3A%20%20%20Improving%20the%20detection%20of%20distant%203d%20objects%20is%20an%20important%20yet%20challenging%0Atask.%20For%20camera-based%203D%20perception%2C%20the%20annotation%20of%203d%20bounding%20relies%0Aheavily%20on%20LiDAR%20for%20accurate%20depth%20information.%20As%20such%2C%20the%20distance%20of%0Aannotation%20is%20often%20limited%20due%20to%20the%20sparsity%20of%20LiDAR%20points%20on%20distant%0Aobjects%2C%20which%20hampers%20the%20capability%20of%20existing%20detectors%20for%20long-range%0Ascenarios.%20We%20address%20this%20challenge%20by%20considering%20only%202D%20box%20supervision%20for%0Adistant%20objects%20since%20they%20are%20easy%20to%20annotate.%20We%20propose%20LR3D%2C%20a%20framework%0Athat%20learns%20to%20recover%20the%20missing%20depth%20of%20distant%20objects.%20LR3D%20adopts%20an%0Aimplicit%20projection%20head%20to%20learn%20the%20generation%20of%20mapping%20between%202D%20boxes%0Aand%20depth%20using%20the%203D%20supervision%20on%20close%20objects.%20This%20mapping%20allows%20the%0Adepth%20estimation%20of%20distant%20objects%20conditioned%20on%20their%202D%20boxes%2C%20making%0Along-range%203D%20detection%20with%202D%20supervision%20feasible.%20Experiments%20show%20that%0Awithout%20distant%203D%20annotations%2C%20LR3D%20allows%20camera-based%20methods%20to%20detect%0Adistant%20objects%20%28over%20200m%29%20with%20comparable%20accuracy%20to%20full%203D%20supervision.%0AOur%20framework%20is%20general%2C%20and%20could%20widely%20benefit%203D%20detection%20methods%20to%20a%0Alarge%20extent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09230v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Distant%203D%20Object%20Detection%20Using%202D%20Box%20Supervision&entry.906535625=Zetong%20Yang%20and%20Zhiding%20Yu%20and%20Chris%20Choy%20and%20Renhao%20Wang%20and%20Anima%20Anandkumar%20and%20Jose%20M.%20Alvarez&entry.1292438233=%20%20Improving%20the%20detection%20of%20distant%203d%20objects%20is%20an%20important%20yet%20challenging%0Atask.%20For%20camera-based%203D%20perception%2C%20the%20annotation%20of%203d%20bounding%20relies%0Aheavily%20on%20LiDAR%20for%20accurate%20depth%20information.%20As%20such%2C%20the%20distance%20of%0Aannotation%20is%20often%20limited%20due%20to%20the%20sparsity%20of%20LiDAR%20points%20on%20distant%0Aobjects%2C%20which%20hampers%20the%20capability%20of%20existing%20detectors%20for%20long-range%0Ascenarios.%20We%20address%20this%20challenge%20by%20considering%20only%202D%20box%20supervision%20for%0Adistant%20objects%20since%20they%20are%20easy%20to%20annotate.%20We%20propose%20LR3D%2C%20a%20framework%0Athat%20learns%20to%20recover%20the%20missing%20depth%20of%20distant%20objects.%20LR3D%20adopts%20an%0Aimplicit%20projection%20head%20to%20learn%20the%20generation%20of%20mapping%20between%202D%20boxes%0Aand%20depth%20using%20the%203D%20supervision%20on%20close%20objects.%20This%20mapping%20allows%20the%0Adepth%20estimation%20of%20distant%20objects%20conditioned%20on%20their%202D%20boxes%2C%20making%0Along-range%203D%20detection%20with%202D%20supervision%20feasible.%20Experiments%20show%20that%0Awithout%20distant%203D%20annotations%2C%20LR3D%20allows%20camera-based%20methods%20to%20detect%0Adistant%20objects%20%28over%20200m%29%20with%20comparable%20accuracy%20to%20full%203D%20supervision.%0AOur%20framework%20is%20general%2C%20and%20could%20widely%20benefit%203D%20detection%20methods%20to%20a%0Alarge%20extent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09230v1&entry.124074799=Read"},
{"title": "COMET: A Comprehensive Cluster Design Methodology for Distributed Deep\n  Learning Training", "author": "Divya Kiran Kadiyala and Saeed Rashidi and Taekyung Heo and Abhimanyu Rajeshkumar Bambhaniya and Tushar Krishna and Alexandros Daglis", "abstract": "  Modern Deep Learning (DL) models have grown to sizes requiring massive\nclusters of specialized, high-end nodes to train. Designing such clusters to\nmaximize both performance and utilization--to amortize their steep cost--is a\nchallenging task requiring careful balance of compute, memory, and network\nresources. Moreover, a plethora of each model's tuning knobs drastically affect\nthe performance, with optimal values often depending on the underlying\ncluster's characteristics, which necessitates a complex cluster-workload\nco-design process. To facilitate the design space exploration of such massive\nDL training clusters, we introduce COMET, a holistic cluster design methodology\nand workflow to jointly study the impact of parallelization strategies and key\ncluster resource provisioning on the performance of distributed DL training. We\ndevelop a step-by-step process to establish a reusable and flexible\nmethodology, and demonstrate its application with case studies of training\nlarge models on cluster configurations of variable compute, memory, and network\nresources. Our case studies demonstrate COMET's utility in identifying\npromising architectural optimization directions and guiding system designers in\nconfiguring key model and cluster parameters. To illustrate, cluster\nconfiguration comparisons identify performance differences of up to 7.7x and\nhighlight performance optimization opportunities of up to 1.4x when employing\nmemory expansion as an optimization technique.\n", "link": "http://arxiv.org/abs/2211.16648v2", "date": "2024-03-14", "relevancy": 2.3452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4703}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4654}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COMET%3A%20A%20Comprehensive%20Cluster%20Design%20Methodology%20for%20Distributed%20Deep%0A%20%20Learning%20Training&body=Title%3A%20COMET%3A%20A%20Comprehensive%20Cluster%20Design%20Methodology%20for%20Distributed%20Deep%0A%20%20Learning%20Training%0AAuthor%3A%20Divya%20Kiran%20Kadiyala%20and%20Saeed%20Rashidi%20and%20Taekyung%20Heo%20and%20Abhimanyu%20Rajeshkumar%20Bambhaniya%20and%20Tushar%20Krishna%20and%20Alexandros%20Daglis%0AAbstract%3A%20%20%20Modern%20Deep%20Learning%20%28DL%29%20models%20have%20grown%20to%20sizes%20requiring%20massive%0Aclusters%20of%20specialized%2C%20high-end%20nodes%20to%20train.%20Designing%20such%20clusters%20to%0Amaximize%20both%20performance%20and%20utilization--to%20amortize%20their%20steep%20cost--is%20a%0Achallenging%20task%20requiring%20careful%20balance%20of%20compute%2C%20memory%2C%20and%20network%0Aresources.%20Moreover%2C%20a%20plethora%20of%20each%20model%27s%20tuning%20knobs%20drastically%20affect%0Athe%20performance%2C%20with%20optimal%20values%20often%20depending%20on%20the%20underlying%0Acluster%27s%20characteristics%2C%20which%20necessitates%20a%20complex%20cluster-workload%0Aco-design%20process.%20To%20facilitate%20the%20design%20space%20exploration%20of%20such%20massive%0ADL%20training%20clusters%2C%20we%20introduce%20COMET%2C%20a%20holistic%20cluster%20design%20methodology%0Aand%20workflow%20to%20jointly%20study%20the%20impact%20of%20parallelization%20strategies%20and%20key%0Acluster%20resource%20provisioning%20on%20the%20performance%20of%20distributed%20DL%20training.%20We%0Adevelop%20a%20step-by-step%20process%20to%20establish%20a%20reusable%20and%20flexible%0Amethodology%2C%20and%20demonstrate%20its%20application%20with%20case%20studies%20of%20training%0Alarge%20models%20on%20cluster%20configurations%20of%20variable%20compute%2C%20memory%2C%20and%20network%0Aresources.%20Our%20case%20studies%20demonstrate%20COMET%27s%20utility%20in%20identifying%0Apromising%20architectural%20optimization%20directions%20and%20guiding%20system%20designers%20in%0Aconfiguring%20key%20model%20and%20cluster%20parameters.%20To%20illustrate%2C%20cluster%0Aconfiguration%20comparisons%20identify%20performance%20differences%20of%20up%20to%207.7x%20and%0Ahighlight%20performance%20optimization%20opportunities%20of%20up%20to%201.4x%20when%20employing%0Amemory%20expansion%20as%20an%20optimization%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.16648v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMET%3A%20A%20Comprehensive%20Cluster%20Design%20Methodology%20for%20Distributed%20Deep%0A%20%20Learning%20Training&entry.906535625=Divya%20Kiran%20Kadiyala%20and%20Saeed%20Rashidi%20and%20Taekyung%20Heo%20and%20Abhimanyu%20Rajeshkumar%20Bambhaniya%20and%20Tushar%20Krishna%20and%20Alexandros%20Daglis&entry.1292438233=%20%20Modern%20Deep%20Learning%20%28DL%29%20models%20have%20grown%20to%20sizes%20requiring%20massive%0Aclusters%20of%20specialized%2C%20high-end%20nodes%20to%20train.%20Designing%20such%20clusters%20to%0Amaximize%20both%20performance%20and%20utilization--to%20amortize%20their%20steep%20cost--is%20a%0Achallenging%20task%20requiring%20careful%20balance%20of%20compute%2C%20memory%2C%20and%20network%0Aresources.%20Moreover%2C%20a%20plethora%20of%20each%20model%27s%20tuning%20knobs%20drastically%20affect%0Athe%20performance%2C%20with%20optimal%20values%20often%20depending%20on%20the%20underlying%0Acluster%27s%20characteristics%2C%20which%20necessitates%20a%20complex%20cluster-workload%0Aco-design%20process.%20To%20facilitate%20the%20design%20space%20exploration%20of%20such%20massive%0ADL%20training%20clusters%2C%20we%20introduce%20COMET%2C%20a%20holistic%20cluster%20design%20methodology%0Aand%20workflow%20to%20jointly%20study%20the%20impact%20of%20parallelization%20strategies%20and%20key%0Acluster%20resource%20provisioning%20on%20the%20performance%20of%20distributed%20DL%20training.%20We%0Adevelop%20a%20step-by-step%20process%20to%20establish%20a%20reusable%20and%20flexible%0Amethodology%2C%20and%20demonstrate%20its%20application%20with%20case%20studies%20of%20training%0Alarge%20models%20on%20cluster%20configurations%20of%20variable%20compute%2C%20memory%2C%20and%20network%0Aresources.%20Our%20case%20studies%20demonstrate%20COMET%27s%20utility%20in%20identifying%0Apromising%20architectural%20optimization%20directions%20and%20guiding%20system%20designers%20in%0Aconfiguring%20key%20model%20and%20cluster%20parameters.%20To%20illustrate%2C%20cluster%0Aconfiguration%20comparisons%20identify%20performance%20differences%20of%20up%20to%207.7x%20and%0Ahighlight%20performance%20optimization%20opportunities%20of%20up%20to%201.4x%20when%20employing%0Amemory%20expansion%20as%20an%20optimization%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.16648v2&entry.124074799=Read"},
{"title": "Breast Cancer Classification Using Gradient Boosting Algorithms Focusing\n  on Reducing the False Negative and SHAP for Explainability", "author": "Jo\u00e3o Manoel Herrera Pinheiro and Marcelo Becker", "abstract": "  Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models.\n", "link": "http://arxiv.org/abs/2403.09548v1", "date": "2024-03-14", "relevancy": 2.3251, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5152}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability&body=Title%3A%20Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability%0AAuthor%3A%20Jo%C3%A3o%20Manoel%20Herrera%20Pinheiro%20and%20Marcelo%20Becker%0AAbstract%3A%20%20%20Cancer%20is%20one%20of%20the%20diseases%20that%20kill%20the%20most%20women%20in%20the%20world%2C%20with%0Abreast%20cancer%20being%20responsible%20for%20the%20highest%20number%20of%20cancer%20cases%20and%0Aconsequently%20deaths.%20However%2C%20it%20can%20be%20prevented%20by%20early%20detection%20and%2C%0Aconsequently%2C%20early%20treatment.%20Any%20development%20for%20detection%20or%20perdition%20this%0Akind%20of%20cancer%20is%20important%20for%20a%20better%20healthy%20life.%20Many%20studies%20focus%20on%20a%0Amodel%20with%20high%20accuracy%20in%20cancer%20prediction%2C%20but%20sometimes%20accuracy%20alone%20may%0Anot%20always%20be%20a%20reliable%20metric.%20This%20study%20implies%20an%20investigative%20approach%0Ato%20studying%20the%20performance%20of%20different%20machine%20learning%20algorithms%20based%20on%0Aboosting%20to%20predict%20breast%20cancer%20focusing%20on%20the%20recall%20metric.%20Boosting%0Amachine%20learning%20algorithms%20has%20been%20proven%20to%20be%20an%20effective%20tool%20for%0Adetecting%20medical%20diseases.%20The%20dataset%20of%20the%20University%20of%20California%2C%20Irvine%0A%28UCI%29%20repository%20has%20been%20utilized%20to%20train%20and%20test%20the%20model%20classifier%20that%0Acontains%20their%20attributes.%20The%20main%20objective%20of%20this%20study%20is%20to%20use%0Astate-of-the-art%20boosting%20algorithms%20such%20as%20AdaBoost%2C%20XGBoost%2C%20CatBoost%20and%0ALightGBM%20to%20predict%20and%20diagnose%20breast%20cancer%20and%20to%20find%20the%20most%20effective%0Ametric%20regarding%20recall%2C%20ROC-AUC%2C%20and%20confusion%20matrix.%20Furthermore%2C%20our%20study%0Ais%20the%20first%20to%20use%20these%20four%20boosting%20algorithms%20with%20Optuna%2C%20a%20library%20for%0Ahyperparameter%20optimization%2C%20and%20the%20SHAP%20method%20to%20improve%20the%0Ainterpretability%20of%20our%20model%2C%20which%20can%20be%20used%20as%20a%20support%20to%20identify%20and%0Apredict%20breast%20cancer.%20We%20were%20able%20to%20improve%20AUC%20or%20recall%20for%20all%20the%20models%0Aand%20reduce%20the%20False%20Negative%20for%20AdaBoost%20and%20LigthGBM%20the%20final%20AUC%20were%20more%0Athan%2099.41%5C%25%20for%20all%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09548v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breast%20Cancer%20Classification%20Using%20Gradient%20Boosting%20Algorithms%20Focusing%0A%20%20on%20Reducing%20the%20False%20Negative%20and%20SHAP%20for%20Explainability&entry.906535625=Jo%C3%A3o%20Manoel%20Herrera%20Pinheiro%20and%20Marcelo%20Becker&entry.1292438233=%20%20Cancer%20is%20one%20of%20the%20diseases%20that%20kill%20the%20most%20women%20in%20the%20world%2C%20with%0Abreast%20cancer%20being%20responsible%20for%20the%20highest%20number%20of%20cancer%20cases%20and%0Aconsequently%20deaths.%20However%2C%20it%20can%20be%20prevented%20by%20early%20detection%20and%2C%0Aconsequently%2C%20early%20treatment.%20Any%20development%20for%20detection%20or%20perdition%20this%0Akind%20of%20cancer%20is%20important%20for%20a%20better%20healthy%20life.%20Many%20studies%20focus%20on%20a%0Amodel%20with%20high%20accuracy%20in%20cancer%20prediction%2C%20but%20sometimes%20accuracy%20alone%20may%0Anot%20always%20be%20a%20reliable%20metric.%20This%20study%20implies%20an%20investigative%20approach%0Ato%20studying%20the%20performance%20of%20different%20machine%20learning%20algorithms%20based%20on%0Aboosting%20to%20predict%20breast%20cancer%20focusing%20on%20the%20recall%20metric.%20Boosting%0Amachine%20learning%20algorithms%20has%20been%20proven%20to%20be%20an%20effective%20tool%20for%0Adetecting%20medical%20diseases.%20The%20dataset%20of%20the%20University%20of%20California%2C%20Irvine%0A%28UCI%29%20repository%20has%20been%20utilized%20to%20train%20and%20test%20the%20model%20classifier%20that%0Acontains%20their%20attributes.%20The%20main%20objective%20of%20this%20study%20is%20to%20use%0Astate-of-the-art%20boosting%20algorithms%20such%20as%20AdaBoost%2C%20XGBoost%2C%20CatBoost%20and%0ALightGBM%20to%20predict%20and%20diagnose%20breast%20cancer%20and%20to%20find%20the%20most%20effective%0Ametric%20regarding%20recall%2C%20ROC-AUC%2C%20and%20confusion%20matrix.%20Furthermore%2C%20our%20study%0Ais%20the%20first%20to%20use%20these%20four%20boosting%20algorithms%20with%20Optuna%2C%20a%20library%20for%0Ahyperparameter%20optimization%2C%20and%20the%20SHAP%20method%20to%20improve%20the%0Ainterpretability%20of%20our%20model%2C%20which%20can%20be%20used%20as%20a%20support%20to%20identify%20and%0Apredict%20breast%20cancer.%20We%20were%20able%20to%20improve%20AUC%20or%20recall%20for%20all%20the%20models%0Aand%20reduce%20the%20False%20Negative%20for%20AdaBoost%20and%20LigthGBM%20the%20final%20AUC%20were%20more%0Athan%2099.41%5C%25%20for%20all%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09548v1&entry.124074799=Read"},
{"title": "Eta Inversion: Designing an Optimal Eta Function for Diffusion-based\n  Real Image Editing", "author": "Wonjun Kang and Kevin Galim and Hyung Il Koo", "abstract": "  Diffusion models have achieved remarkable success in the domain of\ntext-guided image generation and, more recently, in text-guided image editing.\nA commonly adopted strategy for editing real images involves inverting the\ndiffusion process to obtain a noisy representation of the original image, which\nis then denoised to achieve the desired edits. However, current methods for\ndiffusion inversion often struggle to produce edits that are both faithful to\nthe specified text prompt and closely resemble the source image. To overcome\nthese limitations, we introduce a novel and adaptable diffusion inversion\ntechnique for real image editing, which is grounded in a theoretical analysis\nof the role of $\\eta$ in the DDIM sampling equation for enhanced editability.\nBy designing a universal diffusion inversion method with a time- and\nregion-dependent $\\eta$ function, we enable flexible control over the editing\nextent. Through a comprehensive series of quantitative and qualitative\nassessments, involving a comparison with a broad array of recent methods, we\ndemonstrate the superiority of our approach. Our method not only sets a new\nbenchmark in the field but also significantly outperforms existing strategies.\nOur code is available at https://github.com/furiosa-ai/eta-inversion\n", "link": "http://arxiv.org/abs/2403.09468v1", "date": "2024-03-14", "relevancy": 2.3158, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6105}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5766}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5686}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Eta%20Inversion%3A%20Designing%20an%20Optimal%20Eta%20Function%20for%20Diffusion-based%0A%20%20Real%20Image%20Editing&body=Title%3A%20Eta%20Inversion%3A%20Designing%20an%20Optimal%20Eta%20Function%20for%20Diffusion-based%0A%20%20Real%20Image%20Editing%0AAuthor%3A%20Wonjun%20Kang%20and%20Kevin%20Galim%20and%20Hyung%20Il%20Koo%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20remarkable%20success%20in%20the%20domain%20of%0Atext-guided%20image%20generation%20and%2C%20more%20recently%2C%20in%20text-guided%20image%20editing.%0AA%20commonly%20adopted%20strategy%20for%20editing%20real%20images%20involves%20inverting%20the%0Adiffusion%20process%20to%20obtain%20a%20noisy%20representation%20of%20the%20original%20image%2C%20which%0Ais%20then%20denoised%20to%20achieve%20the%20desired%20edits.%20However%2C%20current%20methods%20for%0Adiffusion%20inversion%20often%20struggle%20to%20produce%20edits%20that%20are%20both%20faithful%20to%0Athe%20specified%20text%20prompt%20and%20closely%20resemble%20the%20source%20image.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20and%20adaptable%20diffusion%20inversion%0Atechnique%20for%20real%20image%20editing%2C%20which%20is%20grounded%20in%20a%20theoretical%20analysis%0Aof%20the%20role%20of%20%24%5Ceta%24%20in%20the%20DDIM%20sampling%20equation%20for%20enhanced%20editability.%0ABy%20designing%20a%20universal%20diffusion%20inversion%20method%20with%20a%20time-%20and%0Aregion-dependent%20%24%5Ceta%24%20function%2C%20we%20enable%20flexible%20control%20over%20the%20editing%0Aextent.%20Through%20a%20comprehensive%20series%20of%20quantitative%20and%20qualitative%0Aassessments%2C%20involving%20a%20comparison%20with%20a%20broad%20array%20of%20recent%20methods%2C%20we%0Ademonstrate%20the%20superiority%20of%20our%20approach.%20Our%20method%20not%20only%20sets%20a%20new%0Abenchmark%20in%20the%20field%20but%20also%20significantly%20outperforms%20existing%20strategies.%0AOur%20code%20is%20available%20at%20https%3A//github.com/furiosa-ai/eta-inversion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09468v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eta%20Inversion%3A%20Designing%20an%20Optimal%20Eta%20Function%20for%20Diffusion-based%0A%20%20Real%20Image%20Editing&entry.906535625=Wonjun%20Kang%20and%20Kevin%20Galim%20and%20Hyung%20Il%20Koo&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20remarkable%20success%20in%20the%20domain%20of%0Atext-guided%20image%20generation%20and%2C%20more%20recently%2C%20in%20text-guided%20image%20editing.%0AA%20commonly%20adopted%20strategy%20for%20editing%20real%20images%20involves%20inverting%20the%0Adiffusion%20process%20to%20obtain%20a%20noisy%20representation%20of%20the%20original%20image%2C%20which%0Ais%20then%20denoised%20to%20achieve%20the%20desired%20edits.%20However%2C%20current%20methods%20for%0Adiffusion%20inversion%20often%20struggle%20to%20produce%20edits%20that%20are%20both%20faithful%20to%0Athe%20specified%20text%20prompt%20and%20closely%20resemble%20the%20source%20image.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20and%20adaptable%20diffusion%20inversion%0Atechnique%20for%20real%20image%20editing%2C%20which%20is%20grounded%20in%20a%20theoretical%20analysis%0Aof%20the%20role%20of%20%24%5Ceta%24%20in%20the%20DDIM%20sampling%20equation%20for%20enhanced%20editability.%0ABy%20designing%20a%20universal%20diffusion%20inversion%20method%20with%20a%20time-%20and%0Aregion-dependent%20%24%5Ceta%24%20function%2C%20we%20enable%20flexible%20control%20over%20the%20editing%0Aextent.%20Through%20a%20comprehensive%20series%20of%20quantitative%20and%20qualitative%0Aassessments%2C%20involving%20a%20comparison%20with%20a%20broad%20array%20of%20recent%20methods%2C%20we%0Ademonstrate%20the%20superiority%20of%20our%20approach.%20Our%20method%20not%20only%20sets%20a%20new%0Abenchmark%20in%20the%20field%20but%20also%20significantly%20outperforms%20existing%20strategies.%0AOur%20code%20is%20available%20at%20https%3A//github.com/furiosa-ai/eta-inversion%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09468v1&entry.124074799=Read"},
{"title": "Generative artificial intelligence enhances creativity but reduces the\n  diversity of novel content", "author": "Anil R. Doshi and Oliver P. Hauser", "abstract": "  Creativity is core to being human. Generative artificial intelligence (GenAI)\nholds promise for humans to be more creative by offering new ideas, or less\ncreative by anchoring on GenAI ideas. We study the causal impact of GenAI on\nthe production of a creative output in an online experimental study where some\nwriters are could obtain ideas for a story from a GenAI platform. Access to\nGenAI ideas causes an increase in the writer's creativity with stories being\nevaluated as better written and more enjoyable, especially among less creative\nwriters. However, GenAI-enabled stories are more similar to each other than\nstories by humans alone. Our results have implications for researchers,\npolicy-makers and practitioners interested in bolstering creativity, but point\nto potential downstream consequences from over-reliance.\n", "link": "http://arxiv.org/abs/2312.00506v3", "date": "2024-03-14", "relevancy": 2.3088, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5436}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4399}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4017}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20artificial%20intelligence%20enhances%20creativity%20but%20reduces%20the%0A%20%20diversity%20of%20novel%20content&body=Title%3A%20Generative%20artificial%20intelligence%20enhances%20creativity%20but%20reduces%20the%0A%20%20diversity%20of%20novel%20content%0AAuthor%3A%20Anil%20R.%20Doshi%20and%20Oliver%20P.%20Hauser%0AAbstract%3A%20%20%20Creativity%20is%20core%20to%20being%20human.%20Generative%20artificial%20intelligence%20%28GenAI%29%0Aholds%20promise%20for%20humans%20to%20be%20more%20creative%20by%20offering%20new%20ideas%2C%20or%20less%0Acreative%20by%20anchoring%20on%20GenAI%20ideas.%20We%20study%20the%20causal%20impact%20of%20GenAI%20on%0Athe%20production%20of%20a%20creative%20output%20in%20an%20online%20experimental%20study%20where%20some%0Awriters%20are%20could%20obtain%20ideas%20for%20a%20story%20from%20a%20GenAI%20platform.%20Access%20to%0AGenAI%20ideas%20causes%20an%20increase%20in%20the%20writer%27s%20creativity%20with%20stories%20being%0Aevaluated%20as%20better%20written%20and%20more%20enjoyable%2C%20especially%20among%20less%20creative%0Awriters.%20However%2C%20GenAI-enabled%20stories%20are%20more%20similar%20to%20each%20other%20than%0Astories%20by%20humans%20alone.%20Our%20results%20have%20implications%20for%20researchers%2C%0Apolicy-makers%20and%20practitioners%20interested%20in%20bolstering%20creativity%2C%20but%20point%0Ato%20potential%20downstream%20consequences%20from%20over-reliance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00506v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20artificial%20intelligence%20enhances%20creativity%20but%20reduces%20the%0A%20%20diversity%20of%20novel%20content&entry.906535625=Anil%20R.%20Doshi%20and%20Oliver%20P.%20Hauser&entry.1292438233=%20%20Creativity%20is%20core%20to%20being%20human.%20Generative%20artificial%20intelligence%20%28GenAI%29%0Aholds%20promise%20for%20humans%20to%20be%20more%20creative%20by%20offering%20new%20ideas%2C%20or%20less%0Acreative%20by%20anchoring%20on%20GenAI%20ideas.%20We%20study%20the%20causal%20impact%20of%20GenAI%20on%0Athe%20production%20of%20a%20creative%20output%20in%20an%20online%20experimental%20study%20where%20some%0Awriters%20are%20could%20obtain%20ideas%20for%20a%20story%20from%20a%20GenAI%20platform.%20Access%20to%0AGenAI%20ideas%20causes%20an%20increase%20in%20the%20writer%27s%20creativity%20with%20stories%20being%0Aevaluated%20as%20better%20written%20and%20more%20enjoyable%2C%20especially%20among%20less%20creative%0Awriters.%20However%2C%20GenAI-enabled%20stories%20are%20more%20similar%20to%20each%20other%20than%0Astories%20by%20humans%20alone.%20Our%20results%20have%20implications%20for%20researchers%2C%0Apolicy-makers%20and%20practitioners%20interested%20in%20bolstering%20creativity%2C%20but%20point%0Ato%20potential%20downstream%20consequences%20from%20over-reliance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00506v3&entry.124074799=Read"},
{"title": "EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by\n  Semantic Segmentation", "author": "Nikolai K\u00f6rber and Eduard Kromer and Andreas Siebert and Sascha Hauke and Daniel Mueller-Gritschneder and Bj\u00f6rn Schuller", "abstract": "  We introduce EGIC, an enhanced generative image compression method that\nallows traversing the distortion-perception curve efficiently from a single\nmodel. EGIC is based on two novel building blocks: i) OASIS-C, a conditional\npre-trained semantic segmentation-guided discriminator, which provides both\nspatially and semantically-aware gradient feedback to the generator,\nconditioned on the latent image distribution, and ii) Output Residual\nPrediction (ORP), a retrofit solution for multi-realism image compression that\nallows control over the synthesis process by adjusting the impact of the\nresidual between an MSE-optimized and GAN-optimized decoder output on the\nGAN-based reconstruction. Together, EGIC forms a powerful codec, outperforming\nstate-of-the-art diffusion and GAN-based methods (e.g., HiFiC, MS-ILLM, and\nDIRAC-100), while performing almost on par with VTM-20.0 on the distortion end.\nEGIC is simple to implement, very lightweight, and provides excellent\ninterpolation characteristics, which makes it a promising candidate for\npractical applications targeting the low bit range.\n", "link": "http://arxiv.org/abs/2309.03244v2", "date": "2024-03-14", "relevancy": 2.3039, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5892}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5758}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EGIC%3A%20Enhanced%20Low-Bit-Rate%20Generative%20Image%20Compression%20Guided%20by%0A%20%20Semantic%20Segmentation&body=Title%3A%20EGIC%3A%20Enhanced%20Low-Bit-Rate%20Generative%20Image%20Compression%20Guided%20by%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Nikolai%20K%C3%B6rber%20and%20Eduard%20Kromer%20and%20Andreas%20Siebert%20and%20Sascha%20Hauke%20and%20Daniel%20Mueller-Gritschneder%20and%20Bj%C3%B6rn%20Schuller%0AAbstract%3A%20%20%20We%20introduce%20EGIC%2C%20an%20enhanced%20generative%20image%20compression%20method%20that%0Aallows%20traversing%20the%20distortion-perception%20curve%20efficiently%20from%20a%20single%0Amodel.%20EGIC%20is%20based%20on%20two%20novel%20building%20blocks%3A%20i%29%20OASIS-C%2C%20a%20conditional%0Apre-trained%20semantic%20segmentation-guided%20discriminator%2C%20which%20provides%20both%0Aspatially%20and%20semantically-aware%20gradient%20feedback%20to%20the%20generator%2C%0Aconditioned%20on%20the%20latent%20image%20distribution%2C%20and%20ii%29%20Output%20Residual%0APrediction%20%28ORP%29%2C%20a%20retrofit%20solution%20for%20multi-realism%20image%20compression%20that%0Aallows%20control%20over%20the%20synthesis%20process%20by%20adjusting%20the%20impact%20of%20the%0Aresidual%20between%20an%20MSE-optimized%20and%20GAN-optimized%20decoder%20output%20on%20the%0AGAN-based%20reconstruction.%20Together%2C%20EGIC%20forms%20a%20powerful%20codec%2C%20outperforming%0Astate-of-the-art%20diffusion%20and%20GAN-based%20methods%20%28e.g.%2C%20HiFiC%2C%20MS-ILLM%2C%20and%0ADIRAC-100%29%2C%20while%20performing%20almost%20on%20par%20with%20VTM-20.0%20on%20the%20distortion%20end.%0AEGIC%20is%20simple%20to%20implement%2C%20very%20lightweight%2C%20and%20provides%20excellent%0Ainterpolation%20characteristics%2C%20which%20makes%20it%20a%20promising%20candidate%20for%0Apractical%20applications%20targeting%20the%20low%20bit%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03244v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGIC%3A%20Enhanced%20Low-Bit-Rate%20Generative%20Image%20Compression%20Guided%20by%0A%20%20Semantic%20Segmentation&entry.906535625=Nikolai%20K%C3%B6rber%20and%20Eduard%20Kromer%20and%20Andreas%20Siebert%20and%20Sascha%20Hauke%20and%20Daniel%20Mueller-Gritschneder%20and%20Bj%C3%B6rn%20Schuller&entry.1292438233=%20%20We%20introduce%20EGIC%2C%20an%20enhanced%20generative%20image%20compression%20method%20that%0Aallows%20traversing%20the%20distortion-perception%20curve%20efficiently%20from%20a%20single%0Amodel.%20EGIC%20is%20based%20on%20two%20novel%20building%20blocks%3A%20i%29%20OASIS-C%2C%20a%20conditional%0Apre-trained%20semantic%20segmentation-guided%20discriminator%2C%20which%20provides%20both%0Aspatially%20and%20semantically-aware%20gradient%20feedback%20to%20the%20generator%2C%0Aconditioned%20on%20the%20latent%20image%20distribution%2C%20and%20ii%29%20Output%20Residual%0APrediction%20%28ORP%29%2C%20a%20retrofit%20solution%20for%20multi-realism%20image%20compression%20that%0Aallows%20control%20over%20the%20synthesis%20process%20by%20adjusting%20the%20impact%20of%20the%0Aresidual%20between%20an%20MSE-optimized%20and%20GAN-optimized%20decoder%20output%20on%20the%0AGAN-based%20reconstruction.%20Together%2C%20EGIC%20forms%20a%20powerful%20codec%2C%20outperforming%0Astate-of-the-art%20diffusion%20and%20GAN-based%20methods%20%28e.g.%2C%20HiFiC%2C%20MS-ILLM%2C%20and%0ADIRAC-100%29%2C%20while%20performing%20almost%20on%20par%20with%20VTM-20.0%20on%20the%20distortion%20end.%0AEGIC%20is%20simple%20to%20implement%2C%20very%20lightweight%2C%20and%20provides%20excellent%0Ainterpolation%20characteristics%2C%20which%20makes%20it%20a%20promising%20candidate%20for%0Apractical%20applications%20targeting%20the%20low%20bit%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03244v2&entry.124074799=Read"},
{"title": "VBART: The Turkish LLM", "author": "Meliksah Turker and Mehmet Erdi Ari and Aydin Han", "abstract": "  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n", "link": "http://arxiv.org/abs/2403.01308v2", "date": "2024-03-14", "relevancy": 2.3032, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4885}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4505}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4429}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VBART%3A%20The%20Turkish%20LLM&body=Title%3A%20VBART%3A%20The%20Turkish%20LLM%0AAuthor%3A%20Meliksah%20Turker%20and%20Mehmet%20Erdi%20Ari%20and%20Aydin%20Han%0AAbstract%3A%20%20%20We%20present%20VBART%2C%20the%20first%20Turkish%20sequence-to-sequence%20Large%20Language%0AModels%20%28LLMs%29%20pre-trained%20on%20a%20large%20corpus%20from%20scratch.%20VBART%20are%20compact%0ALLMs%20based%20on%20good%20ideas%20leveraged%20from%20BART%20and%20mBART%20models%20and%20come%20in%20two%0Asizes%2C%20Large%20and%20XLarge.%20Fine-tuned%20VBART%20models%20surpass%20the%20prior%0Astate-of-the-art%20results%20in%20abstractive%20text%20summarization%2C%20title%20generation%2C%0Atext%20paraphrasing%2C%20question%20answering%20and%20question%20generation%20tasks.%20They%20allow%0Afine-tuning%20for%20future%20text%20generation%20tasks%20and%20datasets%2C%20carving%20a%20new%20path%0Afor%20Turkish%20Natural%20Language%20Processing%20%28NLP%29%20research.%20Our%20work%20shows%20that%0Ahaving%20a%20pre-trained%20LLM%20for%20Turkish%20outperforms%20up%20to%203x%20multilingual%20models%2C%0Aimproving%20existing%20results%20and%20providing%20efficient%20models%20for%20training%20and%0Ainference.%20Moreover%2C%20we%20show%20that%20our%20monolingual%20tokenizer%20is%20up%20to%2011x%20more%0Aefficient%20than%20multilingual%20tokenizers.%20Last%20but%20not%20least%2C%20we%20introduce%20a%0Amethod%20to%20enlarge%20an%20existing%20pre-trained%20LLM%20and%20question%20the%20relevancy%20of%0AChinchilla%20Scaling%20Law%20to%20sequence-to-sequence%20masked%20language%20models.%20Our%0Afine-tuned%20models%2C%20tokenizer%20and%20cleaned%20vngrs-web-corpus%20of%20135%20GB%20are%0Apublicly%20available%20at%20huggingface.co/vngrs-ai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01308v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VBART%3A%20The%20Turkish%20LLM&entry.906535625=Meliksah%20Turker%20and%20Mehmet%20Erdi%20Ari%20and%20Aydin%20Han&entry.1292438233=%20%20We%20present%20VBART%2C%20the%20first%20Turkish%20sequence-to-sequence%20Large%20Language%0AModels%20%28LLMs%29%20pre-trained%20on%20a%20large%20corpus%20from%20scratch.%20VBART%20are%20compact%0ALLMs%20based%20on%20good%20ideas%20leveraged%20from%20BART%20and%20mBART%20models%20and%20come%20in%20two%0Asizes%2C%20Large%20and%20XLarge.%20Fine-tuned%20VBART%20models%20surpass%20the%20prior%0Astate-of-the-art%20results%20in%20abstractive%20text%20summarization%2C%20title%20generation%2C%0Atext%20paraphrasing%2C%20question%20answering%20and%20question%20generation%20tasks.%20They%20allow%0Afine-tuning%20for%20future%20text%20generation%20tasks%20and%20datasets%2C%20carving%20a%20new%20path%0Afor%20Turkish%20Natural%20Language%20Processing%20%28NLP%29%20research.%20Our%20work%20shows%20that%0Ahaving%20a%20pre-trained%20LLM%20for%20Turkish%20outperforms%20up%20to%203x%20multilingual%20models%2C%0Aimproving%20existing%20results%20and%20providing%20efficient%20models%20for%20training%20and%0Ainference.%20Moreover%2C%20we%20show%20that%20our%20monolingual%20tokenizer%20is%20up%20to%2011x%20more%0Aefficient%20than%20multilingual%20tokenizers.%20Last%20but%20not%20least%2C%20we%20introduce%20a%0Amethod%20to%20enlarge%20an%20existing%20pre-trained%20LLM%20and%20question%20the%20relevancy%20of%0AChinchilla%20Scaling%20Law%20to%20sequence-to-sequence%20masked%20language%20models.%20Our%0Afine-tuned%20models%2C%20tokenizer%20and%20cleaned%20vngrs-web-corpus%20of%20135%20GB%20are%0Apublicly%20available%20at%20huggingface.co/vngrs-ai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01308v2&entry.124074799=Read"},
{"title": "Video Editing via Factorized Diffusion Distillation", "author": "Uriel Singer and Amit Zohar and Yuval Kirstain and Shelly Sheynin and Adam Polyak and Devi Parikh and Yaniv Taigman", "abstract": "  We introduce Emu Video Edit (EVE), a model that establishes a new\nstate-of-the art in video editing without relying on any supervised video\nediting data. To develop EVE we separately train an image editing adapter and a\nvideo generation adapter, and attach both to the same text-to-image model.\nThen, to align the adapters towards video editing we introduce a new\nunsupervised distillation procedure, Factorized Diffusion Distillation. This\nprocedure distills knowledge from one or more teachers simultaneously, without\nany supervised data. We utilize this procedure to teach EVE to edit videos by\njointly distilling knowledge to (i) precisely edit each individual frame from\nthe image editing adapter, and (ii) ensure temporal consistency among the\nedited frames using the video generation adapter. Finally, to demonstrate the\npotential of our approach in unlocking other capabilities, we align additional\ncombinations of adapters\n", "link": "http://arxiv.org/abs/2403.09334v1", "date": "2024-03-14", "relevancy": 2.2957, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6173}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5924}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5381}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Video%20Editing%20via%20Factorized%20Diffusion%20Distillation&body=Title%3A%20Video%20Editing%20via%20Factorized%20Diffusion%20Distillation%0AAuthor%3A%20Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Shelly%20Sheynin%20and%20Adam%20Polyak%20and%20Devi%20Parikh%20and%20Yaniv%20Taigman%0AAbstract%3A%20%20%20We%20introduce%20Emu%20Video%20Edit%20%28EVE%29%2C%20a%20model%20that%20establishes%20a%20new%0Astate-of-the%20art%20in%20video%20editing%20without%20relying%20on%20any%20supervised%20video%0Aediting%20data.%20To%20develop%20EVE%20we%20separately%20train%20an%20image%20editing%20adapter%20and%20a%0Avideo%20generation%20adapter%2C%20and%20attach%20both%20to%20the%20same%20text-to-image%20model.%0AThen%2C%20to%20align%20the%20adapters%20towards%20video%20editing%20we%20introduce%20a%20new%0Aunsupervised%20distillation%20procedure%2C%20Factorized%20Diffusion%20Distillation.%20This%0Aprocedure%20distills%20knowledge%20from%20one%20or%20more%20teachers%20simultaneously%2C%20without%0Aany%20supervised%20data.%20We%20utilize%20this%20procedure%20to%20teach%20EVE%20to%20edit%20videos%20by%0Ajointly%20distilling%20knowledge%20to%20%28i%29%20precisely%20edit%20each%20individual%20frame%20from%0Athe%20image%20editing%20adapter%2C%20and%20%28ii%29%20ensure%20temporal%20consistency%20among%20the%0Aedited%20frames%20using%20the%20video%20generation%20adapter.%20Finally%2C%20to%20demonstrate%20the%0Apotential%20of%20our%20approach%20in%20unlocking%20other%20capabilities%2C%20we%20align%20additional%0Acombinations%20of%20adapters%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09334v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Editing%20via%20Factorized%20Diffusion%20Distillation&entry.906535625=Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Shelly%20Sheynin%20and%20Adam%20Polyak%20and%20Devi%20Parikh%20and%20Yaniv%20Taigman&entry.1292438233=%20%20We%20introduce%20Emu%20Video%20Edit%20%28EVE%29%2C%20a%20model%20that%20establishes%20a%20new%0Astate-of-the%20art%20in%20video%20editing%20without%20relying%20on%20any%20supervised%20video%0Aediting%20data.%20To%20develop%20EVE%20we%20separately%20train%20an%20image%20editing%20adapter%20and%20a%0Avideo%20generation%20adapter%2C%20and%20attach%20both%20to%20the%20same%20text-to-image%20model.%0AThen%2C%20to%20align%20the%20adapters%20towards%20video%20editing%20we%20introduce%20a%20new%0Aunsupervised%20distillation%20procedure%2C%20Factorized%20Diffusion%20Distillation.%20This%0Aprocedure%20distills%20knowledge%20from%20one%20or%20more%20teachers%20simultaneously%2C%20without%0Aany%20supervised%20data.%20We%20utilize%20this%20procedure%20to%20teach%20EVE%20to%20edit%20videos%20by%0Ajointly%20distilling%20knowledge%20to%20%28i%29%20precisely%20edit%20each%20individual%20frame%20from%0Athe%20image%20editing%20adapter%2C%20and%20%28ii%29%20ensure%20temporal%20consistency%20among%20the%0Aedited%20frames%20using%20the%20video%20generation%20adapter.%20Finally%2C%20to%20demonstrate%20the%0Apotential%20of%20our%20approach%20in%20unlocking%20other%20capabilities%2C%20we%20align%20additional%0Acombinations%20of%20adapters%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09334v1&entry.124074799=Read"},
{"title": "NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on\n  Normalizing Flows and Generative Adversarial Networks", "author": "Young Joo Han and Ha-Jin Yu", "abstract": "  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as generative adversarial networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We simultaneously employ a\npixel-wise noise modeling network based on Normalizing Flows, and spatial\ncorrelation modeling networks based on GAN. In our experiments, our NM-FlowGAN\noutperforms other baselines on the sRGB noise synthesis task. Moreover, the\ndenoising neural network, trained with synthesized image pairs from our model,\nalso shows superior performance compared to other baselines. Our code is\navailable at: \\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n", "link": "http://arxiv.org/abs/2312.10112v2", "date": "2024-03-14", "relevancy": 2.292, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6419}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6025}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5159}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NM-FlowGAN%3A%20Modeling%20sRGB%20Noise%20with%20a%20Hybrid%20Approach%20based%20on%0A%20%20Normalizing%20Flows%20and%20Generative%20Adversarial%20Networks&body=Title%3A%20NM-FlowGAN%3A%20Modeling%20sRGB%20Noise%20with%20a%20Hybrid%20Approach%20based%20on%0A%20%20Normalizing%20Flows%20and%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Young%20Joo%20Han%20and%20Ha-Jin%20Yu%0AAbstract%3A%20%20%20Modeling%20and%20synthesizing%20real%20sRGB%20noise%20is%20crucial%20for%20various%20low-level%0Avision%20tasks%2C%20such%20as%20building%20datasets%20for%20training%20image%20denoising%20systems.%0AThe%20distribution%20of%20real%20sRGB%20noise%20is%20highly%20complex%20and%20affected%20by%20a%0Amultitude%20of%20factors%2C%20making%20its%20accurate%20modeling%20extremely%20challenging.%0ATherefore%2C%20recent%20studies%20have%20proposed%20methods%20that%20employ%20data-driven%0Agenerative%20models%2C%20such%20as%20generative%20adversarial%20networks%20%28GAN%29%20and%0ANormalizing%20Flows.%20These%20studies%20achieve%20more%20accurate%20modeling%20of%20sRGB%20noise%0Acompared%20to%20traditional%20noise%20modeling%20methods.%20However%2C%20there%20are%20performance%0Alimitations%20due%20to%20the%20inherent%20characteristics%20of%20each%20generative%20model.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20NM-FlowGAN%2C%20a%20hybrid%20approach%20that%20exploits%20the%0Astrengths%20of%20both%20GAN%20and%20Normalizing%20Flows.%20We%20simultaneously%20employ%20a%0Apixel-wise%20noise%20modeling%20network%20based%20on%20Normalizing%20Flows%2C%20and%20spatial%0Acorrelation%20modeling%20networks%20based%20on%20GAN.%20In%20our%20experiments%2C%20our%20NM-FlowGAN%0Aoutperforms%20other%20baselines%20on%20the%20sRGB%20noise%20synthesis%20task.%20Moreover%2C%20the%0Adenoising%20neural%20network%2C%20trained%20with%20synthesized%20image%20pairs%20from%20our%20model%2C%0Aalso%20shows%20superior%20performance%20compared%20to%20other%20baselines.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/YoungJooHan/NM-FlowGAN%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10112v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NM-FlowGAN%3A%20Modeling%20sRGB%20Noise%20with%20a%20Hybrid%20Approach%20based%20on%0A%20%20Normalizing%20Flows%20and%20Generative%20Adversarial%20Networks&entry.906535625=Young%20Joo%20Han%20and%20Ha-Jin%20Yu&entry.1292438233=%20%20Modeling%20and%20synthesizing%20real%20sRGB%20noise%20is%20crucial%20for%20various%20low-level%0Avision%20tasks%2C%20such%20as%20building%20datasets%20for%20training%20image%20denoising%20systems.%0AThe%20distribution%20of%20real%20sRGB%20noise%20is%20highly%20complex%20and%20affected%20by%20a%0Amultitude%20of%20factors%2C%20making%20its%20accurate%20modeling%20extremely%20challenging.%0ATherefore%2C%20recent%20studies%20have%20proposed%20methods%20that%20employ%20data-driven%0Agenerative%20models%2C%20such%20as%20generative%20adversarial%20networks%20%28GAN%29%20and%0ANormalizing%20Flows.%20These%20studies%20achieve%20more%20accurate%20modeling%20of%20sRGB%20noise%0Acompared%20to%20traditional%20noise%20modeling%20methods.%20However%2C%20there%20are%20performance%0Alimitations%20due%20to%20the%20inherent%20characteristics%20of%20each%20generative%20model.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20NM-FlowGAN%2C%20a%20hybrid%20approach%20that%20exploits%20the%0Astrengths%20of%20both%20GAN%20and%20Normalizing%20Flows.%20We%20simultaneously%20employ%20a%0Apixel-wise%20noise%20modeling%20network%20based%20on%20Normalizing%20Flows%2C%20and%20spatial%0Acorrelation%20modeling%20networks%20based%20on%20GAN.%20In%20our%20experiments%2C%20our%20NM-FlowGAN%0Aoutperforms%20other%20baselines%20on%20the%20sRGB%20noise%20synthesis%20task.%20Moreover%2C%20the%0Adenoising%20neural%20network%2C%20trained%20with%20synthesized%20image%20pairs%20from%20our%20model%2C%0Aalso%20shows%20superior%20performance%20compared%20to%20other%20baselines.%20Our%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/YoungJooHan/NM-FlowGAN%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10112v2&entry.124074799=Read"},
{"title": "3D-VLA: A 3D Vision-Language-Action Generative World Model", "author": "Haoyu Zhen and Xiaowen Qiu and Peihao Chen and Jincheng Yang and Xin Yan and Yilun Du and Yining Hong and Chuang Gan", "abstract": "  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n", "link": "http://arxiv.org/abs/2403.09631v1", "date": "2024-03-14", "relevancy": 2.2899, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5476}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D-VLA%3A%20A%203D%20Vision-Language-Action%20Generative%20World%20Model&body=Title%3A%203D-VLA%3A%20A%203D%20Vision-Language-Action%20Generative%20World%20Model%0AAuthor%3A%20Haoyu%20Zhen%20and%20Xiaowen%20Qiu%20and%20Peihao%20Chen%20and%20Jincheng%20Yang%20and%20Xin%20Yan%20and%20Yilun%20Du%20and%20Yining%20Hong%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20Recent%20vision-language-action%20%28VLA%29%20models%20rely%20on%202D%20inputs%2C%20lacking%0Aintegration%20with%20the%20broader%20realm%20of%20the%203D%20physical%20world.%20Furthermore%2C%20they%0Aperform%20action%20prediction%20by%20learning%20a%20direct%20mapping%20from%20perception%20to%0Aaction%2C%20neglecting%20the%20vast%20dynamics%20of%20the%20world%20and%20the%20relations%20between%0Aactions%20and%20dynamics.%20In%20contrast%2C%20human%20beings%20are%20endowed%20with%20world%20models%0Athat%20depict%20imagination%20about%20future%20scenarios%20to%20plan%20actions%20accordingly.%20To%0Athis%20end%2C%20we%20propose%203D-VLA%20by%20introducing%20a%20new%20family%20of%20embodied%20foundation%0Amodels%20that%20seamlessly%20link%203D%20perception%2C%20reasoning%2C%20and%20action%20through%20a%0Agenerative%20world%20model.%20Specifically%2C%203D-VLA%20is%20built%20on%20top%20of%20a%203D-based%0Alarge%20language%20model%20%28LLM%29%2C%20and%20a%20set%20of%20interaction%20tokens%20is%20introduced%20to%0Aengage%20with%20the%20embodied%20environment.%20Furthermore%2C%20to%20inject%20generation%0Aabilities%20into%20the%20model%2C%20we%20train%20a%20series%20of%20embodied%20diffusion%20models%20and%0Aalign%20them%20into%20the%20LLM%20for%20predicting%20the%20goal%20images%20and%20point%20clouds.%20To%0Atrain%20our%203D-VLA%2C%20we%20curate%20a%20large-scale%203D%20embodied%20instruction%20dataset%20by%0Aextracting%20vast%203D-related%20information%20from%20existing%20robotics%20datasets.%20Our%0Aexperiments%20on%20held-in%20datasets%20demonstrate%20that%203D-VLA%20significantly%20improves%0Athe%20reasoning%2C%20multimodal%20generation%2C%20and%20planning%20capabilities%20in%20embodied%0Aenvironments%2C%20showcasing%20its%20potential%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09631v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-VLA%3A%20A%203D%20Vision-Language-Action%20Generative%20World%20Model&entry.906535625=Haoyu%20Zhen%20and%20Xiaowen%20Qiu%20and%20Peihao%20Chen%20and%20Jincheng%20Yang%20and%20Xin%20Yan%20and%20Yilun%20Du%20and%20Yining%20Hong%20and%20Chuang%20Gan&entry.1292438233=%20%20Recent%20vision-language-action%20%28VLA%29%20models%20rely%20on%202D%20inputs%2C%20lacking%0Aintegration%20with%20the%20broader%20realm%20of%20the%203D%20physical%20world.%20Furthermore%2C%20they%0Aperform%20action%20prediction%20by%20learning%20a%20direct%20mapping%20from%20perception%20to%0Aaction%2C%20neglecting%20the%20vast%20dynamics%20of%20the%20world%20and%20the%20relations%20between%0Aactions%20and%20dynamics.%20In%20contrast%2C%20human%20beings%20are%20endowed%20with%20world%20models%0Athat%20depict%20imagination%20about%20future%20scenarios%20to%20plan%20actions%20accordingly.%20To%0Athis%20end%2C%20we%20propose%203D-VLA%20by%20introducing%20a%20new%20family%20of%20embodied%20foundation%0Amodels%20that%20seamlessly%20link%203D%20perception%2C%20reasoning%2C%20and%20action%20through%20a%0Agenerative%20world%20model.%20Specifically%2C%203D-VLA%20is%20built%20on%20top%20of%20a%203D-based%0Alarge%20language%20model%20%28LLM%29%2C%20and%20a%20set%20of%20interaction%20tokens%20is%20introduced%20to%0Aengage%20with%20the%20embodied%20environment.%20Furthermore%2C%20to%20inject%20generation%0Aabilities%20into%20the%20model%2C%20we%20train%20a%20series%20of%20embodied%20diffusion%20models%20and%0Aalign%20them%20into%20the%20LLM%20for%20predicting%20the%20goal%20images%20and%20point%20clouds.%20To%0Atrain%20our%203D-VLA%2C%20we%20curate%20a%20large-scale%203D%20embodied%20instruction%20dataset%20by%0Aextracting%20vast%203D-related%20information%20from%20existing%20robotics%20datasets.%20Our%0Aexperiments%20on%20held-in%20datasets%20demonstrate%20that%203D-VLA%20significantly%20improves%0Athe%20reasoning%2C%20multimodal%20generation%2C%20and%20planning%20capabilities%20in%20embodied%0Aenvironments%2C%20showcasing%20its%20potential%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09631v1&entry.124074799=Read"},
{"title": "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D\n  Pose Estimation In Bin-picking Scenarios", "author": "Ding-Tao Huang and En-Te Lin and Lipeng Chen and Li-Fu Liu and Long Zeng", "abstract": "  Despite the success in 6D pose estimation in bin-picking scenarios, existing\nmethods still struggle to produce accurate prediction results for symmetry\nobjects and real world scenarios. The primary bottlenecks include 1) the\nambiguity keypoints caused by object symmetries; 2) the domain gap between real\nand synthetic data. To circumvent these problem, we propose a new 6D pose\nestimation network with symmetric-aware keypoint prediction and self-training\ndomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and\ndeep hough voting to perform reliable detection keypoint under clutter and\nocclusion. Specifically, at the keypoint prediction stage, we designe a robust\n3D keypoints selection strategy considering the symmetry class of objects and\nequivalent keypoints, which facilitate locating 3D keypoints even in highly\noccluded scenes. Additionally, we build an effective filtering algorithm on\npredicted keypoint to dynamically eliminate multiple ambiguity and outlier\nkeypoint candidates. At the domain adaptation stage, we propose the\nself-training framework using a student-teacher training scheme. To carefully\ndistinguish reliable predictions, we harnesses a tailored heuristics for 3D\ngeometry pseudo labelling based on semi-chamfer distance. On public Sil'eane\ndataset, SD-Net achieves state-of-the-art results, obtaining an average\nprecision of 96%. Testing learning and generalization abilities on public\nParametric datasets, SD-Net is 8% higher than the state-of-the-art method. The\ncode is available at https://github.com/dingthuang/SD-Net.\n", "link": "http://arxiv.org/abs/2403.09317v1", "date": "2024-03-14", "relevancy": 2.2892, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6301}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SD-Net%3A%20Symmetric-Aware%20Keypoint%20Prediction%20and%20Domain%20Adaptation%20for%206D%0A%20%20Pose%20Estimation%20In%20Bin-picking%20Scenarios&body=Title%3A%20SD-Net%3A%20Symmetric-Aware%20Keypoint%20Prediction%20and%20Domain%20Adaptation%20for%206D%0A%20%20Pose%20Estimation%20In%20Bin-picking%20Scenarios%0AAuthor%3A%20Ding-Tao%20Huang%20and%20En-Te%20Lin%20and%20Lipeng%20Chen%20and%20Li-Fu%20Liu%20and%20Long%20Zeng%0AAbstract%3A%20%20%20Despite%20the%20success%20in%206D%20pose%20estimation%20in%20bin-picking%20scenarios%2C%20existing%0Amethods%20still%20struggle%20to%20produce%20accurate%20prediction%20results%20for%20symmetry%0Aobjects%20and%20real%20world%20scenarios.%20The%20primary%20bottlenecks%20include%201%29%20the%0Aambiguity%20keypoints%20caused%20by%20object%20symmetries%3B%202%29%20the%20domain%20gap%20between%20real%0Aand%20synthetic%20data.%20To%20circumvent%20these%20problem%2C%20we%20propose%20a%20new%206D%20pose%0Aestimation%20network%20with%20symmetric-aware%20keypoint%20prediction%20and%20self-training%0Adomain%20adaptation%20%28SD-Net%29.%20SD-Net%20builds%20on%20pointwise%20keypoint%20regression%20and%0Adeep%20hough%20voting%20to%20perform%20reliable%20detection%20keypoint%20under%20clutter%20and%0Aocclusion.%20Specifically%2C%20at%20the%20keypoint%20prediction%20stage%2C%20we%20designe%20a%20robust%0A3D%20keypoints%20selection%20strategy%20considering%20the%20symmetry%20class%20of%20objects%20and%0Aequivalent%20keypoints%2C%20which%20facilitate%20locating%203D%20keypoints%20even%20in%20highly%0Aoccluded%20scenes.%20Additionally%2C%20we%20build%20an%20effective%20filtering%20algorithm%20on%0Apredicted%20keypoint%20to%20dynamically%20eliminate%20multiple%20ambiguity%20and%20outlier%0Akeypoint%20candidates.%20At%20the%20domain%20adaptation%20stage%2C%20we%20propose%20the%0Aself-training%20framework%20using%20a%20student-teacher%20training%20scheme.%20To%20carefully%0Adistinguish%20reliable%20predictions%2C%20we%20harnesses%20a%20tailored%20heuristics%20for%203D%0Ageometry%20pseudo%20labelling%20based%20on%20semi-chamfer%20distance.%20On%20public%20Sil%27eane%0Adataset%2C%20SD-Net%20achieves%20state-of-the-art%20results%2C%20obtaining%20an%20average%0Aprecision%20of%2096%25.%20Testing%20learning%20and%20generalization%20abilities%20on%20public%0AParametric%20datasets%2C%20SD-Net%20is%208%25%20higher%20than%20the%20state-of-the-art%20method.%20The%0Acode%20is%20available%20at%20https%3A//github.com/dingthuang/SD-Net.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09317v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SD-Net%3A%20Symmetric-Aware%20Keypoint%20Prediction%20and%20Domain%20Adaptation%20for%206D%0A%20%20Pose%20Estimation%20In%20Bin-picking%20Scenarios&entry.906535625=Ding-Tao%20Huang%20and%20En-Te%20Lin%20and%20Lipeng%20Chen%20and%20Li-Fu%20Liu%20and%20Long%20Zeng&entry.1292438233=%20%20Despite%20the%20success%20in%206D%20pose%20estimation%20in%20bin-picking%20scenarios%2C%20existing%0Amethods%20still%20struggle%20to%20produce%20accurate%20prediction%20results%20for%20symmetry%0Aobjects%20and%20real%20world%20scenarios.%20The%20primary%20bottlenecks%20include%201%29%20the%0Aambiguity%20keypoints%20caused%20by%20object%20symmetries%3B%202%29%20the%20domain%20gap%20between%20real%0Aand%20synthetic%20data.%20To%20circumvent%20these%20problem%2C%20we%20propose%20a%20new%206D%20pose%0Aestimation%20network%20with%20symmetric-aware%20keypoint%20prediction%20and%20self-training%0Adomain%20adaptation%20%28SD-Net%29.%20SD-Net%20builds%20on%20pointwise%20keypoint%20regression%20and%0Adeep%20hough%20voting%20to%20perform%20reliable%20detection%20keypoint%20under%20clutter%20and%0Aocclusion.%20Specifically%2C%20at%20the%20keypoint%20prediction%20stage%2C%20we%20designe%20a%20robust%0A3D%20keypoints%20selection%20strategy%20considering%20the%20symmetry%20class%20of%20objects%20and%0Aequivalent%20keypoints%2C%20which%20facilitate%20locating%203D%20keypoints%20even%20in%20highly%0Aoccluded%20scenes.%20Additionally%2C%20we%20build%20an%20effective%20filtering%20algorithm%20on%0Apredicted%20keypoint%20to%20dynamically%20eliminate%20multiple%20ambiguity%20and%20outlier%0Akeypoint%20candidates.%20At%20the%20domain%20adaptation%20stage%2C%20we%20propose%20the%0Aself-training%20framework%20using%20a%20student-teacher%20training%20scheme.%20To%20carefully%0Adistinguish%20reliable%20predictions%2C%20we%20harnesses%20a%20tailored%20heuristics%20for%203D%0Ageometry%20pseudo%20labelling%20based%20on%20semi-chamfer%20distance.%20On%20public%20Sil%27eane%0Adataset%2C%20SD-Net%20achieves%20state-of-the-art%20results%2C%20obtaining%20an%20average%0Aprecision%20of%2096%25.%20Testing%20learning%20and%20generalization%20abilities%20on%20public%0AParametric%20datasets%2C%20SD-Net%20is%208%25%20higher%20than%20the%20state-of-the-art%20method.%20The%0Acode%20is%20available%20at%20https%3A//github.com/dingthuang/SD-Net.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09317v1&entry.124074799=Read"},
{"title": "Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM\n  and Dense Submaps Built without LiDAR", "author": "Sebasti\u00e1n Barbas Laina and Simon Boche and Sotiris Papatheodorou and Dimos Tzoumanikas and Simon Schaefer and Hanzhi Chen and Stefan Leutenegger", "abstract": "  Forestry constitutes a key element for a sustainable future, while it is\nsupremely challenging to introduce digital processes to improve efficiency. The\nmain limitation is the difficulty of obtaining accurate maps at high temporal\nand spatial resolution as a basis for informed forestry decision-making, due to\nthe vast area forests extend over and the sheer number of trees. To address\nthis challenge, we present an autonomous Micro Aerial Vehicle (MAV) system\nwhich purely relies on cost-effective and light-weight passive visual and\ninertial sensors to perform under-canopy autonomous navigation. We leverage\nvisual-inertial simultaneous localization and mapping (VI-SLAM) for accurate\nMAV state estimates and couple it with a volumetric occupancy submapping system\nto achieve a scalable mapping framework which can be directly used for path\nplanning. As opposed to a monolithic map, submaps inherently deal with\ninevitable drift and corrections from VI-SLAM, since they move with pose\nestimates as they are updated. To ensure the safety of the MAV during\nnavigation, we also propose a novel reference trajectory anchoring scheme that\nmoves and deforms the reference trajectory the MAV is tracking upon state\nupdates from the VI-SLAM system in a consistent way, even upon large changes in\nstate estimates due to loop-closures. We thoroughly validate our system in both\nreal and simulated forest environments with high tree densities in excess of\n400 trees per hectare and at speeds up to 3 m/s - while not encountering a\nsingle collision or system failure. To the best of our knowledge this is the\nfirst system which achieves this level of performance in such unstructured\nenvironment using low-cost passive visual sensors and fully on-board\ncomputation including VI-SLAM.\n", "link": "http://arxiv.org/abs/2403.09596v1", "date": "2024-03-14", "relevancy": 2.288, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5872}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5215}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scalable%20Autonomous%20Drone%20Flight%20in%20the%20Forest%20with%20Visual-Inertial%20SLAM%0A%20%20and%20Dense%20Submaps%20Built%20without%20LiDAR&body=Title%3A%20Scalable%20Autonomous%20Drone%20Flight%20in%20the%20Forest%20with%20Visual-Inertial%20SLAM%0A%20%20and%20Dense%20Submaps%20Built%20without%20LiDAR%0AAuthor%3A%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Dimos%20Tzoumanikas%20and%20Simon%20Schaefer%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20Forestry%20constitutes%20a%20key%20element%20for%20a%20sustainable%20future%2C%20while%20it%20is%0Asupremely%20challenging%20to%20introduce%20digital%20processes%20to%20improve%20efficiency.%20The%0Amain%20limitation%20is%20the%20difficulty%20of%20obtaining%20accurate%20maps%20at%20high%20temporal%0Aand%20spatial%20resolution%20as%20a%20basis%20for%20informed%20forestry%20decision-making%2C%20due%20to%0Athe%20vast%20area%20forests%20extend%20over%20and%20the%20sheer%20number%20of%20trees.%20To%20address%0Athis%20challenge%2C%20we%20present%20an%20autonomous%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%0Awhich%20purely%20relies%20on%20cost-effective%20and%20light-weight%20passive%20visual%20and%0Ainertial%20sensors%20to%20perform%20under-canopy%20autonomous%20navigation.%20We%20leverage%0Avisual-inertial%20simultaneous%20localization%20and%20mapping%20%28VI-SLAM%29%20for%20accurate%0AMAV%20state%20estimates%20and%20couple%20it%20with%20a%20volumetric%20occupancy%20submapping%20system%0Ato%20achieve%20a%20scalable%20mapping%20framework%20which%20can%20be%20directly%20used%20for%20path%0Aplanning.%20As%20opposed%20to%20a%20monolithic%20map%2C%20submaps%20inherently%20deal%20with%0Ainevitable%20drift%20and%20corrections%20from%20VI-SLAM%2C%20since%20they%20move%20with%20pose%0Aestimates%20as%20they%20are%20updated.%20To%20ensure%20the%20safety%20of%20the%20MAV%20during%0Anavigation%2C%20we%20also%20propose%20a%20novel%20reference%20trajectory%20anchoring%20scheme%20that%0Amoves%20and%20deforms%20the%20reference%20trajectory%20the%20MAV%20is%20tracking%20upon%20state%0Aupdates%20from%20the%20VI-SLAM%20system%20in%20a%20consistent%20way%2C%20even%20upon%20large%20changes%20in%0Astate%20estimates%20due%20to%20loop-closures.%20We%20thoroughly%20validate%20our%20system%20in%20both%0Areal%20and%20simulated%20forest%20environments%20with%20high%20tree%20densities%20in%20excess%20of%0A400%20trees%20per%20hectare%20and%20at%20speeds%20up%20to%203%20m/s%20-%20while%20not%20encountering%20a%0Asingle%20collision%20or%20system%20failure.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20system%20which%20achieves%20this%20level%20of%20performance%20in%20such%20unstructured%0Aenvironment%20using%20low-cost%20passive%20visual%20sensors%20and%20fully%20on-board%0Acomputation%20including%20VI-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09596v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Autonomous%20Drone%20Flight%20in%20the%20Forest%20with%20Visual-Inertial%20SLAM%0A%20%20and%20Dense%20Submaps%20Built%20without%20LiDAR&entry.906535625=Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Dimos%20Tzoumanikas%20and%20Simon%20Schaefer%20and%20Hanzhi%20Chen%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20Forestry%20constitutes%20a%20key%20element%20for%20a%20sustainable%20future%2C%20while%20it%20is%0Asupremely%20challenging%20to%20introduce%20digital%20processes%20to%20improve%20efficiency.%20The%0Amain%20limitation%20is%20the%20difficulty%20of%20obtaining%20accurate%20maps%20at%20high%20temporal%0Aand%20spatial%20resolution%20as%20a%20basis%20for%20informed%20forestry%20decision-making%2C%20due%20to%0Athe%20vast%20area%20forests%20extend%20over%20and%20the%20sheer%20number%20of%20trees.%20To%20address%0Athis%20challenge%2C%20we%20present%20an%20autonomous%20Micro%20Aerial%20Vehicle%20%28MAV%29%20system%0Awhich%20purely%20relies%20on%20cost-effective%20and%20light-weight%20passive%20visual%20and%0Ainertial%20sensors%20to%20perform%20under-canopy%20autonomous%20navigation.%20We%20leverage%0Avisual-inertial%20simultaneous%20localization%20and%20mapping%20%28VI-SLAM%29%20for%20accurate%0AMAV%20state%20estimates%20and%20couple%20it%20with%20a%20volumetric%20occupancy%20submapping%20system%0Ato%20achieve%20a%20scalable%20mapping%20framework%20which%20can%20be%20directly%20used%20for%20path%0Aplanning.%20As%20opposed%20to%20a%20monolithic%20map%2C%20submaps%20inherently%20deal%20with%0Ainevitable%20drift%20and%20corrections%20from%20VI-SLAM%2C%20since%20they%20move%20with%20pose%0Aestimates%20as%20they%20are%20updated.%20To%20ensure%20the%20safety%20of%20the%20MAV%20during%0Anavigation%2C%20we%20also%20propose%20a%20novel%20reference%20trajectory%20anchoring%20scheme%20that%0Amoves%20and%20deforms%20the%20reference%20trajectory%20the%20MAV%20is%20tracking%20upon%20state%0Aupdates%20from%20the%20VI-SLAM%20system%20in%20a%20consistent%20way%2C%20even%20upon%20large%20changes%20in%0Astate%20estimates%20due%20to%20loop-closures.%20We%20thoroughly%20validate%20our%20system%20in%20both%0Areal%20and%20simulated%20forest%20environments%20with%20high%20tree%20densities%20in%20excess%20of%0A400%20trees%20per%20hectare%20and%20at%20speeds%20up%20to%203%20m/s%20-%20while%20not%20encountering%20a%0Asingle%20collision%20or%20system%20failure.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20system%20which%20achieves%20this%20level%20of%20performance%20in%20such%20unstructured%0Aenvironment%20using%20low-cost%20passive%20visual%20sensors%20and%20fully%20on-board%0Acomputation%20including%20VI-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09596v1&entry.124074799=Read"},
{"title": "DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning", "author": "Xu Yang and Jiyuan Feng and Songyue Guo and Ye Wang and Ye Ding and Binxing Fang and Qing Liao", "abstract": "  Personalized federated learning becomes a hot research topic that can learn a\npersonalized learning model for each client. Existing personalized federated\nlearning models prefer to aggregate similar clients with similar data\ndistribution to improve the performance of learning models. However,\nsimilaritybased personalized federated learning methods may exacerbate the\nclass imbalanced problem. In this paper, we propose a novel Dynamic\nAffinity-based Personalized Federated Learning model (DA-PFL) to alleviate the\nclass imbalanced problem during federated learning. Specifically, we build an\naffinity metric from a complementary perspective to guide which clients should\nbe aggregated. Then we design a dynamic aggregation strategy to dynamically\naggregate clients based on the affinity metric in each round to reduce the\nclass imbalanced risk. Extensive experiments show that the proposed DA-PFL\nmodel can significantly improve the accuracy of each client in three real-world\ndatasets with state-of-the-art comparison methods.\n", "link": "http://arxiv.org/abs/2403.09284v1", "date": "2024-03-14", "relevancy": 2.2868, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4705}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4528}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4488}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DA-PFL%3A%20Dynamic%20Affinity%20Aggregation%20for%20Personalized%20Federated%20Learning&body=Title%3A%20DA-PFL%3A%20Dynamic%20Affinity%20Aggregation%20for%20Personalized%20Federated%20Learning%0AAuthor%3A%20Xu%20Yang%20and%20Jiyuan%20Feng%20and%20Songyue%20Guo%20and%20Ye%20Wang%20and%20Ye%20Ding%20and%20Binxing%20Fang%20and%20Qing%20Liao%0AAbstract%3A%20%20%20Personalized%20federated%20learning%20becomes%20a%20hot%20research%20topic%20that%20can%20learn%20a%0Apersonalized%20learning%20model%20for%20each%20client.%20Existing%20personalized%20federated%0Alearning%20models%20prefer%20to%20aggregate%20similar%20clients%20with%20similar%20data%0Adistribution%20to%20improve%20the%20performance%20of%20learning%20models.%20However%2C%0Asimilaritybased%20personalized%20federated%20learning%20methods%20may%20exacerbate%20the%0Aclass%20imbalanced%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Dynamic%0AAffinity-based%20Personalized%20Federated%20Learning%20model%20%28DA-PFL%29%20to%20alleviate%20the%0Aclass%20imbalanced%20problem%20during%20federated%20learning.%20Specifically%2C%20we%20build%20an%0Aaffinity%20metric%20from%20a%20complementary%20perspective%20to%20guide%20which%20clients%20should%0Abe%20aggregated.%20Then%20we%20design%20a%20dynamic%20aggregation%20strategy%20to%20dynamically%0Aaggregate%20clients%20based%20on%20the%20affinity%20metric%20in%20each%20round%20to%20reduce%20the%0Aclass%20imbalanced%20risk.%20Extensive%20experiments%20show%20that%20the%20proposed%20DA-PFL%0Amodel%20can%20significantly%20improve%20the%20accuracy%20of%20each%20client%20in%20three%20real-world%0Adatasets%20with%20state-of-the-art%20comparison%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09284v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-PFL%3A%20Dynamic%20Affinity%20Aggregation%20for%20Personalized%20Federated%20Learning&entry.906535625=Xu%20Yang%20and%20Jiyuan%20Feng%20and%20Songyue%20Guo%20and%20Ye%20Wang%20and%20Ye%20Ding%20and%20Binxing%20Fang%20and%20Qing%20Liao&entry.1292438233=%20%20Personalized%20federated%20learning%20becomes%20a%20hot%20research%20topic%20that%20can%20learn%20a%0Apersonalized%20learning%20model%20for%20each%20client.%20Existing%20personalized%20federated%0Alearning%20models%20prefer%20to%20aggregate%20similar%20clients%20with%20similar%20data%0Adistribution%20to%20improve%20the%20performance%20of%20learning%20models.%20However%2C%0Asimilaritybased%20personalized%20federated%20learning%20methods%20may%20exacerbate%20the%0Aclass%20imbalanced%20problem.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Dynamic%0AAffinity-based%20Personalized%20Federated%20Learning%20model%20%28DA-PFL%29%20to%20alleviate%20the%0Aclass%20imbalanced%20problem%20during%20federated%20learning.%20Specifically%2C%20we%20build%20an%0Aaffinity%20metric%20from%20a%20complementary%20perspective%20to%20guide%20which%20clients%20should%0Abe%20aggregated.%20Then%20we%20design%20a%20dynamic%20aggregation%20strategy%20to%20dynamically%0Aaggregate%20clients%20based%20on%20the%20affinity%20metric%20in%20each%20round%20to%20reduce%20the%0Aclass%20imbalanced%20risk.%20Extensive%20experiments%20show%20that%20the%20proposed%20DA-PFL%0Amodel%20can%20significantly%20improve%20the%20accuracy%20of%20each%20client%20in%20three%20real-world%0Adatasets%20with%20state-of-the-art%20comparison%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09284v1&entry.124074799=Read"},
{"title": "Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis", "author": "Fabio Maresca and Filippo Grazioli and Antonio Albanese and Vincenzo Sciancalepore and Gianpiero Negri and Xavier Costa-Perez", "abstract": "  The tremendous hype around autonomous driving is eagerly calling for emerging\nand novel technologies to support advanced mobility use cases. As car\nmanufactures keep developing SAE level 3+ systems to improve the safety and\ncomfort of passengers, traffic authorities need to establish new procedures to\nmanage the transition from human-driven to fully-autonomous vehicles while\nproviding a feedback-loop mechanism to fine-tune envisioned autonomous systems.\nThus, a way to automatically profile autonomous vehicles and differentiate\nthose from human-driven ones is a must. In this paper, we present a\nfully-fledged framework that monitors active vehicles using camera images and\nstate information in order to determine whether vehicles are autonomous,\nwithout requiring any active notification from the vehicles themselves.\nEssentially, it builds on the cooperation among vehicles, which share their\ndata acquired on the road feeding a machine learning model to identify\nautonomous cars. We extensively tested our solution and created the NexusStreet\ndataset, by means of the CARLA simulator, employing an autonomous driving\ncontrol agent and a steering wheel maneuvered by licensed drivers. Experiments\nshow it is possible to discriminate the two behaviors by analyzing video clips\nwith an accuracy of 80%, which improves up to 93% when the target state\ninformation is available. Lastly, we deliberately degraded the state to observe\nhow the framework performs under non-ideal data collection conditions.\n", "link": "http://arxiv.org/abs/2403.09571v1", "date": "2024-03-14", "relevancy": 2.2852, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5976}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5388}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20you%20a%20robot%3F%20Detecting%20Autonomous%20Vehicles%20from%20Behavior%20Analysis&body=Title%3A%20Are%20you%20a%20robot%3F%20Detecting%20Autonomous%20Vehicles%20from%20Behavior%20Analysis%0AAuthor%3A%20Fabio%20Maresca%20and%20Filippo%20Grazioli%20and%20Antonio%20Albanese%20and%20Vincenzo%20Sciancalepore%20and%20Gianpiero%20Negri%20and%20Xavier%20Costa-Perez%0AAbstract%3A%20%20%20The%20tremendous%20hype%20around%20autonomous%20driving%20is%20eagerly%20calling%20for%20emerging%0Aand%20novel%20technologies%20to%20support%20advanced%20mobility%20use%20cases.%20As%20car%0Amanufactures%20keep%20developing%20SAE%20level%203%2B%20systems%20to%20improve%20the%20safety%20and%0Acomfort%20of%20passengers%2C%20traffic%20authorities%20need%20to%20establish%20new%20procedures%20to%0Amanage%20the%20transition%20from%20human-driven%20to%20fully-autonomous%20vehicles%20while%0Aproviding%20a%20feedback-loop%20mechanism%20to%20fine-tune%20envisioned%20autonomous%20systems.%0AThus%2C%20a%20way%20to%20automatically%20profile%20autonomous%20vehicles%20and%20differentiate%0Athose%20from%20human-driven%20ones%20is%20a%20must.%20In%20this%20paper%2C%20we%20present%20a%0Afully-fledged%20framework%20that%20monitors%20active%20vehicles%20using%20camera%20images%20and%0Astate%20information%20in%20order%20to%20determine%20whether%20vehicles%20are%20autonomous%2C%0Awithout%20requiring%20any%20active%20notification%20from%20the%20vehicles%20themselves.%0AEssentially%2C%20it%20builds%20on%20the%20cooperation%20among%20vehicles%2C%20which%20share%20their%0Adata%20acquired%20on%20the%20road%20feeding%20a%20machine%20learning%20model%20to%20identify%0Aautonomous%20cars.%20We%20extensively%20tested%20our%20solution%20and%20created%20the%20NexusStreet%0Adataset%2C%20by%20means%20of%20the%20CARLA%20simulator%2C%20employing%20an%20autonomous%20driving%0Acontrol%20agent%20and%20a%20steering%20wheel%20maneuvered%20by%20licensed%20drivers.%20Experiments%0Ashow%20it%20is%20possible%20to%20discriminate%20the%20two%20behaviors%20by%20analyzing%20video%20clips%0Awith%20an%20accuracy%20of%2080%25%2C%20which%20improves%20up%20to%2093%25%20when%20the%20target%20state%0Ainformation%20is%20available.%20Lastly%2C%20we%20deliberately%20degraded%20the%20state%20to%20observe%0Ahow%20the%20framework%20performs%20under%20non-ideal%20data%20collection%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09571v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20you%20a%20robot%3F%20Detecting%20Autonomous%20Vehicles%20from%20Behavior%20Analysis&entry.906535625=Fabio%20Maresca%20and%20Filippo%20Grazioli%20and%20Antonio%20Albanese%20and%20Vincenzo%20Sciancalepore%20and%20Gianpiero%20Negri%20and%20Xavier%20Costa-Perez&entry.1292438233=%20%20The%20tremendous%20hype%20around%20autonomous%20driving%20is%20eagerly%20calling%20for%20emerging%0Aand%20novel%20technologies%20to%20support%20advanced%20mobility%20use%20cases.%20As%20car%0Amanufactures%20keep%20developing%20SAE%20level%203%2B%20systems%20to%20improve%20the%20safety%20and%0Acomfort%20of%20passengers%2C%20traffic%20authorities%20need%20to%20establish%20new%20procedures%20to%0Amanage%20the%20transition%20from%20human-driven%20to%20fully-autonomous%20vehicles%20while%0Aproviding%20a%20feedback-loop%20mechanism%20to%20fine-tune%20envisioned%20autonomous%20systems.%0AThus%2C%20a%20way%20to%20automatically%20profile%20autonomous%20vehicles%20and%20differentiate%0Athose%20from%20human-driven%20ones%20is%20a%20must.%20In%20this%20paper%2C%20we%20present%20a%0Afully-fledged%20framework%20that%20monitors%20active%20vehicles%20using%20camera%20images%20and%0Astate%20information%20in%20order%20to%20determine%20whether%20vehicles%20are%20autonomous%2C%0Awithout%20requiring%20any%20active%20notification%20from%20the%20vehicles%20themselves.%0AEssentially%2C%20it%20builds%20on%20the%20cooperation%20among%20vehicles%2C%20which%20share%20their%0Adata%20acquired%20on%20the%20road%20feeding%20a%20machine%20learning%20model%20to%20identify%0Aautonomous%20cars.%20We%20extensively%20tested%20our%20solution%20and%20created%20the%20NexusStreet%0Adataset%2C%20by%20means%20of%20the%20CARLA%20simulator%2C%20employing%20an%20autonomous%20driving%0Acontrol%20agent%20and%20a%20steering%20wheel%20maneuvered%20by%20licensed%20drivers.%20Experiments%0Ashow%20it%20is%20possible%20to%20discriminate%20the%20two%20behaviors%20by%20analyzing%20video%20clips%0Awith%20an%20accuracy%20of%2080%25%2C%20which%20improves%20up%20to%2093%25%20when%20the%20target%20state%0Ainformation%20is%20available.%20Lastly%2C%20we%20deliberately%20degraded%20the%20state%20to%20observe%0Ahow%20the%20framework%20performs%20under%20non-ideal%20data%20collection%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09571v1&entry.124074799=Read"},
{"title": "Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition", "author": "Yitian Zhang and Yue Bai and Huan Wang and Yizhou Wang and Yun Fu", "abstract": "  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n", "link": "http://arxiv.org/abs/2403.09506v1", "date": "2024-03-14", "relevancy": 2.2796, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6305}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5728}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5428}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Judge%20by%20the%20Look%3A%20A%20Motion%20Coherent%20Augmentation%20for%20Video%0A%20%20Recognition&body=Title%3A%20Don%27t%20Judge%20by%20the%20Look%3A%20A%20Motion%20Coherent%20Augmentation%20for%20Video%0A%20%20Recognition%0AAuthor%3A%20Yitian%20Zhang%20and%20Yue%20Bai%20and%20Huan%20Wang%20and%20Yizhou%20Wang%20and%20Yun%20Fu%0AAbstract%3A%20%20%20Current%20training%20pipelines%20in%20object%20recognition%20neglect%20Hue%20Jittering%20when%0Adoing%20data%20augmentation%20as%20it%20not%20only%20brings%20appearance%20changes%20that%20are%0Adetrimental%20to%20classification%2C%20but%20also%20the%20implementation%20is%20inefficient%20in%0Apractice.%20In%20this%20study%2C%20we%20investigate%20the%20effect%20of%20hue%20variance%20in%20the%0Acontext%20of%20video%20recognition%20and%20find%20this%20variance%20to%20be%20beneficial%20since%0Astatic%20appearances%20are%20less%20important%20in%20videos%20that%20contain%20motion%0Ainformation.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20data%20augmentation%20method%0Afor%20video%20recognition%2C%20named%20Motion%20Coherent%20Augmentation%20%28MCA%29%2C%20that%0Aintroduces%20appearance%20variation%20in%20videos%20and%20implicitly%20encourages%20the%20model%0Ato%20prioritize%20motion%20patterns%2C%20rather%20than%20static%20appearances.%20Concretely%2C%20we%0Apropose%20an%20operation%20SwapMix%20to%20efficiently%20modify%20the%20appearance%20of%20video%0Asamples%2C%20and%20introduce%20Variation%20Alignment%20%28VA%29%20to%20resolve%20the%20distribution%0Ashift%20caused%20by%20SwapMix%2C%20enforcing%20the%20model%20to%20learn%20appearance%20invariant%0Arepresentations.%20Comprehensive%20empirical%20evaluation%20across%20various%0Aarchitectures%20and%20different%20datasets%20solidly%20validates%20the%20effectiveness%20and%0Ageneralization%20ability%20of%20MCA%2C%20and%20the%20application%20of%20VA%20in%20other%20augmentation%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/BeSpontaneous/MCA-pytorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09506v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Judge%20by%20the%20Look%3A%20A%20Motion%20Coherent%20Augmentation%20for%20Video%0A%20%20Recognition&entry.906535625=Yitian%20Zhang%20and%20Yue%20Bai%20and%20Huan%20Wang%20and%20Yizhou%20Wang%20and%20Yun%20Fu&entry.1292438233=%20%20Current%20training%20pipelines%20in%20object%20recognition%20neglect%20Hue%20Jittering%20when%0Adoing%20data%20augmentation%20as%20it%20not%20only%20brings%20appearance%20changes%20that%20are%0Adetrimental%20to%20classification%2C%20but%20also%20the%20implementation%20is%20inefficient%20in%0Apractice.%20In%20this%20study%2C%20we%20investigate%20the%20effect%20of%20hue%20variance%20in%20the%0Acontext%20of%20video%20recognition%20and%20find%20this%20variance%20to%20be%20beneficial%20since%0Astatic%20appearances%20are%20less%20important%20in%20videos%20that%20contain%20motion%0Ainformation.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20data%20augmentation%20method%0Afor%20video%20recognition%2C%20named%20Motion%20Coherent%20Augmentation%20%28MCA%29%2C%20that%0Aintroduces%20appearance%20variation%20in%20videos%20and%20implicitly%20encourages%20the%20model%0Ato%20prioritize%20motion%20patterns%2C%20rather%20than%20static%20appearances.%20Concretely%2C%20we%0Apropose%20an%20operation%20SwapMix%20to%20efficiently%20modify%20the%20appearance%20of%20video%0Asamples%2C%20and%20introduce%20Variation%20Alignment%20%28VA%29%20to%20resolve%20the%20distribution%0Ashift%20caused%20by%20SwapMix%2C%20enforcing%20the%20model%20to%20learn%20appearance%20invariant%0Arepresentations.%20Comprehensive%20empirical%20evaluation%20across%20various%0Aarchitectures%20and%20different%20datasets%20solidly%20validates%20the%20effectiveness%20and%0Ageneralization%20ability%20of%20MCA%2C%20and%20the%20application%20of%20VA%20in%20other%20augmentation%0Amethods.%20Code%20is%20available%20at%20https%3A//github.com/BeSpontaneous/MCA-pytorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09506v1&entry.124074799=Read"},
{"title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation", "author": "Frank Zhang and Yibo Zhang and Quan Zheng and Rui Ma and Wei Hua and Hujun Bao and Weiwei Xu and Changqing Zou", "abstract": "  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n", "link": "http://arxiv.org/abs/2403.09439v1", "date": "2024-03-14", "relevancy": 2.2771, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5901}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.572}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5582}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203D-SceneDreamer%3A%20Text-Driven%203D-Consistent%20Scene%20Generation&body=Title%3A%203D-SceneDreamer%3A%20Text-Driven%203D-Consistent%20Scene%20Generation%0AAuthor%3A%20Frank%20Zhang%20and%20Yibo%20Zhang%20and%20Quan%20Zheng%20and%20Rui%20Ma%20and%20Wei%20Hua%20and%20Hujun%20Bao%20and%20Weiwei%20Xu%20and%20Changqing%20Zou%0AAbstract%3A%20%20%20Text-driven%203D%20scene%20generation%20techniques%20have%20made%20rapid%20progress%20in%20recent%0Ayears.%20Their%20success%20is%20mainly%20attributed%20to%20using%20existing%20generative%20models%0Ato%20iteratively%20perform%20image%20warping%20and%20inpainting%20to%20generate%203D%20scenes.%0AHowever%2C%20these%20methods%20heavily%20rely%20on%20the%20outputs%20of%20existing%20models%2C%20leading%0Ato%20error%20accumulation%20in%20geometry%20and%20appearance%20that%20prevent%20the%20models%20from%0Abeing%20used%20in%20various%20scenarios%20%28e.g.%2C%20outdoor%20and%20unreal%20scenarios%29.%20To%0Aaddress%20this%20limitation%2C%20we%20generatively%20refine%20the%20newly%20generated%20local%20views%0Aby%20querying%20and%20aggregating%20global%203D%20information%2C%20and%20then%20progressively%0Agenerate%20the%203D%20scene.%20Specifically%2C%20we%20employ%20a%20tri-plane%20features-based%20NeRF%0Aas%20a%20unified%20representation%20of%20the%203D%20scene%20to%20constrain%20global%203D%20consistency%2C%0Aand%20propose%20a%20generative%20refinement%20network%20to%20synthesize%20new%20contents%20with%0Ahigher%20quality%20by%20exploiting%20the%20natural%20image%20prior%20from%202D%20diffusion%20model%20as%0Awell%20as%20the%20global%203D%20information%20of%20the%20current%20scene.%20Our%20extensive%0Aexperiments%20demonstrate%20that%2C%20in%20comparison%20to%20previous%20methods%2C%20our%20approach%0Asupports%20wide%20variety%20of%20scene%20generation%20and%20arbitrary%20camera%20trajectories%0Awith%20improved%20visual%20quality%20and%203D%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09439v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-SceneDreamer%3A%20Text-Driven%203D-Consistent%20Scene%20Generation&entry.906535625=Frank%20Zhang%20and%20Yibo%20Zhang%20and%20Quan%20Zheng%20and%20Rui%20Ma%20and%20Wei%20Hua%20and%20Hujun%20Bao%20and%20Weiwei%20Xu%20and%20Changqing%20Zou&entry.1292438233=%20%20Text-driven%203D%20scene%20generation%20techniques%20have%20made%20rapid%20progress%20in%20recent%0Ayears.%20Their%20success%20is%20mainly%20attributed%20to%20using%20existing%20generative%20models%0Ato%20iteratively%20perform%20image%20warping%20and%20inpainting%20to%20generate%203D%20scenes.%0AHowever%2C%20these%20methods%20heavily%20rely%20on%20the%20outputs%20of%20existing%20models%2C%20leading%0Ato%20error%20accumulation%20in%20geometry%20and%20appearance%20that%20prevent%20the%20models%20from%0Abeing%20used%20in%20various%20scenarios%20%28e.g.%2C%20outdoor%20and%20unreal%20scenarios%29.%20To%0Aaddress%20this%20limitation%2C%20we%20generatively%20refine%20the%20newly%20generated%20local%20views%0Aby%20querying%20and%20aggregating%20global%203D%20information%2C%20and%20then%20progressively%0Agenerate%20the%203D%20scene.%20Specifically%2C%20we%20employ%20a%20tri-plane%20features-based%20NeRF%0Aas%20a%20unified%20representation%20of%20the%203D%20scene%20to%20constrain%20global%203D%20consistency%2C%0Aand%20propose%20a%20generative%20refinement%20network%20to%20synthesize%20new%20contents%20with%0Ahigher%20quality%20by%20exploiting%20the%20natural%20image%20prior%20from%202D%20diffusion%20model%20as%0Awell%20as%20the%20global%203D%20information%20of%20the%20current%20scene.%20Our%20extensive%0Aexperiments%20demonstrate%20that%2C%20in%20comparison%20to%20previous%20methods%2C%20our%20approach%0Asupports%20wide%20variety%20of%20scene%20generation%20and%20arbitrary%20camera%20trajectories%0Awith%20improved%20visual%20quality%20and%203D%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09439v1&entry.124074799=Read"},
{"title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments", "author": "Yinan Deng and Jiahui Wang and Jingyu Zhao and Xinyu Tian and Guangyan Chen and Yi Yang and Yufeng Yue", "abstract": "  Environment maps endowed with sophisticated semantics are pivotal for\nfacilitating seamless interaction between robots and humans, enabling them to\neffectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including\nmultimodal retrieval and open-set classes. However, existing open-vocabulary\nmaps are constrained to closed indoor scenarios and VLM features, thereby\ndiminishing their usability and inference capabilities. Moreover, the absence\nof topological relationships further complicates the accurate querying of\nspecific instances. In this work, we propose OpenGraph, a representation of\nopen-vocabulary hierarchical graph structure designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images using 2D foundation models, encoding the captions with features\nto enhance textual reasoning. Subsequently, 3D incremental panoramic mapping\nwith feature embedding is achieved by projecting images onto LiDAR point\nclouds. Finally, the environment is segmented based on lane graph connectivity\nto construct a hierarchical graph. Validation results from real public dataset\nSemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph\nexhibits the ability to generalize to novel semantic classes and achieve the\nhighest segmentation and query accuracy. The source code of OpenGraph is\npublicly available at https://github.com/BIT-DYN/OpenGraph.\n", "link": "http://arxiv.org/abs/2403.09412v1", "date": "2024-03-14", "relevancy": 2.2744, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5914}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments&body=Title%3A%20OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments%0AAuthor%3A%20Yinan%20Deng%20and%20Jiahui%20Wang%20and%20Jingyu%20Zhao%20and%20Xinyu%20Tian%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Yufeng%20Yue%0AAbstract%3A%20%20%20Environment%20maps%20endowed%20with%20sophisticated%20semantics%20are%20pivotal%20for%0Afacilitating%20seamless%20interaction%20between%20robots%20and%20humans%2C%20enabling%20them%20to%0Aeffectively%20carry%20out%20various%20tasks.%20Open-vocabulary%20maps%2C%20powered%20by%0AVisual-Language%20models%20%28VLMs%29%2C%20possess%20inherent%20advantages%2C%20including%0Amultimodal%20retrieval%20and%20open-set%20classes.%20However%2C%20existing%20open-vocabulary%0Amaps%20are%20constrained%20to%20closed%20indoor%20scenarios%20and%20VLM%20features%2C%20thereby%0Adiminishing%20their%20usability%20and%20inference%20capabilities.%20Moreover%2C%20the%20absence%0Aof%20topological%20relationships%20further%20complicates%20the%20accurate%20querying%20of%0Aspecific%20instances.%20In%20this%20work%2C%20we%20propose%20OpenGraph%2C%20a%20representation%20of%0Aopen-vocabulary%20hierarchical%20graph%20structure%20designed%20for%20large-scale%20outdoor%0Aenvironments.%20OpenGraph%20initially%20extracts%20instances%20and%20their%20captions%20from%0Avisual%20images%20using%202D%20foundation%20models%2C%20encoding%20the%20captions%20with%20features%0Ato%20enhance%20textual%20reasoning.%20Subsequently%2C%203D%20incremental%20panoramic%20mapping%0Awith%20feature%20embedding%20is%20achieved%20by%20projecting%20images%20onto%20LiDAR%20point%0Aclouds.%20Finally%2C%20the%20environment%20is%20segmented%20based%20on%20lane%20graph%20connectivity%0Ato%20construct%20a%20hierarchical%20graph.%20Validation%20results%20from%20real%20public%20dataset%0ASemanticKITTI%20demonstrate%20that%2C%20even%20without%20fine-tuning%20the%20models%2C%20OpenGraph%0Aexhibits%20the%20ability%20to%20generalize%20to%20novel%20semantic%20classes%20and%20achieve%20the%0Ahighest%20segmentation%20and%20query%20accuracy.%20The%20source%20code%20of%20OpenGraph%20is%0Apublicly%20available%20at%20https%3A//github.com/BIT-DYN/OpenGraph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments&entry.906535625=Yinan%20Deng%20and%20Jiahui%20Wang%20and%20Jingyu%20Zhao%20and%20Xinyu%20Tian%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Yufeng%20Yue&entry.1292438233=%20%20Environment%20maps%20endowed%20with%20sophisticated%20semantics%20are%20pivotal%20for%0Afacilitating%20seamless%20interaction%20between%20robots%20and%20humans%2C%20enabling%20them%20to%0Aeffectively%20carry%20out%20various%20tasks.%20Open-vocabulary%20maps%2C%20powered%20by%0AVisual-Language%20models%20%28VLMs%29%2C%20possess%20inherent%20advantages%2C%20including%0Amultimodal%20retrieval%20and%20open-set%20classes.%20However%2C%20existing%20open-vocabulary%0Amaps%20are%20constrained%20to%20closed%20indoor%20scenarios%20and%20VLM%20features%2C%20thereby%0Adiminishing%20their%20usability%20and%20inference%20capabilities.%20Moreover%2C%20the%20absence%0Aof%20topological%20relationships%20further%20complicates%20the%20accurate%20querying%20of%0Aspecific%20instances.%20In%20this%20work%2C%20we%20propose%20OpenGraph%2C%20a%20representation%20of%0Aopen-vocabulary%20hierarchical%20graph%20structure%20designed%20for%20large-scale%20outdoor%0Aenvironments.%20OpenGraph%20initially%20extracts%20instances%20and%20their%20captions%20from%0Avisual%20images%20using%202D%20foundation%20models%2C%20encoding%20the%20captions%20with%20features%0Ato%20enhance%20textual%20reasoning.%20Subsequently%2C%203D%20incremental%20panoramic%20mapping%0Awith%20feature%20embedding%20is%20achieved%20by%20projecting%20images%20onto%20LiDAR%20point%0Aclouds.%20Finally%2C%20the%20environment%20is%20segmented%20based%20on%20lane%20graph%20connectivity%0Ato%20construct%20a%20hierarchical%20graph.%20Validation%20results%20from%20real%20public%20dataset%0ASemanticKITTI%20demonstrate%20that%2C%20even%20without%20fine-tuning%20the%20models%2C%20OpenGraph%0Aexhibits%20the%20ability%20to%20generalize%20to%20novel%20semantic%20classes%20and%20achieve%20the%0Ahighest%20segmentation%20and%20query%20accuracy.%20The%20source%20code%20of%20OpenGraph%20is%0Apublicly%20available%20at%20https%3A//github.com/BIT-DYN/OpenGraph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09412v1&entry.124074799=Read"},
{"title": "GAIA: Zero-shot Talking Avatar Generation", "author": "Tianyu He and Junliang Guo and Runyi Yu and Yuchi Wang and Jialiang Zhu and Kaikai An and Leyi Li and Xu Tan and Chunyu Wang and Han Hu and HsiangTao Wu and Sheng Zhao and Jiang Bian", "abstract": "  Zero-shot talking avatar generation aims at synthesizing natural talking\nvideos from speech and a single portrait image. Previous methods have relied on\ndomain-specific heuristics such as warping-based motion representation and 3D\nMorphable Models, which limit the naturalness and diversity of the generated\navatars. In this work, we introduce GAIA (Generative AI for Avatar), which\neliminates the domain priors in talking avatar generation. In light of the\nobservation that the speech only drives the motion of the avatar while the\nappearance of the avatar and the background typically remain the same\nthroughout the entire video, we divide our approach into two stages: 1)\ndisentangling each frame into motion and appearance representations; 2)\ngenerating motion sequences conditioned on the speech and reference portrait\nimage. We collect a large-scale high-quality talking avatar dataset and train\nthe model on it with different scales (up to 2B parameters). Experimental\nresults verify the superiority, scalability, and flexibility of GAIA as 1) the\nresulting model beats previous baseline models in terms of naturalness,\ndiversity, lip-sync quality, and visual quality; 2) the framework is scalable\nsince larger models yield better results; 3) it is general and enables\ndifferent applications like controllable talking avatar generation and\ntext-instructed avatar generation.\n", "link": "http://arxiv.org/abs/2311.15230v2", "date": "2024-03-14", "relevancy": 2.2724, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5965}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5546}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.531}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GAIA%3A%20Zero-shot%20Talking%20Avatar%20Generation&body=Title%3A%20GAIA%3A%20Zero-shot%20Talking%20Avatar%20Generation%0AAuthor%3A%20Tianyu%20He%20and%20Junliang%20Guo%20and%20Runyi%20Yu%20and%20Yuchi%20Wang%20and%20Jialiang%20Zhu%20and%20Kaikai%20An%20and%20Leyi%20Li%20and%20Xu%20Tan%20and%20Chunyu%20Wang%20and%20Han%20Hu%20and%20HsiangTao%20Wu%20and%20Sheng%20Zhao%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Zero-shot%20talking%20avatar%20generation%20aims%20at%20synthesizing%20natural%20talking%0Avideos%20from%20speech%20and%20a%20single%20portrait%20image.%20Previous%20methods%20have%20relied%20on%0Adomain-specific%20heuristics%20such%20as%20warping-based%20motion%20representation%20and%203D%0AMorphable%20Models%2C%20which%20limit%20the%20naturalness%20and%20diversity%20of%20the%20generated%0Aavatars.%20In%20this%20work%2C%20we%20introduce%20GAIA%20%28Generative%20AI%20for%20Avatar%29%2C%20which%0Aeliminates%20the%20domain%20priors%20in%20talking%20avatar%20generation.%20In%20light%20of%20the%0Aobservation%20that%20the%20speech%20only%20drives%20the%20motion%20of%20the%20avatar%20while%20the%0Aappearance%20of%20the%20avatar%20and%20the%20background%20typically%20remain%20the%20same%0Athroughout%20the%20entire%20video%2C%20we%20divide%20our%20approach%20into%20two%20stages%3A%201%29%0Adisentangling%20each%20frame%20into%20motion%20and%20appearance%20representations%3B%202%29%0Agenerating%20motion%20sequences%20conditioned%20on%20the%20speech%20and%20reference%20portrait%0Aimage.%20We%20collect%20a%20large-scale%20high-quality%20talking%20avatar%20dataset%20and%20train%0Athe%20model%20on%20it%20with%20different%20scales%20%28up%20to%202B%20parameters%29.%20Experimental%0Aresults%20verify%20the%20superiority%2C%20scalability%2C%20and%20flexibility%20of%20GAIA%20as%201%29%20the%0Aresulting%20model%20beats%20previous%20baseline%20models%20in%20terms%20of%20naturalness%2C%0Adiversity%2C%20lip-sync%20quality%2C%20and%20visual%20quality%3B%202%29%20the%20framework%20is%20scalable%0Asince%20larger%20models%20yield%20better%20results%3B%203%29%20it%20is%20general%20and%20enables%0Adifferent%20applications%20like%20controllable%20talking%20avatar%20generation%20and%0Atext-instructed%20avatar%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15230v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAIA%3A%20Zero-shot%20Talking%20Avatar%20Generation&entry.906535625=Tianyu%20He%20and%20Junliang%20Guo%20and%20Runyi%20Yu%20and%20Yuchi%20Wang%20and%20Jialiang%20Zhu%20and%20Kaikai%20An%20and%20Leyi%20Li%20and%20Xu%20Tan%20and%20Chunyu%20Wang%20and%20Han%20Hu%20and%20HsiangTao%20Wu%20and%20Sheng%20Zhao%20and%20Jiang%20Bian&entry.1292438233=%20%20Zero-shot%20talking%20avatar%20generation%20aims%20at%20synthesizing%20natural%20talking%0Avideos%20from%20speech%20and%20a%20single%20portrait%20image.%20Previous%20methods%20have%20relied%20on%0Adomain-specific%20heuristics%20such%20as%20warping-based%20motion%20representation%20and%203D%0AMorphable%20Models%2C%20which%20limit%20the%20naturalness%20and%20diversity%20of%20the%20generated%0Aavatars.%20In%20this%20work%2C%20we%20introduce%20GAIA%20%28Generative%20AI%20for%20Avatar%29%2C%20which%0Aeliminates%20the%20domain%20priors%20in%20talking%20avatar%20generation.%20In%20light%20of%20the%0Aobservation%20that%20the%20speech%20only%20drives%20the%20motion%20of%20the%20avatar%20while%20the%0Aappearance%20of%20the%20avatar%20and%20the%20background%20typically%20remain%20the%20same%0Athroughout%20the%20entire%20video%2C%20we%20divide%20our%20approach%20into%20two%20stages%3A%201%29%0Adisentangling%20each%20frame%20into%20motion%20and%20appearance%20representations%3B%202%29%0Agenerating%20motion%20sequences%20conditioned%20on%20the%20speech%20and%20reference%20portrait%0Aimage.%20We%20collect%20a%20large-scale%20high-quality%20talking%20avatar%20dataset%20and%20train%0Athe%20model%20on%20it%20with%20different%20scales%20%28up%20to%202B%20parameters%29.%20Experimental%0Aresults%20verify%20the%20superiority%2C%20scalability%2C%20and%20flexibility%20of%20GAIA%20as%201%29%20the%0Aresulting%20model%20beats%20previous%20baseline%20models%20in%20terms%20of%20naturalness%2C%0Adiversity%2C%20lip-sync%20quality%2C%20and%20visual%20quality%3B%202%29%20the%20framework%20is%20scalable%0Asince%20larger%20models%20yield%20better%20results%3B%203%29%20it%20is%20general%20and%20enables%0Adifferent%20applications%20like%20controllable%20talking%20avatar%20generation%20and%0Atext-instructed%20avatar%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15230v2&entry.124074799=Read"},
{"title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", "author": "Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Hongyu H\u00e8 and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang", "abstract": "  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n", "link": "http://arxiv.org/abs/2403.09611v1", "date": "2024-03-14", "relevancy": 2.2615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&body=Title%3A%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training%0AAuthor%3A%20Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20consisting%20of%0Aboth%20dense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&entry.906535625=Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang&entry.1292438233=%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20consisting%20of%0Aboth%20dense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09611v1&entry.124074799=Read"},
{"title": "VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance\n  Fields", "author": "Nicolaj Schmid and Cornelius von Einem and Cesar Cadena and Roland Siegwart and Lorenz Hruby and Florian Tschopp", "abstract": "  Autonomous mobile robots are an increasingly integral part of modern factory\nand warehouse operations. Obstacle detection, avoidance and path planning are\ncritical safety-relevant tasks, which are often solved using expensive LiDAR\nsensors and depth cameras. We propose to use cost-effective low-resolution\nranging sensors, such as ultrasonic and infrared time-of-flight sensors by\ndeveloping VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance\nFields. Building upon Instant Neural Graphics Primitives with a Multiresolution\nHash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from\nultrasonic and infrared sensors and utilizes them to update the occupancy grid\nused for ray marching. Experimental evaluation in 2D demonstrates that\nVIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds\nregarding coverage. Notably, in small environments, its accuracy aligns with\nthat of LiDAR measurements, while in larger ones, it is bounded by the utilized\nultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic\nand infrared sensors is highly effective when dealing with sparse data and low\nview variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the\nmapping capabilities and increases the training speed by 46% compared to\nInstant-NGP. Overall, VIRUS-NeRF presents a promising approach for\ncost-effective local mapping in mobile robotics, with potential applications in\nsafety and navigation tasks. The code can be found at\nhttps://github.com/ethz-asl/virus nerf.\n", "link": "http://arxiv.org/abs/2403.09477v1", "date": "2024-03-14", "relevancy": 2.2511, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5183}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields&body=Title%3A%20VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields%0AAuthor%3A%20Nicolaj%20Schmid%20and%20Cornelius%20von%20Einem%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Lorenz%20Hruby%20and%20Florian%20Tschopp%0AAbstract%3A%20%20%20Autonomous%20mobile%20robots%20are%20an%20increasingly%20integral%20part%20of%20modern%20factory%0Aand%20warehouse%20operations.%20Obstacle%20detection%2C%20avoidance%20and%20path%20planning%20are%0Acritical%20safety-relevant%20tasks%2C%20which%20are%20often%20solved%20using%20expensive%20LiDAR%0Asensors%20and%20depth%20cameras.%20We%20propose%20to%20use%20cost-effective%20low-resolution%0Aranging%20sensors%2C%20such%20as%20ultrasonic%20and%20infrared%20time-of-flight%20sensors%20by%0Adeveloping%20VIRUS-NeRF%20-%20Vision%2C%20InfraRed%2C%20and%20UltraSonic%20based%20Neural%20Radiance%0AFields.%20Building%20upon%20Instant%20Neural%20Graphics%20Primitives%20with%20a%20Multiresolution%0AHash%20Encoding%20%28Instant-NGP%29%2C%20VIRUS-NeRF%20incorporates%20depth%20measurements%20from%0Aultrasonic%20and%20infrared%20sensors%20and%20utilizes%20them%20to%20update%20the%20occupancy%20grid%0Aused%20for%20ray%20marching.%20Experimental%20evaluation%20in%202D%20demonstrates%20that%0AVIRUS-NeRF%20achieves%20comparable%20mapping%20performance%20to%20LiDAR%20point%20clouds%0Aregarding%20coverage.%20Notably%2C%20in%20small%20environments%2C%20its%20accuracy%20aligns%20with%0Athat%20of%20LiDAR%20measurements%2C%20while%20in%20larger%20ones%2C%20it%20is%20bounded%20by%20the%20utilized%0Aultrasonic%20sensors.%20An%20in-depth%20ablation%20study%20reveals%20that%20adding%20ultrasonic%0Aand%20infrared%20sensors%20is%20highly%20effective%20when%20dealing%20with%20sparse%20data%20and%20low%0Aview%20variation.%20Further%2C%20the%20proposed%20occupancy%20grid%20of%20VIRUS-NeRF%20improves%20the%0Amapping%20capabilities%20and%20increases%20the%20training%20speed%20by%2046%25%20compared%20to%0AInstant-NGP.%20Overall%2C%20VIRUS-NeRF%20presents%20a%20promising%20approach%20for%0Acost-effective%20local%20mapping%20in%20mobile%20robotics%2C%20with%20potential%20applications%20in%0Asafety%20and%20navigation%20tasks.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ethz-asl/virus%20nerf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09477v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields&entry.906535625=Nicolaj%20Schmid%20and%20Cornelius%20von%20Einem%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Lorenz%20Hruby%20and%20Florian%20Tschopp&entry.1292438233=%20%20Autonomous%20mobile%20robots%20are%20an%20increasingly%20integral%20part%20of%20modern%20factory%0Aand%20warehouse%20operations.%20Obstacle%20detection%2C%20avoidance%20and%20path%20planning%20are%0Acritical%20safety-relevant%20tasks%2C%20which%20are%20often%20solved%20using%20expensive%20LiDAR%0Asensors%20and%20depth%20cameras.%20We%20propose%20to%20use%20cost-effective%20low-resolution%0Aranging%20sensors%2C%20such%20as%20ultrasonic%20and%20infrared%20time-of-flight%20sensors%20by%0Adeveloping%20VIRUS-NeRF%20-%20Vision%2C%20InfraRed%2C%20and%20UltraSonic%20based%20Neural%20Radiance%0AFields.%20Building%20upon%20Instant%20Neural%20Graphics%20Primitives%20with%20a%20Multiresolution%0AHash%20Encoding%20%28Instant-NGP%29%2C%20VIRUS-NeRF%20incorporates%20depth%20measurements%20from%0Aultrasonic%20and%20infrared%20sensors%20and%20utilizes%20them%20to%20update%20the%20occupancy%20grid%0Aused%20for%20ray%20marching.%20Experimental%20evaluation%20in%202D%20demonstrates%20that%0AVIRUS-NeRF%20achieves%20comparable%20mapping%20performance%20to%20LiDAR%20point%20clouds%0Aregarding%20coverage.%20Notably%2C%20in%20small%20environments%2C%20its%20accuracy%20aligns%20with%0Athat%20of%20LiDAR%20measurements%2C%20while%20in%20larger%20ones%2C%20it%20is%20bounded%20by%20the%20utilized%0Aultrasonic%20sensors.%20An%20in-depth%20ablation%20study%20reveals%20that%20adding%20ultrasonic%0Aand%20infrared%20sensors%20is%20highly%20effective%20when%20dealing%20with%20sparse%20data%20and%20low%0Aview%20variation.%20Further%2C%20the%20proposed%20occupancy%20grid%20of%20VIRUS-NeRF%20improves%20the%0Amapping%20capabilities%20and%20increases%20the%20training%20speed%20by%2046%25%20compared%20to%0AInstant-NGP.%20Overall%2C%20VIRUS-NeRF%20presents%20a%20promising%20approach%20for%0Acost-effective%20local%20mapping%20in%20mobile%20robotics%2C%20with%20potential%20applications%20in%0Asafety%20and%20navigation%20tasks.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ethz-asl/virus%20nerf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09477v1&entry.124074799=Read"},
{"title": "Explore In-Context Segmentation via Latent Diffusion Models", "author": "Chaoyang Wang and Xiangtai Li and Henghui Ding and Lu Qi and Jiangning Zhang and Yunhai Tong and Chen Change Loy and Shuicheng Yan", "abstract": "  In-context segmentation has drawn more attention with the introduction of\nvision foundation models. Most existing approaches adopt metric learning or\nmasked image modeling to build the correlation between visual prompts and input\nimage queries. In this work, we explore this problem from a new perspective,\nusing one representative generation model, the latent diffusion model (LDM). We\nobserve a task gap between generation and segmentation in diffusion models, but\nLDM is still an effective minimalist for in-context segmentation. In\nparticular, we propose two meta-architectures and correspondingly design\nseveral output alignment and optimization strategies. We have conducted\ncomprehensive ablation studies and empirically found that the segmentation\nquality counts on output alignment and in-context instructions. Moreover, we\nbuild a new and fair in-context segmentation benchmark that includes both image\nand video datasets. Experiments validate the efficiency of our approach,\ndemonstrating comparable or even stronger results than previous specialist\nmodels or visual foundation models. Our study shows that LDMs can also achieve\ngood enough results for challenging in-context segmentation tasks.\n", "link": "http://arxiv.org/abs/2403.09616v1", "date": "2024-03-14", "relevancy": 2.2503, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5805}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5668}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explore%20In-Context%20Segmentation%20via%20Latent%20Diffusion%20Models&body=Title%3A%20Explore%20In-Context%20Segmentation%20via%20Latent%20Diffusion%20Models%0AAuthor%3A%20Chaoyang%20Wang%20and%20Xiangtai%20Li%20and%20Henghui%20Ding%20and%20Lu%20Qi%20and%20Jiangning%20Zhang%20and%20Yunhai%20Tong%20and%20Chen%20Change%20Loy%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20In-context%20segmentation%20has%20drawn%20more%20attention%20with%20the%20introduction%20of%0Avision%20foundation%20models.%20Most%20existing%20approaches%20adopt%20metric%20learning%20or%0Amasked%20image%20modeling%20to%20build%20the%20correlation%20between%20visual%20prompts%20and%20input%0Aimage%20queries.%20In%20this%20work%2C%20we%20explore%20this%20problem%20from%20a%20new%20perspective%2C%0Ausing%20one%20representative%20generation%20model%2C%20the%20latent%20diffusion%20model%20%28LDM%29.%20We%0Aobserve%20a%20task%20gap%20between%20generation%20and%20segmentation%20in%20diffusion%20models%2C%20but%0ALDM%20is%20still%20an%20effective%20minimalist%20for%20in-context%20segmentation.%20In%0Aparticular%2C%20we%20propose%20two%20meta-architectures%20and%20correspondingly%20design%0Aseveral%20output%20alignment%20and%20optimization%20strategies.%20We%20have%20conducted%0Acomprehensive%20ablation%20studies%20and%20empirically%20found%20that%20the%20segmentation%0Aquality%20counts%20on%20output%20alignment%20and%20in-context%20instructions.%20Moreover%2C%20we%0Abuild%20a%20new%20and%20fair%20in-context%20segmentation%20benchmark%20that%20includes%20both%20image%0Aand%20video%20datasets.%20Experiments%20validate%20the%20efficiency%20of%20our%20approach%2C%0Ademonstrating%20comparable%20or%20even%20stronger%20results%20than%20previous%20specialist%0Amodels%20or%20visual%20foundation%20models.%20Our%20study%20shows%20that%20LDMs%20can%20also%20achieve%0Agood%20enough%20results%20for%20challenging%20in-context%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09616v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore%20In-Context%20Segmentation%20via%20Latent%20Diffusion%20Models&entry.906535625=Chaoyang%20Wang%20and%20Xiangtai%20Li%20and%20Henghui%20Ding%20and%20Lu%20Qi%20and%20Jiangning%20Zhang%20and%20Yunhai%20Tong%20and%20Chen%20Change%20Loy%20and%20Shuicheng%20Yan&entry.1292438233=%20%20In-context%20segmentation%20has%20drawn%20more%20attention%20with%20the%20introduction%20of%0Avision%20foundation%20models.%20Most%20existing%20approaches%20adopt%20metric%20learning%20or%0Amasked%20image%20modeling%20to%20build%20the%20correlation%20between%20visual%20prompts%20and%20input%0Aimage%20queries.%20In%20this%20work%2C%20we%20explore%20this%20problem%20from%20a%20new%20perspective%2C%0Ausing%20one%20representative%20generation%20model%2C%20the%20latent%20diffusion%20model%20%28LDM%29.%20We%0Aobserve%20a%20task%20gap%20between%20generation%20and%20segmentation%20in%20diffusion%20models%2C%20but%0ALDM%20is%20still%20an%20effective%20minimalist%20for%20in-context%20segmentation.%20In%0Aparticular%2C%20we%20propose%20two%20meta-architectures%20and%20correspondingly%20design%0Aseveral%20output%20alignment%20and%20optimization%20strategies.%20We%20have%20conducted%0Acomprehensive%20ablation%20studies%20and%20empirically%20found%20that%20the%20segmentation%0Aquality%20counts%20on%20output%20alignment%20and%20in-context%20instructions.%20Moreover%2C%20we%0Abuild%20a%20new%20and%20fair%20in-context%20segmentation%20benchmark%20that%20includes%20both%20image%0Aand%20video%20datasets.%20Experiments%20validate%20the%20efficiency%20of%20our%20approach%2C%0Ademonstrating%20comparable%20or%20even%20stronger%20results%20than%20previous%20specialist%0Amodels%20or%20visual%20foundation%20models.%20Our%20study%20shows%20that%20LDMs%20can%20also%20achieve%0Agood%20enough%20results%20for%20challenging%20in-context%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09616v1&entry.124074799=Read"},
{"title": "Plug and Play Active Learning for Object Detection", "author": "Chenhongyi Yang and Lichao Huang and Elliot J. Crowley", "abstract": "  Annotating datasets for object detection is an expensive and time-consuming\nendeavor. To minimize this burden, active learning (AL) techniques are employed\nto select the most informative samples for annotation within a constrained\n\"annotation budget\". Traditional AL strategies typically rely on model\nuncertainty or sample diversity for query sampling, while more advanced methods\nhave focused on developing AL-specific object detector architectures to enhance\nperformance. However, these specialized approaches are not readily adaptable to\ndifferent object detectors due to the significant engineering effort required\nfor integration. To overcome this challenge, we introduce Plug and Play Active\nLearning (PPAL), a simple and effective AL strategy for object detection. PPAL\nis a two-stage method comprising uncertainty-based and diversity-based sampling\nphases. In the first stage, our Difficulty Calibrated Uncertainty Sampling\nleverage a category-wise difficulty coefficient that combines both\nclassification and localisation difficulties to re-weight instance\nuncertainties, from which we sample a candidate pool for the subsequent\ndiversity-based sampling. In the second stage, we propose Category Conditioned\nMatching Similarity to better compute the similarities of multi-instance images\nas ensembles of their instance similarities, which is used by the k-Means++\nalgorithm to sample the final AL queries. PPAL makes no change to model\narchitectures or detector training pipelines; hence it can be easily\ngeneralized to different object detectors. We benchmark PPAL on the MS-COCO and\nPascal VOC datasets using different detector architectures and show that our\nmethod outperforms prior work by a large margin. Code is available at\nhttps://github.com/ChenhongyiYang/PPAL\n", "link": "http://arxiv.org/abs/2211.11612v2", "date": "2024-03-14", "relevancy": 2.2501, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5888}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5694}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Plug%20and%20Play%20Active%20Learning%20for%20Object%20Detection&body=Title%3A%20Plug%20and%20Play%20Active%20Learning%20for%20Object%20Detection%0AAuthor%3A%20Chenhongyi%20Yang%20and%20Lichao%20Huang%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20Annotating%20datasets%20for%20object%20detection%20is%20an%20expensive%20and%20time-consuming%0Aendeavor.%20To%20minimize%20this%20burden%2C%20active%20learning%20%28AL%29%20techniques%20are%20employed%0Ato%20select%20the%20most%20informative%20samples%20for%20annotation%20within%20a%20constrained%0A%22annotation%20budget%22.%20Traditional%20AL%20strategies%20typically%20rely%20on%20model%0Auncertainty%20or%20sample%20diversity%20for%20query%20sampling%2C%20while%20more%20advanced%20methods%0Ahave%20focused%20on%20developing%20AL-specific%20object%20detector%20architectures%20to%20enhance%0Aperformance.%20However%2C%20these%20specialized%20approaches%20are%20not%20readily%20adaptable%20to%0Adifferent%20object%20detectors%20due%20to%20the%20significant%20engineering%20effort%20required%0Afor%20integration.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Plug%20and%20Play%20Active%0ALearning%20%28PPAL%29%2C%20a%20simple%20and%20effective%20AL%20strategy%20for%20object%20detection.%20PPAL%0Ais%20a%20two-stage%20method%20comprising%20uncertainty-based%20and%20diversity-based%20sampling%0Aphases.%20In%20the%20first%20stage%2C%20our%20Difficulty%20Calibrated%20Uncertainty%20Sampling%0Aleverage%20a%20category-wise%20difficulty%20coefficient%20that%20combines%20both%0Aclassification%20and%20localisation%20difficulties%20to%20re-weight%20instance%0Auncertainties%2C%20from%20which%20we%20sample%20a%20candidate%20pool%20for%20the%20subsequent%0Adiversity-based%20sampling.%20In%20the%20second%20stage%2C%20we%20propose%20Category%20Conditioned%0AMatching%20Similarity%20to%20better%20compute%20the%20similarities%20of%20multi-instance%20images%0Aas%20ensembles%20of%20their%20instance%20similarities%2C%20which%20is%20used%20by%20the%20k-Means%2B%2B%0Aalgorithm%20to%20sample%20the%20final%20AL%20queries.%20PPAL%20makes%20no%20change%20to%20model%0Aarchitectures%20or%20detector%20training%20pipelines%3B%20hence%20it%20can%20be%20easily%0Ageneralized%20to%20different%20object%20detectors.%20We%20benchmark%20PPAL%20on%20the%20MS-COCO%20and%0APascal%20VOC%20datasets%20using%20different%20detector%20architectures%20and%20show%20that%20our%0Amethod%20outperforms%20prior%20work%20by%20a%20large%20margin.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ChenhongyiYang/PPAL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.11612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plug%20and%20Play%20Active%20Learning%20for%20Object%20Detection&entry.906535625=Chenhongyi%20Yang%20and%20Lichao%20Huang%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20Annotating%20datasets%20for%20object%20detection%20is%20an%20expensive%20and%20time-consuming%0Aendeavor.%20To%20minimize%20this%20burden%2C%20active%20learning%20%28AL%29%20techniques%20are%20employed%0Ato%20select%20the%20most%20informative%20samples%20for%20annotation%20within%20a%20constrained%0A%22annotation%20budget%22.%20Traditional%20AL%20strategies%20typically%20rely%20on%20model%0Auncertainty%20or%20sample%20diversity%20for%20query%20sampling%2C%20while%20more%20advanced%20methods%0Ahave%20focused%20on%20developing%20AL-specific%20object%20detector%20architectures%20to%20enhance%0Aperformance.%20However%2C%20these%20specialized%20approaches%20are%20not%20readily%20adaptable%20to%0Adifferent%20object%20detectors%20due%20to%20the%20significant%20engineering%20effort%20required%0Afor%20integration.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Plug%20and%20Play%20Active%0ALearning%20%28PPAL%29%2C%20a%20simple%20and%20effective%20AL%20strategy%20for%20object%20detection.%20PPAL%0Ais%20a%20two-stage%20method%20comprising%20uncertainty-based%20and%20diversity-based%20sampling%0Aphases.%20In%20the%20first%20stage%2C%20our%20Difficulty%20Calibrated%20Uncertainty%20Sampling%0Aleverage%20a%20category-wise%20difficulty%20coefficient%20that%20combines%20both%0Aclassification%20and%20localisation%20difficulties%20to%20re-weight%20instance%0Auncertainties%2C%20from%20which%20we%20sample%20a%20candidate%20pool%20for%20the%20subsequent%0Adiversity-based%20sampling.%20In%20the%20second%20stage%2C%20we%20propose%20Category%20Conditioned%0AMatching%20Similarity%20to%20better%20compute%20the%20similarities%20of%20multi-instance%20images%0Aas%20ensembles%20of%20their%20instance%20similarities%2C%20which%20is%20used%20by%20the%20k-Means%2B%2B%0Aalgorithm%20to%20sample%20the%20final%20AL%20queries.%20PPAL%20makes%20no%20change%20to%20model%0Aarchitectures%20or%20detector%20training%20pipelines%3B%20hence%20it%20can%20be%20easily%0Ageneralized%20to%20different%20object%20detectors.%20We%20benchmark%20PPAL%20on%20the%20MS-COCO%20and%0APascal%20VOC%20datasets%20using%20different%20detector%20architectures%20and%20show%20that%20our%0Amethod%20outperforms%20prior%20work%20by%20a%20large%20margin.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ChenhongyiYang/PPAL%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.11612v2&entry.124074799=Read"},
{"title": "HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting", "author": "Xian Liu and Xiaohang Zhan and Jiaxiang Tang and Ying Shan and Gang Zeng and Dahua Lin and Xihui Liu and Ziwei Liu", "abstract": "  Realistic 3D human generation from text prompts is a desirable yet\nchallenging task. Existing methods optimize 3D representations like mesh or\nneural fields via score distillation sampling (SDS), which suffers from\ninadequate fine details or excessive training time. In this paper, we propose\nan efficient yet effective framework, HumanGaussian, that generates\nhigh-quality 3D humans with fine-grained geometry and realistic appearance. Our\nkey insight is that 3D Gaussian Splatting is an efficient renderer with\nperiodic Gaussian shrinkage or growing, where such adaptive density control can\nbe naturally guided by intrinsic human structures. Specifically, 1) we first\npropose a Structure-Aware SDS that simultaneously optimizes human appearance\nand geometry. The multi-modal score function from both RGB and depth space is\nleveraged to distill the Gaussian densification and pruning process. 2)\nMoreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS\ninto a noisier generative score and a cleaner classifier score, which well\naddresses the over-saturation issue. The floating artifacts are further\neliminated based on Gaussian size in a prune-only phase to enhance generation\nsmoothness. Extensive experiments demonstrate the superior efficiency and\ncompetitive quality of our framework, rendering vivid 3D humans under diverse\nscenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian\n", "link": "http://arxiv.org/abs/2311.17061v2", "date": "2024-03-14", "relevancy": 2.2334, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5915}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5523}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HumanGaussian%3A%20Text-Driven%203D%20Human%20Generation%20with%20Gaussian%20Splatting&body=Title%3A%20HumanGaussian%3A%20Text-Driven%203D%20Human%20Generation%20with%20Gaussian%20Splatting%0AAuthor%3A%20Xian%20Liu%20and%20Xiaohang%20Zhan%20and%20Jiaxiang%20Tang%20and%20Ying%20Shan%20and%20Gang%20Zeng%20and%20Dahua%20Lin%20and%20Xihui%20Liu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Realistic%203D%20human%20generation%20from%20text%20prompts%20is%20a%20desirable%20yet%0Achallenging%20task.%20Existing%20methods%20optimize%203D%20representations%20like%20mesh%20or%0Aneural%20fields%20via%20score%20distillation%20sampling%20%28SDS%29%2C%20which%20suffers%20from%0Ainadequate%20fine%20details%20or%20excessive%20training%20time.%20In%20this%20paper%2C%20we%20propose%0Aan%20efficient%20yet%20effective%20framework%2C%20HumanGaussian%2C%20that%20generates%0Ahigh-quality%203D%20humans%20with%20fine-grained%20geometry%20and%20realistic%20appearance.%20Our%0Akey%20insight%20is%20that%203D%20Gaussian%20Splatting%20is%20an%20efficient%20renderer%20with%0Aperiodic%20Gaussian%20shrinkage%20or%20growing%2C%20where%20such%20adaptive%20density%20control%20can%0Abe%20naturally%20guided%20by%20intrinsic%20human%20structures.%20Specifically%2C%201%29%20we%20first%0Apropose%20a%20Structure-Aware%20SDS%20that%20simultaneously%20optimizes%20human%20appearance%0Aand%20geometry.%20The%20multi-modal%20score%20function%20from%20both%20RGB%20and%20depth%20space%20is%0Aleveraged%20to%20distill%20the%20Gaussian%20densification%20and%20pruning%20process.%202%29%0AMoreover%2C%20we%20devise%20an%20Annealed%20Negative%20Prompt%20Guidance%20by%20decomposing%20SDS%0Ainto%20a%20noisier%20generative%20score%20and%20a%20cleaner%20classifier%20score%2C%20which%20well%0Aaddresses%20the%20over-saturation%20issue.%20The%20floating%20artifacts%20are%20further%0Aeliminated%20based%20on%20Gaussian%20size%20in%20a%20prune-only%20phase%20to%20enhance%20generation%0Asmoothness.%20Extensive%20experiments%20demonstrate%20the%20superior%20efficiency%20and%0Acompetitive%20quality%20of%20our%20framework%2C%20rendering%20vivid%203D%20humans%20under%20diverse%0Ascenarios.%20Project%20Page%3A%20https%3A//alvinliu0.github.io/projects/HumanGaussian%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17061v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanGaussian%3A%20Text-Driven%203D%20Human%20Generation%20with%20Gaussian%20Splatting&entry.906535625=Xian%20Liu%20and%20Xiaohang%20Zhan%20and%20Jiaxiang%20Tang%20and%20Ying%20Shan%20and%20Gang%20Zeng%20and%20Dahua%20Lin%20and%20Xihui%20Liu%20and%20Ziwei%20Liu&entry.1292438233=%20%20Realistic%203D%20human%20generation%20from%20text%20prompts%20is%20a%20desirable%20yet%0Achallenging%20task.%20Existing%20methods%20optimize%203D%20representations%20like%20mesh%20or%0Aneural%20fields%20via%20score%20distillation%20sampling%20%28SDS%29%2C%20which%20suffers%20from%0Ainadequate%20fine%20details%20or%20excessive%20training%20time.%20In%20this%20paper%2C%20we%20propose%0Aan%20efficient%20yet%20effective%20framework%2C%20HumanGaussian%2C%20that%20generates%0Ahigh-quality%203D%20humans%20with%20fine-grained%20geometry%20and%20realistic%20appearance.%20Our%0Akey%20insight%20is%20that%203D%20Gaussian%20Splatting%20is%20an%20efficient%20renderer%20with%0Aperiodic%20Gaussian%20shrinkage%20or%20growing%2C%20where%20such%20adaptive%20density%20control%20can%0Abe%20naturally%20guided%20by%20intrinsic%20human%20structures.%20Specifically%2C%201%29%20we%20first%0Apropose%20a%20Structure-Aware%20SDS%20that%20simultaneously%20optimizes%20human%20appearance%0Aand%20geometry.%20The%20multi-modal%20score%20function%20from%20both%20RGB%20and%20depth%20space%20is%0Aleveraged%20to%20distill%20the%20Gaussian%20densification%20and%20pruning%20process.%202%29%0AMoreover%2C%20we%20devise%20an%20Annealed%20Negative%20Prompt%20Guidance%20by%20decomposing%20SDS%0Ainto%20a%20noisier%20generative%20score%20and%20a%20cleaner%20classifier%20score%2C%20which%20well%0Aaddresses%20the%20over-saturation%20issue.%20The%20floating%20artifacts%20are%20further%0Aeliminated%20based%20on%20Gaussian%20size%20in%20a%20prune-only%20phase%20to%20enhance%20generation%0Asmoothness.%20Extensive%20experiments%20demonstrate%20the%20superior%20efficiency%20and%0Acompetitive%20quality%20of%20our%20framework%2C%20rendering%20vivid%203D%20humans%20under%20diverse%0Ascenarios.%20Project%20Page%3A%20https%3A//alvinliu0.github.io/projects/HumanGaussian%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17061v2&entry.124074799=Read"},
{"title": "SCP-Diff: Photo-Realistic Semantic Image Synthesis with\n  Spatial-Categorical Joint Prior", "author": "Huan-ang Gao and Mingju Gao and Jiaju Li and Wenyi Li and Rong Zhi and Hao Tang and Hao Zhao", "abstract": "  Semantic image synthesis (SIS) shows good promises for sensor simulation.\nHowever, current best practices in this field, based on GANs, have not yet\nreached the desired level of quality. As latent diffusion models make\nsignificant strides in image generation, we are prompted to evaluate\nControlNet, a notable method for its dense control capabilities. Our\ninvestigation uncovered two primary issues with its results: the presence of\nweird sub-structures within large semantic areas and the misalignment of\ncontent with the semantic mask. Through empirical study, we pinpointed the\ncause of these problems as a mismatch between the noised training data\ndistribution and the standard normal prior applied at the inference stage. To\naddress this challenge, we developed specific noise priors for SIS,\nencompassing spatial, categorical, and a novel spatial-categorical joint prior\nfor inference. This approach, which we have named SCP-Diff, has yielded\nexceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on\nADE20K.The code and models can be accessed via the project page.\n", "link": "http://arxiv.org/abs/2403.09638v1", "date": "2024-03-14", "relevancy": 2.231, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5524}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SCP-Diff%3A%20Photo-Realistic%20Semantic%20Image%20Synthesis%20with%0A%20%20Spatial-Categorical%20Joint%20Prior&body=Title%3A%20SCP-Diff%3A%20Photo-Realistic%20Semantic%20Image%20Synthesis%20with%0A%20%20Spatial-Categorical%20Joint%20Prior%0AAuthor%3A%20Huan-ang%20Gao%20and%20Mingju%20Gao%20and%20Jiaju%20Li%20and%20Wenyi%20Li%20and%20Rong%20Zhi%20and%20Hao%20Tang%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Semantic%20image%20synthesis%20%28SIS%29%20shows%20good%20promises%20for%20sensor%20simulation.%0AHowever%2C%20current%20best%20practices%20in%20this%20field%2C%20based%20on%20GANs%2C%20have%20not%20yet%0Areached%20the%20desired%20level%20of%20quality.%20As%20latent%20diffusion%20models%20make%0Asignificant%20strides%20in%20image%20generation%2C%20we%20are%20prompted%20to%20evaluate%0AControlNet%2C%20a%20notable%20method%20for%20its%20dense%20control%20capabilities.%20Our%0Ainvestigation%20uncovered%20two%20primary%20issues%20with%20its%20results%3A%20the%20presence%20of%0Aweird%20sub-structures%20within%20large%20semantic%20areas%20and%20the%20misalignment%20of%0Acontent%20with%20the%20semantic%20mask.%20Through%20empirical%20study%2C%20we%20pinpointed%20the%0Acause%20of%20these%20problems%20as%20a%20mismatch%20between%20the%20noised%20training%20data%0Adistribution%20and%20the%20standard%20normal%20prior%20applied%20at%20the%20inference%20stage.%20To%0Aaddress%20this%20challenge%2C%20we%20developed%20specific%20noise%20priors%20for%20SIS%2C%0Aencompassing%20spatial%2C%20categorical%2C%20and%20a%20novel%20spatial-categorical%20joint%20prior%0Afor%20inference.%20This%20approach%2C%20which%20we%20have%20named%20SCP-Diff%2C%20has%20yielded%0Aexceptional%20results%2C%20achieving%20an%20FID%20of%2010.53%20on%20Cityscapes%20and%2012.66%20on%0AADE20K.The%20code%20and%20models%20can%20be%20accessed%20via%20the%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCP-Diff%3A%20Photo-Realistic%20Semantic%20Image%20Synthesis%20with%0A%20%20Spatial-Categorical%20Joint%20Prior&entry.906535625=Huan-ang%20Gao%20and%20Mingju%20Gao%20and%20Jiaju%20Li%20and%20Wenyi%20Li%20and%20Rong%20Zhi%20and%20Hao%20Tang%20and%20Hao%20Zhao&entry.1292438233=%20%20Semantic%20image%20synthesis%20%28SIS%29%20shows%20good%20promises%20for%20sensor%20simulation.%0AHowever%2C%20current%20best%20practices%20in%20this%20field%2C%20based%20on%20GANs%2C%20have%20not%20yet%0Areached%20the%20desired%20level%20of%20quality.%20As%20latent%20diffusion%20models%20make%0Asignificant%20strides%20in%20image%20generation%2C%20we%20are%20prompted%20to%20evaluate%0AControlNet%2C%20a%20notable%20method%20for%20its%20dense%20control%20capabilities.%20Our%0Ainvestigation%20uncovered%20two%20primary%20issues%20with%20its%20results%3A%20the%20presence%20of%0Aweird%20sub-structures%20within%20large%20semantic%20areas%20and%20the%20misalignment%20of%0Acontent%20with%20the%20semantic%20mask.%20Through%20empirical%20study%2C%20we%20pinpointed%20the%0Acause%20of%20these%20problems%20as%20a%20mismatch%20between%20the%20noised%20training%20data%0Adistribution%20and%20the%20standard%20normal%20prior%20applied%20at%20the%20inference%20stage.%20To%0Aaddress%20this%20challenge%2C%20we%20developed%20specific%20noise%20priors%20for%20SIS%2C%0Aencompassing%20spatial%2C%20categorical%2C%20and%20a%20novel%20spatial-categorical%20joint%20prior%0Afor%20inference.%20This%20approach%2C%20which%20we%20have%20named%20SCP-Diff%2C%20has%20yielded%0Aexceptional%20results%2C%20achieving%20an%20FID%20of%2010.53%20on%20Cityscapes%20and%2012.66%20on%0AADE20K.The%20code%20and%20models%20can%20be%20accessed%20via%20the%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09638v1&entry.124074799=Read"},
{"title": "Unleashing Network Potentials for Semantic Scene Completion", "author": "Fengyun Wang and Qianru Sun and Dong Zhang and Jinhui Tang", "abstract": "  Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy\nand semantics from a single-view RGB-D image, and recent SSC methods commonly\nadopt multi-modal inputs. However, our investigation reveals two limitations:\nineffective feature learning from single modalities and overfitting to limited\ndatasets. To address these issues, this paper proposes a novel SSC framework -\nAdversarial Modality Modulation Network (AMMNet) - with a fresh perspective of\noptimizing gradient updates. The proposed AMMNet introduces two core modules: a\ncross-modal modulation enabling the interdependence of gradient flows between\nmodalities, and a customized adversarial training scheme leveraging dynamic\ngradient competition. Specifically, the cross-modal modulation adaptively\nre-calibrates the features to better excite representation potentials from each\nsingle modality. The adversarial training employs a minimax game of evolving\ngradients, with customized guidance to strengthen the generator's perception of\nvisual fidelity from both geometric completeness and semantic correctness.\nExtensive experimental results demonstrate that AMMNet outperforms\nstate-of-the-art SSC methods by a large margin, providing a promising direction\nfor improving the effectiveness and generalization of SSC methods.\n", "link": "http://arxiv.org/abs/2403.07560v2", "date": "2024-03-14", "relevancy": 2.2298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5572}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unleashing%20Network%20Potentials%20for%20Semantic%20Scene%20Completion&body=Title%3A%20Unleashing%20Network%20Potentials%20for%20Semantic%20Scene%20Completion%0AAuthor%3A%20Fengyun%20Wang%20and%20Qianru%20Sun%20and%20Dong%20Zhang%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Semantic%20scene%20completion%20%28SSC%29%20aims%20to%20predict%20complete%203D%20voxel%20occupancy%0Aand%20semantics%20from%20a%20single-view%20RGB-D%20image%2C%20and%20recent%20SSC%20methods%20commonly%0Aadopt%20multi-modal%20inputs.%20However%2C%20our%20investigation%20reveals%20two%20limitations%3A%0Aineffective%20feature%20learning%20from%20single%20modalities%20and%20overfitting%20to%20limited%0Adatasets.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20novel%20SSC%20framework%20-%0AAdversarial%20Modality%20Modulation%20Network%20%28AMMNet%29%20-%20with%20a%20fresh%20perspective%20of%0Aoptimizing%20gradient%20updates.%20The%20proposed%20AMMNet%20introduces%20two%20core%20modules%3A%20a%0Across-modal%20modulation%20enabling%20the%20interdependence%20of%20gradient%20flows%20between%0Amodalities%2C%20and%20a%20customized%20adversarial%20training%20scheme%20leveraging%20dynamic%0Agradient%20competition.%20Specifically%2C%20the%20cross-modal%20modulation%20adaptively%0Are-calibrates%20the%20features%20to%20better%20excite%20representation%20potentials%20from%20each%0Asingle%20modality.%20The%20adversarial%20training%20employs%20a%20minimax%20game%20of%20evolving%0Agradients%2C%20with%20customized%20guidance%20to%20strengthen%20the%20generator%27s%20perception%20of%0Avisual%20fidelity%20from%20both%20geometric%20completeness%20and%20semantic%20correctness.%0AExtensive%20experimental%20results%20demonstrate%20that%20AMMNet%20outperforms%0Astate-of-the-art%20SSC%20methods%20by%20a%20large%20margin%2C%20providing%20a%20promising%20direction%0Afor%20improving%20the%20effectiveness%20and%20generalization%20of%20SSC%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07560v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20Network%20Potentials%20for%20Semantic%20Scene%20Completion&entry.906535625=Fengyun%20Wang%20and%20Qianru%20Sun%20and%20Dong%20Zhang%20and%20Jinhui%20Tang&entry.1292438233=%20%20Semantic%20scene%20completion%20%28SSC%29%20aims%20to%20predict%20complete%203D%20voxel%20occupancy%0Aand%20semantics%20from%20a%20single-view%20RGB-D%20image%2C%20and%20recent%20SSC%20methods%20commonly%0Aadopt%20multi-modal%20inputs.%20However%2C%20our%20investigation%20reveals%20two%20limitations%3A%0Aineffective%20feature%20learning%20from%20single%20modalities%20and%20overfitting%20to%20limited%0Adatasets.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20a%20novel%20SSC%20framework%20-%0AAdversarial%20Modality%20Modulation%20Network%20%28AMMNet%29%20-%20with%20a%20fresh%20perspective%20of%0Aoptimizing%20gradient%20updates.%20The%20proposed%20AMMNet%20introduces%20two%20core%20modules%3A%20a%0Across-modal%20modulation%20enabling%20the%20interdependence%20of%20gradient%20flows%20between%0Amodalities%2C%20and%20a%20customized%20adversarial%20training%20scheme%20leveraging%20dynamic%0Agradient%20competition.%20Specifically%2C%20the%20cross-modal%20modulation%20adaptively%0Are-calibrates%20the%20features%20to%20better%20excite%20representation%20potentials%20from%20each%0Asingle%20modality.%20The%20adversarial%20training%20employs%20a%20minimax%20game%20of%20evolving%0Agradients%2C%20with%20customized%20guidance%20to%20strengthen%20the%20generator%27s%20perception%20of%0Avisual%20fidelity%20from%20both%20geometric%20completeness%20and%20semantic%20correctness.%0AExtensive%20experimental%20results%20demonstrate%20that%20AMMNet%20outperforms%0Astate-of-the-art%20SSC%20methods%20by%20a%20large%20margin%2C%20providing%20a%20promising%20direction%0Afor%20improving%20the%20effectiveness%20and%20generalization%20of%20SSC%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07560v2&entry.124074799=Read"},
{"title": "Improving Real-Time Omnidirectional 3D Multi-Person Human Pose\n  Estimation with People Matching and Unsupervised 2D-3D Lifting", "author": "Pawel Knap and Peter Hardy and Alberto Tamajo and Hwasup Lim and Hansung Kim", "abstract": "  Current human pose estimation systems focus on retrieving an accurate 3D\nglobal estimate of a single person. Therefore, this paper presents one of the\nfirst 3D multi-person human pose estimation systems that is able to work in\nreal-time and is also able to handle basic forms of occlusion. First, we adjust\nan off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use\nwith a 360$^\\circ$ panoramic camera and mmWave radar sensors. We then introduce\nseveral contributions, including camera and radar calibrations, and the\nimproved matching of people within the image and radar space. The system\naddresses both the depth and scale ambiguity problems by employing a\nlightweight 2D-3D pose lifting algorithm that is able to work in real-time\nwhile exhibiting accurate performance in both indoor and outdoor environments\nwhich offers both an affordable and scalable solution. Notably, our system's\ntime complexity remains nearly constant irrespective of the number of detected\nindividuals, achieving a frame rate of approximately 7-8 fps on a laptop with a\ncommercial-grade GPU.\n", "link": "http://arxiv.org/abs/2403.09437v1", "date": "2024-03-14", "relevancy": 2.2283, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5812}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5365}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Real-Time%20Omnidirectional%203D%20Multi-Person%20Human%20Pose%0A%20%20Estimation%20with%20People%20Matching%20and%20Unsupervised%202D-3D%20Lifting&body=Title%3A%20Improving%20Real-Time%20Omnidirectional%203D%20Multi-Person%20Human%20Pose%0A%20%20Estimation%20with%20People%20Matching%20and%20Unsupervised%202D-3D%20Lifting%0AAuthor%3A%20Pawel%20Knap%20and%20Peter%20Hardy%20and%20Alberto%20Tamajo%20and%20Hwasup%20Lim%20and%20Hansung%20Kim%0AAbstract%3A%20%20%20Current%20human%20pose%20estimation%20systems%20focus%20on%20retrieving%20an%20accurate%203D%0Aglobal%20estimate%20of%20a%20single%20person.%20Therefore%2C%20this%20paper%20presents%20one%20of%20the%0Afirst%203D%20multi-person%20human%20pose%20estimation%20systems%20that%20is%20able%20to%20work%20in%0Areal-time%20and%20is%20also%20able%20to%20handle%20basic%20forms%20of%20occlusion.%20First%2C%20we%20adjust%0Aan%20off-the-shelf%202D%20detector%20and%20an%20unsupervised%202D-3D%20lifting%20model%20for%20use%0Awith%20a%20360%24%5E%5Ccirc%24%20panoramic%20camera%20and%20mmWave%20radar%20sensors.%20We%20then%20introduce%0Aseveral%20contributions%2C%20including%20camera%20and%20radar%20calibrations%2C%20and%20the%0Aimproved%20matching%20of%20people%20within%20the%20image%20and%20radar%20space.%20The%20system%0Aaddresses%20both%20the%20depth%20and%20scale%20ambiguity%20problems%20by%20employing%20a%0Alightweight%202D-3D%20pose%20lifting%20algorithm%20that%20is%20able%20to%20work%20in%20real-time%0Awhile%20exhibiting%20accurate%20performance%20in%20both%20indoor%20and%20outdoor%20environments%0Awhich%20offers%20both%20an%20affordable%20and%20scalable%20solution.%20Notably%2C%20our%20system%27s%0Atime%20complexity%20remains%20nearly%20constant%20irrespective%20of%20the%20number%20of%20detected%0Aindividuals%2C%20achieving%20a%20frame%20rate%20of%20approximately%207-8%20fps%20on%20a%20laptop%20with%20a%0Acommercial-grade%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09437v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Real-Time%20Omnidirectional%203D%20Multi-Person%20Human%20Pose%0A%20%20Estimation%20with%20People%20Matching%20and%20Unsupervised%202D-3D%20Lifting&entry.906535625=Pawel%20Knap%20and%20Peter%20Hardy%20and%20Alberto%20Tamajo%20and%20Hwasup%20Lim%20and%20Hansung%20Kim&entry.1292438233=%20%20Current%20human%20pose%20estimation%20systems%20focus%20on%20retrieving%20an%20accurate%203D%0Aglobal%20estimate%20of%20a%20single%20person.%20Therefore%2C%20this%20paper%20presents%20one%20of%20the%0Afirst%203D%20multi-person%20human%20pose%20estimation%20systems%20that%20is%20able%20to%20work%20in%0Areal-time%20and%20is%20also%20able%20to%20handle%20basic%20forms%20of%20occlusion.%20First%2C%20we%20adjust%0Aan%20off-the-shelf%202D%20detector%20and%20an%20unsupervised%202D-3D%20lifting%20model%20for%20use%0Awith%20a%20360%24%5E%5Ccirc%24%20panoramic%20camera%20and%20mmWave%20radar%20sensors.%20We%20then%20introduce%0Aseveral%20contributions%2C%20including%20camera%20and%20radar%20calibrations%2C%20and%20the%0Aimproved%20matching%20of%20people%20within%20the%20image%20and%20radar%20space.%20The%20system%0Aaddresses%20both%20the%20depth%20and%20scale%20ambiguity%20problems%20by%20employing%20a%0Alightweight%202D-3D%20pose%20lifting%20algorithm%20that%20is%20able%20to%20work%20in%20real-time%0Awhile%20exhibiting%20accurate%20performance%20in%20both%20indoor%20and%20outdoor%20environments%0Awhich%20offers%20both%20an%20affordable%20and%20scalable%20solution.%20Notably%2C%20our%20system%27s%0Atime%20complexity%20remains%20nearly%20constant%20irrespective%20of%20the%20number%20of%20detected%0Aindividuals%2C%20achieving%20a%20frame%20rate%20of%20approximately%207-8%20fps%20on%20a%20laptop%20with%20a%0Acommercial-grade%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09437v1&entry.124074799=Read"},
{"title": "Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion\n  Models for Sparse-view CT Reconstruction", "author": "Hanyu Chen and Zhixiu Hao and Lin Guo and Liying Xiao", "abstract": "  Sparse-view Computed Tomography (CT) image reconstruction is a promising\napproach to reduce radiation exposure, but it inevitably leads to image\ndegradation. Although diffusion model-based approaches are computationally\nexpensive and suffer from the training-sampling discrepancy, they provide a\npotential solution to the problem. This study introduces a novel Cascaded\nDiffusion with Discrepancy Mitigation (CDDM) framework, including the\nlow-quality image generation in latent space and the high-quality image\ngeneration in pixel space which contains data consistency and discrepancy\nmitigation in a one-step reconstruction process. The cascaded framework\nminimizes computational costs by moving some inference steps from pixel space\nto latent space. The discrepancy mitigation technique addresses the\ntraining-sampling gap induced by data consistency, ensuring the data\ndistribution is close to the original manifold. A specialized Alternating\nDirection Method of Multipliers (ADMM) is employed to process image gradients\nin separate directions, offering a more targeted approach to regularization.\nExperimental results across two datasets demonstrate CDDM's superior\nperformance in high-quality image generation with clearer boundaries compared\nto existing methods, highlighting the framework's computational efficiency.\n", "link": "http://arxiv.org/abs/2403.09355v1", "date": "2024-03-14", "relevancy": 2.2244, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5794}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5704}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5271}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Data%20Consistency%20Induced%20Discrepancy%20in%20Cascaded%20Diffusion%0A%20%20Models%20for%20Sparse-view%20CT%20Reconstruction&body=Title%3A%20Mitigating%20Data%20Consistency%20Induced%20Discrepancy%20in%20Cascaded%20Diffusion%0A%20%20Models%20for%20Sparse-view%20CT%20Reconstruction%0AAuthor%3A%20Hanyu%20Chen%20and%20Zhixiu%20Hao%20and%20Lin%20Guo%20and%20Liying%20Xiao%0AAbstract%3A%20%20%20Sparse-view%20Computed%20Tomography%20%28CT%29%20image%20reconstruction%20is%20a%20promising%0Aapproach%20to%20reduce%20radiation%20exposure%2C%20but%20it%20inevitably%20leads%20to%20image%0Adegradation.%20Although%20diffusion%20model-based%20approaches%20are%20computationally%0Aexpensive%20and%20suffer%20from%20the%20training-sampling%20discrepancy%2C%20they%20provide%20a%0Apotential%20solution%20to%20the%20problem.%20This%20study%20introduces%20a%20novel%20Cascaded%0ADiffusion%20with%20Discrepancy%20Mitigation%20%28CDDM%29%20framework%2C%20including%20the%0Alow-quality%20image%20generation%20in%20latent%20space%20and%20the%20high-quality%20image%0Ageneration%20in%20pixel%20space%20which%20contains%20data%20consistency%20and%20discrepancy%0Amitigation%20in%20a%20one-step%20reconstruction%20process.%20The%20cascaded%20framework%0Aminimizes%20computational%20costs%20by%20moving%20some%20inference%20steps%20from%20pixel%20space%0Ato%20latent%20space.%20The%20discrepancy%20mitigation%20technique%20addresses%20the%0Atraining-sampling%20gap%20induced%20by%20data%20consistency%2C%20ensuring%20the%20data%0Adistribution%20is%20close%20to%20the%20original%20manifold.%20A%20specialized%20Alternating%0ADirection%20Method%20of%20Multipliers%20%28ADMM%29%20is%20employed%20to%20process%20image%20gradients%0Ain%20separate%20directions%2C%20offering%20a%20more%20targeted%20approach%20to%20regularization.%0AExperimental%20results%20across%20two%20datasets%20demonstrate%20CDDM%27s%20superior%0Aperformance%20in%20high-quality%20image%20generation%20with%20clearer%20boundaries%20compared%0Ato%20existing%20methods%2C%20highlighting%20the%20framework%27s%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09355v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Data%20Consistency%20Induced%20Discrepancy%20in%20Cascaded%20Diffusion%0A%20%20Models%20for%20Sparse-view%20CT%20Reconstruction&entry.906535625=Hanyu%20Chen%20and%20Zhixiu%20Hao%20and%20Lin%20Guo%20and%20Liying%20Xiao&entry.1292438233=%20%20Sparse-view%20Computed%20Tomography%20%28CT%29%20image%20reconstruction%20is%20a%20promising%0Aapproach%20to%20reduce%20radiation%20exposure%2C%20but%20it%20inevitably%20leads%20to%20image%0Adegradation.%20Although%20diffusion%20model-based%20approaches%20are%20computationally%0Aexpensive%20and%20suffer%20from%20the%20training-sampling%20discrepancy%2C%20they%20provide%20a%0Apotential%20solution%20to%20the%20problem.%20This%20study%20introduces%20a%20novel%20Cascaded%0ADiffusion%20with%20Discrepancy%20Mitigation%20%28CDDM%29%20framework%2C%20including%20the%0Alow-quality%20image%20generation%20in%20latent%20space%20and%20the%20high-quality%20image%0Ageneration%20in%20pixel%20space%20which%20contains%20data%20consistency%20and%20discrepancy%0Amitigation%20in%20a%20one-step%20reconstruction%20process.%20The%20cascaded%20framework%0Aminimizes%20computational%20costs%20by%20moving%20some%20inference%20steps%20from%20pixel%20space%0Ato%20latent%20space.%20The%20discrepancy%20mitigation%20technique%20addresses%20the%0Atraining-sampling%20gap%20induced%20by%20data%20consistency%2C%20ensuring%20the%20data%0Adistribution%20is%20close%20to%20the%20original%20manifold.%20A%20specialized%20Alternating%0ADirection%20Method%20of%20Multipliers%20%28ADMM%29%20is%20employed%20to%20process%20image%20gradients%0Ain%20separate%20directions%2C%20offering%20a%20more%20targeted%20approach%20to%20regularization.%0AExperimental%20results%20across%20two%20datasets%20demonstrate%20CDDM%27s%20superior%0Aperformance%20in%20high-quality%20image%20generation%20with%20clearer%20boundaries%20compared%0Ato%20existing%20methods%2C%20highlighting%20the%20framework%27s%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09355v1&entry.124074799=Read"},
{"title": "Unsupervised Modality-Transferable Video Highlight Detection with\n  Representation Activation Sequence Learning", "author": "Tingtian Li and Zixun Sun and Xinyu Xiao", "abstract": "  Identifying highlight moments of raw video materials is crucial for improving\nthe efficiency of editing videos that are pervasive on internet platforms.\nHowever, the extensive work of manually labeling footage has created obstacles\nto applying supervised methods to videos of unseen categories. The absence of\nan audio modality that contains valuable cues for highlight detection in many\nvideos also makes it difficult to use multimodal strategies. In this paper, we\npropose a novel model with cross-modal perception for unsupervised highlight\ndetection. The proposed model learns representations with visual-audio level\nsemantics from image-audio pair data via a self-reconstruction task. To achieve\nunsupervised highlight detection, we investigate the latent representations of\nthe network and propose the representation activation sequence learning (RASL)\nmodule with k-point contrastive learning to learn significant representation\nactivations. To connect the visual modality with the audio modality, we use the\nsymmetric contrastive learning (SCL) module to learn the paired visual and\naudio representations. Furthermore, an auxiliary task of masked feature vector\nsequence (FVS) reconstruction is simultaneously conducted during pretraining\nfor representation enhancement. During inference, the cross-modal pretrained\nmodel can generate representations with paired visual-audio semantics given\nonly the visual modality. The RASL module is used to output the highlight\nscores. The experimental results show that the proposed framework achieves\nsuperior performance compared to other state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2403.09401v1", "date": "2024-03-14", "relevancy": 2.224, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning&body=Title%3A%20Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning%0AAuthor%3A%20Tingtian%20Li%20and%20Zixun%20Sun%20and%20Xinyu%20Xiao%0AAbstract%3A%20%20%20Identifying%20highlight%20moments%20of%20raw%20video%20materials%20is%20crucial%20for%20improving%0Athe%20efficiency%20of%20editing%20videos%20that%20are%20pervasive%20on%20internet%20platforms.%0AHowever%2C%20the%20extensive%20work%20of%20manually%20labeling%20footage%20has%20created%20obstacles%0Ato%20applying%20supervised%20methods%20to%20videos%20of%20unseen%20categories.%20The%20absence%20of%0Aan%20audio%20modality%20that%20contains%20valuable%20cues%20for%20highlight%20detection%20in%20many%0Avideos%20also%20makes%20it%20difficult%20to%20use%20multimodal%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%20with%20cross-modal%20perception%20for%20unsupervised%20highlight%0Adetection.%20The%20proposed%20model%20learns%20representations%20with%20visual-audio%20level%0Asemantics%20from%20image-audio%20pair%20data%20via%20a%20self-reconstruction%20task.%20To%20achieve%0Aunsupervised%20highlight%20detection%2C%20we%20investigate%20the%20latent%20representations%20of%0Athe%20network%20and%20propose%20the%20representation%20activation%20sequence%20learning%20%28RASL%29%0Amodule%20with%20k-point%20contrastive%20learning%20to%20learn%20significant%20representation%0Aactivations.%20To%20connect%20the%20visual%20modality%20with%20the%20audio%20modality%2C%20we%20use%20the%0Asymmetric%20contrastive%20learning%20%28SCL%29%20module%20to%20learn%20the%20paired%20visual%20and%0Aaudio%20representations.%20Furthermore%2C%20an%20auxiliary%20task%20of%20masked%20feature%20vector%0Asequence%20%28FVS%29%20reconstruction%20is%20simultaneously%20conducted%20during%20pretraining%0Afor%20representation%20enhancement.%20During%20inference%2C%20the%20cross-modal%20pretrained%0Amodel%20can%20generate%20representations%20with%20paired%20visual-audio%20semantics%20given%0Aonly%20the%20visual%20modality.%20The%20RASL%20module%20is%20used%20to%20output%20the%20highlight%0Ascores.%20The%20experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%0Asuperior%20performance%20compared%20to%20other%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09401v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning&entry.906535625=Tingtian%20Li%20and%20Zixun%20Sun%20and%20Xinyu%20Xiao&entry.1292438233=%20%20Identifying%20highlight%20moments%20of%20raw%20video%20materials%20is%20crucial%20for%20improving%0Athe%20efficiency%20of%20editing%20videos%20that%20are%20pervasive%20on%20internet%20platforms.%0AHowever%2C%20the%20extensive%20work%20of%20manually%20labeling%20footage%20has%20created%20obstacles%0Ato%20applying%20supervised%20methods%20to%20videos%20of%20unseen%20categories.%20The%20absence%20of%0Aan%20audio%20modality%20that%20contains%20valuable%20cues%20for%20highlight%20detection%20in%20many%0Avideos%20also%20makes%20it%20difficult%20to%20use%20multimodal%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%20with%20cross-modal%20perception%20for%20unsupervised%20highlight%0Adetection.%20The%20proposed%20model%20learns%20representations%20with%20visual-audio%20level%0Asemantics%20from%20image-audio%20pair%20data%20via%20a%20self-reconstruction%20task.%20To%20achieve%0Aunsupervised%20highlight%20detection%2C%20we%20investigate%20the%20latent%20representations%20of%0Athe%20network%20and%20propose%20the%20representation%20activation%20sequence%20learning%20%28RASL%29%0Amodule%20with%20k-point%20contrastive%20learning%20to%20learn%20significant%20representation%0Aactivations.%20To%20connect%20the%20visual%20modality%20with%20the%20audio%20modality%2C%20we%20use%20the%0Asymmetric%20contrastive%20learning%20%28SCL%29%20module%20to%20learn%20the%20paired%20visual%20and%0Aaudio%20representations.%20Furthermore%2C%20an%20auxiliary%20task%20of%20masked%20feature%20vector%0Asequence%20%28FVS%29%20reconstruction%20is%20simultaneously%20conducted%20during%20pretraining%0Afor%20representation%20enhancement.%20During%20inference%2C%20the%20cross-modal%20pretrained%0Amodel%20can%20generate%20representations%20with%20paired%20visual-audio%20semantics%20given%0Aonly%20the%20visual%20modality.%20The%20RASL%20module%20is%20used%20to%20output%20the%20highlight%0Ascores.%20The%20experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%0Asuperior%20performance%20compared%20to%20other%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09401v1&entry.124074799=Read"},
{"title": "Consistent Prompting for Rehearsal-Free Continual Learning", "author": "Zhanxin Gao and Jun Cen and Xiaobin Chang", "abstract": "  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n", "link": "http://arxiv.org/abs/2403.08568v2", "date": "2024-03-14", "relevancy": 2.2147, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4501}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4369}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning&body=Title%3A%20Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning%0AAuthor%3A%20Zhanxin%20Gao%20and%20Jun%20Cen%20and%20Xiaobin%20Chang%0AAbstract%3A%20%20%20Continual%20learning%20empowers%20models%20to%20adapt%20autonomously%20to%20the%20ever-changing%0Aenvironment%20or%20data%20streams%20without%20forgetting%20old%20knowledge.%20Prompt-based%0Aapproaches%20are%20built%20on%20frozen%20pre-trained%20models%20to%20learn%20the%20task-specific%0Aprompts%20and%20classifiers%20efficiently.%20Existing%20prompt-based%20methods%20are%0Ainconsistent%20between%20training%20and%20testing%2C%20limiting%20their%20effectiveness.%20Two%0Atypes%20of%20inconsistency%20are%20revealed.%20Test%20predictions%20are%20made%20from%20all%0Aclassifiers%20while%20training%20only%20focuses%20on%20the%20current%20task%20classifier%20without%0Aholistic%20alignment%2C%20leading%20to%20Classifier%20inconsistency.%20Prompt%20inconsistency%0Aindicates%20that%20the%20prompt%20selected%20during%20testing%20may%20not%20correspond%20to%20the%20one%0Aassociated%20with%20this%20task%20during%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprompt-based%20method%2C%20Consistent%20Prompting%20%28CPrompt%29%2C%20for%20more%20aligned%20training%0Aand%20testing.%20Specifically%2C%20all%20existing%20classifiers%20are%20exposed%20to%20prompt%0Atraining%2C%20resulting%20in%20classifier%20consistency%20learning.%20In%20addition%2C%20prompt%0Aconsistency%20learning%20is%20proposed%20to%20enhance%20prediction%20robustness%20and%20boost%0Aprompt%20selection%20accuracy.%20Our%20Consistent%20Prompting%20surpasses%20its%20prompt-based%0Acounterparts%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20continual%0Alearning%20benchmarks.%20Detailed%20analysis%20shows%20that%20improvements%20come%20from%20more%0Aconsistent%20training%20and%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08568v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Prompting%20for%20Rehearsal-Free%20Continual%20Learning&entry.906535625=Zhanxin%20Gao%20and%20Jun%20Cen%20and%20Xiaobin%20Chang&entry.1292438233=%20%20Continual%20learning%20empowers%20models%20to%20adapt%20autonomously%20to%20the%20ever-changing%0Aenvironment%20or%20data%20streams%20without%20forgetting%20old%20knowledge.%20Prompt-based%0Aapproaches%20are%20built%20on%20frozen%20pre-trained%20models%20to%20learn%20the%20task-specific%0Aprompts%20and%20classifiers%20efficiently.%20Existing%20prompt-based%20methods%20are%0Ainconsistent%20between%20training%20and%20testing%2C%20limiting%20their%20effectiveness.%20Two%0Atypes%20of%20inconsistency%20are%20revealed.%20Test%20predictions%20are%20made%20from%20all%0Aclassifiers%20while%20training%20only%20focuses%20on%20the%20current%20task%20classifier%20without%0Aholistic%20alignment%2C%20leading%20to%20Classifier%20inconsistency.%20Prompt%20inconsistency%0Aindicates%20that%20the%20prompt%20selected%20during%20testing%20may%20not%20correspond%20to%20the%20one%0Aassociated%20with%20this%20task%20during%20training.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprompt-based%20method%2C%20Consistent%20Prompting%20%28CPrompt%29%2C%20for%20more%20aligned%20training%0Aand%20testing.%20Specifically%2C%20all%20existing%20classifiers%20are%20exposed%20to%20prompt%0Atraining%2C%20resulting%20in%20classifier%20consistency%20learning.%20In%20addition%2C%20prompt%0Aconsistency%20learning%20is%20proposed%20to%20enhance%20prediction%20robustness%20and%20boost%0Aprompt%20selection%20accuracy.%20Our%20Consistent%20Prompting%20surpasses%20its%20prompt-based%0Acounterparts%20and%20achieves%20state-of-the-art%20performance%20on%20multiple%20continual%0Alearning%20benchmarks.%20Detailed%20analysis%20shows%20that%20improvements%20come%20from%20more%0Aconsistent%20training%20and%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08568v2&entry.124074799=Read"},
{"title": "On the Convergence of Locally Adaptive and Scalable Diffusion-Based\n  Sampling Methods for Deep Bayesian Neural Network Posteriors", "author": "Tim Rensmeyer and Oliver Niggemann", "abstract": "  Achieving robust uncertainty quantification for deep neural networks\nrepresents an important requirement in many real-world applications of deep\nlearning such as medical imaging where it is necessary to assess the\nreliability of a neural network's prediction. Bayesian neural networks are a\npromising approach for modeling uncertainties in deep neural networks.\nUnfortunately, generating samples from the posterior distribution of neural\nnetworks is a major challenge. One significant advance in that direction would\nbe the incorporation of adaptive step sizes, similar to modern neural network\noptimizers, into Monte Carlo Markov chain sampling algorithms without\nsignificantly increasing computational demand. Over the past years, several\npapers have introduced sampling algorithms with claims that they achieve this\nproperty. However, do they indeed converge to the correct distribution? In this\npaper, we demonstrate that these methods can have a substantial bias in the\ndistribution they sample, even in the limit of vanishing step sizes and at full\nbatch size.\n", "link": "http://arxiv.org/abs/2403.08609v2", "date": "2024-03-14", "relevancy": 2.2086, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors&body=Title%3A%20On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors%0AAuthor%3A%20Tim%20Rensmeyer%20and%20Oliver%20Niggemann%0AAbstract%3A%20%20%20Achieving%20robust%20uncertainty%20quantification%20for%20deep%20neural%20networks%0Arepresents%20an%20important%20requirement%20in%20many%20real-world%20applications%20of%20deep%0Alearning%20such%20as%20medical%20imaging%20where%20it%20is%20necessary%20to%20assess%20the%0Areliability%20of%20a%20neural%20network%27s%20prediction.%20Bayesian%20neural%20networks%20are%20a%0Apromising%20approach%20for%20modeling%20uncertainties%20in%20deep%20neural%20networks.%0AUnfortunately%2C%20generating%20samples%20from%20the%20posterior%20distribution%20of%20neural%0Anetworks%20is%20a%20major%20challenge.%20One%20significant%20advance%20in%20that%20direction%20would%0Abe%20the%20incorporation%20of%20adaptive%20step%20sizes%2C%20similar%20to%20modern%20neural%20network%0Aoptimizers%2C%20into%20Monte%20Carlo%20Markov%20chain%20sampling%20algorithms%20without%0Asignificantly%20increasing%20computational%20demand.%20Over%20the%20past%20years%2C%20several%0Apapers%20have%20introduced%20sampling%20algorithms%20with%20claims%20that%20they%20achieve%20this%0Aproperty.%20However%2C%20do%20they%20indeed%20converge%20to%20the%20correct%20distribution%3F%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20these%20methods%20can%20have%20a%20substantial%20bias%20in%20the%0Adistribution%20they%20sample%2C%20even%20in%20the%20limit%20of%20vanishing%20step%20sizes%20and%20at%20full%0Abatch%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08609v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Locally%20Adaptive%20and%20Scalable%20Diffusion-Based%0A%20%20Sampling%20Methods%20for%20Deep%20Bayesian%20Neural%20Network%20Posteriors&entry.906535625=Tim%20Rensmeyer%20and%20Oliver%20Niggemann&entry.1292438233=%20%20Achieving%20robust%20uncertainty%20quantification%20for%20deep%20neural%20networks%0Arepresents%20an%20important%20requirement%20in%20many%20real-world%20applications%20of%20deep%0Alearning%20such%20as%20medical%20imaging%20where%20it%20is%20necessary%20to%20assess%20the%0Areliability%20of%20a%20neural%20network%27s%20prediction.%20Bayesian%20neural%20networks%20are%20a%0Apromising%20approach%20for%20modeling%20uncertainties%20in%20deep%20neural%20networks.%0AUnfortunately%2C%20generating%20samples%20from%20the%20posterior%20distribution%20of%20neural%0Anetworks%20is%20a%20major%20challenge.%20One%20significant%20advance%20in%20that%20direction%20would%0Abe%20the%20incorporation%20of%20adaptive%20step%20sizes%2C%20similar%20to%20modern%20neural%20network%0Aoptimizers%2C%20into%20Monte%20Carlo%20Markov%20chain%20sampling%20algorithms%20without%0Asignificantly%20increasing%20computational%20demand.%20Over%20the%20past%20years%2C%20several%0Apapers%20have%20introduced%20sampling%20algorithms%20with%20claims%20that%20they%20achieve%20this%0Aproperty.%20However%2C%20do%20they%20indeed%20converge%20to%20the%20correct%20distribution%3F%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20these%20methods%20can%20have%20a%20substantial%20bias%20in%20the%0Adistribution%20they%20sample%2C%20even%20in%20the%20limit%20of%20vanishing%20step%20sizes%20and%20at%20full%0Abatch%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08609v2&entry.124074799=Read"},
{"title": "Geometric structure of Deep Learning networks and construction of global\n  ${\\mathcal L}^2$ minimizers", "author": "Thomas Chen and Patricia Mu\u00f1oz Ewald", "abstract": "  In this paper, we explicitly determine local and global minimizers of the\n$\\mathcal{L}^2$ cost function in underparametrized Deep Learning (DL) networks;\nour main goal is to shed light on their geometric structure and properties. We\naccomplish this by a direct construction, without invoking the gradient descent\nflow at any point of this work. We specifically consider $L$ hidden layers, a\nReLU ramp activation function, an $\\mathcal{L}^2$ Schatten class (or\nHilbert-Schmidt) cost function, input and output spaces $\\mathbb{R}^Q$ with\nequal dimension $Q\\geq1$, and hidden layers also defined on $\\mathbb{R}^{Q}$;\nthe training inputs are assumed to be sufficiently clustered. The training\ninput size $N$ can be arbitrarily large - thus, we are considering the\nunderparametrized regime. More general settings are left to future work. We\nconstruct an explicit family of minimizers for the global minimum of the cost\nfunction in the case $L\\geq Q$, which we show to be degenerate. Moreover, we\ndetermine a set of $2^Q-1$ distinct degenerate local minima of the cost\nfunction. In the context presented here, the concatenation of hidden layers of\nthe DL network is reinterpreted as a recursive application of a {\\em truncation\nmap} which \"curates\" the training inputs by minimizing their noise to signal\nratio.\n", "link": "http://arxiv.org/abs/2309.10639v4", "date": "2024-03-14", "relevancy": 2.206, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4675}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4321}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.424}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Geometric%20structure%20of%20Deep%20Learning%20networks%20and%20construction%20of%20global%0A%20%20%24%7B%5Cmathcal%20L%7D%5E2%24%20minimizers&body=Title%3A%20Geometric%20structure%20of%20Deep%20Learning%20networks%20and%20construction%20of%20global%0A%20%20%24%7B%5Cmathcal%20L%7D%5E2%24%20minimizers%0AAuthor%3A%20Thomas%20Chen%20and%20Patricia%20Mu%C3%B1oz%20Ewald%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explicitly%20determine%20local%20and%20global%20minimizers%20of%20the%0A%24%5Cmathcal%7BL%7D%5E2%24%20cost%20function%20in%20underparametrized%20Deep%20Learning%20%28DL%29%20networks%3B%0Aour%20main%20goal%20is%20to%20shed%20light%20on%20their%20geometric%20structure%20and%20properties.%20We%0Aaccomplish%20this%20by%20a%20direct%20construction%2C%20without%20invoking%20the%20gradient%20descent%0Aflow%20at%20any%20point%20of%20this%20work.%20We%20specifically%20consider%20%24L%24%20hidden%20layers%2C%20a%0AReLU%20ramp%20activation%20function%2C%20an%20%24%5Cmathcal%7BL%7D%5E2%24%20Schatten%20class%20%28or%0AHilbert-Schmidt%29%20cost%20function%2C%20input%20and%20output%20spaces%20%24%5Cmathbb%7BR%7D%5EQ%24%20with%0Aequal%20dimension%20%24Q%5Cgeq1%24%2C%20and%20hidden%20layers%20also%20defined%20on%20%24%5Cmathbb%7BR%7D%5E%7BQ%7D%24%3B%0Athe%20training%20inputs%20are%20assumed%20to%20be%20sufficiently%20clustered.%20The%20training%0Ainput%20size%20%24N%24%20can%20be%20arbitrarily%20large%20-%20thus%2C%20we%20are%20considering%20the%0Aunderparametrized%20regime.%20More%20general%20settings%20are%20left%20to%20future%20work.%20We%0Aconstruct%20an%20explicit%20family%20of%20minimizers%20for%20the%20global%20minimum%20of%20the%20cost%0Afunction%20in%20the%20case%20%24L%5Cgeq%20Q%24%2C%20which%20we%20show%20to%20be%20degenerate.%20Moreover%2C%20we%0Adetermine%20a%20set%20of%20%242%5EQ-1%24%20distinct%20degenerate%20local%20minima%20of%20the%20cost%0Afunction.%20In%20the%20context%20presented%20here%2C%20the%20concatenation%20of%20hidden%20layers%20of%0Athe%20DL%20network%20is%20reinterpreted%20as%20a%20recursive%20application%20of%20a%20%7B%5Cem%20truncation%0Amap%7D%20which%20%22curates%22%20the%20training%20inputs%20by%20minimizing%20their%20noise%20to%20signal%0Aratio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10639v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20structure%20of%20Deep%20Learning%20networks%20and%20construction%20of%20global%0A%20%20%24%7B%5Cmathcal%20L%7D%5E2%24%20minimizers&entry.906535625=Thomas%20Chen%20and%20Patricia%20Mu%C3%B1oz%20Ewald&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explicitly%20determine%20local%20and%20global%20minimizers%20of%20the%0A%24%5Cmathcal%7BL%7D%5E2%24%20cost%20function%20in%20underparametrized%20Deep%20Learning%20%28DL%29%20networks%3B%0Aour%20main%20goal%20is%20to%20shed%20light%20on%20their%20geometric%20structure%20and%20properties.%20We%0Aaccomplish%20this%20by%20a%20direct%20construction%2C%20without%20invoking%20the%20gradient%20descent%0Aflow%20at%20any%20point%20of%20this%20work.%20We%20specifically%20consider%20%24L%24%20hidden%20layers%2C%20a%0AReLU%20ramp%20activation%20function%2C%20an%20%24%5Cmathcal%7BL%7D%5E2%24%20Schatten%20class%20%28or%0AHilbert-Schmidt%29%20cost%20function%2C%20input%20and%20output%20spaces%20%24%5Cmathbb%7BR%7D%5EQ%24%20with%0Aequal%20dimension%20%24Q%5Cgeq1%24%2C%20and%20hidden%20layers%20also%20defined%20on%20%24%5Cmathbb%7BR%7D%5E%7BQ%7D%24%3B%0Athe%20training%20inputs%20are%20assumed%20to%20be%20sufficiently%20clustered.%20The%20training%0Ainput%20size%20%24N%24%20can%20be%20arbitrarily%20large%20-%20thus%2C%20we%20are%20considering%20the%0Aunderparametrized%20regime.%20More%20general%20settings%20are%20left%20to%20future%20work.%20We%0Aconstruct%20an%20explicit%20family%20of%20minimizers%20for%20the%20global%20minimum%20of%20the%20cost%0Afunction%20in%20the%20case%20%24L%5Cgeq%20Q%24%2C%20which%20we%20show%20to%20be%20degenerate.%20Moreover%2C%20we%0Adetermine%20a%20set%20of%20%242%5EQ-1%24%20distinct%20degenerate%20local%20minima%20of%20the%20cost%0Afunction.%20In%20the%20context%20presented%20here%2C%20the%20concatenation%20of%20hidden%20layers%20of%0Athe%20DL%20network%20is%20reinterpreted%20as%20a%20recursive%20application%20of%20a%20%7B%5Cem%20truncation%0Amap%7D%20which%20%22curates%22%20the%20training%20inputs%20by%20minimizing%20their%20noise%20to%20signal%0Aratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10639v4&entry.124074799=Read"},
{"title": "Osprey: Pixel Understanding with Visual Instruction Tuning", "author": "Yuqian Yuan and Wentong Li and Jian Liu and Dongqi Tang and Xinjie Luo and Chi Qin and Lei Zhang and Jianke Zhu", "abstract": "  Multimodal large language models (MLLMs) have recently achieved impressive\ngeneral-purpose vision-language capabilities through visual instruction tuning.\nHowever, current MLLMs primarily focus on image-level or box-level\nunderstanding, falling short in achieving fine-grained vision-language\nalignment at pixel level. Besides, the lack of mask-based instruction data\nlimits their advancements. In this paper, we propose Osprey, a mask-text\ninstruction tuning approach, to extend MLLMs by incorporating fine-grained mask\nregions into language instruction, aiming at achieving pixel-wise visual\nunderstanding. To achieve this goal, we first meticulously curate a mask-based\nregion-text dataset with 724K samples, and then design a vision-language model\nby injecting pixel-level representation into LLM. Specifically, Osprey adopts a\nconvolutional CLIP backbone as the vision encoder and employs a mask-aware\nvisual extractor to extract precise visual mask features from high resolution\ninput. Experimental results demonstrate Osprey's superiority in various region\nunderstanding tasks, showcasing its new capability for pixel-level instruction\ntuning. In particular, Osprey can be integrated with Segment Anything Model\n(SAM) seamlessly to obtain multi-granularity semantics. The source code,\ndataset and demo can be found at https://github.com/CircleRadon/Osprey.\n", "link": "http://arxiv.org/abs/2312.10032v3", "date": "2024-03-14", "relevancy": 2.2018, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5952}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5173}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Osprey%3A%20Pixel%20Understanding%20with%20Visual%20Instruction%20Tuning&body=Title%3A%20Osprey%3A%20Pixel%20Understanding%20with%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Yuqian%20Yuan%20and%20Wentong%20Li%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Xinjie%20Luo%20and%20Chi%20Qin%20and%20Lei%20Zhang%20and%20Jianke%20Zhu%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20recently%20achieved%20impressive%0Ageneral-purpose%20vision-language%20capabilities%20through%20visual%20instruction%20tuning.%0AHowever%2C%20current%20MLLMs%20primarily%20focus%20on%20image-level%20or%20box-level%0Aunderstanding%2C%20falling%20short%20in%20achieving%20fine-grained%20vision-language%0Aalignment%20at%20pixel%20level.%20Besides%2C%20the%20lack%20of%20mask-based%20instruction%20data%0Alimits%20their%20advancements.%20In%20this%20paper%2C%20we%20propose%20Osprey%2C%20a%20mask-text%0Ainstruction%20tuning%20approach%2C%20to%20extend%20MLLMs%20by%20incorporating%20fine-grained%20mask%0Aregions%20into%20language%20instruction%2C%20aiming%20at%20achieving%20pixel-wise%20visual%0Aunderstanding.%20To%20achieve%20this%20goal%2C%20we%20first%20meticulously%20curate%20a%20mask-based%0Aregion-text%20dataset%20with%20724K%20samples%2C%20and%20then%20design%20a%20vision-language%20model%0Aby%20injecting%20pixel-level%20representation%20into%20LLM.%20Specifically%2C%20Osprey%20adopts%20a%0Aconvolutional%20CLIP%20backbone%20as%20the%20vision%20encoder%20and%20employs%20a%20mask-aware%0Avisual%20extractor%20to%20extract%20precise%20visual%20mask%20features%20from%20high%20resolution%0Ainput.%20Experimental%20results%20demonstrate%20Osprey%27s%20superiority%20in%20various%20region%0Aunderstanding%20tasks%2C%20showcasing%20its%20new%20capability%20for%20pixel-level%20instruction%0Atuning.%20In%20particular%2C%20Osprey%20can%20be%20integrated%20with%20Segment%20Anything%20Model%0A%28SAM%29%20seamlessly%20to%20obtain%20multi-granularity%20semantics.%20The%20source%20code%2C%0Adataset%20and%20demo%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/Osprey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10032v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Osprey%3A%20Pixel%20Understanding%20with%20Visual%20Instruction%20Tuning&entry.906535625=Yuqian%20Yuan%20and%20Wentong%20Li%20and%20Jian%20Liu%20and%20Dongqi%20Tang%20and%20Xinjie%20Luo%20and%20Chi%20Qin%20and%20Lei%20Zhang%20and%20Jianke%20Zhu&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20recently%20achieved%20impressive%0Ageneral-purpose%20vision-language%20capabilities%20through%20visual%20instruction%20tuning.%0AHowever%2C%20current%20MLLMs%20primarily%20focus%20on%20image-level%20or%20box-level%0Aunderstanding%2C%20falling%20short%20in%20achieving%20fine-grained%20vision-language%0Aalignment%20at%20pixel%20level.%20Besides%2C%20the%20lack%20of%20mask-based%20instruction%20data%0Alimits%20their%20advancements.%20In%20this%20paper%2C%20we%20propose%20Osprey%2C%20a%20mask-text%0Ainstruction%20tuning%20approach%2C%20to%20extend%20MLLMs%20by%20incorporating%20fine-grained%20mask%0Aregions%20into%20language%20instruction%2C%20aiming%20at%20achieving%20pixel-wise%20visual%0Aunderstanding.%20To%20achieve%20this%20goal%2C%20we%20first%20meticulously%20curate%20a%20mask-based%0Aregion-text%20dataset%20with%20724K%20samples%2C%20and%20then%20design%20a%20vision-language%20model%0Aby%20injecting%20pixel-level%20representation%20into%20LLM.%20Specifically%2C%20Osprey%20adopts%20a%0Aconvolutional%20CLIP%20backbone%20as%20the%20vision%20encoder%20and%20employs%20a%20mask-aware%0Avisual%20extractor%20to%20extract%20precise%20visual%20mask%20features%20from%20high%20resolution%0Ainput.%20Experimental%20results%20demonstrate%20Osprey%27s%20superiority%20in%20various%20region%0Aunderstanding%20tasks%2C%20showcasing%20its%20new%20capability%20for%20pixel-level%20instruction%0Atuning.%20In%20particular%2C%20Osprey%20can%20be%20integrated%20with%20Segment%20Anything%20Model%0A%28SAM%29%20seamlessly%20to%20obtain%20multi-granularity%20semantics.%20The%20source%20code%2C%0Adataset%20and%20demo%20can%20be%20found%20at%20https%3A//github.com/CircleRadon/Osprey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10032v3&entry.124074799=Read"},
{"title": "XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via\n  Controllable Diffusion Model", "author": "Anees Ur Rehman Hashmi and Ibrahim Almakky and Mohammad Areeb Qazi and Santosh Sanjeev and Vijay Ram Papineni and Dwarikanath Mahapatra and Mohammad Yaqub", "abstract": "  Large-scale generative models have demonstrated impressive capacity in\nproducing visually compelling images, with increasing applications in medical\nimaging. However, they continue to grapple with the challenge of image\nhallucination and the generation of anatomically inaccurate outputs. These\nlimitations are mainly due to the sole reliance on textual inputs and lack of\nspatial control over the generated images, hindering the potential usefulness\nof such models in real-life settings. We present XReal, a novel controllable\ndiffusion model for generating realistic chest X-ray images through precise\nanatomy and pathology location control. Our lightweight method can seamlessly\nintegrate spatial control in a pre-trained text-to-image diffusion model\nwithout fine-tuning, retaining its existing knowledge while enhancing its\ngeneration capabilities. XReal outperforms state-of-the-art x-ray diffusion\nmodels in quantitative and qualitative metrics while showing 13% and 10%\nanatomy and pathology realism gain, respectively, based on the expert\nradiologist evaluation. Our model holds promise for advancing generative models\nin medical imaging, offering greater precision and adaptability while inviting\nfurther exploration in this evolving field. A large synthetically generated\ndata with annotations and code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/XReal.\n", "link": "http://arxiv.org/abs/2403.09240v1", "date": "2024-03-14", "relevancy": 2.1948, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5871}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5412}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5409}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XReal%3A%20Realistic%20Anatomy%20and%20Pathology-Aware%20X-ray%20Generation%20via%0A%20%20Controllable%20Diffusion%20Model&body=Title%3A%20XReal%3A%20Realistic%20Anatomy%20and%20Pathology-Aware%20X-ray%20Generation%20via%0A%20%20Controllable%20Diffusion%20Model%0AAuthor%3A%20Anees%20Ur%20Rehman%20Hashmi%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Areeb%20Qazi%20and%20Santosh%20Sanjeev%20and%20Vijay%20Ram%20Papineni%20and%20Dwarikanath%20Mahapatra%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Large-scale%20generative%20models%20have%20demonstrated%20impressive%20capacity%20in%0Aproducing%20visually%20compelling%20images%2C%20with%20increasing%20applications%20in%20medical%0Aimaging.%20However%2C%20they%20continue%20to%20grapple%20with%20the%20challenge%20of%20image%0Ahallucination%20and%20the%20generation%20of%20anatomically%20inaccurate%20outputs.%20These%0Alimitations%20are%20mainly%20due%20to%20the%20sole%20reliance%20on%20textual%20inputs%20and%20lack%20of%0Aspatial%20control%20over%20the%20generated%20images%2C%20hindering%20the%20potential%20usefulness%0Aof%20such%20models%20in%20real-life%20settings.%20We%20present%20XReal%2C%20a%20novel%20controllable%0Adiffusion%20model%20for%20generating%20realistic%20chest%20X-ray%20images%20through%20precise%0Aanatomy%20and%20pathology%20location%20control.%20Our%20lightweight%20method%20can%20seamlessly%0Aintegrate%20spatial%20control%20in%20a%20pre-trained%20text-to-image%20diffusion%20model%0Awithout%20fine-tuning%2C%20retaining%20its%20existing%20knowledge%20while%20enhancing%20its%0Ageneration%20capabilities.%20XReal%20outperforms%20state-of-the-art%20x-ray%20diffusion%0Amodels%20in%20quantitative%20and%20qualitative%20metrics%20while%20showing%2013%25%20and%2010%25%0Aanatomy%20and%20pathology%20realism%20gain%2C%20respectively%2C%20based%20on%20the%20expert%0Aradiologist%20evaluation.%20Our%20model%20holds%20promise%20for%20advancing%20generative%20models%0Ain%20medical%20imaging%2C%20offering%20greater%20precision%20and%20adaptability%20while%20inviting%0Afurther%20exploration%20in%20this%20evolving%20field.%20A%20large%20synthetically%20generated%0Adata%20with%20annotations%20and%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/XReal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09240v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XReal%3A%20Realistic%20Anatomy%20and%20Pathology-Aware%20X-ray%20Generation%20via%0A%20%20Controllable%20Diffusion%20Model&entry.906535625=Anees%20Ur%20Rehman%20Hashmi%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Areeb%20Qazi%20and%20Santosh%20Sanjeev%20and%20Vijay%20Ram%20Papineni%20and%20Dwarikanath%20Mahapatra%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Large-scale%20generative%20models%20have%20demonstrated%20impressive%20capacity%20in%0Aproducing%20visually%20compelling%20images%2C%20with%20increasing%20applications%20in%20medical%0Aimaging.%20However%2C%20they%20continue%20to%20grapple%20with%20the%20challenge%20of%20image%0Ahallucination%20and%20the%20generation%20of%20anatomically%20inaccurate%20outputs.%20These%0Alimitations%20are%20mainly%20due%20to%20the%20sole%20reliance%20on%20textual%20inputs%20and%20lack%20of%0Aspatial%20control%20over%20the%20generated%20images%2C%20hindering%20the%20potential%20usefulness%0Aof%20such%20models%20in%20real-life%20settings.%20We%20present%20XReal%2C%20a%20novel%20controllable%0Adiffusion%20model%20for%20generating%20realistic%20chest%20X-ray%20images%20through%20precise%0Aanatomy%20and%20pathology%20location%20control.%20Our%20lightweight%20method%20can%20seamlessly%0Aintegrate%20spatial%20control%20in%20a%20pre-trained%20text-to-image%20diffusion%20model%0Awithout%20fine-tuning%2C%20retaining%20its%20existing%20knowledge%20while%20enhancing%20its%0Ageneration%20capabilities.%20XReal%20outperforms%20state-of-the-art%20x-ray%20diffusion%0Amodels%20in%20quantitative%20and%20qualitative%20metrics%20while%20showing%2013%25%20and%2010%25%0Aanatomy%20and%20pathology%20realism%20gain%2C%20respectively%2C%20based%20on%20the%20expert%0Aradiologist%20evaluation.%20Our%20model%20holds%20promise%20for%20advancing%20generative%20models%0Ain%20medical%20imaging%2C%20offering%20greater%20precision%20and%20adaptability%20while%20inviting%0Afurther%20exploration%20in%20this%20evolving%20field.%20A%20large%20synthetically%20generated%0Adata%20with%20annotations%20and%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/BioMedIA-MBZUAI/XReal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09240v1&entry.124074799=Read"},
{"title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling\n  and Visual-Language Co-Referring", "author": "Yufei Zhan and Yousong Zhu and Hongyin Zhao and Fan Yang and Ming Tang and Jinqiao Wang", "abstract": "  Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.\n", "link": "http://arxiv.org/abs/2403.09333v1", "date": "2024-03-14", "relevancy": 2.1855, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5498}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5446}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5422}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Griffon%20v2%3A%20Advancing%20Multimodal%20Perception%20with%20High-Resolution%20Scaling%0A%20%20and%20Visual-Language%20Co-Referring&body=Title%3A%20Griffon%20v2%3A%20Advancing%20Multimodal%20Perception%20with%20High-Resolution%20Scaling%0A%20%20and%20Visual-Language%20Co-Referring%0AAuthor%3A%20Yufei%20Zhan%20and%20Yousong%20Zhu%20and%20Hongyin%20Zhao%20and%20Fan%20Yang%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Large%20Vision%20Language%20Models%20have%20achieved%20fine-grained%20object%20perception%2C%0Abut%20the%20limitation%20of%20image%20resolution%20remains%20a%20significant%20obstacle%20to%0Asurpass%20the%20performance%20of%20task-specific%20experts%20in%20complex%20and%20dense%0Ascenarios.%20Such%20limitation%20further%20restricts%20the%20model%27s%20potential%20to%20achieve%0Anuanced%20visual%20and%20language%20referring%20in%20domains%20such%20as%20GUI%20Agents%2C%20Counting%0Aand%20%5Cetc.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20unified%20high-resolution%0Ageneralist%20model%2C%20Griffon%20v2%2C%20enabling%20flexible%20object%20referring%20with%20visual%0Aand%20textual%20prompts.%20To%20efficiently%20scaling%20up%20image%20resolution%2C%20we%20design%20a%0Asimple%20and%20lightweight%20down-sampling%20projector%20to%20overcome%20the%20input%20tokens%0Aconstraint%20in%20Large%20Language%20Models.%20This%20design%20inherently%20preserves%20the%0Acomplete%20contexts%20and%20fine%20details%2C%20and%20significantly%20improves%20multimodal%0Aperception%20ability%20especially%20for%20small%20objects.%20Building%20upon%20this%2C%20we%20further%0Aequip%20the%20model%20with%20visual-language%20co-referring%20capabilities%20through%20a%0Aplug-and-play%20visual%20tokenizer.%20It%20enables%20user-friendly%20interaction%20with%0Aflexible%20target%20images%2C%20free-form%20texts%20and%20even%20coordinates.%20Experiments%0Ademonstrate%20that%20Griffon%20v2%20can%20localize%20any%20objects%20of%20interest%20with%20visual%0Aand%20textual%20referring%2C%20achieve%20state-of-the-art%20performance%20on%20REC%2C%20phrase%0Agrounding%2C%20and%20REG%20tasks%2C%20and%20outperform%20expert%20models%20in%20object%20detection%20and%0Aobject%20counting.%20Data%2C%20codes%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/jefferyZhan/Griffon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09333v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Griffon%20v2%3A%20Advancing%20Multimodal%20Perception%20with%20High-Resolution%20Scaling%0A%20%20and%20Visual-Language%20Co-Referring&entry.906535625=Yufei%20Zhan%20and%20Yousong%20Zhu%20and%20Hongyin%20Zhao%20and%20Fan%20Yang%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Large%20Vision%20Language%20Models%20have%20achieved%20fine-grained%20object%20perception%2C%0Abut%20the%20limitation%20of%20image%20resolution%20remains%20a%20significant%20obstacle%20to%0Asurpass%20the%20performance%20of%20task-specific%20experts%20in%20complex%20and%20dense%0Ascenarios.%20Such%20limitation%20further%20restricts%20the%20model%27s%20potential%20to%20achieve%0Anuanced%20visual%20and%20language%20referring%20in%20domains%20such%20as%20GUI%20Agents%2C%20Counting%0Aand%20%5Cetc.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20unified%20high-resolution%0Ageneralist%20model%2C%20Griffon%20v2%2C%20enabling%20flexible%20object%20referring%20with%20visual%0Aand%20textual%20prompts.%20To%20efficiently%20scaling%20up%20image%20resolution%2C%20we%20design%20a%0Asimple%20and%20lightweight%20down-sampling%20projector%20to%20overcome%20the%20input%20tokens%0Aconstraint%20in%20Large%20Language%20Models.%20This%20design%20inherently%20preserves%20the%0Acomplete%20contexts%20and%20fine%20details%2C%20and%20significantly%20improves%20multimodal%0Aperception%20ability%20especially%20for%20small%20objects.%20Building%20upon%20this%2C%20we%20further%0Aequip%20the%20model%20with%20visual-language%20co-referring%20capabilities%20through%20a%0Aplug-and-play%20visual%20tokenizer.%20It%20enables%20user-friendly%20interaction%20with%0Aflexible%20target%20images%2C%20free-form%20texts%20and%20even%20coordinates.%20Experiments%0Ademonstrate%20that%20Griffon%20v2%20can%20localize%20any%20objects%20of%20interest%20with%20visual%0Aand%20textual%20referring%2C%20achieve%20state-of-the-art%20performance%20on%20REC%2C%20phrase%0Agrounding%2C%20and%20REG%20tasks%2C%20and%20outperform%20expert%20models%20in%20object%20detection%20and%0Aobject%20counting.%20Data%2C%20codes%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/jefferyZhan/Griffon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09333v1&entry.124074799=Read"},
{"title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance", "author": "Mingyuan Sun and Donghao Zhang and Zongyuan Ge and Jiaxu Wang and Jia Li and Zheng Fang and Renjing Xu", "abstract": "  Event camera, a novel bio-inspired vision sensor, has drawn a lot of\nattention for its low latency, low power consumption, and high dynamic range.\nCurrently, overfitting remains a critical problem in event-based classification\ntasks for Spiking Neural Network (SNN) due to its relatively weak spatial\nrepresentation capability. Data augmentation is a simple but efficient method\nto alleviate overfitting and improve the generalization ability of neural\nnetworks, and saliency-based augmentation methods are proven to be effective in\nthe image processing field. However, there is no approach available for\nextracting saliency maps from SNNs. Therefore, for the first time, we present\nSpiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking\nLayer-wise Relevance Propagation rule (SLRP) in order for SNN to generate\nstable and accurate CAMs and saliency maps. Based on this, we propose EventRPG,\nwhich leverages relevance propagation on the spiking neural network for more\nefficient augmentation. Our proposed method has been evaluated on several SNN\nstructures, achieving state-of-the-art performance in object recognition tasks\nincluding N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as\nwell as action recognition task SL-Animals with an accuracy of 91.59%. Our code\nis available at https://github.com/myuansun/EventRPG.\n", "link": "http://arxiv.org/abs/2403.09274v1", "date": "2024-03-14", "relevancy": 2.1786, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5503}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5465}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5406}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EventRPG%3A%20Event%20Data%20Augmentation%20with%20Relevance%20Propagation%20Guidance&body=Title%3A%20EventRPG%3A%20Event%20Data%20Augmentation%20with%20Relevance%20Propagation%20Guidance%0AAuthor%3A%20Mingyuan%20Sun%20and%20Donghao%20Zhang%20and%20Zongyuan%20Ge%20and%20Jiaxu%20Wang%20and%20Jia%20Li%20and%20Zheng%20Fang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Event%20camera%2C%20a%20novel%20bio-inspired%20vision%20sensor%2C%20has%20drawn%20a%20lot%20of%0Aattention%20for%20its%20low%20latency%2C%20low%20power%20consumption%2C%20and%20high%20dynamic%20range.%0ACurrently%2C%20overfitting%20remains%20a%20critical%20problem%20in%20event-based%20classification%0Atasks%20for%20Spiking%20Neural%20Network%20%28SNN%29%20due%20to%20its%20relatively%20weak%20spatial%0Arepresentation%20capability.%20Data%20augmentation%20is%20a%20simple%20but%20efficient%20method%0Ato%20alleviate%20overfitting%20and%20improve%20the%20generalization%20ability%20of%20neural%0Anetworks%2C%20and%20saliency-based%20augmentation%20methods%20are%20proven%20to%20be%20effective%20in%0Athe%20image%20processing%20field.%20However%2C%20there%20is%20no%20approach%20available%20for%0Aextracting%20saliency%20maps%20from%20SNNs.%20Therefore%2C%20for%20the%20first%20time%2C%20we%20present%0ASpiking%20Layer-Time-wise%20Relevance%20Propagation%20rule%20%28SLTRP%29%20and%20Spiking%0ALayer-wise%20Relevance%20Propagation%20rule%20%28SLRP%29%20in%20order%20for%20SNN%20to%20generate%0Astable%20and%20accurate%20CAMs%20and%20saliency%20maps.%20Based%20on%20this%2C%20we%20propose%20EventRPG%2C%0Awhich%20leverages%20relevance%20propagation%20on%20the%20spiking%20neural%20network%20for%20more%0Aefficient%20augmentation.%20Our%20proposed%20method%20has%20been%20evaluated%20on%20several%20SNN%0Astructures%2C%20achieving%20state-of-the-art%20performance%20in%20object%20recognition%20tasks%0Aincluding%20N-Caltech101%2C%20CIFAR10-DVS%2C%20with%20accuracies%20of%2085.62%25%20and%2085.55%25%2C%20as%0Awell%20as%20action%20recognition%20task%20SL-Animals%20with%20an%20accuracy%20of%2091.59%25.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/myuansun/EventRPG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09274v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventRPG%3A%20Event%20Data%20Augmentation%20with%20Relevance%20Propagation%20Guidance&entry.906535625=Mingyuan%20Sun%20and%20Donghao%20Zhang%20and%20Zongyuan%20Ge%20and%20Jiaxu%20Wang%20and%20Jia%20Li%20and%20Zheng%20Fang%20and%20Renjing%20Xu&entry.1292438233=%20%20Event%20camera%2C%20a%20novel%20bio-inspired%20vision%20sensor%2C%20has%20drawn%20a%20lot%20of%0Aattention%20for%20its%20low%20latency%2C%20low%20power%20consumption%2C%20and%20high%20dynamic%20range.%0ACurrently%2C%20overfitting%20remains%20a%20critical%20problem%20in%20event-based%20classification%0Atasks%20for%20Spiking%20Neural%20Network%20%28SNN%29%20due%20to%20its%20relatively%20weak%20spatial%0Arepresentation%20capability.%20Data%20augmentation%20is%20a%20simple%20but%20efficient%20method%0Ato%20alleviate%20overfitting%20and%20improve%20the%20generalization%20ability%20of%20neural%0Anetworks%2C%20and%20saliency-based%20augmentation%20methods%20are%20proven%20to%20be%20effective%20in%0Athe%20image%20processing%20field.%20However%2C%20there%20is%20no%20approach%20available%20for%0Aextracting%20saliency%20maps%20from%20SNNs.%20Therefore%2C%20for%20the%20first%20time%2C%20we%20present%0ASpiking%20Layer-Time-wise%20Relevance%20Propagation%20rule%20%28SLTRP%29%20and%20Spiking%0ALayer-wise%20Relevance%20Propagation%20rule%20%28SLRP%29%20in%20order%20for%20SNN%20to%20generate%0Astable%20and%20accurate%20CAMs%20and%20saliency%20maps.%20Based%20on%20this%2C%20we%20propose%20EventRPG%2C%0Awhich%20leverages%20relevance%20propagation%20on%20the%20spiking%20neural%20network%20for%20more%0Aefficient%20augmentation.%20Our%20proposed%20method%20has%20been%20evaluated%20on%20several%20SNN%0Astructures%2C%20achieving%20state-of-the-art%20performance%20in%20object%20recognition%20tasks%0Aincluding%20N-Caltech101%2C%20CIFAR10-DVS%2C%20with%20accuracies%20of%2085.62%25%20and%2085.55%25%2C%20as%0Awell%20as%20action%20recognition%20task%20SL-Animals%20with%20an%20accuracy%20of%2091.59%25.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/myuansun/EventRPG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09274v1&entry.124074799=Read"},
{"title": "RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient\n  Neural Fields", "author": "Doriand Petit and Steve Bourgeois and Dumitru Pavel and Vincent Gay-Bellile and Florian Chabot and Loic Barthe", "abstract": "  Recent advances in Neural Fields mostly rely on developing task-specific\nsupervision which often complicates the models. Rather than developing\nhard-to-combine and specific modules, another approach generally overlooked is\nto directly inject generic priors on the scene representation (also called\ninductive biases) into the NeRF architecture. Based on this idea, we propose\nthe RING-NeRF architecture which includes two inductive biases : a continuous\nmulti-scale representation of the scene and an invariance of the decoder's\nlatent space over spatial and scale domains. We also design a single\nreconstruction process that takes advantage of those inductive biases and\nexperimentally demonstrates on-par performances in terms of quality with\ndedicated architecture on multiple tasks (anti-aliasing, few view\nreconstruction, SDF reconstruction without scene-specific initialization) while\nbeing more efficient. Moreover, RING-NeRF has the distinctive ability to\ndynamically increase the resolution of the model, opening the way to adaptive\nreconstruction.\n", "link": "http://arxiv.org/abs/2312.03357v2", "date": "2024-03-14", "relevancy": 2.1753, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5536}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5417}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RING-NeRF%20%3A%20Rethinking%20Inductive%20Biases%20for%20Versatile%20and%20Efficient%0A%20%20Neural%20Fields&body=Title%3A%20RING-NeRF%20%3A%20Rethinking%20Inductive%20Biases%20for%20Versatile%20and%20Efficient%0A%20%20Neural%20Fields%0AAuthor%3A%20Doriand%20Petit%20and%20Steve%20Bourgeois%20and%20Dumitru%20Pavel%20and%20Vincent%20Gay-Bellile%20and%20Florian%20Chabot%20and%20Loic%20Barthe%0AAbstract%3A%20%20%20Recent%20advances%20in%20Neural%20Fields%20mostly%20rely%20on%20developing%20task-specific%0Asupervision%20which%20often%20complicates%20the%20models.%20Rather%20than%20developing%0Ahard-to-combine%20and%20specific%20modules%2C%20another%20approach%20generally%20overlooked%20is%0Ato%20directly%20inject%20generic%20priors%20on%20the%20scene%20representation%20%28also%20called%0Ainductive%20biases%29%20into%20the%20NeRF%20architecture.%20Based%20on%20this%20idea%2C%20we%20propose%0Athe%20RING-NeRF%20architecture%20which%20includes%20two%20inductive%20biases%20%3A%20a%20continuous%0Amulti-scale%20representation%20of%20the%20scene%20and%20an%20invariance%20of%20the%20decoder%27s%0Alatent%20space%20over%20spatial%20and%20scale%20domains.%20We%20also%20design%20a%20single%0Areconstruction%20process%20that%20takes%20advantage%20of%20those%20inductive%20biases%20and%0Aexperimentally%20demonstrates%20on-par%20performances%20in%20terms%20of%20quality%20with%0Adedicated%20architecture%20on%20multiple%20tasks%20%28anti-aliasing%2C%20few%20view%0Areconstruction%2C%20SDF%20reconstruction%20without%20scene-specific%20initialization%29%20while%0Abeing%20more%20efficient.%20Moreover%2C%20RING-NeRF%20has%20the%20distinctive%20ability%20to%0Adynamically%20increase%20the%20resolution%20of%20the%20model%2C%20opening%20the%20way%20to%20adaptive%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03357v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RING-NeRF%20%3A%20Rethinking%20Inductive%20Biases%20for%20Versatile%20and%20Efficient%0A%20%20Neural%20Fields&entry.906535625=Doriand%20Petit%20and%20Steve%20Bourgeois%20and%20Dumitru%20Pavel%20and%20Vincent%20Gay-Bellile%20and%20Florian%20Chabot%20and%20Loic%20Barthe&entry.1292438233=%20%20Recent%20advances%20in%20Neural%20Fields%20mostly%20rely%20on%20developing%20task-specific%0Asupervision%20which%20often%20complicates%20the%20models.%20Rather%20than%20developing%0Ahard-to-combine%20and%20specific%20modules%2C%20another%20approach%20generally%20overlooked%20is%0Ato%20directly%20inject%20generic%20priors%20on%20the%20scene%20representation%20%28also%20called%0Ainductive%20biases%29%20into%20the%20NeRF%20architecture.%20Based%20on%20this%20idea%2C%20we%20propose%0Athe%20RING-NeRF%20architecture%20which%20includes%20two%20inductive%20biases%20%3A%20a%20continuous%0Amulti-scale%20representation%20of%20the%20scene%20and%20an%20invariance%20of%20the%20decoder%27s%0Alatent%20space%20over%20spatial%20and%20scale%20domains.%20We%20also%20design%20a%20single%0Areconstruction%20process%20that%20takes%20advantage%20of%20those%20inductive%20biases%20and%0Aexperimentally%20demonstrates%20on-par%20performances%20in%20terms%20of%20quality%20with%0Adedicated%20architecture%20on%20multiple%20tasks%20%28anti-aliasing%2C%20few%20view%0Areconstruction%2C%20SDF%20reconstruction%20without%20scene-specific%20initialization%29%20while%0Abeing%20more%20efficient.%20Moreover%2C%20RING-NeRF%20has%20the%20distinctive%20ability%20to%0Adynamically%20increase%20the%20resolution%20of%20the%20model%2C%20opening%20the%20way%20to%20adaptive%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03357v2&entry.124074799=Read"},
{"title": "SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with\n  Spike Streams", "author": "Kang Chen and Shiyan Chen and Jiyuan Zhang and Baoyue Zhang and Yajing Zheng and Tiejun Huang and Zhaofei Yu", "abstract": "  Reconstructing a sequence of sharp images from the blurry input is crucial\nfor enhancing our insights into the captured scene and poses a significant\nchallenge due to the limited temporal features embedded in the image. Spike\ncameras, sampling at rates up to 40,000 Hz, have proven effective in capturing\nmotion features and beneficial for solving this ill-posed problem. Nonetheless,\nexisting methods fall into the supervised learning paradigm, which suffers from\nnotable performance degradation when applied to real-world scenarios that\ndiverge from the synthetic training data domain. Moreover, the quality of\nreconstructed images is capped by the generated images based on motion analysis\ninterpolation, which inherently differs from the actual scene, affecting the\ngeneralization ability of these methods in real high-speed scenarios. To\naddress these challenges, we propose the first self-supervised framework for\nthe task of spike-guided motion deblurring. Our approach begins with the\nformulation of a spike-guided deblurring model that explores the theoretical\nrelationships among spike streams, blurry images, and their corresponding sharp\nsequences. We subsequently develop a self-supervised cascaded framework to\nalleviate the issues of spike noise and spatial-resolution mismatching\nencountered in the deblurring model. With knowledge distillation and\nre-blurring loss, we further design a lightweight deblur network to generate\nhigh-quality sequences with brightness and texture consistency with the\noriginal input. Quantitative and qualitative experiments conducted on our\nreal-world and synthetic datasets with spikes validate the superior\ngeneralization of the proposed framework. Our code, data and trained models\nwill be available at \\url{https://github.com/chenkang455/S-SDM}.\n", "link": "http://arxiv.org/abs/2403.09486v1", "date": "2024-03-14", "relevancy": 2.1748, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5459}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5443}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams&body=Title%3A%20SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams%0AAuthor%3A%20Kang%20Chen%20and%20Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Baoyue%20Zhang%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu%0AAbstract%3A%20%20%20Reconstructing%20a%20sequence%20of%20sharp%20images%20from%20the%20blurry%20input%20is%20crucial%0Afor%20enhancing%20our%20insights%20into%20the%20captured%20scene%20and%20poses%20a%20significant%0Achallenge%20due%20to%20the%20limited%20temporal%20features%20embedded%20in%20the%20image.%20Spike%0Acameras%2C%20sampling%20at%20rates%20up%20to%2040%2C000%20Hz%2C%20have%20proven%20effective%20in%20capturing%0Amotion%20features%20and%20beneficial%20for%20solving%20this%20ill-posed%20problem.%20Nonetheless%2C%0Aexisting%20methods%20fall%20into%20the%20supervised%20learning%20paradigm%2C%20which%20suffers%20from%0Anotable%20performance%20degradation%20when%20applied%20to%20real-world%20scenarios%20that%0Adiverge%20from%20the%20synthetic%20training%20data%20domain.%20Moreover%2C%20the%20quality%20of%0Areconstructed%20images%20is%20capped%20by%20the%20generated%20images%20based%20on%20motion%20analysis%0Ainterpolation%2C%20which%20inherently%20differs%20from%20the%20actual%20scene%2C%20affecting%20the%0Ageneralization%20ability%20of%20these%20methods%20in%20real%20high-speed%20scenarios.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20the%20first%20self-supervised%20framework%20for%0Athe%20task%20of%20spike-guided%20motion%20deblurring.%20Our%20approach%20begins%20with%20the%0Aformulation%20of%20a%20spike-guided%20deblurring%20model%20that%20explores%20the%20theoretical%0Arelationships%20among%20spike%20streams%2C%20blurry%20images%2C%20and%20their%20corresponding%20sharp%0Asequences.%20We%20subsequently%20develop%20a%20self-supervised%20cascaded%20framework%20to%0Aalleviate%20the%20issues%20of%20spike%20noise%20and%20spatial-resolution%20mismatching%0Aencountered%20in%20the%20deblurring%20model.%20With%20knowledge%20distillation%20and%0Are-blurring%20loss%2C%20we%20further%20design%20a%20lightweight%20deblur%20network%20to%20generate%0Ahigh-quality%20sequences%20with%20brightness%20and%20texture%20consistency%20with%20the%0Aoriginal%20input.%20Quantitative%20and%20qualitative%20experiments%20conducted%20on%20our%0Areal-world%20and%20synthetic%20datasets%20with%20spikes%20validate%20the%20superior%0Ageneralization%20of%20the%20proposed%20framework.%20Our%20code%2C%20data%20and%20trained%20models%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenkang455/S-SDM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09486v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeReveal%3A%20Unlocking%20Temporal%20Sequences%20from%20Real%20Blurry%20Inputs%20with%0A%20%20Spike%20Streams&entry.906535625=Kang%20Chen%20and%20Shiyan%20Chen%20and%20Jiyuan%20Zhang%20and%20Baoyue%20Zhang%20and%20Yajing%20Zheng%20and%20Tiejun%20Huang%20and%20Zhaofei%20Yu&entry.1292438233=%20%20Reconstructing%20a%20sequence%20of%20sharp%20images%20from%20the%20blurry%20input%20is%20crucial%0Afor%20enhancing%20our%20insights%20into%20the%20captured%20scene%20and%20poses%20a%20significant%0Achallenge%20due%20to%20the%20limited%20temporal%20features%20embedded%20in%20the%20image.%20Spike%0Acameras%2C%20sampling%20at%20rates%20up%20to%2040%2C000%20Hz%2C%20have%20proven%20effective%20in%20capturing%0Amotion%20features%20and%20beneficial%20for%20solving%20this%20ill-posed%20problem.%20Nonetheless%2C%0Aexisting%20methods%20fall%20into%20the%20supervised%20learning%20paradigm%2C%20which%20suffers%20from%0Anotable%20performance%20degradation%20when%20applied%20to%20real-world%20scenarios%20that%0Adiverge%20from%20the%20synthetic%20training%20data%20domain.%20Moreover%2C%20the%20quality%20of%0Areconstructed%20images%20is%20capped%20by%20the%20generated%20images%20based%20on%20motion%20analysis%0Ainterpolation%2C%20which%20inherently%20differs%20from%20the%20actual%20scene%2C%20affecting%20the%0Ageneralization%20ability%20of%20these%20methods%20in%20real%20high-speed%20scenarios.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20the%20first%20self-supervised%20framework%20for%0Athe%20task%20of%20spike-guided%20motion%20deblurring.%20Our%20approach%20begins%20with%20the%0Aformulation%20of%20a%20spike-guided%20deblurring%20model%20that%20explores%20the%20theoretical%0Arelationships%20among%20spike%20streams%2C%20blurry%20images%2C%20and%20their%20corresponding%20sharp%0Asequences.%20We%20subsequently%20develop%20a%20self-supervised%20cascaded%20framework%20to%0Aalleviate%20the%20issues%20of%20spike%20noise%20and%20spatial-resolution%20mismatching%0Aencountered%20in%20the%20deblurring%20model.%20With%20knowledge%20distillation%20and%0Are-blurring%20loss%2C%20we%20further%20design%20a%20lightweight%20deblur%20network%20to%20generate%0Ahigh-quality%20sequences%20with%20brightness%20and%20texture%20consistency%20with%20the%0Aoriginal%20input.%20Quantitative%20and%20qualitative%20experiments%20conducted%20on%20our%0Areal-world%20and%20synthetic%20datasets%20with%20spikes%20validate%20the%20superior%0Ageneralization%20of%20the%20proposed%20framework.%20Our%20code%2C%20data%20and%20trained%20models%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenkang455/S-SDM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09486v1&entry.124074799=Read"},
{"title": "Mind the map! Accounting for existing map information when estimating\n  online HDMaps from sensor", "author": "R\u00e9my Sun and Li Yang and Diane Lingrand and Fr\u00e9d\u00e9ric Precioso", "abstract": "  While HDMaps are a crucial component of autonomous driving, they are\nexpensive to acquire and maintain. Estimating these maps from sensors therefore\npromises to significantly lighten costs. These estimations however overlook\nexisting HDMaps, with current methods at most geolocalizing low quality maps or\nconsidering a general database of known maps. In this paper, we propose to\naccount for existing maps of the precise situation studied when estimating\nHDMaps. We identify 3 reasonable types of useful existing maps (minimalist,\nnoisy, and outdated). We also introduce MapEX, a novel online HDMap estimation\nframework that accounts for existing maps. MapEX achieves this by encoding map\nelements into query tokens and by refining the matching algorithm used to train\nclassic query based map estimation models. We demonstrate that MapEX brings\nsignificant improvements on the nuScenes dataset. For instance, MapEX - given\nnoisy maps - improves by 38% over the MapTRv2 detector it is based on and by 8%\nover the current SOTA.\n", "link": "http://arxiv.org/abs/2311.10517v2", "date": "2024-03-14", "relevancy": 2.1737, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5954}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4984}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20map%21%20Accounting%20for%20existing%20map%20information%20when%20estimating%0A%20%20online%20HDMaps%20from%20sensor&body=Title%3A%20Mind%20the%20map%21%20Accounting%20for%20existing%20map%20information%20when%20estimating%0A%20%20online%20HDMaps%20from%20sensor%0AAuthor%3A%20R%C3%A9my%20Sun%20and%20Li%20Yang%20and%20Diane%20Lingrand%20and%20Fr%C3%A9d%C3%A9ric%20Precioso%0AAbstract%3A%20%20%20While%20HDMaps%20are%20a%20crucial%20component%20of%20autonomous%20driving%2C%20they%20are%0Aexpensive%20to%20acquire%20and%20maintain.%20Estimating%20these%20maps%20from%20sensors%20therefore%0Apromises%20to%20significantly%20lighten%20costs.%20These%20estimations%20however%20overlook%0Aexisting%20HDMaps%2C%20with%20current%20methods%20at%20most%20geolocalizing%20low%20quality%20maps%20or%0Aconsidering%20a%20general%20database%20of%20known%20maps.%20In%20this%20paper%2C%20we%20propose%20to%0Aaccount%20for%20existing%20maps%20of%20the%20precise%20situation%20studied%20when%20estimating%0AHDMaps.%20We%20identify%203%20reasonable%20types%20of%20useful%20existing%20maps%20%28minimalist%2C%0Anoisy%2C%20and%20outdated%29.%20We%20also%20introduce%20MapEX%2C%20a%20novel%20online%20HDMap%20estimation%0Aframework%20that%20accounts%20for%20existing%20maps.%20MapEX%20achieves%20this%20by%20encoding%20map%0Aelements%20into%20query%20tokens%20and%20by%20refining%20the%20matching%20algorithm%20used%20to%20train%0Aclassic%20query%20based%20map%20estimation%20models.%20We%20demonstrate%20that%20MapEX%20brings%0Asignificant%20improvements%20on%20the%20nuScenes%20dataset.%20For%20instance%2C%20MapEX%20-%20given%0Anoisy%20maps%20-%20improves%20by%2038%25%20over%20the%20MapTRv2%20detector%20it%20is%20based%20on%20and%20by%208%25%0Aover%20the%20current%20SOTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10517v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20map%21%20Accounting%20for%20existing%20map%20information%20when%20estimating%0A%20%20online%20HDMaps%20from%20sensor&entry.906535625=R%C3%A9my%20Sun%20and%20Li%20Yang%20and%20Diane%20Lingrand%20and%20Fr%C3%A9d%C3%A9ric%20Precioso&entry.1292438233=%20%20While%20HDMaps%20are%20a%20crucial%20component%20of%20autonomous%20driving%2C%20they%20are%0Aexpensive%20to%20acquire%20and%20maintain.%20Estimating%20these%20maps%20from%20sensors%20therefore%0Apromises%20to%20significantly%20lighten%20costs.%20These%20estimations%20however%20overlook%0Aexisting%20HDMaps%2C%20with%20current%20methods%20at%20most%20geolocalizing%20low%20quality%20maps%20or%0Aconsidering%20a%20general%20database%20of%20known%20maps.%20In%20this%20paper%2C%20we%20propose%20to%0Aaccount%20for%20existing%20maps%20of%20the%20precise%20situation%20studied%20when%20estimating%0AHDMaps.%20We%20identify%203%20reasonable%20types%20of%20useful%20existing%20maps%20%28minimalist%2C%0Anoisy%2C%20and%20outdated%29.%20We%20also%20introduce%20MapEX%2C%20a%20novel%20online%20HDMap%20estimation%0Aframework%20that%20accounts%20for%20existing%20maps.%20MapEX%20achieves%20this%20by%20encoding%20map%0Aelements%20into%20query%20tokens%20and%20by%20refining%20the%20matching%20algorithm%20used%20to%20train%0Aclassic%20query%20based%20map%20estimation%20models.%20We%20demonstrate%20that%20MapEX%20brings%0Asignificant%20improvements%20on%20the%20nuScenes%20dataset.%20For%20instance%2C%20MapEX%20-%20given%0Anoisy%20maps%20-%20improves%20by%2038%25%20over%20the%20MapTRv2%20detector%20it%20is%20based%20on%20and%20by%208%25%0Aover%20the%20current%20SOTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10517v2&entry.124074799=Read"},
{"title": "To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation", "author": "Abdul Hameed Azeemi and Ihsan Ayyub Qazi and Agha Ali Raza", "abstract": "  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n", "link": "http://arxiv.org/abs/2403.09259v1", "date": "2024-03-14", "relevancy": 2.1633, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&body=Title%3A%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation%0AAuthor%3A%20Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20HUDS%2C%20a%20hybrid%20AL%20strategy%20for%20domain%20adaptation%20in%20NMT%20that%20combines%0Auncertainty%20and%20diversity%20for%20sentence%20selection.%20HUDS%20computes%20uncertainty%0Ascores%20for%20unlabeled%20sentences%20and%20subsequently%20stratifies%20them.%20It%20then%0Aclusters%20sentence%20embeddings%20within%20each%20stratum%20using%20k-MEANS%20and%20computes%0Adiversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%20hybrid%20score%20that%0Acombines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%20top%20instances%20for%0Aannotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%20German-English%0Adatasets%20demonstrate%20the%20better%20performance%20of%20HUDS%20over%20other%20strong%20AL%0Abaselines.%20We%20analyze%20the%20sentence%20selection%20with%20HUDS%20and%20show%20that%20it%0Aprioritizes%20diverse%20instances%20having%20high%20model%20uncertainty%20for%20annotation%20in%0Aearly%20AL%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09259v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&entry.906535625=Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza&entry.1292438233=%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20HUDS%2C%20a%20hybrid%20AL%20strategy%20for%20domain%20adaptation%20in%20NMT%20that%20combines%0Auncertainty%20and%20diversity%20for%20sentence%20selection.%20HUDS%20computes%20uncertainty%0Ascores%20for%20unlabeled%20sentences%20and%20subsequently%20stratifies%20them.%20It%20then%0Aclusters%20sentence%20embeddings%20within%20each%20stratum%20using%20k-MEANS%20and%20computes%0Adiversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%20hybrid%20score%20that%0Acombines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%20top%20instances%20for%0Aannotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%20German-English%0Adatasets%20demonstrate%20the%20better%20performance%20of%20HUDS%20over%20other%20strong%20AL%0Abaselines.%20We%20analyze%20the%20sentence%20selection%20with%20HUDS%20and%20show%20that%20it%0Aprioritizes%20diverse%20instances%20having%20high%20model%20uncertainty%20for%20annotation%20in%0Aearly%20AL%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09259v1&entry.124074799=Read"},
{"title": "Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative\n  Privacy Risk", "author": "Zhangheng Li and Junyuan Hong and Bo Li and Zhangyang Wang", "abstract": "  While diffusion models have recently demonstrated remarkable progress in\ngenerating realistic images, privacy risks also arise: published models or APIs\ncould generate training images and thus leak privacy-sensitive training\ninformation. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that\nfine-tuning the pre-trained models with manipulated data can amplify the\nexisting privacy risks. We demonstrate that S2L could occur in various standard\nfine-tuning strategies for diffusion models, including concept-injection\nmethods (DreamBooth and Textual Inversion) and parameter-efficient methods\n(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L\ncan amplify the state-of-the-art membership inference attack (MIA) on diffusion\nmodels by $5.4\\%$ (absolute difference) AUC and can increase extracted private\nsamples from almost $0$ samples to $16.3$ samples on average per target domain.\nThis discovery underscores that the privacy risk with diffusion models is even\nmore severe than previously recognized. Codes are available at\nhttps://github.com/VITA-Group/Shake-to-Leak.\n", "link": "http://arxiv.org/abs/2403.09450v1", "date": "2024-03-14", "relevancy": 2.1606, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5641}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5358}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Shake%20to%20Leak%3A%20Fine-tuning%20Diffusion%20Models%20Can%20Amplify%20the%20Generative%0A%20%20Privacy%20Risk&body=Title%3A%20Shake%20to%20Leak%3A%20Fine-tuning%20Diffusion%20Models%20Can%20Amplify%20the%20Generative%0A%20%20Privacy%20Risk%0AAuthor%3A%20Zhangheng%20Li%20and%20Junyuan%20Hong%20and%20Bo%20Li%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%20While%20diffusion%20models%20have%20recently%20demonstrated%20remarkable%20progress%20in%0Agenerating%20realistic%20images%2C%20privacy%20risks%20also%20arise%3A%20published%20models%20or%20APIs%0Acould%20generate%20training%20images%20and%20thus%20leak%20privacy-sensitive%20training%0Ainformation.%20In%20this%20paper%2C%20we%20reveal%20a%20new%20risk%2C%20Shake-to-Leak%20%28S2L%29%2C%20that%0Afine-tuning%20the%20pre-trained%20models%20with%20manipulated%20data%20can%20amplify%20the%0Aexisting%20privacy%20risks.%20We%20demonstrate%20that%20S2L%20could%20occur%20in%20various%20standard%0Afine-tuning%20strategies%20for%20diffusion%20models%2C%20including%20concept-injection%0Amethods%20%28DreamBooth%20and%20Textual%20Inversion%29%20and%20parameter-efficient%20methods%0A%28LoRA%20and%20Hypernetwork%29%2C%20as%20well%20as%20their%20combinations.%20In%20the%20worst%20case%2C%20S2L%0Acan%20amplify%20the%20state-of-the-art%20membership%20inference%20attack%20%28MIA%29%20on%20diffusion%0Amodels%20by%20%245.4%5C%25%24%20%28absolute%20difference%29%20AUC%20and%20can%20increase%20extracted%20private%0Asamples%20from%20almost%20%240%24%20samples%20to%20%2416.3%24%20samples%20on%20average%20per%20target%20domain.%0AThis%20discovery%20underscores%20that%20the%20privacy%20risk%20with%20diffusion%20models%20is%20even%0Amore%20severe%20than%20previously%20recognized.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/VITA-Group/Shake-to-Leak.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09450v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shake%20to%20Leak%3A%20Fine-tuning%20Diffusion%20Models%20Can%20Amplify%20the%20Generative%0A%20%20Privacy%20Risk&entry.906535625=Zhangheng%20Li%20and%20Junyuan%20Hong%20and%20Bo%20Li%20and%20Zhangyang%20Wang&entry.1292438233=%20%20While%20diffusion%20models%20have%20recently%20demonstrated%20remarkable%20progress%20in%0Agenerating%20realistic%20images%2C%20privacy%20risks%20also%20arise%3A%20published%20models%20or%20APIs%0Acould%20generate%20training%20images%20and%20thus%20leak%20privacy-sensitive%20training%0Ainformation.%20In%20this%20paper%2C%20we%20reveal%20a%20new%20risk%2C%20Shake-to-Leak%20%28S2L%29%2C%20that%0Afine-tuning%20the%20pre-trained%20models%20with%20manipulated%20data%20can%20amplify%20the%0Aexisting%20privacy%20risks.%20We%20demonstrate%20that%20S2L%20could%20occur%20in%20various%20standard%0Afine-tuning%20strategies%20for%20diffusion%20models%2C%20including%20concept-injection%0Amethods%20%28DreamBooth%20and%20Textual%20Inversion%29%20and%20parameter-efficient%20methods%0A%28LoRA%20and%20Hypernetwork%29%2C%20as%20well%20as%20their%20combinations.%20In%20the%20worst%20case%2C%20S2L%0Acan%20amplify%20the%20state-of-the-art%20membership%20inference%20attack%20%28MIA%29%20on%20diffusion%0Amodels%20by%20%245.4%5C%25%24%20%28absolute%20difference%29%20AUC%20and%20can%20increase%20extracted%20private%0Asamples%20from%20almost%20%240%24%20samples%20to%20%2416.3%24%20samples%20on%20average%20per%20target%20domain.%0AThis%20discovery%20underscores%20that%20the%20privacy%20risk%20with%20diffusion%20models%20is%20even%0Amore%20severe%20than%20previously%20recognized.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/VITA-Group/Shake-to-Leak.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09450v1&entry.124074799=Read"},
{"title": "Efficient Transferability Assessment for Selection of Pre-trained\n  Detectors", "author": "Zhao Wang and Aoxue Li and Zhenguo Li and Qi Dou", "abstract": "  Large-scale pre-training followed by downstream fine-tuning is an effective\nsolution for transferring deep-learning-based models. Since finetuning all\npossible pre-trained models is computational costly, we aim to predict the\ntransferability performance of these pre-trained models in a computational\nefficient manner. Different from previous work that seek out suitable models\nfor downstream classification and segmentation tasks, this paper studies the\nefficient transferability assessment of pre-trained object detectors. To this\nend, we build up a detector transferability benchmark which contains a large\nand diverse zoo of pre-trained detectors with various architectures, source\ndatasets and training schemes. Given this zoo, we adopt 7 target datasets from\n5 diverse domains as the downstream target tasks for evaluation. Further, we\npropose to assess classification and regression sub-tasks simultaneously in a\nunified framework. Additionally, we design a complementary metric for\nevaluating tasks with varying objects. Experimental results demonstrate that\nour method outperforms other state-of-the-art approaches in assessing\ntransferability under different target domains while efficiently reducing\nwall-clock time 32$\\times$ and requires a mere 5.2\\% memory footprint compared\nto brute-force fine-tuning of all pre-trained detectors.\n", "link": "http://arxiv.org/abs/2403.09432v1", "date": "2024-03-14", "relevancy": 2.1528, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5562}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5211}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Efficient%20Transferability%20Assessment%20for%20Selection%20of%20Pre-trained%0A%20%20Detectors&body=Title%3A%20Efficient%20Transferability%20Assessment%20for%20Selection%20of%20Pre-trained%0A%20%20Detectors%0AAuthor%3A%20Zhao%20Wang%20and%20Aoxue%20Li%20and%20Zhenguo%20Li%20and%20Qi%20Dou%0AAbstract%3A%20%20%20Large-scale%20pre-training%20followed%20by%20downstream%20fine-tuning%20is%20an%20effective%0Asolution%20for%20transferring%20deep-learning-based%20models.%20Since%20finetuning%20all%0Apossible%20pre-trained%20models%20is%20computational%20costly%2C%20we%20aim%20to%20predict%20the%0Atransferability%20performance%20of%20these%20pre-trained%20models%20in%20a%20computational%0Aefficient%20manner.%20Different%20from%20previous%20work%20that%20seek%20out%20suitable%20models%0Afor%20downstream%20classification%20and%20segmentation%20tasks%2C%20this%20paper%20studies%20the%0Aefficient%20transferability%20assessment%20of%20pre-trained%20object%20detectors.%20To%20this%0Aend%2C%20we%20build%20up%20a%20detector%20transferability%20benchmark%20which%20contains%20a%20large%0Aand%20diverse%20zoo%20of%20pre-trained%20detectors%20with%20various%20architectures%2C%20source%0Adatasets%20and%20training%20schemes.%20Given%20this%20zoo%2C%20we%20adopt%207%20target%20datasets%20from%0A5%20diverse%20domains%20as%20the%20downstream%20target%20tasks%20for%20evaluation.%20Further%2C%20we%0Apropose%20to%20assess%20classification%20and%20regression%20sub-tasks%20simultaneously%20in%20a%0Aunified%20framework.%20Additionally%2C%20we%20design%20a%20complementary%20metric%20for%0Aevaluating%20tasks%20with%20varying%20objects.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20approaches%20in%20assessing%0Atransferability%20under%20different%20target%20domains%20while%20efficiently%20reducing%0Awall-clock%20time%2032%24%5Ctimes%24%20and%20requires%20a%20mere%205.2%5C%25%20memory%20footprint%20compared%0Ato%20brute-force%20fine-tuning%20of%20all%20pre-trained%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09432v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Transferability%20Assessment%20for%20Selection%20of%20Pre-trained%0A%20%20Detectors&entry.906535625=Zhao%20Wang%20and%20Aoxue%20Li%20and%20Zhenguo%20Li%20and%20Qi%20Dou&entry.1292438233=%20%20Large-scale%20pre-training%20followed%20by%20downstream%20fine-tuning%20is%20an%20effective%0Asolution%20for%20transferring%20deep-learning-based%20models.%20Since%20finetuning%20all%0Apossible%20pre-trained%20models%20is%20computational%20costly%2C%20we%20aim%20to%20predict%20the%0Atransferability%20performance%20of%20these%20pre-trained%20models%20in%20a%20computational%0Aefficient%20manner.%20Different%20from%20previous%20work%20that%20seek%20out%20suitable%20models%0Afor%20downstream%20classification%20and%20segmentation%20tasks%2C%20this%20paper%20studies%20the%0Aefficient%20transferability%20assessment%20of%20pre-trained%20object%20detectors.%20To%20this%0Aend%2C%20we%20build%20up%20a%20detector%20transferability%20benchmark%20which%20contains%20a%20large%0Aand%20diverse%20zoo%20of%20pre-trained%20detectors%20with%20various%20architectures%2C%20source%0Adatasets%20and%20training%20schemes.%20Given%20this%20zoo%2C%20we%20adopt%207%20target%20datasets%20from%0A5%20diverse%20domains%20as%20the%20downstream%20target%20tasks%20for%20evaluation.%20Further%2C%20we%0Apropose%20to%20assess%20classification%20and%20regression%20sub-tasks%20simultaneously%20in%20a%0Aunified%20framework.%20Additionally%2C%20we%20design%20a%20complementary%20metric%20for%0Aevaluating%20tasks%20with%20varying%20objects.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20approaches%20in%20assessing%0Atransferability%20under%20different%20target%20domains%20while%20efficiently%20reducing%0Awall-clock%20time%2032%24%5Ctimes%24%20and%20requires%20a%20mere%205.2%5C%25%20memory%20footprint%20compared%0Ato%20brute-force%20fine-tuning%20of%20all%20pre-trained%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09432v1&entry.124074799=Read"},
{"title": "CacheGen: Fast Context Loading for Language Model Applications via KV\n  Cache Streaming", "author": "Yuhan Liu and Hanchen Li and Yihua Cheng and Siddhant Ray and Yuyang Huang and Qizheng Zhang and Kuntai Du and Jiayi Yao and Shan Lu and Ganesh Ananthanarayanan and Michael Maire and Henry Hoffmann and Ari Holtzman and Junchen Jiang", "abstract": "  As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.\n", "link": "http://arxiv.org/abs/2310.07240v3", "date": "2024-03-14", "relevancy": 2.1513, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4272}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4262}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CacheGen%3A%20Fast%20Context%20Loading%20for%20Language%20Model%20Applications%20via%20KV%0A%20%20Cache%20Streaming&body=Title%3A%20CacheGen%3A%20Fast%20Context%20Loading%20for%20Language%20Model%20Applications%20via%20KV%0A%20%20Cache%20Streaming%0AAuthor%3A%20Yuhan%20Liu%20and%20Hanchen%20Li%20and%20Yihua%20Cheng%20and%20Siddhant%20Ray%20and%20Yuyang%20Huang%20and%20Qizheng%20Zhang%20and%20Kuntai%20Du%20and%20Jiayi%20Yao%20and%20Shan%20Lu%20and%20Ganesh%20Ananthanarayanan%20and%20Michael%20Maire%20and%20Henry%20Hoffmann%20and%20Ari%20Holtzman%20and%20Junchen%20Jiang%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20take%20on%20complex%20tasks%2C%20their%20inputs%20are%0Asupplemented%20with%20longer%20contexts%20that%20incorporate%20domain%20knowledge%20or%0Auser-specific%20information.%20Yet%20using%20long%20contexts%20poses%20a%20challenge%20for%0Aresponsive%20LLM%20systems%2C%20as%20nothing%20can%20be%20generated%20until%20the%20whole%20context%20is%0Aprocessed%20by%20the%20LLM.%20While%20the%20context-processing%20delay%20can%20be%20reduced%20by%0Areusing%20the%20KV%20cache%20of%20a%20context%20across%20different%20inputs%2C%20fetching%20the%20KV%0Acache%2C%20which%20contains%20large%20tensors%2C%20over%20the%20network%20can%20cause%20extra%20network%0Adelays.%0A%20%20CacheGen%20is%20a%20fast%20context-loading%20module%20for%20LLM%20systems.%20First%2C%20CacheGen%0Auses%20a%20custom%20tensor%20encoder%2C%20which%20embraces%20KV%20cache%27s%20distributional%0Aproperties%2C%20to%20encode%20a%20KV%20cache%20into%20more%20compact%20bitstream%20representations%0Awith%20negligible%20encoding/decoding%20overhead.%20This%20reduces%20the%20bandwidth%20demand%0Ato%20fetch%20the%20KV%20cache.%20Second%2C%20to%20maintain%20low%20context-loading%20delay%20and%20high%0Ageneration%20quality%2C%20CacheGen%20adapts%20the%20streaming%20strategies%20to%20cope%20with%0Achanges%20in%20available%20bandwidth.%20When%20available%20bandwidth%20drops%2C%20CacheGen%20may%0Araise%20the%20compression%20level%20for%20a%20part%20of%20the%20context%20or%20choose%20to%20recompute%0Aits%20KV%20cache%20on%20the%20fly.%20We%20test%20CacheGen%20on%20four%20popular%20LLMs%20of%20various%20sizes%0Aand%20four%20datasets%20%28662%20contexts%20in%20total%29.%20Compared%20to%20the%20recent%20systems%20that%0Areuse%20the%20KV%20cache%2C%20CacheGen%20reduces%20the%20KV%20cache%20size%20by%203.7-4.3x%20and%20the%0Atotal%20delay%20in%20fetching%20and%20processing%20contexts%20by%202.7-3.2x%20while%20having%0Anegligible%20impact%20on%20the%20LLM%20response%20quality%20in%20accuracy%20or%20perplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07240v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CacheGen%3A%20Fast%20Context%20Loading%20for%20Language%20Model%20Applications%20via%20KV%0A%20%20Cache%20Streaming&entry.906535625=Yuhan%20Liu%20and%20Hanchen%20Li%20and%20Yihua%20Cheng%20and%20Siddhant%20Ray%20and%20Yuyang%20Huang%20and%20Qizheng%20Zhang%20and%20Kuntai%20Du%20and%20Jiayi%20Yao%20and%20Shan%20Lu%20and%20Ganesh%20Ananthanarayanan%20and%20Michael%20Maire%20and%20Henry%20Hoffmann%20and%20Ari%20Holtzman%20and%20Junchen%20Jiang&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20take%20on%20complex%20tasks%2C%20their%20inputs%20are%0Asupplemented%20with%20longer%20contexts%20that%20incorporate%20domain%20knowledge%20or%0Auser-specific%20information.%20Yet%20using%20long%20contexts%20poses%20a%20challenge%20for%0Aresponsive%20LLM%20systems%2C%20as%20nothing%20can%20be%20generated%20until%20the%20whole%20context%20is%0Aprocessed%20by%20the%20LLM.%20While%20the%20context-processing%20delay%20can%20be%20reduced%20by%0Areusing%20the%20KV%20cache%20of%20a%20context%20across%20different%20inputs%2C%20fetching%20the%20KV%0Acache%2C%20which%20contains%20large%20tensors%2C%20over%20the%20network%20can%20cause%20extra%20network%0Adelays.%0A%20%20CacheGen%20is%20a%20fast%20context-loading%20module%20for%20LLM%20systems.%20First%2C%20CacheGen%0Auses%20a%20custom%20tensor%20encoder%2C%20which%20embraces%20KV%20cache%27s%20distributional%0Aproperties%2C%20to%20encode%20a%20KV%20cache%20into%20more%20compact%20bitstream%20representations%0Awith%20negligible%20encoding/decoding%20overhead.%20This%20reduces%20the%20bandwidth%20demand%0Ato%20fetch%20the%20KV%20cache.%20Second%2C%20to%20maintain%20low%20context-loading%20delay%20and%20high%0Ageneration%20quality%2C%20CacheGen%20adapts%20the%20streaming%20strategies%20to%20cope%20with%0Achanges%20in%20available%20bandwidth.%20When%20available%20bandwidth%20drops%2C%20CacheGen%20may%0Araise%20the%20compression%20level%20for%20a%20part%20of%20the%20context%20or%20choose%20to%20recompute%0Aits%20KV%20cache%20on%20the%20fly.%20We%20test%20CacheGen%20on%20four%20popular%20LLMs%20of%20various%20sizes%0Aand%20four%20datasets%20%28662%20contexts%20in%20total%29.%20Compared%20to%20the%20recent%20systems%20that%0Areuse%20the%20KV%20cache%2C%20CacheGen%20reduces%20the%20KV%20cache%20size%20by%203.7-4.3x%20and%20the%0Atotal%20delay%20in%20fetching%20and%20processing%20contexts%20by%202.7-3.2x%20while%20having%0Anegligible%20impact%20on%20the%20LLM%20response%20quality%20in%20accuracy%20or%20perplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07240v3&entry.124074799=Read"},
{"title": "Kernelized Concept Erasure", "author": "Shauli Ravfogel and Francisco Vargas and Yoav Goldberg and Ryan Cotterell", "abstract": "  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n", "link": "http://arxiv.org/abs/2201.12191v5", "date": "2024-03-14", "relevancy": 2.1488, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4293}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4253}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kernelized%20Concept%20Erasure&body=Title%3A%20Kernelized%20Concept%20Erasure%0AAuthor%3A%20Shauli%20Ravfogel%20and%20Francisco%20Vargas%20and%20Yoav%20Goldberg%20and%20Ryan%20Cotterell%0AAbstract%3A%20%20%20The%20representation%20space%20of%20neural%20models%20for%20textual%20data%20emerges%20in%20an%0Aunsupervised%20manner%20during%20training.%20Understanding%20how%20those%20representations%0Aencode%20human-interpretable%20concepts%20is%20a%20fundamental%20problem.%20One%20prominent%0Aapproach%20for%20the%20identification%20of%20concepts%20in%20neural%20representations%20is%0Asearching%20for%20a%20linear%20subspace%20whose%20erasure%20prevents%20the%20prediction%20of%20the%0Aconcept%20from%20the%20representations.%20However%2C%20while%20many%20linear%20erasure%20algorithms%0Aare%20tractable%20and%20interpretable%2C%20neural%20networks%20do%20not%20necessarily%20represent%0Aconcepts%20in%20a%20linear%20manner.%20To%20identify%20non-linearly%20encoded%20concepts%2C%20we%0Apropose%20a%20kernelization%20of%20a%20linear%20minimax%20game%20for%20concept%20erasure.%20We%0Ademonstrate%20that%20it%20is%20possible%20to%20prevent%20specific%20non-linear%20adversaries%20from%0Apredicting%20the%20concept.%20However%2C%20the%20protection%20does%20not%20transfer%20to%20different%0Anonlinear%20adversaries.%20Therefore%2C%20exhaustively%20erasing%20a%20non-linearly%20encoded%0Aconcept%20remains%20an%20open%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.12191v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernelized%20Concept%20Erasure&entry.906535625=Shauli%20Ravfogel%20and%20Francisco%20Vargas%20and%20Yoav%20Goldberg%20and%20Ryan%20Cotterell&entry.1292438233=%20%20The%20representation%20space%20of%20neural%20models%20for%20textual%20data%20emerges%20in%20an%0Aunsupervised%20manner%20during%20training.%20Understanding%20how%20those%20representations%0Aencode%20human-interpretable%20concepts%20is%20a%20fundamental%20problem.%20One%20prominent%0Aapproach%20for%20the%20identification%20of%20concepts%20in%20neural%20representations%20is%0Asearching%20for%20a%20linear%20subspace%20whose%20erasure%20prevents%20the%20prediction%20of%20the%0Aconcept%20from%20the%20representations.%20However%2C%20while%20many%20linear%20erasure%20algorithms%0Aare%20tractable%20and%20interpretable%2C%20neural%20networks%20do%20not%20necessarily%20represent%0Aconcepts%20in%20a%20linear%20manner.%20To%20identify%20non-linearly%20encoded%20concepts%2C%20we%0Apropose%20a%20kernelization%20of%20a%20linear%20minimax%20game%20for%20concept%20erasure.%20We%0Ademonstrate%20that%20it%20is%20possible%20to%20prevent%20specific%20non-linear%20adversaries%20from%0Apredicting%20the%20concept.%20However%2C%20the%20protection%20does%20not%20transfer%20to%20different%0Anonlinear%20adversaries.%20Therefore%2C%20exhaustively%20erasing%20a%20non-linearly%20encoded%0Aconcept%20remains%20an%20open%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.12191v5&entry.124074799=Read"},
{"title": "Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph", "author": "Donglin Di and Jiahui Yang and Chaofan Luo and Zhou Xue and Wei Chen and Xun Yang and Yue Gao", "abstract": "  Text-to-3D generation represents an exciting field that has seen rapid\nadvancements, facilitating the transformation of textual descriptions into\ndetailed 3D models. However, current progress often neglects the intricate\nhigh-order correlation of geometry and texture within 3D objects, leading to\nchallenges such as over-smoothness, over-saturation and the Janus problem. In\nthis work, we propose a method named ``3D Gaussian Generation via Hypergraph\n(Hyper-3DG)'', designed to capture the sophisticated high-order correlations\npresent within 3D objects. Our framework is anchored by a well-established\nmainflow and an essential module, named ``Geometry and Texture Hypergraph\nRefiner (HGRefiner)''. This module not only refines the representation of 3D\nGaussians but also accelerates the update process of these 3D Gaussians by\nconducting the Patch-3DGS Hypergraph Learning on both explicit attributes and\nlatent visual features. Our framework allows for the production of finely\ngenerated 3D objects within a cohesive optimization, effectively circumventing\ndegradation. Extensive experimentation has shown that our proposed method\nsignificantly enhances the quality of 3D generation while incurring no\nadditional computational overhead for the underlying framework. (Project code:\nhttps://github.com/yjhboy/Hyper3DG)\n", "link": "http://arxiv.org/abs/2403.09236v1", "date": "2024-03-14", "relevancy": 2.1488, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5595}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hyper-3DG%3A%20Text-to-3D%20Gaussian%20Generation%20via%20Hypergraph&body=Title%3A%20Hyper-3DG%3A%20Text-to-3D%20Gaussian%20Generation%20via%20Hypergraph%0AAuthor%3A%20Donglin%20Di%20and%20Jiahui%20Yang%20and%20Chaofan%20Luo%20and%20Zhou%20Xue%20and%20Wei%20Chen%20and%20Xun%20Yang%20and%20Yue%20Gao%0AAbstract%3A%20%20%20Text-to-3D%20generation%20represents%20an%20exciting%20field%20that%20has%20seen%20rapid%0Aadvancements%2C%20facilitating%20the%20transformation%20of%20textual%20descriptions%20into%0Adetailed%203D%20models.%20However%2C%20current%20progress%20often%20neglects%20the%20intricate%0Ahigh-order%20correlation%20of%20geometry%20and%20texture%20within%203D%20objects%2C%20leading%20to%0Achallenges%20such%20as%20over-smoothness%2C%20over-saturation%20and%20the%20Janus%20problem.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20named%20%60%603D%20Gaussian%20Generation%20via%20Hypergraph%0A%28Hyper-3DG%29%27%27%2C%20designed%20to%20capture%20the%20sophisticated%20high-order%20correlations%0Apresent%20within%203D%20objects.%20Our%20framework%20is%20anchored%20by%20a%20well-established%0Amainflow%20and%20an%20essential%20module%2C%20named%20%60%60Geometry%20and%20Texture%20Hypergraph%0ARefiner%20%28HGRefiner%29%27%27.%20This%20module%20not%20only%20refines%20the%20representation%20of%203D%0AGaussians%20but%20also%20accelerates%20the%20update%20process%20of%20these%203D%20Gaussians%20by%0Aconducting%20the%20Patch-3DGS%20Hypergraph%20Learning%20on%20both%20explicit%20attributes%20and%0Alatent%20visual%20features.%20Our%20framework%20allows%20for%20the%20production%20of%20finely%0Agenerated%203D%20objects%20within%20a%20cohesive%20optimization%2C%20effectively%20circumventing%0Adegradation.%20Extensive%20experimentation%20has%20shown%20that%20our%20proposed%20method%0Asignificantly%20enhances%20the%20quality%20of%203D%20generation%20while%20incurring%20no%0Aadditional%20computational%20overhead%20for%20the%20underlying%20framework.%20%28Project%20code%3A%0Ahttps%3A//github.com/yjhboy/Hyper3DG%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09236v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyper-3DG%3A%20Text-to-3D%20Gaussian%20Generation%20via%20Hypergraph&entry.906535625=Donglin%20Di%20and%20Jiahui%20Yang%20and%20Chaofan%20Luo%20and%20Zhou%20Xue%20and%20Wei%20Chen%20and%20Xun%20Yang%20and%20Yue%20Gao&entry.1292438233=%20%20Text-to-3D%20generation%20represents%20an%20exciting%20field%20that%20has%20seen%20rapid%0Aadvancements%2C%20facilitating%20the%20transformation%20of%20textual%20descriptions%20into%0Adetailed%203D%20models.%20However%2C%20current%20progress%20often%20neglects%20the%20intricate%0Ahigh-order%20correlation%20of%20geometry%20and%20texture%20within%203D%20objects%2C%20leading%20to%0Achallenges%20such%20as%20over-smoothness%2C%20over-saturation%20and%20the%20Janus%20problem.%20In%0Athis%20work%2C%20we%20propose%20a%20method%20named%20%60%603D%20Gaussian%20Generation%20via%20Hypergraph%0A%28Hyper-3DG%29%27%27%2C%20designed%20to%20capture%20the%20sophisticated%20high-order%20correlations%0Apresent%20within%203D%20objects.%20Our%20framework%20is%20anchored%20by%20a%20well-established%0Amainflow%20and%20an%20essential%20module%2C%20named%20%60%60Geometry%20and%20Texture%20Hypergraph%0ARefiner%20%28HGRefiner%29%27%27.%20This%20module%20not%20only%20refines%20the%20representation%20of%203D%0AGaussians%20but%20also%20accelerates%20the%20update%20process%20of%20these%203D%20Gaussians%20by%0Aconducting%20the%20Patch-3DGS%20Hypergraph%20Learning%20on%20both%20explicit%20attributes%20and%0Alatent%20visual%20features.%20Our%20framework%20allows%20for%20the%20production%20of%20finely%0Agenerated%203D%20objects%20within%20a%20cohesive%20optimization%2C%20effectively%20circumventing%0Adegradation.%20Extensive%20experimentation%20has%20shown%20that%20our%20proposed%20method%0Asignificantly%20enhances%20the%20quality%20of%203D%20generation%20while%20incurring%20no%0Aadditional%20computational%20overhead%20for%20the%20underlying%20framework.%20%28Project%20code%3A%0Ahttps%3A//github.com/yjhboy/Hyper3DG%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09236v1&entry.124074799=Read"},
{"title": "DiffSF: Diffusion Models for Scene Flow Estimation", "author": "Yushan Zhang and Bastian Wandt and Maria Magnusson and Michael Felsberg", "abstract": "  Scene flow estimation is an essential ingredient for a variety of real-world\napplications, especially for autonomous agents, such as self-driving cars and\nrobots. While recent scene flow estimation approaches achieve a reasonable\naccuracy, their applicability to real-world systems additionally benefits from\na reliability measure. Aiming at improving accuracy while additionally\nproviding an estimate for uncertainty, we propose DiffSF that combines\ntransformer-based scene flow estimation with denoising diffusion models. In the\ndiffusion process, the ground truth scene flow vector field is gradually\nperturbed by adding Gaussian noise. In the reverse process, starting from\nrandomly sampled Gaussian noise, the scene flow vector field prediction is\nrecovered by conditioning on a source and a target point cloud. We show that\nthe diffusion process greatly increases the robustness of predictions compared\nto prior approaches resulting in state-of-the-art performance on standard scene\nflow estimation benchmarks. Moreover, by sampling multiple times with different\ninitial states, the denoising process predicts multiple hypotheses, which\nenables measuring the output uncertainty, allowing our approach to detect a\nmajority of the inaccurate predictions. The code is available at\nhttps://github.com/ZhangYushan3/DiffSF.\n", "link": "http://arxiv.org/abs/2403.05327v2", "date": "2024-03-14", "relevancy": 2.1469, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5339}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiffSF%3A%20Diffusion%20Models%20for%20Scene%20Flow%20Estimation&body=Title%3A%20DiffSF%3A%20Diffusion%20Models%20for%20Scene%20Flow%20Estimation%0AAuthor%3A%20Yushan%20Zhang%20and%20Bastian%20Wandt%20and%20Maria%20Magnusson%20and%20Michael%20Felsberg%0AAbstract%3A%20%20%20Scene%20flow%20estimation%20is%20an%20essential%20ingredient%20for%20a%20variety%20of%20real-world%0Aapplications%2C%20especially%20for%20autonomous%20agents%2C%20such%20as%20self-driving%20cars%20and%0Arobots.%20While%20recent%20scene%20flow%20estimation%20approaches%20achieve%20a%20reasonable%0Aaccuracy%2C%20their%20applicability%20to%20real-world%20systems%20additionally%20benefits%20from%0Aa%20reliability%20measure.%20Aiming%20at%20improving%20accuracy%20while%20additionally%0Aproviding%20an%20estimate%20for%20uncertainty%2C%20we%20propose%20DiffSF%20that%20combines%0Atransformer-based%20scene%20flow%20estimation%20with%20denoising%20diffusion%20models.%20In%20the%0Adiffusion%20process%2C%20the%20ground%20truth%20scene%20flow%20vector%20field%20is%20gradually%0Aperturbed%20by%20adding%20Gaussian%20noise.%20In%20the%20reverse%20process%2C%20starting%20from%0Arandomly%20sampled%20Gaussian%20noise%2C%20the%20scene%20flow%20vector%20field%20prediction%20is%0Arecovered%20by%20conditioning%20on%20a%20source%20and%20a%20target%20point%20cloud.%20We%20show%20that%0Athe%20diffusion%20process%20greatly%20increases%20the%20robustness%20of%20predictions%20compared%0Ato%20prior%20approaches%20resulting%20in%20state-of-the-art%20performance%20on%20standard%20scene%0Aflow%20estimation%20benchmarks.%20Moreover%2C%20by%20sampling%20multiple%20times%20with%20different%0Ainitial%20states%2C%20the%20denoising%20process%20predicts%20multiple%20hypotheses%2C%20which%0Aenables%20measuring%20the%20output%20uncertainty%2C%20allowing%20our%20approach%20to%20detect%20a%0Amajority%20of%20the%20inaccurate%20predictions.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhangYushan3/DiffSF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05327v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffSF%3A%20Diffusion%20Models%20for%20Scene%20Flow%20Estimation&entry.906535625=Yushan%20Zhang%20and%20Bastian%20Wandt%20and%20Maria%20Magnusson%20and%20Michael%20Felsberg&entry.1292438233=%20%20Scene%20flow%20estimation%20is%20an%20essential%20ingredient%20for%20a%20variety%20of%20real-world%0Aapplications%2C%20especially%20for%20autonomous%20agents%2C%20such%20as%20self-driving%20cars%20and%0Arobots.%20While%20recent%20scene%20flow%20estimation%20approaches%20achieve%20a%20reasonable%0Aaccuracy%2C%20their%20applicability%20to%20real-world%20systems%20additionally%20benefits%20from%0Aa%20reliability%20measure.%20Aiming%20at%20improving%20accuracy%20while%20additionally%0Aproviding%20an%20estimate%20for%20uncertainty%2C%20we%20propose%20DiffSF%20that%20combines%0Atransformer-based%20scene%20flow%20estimation%20with%20denoising%20diffusion%20models.%20In%20the%0Adiffusion%20process%2C%20the%20ground%20truth%20scene%20flow%20vector%20field%20is%20gradually%0Aperturbed%20by%20adding%20Gaussian%20noise.%20In%20the%20reverse%20process%2C%20starting%20from%0Arandomly%20sampled%20Gaussian%20noise%2C%20the%20scene%20flow%20vector%20field%20prediction%20is%0Arecovered%20by%20conditioning%20on%20a%20source%20and%20a%20target%20point%20cloud.%20We%20show%20that%0Athe%20diffusion%20process%20greatly%20increases%20the%20robustness%20of%20predictions%20compared%0Ato%20prior%20approaches%20resulting%20in%20state-of-the-art%20performance%20on%20standard%20scene%0Aflow%20estimation%20benchmarks.%20Moreover%2C%20by%20sampling%20multiple%20times%20with%20different%0Ainitial%20states%2C%20the%20denoising%20process%20predicts%20multiple%20hypotheses%2C%20which%0Aenables%20measuring%20the%20output%20uncertainty%2C%20allowing%20our%20approach%20to%20detect%20a%0Amajority%20of%20the%20inaccurate%20predictions.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhangYushan3/DiffSF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05327v2&entry.124074799=Read"},
{"title": "Semantic Residual Prompts for Continual Learning", "author": "Martin Menabue and Emanuele Frascaroli and Matteo Boschini and Enver Sangineto and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and focus training on a few parameter vectors termed prompts. Most of\nthese methods organize these vectors in a pool of key-value pairs, and use the\ninput image as query to retrieve the prompts (values). However, as keys are\nlearned while tasks progress, the prompting selection strategy is itself\nsubject to catastrophic forgetting, an issue often overlooked by existing\napproaches. For instance, prompts introduced to accommodate new tasks might end\nup interfering with previously learned prompts. To make the selection strategy\nmore stable, we ask a foundational model (CLIP) to select our prompt within a\ntwo-level adaptation mechanism. Specifically, the first level leverages\nstandard textual prompts for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets.\n", "link": "http://arxiv.org/abs/2403.06870v2", "date": "2024-03-14", "relevancy": 2.1455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5059}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5027}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning&body=Title%3A%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning%0AAuthor%3A%20Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20focus%20training%20on%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%0Athese%20methods%20organize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%2C%20and%20use%20the%0Ainput%20image%20as%20query%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%0Alearned%20while%20tasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%0Asubject%20to%20catastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%0Aapproaches.%20For%20instance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%0Aup%20interfering%20with%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%0Amore%20stable%2C%20we%20ask%20a%20foundational%20model%20%28CLIP%29%20to%20select%20our%20prompt%20within%20a%0Atwo-level%20adaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%0Astandard%20textual%20prompts%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06870v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Residual%20Prompts%20for%20Continual%20Learning&entry.906535625=Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20focus%20training%20on%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%0Athese%20methods%20organize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%2C%20and%20use%20the%0Ainput%20image%20as%20query%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%0Alearned%20while%20tasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%0Asubject%20to%20catastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%0Aapproaches.%20For%20instance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%0Aup%20interfering%20with%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%0Amore%20stable%2C%20we%20ask%20a%20foundational%20model%20%28CLIP%29%20to%20select%20our%20prompt%20within%20a%0Atwo-level%20adaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%0Astandard%20textual%20prompts%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06870v2&entry.124074799=Read"},
{"title": "Score-Guided Diffusion for 3D Human Recovery", "author": "Anastasis Stathopoulos and Ligong Han and Dimitris Metaxas", "abstract": "  We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for\nsolving inverse problems for 3D human pose and shape reconstruction. These\ninverse problems involve fitting a human body model to image observations,\ntraditionally solved through optimization techniques. ScoreHMR mimics model\nfitting approaches, but alignment with the image observation is achieved\nthrough score guidance in the latent space of a diffusion model. The diffusion\nmodel is trained to capture the conditional distribution of the human model\nparameters given an input image. By guiding its denoising process with a\ntask-specific score, ScoreHMR effectively solves inverse problems for various\napplications without the need for retraining the task-agnostic diffusion model.\nWe evaluate our approach on three settings/applications. These are: (i)\nsingle-frame model fitting; (ii) reconstruction from multiple uncalibrated\nviews; (iii) reconstructing humans in video sequences. ScoreHMR consistently\noutperforms all optimization baselines on popular benchmarks across all\nsettings. We make our code and models available at the\nhttps://statho.github.io/ScoreHMR.\n", "link": "http://arxiv.org/abs/2403.09623v1", "date": "2024-03-14", "relevancy": 2.1313, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5414}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Score-Guided%20Diffusion%20for%203D%20Human%20Recovery&body=Title%3A%20Score-Guided%20Diffusion%20for%203D%20Human%20Recovery%0AAuthor%3A%20Anastasis%20Stathopoulos%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%0AAbstract%3A%20%20%20We%20present%20Score-Guided%20Human%20Mesh%20Recovery%20%28ScoreHMR%29%2C%20an%20approach%20for%0Asolving%20inverse%20problems%20for%203D%20human%20pose%20and%20shape%20reconstruction.%20These%0Ainverse%20problems%20involve%20fitting%20a%20human%20body%20model%20to%20image%20observations%2C%0Atraditionally%20solved%20through%20optimization%20techniques.%20ScoreHMR%20mimics%20model%0Afitting%20approaches%2C%20but%20alignment%20with%20the%20image%20observation%20is%20achieved%0Athrough%20score%20guidance%20in%20the%20latent%20space%20of%20a%20diffusion%20model.%20The%20diffusion%0Amodel%20is%20trained%20to%20capture%20the%20conditional%20distribution%20of%20the%20human%20model%0Aparameters%20given%20an%20input%20image.%20By%20guiding%20its%20denoising%20process%20with%20a%0Atask-specific%20score%2C%20ScoreHMR%20effectively%20solves%20inverse%20problems%20for%20various%0Aapplications%20without%20the%20need%20for%20retraining%20the%20task-agnostic%20diffusion%20model.%0AWe%20evaluate%20our%20approach%20on%20three%20settings/applications.%20These%20are%3A%20%28i%29%0Asingle-frame%20model%20fitting%3B%20%28ii%29%20reconstruction%20from%20multiple%20uncalibrated%0Aviews%3B%20%28iii%29%20reconstructing%20humans%20in%20video%20sequences.%20ScoreHMR%20consistently%0Aoutperforms%20all%20optimization%20baselines%20on%20popular%20benchmarks%20across%20all%0Asettings.%20We%20make%20our%20code%20and%20models%20available%20at%20the%0Ahttps%3A//statho.github.io/ScoreHMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-Guided%20Diffusion%20for%203D%20Human%20Recovery&entry.906535625=Anastasis%20Stathopoulos%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas&entry.1292438233=%20%20We%20present%20Score-Guided%20Human%20Mesh%20Recovery%20%28ScoreHMR%29%2C%20an%20approach%20for%0Asolving%20inverse%20problems%20for%203D%20human%20pose%20and%20shape%20reconstruction.%20These%0Ainverse%20problems%20involve%20fitting%20a%20human%20body%20model%20to%20image%20observations%2C%0Atraditionally%20solved%20through%20optimization%20techniques.%20ScoreHMR%20mimics%20model%0Afitting%20approaches%2C%20but%20alignment%20with%20the%20image%20observation%20is%20achieved%0Athrough%20score%20guidance%20in%20the%20latent%20space%20of%20a%20diffusion%20model.%20The%20diffusion%0Amodel%20is%20trained%20to%20capture%20the%20conditional%20distribution%20of%20the%20human%20model%0Aparameters%20given%20an%20input%20image.%20By%20guiding%20its%20denoising%20process%20with%20a%0Atask-specific%20score%2C%20ScoreHMR%20effectively%20solves%20inverse%20problems%20for%20various%0Aapplications%20without%20the%20need%20for%20retraining%20the%20task-agnostic%20diffusion%20model.%0AWe%20evaluate%20our%20approach%20on%20three%20settings/applications.%20These%20are%3A%20%28i%29%0Asingle-frame%20model%20fitting%3B%20%28ii%29%20reconstruction%20from%20multiple%20uncalibrated%0Aviews%3B%20%28iii%29%20reconstructing%20humans%20in%20video%20sequences.%20ScoreHMR%20consistently%0Aoutperforms%20all%20optimization%20baselines%20on%20popular%20benchmarks%20across%20all%0Asettings.%20We%20make%20our%20code%20and%20models%20available%20at%20the%0Ahttps%3A//statho.github.io/ScoreHMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09623v1&entry.124074799=Read"},
{"title": "Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation\n  with Limited Annotations", "author": "Xinyu Xiong and Churan Wang and Wenxue Li and Guanbin Li", "abstract": "  Accurate identification of breast masses is crucial in diagnosing breast\ncancer; however, it can be challenging due to their small size and being\ncamouflaged in surrounding normal glands. Worse still, it is also expensive in\nclinical practice to obtain adequate pixel-wise annotations for training deep\nneural networks. To overcome these two difficulties with one stone, we propose\na semi- and weakly-supervised learning framework for mass segmentation that\nutilizes limited strongly-labeled samples and sufficient weakly-labeled samples\nto achieve satisfactory performance. The framework consists of an auxiliary\nbranch to exclude lesion-irrelevant background areas, a segmentation branch for\nfinal prediction, and a spatial prompting module to integrate the complementary\ninformation of the two branches. We further disentangle encoded obscure\nfeatures into lesion-related and others to boost performance. Experiments on\nCBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2403.09315v1", "date": "2024-03-14", "relevancy": 2.1271, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5337}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semi-%20and%20Weakly-Supervised%20Learning%20for%20Mammogram%20Mass%20Segmentation%0A%20%20with%20Limited%20Annotations&body=Title%3A%20Semi-%20and%20Weakly-Supervised%20Learning%20for%20Mammogram%20Mass%20Segmentation%0A%20%20with%20Limited%20Annotations%0AAuthor%3A%20Xinyu%20Xiong%20and%20Churan%20Wang%20and%20Wenxue%20Li%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Accurate%20identification%20of%20breast%20masses%20is%20crucial%20in%20diagnosing%20breast%0Acancer%3B%20however%2C%20it%20can%20be%20challenging%20due%20to%20their%20small%20size%20and%20being%0Acamouflaged%20in%20surrounding%20normal%20glands.%20Worse%20still%2C%20it%20is%20also%20expensive%20in%0Aclinical%20practice%20to%20obtain%20adequate%20pixel-wise%20annotations%20for%20training%20deep%0Aneural%20networks.%20To%20overcome%20these%20two%20difficulties%20with%20one%20stone%2C%20we%20propose%0Aa%20semi-%20and%20weakly-supervised%20learning%20framework%20for%20mass%20segmentation%20that%0Autilizes%20limited%20strongly-labeled%20samples%20and%20sufficient%20weakly-labeled%20samples%0Ato%20achieve%20satisfactory%20performance.%20The%20framework%20consists%20of%20an%20auxiliary%0Abranch%20to%20exclude%20lesion-irrelevant%20background%20areas%2C%20a%20segmentation%20branch%20for%0Afinal%20prediction%2C%20and%20a%20spatial%20prompting%20module%20to%20integrate%20the%20complementary%0Ainformation%20of%20the%20two%20branches.%20We%20further%20disentangle%20encoded%20obscure%0Afeatures%20into%20lesion-related%20and%20others%20to%20boost%20performance.%20Experiments%20on%0ACBIS-DDSM%20and%20INbreast%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09315v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-%20and%20Weakly-Supervised%20Learning%20for%20Mammogram%20Mass%20Segmentation%0A%20%20with%20Limited%20Annotations&entry.906535625=Xinyu%20Xiong%20and%20Churan%20Wang%20and%20Wenxue%20Li%20and%20Guanbin%20Li&entry.1292438233=%20%20Accurate%20identification%20of%20breast%20masses%20is%20crucial%20in%20diagnosing%20breast%0Acancer%3B%20however%2C%20it%20can%20be%20challenging%20due%20to%20their%20small%20size%20and%20being%0Acamouflaged%20in%20surrounding%20normal%20glands.%20Worse%20still%2C%20it%20is%20also%20expensive%20in%0Aclinical%20practice%20to%20obtain%20adequate%20pixel-wise%20annotations%20for%20training%20deep%0Aneural%20networks.%20To%20overcome%20these%20two%20difficulties%20with%20one%20stone%2C%20we%20propose%0Aa%20semi-%20and%20weakly-supervised%20learning%20framework%20for%20mass%20segmentation%20that%0Autilizes%20limited%20strongly-labeled%20samples%20and%20sufficient%20weakly-labeled%20samples%0Ato%20achieve%20satisfactory%20performance.%20The%20framework%20consists%20of%20an%20auxiliary%0Abranch%20to%20exclude%20lesion-irrelevant%20background%20areas%2C%20a%20segmentation%20branch%20for%0Afinal%20prediction%2C%20and%20a%20spatial%20prompting%20module%20to%20integrate%20the%20complementary%0Ainformation%20of%20the%20two%20branches.%20We%20further%20disentangle%20encoded%20obscure%0Afeatures%20into%20lesion-related%20and%20others%20to%20boost%20performance.%20Experiments%20on%0ACBIS-DDSM%20and%20INbreast%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09315v1&entry.124074799=Read"},
{"title": "Compute-first optical detection for noise-resilient visual perception", "author": "Jungmin Kim and Nanfang Yu and Zongfu Yu", "abstract": "  In the context of visual perception, the optical signal from a scene is\ntransferred into the electronic domain by detectors in the form of image data,\nwhich are then processed for the extraction of visual information. In noisy and\nweak-signal environments such as thermal imaging for night vision applications,\nhowever, the performance of neural computing tasks faces a significant\nbottleneck due to the inherent degradation of data quality upon noisy\ndetection. Here, we propose a concept of optical signal processing before\ndetection to address this issue. We demonstrate that spatially redistributing\noptical signals through a properly designed linear transformer can enhance the\ndetection noise resilience of visual perception tasks, as benchmarked with the\nMNIST classification. Our idea is supported by a quantitative analysis\ndetailing the relationship between signal concentration and noise robustness,\nas well as its practical implementation in an incoherent imaging system. This\ncompute-first detection scheme can pave the way for advancing infrared machine\nvision technologies widely used for industrial and defense applications.\n", "link": "http://arxiv.org/abs/2403.09612v1", "date": "2024-03-14", "relevancy": 2.1184, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5763}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4858}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Compute-first%20optical%20detection%20for%20noise-resilient%20visual%20perception&body=Title%3A%20Compute-first%20optical%20detection%20for%20noise-resilient%20visual%20perception%0AAuthor%3A%20Jungmin%20Kim%20and%20Nanfang%20Yu%20and%20Zongfu%20Yu%0AAbstract%3A%20%20%20In%20the%20context%20of%20visual%20perception%2C%20the%20optical%20signal%20from%20a%20scene%20is%0Atransferred%20into%20the%20electronic%20domain%20by%20detectors%20in%20the%20form%20of%20image%20data%2C%0Awhich%20are%20then%20processed%20for%20the%20extraction%20of%20visual%20information.%20In%20noisy%20and%0Aweak-signal%20environments%20such%20as%20thermal%20imaging%20for%20night%20vision%20applications%2C%0Ahowever%2C%20the%20performance%20of%20neural%20computing%20tasks%20faces%20a%20significant%0Abottleneck%20due%20to%20the%20inherent%20degradation%20of%20data%20quality%20upon%20noisy%0Adetection.%20Here%2C%20we%20propose%20a%20concept%20of%20optical%20signal%20processing%20before%0Adetection%20to%20address%20this%20issue.%20We%20demonstrate%20that%20spatially%20redistributing%0Aoptical%20signals%20through%20a%20properly%20designed%20linear%20transformer%20can%20enhance%20the%0Adetection%20noise%20resilience%20of%20visual%20perception%20tasks%2C%20as%20benchmarked%20with%20the%0AMNIST%20classification.%20Our%20idea%20is%20supported%20by%20a%20quantitative%20analysis%0Adetailing%20the%20relationship%20between%20signal%20concentration%20and%20noise%20robustness%2C%0Aas%20well%20as%20its%20practical%20implementation%20in%20an%20incoherent%20imaging%20system.%20This%0Acompute-first%20detection%20scheme%20can%20pave%20the%20way%20for%20advancing%20infrared%20machine%0Avision%20technologies%20widely%20used%20for%20industrial%20and%20defense%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compute-first%20optical%20detection%20for%20noise-resilient%20visual%20perception&entry.906535625=Jungmin%20Kim%20and%20Nanfang%20Yu%20and%20Zongfu%20Yu&entry.1292438233=%20%20In%20the%20context%20of%20visual%20perception%2C%20the%20optical%20signal%20from%20a%20scene%20is%0Atransferred%20into%20the%20electronic%20domain%20by%20detectors%20in%20the%20form%20of%20image%20data%2C%0Awhich%20are%20then%20processed%20for%20the%20extraction%20of%20visual%20information.%20In%20noisy%20and%0Aweak-signal%20environments%20such%20as%20thermal%20imaging%20for%20night%20vision%20applications%2C%0Ahowever%2C%20the%20performance%20of%20neural%20computing%20tasks%20faces%20a%20significant%0Abottleneck%20due%20to%20the%20inherent%20degradation%20of%20data%20quality%20upon%20noisy%0Adetection.%20Here%2C%20we%20propose%20a%20concept%20of%20optical%20signal%20processing%20before%0Adetection%20to%20address%20this%20issue.%20We%20demonstrate%20that%20spatially%20redistributing%0Aoptical%20signals%20through%20a%20properly%20designed%20linear%20transformer%20can%20enhance%20the%0Adetection%20noise%20resilience%20of%20visual%20perception%20tasks%2C%20as%20benchmarked%20with%20the%0AMNIST%20classification.%20Our%20idea%20is%20supported%20by%20a%20quantitative%20analysis%0Adetailing%20the%20relationship%20between%20signal%20concentration%20and%20noise%20robustness%2C%0Aas%20well%20as%20its%20practical%20implementation%20in%20an%20incoherent%20imaging%20system.%20This%0Acompute-first%20detection%20scheme%20can%20pave%20the%20way%20for%20advancing%20infrared%20machine%0Avision%20technologies%20widely%20used%20for%20industrial%20and%20defense%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09612v1&entry.124074799=Read"},
{"title": "M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment", "author": "Long Nguyen-Phuoc and Renald Gaboriau and Dimitri Delacroix and Laurent Navarro", "abstract": "  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n", "link": "http://arxiv.org/abs/2403.09451v1", "date": "2024-03-14", "relevancy": 2.1156, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.525}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4847}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20M%26M%3A%20Multimodal-Multitask%20Model%20Integrating%20Audiovisual%20Cues%20in%0A%20%20Cognitive%20Load%20Assessment&body=Title%3A%20M%26M%3A%20Multimodal-Multitask%20Model%20Integrating%20Audiovisual%20Cues%20in%0A%20%20Cognitive%20Load%20Assessment%0AAuthor%3A%20Long%20Nguyen-Phuoc%20and%20Renald%20Gaboriau%20and%20Dimitri%20Delacroix%20and%20Laurent%20Navarro%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20M%26M%20model%2C%20a%20novel%20multimodal-multitask%20learning%0Aframework%2C%20applied%20to%20the%20AVCAffe%20dataset%20for%20cognitive%20load%20assessment%20%28CLA%29.%0AM%26M%20uniquely%20integrates%20audiovisual%20cues%20through%20a%20dual-pathway%20architecture%2C%0Afeaturing%20specialized%20streams%20for%20audio%20and%20video%20inputs.%20A%20key%20innovation%20lies%0Ain%20its%20cross-modality%20multihead%20attention%20mechanism%2C%20fusing%20the%20different%0Amodalities%20for%20synchronized%20multitasking.%20Another%20notable%20feature%20is%20the%0Amodel%27s%20three%20specialized%20branches%2C%20each%20tailored%20to%20a%20specific%20cognitive%20load%0Alabel%2C%20enabling%20nuanced%2C%20task-specific%20analysis.%20While%20it%20shows%20modest%0Aperformance%20compared%20to%20the%20AVCAffe%27s%20single-task%20baseline%2C%20M%5C%26M%20demonstrates%20a%0Apromising%20framework%20for%20integrated%20multimodal%20processing.%20This%20work%20paves%20the%0Away%20for%20future%20enhancements%20in%20multimodal-multitask%20learning%20systems%2C%0Aemphasizing%20the%20fusion%20of%20diverse%20data%20types%20for%20complex%20task%20handling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09451v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%26M%3A%20Multimodal-Multitask%20Model%20Integrating%20Audiovisual%20Cues%20in%0A%20%20Cognitive%20Load%20Assessment&entry.906535625=Long%20Nguyen-Phuoc%20and%20Renald%20Gaboriau%20and%20Dimitri%20Delacroix%20and%20Laurent%20Navarro&entry.1292438233=%20%20This%20paper%20introduces%20the%20M%26M%20model%2C%20a%20novel%20multimodal-multitask%20learning%0Aframework%2C%20applied%20to%20the%20AVCAffe%20dataset%20for%20cognitive%20load%20assessment%20%28CLA%29.%0AM%26M%20uniquely%20integrates%20audiovisual%20cues%20through%20a%20dual-pathway%20architecture%2C%0Afeaturing%20specialized%20streams%20for%20audio%20and%20video%20inputs.%20A%20key%20innovation%20lies%0Ain%20its%20cross-modality%20multihead%20attention%20mechanism%2C%20fusing%20the%20different%0Amodalities%20for%20synchronized%20multitasking.%20Another%20notable%20feature%20is%20the%0Amodel%27s%20three%20specialized%20branches%2C%20each%20tailored%20to%20a%20specific%20cognitive%20load%0Alabel%2C%20enabling%20nuanced%2C%20task-specific%20analysis.%20While%20it%20shows%20modest%0Aperformance%20compared%20to%20the%20AVCAffe%27s%20single-task%20baseline%2C%20M%5C%26M%20demonstrates%20a%0Apromising%20framework%20for%20integrated%20multimodal%20processing.%20This%20work%20paves%20the%0Away%20for%20future%20enhancements%20in%20multimodal-multitask%20learning%20systems%2C%0Aemphasizing%20the%20fusion%20of%20diverse%20data%20types%20for%20complex%20task%20handling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09451v1&entry.124074799=Read"},
{"title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for\n  Continual Learning on Vision-Language Models", "author": "Yu-Chu Yu and Chi-Pin Huang and Jr-Jen Chen and Kai-Po Chang and Yung-Hsuan Lai and Fu-En Yang and Yu-Chiang Frank Wang", "abstract": "  Large-scale vision-language models (VLMs) have shown a strong zero-shot\ngeneralization capability on unseen-domain data. However, when adapting\npre-trained VLMs to a sequence of downstream tasks, they are prone to\nforgetting previously learned knowledge and degrade their zero-shot\nclassification capability. To tackle this problem, we propose a unique\nSelective Dual-Teacher Knowledge Transfer framework that leverages the most\nrecent fine-tuned and the original pre-trained VLMs as dual teachers to\npreserve the previously learned knowledge and zero-shot capabilities,\nrespectively. With only access to an unlabeled reference dataset, our proposed\nframework performs a selective knowledge distillation mechanism by measuring\nthe feature discrepancy from the dual teacher VLMs. Consequently, our selective\ndual-teacher knowledge distillation would mitigate catastrophic forgetting of\npreviously learned knowledge while preserving the zero-shot capabilities from\npre-trained VLMs. Through extensive experiments on benchmark datasets, we show\nthat our proposed framework is favorable against state-of-the-art continual\nlearning approaches for preventing catastrophic forgetting and zero-shot\ndegradation.\n", "link": "http://arxiv.org/abs/2403.09296v1", "date": "2024-03-14", "relevancy": 2.1137, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5024}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.499}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Select%20and%20Distill%3A%20Selective%20Dual-Teacher%20Knowledge%20Transfer%20for%0A%20%20Continual%20Learning%20on%20Vision-Language%20Models&body=Title%3A%20Select%20and%20Distill%3A%20Selective%20Dual-Teacher%20Knowledge%20Transfer%20for%0A%20%20Continual%20Learning%20on%20Vision-Language%20Models%0AAuthor%3A%20Yu-Chu%20Yu%20and%20Chi-Pin%20Huang%20and%20Jr-Jen%20Chen%20and%20Kai-Po%20Chang%20and%20Yung-Hsuan%20Lai%20and%20Fu-En%20Yang%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Large-scale%20vision-language%20models%20%28VLMs%29%20have%20shown%20a%20strong%20zero-shot%0Ageneralization%20capability%20on%20unseen-domain%20data.%20However%2C%20when%20adapting%0Apre-trained%20VLMs%20to%20a%20sequence%20of%20downstream%20tasks%2C%20they%20are%20prone%20to%0Aforgetting%20previously%20learned%20knowledge%20and%20degrade%20their%20zero-shot%0Aclassification%20capability.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20unique%0ASelective%20Dual-Teacher%20Knowledge%20Transfer%20framework%20that%20leverages%20the%20most%0Arecent%20fine-tuned%20and%20the%20original%20pre-trained%20VLMs%20as%20dual%20teachers%20to%0Apreserve%20the%20previously%20learned%20knowledge%20and%20zero-shot%20capabilities%2C%0Arespectively.%20With%20only%20access%20to%20an%20unlabeled%20reference%20dataset%2C%20our%20proposed%0Aframework%20performs%20a%20selective%20knowledge%20distillation%20mechanism%20by%20measuring%0Athe%20feature%20discrepancy%20from%20the%20dual%20teacher%20VLMs.%20Consequently%2C%20our%20selective%0Adual-teacher%20knowledge%20distillation%20would%20mitigate%20catastrophic%20forgetting%20of%0Apreviously%20learned%20knowledge%20while%20preserving%20the%20zero-shot%20capabilities%20from%0Apre-trained%20VLMs.%20Through%20extensive%20experiments%20on%20benchmark%20datasets%2C%20we%20show%0Athat%20our%20proposed%20framework%20is%20favorable%20against%20state-of-the-art%20continual%0Alearning%20approaches%20for%20preventing%20catastrophic%20forgetting%20and%20zero-shot%0Adegradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09296v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Select%20and%20Distill%3A%20Selective%20Dual-Teacher%20Knowledge%20Transfer%20for%0A%20%20Continual%20Learning%20on%20Vision-Language%20Models&entry.906535625=Yu-Chu%20Yu%20and%20Chi-Pin%20Huang%20and%20Jr-Jen%20Chen%20and%20Kai-Po%20Chang%20and%20Yung-Hsuan%20Lai%20and%20Fu-En%20Yang%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Large-scale%20vision-language%20models%20%28VLMs%29%20have%20shown%20a%20strong%20zero-shot%0Ageneralization%20capability%20on%20unseen-domain%20data.%20However%2C%20when%20adapting%0Apre-trained%20VLMs%20to%20a%20sequence%20of%20downstream%20tasks%2C%20they%20are%20prone%20to%0Aforgetting%20previously%20learned%20knowledge%20and%20degrade%20their%20zero-shot%0Aclassification%20capability.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20unique%0ASelective%20Dual-Teacher%20Knowledge%20Transfer%20framework%20that%20leverages%20the%20most%0Arecent%20fine-tuned%20and%20the%20original%20pre-trained%20VLMs%20as%20dual%20teachers%20to%0Apreserve%20the%20previously%20learned%20knowledge%20and%20zero-shot%20capabilities%2C%0Arespectively.%20With%20only%20access%20to%20an%20unlabeled%20reference%20dataset%2C%20our%20proposed%0Aframework%20performs%20a%20selective%20knowledge%20distillation%20mechanism%20by%20measuring%0Athe%20feature%20discrepancy%20from%20the%20dual%20teacher%20VLMs.%20Consequently%2C%20our%20selective%0Adual-teacher%20knowledge%20distillation%20would%20mitigate%20catastrophic%20forgetting%20of%0Apreviously%20learned%20knowledge%20while%20preserving%20the%20zero-shot%20capabilities%20from%0Apre-trained%20VLMs.%20Through%20extensive%20experiments%20on%20benchmark%20datasets%2C%20we%20show%0Athat%20our%20proposed%20framework%20is%20favorable%20against%20state-of-the-art%20continual%0Alearning%20approaches%20for%20preventing%20catastrophic%20forgetting%20and%20zero-shot%0Adegradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09296v1&entry.124074799=Read"},
{"title": "Zero-Shot Object Goal Visual Navigation With Class-Independent\n  Relationship Network", "author": "Xinting Li and Shiguang Zhang and Yue LU and Kerry Dang and Lingyan Ran", "abstract": "  This paper investigates the zero-shot object goal visual navigation problem.\nIn the object goal visual navigation task, the agent needs to locate navigation\ntargets from its egocentric visual input. \"Zero-shot\" means that the target the\nagent needs to find is not trained during the training phase. To address the\nissue of coupling navigation ability with target features during training, we\npropose the Class-Independent Relationship Network (CIRN). This method combines\ntarget detection information with the relative semantic similarity between the\ntarget and the navigation target, and constructs a brand new state\nrepresentation based on similarity ranking, this state representation does not\ninclude target feature or environment feature, effectively decoupling the\nagent's navigation ability from target features. And a Graph Convolutional\nNetwork (GCN) is employed to learn the relationships between different objects\nbased on their similarities. During testing, our approach demonstrates strong\ngeneralization capabilities, including zero-shot navigation tasks with\ndifferent targets and environments. Through extensive experiments in the\nAI2-THOR virtual environment, our method outperforms the current\nstate-of-the-art approaches in the zero-shot object goal visual navigation\ntask. Furthermore, we conducted experiments in more challenging cross-target\nand cross-scene settings, which further validate the robustness and\ngeneralization ability of our method. Our code is available at:\nhttps://github.com/SmartAndCleverRobot/ICRA-CIRN.\n", "link": "http://arxiv.org/abs/2310.09883v2", "date": "2024-03-14", "relevancy": 2.1126, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5254}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5052}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Object%20Goal%20Visual%20Navigation%20With%20Class-Independent%0A%20%20Relationship%20Network&body=Title%3A%20Zero-Shot%20Object%20Goal%20Visual%20Navigation%20With%20Class-Independent%0A%20%20Relationship%20Network%0AAuthor%3A%20Xinting%20Li%20and%20Shiguang%20Zhang%20and%20Yue%20LU%20and%20Kerry%20Dang%20and%20Lingyan%20Ran%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20zero-shot%20object%20goal%20visual%20navigation%20problem.%0AIn%20the%20object%20goal%20visual%20navigation%20task%2C%20the%20agent%20needs%20to%20locate%20navigation%0Atargets%20from%20its%20egocentric%20visual%20input.%20%22Zero-shot%22%20means%20that%20the%20target%20the%0Aagent%20needs%20to%20find%20is%20not%20trained%20during%20the%20training%20phase.%20To%20address%20the%0Aissue%20of%20coupling%20navigation%20ability%20with%20target%20features%20during%20training%2C%20we%0Apropose%20the%20Class-Independent%20Relationship%20Network%20%28CIRN%29.%20This%20method%20combines%0Atarget%20detection%20information%20with%20the%20relative%20semantic%20similarity%20between%20the%0Atarget%20and%20the%20navigation%20target%2C%20and%20constructs%20a%20brand%20new%20state%0Arepresentation%20based%20on%20similarity%20ranking%2C%20this%20state%20representation%20does%20not%0Ainclude%20target%20feature%20or%20environment%20feature%2C%20effectively%20decoupling%20the%0Aagent%27s%20navigation%20ability%20from%20target%20features.%20And%20a%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20is%20employed%20to%20learn%20the%20relationships%20between%20different%20objects%0Abased%20on%20their%20similarities.%20During%20testing%2C%20our%20approach%20demonstrates%20strong%0Ageneralization%20capabilities%2C%20including%20zero-shot%20navigation%20tasks%20with%0Adifferent%20targets%20and%20environments.%20Through%20extensive%20experiments%20in%20the%0AAI2-THOR%20virtual%20environment%2C%20our%20method%20outperforms%20the%20current%0Astate-of-the-art%20approaches%20in%20the%20zero-shot%20object%20goal%20visual%20navigation%0Atask.%20Furthermore%2C%20we%20conducted%20experiments%20in%20more%20challenging%20cross-target%0Aand%20cross-scene%20settings%2C%20which%20further%20validate%20the%20robustness%20and%0Ageneralization%20ability%20of%20our%20method.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SmartAndCleverRobot/ICRA-CIRN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09883v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Object%20Goal%20Visual%20Navigation%20With%20Class-Independent%0A%20%20Relationship%20Network&entry.906535625=Xinting%20Li%20and%20Shiguang%20Zhang%20and%20Yue%20LU%20and%20Kerry%20Dang%20and%20Lingyan%20Ran&entry.1292438233=%20%20This%20paper%20investigates%20the%20zero-shot%20object%20goal%20visual%20navigation%20problem.%0AIn%20the%20object%20goal%20visual%20navigation%20task%2C%20the%20agent%20needs%20to%20locate%20navigation%0Atargets%20from%20its%20egocentric%20visual%20input.%20%22Zero-shot%22%20means%20that%20the%20target%20the%0Aagent%20needs%20to%20find%20is%20not%20trained%20during%20the%20training%20phase.%20To%20address%20the%0Aissue%20of%20coupling%20navigation%20ability%20with%20target%20features%20during%20training%2C%20we%0Apropose%20the%20Class-Independent%20Relationship%20Network%20%28CIRN%29.%20This%20method%20combines%0Atarget%20detection%20information%20with%20the%20relative%20semantic%20similarity%20between%20the%0Atarget%20and%20the%20navigation%20target%2C%20and%20constructs%20a%20brand%20new%20state%0Arepresentation%20based%20on%20similarity%20ranking%2C%20this%20state%20representation%20does%20not%0Ainclude%20target%20feature%20or%20environment%20feature%2C%20effectively%20decoupling%20the%0Aagent%27s%20navigation%20ability%20from%20target%20features.%20And%20a%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20is%20employed%20to%20learn%20the%20relationships%20between%20different%20objects%0Abased%20on%20their%20similarities.%20During%20testing%2C%20our%20approach%20demonstrates%20strong%0Ageneralization%20capabilities%2C%20including%20zero-shot%20navigation%20tasks%20with%0Adifferent%20targets%20and%20environments.%20Through%20extensive%20experiments%20in%20the%0AAI2-THOR%20virtual%20environment%2C%20our%20method%20outperforms%20the%20current%0Astate-of-the-art%20approaches%20in%20the%20zero-shot%20object%20goal%20visual%20navigation%0Atask.%20Furthermore%2C%20we%20conducted%20experiments%20in%20more%20challenging%20cross-target%0Aand%20cross-scene%20settings%2C%20which%20further%20validate%20the%20robustness%20and%0Ageneralization%20ability%20of%20our%20method.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/SmartAndCleverRobot/ICRA-CIRN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09883v2&entry.124074799=Read"},
{"title": "StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images", "author": "Robert Jewsbury and Ruoyu Wang and Abhir Bhalerao and Nasir Rajpoot and Quoc Dang Vu", "abstract": "  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art GAN and handcrafted methods in\nterms of the quality of normalized images. Additionally, compared to existing\napproaches, it improves the performance of nuclei instance segmentation and\nclassification models when used as a test time augmentation method on the\nchallenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel\nWhole Slide Images (WSIs) and demonstrate improved performance in terms of\ncomputational efficiency, image quality and consistency across tiles over\ncurrent methods.\n", "link": "http://arxiv.org/abs/2403.09302v1", "date": "2024-03-14", "relevancy": 2.1122, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5502}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5319}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5154}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20StainFuser%3A%20Controlling%20Diffusion%20for%20Faster%20Neural%20Style%20Transfer%20in%0A%20%20Multi-Gigapixel%20Histology%20Images&body=Title%3A%20StainFuser%3A%20Controlling%20Diffusion%20for%20Faster%20Neural%20Style%20Transfer%20in%0A%20%20Multi-Gigapixel%20Histology%20Images%0AAuthor%3A%20Robert%20Jewsbury%20and%20Ruoyu%20Wang%20and%20Abhir%20Bhalerao%20and%20Nasir%20Rajpoot%20and%20Quoc%20Dang%20Vu%0AAbstract%3A%20%20%20Stain%20normalization%20algorithms%20aim%20to%20transform%20the%20color%20and%20intensity%0Acharacteristics%20of%20a%20source%20multi-gigapixel%20histology%20image%20to%20match%20those%20of%20a%0Atarget%20image%2C%20mitigating%20inconsistencies%20in%20the%20appearance%20of%20stains%20used%20to%0Ahighlight%20cellular%20components%20in%20the%20images.%20We%20propose%20a%20new%20approach%2C%0AStainFuser%2C%20which%20treats%20this%20problem%20as%20a%20style%20transfer%20task%20using%20a%20novel%0AConditional%20Latent%20Diffusion%20architecture%2C%20eliminating%20the%20need%20for%20handcrafted%0Acolor%20components.%20With%20this%20method%2C%20we%20curate%20SPI-2M%20the%20largest%20stain%0Anormalization%20dataset%20to%20date%20of%20over%202%20million%20histology%20images%20with%20neural%0Astyle%20transfer%20for%20high-quality%20transformations.%20Trained%20on%20this%20data%2C%0AStainFuser%20outperforms%20current%20state-of-the-art%20GAN%20and%20handcrafted%20methods%20in%0Aterms%20of%20the%20quality%20of%20normalized%20images.%20Additionally%2C%20compared%20to%20existing%0Aapproaches%2C%20it%20improves%20the%20performance%20of%20nuclei%20instance%20segmentation%20and%0Aclassification%20models%20when%20used%20as%20a%20test%20time%20augmentation%20method%20on%20the%0Achallenging%20CoNIC%20dataset.%20Finally%2C%20we%20apply%20StainFuser%20on%20multi-gigapixel%0AWhole%20Slide%20Images%20%28WSIs%29%20and%20demonstrate%20improved%20performance%20in%20terms%20of%0Acomputational%20efficiency%2C%20image%20quality%20and%20consistency%20across%20tiles%20over%0Acurrent%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09302v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StainFuser%3A%20Controlling%20Diffusion%20for%20Faster%20Neural%20Style%20Transfer%20in%0A%20%20Multi-Gigapixel%20Histology%20Images&entry.906535625=Robert%20Jewsbury%20and%20Ruoyu%20Wang%20and%20Abhir%20Bhalerao%20and%20Nasir%20Rajpoot%20and%20Quoc%20Dang%20Vu&entry.1292438233=%20%20Stain%20normalization%20algorithms%20aim%20to%20transform%20the%20color%20and%20intensity%0Acharacteristics%20of%20a%20source%20multi-gigapixel%20histology%20image%20to%20match%20those%20of%20a%0Atarget%20image%2C%20mitigating%20inconsistencies%20in%20the%20appearance%20of%20stains%20used%20to%0Ahighlight%20cellular%20components%20in%20the%20images.%20We%20propose%20a%20new%20approach%2C%0AStainFuser%2C%20which%20treats%20this%20problem%20as%20a%20style%20transfer%20task%20using%20a%20novel%0AConditional%20Latent%20Diffusion%20architecture%2C%20eliminating%20the%20need%20for%20handcrafted%0Acolor%20components.%20With%20this%20method%2C%20we%20curate%20SPI-2M%20the%20largest%20stain%0Anormalization%20dataset%20to%20date%20of%20over%202%20million%20histology%20images%20with%20neural%0Astyle%20transfer%20for%20high-quality%20transformations.%20Trained%20on%20this%20data%2C%0AStainFuser%20outperforms%20current%20state-of-the-art%20GAN%20and%20handcrafted%20methods%20in%0Aterms%20of%20the%20quality%20of%20normalized%20images.%20Additionally%2C%20compared%20to%20existing%0Aapproaches%2C%20it%20improves%20the%20performance%20of%20nuclei%20instance%20segmentation%20and%0Aclassification%20models%20when%20used%20as%20a%20test%20time%20augmentation%20method%20on%20the%0Achallenging%20CoNIC%20dataset.%20Finally%2C%20we%20apply%20StainFuser%20on%20multi-gigapixel%0AWhole%20Slide%20Images%20%28WSIs%29%20and%20demonstrate%20improved%20performance%20in%20terms%20of%0Acomputational%20efficiency%2C%20image%20quality%20and%20consistency%20across%20tiles%20over%0Acurrent%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09302v1&entry.124074799=Read"},
{"title": "Stable Nonconvex-Nonconcave Training via Linear Interpolation", "author": "Thomas Pethick and Wanyun Xie and Volkan Cevher", "abstract": "  This paper presents a theoretical analysis of linear interpolation as a\nprincipled method for stabilizing (large-scale) neural network training. We\nargue that instabilities in the optimization process are often caused by the\nnonmonotonicity of the loss landscape and show how linear interpolation can\nhelp by leveraging the theory of nonexpansive operators. We construct a new\noptimization scheme called relaxed approximate proximal point (RAPP), which is\nthe first explicit method without anchoring to achieve last iterate convergence\nrates for $\\rho$-comonotone problems while only requiring $\\rho >\n-\\tfrac{1}{2L}$. The construction extends to constrained and regularized\nsettings. By replacing the inner optimizer in RAPP we rediscover the family of\nLookahead algorithms for which we establish convergence in cohypomonotone\nproblems even when the base optimizer is taken to be gradient descent ascent.\nThe range of cohypomonotone problems in which Lookahead converges is further\nexpanded by exploiting that Lookahead inherits the properties of the base\noptimizer. We corroborate the results with experiments on generative\nadversarial networks which demonstrates the benefits of the linear\ninterpolation present in both RAPP and Lookahead.\n", "link": "http://arxiv.org/abs/2310.13459v4", "date": "2024-03-14", "relevancy": 1.8818, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4719}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4717}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stable%20Nonconvex-Nonconcave%20Training%20via%20Linear%20Interpolation&body=Title%3A%20Stable%20Nonconvex-Nonconcave%20Training%20via%20Linear%20Interpolation%0AAuthor%3A%20Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20theoretical%20analysis%20of%20linear%20interpolation%20as%20a%0Aprincipled%20method%20for%20stabilizing%20%28large-scale%29%20neural%20network%20training.%20We%0Aargue%20that%20instabilities%20in%20the%20optimization%20process%20are%20often%20caused%20by%20the%0Anonmonotonicity%20of%20the%20loss%20landscape%20and%20show%20how%20linear%20interpolation%20can%0Ahelp%20by%20leveraging%20the%20theory%20of%20nonexpansive%20operators.%20We%20construct%20a%20new%0Aoptimization%20scheme%20called%20relaxed%20approximate%20proximal%20point%20%28RAPP%29%2C%20which%20is%0Athe%20first%20explicit%20method%20without%20anchoring%20to%20achieve%20last%20iterate%20convergence%0Arates%20for%20%24%5Crho%24-comonotone%20problems%20while%20only%20requiring%20%24%5Crho%20%3E%0A-%5Ctfrac%7B1%7D%7B2L%7D%24.%20The%20construction%20extends%20to%20constrained%20and%20regularized%0Asettings.%20By%20replacing%20the%20inner%20optimizer%20in%20RAPP%20we%20rediscover%20the%20family%20of%0ALookahead%20algorithms%20for%20which%20we%20establish%20convergence%20in%20cohypomonotone%0Aproblems%20even%20when%20the%20base%20optimizer%20is%20taken%20to%20be%20gradient%20descent%20ascent.%0AThe%20range%20of%20cohypomonotone%20problems%20in%20which%20Lookahead%20converges%20is%20further%0Aexpanded%20by%20exploiting%20that%20Lookahead%20inherits%20the%20properties%20of%20the%20base%0Aoptimizer.%20We%20corroborate%20the%20results%20with%20experiments%20on%20generative%0Aadversarial%20networks%20which%20demonstrates%20the%20benefits%20of%20the%20linear%0Ainterpolation%20present%20in%20both%20RAPP%20and%20Lookahead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13459v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Nonconvex-Nonconcave%20Training%20via%20Linear%20Interpolation&entry.906535625=Thomas%20Pethick%20and%20Wanyun%20Xie%20and%20Volkan%20Cevher&entry.1292438233=%20%20This%20paper%20presents%20a%20theoretical%20analysis%20of%20linear%20interpolation%20as%20a%0Aprincipled%20method%20for%20stabilizing%20%28large-scale%29%20neural%20network%20training.%20We%0Aargue%20that%20instabilities%20in%20the%20optimization%20process%20are%20often%20caused%20by%20the%0Anonmonotonicity%20of%20the%20loss%20landscape%20and%20show%20how%20linear%20interpolation%20can%0Ahelp%20by%20leveraging%20the%20theory%20of%20nonexpansive%20operators.%20We%20construct%20a%20new%0Aoptimization%20scheme%20called%20relaxed%20approximate%20proximal%20point%20%28RAPP%29%2C%20which%20is%0Athe%20first%20explicit%20method%20without%20anchoring%20to%20achieve%20last%20iterate%20convergence%0Arates%20for%20%24%5Crho%24-comonotone%20problems%20while%20only%20requiring%20%24%5Crho%20%3E%0A-%5Ctfrac%7B1%7D%7B2L%7D%24.%20The%20construction%20extends%20to%20constrained%20and%20regularized%0Asettings.%20By%20replacing%20the%20inner%20optimizer%20in%20RAPP%20we%20rediscover%20the%20family%20of%0ALookahead%20algorithms%20for%20which%20we%20establish%20convergence%20in%20cohypomonotone%0Aproblems%20even%20when%20the%20base%20optimizer%20is%20taken%20to%20be%20gradient%20descent%20ascent.%0AThe%20range%20of%20cohypomonotone%20problems%20in%20which%20Lookahead%20converges%20is%20further%0Aexpanded%20by%20exploiting%20that%20Lookahead%20inherits%20the%20properties%20of%20the%20base%0Aoptimizer.%20We%20corroborate%20the%20results%20with%20experiments%20on%20generative%0Aadversarial%20networks%20which%20demonstrates%20the%20benefits%20of%20the%20linear%0Ainterpolation%20present%20in%20both%20RAPP%20and%20Lookahead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13459v4&entry.124074799=Read"},
{"title": "Smooth Tchebycheff Scalarization for Multi-Objective Optimization", "author": "Xi Lin and Xiaoyuan Zhang and Zhiyuan Yang and Fei Liu and Zhenkun Wang and Qingfu Zhang", "abstract": "  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent different optimal\ntrade-offs among the objectives for a given problem. However, these existing\nmethods could have high computational complexity or may not have good\ntheoretical properties for solving a general differentiable multi-objective\noptimization problem. In this work, by leveraging the smooth optimization\ntechnique, we propose a novel and lightweight smooth Tchebycheff scalarization\napproach for gradient-based multi-objective optimization. It has good\ntheoretical properties for finding all Pareto solutions with valid trade-off\npreferences, while enjoying significantly lower computational complexity\ncompared to other methods. Experimental results on various real-world\napplication problems fully demonstrate the effectiveness of our proposed\nmethod.\n", "link": "http://arxiv.org/abs/2402.19078v2", "date": "2024-03-14", "relevancy": 1.258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.418}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4093}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Smooth%20Tchebycheff%20Scalarization%20for%20Multi-Objective%20Optimization&body=Title%3A%20Smooth%20Tchebycheff%20Scalarization%20for%20Multi-Objective%20Optimization%0AAuthor%3A%20Xi%20Lin%20and%20Xiaoyuan%20Zhang%20and%20Zhiyuan%20Yang%20and%20Fei%20Liu%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang%0AAbstract%3A%20%20%20Multi-objective%20optimization%20problems%20can%20be%20found%20in%20many%20real-world%0Aapplications%2C%20where%20the%20objectives%20often%20conflict%20each%20other%20and%20cannot%20be%0Aoptimized%20by%20a%20single%20solution.%20In%20the%20past%20few%20decades%2C%20numerous%20methods%20have%0Abeen%20proposed%20to%20find%20Pareto%20solutions%20that%20represent%20different%20optimal%0Atrade-offs%20among%20the%20objectives%20for%20a%20given%20problem.%20However%2C%20these%20existing%0Amethods%20could%20have%20high%20computational%20complexity%20or%20may%20not%20have%20good%0Atheoretical%20properties%20for%20solving%20a%20general%20differentiable%20multi-objective%0Aoptimization%20problem.%20In%20this%20work%2C%20by%20leveraging%20the%20smooth%20optimization%0Atechnique%2C%20we%20propose%20a%20novel%20and%20lightweight%20smooth%20Tchebycheff%20scalarization%0Aapproach%20for%20gradient-based%20multi-objective%20optimization.%20It%20has%20good%0Atheoretical%20properties%20for%20finding%20all%20Pareto%20solutions%20with%20valid%20trade-off%0Apreferences%2C%20while%20enjoying%20significantly%20lower%20computational%20complexity%0Acompared%20to%20other%20methods.%20Experimental%20results%20on%20various%20real-world%0Aapplication%20problems%20fully%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19078v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Tchebycheff%20Scalarization%20for%20Multi-Objective%20Optimization&entry.906535625=Xi%20Lin%20and%20Xiaoyuan%20Zhang%20and%20Zhiyuan%20Yang%20and%20Fei%20Liu%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang&entry.1292438233=%20%20Multi-objective%20optimization%20problems%20can%20be%20found%20in%20many%20real-world%0Aapplications%2C%20where%20the%20objectives%20often%20conflict%20each%20other%20and%20cannot%20be%0Aoptimized%20by%20a%20single%20solution.%20In%20the%20past%20few%20decades%2C%20numerous%20methods%20have%0Abeen%20proposed%20to%20find%20Pareto%20solutions%20that%20represent%20different%20optimal%0Atrade-offs%20among%20the%20objectives%20for%20a%20given%20problem.%20However%2C%20these%20existing%0Amethods%20could%20have%20high%20computational%20complexity%20or%20may%20not%20have%20good%0Atheoretical%20properties%20for%20solving%20a%20general%20differentiable%20multi-objective%0Aoptimization%20problem.%20In%20this%20work%2C%20by%20leveraging%20the%20smooth%20optimization%0Atechnique%2C%20we%20propose%20a%20novel%20and%20lightweight%20smooth%20Tchebycheff%20scalarization%0Aapproach%20for%20gradient-based%20multi-objective%20optimization.%20It%20has%20good%0Atheoretical%20properties%20for%20finding%20all%20Pareto%20solutions%20with%20valid%20trade-off%0Apreferences%2C%20while%20enjoying%20significantly%20lower%20computational%20complexity%0Acompared%20to%20other%20methods.%20Experimental%20results%20on%20various%20real-world%0Aapplication%20problems%20fully%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19078v2&entry.124074799=Read"},
{"title": "Assessing the Impact of Sequence Length Learning on Classification Tasks\n  for Transformer Encoder Models", "author": "Jean-Thomas Baillargeon and Luc Lamontagne", "abstract": "  Classification algorithms using Transformer architectures can be affected by\nthe sequence length learning problem whenever observations from different\nclasses have a different length distribution. This problem causes models to use\nsequence length as a predictive feature instead of relying on important textual\ninformation. Although most public datasets are not affected by this problem,\nprivately owned corpora for fields such as medicine and insurance may carry\nthis data bias. The exploitation of this sequence length feature poses\nchallenges throughout the value chain as these machine learning models can be\nused in critical applications. In this paper, we empirically expose this\nproblem and present approaches to minimize its impacts.\n", "link": "http://arxiv.org/abs/2212.08399v2", "date": "2024-03-14", "relevancy": 1.7828, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4759}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4334}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models&body=Title%3A%20Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models%0AAuthor%3A%20Jean-Thomas%20Baillargeon%20and%20Luc%20Lamontagne%0AAbstract%3A%20%20%20Classification%20algorithms%20using%20Transformer%20architectures%20can%20be%20affected%20by%0Athe%20sequence%20length%20learning%20problem%20whenever%20observations%20from%20different%0Aclasses%20have%20a%20different%20length%20distribution.%20This%20problem%20causes%20models%20to%20use%0Asequence%20length%20as%20a%20predictive%20feature%20instead%20of%20relying%20on%20important%20textual%0Ainformation.%20Although%20most%20public%20datasets%20are%20not%20affected%20by%20this%20problem%2C%0Aprivately%20owned%20corpora%20for%20fields%20such%20as%20medicine%20and%20insurance%20may%20carry%0Athis%20data%20bias.%20The%20exploitation%20of%20this%20sequence%20length%20feature%20poses%0Achallenges%20throughout%20the%20value%20chain%20as%20these%20machine%20learning%20models%20can%20be%0Aused%20in%20critical%20applications.%20In%20this%20paper%2C%20we%20empirically%20expose%20this%0Aproblem%20and%20present%20approaches%20to%20minimize%20its%20impacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08399v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models&entry.906535625=Jean-Thomas%20Baillargeon%20and%20Luc%20Lamontagne&entry.1292438233=%20%20Classification%20algorithms%20using%20Transformer%20architectures%20can%20be%20affected%20by%0Athe%20sequence%20length%20learning%20problem%20whenever%20observations%20from%20different%0Aclasses%20have%20a%20different%20length%20distribution.%20This%20problem%20causes%20models%20to%20use%0Asequence%20length%20as%20a%20predictive%20feature%20instead%20of%20relying%20on%20important%20textual%0Ainformation.%20Although%20most%20public%20datasets%20are%20not%20affected%20by%20this%20problem%2C%0Aprivately%20owned%20corpora%20for%20fields%20such%20as%20medicine%20and%20insurance%20may%20carry%0Athis%20data%20bias.%20The%20exploitation%20of%20this%20sequence%20length%20feature%20poses%0Achallenges%20throughout%20the%20value%20chain%20as%20these%20machine%20learning%20models%20can%20be%0Aused%20in%20critical%20applications.%20In%20this%20paper%2C%20we%20empirically%20expose%20this%0Aproblem%20and%20present%20approaches%20to%20minimize%20its%20impacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08399v2&entry.124074799=Read"},
{"title": "Is Data All That Matters? The Role of Control Frequency for\n  Learning-Based Sampled-Data Control of Uncertain Systems", "author": "Ralf R\u00f6mer and Lukas Brunke and Siqi Zhou and Angela P. Schoellig", "abstract": "  Learning models or control policies from data has become a powerful tool to\nimprove the performance of uncertain systems. While a strong focus has been\nplaced on increasing the amount and quality of data to improve performance,\ndata can never fully eliminate uncertainty, making feedback necessary to ensure\nstability and performance. We show that the control frequency at which the\ninput is recalculated is a crucial design parameter, yet it has hardly been\nconsidered before. We address this gap by combining probabilistic model\nlearning and sampled-data control. We use Gaussian processes (GPs) to learn a\ncontinuous-time model and compute a corresponding discrete-time controller. The\nresult is an uncertain sampled-data control system, for which we derive robust\nstability conditions. We formulate semidefinite programs to compute the minimum\ncontrol frequency required for stability and to optimize performance. As a\nresult, our approach enables us to study the effect of both control frequency\nand data on stability and closed-loop performance. We show in numerical\nsimulations of a quadrotor that performance can be improved by increasing\neither the amount of data or the control frequency, and that we can trade off\none for the other. For example, by increasing the control frequency by 33%, we\ncan reduce the number of data points by half while still achieving similar\nperformance.\n", "link": "http://arxiv.org/abs/2403.09504v1", "date": "2024-03-14", "relevancy": 1.8621, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4704}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4634}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Is%20Data%20All%20That%20Matters%3F%20The%20Role%20of%20Control%20Frequency%20for%0A%20%20Learning-Based%20Sampled-Data%20Control%20of%20Uncertain%20Systems&body=Title%3A%20Is%20Data%20All%20That%20Matters%3F%20The%20Role%20of%20Control%20Frequency%20for%0A%20%20Learning-Based%20Sampled-Data%20Control%20of%20Uncertain%20Systems%0AAuthor%3A%20Ralf%20R%C3%B6mer%20and%20Lukas%20Brunke%20and%20Siqi%20Zhou%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20%20%20Learning%20models%20or%20control%20policies%20from%20data%20has%20become%20a%20powerful%20tool%20to%0Aimprove%20the%20performance%20of%20uncertain%20systems.%20While%20a%20strong%20focus%20has%20been%0Aplaced%20on%20increasing%20the%20amount%20and%20quality%20of%20data%20to%20improve%20performance%2C%0Adata%20can%20never%20fully%20eliminate%20uncertainty%2C%20making%20feedback%20necessary%20to%20ensure%0Astability%20and%20performance.%20We%20show%20that%20the%20control%20frequency%20at%20which%20the%0Ainput%20is%20recalculated%20is%20a%20crucial%20design%20parameter%2C%20yet%20it%20has%20hardly%20been%0Aconsidered%20before.%20We%20address%20this%20gap%20by%20combining%20probabilistic%20model%0Alearning%20and%20sampled-data%20control.%20We%20use%20Gaussian%20processes%20%28GPs%29%20to%20learn%20a%0Acontinuous-time%20model%20and%20compute%20a%20corresponding%20discrete-time%20controller.%20The%0Aresult%20is%20an%20uncertain%20sampled-data%20control%20system%2C%20for%20which%20we%20derive%20robust%0Astability%20conditions.%20We%20formulate%20semidefinite%20programs%20to%20compute%20the%20minimum%0Acontrol%20frequency%20required%20for%20stability%20and%20to%20optimize%20performance.%20As%20a%0Aresult%2C%20our%20approach%20enables%20us%20to%20study%20the%20effect%20of%20both%20control%20frequency%0Aand%20data%20on%20stability%20and%20closed-loop%20performance.%20We%20show%20in%20numerical%0Asimulations%20of%20a%20quadrotor%20that%20performance%20can%20be%20improved%20by%20increasing%0Aeither%20the%20amount%20of%20data%20or%20the%20control%20frequency%2C%20and%20that%20we%20can%20trade%20off%0Aone%20for%20the%20other.%20For%20example%2C%20by%20increasing%20the%20control%20frequency%20by%2033%25%2C%20we%0Acan%20reduce%20the%20number%20of%20data%20points%20by%20half%20while%20still%20achieving%20similar%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09504v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Data%20All%20That%20Matters%3F%20The%20Role%20of%20Control%20Frequency%20for%0A%20%20Learning-Based%20Sampled-Data%20Control%20of%20Uncertain%20Systems&entry.906535625=Ralf%20R%C3%B6mer%20and%20Lukas%20Brunke%20and%20Siqi%20Zhou%20and%20Angela%20P.%20Schoellig&entry.1292438233=%20%20Learning%20models%20or%20control%20policies%20from%20data%20has%20become%20a%20powerful%20tool%20to%0Aimprove%20the%20performance%20of%20uncertain%20systems.%20While%20a%20strong%20focus%20has%20been%0Aplaced%20on%20increasing%20the%20amount%20and%20quality%20of%20data%20to%20improve%20performance%2C%0Adata%20can%20never%20fully%20eliminate%20uncertainty%2C%20making%20feedback%20necessary%20to%20ensure%0Astability%20and%20performance.%20We%20show%20that%20the%20control%20frequency%20at%20which%20the%0Ainput%20is%20recalculated%20is%20a%20crucial%20design%20parameter%2C%20yet%20it%20has%20hardly%20been%0Aconsidered%20before.%20We%20address%20this%20gap%20by%20combining%20probabilistic%20model%0Alearning%20and%20sampled-data%20control.%20We%20use%20Gaussian%20processes%20%28GPs%29%20to%20learn%20a%0Acontinuous-time%20model%20and%20compute%20a%20corresponding%20discrete-time%20controller.%20The%0Aresult%20is%20an%20uncertain%20sampled-data%20control%20system%2C%20for%20which%20we%20derive%20robust%0Astability%20conditions.%20We%20formulate%20semidefinite%20programs%20to%20compute%20the%20minimum%0Acontrol%20frequency%20required%20for%20stability%20and%20to%20optimize%20performance.%20As%20a%0Aresult%2C%20our%20approach%20enables%20us%20to%20study%20the%20effect%20of%20both%20control%20frequency%0Aand%20data%20on%20stability%20and%20closed-loop%20performance.%20We%20show%20in%20numerical%0Asimulations%20of%20a%20quadrotor%20that%20performance%20can%20be%20improved%20by%20increasing%0Aeither%20the%20amount%20of%20data%20or%20the%20control%20frequency%2C%20and%20that%20we%20can%20trade%20off%0Aone%20for%20the%20other.%20For%20example%2C%20by%20increasing%20the%20control%20frequency%20by%2033%25%2C%20we%0Acan%20reduce%20the%20number%20of%20data%20points%20by%20half%20while%20still%20achieving%20similar%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09504v1&entry.124074799=Read"},
{"title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models", "author": "Zunnan Xu and Yukang Lin and Haonan Han and Sicheng Yang and Ronghui Li and Yachao Zhang and Xiu Li", "abstract": "  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models.\n", "link": "http://arxiv.org/abs/2403.09471v1", "date": "2024-03-14", "relevancy": 1.6146, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5425}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5276}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&body=Title%3A%20MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models%0AAuthor%3A%20Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li%0AAbstract%3A%20%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09471v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaTalk%3A%20Efficient%20Holistic%20Gesture%20Synthesis%20with%20Selective%20State%0A%20%20Space%20Models&entry.906535625=Zunnan%20Xu%20and%20Yukang%20Lin%20and%20Haonan%20Han%20and%20Sicheng%20Yang%20and%20Ronghui%20Li%20and%20Yachao%20Zhang%20and%20Xiu%20Li&entry.1292438233=%20%20Gesture%20synthesis%20is%20a%20vital%20realm%20of%20human-computer%20interaction%2C%20with%0Awide-ranging%20applications%20across%20various%20fields%20like%20film%2C%20robotics%2C%20and%0Avirtual%20reality.%20Recent%20advancements%20have%20utilized%20the%20diffusion%20model%20and%0Aattention%20mechanisms%20to%20improve%20gesture%20synthesis.%20However%2C%20due%20to%20the%20high%0Acomputational%20complexity%20of%20these%20techniques%2C%20generating%20long%20and%20diverse%0Asequences%20with%20low%20latency%20remains%20a%20challenge.%20We%20explore%20the%20potential%20of%0Astate%20space%20models%20%28SSMs%29%20to%20address%20the%20challenge%2C%20implementing%20a%20two-stage%0Amodeling%20strategy%20with%20discrete%20motion%20priors%20to%20enhance%20the%20quality%20of%0Agestures.%20Leveraging%20the%20foundational%20Mamba%20block%2C%20we%20introduce%20MambaTalk%2C%0Aenhancing%20gesture%20diversity%20and%20rhythm%20through%20multimodal%20integration.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20matches%20or%20exceeds%20the%0Aperformance%20of%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09471v1&entry.124074799=Read"},
{"title": "SynFundus-1M: A High-quality Million-scale Synthetic fundus images\n  Dataset with Fifteen Types of Annotation", "author": "Fangxin Shang and Jie Fu and Yehui Yang and Haifeng Huang and Junwei Liu and Lei Ma", "abstract": "  Large-scale public datasets with high-quality annotations are rarely\navailable for intelligent medical imaging research, due to data privacy\nconcerns and the cost of annotations. In this paper, we release SynFundus-1M, a\nhigh-quality synthetic dataset containing over one million fundus images in\nterms of \\textbf{eleven disease types}. Furthermore, we deliberately assign\nfour readability labels to the key regions of the fundus images. To the best of\nour knowledge, SynFundus-1M is currently the largest fundus dataset with the\nmost sophisticated annotations. Leveraging over 1.3 million private authentic\nfundus images from various scenarios, we trained a powerful Denoising Diffusion\nProbabilistic Model, named SynFundus-Generator. The released SynFundus-1M are\ngenerated by SynFundus-Generator under predefined conditions. To demonstrate\nthe value of SynFundus-1M, extensive experiments are designed in terms of the\nfollowing aspect: 1) Authenticity of the images: we randomly blend the\nsynthetic images with authentic fundus images, and find that experienced\nannotators can hardly distinguish the synthetic images from authentic ones.\nMoreover, we show that the disease-related vision features (e.g. lesions) are\nwell simulated in the synthetic images. 2) Effectiveness for down-stream\nfine-tuning and pretraining: we demonstrate that retinal disease diagnosis\nmodels of either convolutional neural networks (CNN) or Vision Transformer\n(ViT) architectures can benefit from SynFundus-1M, and compared to the datasets\ncommonly used for pretraining, models trained on SynFundus-1M not only achieve\nsuperior performance but also demonstrate faster convergence on various\ndownstream tasks. SynFundus-1M is already public available for the open-source\ncommunity.\n", "link": "http://arxiv.org/abs/2312.00377v4", "date": "2024-03-14", "relevancy": 1.5554, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5274}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5259}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SynFundus-1M%3A%20A%20High-quality%20Million-scale%20Synthetic%20fundus%20images%0A%20%20Dataset%20with%20Fifteen%20Types%20of%20Annotation&body=Title%3A%20SynFundus-1M%3A%20A%20High-quality%20Million-scale%20Synthetic%20fundus%20images%0A%20%20Dataset%20with%20Fifteen%20Types%20of%20Annotation%0AAuthor%3A%20Fangxin%20Shang%20and%20Jie%20Fu%20and%20Yehui%20Yang%20and%20Haifeng%20Huang%20and%20Junwei%20Liu%20and%20Lei%20Ma%0AAbstract%3A%20%20%20Large-scale%20public%20datasets%20with%20high-quality%20annotations%20are%20rarely%0Aavailable%20for%20intelligent%20medical%20imaging%20research%2C%20due%20to%20data%20privacy%0Aconcerns%20and%20the%20cost%20of%20annotations.%20In%20this%20paper%2C%20we%20release%20SynFundus-1M%2C%20a%0Ahigh-quality%20synthetic%20dataset%20containing%20over%20one%20million%20fundus%20images%20in%0Aterms%20of%20%5Ctextbf%7Beleven%20disease%20types%7D.%20Furthermore%2C%20we%20deliberately%20assign%0Afour%20readability%20labels%20to%20the%20key%20regions%20of%20the%20fundus%20images.%20To%20the%20best%20of%0Aour%20knowledge%2C%20SynFundus-1M%20is%20currently%20the%20largest%20fundus%20dataset%20with%20the%0Amost%20sophisticated%20annotations.%20Leveraging%20over%201.3%20million%20private%20authentic%0Afundus%20images%20from%20various%20scenarios%2C%20we%20trained%20a%20powerful%20Denoising%20Diffusion%0AProbabilistic%20Model%2C%20named%20SynFundus-Generator.%20The%20released%20SynFundus-1M%20are%0Agenerated%20by%20SynFundus-Generator%20under%20predefined%20conditions.%20To%20demonstrate%0Athe%20value%20of%20SynFundus-1M%2C%20extensive%20experiments%20are%20designed%20in%20terms%20of%20the%0Afollowing%20aspect%3A%201%29%20Authenticity%20of%20the%20images%3A%20we%20randomly%20blend%20the%0Asynthetic%20images%20with%20authentic%20fundus%20images%2C%20and%20find%20that%20experienced%0Aannotators%20can%20hardly%20distinguish%20the%20synthetic%20images%20from%20authentic%20ones.%0AMoreover%2C%20we%20show%20that%20the%20disease-related%20vision%20features%20%28e.g.%20lesions%29%20are%0Awell%20simulated%20in%20the%20synthetic%20images.%202%29%20Effectiveness%20for%20down-stream%0Afine-tuning%20and%20pretraining%3A%20we%20demonstrate%20that%20retinal%20disease%20diagnosis%0Amodels%20of%20either%20convolutional%20neural%20networks%20%28CNN%29%20or%20Vision%20Transformer%0A%28ViT%29%20architectures%20can%20benefit%20from%20SynFundus-1M%2C%20and%20compared%20to%20the%20datasets%0Acommonly%20used%20for%20pretraining%2C%20models%20trained%20on%20SynFundus-1M%20not%20only%20achieve%0Asuperior%20performance%20but%20also%20demonstrate%20faster%20convergence%20on%20various%0Adownstream%20tasks.%20SynFundus-1M%20is%20already%20public%20available%20for%20the%20open-source%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00377v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynFundus-1M%3A%20A%20High-quality%20Million-scale%20Synthetic%20fundus%20images%0A%20%20Dataset%20with%20Fifteen%20Types%20of%20Annotation&entry.906535625=Fangxin%20Shang%20and%20Jie%20Fu%20and%20Yehui%20Yang%20and%20Haifeng%20Huang%20and%20Junwei%20Liu%20and%20Lei%20Ma&entry.1292438233=%20%20Large-scale%20public%20datasets%20with%20high-quality%20annotations%20are%20rarely%0Aavailable%20for%20intelligent%20medical%20imaging%20research%2C%20due%20to%20data%20privacy%0Aconcerns%20and%20the%20cost%20of%20annotations.%20In%20this%20paper%2C%20we%20release%20SynFundus-1M%2C%20a%0Ahigh-quality%20synthetic%20dataset%20containing%20over%20one%20million%20fundus%20images%20in%0Aterms%20of%20%5Ctextbf%7Beleven%20disease%20types%7D.%20Furthermore%2C%20we%20deliberately%20assign%0Afour%20readability%20labels%20to%20the%20key%20regions%20of%20the%20fundus%20images.%20To%20the%20best%20of%0Aour%20knowledge%2C%20SynFundus-1M%20is%20currently%20the%20largest%20fundus%20dataset%20with%20the%0Amost%20sophisticated%20annotations.%20Leveraging%20over%201.3%20million%20private%20authentic%0Afundus%20images%20from%20various%20scenarios%2C%20we%20trained%20a%20powerful%20Denoising%20Diffusion%0AProbabilistic%20Model%2C%20named%20SynFundus-Generator.%20The%20released%20SynFundus-1M%20are%0Agenerated%20by%20SynFundus-Generator%20under%20predefined%20conditions.%20To%20demonstrate%0Athe%20value%20of%20SynFundus-1M%2C%20extensive%20experiments%20are%20designed%20in%20terms%20of%20the%0Afollowing%20aspect%3A%201%29%20Authenticity%20of%20the%20images%3A%20we%20randomly%20blend%20the%0Asynthetic%20images%20with%20authentic%20fundus%20images%2C%20and%20find%20that%20experienced%0Aannotators%20can%20hardly%20distinguish%20the%20synthetic%20images%20from%20authentic%20ones.%0AMoreover%2C%20we%20show%20that%20the%20disease-related%20vision%20features%20%28e.g.%20lesions%29%20are%0Awell%20simulated%20in%20the%20synthetic%20images.%202%29%20Effectiveness%20for%20down-stream%0Afine-tuning%20and%20pretraining%3A%20we%20demonstrate%20that%20retinal%20disease%20diagnosis%0Amodels%20of%20either%20convolutional%20neural%20networks%20%28CNN%29%20or%20Vision%20Transformer%0A%28ViT%29%20architectures%20can%20benefit%20from%20SynFundus-1M%2C%20and%20compared%20to%20the%20datasets%0Acommonly%20used%20for%20pretraining%2C%20models%20trained%20on%20SynFundus-1M%20not%20only%20achieve%0Asuperior%20performance%20but%20also%20demonstrate%20faster%20convergence%20on%20various%0Adownstream%20tasks.%20SynFundus-1M%20is%20already%20public%20available%20for%20the%20open-source%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00377v4&entry.124074799=Read"},
{"title": "Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records", "author": "Erlend Frayling and Jake Lever and Graham McDonald", "abstract": "  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n", "link": "http://arxiv.org/abs/2403.08664v2", "date": "2024-03-14", "relevancy": 1.8062, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4536}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4441}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records&body=Title%3A%20Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records%0AAuthor%3A%20Erlend%20Frayling%20and%20Jake%20Lever%20and%20Graham%20McDonald%0AAbstract%3A%20%20%20The%20challenge%20of%20accessing%20historical%20patient%20data%20for%20clinical%20research%2C%0Awhile%20adhering%20to%20privacy%20regulations%2C%20is%20a%20significant%20obstacle%20in%20medical%0Ascience.%20An%20innovative%20approach%20to%20circumvent%20this%20issue%20involves%20utilising%0Asynthetic%20medical%20records%20that%20mirror%20real%20patient%20data%20without%20compromising%0Aindividual%20privacy.%20The%20creation%20of%20these%20synthetic%20datasets%2C%20particularly%0Awithout%20using%20actual%20patient%20data%20to%20train%20Large%20Language%20Models%20%28LLMs%29%2C%0Apresents%20a%20novel%20solution%20as%20gaining%20access%20to%20sensitive%20patient%20information%20to%0Atrain%20models%20is%20also%20a%20challenge.%20This%20study%20assesses%20the%20capability%20of%20the%0ALlama%202%20LLM%20to%20create%20synthetic%20medical%20records%20that%20accurately%20reflect%20real%0Apatient%20information%2C%20employing%20zero-shot%20and%20few-shot%20prompting%20strategies%20for%0Acomparison%20against%20fine-tuned%20methodologies%20that%20do%20require%20sensitive%20patient%0Adata%20during%20training.%20We%20focus%20on%20generating%20synthetic%20narratives%20for%20the%0AHistory%20of%20Present%20Illness%20section%2C%20utilising%20data%20from%20the%20MIMIC-IV%20dataset%0Afor%20comparison.%20In%20this%20work%20introduce%20a%20novel%20prompting%20technique%20that%0Aleverages%20a%20chain-of-thought%20approach%2C%20enhancing%20the%20model%27s%20ability%20to%0Agenerate%20more%20accurate%20and%20contextually%20relevant%20medical%20narratives%20without%0Aprior%20fine-tuning.%20Our%20findings%20suggest%20that%20this%20chain-of-thought%20prompted%0Aapproach%20allows%20the%20zero-shot%20model%20to%20achieve%20results%20on%20par%20with%20those%20of%0Afine-tuned%20models%2C%20based%20on%20Rouge%20metrics%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08664v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20and%20Few-shot%20Generation%20Strategies%20for%20Artificial%20Clinical%0A%20%20Records&entry.906535625=Erlend%20Frayling%20and%20Jake%20Lever%20and%20Graham%20McDonald&entry.1292438233=%20%20The%20challenge%20of%20accessing%20historical%20patient%20data%20for%20clinical%20research%2C%0Awhile%20adhering%20to%20privacy%20regulations%2C%20is%20a%20significant%20obstacle%20in%20medical%0Ascience.%20An%20innovative%20approach%20to%20circumvent%20this%20issue%20involves%20utilising%0Asynthetic%20medical%20records%20that%20mirror%20real%20patient%20data%20without%20compromising%0Aindividual%20privacy.%20The%20creation%20of%20these%20synthetic%20datasets%2C%20particularly%0Awithout%20using%20actual%20patient%20data%20to%20train%20Large%20Language%20Models%20%28LLMs%29%2C%0Apresents%20a%20novel%20solution%20as%20gaining%20access%20to%20sensitive%20patient%20information%20to%0Atrain%20models%20is%20also%20a%20challenge.%20This%20study%20assesses%20the%20capability%20of%20the%0ALlama%202%20LLM%20to%20create%20synthetic%20medical%20records%20that%20accurately%20reflect%20real%0Apatient%20information%2C%20employing%20zero-shot%20and%20few-shot%20prompting%20strategies%20for%0Acomparison%20against%20fine-tuned%20methodologies%20that%20do%20require%20sensitive%20patient%0Adata%20during%20training.%20We%20focus%20on%20generating%20synthetic%20narratives%20for%20the%0AHistory%20of%20Present%20Illness%20section%2C%20utilising%20data%20from%20the%20MIMIC-IV%20dataset%0Afor%20comparison.%20In%20this%20work%20introduce%20a%20novel%20prompting%20technique%20that%0Aleverages%20a%20chain-of-thought%20approach%2C%20enhancing%20the%20model%27s%20ability%20to%0Agenerate%20more%20accurate%20and%20contextually%20relevant%20medical%20narratives%20without%0Aprior%20fine-tuning.%20Our%20findings%20suggest%20that%20this%20chain-of-thought%20prompted%0Aapproach%20allows%20the%20zero-shot%20model%20to%20achieve%20results%20on%20par%20with%20those%20of%0Afine-tuned%20models%2C%20based%20on%20Rouge%20metrics%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08664v2&entry.124074799=Read"},
{"title": "Kernelized Reinforcement Learning with Order Optimal Regret Bounds", "author": "Sattar Vakili and Julia Olkhovskaya", "abstract": "  Reinforcement learning (RL) has shown empirical success in various real world\nsettings with complex models and large state-action spaces. The existing\nanalytical results, however, typically focus on settings with a small number of\nstate-actions or simple models such as linearly modeled state-action value\nfunctions. To derive RL policies that efficiently handle large state-action\nspaces with more general value functions, some recent works have considered\nnonlinear function approximation using kernel ridge regression. We propose\n$\\pi$-KRVI, an optimistic modification of least-squares value iteration, when\nthe state-action value function is represented by a reproducing kernel Hilbert\nspace (RKHS). We prove the first order-optimal regret guarantees under a\ngeneral setting. Our results show a significant polynomial in the number of\nepisodes improvement over the state of the art. In particular, with highly\nnon-smooth kernels (such as Neural Tangent kernel or some Mat\\'ern kernels) the\nexisting results lead to trivial (superlinear in the number of episodes) regret\nbounds. We show a sublinear regret bound that is order optimal in the case of\nMat\\'ern kernels where a lower bound on regret is known.\n", "link": "http://arxiv.org/abs/2306.07745v3", "date": "2024-03-14", "relevancy": 1.7638, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4382}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4267}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kernelized%20Reinforcement%20Learning%20with%20Order%20Optimal%20Regret%20Bounds&body=Title%3A%20Kernelized%20Reinforcement%20Learning%20with%20Order%20Optimal%20Regret%20Bounds%0AAuthor%3A%20Sattar%20Vakili%20and%20Julia%20Olkhovskaya%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20shown%20empirical%20success%20in%20various%20real%20world%0Asettings%20with%20complex%20models%20and%20large%20state-action%20spaces.%20The%20existing%0Aanalytical%20results%2C%20however%2C%20typically%20focus%20on%20settings%20with%20a%20small%20number%20of%0Astate-actions%20or%20simple%20models%20such%20as%20linearly%20modeled%20state-action%20value%0Afunctions.%20To%20derive%20RL%20policies%20that%20efficiently%20handle%20large%20state-action%0Aspaces%20with%20more%20general%20value%20functions%2C%20some%20recent%20works%20have%20considered%0Anonlinear%20function%20approximation%20using%20kernel%20ridge%20regression.%20We%20propose%0A%24%5Cpi%24-KRVI%2C%20an%20optimistic%20modification%20of%20least-squares%20value%20iteration%2C%20when%0Athe%20state-action%20value%20function%20is%20represented%20by%20a%20reproducing%20kernel%20Hilbert%0Aspace%20%28RKHS%29.%20We%20prove%20the%20first%20order-optimal%20regret%20guarantees%20under%20a%0Ageneral%20setting.%20Our%20results%20show%20a%20significant%20polynomial%20in%20the%20number%20of%0Aepisodes%20improvement%20over%20the%20state%20of%20the%20art.%20In%20particular%2C%20with%20highly%0Anon-smooth%20kernels%20%28such%20as%20Neural%20Tangent%20kernel%20or%20some%20Mat%5C%27ern%20kernels%29%20the%0Aexisting%20results%20lead%20to%20trivial%20%28superlinear%20in%20the%20number%20of%20episodes%29%20regret%0Abounds.%20We%20show%20a%20sublinear%20regret%20bound%20that%20is%20order%20optimal%20in%20the%20case%20of%0AMat%5C%27ern%20kernels%20where%20a%20lower%20bound%20on%20regret%20is%20known.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07745v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernelized%20Reinforcement%20Learning%20with%20Order%20Optimal%20Regret%20Bounds&entry.906535625=Sattar%20Vakili%20and%20Julia%20Olkhovskaya&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20shown%20empirical%20success%20in%20various%20real%20world%0Asettings%20with%20complex%20models%20and%20large%20state-action%20spaces.%20The%20existing%0Aanalytical%20results%2C%20however%2C%20typically%20focus%20on%20settings%20with%20a%20small%20number%20of%0Astate-actions%20or%20simple%20models%20such%20as%20linearly%20modeled%20state-action%20value%0Afunctions.%20To%20derive%20RL%20policies%20that%20efficiently%20handle%20large%20state-action%0Aspaces%20with%20more%20general%20value%20functions%2C%20some%20recent%20works%20have%20considered%0Anonlinear%20function%20approximation%20using%20kernel%20ridge%20regression.%20We%20propose%0A%24%5Cpi%24-KRVI%2C%20an%20optimistic%20modification%20of%20least-squares%20value%20iteration%2C%20when%0Athe%20state-action%20value%20function%20is%20represented%20by%20a%20reproducing%20kernel%20Hilbert%0Aspace%20%28RKHS%29.%20We%20prove%20the%20first%20order-optimal%20regret%20guarantees%20under%20a%0Ageneral%20setting.%20Our%20results%20show%20a%20significant%20polynomial%20in%20the%20number%20of%0Aepisodes%20improvement%20over%20the%20state%20of%20the%20art.%20In%20particular%2C%20with%20highly%0Anon-smooth%20kernels%20%28such%20as%20Neural%20Tangent%20kernel%20or%20some%20Mat%5C%27ern%20kernels%29%20the%0Aexisting%20results%20lead%20to%20trivial%20%28superlinear%20in%20the%20number%20of%20episodes%29%20regret%0Abounds.%20We%20show%20a%20sublinear%20regret%20bound%20that%20is%20order%20optimal%20in%20the%20case%20of%0AMat%5C%27ern%20kernels%20where%20a%20lower%20bound%20on%20regret%20is%20known.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07745v3&entry.124074799=Read"},
{"title": "Logits of API-Protected LLMs Leak Proprietary Information", "author": "Matthew Finlayson and Swabha Swayamdipta and Xiang Ren", "abstract": "  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n", "link": "http://arxiv.org/abs/2403.09539v1", "date": "2024-03-14", "relevancy": 1.8403, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4773}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4476}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information&body=Title%3A%20Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information%0AAuthor%3A%20Matthew%20Finlayson%20and%20Swabha%20Swayamdipta%20and%20Xiang%20Ren%0AAbstract%3A%20%20%20The%20commercialization%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20the%20common%0Apractice%20of%20high-level%20API-only%20access%20to%20proprietary%20models.%20In%20this%20work%2C%20we%0Ashow%20that%20even%20with%20a%20conservative%20assumption%20about%20the%20model%20architecture%2C%20it%0Ais%20possible%20to%20learn%20a%20surprisingly%20large%20amount%20of%20non-public%20information%0Aabout%20an%20API-protected%20LLM%20from%20a%20relatively%20small%20number%20of%20API%20queries%20%28e.g.%2C%0Acosting%20under%20%241%2C000%20for%20OpenAI%27s%20gpt-3.5-turbo%29.%20Our%20findings%20are%20centered%20on%0Aone%20key%20observation%3A%20most%20modern%20LLMs%20suffer%20from%20a%20softmax%20bottleneck%2C%20which%0Arestricts%20the%20model%20outputs%20to%20a%20linear%20subspace%20of%20the%20full%20output%20space.%20We%0Ashow%20that%20this%20lends%20itself%20to%20a%20model%20image%20or%20a%20model%20signature%20which%20unlocks%0Aseveral%20capabilities%20with%20affordable%20cost%3A%20efficiently%20discovering%20the%20LLM%27s%0Ahidden%20size%2C%20obtaining%20full-vocabulary%20outputs%2C%20detecting%20and%20disambiguating%0Adifferent%20model%20updates%2C%20identifying%20the%20source%20LLM%20given%20a%20single%20full%20LLM%0Aoutput%2C%20and%20even%20estimating%20the%20output%20layer%20parameters.%20Our%20empirical%0Ainvestigations%20show%20the%20effectiveness%20of%20our%20methods%2C%20which%20allow%20us%20to%0Aestimate%20the%20embedding%20size%20of%20OpenAI%27s%20gpt-3.5-turbo%20to%20be%20about%204%2C096.%0ALastly%2C%20we%20discuss%20ways%20that%20LLM%20providers%20can%20guard%20against%20these%20attacks%2C%20as%0Awell%20as%20how%20these%20capabilities%20can%20be%20viewed%20as%20a%20feature%20%28rather%20than%20a%20bug%29%0Aby%20allowing%20for%20greater%20transparency%20and%20accountability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09539v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information&entry.906535625=Matthew%20Finlayson%20and%20Swabha%20Swayamdipta%20and%20Xiang%20Ren&entry.1292438233=%20%20The%20commercialization%20of%20large%20language%20models%20%28LLMs%29%20has%20led%20to%20the%20common%0Apractice%20of%20high-level%20API-only%20access%20to%20proprietary%20models.%20In%20this%20work%2C%20we%0Ashow%20that%20even%20with%20a%20conservative%20assumption%20about%20the%20model%20architecture%2C%20it%0Ais%20possible%20to%20learn%20a%20surprisingly%20large%20amount%20of%20non-public%20information%0Aabout%20an%20API-protected%20LLM%20from%20a%20relatively%20small%20number%20of%20API%20queries%20%28e.g.%2C%0Acosting%20under%20%241%2C000%20for%20OpenAI%27s%20gpt-3.5-turbo%29.%20Our%20findings%20are%20centered%20on%0Aone%20key%20observation%3A%20most%20modern%20LLMs%20suffer%20from%20a%20softmax%20bottleneck%2C%20which%0Arestricts%20the%20model%20outputs%20to%20a%20linear%20subspace%20of%20the%20full%20output%20space.%20We%0Ashow%20that%20this%20lends%20itself%20to%20a%20model%20image%20or%20a%20model%20signature%20which%20unlocks%0Aseveral%20capabilities%20with%20affordable%20cost%3A%20efficiently%20discovering%20the%20LLM%27s%0Ahidden%20size%2C%20obtaining%20full-vocabulary%20outputs%2C%20detecting%20and%20disambiguating%0Adifferent%20model%20updates%2C%20identifying%20the%20source%20LLM%20given%20a%20single%20full%20LLM%0Aoutput%2C%20and%20even%20estimating%20the%20output%20layer%20parameters.%20Our%20empirical%0Ainvestigations%20show%20the%20effectiveness%20of%20our%20methods%2C%20which%20allow%20us%20to%0Aestimate%20the%20embedding%20size%20of%20OpenAI%27s%20gpt-3.5-turbo%20to%20be%20about%204%2C096.%0ALastly%2C%20we%20discuss%20ways%20that%20LLM%20providers%20can%20guard%20against%20these%20attacks%2C%20as%0Awell%20as%20how%20these%20capabilities%20can%20be%20viewed%20as%20a%20feature%20%28rather%20than%20a%20bug%29%0Aby%20allowing%20for%20greater%20transparency%20and%20accountability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09539v1&entry.124074799=Read"},
{"title": "Laying the Foundation First? Investigating the Generalization from\n  Atomic Skills to Complex Reasoning Tasks", "author": "Yuncheng Huang and Qianyu He and Yipei Xu and Jiaqing Liang and Yanghua Xiao", "abstract": "  Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.\n", "link": "http://arxiv.org/abs/2403.09479v1", "date": "2024-03-14", "relevancy": 1.3701, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4746}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Laying%20the%20Foundation%20First%3F%20Investigating%20the%20Generalization%20from%0A%20%20Atomic%20Skills%20to%20Complex%20Reasoning%20Tasks&body=Title%3A%20Laying%20the%20Foundation%20First%3F%20Investigating%20the%20Generalization%20from%0A%20%20Atomic%20Skills%20to%20Complex%20Reasoning%20Tasks%0AAuthor%3A%20Yuncheng%20Huang%20and%20Qianyu%20He%20and%20Yipei%20Xu%20and%20Jiaqing%20Liang%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Current%20language%20models%20have%20demonstrated%20their%20capability%20to%20develop%20basic%0Areasoning%2C%20but%20struggle%20in%20more%20complicated%20reasoning%20tasks%20that%20require%20a%0Acombination%20of%20atomic%20skills%2C%20such%20as%20math%20word%20problem%20requiring%20skills%20like%0Aarithmetic%20and%20unit%20conversion.%20Previous%20methods%20either%20do%20not%20improve%20the%0Ainherent%20atomic%20skills%20of%20models%20or%20not%20attempt%20to%20generalize%20the%20atomic%20skills%0Ato%20complex%20reasoning%20tasks.%20In%20this%20paper%2C%20we%20first%20propose%20a%20probing%20framework%0Ato%20investigate%20whether%20the%20atomic%20skill%20can%20spontaneously%20generalize%20to%20complex%0Areasoning%20tasks.%20Then%2C%20we%20introduce%20a%20hierarchical%20curriculum%20learning%20training%0Astrategy%20to%20achieve%20better%20skill%20generalization.%20In%20our%20experiments%2C%20we%20find%0Athat%20atomic%20skills%20can%20not%20spontaneously%20generalize%20to%20compositional%20tasks.%20By%0Aleveraging%20hierarchical%20curriculum%20learning%2C%20we%20successfully%20induce%0Ageneralization%2C%20significantly%20improve%20the%20performance%20of%20open-source%20LMs%20on%0Acomplex%20reasoning%20tasks.%20Promisingly%2C%20the%20skill%20generalization%20exhibit%0Aeffective%20in%20cross-dataset%20and%20cross-domain%20scenarios.%20Complex%20reasoning%20can%0Aalso%20help%20enhance%20atomic%20skills.%20Our%20findings%20offer%20valuable%20guidance%20for%0Adesigning%20better%20training%20strategies%20for%20complex%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09479v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Laying%20the%20Foundation%20First%3F%20Investigating%20the%20Generalization%20from%0A%20%20Atomic%20Skills%20to%20Complex%20Reasoning%20Tasks&entry.906535625=Yuncheng%20Huang%20and%20Qianyu%20He%20and%20Yipei%20Xu%20and%20Jiaqing%20Liang%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Current%20language%20models%20have%20demonstrated%20their%20capability%20to%20develop%20basic%0Areasoning%2C%20but%20struggle%20in%20more%20complicated%20reasoning%20tasks%20that%20require%20a%0Acombination%20of%20atomic%20skills%2C%20such%20as%20math%20word%20problem%20requiring%20skills%20like%0Aarithmetic%20and%20unit%20conversion.%20Previous%20methods%20either%20do%20not%20improve%20the%0Ainherent%20atomic%20skills%20of%20models%20or%20not%20attempt%20to%20generalize%20the%20atomic%20skills%0Ato%20complex%20reasoning%20tasks.%20In%20this%20paper%2C%20we%20first%20propose%20a%20probing%20framework%0Ato%20investigate%20whether%20the%20atomic%20skill%20can%20spontaneously%20generalize%20to%20complex%0Areasoning%20tasks.%20Then%2C%20we%20introduce%20a%20hierarchical%20curriculum%20learning%20training%0Astrategy%20to%20achieve%20better%20skill%20generalization.%20In%20our%20experiments%2C%20we%20find%0Athat%20atomic%20skills%20can%20not%20spontaneously%20generalize%20to%20compositional%20tasks.%20By%0Aleveraging%20hierarchical%20curriculum%20learning%2C%20we%20successfully%20induce%0Ageneralization%2C%20significantly%20improve%20the%20performance%20of%20open-source%20LMs%20on%0Acomplex%20reasoning%20tasks.%20Promisingly%2C%20the%20skill%20generalization%20exhibit%0Aeffective%20in%20cross-dataset%20and%20cross-domain%20scenarios.%20Complex%20reasoning%20can%0Aalso%20help%20enhance%20atomic%20skills.%20Our%20findings%20offer%20valuable%20guidance%20for%0Adesigning%20better%20training%20strategies%20for%20complex%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09479v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


