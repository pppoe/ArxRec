<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding", "author": "Chengyao Wang and Li Jiang and Xiaoyang Wu and Zhuotao Tian and Bohao Peng and Hengshuang Zhao and Jiaya Jia", "abstract": "  Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.\n", "link": "http://arxiv.org/abs/2403.09639v1", "date": "2024-03-14", "relevancy": 2.965, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding&body=Title%3A%20GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding%0AAuthor%3A%20Chengyao%20Wang%20and%20Li%20Jiang%20and%20Xiaoyang%20Wu%20and%20Zhuotao%20Tian%20and%20Bohao%20Peng%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Self-supervised%203D%20representation%20learning%20aims%20to%20learn%20effective%0Arepresentations%20from%20large-scale%20unlabeled%20point%20clouds.%20Most%20existing%0Aapproaches%20adopt%20point%20discrimination%20as%20the%20pretext%20task%2C%20which%20assigns%0Amatched%20points%20in%20two%20distinct%20views%20as%20positive%20pairs%20and%20unmatched%20points%20as%0Anegative%20pairs.%20However%2C%20this%20approach%20often%20results%20in%20semantically%20identical%0Apoints%20having%20dissimilar%20representations%2C%20leading%20to%20a%20high%20number%20of%20false%0Anegatives%20and%20introducing%20a%20%22semantic%20conflict%22%20problem.%20To%20address%20this%20issue%2C%0Awe%20propose%20GroupContrast%2C%20a%20novel%20approach%20that%20combines%20segment%20grouping%20and%0Asemantic-aware%20contrastive%20learning.%20Segment%20grouping%20partitions%20points%20into%0Asemantically%20meaningful%20regions%2C%20which%20enhances%20semantic%20coherence%20and%20provides%0Asemantic%20guidance%20for%20the%20subsequent%20contrastive%20representation%20learning.%0ASemantic-aware%20contrastive%20learning%20augments%20the%20semantic%20information%20extracted%0Afrom%20segment%20grouping%20and%20helps%20to%20alleviate%20the%20issue%20of%20%22semantic%20conflict%22.%0AWe%20conducted%20extensive%20experiments%20on%20multiple%203D%20scene%20understanding%20tasks.%0AThe%20results%20demonstrate%20that%20GroupContrast%20learns%20semantically%20meaningful%0Arepresentations%20and%20achieves%20promising%20transfer%20learning%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroupContrast%3A%20Semantic-aware%20Self-supervised%20Representation%20Learning%0A%20%20for%203D%20Understanding&entry.906535625=Chengyao%20Wang%20and%20Li%20Jiang%20and%20Xiaoyang%20Wu%20and%20Zhuotao%20Tian%20and%20Bohao%20Peng%20and%20Hengshuang%20Zhao%20and%20Jiaya%20Jia&entry.1292438233=%20%20Self-supervised%203D%20representation%20learning%20aims%20to%20learn%20effective%0Arepresentations%20from%20large-scale%20unlabeled%20point%20clouds.%20Most%20existing%0Aapproaches%20adopt%20point%20discrimination%20as%20the%20pretext%20task%2C%20which%20assigns%0Amatched%20points%20in%20two%20distinct%20views%20as%20positive%20pairs%20and%20unmatched%20points%20as%0Anegative%20pairs.%20However%2C%20this%20approach%20often%20results%20in%20semantically%20identical%0Apoints%20having%20dissimilar%20representations%2C%20leading%20to%20a%20high%20number%20of%20false%0Anegatives%20and%20introducing%20a%20%22semantic%20conflict%22%20problem.%20To%20address%20this%20issue%2C%0Awe%20propose%20GroupContrast%2C%20a%20novel%20approach%20that%20combines%20segment%20grouping%20and%0Asemantic-aware%20contrastive%20learning.%20Segment%20grouping%20partitions%20points%20into%0Asemantically%20meaningful%20regions%2C%20which%20enhances%20semantic%20coherence%20and%20provides%0Asemantic%20guidance%20for%20the%20subsequent%20contrastive%20representation%20learning.%0ASemantic-aware%20contrastive%20learning%20augments%20the%20semantic%20information%20extracted%0Afrom%20segment%20grouping%20and%20helps%20to%20alleviate%20the%20issue%20of%20%22semantic%20conflict%22.%0AWe%20conducted%20extensive%20experiments%20on%20multiple%203D%20scene%20understanding%20tasks.%0AThe%20results%20demonstrate%20that%20GroupContrast%20learns%20semantically%20meaningful%0Arepresentations%20and%20achieves%20promising%20transfer%20learning%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09639v1&entry.124074799=Read"},
{"title": "Anomaly Detection by Adapting a pre-trained Vision Language Model", "author": "Yuxuan Cai and Xinwei He and Dingkang Liang and Ao Tong and Xiang Bai", "abstract": "  Recently, large vision and language models have shown their success when\nadapting them to many downstream tasks. In this paper, we present a unified\nframework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP\nmodel. To this end, we make two important improvements: 1) To acquire unified\nanomaly detection across industrial images of multiple categories, we introduce\nthe learnable prompt and propose to associate it with abnormal patterns through\nself-supervised learning. 2) To fully exploit the representation power of CLIP,\nwe introduce an anomaly region refinement strategy to refine the localization\nquality. During testing, the anomalies are localized by directly calculating\nthe similarity between the representation of the learnable prompt and the\nimage. Comprehensive experiments demonstrate the superiority of our framework,\ne.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and\nVisA for anomaly detection and localization. In addition, the proposed method\nalso achieves encouraging performance with marginal training data, which is\nmore challenging.\n", "link": "http://arxiv.org/abs/2403.09493v1", "date": "2024-03-14", "relevancy": 2.8603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6252}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.542}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model&body=Title%3A%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model%0AAuthor%3A%20Yuxuan%20Cai%20and%20Xinwei%20He%20and%20Dingkang%20Liang%20and%20Ao%20Tong%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Recently%2C%20large%20vision%20and%20language%20models%20have%20shown%20their%20success%20when%0Aadapting%20them%20to%20many%20downstream%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20unified%0Aframework%20named%20CLIP-ADA%20for%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20CLIP%0Amodel.%20To%20this%20end%2C%20we%20make%20two%20important%20improvements%3A%201%29%20To%20acquire%20unified%0Aanomaly%20detection%20across%20industrial%20images%20of%20multiple%20categories%2C%20we%20introduce%0Athe%20learnable%20prompt%20and%20propose%20to%20associate%20it%20with%20abnormal%20patterns%20through%0Aself-supervised%20learning.%202%29%20To%20fully%20exploit%20the%20representation%20power%20of%20CLIP%2C%0Awe%20introduce%20an%20anomaly%20region%20refinement%20strategy%20to%20refine%20the%20localization%0Aquality.%20During%20testing%2C%20the%20anomalies%20are%20localized%20by%20directly%20calculating%0Athe%20similarity%20between%20the%20representation%20of%20the%20learnable%20prompt%20and%20the%0Aimage.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20framework%2C%0Ae.g.%2C%20we%20achieve%20the%20state-of-the-art%2097.5/55.6%20and%2089.3/33.1%20on%20MVTec-AD%20and%0AVisA%20for%20anomaly%20detection%20and%20localization.%20In%20addition%2C%20the%20proposed%20method%0Aalso%20achieves%20encouraging%20performance%20with%20marginal%20training%20data%2C%20which%20is%0Amore%20challenging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09493v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20Vision%20Language%20Model&entry.906535625=Yuxuan%20Cai%20and%20Xinwei%20He%20and%20Dingkang%20Liang%20and%20Ao%20Tong%20and%20Xiang%20Bai&entry.1292438233=%20%20Recently%2C%20large%20vision%20and%20language%20models%20have%20shown%20their%20success%20when%0Aadapting%20them%20to%20many%20downstream%20tasks.%20In%20this%20paper%2C%20we%20present%20a%20unified%0Aframework%20named%20CLIP-ADA%20for%20Anomaly%20Detection%20by%20Adapting%20a%20pre-trained%20CLIP%0Amodel.%20To%20this%20end%2C%20we%20make%20two%20important%20improvements%3A%201%29%20To%20acquire%20unified%0Aanomaly%20detection%20across%20industrial%20images%20of%20multiple%20categories%2C%20we%20introduce%0Athe%20learnable%20prompt%20and%20propose%20to%20associate%20it%20with%20abnormal%20patterns%20through%0Aself-supervised%20learning.%202%29%20To%20fully%20exploit%20the%20representation%20power%20of%20CLIP%2C%0Awe%20introduce%20an%20anomaly%20region%20refinement%20strategy%20to%20refine%20the%20localization%0Aquality.%20During%20testing%2C%20the%20anomalies%20are%20localized%20by%20directly%20calculating%0Athe%20similarity%20between%20the%20representation%20of%20the%20learnable%20prompt%20and%20the%0Aimage.%20Comprehensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20framework%2C%0Ae.g.%2C%20we%20achieve%20the%20state-of-the-art%2097.5/55.6%20and%2089.3/33.1%20on%20MVTec-AD%20and%0AVisA%20for%20anomaly%20detection%20and%20localization.%20In%20addition%2C%20the%20proposed%20method%0Aalso%20achieves%20encouraging%20performance%20with%20marginal%20training%20data%2C%20which%20is%0Amore%20challenging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09493v1&entry.124074799=Read"},
{"title": "PosSAM: Panoptic Open-vocabulary Segment Anything", "author": "Vibashan VS and Shubhankar Borse and Hyojin Park and Debasmit Das and Vishal Patel and Munawar Hayat and Fatih Porikli", "abstract": "  In this paper, we introduce an open-vocabulary panoptic segmentation model\nthat effectively unifies the strengths of the Segment Anything Model (SAM) with\nthe vision-language CLIP model in an end-to-end framework. While SAM excels in\ngenerating spatially-aware masks, it's decoder falls short in recognizing\nobject class information and tends to oversegment without additional guidance.\nExisting approaches address this limitation by using multi-stage techniques and\nemploying separate models to generate class-aware prompts, such as bounding\nboxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model\nwhich leverages SAM's spatially rich features to produce instance-aware masks\nand harnesses CLIP's semantically discriminative features for effective\ninstance classification. Specifically, we address the limitations of SAM and\npropose a novel Local Discriminative Pooling (LDP) module leveraging\nclass-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary\nclassification. Furthermore, we introduce a Mask-Aware Selective Ensembling\n(MASE) algorithm that adaptively enhances the quality of generated masks and\nboosts the performance of open-vocabulary classification during inference for\neach image. We conducted extensive experiments to demonstrate our methods\nstrong generalization properties across multiple datasets, achieving\nstate-of-the-art performance with substantial improvements over SOTA\nopen-vocabulary panoptic segmentation methods. In both COCO to ADE20K and\nADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art\nmethods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website:\nhttps://vibashan.github.io/possam-web/.\n", "link": "http://arxiv.org/abs/2403.09620v1", "date": "2024-03-14", "relevancy": 2.8479, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5852}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5263}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything&body=Title%3A%20PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything%0AAuthor%3A%20Vibashan%20VS%20and%20Shubhankar%20Borse%20and%20Hyojin%20Park%20and%20Debasmit%20Das%20and%20Vishal%20Patel%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20open-vocabulary%20panoptic%20segmentation%20model%0Athat%20effectively%20unifies%20the%20strengths%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%0Athe%20vision-language%20CLIP%20model%20in%20an%20end-to-end%20framework.%20While%20SAM%20excels%20in%0Agenerating%20spatially-aware%20masks%2C%20it%27s%20decoder%20falls%20short%20in%20recognizing%0Aobject%20class%20information%20and%20tends%20to%20oversegment%20without%20additional%20guidance.%0AExisting%20approaches%20address%20this%20limitation%20by%20using%20multi-stage%20techniques%20and%0Aemploying%20separate%20models%20to%20generate%20class-aware%20prompts%2C%20such%20as%20bounding%0Aboxes%20or%20segmentation%20masks.%20Our%20proposed%20method%2C%20PosSAM%20is%20an%20end-to-end%20model%0Awhich%20leverages%20SAM%27s%20spatially%20rich%20features%20to%20produce%20instance-aware%20masks%0Aand%20harnesses%20CLIP%27s%20semantically%20discriminative%20features%20for%20effective%0Ainstance%20classification.%20Specifically%2C%20we%20address%20the%20limitations%20of%20SAM%20and%0Apropose%20a%20novel%20Local%20Discriminative%20Pooling%20%28LDP%29%20module%20leveraging%0Aclass-agnostic%20SAM%20and%20class-aware%20CLIP%20features%20for%20unbiased%20open-vocabulary%0Aclassification.%20Furthermore%2C%20we%20introduce%20a%20Mask-Aware%20Selective%20Ensembling%0A%28MASE%29%20algorithm%20that%20adaptively%20enhances%20the%20quality%20of%20generated%20masks%20and%0Aboosts%20the%20performance%20of%20open-vocabulary%20classification%20during%20inference%20for%0Aeach%20image.%20We%20conducted%20extensive%20experiments%20to%20demonstrate%20our%20methods%0Astrong%20generalization%20properties%20across%20multiple%20datasets%2C%20achieving%0Astate-of-the-art%20performance%20with%20substantial%20improvements%20over%20SOTA%0Aopen-vocabulary%20panoptic%20segmentation%20methods.%20In%20both%20COCO%20to%20ADE20K%20and%0AADE20K%20to%20COCO%20settings%2C%20PosSAM%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20by%20a%20large%20margin%2C%202.4%20PQ%20and%204.6%20PQ%2C%20respectively.%20Project%20Website%3A%0Ahttps%3A//vibashan.github.io/possam-web/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PosSAM%3A%20Panoptic%20Open-vocabulary%20Segment%20Anything&entry.906535625=Vibashan%20VS%20and%20Shubhankar%20Borse%20and%20Hyojin%20Park%20and%20Debasmit%20Das%20and%20Vishal%20Patel%20and%20Munawar%20Hayat%20and%20Fatih%20Porikli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20open-vocabulary%20panoptic%20segmentation%20model%0Athat%20effectively%20unifies%20the%20strengths%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%0Athe%20vision-language%20CLIP%20model%20in%20an%20end-to-end%20framework.%20While%20SAM%20excels%20in%0Agenerating%20spatially-aware%20masks%2C%20it%27s%20decoder%20falls%20short%20in%20recognizing%0Aobject%20class%20information%20and%20tends%20to%20oversegment%20without%20additional%20guidance.%0AExisting%20approaches%20address%20this%20limitation%20by%20using%20multi-stage%20techniques%20and%0Aemploying%20separate%20models%20to%20generate%20class-aware%20prompts%2C%20such%20as%20bounding%0Aboxes%20or%20segmentation%20masks.%20Our%20proposed%20method%2C%20PosSAM%20is%20an%20end-to-end%20model%0Awhich%20leverages%20SAM%27s%20spatially%20rich%20features%20to%20produce%20instance-aware%20masks%0Aand%20harnesses%20CLIP%27s%20semantically%20discriminative%20features%20for%20effective%0Ainstance%20classification.%20Specifically%2C%20we%20address%20the%20limitations%20of%20SAM%20and%0Apropose%20a%20novel%20Local%20Discriminative%20Pooling%20%28LDP%29%20module%20leveraging%0Aclass-agnostic%20SAM%20and%20class-aware%20CLIP%20features%20for%20unbiased%20open-vocabulary%0Aclassification.%20Furthermore%2C%20we%20introduce%20a%20Mask-Aware%20Selective%20Ensembling%0A%28MASE%29%20algorithm%20that%20adaptively%20enhances%20the%20quality%20of%20generated%20masks%20and%0Aboosts%20the%20performance%20of%20open-vocabulary%20classification%20during%20inference%20for%0Aeach%20image.%20We%20conducted%20extensive%20experiments%20to%20demonstrate%20our%20methods%0Astrong%20generalization%20properties%20across%20multiple%20datasets%2C%20achieving%0Astate-of-the-art%20performance%20with%20substantial%20improvements%20over%20SOTA%0Aopen-vocabulary%20panoptic%20segmentation%20methods.%20In%20both%20COCO%20to%20ADE20K%20and%0AADE20K%20to%20COCO%20settings%2C%20PosSAM%20outperforms%20the%20previous%20state-of-the-art%0Amethods%20by%20a%20large%20margin%2C%202.4%20PQ%20and%204.6%20PQ%2C%20respectively.%20Project%20Website%3A%0Ahttps%3A//vibashan.github.io/possam-web/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09620v1&entry.124074799=Read"},
{"title": "Envision3D: One Image to 3D with Anchor Views Interpolation", "author": "Yatian Pang and Tanghui Jia and Yujun Shi and Zhenyu Tang and Junwu Zhang and Xinhua Cheng and Xing Zhou and Francis E. H. Tay and Li Yuan", "abstract": "  We present Envision3D, a novel method for efficiently generating high-quality\n3D content from a single image. Recent methods that extract 3D content from\nmulti-view images generated by diffusion models show great potential. However,\nit is still challenging for diffusion models to generate dense multi-view\nconsistent images, which is crucial for the quality of 3D content extraction.\nTo address this issue, we propose a novel cascade diffusion framework, which\ndecomposes the challenging dense views generation task into two tractable\nstages, namely anchor views generation and anchor views interpolation. In the\nfirst stage, we train the image diffusion model to generate global consistent\nanchor views conditioning on image-normal pairs. Subsequently, leveraging our\nvideo diffusion model fine-tuned on consecutive multi-view images, we conduct\ninterpolation on the previous anchor views to generate extra dense views. This\nframework yields dense, multi-view consistent images, providing comprehensive\n3D information. To further enhance the overall generation quality, we introduce\na coarse-to-fine sampling strategy for the reconstruction algorithm to robustly\nextract textured meshes from the generated dense images. Extensive experiments\ndemonstrate that our method is capable of generating high-quality 3D content in\nterms of texture and geometry, surpassing previous image-to-3D baseline\nmethods.\n", "link": "http://arxiv.org/abs/2403.08902v1", "date": "2024-03-13", "relevancy": 2.8329, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5827}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Envision3D%3A%20One%20Image%20to%203D%20with%20Anchor%20Views%20Interpolation&body=Title%3A%20Envision3D%3A%20One%20Image%20to%203D%20with%20Anchor%20Views%20Interpolation%0AAuthor%3A%20Yatian%20Pang%20and%20Tanghui%20Jia%20and%20Yujun%20Shi%20and%20Zhenyu%20Tang%20and%20Junwu%20Zhang%20and%20Xinhua%20Cheng%20and%20Xing%20Zhou%20and%20Francis%20E.%20H.%20Tay%20and%20Li%20Yuan%0AAbstract%3A%20%20%20We%20present%20Envision3D%2C%20a%20novel%20method%20for%20efficiently%20generating%20high-quality%0A3D%20content%20from%20a%20single%20image.%20Recent%20methods%20that%20extract%203D%20content%20from%0Amulti-view%20images%20generated%20by%20diffusion%20models%20show%20great%20potential.%20However%2C%0Ait%20is%20still%20challenging%20for%20diffusion%20models%20to%20generate%20dense%20multi-view%0Aconsistent%20images%2C%20which%20is%20crucial%20for%20the%20quality%20of%203D%20content%20extraction.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20cascade%20diffusion%20framework%2C%20which%0Adecomposes%20the%20challenging%20dense%20views%20generation%20task%20into%20two%20tractable%0Astages%2C%20namely%20anchor%20views%20generation%20and%20anchor%20views%20interpolation.%20In%20the%0Afirst%20stage%2C%20we%20train%20the%20image%20diffusion%20model%20to%20generate%20global%20consistent%0Aanchor%20views%20conditioning%20on%20image-normal%20pairs.%20Subsequently%2C%20leveraging%20our%0Avideo%20diffusion%20model%20fine-tuned%20on%20consecutive%20multi-view%20images%2C%20we%20conduct%0Ainterpolation%20on%20the%20previous%20anchor%20views%20to%20generate%20extra%20dense%20views.%20This%0Aframework%20yields%20dense%2C%20multi-view%20consistent%20images%2C%20providing%20comprehensive%0A3D%20information.%20To%20further%20enhance%20the%20overall%20generation%20quality%2C%20we%20introduce%0Aa%20coarse-to-fine%20sampling%20strategy%20for%20the%20reconstruction%20algorithm%20to%20robustly%0Aextract%20textured%20meshes%20from%20the%20generated%20dense%20images.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20is%20capable%20of%20generating%20high-quality%203D%20content%20in%0Aterms%20of%20texture%20and%20geometry%2C%20surpassing%20previous%20image-to-3D%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08902v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Envision3D%3A%20One%20Image%20to%203D%20with%20Anchor%20Views%20Interpolation&entry.906535625=Yatian%20Pang%20and%20Tanghui%20Jia%20and%20Yujun%20Shi%20and%20Zhenyu%20Tang%20and%20Junwu%20Zhang%20and%20Xinhua%20Cheng%20and%20Xing%20Zhou%20and%20Francis%20E.%20H.%20Tay%20and%20Li%20Yuan&entry.1292438233=%20%20We%20present%20Envision3D%2C%20a%20novel%20method%20for%20efficiently%20generating%20high-quality%0A3D%20content%20from%20a%20single%20image.%20Recent%20methods%20that%20extract%203D%20content%20from%0Amulti-view%20images%20generated%20by%20diffusion%20models%20show%20great%20potential.%20However%2C%0Ait%20is%20still%20challenging%20for%20diffusion%20models%20to%20generate%20dense%20multi-view%0Aconsistent%20images%2C%20which%20is%20crucial%20for%20the%20quality%20of%203D%20content%20extraction.%0ATo%20address%20this%20issue%2C%20we%20propose%20a%20novel%20cascade%20diffusion%20framework%2C%20which%0Adecomposes%20the%20challenging%20dense%20views%20generation%20task%20into%20two%20tractable%0Astages%2C%20namely%20anchor%20views%20generation%20and%20anchor%20views%20interpolation.%20In%20the%0Afirst%20stage%2C%20we%20train%20the%20image%20diffusion%20model%20to%20generate%20global%20consistent%0Aanchor%20views%20conditioning%20on%20image-normal%20pairs.%20Subsequently%2C%20leveraging%20our%0Avideo%20diffusion%20model%20fine-tuned%20on%20consecutive%20multi-view%20images%2C%20we%20conduct%0Ainterpolation%20on%20the%20previous%20anchor%20views%20to%20generate%20extra%20dense%20views.%20This%0Aframework%20yields%20dense%2C%20multi-view%20consistent%20images%2C%20providing%20comprehensive%0A3D%20information.%20To%20further%20enhance%20the%20overall%20generation%20quality%2C%20we%20introduce%0Aa%20coarse-to-fine%20sampling%20strategy%20for%20the%20reconstruction%20algorithm%20to%20robustly%0Aextract%20textured%20meshes%20from%20the%20generated%20dense%20images.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20is%20capable%20of%20generating%20high-quality%203D%20content%20in%0Aterms%20of%20texture%20and%20geometry%2C%20surpassing%20previous%20image-to-3D%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08902v1&entry.124074799=Read"},
{"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn\n  Text-to-Image Generation", "author": "Minbin Huang and Yanxin Long and Xinchi Deng and Ruihang Chu and Jiangfeng Xiong and Xiaodan Liang and Hong Cheng and Qinglin Lu and Wei Liu", "abstract": "  Text-to-image (T2I) generation models have significantly advanced in recent\nyears. However, effective interaction with these models is challenging for\naverage users due to the need for specialized prompt engineering knowledge and\nthe inability to perform multi-turn image generation, hindering a dynamic and\niterative creation process. Recent attempts have tried to equip Multi-modal\nLarge Language Models (MLLMs) with T2I models to bring the user's natural\nlanguage instructions into reality. Hence, the output modality of MLLMs is\nextended, and the multi-turn generation quality of T2I models is enhanced\nthanks to the strong multi-modal comprehension ability of MLLMs. However, many\nof these works face challenges in identifying correct output modalities and\ngenerating coherent images accordingly as the number of output modalities\nincreases and the conversations go deeper. Therefore, we propose DialogGen, an\neffective pipeline to align off-the-shelf MLLMs and T2I models to build a\nMulti-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image\ngeneration. It is composed of drawing prompt alignment, careful training data\ncuration, and error correction. Moreover, as the field of MIDS flourishes,\ncomprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms\nof output modality correctness and multi-modal output coherence. To address\nthis issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a\ncomprehensive bilingual benchmark designed to assess the ability of MLLMs to\ngenerate accurate and coherent multi-modal content that supports image editing.\nIt contains two evaluation metrics to measure the model's ability to switch\nmodalities and the coherence of the output images. Our extensive experiments on\nDialogBen and user study demonstrate the effectiveness of DialogGen compared\nwith other State-of-the-Art models.\n", "link": "http://arxiv.org/abs/2403.08857v1", "date": "2024-03-13", "relevancy": 2.828, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5784}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&body=Title%3A%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08857v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&entry.906535625=Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08857v1&entry.124074799=Read"},
{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "author": "Qingqiu Li and Xiaohan Yan and Jilan Xu and Runtian Yuan and Yuejie Zhang and Rui Feng and Quanli Shen and Xiaobo Zhang and Shujun Wang", "abstract": "  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.09294v1", "date": "2024-03-14", "relevancy": 2.8002, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5971}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5439}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5392}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training&body=Title%3A%20Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training%0AAuthor%3A%20Qingqiu%20Li%20and%20Xiaohan%20Yan%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Shujun%20Wang%0AAbstract%3A%20%20%20Learning%20medical%20visual%20representations%20through%20vision-language%20pre-training%0Ahas%20reached%20remarkable%20progress.%20Despite%20the%20promising%20performance%2C%20it%20still%0Afaces%20challenges%2C%20i.e.%2C%20local%20alignment%20lacks%20interpretability%20and%20clinical%0Arelevance%2C%20and%20the%20insufficient%20internal%20and%20external%20representation%20learning%0Aof%20image-report%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Anatomical%0AStructure-Guided%20%28ASG%29%20framework.%20Specifically%2C%20we%20parse%20raw%20reports%20into%0Atriplets%20%3Canatomical%20region%2C%20finding%2C%20existence%3E%2C%20and%20fully%20utilize%20each%0Aelement%20as%20supervision%20to%20enhance%20representation%20learning.%20For%20anatomical%0Aregion%2C%20we%20design%20an%20automatic%20anatomical%20region-sentence%20alignment%20paradigm%20in%0Acollaboration%20with%20radiologists%2C%20considering%20them%20as%20the%20minimum%20semantic%20units%0Ato%20explore%20fine-grained%20local%20alignment.%20For%20finding%20and%20existence%2C%20we%20regard%0Athem%20as%20image%20tags%2C%20applying%20an%20image-tag%20recognition%20decoder%20to%20associate%0Aimage%20features%20with%20their%20respective%20tags%20within%20each%20sample%20and%20constructing%0Asoft%20labels%20for%20contrastive%20learning%20to%20improve%20the%20semantic%20association%20of%0Adifferent%20image-report%20pairs.%20We%20evaluate%20the%20proposed%20ASG%20framework%20on%20two%0Adownstream%20tasks%2C%20including%20five%20public%20benchmarks.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09294v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomical%20Structure-Guided%20Medical%20Vision-Language%20Pre-training&entry.906535625=Qingqiu%20Li%20and%20Xiaohan%20Yan%20and%20Jilan%20Xu%20and%20Runtian%20Yuan%20and%20Yuejie%20Zhang%20and%20Rui%20Feng%20and%20Quanli%20Shen%20and%20Xiaobo%20Zhang%20and%20Shujun%20Wang&entry.1292438233=%20%20Learning%20medical%20visual%20representations%20through%20vision-language%20pre-training%0Ahas%20reached%20remarkable%20progress.%20Despite%20the%20promising%20performance%2C%20it%20still%0Afaces%20challenges%2C%20i.e.%2C%20local%20alignment%20lacks%20interpretability%20and%20clinical%0Arelevance%2C%20and%20the%20insufficient%20internal%20and%20external%20representation%20learning%0Aof%20image-report%20pairs.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Anatomical%0AStructure-Guided%20%28ASG%29%20framework.%20Specifically%2C%20we%20parse%20raw%20reports%20into%0Atriplets%20%3Canatomical%20region%2C%20finding%2C%20existence%3E%2C%20and%20fully%20utilize%20each%0Aelement%20as%20supervision%20to%20enhance%20representation%20learning.%20For%20anatomical%0Aregion%2C%20we%20design%20an%20automatic%20anatomical%20region-sentence%20alignment%20paradigm%20in%0Acollaboration%20with%20radiologists%2C%20considering%20them%20as%20the%20minimum%20semantic%20units%0Ato%20explore%20fine-grained%20local%20alignment.%20For%20finding%20and%20existence%2C%20we%20regard%0Athem%20as%20image%20tags%2C%20applying%20an%20image-tag%20recognition%20decoder%20to%20associate%0Aimage%20features%20with%20their%20respective%20tags%20within%20each%20sample%20and%20constructing%0Asoft%20labels%20for%20contrastive%20learning%20to%20improve%20the%20semantic%20association%20of%0Adifferent%20image-report%20pairs.%20We%20evaluate%20the%20proposed%20ASG%20framework%20on%20two%0Adownstream%20tasks%2C%20including%20five%20public%20benchmarks.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09294v1&entry.124074799=Read"},
{"title": "Probabilistic Contrastive Learning for Long-Tailed Visual Recognition", "author": "Chaoqun Du and Yulin Wang and Shiji Song and Gao Huang", "abstract": "  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n", "link": "http://arxiv.org/abs/2403.06726v2", "date": "2024-03-14", "relevancy": 2.7962, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6068}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5441}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&body=Title%3A%20Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition%0AAuthor%3A%20Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06726v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Contrastive%20Learning%20for%20Long-Tailed%20Visual%20Recognition&entry.906535625=Chaoqun%20Du%20and%20Yulin%20Wang%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Long-tailed%20distributions%20frequently%20emerge%20in%20real-world%20data%2C%20where%20a%20large%0Anumber%20of%20minority%20categories%20contain%20a%20limited%20number%20of%20samples.%20Such%0Aimbalance%20issue%20considerably%20impairs%20the%20performance%20of%20standard%20supervised%0Alearning%20algorithms%2C%20which%20are%20mainly%20designed%20for%20balanced%20training%20sets.%0ARecent%20investigations%20have%20revealed%20that%20supervised%20contrastive%20learning%0Aexhibits%20promising%20potential%20in%20alleviating%20the%20data%20imbalance.%20However%2C%20the%0Aperformance%20of%20supervised%20contrastive%20learning%20is%20plagued%20by%20an%20inherent%0Achallenge%3A%20it%20necessitates%20sufficiently%20large%20batches%20of%20training%20data%20to%0Aconstruct%20contrastive%20pairs%20that%20cover%20all%20categories%2C%20yet%20this%20requirement%20is%0Adifficult%20to%20meet%20in%20the%20context%20of%20class-imbalanced%20data.%20To%20overcome%20this%0Aobstacle%2C%20we%20propose%20a%20novel%20probabilistic%20contrastive%20%28ProCo%29%20learning%0Aalgorithm%20that%20estimates%20the%20data%20distribution%20of%20the%20samples%20from%20each%20class%0Ain%20the%20feature%20space%2C%20and%20samples%20contrastive%20pairs%20accordingly.%20In%20fact%2C%0Aestimating%20the%20distributions%20of%20all%20classes%20using%20features%20in%20a%20small%20batch%2C%0Aparticularly%20for%20imbalanced%20data%2C%20is%20not%20feasible.%20Our%20key%20idea%20is%20to%20introduce%0Aa%20reasonable%20and%20simple%20assumption%20that%20the%20normalized%20features%20in%20contrastive%0Alearning%20follow%20a%20mixture%20of%20von%20Mises-Fisher%20%28vMF%29%20distributions%20on%20unit%0Aspace%2C%20which%20brings%20two-fold%20benefits.%20First%2C%20the%20distribution%20parameters%20can%0Abe%20estimated%20using%20only%20the%20first%20sample%20moment%2C%20which%20can%20be%20efficiently%0Acomputed%20in%20an%20online%20manner%20across%20different%20batches.%20Second%2C%20based%20on%20the%0Aestimated%20distribution%2C%20the%20vMF%20distribution%20allows%20us%20to%20sample%20an%20infinite%0Anumber%20of%20contrastive%20pairs%20and%20derive%20a%20closed%20form%20of%20the%20expected%0Acontrastive%20loss%20for%20efficient%20optimization.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/ProCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06726v2&entry.124074799=Read"},
{"title": "Convolutional Neural Network-based Efficient Dense Point Cloud\n  Generation using Unsigned Distance Fields", "author": "Abol Basher and Jani Boutellier", "abstract": "  Dense point cloud generation from a sparse or incomplete point cloud is a\ncrucial and challenging problem in 3D computer vision and computer graphics. So\nfar, the existing methods are either computationally too expensive, suffer from\nlimited resolution, or both. In addition, some methods are strictly limited to\nwatertight surfaces -- another major obstacle for a number of applications. To\naddress these issues, we propose a lightweight Convolutional Neural Network\nthat learns and predicts the unsigned distance field for arbitrary 3D shapes\nfor dense point cloud generation using the recently emerged concept of implicit\nfunction learning. Experiments demonstrate that the proposed architecture\noutperforms the state of the art by 7.8x less model parameters, 2.4x faster\ninference time and up to 24.8% improved generation quality compared to the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2203.11537v3", "date": "2024-03-13", "relevancy": 2.7869, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Neural%20Network-based%20Efficient%20Dense%20Point%20Cloud%0A%20%20Generation%20using%20Unsigned%20Distance%20Fields&body=Title%3A%20Convolutional%20Neural%20Network-based%20Efficient%20Dense%20Point%20Cloud%0A%20%20Generation%20using%20Unsigned%20Distance%20Fields%0AAuthor%3A%20Abol%20Basher%20and%20Jani%20Boutellier%0AAbstract%3A%20%20%20Dense%20point%20cloud%20generation%20from%20a%20sparse%20or%20incomplete%20point%20cloud%20is%20a%0Acrucial%20and%20challenging%20problem%20in%203D%20computer%20vision%20and%20computer%20graphics.%20So%0Afar%2C%20the%20existing%20methods%20are%20either%20computationally%20too%20expensive%2C%20suffer%20from%0Alimited%20resolution%2C%20or%20both.%20In%20addition%2C%20some%20methods%20are%20strictly%20limited%20to%0Awatertight%20surfaces%20--%20another%20major%20obstacle%20for%20a%20number%20of%20applications.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20lightweight%20Convolutional%20Neural%20Network%0Athat%20learns%20and%20predicts%20the%20unsigned%20distance%20field%20for%20arbitrary%203D%20shapes%0Afor%20dense%20point%20cloud%20generation%20using%20the%20recently%20emerged%20concept%20of%20implicit%0Afunction%20learning.%20Experiments%20demonstrate%20that%20the%20proposed%20architecture%0Aoutperforms%20the%20state%20of%20the%20art%20by%207.8x%20less%20model%20parameters%2C%202.4x%20faster%0Ainference%20time%20and%20up%20to%2024.8%25%20improved%20generation%20quality%20compared%20to%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.11537v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Neural%20Network-based%20Efficient%20Dense%20Point%20Cloud%0A%20%20Generation%20using%20Unsigned%20Distance%20Fields&entry.906535625=Abol%20Basher%20and%20Jani%20Boutellier&entry.1292438233=%20%20Dense%20point%20cloud%20generation%20from%20a%20sparse%20or%20incomplete%20point%20cloud%20is%20a%0Acrucial%20and%20challenging%20problem%20in%203D%20computer%20vision%20and%20computer%20graphics.%20So%0Afar%2C%20the%20existing%20methods%20are%20either%20computationally%20too%20expensive%2C%20suffer%20from%0Alimited%20resolution%2C%20or%20both.%20In%20addition%2C%20some%20methods%20are%20strictly%20limited%20to%0Awatertight%20surfaces%20--%20another%20major%20obstacle%20for%20a%20number%20of%20applications.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20a%20lightweight%20Convolutional%20Neural%20Network%0Athat%20learns%20and%20predicts%20the%20unsigned%20distance%20field%20for%20arbitrary%203D%20shapes%0Afor%20dense%20point%20cloud%20generation%20using%20the%20recently%20emerged%20concept%20of%20implicit%0Afunction%20learning.%20Experiments%20demonstrate%20that%20the%20proposed%20architecture%0Aoutperforms%20the%20state%20of%20the%20art%20by%207.8x%20less%20model%20parameters%2C%202.4x%20faster%0Ainference%20time%20and%20up%20to%2024.8%25%20improved%20generation%20quality%20compared%20to%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.11537v3&entry.124074799=Read"},
{"title": "Perspective-Equivariant Imaging: an Unsupervised Framework for\n  Multispectral Pansharpening", "author": "Andrew Wang and Mike Davies", "abstract": "  Ill-posed image reconstruction problems appear in many scenarios such as\nremote sensing, where obtaining high quality images is crucial for\nenvironmental monitoring, disaster management and urban planning. Deep learning\nhas seen great success in overcoming the limitations of traditional methods.\nHowever, these inverse problems rarely come with ground truth data,\nhighlighting the importance of unsupervised learning from partial and noisy\nmeasurements alone. We propose perspective-equivariant imaging (EI), a\nframework that leverages perspective variability in optical camera-based\nimaging systems, such as satellites or handheld cameras, to recover information\nlost in ill-posed optical camera imaging problems. This extends previous EI\nwork to include a much richer non-linear class of group transforms and is shown\nto be an excellent prior for satellite and urban image data, where\nperspective-EI achieves state-of-the-art results in multispectral\npansharpening, outperforming other unsupervised methods in the literature. Code\nat https://andrewwango.github.io/perspective-equivariant-imaging\n", "link": "http://arxiv.org/abs/2403.09327v1", "date": "2024-03-14", "relevancy": 2.7863, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5754}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5518}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5445}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening&body=Title%3A%20Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening%0AAuthor%3A%20Andrew%20Wang%20and%20Mike%20Davies%0AAbstract%3A%20%20%20Ill-posed%20image%20reconstruction%20problems%20appear%20in%20many%20scenarios%20such%20as%0Aremote%20sensing%2C%20where%20obtaining%20high%20quality%20images%20is%20crucial%20for%0Aenvironmental%20monitoring%2C%20disaster%20management%20and%20urban%20planning.%20Deep%20learning%0Ahas%20seen%20great%20success%20in%20overcoming%20the%20limitations%20of%20traditional%20methods.%0AHowever%2C%20these%20inverse%20problems%20rarely%20come%20with%20ground%20truth%20data%2C%0Ahighlighting%20the%20importance%20of%20unsupervised%20learning%20from%20partial%20and%20noisy%0Ameasurements%20alone.%20We%20propose%20perspective-equivariant%20imaging%20%28EI%29%2C%20a%0Aframework%20that%20leverages%20perspective%20variability%20in%20optical%20camera-based%0Aimaging%20systems%2C%20such%20as%20satellites%20or%20handheld%20cameras%2C%20to%20recover%20information%0Alost%20in%20ill-posed%20optical%20camera%20imaging%20problems.%20This%20extends%20previous%20EI%0Awork%20to%20include%20a%20much%20richer%20non-linear%20class%20of%20group%20transforms%20and%20is%20shown%0Ato%20be%20an%20excellent%20prior%20for%20satellite%20and%20urban%20image%20data%2C%20where%0Aperspective-EI%20achieves%20state-of-the-art%20results%20in%20multispectral%0Apansharpening%2C%20outperforming%20other%20unsupervised%20methods%20in%20the%20literature.%20Code%0Aat%20https%3A//andrewwango.github.io/perspective-equivariant-imaging%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09327v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perspective-Equivariant%20Imaging%3A%20an%20Unsupervised%20Framework%20for%0A%20%20Multispectral%20Pansharpening&entry.906535625=Andrew%20Wang%20and%20Mike%20Davies&entry.1292438233=%20%20Ill-posed%20image%20reconstruction%20problems%20appear%20in%20many%20scenarios%20such%20as%0Aremote%20sensing%2C%20where%20obtaining%20high%20quality%20images%20is%20crucial%20for%0Aenvironmental%20monitoring%2C%20disaster%20management%20and%20urban%20planning.%20Deep%20learning%0Ahas%20seen%20great%20success%20in%20overcoming%20the%20limitations%20of%20traditional%20methods.%0AHowever%2C%20these%20inverse%20problems%20rarely%20come%20with%20ground%20truth%20data%2C%0Ahighlighting%20the%20importance%20of%20unsupervised%20learning%20from%20partial%20and%20noisy%0Ameasurements%20alone.%20We%20propose%20perspective-equivariant%20imaging%20%28EI%29%2C%20a%0Aframework%20that%20leverages%20perspective%20variability%20in%20optical%20camera-based%0Aimaging%20systems%2C%20such%20as%20satellites%20or%20handheld%20cameras%2C%20to%20recover%20information%0Alost%20in%20ill-posed%20optical%20camera%20imaging%20problems.%20This%20extends%20previous%20EI%0Awork%20to%20include%20a%20much%20richer%20non-linear%20class%20of%20group%20transforms%20and%20is%20shown%0Ato%20be%20an%20excellent%20prior%20for%20satellite%20and%20urban%20image%20data%2C%20where%0Aperspective-EI%20achieves%20state-of-the-art%20results%20in%20multispectral%0Apansharpening%2C%20outperforming%20other%20unsupervised%20methods%20in%20the%20literature.%20Code%0Aat%20https%3A//andrewwango.github.io/perspective-equivariant-imaging%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09327v1&entry.124074799=Read"},
{"title": "SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion\n  using a 3D Recurrent U-Net", "author": "Helin Cao and Sven Behnke", "abstract": "  We introduce SLCF-Net, a novel approach for the Semantic Scene Completion\n(SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates\nmissing geometry and semantics in a scene from sequences of RGB images and\nsparse LiDAR measurements. The images are semantically segmented by a\npre-trained 2D U-Net and a dense depth prior is estimated from a\ndepth-conditioned pipeline fueled by Depth Anything. To associate the 2D image\nfeatures with the 3D scene volume, we introduce Gaussian-decay Depth-prior\nProjection (GDP). This module projects the 2D features into the 3D volume along\nthe line of sight with a Gaussian-decay function, centered around the depth\nprior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden\n3D U-Net state using the sensor motion and design a novel loss to ensure\ntemporal consistency. We evaluate our approach on the SemanticKITTI dataset and\ncompare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics\nand shows great temporal consistency.\n", "link": "http://arxiv.org/abs/2403.08885v1", "date": "2024-03-13", "relevancy": 2.7725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5704}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SLCF-Net%3A%20Sequential%20LiDAR-Camera%20Fusion%20for%20Semantic%20Scene%20Completion%0A%20%20using%20a%203D%20Recurrent%20U-Net&body=Title%3A%20SLCF-Net%3A%20Sequential%20LiDAR-Camera%20Fusion%20for%20Semantic%20Scene%20Completion%0A%20%20using%20a%203D%20Recurrent%20U-Net%0AAuthor%3A%20Helin%20Cao%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20We%20introduce%20SLCF-Net%2C%20a%20novel%20approach%20for%20the%20Semantic%20Scene%20Completion%0A%28SSC%29%20task%20that%20sequentially%20fuses%20LiDAR%20and%20camera%20data.%20It%20jointly%20estimates%0Amissing%20geometry%20and%20semantics%20in%20a%20scene%20from%20sequences%20of%20RGB%20images%20and%0Asparse%20LiDAR%20measurements.%20The%20images%20are%20semantically%20segmented%20by%20a%0Apre-trained%202D%20U-Net%20and%20a%20dense%20depth%20prior%20is%20estimated%20from%20a%0Adepth-conditioned%20pipeline%20fueled%20by%20Depth%20Anything.%20To%20associate%20the%202D%20image%0Afeatures%20with%20the%203D%20scene%20volume%2C%20we%20introduce%20Gaussian-decay%20Depth-prior%0AProjection%20%28GDP%29.%20This%20module%20projects%20the%202D%20features%20into%20the%203D%20volume%20along%0Athe%20line%20of%20sight%20with%20a%20Gaussian-decay%20function%2C%20centered%20around%20the%20depth%0Aprior.%20Volumetric%20semantics%20is%20computed%20by%20a%203D%20U-Net.%20We%20propagate%20the%20hidden%0A3D%20U-Net%20state%20using%20the%20sensor%20motion%20and%20design%20a%20novel%20loss%20to%20ensure%0Atemporal%20consistency.%20We%20evaluate%20our%20approach%20on%20the%20SemanticKITTI%20dataset%20and%0Acompare%20it%20with%20leading%20SSC%20approaches.%20The%20SLCF-Net%20excels%20in%20all%20SSC%20metrics%0Aand%20shows%20great%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08885v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLCF-Net%3A%20Sequential%20LiDAR-Camera%20Fusion%20for%20Semantic%20Scene%20Completion%0A%20%20using%20a%203D%20Recurrent%20U-Net&entry.906535625=Helin%20Cao%20and%20Sven%20Behnke&entry.1292438233=%20%20We%20introduce%20SLCF-Net%2C%20a%20novel%20approach%20for%20the%20Semantic%20Scene%20Completion%0A%28SSC%29%20task%20that%20sequentially%20fuses%20LiDAR%20and%20camera%20data.%20It%20jointly%20estimates%0Amissing%20geometry%20and%20semantics%20in%20a%20scene%20from%20sequences%20of%20RGB%20images%20and%0Asparse%20LiDAR%20measurements.%20The%20images%20are%20semantically%20segmented%20by%20a%0Apre-trained%202D%20U-Net%20and%20a%20dense%20depth%20prior%20is%20estimated%20from%20a%0Adepth-conditioned%20pipeline%20fueled%20by%20Depth%20Anything.%20To%20associate%20the%202D%20image%0Afeatures%20with%20the%203D%20scene%20volume%2C%20we%20introduce%20Gaussian-decay%20Depth-prior%0AProjection%20%28GDP%29.%20This%20module%20projects%20the%202D%20features%20into%20the%203D%20volume%20along%0Athe%20line%20of%20sight%20with%20a%20Gaussian-decay%20function%2C%20centered%20around%20the%20depth%0Aprior.%20Volumetric%20semantics%20is%20computed%20by%20a%203D%20U-Net.%20We%20propagate%20the%20hidden%0A3D%20U-Net%20state%20using%20the%20sensor%20motion%20and%20design%20a%20novel%20loss%20to%20ensure%0Atemporal%20consistency.%20We%20evaluate%20our%20approach%20on%20the%20SemanticKITTI%20dataset%20and%0Acompare%20it%20with%20leading%20SSC%20approaches.%20The%20SLCF-Net%20excels%20in%20all%20SSC%20metrics%0Aand%20shows%20great%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08885v1&entry.124074799=Read"},
{"title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for\n  Hierarchical Understanding", "author": "Peng Xia and Xingtong Yu and Ming Hu and Lie Ju and Zhiyong Wang and Peibo Duan and Zongyuan Ge", "abstract": "  Object categories are typically organized into a multi-granularity taxonomic\nhierarchy. When classifying categories at different hierarchy levels,\ntraditional uni-modal approaches focus primarily on image features, revealing\nlimitations in complex scenarios. Recent studies integrating Vision-Language\nModels (VLMs) with class hierarchies have shown promise, yet they fall short of\nfully exploiting the hierarchical relationships. These efforts are constrained\nby their inability to perform effectively across varied granularity of\ncategories. To tackle this issue, we propose a novel framework (HGCLIP) that\neffectively combines CLIP with a deeper exploitation of the Hierarchical class\nstructure via Graph representation learning. We explore constructing the class\nhierarchy into a graph, with its nodes representing the textual or image\nfeatures of each category. After passing through a graph encoder, the textual\nfeatures incorporate hierarchical structure information, while the image\nfeatures emphasize class-aware features derived from prototypes through the\nattention mechanism. Our approach demonstrates significant improvements on 11\ndiverse visual recognition benchmarks. Our codes are fully available at\nhttps://github.com/richard-peng-xia/HGCLIP.\n", "link": "http://arxiv.org/abs/2311.14064v2", "date": "2024-03-14", "relevancy": 2.7639, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.578}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5618}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding&body=Title%3A%20HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding%0AAuthor%3A%20Peng%20Xia%20and%20Xingtong%20Yu%20and%20Ming%20Hu%20and%20Lie%20Ju%20and%20Zhiyong%20Wang%20and%20Peibo%20Duan%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20Object%20categories%20are%20typically%20organized%20into%20a%20multi-granularity%20taxonomic%0Ahierarchy.%20When%20classifying%20categories%20at%20different%20hierarchy%20levels%2C%0Atraditional%20uni-modal%20approaches%20focus%20primarily%20on%20image%20features%2C%20revealing%0Alimitations%20in%20complex%20scenarios.%20Recent%20studies%20integrating%20Vision-Language%0AModels%20%28VLMs%29%20with%20class%20hierarchies%20have%20shown%20promise%2C%20yet%20they%20fall%20short%20of%0Afully%20exploiting%20the%20hierarchical%20relationships.%20These%20efforts%20are%20constrained%0Aby%20their%20inability%20to%20perform%20effectively%20across%20varied%20granularity%20of%0Acategories.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20%28HGCLIP%29%20that%0Aeffectively%20combines%20CLIP%20with%20a%20deeper%20exploitation%20of%20the%20Hierarchical%20class%0Astructure%20via%20Graph%20representation%20learning.%20We%20explore%20constructing%20the%20class%0Ahierarchy%20into%20a%20graph%2C%20with%20its%20nodes%20representing%20the%20textual%20or%20image%0Afeatures%20of%20each%20category.%20After%20passing%20through%20a%20graph%20encoder%2C%20the%20textual%0Afeatures%20incorporate%20hierarchical%20structure%20information%2C%20while%20the%20image%0Afeatures%20emphasize%20class-aware%20features%20derived%20from%20prototypes%20through%20the%0Aattention%20mechanism.%20Our%20approach%20demonstrates%20significant%20improvements%20on%2011%0Adiverse%20visual%20recognition%20benchmarks.%20Our%20codes%20are%20fully%20available%20at%0Ahttps%3A//github.com/richard-peng-xia/HGCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14064v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGCLIP%3A%20Exploring%20Vision-Language%20Models%20with%20Graph%20Representations%20for%0A%20%20Hierarchical%20Understanding&entry.906535625=Peng%20Xia%20and%20Xingtong%20Yu%20and%20Ming%20Hu%20and%20Lie%20Ju%20and%20Zhiyong%20Wang%20and%20Peibo%20Duan%20and%20Zongyuan%20Ge&entry.1292438233=%20%20Object%20categories%20are%20typically%20organized%20into%20a%20multi-granularity%20taxonomic%0Ahierarchy.%20When%20classifying%20categories%20at%20different%20hierarchy%20levels%2C%0Atraditional%20uni-modal%20approaches%20focus%20primarily%20on%20image%20features%2C%20revealing%0Alimitations%20in%20complex%20scenarios.%20Recent%20studies%20integrating%20Vision-Language%0AModels%20%28VLMs%29%20with%20class%20hierarchies%20have%20shown%20promise%2C%20yet%20they%20fall%20short%20of%0Afully%20exploiting%20the%20hierarchical%20relationships.%20These%20efforts%20are%20constrained%0Aby%20their%20inability%20to%20perform%20effectively%20across%20varied%20granularity%20of%0Acategories.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20%28HGCLIP%29%20that%0Aeffectively%20combines%20CLIP%20with%20a%20deeper%20exploitation%20of%20the%20Hierarchical%20class%0Astructure%20via%20Graph%20representation%20learning.%20We%20explore%20constructing%20the%20class%0Ahierarchy%20into%20a%20graph%2C%20with%20its%20nodes%20representing%20the%20textual%20or%20image%0Afeatures%20of%20each%20category.%20After%20passing%20through%20a%20graph%20encoder%2C%20the%20textual%0Afeatures%20incorporate%20hierarchical%20structure%20information%2C%20while%20the%20image%0Afeatures%20emphasize%20class-aware%20features%20derived%20from%20prototypes%20through%20the%0Aattention%20mechanism.%20Our%20approach%20demonstrates%20significant%20improvements%20on%2011%0Adiverse%20visual%20recognition%20benchmarks.%20Our%20codes%20are%20fully%20available%20at%0Ahttps%3A//github.com/richard-peng-xia/HGCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14064v2&entry.124074799=Read"},
{"title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap\n  for Domain-Adaptive Object Detection", "author": "Dinh Phat Do and Taehoon Kim and Jaemin Na and Jiwon Kim and Keonho Lee and Kyunghwan Cho and Wonjun Hwang", "abstract": "  Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .\n", "link": "http://arxiv.org/abs/2403.09359v1", "date": "2024-03-14", "relevancy": 2.7576, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5943}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5552}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.505}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection&body=Title%3A%20D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection%0AAuthor%3A%20Dinh%20Phat%20Do%20and%20Taehoon%20Kim%20and%20Jaemin%20Na%20and%20Jiwon%20Kim%20and%20Keonho%20Lee%20and%20Kyunghwan%20Cho%20and%20Wonjun%20Hwang%0AAbstract%3A%20%20%20Domain%20adaptation%20for%20object%20detection%20typically%20entails%20transferring%0Aknowledge%20from%20one%20visible%20domain%20to%20another%20visible%20domain.%20However%2C%20there%20are%0Alimited%20studies%20on%20adapting%20from%20the%20visible%20to%20the%20thermal%20domain%2C%20because%20the%0Adomain%20gap%20between%20the%20visible%20and%20thermal%20domains%20is%20much%20larger%20than%0Aexpected%2C%20and%20traditional%20domain%20adaptation%20can%20not%20successfully%20facilitate%0Alearning%20in%20this%20situation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0ADistinctive%20Dual-Domain%20Teacher%20%28D3T%29%20framework%20that%20employs%20distinct%20training%0Aparadigms%20for%20each%20domain.%20Specifically%2C%20we%20segregate%20the%20source%20and%20target%0Atraining%20sets%20for%20building%20dual-teachers%20and%20successively%20deploy%20exponential%0Amoving%20average%20to%20the%20student%20model%20to%20individual%20teachers%20of%20each%20domain.%20The%0Aframework%20further%20incorporates%20a%20zigzag%20learning%20method%20between%20dual%20teachers%2C%0Afacilitating%20a%20gradual%20transition%20from%20the%20visible%20to%20thermal%20domains%20during%0Atraining.%20We%20validate%20the%20superiority%20of%20our%20method%20through%20newly%20designed%0Aexperimental%20protocols%20with%20well-known%20thermal%20datasets%2C%20i.e.%2C%20FLIR%20and%20KAIST.%0ASource%20code%20is%20available%20at%20https%3A//github.com/EdwardDo69/D3T%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09359v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3T%3A%20Distinctive%20Dual-Domain%20Teacher%20Zigzagging%20Across%20RGB-Thermal%20Gap%0A%20%20for%20Domain-Adaptive%20Object%20Detection&entry.906535625=Dinh%20Phat%20Do%20and%20Taehoon%20Kim%20and%20Jaemin%20Na%20and%20Jiwon%20Kim%20and%20Keonho%20Lee%20and%20Kyunghwan%20Cho%20and%20Wonjun%20Hwang&entry.1292438233=%20%20Domain%20adaptation%20for%20object%20detection%20typically%20entails%20transferring%0Aknowledge%20from%20one%20visible%20domain%20to%20another%20visible%20domain.%20However%2C%20there%20are%0Alimited%20studies%20on%20adapting%20from%20the%20visible%20to%20the%20thermal%20domain%2C%20because%20the%0Adomain%20gap%20between%20the%20visible%20and%20thermal%20domains%20is%20much%20larger%20than%0Aexpected%2C%20and%20traditional%20domain%20adaptation%20can%20not%20successfully%20facilitate%0Alearning%20in%20this%20situation.%20To%20overcome%20this%20challenge%2C%20we%20propose%20a%0ADistinctive%20Dual-Domain%20Teacher%20%28D3T%29%20framework%20that%20employs%20distinct%20training%0Aparadigms%20for%20each%20domain.%20Specifically%2C%20we%20segregate%20the%20source%20and%20target%0Atraining%20sets%20for%20building%20dual-teachers%20and%20successively%20deploy%20exponential%0Amoving%20average%20to%20the%20student%20model%20to%20individual%20teachers%20of%20each%20domain.%20The%0Aframework%20further%20incorporates%20a%20zigzag%20learning%20method%20between%20dual%20teachers%2C%0Afacilitating%20a%20gradual%20transition%20from%20the%20visible%20to%20thermal%20domains%20during%0Atraining.%20We%20validate%20the%20superiority%20of%20our%20method%20through%20newly%20designed%0Aexperimental%20protocols%20with%20well-known%20thermal%20datasets%2C%20i.e.%2C%20FLIR%20and%20KAIST.%0ASource%20code%20is%20available%20at%20https%3A//github.com/EdwardDo69/D3T%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09359v1&entry.124074799=Read"},
{"title": "PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of\n  Interest", "author": "Jiajun Deng and Sha Zhang and Feras Dayoub and Wanli Ouyang and Yanyong Zhang and Ian Reid", "abstract": "  In this work, we present PoIFusion, a simple yet effective multi-modal 3D\nobject detection framework to fuse the information of RGB images and LiDAR\npoint clouds at the point of interest (abbreviated as PoI). Technically, our\nPoIFusion follows the paradigm of query-based object detection, formulating\nobject queries as dynamic 3D boxes. The PoIs are adaptively generated from each\nquery box on the fly, serving as the keypoints to represent a 3D object and\nplay the role of basic units in multi-modal fusion. Specifically, we project\nPoIs into the view of each modality to sample the corresponding feature and\nintegrate the multi-modal features at each PoI through a dynamic fusion block.\nFurthermore, the features of PoIs derived from the same query box are\naggregated together to update the query feature. Our approach prevents\ninformation loss caused by view transformation and eliminates the\ncomputation-intensive global attention, making the multi-modal 3D object\ndetector more applicable. We conducted extensive experiments on the nuScenes\ndataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\\% NDS\nand 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object\ndetection benchmark. Codes will be made available via\n\\url{https://djiajunustc.github.io/projects/poifusion}.\n", "link": "http://arxiv.org/abs/2403.09212v1", "date": "2024-03-14", "relevancy": 2.7387, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5596}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5454}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PoIFusion%3A%20Multi-Modal%203D%20Object%20Detection%20via%20Fusion%20at%20Points%20of%0A%20%20Interest&body=Title%3A%20PoIFusion%3A%20Multi-Modal%203D%20Object%20Detection%20via%20Fusion%20at%20Points%20of%0A%20%20Interest%0AAuthor%3A%20Jiajun%20Deng%20and%20Sha%20Zhang%20and%20Feras%20Dayoub%20and%20Wanli%20Ouyang%20and%20Yanyong%20Zhang%20and%20Ian%20Reid%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20PoIFusion%2C%20a%20simple%20yet%20effective%20multi-modal%203D%0Aobject%20detection%20framework%20to%20fuse%20the%20information%20of%20RGB%20images%20and%20LiDAR%0Apoint%20clouds%20at%20the%20point%20of%20interest%20%28abbreviated%20as%20PoI%29.%20Technically%2C%20our%0APoIFusion%20follows%20the%20paradigm%20of%20query-based%20object%20detection%2C%20formulating%0Aobject%20queries%20as%20dynamic%203D%20boxes.%20The%20PoIs%20are%20adaptively%20generated%20from%20each%0Aquery%20box%20on%20the%20fly%2C%20serving%20as%20the%20keypoints%20to%20represent%20a%203D%20object%20and%0Aplay%20the%20role%20of%20basic%20units%20in%20multi-modal%20fusion.%20Specifically%2C%20we%20project%0APoIs%20into%20the%20view%20of%20each%20modality%20to%20sample%20the%20corresponding%20feature%20and%0Aintegrate%20the%20multi-modal%20features%20at%20each%20PoI%20through%20a%20dynamic%20fusion%20block.%0AFurthermore%2C%20the%20features%20of%20PoIs%20derived%20from%20the%20same%20query%20box%20are%0Aaggregated%20together%20to%20update%20the%20query%20feature.%20Our%20approach%20prevents%0Ainformation%20loss%20caused%20by%20view%20transformation%20and%20eliminates%20the%0Acomputation-intensive%20global%20attention%2C%20making%20the%20multi-modal%203D%20object%0Adetector%20more%20applicable.%20We%20conducted%20extensive%20experiments%20on%20the%20nuScenes%0Adataset%20to%20evaluate%20our%20approach.%20Remarkably%2C%20our%20PoIFusion%20achieves%2074.9%5C%25%20NDS%0Aand%2073.4%5C%25%20mAP%2C%20setting%20a%20state-of-the-art%20record%20on%20the%20multi-modal%203D%20object%0Adetection%20benchmark.%20Codes%20will%20be%20made%20available%20via%0A%5Curl%7Bhttps%3A//djiajunustc.github.io/projects/poifusion%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09212v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoIFusion%3A%20Multi-Modal%203D%20Object%20Detection%20via%20Fusion%20at%20Points%20of%0A%20%20Interest&entry.906535625=Jiajun%20Deng%20and%20Sha%20Zhang%20and%20Feras%20Dayoub%20and%20Wanli%20Ouyang%20and%20Yanyong%20Zhang%20and%20Ian%20Reid&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20PoIFusion%2C%20a%20simple%20yet%20effective%20multi-modal%203D%0Aobject%20detection%20framework%20to%20fuse%20the%20information%20of%20RGB%20images%20and%20LiDAR%0Apoint%20clouds%20at%20the%20point%20of%20interest%20%28abbreviated%20as%20PoI%29.%20Technically%2C%20our%0APoIFusion%20follows%20the%20paradigm%20of%20query-based%20object%20detection%2C%20formulating%0Aobject%20queries%20as%20dynamic%203D%20boxes.%20The%20PoIs%20are%20adaptively%20generated%20from%20each%0Aquery%20box%20on%20the%20fly%2C%20serving%20as%20the%20keypoints%20to%20represent%20a%203D%20object%20and%0Aplay%20the%20role%20of%20basic%20units%20in%20multi-modal%20fusion.%20Specifically%2C%20we%20project%0APoIs%20into%20the%20view%20of%20each%20modality%20to%20sample%20the%20corresponding%20feature%20and%0Aintegrate%20the%20multi-modal%20features%20at%20each%20PoI%20through%20a%20dynamic%20fusion%20block.%0AFurthermore%2C%20the%20features%20of%20PoIs%20derived%20from%20the%20same%20query%20box%20are%0Aaggregated%20together%20to%20update%20the%20query%20feature.%20Our%20approach%20prevents%0Ainformation%20loss%20caused%20by%20view%20transformation%20and%20eliminates%20the%0Acomputation-intensive%20global%20attention%2C%20making%20the%20multi-modal%203D%20object%0Adetector%20more%20applicable.%20We%20conducted%20extensive%20experiments%20on%20the%20nuScenes%0Adataset%20to%20evaluate%20our%20approach.%20Remarkably%2C%20our%20PoIFusion%20achieves%2074.9%5C%25%20NDS%0Aand%2073.4%5C%25%20mAP%2C%20setting%20a%20state-of-the-art%20record%20on%20the%20multi-modal%203D%20object%0Adetection%20benchmark.%20Codes%20will%20be%20made%20available%20via%0A%5Curl%7Bhttps%3A//djiajunustc.github.io/projects/poifusion%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09212v1&entry.124074799=Read"},
{"title": "TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions", "author": "Maytus Piriyajitakonkij and Mingfei Sun and Mengmi Zhang and Wei Pan", "abstract": "  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n", "link": "http://arxiv.org/abs/2403.01977v2", "date": "2024-03-14", "relevancy": 2.731, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.561}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5396}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.538}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions&body=Title%3A%20TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions%0AAuthor%3A%20Maytus%20Piriyajitakonkij%20and%20Mingfei%20Sun%20and%20Mengmi%20Zhang%20and%20Wei%20Pan%0AAbstract%3A%20%20%20Robot%20navigation%20under%20visual%20corruption%20presents%20a%20formidable%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20a%20Test-time%20Adaptation%20%28TTA%29%20method%2C%20named%20as%20TTA-Nav%2C%0Afor%20point-goal%20navigation%20under%20visual%20corruptions.%20Our%20%22plug-and-play%22%20method%0Aincorporates%20a%20top-down%20decoder%20to%20a%20pre-trained%20navigation%20model.%20Firstly%2C%20the%0Apre-trained%20navigation%20model%20gets%20a%20corrupted%20image%20and%20extracts%20features.%0ASecondly%2C%20the%20top-down%20decoder%20produces%20the%20reconstruction%20given%20the%20high-level%0Afeatures%20extracted%20by%20the%20pre-trained%20model.%20Then%2C%20it%20feeds%20the%20reconstruction%0Aof%20a%20corrupted%20image%20back%20to%20the%20pre-trained%20model.%20Finally%2C%20the%20pre-trained%0Amodel%20does%20forward%20pass%20again%20to%20output%20action.%20Despite%20being%20trained%20solely%20on%0Aclean%20images%2C%20the%20top-down%20decoder%20can%20reconstruct%20cleaner%20images%20from%0Acorrupted%20ones%20without%20the%20need%20for%20gradient-based%20adaptation.%20The%20pre-trained%0Anavigation%20model%20with%20our%20top-down%20decoder%20significantly%20enhances%20navigation%0Aperformance%20across%20almost%20all%20visual%20corruptions%20in%20our%20benchmarks.%20Our%20method%0Aimproves%20the%20success%20rate%20of%20point-goal%20navigation%20from%20the%20state-of-the-art%0Aresult%20of%2046%25%20to%2094%25%20on%20the%20most%20severe%20corruption.%20This%20suggests%20its%20potential%0Afor%20broader%20application%20in%20robotic%20visual%20navigation.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/tta-nav%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01977v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTA-Nav%3A%20Test-time%20Adaptive%20Reconstruction%20for%20Point-Goal%20Navigation%0A%20%20under%20Visual%20Corruptions&entry.906535625=Maytus%20Piriyajitakonkij%20and%20Mingfei%20Sun%20and%20Mengmi%20Zhang%20and%20Wei%20Pan&entry.1292438233=%20%20Robot%20navigation%20under%20visual%20corruption%20presents%20a%20formidable%20challenge.%20To%0Aaddress%20this%2C%20we%20propose%20a%20Test-time%20Adaptation%20%28TTA%29%20method%2C%20named%20as%20TTA-Nav%2C%0Afor%20point-goal%20navigation%20under%20visual%20corruptions.%20Our%20%22plug-and-play%22%20method%0Aincorporates%20a%20top-down%20decoder%20to%20a%20pre-trained%20navigation%20model.%20Firstly%2C%20the%0Apre-trained%20navigation%20model%20gets%20a%20corrupted%20image%20and%20extracts%20features.%0ASecondly%2C%20the%20top-down%20decoder%20produces%20the%20reconstruction%20given%20the%20high-level%0Afeatures%20extracted%20by%20the%20pre-trained%20model.%20Then%2C%20it%20feeds%20the%20reconstruction%0Aof%20a%20corrupted%20image%20back%20to%20the%20pre-trained%20model.%20Finally%2C%20the%20pre-trained%0Amodel%20does%20forward%20pass%20again%20to%20output%20action.%20Despite%20being%20trained%20solely%20on%0Aclean%20images%2C%20the%20top-down%20decoder%20can%20reconstruct%20cleaner%20images%20from%0Acorrupted%20ones%20without%20the%20need%20for%20gradient-based%20adaptation.%20The%20pre-trained%0Anavigation%20model%20with%20our%20top-down%20decoder%20significantly%20enhances%20navigation%0Aperformance%20across%20almost%20all%20visual%20corruptions%20in%20our%20benchmarks.%20Our%20method%0Aimproves%20the%20success%20rate%20of%20point-goal%20navigation%20from%20the%20state-of-the-art%0Aresult%20of%2046%25%20to%2094%25%20on%20the%20most%20severe%20corruption.%20This%20suggests%20its%20potential%0Afor%20broader%20application%20in%20robotic%20visual%20navigation.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/tta-nav%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01977v2&entry.124074799=Read"},
{"title": "LocalMamba: Visual State Space Model with Windowed Selective Scan", "author": "Tao Huang and Xiaohuan Pei and Shan You and Fei Wang and Chen Qian and Chang Xu", "abstract": "  Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.\n", "link": "http://arxiv.org/abs/2403.09338v1", "date": "2024-03-14", "relevancy": 2.7301, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5595}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan&body=Title%3A%20LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan%0AAuthor%3A%20Tao%20Huang%20and%20Xiaohuan%20Pei%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Chang%20Xu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20state%20space%20models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asignificant%20progress%20in%20modeling%20long%20sequences%20for%20tasks%20like%20language%0Aunderstanding.%20Yet%2C%20their%20application%20in%20vision%20tasks%20has%20not%20markedly%0Asurpassed%20the%20performance%20of%20traditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Vision%20Transformers%20%28ViTs%29.%20This%20paper%20posits%20that%20the%20key%20to%20enhancing%0AVision%20Mamba%20%28ViM%29%20lies%20in%20optimizing%20scan%20directions%20for%20sequence%20modeling.%0ATraditional%20ViM%20approaches%2C%20which%20flatten%20spatial%20tokens%2C%20overlook%20the%0Apreservation%20of%20local%202D%20dependencies%2C%20thereby%20elongating%20the%20distance%20between%0Aadjacent%20tokens.%20We%20introduce%20a%20novel%20local%20scanning%20strategy%20that%20divides%0Aimages%20into%20distinct%20windows%2C%20effectively%20capturing%20local%20dependencies%20while%0Amaintaining%20a%20global%20perspective.%20Additionally%2C%20acknowledging%20the%20varying%0Apreferences%20for%20scan%20patterns%20across%20different%20network%20layers%2C%20we%20propose%20a%0Adynamic%20method%20to%20independently%20search%20for%20the%20optimal%20scan%20choices%20for%20each%0Alayer%2C%20substantially%20improving%20performance.%20Extensive%20experiments%20across%20both%0Aplain%20and%20hierarchical%20models%20underscore%20our%20approach%27s%20superiority%20in%0Aeffectively%20capturing%20image%20representations.%20For%20example%2C%20our%20model%0Asignificantly%20outperforms%20Vim-Ti%20by%203.1%25%20on%20ImageNet%20with%20the%20same%201.5G%20FLOPs.%0ACode%20is%20available%20at%3A%20https%3A//github.com/hunto/LocalMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09338v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalMamba%3A%20Visual%20State%20Space%20Model%20with%20Windowed%20Selective%20Scan&entry.906535625=Tao%20Huang%20and%20Xiaohuan%20Pei%20and%20Shan%20You%20and%20Fei%20Wang%20and%20Chen%20Qian%20and%20Chang%20Xu&entry.1292438233=%20%20Recent%20advancements%20in%20state%20space%20models%2C%20notably%20Mamba%2C%20have%20demonstrated%0Asignificant%20progress%20in%20modeling%20long%20sequences%20for%20tasks%20like%20language%0Aunderstanding.%20Yet%2C%20their%20application%20in%20vision%20tasks%20has%20not%20markedly%0Asurpassed%20the%20performance%20of%20traditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Aand%20Vision%20Transformers%20%28ViTs%29.%20This%20paper%20posits%20that%20the%20key%20to%20enhancing%0AVision%20Mamba%20%28ViM%29%20lies%20in%20optimizing%20scan%20directions%20for%20sequence%20modeling.%0ATraditional%20ViM%20approaches%2C%20which%20flatten%20spatial%20tokens%2C%20overlook%20the%0Apreservation%20of%20local%202D%20dependencies%2C%20thereby%20elongating%20the%20distance%20between%0Aadjacent%20tokens.%20We%20introduce%20a%20novel%20local%20scanning%20strategy%20that%20divides%0Aimages%20into%20distinct%20windows%2C%20effectively%20capturing%20local%20dependencies%20while%0Amaintaining%20a%20global%20perspective.%20Additionally%2C%20acknowledging%20the%20varying%0Apreferences%20for%20scan%20patterns%20across%20different%20network%20layers%2C%20we%20propose%20a%0Adynamic%20method%20to%20independently%20search%20for%20the%20optimal%20scan%20choices%20for%20each%0Alayer%2C%20substantially%20improving%20performance.%20Extensive%20experiments%20across%20both%0Aplain%20and%20hierarchical%20models%20underscore%20our%20approach%27s%20superiority%20in%0Aeffectively%20capturing%20image%20representations.%20For%20example%2C%20our%20model%0Asignificantly%20outperforms%20Vim-Ti%20by%203.1%25%20on%20ImageNet%20with%20the%20same%201.5G%20FLOPs.%0ACode%20is%20available%20at%3A%20https%3A//github.com/hunto/LocalMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09338v1&entry.124074799=Read"},
{"title": "Towards the Uncharted: Density-Descending Feature Perturbation for\n  Semi-supervised Semantic Segmentation", "author": "Xiaoyang Wang and Huihui Bai and Limin Yu and Yao Zhao and Jimin Xiao", "abstract": "  Semi-supervised semantic segmentation allows model to mine effective\nsupervision from unlabeled data to complement label-guided training. Recent\nresearch has primarily focused on consistency regularization techniques,\nexploring perturbation-invariant training at both the image and feature levels.\nIn this work, we proposed a novel feature-level consistency learning framework\nnamed Density-Descending Feature Perturbation (DDFP). Inspired by the\nlow-density separation assumption in semi-supervised learning, our key insight\nis that feature density can shed a light on the most promising direction for\nthe segmentation classifier to explore, which is the regions with lower\ndensity. We propose to shift features with confident predictions towards\nlower-density regions by perturbation injection. The perturbed features are\nthen supervised by the predictions on the original features, thereby compelling\nthe classifier to explore less dense regions to effectively regularize the\ndecision boundary. Central to our method is the estimation of feature density.\nTo this end, we introduce a lightweight density estimator based on normalizing\nflow, allowing for efficient capture of the feature density distribution in an\nonline manner. By extracting gradients from the density estimator, we can\ndetermine the direction towards less dense regions for each feature. The\nproposed DDFP outperforms other designs on feature-level perturbations and\nshows state of the art performances on both Pascal VOC and Cityscapes dataset\nunder various partition protocols. The project is available at\nhttps://github.com/Gavinwxy/DDFP.\n", "link": "http://arxiv.org/abs/2403.06462v2", "date": "2024-03-14", "relevancy": 2.7288, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5568}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20the%20Uncharted%3A%20Density-Descending%20Feature%20Perturbation%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation&body=Title%3A%20Towards%20the%20Uncharted%3A%20Density-Descending%20Feature%20Perturbation%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation%0AAuthor%3A%20Xiaoyang%20Wang%20and%20Huihui%20Bai%20and%20Limin%20Yu%20and%20Yao%20Zhao%20and%20Jimin%20Xiao%0AAbstract%3A%20%20%20Semi-supervised%20semantic%20segmentation%20allows%20model%20to%20mine%20effective%0Asupervision%20from%20unlabeled%20data%20to%20complement%20label-guided%20training.%20Recent%0Aresearch%20has%20primarily%20focused%20on%20consistency%20regularization%20techniques%2C%0Aexploring%20perturbation-invariant%20training%20at%20both%20the%20image%20and%20feature%20levels.%0AIn%20this%20work%2C%20we%20proposed%20a%20novel%20feature-level%20consistency%20learning%20framework%0Anamed%20Density-Descending%20Feature%20Perturbation%20%28DDFP%29.%20Inspired%20by%20the%0Alow-density%20separation%20assumption%20in%20semi-supervised%20learning%2C%20our%20key%20insight%0Ais%20that%20feature%20density%20can%20shed%20a%20light%20on%20the%20most%20promising%20direction%20for%0Athe%20segmentation%20classifier%20to%20explore%2C%20which%20is%20the%20regions%20with%20lower%0Adensity.%20We%20propose%20to%20shift%20features%20with%20confident%20predictions%20towards%0Alower-density%20regions%20by%20perturbation%20injection.%20The%20perturbed%20features%20are%0Athen%20supervised%20by%20the%20predictions%20on%20the%20original%20features%2C%20thereby%20compelling%0Athe%20classifier%20to%20explore%20less%20dense%20regions%20to%20effectively%20regularize%20the%0Adecision%20boundary.%20Central%20to%20our%20method%20is%20the%20estimation%20of%20feature%20density.%0ATo%20this%20end%2C%20we%20introduce%20a%20lightweight%20density%20estimator%20based%20on%20normalizing%0Aflow%2C%20allowing%20for%20efficient%20capture%20of%20the%20feature%20density%20distribution%20in%20an%0Aonline%20manner.%20By%20extracting%20gradients%20from%20the%20density%20estimator%2C%20we%20can%0Adetermine%20the%20direction%20towards%20less%20dense%20regions%20for%20each%20feature.%20The%0Aproposed%20DDFP%20outperforms%20other%20designs%20on%20feature-level%20perturbations%20and%0Ashows%20state%20of%20the%20art%20performances%20on%20both%20Pascal%20VOC%20and%20Cityscapes%20dataset%0Aunder%20various%20partition%20protocols.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/Gavinwxy/DDFP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06462v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20the%20Uncharted%3A%20Density-Descending%20Feature%20Perturbation%20for%0A%20%20Semi-supervised%20Semantic%20Segmentation&entry.906535625=Xiaoyang%20Wang%20and%20Huihui%20Bai%20and%20Limin%20Yu%20and%20Yao%20Zhao%20and%20Jimin%20Xiao&entry.1292438233=%20%20Semi-supervised%20semantic%20segmentation%20allows%20model%20to%20mine%20effective%0Asupervision%20from%20unlabeled%20data%20to%20complement%20label-guided%20training.%20Recent%0Aresearch%20has%20primarily%20focused%20on%20consistency%20regularization%20techniques%2C%0Aexploring%20perturbation-invariant%20training%20at%20both%20the%20image%20and%20feature%20levels.%0AIn%20this%20work%2C%20we%20proposed%20a%20novel%20feature-level%20consistency%20learning%20framework%0Anamed%20Density-Descending%20Feature%20Perturbation%20%28DDFP%29.%20Inspired%20by%20the%0Alow-density%20separation%20assumption%20in%20semi-supervised%20learning%2C%20our%20key%20insight%0Ais%20that%20feature%20density%20can%20shed%20a%20light%20on%20the%20most%20promising%20direction%20for%0Athe%20segmentation%20classifier%20to%20explore%2C%20which%20is%20the%20regions%20with%20lower%0Adensity.%20We%20propose%20to%20shift%20features%20with%20confident%20predictions%20towards%0Alower-density%20regions%20by%20perturbation%20injection.%20The%20perturbed%20features%20are%0Athen%20supervised%20by%20the%20predictions%20on%20the%20original%20features%2C%20thereby%20compelling%0Athe%20classifier%20to%20explore%20less%20dense%20regions%20to%20effectively%20regularize%20the%0Adecision%20boundary.%20Central%20to%20our%20method%20is%20the%20estimation%20of%20feature%20density.%0ATo%20this%20end%2C%20we%20introduce%20a%20lightweight%20density%20estimator%20based%20on%20normalizing%0Aflow%2C%20allowing%20for%20efficient%20capture%20of%20the%20feature%20density%20distribution%20in%20an%0Aonline%20manner.%20By%20extracting%20gradients%20from%20the%20density%20estimator%2C%20we%20can%0Adetermine%20the%20direction%20towards%20less%20dense%20regions%20for%20each%20feature.%20The%0Aproposed%20DDFP%20outperforms%20other%20designs%20on%20feature-level%20perturbations%20and%0Ashows%20state%20of%20the%20art%20performances%20on%20both%20Pascal%20VOC%20and%20Cityscapes%20dataset%0Aunder%20various%20partition%20protocols.%20The%20project%20is%20available%20at%0Ahttps%3A//github.com/Gavinwxy/DDFP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06462v2&entry.124074799=Read"},
{"title": "DyRA: Portable Dynamic Resolution Adjustment Network for Existing\n  Detectors", "author": "Daeun Seo and Hoeseok Yang and Hyungshin Kim", "abstract": "  Achieving constant accuracy in object detection is challenging due to the\ninherent variability of object sizes. One effective approach to this problem\ninvolves optimizing input resolution, referred to as a multi-resolution\nstrategy. Previous approaches to resolution optimization have often been based\non pre-defined resolutions with manual selection. However, there is a lack of\nstudy on run-time resolution optimization for existing architectures. This\npaper introduces DyRA, a dynamic resolution adjustment network providing an\nimage-specific scale factor for existing detectors. This network is co-trained\nwith detectors utilizing specially designed loss functions, namely\nParetoScaleLoss and BalanceLoss. ParetoScaleLoss determines an adaptive scale\nfactor for robustness, while BalanceLoss optimizes overall scale factors\naccording to the localization performance of the detector. The loss function is\ndevised to minimize the accuracy drop across contrasting objectives of\ndifferent-sized objects for scaling. Our proposed network can improve accuracy\nacross various models, including RetinaNet, Faster-RCNN, FCOS, DINO, and\nH-Deformable-DETR. The code is available at\nhttps://github.com/DaEunFullGrace/DyRA.git.\n", "link": "http://arxiv.org/abs/2311.17098v3", "date": "2024-03-14", "relevancy": 2.7099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5231}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors&body=Title%3A%20DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors%0AAuthor%3A%20Daeun%20Seo%20and%20Hoeseok%20Yang%20and%20Hyungshin%20Kim%0AAbstract%3A%20%20%20Achieving%20constant%20accuracy%20in%20object%20detection%20is%20challenging%20due%20to%20the%0Ainherent%20variability%20of%20object%20sizes.%20One%20effective%20approach%20to%20this%20problem%0Ainvolves%20optimizing%20input%20resolution%2C%20referred%20to%20as%20a%20multi-resolution%0Astrategy.%20Previous%20approaches%20to%20resolution%20optimization%20have%20often%20been%20based%0Aon%20pre-defined%20resolutions%20with%20manual%20selection.%20However%2C%20there%20is%20a%20lack%20of%0Astudy%20on%20run-time%20resolution%20optimization%20for%20existing%20architectures.%20This%0Apaper%20introduces%20DyRA%2C%20a%20dynamic%20resolution%20adjustment%20network%20providing%20an%0Aimage-specific%20scale%20factor%20for%20existing%20detectors.%20This%20network%20is%20co-trained%0Awith%20detectors%20utilizing%20specially%20designed%20loss%20functions%2C%20namely%0AParetoScaleLoss%20and%20BalanceLoss.%20ParetoScaleLoss%20determines%20an%20adaptive%20scale%0Afactor%20for%20robustness%2C%20while%20BalanceLoss%20optimizes%20overall%20scale%20factors%0Aaccording%20to%20the%20localization%20performance%20of%20the%20detector.%20The%20loss%20function%20is%0Adevised%20to%20minimize%20the%20accuracy%20drop%20across%20contrasting%20objectives%20of%0Adifferent-sized%20objects%20for%20scaling.%20Our%20proposed%20network%20can%20improve%20accuracy%0Aacross%20various%20models%2C%20including%20RetinaNet%2C%20Faster-RCNN%2C%20FCOS%2C%20DINO%2C%20and%0AH-Deformable-DETR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DaEunFullGrace/DyRA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17098v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyRA%3A%20Portable%20Dynamic%20Resolution%20Adjustment%20Network%20for%20Existing%0A%20%20Detectors&entry.906535625=Daeun%20Seo%20and%20Hoeseok%20Yang%20and%20Hyungshin%20Kim&entry.1292438233=%20%20Achieving%20constant%20accuracy%20in%20object%20detection%20is%20challenging%20due%20to%20the%0Ainherent%20variability%20of%20object%20sizes.%20One%20effective%20approach%20to%20this%20problem%0Ainvolves%20optimizing%20input%20resolution%2C%20referred%20to%20as%20a%20multi-resolution%0Astrategy.%20Previous%20approaches%20to%20resolution%20optimization%20have%20often%20been%20based%0Aon%20pre-defined%20resolutions%20with%20manual%20selection.%20However%2C%20there%20is%20a%20lack%20of%0Astudy%20on%20run-time%20resolution%20optimization%20for%20existing%20architectures.%20This%0Apaper%20introduces%20DyRA%2C%20a%20dynamic%20resolution%20adjustment%20network%20providing%20an%0Aimage-specific%20scale%20factor%20for%20existing%20detectors.%20This%20network%20is%20co-trained%0Awith%20detectors%20utilizing%20specially%20designed%20loss%20functions%2C%20namely%0AParetoScaleLoss%20and%20BalanceLoss.%20ParetoScaleLoss%20determines%20an%20adaptive%20scale%0Afactor%20for%20robustness%2C%20while%20BalanceLoss%20optimizes%20overall%20scale%20factors%0Aaccording%20to%20the%20localization%20performance%20of%20the%20detector.%20The%20loss%20function%20is%0Adevised%20to%20minimize%20the%20accuracy%20drop%20across%20contrasting%20objectives%20of%0Adifferent-sized%20objects%20for%20scaling.%20Our%20proposed%20network%20can%20improve%20accuracy%0Aacross%20various%20models%2C%20including%20RetinaNet%2C%20Faster-RCNN%2C%20FCOS%2C%20DINO%2C%20and%0AH-Deformable-DETR.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/DaEunFullGrace/DyRA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17098v3&entry.124074799=Read"},
{"title": "Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression", "author": "Xinjie Zhang and Shenyuan Gao and Zhening Liu and Xingtong Ge and Dailan He and Tongda Xu and Yan Wang and Jun Zhang", "abstract": "  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n", "link": "http://arxiv.org/abs/2403.08505v1", "date": "2024-03-13", "relevancy": 2.7098, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.536}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5214}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Content-aware%20Masked%20Image%20Modeling%20Transformer%20for%20Stereo%20Image%0A%20%20Compression&body=Title%3A%20Content-aware%20Masked%20Image%20Modeling%20Transformer%20for%20Stereo%20Image%0A%20%20Compression%0AAuthor%3A%20Xinjie%20Zhang%20and%20Shenyuan%20Gao%20and%20Zhening%20Liu%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Existing%20learning-based%20stereo%20image%20codec%20adopt%20sophisticated%20transformation%0Awith%20simple%20entropy%20models%20derived%20from%20single%20image%20codecs%20to%20encode%20latent%0Arepresentations.%20However%2C%20those%20entropy%20models%20struggle%20to%20effectively%20capture%0Athe%20spatial-disparity%20characteristics%20inherent%20in%20stereo%20images%2C%20which%20leads%20to%0Asuboptimal%20rate-distortion%20results.%20In%20this%20paper%2C%20we%20propose%20a%20stereo%20image%0Acompression%20framework%2C%20named%20CAMSIC.%20CAMSIC%20independently%20transforms%20each%20image%0Ato%20latent%20representation%20and%20employs%20a%20powerful%20decoder-free%20Transformer%0Aentropy%20model%20to%20capture%20both%20spatial%20and%20disparity%20dependencies%2C%20by%0Aintroducing%20a%20novel%20content-aware%20masked%20image%20modeling%20%28MIM%29%20technique.%20Our%0Acontent-aware%20MIM%20facilitates%20efficient%20bidirectional%20interaction%20between%20prior%0Ainformation%20and%20estimated%20tokens%2C%20which%20naturally%20obviates%20the%20need%20for%20an%0Aextra%20Transformer%20decoder.%20Experiments%20show%20that%20our%20stereo%20image%20codec%0Aachieves%20state-of-the-art%20rate-distortion%20performance%20on%20two%20stereo%20image%0Adatasets%20Cityscapes%20and%20InStereo2K%20with%20fast%20encoding%20and%20decoding%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08505v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-aware%20Masked%20Image%20Modeling%20Transformer%20for%20Stereo%20Image%0A%20%20Compression&entry.906535625=Xinjie%20Zhang%20and%20Shenyuan%20Gao%20and%20Zhening%20Liu%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Jun%20Zhang&entry.1292438233=%20%20Existing%20learning-based%20stereo%20image%20codec%20adopt%20sophisticated%20transformation%0Awith%20simple%20entropy%20models%20derived%20from%20single%20image%20codecs%20to%20encode%20latent%0Arepresentations.%20However%2C%20those%20entropy%20models%20struggle%20to%20effectively%20capture%0Athe%20spatial-disparity%20characteristics%20inherent%20in%20stereo%20images%2C%20which%20leads%20to%0Asuboptimal%20rate-distortion%20results.%20In%20this%20paper%2C%20we%20propose%20a%20stereo%20image%0Acompression%20framework%2C%20named%20CAMSIC.%20CAMSIC%20independently%20transforms%20each%20image%0Ato%20latent%20representation%20and%20employs%20a%20powerful%20decoder-free%20Transformer%0Aentropy%20model%20to%20capture%20both%20spatial%20and%20disparity%20dependencies%2C%20by%0Aintroducing%20a%20novel%20content-aware%20masked%20image%20modeling%20%28MIM%29%20technique.%20Our%0Acontent-aware%20MIM%20facilitates%20efficient%20bidirectional%20interaction%20between%20prior%0Ainformation%20and%20estimated%20tokens%2C%20which%20naturally%20obviates%20the%20need%20for%20an%0Aextra%20Transformer%20decoder.%20Experiments%20show%20that%20our%20stereo%20image%20codec%0Aachieves%20state-of-the-art%20rate-distortion%20performance%20on%20two%20stereo%20image%0Adatasets%20Cityscapes%20and%20InStereo2K%20with%20fast%20encoding%20and%20decoding%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08505v1&entry.124074799=Read"},
{"title": "VDNA-PR: Using General Dataset Representations for Robust Sequential\n  Visual Place Recognition", "author": "Benjamin Ramtoula and Daniele De Martini and Matthew Gadd and Paul Newman", "abstract": "  This paper adapts a general dataset representation technique to produce\nrobust Visual Place Recognition (VPR) descriptors, crucial to enable real-world\nmobile robot localisation. Two parallel lines of work on VPR have shown, on one\nside, that general-purpose off-the-shelf feature representations can provide\nrobustness to domain shifts, and, on the other, that fused information from\nsequences of images improves performance. In our recent work on measuring\ndomain gaps between image datasets, we proposed a Visual Distribution of Neuron\nActivations (VDNA) representation to represent datasets of images. This\nrepresentation can naturally handle image sequences and provides a general and\ngranular feature representation derived from a general-purpose model. Moreover,\nour representation is based on tracking neuron activation values over the list\nof images to represent and is not limited to a particular neural network layer,\ntherefore having access to high- and low-level concepts. This work shows how\nVDNAs can be used for VPR by learning a very lightweight and simple encoder to\ngenerate task-specific descriptors. Our experiments show that our\nrepresentation can allow for better robustness than current solutions to\nserious domain shifts away from the training data distribution, such as to\nindoor environments and aerial imagery.\n", "link": "http://arxiv.org/abs/2403.09025v1", "date": "2024-03-14", "relevancy": 2.7087, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5407}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5325}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VDNA-PR%3A%20Using%20General%20Dataset%20Representations%20for%20Robust%20Sequential%0A%20%20Visual%20Place%20Recognition&body=Title%3A%20VDNA-PR%3A%20Using%20General%20Dataset%20Representations%20for%20Robust%20Sequential%0A%20%20Visual%20Place%20Recognition%0AAuthor%3A%20Benjamin%20Ramtoula%20and%20Daniele%20De%20Martini%20and%20Matthew%20Gadd%20and%20Paul%20Newman%0AAbstract%3A%20%20%20This%20paper%20adapts%20a%20general%20dataset%20representation%20technique%20to%20produce%0Arobust%20Visual%20Place%20Recognition%20%28VPR%29%20descriptors%2C%20crucial%20to%20enable%20real-world%0Amobile%20robot%20localisation.%20Two%20parallel%20lines%20of%20work%20on%20VPR%20have%20shown%2C%20on%20one%0Aside%2C%20that%20general-purpose%20off-the-shelf%20feature%20representations%20can%20provide%0Arobustness%20to%20domain%20shifts%2C%20and%2C%20on%20the%20other%2C%20that%20fused%20information%20from%0Asequences%20of%20images%20improves%20performance.%20In%20our%20recent%20work%20on%20measuring%0Adomain%20gaps%20between%20image%20datasets%2C%20we%20proposed%20a%20Visual%20Distribution%20of%20Neuron%0AActivations%20%28VDNA%29%20representation%20to%20represent%20datasets%20of%20images.%20This%0Arepresentation%20can%20naturally%20handle%20image%20sequences%20and%20provides%20a%20general%20and%0Agranular%20feature%20representation%20derived%20from%20a%20general-purpose%20model.%20Moreover%2C%0Aour%20representation%20is%20based%20on%20tracking%20neuron%20activation%20values%20over%20the%20list%0Aof%20images%20to%20represent%20and%20is%20not%20limited%20to%20a%20particular%20neural%20network%20layer%2C%0Atherefore%20having%20access%20to%20high-%20and%20low-level%20concepts.%20This%20work%20shows%20how%0AVDNAs%20can%20be%20used%20for%20VPR%20by%20learning%20a%20very%20lightweight%20and%20simple%20encoder%20to%0Agenerate%20task-specific%20descriptors.%20Our%20experiments%20show%20that%20our%0Arepresentation%20can%20allow%20for%20better%20robustness%20than%20current%20solutions%20to%0Aserious%20domain%20shifts%20away%20from%20the%20training%20data%20distribution%2C%20such%20as%20to%0Aindoor%20environments%20and%20aerial%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09025v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VDNA-PR%3A%20Using%20General%20Dataset%20Representations%20for%20Robust%20Sequential%0A%20%20Visual%20Place%20Recognition&entry.906535625=Benjamin%20Ramtoula%20and%20Daniele%20De%20Martini%20and%20Matthew%20Gadd%20and%20Paul%20Newman&entry.1292438233=%20%20This%20paper%20adapts%20a%20general%20dataset%20representation%20technique%20to%20produce%0Arobust%20Visual%20Place%20Recognition%20%28VPR%29%20descriptors%2C%20crucial%20to%20enable%20real-world%0Amobile%20robot%20localisation.%20Two%20parallel%20lines%20of%20work%20on%20VPR%20have%20shown%2C%20on%20one%0Aside%2C%20that%20general-purpose%20off-the-shelf%20feature%20representations%20can%20provide%0Arobustness%20to%20domain%20shifts%2C%20and%2C%20on%20the%20other%2C%20that%20fused%20information%20from%0Asequences%20of%20images%20improves%20performance.%20In%20our%20recent%20work%20on%20measuring%0Adomain%20gaps%20between%20image%20datasets%2C%20we%20proposed%20a%20Visual%20Distribution%20of%20Neuron%0AActivations%20%28VDNA%29%20representation%20to%20represent%20datasets%20of%20images.%20This%0Arepresentation%20can%20naturally%20handle%20image%20sequences%20and%20provides%20a%20general%20and%0Agranular%20feature%20representation%20derived%20from%20a%20general-purpose%20model.%20Moreover%2C%0Aour%20representation%20is%20based%20on%20tracking%20neuron%20activation%20values%20over%20the%20list%0Aof%20images%20to%20represent%20and%20is%20not%20limited%20to%20a%20particular%20neural%20network%20layer%2C%0Atherefore%20having%20access%20to%20high-%20and%20low-level%20concepts.%20This%20work%20shows%20how%0AVDNAs%20can%20be%20used%20for%20VPR%20by%20learning%20a%20very%20lightweight%20and%20simple%20encoder%20to%0Agenerate%20task-specific%20descriptors.%20Our%20experiments%20show%20that%20our%0Arepresentation%20can%20allow%20for%20better%20robustness%20than%20current%20solutions%20to%0Aserious%20domain%20shifts%20away%20from%20the%20training%20data%20distribution%2C%20such%20as%20to%0Aindoor%20environments%20and%20aerial%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09025v1&entry.124074799=Read"},
{"title": "SLiMe: Segment Like Me", "author": "Aliasghar Khani and Saeid Asgari Taghanaki and Aditya Sanghi and Ali Mahdavi Amiri and Ghassan Hamarneh", "abstract": "  Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.\n", "link": "http://arxiv.org/abs/2309.03179v4", "date": "2024-03-14", "relevancy": 2.7047, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5371}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5225}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SLiMe%3A%20Segment%20Like%20Me&body=Title%3A%20SLiMe%3A%20Segment%20Like%20Me%0AAuthor%3A%20Aliasghar%20Khani%20and%20Saeid%20Asgari%20Taghanaki%20and%20Aditya%20Sanghi%20and%20Ali%20Mahdavi%20Amiri%20and%20Ghassan%20Hamarneh%0AAbstract%3A%20%20%20Significant%20strides%20have%20been%20made%20using%20large%20vision-language%20models%2C%20like%0AStable%20Diffusion%20%28SD%29%2C%20for%20a%20variety%20of%20downstream%20tasks%2C%20including%20image%0Aediting%2C%20image%20correspondence%2C%20and%203D%20shape%20generation.%20Inspired%20by%20these%0Aadvancements%2C%20we%20explore%20leveraging%20these%20extensive%20vision-language%20models%20for%0Asegmenting%20images%20at%20any%20desired%20granularity%20using%20as%20few%20as%20one%20annotated%0Asample%20by%20proposing%20SLiMe.%20SLiMe%20frames%20this%20problem%20as%20an%20optimization%20task.%0ASpecifically%2C%20given%20a%20single%20training%20image%20and%20its%20segmentation%20mask%2C%20we%20first%0Aextract%20attention%20maps%2C%20including%20our%20novel%20%22weighted%20accumulated%0Aself-attention%20map%22%20from%20the%20SD%20prior.%20Then%2C%20using%20the%20extracted%20attention%0Amaps%2C%20the%20text%20embeddings%20of%20Stable%20Diffusion%20are%20optimized%20such%20that%2C%20each%20of%0Athem%2C%20learn%20about%20a%20single%20segmented%20region%20from%20the%20training%20image.%20These%0Alearned%20embeddings%20then%20highlight%20the%20segmented%20region%20in%20the%20attention%20maps%2C%0Awhich%20in%20turn%20can%20then%20be%20used%20to%20derive%20the%20segmentation%20map.%20This%20enables%0ASLiMe%20to%20segment%20any%20real-world%20image%20during%20inference%20with%20the%20granularity%20of%0Athe%20segmented%20region%20in%20the%20training%20image%2C%20using%20just%20one%20example.%20Moreover%2C%0Aleveraging%20additional%20training%20data%20when%20available%2C%20i.e.%20few-shot%2C%20improves%20the%0Aperformance%20of%20SLiMe.%20We%20carried%20out%20a%20knowledge-rich%20set%20of%20experiments%0Aexamining%20various%20design%20factors%20and%20showed%20that%20SLiMe%20outperforms%20other%0Aexisting%20one-shot%20and%20few-shot%20segmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03179v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLiMe%3A%20Segment%20Like%20Me&entry.906535625=Aliasghar%20Khani%20and%20Saeid%20Asgari%20Taghanaki%20and%20Aditya%20Sanghi%20and%20Ali%20Mahdavi%20Amiri%20and%20Ghassan%20Hamarneh&entry.1292438233=%20%20Significant%20strides%20have%20been%20made%20using%20large%20vision-language%20models%2C%20like%0AStable%20Diffusion%20%28SD%29%2C%20for%20a%20variety%20of%20downstream%20tasks%2C%20including%20image%0Aediting%2C%20image%20correspondence%2C%20and%203D%20shape%20generation.%20Inspired%20by%20these%0Aadvancements%2C%20we%20explore%20leveraging%20these%20extensive%20vision-language%20models%20for%0Asegmenting%20images%20at%20any%20desired%20granularity%20using%20as%20few%20as%20one%20annotated%0Asample%20by%20proposing%20SLiMe.%20SLiMe%20frames%20this%20problem%20as%20an%20optimization%20task.%0ASpecifically%2C%20given%20a%20single%20training%20image%20and%20its%20segmentation%20mask%2C%20we%20first%0Aextract%20attention%20maps%2C%20including%20our%20novel%20%22weighted%20accumulated%0Aself-attention%20map%22%20from%20the%20SD%20prior.%20Then%2C%20using%20the%20extracted%20attention%0Amaps%2C%20the%20text%20embeddings%20of%20Stable%20Diffusion%20are%20optimized%20such%20that%2C%20each%20of%0Athem%2C%20learn%20about%20a%20single%20segmented%20region%20from%20the%20training%20image.%20These%0Alearned%20embeddings%20then%20highlight%20the%20segmented%20region%20in%20the%20attention%20maps%2C%0Awhich%20in%20turn%20can%20then%20be%20used%20to%20derive%20the%20segmentation%20map.%20This%20enables%0ASLiMe%20to%20segment%20any%20real-world%20image%20during%20inference%20with%20the%20granularity%20of%0Athe%20segmented%20region%20in%20the%20training%20image%2C%20using%20just%20one%20example.%20Moreover%2C%0Aleveraging%20additional%20training%20data%20when%20available%2C%20i.e.%20few-shot%2C%20improves%20the%0Aperformance%20of%20SLiMe.%20We%20carried%20out%20a%20knowledge-rich%20set%20of%20experiments%0Aexamining%20various%20design%20factors%20and%20showed%20that%20SLiMe%20outperforms%20other%0Aexisting%20one-shot%20and%20few-shot%20segmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03179v4&entry.124074799=Read"},
{"title": "Continual Segmentation with Disentangled Objectness Learning and Class\n  Recognition", "author": "Yizheng Gong and Siyue Yu and Xiaoyang Wang and Jimin Xiao", "abstract": "  Most continual segmentation methods tackle the problem as a per-pixel\nclassification task. However, such a paradigm is very challenging, and we find\nquery-based segmenters with built-in objectness have inherent advantages\ncompared with per-pixel ones, as objectness has strong transfer ability and\nforgetting resistance. Based on these findings, we propose CoMasTRe by\ndisentangling continual segmentation into two stages: forgetting-resistant\ncontinual objectness learning and well-researched continual classification.\nCoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at\nthe first stage and leaving recognition to the second stage. During continual\nlearning, a simple but effective distillation is adopted to strengthen\nobjectness. To further mitigate the forgetting of old classes, we design a\nmulti-label class distillation strategy suited for segmentation. We assess the\neffectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show\nthat our method outperforms per-pixel and query-based methods on both datasets.\nCode will be available at https://github.com/jordangong/CoMasTRe.\n", "link": "http://arxiv.org/abs/2403.03477v2", "date": "2024-03-14", "relevancy": 2.6934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5365}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.519}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Segmentation%20with%20Disentangled%20Objectness%20Learning%20and%20Class%0A%20%20Recognition&body=Title%3A%20Continual%20Segmentation%20with%20Disentangled%20Objectness%20Learning%20and%20Class%0A%20%20Recognition%0AAuthor%3A%20Yizheng%20Gong%20and%20Siyue%20Yu%20and%20Xiaoyang%20Wang%20and%20Jimin%20Xiao%0AAbstract%3A%20%20%20Most%20continual%20segmentation%20methods%20tackle%20the%20problem%20as%20a%20per-pixel%0Aclassification%20task.%20However%2C%20such%20a%20paradigm%20is%20very%20challenging%2C%20and%20we%20find%0Aquery-based%20segmenters%20with%20built-in%20objectness%20have%20inherent%20advantages%0Acompared%20with%20per-pixel%20ones%2C%20as%20objectness%20has%20strong%20transfer%20ability%20and%0Aforgetting%20resistance.%20Based%20on%20these%20findings%2C%20we%20propose%20CoMasTRe%20by%0Adisentangling%20continual%20segmentation%20into%20two%20stages%3A%20forgetting-resistant%0Acontinual%20objectness%20learning%20and%20well-researched%20continual%20classification.%0ACoMasTRe%20uses%20a%20two-stage%20segmenter%20learning%20class-agnostic%20mask%20proposals%20at%0Athe%20first%20stage%20and%20leaving%20recognition%20to%20the%20second%20stage.%20During%20continual%0Alearning%2C%20a%20simple%20but%20effective%20distillation%20is%20adopted%20to%20strengthen%0Aobjectness.%20To%20further%20mitigate%20the%20forgetting%20of%20old%20classes%2C%20we%20design%20a%0Amulti-label%20class%20distillation%20strategy%20suited%20for%20segmentation.%20We%20assess%20the%0Aeffectiveness%20of%20CoMasTRe%20on%20PASCAL%20VOC%20and%20ADE20K.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20per-pixel%20and%20query-based%20methods%20on%20both%20datasets.%0ACode%20will%20be%20available%20at%20https%3A//github.com/jordangong/CoMasTRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03477v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Segmentation%20with%20Disentangled%20Objectness%20Learning%20and%20Class%0A%20%20Recognition&entry.906535625=Yizheng%20Gong%20and%20Siyue%20Yu%20and%20Xiaoyang%20Wang%20and%20Jimin%20Xiao&entry.1292438233=%20%20Most%20continual%20segmentation%20methods%20tackle%20the%20problem%20as%20a%20per-pixel%0Aclassification%20task.%20However%2C%20such%20a%20paradigm%20is%20very%20challenging%2C%20and%20we%20find%0Aquery-based%20segmenters%20with%20built-in%20objectness%20have%20inherent%20advantages%0Acompared%20with%20per-pixel%20ones%2C%20as%20objectness%20has%20strong%20transfer%20ability%20and%0Aforgetting%20resistance.%20Based%20on%20these%20findings%2C%20we%20propose%20CoMasTRe%20by%0Adisentangling%20continual%20segmentation%20into%20two%20stages%3A%20forgetting-resistant%0Acontinual%20objectness%20learning%20and%20well-researched%20continual%20classification.%0ACoMasTRe%20uses%20a%20two-stage%20segmenter%20learning%20class-agnostic%20mask%20proposals%20at%0Athe%20first%20stage%20and%20leaving%20recognition%20to%20the%20second%20stage.%20During%20continual%0Alearning%2C%20a%20simple%20but%20effective%20distillation%20is%20adopted%20to%20strengthen%0Aobjectness.%20To%20further%20mitigate%20the%20forgetting%20of%20old%20classes%2C%20we%20design%20a%0Amulti-label%20class%20distillation%20strategy%20suited%20for%20segmentation.%20We%20assess%20the%0Aeffectiveness%20of%20CoMasTRe%20on%20PASCAL%20VOC%20and%20ADE20K.%20Extensive%20experiments%20show%0Athat%20our%20method%20outperforms%20per-pixel%20and%20query-based%20methods%20on%20both%20datasets.%0ACode%20will%20be%20available%20at%20https%3A//github.com/jordangong/CoMasTRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03477v2&entry.124074799=Read"},
{"title": "Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images", "author": "Giuseppe Cartella and Vittorio Cuculo and Marcella Cornia and Rita Cucchiara", "abstract": "  Creating high-quality and realistic images is now possible thanks to the\nimpressive advancements in image generation. A description in natural language\nof your desired output is all you need to obtain breathtaking results. However,\nas the use of generative models grows, so do concerns about the propagation of\nmalicious content and misinformation. Consequently, the research community is\nactively working on the development of novel fake detection techniques,\nprimarily focusing on low-level features and possible fingerprints left by\ngenerative models during the image generation process. In a different vein, in\nour work, we leverage human semantic knowledge to investigate the possibility\nof being included in frameworks of fake image detection. To achieve this, we\ncollect a novel dataset of partially manipulated images using diffusion models\nand conduct an eye-tracking experiment to record the eye movements of different\nobservers while viewing real and fake stimuli. A preliminary statistical\nanalysis is conducted to explore the distinctive patterns in how humans\nperceive genuine and altered images. Statistical findings reveal that, when\nperceiving counterfeit samples, humans tend to focus on more confined regions\nof the image, in contrast to the more dispersed observational pattern observed\nwhen viewing genuine images. Our dataset is publicly available at:\nhttps://github.com/aimagelab/unveiling-the-truth.\n", "link": "http://arxiv.org/abs/2403.08933v1", "date": "2024-03-13", "relevancy": 2.6928, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5456}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5421}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.528}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Truth%3A%20Exploring%20Human%20Gaze%20Patterns%20in%20Fake%20Images&body=Title%3A%20Unveiling%20the%20Truth%3A%20Exploring%20Human%20Gaze%20Patterns%20in%20Fake%20Images%0AAuthor%3A%20Giuseppe%20Cartella%20and%20Vittorio%20Cuculo%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Creating%20high-quality%20and%20realistic%20images%20is%20now%20possible%20thanks%20to%20the%0Aimpressive%20advancements%20in%20image%20generation.%20A%20description%20in%20natural%20language%0Aof%20your%20desired%20output%20is%20all%20you%20need%20to%20obtain%20breathtaking%20results.%20However%2C%0Aas%20the%20use%20of%20generative%20models%20grows%2C%20so%20do%20concerns%20about%20the%20propagation%20of%0Amalicious%20content%20and%20misinformation.%20Consequently%2C%20the%20research%20community%20is%0Aactively%20working%20on%20the%20development%20of%20novel%20fake%20detection%20techniques%2C%0Aprimarily%20focusing%20on%20low-level%20features%20and%20possible%20fingerprints%20left%20by%0Agenerative%20models%20during%20the%20image%20generation%20process.%20In%20a%20different%20vein%2C%20in%0Aour%20work%2C%20we%20leverage%20human%20semantic%20knowledge%20to%20investigate%20the%20possibility%0Aof%20being%20included%20in%20frameworks%20of%20fake%20image%20detection.%20To%20achieve%20this%2C%20we%0Acollect%20a%20novel%20dataset%20of%20partially%20manipulated%20images%20using%20diffusion%20models%0Aand%20conduct%20an%20eye-tracking%20experiment%20to%20record%20the%20eye%20movements%20of%20different%0Aobservers%20while%20viewing%20real%20and%20fake%20stimuli.%20A%20preliminary%20statistical%0Aanalysis%20is%20conducted%20to%20explore%20the%20distinctive%20patterns%20in%20how%20humans%0Aperceive%20genuine%20and%20altered%20images.%20Statistical%20findings%20reveal%20that%2C%20when%0Aperceiving%20counterfeit%20samples%2C%20humans%20tend%20to%20focus%20on%20more%20confined%20regions%0Aof%20the%20image%2C%20in%20contrast%20to%20the%20more%20dispersed%20observational%20pattern%20observed%0Awhen%20viewing%20genuine%20images.%20Our%20dataset%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/aimagelab/unveiling-the-truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Truth%3A%20Exploring%20Human%20Gaze%20Patterns%20in%20Fake%20Images&entry.906535625=Giuseppe%20Cartella%20and%20Vittorio%20Cuculo%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Creating%20high-quality%20and%20realistic%20images%20is%20now%20possible%20thanks%20to%20the%0Aimpressive%20advancements%20in%20image%20generation.%20A%20description%20in%20natural%20language%0Aof%20your%20desired%20output%20is%20all%20you%20need%20to%20obtain%20breathtaking%20results.%20However%2C%0Aas%20the%20use%20of%20generative%20models%20grows%2C%20so%20do%20concerns%20about%20the%20propagation%20of%0Amalicious%20content%20and%20misinformation.%20Consequently%2C%20the%20research%20community%20is%0Aactively%20working%20on%20the%20development%20of%20novel%20fake%20detection%20techniques%2C%0Aprimarily%20focusing%20on%20low-level%20features%20and%20possible%20fingerprints%20left%20by%0Agenerative%20models%20during%20the%20image%20generation%20process.%20In%20a%20different%20vein%2C%20in%0Aour%20work%2C%20we%20leverage%20human%20semantic%20knowledge%20to%20investigate%20the%20possibility%0Aof%20being%20included%20in%20frameworks%20of%20fake%20image%20detection.%20To%20achieve%20this%2C%20we%0Acollect%20a%20novel%20dataset%20of%20partially%20manipulated%20images%20using%20diffusion%20models%0Aand%20conduct%20an%20eye-tracking%20experiment%20to%20record%20the%20eye%20movements%20of%20different%0Aobservers%20while%20viewing%20real%20and%20fake%20stimuli.%20A%20preliminary%20statistical%0Aanalysis%20is%20conducted%20to%20explore%20the%20distinctive%20patterns%20in%20how%20humans%0Aperceive%20genuine%20and%20altered%20images.%20Statistical%20findings%20reveal%20that%2C%20when%0Aperceiving%20counterfeit%20samples%2C%20humans%20tend%20to%20focus%20on%20more%20confined%20regions%0Aof%20the%20image%2C%20in%20contrast%20to%20the%20more%20dispersed%20observational%20pattern%20observed%0Awhen%20viewing%20genuine%20images.%20Our%20dataset%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/aimagelab/unveiling-the-truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08933v1&entry.124074799=Read"},
{"title": "Plug-and-Play Regularization on Magnitude with Deep Priors for 3D\n  Near-Field MIMO Imaging", "author": "Okyanus Oral and Figen S. Oktem", "abstract": "  Near-field radar imaging systems are used in a wide range of applications\nsuch as concealed weapon detection and medical diagnosis. In this paper, we\nconsider the problem of reconstructing the three-dimensional (3D)\ncomplex-valued reflectivity distribution of the near-field scene by enforcing\nregularization on its magnitude. We solve this inverse problem by using the\nalternating direction method of multipliers (ADMM) framework. For this, we\nprovide a general expression for the proximal mapping associated with such\nregularization functionals. This equivalently corresponds to the solution of a\ncomplex-valued denoising problem which involves regularization on the\nmagnitude. By utilizing this expression, we develop a novel and efficient\nplug-and-play (PnP) reconstruction method that consists of simple update steps.\nDue to the success of data-adaptive deep priors in imaging, we also train a 3D\ndeep denoiser to exploit within the developed PnP framework. The effectiveness\nof the developed approach is demonstrated for multiple-input multiple-output\n(MIMO) imaging under various compressive and noisy observation scenarios using\nboth simulated and experimental data. The performance is also compared with the\ncommonly used direct inversion and sparsity-based reconstruction approaches.\nThe results demonstrate that the developed technique not only provides\nstate-of-the-art performance for 3D real-world targets, but also enables fast\ncomputation. Our approach provides a unified general framework to effectively\nhandle arbitrary regularization on the magnitude of a complex-valued unknown\nand is equally applicable to other radar image formation problems (including\nSAR).\n", "link": "http://arxiv.org/abs/2312.16024v2", "date": "2024-03-13", "relevancy": 2.6806, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Plug-and-Play%20Regularization%20on%20Magnitude%20with%20Deep%20Priors%20for%203D%0A%20%20Near-Field%20MIMO%20Imaging&body=Title%3A%20Plug-and-Play%20Regularization%20on%20Magnitude%20with%20Deep%20Priors%20for%203D%0A%20%20Near-Field%20MIMO%20Imaging%0AAuthor%3A%20Okyanus%20Oral%20and%20Figen%20S.%20Oktem%0AAbstract%3A%20%20%20Near-field%20radar%20imaging%20systems%20are%20used%20in%20a%20wide%20range%20of%20applications%0Asuch%20as%20concealed%20weapon%20detection%20and%20medical%20diagnosis.%20In%20this%20paper%2C%20we%0Aconsider%20the%20problem%20of%20reconstructing%20the%20three-dimensional%20%283D%29%0Acomplex-valued%20reflectivity%20distribution%20of%20the%20near-field%20scene%20by%20enforcing%0Aregularization%20on%20its%20magnitude.%20We%20solve%20this%20inverse%20problem%20by%20using%20the%0Aalternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20framework.%20For%20this%2C%20we%0Aprovide%20a%20general%20expression%20for%20the%20proximal%20mapping%20associated%20with%20such%0Aregularization%20functionals.%20This%20equivalently%20corresponds%20to%20the%20solution%20of%20a%0Acomplex-valued%20denoising%20problem%20which%20involves%20regularization%20on%20the%0Amagnitude.%20By%20utilizing%20this%20expression%2C%20we%20develop%20a%20novel%20and%20efficient%0Aplug-and-play%20%28PnP%29%20reconstruction%20method%20that%20consists%20of%20simple%20update%20steps.%0ADue%20to%20the%20success%20of%20data-adaptive%20deep%20priors%20in%20imaging%2C%20we%20also%20train%20a%203D%0Adeep%20denoiser%20to%20exploit%20within%20the%20developed%20PnP%20framework.%20The%20effectiveness%0Aof%20the%20developed%20approach%20is%20demonstrated%20for%20multiple-input%20multiple-output%0A%28MIMO%29%20imaging%20under%20various%20compressive%20and%20noisy%20observation%20scenarios%20using%0Aboth%20simulated%20and%20experimental%20data.%20The%20performance%20is%20also%20compared%20with%20the%0Acommonly%20used%20direct%20inversion%20and%20sparsity-based%20reconstruction%20approaches.%0AThe%20results%20demonstrate%20that%20the%20developed%20technique%20not%20only%20provides%0Astate-of-the-art%20performance%20for%203D%20real-world%20targets%2C%20but%20also%20enables%20fast%0Acomputation.%20Our%20approach%20provides%20a%20unified%20general%20framework%20to%20effectively%0Ahandle%20arbitrary%20regularization%20on%20the%20magnitude%20of%20a%20complex-valued%20unknown%0Aand%20is%20equally%20applicable%20to%20other%20radar%20image%20formation%20problems%20%28including%0ASAR%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16024v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plug-and-Play%20Regularization%20on%20Magnitude%20with%20Deep%20Priors%20for%203D%0A%20%20Near-Field%20MIMO%20Imaging&entry.906535625=Okyanus%20Oral%20and%20Figen%20S.%20Oktem&entry.1292438233=%20%20Near-field%20radar%20imaging%20systems%20are%20used%20in%20a%20wide%20range%20of%20applications%0Asuch%20as%20concealed%20weapon%20detection%20and%20medical%20diagnosis.%20In%20this%20paper%2C%20we%0Aconsider%20the%20problem%20of%20reconstructing%20the%20three-dimensional%20%283D%29%0Acomplex-valued%20reflectivity%20distribution%20of%20the%20near-field%20scene%20by%20enforcing%0Aregularization%20on%20its%20magnitude.%20We%20solve%20this%20inverse%20problem%20by%20using%20the%0Aalternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20framework.%20For%20this%2C%20we%0Aprovide%20a%20general%20expression%20for%20the%20proximal%20mapping%20associated%20with%20such%0Aregularization%20functionals.%20This%20equivalently%20corresponds%20to%20the%20solution%20of%20a%0Acomplex-valued%20denoising%20problem%20which%20involves%20regularization%20on%20the%0Amagnitude.%20By%20utilizing%20this%20expression%2C%20we%20develop%20a%20novel%20and%20efficient%0Aplug-and-play%20%28PnP%29%20reconstruction%20method%20that%20consists%20of%20simple%20update%20steps.%0ADue%20to%20the%20success%20of%20data-adaptive%20deep%20priors%20in%20imaging%2C%20we%20also%20train%20a%203D%0Adeep%20denoiser%20to%20exploit%20within%20the%20developed%20PnP%20framework.%20The%20effectiveness%0Aof%20the%20developed%20approach%20is%20demonstrated%20for%20multiple-input%20multiple-output%0A%28MIMO%29%20imaging%20under%20various%20compressive%20and%20noisy%20observation%20scenarios%20using%0Aboth%20simulated%20and%20experimental%20data.%20The%20performance%20is%20also%20compared%20with%20the%0Acommonly%20used%20direct%20inversion%20and%20sparsity-based%20reconstruction%20approaches.%0AThe%20results%20demonstrate%20that%20the%20developed%20technique%20not%20only%20provides%0Astate-of-the-art%20performance%20for%203D%20real-world%20targets%2C%20but%20also%20enables%20fast%0Acomputation.%20Our%20approach%20provides%20a%20unified%20general%20framework%20to%20effectively%0Ahandle%20arbitrary%20regularization%20on%20the%20magnitude%20of%20a%20complex-valued%20unknown%0Aand%20is%20equally%20applicable%20to%20other%20radar%20image%20formation%20problems%20%28including%0ASAR%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16024v2&entry.124074799=Read"},
{"title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly\n  Detection", "author": "Qihang Zhou and Guansong Pang and Yu Tian and Shibo He and Jiming Chen", "abstract": "  Zero-shot anomaly detection (ZSAD) requires detection models trained using\nauxiliary data to detect anomalies without any training sample in a target\ndataset. It is a crucial task when training data is not accessible due to\nvarious concerns, eg, data privacy, yet it is challenging since the models need\nto generalize to anomalies across different domains where the appearance of\nforeground objects, abnormal regions, and background features, such as\ndefects/tumors on different products/organs, can vary significantly. Recently\nlarge pre-trained vision-language models (VLMs), such as CLIP, have\ndemonstrated strong zero-shot recognition ability in various vision tasks,\nincluding anomaly detection. However, their ZSAD performance is weak since the\nVLMs focus more on modeling the class semantics of the foreground objects\nrather than the abnormality/normality in the images. In this paper we introduce\na novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across\ndifferent domains. The key insight of AnomalyCLIP is to learn object-agnostic\ntext prompts that capture generic normality and abnormality in an image\nregardless of its foreground objects. This allows our model to focus on the\nabnormal image regions rather than the object semantics, enabling generalized\nnormality and abnormality recognition on diverse types of objects. Large-scale\nexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIP\nachieves superior zero-shot performance of detecting and segmenting anomalies\nin datasets of highly diverse class semantics from various defect inspection\nand medical imaging domains. Code will be made available at\nhttps://github.com/zqhang/AnomalyCLIP.\n", "link": "http://arxiv.org/abs/2310.18961v7", "date": "2024-03-14", "relevancy": 2.678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5801}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection&body=Title%3A%20AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Qihang%20Zhou%20and%20Guansong%20Pang%20and%20Yu%20Tian%20and%20Shibo%20He%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20requires%20detection%20models%20trained%20using%0Aauxiliary%20data%20to%20detect%20anomalies%20without%20any%20training%20sample%20in%20a%20target%0Adataset.%20It%20is%20a%20crucial%20task%20when%20training%20data%20is%20not%20accessible%20due%20to%0Avarious%20concerns%2C%20eg%2C%20data%20privacy%2C%20yet%20it%20is%20challenging%20since%20the%20models%20need%0Ato%20generalize%20to%20anomalies%20across%20different%20domains%20where%20the%20appearance%20of%0Aforeground%20objects%2C%20abnormal%20regions%2C%20and%20background%20features%2C%20such%20as%0Adefects/tumors%20on%20different%20products/organs%2C%20can%20vary%20significantly.%20Recently%0Alarge%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Ademonstrated%20strong%20zero-shot%20recognition%20ability%20in%20various%20vision%20tasks%2C%0Aincluding%20anomaly%20detection.%20However%2C%20their%20ZSAD%20performance%20is%20weak%20since%20the%0AVLMs%20focus%20more%20on%20modeling%20the%20class%20semantics%20of%20the%20foreground%20objects%0Arather%20than%20the%20abnormality/normality%20in%20the%20images.%20In%20this%20paper%20we%20introduce%0Aa%20novel%20approach%2C%20namely%20AnomalyCLIP%2C%20to%20adapt%20CLIP%20for%20accurate%20ZSAD%20across%0Adifferent%20domains.%20The%20key%20insight%20of%20AnomalyCLIP%20is%20to%20learn%20object-agnostic%0Atext%20prompts%20that%20capture%20generic%20normality%20and%20abnormality%20in%20an%20image%0Aregardless%20of%20its%20foreground%20objects.%20This%20allows%20our%20model%20to%20focus%20on%20the%0Aabnormal%20image%20regions%20rather%20than%20the%20object%20semantics%2C%20enabling%20generalized%0Anormality%20and%20abnormality%20recognition%20on%20diverse%20types%20of%20objects.%20Large-scale%0Aexperiments%20on%2017%20real-world%20anomaly%20detection%20datasets%20show%20that%20AnomalyCLIP%0Aachieves%20superior%20zero-shot%20performance%20of%20detecting%20and%20segmenting%20anomalies%0Ain%20datasets%20of%20highly%20diverse%20class%20semantics%20from%20various%20defect%20inspection%0Aand%20medical%20imaging%20domains.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/zqhang/AnomalyCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18961v7", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyCLIP%3A%20Object-agnostic%20Prompt%20Learning%20for%20Zero-shot%20Anomaly%0A%20%20Detection&entry.906535625=Qihang%20Zhou%20and%20Guansong%20Pang%20and%20Yu%20Tian%20and%20Shibo%20He%20and%20Jiming%20Chen&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20requires%20detection%20models%20trained%20using%0Aauxiliary%20data%20to%20detect%20anomalies%20without%20any%20training%20sample%20in%20a%20target%0Adataset.%20It%20is%20a%20crucial%20task%20when%20training%20data%20is%20not%20accessible%20due%20to%0Avarious%20concerns%2C%20eg%2C%20data%20privacy%2C%20yet%20it%20is%20challenging%20since%20the%20models%20need%0Ato%20generalize%20to%20anomalies%20across%20different%20domains%20where%20the%20appearance%20of%0Aforeground%20objects%2C%20abnormal%20regions%2C%20and%20background%20features%2C%20such%20as%0Adefects/tumors%20on%20different%20products/organs%2C%20can%20vary%20significantly.%20Recently%0Alarge%20pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%0Ademonstrated%20strong%20zero-shot%20recognition%20ability%20in%20various%20vision%20tasks%2C%0Aincluding%20anomaly%20detection.%20However%2C%20their%20ZSAD%20performance%20is%20weak%20since%20the%0AVLMs%20focus%20more%20on%20modeling%20the%20class%20semantics%20of%20the%20foreground%20objects%0Arather%20than%20the%20abnormality/normality%20in%20the%20images.%20In%20this%20paper%20we%20introduce%0Aa%20novel%20approach%2C%20namely%20AnomalyCLIP%2C%20to%20adapt%20CLIP%20for%20accurate%20ZSAD%20across%0Adifferent%20domains.%20The%20key%20insight%20of%20AnomalyCLIP%20is%20to%20learn%20object-agnostic%0Atext%20prompts%20that%20capture%20generic%20normality%20and%20abnormality%20in%20an%20image%0Aregardless%20of%20its%20foreground%20objects.%20This%20allows%20our%20model%20to%20focus%20on%20the%0Aabnormal%20image%20regions%20rather%20than%20the%20object%20semantics%2C%20enabling%20generalized%0Anormality%20and%20abnormality%20recognition%20on%20diverse%20types%20of%20objects.%20Large-scale%0Aexperiments%20on%2017%20real-world%20anomaly%20detection%20datasets%20show%20that%20AnomalyCLIP%0Aachieves%20superior%20zero-shot%20performance%20of%20detecting%20and%20segmenting%20anomalies%0Ain%20datasets%20of%20highly%20diverse%20class%20semantics%20from%20various%20defect%20inspection%0Aand%20medical%20imaging%20domains.%20Code%20will%20be%20made%20available%20at%0Ahttps%3A//github.com/zqhang/AnomalyCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18961v7&entry.124074799=Read"},
{"title": "DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local\n  Climate Zone Classification", "author": "Qianqian Wu and Xianping Ma and Jialu Sui and Man-On Pun", "abstract": "  Recent advancements in remote sensing (RS) technologies have shown their\npotential in accurately classifying local climate zones (LCZs). However,\ntraditional scene-level methods using convolutional neural networks (CNNs)\noften struggle to integrate prior knowledge of ground objects effectively.\nMoreover, commonly utilized data sources like Sentinel-2 encounter difficulties\nin capturing detailed ground object information. To tackle these challenges, we\npropose a data fusion method that integrates ground object priors extracted\nfrom high-resolution Google imagery with Sentinel-2 multispectral imagery. The\nproposed method introduces a novel Dual-stream Fusion framework for LCZ\nclassification (DF4LCZ), integrating instance-based location features from\nGoogle imagery with the scene-level spatial-spectral features extracted from\nSentinel-2 imagery. The framework incorporates a Graph Convolutional Network\n(GCN) module empowered by the Segment Anything Model (SAM) to enhance feature\nextraction from Google imagery. Simultaneously, the framework employs a 3D-CNN\narchitecture to learn the spectral-spatial features of Sentinel-2 imagery.\nExperiments are conducted on a multi-source remote sensing image dataset\nspecifically designed for LCZ classification, validating the effectiveness of\nthe proposed DF4LCZ. The related code and dataset are available at\nhttps://github.com/ctrlovefly/DF4LCZ.\n", "link": "http://arxiv.org/abs/2403.09367v1", "date": "2024-03-14", "relevancy": 2.67, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification&body=Title%3A%20DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification%0AAuthor%3A%20Qianqian%20Wu%20and%20Xianping%20Ma%20and%20Jialu%20Sui%20and%20Man-On%20Pun%0AAbstract%3A%20%20%20Recent%20advancements%20in%20remote%20sensing%20%28RS%29%20technologies%20have%20shown%20their%0Apotential%20in%20accurately%20classifying%20local%20climate%20zones%20%28LCZs%29.%20However%2C%0Atraditional%20scene-level%20methods%20using%20convolutional%20neural%20networks%20%28CNNs%29%0Aoften%20struggle%20to%20integrate%20prior%20knowledge%20of%20ground%20objects%20effectively.%0AMoreover%2C%20commonly%20utilized%20data%20sources%20like%20Sentinel-2%20encounter%20difficulties%0Ain%20capturing%20detailed%20ground%20object%20information.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20data%20fusion%20method%20that%20integrates%20ground%20object%20priors%20extracted%0Afrom%20high-resolution%20Google%20imagery%20with%20Sentinel-2%20multispectral%20imagery.%20The%0Aproposed%20method%20introduces%20a%20novel%20Dual-stream%20Fusion%20framework%20for%20LCZ%0Aclassification%20%28DF4LCZ%29%2C%20integrating%20instance-based%20location%20features%20from%0AGoogle%20imagery%20with%20the%20scene-level%20spatial-spectral%20features%20extracted%20from%0ASentinel-2%20imagery.%20The%20framework%20incorporates%20a%20Graph%20Convolutional%20Network%0A%28GCN%29%20module%20empowered%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20feature%0Aextraction%20from%20Google%20imagery.%20Simultaneously%2C%20the%20framework%20employs%20a%203D-CNN%0Aarchitecture%20to%20learn%20the%20spectral-spatial%20features%20of%20Sentinel-2%20imagery.%0AExperiments%20are%20conducted%20on%20a%20multi-source%20remote%20sensing%20image%20dataset%0Aspecifically%20designed%20for%20LCZ%20classification%2C%20validating%20the%20effectiveness%20of%0Athe%20proposed%20DF4LCZ.%20The%20related%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ctrlovefly/DF4LCZ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09367v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DF4LCZ%3A%20A%20SAM-Empowered%20Data%20Fusion%20Framework%20for%20Scene-Level%20Local%0A%20%20Climate%20Zone%20Classification&entry.906535625=Qianqian%20Wu%20and%20Xianping%20Ma%20and%20Jialu%20Sui%20and%20Man-On%20Pun&entry.1292438233=%20%20Recent%20advancements%20in%20remote%20sensing%20%28RS%29%20technologies%20have%20shown%20their%0Apotential%20in%20accurately%20classifying%20local%20climate%20zones%20%28LCZs%29.%20However%2C%0Atraditional%20scene-level%20methods%20using%20convolutional%20neural%20networks%20%28CNNs%29%0Aoften%20struggle%20to%20integrate%20prior%20knowledge%20of%20ground%20objects%20effectively.%0AMoreover%2C%20commonly%20utilized%20data%20sources%20like%20Sentinel-2%20encounter%20difficulties%0Ain%20capturing%20detailed%20ground%20object%20information.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20data%20fusion%20method%20that%20integrates%20ground%20object%20priors%20extracted%0Afrom%20high-resolution%20Google%20imagery%20with%20Sentinel-2%20multispectral%20imagery.%20The%0Aproposed%20method%20introduces%20a%20novel%20Dual-stream%20Fusion%20framework%20for%20LCZ%0Aclassification%20%28DF4LCZ%29%2C%20integrating%20instance-based%20location%20features%20from%0AGoogle%20imagery%20with%20the%20scene-level%20spatial-spectral%20features%20extracted%20from%0ASentinel-2%20imagery.%20The%20framework%20incorporates%20a%20Graph%20Convolutional%20Network%0A%28GCN%29%20module%20empowered%20by%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20enhance%20feature%0Aextraction%20from%20Google%20imagery.%20Simultaneously%2C%20the%20framework%20employs%20a%203D-CNN%0Aarchitecture%20to%20learn%20the%20spectral-spatial%20features%20of%20Sentinel-2%20imagery.%0AExperiments%20are%20conducted%20on%20a%20multi-source%20remote%20sensing%20image%20dataset%0Aspecifically%20designed%20for%20LCZ%20classification%2C%20validating%20the%20effectiveness%20of%0Athe%20proposed%20DF4LCZ.%20The%20related%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/ctrlovefly/DF4LCZ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09367v1&entry.124074799=Read"},
{"title": "SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash\n  Attention to Achieve 30 times Acceleration", "author": "Yanfei Songa and Bangzheng Pua and Peng Wanga and Hongxu Jiang and Dong Donga and Yiqing Shen", "abstract": "  Segment Anything Model (SAM) has garnered significant attention in\nsegmentation tasks due to their zero-shot generalization ability. However, a\nbroader application of SAMs to real-world practice has been restricted by their\nlow inference speed and high computational memory demands, which mainly stem\nfrom the attention mechanism. Existing work concentrated on optimizing the\nencoder, yet has not adequately addressed the inefficiency of the attention\nmechanism itself, even when distilled to a smaller model, which thus leaves\nspace for further improvement. In response, we introduce SAM-Lightening, a\nvariant of SAM, that features a re-engineered attention mechanism, termed\nDilated Flash Attention. It not only facilitates higher parallelism, enhancing\nprocessing efficiency but also retains compatibility with the existing\nFlashAttention. Correspondingly, we propose a progressive distillation to\nenable an efficient knowledge transfer from the vanilla SAM without costly\ntraining from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening\nsignificantly outperforms the state-of-the-art methods in both run-time\nefficiency and segmentation accuracy. Specifically, it can achieve an inference\nspeed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels,\nwhich is 30.1 times faster than the vanilla SAM and 2.1 times than the\nstate-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the\nvanilla SAM. The code and weights are available at\nhttps://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.\n", "link": "http://arxiv.org/abs/2403.09195v1", "date": "2024-03-14", "relevancy": 2.6643, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SAM-Lightening%3A%20A%20Lightweight%20Segment%20Anything%20Model%20with%20Dilated%20Flash%0A%20%20Attention%20to%20Achieve%2030%20times%20Acceleration&body=Title%3A%20SAM-Lightening%3A%20A%20Lightweight%20Segment%20Anything%20Model%20with%20Dilated%20Flash%0A%20%20Attention%20to%20Achieve%2030%20times%20Acceleration%0AAuthor%3A%20Yanfei%20Songa%20and%20Bangzheng%20Pua%20and%20Peng%20Wanga%20and%20Hongxu%20Jiang%20and%20Dong%20Donga%20and%20Yiqing%20Shen%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20garnered%20significant%20attention%20in%0Asegmentation%20tasks%20due%20to%20their%20zero-shot%20generalization%20ability.%20However%2C%20a%0Abroader%20application%20of%20SAMs%20to%20real-world%20practice%20has%20been%20restricted%20by%20their%0Alow%20inference%20speed%20and%20high%20computational%20memory%20demands%2C%20which%20mainly%20stem%0Afrom%20the%20attention%20mechanism.%20Existing%20work%20concentrated%20on%20optimizing%20the%0Aencoder%2C%20yet%20has%20not%20adequately%20addressed%20the%20inefficiency%20of%20the%20attention%0Amechanism%20itself%2C%20even%20when%20distilled%20to%20a%20smaller%20model%2C%20which%20thus%20leaves%0Aspace%20for%20further%20improvement.%20In%20response%2C%20we%20introduce%20SAM-Lightening%2C%20a%0Avariant%20of%20SAM%2C%20that%20features%20a%20re-engineered%20attention%20mechanism%2C%20termed%0ADilated%20Flash%20Attention.%20It%20not%20only%20facilitates%20higher%20parallelism%2C%20enhancing%0Aprocessing%20efficiency%20but%20also%20retains%20compatibility%20with%20the%20existing%0AFlashAttention.%20Correspondingly%2C%20we%20propose%20a%20progressive%20distillation%20to%0Aenable%20an%20efficient%20knowledge%20transfer%20from%20the%20vanilla%20SAM%20without%20costly%0Atraining%20from%20scratch.%20Experiments%20on%20COCO%20and%20LVIS%20reveal%20that%20SAM-Lightening%0Asignificantly%20outperforms%20the%20state-of-the-art%20methods%20in%20both%20run-time%0Aefficiency%20and%20segmentation%20accuracy.%20Specifically%2C%20it%20can%20achieve%20an%20inference%0Aspeed%20of%207%20milliseconds%20%28ms%29%20per%20image%2C%20for%20images%20of%20size%201024%2A1024%20pixels%2C%0Awhich%20is%2030.1%20times%20faster%20than%20the%20vanilla%20SAM%20and%202.1%20times%20than%20the%0Astate-of-the-art.%20Moreover%2C%20it%20takes%20only%20244MB%20memory%2C%20which%20is%203.5%5C%25%20of%20the%0Avanilla%20SAM.%20The%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/SAM-LIGHTENING-BC25/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09195v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-Lightening%3A%20A%20Lightweight%20Segment%20Anything%20Model%20with%20Dilated%20Flash%0A%20%20Attention%20to%20Achieve%2030%20times%20Acceleration&entry.906535625=Yanfei%20Songa%20and%20Bangzheng%20Pua%20and%20Peng%20Wanga%20and%20Hongxu%20Jiang%20and%20Dong%20Donga%20and%20Yiqing%20Shen&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20garnered%20significant%20attention%20in%0Asegmentation%20tasks%20due%20to%20their%20zero-shot%20generalization%20ability.%20However%2C%20a%0Abroader%20application%20of%20SAMs%20to%20real-world%20practice%20has%20been%20restricted%20by%20their%0Alow%20inference%20speed%20and%20high%20computational%20memory%20demands%2C%20which%20mainly%20stem%0Afrom%20the%20attention%20mechanism.%20Existing%20work%20concentrated%20on%20optimizing%20the%0Aencoder%2C%20yet%20has%20not%20adequately%20addressed%20the%20inefficiency%20of%20the%20attention%0Amechanism%20itself%2C%20even%20when%20distilled%20to%20a%20smaller%20model%2C%20which%20thus%20leaves%0Aspace%20for%20further%20improvement.%20In%20response%2C%20we%20introduce%20SAM-Lightening%2C%20a%0Avariant%20of%20SAM%2C%20that%20features%20a%20re-engineered%20attention%20mechanism%2C%20termed%0ADilated%20Flash%20Attention.%20It%20not%20only%20facilitates%20higher%20parallelism%2C%20enhancing%0Aprocessing%20efficiency%20but%20also%20retains%20compatibility%20with%20the%20existing%0AFlashAttention.%20Correspondingly%2C%20we%20propose%20a%20progressive%20distillation%20to%0Aenable%20an%20efficient%20knowledge%20transfer%20from%20the%20vanilla%20SAM%20without%20costly%0Atraining%20from%20scratch.%20Experiments%20on%20COCO%20and%20LVIS%20reveal%20that%20SAM-Lightening%0Asignificantly%20outperforms%20the%20state-of-the-art%20methods%20in%20both%20run-time%0Aefficiency%20and%20segmentation%20accuracy.%20Specifically%2C%20it%20can%20achieve%20an%20inference%0Aspeed%20of%207%20milliseconds%20%28ms%29%20per%20image%2C%20for%20images%20of%20size%201024%2A1024%20pixels%2C%0Awhich%20is%2030.1%20times%20faster%20than%20the%20vanilla%20SAM%20and%202.1%20times%20than%20the%0Astate-of-the-art.%20Moreover%2C%20it%20takes%20only%20244MB%20memory%2C%20which%20is%203.5%5C%25%20of%20the%0Avanilla%20SAM.%20The%20code%20and%20weights%20are%20available%20at%0Ahttps%3A//anonymous.4open.science/r/SAM-LIGHTENING-BC25/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09195v1&entry.124074799=Read"},
{"title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatial Relation Matching", "author": "Meng Chu and Zhedong Zheng and Wei Ji and Tingyu Wang and Tat-Seng Chua", "abstract": "  Navigating drones through natural language commands remains challenging due\nto the dearth of accessible multi-modal datasets and the stringent precision\nrequirements for aligning visual and textual data. To address this pressing\nneed, we introduce GeoText-1652, a new natural language-guided geo-localization\nbenchmark. This dataset is systematically constructed through an interactive\nhuman-computer process leveraging Large Language Model (LLM) driven annotation\ntechniques in conjunction with pre-trained vision models. GeoText-1652 extends\nthe established University-1652 image dataset with spatial-aware text\nannotations, thereby establishing one-to-one correspondences between image,\ntext, and bounding box elements. We further introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains a competitive recall rate\ncomparing other prevailing cross-modality methods. This underscores the\npromising potential of our approach in elevating drone control and navigation\nthrough the seamless integration of natural language commands in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2311.12751v2", "date": "2024-03-14", "relevancy": 2.6635, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5414}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching&body=Title%3A%20Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching%0AAuthor%3A%20Meng%20Chu%20and%20Zhedong%20Zheng%20and%20Wei%20Ji%20and%20Tingyu%20Wang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Navigating%20drones%20through%20natural%20language%20commands%20remains%20challenging%20due%0Ato%20the%20dearth%20of%20accessible%20multi-modal%20datasets%20and%20the%20stringent%20precision%0Arequirements%20for%20aligning%20visual%20and%20textual%20data.%20To%20address%20this%20pressing%0Aneed%2C%20we%20introduce%20GeoText-1652%2C%20a%20new%20natural%20language-guided%20geo-localization%0Abenchmark.%20This%20dataset%20is%20systematically%20constructed%20through%20an%20interactive%0Ahuman-computer%20process%20leveraging%20Large%20Language%20Model%20%28LLM%29%20driven%20annotation%0Atechniques%20in%20conjunction%20with%20pre-trained%20vision%20models.%20GeoText-1652%20extends%0Athe%20established%20University-1652%20image%20dataset%20with%20spatial-aware%20text%0Aannotations%2C%20thereby%20establishing%20one-to-one%20correspondences%20between%20image%2C%0Atext%2C%20and%20bounding%20box%20elements.%20We%20further%20introduce%20a%20new%20optimization%0Aobjective%20to%20leverage%20fine-grained%20spatial%20associations%2C%20called%20blending%0Aspatial%20matching%2C%20for%20region-level%20spatial%20relation%20matching.%20Extensive%0Aexperiments%20reveal%20that%20our%20approach%20maintains%20a%20competitive%20recall%20rate%0Acomparing%20other%20prevailing%20cross-modality%20methods.%20This%20underscores%20the%0Apromising%20potential%20of%20our%20approach%20in%20elevating%20drone%20control%20and%20navigation%0Athrough%20the%20seamless%20integration%20of%20natural%20language%20commands%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12751v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Natural%20Language-Guided%20Drones%3A%20GeoText-1652%20Benchmark%20with%0A%20%20Spatial%20Relation%20Matching&entry.906535625=Meng%20Chu%20and%20Zhedong%20Zheng%20and%20Wei%20Ji%20and%20Tingyu%20Wang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Navigating%20drones%20through%20natural%20language%20commands%20remains%20challenging%20due%0Ato%20the%20dearth%20of%20accessible%20multi-modal%20datasets%20and%20the%20stringent%20precision%0Arequirements%20for%20aligning%20visual%20and%20textual%20data.%20To%20address%20this%20pressing%0Aneed%2C%20we%20introduce%20GeoText-1652%2C%20a%20new%20natural%20language-guided%20geo-localization%0Abenchmark.%20This%20dataset%20is%20systematically%20constructed%20through%20an%20interactive%0Ahuman-computer%20process%20leveraging%20Large%20Language%20Model%20%28LLM%29%20driven%20annotation%0Atechniques%20in%20conjunction%20with%20pre-trained%20vision%20models.%20GeoText-1652%20extends%0Athe%20established%20University-1652%20image%20dataset%20with%20spatial-aware%20text%0Aannotations%2C%20thereby%20establishing%20one-to-one%20correspondences%20between%20image%2C%0Atext%2C%20and%20bounding%20box%20elements.%20We%20further%20introduce%20a%20new%20optimization%0Aobjective%20to%20leverage%20fine-grained%20spatial%20associations%2C%20called%20blending%0Aspatial%20matching%2C%20for%20region-level%20spatial%20relation%20matching.%20Extensive%0Aexperiments%20reveal%20that%20our%20approach%20maintains%20a%20competitive%20recall%20rate%0Acomparing%20other%20prevailing%20cross-modality%20methods.%20This%20underscores%20the%0Apromising%20potential%20of%20our%20approach%20in%20elevating%20drone%20control%20and%20navigation%0Athrough%20the%20seamless%20integration%20of%20natural%20language%20commands%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12751v2&entry.124074799=Read"},
{"title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety\n  Filters of Text-to-Image Models", "author": "Yimo Deng and Huangxun Chen", "abstract": "  Text-to-image (TTI) models offer many innovative services but also raise\nethical concerns due to their potential to generate unethical images. Most\npublic TTI services employ safety filters to prevent unintended images. In this\nwork, we introduce the Divide-and-Conquer Attack to circumvent the safety\nfilters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our\nattack leverages LLMs as text transformation agents to create adversarial\nprompts. We design attack helper prompts that effectively guide LLMs to break\ndown an unethical drawing intent into multiple benign descriptions of\nindividual image elements, allowing them to bypass safety filters while still\ngenerating unethical images. Because the latent harmful meaning only becomes\napparent when all individual elements are drawn together. Our evaluation\ndemonstrates that our attack successfully circumvents multiple strong\nclosed-box safety filters. The comprehensive success rate of DACA bypassing the\nsafety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while\nthe success rate for bypassing Midjourney V6 exceeds 75%. Our findings have\nmore severe security implications than methods of manual crafting or iterative\nTTI model querying due to lower attack barrier, enhanced interpretability , and\nbetter adaptation to defense. Our prototype is available at:\nhttps://github.com/researchcode001/Divide-and-Conquer-Attack\n", "link": "http://arxiv.org/abs/2312.07130v3", "date": "2024-03-14", "relevancy": 2.6595, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5172}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models&body=Title%3A%20Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models%0AAuthor%3A%20Yimo%20Deng%20and%20Huangxun%20Chen%0AAbstract%3A%20%20%20Text-to-image%20%28TTI%29%20models%20offer%20many%20innovative%20services%20but%20also%20raise%0Aethical%20concerns%20due%20to%20their%20potential%20to%20generate%20unethical%20images.%20Most%0Apublic%20TTI%20services%20employ%20safety%20filters%20to%20prevent%20unintended%20images.%20In%20this%0Awork%2C%20we%20introduce%20the%20Divide-and-Conquer%20Attack%20to%20circumvent%20the%20safety%0Afilters%20of%20state-of%20the-art%20TTI%20models%2C%20including%20DALL-E%203%20and%20Midjourney.%20Our%0Aattack%20leverages%20LLMs%20as%20text%20transformation%20agents%20to%20create%20adversarial%0Aprompts.%20We%20design%20attack%20helper%20prompts%20that%20effectively%20guide%20LLMs%20to%20break%0Adown%20an%20unethical%20drawing%20intent%20into%20multiple%20benign%20descriptions%20of%0Aindividual%20image%20elements%2C%20allowing%20them%20to%20bypass%20safety%20filters%20while%20still%0Agenerating%20unethical%20images.%20Because%20the%20latent%20harmful%20meaning%20only%20becomes%0Aapparent%20when%20all%20individual%20elements%20are%20drawn%20together.%20Our%20evaluation%0Ademonstrates%20that%20our%20attack%20successfully%20circumvents%20multiple%20strong%0Aclosed-box%20safety%20filters.%20The%20comprehensive%20success%20rate%20of%20DACA%20bypassing%20the%0Asafety%20filters%20of%20the%20state-of-the-art%20TTI%20engine%20DALL-E%203%20is%20above%2085%25%2C%20while%0Athe%20success%20rate%20for%20bypassing%20Midjourney%20V6%20exceeds%2075%25.%20Our%20findings%20have%0Amore%20severe%20security%20implications%20than%20methods%20of%20manual%20crafting%20or%20iterative%0ATTI%20model%20querying%20due%20to%20lower%20attack%20barrier%2C%20enhanced%20interpretability%20%2C%20and%0Abetter%20adaptation%20to%20defense.%20Our%20prototype%20is%20available%20at%3A%0Ahttps%3A//github.com/researchcode001/Divide-and-Conquer-Attack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07130v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide-and-Conquer%20Attack%3A%20Harnessing%20the%20Power%20of%20LLM%20to%20Bypass%20Safety%0A%20%20Filters%20of%20Text-to-Image%20Models&entry.906535625=Yimo%20Deng%20and%20Huangxun%20Chen&entry.1292438233=%20%20Text-to-image%20%28TTI%29%20models%20offer%20many%20innovative%20services%20but%20also%20raise%0Aethical%20concerns%20due%20to%20their%20potential%20to%20generate%20unethical%20images.%20Most%0Apublic%20TTI%20services%20employ%20safety%20filters%20to%20prevent%20unintended%20images.%20In%20this%0Awork%2C%20we%20introduce%20the%20Divide-and-Conquer%20Attack%20to%20circumvent%20the%20safety%0Afilters%20of%20state-of%20the-art%20TTI%20models%2C%20including%20DALL-E%203%20and%20Midjourney.%20Our%0Aattack%20leverages%20LLMs%20as%20text%20transformation%20agents%20to%20create%20adversarial%0Aprompts.%20We%20design%20attack%20helper%20prompts%20that%20effectively%20guide%20LLMs%20to%20break%0Adown%20an%20unethical%20drawing%20intent%20into%20multiple%20benign%20descriptions%20of%0Aindividual%20image%20elements%2C%20allowing%20them%20to%20bypass%20safety%20filters%20while%20still%0Agenerating%20unethical%20images.%20Because%20the%20latent%20harmful%20meaning%20only%20becomes%0Aapparent%20when%20all%20individual%20elements%20are%20drawn%20together.%20Our%20evaluation%0Ademonstrates%20that%20our%20attack%20successfully%20circumvents%20multiple%20strong%0Aclosed-box%20safety%20filters.%20The%20comprehensive%20success%20rate%20of%20DACA%20bypassing%20the%0Asafety%20filters%20of%20the%20state-of-the-art%20TTI%20engine%20DALL-E%203%20is%20above%2085%25%2C%20while%0Athe%20success%20rate%20for%20bypassing%20Midjourney%20V6%20exceeds%2075%25.%20Our%20findings%20have%0Amore%20severe%20security%20implications%20than%20methods%20of%20manual%20crafting%20or%20iterative%0ATTI%20model%20querying%20due%20to%20lower%20attack%20barrier%2C%20enhanced%20interpretability%20%2C%20and%0Abetter%20adaptation%20to%20defense.%20Our%20prototype%20is%20available%20at%3A%0Ahttps%3A//github.com/researchcode001/Divide-and-Conquer-Attack%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07130v3&entry.124074799=Read"},
{"title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for\n  Recognizing Low-Quality Images", "author": "Jinsung Jeon and Hyundong Jin and Jonghyun Choi and Sanghyun Hong and Dongeun Lee and Kookjin Lee and Noseong Park", "abstract": "  A standard practice in developing image recognition models is to train a\nmodel on a specific image resolution and then deploy it. However, in real-world\ninference, models often encounter images different from the training sets in\nresolution and/or subject to natural variations such as weather changes, noise\ntypes and compression artifacts. While traditional solutions involve training\nmultiple models for different resolutions or input variations, these methods\nare computationally expensive and thus do not scale in practice. To this end,\nwe propose a novel neural network model, parallel-structured and all-component\nFourier neural operator (PAC-FNO), that addresses the problem. Unlike\nconventional feed-forward neural networks, PAC-FNO operates in the frequency\ndomain, allowing it to handle images of varying resolutions within a single\nmodel. We also propose a two-stage algorithm for training PAC-FNO with a\nminimal modification to the original, downstream model. Moreover, the proposed\nPAC-FNO is ready to work with existing image recognition models. Extensively\nevaluating methods with seven image recognition benchmarks, we show that the\nproposed PAC-FNO improves the performance of existing baseline models on images\nwith various resolutions by up to 77.1% and various types of natural variations\nin the images at inference.\n", "link": "http://arxiv.org/abs/2402.12721v4", "date": "2024-03-14", "relevancy": 2.6567, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5129}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PAC-FNO%3A%20Parallel-Structured%20All-Component%20Fourier%20Neural%20Operators%20for%0A%20%20Recognizing%20Low-Quality%20Images&body=Title%3A%20PAC-FNO%3A%20Parallel-Structured%20All-Component%20Fourier%20Neural%20Operators%20for%0A%20%20Recognizing%20Low-Quality%20Images%0AAuthor%3A%20Jinsung%20Jeon%20and%20Hyundong%20Jin%20and%20Jonghyun%20Choi%20and%20Sanghyun%20Hong%20and%20Dongeun%20Lee%20and%20Kookjin%20Lee%20and%20Noseong%20Park%0AAbstract%3A%20%20%20A%20standard%20practice%20in%20developing%20image%20recognition%20models%20is%20to%20train%20a%0Amodel%20on%20a%20specific%20image%20resolution%20and%20then%20deploy%20it.%20However%2C%20in%20real-world%0Ainference%2C%20models%20often%20encounter%20images%20different%20from%20the%20training%20sets%20in%0Aresolution%20and/or%20subject%20to%20natural%20variations%20such%20as%20weather%20changes%2C%20noise%0Atypes%20and%20compression%20artifacts.%20While%20traditional%20solutions%20involve%20training%0Amultiple%20models%20for%20different%20resolutions%20or%20input%20variations%2C%20these%20methods%0Aare%20computationally%20expensive%20and%20thus%20do%20not%20scale%20in%20practice.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20neural%20network%20model%2C%20parallel-structured%20and%20all-component%0AFourier%20neural%20operator%20%28PAC-FNO%29%2C%20that%20addresses%20the%20problem.%20Unlike%0Aconventional%20feed-forward%20neural%20networks%2C%20PAC-FNO%20operates%20in%20the%20frequency%0Adomain%2C%20allowing%20it%20to%20handle%20images%20of%20varying%20resolutions%20within%20a%20single%0Amodel.%20We%20also%20propose%20a%20two-stage%20algorithm%20for%20training%20PAC-FNO%20with%20a%0Aminimal%20modification%20to%20the%20original%2C%20downstream%20model.%20Moreover%2C%20the%20proposed%0APAC-FNO%20is%20ready%20to%20work%20with%20existing%20image%20recognition%20models.%20Extensively%0Aevaluating%20methods%20with%20seven%20image%20recognition%20benchmarks%2C%20we%20show%20that%20the%0Aproposed%20PAC-FNO%20improves%20the%20performance%20of%20existing%20baseline%20models%20on%20images%0Awith%20various%20resolutions%20by%20up%20to%2077.1%25%20and%20various%20types%20of%20natural%20variations%0Ain%20the%20images%20at%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12721v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAC-FNO%3A%20Parallel-Structured%20All-Component%20Fourier%20Neural%20Operators%20for%0A%20%20Recognizing%20Low-Quality%20Images&entry.906535625=Jinsung%20Jeon%20and%20Hyundong%20Jin%20and%20Jonghyun%20Choi%20and%20Sanghyun%20Hong%20and%20Dongeun%20Lee%20and%20Kookjin%20Lee%20and%20Noseong%20Park&entry.1292438233=%20%20A%20standard%20practice%20in%20developing%20image%20recognition%20models%20is%20to%20train%20a%0Amodel%20on%20a%20specific%20image%20resolution%20and%20then%20deploy%20it.%20However%2C%20in%20real-world%0Ainference%2C%20models%20often%20encounter%20images%20different%20from%20the%20training%20sets%20in%0Aresolution%20and/or%20subject%20to%20natural%20variations%20such%20as%20weather%20changes%2C%20noise%0Atypes%20and%20compression%20artifacts.%20While%20traditional%20solutions%20involve%20training%0Amultiple%20models%20for%20different%20resolutions%20or%20input%20variations%2C%20these%20methods%0Aare%20computationally%20expensive%20and%20thus%20do%20not%20scale%20in%20practice.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20neural%20network%20model%2C%20parallel-structured%20and%20all-component%0AFourier%20neural%20operator%20%28PAC-FNO%29%2C%20that%20addresses%20the%20problem.%20Unlike%0Aconventional%20feed-forward%20neural%20networks%2C%20PAC-FNO%20operates%20in%20the%20frequency%0Adomain%2C%20allowing%20it%20to%20handle%20images%20of%20varying%20resolutions%20within%20a%20single%0Amodel.%20We%20also%20propose%20a%20two-stage%20algorithm%20for%20training%20PAC-FNO%20with%20a%0Aminimal%20modification%20to%20the%20original%2C%20downstream%20model.%20Moreover%2C%20the%20proposed%0APAC-FNO%20is%20ready%20to%20work%20with%20existing%20image%20recognition%20models.%20Extensively%0Aevaluating%20methods%20with%20seven%20image%20recognition%20benchmarks%2C%20we%20show%20that%20the%0Aproposed%20PAC-FNO%20improves%20the%20performance%20of%20existing%20baseline%20models%20on%20images%0Awith%20various%20resolutions%20by%20up%20to%2077.1%25%20and%20various%20types%20of%20natural%20variations%0Ain%20the%20images%20at%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12721v4&entry.124074799=Read"},
{"title": "HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction", "author": "Yi Zhou and Hui Zhang and Jiaqian Yu and Yifan Yang and Sangil Jung and Seung-In Park and ByungIn Yoo", "abstract": "  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n", "link": "http://arxiv.org/abs/2403.08639v1", "date": "2024-03-13", "relevancy": 2.6496, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&body=Title%3A%20HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction%0AAuthor%3A%20Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo%0AAbstract%3A%20%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HIMap%3A%20HybrId%20Representation%20Learning%20for%20End-to-end%20Vectorized%20HD%20Map%0A%20%20Construction&entry.906535625=Yi%20Zhou%20and%20Hui%20Zhang%20and%20Jiaqian%20Yu%20and%20Yifan%20Yang%20and%20Sangil%20Jung%20and%20Seung-In%20Park%20and%20ByungIn%20Yoo&entry.1292438233=%20%20Vectorized%20High-Definition%20%28HD%29%20map%20construction%20requires%20predictions%20of%20the%0Acategory%20and%20point%20coordinates%20of%20map%20elements%20%28e.g.%20road%20boundary%2C%20lane%0Adivider%2C%20pedestrian%20crossing%2C%20etc.%29.%20State-of-the-art%20methods%20are%20mainly%20based%0Aon%20point-level%20representation%20learning%20for%20regressing%20accurate%20point%0Acoordinates.%20However%2C%20this%20pipeline%20has%20limitations%20in%20obtaining%20element-level%0Ainformation%20and%20handling%20element-level%20failures%2C%20e.g.%20erroneous%20element%20shape%0Aor%20entanglement%20between%20elements.%20To%20tackle%20the%20above%20issues%2C%20we%20propose%20a%0Asimple%20yet%20effective%20HybrId%20framework%20named%20HIMap%20to%20sufficiently%20learn%20and%0Ainteract%20both%20point-level%20and%20element-level%20information.%20Concretely%2C%20we%0Aintroduce%20a%20hybrid%20representation%20called%20HIQuery%20to%20represent%20all%20map%20elements%2C%0Aand%20propose%20a%20point-element%20interactor%20to%20interactively%20extract%20and%20encode%20the%0Ahybrid%20information%20of%20elements%2C%20e.g.%20point%20position%20and%20element%20shape%2C%20into%20the%0AHIQuery.%20Additionally%2C%20we%20present%20a%20point-element%20consistency%20constraint%20to%0Aenhance%20the%20consistency%20between%20the%20point-level%20and%20element-level%20information.%0AFinally%2C%20the%20output%20point-element%20integrated%20HIQuery%20can%20be%20directly%20converted%0Ainto%20map%20elements%27%20class%2C%20point%20coordinates%2C%20and%20mask.%20We%20conduct%20extensive%0Aexperiments%20and%20consistently%20outperform%20previous%20methods%20on%20both%20nuScenes%20and%0AArgoverse2%20datasets.%20Notably%2C%20our%20method%20achieves%20%2477.8%24%20mAP%20on%20the%20nuScenes%0Adataset%2C%20remarkably%20superior%20to%20previous%20SOTAs%20by%20%248.3%24%20mAP%20at%20least.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08639v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Covariance Estimation", "author": "Tzvi Diskin and Ami Wiesel", "abstract": "  We consider the use of deep learning for covariance estimation. We propose to\nglobally learn a neural network that will then be applied locally at inference\ntime. Leveraging recent advancements in self-supervised foundational models, we\ntrain the network without any labeling by simply masking different samples and\nlearning to predict their covariance given their surrounding neighbors. The\narchitecture is based on the popular attention mechanism. Its main advantage\nover classical methods is the automatic exploitation of global characteristics\nwithout any distributional assumptions or regularization. It can be pre-trained\nas a foundation model and then be repurposed for various downstream tasks,\ne.g., adaptive target detection in radar or hyperspectral imagery.\n", "link": "http://arxiv.org/abs/2403.08662v1", "date": "2024-03-13", "relevancy": 2.6399, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5537}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.526}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Covariance%20Estimation&body=Title%3A%20Self-Supervised%20Learning%20for%20Covariance%20Estimation%0AAuthor%3A%20Tzvi%20Diskin%20and%20Ami%20Wiesel%0AAbstract%3A%20%20%20We%20consider%20the%20use%20of%20deep%20learning%20for%20covariance%20estimation.%20We%20propose%20to%0Aglobally%20learn%20a%20neural%20network%20that%20will%20then%20be%20applied%20locally%20at%20inference%0Atime.%20Leveraging%20recent%20advancements%20in%20self-supervised%20foundational%20models%2C%20we%0Atrain%20the%20network%20without%20any%20labeling%20by%20simply%20masking%20different%20samples%20and%0Alearning%20to%20predict%20their%20covariance%20given%20their%20surrounding%20neighbors.%20The%0Aarchitecture%20is%20based%20on%20the%20popular%20attention%20mechanism.%20Its%20main%20advantage%0Aover%20classical%20methods%20is%20the%20automatic%20exploitation%20of%20global%20characteristics%0Awithout%20any%20distributional%20assumptions%20or%20regularization.%20It%20can%20be%20pre-trained%0Aas%20a%20foundation%20model%20and%20then%20be%20repurposed%20for%20various%20downstream%20tasks%2C%0Ae.g.%2C%20adaptive%20target%20detection%20in%20radar%20or%20hyperspectral%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Covariance%20Estimation&entry.906535625=Tzvi%20Diskin%20and%20Ami%20Wiesel&entry.1292438233=%20%20We%20consider%20the%20use%20of%20deep%20learning%20for%20covariance%20estimation.%20We%20propose%20to%0Aglobally%20learn%20a%20neural%20network%20that%20will%20then%20be%20applied%20locally%20at%20inference%0Atime.%20Leveraging%20recent%20advancements%20in%20self-supervised%20foundational%20models%2C%20we%0Atrain%20the%20network%20without%20any%20labeling%20by%20simply%20masking%20different%20samples%20and%0Alearning%20to%20predict%20their%20covariance%20given%20their%20surrounding%20neighbors.%20The%0Aarchitecture%20is%20based%20on%20the%20popular%20attention%20mechanism.%20Its%20main%20advantage%0Aover%20classical%20methods%20is%20the%20automatic%20exploitation%20of%20global%20characteristics%0Awithout%20any%20distributional%20assumptions%20or%20regularization.%20It%20can%20be%20pre-trained%0Aas%20a%20foundation%20model%20and%20then%20be%20repurposed%20for%20various%20downstream%20tasks%2C%0Ae.g.%2C%20adaptive%20target%20detection%20in%20radar%20or%20hyperspectral%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08662v1&entry.124074799=Read"},
{"title": "Generalizable Two-Branch Framework for Image Class-Incremental Learning", "author": "Chao Wu and Xiaobin Chang and Ruixuan Wang", "abstract": "  Deep neural networks often severely forget previously learned knowledge when\nlearning new knowledge. Various continual learning (CL) methods have been\nproposed to handle such a catastrophic forgetting issue from different\nperspectives and achieved substantial improvements. In this paper, a novel\ntwo-branch continual learning framework is proposed to further enhance most\nexisting CL methods. Specifically, the main branch can be any existing CL model\nand the newly introduced side branch is a lightweight convolutional network.\nThe output of each main branch block is modulated by the output of the\ncorresponding side branch block. Such a simple two-branch model can then be\neasily implemented and learned with the vanilla optimization setting without\nwhistles and bells. Extensive experiments with various settings on multiple\nimage datasets show that the proposed framework yields consistent improvements\nover state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2402.18086v4", "date": "2024-03-13", "relevancy": 2.6336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5339}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5334}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Two-Branch%20Framework%20for%20Image%20Class-Incremental%20Learning&body=Title%3A%20Generalizable%20Two-Branch%20Framework%20for%20Image%20Class-Incremental%20Learning%0AAuthor%3A%20Chao%20Wu%20and%20Xiaobin%20Chang%20and%20Ruixuan%20Wang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20severely%20forget%20previously%20learned%20knowledge%20when%0Alearning%20new%20knowledge.%20Various%20continual%20learning%20%28CL%29%20methods%20have%20been%0Aproposed%20to%20handle%20such%20a%20catastrophic%20forgetting%20issue%20from%20different%0Aperspectives%20and%20achieved%20substantial%20improvements.%20In%20this%20paper%2C%20a%20novel%0Atwo-branch%20continual%20learning%20framework%20is%20proposed%20to%20further%20enhance%20most%0Aexisting%20CL%20methods.%20Specifically%2C%20the%20main%20branch%20can%20be%20any%20existing%20CL%20model%0Aand%20the%20newly%20introduced%20side%20branch%20is%20a%20lightweight%20convolutional%20network.%0AThe%20output%20of%20each%20main%20branch%20block%20is%20modulated%20by%20the%20output%20of%20the%0Acorresponding%20side%20branch%20block.%20Such%20a%20simple%20two-branch%20model%20can%20then%20be%0Aeasily%20implemented%20and%20learned%20with%20the%20vanilla%20optimization%20setting%20without%0Awhistles%20and%20bells.%20Extensive%20experiments%20with%20various%20settings%20on%20multiple%0Aimage%20datasets%20show%20that%20the%20proposed%20framework%20yields%20consistent%20improvements%0Aover%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18086v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Two-Branch%20Framework%20for%20Image%20Class-Incremental%20Learning&entry.906535625=Chao%20Wu%20and%20Xiaobin%20Chang%20and%20Ruixuan%20Wang&entry.1292438233=%20%20Deep%20neural%20networks%20often%20severely%20forget%20previously%20learned%20knowledge%20when%0Alearning%20new%20knowledge.%20Various%20continual%20learning%20%28CL%29%20methods%20have%20been%0Aproposed%20to%20handle%20such%20a%20catastrophic%20forgetting%20issue%20from%20different%0Aperspectives%20and%20achieved%20substantial%20improvements.%20In%20this%20paper%2C%20a%20novel%0Atwo-branch%20continual%20learning%20framework%20is%20proposed%20to%20further%20enhance%20most%0Aexisting%20CL%20methods.%20Specifically%2C%20the%20main%20branch%20can%20be%20any%20existing%20CL%20model%0Aand%20the%20newly%20introduced%20side%20branch%20is%20a%20lightweight%20convolutional%20network.%0AThe%20output%20of%20each%20main%20branch%20block%20is%20modulated%20by%20the%20output%20of%20the%0Acorresponding%20side%20branch%20block.%20Such%20a%20simple%20two-branch%20model%20can%20then%20be%0Aeasily%20implemented%20and%20learned%20with%20the%20vanilla%20optimization%20setting%20without%0Awhistles%20and%20bells.%20Extensive%20experiments%20with%20various%20settings%20on%20multiple%0Aimage%20datasets%20show%20that%20the%20proposed%20framework%20yields%20consistent%20improvements%0Aover%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18086v4&entry.124074799=Read"},
{"title": "Generalized Relevance Learning Grassmann Quantization", "author": "M. Mohammadi and M. Babai and M. H. F. Wilkinson", "abstract": "  Due to advancements in digital cameras, it is easy to gather multiple images\n(or videos) from an object under different conditions. Therefore, image-set\nclassification has attracted more attention, and different solutions were\nproposed to model them. A popular way to model image sets is subspaces, which\nform a manifold called the Grassmann manifold. In this contribution, we extend\nthe application of Generalized Relevance Learning Vector Quantization to deal\nwith Grassmann manifold. The proposed model returns a set of prototype\nsubspaces and a relevance vector. While prototypes model typical behaviours\nwithin classes, the relevance factors specify the most discriminative principal\nvectors (or images) for the classification task. They both provide insights\ninto the model's decisions by highlighting influential images and pixels for\npredictions. Moreover, due to learning prototypes, the model complexity of the\nnew method during inference is independent of dataset size, unlike previous\nworks. We applied it to several recognition tasks including handwritten digit\nrecognition, face recognition, activity recognition, and object recognition.\nExperiments demonstrate that it outperforms previous works with lower\ncomplexity and can successfully model the variation, such as handwritten style\nor lighting conditions. Moreover, the presence of relevances makes the model\nrobust to the selection of subspaces' dimensionality.\n", "link": "http://arxiv.org/abs/2403.09183v1", "date": "2024-03-14", "relevancy": 2.6318, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5422}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5218}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalized%20Relevance%20Learning%20Grassmann%20Quantization&body=Title%3A%20Generalized%20Relevance%20Learning%20Grassmann%20Quantization%0AAuthor%3A%20M.%20Mohammadi%20and%20M.%20Babai%20and%20M.%20H.%20F.%20Wilkinson%0AAbstract%3A%20%20%20Due%20to%20advancements%20in%20digital%20cameras%2C%20it%20is%20easy%20to%20gather%20multiple%20images%0A%28or%20videos%29%20from%20an%20object%20under%20different%20conditions.%20Therefore%2C%20image-set%0Aclassification%20has%20attracted%20more%20attention%2C%20and%20different%20solutions%20were%0Aproposed%20to%20model%20them.%20A%20popular%20way%20to%20model%20image%20sets%20is%20subspaces%2C%20which%0Aform%20a%20manifold%20called%20the%20Grassmann%20manifold.%20In%20this%20contribution%2C%20we%20extend%0Athe%20application%20of%20Generalized%20Relevance%20Learning%20Vector%20Quantization%20to%20deal%0Awith%20Grassmann%20manifold.%20The%20proposed%20model%20returns%20a%20set%20of%20prototype%0Asubspaces%20and%20a%20relevance%20vector.%20While%20prototypes%20model%20typical%20behaviours%0Awithin%20classes%2C%20the%20relevance%20factors%20specify%20the%20most%20discriminative%20principal%0Avectors%20%28or%20images%29%20for%20the%20classification%20task.%20They%20both%20provide%20insights%0Ainto%20the%20model%27s%20decisions%20by%20highlighting%20influential%20images%20and%20pixels%20for%0Apredictions.%20Moreover%2C%20due%20to%20learning%20prototypes%2C%20the%20model%20complexity%20of%20the%0Anew%20method%20during%20inference%20is%20independent%20of%20dataset%20size%2C%20unlike%20previous%0Aworks.%20We%20applied%20it%20to%20several%20recognition%20tasks%20including%20handwritten%20digit%0Arecognition%2C%20face%20recognition%2C%20activity%20recognition%2C%20and%20object%20recognition.%0AExperiments%20demonstrate%20that%20it%20outperforms%20previous%20works%20with%20lower%0Acomplexity%20and%20can%20successfully%20model%20the%20variation%2C%20such%20as%20handwritten%20style%0Aor%20lighting%20conditions.%20Moreover%2C%20the%20presence%20of%20relevances%20makes%20the%20model%0Arobust%20to%20the%20selection%20of%20subspaces%27%20dimensionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09183v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Relevance%20Learning%20Grassmann%20Quantization&entry.906535625=M.%20Mohammadi%20and%20M.%20Babai%20and%20M.%20H.%20F.%20Wilkinson&entry.1292438233=%20%20Due%20to%20advancements%20in%20digital%20cameras%2C%20it%20is%20easy%20to%20gather%20multiple%20images%0A%28or%20videos%29%20from%20an%20object%20under%20different%20conditions.%20Therefore%2C%20image-set%0Aclassification%20has%20attracted%20more%20attention%2C%20and%20different%20solutions%20were%0Aproposed%20to%20model%20them.%20A%20popular%20way%20to%20model%20image%20sets%20is%20subspaces%2C%20which%0Aform%20a%20manifold%20called%20the%20Grassmann%20manifold.%20In%20this%20contribution%2C%20we%20extend%0Athe%20application%20of%20Generalized%20Relevance%20Learning%20Vector%20Quantization%20to%20deal%0Awith%20Grassmann%20manifold.%20The%20proposed%20model%20returns%20a%20set%20of%20prototype%0Asubspaces%20and%20a%20relevance%20vector.%20While%20prototypes%20model%20typical%20behaviours%0Awithin%20classes%2C%20the%20relevance%20factors%20specify%20the%20most%20discriminative%20principal%0Avectors%20%28or%20images%29%20for%20the%20classification%20task.%20They%20both%20provide%20insights%0Ainto%20the%20model%27s%20decisions%20by%20highlighting%20influential%20images%20and%20pixels%20for%0Apredictions.%20Moreover%2C%20due%20to%20learning%20prototypes%2C%20the%20model%20complexity%20of%20the%0Anew%20method%20during%20inference%20is%20independent%20of%20dataset%20size%2C%20unlike%20previous%0Aworks.%20We%20applied%20it%20to%20several%20recognition%20tasks%20including%20handwritten%20digit%0Arecognition%2C%20face%20recognition%2C%20activity%20recognition%2C%20and%20object%20recognition.%0AExperiments%20demonstrate%20that%20it%20outperforms%20previous%20works%20with%20lower%0Acomplexity%20and%20can%20successfully%20model%20the%20variation%2C%20such%20as%20handwritten%20style%0Aor%20lighting%20conditions.%20Moreover%2C%20the%20presence%20of%20relevances%20makes%20the%20model%0Arobust%20to%20the%20selection%20of%20subspaces%27%20dimensionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09183v1&entry.124074799=Read"},
{"title": "SSR-Encoder: Encoding Selective Subject Representation for\n  Subject-Driven Generation", "author": "Yuxuan Zhang and Yiren Song and Jiaming Liu and Rui Wang and Jinpeng Yu and Hao Tang and Huaxia Li and Xu Tang and Yao Hu and Han Pan and Zhongliang Jing", "abstract": "  Recent advancements in subject-driven image generation have led to zero-shot\ngeneration, yet precise selection and focus on crucial subject representations\nremain challenging. Addressing this, we introduce the SSR-Encoder, a novel\narchitecture designed for selectively capturing any subject from single or\nmultiple reference images. It responds to various query modalities including\ntext and masks, without necessitating test-time fine-tuning. The SSR-Encoder\ncombines a Token-to-Patch Aligner that aligns query inputs with image patches\nand a Detail-Preserving Subject Encoder for extracting and preserving fine\nfeatures of the subjects, thereby generating subject embeddings. These\nembeddings, used in conjunction with original text embeddings, condition the\ngeneration process. Characterized by its model generalizability and efficiency,\nthe SSR-Encoder adapts to a range of custom models and control modules.\nEnhanced by the Embedding Consistency Regularization Loss for improved\ntraining, our extensive experiments demonstrate its effectiveness in versatile\nand high-quality image generation, indicating its broad applicability. Project\npage: https://ssr-encoder.github.io\n", "link": "http://arxiv.org/abs/2312.16272v2", "date": "2024-03-14", "relevancy": 2.6277, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5509}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5164}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation&body=Title%3A%20SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation%0AAuthor%3A%20Yuxuan%20Zhang%20and%20Yiren%20Song%20and%20Jiaming%20Liu%20and%20Rui%20Wang%20and%20Jinpeng%20Yu%20and%20Hao%20Tang%20and%20Huaxia%20Li%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Han%20Pan%20and%20Zhongliang%20Jing%0AAbstract%3A%20%20%20Recent%20advancements%20in%20subject-driven%20image%20generation%20have%20led%20to%20zero-shot%0Ageneration%2C%20yet%20precise%20selection%20and%20focus%20on%20crucial%20subject%20representations%0Aremain%20challenging.%20Addressing%20this%2C%20we%20introduce%20the%20SSR-Encoder%2C%20a%20novel%0Aarchitecture%20designed%20for%20selectively%20capturing%20any%20subject%20from%20single%20or%0Amultiple%20reference%20images.%20It%20responds%20to%20various%20query%20modalities%20including%0Atext%20and%20masks%2C%20without%20necessitating%20test-time%20fine-tuning.%20The%20SSR-Encoder%0Acombines%20a%20Token-to-Patch%20Aligner%20that%20aligns%20query%20inputs%20with%20image%20patches%0Aand%20a%20Detail-Preserving%20Subject%20Encoder%20for%20extracting%20and%20preserving%20fine%0Afeatures%20of%20the%20subjects%2C%20thereby%20generating%20subject%20embeddings.%20These%0Aembeddings%2C%20used%20in%20conjunction%20with%20original%20text%20embeddings%2C%20condition%20the%0Ageneration%20process.%20Characterized%20by%20its%20model%20generalizability%20and%20efficiency%2C%0Athe%20SSR-Encoder%20adapts%20to%20a%20range%20of%20custom%20models%20and%20control%20modules.%0AEnhanced%20by%20the%20Embedding%20Consistency%20Regularization%20Loss%20for%20improved%0Atraining%2C%20our%20extensive%20experiments%20demonstrate%20its%20effectiveness%20in%20versatile%0Aand%20high-quality%20image%20generation%2C%20indicating%20its%20broad%20applicability.%20Project%0Apage%3A%20https%3A//ssr-encoder.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16272v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSR-Encoder%3A%20Encoding%20Selective%20Subject%20Representation%20for%0A%20%20Subject-Driven%20Generation&entry.906535625=Yuxuan%20Zhang%20and%20Yiren%20Song%20and%20Jiaming%20Liu%20and%20Rui%20Wang%20and%20Jinpeng%20Yu%20and%20Hao%20Tang%20and%20Huaxia%20Li%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Han%20Pan%20and%20Zhongliang%20Jing&entry.1292438233=%20%20Recent%20advancements%20in%20subject-driven%20image%20generation%20have%20led%20to%20zero-shot%0Ageneration%2C%20yet%20precise%20selection%20and%20focus%20on%20crucial%20subject%20representations%0Aremain%20challenging.%20Addressing%20this%2C%20we%20introduce%20the%20SSR-Encoder%2C%20a%20novel%0Aarchitecture%20designed%20for%20selectively%20capturing%20any%20subject%20from%20single%20or%0Amultiple%20reference%20images.%20It%20responds%20to%20various%20query%20modalities%20including%0Atext%20and%20masks%2C%20without%20necessitating%20test-time%20fine-tuning.%20The%20SSR-Encoder%0Acombines%20a%20Token-to-Patch%20Aligner%20that%20aligns%20query%20inputs%20with%20image%20patches%0Aand%20a%20Detail-Preserving%20Subject%20Encoder%20for%20extracting%20and%20preserving%20fine%0Afeatures%20of%20the%20subjects%2C%20thereby%20generating%20subject%20embeddings.%20These%0Aembeddings%2C%20used%20in%20conjunction%20with%20original%20text%20embeddings%2C%20condition%20the%0Ageneration%20process.%20Characterized%20by%20its%20model%20generalizability%20and%20efficiency%2C%0Athe%20SSR-Encoder%20adapts%20to%20a%20range%20of%20custom%20models%20and%20control%20modules.%0AEnhanced%20by%20the%20Embedding%20Consistency%20Regularization%20Loss%20for%20improved%0Atraining%2C%20our%20extensive%20experiments%20demonstrate%20its%20effectiveness%20in%20versatile%0Aand%20high-quality%20image%20generation%2C%20indicating%20its%20broad%20applicability.%20Project%0Apage%3A%20https%3A//ssr-encoder.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16272v2&entry.124074799=Read"},
{"title": "Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier", "author": "Fan Zhang and Wei Qin and Weijieying Ren and Lei Wang and Zetong Chen and Richang Hong", "abstract": "  In the real-world setting, data often follows a long-tailed distribution,\nwhere head classes contain significantly more training samples than tail\nclasses. Consequently, models trained on such data tend to be biased toward\nhead classes. The medium of this bias is imbalanced gradients, which include\nnot only the ratio of scale between positive and negative gradients but also\nimbalanced gradients from different negative classes. Therefore, we propose the\nGradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on\naccumulated gradients to balance the optimization process. Additionally, We\nfind that most of the solutions to long-tailed problems are still biased\ntowards head classes in the end, and we propose a simple and post hoc\nprediction re-balancing strategy to further mitigate the basis toward head\nclass. Extensive experiments are conducted on multiple popular long-tailed\nrecognition benchmark datasets to evaluate the effectiveness of these two\ndesigns. Our approach achieves top-1 accuracy of 48.5\\%, 41.4\\%, and 73.3\\% on\nCIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art\nmethod GCL by a significant margin of 3.62\\%, 0.76\\% and 1.2\\%, respectively.\nCode is available at https://github.com/lt-project-repository/lt-project.\n", "link": "http://arxiv.org/abs/2403.09036v1", "date": "2024-03-14", "relevancy": 2.6176, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4741}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gradient-Aware%20Logit%20Adjustment%20Loss%20for%20Long-tailed%20Classifier&body=Title%3A%20Gradient-Aware%20Logit%20Adjustment%20Loss%20for%20Long-tailed%20Classifier%0AAuthor%3A%20Fan%20Zhang%20and%20Wei%20Qin%20and%20Weijieying%20Ren%20and%20Lei%20Wang%20and%20Zetong%20Chen%20and%20Richang%20Hong%0AAbstract%3A%20%20%20In%20the%20real-world%20setting%2C%20data%20often%20follows%20a%20long-tailed%20distribution%2C%0Awhere%20head%20classes%20contain%20significantly%20more%20training%20samples%20than%20tail%0Aclasses.%20Consequently%2C%20models%20trained%20on%20such%20data%20tend%20to%20be%20biased%20toward%0Ahead%20classes.%20The%20medium%20of%20this%20bias%20is%20imbalanced%20gradients%2C%20which%20include%0Anot%20only%20the%20ratio%20of%20scale%20between%20positive%20and%20negative%20gradients%20but%20also%0Aimbalanced%20gradients%20from%20different%20negative%20classes.%20Therefore%2C%20we%20propose%20the%0AGradient-Aware%20Logit%20Adjustment%20%28GALA%29%20loss%2C%20which%20adjusts%20the%20logits%20based%20on%0Aaccumulated%20gradients%20to%20balance%20the%20optimization%20process.%20Additionally%2C%20We%0Afind%20that%20most%20of%20the%20solutions%20to%20long-tailed%20problems%20are%20still%20biased%0Atowards%20head%20classes%20in%20the%20end%2C%20and%20we%20propose%20a%20simple%20and%20post%20hoc%0Aprediction%20re-balancing%20strategy%20to%20further%20mitigate%20the%20basis%20toward%20head%0Aclass.%20Extensive%20experiments%20are%20conducted%20on%20multiple%20popular%20long-tailed%0Arecognition%20benchmark%20datasets%20to%20evaluate%20the%20effectiveness%20of%20these%20two%0Adesigns.%20Our%20approach%20achieves%20top-1%20accuracy%20of%2048.5%5C%25%2C%2041.4%5C%25%2C%20and%2073.3%5C%25%20on%0ACIFAR100-LT%2C%20Places-LT%2C%20and%20iNaturalist%2C%20outperforming%20the%20state-of-the-art%0Amethod%20GCL%20by%20a%20significant%20margin%20of%203.62%5C%25%2C%200.76%5C%25%20and%201.2%5C%25%2C%20respectively.%0ACode%20is%20available%20at%20https%3A//github.com/lt-project-repository/lt-project.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09036v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Aware%20Logit%20Adjustment%20Loss%20for%20Long-tailed%20Classifier&entry.906535625=Fan%20Zhang%20and%20Wei%20Qin%20and%20Weijieying%20Ren%20and%20Lei%20Wang%20and%20Zetong%20Chen%20and%20Richang%20Hong&entry.1292438233=%20%20In%20the%20real-world%20setting%2C%20data%20often%20follows%20a%20long-tailed%20distribution%2C%0Awhere%20head%20classes%20contain%20significantly%20more%20training%20samples%20than%20tail%0Aclasses.%20Consequently%2C%20models%20trained%20on%20such%20data%20tend%20to%20be%20biased%20toward%0Ahead%20classes.%20The%20medium%20of%20this%20bias%20is%20imbalanced%20gradients%2C%20which%20include%0Anot%20only%20the%20ratio%20of%20scale%20between%20positive%20and%20negative%20gradients%20but%20also%0Aimbalanced%20gradients%20from%20different%20negative%20classes.%20Therefore%2C%20we%20propose%20the%0AGradient-Aware%20Logit%20Adjustment%20%28GALA%29%20loss%2C%20which%20adjusts%20the%20logits%20based%20on%0Aaccumulated%20gradients%20to%20balance%20the%20optimization%20process.%20Additionally%2C%20We%0Afind%20that%20most%20of%20the%20solutions%20to%20long-tailed%20problems%20are%20still%20biased%0Atowards%20head%20classes%20in%20the%20end%2C%20and%20we%20propose%20a%20simple%20and%20post%20hoc%0Aprediction%20re-balancing%20strategy%20to%20further%20mitigate%20the%20basis%20toward%20head%0Aclass.%20Extensive%20experiments%20are%20conducted%20on%20multiple%20popular%20long-tailed%0Arecognition%20benchmark%20datasets%20to%20evaluate%20the%20effectiveness%20of%20these%20two%0Adesigns.%20Our%20approach%20achieves%20top-1%20accuracy%20of%2048.5%5C%25%2C%2041.4%5C%25%2C%20and%2073.3%5C%25%20on%0ACIFAR100-LT%2C%20Places-LT%2C%20and%20iNaturalist%2C%20outperforming%20the%20state-of-the-art%0Amethod%20GCL%20by%20a%20significant%20margin%20of%203.62%5C%25%2C%200.76%5C%25%20and%201.2%5C%25%2C%20respectively.%0ACode%20is%20available%20at%20https%3A//github.com/lt-project-repository/lt-project.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09036v1&entry.124074799=Read"},
{"title": "Taming Cross-Domain Representation Variance in Federated Prototype\n  Learning with Heterogeneous Data Domains", "author": "Lei Wang and Jieming Bian and Letian Zhang and Chen Chen and Jie Xu", "abstract": "  Federated learning (FL) allows collaborative machine learning training\nwithout sharing private data. While most FL methods assume identical data\ndomains across clients, real-world scenarios often involve heterogeneous data\ndomains. Federated Prototype Learning (FedPL) addresses this issue, using mean\nfeature vectors as prototypes to enhance model generalization. However,\nexisting FedPL methods create the same number of prototypes for each client,\nleading to cross-domain performance gaps and disparities for clients with\nvaried data distributions. To mitigate cross-domain feature representation\nvariance, we introduce FedPLVM, which establishes variance-aware dual-level\nprototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The\ndual-level prototypes clustering strategy creates local clustered prototypes\nbased on private data features, then performs global prototypes clustering to\nreduce communication complexity and preserve local data privacy. The\n$\\alpha$-sparsity prototype loss aligns samples from underrepresented domains,\nenhancing intra-class similarity and reducing inter-class similarity.\nEvaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our\nmethod's superiority over existing approaches.\n", "link": "http://arxiv.org/abs/2403.09048v1", "date": "2024-03-14", "relevancy": 2.6159, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5031}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4974}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Taming%20Cross-Domain%20Representation%20Variance%20in%20Federated%20Prototype%0A%20%20Learning%20with%20Heterogeneous%20Data%20Domains&body=Title%3A%20Taming%20Cross-Domain%20Representation%20Variance%20in%20Federated%20Prototype%0A%20%20Learning%20with%20Heterogeneous%20Data%20Domains%0AAuthor%3A%20Lei%20Wang%20and%20Jieming%20Bian%20and%20Letian%20Zhang%20and%20Chen%20Chen%20and%20Jie%20Xu%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20allows%20collaborative%20machine%20learning%20training%0Awithout%20sharing%20private%20data.%20While%20most%20FL%20methods%20assume%20identical%20data%0Adomains%20across%20clients%2C%20real-world%20scenarios%20often%20involve%20heterogeneous%20data%0Adomains.%20Federated%20Prototype%20Learning%20%28FedPL%29%20addresses%20this%20issue%2C%20using%20mean%0Afeature%20vectors%20as%20prototypes%20to%20enhance%20model%20generalization.%20However%2C%0Aexisting%20FedPL%20methods%20create%20the%20same%20number%20of%20prototypes%20for%20each%20client%2C%0Aleading%20to%20cross-domain%20performance%20gaps%20and%20disparities%20for%20clients%20with%0Avaried%20data%20distributions.%20To%20mitigate%20cross-domain%20feature%20representation%0Avariance%2C%20we%20introduce%20FedPLVM%2C%20which%20establishes%20variance-aware%20dual-level%0Aprototypes%20clustering%20and%20employs%20a%20novel%20%24%5Calpha%24-sparsity%20prototype%20loss.%20The%0Adual-level%20prototypes%20clustering%20strategy%20creates%20local%20clustered%20prototypes%0Abased%20on%20private%20data%20features%2C%20then%20performs%20global%20prototypes%20clustering%20to%0Areduce%20communication%20complexity%20and%20preserve%20local%20data%20privacy.%20The%0A%24%5Calpha%24-sparsity%20prototype%20loss%20aligns%20samples%20from%20underrepresented%20domains%2C%0Aenhancing%20intra-class%20similarity%20and%20reducing%20inter-class%20similarity.%0AEvaluations%20on%20Digit-5%2C%20Office-10%2C%20and%20DomainNet%20datasets%20demonstrate%20our%0Amethod%27s%20superiority%20over%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09048v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Cross-Domain%20Representation%20Variance%20in%20Federated%20Prototype%0A%20%20Learning%20with%20Heterogeneous%20Data%20Domains&entry.906535625=Lei%20Wang%20and%20Jieming%20Bian%20and%20Letian%20Zhang%20and%20Chen%20Chen%20and%20Jie%20Xu&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20allows%20collaborative%20machine%20learning%20training%0Awithout%20sharing%20private%20data.%20While%20most%20FL%20methods%20assume%20identical%20data%0Adomains%20across%20clients%2C%20real-world%20scenarios%20often%20involve%20heterogeneous%20data%0Adomains.%20Federated%20Prototype%20Learning%20%28FedPL%29%20addresses%20this%20issue%2C%20using%20mean%0Afeature%20vectors%20as%20prototypes%20to%20enhance%20model%20generalization.%20However%2C%0Aexisting%20FedPL%20methods%20create%20the%20same%20number%20of%20prototypes%20for%20each%20client%2C%0Aleading%20to%20cross-domain%20performance%20gaps%20and%20disparities%20for%20clients%20with%0Avaried%20data%20distributions.%20To%20mitigate%20cross-domain%20feature%20representation%0Avariance%2C%20we%20introduce%20FedPLVM%2C%20which%20establishes%20variance-aware%20dual-level%0Aprototypes%20clustering%20and%20employs%20a%20novel%20%24%5Calpha%24-sparsity%20prototype%20loss.%20The%0Adual-level%20prototypes%20clustering%20strategy%20creates%20local%20clustered%20prototypes%0Abased%20on%20private%20data%20features%2C%20then%20performs%20global%20prototypes%20clustering%20to%0Areduce%20communication%20complexity%20and%20preserve%20local%20data%20privacy.%20The%0A%24%5Calpha%24-sparsity%20prototype%20loss%20aligns%20samples%20from%20underrepresented%20domains%2C%0Aenhancing%20intra-class%20similarity%20and%20reducing%20inter-class%20similarity.%0AEvaluations%20on%20Digit-5%2C%20Office-10%2C%20and%20DomainNet%20datasets%20demonstrate%20our%0Amethod%27s%20superiority%20over%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09048v1&entry.124074799=Read"},
{"title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph", "author": "Ruotong Liao and Xu Jia and Yunpu Ma and Yangzhe Li and Volker Tresp", "abstract": "  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n", "link": "http://arxiv.org/abs/2310.07793v4", "date": "2024-03-13", "relevancy": 2.611, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5529}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5222}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4914}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&body=Title%3A%20GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph%0AAuthor%3A%20Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%20Code%20and%20data%20are%20released%20here%3A%0Ahttps%3A//github.com/mayhugotong/GenTKG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07793v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenTKG%3A%20Generative%20Forecasting%20on%20Temporal%20Knowledge%20Graph&entry.906535625=Ruotong%20Liao%20and%20Xu%20Jia%20and%20Yunpu%20Ma%20and%20Yangzhe%20Li%20and%20Volker%20Tresp&entry.1292438233=%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20ignited%20interest%0Ain%20the%20temporal%20knowledge%20graph%20%28tKG%29%20domain%2C%20where%20conventional%0Aembedding-based%20and%20rule-based%20methods%20dominate.%20The%20question%20remains%20open%20of%0Awhether%20pre-trained%20LLMs%20can%20understand%20structured%20temporal%20relational%20data%20and%0Areplace%20them%20as%20the%20foundation%20model%20for%20temporal%20relational%20forecasting.%0ATherefore%2C%20we%20bring%20temporal%20knowledge%20forecasting%20into%20the%20generative%20setting.%0AHowever%2C%20challenges%20occur%20in%20the%20huge%20chasms%20between%20complex%20temporal%20graph%0Adata%20structure%20and%20sequential%20natural%20expressions%20LLMs%20can%20handle%2C%20and%20between%0Athe%20enormous%20data%20sizes%20of%20tKGs%20and%20heavy%20computation%20costs%20of%20finetuning%20LLMs.%0ATo%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20retrieval-augmented%20generation%0Aframework%20named%20GenTKG%20combining%20a%20temporal%20logical%20rule-based%20retrieval%0Astrategy%20and%20few-shot%20parameter-efficient%20instruction%20tuning%20to%20solve%20the%20above%0Achallenges%2C%20respectively.%20Extensive%20experiments%20have%20shown%20that%20GenTKG%0Aoutperforms%20conventional%20methods%20of%20temporal%20relational%20forecasting%20with%20low%0Acomputation%20resources%20using%20extremely%20limited%20training%20data%20as%20few%20as%2016%0Asamples.%20GenTKG%20also%20highlights%20remarkable%20cross-domain%20generalizability%20with%0Aoutperforming%20performance%20on%20unseen%20datasets%20without%20re-training%2C%20and%20in-domain%0Ageneralizability%20regardless%20of%20time%20split%20in%20the%20same%20dataset.%20Our%20work%20reveals%0Athe%20huge%20potential%20of%20LLMs%20in%20the%20tKG%20domain%20and%20opens%20a%20new%20frontier%20for%0Agenerative%20forecasting%20on%20tKGs.%20Code%20and%20data%20are%20released%20here%3A%0Ahttps%3A//github.com/mayhugotong/GenTKG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07793v4&entry.124074799=Read"},
{"title": "DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers", "author": "Xiao Ma and Shengfeng He and Hezhe Qiao and Dong Ma", "abstract": "  Enabling efficient and accurate deep neural network (DNN) inference on\nmicrocontrollers is non-trivial due to the constrained on-chip resources.\nCurrent methodologies primarily focus on compressing larger models yet at the\nexpense of model accuracy. In this paper, we rethink the problem from the\ninverse perspective by constructing small/weak models directly and improving\ntheir accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference\nframework with a selector-classifiers architecture, where the selector routes\neach input sample to the appropriate classifier for classification. DiTMoS is\ngrounded on a key insight: a composition of weak models can exhibit high\ndiversity and the union of them can significantly boost the accuracy upper\nbound. To approach the upper bound, DiTMoS introduces three strategies\nincluding diverse training data splitting to increase the classifiers'\ndiversity, adversarial selector-classifiers training to ensure synergistic\ninteractions thereby maximizing their complementarity, and heterogeneous\nfeature aggregation to improve the capacity of classifiers. We further propose\na network slicing technique to alleviate the extra memory overhead incurred by\nfeature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and\nevaluate it based on three time-series datasets for human activity recognition,\nkeywords spotting, and emotion recognition, respectively. The experiment\nresults manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement\ncompared to the best baseline; (b) network slicing almost completely eliminates\nthe memory overhead incurred by feature aggregation with a marginal increase of\nlatency.\n", "link": "http://arxiv.org/abs/2403.09035v1", "date": "2024-03-14", "relevancy": 2.61, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5113}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiTMoS%3A%20Delving%20into%20Diverse%20Tiny-Model%20Selection%20on%20Microcontrollers&body=Title%3A%20DiTMoS%3A%20Delving%20into%20Diverse%20Tiny-Model%20Selection%20on%20Microcontrollers%0AAuthor%3A%20Xiao%20Ma%20and%20Shengfeng%20He%20and%20Hezhe%20Qiao%20and%20Dong%20Ma%0AAbstract%3A%20%20%20Enabling%20efficient%20and%20accurate%20deep%20neural%20network%20%28DNN%29%20inference%20on%0Amicrocontrollers%20is%20non-trivial%20due%20to%20the%20constrained%20on-chip%20resources.%0ACurrent%20methodologies%20primarily%20focus%20on%20compressing%20larger%20models%20yet%20at%20the%0Aexpense%20of%20model%20accuracy.%20In%20this%20paper%2C%20we%20rethink%20the%20problem%20from%20the%0Ainverse%20perspective%20by%20constructing%20small/weak%20models%20directly%20and%20improving%0Atheir%20accuracy.%20Thus%2C%20we%20introduce%20DiTMoS%2C%20a%20novel%20DNN%20training%20and%20inference%0Aframework%20with%20a%20selector-classifiers%20architecture%2C%20where%20the%20selector%20routes%0Aeach%20input%20sample%20to%20the%20appropriate%20classifier%20for%20classification.%20DiTMoS%20is%0Agrounded%20on%20a%20key%20insight%3A%20a%20composition%20of%20weak%20models%20can%20exhibit%20high%0Adiversity%20and%20the%20union%20of%20them%20can%20significantly%20boost%20the%20accuracy%20upper%0Abound.%20To%20approach%20the%20upper%20bound%2C%20DiTMoS%20introduces%20three%20strategies%0Aincluding%20diverse%20training%20data%20splitting%20to%20increase%20the%20classifiers%27%0Adiversity%2C%20adversarial%20selector-classifiers%20training%20to%20ensure%20synergistic%0Ainteractions%20thereby%20maximizing%20their%20complementarity%2C%20and%20heterogeneous%0Afeature%20aggregation%20to%20improve%20the%20capacity%20of%20classifiers.%20We%20further%20propose%0Aa%20network%20slicing%20technique%20to%20alleviate%20the%20extra%20memory%20overhead%20incurred%20by%0Afeature%20aggregation.%20We%20deploy%20DiTMoS%20on%20the%20Neucleo%20STM32F767ZI%20board%20and%0Aevaluate%20it%20based%20on%20three%20time-series%20datasets%20for%20human%20activity%20recognition%2C%0Akeywords%20spotting%2C%20and%20emotion%20recognition%2C%20respectively.%20The%20experiment%0Aresults%20manifest%20that%3A%20%28a%29%20DiTMoS%20achieves%20up%20to%2013.4%25%20accuracy%20improvement%0Acompared%20to%20the%20best%20baseline%3B%20%28b%29%20network%20slicing%20almost%20completely%20eliminates%0Athe%20memory%20overhead%20incurred%20by%20feature%20aggregation%20with%20a%20marginal%20increase%20of%0Alatency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09035v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiTMoS%3A%20Delving%20into%20Diverse%20Tiny-Model%20Selection%20on%20Microcontrollers&entry.906535625=Xiao%20Ma%20and%20Shengfeng%20He%20and%20Hezhe%20Qiao%20and%20Dong%20Ma&entry.1292438233=%20%20Enabling%20efficient%20and%20accurate%20deep%20neural%20network%20%28DNN%29%20inference%20on%0Amicrocontrollers%20is%20non-trivial%20due%20to%20the%20constrained%20on-chip%20resources.%0ACurrent%20methodologies%20primarily%20focus%20on%20compressing%20larger%20models%20yet%20at%20the%0Aexpense%20of%20model%20accuracy.%20In%20this%20paper%2C%20we%20rethink%20the%20problem%20from%20the%0Ainverse%20perspective%20by%20constructing%20small/weak%20models%20directly%20and%20improving%0Atheir%20accuracy.%20Thus%2C%20we%20introduce%20DiTMoS%2C%20a%20novel%20DNN%20training%20and%20inference%0Aframework%20with%20a%20selector-classifiers%20architecture%2C%20where%20the%20selector%20routes%0Aeach%20input%20sample%20to%20the%20appropriate%20classifier%20for%20classification.%20DiTMoS%20is%0Agrounded%20on%20a%20key%20insight%3A%20a%20composition%20of%20weak%20models%20can%20exhibit%20high%0Adiversity%20and%20the%20union%20of%20them%20can%20significantly%20boost%20the%20accuracy%20upper%0Abound.%20To%20approach%20the%20upper%20bound%2C%20DiTMoS%20introduces%20three%20strategies%0Aincluding%20diverse%20training%20data%20splitting%20to%20increase%20the%20classifiers%27%0Adiversity%2C%20adversarial%20selector-classifiers%20training%20to%20ensure%20synergistic%0Ainteractions%20thereby%20maximizing%20their%20complementarity%2C%20and%20heterogeneous%0Afeature%20aggregation%20to%20improve%20the%20capacity%20of%20classifiers.%20We%20further%20propose%0Aa%20network%20slicing%20technique%20to%20alleviate%20the%20extra%20memory%20overhead%20incurred%20by%0Afeature%20aggregation.%20We%20deploy%20DiTMoS%20on%20the%20Neucleo%20STM32F767ZI%20board%20and%0Aevaluate%20it%20based%20on%20three%20time-series%20datasets%20for%20human%20activity%20recognition%2C%0Akeywords%20spotting%2C%20and%20emotion%20recognition%2C%20respectively.%20The%20experiment%0Aresults%20manifest%20that%3A%20%28a%29%20DiTMoS%20achieves%20up%20to%2013.4%25%20accuracy%20improvement%0Acompared%20to%20the%20best%20baseline%3B%20%28b%29%20network%20slicing%20almost%20completely%20eliminates%0Athe%20memory%20overhead%20incurred%20by%20feature%20aggregation%20with%20a%20marginal%20increase%20of%0Alatency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09035v1&entry.124074799=Read"},
{"title": "Adversarial Training with OCR Modality Perturbation for Scene-Text\n  Visual Question Answering", "author": "Zhixuan Shen and Haonan Luo and Sijia Li and Tianrui Li", "abstract": "  Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.\n", "link": "http://arxiv.org/abs/2403.09288v1", "date": "2024-03-14", "relevancy": 2.6011, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5007}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5002}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering&body=Title%3A%20Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Zhixuan%20Shen%20and%20Haonan%20Luo%20and%20Sijia%20Li%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Scene-Text%20Visual%20Question%20Answering%20%28ST-VQA%29%20aims%20to%20understand%20scene%20text%0Ain%20images%20and%20answer%20questions%20related%20to%20the%20text%20content.%20Most%20existing%0Amethods%20heavily%20rely%20on%20the%20accuracy%20of%20Optical%20Character%20Recognition%20%28OCR%29%0Asystems%2C%20and%20aggressive%20fine-tuning%20based%20on%20limited%20spatial%20location%0Ainformation%20and%20erroneous%20OCR%20text%20information%20often%20leads%20to%20inevitable%0Aoverfitting.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20adversarial%20training%0Aarchitecture%20with%20spatial%20awareness%20capabilities.%20Specifically%2C%20we%20introduce%20an%0AAdversarial%20OCR%20Enhancement%20%28AOE%29%20module%2C%20which%20leverages%20adversarial%20training%0Ain%20the%20embedding%20space%20of%20OCR%20modality%20to%20enhance%20fault-tolerant%20representation%0Aof%20OCR%20texts%2C%20thereby%20reducing%20noise%20caused%20by%20OCR%20errors.%20Simultaneously%2C%20We%0Aadd%20a%20Spatial-Aware%20Self-Attention%20%28SASA%29%20mechanism%20to%20help%20the%20model%20better%0Acapture%20the%20spatial%20relationships%20among%20OCR%20tokens.%20Various%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20significant%20performance%20improvements%20on%0Aboth%20the%20ST-VQA%20and%20TextVQA%20datasets%20and%20provides%20a%20novel%20paradigm%20for%0Amultimodal%20adversarial%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09288v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Training%20with%20OCR%20Modality%20Perturbation%20for%20Scene-Text%0A%20%20Visual%20Question%20Answering&entry.906535625=Zhixuan%20Shen%20and%20Haonan%20Luo%20and%20Sijia%20Li%20and%20Tianrui%20Li&entry.1292438233=%20%20Scene-Text%20Visual%20Question%20Answering%20%28ST-VQA%29%20aims%20to%20understand%20scene%20text%0Ain%20images%20and%20answer%20questions%20related%20to%20the%20text%20content.%20Most%20existing%0Amethods%20heavily%20rely%20on%20the%20accuracy%20of%20Optical%20Character%20Recognition%20%28OCR%29%0Asystems%2C%20and%20aggressive%20fine-tuning%20based%20on%20limited%20spatial%20location%0Ainformation%20and%20erroneous%20OCR%20text%20information%20often%20leads%20to%20inevitable%0Aoverfitting.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20adversarial%20training%0Aarchitecture%20with%20spatial%20awareness%20capabilities.%20Specifically%2C%20we%20introduce%20an%0AAdversarial%20OCR%20Enhancement%20%28AOE%29%20module%2C%20which%20leverages%20adversarial%20training%0Ain%20the%20embedding%20space%20of%20OCR%20modality%20to%20enhance%20fault-tolerant%20representation%0Aof%20OCR%20texts%2C%20thereby%20reducing%20noise%20caused%20by%20OCR%20errors.%20Simultaneously%2C%20We%0Aadd%20a%20Spatial-Aware%20Self-Attention%20%28SASA%29%20mechanism%20to%20help%20the%20model%20better%0Acapture%20the%20spatial%20relationships%20among%20OCR%20tokens.%20Various%20experiments%0Ademonstrate%20that%20our%20method%20achieves%20significant%20performance%20improvements%20on%0Aboth%20the%20ST-VQA%20and%20TextVQA%20datasets%20and%20provides%20a%20novel%20paradigm%20for%0Amultimodal%20adversarial%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09288v1&entry.124074799=Read"},
{"title": "Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly\n  Detection in Dynamic Graphs", "author": "Jie Liu and Xuequn Shang and Xiaolin Han and Wentao Zhang and Hongzhi Yin", "abstract": "  Anomaly detection in dynamic graphs presents a significant challenge due to\nthe temporal evolution of graph structures and attributes. The conventional\napproaches that tackle this problem typically employ an unsupervised learning\nframework, capturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing. However, these\nmethods face critical drawbacks: they either only depend on proxy tasks for\ngeneral representation without directly pinpointing normal patterns, or they\nneglect to differentiate between spatial and temporal normality patterns,\nleading to diminished efficacy in anomaly detection. To address these\nchallenges, we introduce a novel Spatial-Temporal memories-enhanced graph\nautoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)\nand gated temporal convolution layers to extract spatial features and temporal\nfeatures, respectively. Then STRIPE incorporates separate spatial and temporal\nmemory networks, which capture and store prototypes of normal patterns, thereby\npreserving the uniqueness of spatial and temporal normality. After that,\nthrough a mutual attention mechanism, these stored patterns are then retrieved\nand integrated with encoded graph embeddings. Finally, the integrated features\nare fed into the decoder to reconstruct the graph streams which serve as the\nproxy task for anomaly detection. This comprehensive approach not only\nminimizes reconstruction errors but also refines the model by emphasizing the\ncompactness and distinctiveness of the embeddings in relation to the nearest\nmemory prototypes. Through extensive testing, STRIPE has demonstrated a\nsuperior capability to discern anomalies by effectively leveraging the distinct\nspatial and temporal dynamics of dynamic graphs, significantly outperforming\nexisting methodologies, with an average improvement of 15.39% on AUC values.\n", "link": "http://arxiv.org/abs/2403.09039v1", "date": "2024-03-14", "relevancy": 2.5974, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5284}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5138}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spatial-temporal%20Memories%20Enhanced%20Graph%20Autoencoder%20for%20Anomaly%0A%20%20Detection%20in%20Dynamic%20Graphs&body=Title%3A%20Spatial-temporal%20Memories%20Enhanced%20Graph%20Autoencoder%20for%20Anomaly%0A%20%20Detection%20in%20Dynamic%20Graphs%0AAuthor%3A%20Jie%20Liu%20and%20Xuequn%20Shang%20and%20Xiaolin%20Han%20and%20Wentao%20Zhang%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20dynamic%20graphs%20presents%20a%20significant%20challenge%20due%20to%0Athe%20temporal%20evolution%20of%20graph%20structures%20and%20attributes.%20The%20conventional%0Aapproaches%20that%20tackle%20this%20problem%20typically%20employ%20an%20unsupervised%20learning%0Aframework%2C%20capturing%20normality%20patterns%20with%20exclusive%20normal%20data%20during%0Atraining%20and%20identifying%20deviations%20as%20anomalies%20during%20testing.%20However%2C%20these%0Amethods%20face%20critical%20drawbacks%3A%20they%20either%20only%20depend%20on%20proxy%20tasks%20for%0Ageneral%20representation%20without%20directly%20pinpointing%20normal%20patterns%2C%20or%20they%0Aneglect%20to%20differentiate%20between%20spatial%20and%20temporal%20normality%20patterns%2C%0Aleading%20to%20diminished%20efficacy%20in%20anomaly%20detection.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20Spatial-Temporal%20memories-enhanced%20graph%0Aautoencoder%20%28STRIPE%29.%20Initially%2C%20STRIPE%20employs%20Graph%20Neural%20Networks%20%28GNNs%29%0Aand%20gated%20temporal%20convolution%20layers%20to%20extract%20spatial%20features%20and%20temporal%0Afeatures%2C%20respectively.%20Then%20STRIPE%20incorporates%20separate%20spatial%20and%20temporal%0Amemory%20networks%2C%20which%20capture%20and%20store%20prototypes%20of%20normal%20patterns%2C%20thereby%0Apreserving%20the%20uniqueness%20of%20spatial%20and%20temporal%20normality.%20After%20that%2C%0Athrough%20a%20mutual%20attention%20mechanism%2C%20these%20stored%20patterns%20are%20then%20retrieved%0Aand%20integrated%20with%20encoded%20graph%20embeddings.%20Finally%2C%20the%20integrated%20features%0Aare%20fed%20into%20the%20decoder%20to%20reconstruct%20the%20graph%20streams%20which%20serve%20as%20the%0Aproxy%20task%20for%20anomaly%20detection.%20This%20comprehensive%20approach%20not%20only%0Aminimizes%20reconstruction%20errors%20but%20also%20refines%20the%20model%20by%20emphasizing%20the%0Acompactness%20and%20distinctiveness%20of%20the%20embeddings%20in%20relation%20to%20the%20nearest%0Amemory%20prototypes.%20Through%20extensive%20testing%2C%20STRIPE%20has%20demonstrated%20a%0Asuperior%20capability%20to%20discern%20anomalies%20by%20effectively%20leveraging%20the%20distinct%0Aspatial%20and%20temporal%20dynamics%20of%20dynamic%20graphs%2C%20significantly%20outperforming%0Aexisting%20methodologies%2C%20with%20an%20average%20improvement%20of%2015.39%25%20on%20AUC%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09039v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-temporal%20Memories%20Enhanced%20Graph%20Autoencoder%20for%20Anomaly%0A%20%20Detection%20in%20Dynamic%20Graphs&entry.906535625=Jie%20Liu%20and%20Xuequn%20Shang%20and%20Xiaolin%20Han%20and%20Wentao%20Zhang%20and%20Hongzhi%20Yin&entry.1292438233=%20%20Anomaly%20detection%20in%20dynamic%20graphs%20presents%20a%20significant%20challenge%20due%20to%0Athe%20temporal%20evolution%20of%20graph%20structures%20and%20attributes.%20The%20conventional%0Aapproaches%20that%20tackle%20this%20problem%20typically%20employ%20an%20unsupervised%20learning%0Aframework%2C%20capturing%20normality%20patterns%20with%20exclusive%20normal%20data%20during%0Atraining%20and%20identifying%20deviations%20as%20anomalies%20during%20testing.%20However%2C%20these%0Amethods%20face%20critical%20drawbacks%3A%20they%20either%20only%20depend%20on%20proxy%20tasks%20for%0Ageneral%20representation%20without%20directly%20pinpointing%20normal%20patterns%2C%20or%20they%0Aneglect%20to%20differentiate%20between%20spatial%20and%20temporal%20normality%20patterns%2C%0Aleading%20to%20diminished%20efficacy%20in%20anomaly%20detection.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20Spatial-Temporal%20memories-enhanced%20graph%0Aautoencoder%20%28STRIPE%29.%20Initially%2C%20STRIPE%20employs%20Graph%20Neural%20Networks%20%28GNNs%29%0Aand%20gated%20temporal%20convolution%20layers%20to%20extract%20spatial%20features%20and%20temporal%0Afeatures%2C%20respectively.%20Then%20STRIPE%20incorporates%20separate%20spatial%20and%20temporal%0Amemory%20networks%2C%20which%20capture%20and%20store%20prototypes%20of%20normal%20patterns%2C%20thereby%0Apreserving%20the%20uniqueness%20of%20spatial%20and%20temporal%20normality.%20After%20that%2C%0Athrough%20a%20mutual%20attention%20mechanism%2C%20these%20stored%20patterns%20are%20then%20retrieved%0Aand%20integrated%20with%20encoded%20graph%20embeddings.%20Finally%2C%20the%20integrated%20features%0Aare%20fed%20into%20the%20decoder%20to%20reconstruct%20the%20graph%20streams%20which%20serve%20as%20the%0Aproxy%20task%20for%20anomaly%20detection.%20This%20comprehensive%20approach%20not%20only%0Aminimizes%20reconstruction%20errors%20but%20also%20refines%20the%20model%20by%20emphasizing%20the%0Acompactness%20and%20distinctiveness%20of%20the%20embeddings%20in%20relation%20to%20the%20nearest%0Amemory%20prototypes.%20Through%20extensive%20testing%2C%20STRIPE%20has%20demonstrated%20a%0Asuperior%20capability%20to%20discern%20anomalies%20by%20effectively%20leveraging%20the%20distinct%0Aspatial%20and%20temporal%20dynamics%20of%20dynamic%20graphs%2C%20significantly%20outperforming%0Aexisting%20methodologies%2C%20with%20an%20average%20improvement%20of%2015.39%25%20on%20AUC%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09039v1&entry.124074799=Read"},
{"title": "Faceptor: A Generalist Model for Face Perception", "author": "Lixiong Qin and Mei Wang and Xuannan Liu and Yuhang Zhang and Wei Deng and Xiaoshuai Song and Weiran Xu and Weihong Deng", "abstract": "  With the comprehensive research conducted on various face analysis tasks,\nthere is a growing interest among researchers to develop a unified approach to\nface perception. Existing methods mainly discuss unified representation and\ntraining, which lack task extensibility and application efficiency. To tackle\nthis issue, we focus on the unified model structure, exploring a face\ngeneralist model. As an intuitive design, Naive Faceptor enables tasks with the\nsame output shape and granularity to share the structural design of the\nstandardized output head, achieving improved task extensibility. Furthermore,\nFaceptor is proposed to adopt a well-designed single-encoder dual-decoder\narchitecture, allowing task-specific queries to represent new-coming semantics.\nThis design enhances the unification of model structure while improving\napplication efficiency in terms of storage overhead. Additionally, we introduce\nLayer-Attention into Faceptor, enabling the model to adaptively select features\nfrom optimal layers to perform the desired tasks. Through joint training on 13\nface perception datasets, Faceptor achieves exceptional performance in facial\nlandmark localization, face parsing, age estimation, expression recognition,\nbinary attribute classification, and face recognition, achieving or surpassing\nspecialized methods in most tasks. Our training framework can also be applied\nto auxiliary supervised learning, significantly improving performance in\ndata-sparse tasks such as age estimation and expression recognition. The code\nand models will be made publicly available at\nhttps://github.com/lxq1000/Faceptor.\n", "link": "http://arxiv.org/abs/2403.09500v1", "date": "2024-03-14", "relevancy": 2.5818, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5186}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception&body=Title%3A%20Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception%0AAuthor%3A%20Lixiong%20Qin%20and%20Mei%20Wang%20and%20Xuannan%20Liu%20and%20Yuhang%20Zhang%20and%20Wei%20Deng%20and%20Xiaoshuai%20Song%20and%20Weiran%20Xu%20and%20Weihong%20Deng%0AAbstract%3A%20%20%20With%20the%20comprehensive%20research%20conducted%20on%20various%20face%20analysis%20tasks%2C%0Athere%20is%20a%20growing%20interest%20among%20researchers%20to%20develop%20a%20unified%20approach%20to%0Aface%20perception.%20Existing%20methods%20mainly%20discuss%20unified%20representation%20and%0Atraining%2C%20which%20lack%20task%20extensibility%20and%20application%20efficiency.%20To%20tackle%0Athis%20issue%2C%20we%20focus%20on%20the%20unified%20model%20structure%2C%20exploring%20a%20face%0Ageneralist%20model.%20As%20an%20intuitive%20design%2C%20Naive%20Faceptor%20enables%20tasks%20with%20the%0Asame%20output%20shape%20and%20granularity%20to%20share%20the%20structural%20design%20of%20the%0Astandardized%20output%20head%2C%20achieving%20improved%20task%20extensibility.%20Furthermore%2C%0AFaceptor%20is%20proposed%20to%20adopt%20a%20well-designed%20single-encoder%20dual-decoder%0Aarchitecture%2C%20allowing%20task-specific%20queries%20to%20represent%20new-coming%20semantics.%0AThis%20design%20enhances%20the%20unification%20of%20model%20structure%20while%20improving%0Aapplication%20efficiency%20in%20terms%20of%20storage%20overhead.%20Additionally%2C%20we%20introduce%0ALayer-Attention%20into%20Faceptor%2C%20enabling%20the%20model%20to%20adaptively%20select%20features%0Afrom%20optimal%20layers%20to%20perform%20the%20desired%20tasks.%20Through%20joint%20training%20on%2013%0Aface%20perception%20datasets%2C%20Faceptor%20achieves%20exceptional%20performance%20in%20facial%0Alandmark%20localization%2C%20face%20parsing%2C%20age%20estimation%2C%20expression%20recognition%2C%0Abinary%20attribute%20classification%2C%20and%20face%20recognition%2C%20achieving%20or%20surpassing%0Aspecialized%20methods%20in%20most%20tasks.%20Our%20training%20framework%20can%20also%20be%20applied%0Ato%20auxiliary%20supervised%20learning%2C%20significantly%20improving%20performance%20in%0Adata-sparse%20tasks%20such%20as%20age%20estimation%20and%20expression%20recognition.%20The%20code%0Aand%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lxq1000/Faceptor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09500v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faceptor%3A%20A%20Generalist%20Model%20for%20Face%20Perception&entry.906535625=Lixiong%20Qin%20and%20Mei%20Wang%20and%20Xuannan%20Liu%20and%20Yuhang%20Zhang%20and%20Wei%20Deng%20and%20Xiaoshuai%20Song%20and%20Weiran%20Xu%20and%20Weihong%20Deng&entry.1292438233=%20%20With%20the%20comprehensive%20research%20conducted%20on%20various%20face%20analysis%20tasks%2C%0Athere%20is%20a%20growing%20interest%20among%20researchers%20to%20develop%20a%20unified%20approach%20to%0Aface%20perception.%20Existing%20methods%20mainly%20discuss%20unified%20representation%20and%0Atraining%2C%20which%20lack%20task%20extensibility%20and%20application%20efficiency.%20To%20tackle%0Athis%20issue%2C%20we%20focus%20on%20the%20unified%20model%20structure%2C%20exploring%20a%20face%0Ageneralist%20model.%20As%20an%20intuitive%20design%2C%20Naive%20Faceptor%20enables%20tasks%20with%20the%0Asame%20output%20shape%20and%20granularity%20to%20share%20the%20structural%20design%20of%20the%0Astandardized%20output%20head%2C%20achieving%20improved%20task%20extensibility.%20Furthermore%2C%0AFaceptor%20is%20proposed%20to%20adopt%20a%20well-designed%20single-encoder%20dual-decoder%0Aarchitecture%2C%20allowing%20task-specific%20queries%20to%20represent%20new-coming%20semantics.%0AThis%20design%20enhances%20the%20unification%20of%20model%20structure%20while%20improving%0Aapplication%20efficiency%20in%20terms%20of%20storage%20overhead.%20Additionally%2C%20we%20introduce%0ALayer-Attention%20into%20Faceptor%2C%20enabling%20the%20model%20to%20adaptively%20select%20features%0Afrom%20optimal%20layers%20to%20perform%20the%20desired%20tasks.%20Through%20joint%20training%20on%2013%0Aface%20perception%20datasets%2C%20Faceptor%20achieves%20exceptional%20performance%20in%20facial%0Alandmark%20localization%2C%20face%20parsing%2C%20age%20estimation%2C%20expression%20recognition%2C%0Abinary%20attribute%20classification%2C%20and%20face%20recognition%2C%20achieving%20or%20surpassing%0Aspecialized%20methods%20in%20most%20tasks.%20Our%20training%20framework%20can%20also%20be%20applied%0Ato%20auxiliary%20supervised%20learning%2C%20significantly%20improving%20performance%20in%0Adata-sparse%20tasks%20such%20as%20age%20estimation%20and%20expression%20recognition.%20The%20code%0Aand%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/lxq1000/Faceptor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09500v1&entry.124074799=Read"},
{"title": "DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs", "author": "Jiawen Zhu and Huayi Tang and Zhi-Qi Cheng and Jun-Yan He and Bin Luo and Shihao Qiu and Shengming Li and Huchuan Lu", "abstract": "  Existing nighttime unmanned aerial vehicle (UAV) trackers follow an\n\"Enhance-then-Track\" architecture - first using a light enhancer to brighten\nthe nighttime video, then employing a daytime tracker to locate the object.\nThis separate enhancement and tracking fails to build an end-to-end trainable\nvision system. To address this, we propose a novel architecture called Darkness\nClue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by\nefficiently learning to generate darkness clue prompts. Without a separate\nenhancer, DCPT directly encodes anti-dark capabilities into prompts using a\ndarkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing\nand undermining projections for darkness clues. It then injects these learned\nvisual prompts into a daytime tracker with fixed parameters across transformer\nlayers. Moreover, a gated feature aggregation mechanism enables adaptive fusion\nbetween prompts and between prompts and the base model. Extensive experiments\nshow state-of-the-art performance for DCPT on multiple dark scenario\nbenchmarks. The unified end-to-end learning of enhancement and tracking in DCPT\nenables a more trainable system. The darkness clue prompting efficiently\ninjects anti-dark knowledge without extra modules. Code is available at\nhttps://github.com/bearyi26/DCPT.\n", "link": "http://arxiv.org/abs/2309.10491v4", "date": "2024-03-14", "relevancy": 2.5812, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5282}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5278}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4927}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs&body=Title%3A%20DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs%0AAuthor%3A%20Jiawen%20Zhu%20and%20Huayi%20Tang%20and%20Zhi-Qi%20Cheng%20and%20Jun-Yan%20He%20and%20Bin%20Luo%20and%20Shihao%20Qiu%20and%20Shengming%20Li%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Existing%20nighttime%20unmanned%20aerial%20vehicle%20%28UAV%29%20trackers%20follow%20an%0A%22Enhance-then-Track%22%20architecture%20-%20first%20using%20a%20light%20enhancer%20to%20brighten%0Athe%20nighttime%20video%2C%20then%20employing%20a%20daytime%20tracker%20to%20locate%20the%20object.%0AThis%20separate%20enhancement%20and%20tracking%20fails%20to%20build%20an%20end-to-end%20trainable%0Avision%20system.%20To%20address%20this%2C%20we%20propose%20a%20novel%20architecture%20called%20Darkness%0AClue-Prompted%20Tracking%20%28DCPT%29%20that%20achieves%20robust%20UAV%20tracking%20at%20night%20by%0Aefficiently%20learning%20to%20generate%20darkness%20clue%20prompts.%20Without%20a%20separate%0Aenhancer%2C%20DCPT%20directly%20encodes%20anti-dark%20capabilities%20into%20prompts%20using%20a%0Adarkness%20clue%20prompter%20%28DCP%29.%20Specifically%2C%20DCP%20iteratively%20learns%20emphasizing%0Aand%20undermining%20projections%20for%20darkness%20clues.%20It%20then%20injects%20these%20learned%0Avisual%20prompts%20into%20a%20daytime%20tracker%20with%20fixed%20parameters%20across%20transformer%0Alayers.%20Moreover%2C%20a%20gated%20feature%20aggregation%20mechanism%20enables%20adaptive%20fusion%0Abetween%20prompts%20and%20between%20prompts%20and%20the%20base%20model.%20Extensive%20experiments%0Ashow%20state-of-the-art%20performance%20for%20DCPT%20on%20multiple%20dark%20scenario%0Abenchmarks.%20The%20unified%20end-to-end%20learning%20of%20enhancement%20and%20tracking%20in%20DCPT%0Aenables%20a%20more%20trainable%20system.%20The%20darkness%20clue%20prompting%20efficiently%0Ainjects%20anti-dark%20knowledge%20without%20extra%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bearyi26/DCPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10491v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCPT%3A%20Darkness%20Clue-Prompted%20Tracking%20in%20Nighttime%20UAVs&entry.906535625=Jiawen%20Zhu%20and%20Huayi%20Tang%20and%20Zhi-Qi%20Cheng%20and%20Jun-Yan%20He%20and%20Bin%20Luo%20and%20Shihao%20Qiu%20and%20Shengming%20Li%20and%20Huchuan%20Lu&entry.1292438233=%20%20Existing%20nighttime%20unmanned%20aerial%20vehicle%20%28UAV%29%20trackers%20follow%20an%0A%22Enhance-then-Track%22%20architecture%20-%20first%20using%20a%20light%20enhancer%20to%20brighten%0Athe%20nighttime%20video%2C%20then%20employing%20a%20daytime%20tracker%20to%20locate%20the%20object.%0AThis%20separate%20enhancement%20and%20tracking%20fails%20to%20build%20an%20end-to-end%20trainable%0Avision%20system.%20To%20address%20this%2C%20we%20propose%20a%20novel%20architecture%20called%20Darkness%0AClue-Prompted%20Tracking%20%28DCPT%29%20that%20achieves%20robust%20UAV%20tracking%20at%20night%20by%0Aefficiently%20learning%20to%20generate%20darkness%20clue%20prompts.%20Without%20a%20separate%0Aenhancer%2C%20DCPT%20directly%20encodes%20anti-dark%20capabilities%20into%20prompts%20using%20a%0Adarkness%20clue%20prompter%20%28DCP%29.%20Specifically%2C%20DCP%20iteratively%20learns%20emphasizing%0Aand%20undermining%20projections%20for%20darkness%20clues.%20It%20then%20injects%20these%20learned%0Avisual%20prompts%20into%20a%20daytime%20tracker%20with%20fixed%20parameters%20across%20transformer%0Alayers.%20Moreover%2C%20a%20gated%20feature%20aggregation%20mechanism%20enables%20adaptive%20fusion%0Abetween%20prompts%20and%20between%20prompts%20and%20the%20base%20model.%20Extensive%20experiments%0Ashow%20state-of-the-art%20performance%20for%20DCPT%20on%20multiple%20dark%20scenario%0Abenchmarks.%20The%20unified%20end-to-end%20learning%20of%20enhancement%20and%20tracking%20in%20DCPT%0Aenables%20a%20more%20trainable%20system.%20The%20darkness%20clue%20prompting%20efficiently%0Ainjects%20anti-dark%20knowledge%20without%20extra%20modules.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bearyi26/DCPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10491v4&entry.124074799=Read"},
{"title": "HawkI: Homography & Mutual Information Guidance for 3D-free Single Image\n  to Aerial View", "author": "Divya Kothandaraman and Tianyi Zhou and Ming Lin and Dinesh Manocha", "abstract": "  We present HawkI, for synthesizing aerial-view images from text and an\nexemplar image, without any additional multi-view or 3D information for\nfinetuning or at inference. HawkI uses techniques from classical computer\nvision and information theory. It seamlessly blends the visual features from\nthe input image within a pretrained text-to-2Dimage stable diffusion model with\na test-time optimization process for a careful bias-variance trade-off, which\nuses an Inverse Perspective Mapping (IPM) homography transformation to provide\nsubtle cues for aerialview synthesis. At inference, HawkI employs a unique\nmutual information guidance formulation to steer the generated image towards\nfaithfully replicating the semantic details of the input-image, while\nmaintaining a realistic aerial perspective. Mutual information guidance\nmaximizes the semantic consistency between the generated image and the input\nimage, without enforcing pixel-level correspondence between vastly different\nviewpoints. Through extensive qualitative and quantitative comparisons against\ntext + exemplar-image based methods and 3D/ multi-view based novel-view\nsynthesis methods on proposed synthetic and real datasets, we demonstrate that\nour method achieves a significantly better bias-variance trade-off towards\ngenerating high fidelity aerial-view images.Code and data is available at\nhttps://github.com/divyakraman/HawkI2024.\n", "link": "http://arxiv.org/abs/2311.15478v2", "date": "2024-03-13", "relevancy": 2.5752, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5189}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5185}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5077}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HawkI%3A%20Homography%20%26%20Mutual%20Information%20Guidance%20for%203D-free%20Single%20Image%0A%20%20to%20Aerial%20View&body=Title%3A%20HawkI%3A%20Homography%20%26%20Mutual%20Information%20Guidance%20for%203D-free%20Single%20Image%0A%20%20to%20Aerial%20View%0AAuthor%3A%20Divya%20Kothandaraman%20and%20Tianyi%20Zhou%20and%20Ming%20Lin%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20HawkI%2C%20for%20synthesizing%20aerial-view%20images%20from%20text%20and%20an%0Aexemplar%20image%2C%20without%20any%20additional%20multi-view%20or%203D%20information%20for%0Afinetuning%20or%20at%20inference.%20HawkI%20uses%20techniques%20from%20classical%20computer%0Avision%20and%20information%20theory.%20It%20seamlessly%20blends%20the%20visual%20features%20from%0Athe%20input%20image%20within%20a%20pretrained%20text-to-2Dimage%20stable%20diffusion%20model%20with%0Aa%20test-time%20optimization%20process%20for%20a%20careful%20bias-variance%20trade-off%2C%20which%0Auses%20an%20Inverse%20Perspective%20Mapping%20%28IPM%29%20homography%20transformation%20to%20provide%0Asubtle%20cues%20for%20aerialview%20synthesis.%20At%20inference%2C%20HawkI%20employs%20a%20unique%0Amutual%20information%20guidance%20formulation%20to%20steer%20the%20generated%20image%20towards%0Afaithfully%20replicating%20the%20semantic%20details%20of%20the%20input-image%2C%20while%0Amaintaining%20a%20realistic%20aerial%20perspective.%20Mutual%20information%20guidance%0Amaximizes%20the%20semantic%20consistency%20between%20the%20generated%20image%20and%20the%20input%0Aimage%2C%20without%20enforcing%20pixel-level%20correspondence%20between%20vastly%20different%0Aviewpoints.%20Through%20extensive%20qualitative%20and%20quantitative%20comparisons%20against%0Atext%20%2B%20exemplar-image%20based%20methods%20and%203D/%20multi-view%20based%20novel-view%0Asynthesis%20methods%20on%20proposed%20synthetic%20and%20real%20datasets%2C%20we%20demonstrate%20that%0Aour%20method%20achieves%20a%20significantly%20better%20bias-variance%20trade-off%20towards%0Agenerating%20high%20fidelity%20aerial-view%20images.Code%20and%20data%20is%20available%20at%0Ahttps%3A//github.com/divyakraman/HawkI2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15478v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HawkI%3A%20Homography%20%26%20Mutual%20Information%20Guidance%20for%203D-free%20Single%20Image%0A%20%20to%20Aerial%20View&entry.906535625=Divya%20Kothandaraman%20and%20Tianyi%20Zhou%20and%20Ming%20Lin%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20HawkI%2C%20for%20synthesizing%20aerial-view%20images%20from%20text%20and%20an%0Aexemplar%20image%2C%20without%20any%20additional%20multi-view%20or%203D%20information%20for%0Afinetuning%20or%20at%20inference.%20HawkI%20uses%20techniques%20from%20classical%20computer%0Avision%20and%20information%20theory.%20It%20seamlessly%20blends%20the%20visual%20features%20from%0Athe%20input%20image%20within%20a%20pretrained%20text-to-2Dimage%20stable%20diffusion%20model%20with%0Aa%20test-time%20optimization%20process%20for%20a%20careful%20bias-variance%20trade-off%2C%20which%0Auses%20an%20Inverse%20Perspective%20Mapping%20%28IPM%29%20homography%20transformation%20to%20provide%0Asubtle%20cues%20for%20aerialview%20synthesis.%20At%20inference%2C%20HawkI%20employs%20a%20unique%0Amutual%20information%20guidance%20formulation%20to%20steer%20the%20generated%20image%20towards%0Afaithfully%20replicating%20the%20semantic%20details%20of%20the%20input-image%2C%20while%0Amaintaining%20a%20realistic%20aerial%20perspective.%20Mutual%20information%20guidance%0Amaximizes%20the%20semantic%20consistency%20between%20the%20generated%20image%20and%20the%20input%0Aimage%2C%20without%20enforcing%20pixel-level%20correspondence%20between%20vastly%20different%0Aviewpoints.%20Through%20extensive%20qualitative%20and%20quantitative%20comparisons%20against%0Atext%20%2B%20exemplar-image%20based%20methods%20and%203D/%20multi-view%20based%20novel-view%0Asynthesis%20methods%20on%20proposed%20synthetic%20and%20real%20datasets%2C%20we%20demonstrate%20that%0Aour%20method%20achieves%20a%20significantly%20better%20bias-variance%20trade-off%20towards%0Agenerating%20high%20fidelity%20aerial-view%20images.Code%20and%20data%20is%20available%20at%0Ahttps%3A//github.com/divyakraman/HawkI2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15478v2&entry.124074799=Read"},
{"title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "author": "Enric Corona and Andrei Zanfir and Eduard Gabriel Bazavan and Nikos Kolotouros and Thiemo Alldieck and Cristian Sminchisescu", "abstract": "  We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.\n", "link": "http://arxiv.org/abs/2403.08764v1", "date": "2024-03-13", "relevancy": 2.5748, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7071}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6672}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5709}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis&body=Title%3A%20VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis%0AAuthor%3A%20Enric%20Corona%20and%20Andrei%20Zanfir%20and%20Eduard%20Gabriel%20Bazavan%20and%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Cristian%20Sminchisescu%0AAbstract%3A%20%20%20We%20propose%20VLOGGER%2C%20a%20method%20for%20audio-driven%20human%20video%20generation%20from%20a%0Asingle%20input%20image%20of%20a%20person%2C%20which%20builds%20on%20the%20success%20of%20recent%0Agenerative%20diffusion%20models.%20Our%20method%20consists%20of%201%29%20a%20stochastic%0Ahuman-to-3d-motion%20diffusion%20model%2C%20and%202%29%20a%20novel%20diffusion-based%20architecture%0Athat%20augments%20text-to-image%20models%20with%20both%20spatial%20and%20temporal%20controls.%0AThis%20supports%20the%20generation%20of%20high%20quality%20video%20of%20variable%20length%2C%20easily%0Acontrollable%20through%20high-level%20representations%20of%20human%20faces%20and%20bodies.%20In%0Acontrast%20to%20previous%20work%2C%20our%20method%20does%20not%20require%20training%20for%20each%0Aperson%2C%20does%20not%20rely%20on%20face%20detection%20and%20cropping%2C%20generates%20the%20complete%0Aimage%20%28not%20just%20the%20face%20or%20the%20lips%29%2C%20and%20considers%20a%20broad%20spectrum%20of%0Ascenarios%20%28e.g.%20visible%20torso%20or%20diverse%20subject%20identities%29%20that%20are%20critical%0Ato%20correctly%20synthesize%20humans%20who%20communicate.%20We%20also%20curate%20MENTOR%2C%20a%20new%0Aand%20diverse%20dataset%20with%203d%20pose%20and%20expression%20annotations%2C%20one%20order%20of%0Amagnitude%20larger%20than%20previous%20ones%20%28800%2C000%20identities%29%20and%20with%20dynamic%0Agestures%2C%20on%20which%20we%20train%20and%20ablate%20our%20main%20technical%20contributions.%0A%20%20VLOGGER%20outperforms%20state-of-the-art%20methods%20in%20three%20public%20benchmarks%2C%0Aconsidering%20image%20quality%2C%20identity%20preservation%20and%20temporal%20consistency%20while%0Aalso%20generating%20upper-body%20gestures.%20We%20analyze%20the%20performance%20of%20VLOGGER%20with%0Arespect%20to%20multiple%20diversity%20metrics%2C%20showing%20that%20our%20architectural%20choices%0Aand%20the%20use%20of%20MENTOR%20benefit%20training%20a%20fair%20and%20unbiased%20model%20at%20scale.%0AFinally%20we%20show%20applications%20in%20video%20editing%20and%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08764v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLOGGER%3A%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20Synthesis&entry.906535625=Enric%20Corona%20and%20Andrei%20Zanfir%20and%20Eduard%20Gabriel%20Bazavan%20and%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Cristian%20Sminchisescu&entry.1292438233=%20%20We%20propose%20VLOGGER%2C%20a%20method%20for%20audio-driven%20human%20video%20generation%20from%20a%0Asingle%20input%20image%20of%20a%20person%2C%20which%20builds%20on%20the%20success%20of%20recent%0Agenerative%20diffusion%20models.%20Our%20method%20consists%20of%201%29%20a%20stochastic%0Ahuman-to-3d-motion%20diffusion%20model%2C%20and%202%29%20a%20novel%20diffusion-based%20architecture%0Athat%20augments%20text-to-image%20models%20with%20both%20spatial%20and%20temporal%20controls.%0AThis%20supports%20the%20generation%20of%20high%20quality%20video%20of%20variable%20length%2C%20easily%0Acontrollable%20through%20high-level%20representations%20of%20human%20faces%20and%20bodies.%20In%0Acontrast%20to%20previous%20work%2C%20our%20method%20does%20not%20require%20training%20for%20each%0Aperson%2C%20does%20not%20rely%20on%20face%20detection%20and%20cropping%2C%20generates%20the%20complete%0Aimage%20%28not%20just%20the%20face%20or%20the%20lips%29%2C%20and%20considers%20a%20broad%20spectrum%20of%0Ascenarios%20%28e.g.%20visible%20torso%20or%20diverse%20subject%20identities%29%20that%20are%20critical%0Ato%20correctly%20synthesize%20humans%20who%20communicate.%20We%20also%20curate%20MENTOR%2C%20a%20new%0Aand%20diverse%20dataset%20with%203d%20pose%20and%20expression%20annotations%2C%20one%20order%20of%0Amagnitude%20larger%20than%20previous%20ones%20%28800%2C000%20identities%29%20and%20with%20dynamic%0Agestures%2C%20on%20which%20we%20train%20and%20ablate%20our%20main%20technical%20contributions.%0A%20%20VLOGGER%20outperforms%20state-of-the-art%20methods%20in%20three%20public%20benchmarks%2C%0Aconsidering%20image%20quality%2C%20identity%20preservation%20and%20temporal%20consistency%20while%0Aalso%20generating%20upper-body%20gestures.%20We%20analyze%20the%20performance%20of%20VLOGGER%20with%0Arespect%20to%20multiple%20diversity%20metrics%2C%20showing%20that%20our%20architectural%20choices%0Aand%20the%20use%20of%20MENTOR%20benefit%20training%20a%20fair%20and%20unbiased%20model%20at%20scale.%0AFinally%20we%20show%20applications%20in%20video%20editing%20and%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08764v1&entry.124074799=Read"},
{"title": "PSDiff: Diffusion Model for Person Search with Iterative and\n  Collaborative Refinement", "author": "Chengyou Jia and Minnan Luo and Zhuohang Dang and Guang Dai and Xiaojun Chang and Jingdong Wang", "abstract": "  Dominant Person Search methods aim to localize and recognize query persons in\na unified network, which jointly optimizes two sub-tasks, \\ie, pedestrian\ndetection and Re-IDentification (ReID). Despite significant progress, current\nmethods face two primary challenges: 1) the pedestrian candidates learned\nwithin detectors are suboptimal for the ReID task. 2) the potential for\ncollaboration between two sub-tasks is overlooked. To address these issues, we\npresent a novel Person Search framework based on the Diffusion model, PSDiff.\nPSDiff formulates the person search as a dual denoising process from noisy\nboxes and ReID embeddings to ground truths. Distinct from the conventional\nDetection-to-ReID approach, our denoising paradigm discards prior pedestrian\ncandidates generated by detectors, thereby avoiding the local optimum problem\nof the ReID task. Following the new paradigm, we further design a new\nCollaborative Denoising Layer (CDL) to optimize detection and ReID sub-tasks in\nan iterative and collaborative way, which makes two sub-tasks mutually\nbeneficial. Extensive experiments on the standard benchmarks show that PSDiff\nachieves state-of-the-art performance with fewer parameters and elastic\ncomputing overhead.\n", "link": "http://arxiv.org/abs/2309.11125v2", "date": "2024-03-13", "relevancy": 2.5696, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5189}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PSDiff%3A%20Diffusion%20Model%20for%20Person%20Search%20with%20Iterative%20and%0A%20%20Collaborative%20Refinement&body=Title%3A%20PSDiff%3A%20Diffusion%20Model%20for%20Person%20Search%20with%20Iterative%20and%0A%20%20Collaborative%20Refinement%0AAuthor%3A%20Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Guang%20Dai%20and%20Xiaojun%20Chang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Dominant%20Person%20Search%20methods%20aim%20to%20localize%20and%20recognize%20query%20persons%20in%0Aa%20unified%20network%2C%20which%20jointly%20optimizes%20two%20sub-tasks%2C%20%5Cie%2C%20pedestrian%0Adetection%20and%20Re-IDentification%20%28ReID%29.%20Despite%20significant%20progress%2C%20current%0Amethods%20face%20two%20primary%20challenges%3A%201%29%20the%20pedestrian%20candidates%20learned%0Awithin%20detectors%20are%20suboptimal%20for%20the%20ReID%20task.%202%29%20the%20potential%20for%0Acollaboration%20between%20two%20sub-tasks%20is%20overlooked.%20To%20address%20these%20issues%2C%20we%0Apresent%20a%20novel%20Person%20Search%20framework%20based%20on%20the%20Diffusion%20model%2C%20PSDiff.%0APSDiff%20formulates%20the%20person%20search%20as%20a%20dual%20denoising%20process%20from%20noisy%0Aboxes%20and%20ReID%20embeddings%20to%20ground%20truths.%20Distinct%20from%20the%20conventional%0ADetection-to-ReID%20approach%2C%20our%20denoising%20paradigm%20discards%20prior%20pedestrian%0Acandidates%20generated%20by%20detectors%2C%20thereby%20avoiding%20the%20local%20optimum%20problem%0Aof%20the%20ReID%20task.%20Following%20the%20new%20paradigm%2C%20we%20further%20design%20a%20new%0ACollaborative%20Denoising%20Layer%20%28CDL%29%20to%20optimize%20detection%20and%20ReID%20sub-tasks%20in%0Aan%20iterative%20and%20collaborative%20way%2C%20which%20makes%20two%20sub-tasks%20mutually%0Abeneficial.%20Extensive%20experiments%20on%20the%20standard%20benchmarks%20show%20that%20PSDiff%0Aachieves%20state-of-the-art%20performance%20with%20fewer%20parameters%20and%20elastic%0Acomputing%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11125v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSDiff%3A%20Diffusion%20Model%20for%20Person%20Search%20with%20Iterative%20and%0A%20%20Collaborative%20Refinement&entry.906535625=Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Guang%20Dai%20and%20Xiaojun%20Chang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Dominant%20Person%20Search%20methods%20aim%20to%20localize%20and%20recognize%20query%20persons%20in%0Aa%20unified%20network%2C%20which%20jointly%20optimizes%20two%20sub-tasks%2C%20%5Cie%2C%20pedestrian%0Adetection%20and%20Re-IDentification%20%28ReID%29.%20Despite%20significant%20progress%2C%20current%0Amethods%20face%20two%20primary%20challenges%3A%201%29%20the%20pedestrian%20candidates%20learned%0Awithin%20detectors%20are%20suboptimal%20for%20the%20ReID%20task.%202%29%20the%20potential%20for%0Acollaboration%20between%20two%20sub-tasks%20is%20overlooked.%20To%20address%20these%20issues%2C%20we%0Apresent%20a%20novel%20Person%20Search%20framework%20based%20on%20the%20Diffusion%20model%2C%20PSDiff.%0APSDiff%20formulates%20the%20person%20search%20as%20a%20dual%20denoising%20process%20from%20noisy%0Aboxes%20and%20ReID%20embeddings%20to%20ground%20truths.%20Distinct%20from%20the%20conventional%0ADetection-to-ReID%20approach%2C%20our%20denoising%20paradigm%20discards%20prior%20pedestrian%0Acandidates%20generated%20by%20detectors%2C%20thereby%20avoiding%20the%20local%20optimum%20problem%0Aof%20the%20ReID%20task.%20Following%20the%20new%20paradigm%2C%20we%20further%20design%20a%20new%0ACollaborative%20Denoising%20Layer%20%28CDL%29%20to%20optimize%20detection%20and%20ReID%20sub-tasks%20in%0Aan%20iterative%20and%20collaborative%20way%2C%20which%20makes%20two%20sub-tasks%20mutually%0Abeneficial.%20Extensive%20experiments%20on%20the%20standard%20benchmarks%20show%20that%20PSDiff%0Aachieves%20state-of-the-art%20performance%20with%20fewer%20parameters%20and%20elastic%0Acomputing%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11125v2&entry.124074799=Read"},
{"title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query\n  Correspondence Mining", "author": "Jiahao Nie and Yun Xing and Gongjie Zhang and Pei Yan and Aoran Xiao and Yap-Peng Tan and Alex C. Kot and Shijian Lu", "abstract": "  Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting\nnovel categories from a distinct domain using only limited exemplars. In this\npaper, we undertake a comprehensive study of CD-FSS and uncover two crucial\ninsights: (i) the necessity of a fine-tuning stage to effectively transfer the\nlearned meta-knowledge across domains, and (ii) the overfitting risk during the\nna\\\"ive fine-tuning due to the scarcity of novel category examples. With these\ninsights, we propose a novel cross-domain fine-tuning strategy that addresses\nthe challenging CD-FSS tasks. We first design Bi-directional Few-shot\nPrediction (BFP), which establishes support-query correspondence in a\nbi-directional manner, crafting augmented supervision to reduce the overfitting\nrisk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which\nis a recursive framework to capture the support-query correspondence\niteratively, targeting maximal exploitation of supervisory signals from the\nsparse novel category samples. Extensive empirical evaluations show that our\nmethod significantly outperforms the state-of-the-arts (+7.8\\%), which verifies\nthat IFA tackles the cross-domain challenges and mitigates the overfitting\nsimultaneously. The code is available at: https://github.com/niejiahao1998/IFA.\n", "link": "http://arxiv.org/abs/2401.08407v2", "date": "2024-03-13", "relevancy": 2.5684, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining&body=Title%3A%20Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining%0AAuthor%3A%20Jiahao%20Nie%20and%20Yun%20Xing%20and%20Gongjie%20Zhang%20and%20Pei%20Yan%20and%20Aoran%20Xiao%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20poses%20the%20challenge%20of%20segmenting%0Anovel%20categories%20from%20a%20distinct%20domain%20using%20only%20limited%20exemplars.%20In%20this%0Apaper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20CD-FSS%20and%20uncover%20two%20crucial%0Ainsights%3A%20%28i%29%20the%20necessity%20of%20a%20fine-tuning%20stage%20to%20effectively%20transfer%20the%0Alearned%20meta-knowledge%20across%20domains%2C%20and%20%28ii%29%20the%20overfitting%20risk%20during%20the%0Ana%5C%22ive%20fine-tuning%20due%20to%20the%20scarcity%20of%20novel%20category%20examples.%20With%20these%0Ainsights%2C%20we%20propose%20a%20novel%20cross-domain%20fine-tuning%20strategy%20that%20addresses%0Athe%20challenging%20CD-FSS%20tasks.%20We%20first%20design%20Bi-directional%20Few-shot%0APrediction%20%28BFP%29%2C%20which%20establishes%20support-query%20correspondence%20in%20a%0Abi-directional%20manner%2C%20crafting%20augmented%20supervision%20to%20reduce%20the%20overfitting%0Arisk.%20Then%20we%20further%20extend%20BFP%20into%20Iterative%20Few-shot%20Adaptor%20%28IFA%29%2C%20which%0Ais%20a%20recursive%20framework%20to%20capture%20the%20support-query%20correspondence%0Aiteratively%2C%20targeting%20maximal%20exploitation%20of%20supervisory%20signals%20from%20the%0Asparse%20novel%20category%20samples.%20Extensive%20empirical%20evaluations%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-arts%20%28%2B7.8%5C%25%29%2C%20which%20verifies%0Athat%20IFA%20tackles%20the%20cross-domain%20challenges%20and%20mitigates%20the%20overfitting%0Asimultaneously.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/niejiahao1998/IFA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08407v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Few-Shot%20Segmentation%20via%20Iterative%20Support-Query%0A%20%20Correspondence%20Mining&entry.906535625=Jiahao%20Nie%20and%20Yun%20Xing%20and%20Gongjie%20Zhang%20and%20Pei%20Yan%20and%20Aoran%20Xiao%20and%20Yap-Peng%20Tan%20and%20Alex%20C.%20Kot%20and%20Shijian%20Lu&entry.1292438233=%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20poses%20the%20challenge%20of%20segmenting%0Anovel%20categories%20from%20a%20distinct%20domain%20using%20only%20limited%20exemplars.%20In%20this%0Apaper%2C%20we%20undertake%20a%20comprehensive%20study%20of%20CD-FSS%20and%20uncover%20two%20crucial%0Ainsights%3A%20%28i%29%20the%20necessity%20of%20a%20fine-tuning%20stage%20to%20effectively%20transfer%20the%0Alearned%20meta-knowledge%20across%20domains%2C%20and%20%28ii%29%20the%20overfitting%20risk%20during%20the%0Ana%5C%22ive%20fine-tuning%20due%20to%20the%20scarcity%20of%20novel%20category%20examples.%20With%20these%0Ainsights%2C%20we%20propose%20a%20novel%20cross-domain%20fine-tuning%20strategy%20that%20addresses%0Athe%20challenging%20CD-FSS%20tasks.%20We%20first%20design%20Bi-directional%20Few-shot%0APrediction%20%28BFP%29%2C%20which%20establishes%20support-query%20correspondence%20in%20a%0Abi-directional%20manner%2C%20crafting%20augmented%20supervision%20to%20reduce%20the%20overfitting%0Arisk.%20Then%20we%20further%20extend%20BFP%20into%20Iterative%20Few-shot%20Adaptor%20%28IFA%29%2C%20which%0Ais%20a%20recursive%20framework%20to%20capture%20the%20support-query%20correspondence%0Aiteratively%2C%20targeting%20maximal%20exploitation%20of%20supervisory%20signals%20from%20the%0Asparse%20novel%20category%20samples.%20Extensive%20empirical%20evaluations%20show%20that%20our%0Amethod%20significantly%20outperforms%20the%20state-of-the-arts%20%28%2B7.8%5C%25%29%2C%20which%20verifies%0Athat%20IFA%20tackles%20the%20cross-domain%20challenges%20and%20mitigates%20the%20overfitting%0Asimultaneously.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/niejiahao1998/IFA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08407v2&entry.124074799=Read"},
{"title": "Continual Adversarial Defense", "author": "Qian Wang and Yaoyao Liu and Hefei Ling and Yingwei Li and Qihao Liu and Ping Li and Jiazhong Chen and Alan Yuille and Ning Yu", "abstract": "  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. The defense\nsystem must gather online few-shot defense feedback to promptly enhance itself,\nleveraging efficient memory utilization. Therefore, we propose the first\ncontinual adversarial defense (CAD) framework that adapts to any attacks in a\ndynamic scenario, where various attacks emerge stage by stage. In practice, CAD\nis modeled under four principles: (1) continual adaptation to new attacks\nwithout catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient\nadaptation, and (4) high accuracy on both clean and adversarial images. We\nexplore and integrate cutting-edge continual learning, few-shot learning, and\nensemble learning techniques to qualify the principles. Experiments conducted\non CIFAR-10 and ImageNet-100 validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal feedback and a low cost of defense failure, while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n", "link": "http://arxiv.org/abs/2312.09481v2", "date": "2024-03-13", "relevancy": 2.5679, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5221}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5104}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Adversarial%20Defense&body=Title%3A%20Continual%20Adversarial%20Defense%0AAuthor%3A%20Qian%20Wang%20and%20Yaoyao%20Liu%20and%20Hefei%20Ling%20and%20Yingwei%20Li%20and%20Qihao%20Liu%20and%20Ping%20Li%20and%20Jiazhong%20Chen%20and%20Alan%20Yuille%20and%20Ning%20Yu%0AAbstract%3A%20%20%20In%20response%20to%20the%20rapidly%20evolving%20nature%20of%20adversarial%20attacks%20against%0Avisual%20classifiers%20on%20a%20monthly%20basis%2C%20numerous%20defenses%20have%20been%20proposed%20to%0Ageneralize%20against%20as%20many%20known%20attacks%20as%20possible.%20However%2C%20designing%20a%0Adefense%20method%20that%20generalizes%20to%20all%20types%20of%20attacks%20is%20not%20realistic%0Abecause%20the%20environment%20in%20which%20defense%20systems%20operate%20is%20dynamic%20and%0Acomprises%20various%20unique%20attacks%20that%20emerge%20as%20time%20goes%20on.%20The%20defense%0Asystem%20must%20gather%20online%20few-shot%20defense%20feedback%20to%20promptly%20enhance%20itself%2C%0Aleveraging%20efficient%20memory%20utilization.%20Therefore%2C%20we%20propose%20the%20first%0Acontinual%20adversarial%20defense%20%28CAD%29%20framework%20that%20adapts%20to%20any%20attacks%20in%20a%0Adynamic%20scenario%2C%20where%20various%20attacks%20emerge%20stage%20by%20stage.%20In%20practice%2C%20CAD%0Ais%20modeled%20under%20four%20principles%3A%20%281%29%20continual%20adaptation%20to%20new%20attacks%0Awithout%20catastrophic%20forgetting%2C%20%282%29%20few-shot%20adaptation%2C%20%283%29%20memory-efficient%0Aadaptation%2C%20and%20%284%29%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20images.%20We%0Aexplore%20and%20integrate%20cutting-edge%20continual%20learning%2C%20few-shot%20learning%2C%20and%0Aensemble%20learning%20techniques%20to%20qualify%20the%20principles.%20Experiments%20conducted%0Aon%20CIFAR-10%20and%20ImageNet-100%20validate%20the%20effectiveness%20of%20our%20approach%20against%0Amultiple%20stages%20of%20modern%20adversarial%20attacks%20and%20demonstrate%20significant%0Aimprovements%20over%20numerous%20baseline%20methods.%20In%20particular%2C%20CAD%20is%20capable%20of%0Aquickly%20adapting%20with%20minimal%20feedback%20and%20a%20low%20cost%20of%20defense%20failure%2C%20while%0Amaintaining%20good%20performance%20against%20previous%20attacks.%20Our%20research%20sheds%20light%0Aon%20a%20brand-new%20paradigm%20for%20continual%20defense%20adaptation%20against%20dynamic%20and%0Aevolving%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09481v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Adversarial%20Defense&entry.906535625=Qian%20Wang%20and%20Yaoyao%20Liu%20and%20Hefei%20Ling%20and%20Yingwei%20Li%20and%20Qihao%20Liu%20and%20Ping%20Li%20and%20Jiazhong%20Chen%20and%20Alan%20Yuille%20and%20Ning%20Yu&entry.1292438233=%20%20In%20response%20to%20the%20rapidly%20evolving%20nature%20of%20adversarial%20attacks%20against%0Avisual%20classifiers%20on%20a%20monthly%20basis%2C%20numerous%20defenses%20have%20been%20proposed%20to%0Ageneralize%20against%20as%20many%20known%20attacks%20as%20possible.%20However%2C%20designing%20a%0Adefense%20method%20that%20generalizes%20to%20all%20types%20of%20attacks%20is%20not%20realistic%0Abecause%20the%20environment%20in%20which%20defense%20systems%20operate%20is%20dynamic%20and%0Acomprises%20various%20unique%20attacks%20that%20emerge%20as%20time%20goes%20on.%20The%20defense%0Asystem%20must%20gather%20online%20few-shot%20defense%20feedback%20to%20promptly%20enhance%20itself%2C%0Aleveraging%20efficient%20memory%20utilization.%20Therefore%2C%20we%20propose%20the%20first%0Acontinual%20adversarial%20defense%20%28CAD%29%20framework%20that%20adapts%20to%20any%20attacks%20in%20a%0Adynamic%20scenario%2C%20where%20various%20attacks%20emerge%20stage%20by%20stage.%20In%20practice%2C%20CAD%0Ais%20modeled%20under%20four%20principles%3A%20%281%29%20continual%20adaptation%20to%20new%20attacks%0Awithout%20catastrophic%20forgetting%2C%20%282%29%20few-shot%20adaptation%2C%20%283%29%20memory-efficient%0Aadaptation%2C%20and%20%284%29%20high%20accuracy%20on%20both%20clean%20and%20adversarial%20images.%20We%0Aexplore%20and%20integrate%20cutting-edge%20continual%20learning%2C%20few-shot%20learning%2C%20and%0Aensemble%20learning%20techniques%20to%20qualify%20the%20principles.%20Experiments%20conducted%0Aon%20CIFAR-10%20and%20ImageNet-100%20validate%20the%20effectiveness%20of%20our%20approach%20against%0Amultiple%20stages%20of%20modern%20adversarial%20attacks%20and%20demonstrate%20significant%0Aimprovements%20over%20numerous%20baseline%20methods.%20In%20particular%2C%20CAD%20is%20capable%20of%0Aquickly%20adapting%20with%20minimal%20feedback%20and%20a%20low%20cost%20of%20defense%20failure%2C%20while%0Amaintaining%20good%20performance%20against%20previous%20attacks.%20Our%20research%20sheds%20light%0Aon%20a%20brand-new%20paradigm%20for%20continual%20defense%20adaptation%20against%20dynamic%20and%0Aevolving%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09481v2&entry.124074799=Read"},
{"title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts", "author": "Shengzhuang Chen and Jihoon Tack and Yunqiao Yang and Yee Whye Teh and Jonathan Richard Schwarz and Ying Wei", "abstract": "  Conventional wisdom suggests parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-domain (OOD) tasks. In\nthis paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse\nmixture-of-experts approaches and trained to isolate subsets of pre-trained\nparameters automatically for meta-tuning on each task. SMAT successfully\novercomes OOD sensitivity and delivers on the promise of enhancing the transfer\nabilities of vision foundation models beyond parameter-efficient finetuning. We\nestablish new state-of-the-art results on a challenging combination of\nMeta-Dataset augmented with additional OOD tasks in both zero-shot and\ngradient-based adaptation settings. In addition, we provide a thorough analysis\nof the superiority of learned over hand-designed sparsity patterns for sparse\nexpert methods and the pivotal importance of the sparsity level in balancing\nbetween in-domain and out-of-domain generalization. Our code is publicly\navailable.\n", "link": "http://arxiv.org/abs/2403.08477v1", "date": "2024-03-13", "relevancy": 2.5653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&body=Title%3A%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts%0AAuthor%3A%20Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei%0AAbstract%3A%20%20%20Conventional%20wisdom%20suggests%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-domain%20%28OOD%29%20tasks.%20In%0Athis%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%20sparse%0Amixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%20pre-trained%0Aparameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%20successfully%0Aovercomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%20the%20transfer%0Aabilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%20finetuning.%20We%0Aestablish%20new%20state-of-the-art%20results%20on%20a%20challenging%20combination%20of%0AMeta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%20zero-shot%20and%0Agradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%20thorough%20analysis%0Aof%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%20patterns%20for%20sparse%0Aexpert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%20level%20in%20balancing%0Abetween%20in-domain%20and%20out-of-domain%20generalization.%20Our%20code%20is%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08477v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&entry.906535625=Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei&entry.1292438233=%20%20Conventional%20wisdom%20suggests%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-domain%20%28OOD%29%20tasks.%20In%0Athis%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%20sparse%0Amixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%20pre-trained%0Aparameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%20successfully%0Aovercomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%20the%20transfer%0Aabilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%20finetuning.%20We%0Aestablish%20new%20state-of-the-art%20results%20on%20a%20challenging%20combination%20of%0AMeta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%20zero-shot%20and%0Agradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%20thorough%20analysis%0Aof%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%20patterns%20for%20sparse%0Aexpert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%20level%20in%20balancing%0Abetween%20in-domain%20and%20out-of-domain%20generalization.%20Our%20code%20is%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08477v1&entry.124074799=Read"},
{"title": "Refractive COLMAP: Refractive Structure-from-Motion Revisited", "author": "Mengkun She and Felix Seegr\u00e4ber and David Nakath and Kevin K\u00f6ser", "abstract": "  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n", "link": "http://arxiv.org/abs/2403.08640v1", "date": "2024-03-13", "relevancy": 2.5593, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4786}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&body=Title%3A%20Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited%0AAuthor%3A%20Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08640v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refractive%20COLMAP%3A%20Refractive%20Structure-from-Motion%20Revisited&entry.906535625=Mengkun%20She%20and%20Felix%20Seegr%C3%A4ber%20and%20David%20Nakath%20and%20Kevin%20K%C3%B6ser&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20complete%20refractive%20Structure-from-Motion%20%28RSfM%29%0Aframework%20for%20underwater%203D%20reconstruction%20using%20refractive%20camera%20setups%20%28for%0Aboth%2C%20flat-%20and%20dome-port%20underwater%20housings%29.%20Despite%20notable%20achievements%20in%0Arefractive%20multi-view%20geometry%20over%20the%20past%20decade%2C%20a%20robust%2C%20complete%20and%0Apublicly%20available%20solution%20for%20such%20tasks%20is%20not%20available%20at%20present%2C%20and%0Aoften%20practical%20applications%20have%20to%20resort%20to%20approximating%20refraction%20effects%0Aby%20the%20intrinsic%20%28distortion%29%20parameters%20of%20a%20pinhole%20camera%20model.%20To%20fill%0Athis%20gap%2C%20we%20have%20integrated%20refraction%20considerations%20throughout%20the%20entire%0ASfM%20process%20within%20the%20state-of-the-art%2C%20open-source%20SfM%20framework%20COLMAP.%0ANumerical%20simulations%20and%20reconstruction%20results%20on%20synthetically%20generated%20but%0Aphoto-realistic%20images%20with%20ground%20truth%20validate%20that%20enabling%20refraction%20does%0Anot%20compromise%20accuracy%20or%20robustness%20as%20compared%20to%20in-air%20reconstructions.%0AFinally%2C%20we%20demonstrate%20the%20capability%20of%20our%20approach%20for%20large-scale%0Arefractive%20scenarios%20using%20a%20dataset%20consisting%20of%20nearly%206000%20images.%20The%0Aimplementation%20is%20released%20as%20open-source%20at%3A%0Ahttps%3A//cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08640v1&entry.124074799=Read"},
{"title": "ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks", "author": "Zhaoliang Chen and Zhihao Wu and Ylli Sadikaj and Claudia Plant and Hong-Ning Dai and Shiping Wang and Wenzhong Guo", "abstract": "  Although Graph Neural Networks (GNNs) have exhibited the powerful ability to\ngather graph-structured information from neighborhood nodes via various\nmessage-passing mechanisms, the performance of GNNs is limited by poor\ngeneralization and fragile robustness caused by noisy and redundant graph data.\nAs a prominent solution, Graph Augmentation Learning (GAL) has recently\nreceived increasing attention. Among prior GAL approaches, edge-dropping\nmethods that randomly remove edges from a graph during training are effective\ntechniques to improve the robustness of GNNs. However, randomly dropping edges\noften results in bypassing critical edges, consequently weakening the\neffectiveness of message passing. In this paper, we propose a novel adversarial\nedge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor\nguiding the removal of edges, which can be flexibly incorporated into diverse\nGNN backbones. Employing an adversarial training framework, the edge predictor\nutilizes the line graph transformed from the original graph to estimate the\nedges to be dropped, which improves the interpretability of the edge-dropping\nmethod. The proposed ADEdgeDrop is optimized alternately by stochastic gradient\ndescent and projected gradient descent. Comprehensive experiments on six graph\nbenchmark datasets demonstrate that the proposed ADEdgeDrop outperforms\nstate-of-the-art baselines across various GNN backbones, demonstrating improved\ngeneralization and robustness.\n", "link": "http://arxiv.org/abs/2403.09171v1", "date": "2024-03-14", "relevancy": 2.5565, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5638}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4949}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4752}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ADEdgeDrop%3A%20Adversarial%20Edge%20Dropping%20for%20Robust%20Graph%20Neural%20Networks&body=Title%3A%20ADEdgeDrop%3A%20Adversarial%20Edge%20Dropping%20for%20Robust%20Graph%20Neural%20Networks%0AAuthor%3A%20Zhaoliang%20Chen%20and%20Zhihao%20Wu%20and%20Ylli%20Sadikaj%20and%20Claudia%20Plant%20and%20Hong-Ning%20Dai%20and%20Shiping%20Wang%20and%20Wenzhong%20Guo%0AAbstract%3A%20%20%20Although%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20exhibited%20the%20powerful%20ability%20to%0Agather%20graph-structured%20information%20from%20neighborhood%20nodes%20via%20various%0Amessage-passing%20mechanisms%2C%20the%20performance%20of%20GNNs%20is%20limited%20by%20poor%0Ageneralization%20and%20fragile%20robustness%20caused%20by%20noisy%20and%20redundant%20graph%20data.%0AAs%20a%20prominent%20solution%2C%20Graph%20Augmentation%20Learning%20%28GAL%29%20has%20recently%0Areceived%20increasing%20attention.%20Among%20prior%20GAL%20approaches%2C%20edge-dropping%0Amethods%20that%20randomly%20remove%20edges%20from%20a%20graph%20during%20training%20are%20effective%0Atechniques%20to%20improve%20the%20robustness%20of%20GNNs.%20However%2C%20randomly%20dropping%20edges%0Aoften%20results%20in%20bypassing%20critical%20edges%2C%20consequently%20weakening%20the%0Aeffectiveness%20of%20message%20passing.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20adversarial%0Aedge-dropping%20method%20%28ADEdgeDrop%29%20that%20leverages%20an%20adversarial%20edge%20predictor%0Aguiding%20the%20removal%20of%20edges%2C%20which%20can%20be%20flexibly%20incorporated%20into%20diverse%0AGNN%20backbones.%20Employing%20an%20adversarial%20training%20framework%2C%20the%20edge%20predictor%0Autilizes%20the%20line%20graph%20transformed%20from%20the%20original%20graph%20to%20estimate%20the%0Aedges%20to%20be%20dropped%2C%20which%20improves%20the%20interpretability%20of%20the%20edge-dropping%0Amethod.%20The%20proposed%20ADEdgeDrop%20is%20optimized%20alternately%20by%20stochastic%20gradient%0Adescent%20and%20projected%20gradient%20descent.%20Comprehensive%20experiments%20on%20six%20graph%0Abenchmark%20datasets%20demonstrate%20that%20the%20proposed%20ADEdgeDrop%20outperforms%0Astate-of-the-art%20baselines%20across%20various%20GNN%20backbones%2C%20demonstrating%20improved%0Ageneralization%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09171v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADEdgeDrop%3A%20Adversarial%20Edge%20Dropping%20for%20Robust%20Graph%20Neural%20Networks&entry.906535625=Zhaoliang%20Chen%20and%20Zhihao%20Wu%20and%20Ylli%20Sadikaj%20and%20Claudia%20Plant%20and%20Hong-Ning%20Dai%20and%20Shiping%20Wang%20and%20Wenzhong%20Guo&entry.1292438233=%20%20Although%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20exhibited%20the%20powerful%20ability%20to%0Agather%20graph-structured%20information%20from%20neighborhood%20nodes%20via%20various%0Amessage-passing%20mechanisms%2C%20the%20performance%20of%20GNNs%20is%20limited%20by%20poor%0Ageneralization%20and%20fragile%20robustness%20caused%20by%20noisy%20and%20redundant%20graph%20data.%0AAs%20a%20prominent%20solution%2C%20Graph%20Augmentation%20Learning%20%28GAL%29%20has%20recently%0Areceived%20increasing%20attention.%20Among%20prior%20GAL%20approaches%2C%20edge-dropping%0Amethods%20that%20randomly%20remove%20edges%20from%20a%20graph%20during%20training%20are%20effective%0Atechniques%20to%20improve%20the%20robustness%20of%20GNNs.%20However%2C%20randomly%20dropping%20edges%0Aoften%20results%20in%20bypassing%20critical%20edges%2C%20consequently%20weakening%20the%0Aeffectiveness%20of%20message%20passing.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20adversarial%0Aedge-dropping%20method%20%28ADEdgeDrop%29%20that%20leverages%20an%20adversarial%20edge%20predictor%0Aguiding%20the%20removal%20of%20edges%2C%20which%20can%20be%20flexibly%20incorporated%20into%20diverse%0AGNN%20backbones.%20Employing%20an%20adversarial%20training%20framework%2C%20the%20edge%20predictor%0Autilizes%20the%20line%20graph%20transformed%20from%20the%20original%20graph%20to%20estimate%20the%0Aedges%20to%20be%20dropped%2C%20which%20improves%20the%20interpretability%20of%20the%20edge-dropping%0Amethod.%20The%20proposed%20ADEdgeDrop%20is%20optimized%20alternately%20by%20stochastic%20gradient%0Adescent%20and%20projected%20gradient%20descent.%20Comprehensive%20experiments%20on%20six%20graph%0Abenchmark%20datasets%20demonstrate%20that%20the%20proposed%20ADEdgeDrop%20outperforms%0Astate-of-the-art%20baselines%20across%20various%20GNN%20backbones%2C%20demonstrating%20improved%0Ageneralization%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09171v1&entry.124074799=Read"},
{"title": "Dual Branch Deep Learning Network for Detection and Stage Grading of\n  Diabetic Retinopathy", "author": "Hossein Shakibania and Sina Raoufi and Behnam Pourafkham and Hassan Khotanlou and Muharram Mansoorizadeh", "abstract": "  Diabetic retinopathy is a severe complication of diabetes that can lead to\npermanent blindness if not treated promptly. Early and accurate diagnosis of\nthe disease is essential for successful treatment. This paper introduces a deep\nlearning method for the detection and stage grading of diabetic retinopathy,\nusing a single fundus retinal image. Our model utilizes transfer learning,\nemploying two state-of-the-art pre-trained models as feature extractors and\nfine-tuning them on a new dataset. The proposed model is trained on a large\nmulti-center dataset, including the APTOS 2019 dataset, obtained from publicly\navailable sources. It achieves remarkable performance in diabetic retinopathy\ndetection and stage classification on the APTOS 2019, outperforming the\nestablished literature. For binary classification, the proposed approach\nachieves an accuracy of 98.50, a sensitivity of 99.46, and a specificity of\n97.51. In stage grading, it achieves a quadratic weighted kappa of 93.00, an\naccuracy of 89.60, a sensitivity of 89.60, and a specificity of 97.72. The\nproposed approach serves as a reliable screening and stage grading tool for\ndiabetic retinopathy, offering significant potential to enhance clinical\ndecision-making and patient care.\n", "link": "http://arxiv.org/abs/2308.09945v2", "date": "2024-03-13", "relevancy": 2.5424, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5196}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Branch%20Deep%20Learning%20Network%20for%20Detection%20and%20Stage%20Grading%20of%0A%20%20Diabetic%20Retinopathy&body=Title%3A%20Dual%20Branch%20Deep%20Learning%20Network%20for%20Detection%20and%20Stage%20Grading%20of%0A%20%20Diabetic%20Retinopathy%0AAuthor%3A%20Hossein%20Shakibania%20and%20Sina%20Raoufi%20and%20Behnam%20Pourafkham%20and%20Hassan%20Khotanlou%20and%20Muharram%20Mansoorizadeh%0AAbstract%3A%20%20%20Diabetic%20retinopathy%20is%20a%20severe%20complication%20of%20diabetes%20that%20can%20lead%20to%0Apermanent%20blindness%20if%20not%20treated%20promptly.%20Early%20and%20accurate%20diagnosis%20of%0Athe%20disease%20is%20essential%20for%20successful%20treatment.%20This%20paper%20introduces%20a%20deep%0Alearning%20method%20for%20the%20detection%20and%20stage%20grading%20of%20diabetic%20retinopathy%2C%0Ausing%20a%20single%20fundus%20retinal%20image.%20Our%20model%20utilizes%20transfer%20learning%2C%0Aemploying%20two%20state-of-the-art%20pre-trained%20models%20as%20feature%20extractors%20and%0Afine-tuning%20them%20on%20a%20new%20dataset.%20The%20proposed%20model%20is%20trained%20on%20a%20large%0Amulti-center%20dataset%2C%20including%20the%20APTOS%202019%20dataset%2C%20obtained%20from%20publicly%0Aavailable%20sources.%20It%20achieves%20remarkable%20performance%20in%20diabetic%20retinopathy%0Adetection%20and%20stage%20classification%20on%20the%20APTOS%202019%2C%20outperforming%20the%0Aestablished%20literature.%20For%20binary%20classification%2C%20the%20proposed%20approach%0Aachieves%20an%20accuracy%20of%2098.50%2C%20a%20sensitivity%20of%2099.46%2C%20and%20a%20specificity%20of%0A97.51.%20In%20stage%20grading%2C%20it%20achieves%20a%20quadratic%20weighted%20kappa%20of%2093.00%2C%20an%0Aaccuracy%20of%2089.60%2C%20a%20sensitivity%20of%2089.60%2C%20and%20a%20specificity%20of%2097.72.%20The%0Aproposed%20approach%20serves%20as%20a%20reliable%20screening%20and%20stage%20grading%20tool%20for%0Adiabetic%20retinopathy%2C%20offering%20significant%20potential%20to%20enhance%20clinical%0Adecision-making%20and%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09945v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Branch%20Deep%20Learning%20Network%20for%20Detection%20and%20Stage%20Grading%20of%0A%20%20Diabetic%20Retinopathy&entry.906535625=Hossein%20Shakibania%20and%20Sina%20Raoufi%20and%20Behnam%20Pourafkham%20and%20Hassan%20Khotanlou%20and%20Muharram%20Mansoorizadeh&entry.1292438233=%20%20Diabetic%20retinopathy%20is%20a%20severe%20complication%20of%20diabetes%20that%20can%20lead%20to%0Apermanent%20blindness%20if%20not%20treated%20promptly.%20Early%20and%20accurate%20diagnosis%20of%0Athe%20disease%20is%20essential%20for%20successful%20treatment.%20This%20paper%20introduces%20a%20deep%0Alearning%20method%20for%20the%20detection%20and%20stage%20grading%20of%20diabetic%20retinopathy%2C%0Ausing%20a%20single%20fundus%20retinal%20image.%20Our%20model%20utilizes%20transfer%20learning%2C%0Aemploying%20two%20state-of-the-art%20pre-trained%20models%20as%20feature%20extractors%20and%0Afine-tuning%20them%20on%20a%20new%20dataset.%20The%20proposed%20model%20is%20trained%20on%20a%20large%0Amulti-center%20dataset%2C%20including%20the%20APTOS%202019%20dataset%2C%20obtained%20from%20publicly%0Aavailable%20sources.%20It%20achieves%20remarkable%20performance%20in%20diabetic%20retinopathy%0Adetection%20and%20stage%20classification%20on%20the%20APTOS%202019%2C%20outperforming%20the%0Aestablished%20literature.%20For%20binary%20classification%2C%20the%20proposed%20approach%0Aachieves%20an%20accuracy%20of%2098.50%2C%20a%20sensitivity%20of%2099.46%2C%20and%20a%20specificity%20of%0A97.51.%20In%20stage%20grading%2C%20it%20achieves%20a%20quadratic%20weighted%20kappa%20of%2093.00%2C%20an%0Aaccuracy%20of%2089.60%2C%20a%20sensitivity%20of%2089.60%2C%20and%20a%20specificity%20of%2097.72.%20The%0Aproposed%20approach%20serves%20as%20a%20reliable%20screening%20and%20stage%20grading%20tool%20for%0Adiabetic%20retinopathy%2C%20offering%20significant%20potential%20to%20enhance%20clinical%0Adecision-making%20and%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09945v2&entry.124074799=Read"},
{"title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models\n  for Generic Graph Mining", "author": "Yanchao Tan and Hang Lv and Xinyi Huang and Jiawei Zhang and Shiping Wang and Carl Yang", "abstract": "  Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.\n", "link": "http://arxiv.org/abs/2403.04780v2", "date": "2024-03-13", "relevancy": 2.5285, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5243}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5023}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4905}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining&body=Title%3A%20MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining%0AAuthor%3A%20Yanchao%20Tan%20and%20Hang%20Lv%20and%20Xinyi%20Huang%20and%20Jiawei%20Zhang%20and%20Shiping%20Wang%20and%20Carl%20Yang%0AAbstract%3A%20%20%20Graphs%20with%20abundant%20attributes%20are%20essential%20in%20modeling%20interconnected%0Aentities%20and%20improving%20predictions%20in%20various%20real-world%20applications.%0ATraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20are%20commonly%20used%20for%20modeling%0Aattributed%20graphs%2C%20need%20to%20be%20re-trained%20every%20time%20when%20applied%20to%20different%0Agraph%20tasks%20and%20datasets.%20Although%20the%20emergence%20of%20Large%20Language%20Models%0A%28LLMs%29%20has%20introduced%20a%20new%20paradigm%20in%20natural%20language%20processing%2C%20the%0Agenerative%20potential%20of%20LLMs%20in%20graph%20mining%20remains%20largely%20under-explored.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20framework%20MuseGraph%2C%20which%20seamlessly%20integrates%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20and%20facilitates%20a%20more%20effective%20and%20generic%0Aapproach%20for%20graph%20mining%20across%20different%20tasks%20and%20datasets.%20Specifically%2C%20we%0Afirst%20introduce%20a%20compact%20graph%20description%20via%20the%20proposed%20adaptive%20input%0Ageneration%20to%20encapsulate%20key%20information%20from%20the%20graph%20under%20the%20constraints%0Aof%20language%20token%20limitations.%20Then%2C%20we%20propose%20a%20diverse%20instruction%0Ageneration%20mechanism%2C%20which%20distills%20the%20reasoning%20capabilities%20from%20LLMs%0A%28e.g.%2C%20GPT-4%29%20to%20create%20task-specific%20Chain-of-Thought-based%20instruction%0Apackages%20for%20different%20graph%20tasks.%20Finally%2C%20we%20propose%20a%20graph-aware%0Ainstruction%20tuning%20with%20a%20dynamic%20instruction%20package%20allocation%20strategy%0Aacross%20tasks%20and%20datasets%2C%20ensuring%20the%20effectiveness%20and%20generalization%20of%20the%0Atraining%20process.%20Our%20experimental%20results%20demonstrate%20significant%20improvements%0Ain%20different%20graph%20tasks%2C%20showcasing%20the%20potential%20of%20our%20MuseGraph%20in%0Aenhancing%20the%20accuracy%20of%20graph-oriented%20downstream%20tasks%20while%20keeping%20the%0Ageneration%20powers%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04780v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuseGraph%3A%20Graph-oriented%20Instruction%20Tuning%20of%20Large%20Language%20Models%0A%20%20for%20Generic%20Graph%20Mining&entry.906535625=Yanchao%20Tan%20and%20Hang%20Lv%20and%20Xinyi%20Huang%20and%20Jiawei%20Zhang%20and%20Shiping%20Wang%20and%20Carl%20Yang&entry.1292438233=%20%20Graphs%20with%20abundant%20attributes%20are%20essential%20in%20modeling%20interconnected%0Aentities%20and%20improving%20predictions%20in%20various%20real-world%20applications.%0ATraditional%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20which%20are%20commonly%20used%20for%20modeling%0Aattributed%20graphs%2C%20need%20to%20be%20re-trained%20every%20time%20when%20applied%20to%20different%0Agraph%20tasks%20and%20datasets.%20Although%20the%20emergence%20of%20Large%20Language%20Models%0A%28LLMs%29%20has%20introduced%20a%20new%20paradigm%20in%20natural%20language%20processing%2C%20the%0Agenerative%20potential%20of%20LLMs%20in%20graph%20mining%20remains%20largely%20under-explored.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20framework%20MuseGraph%2C%20which%20seamlessly%20integrates%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20and%20facilitates%20a%20more%20effective%20and%20generic%0Aapproach%20for%20graph%20mining%20across%20different%20tasks%20and%20datasets.%20Specifically%2C%20we%0Afirst%20introduce%20a%20compact%20graph%20description%20via%20the%20proposed%20adaptive%20input%0Ageneration%20to%20encapsulate%20key%20information%20from%20the%20graph%20under%20the%20constraints%0Aof%20language%20token%20limitations.%20Then%2C%20we%20propose%20a%20diverse%20instruction%0Ageneration%20mechanism%2C%20which%20distills%20the%20reasoning%20capabilities%20from%20LLMs%0A%28e.g.%2C%20GPT-4%29%20to%20create%20task-specific%20Chain-of-Thought-based%20instruction%0Apackages%20for%20different%20graph%20tasks.%20Finally%2C%20we%20propose%20a%20graph-aware%0Ainstruction%20tuning%20with%20a%20dynamic%20instruction%20package%20allocation%20strategy%0Aacross%20tasks%20and%20datasets%2C%20ensuring%20the%20effectiveness%20and%20generalization%20of%20the%0Atraining%20process.%20Our%20experimental%20results%20demonstrate%20significant%20improvements%0Ain%20different%20graph%20tasks%2C%20showcasing%20the%20potential%20of%20our%20MuseGraph%20in%0Aenhancing%20the%20accuracy%20of%20graph-oriented%20downstream%20tasks%20while%20keeping%20the%0Ageneration%20powers%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04780v2&entry.124074799=Read"},
{"title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph", "author": "Yifei Zhang and Hao Zhao and Hongyang Li and Siheng Chen", "abstract": "  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n", "link": "http://arxiv.org/abs/2403.08770v1", "date": "2024-03-13", "relevancy": 2.5279, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.546}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4808}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph&body=Title%3A%20FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph%0AAuthor%3A%20Yifei%20Zhang%20and%20Hao%20Zhao%20and%20Hongyang%20Li%20and%20Siheng%20Chen%0AAbstract%3A%20%20%203D%20correspondence%2C%20i.e.%2C%20a%20pair%20of%203D%20points%2C%20is%20a%20fundamental%20concept%20in%0Acomputer%20vision.%20A%20set%20of%203D%20correspondences%2C%20when%20equipped%20with%20compatibility%0Aedges%2C%20forms%20a%20correspondence%20graph.%20This%20graph%20is%20a%20critical%20component%20in%0Aseveral%20state-of-the-art%203D%20point%20cloud%20registration%20approaches%2C%20e.g.%2C%20the%20one%0Abased%20on%20maximal%20cliques%20%28MAC%29.%20However%2C%20its%20properties%20have%20not%20been%20well%0Aunderstood.%20So%20we%20present%20the%20first%20study%20that%20introduces%20graph%20signal%0Aprocessing%20into%20the%20domain%20of%20correspondence%20graph.%20We%20exploit%20the%20generalized%0Adegree%20signal%20on%20correspondence%20graph%20and%20pursue%20sampling%20strategies%20that%0Apreserve%20high-frequency%20components%20of%20this%20signal.%20To%20address%20time-consuming%0Asingular%20value%20decomposition%20in%20deterministic%20sampling%2C%20we%20resort%20to%20a%0Astochastic%20approximate%20sampling%20strategy.%20As%20such%2C%20the%20core%20of%20our%20method%20is%0Athe%20stochastic%20spectral%20sampling%20of%20correspondence%20graph.%20As%20an%20application%2C%20we%0Abuild%20a%20complete%203D%20registration%20algorithm%20termed%20as%20FastMAC%2C%20that%20reaches%0Areal-time%20speed%20while%20leading%20to%20little%20to%20none%20performance%20drop.%20Through%0Aextensive%20experiments%2C%20we%20validate%20that%20FastMAC%20works%20for%20both%20indoor%20and%0Aoutdoor%20benchmarks.%20For%20example%2C%20FastMAC%20can%20accelerate%20MAC%20by%2080%20times%20while%0Amaintaining%20high%20registration%20success%20rate%20on%20KITTI.%20Codes%20are%20publicly%0Aavailable%20at%20https%3A//github.com/Forrest-110/FastMAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08770v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastMAC%3A%20Stochastic%20Spectral%20Sampling%20of%20Correspondence%20Graph&entry.906535625=Yifei%20Zhang%20and%20Hao%20Zhao%20and%20Hongyang%20Li%20and%20Siheng%20Chen&entry.1292438233=%20%203D%20correspondence%2C%20i.e.%2C%20a%20pair%20of%203D%20points%2C%20is%20a%20fundamental%20concept%20in%0Acomputer%20vision.%20A%20set%20of%203D%20correspondences%2C%20when%20equipped%20with%20compatibility%0Aedges%2C%20forms%20a%20correspondence%20graph.%20This%20graph%20is%20a%20critical%20component%20in%0Aseveral%20state-of-the-art%203D%20point%20cloud%20registration%20approaches%2C%20e.g.%2C%20the%20one%0Abased%20on%20maximal%20cliques%20%28MAC%29.%20However%2C%20its%20properties%20have%20not%20been%20well%0Aunderstood.%20So%20we%20present%20the%20first%20study%20that%20introduces%20graph%20signal%0Aprocessing%20into%20the%20domain%20of%20correspondence%20graph.%20We%20exploit%20the%20generalized%0Adegree%20signal%20on%20correspondence%20graph%20and%20pursue%20sampling%20strategies%20that%0Apreserve%20high-frequency%20components%20of%20this%20signal.%20To%20address%20time-consuming%0Asingular%20value%20decomposition%20in%20deterministic%20sampling%2C%20we%20resort%20to%20a%0Astochastic%20approximate%20sampling%20strategy.%20As%20such%2C%20the%20core%20of%20our%20method%20is%0Athe%20stochastic%20spectral%20sampling%20of%20correspondence%20graph.%20As%20an%20application%2C%20we%0Abuild%20a%20complete%203D%20registration%20algorithm%20termed%20as%20FastMAC%2C%20that%20reaches%0Areal-time%20speed%20while%20leading%20to%20little%20to%20none%20performance%20drop.%20Through%0Aextensive%20experiments%2C%20we%20validate%20that%20FastMAC%20works%20for%20both%20indoor%20and%0Aoutdoor%20benchmarks.%20For%20example%2C%20FastMAC%20can%20accelerate%20MAC%20by%2080%20times%20while%0Amaintaining%20high%20registration%20success%20rate%20on%20KITTI.%20Codes%20are%20publicly%0Aavailable%20at%20https%3A//github.com/Forrest-110/FastMAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08770v1&entry.124074799=Read"},
{"title": "Rethinking Class-incremental Learning in the Era of Large Pre-trained\n  Models via Test-Time Adaptation", "author": "Imad Eddine Marouf and Subhankar Roy and Enzo Tartaglione and St\u00e9phane Lathuili\u00e8re", "abstract": "  Class-incremental learning (CIL) is a challenging task that involves\nsequentially learning to categorize classes from new tasks without forgetting\npreviously learned information. The advent of large pre-trained models (PTMs)\nhas fast-tracked the progress in CIL due to the highly transferable PTM\nrepresentations, where tuning a small set of parameters leads to\nstate-of-the-art performance when compared with the traditional CIL methods\nthat are trained from scratch. However, repeated fine-tuning on each task\ndestroys the rich representations of the PTMs and further leads to forgetting\nprevious tasks. To strike a balance between the stability and plasticity of\nPTMs for CIL, we propose a novel perspective of eliminating training on every\nnew task and instead train PTM only on the first task, and then refine its\nrepresentation at inference time using test-time adaptation (TTA). Concretely,\nwe propose Test-Time Adaptation for Class-Incremental Learning (TTACIL) that\nfirst fine-tunes PTMs using Adapters on the first task, then adjusts Layer Norm\nparameters of the PTM on each test instance for learning task-specific\nfeatures, and finally resets them back to the adapted model to preserve\nstability. As a consequence, our TTACIL does not undergo any forgetting, while\nbenefiting each task with the rich PTM features. Additionally, by design, our\nTTACIL is robust to common data corruptions. Our method outperforms several\nstate-of-the-art CIL methods when evaluated on multiple CIL benchmarks under\nboth clean and corrupted data. Code is available at:\nhttps://github.com/IemProg/TTACIL.\n", "link": "http://arxiv.org/abs/2310.11482v2", "date": "2024-03-14", "relevancy": 2.5263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5697}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4714}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation&body=Title%3A%20Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation%0AAuthor%3A%20Imad%20Eddine%20Marouf%20and%20Subhankar%20Roy%20and%20Enzo%20Tartaglione%20and%20St%C3%A9phane%20Lathuili%C3%A8re%0AAbstract%3A%20%20%20Class-incremental%20learning%20%28CIL%29%20is%20a%20challenging%20task%20that%20involves%0Asequentially%20learning%20to%20categorize%20classes%20from%20new%20tasks%20without%20forgetting%0Apreviously%20learned%20information.%20The%20advent%20of%20large%20pre-trained%20models%20%28PTMs%29%0Ahas%20fast-tracked%20the%20progress%20in%20CIL%20due%20to%20the%20highly%20transferable%20PTM%0Arepresentations%2C%20where%20tuning%20a%20small%20set%20of%20parameters%20leads%20to%0Astate-of-the-art%20performance%20when%20compared%20with%20the%20traditional%20CIL%20methods%0Athat%20are%20trained%20from%20scratch.%20However%2C%20repeated%20fine-tuning%20on%20each%20task%0Adestroys%20the%20rich%20representations%20of%20the%20PTMs%20and%20further%20leads%20to%20forgetting%0Aprevious%20tasks.%20To%20strike%20a%20balance%20between%20the%20stability%20and%20plasticity%20of%0APTMs%20for%20CIL%2C%20we%20propose%20a%20novel%20perspective%20of%20eliminating%20training%20on%20every%0Anew%20task%20and%20instead%20train%20PTM%20only%20on%20the%20first%20task%2C%20and%20then%20refine%20its%0Arepresentation%20at%20inference%20time%20using%20test-time%20adaptation%20%28TTA%29.%20Concretely%2C%0Awe%20propose%20Test-Time%20Adaptation%20for%20Class-Incremental%20Learning%20%28TTACIL%29%20that%0Afirst%20fine-tunes%20PTMs%20using%20Adapters%20on%20the%20first%20task%2C%20then%20adjusts%20Layer%20Norm%0Aparameters%20of%20the%20PTM%20on%20each%20test%20instance%20for%20learning%20task-specific%0Afeatures%2C%20and%20finally%20resets%20them%20back%20to%20the%20adapted%20model%20to%20preserve%0Astability.%20As%20a%20consequence%2C%20our%20TTACIL%20does%20not%20undergo%20any%20forgetting%2C%20while%0Abenefiting%20each%20task%20with%20the%20rich%20PTM%20features.%20Additionally%2C%20by%20design%2C%20our%0ATTACIL%20is%20robust%20to%20common%20data%20corruptions.%20Our%20method%20outperforms%20several%0Astate-of-the-art%20CIL%20methods%20when%20evaluated%20on%20multiple%20CIL%20benchmarks%20under%0Aboth%20clean%20and%20corrupted%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IemProg/TTACIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11482v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Class-incremental%20Learning%20in%20the%20Era%20of%20Large%20Pre-trained%0A%20%20Models%20via%20Test-Time%20Adaptation&entry.906535625=Imad%20Eddine%20Marouf%20and%20Subhankar%20Roy%20and%20Enzo%20Tartaglione%20and%20St%C3%A9phane%20Lathuili%C3%A8re&entry.1292438233=%20%20Class-incremental%20learning%20%28CIL%29%20is%20a%20challenging%20task%20that%20involves%0Asequentially%20learning%20to%20categorize%20classes%20from%20new%20tasks%20without%20forgetting%0Apreviously%20learned%20information.%20The%20advent%20of%20large%20pre-trained%20models%20%28PTMs%29%0Ahas%20fast-tracked%20the%20progress%20in%20CIL%20due%20to%20the%20highly%20transferable%20PTM%0Arepresentations%2C%20where%20tuning%20a%20small%20set%20of%20parameters%20leads%20to%0Astate-of-the-art%20performance%20when%20compared%20with%20the%20traditional%20CIL%20methods%0Athat%20are%20trained%20from%20scratch.%20However%2C%20repeated%20fine-tuning%20on%20each%20task%0Adestroys%20the%20rich%20representations%20of%20the%20PTMs%20and%20further%20leads%20to%20forgetting%0Aprevious%20tasks.%20To%20strike%20a%20balance%20between%20the%20stability%20and%20plasticity%20of%0APTMs%20for%20CIL%2C%20we%20propose%20a%20novel%20perspective%20of%20eliminating%20training%20on%20every%0Anew%20task%20and%20instead%20train%20PTM%20only%20on%20the%20first%20task%2C%20and%20then%20refine%20its%0Arepresentation%20at%20inference%20time%20using%20test-time%20adaptation%20%28TTA%29.%20Concretely%2C%0Awe%20propose%20Test-Time%20Adaptation%20for%20Class-Incremental%20Learning%20%28TTACIL%29%20that%0Afirst%20fine-tunes%20PTMs%20using%20Adapters%20on%20the%20first%20task%2C%20then%20adjusts%20Layer%20Norm%0Aparameters%20of%20the%20PTM%20on%20each%20test%20instance%20for%20learning%20task-specific%0Afeatures%2C%20and%20finally%20resets%20them%20back%20to%20the%20adapted%20model%20to%20preserve%0Astability.%20As%20a%20consequence%2C%20our%20TTACIL%20does%20not%20undergo%20any%20forgetting%2C%20while%0Abenefiting%20each%20task%20with%20the%20rich%20PTM%20features.%20Additionally%2C%20by%20design%2C%20our%0ATTACIL%20is%20robust%20to%20common%20data%20corruptions.%20Our%20method%20outperforms%20several%0Astate-of-the-art%20CIL%20methods%20when%20evaluated%20on%20multiple%20CIL%20benchmarks%20under%0Aboth%20clean%20and%20corrupted%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/IemProg/TTACIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11482v2&entry.124074799=Read"},
{"title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning", "author": "Hanxun Huang and Ricardo J. G. B. Campello and Sarah Monazam Erfani and Xingjun Ma and Michael E. Houle and James Bailey", "abstract": "  Representations learned via self-supervised learning (SSL) can be susceptible\nto dimensional collapse, where the learned representation subspace is of\nextremely low dimensionality and thus fails to represent the full data\ndistribution and modalities. Dimensional collapse also known as the\n\"underfilling\" phenomenon is one of the major causes of degraded performance on\ndownstream tasks. Previous work has investigated the dimensional collapse\nproblem of SSL at a global level. In this paper, we demonstrate that\nrepresentations can span over high dimensional space globally, but collapse\nlocally. To address this, we propose a method called $\\textit{local\ndimensionality regularization (LDReg)}$. Our formulation is based on the\nderivation of the Fisher-Rao metric to compare and optimize local distance\ndistributions at an asymptotically small radius for each data point. By\nincreasing the local intrinsic dimensionality, we demonstrate through a range\nof experiments that LDReg improves the representation quality of SSL. The\nresults also show that LDReg can regularize dimensionality at both local and\nglobal levels.\n", "link": "http://arxiv.org/abs/2401.10474v2", "date": "2024-03-14", "relevancy": 2.5196, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5513}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4988}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4617}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LDReg%3A%20Local%20Dimensionality%20Regularized%20Self-Supervised%20Learning&body=Title%3A%20LDReg%3A%20Local%20Dimensionality%20Regularized%20Self-Supervised%20Learning%0AAuthor%3A%20Hanxun%20Huang%20and%20Ricardo%20J.%20G.%20B.%20Campello%20and%20Sarah%20Monazam%20Erfani%20and%20Xingjun%20Ma%20and%20Michael%20E.%20Houle%20and%20James%20Bailey%0AAbstract%3A%20%20%20Representations%20learned%20via%20self-supervised%20learning%20%28SSL%29%20can%20be%20susceptible%0Ato%20dimensional%20collapse%2C%20where%20the%20learned%20representation%20subspace%20is%20of%0Aextremely%20low%20dimensionality%20and%20thus%20fails%20to%20represent%20the%20full%20data%0Adistribution%20and%20modalities.%20Dimensional%20collapse%20also%20known%20as%20the%0A%22underfilling%22%20phenomenon%20is%20one%20of%20the%20major%20causes%20of%20degraded%20performance%20on%0Adownstream%20tasks.%20Previous%20work%20has%20investigated%20the%20dimensional%20collapse%0Aproblem%20of%20SSL%20at%20a%20global%20level.%20In%20this%20paper%2C%20we%20demonstrate%20that%0Arepresentations%20can%20span%20over%20high%20dimensional%20space%20globally%2C%20but%20collapse%0Alocally.%20To%20address%20this%2C%20we%20propose%20a%20method%20called%20%24%5Ctextit%7Blocal%0Adimensionality%20regularization%20%28LDReg%29%7D%24.%20Our%20formulation%20is%20based%20on%20the%0Aderivation%20of%20the%20Fisher-Rao%20metric%20to%20compare%20and%20optimize%20local%20distance%0Adistributions%20at%20an%20asymptotically%20small%20radius%20for%20each%20data%20point.%20By%0Aincreasing%20the%20local%20intrinsic%20dimensionality%2C%20we%20demonstrate%20through%20a%20range%0Aof%20experiments%20that%20LDReg%20improves%20the%20representation%20quality%20of%20SSL.%20The%0Aresults%20also%20show%20that%20LDReg%20can%20regularize%20dimensionality%20at%20both%20local%20and%0Aglobal%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10474v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDReg%3A%20Local%20Dimensionality%20Regularized%20Self-Supervised%20Learning&entry.906535625=Hanxun%20Huang%20and%20Ricardo%20J.%20G.%20B.%20Campello%20and%20Sarah%20Monazam%20Erfani%20and%20Xingjun%20Ma%20and%20Michael%20E.%20Houle%20and%20James%20Bailey&entry.1292438233=%20%20Representations%20learned%20via%20self-supervised%20learning%20%28SSL%29%20can%20be%20susceptible%0Ato%20dimensional%20collapse%2C%20where%20the%20learned%20representation%20subspace%20is%20of%0Aextremely%20low%20dimensionality%20and%20thus%20fails%20to%20represent%20the%20full%20data%0Adistribution%20and%20modalities.%20Dimensional%20collapse%20also%20known%20as%20the%0A%22underfilling%22%20phenomenon%20is%20one%20of%20the%20major%20causes%20of%20degraded%20performance%20on%0Adownstream%20tasks.%20Previous%20work%20has%20investigated%20the%20dimensional%20collapse%0Aproblem%20of%20SSL%20at%20a%20global%20level.%20In%20this%20paper%2C%20we%20demonstrate%20that%0Arepresentations%20can%20span%20over%20high%20dimensional%20space%20globally%2C%20but%20collapse%0Alocally.%20To%20address%20this%2C%20we%20propose%20a%20method%20called%20%24%5Ctextit%7Blocal%0Adimensionality%20regularization%20%28LDReg%29%7D%24.%20Our%20formulation%20is%20based%20on%20the%0Aderivation%20of%20the%20Fisher-Rao%20metric%20to%20compare%20and%20optimize%20local%20distance%0Adistributions%20at%20an%20asymptotically%20small%20radius%20for%20each%20data%20point.%20By%0Aincreasing%20the%20local%20intrinsic%20dimensionality%2C%20we%20demonstrate%20through%20a%20range%0Aof%20experiments%20that%20LDReg%20improves%20the%20representation%20quality%20of%20SSL.%20The%0Aresults%20also%20show%20that%20LDReg%20can%20regularize%20dimensionality%20at%20both%20local%20and%0Aglobal%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10474v2&entry.124074799=Read"},
{"title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks", "author": "Anna Bair and Hongxu Yin and Maying Shen and Pavlo Molchanov and Jose Alvarez", "abstract": "  Robustness and compactness are two essential attributes of deep learning\nmodels that are deployed in the real world. The goals of robustness and\ncompactness may seem to be at odds, since robustness requires generalization\nacross domains, while the process of compression exploits specificity in one\ndomain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\nthese goals through the lens of network sharpness. The AdaSAP method produces\nsparse networks that are robust to input variations which are unseen at\ntraining time. We achieve this by strategically incorporating weight\nperturbations in order to optimize the loss landscape. This allows the model to\nbe both primed for pruning and regularized for improved robustness. AdaSAP\nimproves the robust accuracy of pruned models on image classification by up to\n+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\ncorrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\ncriteria, and network architectures, outperforming recent pruning art by large\nmargins.\n", "link": "http://arxiv.org/abs/2306.14306v2", "date": "2024-03-13", "relevancy": 2.5011, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks&body=Title%3A%20Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks%0AAuthor%3A%20Anna%20Bair%20and%20Hongxu%20Yin%20and%20Maying%20Shen%20and%20Pavlo%20Molchanov%20and%20Jose%20Alvarez%0AAbstract%3A%20%20%20Robustness%20and%20compactness%20are%20two%20essential%20attributes%20of%20deep%20learning%0Amodels%20that%20are%20deployed%20in%20the%20real%20world.%20The%20goals%20of%20robustness%20and%0Acompactness%20may%20seem%20to%20be%20at%20odds%2C%20since%20robustness%20requires%20generalization%0Aacross%20domains%2C%20while%20the%20process%20of%20compression%20exploits%20specificity%20in%20one%0Adomain.%20We%20introduce%20Adaptive%20Sharpness-Aware%20Pruning%20%28AdaSAP%29%2C%20which%20unifies%0Athese%20goals%20through%20the%20lens%20of%20network%20sharpness.%20The%20AdaSAP%20method%20produces%0Asparse%20networks%20that%20are%20robust%20to%20input%20variations%20which%20are%20unseen%20at%0Atraining%20time.%20We%20achieve%20this%20by%20strategically%20incorporating%20weight%0Aperturbations%20in%20order%20to%20optimize%20the%20loss%20landscape.%20This%20allows%20the%20model%20to%0Abe%20both%20primed%20for%20pruning%20and%20regularized%20for%20improved%20robustness.%20AdaSAP%0Aimproves%20the%20robust%20accuracy%20of%20pruned%20models%20on%20image%20classification%20by%20up%20to%0A%2B6%25%20on%20ImageNet%20C%20and%20%2B4%25%20on%20ImageNet%20V2%2C%20and%20on%20object%20detection%20by%20%2B4%25%20on%20a%0Acorrupted%20Pascal%20VOC%20dataset%2C%20over%20a%20wide%20range%20of%20compression%20ratios%2C%20pruning%0Acriteria%2C%20and%20network%20architectures%2C%20outperforming%20recent%20pruning%20art%20by%20large%0Amargins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14306v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Sharpness-Aware%20Pruning%20for%20Robust%20Sparse%20Networks&entry.906535625=Anna%20Bair%20and%20Hongxu%20Yin%20and%20Maying%20Shen%20and%20Pavlo%20Molchanov%20and%20Jose%20Alvarez&entry.1292438233=%20%20Robustness%20and%20compactness%20are%20two%20essential%20attributes%20of%20deep%20learning%0Amodels%20that%20are%20deployed%20in%20the%20real%20world.%20The%20goals%20of%20robustness%20and%0Acompactness%20may%20seem%20to%20be%20at%20odds%2C%20since%20robustness%20requires%20generalization%0Aacross%20domains%2C%20while%20the%20process%20of%20compression%20exploits%20specificity%20in%20one%0Adomain.%20We%20introduce%20Adaptive%20Sharpness-Aware%20Pruning%20%28AdaSAP%29%2C%20which%20unifies%0Athese%20goals%20through%20the%20lens%20of%20network%20sharpness.%20The%20AdaSAP%20method%20produces%0Asparse%20networks%20that%20are%20robust%20to%20input%20variations%20which%20are%20unseen%20at%0Atraining%20time.%20We%20achieve%20this%20by%20strategically%20incorporating%20weight%0Aperturbations%20in%20order%20to%20optimize%20the%20loss%20landscape.%20This%20allows%20the%20model%20to%0Abe%20both%20primed%20for%20pruning%20and%20regularized%20for%20improved%20robustness.%20AdaSAP%0Aimproves%20the%20robust%20accuracy%20of%20pruned%20models%20on%20image%20classification%20by%20up%20to%0A%2B6%25%20on%20ImageNet%20C%20and%20%2B4%25%20on%20ImageNet%20V2%2C%20and%20on%20object%20detection%20by%20%2B4%25%20on%20a%0Acorrupted%20Pascal%20VOC%20dataset%2C%20over%20a%20wide%20range%20of%20compression%20ratios%2C%20pruning%0Acriteria%2C%20and%20network%20architectures%2C%20outperforming%20recent%20pruning%20art%20by%20large%0Amargins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14306v2&entry.124074799=Read"},
{"title": "Adversarial Fine-tuning of Compressed Neural Networks for Joint\n  Improvement of Robustness and Efficiency", "author": "Hallgrimur Thorsteinsson and Valdemar J Henriksen and Tong Chen and Raghavendra Selvan", "abstract": "  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n", "link": "http://arxiv.org/abs/2403.09441v1", "date": "2024-03-14", "relevancy": 2.5002, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4996}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4963}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency&body=Title%3A%20Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency%0AAuthor%3A%20Hallgrimur%20Thorsteinsson%20and%20Valdemar%20J%20Henriksen%20and%20Tong%20Chen%20and%20Raghavendra%20Selvan%0AAbstract%3A%20%20%20As%20deep%20learning%20%28DL%29%20models%20are%20increasingly%20being%20integrated%20into%20our%0Aeveryday%20lives%2C%20ensuring%20their%20safety%20by%20making%20them%20robust%20against%20adversarial%0Aattacks%20has%20become%20increasingly%20critical.%20DL%20models%20have%20been%20found%20to%20be%0Asusceptible%20to%20adversarial%20attacks%20which%20can%20be%20achieved%20by%20introducing%20small%2C%0Atargeted%20perturbations%20to%20disrupt%20the%20input%20data.%20Adversarial%20training%20has%20been%0Apresented%20as%20a%20mitigation%20strategy%20which%20can%20result%20in%20more%20robust%20models.%20This%0Aadversarial%20robustness%20comes%20with%20additional%20computational%20costs%20required%20to%0Adesign%20adversarial%20attacks%20during%20training.%20The%20two%20objectives%20--%20adversarial%0Arobustness%20and%20computational%20efficiency%20--%20then%20appear%20to%20be%20in%20conflict%20of%0Aeach%20other.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20two%20different%20model%0Acompression%20methods%20--%20structured%20weight%20pruning%20and%20quantization%20--%20on%0Aadversarial%20robustness.%20We%20specifically%20explore%20the%20effects%20of%20fine-tuning%20on%0Acompressed%20models%2C%20and%20present%20the%20trade-off%20between%20standard%20fine-tuning%20and%0Aadversarial%20fine-tuning.%20Our%20results%20show%20that%20compression%20does%20not%20inherently%0Alead%20to%20loss%20in%20model%20robustness%20and%20adversarial%20fine-tuning%20of%20a%20compressed%0Amodel%20can%20yield%20large%20improvement%20to%20the%20robustness%20performance%20of%20models.%20We%0Apresent%20experiments%20on%20two%20benchmark%20datasets%20showing%20that%20adversarial%0Afine-tuning%20of%20compressed%20models%20can%20achieve%20robustness%20performance%20comparable%0Ato%20adversarially%20trained%20models%2C%20while%20also%20improving%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09441v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Fine-tuning%20of%20Compressed%20Neural%20Networks%20for%20Joint%0A%20%20Improvement%20of%20Robustness%20and%20Efficiency&entry.906535625=Hallgrimur%20Thorsteinsson%20and%20Valdemar%20J%20Henriksen%20and%20Tong%20Chen%20and%20Raghavendra%20Selvan&entry.1292438233=%20%20As%20deep%20learning%20%28DL%29%20models%20are%20increasingly%20being%20integrated%20into%20our%0Aeveryday%20lives%2C%20ensuring%20their%20safety%20by%20making%20them%20robust%20against%20adversarial%0Aattacks%20has%20become%20increasingly%20critical.%20DL%20models%20have%20been%20found%20to%20be%0Asusceptible%20to%20adversarial%20attacks%20which%20can%20be%20achieved%20by%20introducing%20small%2C%0Atargeted%20perturbations%20to%20disrupt%20the%20input%20data.%20Adversarial%20training%20has%20been%0Apresented%20as%20a%20mitigation%20strategy%20which%20can%20result%20in%20more%20robust%20models.%20This%0Aadversarial%20robustness%20comes%20with%20additional%20computational%20costs%20required%20to%0Adesign%20adversarial%20attacks%20during%20training.%20The%20two%20objectives%20--%20adversarial%0Arobustness%20and%20computational%20efficiency%20--%20then%20appear%20to%20be%20in%20conflict%20of%0Aeach%20other.%20In%20this%20work%2C%20we%20explore%20the%20effects%20of%20two%20different%20model%0Acompression%20methods%20--%20structured%20weight%20pruning%20and%20quantization%20--%20on%0Aadversarial%20robustness.%20We%20specifically%20explore%20the%20effects%20of%20fine-tuning%20on%0Acompressed%20models%2C%20and%20present%20the%20trade-off%20between%20standard%20fine-tuning%20and%0Aadversarial%20fine-tuning.%20Our%20results%20show%20that%20compression%20does%20not%20inherently%0Alead%20to%20loss%20in%20model%20robustness%20and%20adversarial%20fine-tuning%20of%20a%20compressed%0Amodel%20can%20yield%20large%20improvement%20to%20the%20robustness%20performance%20of%20models.%20We%0Apresent%20experiments%20on%20two%20benchmark%20datasets%20showing%20that%20adversarial%0Afine-tuning%20of%20compressed%20models%20can%20achieve%20robustness%20performance%20comparable%0Ato%20adversarially%20trained%20models%2C%20while%20also%20improving%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09441v1&entry.124074799=Read"},
{"title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes", "author": "Avyav Kumar Singh and Ekaterina Shutova and Helen Yannakoudakis", "abstract": "  Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2210.17437v3", "date": "2024-03-14", "relevancy": 2.4971, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4943}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes&body=Title%3A%20Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes%0AAuthor%3A%20Avyav%20Kumar%20Singh%20and%20Ekaterina%20Shutova%20and%20Helen%20Yannakoudakis%0AAbstract%3A%20%20%20Existing%20approaches%20to%20few-shot%20learning%20in%20NLP%20rely%20on%20large%20language%20models%0Aand%20fine-tuning%20of%20these%20to%20generalise%20on%20out-of-distribution%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20%22extreme%22%20few-shot%20learning%2C%0Awherein%20models%20are%20exposed%20to%20as%20little%20as%204%20examples%20per%20class%2C%20based%20on%0Asoft-label%20prototypes%20that%20collectively%20capture%20the%20distribution%20of%20different%0Aclasses%20across%20the%20input%20domain%20space.%20Inspired%20by%20previous%20work%20%28Sucholutsky%0Aet%20al.%2C%202021%29%20on%20univariate%20or%20simple%20multivariate%20%28synthetic%29%20data%2C%20we%20propose%0Aa%20novel%20approach%20that%20is%20effective%20on%20large%2C%20high-dimensional%20and%20real-world%0Adatasets.%20We%20learn%20soft-label%20prototypes%20within%20a%20neural%20framework%20%28DeepSLP%29%0Aand%20we%20experimentally%20demonstrate%20that%20it%20achieves%20superior%20performance%20on%0A31/48%20tested%20tasks%20and%20few-shot%20settings%20while%20closely%20matching%20the%20performance%0Aof%20strong%20baselines%20on%20the%20rest.%20We%20focus%20on%20learning%20previously%20unseen%20NLP%0Atasks%20from%20very%20few%20examples%20%284%2C%208%2C%2016%29%20per%20label%20and%20present%20an%20in-depth%0Aanalysis%20of%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.17437v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20New%20Tasks%20from%20a%20Few%20Examples%20with%20Soft-Label%20Prototypes&entry.906535625=Avyav%20Kumar%20Singh%20and%20Ekaterina%20Shutova%20and%20Helen%20Yannakoudakis&entry.1292438233=%20%20Existing%20approaches%20to%20few-shot%20learning%20in%20NLP%20rely%20on%20large%20language%20models%0Aand%20fine-tuning%20of%20these%20to%20generalise%20on%20out-of-distribution%20data.%20In%20this%0Awork%2C%20we%20propose%20a%20simple%20yet%20powerful%20approach%20to%20%22extreme%22%20few-shot%20learning%2C%0Awherein%20models%20are%20exposed%20to%20as%20little%20as%204%20examples%20per%20class%2C%20based%20on%0Asoft-label%20prototypes%20that%20collectively%20capture%20the%20distribution%20of%20different%0Aclasses%20across%20the%20input%20domain%20space.%20Inspired%20by%20previous%20work%20%28Sucholutsky%0Aet%20al.%2C%202021%29%20on%20univariate%20or%20simple%20multivariate%20%28synthetic%29%20data%2C%20we%20propose%0Aa%20novel%20approach%20that%20is%20effective%20on%20large%2C%20high-dimensional%20and%20real-world%0Adatasets.%20We%20learn%20soft-label%20prototypes%20within%20a%20neural%20framework%20%28DeepSLP%29%0Aand%20we%20experimentally%20demonstrate%20that%20it%20achieves%20superior%20performance%20on%0A31/48%20tested%20tasks%20and%20few-shot%20settings%20while%20closely%20matching%20the%20performance%0Aof%20strong%20baselines%20on%20the%20rest.%20We%20focus%20on%20learning%20previously%20unseen%20NLP%0Atasks%20from%20very%20few%20examples%20%284%2C%208%2C%2016%29%20per%20label%20and%20present%20an%20in-depth%0Aanalysis%20of%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.17437v3&entry.124074799=Read"},
{"title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting", "author": "Jaewoo Jung and Jisang Han and Honggyu An and Jiwon Kang and Seonghoon Park and Seungryong Kim", "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When trained with randomly initialized\npoint clouds, 3DGS fails to maintain its ability to produce high-quality\nimages, undergoing large performance drops of 4-5 dB in PSNR. Through extensive\nanalysis of SfM initialization in the frequency domain and analysis of a 1D\nregression task with multiple 1D Gaussians, we propose a novel optimization\nstrategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D\nGaussian Splatting), that successfully trains 3D Gaussians from random point\nclouds. We show the effectiveness of our strategy through quantitative and\nqualitative comparisons on multiple datasets, largely improving the performance\nin all settings. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n", "link": "http://arxiv.org/abs/2403.09413v1", "date": "2024-03-14", "relevancy": 2.4952, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&body=Title%3A%20Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20trained%20with%20randomly%20initialized%0Apoint%20clouds%2C%203DGS%20fails%20to%20maintain%20its%20ability%20to%20produce%20high-quality%0Aimages%2C%20undergoing%20large%20performance%20drops%20of%204-5%20dB%20in%20PSNR.%20Through%20extensive%0Aanalysis%20of%20SfM%20initialization%20in%20the%20frequency%20domain%20and%20analysis%20of%20a%201D%0Aregression%20task%20with%20multiple%201D%20Gaussians%2C%20we%20propose%20a%20novel%20optimization%0Astrategy%20dubbed%20RAIN-GS%20%28Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%0AGaussian%20Splatting%29%2C%20that%20successfully%20trains%203D%20Gaussians%20from%20random%20point%0Aclouds.%20We%20show%20the%20effectiveness%20of%20our%20strategy%20through%20quantitative%20and%0Aqualitative%20comparisons%20on%20multiple%20datasets%2C%20largely%20improving%20the%20performance%0Ain%20all%20settings.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09413v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%20Gaussian%20Splatting&entry.906535625=Jaewoo%20Jung%20and%20Jisang%20Han%20and%20Honggyu%20An%20and%20Jiwon%20Kang%20and%20Seonghoon%20Park%20and%20Seungryong%20Kim&entry.1292438233=%20%203D%20Gaussian%20splatting%20%283DGS%29%20has%20recently%20demonstrated%20impressive%0Acapabilities%20in%20real-time%20novel%20view%20synthesis%20and%203D%20reconstruction.%20However%2C%0A3DGS%20heavily%20depends%20on%20the%20accurate%20initialization%20derived%20from%0AStructure-from-Motion%20%28SfM%29%20methods.%20When%20trained%20with%20randomly%20initialized%0Apoint%20clouds%2C%203DGS%20fails%20to%20maintain%20its%20ability%20to%20produce%20high-quality%0Aimages%2C%20undergoing%20large%20performance%20drops%20of%204-5%20dB%20in%20PSNR.%20Through%20extensive%0Aanalysis%20of%20SfM%20initialization%20in%20the%20frequency%20domain%20and%20analysis%20of%20a%201D%0Aregression%20task%20with%20multiple%201D%20Gaussians%2C%20we%20propose%20a%20novel%20optimization%0Astrategy%20dubbed%20RAIN-GS%20%28Relaxing%20Accurate%20Initialization%20Constraint%20for%203D%0AGaussian%20Splatting%29%2C%20that%20successfully%20trains%203D%20Gaussians%20from%20random%20point%0Aclouds.%20We%20show%20the%20effectiveness%20of%20our%20strategy%20through%20quantitative%20and%0Aqualitative%20comparisons%20on%20multiple%20datasets%2C%20largely%20improving%20the%20performance%0Ain%20all%20settings.%20Our%20project%20page%20and%20code%20can%20be%20found%20at%0Ahttps%3A//ku-cvlab.github.io/RAIN-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09413v1&entry.124074799=Read"},
{"title": "CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise\n  Classification", "author": "Yiming Ma and Victor Sanchez and Tanaya Guha", "abstract": "  The CLIP (Contrastive Language-Image Pretraining) model has exhibited\noutstanding performance in recognition problems, such as zero-shot image\nclassification and object detection. However, its ability to count remains\nunderstudied due to the inherent challenges of transforming counting--a\nregression task--into a recognition task. In this paper, we investigate CLIP's\npotential in counting, focusing specifically on estimating crowd sizes.\nExisting classification-based crowd-counting methods have encountered issues,\nincluding inappropriate discretization strategies, which impede the application\nof CLIP and result in suboptimal performance. To address these challenges, we\npropose the Enhanced Blockwise Classification (EBC) framework. In contrast to\nprevious methods, EBC relies on integer-valued bins that facilitate the\nlearning of robust decision boundaries. Within our model-agnostic EBC\nframework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting\nmodel capable of generating density maps. Comprehensive evaluations across\ndiverse crowd-counting datasets demonstrate the state-of-the-art performance of\nour methods. Particularly, EBC can improve existing models by up to 76.9%.\nMoreover, our CLIP-EBC model surpasses current crowd-counting methods,\nachieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part\nB datasets, respectively. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.09281v1", "date": "2024-03-14", "relevancy": 2.4909, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5148}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4938}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.486}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&body=Title%3A%20CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification%0AAuthor%3A%20Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha%0AAbstract%3A%20%20%20The%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20has%20exhibited%0Aoutstanding%20performance%20in%20recognition%20problems%2C%20such%20as%20zero-shot%20image%0Aclassification%20and%20object%20detection.%20However%2C%20its%20ability%20to%20count%20remains%0Aunderstudied%20due%20to%20the%20inherent%20challenges%20of%20transforming%20counting--a%0Aregression%20task--into%20a%20recognition%20task.%20In%20this%20paper%2C%20we%20investigate%20CLIP%27s%0Apotential%20in%20counting%2C%20focusing%20specifically%20on%20estimating%20crowd%20sizes.%0AExisting%20classification-based%20crowd-counting%20methods%20have%20encountered%20issues%2C%0Aincluding%20inappropriate%20discretization%20strategies%2C%20which%20impede%20the%20application%0Aof%20CLIP%20and%20result%20in%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Enhanced%20Blockwise%20Classification%20%28EBC%29%20framework.%20In%20contrast%20to%0Aprevious%20methods%2C%20EBC%20relies%20on%20integer-valued%20bins%20that%20facilitate%20the%0Alearning%20of%20robust%20decision%20boundaries.%20Within%20our%20model-agnostic%20EBC%0Aframework%2C%20we%20introduce%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20crowd-counting%0Amodel%20capable%20of%20generating%20density%20maps.%20Comprehensive%20evaluations%20across%0Adiverse%20crowd-counting%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20methods.%20Particularly%2C%20EBC%20can%20improve%20existing%20models%20by%20up%20to%2076.9%25.%0AMoreover%2C%20our%20CLIP-EBC%20model%20surpasses%20current%20crowd-counting%20methods%2C%0Aachieving%20mean%20absolute%20errors%20of%2055.0%20and%206.3%20on%20ShanghaiTech%20part%20A%20and%20part%0AB%20datasets%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09281v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-EBC%3A%20CLIP%20Can%20Count%20Accurately%20through%20Enhanced%20Blockwise%0A%20%20Classification&entry.906535625=Yiming%20Ma%20and%20Victor%20Sanchez%20and%20Tanaya%20Guha&entry.1292438233=%20%20The%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20has%20exhibited%0Aoutstanding%20performance%20in%20recognition%20problems%2C%20such%20as%20zero-shot%20image%0Aclassification%20and%20object%20detection.%20However%2C%20its%20ability%20to%20count%20remains%0Aunderstudied%20due%20to%20the%20inherent%20challenges%20of%20transforming%20counting--a%0Aregression%20task--into%20a%20recognition%20task.%20In%20this%20paper%2C%20we%20investigate%20CLIP%27s%0Apotential%20in%20counting%2C%20focusing%20specifically%20on%20estimating%20crowd%20sizes.%0AExisting%20classification-based%20crowd-counting%20methods%20have%20encountered%20issues%2C%0Aincluding%20inappropriate%20discretization%20strategies%2C%20which%20impede%20the%20application%0Aof%20CLIP%20and%20result%20in%20suboptimal%20performance.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Enhanced%20Blockwise%20Classification%20%28EBC%29%20framework.%20In%20contrast%20to%0Aprevious%20methods%2C%20EBC%20relies%20on%20integer-valued%20bins%20that%20facilitate%20the%0Alearning%20of%20robust%20decision%20boundaries.%20Within%20our%20model-agnostic%20EBC%0Aframework%2C%20we%20introduce%20CLIP-EBC%2C%20the%20first%20fully%20CLIP-based%20crowd-counting%0Amodel%20capable%20of%20generating%20density%20maps.%20Comprehensive%20evaluations%20across%0Adiverse%20crowd-counting%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%0Aour%20methods.%20Particularly%2C%20EBC%20can%20improve%20existing%20models%20by%20up%20to%2076.9%25.%0AMoreover%2C%20our%20CLIP-EBC%20model%20surpasses%20current%20crowd-counting%20methods%2C%0Aachieving%20mean%20absolute%20errors%20of%2055.0%20and%206.3%20on%20ShanghaiTech%20part%20A%20and%20part%0AB%20datasets%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09281v1&entry.124074799=Read"},
{"title": "Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training", "author": "Yanlai Yang and Matt Jones and Michael C. Mozer and Mengye Ren", "abstract": "  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n", "link": "http://arxiv.org/abs/2403.09613v1", "date": "2024-03-14", "relevancy": 2.4908, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training&body=Title%3A%20Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training%0AAuthor%3A%20Yanlai%20Yang%20and%20Matt%20Jones%20and%20Michael%20C.%20Mozer%20and%20Mengye%20Ren%0AAbstract%3A%20%20%20We%20explore%20the%20training%20dynamics%20of%20neural%20networks%20in%20a%20structured%20non-IID%0Asetting%20where%20documents%20are%20presented%20cyclically%20in%20a%20fixed%2C%20repeated%20sequence.%0ATypically%2C%20networks%20suffer%20from%20catastrophic%20interference%20when%20training%20on%20a%0Asequence%20of%20documents%3B%20however%2C%20we%20discover%20a%20curious%20and%20remarkable%20property%0Aof%20LLMs%20fine-tuned%20sequentially%20in%20this%20setting%3A%20they%20exhibit%20anticipatory%0Abehavior%2C%20recovering%20from%20the%20forgetting%20on%20documents%20before%20encountering%20them%0Aagain.%20The%20behavior%20emerges%20and%20becomes%20more%20robust%20as%20the%20architecture%20scales%0Aup%20its%20number%20of%20parameters.%20Through%20comprehensive%20experiments%20and%0Avisualizations%2C%20we%20uncover%20new%20insights%20into%20training%20over-parameterized%0Anetworks%20in%20structured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09613v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reawakening%20knowledge%3A%20Anticipatory%20recovery%20from%20catastrophic%0A%20%20interference%20via%20structured%20training&entry.906535625=Yanlai%20Yang%20and%20Matt%20Jones%20and%20Michael%20C.%20Mozer%20and%20Mengye%20Ren&entry.1292438233=%20%20We%20explore%20the%20training%20dynamics%20of%20neural%20networks%20in%20a%20structured%20non-IID%0Asetting%20where%20documents%20are%20presented%20cyclically%20in%20a%20fixed%2C%20repeated%20sequence.%0ATypically%2C%20networks%20suffer%20from%20catastrophic%20interference%20when%20training%20on%20a%0Asequence%20of%20documents%3B%20however%2C%20we%20discover%20a%20curious%20and%20remarkable%20property%0Aof%20LLMs%20fine-tuned%20sequentially%20in%20this%20setting%3A%20they%20exhibit%20anticipatory%0Abehavior%2C%20recovering%20from%20the%20forgetting%20on%20documents%20before%20encountering%20them%0Aagain.%20The%20behavior%20emerges%20and%20becomes%20more%20robust%20as%20the%20architecture%20scales%0Aup%20its%20number%20of%20parameters.%20Through%20comprehensive%20experiments%20and%0Avisualizations%2C%20we%20uncover%20new%20insights%20into%20training%20over-parameterized%0Anetworks%20in%20structured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09613v1&entry.124074799=Read"},
{"title": "CardioCaps: Attention-based Capsule Network for Class-Imbalanced\n  Echocardiogram Classification", "author": "Hyunkyung Han and Jihyeon Seong and Jaesik Choi", "abstract": "  Capsule Neural Networks (CapsNets) is a novel architecture that utilizes\nvector-wise representations formed by multiple neurons. Specifically, the\nDynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic\nrouting mechanism to train capsules and acquire translation-equivariance\nproperties, enhancing its robustness compared to traditional Convolutional\nNeural Networks (CNNs). Echocardiograms, which capture moving images of the\nheart, present unique challenges for traditional image classification methods.\nIn this paper, we explore the potential of DR-CapsNets and propose CardioCaps,\na novel attention-based DR-CapsNet architecture for class-imbalanced\nechocardiogram classification. CardioCaps comprises two key components: a\nweighted margin loss incorporating a regression auxiliary loss and an attention\nmechanism. First, the weighted margin loss prioritizes positive cases,\nsupplemented by an auxiliary loss function based on the Ejection Fraction (EF)\nregression task, a crucial measure of cardiac function. This approach enhances\nthe model's resilience in the face of class imbalance. Second, recognizing the\nquadratic complexity of dynamic routing leading to training inefficiencies, we\nadopt the attention mechanism as a more computationally efficient alternative.\nOur results demonstrate that CardioCaps surpasses traditional machine learning\nbaseline methods, including Logistic Regression, Random Forest, and XGBoost\nwith sampling methods and a class weight matrix. Furthermore, CardioCaps\noutperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets,\nand ViTs, as well as advanced CapsNets methods such as EM-CapsNets and\nEfficient-CapsNets. Notably, our model demonstrates robustness to class\nimbalance, achieving high precision even in datasets with a substantial\nproportion of negative cases.\n", "link": "http://arxiv.org/abs/2403.09108v1", "date": "2024-03-14", "relevancy": 2.4822, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5288}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4925}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.468}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CardioCaps%3A%20Attention-based%20Capsule%20Network%20for%20Class-Imbalanced%0A%20%20Echocardiogram%20Classification&body=Title%3A%20CardioCaps%3A%20Attention-based%20Capsule%20Network%20for%20Class-Imbalanced%0A%20%20Echocardiogram%20Classification%0AAuthor%3A%20Hyunkyung%20Han%20and%20Jihyeon%20Seong%20and%20Jaesik%20Choi%0AAbstract%3A%20%20%20Capsule%20Neural%20Networks%20%28CapsNets%29%20is%20a%20novel%20architecture%20that%20utilizes%0Avector-wise%20representations%20formed%20by%20multiple%20neurons.%20Specifically%2C%20the%0ADynamic%20Routing%20CapsNets%20%28DR-CapsNets%29%20employ%20an%20affine%20matrix%20and%20dynamic%0Arouting%20mechanism%20to%20train%20capsules%20and%20acquire%20translation-equivariance%0Aproperties%2C%20enhancing%20its%20robustness%20compared%20to%20traditional%20Convolutional%0ANeural%20Networks%20%28CNNs%29.%20Echocardiograms%2C%20which%20capture%20moving%20images%20of%20the%0Aheart%2C%20present%20unique%20challenges%20for%20traditional%20image%20classification%20methods.%0AIn%20this%20paper%2C%20we%20explore%20the%20potential%20of%20DR-CapsNets%20and%20propose%20CardioCaps%2C%0Aa%20novel%20attention-based%20DR-CapsNet%20architecture%20for%20class-imbalanced%0Aechocardiogram%20classification.%20CardioCaps%20comprises%20two%20key%20components%3A%20a%0Aweighted%20margin%20loss%20incorporating%20a%20regression%20auxiliary%20loss%20and%20an%20attention%0Amechanism.%20First%2C%20the%20weighted%20margin%20loss%20prioritizes%20positive%20cases%2C%0Asupplemented%20by%20an%20auxiliary%20loss%20function%20based%20on%20the%20Ejection%20Fraction%20%28EF%29%0Aregression%20task%2C%20a%20crucial%20measure%20of%20cardiac%20function.%20This%20approach%20enhances%0Athe%20model%27s%20resilience%20in%20the%20face%20of%20class%20imbalance.%20Second%2C%20recognizing%20the%0Aquadratic%20complexity%20of%20dynamic%20routing%20leading%20to%20training%20inefficiencies%2C%20we%0Aadopt%20the%20attention%20mechanism%20as%20a%20more%20computationally%20efficient%20alternative.%0AOur%20results%20demonstrate%20that%20CardioCaps%20surpasses%20traditional%20machine%20learning%0Abaseline%20methods%2C%20including%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%20XGBoost%0Awith%20sampling%20methods%20and%20a%20class%20weight%20matrix.%20Furthermore%2C%20CardioCaps%0Aoutperforms%20other%20deep%20learning%20baseline%20methods%20such%20as%20CNNs%2C%20ResNets%2C%20U-Nets%2C%0Aand%20ViTs%2C%20as%20well%20as%20advanced%20CapsNets%20methods%20such%20as%20EM-CapsNets%20and%0AEfficient-CapsNets.%20Notably%2C%20our%20model%20demonstrates%20robustness%20to%20class%0Aimbalance%2C%20achieving%20high%20precision%20even%20in%20datasets%20with%20a%20substantial%0Aproportion%20of%20negative%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09108v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardioCaps%3A%20Attention-based%20Capsule%20Network%20for%20Class-Imbalanced%0A%20%20Echocardiogram%20Classification&entry.906535625=Hyunkyung%20Han%20and%20Jihyeon%20Seong%20and%20Jaesik%20Choi&entry.1292438233=%20%20Capsule%20Neural%20Networks%20%28CapsNets%29%20is%20a%20novel%20architecture%20that%20utilizes%0Avector-wise%20representations%20formed%20by%20multiple%20neurons.%20Specifically%2C%20the%0ADynamic%20Routing%20CapsNets%20%28DR-CapsNets%29%20employ%20an%20affine%20matrix%20and%20dynamic%0Arouting%20mechanism%20to%20train%20capsules%20and%20acquire%20translation-equivariance%0Aproperties%2C%20enhancing%20its%20robustness%20compared%20to%20traditional%20Convolutional%0ANeural%20Networks%20%28CNNs%29.%20Echocardiograms%2C%20which%20capture%20moving%20images%20of%20the%0Aheart%2C%20present%20unique%20challenges%20for%20traditional%20image%20classification%20methods.%0AIn%20this%20paper%2C%20we%20explore%20the%20potential%20of%20DR-CapsNets%20and%20propose%20CardioCaps%2C%0Aa%20novel%20attention-based%20DR-CapsNet%20architecture%20for%20class-imbalanced%0Aechocardiogram%20classification.%20CardioCaps%20comprises%20two%20key%20components%3A%20a%0Aweighted%20margin%20loss%20incorporating%20a%20regression%20auxiliary%20loss%20and%20an%20attention%0Amechanism.%20First%2C%20the%20weighted%20margin%20loss%20prioritizes%20positive%20cases%2C%0Asupplemented%20by%20an%20auxiliary%20loss%20function%20based%20on%20the%20Ejection%20Fraction%20%28EF%29%0Aregression%20task%2C%20a%20crucial%20measure%20of%20cardiac%20function.%20This%20approach%20enhances%0Athe%20model%27s%20resilience%20in%20the%20face%20of%20class%20imbalance.%20Second%2C%20recognizing%20the%0Aquadratic%20complexity%20of%20dynamic%20routing%20leading%20to%20training%20inefficiencies%2C%20we%0Aadopt%20the%20attention%20mechanism%20as%20a%20more%20computationally%20efficient%20alternative.%0AOur%20results%20demonstrate%20that%20CardioCaps%20surpasses%20traditional%20machine%20learning%0Abaseline%20methods%2C%20including%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%20XGBoost%0Awith%20sampling%20methods%20and%20a%20class%20weight%20matrix.%20Furthermore%2C%20CardioCaps%0Aoutperforms%20other%20deep%20learning%20baseline%20methods%20such%20as%20CNNs%2C%20ResNets%2C%20U-Nets%2C%0Aand%20ViTs%2C%20as%20well%20as%20advanced%20CapsNets%20methods%20such%20as%20EM-CapsNets%20and%0AEfficient-CapsNets.%20Notably%2C%20our%20model%20demonstrates%20robustness%20to%20class%0Aimbalance%2C%20achieving%20high%20precision%20even%20in%20datasets%20with%20a%20substantial%0Aproportion%20of%20negative%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09108v1&entry.124074799=Read"},
{"title": "Explorations in Texture Learning", "author": "Blaine Hoak and Patrick McDaniel", "abstract": "  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n", "link": "http://arxiv.org/abs/2403.09543v1", "date": "2024-03-14", "relevancy": 2.4816, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4957}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4636}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explorations%20in%20Texture%20Learning&body=Title%3A%20Explorations%20in%20Texture%20Learning%0AAuthor%3A%20Blaine%20Hoak%20and%20Patrick%20McDaniel%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20%5Ctextit%7Btexture%20learning%7D%3A%20the%20identification%20of%0Atextures%20learned%20by%20object%20classification%20models%2C%20and%20the%20extent%20to%20which%20they%0Arely%20on%20these%20textures.%20We%20build%20texture-object%20associations%20that%20uncover%20new%0Ainsights%20about%20the%20relationships%20between%20texture%20and%20object%20classes%20in%20CNNs%20and%0Afind%20three%20classes%20of%20results%3A%20associations%20that%20are%20strong%20and%20expected%2C%0Astrong%20and%20not%20expected%2C%20and%20expected%20but%20not%20present.%20Our%20analysis%0Ademonstrates%20that%20investigations%20in%20texture%20learning%20enable%20new%20methods%20for%0Ainterpretability%20and%20have%20the%20potential%20to%20uncover%20unexpected%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09543v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorations%20in%20Texture%20Learning&entry.906535625=Blaine%20Hoak%20and%20Patrick%20McDaniel&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20%5Ctextit%7Btexture%20learning%7D%3A%20the%20identification%20of%0Atextures%20learned%20by%20object%20classification%20models%2C%20and%20the%20extent%20to%20which%20they%0Arely%20on%20these%20textures.%20We%20build%20texture-object%20associations%20that%20uncover%20new%0Ainsights%20about%20the%20relationships%20between%20texture%20and%20object%20classes%20in%20CNNs%20and%0Afind%20three%20classes%20of%20results%3A%20associations%20that%20are%20strong%20and%20expected%2C%0Astrong%20and%20not%20expected%2C%20and%20expected%20but%20not%20present.%20Our%20analysis%0Ademonstrates%20that%20investigations%20in%20texture%20learning%20enable%20new%20methods%20for%0Ainterpretability%20and%20have%20the%20potential%20to%20uncover%20unexpected%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09543v1&entry.124074799=Read"},
{"title": "Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve\n  Generalization Performance of Deep Classification Models", "author": "Mohammad Lashkari and Amin Gheibi", "abstract": "  The generalization performance of deep neural networks in classification\ntasks is a major concern in machine learning research. Despite widespread\ntechniques used to diminish the over-fitting issue such as data augmentation,\npseudo-labeling, regularization, and ensemble learning, this performance still\nneeds to be enhanced with other approaches. In recent years, it has been\ntheoretically demonstrated that the loss function characteristics i.e. its\nLipschitzness and maximum value affect the generalization performance of deep\nneural networks which can be utilized as a guidance to propose novel distance\nmeasures. In this paper, by analyzing the aforementioned characteristics, we\nintroduce a distance called Reduced Jeffries-Matusita as a loss function for\ntraining deep classification models to reduce the over-fitting issue. In our\nexperiments, we evaluate the new loss function in two different problems: image\nclassification in computer vision and node classification in the context of\ngraph learning. The results show that the new distance measure stabilizes the\ntraining process significantly, enhances the generalization ability, and\nimproves the performance of the models in the Accuracy and F1-score metrics,\neven if the training set size is small.\n", "link": "http://arxiv.org/abs/2403.08408v1", "date": "2024-03-13", "relevancy": 2.4799, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5011}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reduced%20Jeffries-Matusita%20distance%3A%20A%20Novel%20Loss%20Function%20to%20Improve%0A%20%20Generalization%20Performance%20of%20Deep%20Classification%20Models&body=Title%3A%20Reduced%20Jeffries-Matusita%20distance%3A%20A%20Novel%20Loss%20Function%20to%20Improve%0A%20%20Generalization%20Performance%20of%20Deep%20Classification%20Models%0AAuthor%3A%20Mohammad%20Lashkari%20and%20Amin%20Gheibi%0AAbstract%3A%20%20%20The%20generalization%20performance%20of%20deep%20neural%20networks%20in%20classification%0Atasks%20is%20a%20major%20concern%20in%20machine%20learning%20research.%20Despite%20widespread%0Atechniques%20used%20to%20diminish%20the%20over-fitting%20issue%20such%20as%20data%20augmentation%2C%0Apseudo-labeling%2C%20regularization%2C%20and%20ensemble%20learning%2C%20this%20performance%20still%0Aneeds%20to%20be%20enhanced%20with%20other%20approaches.%20In%20recent%20years%2C%20it%20has%20been%0Atheoretically%20demonstrated%20that%20the%20loss%20function%20characteristics%20i.e.%20its%0ALipschitzness%20and%20maximum%20value%20affect%20the%20generalization%20performance%20of%20deep%0Aneural%20networks%20which%20can%20be%20utilized%20as%20a%20guidance%20to%20propose%20novel%20distance%0Ameasures.%20In%20this%20paper%2C%20by%20analyzing%20the%20aforementioned%20characteristics%2C%20we%0Aintroduce%20a%20distance%20called%20Reduced%20Jeffries-Matusita%20as%20a%20loss%20function%20for%0Atraining%20deep%20classification%20models%20to%20reduce%20the%20over-fitting%20issue.%20In%20our%0Aexperiments%2C%20we%20evaluate%20the%20new%20loss%20function%20in%20two%20different%20problems%3A%20image%0Aclassification%20in%20computer%20vision%20and%20node%20classification%20in%20the%20context%20of%0Agraph%20learning.%20The%20results%20show%20that%20the%20new%20distance%20measure%20stabilizes%20the%0Atraining%20process%20significantly%2C%20enhances%20the%20generalization%20ability%2C%20and%0Aimproves%20the%20performance%20of%20the%20models%20in%20the%20Accuracy%20and%20F1-score%20metrics%2C%0Aeven%20if%20the%20training%20set%20size%20is%20small.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08408v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reduced%20Jeffries-Matusita%20distance%3A%20A%20Novel%20Loss%20Function%20to%20Improve%0A%20%20Generalization%20Performance%20of%20Deep%20Classification%20Models&entry.906535625=Mohammad%20Lashkari%20and%20Amin%20Gheibi&entry.1292438233=%20%20The%20generalization%20performance%20of%20deep%20neural%20networks%20in%20classification%0Atasks%20is%20a%20major%20concern%20in%20machine%20learning%20research.%20Despite%20widespread%0Atechniques%20used%20to%20diminish%20the%20over-fitting%20issue%20such%20as%20data%20augmentation%2C%0Apseudo-labeling%2C%20regularization%2C%20and%20ensemble%20learning%2C%20this%20performance%20still%0Aneeds%20to%20be%20enhanced%20with%20other%20approaches.%20In%20recent%20years%2C%20it%20has%20been%0Atheoretically%20demonstrated%20that%20the%20loss%20function%20characteristics%20i.e.%20its%0ALipschitzness%20and%20maximum%20value%20affect%20the%20generalization%20performance%20of%20deep%0Aneural%20networks%20which%20can%20be%20utilized%20as%20a%20guidance%20to%20propose%20novel%20distance%0Ameasures.%20In%20this%20paper%2C%20by%20analyzing%20the%20aforementioned%20characteristics%2C%20we%0Aintroduce%20a%20distance%20called%20Reduced%20Jeffries-Matusita%20as%20a%20loss%20function%20for%0Atraining%20deep%20classification%20models%20to%20reduce%20the%20over-fitting%20issue.%20In%20our%0Aexperiments%2C%20we%20evaluate%20the%20new%20loss%20function%20in%20two%20different%20problems%3A%20image%0Aclassification%20in%20computer%20vision%20and%20node%20classification%20in%20the%20context%20of%0Agraph%20learning.%20The%20results%20show%20that%20the%20new%20distance%20measure%20stabilizes%20the%0Atraining%20process%20significantly%2C%20enhances%20the%20generalization%20ability%2C%20and%0Aimproves%20the%20performance%20of%20the%20models%20in%20the%20Accuracy%20and%20F1-score%20metrics%2C%0Aeven%20if%20the%20training%20set%20size%20is%20small.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08408v1&entry.124074799=Read"},
{"title": "Data-Efficient Sleep Staging with Synthetic Time Series Pretraining", "author": "Niklas Grieger and Siamak Mehrkanoon and Stephan Bialonski", "abstract": "  Analyzing electroencephalographic (EEG) time series can be challenging,\nespecially with deep neural networks, due to the large variability among human\nsubjects and often small datasets. To address these challenges, various\nstrategies, such as self-supervised learning, have been suggested, but they\ntypically rely on extensive empirical datasets. Inspired by recent advances in\ncomputer vision, we propose a pretraining task termed \"frequency pretraining\"\nto pretrain a neural network for sleep staging by predicting the frequency\ncontent of randomly generated synthetic time series. Our experiments\ndemonstrate that our method surpasses fully supervised learning in scenarios\nwith limited data and few subjects, and matches its performance in regimes with\nmany subjects. Furthermore, our results underline the relevance of frequency\ninformation for sleep stage scoring, while also demonstrating that deep neural\nnetworks utilize information beyond frequencies to enhance sleep staging\nperformance, which is consistent with previous research. We anticipate that our\napproach will be advantageous across a broad spectrum of applications where EEG\ndata is limited or derived from a small number of subjects, including the\ndomain of brain-computer interfaces.\n", "link": "http://arxiv.org/abs/2403.08592v1", "date": "2024-03-13", "relevancy": 2.4649, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5384}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4694}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining&body=Title%3A%20Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining%0AAuthor%3A%20Niklas%20Grieger%20and%20Siamak%20Mehrkanoon%20and%20Stephan%20Bialonski%0AAbstract%3A%20%20%20Analyzing%20electroencephalographic%20%28EEG%29%20time%20series%20can%20be%20challenging%2C%0Aespecially%20with%20deep%20neural%20networks%2C%20due%20to%20the%20large%20variability%20among%20human%0Asubjects%20and%20often%20small%20datasets.%20To%20address%20these%20challenges%2C%20various%0Astrategies%2C%20such%20as%20self-supervised%20learning%2C%20have%20been%20suggested%2C%20but%20they%0Atypically%20rely%20on%20extensive%20empirical%20datasets.%20Inspired%20by%20recent%20advances%20in%0Acomputer%20vision%2C%20we%20propose%20a%20pretraining%20task%20termed%20%22frequency%20pretraining%22%0Ato%20pretrain%20a%20neural%20network%20for%20sleep%20staging%20by%20predicting%20the%20frequency%0Acontent%20of%20randomly%20generated%20synthetic%20time%20series.%20Our%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20fully%20supervised%20learning%20in%20scenarios%0Awith%20limited%20data%20and%20few%20subjects%2C%20and%20matches%20its%20performance%20in%20regimes%20with%0Amany%20subjects.%20Furthermore%2C%20our%20results%20underline%20the%20relevance%20of%20frequency%0Ainformation%20for%20sleep%20stage%20scoring%2C%20while%20also%20demonstrating%20that%20deep%20neural%0Anetworks%20utilize%20information%20beyond%20frequencies%20to%20enhance%20sleep%20staging%0Aperformance%2C%20which%20is%20consistent%20with%20previous%20research.%20We%20anticipate%20that%20our%0Aapproach%20will%20be%20advantageous%20across%20a%20broad%20spectrum%20of%20applications%20where%20EEG%0Adata%20is%20limited%20or%20derived%20from%20a%20small%20number%20of%20subjects%2C%20including%20the%0Adomain%20of%20brain-computer%20interfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08592v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Sleep%20Staging%20with%20Synthetic%20Time%20Series%20Pretraining&entry.906535625=Niklas%20Grieger%20and%20Siamak%20Mehrkanoon%20and%20Stephan%20Bialonski&entry.1292438233=%20%20Analyzing%20electroencephalographic%20%28EEG%29%20time%20series%20can%20be%20challenging%2C%0Aespecially%20with%20deep%20neural%20networks%2C%20due%20to%20the%20large%20variability%20among%20human%0Asubjects%20and%20often%20small%20datasets.%20To%20address%20these%20challenges%2C%20various%0Astrategies%2C%20such%20as%20self-supervised%20learning%2C%20have%20been%20suggested%2C%20but%20they%0Atypically%20rely%20on%20extensive%20empirical%20datasets.%20Inspired%20by%20recent%20advances%20in%0Acomputer%20vision%2C%20we%20propose%20a%20pretraining%20task%20termed%20%22frequency%20pretraining%22%0Ato%20pretrain%20a%20neural%20network%20for%20sleep%20staging%20by%20predicting%20the%20frequency%0Acontent%20of%20randomly%20generated%20synthetic%20time%20series.%20Our%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20fully%20supervised%20learning%20in%20scenarios%0Awith%20limited%20data%20and%20few%20subjects%2C%20and%20matches%20its%20performance%20in%20regimes%20with%0Amany%20subjects.%20Furthermore%2C%20our%20results%20underline%20the%20relevance%20of%20frequency%0Ainformation%20for%20sleep%20stage%20scoring%2C%20while%20also%20demonstrating%20that%20deep%20neural%0Anetworks%20utilize%20information%20beyond%20frequencies%20to%20enhance%20sleep%20staging%0Aperformance%2C%20which%20is%20consistent%20with%20previous%20research.%20We%20anticipate%20that%20our%0Aapproach%20will%20be%20advantageous%20across%20a%20broad%20spectrum%20of%20applications%20where%20EEG%0Adata%20is%20limited%20or%20derived%20from%20a%20small%20number%20of%20subjects%2C%20including%20the%0Adomain%20of%20brain-computer%20interfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08592v1&entry.124074799=Read"},
{"title": "WSI-SAM: Multi-resolution Segment Anything Model (SAM) for\n  histopathology whole-slide images", "author": "Hong Liu and Haosen Yang and Paul J. van Diest and Josien P. W. Pluim and Mitko Veta", "abstract": "  The Segment Anything Model (SAM) marks a significant advancement in\nsegmentation models, offering powerful zero-shot capabilities and dynamic\nprompting. However, existing medical SAMs are not suitable for the multi-scale\nnature of whole-slide images (WSIs), restricting their effectiveness. To\nresolve this drawback, we present WSI-SAM, enhancing SAM with precise object\nsegmentation capabilities for histopathology images using multi-resolution\npatches, while preserving its original prompt-driven design, efficiency, and\nzero-shot adaptability. To fully exploit pretrained knowledge while minimizing\ntraining overhead, we keep SAM frozen, only introducing minimal additional\nparameters and computation. In particular, we introduce High-Resolution (HR)\ntoken, Low-Resolution (LR) token and dual mask decoder. This decoder integrates\nthe original SAM mask decoder with a lightweight fusion module that integrates\nfeatures at multiple scales. Instead of predicting a mask independently, we\nintegrate HR and LR token at intermediate layer to jointly learn features of\nthe same object across multiple resolutions. Experiments show that our WSI-SAM\noutperforms state-of-the-art SAM and its variants. In particular, our model\noutperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ\n(DCIS) segmentation tasks and breast cancer metastasis segmentation task\n(CAMELYON16 dataset). The code will be available at\nhttps://github.com/HongLiuuuuu/WSI-SAM.\n", "link": "http://arxiv.org/abs/2403.09257v1", "date": "2024-03-14", "relevancy": 2.4647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images&body=Title%3A%20WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images%0AAuthor%3A%20Hong%20Liu%20and%20Haosen%20Yang%20and%20Paul%20J.%20van%20Diest%20and%20Josien%20P.%20W.%20Pluim%20and%20Mitko%20Veta%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20advancement%20in%0Asegmentation%20models%2C%20offering%20powerful%20zero-shot%20capabilities%20and%20dynamic%0Aprompting.%20However%2C%20existing%20medical%20SAMs%20are%20not%20suitable%20for%20the%20multi-scale%0Anature%20of%20whole-slide%20images%20%28WSIs%29%2C%20restricting%20their%20effectiveness.%20To%0Aresolve%20this%20drawback%2C%20we%20present%20WSI-SAM%2C%20enhancing%20SAM%20with%20precise%20object%0Asegmentation%20capabilities%20for%20histopathology%20images%20using%20multi-resolution%0Apatches%2C%20while%20preserving%20its%20original%20prompt-driven%20design%2C%20efficiency%2C%20and%0Azero-shot%20adaptability.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%0Atraining%20overhead%2C%20we%20keep%20SAM%20frozen%2C%20only%20introducing%20minimal%20additional%0Aparameters%20and%20computation.%20In%20particular%2C%20we%20introduce%20High-Resolution%20%28HR%29%0Atoken%2C%20Low-Resolution%20%28LR%29%20token%20and%20dual%20mask%20decoder.%20This%20decoder%20integrates%0Athe%20original%20SAM%20mask%20decoder%20with%20a%20lightweight%20fusion%20module%20that%20integrates%0Afeatures%20at%20multiple%20scales.%20Instead%20of%20predicting%20a%20mask%20independently%2C%20we%0Aintegrate%20HR%20and%20LR%20token%20at%20intermediate%20layer%20to%20jointly%20learn%20features%20of%0Athe%20same%20object%20across%20multiple%20resolutions.%20Experiments%20show%20that%20our%20WSI-SAM%0Aoutperforms%20state-of-the-art%20SAM%20and%20its%20variants.%20In%20particular%2C%20our%20model%0Aoutperforms%20SAM%20by%204.1%20and%202.5%20percent%20points%20on%20a%20ductal%20carcinoma%20in%20situ%0A%28DCIS%29%20segmentation%20tasks%20and%20breast%20cancer%20metastasis%20segmentation%20task%0A%28CAMELYON16%20dataset%29.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HongLiuuuuu/WSI-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09257v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WSI-SAM%3A%20Multi-resolution%20Segment%20Anything%20Model%20%28SAM%29%20for%0A%20%20histopathology%20whole-slide%20images&entry.906535625=Hong%20Liu%20and%20Haosen%20Yang%20and%20Paul%20J.%20van%20Diest%20and%20Josien%20P.%20W.%20Pluim%20and%20Mitko%20Veta&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20marks%20a%20significant%20advancement%20in%0Asegmentation%20models%2C%20offering%20powerful%20zero-shot%20capabilities%20and%20dynamic%0Aprompting.%20However%2C%20existing%20medical%20SAMs%20are%20not%20suitable%20for%20the%20multi-scale%0Anature%20of%20whole-slide%20images%20%28WSIs%29%2C%20restricting%20their%20effectiveness.%20To%0Aresolve%20this%20drawback%2C%20we%20present%20WSI-SAM%2C%20enhancing%20SAM%20with%20precise%20object%0Asegmentation%20capabilities%20for%20histopathology%20images%20using%20multi-resolution%0Apatches%2C%20while%20preserving%20its%20original%20prompt-driven%20design%2C%20efficiency%2C%20and%0Azero-shot%20adaptability.%20To%20fully%20exploit%20pretrained%20knowledge%20while%20minimizing%0Atraining%20overhead%2C%20we%20keep%20SAM%20frozen%2C%20only%20introducing%20minimal%20additional%0Aparameters%20and%20computation.%20In%20particular%2C%20we%20introduce%20High-Resolution%20%28HR%29%0Atoken%2C%20Low-Resolution%20%28LR%29%20token%20and%20dual%20mask%20decoder.%20This%20decoder%20integrates%0Athe%20original%20SAM%20mask%20decoder%20with%20a%20lightweight%20fusion%20module%20that%20integrates%0Afeatures%20at%20multiple%20scales.%20Instead%20of%20predicting%20a%20mask%20independently%2C%20we%0Aintegrate%20HR%20and%20LR%20token%20at%20intermediate%20layer%20to%20jointly%20learn%20features%20of%0Athe%20same%20object%20across%20multiple%20resolutions.%20Experiments%20show%20that%20our%20WSI-SAM%0Aoutperforms%20state-of-the-art%20SAM%20and%20its%20variants.%20In%20particular%2C%20our%20model%0Aoutperforms%20SAM%20by%204.1%20and%202.5%20percent%20points%20on%20a%20ductal%20carcinoma%20in%20situ%0A%28DCIS%29%20segmentation%20tasks%20and%20breast%20cancer%20metastasis%20segmentation%20task%0A%28CAMELYON16%20dataset%29.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/HongLiuuuuu/WSI-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09257v1&entry.124074799=Read"},
{"title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning", "author": "Zeju Qiu and Weiyang Liu and Haiwen Feng and Yuxuan Xue and Yao Feng and Zhen Liu and Dan Zhang and Adrian Weller and Bernhard Sch\u00f6lkopf", "abstract": "  Large text-to-image diffusion models have impressive capabilities in\ngenerating photorealistic images from text prompts. How to effectively guide or\ncontrol these powerful models to perform different downstream tasks becomes an\nimportant open problem. To tackle this challenge, we introduce a principled\nfinetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image\ndiffusion models to downstream tasks. Unlike existing methods, OFT can provably\npreserve hyperspherical energy which characterizes the pairwise neuron\nrelationship on the unit hypersphere. We find that this property is crucial for\npreserving the semantic generation ability of text-to-image diffusion models.\nTo improve finetuning stability, we further propose Constrained Orthogonal\nFinetuning (COFT) which imposes an additional radius constraint to the\nhypersphere. Specifically, we consider two important finetuning text-to-image\ntasks: subject-driven generation where the goal is to generate subject-specific\nimages given a few images of a subject and a text prompt, and controllable\ngeneration where the goal is to enable the model to take in additional control\nsignals. We empirically show that our OFT framework outperforms existing\nmethods in generation quality and convergence speed.\n", "link": "http://arxiv.org/abs/2306.07280v3", "date": "2024-03-14", "relevancy": 2.4638, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6171}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6012}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Controlling%20Text-to-Image%20Diffusion%20by%20Orthogonal%20Finetuning&body=Title%3A%20Controlling%20Text-to-Image%20Diffusion%20by%20Orthogonal%20Finetuning%0AAuthor%3A%20Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Yuxuan%20Xue%20and%20Yao%20Feng%20and%20Zhen%20Liu%20and%20Dan%20Zhang%20and%20Adrian%20Weller%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Large%20text-to-image%20diffusion%20models%20have%20impressive%20capabilities%20in%0Agenerating%20photorealistic%20images%20from%20text%20prompts.%20How%20to%20effectively%20guide%20or%0Acontrol%20these%20powerful%20models%20to%20perform%20different%20downstream%20tasks%20becomes%20an%0Aimportant%20open%20problem.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20principled%0Afinetuning%20method%20--%20Orthogonal%20Finetuning%20%28OFT%29%2C%20for%20adapting%20text-to-image%0Adiffusion%20models%20to%20downstream%20tasks.%20Unlike%20existing%20methods%2C%20OFT%20can%20provably%0Apreserve%20hyperspherical%20energy%20which%20characterizes%20the%20pairwise%20neuron%0Arelationship%20on%20the%20unit%20hypersphere.%20We%20find%20that%20this%20property%20is%20crucial%20for%0Apreserving%20the%20semantic%20generation%20ability%20of%20text-to-image%20diffusion%20models.%0ATo%20improve%20finetuning%20stability%2C%20we%20further%20propose%20Constrained%20Orthogonal%0AFinetuning%20%28COFT%29%20which%20imposes%20an%20additional%20radius%20constraint%20to%20the%0Ahypersphere.%20Specifically%2C%20we%20consider%20two%20important%20finetuning%20text-to-image%0Atasks%3A%20subject-driven%20generation%20where%20the%20goal%20is%20to%20generate%20subject-specific%0Aimages%20given%20a%20few%20images%20of%20a%20subject%20and%20a%20text%20prompt%2C%20and%20controllable%0Ageneration%20where%20the%20goal%20is%20to%20enable%20the%20model%20to%20take%20in%20additional%20control%0Asignals.%20We%20empirically%20show%20that%20our%20OFT%20framework%20outperforms%20existing%0Amethods%20in%20generation%20quality%20and%20convergence%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07280v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Text-to-Image%20Diffusion%20by%20Orthogonal%20Finetuning&entry.906535625=Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Yuxuan%20Xue%20and%20Yao%20Feng%20and%20Zhen%20Liu%20and%20Dan%20Zhang%20and%20Adrian%20Weller%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Large%20text-to-image%20diffusion%20models%20have%20impressive%20capabilities%20in%0Agenerating%20photorealistic%20images%20from%20text%20prompts.%20How%20to%20effectively%20guide%20or%0Acontrol%20these%20powerful%20models%20to%20perform%20different%20downstream%20tasks%20becomes%20an%0Aimportant%20open%20problem.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20a%20principled%0Afinetuning%20method%20--%20Orthogonal%20Finetuning%20%28OFT%29%2C%20for%20adapting%20text-to-image%0Adiffusion%20models%20to%20downstream%20tasks.%20Unlike%20existing%20methods%2C%20OFT%20can%20provably%0Apreserve%20hyperspherical%20energy%20which%20characterizes%20the%20pairwise%20neuron%0Arelationship%20on%20the%20unit%20hypersphere.%20We%20find%20that%20this%20property%20is%20crucial%20for%0Apreserving%20the%20semantic%20generation%20ability%20of%20text-to-image%20diffusion%20models.%0ATo%20improve%20finetuning%20stability%2C%20we%20further%20propose%20Constrained%20Orthogonal%0AFinetuning%20%28COFT%29%20which%20imposes%20an%20additional%20radius%20constraint%20to%20the%0Ahypersphere.%20Specifically%2C%20we%20consider%20two%20important%20finetuning%20text-to-image%0Atasks%3A%20subject-driven%20generation%20where%20the%20goal%20is%20to%20generate%20subject-specific%0Aimages%20given%20a%20few%20images%20of%20a%20subject%20and%20a%20text%20prompt%2C%20and%20controllable%0Ageneration%20where%20the%20goal%20is%20to%20enable%20the%20model%20to%20take%20in%20additional%20control%0Asignals.%20We%20empirically%20show%20that%20our%20OFT%20framework%20outperforms%20existing%0Amethods%20in%20generation%20quality%20and%20convergence%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07280v3&entry.124074799=Read"},
{"title": "Exploiting Structural Consistency of Chest Anatomy for Unsupervised\n  Anomaly Detection in Radiography Images", "author": "Tiange Xiang and Yixiao Zhang and Yongyi Lu and Alan Yuille and Chaoyi Zhang and Weidong Cai and Zongwei Zhou", "abstract": "  Radiography imaging protocols focus on particular body regions, therefore\nproducing images of great similarity and yielding recurrent anatomical\nstructures across patients. Exploiting this structured information could\npotentially ease the detection of anomalies from radiography images. To this\nend, we propose a Simple Space-Aware Memory Matrix for In-painting and\nDetecting anomalies from radiography images (abbreviated as SimSID). We\nformulate anomaly detection as an image reconstruction task, consisting of a\nspace-aware memory matrix and an in-painting block in the feature space. During\nthe training, SimSID can taxonomize the ingrained anatomical structures into\nrecurrent visual patterns, and in the inference, it can identify anomalies\n(unseen/modified visual patterns) from the test image. Our SimSID surpasses the\nstate of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9%\nAUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.\nCode: https://github.com/MrGiovanni/SimSID\n", "link": "http://arxiv.org/abs/2403.08689v1", "date": "2024-03-13", "relevancy": 2.462, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4893}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4875}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images&body=Title%3A%20Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images%0AAuthor%3A%20Tiange%20Xiang%20and%20Yixiao%20Zhang%20and%20Yongyi%20Lu%20and%20Alan%20Yuille%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Radiography%20imaging%20protocols%20focus%20on%20particular%20body%20regions%2C%20therefore%0Aproducing%20images%20of%20great%20similarity%20and%20yielding%20recurrent%20anatomical%0Astructures%20across%20patients.%20Exploiting%20this%20structured%20information%20could%0Apotentially%20ease%20the%20detection%20of%20anomalies%20from%20radiography%20images.%20To%20this%0Aend%2C%20we%20propose%20a%20Simple%20Space-Aware%20Memory%20Matrix%20for%20In-painting%20and%0ADetecting%20anomalies%20from%20radiography%20images%20%28abbreviated%20as%20SimSID%29.%20We%0Aformulate%20anomaly%20detection%20as%20an%20image%20reconstruction%20task%2C%20consisting%20of%20a%0Aspace-aware%20memory%20matrix%20and%20an%20in-painting%20block%20in%20the%20feature%20space.%20During%0Athe%20training%2C%20SimSID%20can%20taxonomize%20the%20ingrained%20anatomical%20structures%20into%0Arecurrent%20visual%20patterns%2C%20and%20in%20the%20inference%2C%20it%20can%20identify%20anomalies%0A%28unseen/modified%20visual%20patterns%29%20from%20the%20test%20image.%20Our%20SimSID%20surpasses%20the%0Astate%20of%20the%20arts%20in%20unsupervised%20anomaly%20detection%20by%20%2B8.0%25%2C%20%2B5.0%25%2C%20and%20%2B9.9%25%0AAUC%20scores%20on%20ZhangLab%2C%20COVIDx%2C%20and%20CheXpert%20benchmark%20datasets%2C%20respectively.%0ACode%3A%20https%3A//github.com/MrGiovanni/SimSID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Structural%20Consistency%20of%20Chest%20Anatomy%20for%20Unsupervised%0A%20%20Anomaly%20Detection%20in%20Radiography%20Images&entry.906535625=Tiange%20Xiang%20and%20Yixiao%20Zhang%20and%20Yongyi%20Lu%20and%20Alan%20Yuille%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Radiography%20imaging%20protocols%20focus%20on%20particular%20body%20regions%2C%20therefore%0Aproducing%20images%20of%20great%20similarity%20and%20yielding%20recurrent%20anatomical%0Astructures%20across%20patients.%20Exploiting%20this%20structured%20information%20could%0Apotentially%20ease%20the%20detection%20of%20anomalies%20from%20radiography%20images.%20To%20this%0Aend%2C%20we%20propose%20a%20Simple%20Space-Aware%20Memory%20Matrix%20for%20In-painting%20and%0ADetecting%20anomalies%20from%20radiography%20images%20%28abbreviated%20as%20SimSID%29.%20We%0Aformulate%20anomaly%20detection%20as%20an%20image%20reconstruction%20task%2C%20consisting%20of%20a%0Aspace-aware%20memory%20matrix%20and%20an%20in-painting%20block%20in%20the%20feature%20space.%20During%0Athe%20training%2C%20SimSID%20can%20taxonomize%20the%20ingrained%20anatomical%20structures%20into%0Arecurrent%20visual%20patterns%2C%20and%20in%20the%20inference%2C%20it%20can%20identify%20anomalies%0A%28unseen/modified%20visual%20patterns%29%20from%20the%20test%20image.%20Our%20SimSID%20surpasses%20the%0Astate%20of%20the%20arts%20in%20unsupervised%20anomaly%20detection%20by%20%2B8.0%25%2C%20%2B5.0%25%2C%20and%20%2B9.9%25%0AAUC%20scores%20on%20ZhangLab%2C%20COVIDx%2C%20and%20CheXpert%20benchmark%20datasets%2C%20respectively.%0ACode%3A%20https%3A//github.com/MrGiovanni/SimSID%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08689v1&entry.124074799=Read"},
{"title": "Collision-Free Robot Navigation in Crowded Environments using Learning\n  based Convex Model Predictive Control", "author": "Zhuanglei Wen and Mingze Dong and Xiai Chen", "abstract": "  Navigating robots safely and efficiently in crowded and complex environments\nremains a significant challenge. However, due to the dynamic and intricate\nnature of these settings, planning efficient and collision-free paths for\nrobots to track is particularly difficult. In this paper, we uniquely bridge\nthe robot's perception, decision-making and control processes by utilizing the\nconvex obstacle-free region computed from 2D LiDAR data. The overall pipeline\nis threefold: (1) We proposes a robot navigation framework that utilizes deep\nreinforcement learning (DRL), conceptualizing the observation as the convex\nobstacle-free region, a departure from general reliance on raw sensor inputs.\n(2) We design the action space, derived from the intersection of the robot's\nkinematic limits and the convex region, to enable efficient sampling of\ninherently collision-free reference points. These actions assists in guiding\nthe robot to move towards the goal and interact with other obstacles during\nnavigation. (3) We employ model predictive control (MPC) to track the\ntrajectory formed by the reference points while satisfying constraints imposed\nby the convex obstacle-free region and the robot's kinodynamic limits. The\neffectiveness of proposed improvements has been validated through two sets of\nablation studies and a comparative experiment against the Timed Elastic Band\n(TEB), demonstrating improved navigation performance in crowded and complex\nenvironments.\n", "link": "http://arxiv.org/abs/2403.01450v2", "date": "2024-03-14", "relevancy": 2.4566, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6234}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6231}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6015}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control&body=Title%3A%20Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control%0AAuthor%3A%20Zhuanglei%20Wen%20and%20Mingze%20Dong%20and%20Xiai%20Chen%0AAbstract%3A%20%20%20Navigating%20robots%20safely%20and%20efficiently%20in%20crowded%20and%20complex%20environments%0Aremains%20a%20significant%20challenge.%20However%2C%20due%20to%20the%20dynamic%20and%20intricate%0Anature%20of%20these%20settings%2C%20planning%20efficient%20and%20collision-free%20paths%20for%0Arobots%20to%20track%20is%20particularly%20difficult.%20In%20this%20paper%2C%20we%20uniquely%20bridge%0Athe%20robot%27s%20perception%2C%20decision-making%20and%20control%20processes%20by%20utilizing%20the%0Aconvex%20obstacle-free%20region%20computed%20from%202D%20LiDAR%20data.%20The%20overall%20pipeline%0Ais%20threefold%3A%20%281%29%20We%20proposes%20a%20robot%20navigation%20framework%20that%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%2C%20conceptualizing%20the%20observation%20as%20the%20convex%0Aobstacle-free%20region%2C%20a%20departure%20from%20general%20reliance%20on%20raw%20sensor%20inputs.%0A%282%29%20We%20design%20the%20action%20space%2C%20derived%20from%20the%20intersection%20of%20the%20robot%27s%0Akinematic%20limits%20and%20the%20convex%20region%2C%20to%20enable%20efficient%20sampling%20of%0Ainherently%20collision-free%20reference%20points.%20These%20actions%20assists%20in%20guiding%0Athe%20robot%20to%20move%20towards%20the%20goal%20and%20interact%20with%20other%20obstacles%20during%0Anavigation.%20%283%29%20We%20employ%20model%20predictive%20control%20%28MPC%29%20to%20track%20the%0Atrajectory%20formed%20by%20the%20reference%20points%20while%20satisfying%20constraints%20imposed%0Aby%20the%20convex%20obstacle-free%20region%20and%20the%20robot%27s%20kinodynamic%20limits.%20The%0Aeffectiveness%20of%20proposed%20improvements%20has%20been%20validated%20through%20two%20sets%20of%0Aablation%20studies%20and%20a%20comparative%20experiment%20against%20the%20Timed%20Elastic%20Band%0A%28TEB%29%2C%20demonstrating%20improved%20navigation%20performance%20in%20crowded%20and%20complex%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01450v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision-Free%20Robot%20Navigation%20in%20Crowded%20Environments%20using%20Learning%0A%20%20based%20Convex%20Model%20Predictive%20Control&entry.906535625=Zhuanglei%20Wen%20and%20Mingze%20Dong%20and%20Xiai%20Chen&entry.1292438233=%20%20Navigating%20robots%20safely%20and%20efficiently%20in%20crowded%20and%20complex%20environments%0Aremains%20a%20significant%20challenge.%20However%2C%20due%20to%20the%20dynamic%20and%20intricate%0Anature%20of%20these%20settings%2C%20planning%20efficient%20and%20collision-free%20paths%20for%0Arobots%20to%20track%20is%20particularly%20difficult.%20In%20this%20paper%2C%20we%20uniquely%20bridge%0Athe%20robot%27s%20perception%2C%20decision-making%20and%20control%20processes%20by%20utilizing%20the%0Aconvex%20obstacle-free%20region%20computed%20from%202D%20LiDAR%20data.%20The%20overall%20pipeline%0Ais%20threefold%3A%20%281%29%20We%20proposes%20a%20robot%20navigation%20framework%20that%20utilizes%20deep%0Areinforcement%20learning%20%28DRL%29%2C%20conceptualizing%20the%20observation%20as%20the%20convex%0Aobstacle-free%20region%2C%20a%20departure%20from%20general%20reliance%20on%20raw%20sensor%20inputs.%0A%282%29%20We%20design%20the%20action%20space%2C%20derived%20from%20the%20intersection%20of%20the%20robot%27s%0Akinematic%20limits%20and%20the%20convex%20region%2C%20to%20enable%20efficient%20sampling%20of%0Ainherently%20collision-free%20reference%20points.%20These%20actions%20assists%20in%20guiding%0Athe%20robot%20to%20move%20towards%20the%20goal%20and%20interact%20with%20other%20obstacles%20during%0Anavigation.%20%283%29%20We%20employ%20model%20predictive%20control%20%28MPC%29%20to%20track%20the%0Atrajectory%20formed%20by%20the%20reference%20points%20while%20satisfying%20constraints%20imposed%0Aby%20the%20convex%20obstacle-free%20region%20and%20the%20robot%27s%20kinodynamic%20limits.%20The%0Aeffectiveness%20of%20proposed%20improvements%20has%20been%20validated%20through%20two%20sets%20of%0Aablation%20studies%20and%20a%20comparative%20experiment%20against%20the%20Timed%20Elastic%20Band%0A%28TEB%29%2C%20demonstrating%20improved%20navigation%20performance%20in%20crowded%20and%20complex%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01450v2&entry.124074799=Read"},
{"title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface", "author": "Linyi Jin and Nilesh Kulkarni and David Fouhey", "abstract": "  This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.\n", "link": "http://arxiv.org/abs/2403.08768v1", "date": "2024-03-13", "relevancy": 2.4521, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.499}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4918}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4805}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface&body=Title%3A%203DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface%0AAuthor%3A%20Linyi%20Jin%20and%20Nilesh%20Kulkarni%20and%20David%20Fouhey%0AAbstract%3A%20%20%20This%20paper%20introduces%203DFIRES%2C%20a%20novel%20system%20for%20scene-level%203D%0Areconstruction%20from%20posed%20images.%20Designed%20to%20work%20with%20as%20few%20as%20one%20view%2C%0A3DFIRES%20reconstructs%20the%20complete%20geometry%20of%20unseen%20scenes%2C%20including%20hidden%0Asurfaces.%20With%20multiple%20view%20inputs%2C%20our%20method%20produces%20full%20reconstruction%0Awithin%20all%20camera%20frustums.%20A%20key%20feature%20of%20our%20approach%20is%20the%20fusion%20of%0Amulti-view%20information%20at%20the%20feature%20level%2C%20enabling%20the%20production%20of%0Acoherent%20and%20comprehensive%203D%20reconstruction.%20We%20train%20our%20system%20on%0Anon-watertight%20scans%20from%20large-scale%20real%20scene%20dataset.%20We%20show%20it%20matches%0Athe%20efficacy%20of%20single-view%20reconstruction%20methods%20with%20only%20one%20input%20and%0Asurpasses%20existing%20techniques%20in%20both%20quantitative%20and%20qualitative%20measures%20for%0Asparse-view%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08768v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DFIRES%3A%20Few%20Image%203D%20REconstruction%20for%20Scenes%20with%20Hidden%20Surface&entry.906535625=Linyi%20Jin%20and%20Nilesh%20Kulkarni%20and%20David%20Fouhey&entry.1292438233=%20%20This%20paper%20introduces%203DFIRES%2C%20a%20novel%20system%20for%20scene-level%203D%0Areconstruction%20from%20posed%20images.%20Designed%20to%20work%20with%20as%20few%20as%20one%20view%2C%0A3DFIRES%20reconstructs%20the%20complete%20geometry%20of%20unseen%20scenes%2C%20including%20hidden%0Asurfaces.%20With%20multiple%20view%20inputs%2C%20our%20method%20produces%20full%20reconstruction%0Awithin%20all%20camera%20frustums.%20A%20key%20feature%20of%20our%20approach%20is%20the%20fusion%20of%0Amulti-view%20information%20at%20the%20feature%20level%2C%20enabling%20the%20production%20of%0Acoherent%20and%20comprehensive%203D%20reconstruction.%20We%20train%20our%20system%20on%0Anon-watertight%20scans%20from%20large-scale%20real%20scene%20dataset.%20We%20show%20it%20matches%0Athe%20efficacy%20of%20single-view%20reconstruction%20methods%20with%20only%20one%20input%20and%0Asurpasses%20existing%20techniques%20in%20both%20quantitative%20and%20qualitative%20measures%20for%0Asparse-view%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08768v1&entry.124074799=Read"},
{"title": "Intention-driven Ego-to-Exo Video Generation", "author": "Hongchen Luo and Kai Zhu and Wei Zhai and Yang Cao", "abstract": "  Ego-to-exo video generation refers to generating the corresponding exocentric\nvideo according to the egocentric video, providing valuable applications in\nAR/VR and embodied AI. Benefiting from advancements in diffusion model\ntechniques, notable progress has been achieved in video generation. However,\nexisting methods build upon the spatiotemporal consistency assumptions between\nadjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to\ndrastic changes in views. To this end, this paper proposes an Intention-Driven\nEgo-to-exo video generation framework (IDE) that leverages action intention\nconsisting of human movement and action description as view-independent\nrepresentation to guide video generation, preserving the consistency of content\nand motion. Specifically, the egocentric head trajectory is first estimated\nthrough multi-view stereo matching. Then, cross-view feature perception module\nis introduced to establish correspondences between exo- and ego- views, guiding\nthe trajectory transformation module to infer human full-body movement from the\nhead trajectory. Meanwhile, we present an action description unit that maps the\naction semantics into the feature space consistent with the exocentric image.\nFinally, the inferred human movement and high-level action descriptions jointly\nguide the generation of exocentric motion and interaction content (i.e.,\ncorresponding optical flow and occlusion maps) in the backward process of the\ndiffusion model, ultimately warping them into the corresponding exocentric\nvideo. We conduct extensive experiments on the relevant dataset with diverse\nexo-ego video pairs, and our IDE outperforms state-of-the-art models in both\nsubjective and objective assessments, demonstrating its efficacy in ego-to-exo\nvideo generation.\n", "link": "http://arxiv.org/abs/2403.09194v1", "date": "2024-03-14", "relevancy": 2.442, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6317}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5936}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intention-driven%20Ego-to-Exo%20Video%20Generation&body=Title%3A%20Intention-driven%20Ego-to-Exo%20Video%20Generation%0AAuthor%3A%20Hongchen%20Luo%20and%20Kai%20Zhu%20and%20Wei%20Zhai%20and%20Yang%20Cao%0AAbstract%3A%20%20%20Ego-to-exo%20video%20generation%20refers%20to%20generating%20the%20corresponding%20exocentric%0Avideo%20according%20to%20the%20egocentric%20video%2C%20providing%20valuable%20applications%20in%0AAR/VR%20and%20embodied%20AI.%20Benefiting%20from%20advancements%20in%20diffusion%20model%0Atechniques%2C%20notable%20progress%20has%20been%20achieved%20in%20video%20generation.%20However%2C%0Aexisting%20methods%20build%20upon%20the%20spatiotemporal%20consistency%20assumptions%20between%0Aadjacent%20frames%2C%20which%20cannot%20be%20satisfied%20in%20the%20ego-to-exo%20scenarios%20due%20to%0Adrastic%20changes%20in%20views.%20To%20this%20end%2C%20this%20paper%20proposes%20an%20Intention-Driven%0AEgo-to-exo%20video%20generation%20framework%20%28IDE%29%20that%20leverages%20action%20intention%0Aconsisting%20of%20human%20movement%20and%20action%20description%20as%20view-independent%0Arepresentation%20to%20guide%20video%20generation%2C%20preserving%20the%20consistency%20of%20content%0Aand%20motion.%20Specifically%2C%20the%20egocentric%20head%20trajectory%20is%20first%20estimated%0Athrough%20multi-view%20stereo%20matching.%20Then%2C%20cross-view%20feature%20perception%20module%0Ais%20introduced%20to%20establish%20correspondences%20between%20exo-%20and%20ego-%20views%2C%20guiding%0Athe%20trajectory%20transformation%20module%20to%20infer%20human%20full-body%20movement%20from%20the%0Ahead%20trajectory.%20Meanwhile%2C%20we%20present%20an%20action%20description%20unit%20that%20maps%20the%0Aaction%20semantics%20into%20the%20feature%20space%20consistent%20with%20the%20exocentric%20image.%0AFinally%2C%20the%20inferred%20human%20movement%20and%20high-level%20action%20descriptions%20jointly%0Aguide%20the%20generation%20of%20exocentric%20motion%20and%20interaction%20content%20%28i.e.%2C%0Acorresponding%20optical%20flow%20and%20occlusion%20maps%29%20in%20the%20backward%20process%20of%20the%0Adiffusion%20model%2C%20ultimately%20warping%20them%20into%20the%20corresponding%20exocentric%0Avideo.%20We%20conduct%20extensive%20experiments%20on%20the%20relevant%20dataset%20with%20diverse%0Aexo-ego%20video%20pairs%2C%20and%20our%20IDE%20outperforms%20state-of-the-art%20models%20in%20both%0Asubjective%20and%20objective%20assessments%2C%20demonstrating%20its%20efficacy%20in%20ego-to-exo%0Avideo%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09194v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intention-driven%20Ego-to-Exo%20Video%20Generation&entry.906535625=Hongchen%20Luo%20and%20Kai%20Zhu%20and%20Wei%20Zhai%20and%20Yang%20Cao&entry.1292438233=%20%20Ego-to-exo%20video%20generation%20refers%20to%20generating%20the%20corresponding%20exocentric%0Avideo%20according%20to%20the%20egocentric%20video%2C%20providing%20valuable%20applications%20in%0AAR/VR%20and%20embodied%20AI.%20Benefiting%20from%20advancements%20in%20diffusion%20model%0Atechniques%2C%20notable%20progress%20has%20been%20achieved%20in%20video%20generation.%20However%2C%0Aexisting%20methods%20build%20upon%20the%20spatiotemporal%20consistency%20assumptions%20between%0Aadjacent%20frames%2C%20which%20cannot%20be%20satisfied%20in%20the%20ego-to-exo%20scenarios%20due%20to%0Adrastic%20changes%20in%20views.%20To%20this%20end%2C%20this%20paper%20proposes%20an%20Intention-Driven%0AEgo-to-exo%20video%20generation%20framework%20%28IDE%29%20that%20leverages%20action%20intention%0Aconsisting%20of%20human%20movement%20and%20action%20description%20as%20view-independent%0Arepresentation%20to%20guide%20video%20generation%2C%20preserving%20the%20consistency%20of%20content%0Aand%20motion.%20Specifically%2C%20the%20egocentric%20head%20trajectory%20is%20first%20estimated%0Athrough%20multi-view%20stereo%20matching.%20Then%2C%20cross-view%20feature%20perception%20module%0Ais%20introduced%20to%20establish%20correspondences%20between%20exo-%20and%20ego-%20views%2C%20guiding%0Athe%20trajectory%20transformation%20module%20to%20infer%20human%20full-body%20movement%20from%20the%0Ahead%20trajectory.%20Meanwhile%2C%20we%20present%20an%20action%20description%20unit%20that%20maps%20the%0Aaction%20semantics%20into%20the%20feature%20space%20consistent%20with%20the%20exocentric%20image.%0AFinally%2C%20the%20inferred%20human%20movement%20and%20high-level%20action%20descriptions%20jointly%0Aguide%20the%20generation%20of%20exocentric%20motion%20and%20interaction%20content%20%28i.e.%2C%0Acorresponding%20optical%20flow%20and%20occlusion%20maps%29%20in%20the%20backward%20process%20of%20the%0Adiffusion%20model%2C%20ultimately%20warping%20them%20into%20the%20corresponding%20exocentric%0Avideo.%20We%20conduct%20extensive%20experiments%20on%20the%20relevant%20dataset%20with%20diverse%0Aexo-ego%20video%20pairs%2C%20and%20our%20IDE%20outperforms%20state-of-the-art%20models%20in%20both%0Asubjective%20and%20objective%20assessments%2C%20demonstrating%20its%20efficacy%20in%20ego-to-exo%0Avideo%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09194v1&entry.124074799=Read"},
{"title": "Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and\n  Personalized Stylization", "author": "Tao Yang and Rongyuan Wu and Peiran Ren and Xuansong Xie and Lei Zhang", "abstract": "  Diffusion models have demonstrated impressive performance in various image\ngeneration, editing, enhancement and translation tasks. In particular, the\npre-trained text-to-image stable diffusion models provide a potential solution\nto the challenging realistic image super-resolution (Real-ISR) and image\nstylization problems with their strong generative priors. However, the existing\nmethods along this line often fail to keep faithful pixel-wise image\nstructures. If extra skip connections are used to reproduce details, additional\ntraining in image space will be required, limiting the application to tasks in\nlatent space such as image stylization. In this work, we propose a pixel-aware\nstable diffusion (PASD) network to achieve robust Real-ISR and personalized\nimage stylization. Specifically, a pixel-aware cross attention module is\nintroduced to enable diffusion models perceiving image local structures in\npixel-wise level, while a degradation removal module is used to extract\ndegradation insensitive features to guide the diffusion process together with\nimage high level information. An adjustable noise schedule is introduced to\nfurther improve the image restoration results. By simply replacing the base\ndiffusion model with a stylized one, PASD can generate diverse stylized images\nwithout collecting pairwise training data, and by shifting the base model with\nan aesthetic one, PASD can bring old photos back to life. Extensive experiments\nin a variety of image enhancement and stylization tasks demonstrate the\neffectiveness of our proposed PASD approach. Our source codes are available at\n\\url{https://github.com/yangxy/PASD/}.\n", "link": "http://arxiv.org/abs/2308.14469v3", "date": "2024-03-14", "relevancy": 2.4405, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6286}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.58}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&body=Title%3A%20Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization%0AAuthor%3A%20Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20are%20used%20to%20reproduce%20details%2C%20additional%0Atraining%20in%20image%20space%20will%20be%20required%2C%20limiting%20the%20application%20to%20tasks%20in%0Alatent%20space%20such%20as%20image%20stylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%0Astable%20diffusion%20%28PASD%29%20network%20to%20achieve%20robust%20Real-ISR%20and%20personalized%0Aimage%20stylization.%20Specifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%0Aintroduced%20to%20enable%20diffusion%20models%20perceiving%20image%20local%20structures%20in%0Apixel-wise%20level%2C%20while%20a%20degradation%20removal%20module%20is%20used%20to%20extract%0Adegradation%20insensitive%20features%20to%20guide%20the%20diffusion%20process%20together%20with%0Aimage%20high%20level%20information.%20An%20adjustable%20noise%20schedule%20is%20introduced%20to%0Afurther%20improve%20the%20image%20restoration%20results.%20By%20simply%20replacing%20the%20base%0Adiffusion%20model%20with%20a%20stylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%0Awithout%20collecting%20pairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%0Aan%20aesthetic%20one%2C%20PASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%0Ain%20a%20variety%20of%20image%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14469v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Aware%20Stable%20Diffusion%20for%20Realistic%20Image%20Super-resolution%20and%0A%20%20Personalized%20Stylization&entry.906535625=Tao%20Yang%20and%20Rongyuan%20Wu%20and%20Peiran%20Ren%20and%20Xuansong%20Xie%20and%20Lei%20Zhang&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20impressive%20performance%20in%20various%20image%0Ageneration%2C%20editing%2C%20enhancement%20and%20translation%20tasks.%20In%20particular%2C%20the%0Apre-trained%20text-to-image%20stable%20diffusion%20models%20provide%20a%20potential%20solution%0Ato%20the%20challenging%20realistic%20image%20super-resolution%20%28Real-ISR%29%20and%20image%0Astylization%20problems%20with%20their%20strong%20generative%20priors.%20However%2C%20the%20existing%0Amethods%20along%20this%20line%20often%20fail%20to%20keep%20faithful%20pixel-wise%20image%0Astructures.%20If%20extra%20skip%20connections%20are%20used%20to%20reproduce%20details%2C%20additional%0Atraining%20in%20image%20space%20will%20be%20required%2C%20limiting%20the%20application%20to%20tasks%20in%0Alatent%20space%20such%20as%20image%20stylization.%20In%20this%20work%2C%20we%20propose%20a%20pixel-aware%0Astable%20diffusion%20%28PASD%29%20network%20to%20achieve%20robust%20Real-ISR%20and%20personalized%0Aimage%20stylization.%20Specifically%2C%20a%20pixel-aware%20cross%20attention%20module%20is%0Aintroduced%20to%20enable%20diffusion%20models%20perceiving%20image%20local%20structures%20in%0Apixel-wise%20level%2C%20while%20a%20degradation%20removal%20module%20is%20used%20to%20extract%0Adegradation%20insensitive%20features%20to%20guide%20the%20diffusion%20process%20together%20with%0Aimage%20high%20level%20information.%20An%20adjustable%20noise%20schedule%20is%20introduced%20to%0Afurther%20improve%20the%20image%20restoration%20results.%20By%20simply%20replacing%20the%20base%0Adiffusion%20model%20with%20a%20stylized%20one%2C%20PASD%20can%20generate%20diverse%20stylized%20images%0Awithout%20collecting%20pairwise%20training%20data%2C%20and%20by%20shifting%20the%20base%20model%20with%0Aan%20aesthetic%20one%2C%20PASD%20can%20bring%20old%20photos%20back%20to%20life.%20Extensive%20experiments%0Ain%20a%20variety%20of%20image%20enhancement%20and%20stylization%20tasks%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20PASD%20approach.%20Our%20source%20codes%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yangxy/PASD/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14469v3&entry.124074799=Read"},
{"title": "SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form\n  Layout-to-Image Generation", "author": "Chengyou Jia and Minnan Luo and Zhuohang Dang and Guang Dai and Xiaojun Chang and Mengmeng Wang and Jingdong Wang", "abstract": "  Despite significant progress in Text-to-Image (T2I) generative models, even\nlengthy and complex text descriptions still struggle to convey detailed\ncontrols. In contrast, Layout-to-Image (L2I) generation, aiming to generate\nrealistic and complex scene images from user-specified layouts, has risen to\nprominence. However, existing methods transform layout information into tokens\nor RGB images for conditional control in the generative process, leading to\ninsufficient spatial and semantic controllability of individual instances. To\naddress these limitations, we propose a novel Spatial-Semantic Map Guided\n(SSMG) diffusion model that adopts the feature map, derived from the layout, as\nguidance. Owing to rich spatial and semantic information encapsulated in\nwell-designed feature maps, SSMG achieves superior generation quality with\nsufficient spatial and semantic controllability compared to previous works.\nAdditionally, we propose the Relation-Sensitive Attention (RSA) and\nLocation-Sensitive Attention (LSA) mechanisms. The former aims to model the\nrelationships among multiple objects within scenes while the latter is designed\nto heighten the model's sensitivity to the spatial information embedded in the\nguidance. Extensive experiments demonstrate that SSMG achieves highly promising\nresults, setting a new state-of-the-art across a range of metrics encompassing\nfidelity, diversity, and controllability.\n", "link": "http://arxiv.org/abs/2308.10156v2", "date": "2024-03-13", "relevancy": 2.4387, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.644}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.598}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5801}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SSMG%3A%20Spatial-Semantic%20Map%20Guided%20Diffusion%20Model%20for%20Free-form%0A%20%20Layout-to-Image%20Generation&body=Title%3A%20SSMG%3A%20Spatial-Semantic%20Map%20Guided%20Diffusion%20Model%20for%20Free-form%0A%20%20Layout-to-Image%20Generation%0AAuthor%3A%20Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Guang%20Dai%20and%20Xiaojun%20Chang%20and%20Mengmeng%20Wang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20Text-to-Image%20%28T2I%29%20generative%20models%2C%20even%0Alengthy%20and%20complex%20text%20descriptions%20still%20struggle%20to%20convey%20detailed%0Acontrols.%20In%20contrast%2C%20Layout-to-Image%20%28L2I%29%20generation%2C%20aiming%20to%20generate%0Arealistic%20and%20complex%20scene%20images%20from%20user-specified%20layouts%2C%20has%20risen%20to%0Aprominence.%20However%2C%20existing%20methods%20transform%20layout%20information%20into%20tokens%0Aor%20RGB%20images%20for%20conditional%20control%20in%20the%20generative%20process%2C%20leading%20to%0Ainsufficient%20spatial%20and%20semantic%20controllability%20of%20individual%20instances.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20Spatial-Semantic%20Map%20Guided%0A%28SSMG%29%20diffusion%20model%20that%20adopts%20the%20feature%20map%2C%20derived%20from%20the%20layout%2C%20as%0Aguidance.%20Owing%20to%20rich%20spatial%20and%20semantic%20information%20encapsulated%20in%0Awell-designed%20feature%20maps%2C%20SSMG%20achieves%20superior%20generation%20quality%20with%0Asufficient%20spatial%20and%20semantic%20controllability%20compared%20to%20previous%20works.%0AAdditionally%2C%20we%20propose%20the%20Relation-Sensitive%20Attention%20%28RSA%29%20and%0ALocation-Sensitive%20Attention%20%28LSA%29%20mechanisms.%20The%20former%20aims%20to%20model%20the%0Arelationships%20among%20multiple%20objects%20within%20scenes%20while%20the%20latter%20is%20designed%0Ato%20heighten%20the%20model%27s%20sensitivity%20to%20the%20spatial%20information%20embedded%20in%20the%0Aguidance.%20Extensive%20experiments%20demonstrate%20that%20SSMG%20achieves%20highly%20promising%0Aresults%2C%20setting%20a%20new%20state-of-the-art%20across%20a%20range%20of%20metrics%20encompassing%0Afidelity%2C%20diversity%2C%20and%20controllability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10156v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSMG%3A%20Spatial-Semantic%20Map%20Guided%20Diffusion%20Model%20for%20Free-form%0A%20%20Layout-to-Image%20Generation&entry.906535625=Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Guang%20Dai%20and%20Xiaojun%20Chang%20and%20Mengmeng%20Wang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Despite%20significant%20progress%20in%20Text-to-Image%20%28T2I%29%20generative%20models%2C%20even%0Alengthy%20and%20complex%20text%20descriptions%20still%20struggle%20to%20convey%20detailed%0Acontrols.%20In%20contrast%2C%20Layout-to-Image%20%28L2I%29%20generation%2C%20aiming%20to%20generate%0Arealistic%20and%20complex%20scene%20images%20from%20user-specified%20layouts%2C%20has%20risen%20to%0Aprominence.%20However%2C%20existing%20methods%20transform%20layout%20information%20into%20tokens%0Aor%20RGB%20images%20for%20conditional%20control%20in%20the%20generative%20process%2C%20leading%20to%0Ainsufficient%20spatial%20and%20semantic%20controllability%20of%20individual%20instances.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20Spatial-Semantic%20Map%20Guided%0A%28SSMG%29%20diffusion%20model%20that%20adopts%20the%20feature%20map%2C%20derived%20from%20the%20layout%2C%20as%0Aguidance.%20Owing%20to%20rich%20spatial%20and%20semantic%20information%20encapsulated%20in%0Awell-designed%20feature%20maps%2C%20SSMG%20achieves%20superior%20generation%20quality%20with%0Asufficient%20spatial%20and%20semantic%20controllability%20compared%20to%20previous%20works.%0AAdditionally%2C%20we%20propose%20the%20Relation-Sensitive%20Attention%20%28RSA%29%20and%0ALocation-Sensitive%20Attention%20%28LSA%29%20mechanisms.%20The%20former%20aims%20to%20model%20the%0Arelationships%20among%20multiple%20objects%20within%20scenes%20while%20the%20latter%20is%20designed%0Ato%20heighten%20the%20model%27s%20sensitivity%20to%20the%20spatial%20information%20embedded%20in%20the%0Aguidance.%20Extensive%20experiments%20demonstrate%20that%20SSMG%20achieves%20highly%20promising%0Aresults%2C%20setting%20a%20new%20state-of-the-art%20across%20a%20range%20of%20metrics%20encompassing%0Afidelity%2C%20diversity%2C%20and%20controllability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10156v2&entry.124074799=Read"},
{"title": "Soften to Defend: Towards Adversarial Robustness via Self-Guided Label\n  Refinement", "author": "Daiwei Yu and Zhuorong Li and Lina Wei and Canghong Jin and Yun Zhang and Sixian Chan", "abstract": "  Adversarial training (AT) is currently one of the most effective ways to\nobtain the robustness of deep neural networks against adversarial attacks.\nHowever, most AT methods suffer from robust overfitting, i.e., a significant\ngeneralization gap in adversarial robustness between the training and testing\ncurves. In this paper, we first identify a connection between robust\noverfitting and the excessive memorization of noisy labels in AT from a view of\ngradient norm. As such label noise is mainly caused by a distribution mismatch\nand improper label assignments, we are motivated to propose a label refinement\napproach for AT. Specifically, our Self-Guided Label Refinement first\nself-refines a more accurate and informative label distribution from\nover-confident hard labels, and then it calibrates the training by dynamically\nincorporating knowledge from self-distilled models into the current model and\nthus requiring no external teachers. Empirical results demonstrate that our\nmethod can simultaneously boost the standard accuracy and robust performance\nacross multiple benchmark datasets, attack types, and architectures. In\naddition, we also provide a set of analyses from the perspectives of\ninformation theory to dive into our method and suggest the importance of soft\nlabels for robust generalization.\n", "link": "http://arxiv.org/abs/2403.09101v1", "date": "2024-03-14", "relevancy": 2.4365, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4918}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4864}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4837}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Soften%20to%20Defend%3A%20Towards%20Adversarial%20Robustness%20via%20Self-Guided%20Label%0A%20%20Refinement&body=Title%3A%20Soften%20to%20Defend%3A%20Towards%20Adversarial%20Robustness%20via%20Self-Guided%20Label%0A%20%20Refinement%0AAuthor%3A%20Daiwei%20Yu%20and%20Zhuorong%20Li%20and%20Lina%20Wei%20and%20Canghong%20Jin%20and%20Yun%20Zhang%20and%20Sixian%20Chan%0AAbstract%3A%20%20%20Adversarial%20training%20%28AT%29%20is%20currently%20one%20of%20the%20most%20effective%20ways%20to%0Aobtain%20the%20robustness%20of%20deep%20neural%20networks%20against%20adversarial%20attacks.%0AHowever%2C%20most%20AT%20methods%20suffer%20from%20robust%20overfitting%2C%20i.e.%2C%20a%20significant%0Ageneralization%20gap%20in%20adversarial%20robustness%20between%20the%20training%20and%20testing%0Acurves.%20In%20this%20paper%2C%20we%20first%20identify%20a%20connection%20between%20robust%0Aoverfitting%20and%20the%20excessive%20memorization%20of%20noisy%20labels%20in%20AT%20from%20a%20view%20of%0Agradient%20norm.%20As%20such%20label%20noise%20is%20mainly%20caused%20by%20a%20distribution%20mismatch%0Aand%20improper%20label%20assignments%2C%20we%20are%20motivated%20to%20propose%20a%20label%20refinement%0Aapproach%20for%20AT.%20Specifically%2C%20our%20Self-Guided%20Label%20Refinement%20first%0Aself-refines%20a%20more%20accurate%20and%20informative%20label%20distribution%20from%0Aover-confident%20hard%20labels%2C%20and%20then%20it%20calibrates%20the%20training%20by%20dynamically%0Aincorporating%20knowledge%20from%20self-distilled%20models%20into%20the%20current%20model%20and%0Athus%20requiring%20no%20external%20teachers.%20Empirical%20results%20demonstrate%20that%20our%0Amethod%20can%20simultaneously%20boost%20the%20standard%20accuracy%20and%20robust%20performance%0Aacross%20multiple%20benchmark%20datasets%2C%20attack%20types%2C%20and%20architectures.%20In%0Aaddition%2C%20we%20also%20provide%20a%20set%20of%20analyses%20from%20the%20perspectives%20of%0Ainformation%20theory%20to%20dive%20into%20our%20method%20and%20suggest%20the%20importance%20of%20soft%0Alabels%20for%20robust%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09101v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soften%20to%20Defend%3A%20Towards%20Adversarial%20Robustness%20via%20Self-Guided%20Label%0A%20%20Refinement&entry.906535625=Daiwei%20Yu%20and%20Zhuorong%20Li%20and%20Lina%20Wei%20and%20Canghong%20Jin%20and%20Yun%20Zhang%20and%20Sixian%20Chan&entry.1292438233=%20%20Adversarial%20training%20%28AT%29%20is%20currently%20one%20of%20the%20most%20effective%20ways%20to%0Aobtain%20the%20robustness%20of%20deep%20neural%20networks%20against%20adversarial%20attacks.%0AHowever%2C%20most%20AT%20methods%20suffer%20from%20robust%20overfitting%2C%20i.e.%2C%20a%20significant%0Ageneralization%20gap%20in%20adversarial%20robustness%20between%20the%20training%20and%20testing%0Acurves.%20In%20this%20paper%2C%20we%20first%20identify%20a%20connection%20between%20robust%0Aoverfitting%20and%20the%20excessive%20memorization%20of%20noisy%20labels%20in%20AT%20from%20a%20view%20of%0Agradient%20norm.%20As%20such%20label%20noise%20is%20mainly%20caused%20by%20a%20distribution%20mismatch%0Aand%20improper%20label%20assignments%2C%20we%20are%20motivated%20to%20propose%20a%20label%20refinement%0Aapproach%20for%20AT.%20Specifically%2C%20our%20Self-Guided%20Label%20Refinement%20first%0Aself-refines%20a%20more%20accurate%20and%20informative%20label%20distribution%20from%0Aover-confident%20hard%20labels%2C%20and%20then%20it%20calibrates%20the%20training%20by%20dynamically%0Aincorporating%20knowledge%20from%20self-distilled%20models%20into%20the%20current%20model%20and%0Athus%20requiring%20no%20external%20teachers.%20Empirical%20results%20demonstrate%20that%20our%0Amethod%20can%20simultaneously%20boost%20the%20standard%20accuracy%20and%20robust%20performance%0Aacross%20multiple%20benchmark%20datasets%2C%20attack%20types%2C%20and%20architectures.%20In%0Aaddition%2C%20we%20also%20provide%20a%20set%20of%20analyses%20from%20the%20perspectives%20of%0Ainformation%20theory%20to%20dive%20into%20our%20method%20and%20suggest%20the%20importance%20of%20soft%0Alabels%20for%20robust%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09101v1&entry.124074799=Read"},
{"title": "A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary\n  Analysis", "author": "Linxi Zhao and Jiankai Tang and Dongyu Chen and Xiaohong Liu and Yong Zhou and Yuanchun Shi and Guangyu Wang and Yuntao Wang", "abstract": "  Nailfold capillaroscopy is widely used in assessing health conditions,\nhighlighting the pressing need for an automated nailfold capillary analysis\nsystem. In this study, we present a pioneering effort in constructing a\ncomprehensive nailfold capillary dataset-321 images, 219 videos from 68\nsubjects, with clinic reports and expert annotations-that serves as a crucial\nresource for training deep-learning models. Leveraging this dataset, we\nfinetuned three deep learning models with expert annotations as supervised\nlabels and integrated them into a novel end-to-end nailfold capillary analysis\npipeline. This pipeline excels in automatically detecting and measuring a wide\nrange of size factors, morphological features, and dynamic aspects of nailfold\ncapillaries. We compared our outcomes with clinical reports. Experiment results\nshowed that our automated pipeline achieves an average of sub-pixel level\nprecision in measurements and 89.9% accuracy in identifying morphological\nabnormalities. These results underscore its potential for advancing\nquantitative medical research and enabling pervasive computing in healthcare.\nOur data and code are available at\nhttps://github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.\n", "link": "http://arxiv.org/abs/2312.05930v2", "date": "2024-03-14", "relevancy": 2.4363, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4895}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis&body=Title%3A%20A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis%0AAuthor%3A%20Linxi%20Zhao%20and%20Jiankai%20Tang%20and%20Dongyu%20Chen%20and%20Xiaohong%20Liu%20and%20Yong%20Zhou%20and%20Yuanchun%20Shi%20and%20Guangyu%20Wang%20and%20Yuntao%20Wang%0AAbstract%3A%20%20%20Nailfold%20capillaroscopy%20is%20widely%20used%20in%20assessing%20health%20conditions%2C%0Ahighlighting%20the%20pressing%20need%20for%20an%20automated%20nailfold%20capillary%20analysis%0Asystem.%20In%20this%20study%2C%20we%20present%20a%20pioneering%20effort%20in%20constructing%20a%0Acomprehensive%20nailfold%20capillary%20dataset-321%20images%2C%20219%20videos%20from%2068%0Asubjects%2C%20with%20clinic%20reports%20and%20expert%20annotations-that%20serves%20as%20a%20crucial%0Aresource%20for%20training%20deep-learning%20models.%20Leveraging%20this%20dataset%2C%20we%0Afinetuned%20three%20deep%20learning%20models%20with%20expert%20annotations%20as%20supervised%0Alabels%20and%20integrated%20them%20into%20a%20novel%20end-to-end%20nailfold%20capillary%20analysis%0Apipeline.%20This%20pipeline%20excels%20in%20automatically%20detecting%20and%20measuring%20a%20wide%0Arange%20of%20size%20factors%2C%20morphological%20features%2C%20and%20dynamic%20aspects%20of%20nailfold%0Acapillaries.%20We%20compared%20our%20outcomes%20with%20clinical%20reports.%20Experiment%20results%0Ashowed%20that%20our%20automated%20pipeline%20achieves%20an%20average%20of%20sub-pixel%20level%0Aprecision%20in%20measurements%20and%2089.9%25%20accuracy%20in%20identifying%20morphological%0Aabnormalities.%20These%20results%20underscore%20its%20potential%20for%20advancing%0Aquantitative%20medical%20research%20and%20enabling%20pervasive%20computing%20in%20healthcare.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05930v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Dataset%20and%20Automated%20Pipeline%20for%20Nailfold%20Capillary%0A%20%20Analysis&entry.906535625=Linxi%20Zhao%20and%20Jiankai%20Tang%20and%20Dongyu%20Chen%20and%20Xiaohong%20Liu%20and%20Yong%20Zhou%20and%20Yuanchun%20Shi%20and%20Guangyu%20Wang%20and%20Yuntao%20Wang&entry.1292438233=%20%20Nailfold%20capillaroscopy%20is%20widely%20used%20in%20assessing%20health%20conditions%2C%0Ahighlighting%20the%20pressing%20need%20for%20an%20automated%20nailfold%20capillary%20analysis%0Asystem.%20In%20this%20study%2C%20we%20present%20a%20pioneering%20effort%20in%20constructing%20a%0Acomprehensive%20nailfold%20capillary%20dataset-321%20images%2C%20219%20videos%20from%2068%0Asubjects%2C%20with%20clinic%20reports%20and%20expert%20annotations-that%20serves%20as%20a%20crucial%0Aresource%20for%20training%20deep-learning%20models.%20Leveraging%20this%20dataset%2C%20we%0Afinetuned%20three%20deep%20learning%20models%20with%20expert%20annotations%20as%20supervised%0Alabels%20and%20integrated%20them%20into%20a%20novel%20end-to-end%20nailfold%20capillary%20analysis%0Apipeline.%20This%20pipeline%20excels%20in%20automatically%20detecting%20and%20measuring%20a%20wide%0Arange%20of%20size%20factors%2C%20morphological%20features%2C%20and%20dynamic%20aspects%20of%20nailfold%0Acapillaries.%20We%20compared%20our%20outcomes%20with%20clinical%20reports.%20Experiment%20results%0Ashowed%20that%20our%20automated%20pipeline%20achieves%20an%20average%20of%20sub-pixel%20level%0Aprecision%20in%20measurements%20and%2089.9%25%20accuracy%20in%20identifying%20morphological%0Aabnormalities.%20These%20results%20underscore%20its%20potential%20for%20advancing%0Aquantitative%20medical%20research%20and%20enabling%20pervasive%20computing%20in%20healthcare.%0AOur%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/THU-CS-PI-LAB/ANFC-Automated-Nailfold-Capillary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05930v2&entry.124074799=Read"},
{"title": "Resisting Backdoor Attacks in Federated Learning via Bidirectional\n  Elections and Individual Perspective", "author": "Zhen Qin and Feiyi Chen and Chen Zhi and Xueqiang Yan and Shuiguang Deng", "abstract": "  Existing approaches defend against backdoor attacks in federated learning\n(FL) mainly through a) mitigating the impact of infected models, or b)\nexcluding infected models. The former negatively impacts model accuracy, while\nthe latter usually relies on globally clear boundaries between benign and\ninfected model updates. However, model updates are easy to be mixed and\nscattered throughout in reality due to the diverse distributions of local data.\nThis work focuses on excluding infected models in FL. Unlike previous\nperspectives from a global view, we propose Snowball, a novel anti-backdoor FL\nframework through bidirectional elections from an individual perspective\ninspired by one principle deduced by us and two principles in FL and deep\nlearning. It is characterized by a) bottom-up election, where each candidate\nmodel update votes to several peer ones such that a few model updates are\nelected as selectees for aggregation; and b) top-down election, where selectees\nprogressively enlarge themselves through picking up from the candidates. We\ncompare Snowball with state-of-the-art defenses to backdoor attacks in FL on\nfive real-world datasets, demonstrating its superior resistance to backdoor\nattacks and slight impact on the accuracy of the global model.\n", "link": "http://arxiv.org/abs/2309.16456v2", "date": "2024-03-13", "relevancy": 2.4222, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4946}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4879}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective&body=Title%3A%20Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective%0AAuthor%3A%20Zhen%20Qin%20and%20Feiyi%20Chen%20and%20Chen%20Zhi%20and%20Xueqiang%20Yan%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20Existing%20approaches%20defend%20against%20backdoor%20attacks%20in%20federated%20learning%0A%28FL%29%20mainly%20through%20a%29%20mitigating%20the%20impact%20of%20infected%20models%2C%20or%20b%29%0Aexcluding%20infected%20models.%20The%20former%20negatively%20impacts%20model%20accuracy%2C%20while%0Athe%20latter%20usually%20relies%20on%20globally%20clear%20boundaries%20between%20benign%20and%0Ainfected%20model%20updates.%20However%2C%20model%20updates%20are%20easy%20to%20be%20mixed%20and%0Ascattered%20throughout%20in%20reality%20due%20to%20the%20diverse%20distributions%20of%20local%20data.%0AThis%20work%20focuses%20on%20excluding%20infected%20models%20in%20FL.%20Unlike%20previous%0Aperspectives%20from%20a%20global%20view%2C%20we%20propose%20Snowball%2C%20a%20novel%20anti-backdoor%20FL%0Aframework%20through%20bidirectional%20elections%20from%20an%20individual%20perspective%0Ainspired%20by%20one%20principle%20deduced%20by%20us%20and%20two%20principles%20in%20FL%20and%20deep%0Alearning.%20It%20is%20characterized%20by%20a%29%20bottom-up%20election%2C%20where%20each%20candidate%0Amodel%20update%20votes%20to%20several%20peer%20ones%20such%20that%20a%20few%20model%20updates%20are%0Aelected%20as%20selectees%20for%20aggregation%3B%20and%20b%29%20top-down%20election%2C%20where%20selectees%0Aprogressively%20enlarge%20themselves%20through%20picking%20up%20from%20the%20candidates.%20We%0Acompare%20Snowball%20with%20state-of-the-art%20defenses%20to%20backdoor%20attacks%20in%20FL%20on%0Afive%20real-world%20datasets%2C%20demonstrating%20its%20superior%20resistance%20to%20backdoor%0Aattacks%20and%20slight%20impact%20on%20the%20accuracy%20of%20the%20global%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16456v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resisting%20Backdoor%20Attacks%20in%20Federated%20Learning%20via%20Bidirectional%0A%20%20Elections%20and%20Individual%20Perspective&entry.906535625=Zhen%20Qin%20and%20Feiyi%20Chen%20and%20Chen%20Zhi%20and%20Xueqiang%20Yan%20and%20Shuiguang%20Deng&entry.1292438233=%20%20Existing%20approaches%20defend%20against%20backdoor%20attacks%20in%20federated%20learning%0A%28FL%29%20mainly%20through%20a%29%20mitigating%20the%20impact%20of%20infected%20models%2C%20or%20b%29%0Aexcluding%20infected%20models.%20The%20former%20negatively%20impacts%20model%20accuracy%2C%20while%0Athe%20latter%20usually%20relies%20on%20globally%20clear%20boundaries%20between%20benign%20and%0Ainfected%20model%20updates.%20However%2C%20model%20updates%20are%20easy%20to%20be%20mixed%20and%0Ascattered%20throughout%20in%20reality%20due%20to%20the%20diverse%20distributions%20of%20local%20data.%0AThis%20work%20focuses%20on%20excluding%20infected%20models%20in%20FL.%20Unlike%20previous%0Aperspectives%20from%20a%20global%20view%2C%20we%20propose%20Snowball%2C%20a%20novel%20anti-backdoor%20FL%0Aframework%20through%20bidirectional%20elections%20from%20an%20individual%20perspective%0Ainspired%20by%20one%20principle%20deduced%20by%20us%20and%20two%20principles%20in%20FL%20and%20deep%0Alearning.%20It%20is%20characterized%20by%20a%29%20bottom-up%20election%2C%20where%20each%20candidate%0Amodel%20update%20votes%20to%20several%20peer%20ones%20such%20that%20a%20few%20model%20updates%20are%0Aelected%20as%20selectees%20for%20aggregation%3B%20and%20b%29%20top-down%20election%2C%20where%20selectees%0Aprogressively%20enlarge%20themselves%20through%20picking%20up%20from%20the%20candidates.%20We%0Acompare%20Snowball%20with%20state-of-the-art%20defenses%20to%20backdoor%20attacks%20in%20FL%20on%0Afive%20real-world%20datasets%2C%20demonstrating%20its%20superior%20resistance%20to%20backdoor%0Aattacks%20and%20slight%20impact%20on%20the%20accuracy%20of%20the%20global%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16456v2&entry.124074799=Read"},
{"title": "Deep Learning for In-Orbit Cloud Segmentation and Classification in\n  Hyperspectral Satellite Data", "author": "Daniel Kovac and Jan Mucha and Jon Alvarez Justo and Jiri Mekyska and Zoltan Galaz and Krystof Novotny and Radoslav Pitonak and Jan Knezik and Jonas Herec and Tor Arne Johansen", "abstract": "  This article explores the latest Convolutional Neural Networks (CNNs) for\ncloud detection aboard hyperspectral satellites. The performance of the latest\n1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and\n2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.\nEvaluation criteria include precision and computational efficiency for in-orbit\ndeployment. Experiments utilize NASA's EO-1 Hyperion data, with varying\nspectral channel numbers after Principal Component Analysis. Results indicate\nthat 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,\nwhile maintaining compactness with larger spectral channel sets, albeit with\nincreased inference times. However, the performance of 1D CNN degrades with\nsignificant channel reduction. In this context, the 2D-Justo-UNet-Simple offers\nthe best balance for in-orbit deployment, considering precision, memory, and\ntime costs. While nnU-net is suitable for on-ground processing, deployment of\nlightweight 1D-Justo-LiuNet is recommended for high-precision applications.\nAlternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced\ncosts between timing and precision in orbit.\n", "link": "http://arxiv.org/abs/2403.08695v1", "date": "2024-03-13", "relevancy": 2.4195, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5132}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4795}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4591}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data&body=Title%3A%20Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data%0AAuthor%3A%20Daniel%20Kovac%20and%20Jan%20Mucha%20and%20Jon%20Alvarez%20Justo%20and%20Jiri%20Mekyska%20and%20Zoltan%20Galaz%20and%20Krystof%20Novotny%20and%20Radoslav%20Pitonak%20and%20Jan%20Knezik%20and%20Jonas%20Herec%20and%20Tor%20Arne%20Johansen%0AAbstract%3A%20%20%20This%20article%20explores%20the%20latest%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Acloud%20detection%20aboard%20hyperspectral%20satellites.%20The%20performance%20of%20the%20latest%0A1D%20CNN%20%281D-Justo-LiuNet%29%20and%20two%20recent%202D%20CNNs%20%28nnU-net%20and%0A2D-Justo-UNet-Simple%29%20for%20cloud%20segmentation%20and%20classification%20is%20assessed.%0AEvaluation%20criteria%20include%20precision%20and%20computational%20efficiency%20for%20in-orbit%0Adeployment.%20Experiments%20utilize%20NASA%27s%20EO-1%20Hyperion%20data%2C%20with%20varying%0Aspectral%20channel%20numbers%20after%20Principal%20Component%20Analysis.%20Results%20indicate%0Athat%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%2C%20outperforming%202D%20CNNs%2C%0Awhile%20maintaining%20compactness%20with%20larger%20spectral%20channel%20sets%2C%20albeit%20with%0Aincreased%20inference%20times.%20However%2C%20the%20performance%20of%201D%20CNN%20degrades%20with%0Asignificant%20channel%20reduction.%20In%20this%20context%2C%20the%202D-Justo-UNet-Simple%20offers%0Athe%20best%20balance%20for%20in-orbit%20deployment%2C%20considering%20precision%2C%20memory%2C%20and%0Atime%20costs.%20While%20nnU-net%20is%20suitable%20for%20on-ground%20processing%2C%20deployment%20of%0Alightweight%201D-Justo-LiuNet%20is%20recommended%20for%20high-precision%20applications.%0AAlternatively%2C%20lightweight%202D-Justo-UNet-Simple%20is%20recommended%20for%20balanced%0Acosts%20between%20timing%20and%20precision%20in%20orbit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20In-Orbit%20Cloud%20Segmentation%20and%20Classification%20in%0A%20%20Hyperspectral%20Satellite%20Data&entry.906535625=Daniel%20Kovac%20and%20Jan%20Mucha%20and%20Jon%20Alvarez%20Justo%20and%20Jiri%20Mekyska%20and%20Zoltan%20Galaz%20and%20Krystof%20Novotny%20and%20Radoslav%20Pitonak%20and%20Jan%20Knezik%20and%20Jonas%20Herec%20and%20Tor%20Arne%20Johansen&entry.1292438233=%20%20This%20article%20explores%20the%20latest%20Convolutional%20Neural%20Networks%20%28CNNs%29%20for%0Acloud%20detection%20aboard%20hyperspectral%20satellites.%20The%20performance%20of%20the%20latest%0A1D%20CNN%20%281D-Justo-LiuNet%29%20and%20two%20recent%202D%20CNNs%20%28nnU-net%20and%0A2D-Justo-UNet-Simple%29%20for%20cloud%20segmentation%20and%20classification%20is%20assessed.%0AEvaluation%20criteria%20include%20precision%20and%20computational%20efficiency%20for%20in-orbit%0Adeployment.%20Experiments%20utilize%20NASA%27s%20EO-1%20Hyperion%20data%2C%20with%20varying%0Aspectral%20channel%20numbers%20after%20Principal%20Component%20Analysis.%20Results%20indicate%0Athat%201D-Justo-LiuNet%20achieves%20the%20highest%20accuracy%2C%20outperforming%202D%20CNNs%2C%0Awhile%20maintaining%20compactness%20with%20larger%20spectral%20channel%20sets%2C%20albeit%20with%0Aincreased%20inference%20times.%20However%2C%20the%20performance%20of%201D%20CNN%20degrades%20with%0Asignificant%20channel%20reduction.%20In%20this%20context%2C%20the%202D-Justo-UNet-Simple%20offers%0Athe%20best%20balance%20for%20in-orbit%20deployment%2C%20considering%20precision%2C%20memory%2C%20and%0Atime%20costs.%20While%20nnU-net%20is%20suitable%20for%20on-ground%20processing%2C%20deployment%20of%0Alightweight%201D-Justo-LiuNet%20is%20recommended%20for%20high-precision%20applications.%0AAlternatively%2C%20lightweight%202D-Justo-UNet-Simple%20is%20recommended%20for%20balanced%0Acosts%20between%20timing%20and%20precision%20in%20orbit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08695v1&entry.124074799=Read"},
{"title": "Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its\n  Experimental Evaluation", "author": "Carlo Grigioni and Franca Corradini and Alessandro Antonucci and J\u00e9r\u00f4me Guzzi and Francesco Flammini", "abstract": "  Safe road-crossing by self-driving vehicles is a crucial problem to address\nin smart-cities. In this paper, we introduce a multi-sensor fusion approach to\nsupport road-crossing decisions in a system composed by an autonomous\nwheelchair and a flying drone featuring a robust sensory system made of diverse\nand redundant components. To that aim, we designed an analytical danger\nfunction based on explainable physical conditions evaluated by single sensors,\nincluding those using machine learning and artificial vision. As a\nproof-of-concept, we provide an experimental evaluation in a laboratory\nenvironment, showing the advantages of using multiple sensors, which can\nimprove decision accuracy and effectively support safety assessment. We made\nthe dataset available to the scientific community for further experimentation.\nThe work has been developed in the context of an European project named\nREXASI-PRO, which aims to develop trustworthy artificial intelligence for\nsocial navigation of people with reduced mobility.\n", "link": "http://arxiv.org/abs/2403.08984v1", "date": "2024-03-13", "relevancy": 2.4171, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5979}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5586}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20Road-Crossing%20by%20Autonomous%20Wheelchairs%3A%20a%20Novel%20Dataset%20and%20its%0A%20%20Experimental%20Evaluation&body=Title%3A%20Safe%20Road-Crossing%20by%20Autonomous%20Wheelchairs%3A%20a%20Novel%20Dataset%20and%20its%0A%20%20Experimental%20Evaluation%0AAuthor%3A%20Carlo%20Grigioni%20and%20Franca%20Corradini%20and%20Alessandro%20Antonucci%20and%20J%C3%A9r%C3%B4me%20Guzzi%20and%20Francesco%20Flammini%0AAbstract%3A%20%20%20Safe%20road-crossing%20by%20self-driving%20vehicles%20is%20a%20crucial%20problem%20to%20address%0Ain%20smart-cities.%20In%20this%20paper%2C%20we%20introduce%20a%20multi-sensor%20fusion%20approach%20to%0Asupport%20road-crossing%20decisions%20in%20a%20system%20composed%20by%20an%20autonomous%0Awheelchair%20and%20a%20flying%20drone%20featuring%20a%20robust%20sensory%20system%20made%20of%20diverse%0Aand%20redundant%20components.%20To%20that%20aim%2C%20we%20designed%20an%20analytical%20danger%0Afunction%20based%20on%20explainable%20physical%20conditions%20evaluated%20by%20single%20sensors%2C%0Aincluding%20those%20using%20machine%20learning%20and%20artificial%20vision.%20As%20a%0Aproof-of-concept%2C%20we%20provide%20an%20experimental%20evaluation%20in%20a%20laboratory%0Aenvironment%2C%20showing%20the%20advantages%20of%20using%20multiple%20sensors%2C%20which%20can%0Aimprove%20decision%20accuracy%20and%20effectively%20support%20safety%20assessment.%20We%20made%0Athe%20dataset%20available%20to%20the%20scientific%20community%20for%20further%20experimentation.%0AThe%20work%20has%20been%20developed%20in%20the%20context%20of%20an%20European%20project%20named%0AREXASI-PRO%2C%20which%20aims%20to%20develop%20trustworthy%20artificial%20intelligence%20for%0Asocial%20navigation%20of%20people%20with%20reduced%20mobility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08984v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Road-Crossing%20by%20Autonomous%20Wheelchairs%3A%20a%20Novel%20Dataset%20and%20its%0A%20%20Experimental%20Evaluation&entry.906535625=Carlo%20Grigioni%20and%20Franca%20Corradini%20and%20Alessandro%20Antonucci%20and%20J%C3%A9r%C3%B4me%20Guzzi%20and%20Francesco%20Flammini&entry.1292438233=%20%20Safe%20road-crossing%20by%20self-driving%20vehicles%20is%20a%20crucial%20problem%20to%20address%0Ain%20smart-cities.%20In%20this%20paper%2C%20we%20introduce%20a%20multi-sensor%20fusion%20approach%20to%0Asupport%20road-crossing%20decisions%20in%20a%20system%20composed%20by%20an%20autonomous%0Awheelchair%20and%20a%20flying%20drone%20featuring%20a%20robust%20sensory%20system%20made%20of%20diverse%0Aand%20redundant%20components.%20To%20that%20aim%2C%20we%20designed%20an%20analytical%20danger%0Afunction%20based%20on%20explainable%20physical%20conditions%20evaluated%20by%20single%20sensors%2C%0Aincluding%20those%20using%20machine%20learning%20and%20artificial%20vision.%20As%20a%0Aproof-of-concept%2C%20we%20provide%20an%20experimental%20evaluation%20in%20a%20laboratory%0Aenvironment%2C%20showing%20the%20advantages%20of%20using%20multiple%20sensors%2C%20which%20can%0Aimprove%20decision%20accuracy%20and%20effectively%20support%20safety%20assessment.%20We%20made%0Athe%20dataset%20available%20to%20the%20scientific%20community%20for%20further%20experimentation.%0AThe%20work%20has%20been%20developed%20in%20the%20context%20of%20an%20European%20project%20named%0AREXASI-PRO%2C%20which%20aims%20to%20develop%20trustworthy%20artificial%20intelligence%20for%0Asocial%20navigation%20of%20people%20with%20reduced%20mobility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08984v1&entry.124074799=Read"},
{"title": "Learning to optimize with convergence guarantees using nonlinear system\n  theory", "author": "Andrea Martin and Luca Furieri", "abstract": "  The increasing reliance on numerical methods for controlling dynamical\nsystems and training machine learning models underscores the need to devise\nalgorithms that dependably and efficiently navigate complex optimization\nlandscapes. Classical gradient descent methods offer strong theoretical\nguarantees for convex problems; however, they demand meticulous hyperparameter\ntuning for non-convex ones. The emerging paradigm of learning to optimize (L2O)\nautomates the discovery of algorithms with optimized performance leveraging\nlearning models and data - yet, it lacks a theoretical framework to analyze\nconvergence and robustness of the learned algorithms. In this paper, we fill\nthis gap by harnessing nonlinear system theory. Specifically, we propose an\nunconstrained parametrization of all convergent algorithms for smooth\nnon-convex objective functions. Notably, our framework is directly compatible\nwith automatic differentiation tools, ensuring convergence by design while\nlearning to optimize.\n", "link": "http://arxiv.org/abs/2403.09389v1", "date": "2024-03-14", "relevancy": 2.4081, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4885}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4808}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory&body=Title%3A%20Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory%0AAuthor%3A%20Andrea%20Martin%20and%20Luca%20Furieri%0AAbstract%3A%20%20%20The%20increasing%20reliance%20on%20numerical%20methods%20for%20controlling%20dynamical%0Asystems%20and%20training%20machine%20learning%20models%20underscores%20the%20need%20to%20devise%0Aalgorithms%20that%20dependably%20and%20efficiently%20navigate%20complex%20optimization%0Alandscapes.%20Classical%20gradient%20descent%20methods%20offer%20strong%20theoretical%0Aguarantees%20for%20convex%20problems%3B%20however%2C%20they%20demand%20meticulous%20hyperparameter%0Atuning%20for%20non-convex%20ones.%20The%20emerging%20paradigm%20of%20learning%20to%20optimize%20%28L2O%29%0Aautomates%20the%20discovery%20of%20algorithms%20with%20optimized%20performance%20leveraging%0Alearning%20models%20and%20data%20-%20yet%2C%20it%20lacks%20a%20theoretical%20framework%20to%20analyze%0Aconvergence%20and%20robustness%20of%20the%20learned%20algorithms.%20In%20this%20paper%2C%20we%20fill%0Athis%20gap%20by%20harnessing%20nonlinear%20system%20theory.%20Specifically%2C%20we%20propose%20an%0Aunconstrained%20parametrization%20of%20all%20convergent%20algorithms%20for%20smooth%0Anon-convex%20objective%20functions.%20Notably%2C%20our%20framework%20is%20directly%20compatible%0Awith%20automatic%20differentiation%20tools%2C%20ensuring%20convergence%20by%20design%20while%0Alearning%20to%20optimize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09389v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20optimize%20with%20convergence%20guarantees%20using%20nonlinear%20system%0A%20%20theory&entry.906535625=Andrea%20Martin%20and%20Luca%20Furieri&entry.1292438233=%20%20The%20increasing%20reliance%20on%20numerical%20methods%20for%20controlling%20dynamical%0Asystems%20and%20training%20machine%20learning%20models%20underscores%20the%20need%20to%20devise%0Aalgorithms%20that%20dependably%20and%20efficiently%20navigate%20complex%20optimization%0Alandscapes.%20Classical%20gradient%20descent%20methods%20offer%20strong%20theoretical%0Aguarantees%20for%20convex%20problems%3B%20however%2C%20they%20demand%20meticulous%20hyperparameter%0Atuning%20for%20non-convex%20ones.%20The%20emerging%20paradigm%20of%20learning%20to%20optimize%20%28L2O%29%0Aautomates%20the%20discovery%20of%20algorithms%20with%20optimized%20performance%20leveraging%0Alearning%20models%20and%20data%20-%20yet%2C%20it%20lacks%20a%20theoretical%20framework%20to%20analyze%0Aconvergence%20and%20robustness%20of%20the%20learned%20algorithms.%20In%20this%20paper%2C%20we%20fill%0Athis%20gap%20by%20harnessing%20nonlinear%20system%20theory.%20Specifically%2C%20we%20propose%20an%0Aunconstrained%20parametrization%20of%20all%20convergent%20algorithms%20for%20smooth%0Anon-convex%20objective%20functions.%20Notably%2C%20our%20framework%20is%20directly%20compatible%0Awith%20automatic%20differentiation%20tools%2C%20ensuring%20convergence%20by%20design%20while%0Alearning%20to%20optimize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09389v1&entry.124074799=Read"},
{"title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding", "author": "Chris Kelly and Luhui Hu and Jiayin Hu and Yu Tian and Deshun Yang and Bang Yang and Cindy Yang and Zihao Li and Zaoshan Huang and Yuexian Zou", "abstract": "  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n", "link": "http://arxiv.org/abs/2403.09530v1", "date": "2024-03-14", "relevancy": 2.4034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6291}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&body=Title%3A%20VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding%0AAuthor%3A%20Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou%0AAbstract%3A%20%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09530v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionGPT-3D%3A%20A%20Generalized%20Multimodal%20Agent%20for%20Enhanced%203D%20Vision%0A%20%20Understanding&entry.906535625=Chris%20Kelly%20and%20Luhui%20Hu%20and%20Jiayin%20Hu%20and%20Yu%20Tian%20and%20Deshun%20Yang%20and%20Bang%20Yang%20and%20Cindy%20Yang%20and%20Zihao%20Li%20and%20Zaoshan%20Huang%20and%20Yuexian%20Zou&entry.1292438233=%20%20The%20evolution%20of%20text%20to%20visual%20components%20facilitates%20people%27s%20daily%20lives%2C%0Asuch%20as%20generating%20image%2C%20videos%20from%20text%20and%20identifying%20the%20desired%20elements%0Awithin%20the%20images.%20Computer%20vision%20models%20involving%20the%20multimodal%20abilities%20in%0Athe%20previous%20days%20are%20focused%20on%20image%20detection%2C%20classification%20based%20on%0Awell-defined%20objects.%20Large%20language%20models%20%28LLMs%29%20introduces%20the%0Atransformation%20from%20nature%20language%20to%20visual%20objects%2C%20which%20present%20the%20visual%0Alayout%20for%20text%20contexts.%20OpenAI%20GPT-4%20has%20emerged%20as%20the%20pinnacle%20in%20LLMs%2C%0Awhile%20the%20computer%20vision%20%28CV%29%20domain%20boasts%20a%20plethora%20of%20state-of-the-art%0A%28SOTA%29%20models%20and%20algorithms%20to%20convert%202D%20images%20to%20their%203D%20representations.%0AHowever%2C%20the%20mismatching%20between%20the%20algorithms%20with%20the%20problem%20could%20lead%20to%0Aundesired%20results.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20an%20unified%0AVisionGPT-3D%20framework%20to%20consolidate%20the%20state-of-the-art%20vision%20models%2C%0Athereby%20facilitating%20the%20development%20of%20vision-oriented%20AI.%20VisionGPT-3D%0Aprovides%20a%20versatile%20multimodal%20framework%20building%20upon%20the%20strengths%20of%0Amultimodal%20foundation%20models.%20It%20seamlessly%20integrates%20various%20SOTA%20vision%0Amodels%20and%20brings%20the%20automation%20in%20the%20selection%20of%20SOTA%20vision%20models%2C%0Aidentifies%20the%20suitable%203D%20mesh%20creation%20algorithms%20corresponding%20to%202D%20depth%0Amaps%20analysis%2C%20generates%20optimal%20results%20based%20on%20diverse%20multimodal%20inputs%0Asuch%20as%20text%20prompts.%0A%20%20Keywords%3A%20VisionGPT-3D%2C%203D%20vision%20understanding%2C%20Multimodal%20agent%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09530v1&entry.124074799=Read"},
{"title": "The NeRFect Match: Exploring NeRF Features for Visual Localization", "author": "Qunjie Zhou and Maxim Maximov and Or Litany and Laura Leal-Taix\u00e9", "abstract": "  In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene\nrepresentation for visual localization. Recently, NeRF has been employed to\nenhance pose regression and scene coordinate regression models by augmenting\nthe training database, providing auxiliary supervision through rendered images,\nor serving as an iterative refinement module. We extend its recognized\nadvantages -- its ability to provide a compact scene representation with\nrealistic appearances and accurate geometry -- by exploring the potential of\nNeRF's internal features in establishing precise 2D-3D matches for\nlocalization. To this end, we conduct a comprehensive examination of NeRF's\nimplicit knowledge, acquired through view synthesis, for matching under various\nconditions. This includes exploring different matching network architectures,\nextracting encoder features at multiple layers, and varying training\nconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D\nmatching function that capitalizes on the internal knowledge of NeRF learned\nvia view synthesis. Our evaluation of NeRFMatch on standard localization\nbenchmarks, within a structure-based pipeline, sets a new state-of-the-art for\nlocalization performance on Cambridge Landmarks.\n", "link": "http://arxiv.org/abs/2403.09577v1", "date": "2024-03-14", "relevancy": 2.4021, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&body=Title%3A%20The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization%0AAuthor%3A%20Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09577v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NeRFect%20Match%3A%20Exploring%20NeRF%20Features%20for%20Visual%20Localization&entry.906535625=Qunjie%20Zhou%20and%20Maxim%20Maximov%20and%20Or%20Litany%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20the%20use%20of%20Neural%20Radiance%20Fields%20%28NeRF%29%20as%20a%20scene%0Arepresentation%20for%20visual%20localization.%20Recently%2C%20NeRF%20has%20been%20employed%20to%0Aenhance%20pose%20regression%20and%20scene%20coordinate%20regression%20models%20by%20augmenting%0Athe%20training%20database%2C%20providing%20auxiliary%20supervision%20through%20rendered%20images%2C%0Aor%20serving%20as%20an%20iterative%20refinement%20module.%20We%20extend%20its%20recognized%0Aadvantages%20--%20its%20ability%20to%20provide%20a%20compact%20scene%20representation%20with%0Arealistic%20appearances%20and%20accurate%20geometry%20--%20by%20exploring%20the%20potential%20of%0ANeRF%27s%20internal%20features%20in%20establishing%20precise%202D-3D%20matches%20for%0Alocalization.%20To%20this%20end%2C%20we%20conduct%20a%20comprehensive%20examination%20of%20NeRF%27s%0Aimplicit%20knowledge%2C%20acquired%20through%20view%20synthesis%2C%20for%20matching%20under%20various%0Aconditions.%20This%20includes%20exploring%20different%20matching%20network%20architectures%2C%0Aextracting%20encoder%20features%20at%20multiple%20layers%2C%20and%20varying%20training%0Aconfigurations.%20Significantly%2C%20we%20introduce%20NeRFMatch%2C%20an%20advanced%202D-3D%0Amatching%20function%20that%20capitalizes%20on%20the%20internal%20knowledge%20of%20NeRF%20learned%0Avia%20view%20synthesis.%20Our%20evaluation%20of%20NeRFMatch%20on%20standard%20localization%0Abenchmarks%2C%20within%20a%20structure-based%20pipeline%2C%20sets%20a%20new%20state-of-the-art%20for%0Alocalization%20performance%20on%20Cambridge%20Landmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09577v1&entry.124074799=Read"},
{"title": "A Hierarchical Fused Quantum Fuzzy Neural Network for Image\n  Classification", "author": "Sheng-Yao Wu and Run-Ze Li and Yan-Qi Song and Su-Juan Qin and Qiao-Yan Wen and Fei Gao", "abstract": "  Neural network is a powerful learning paradigm for data feature learning in\nthe era of big data. However, most neural network models are deterministic\nmodels that ignore the uncertainty of data. Fuzzy neural networks are proposed\nto address this problem. FDNN is a hierarchical deep neural network that\nderives information from both fuzzy and neural representations, the\nrepresentations are then fused to form representation to be classified. FDNN\nperform well on uncertain data classification tasks. In this paper, we proposed\na novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from\nclassical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership\nfunctions in fuzzy neural network. We conducted simulated experiment on two\ntypes of datasets (Dirty-MNIST and 15-Scene), the results show that the\nproposed model can outperform several existing methods. In addition, we\ndemonstrate the robustness of the proposed quantum circuit.\n", "link": "http://arxiv.org/abs/2403.09318v1", "date": "2024-03-14", "relevancy": 2.4008, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4942}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4657}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification&body=Title%3A%20A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification%0AAuthor%3A%20Sheng-Yao%20Wu%20and%20Run-Ze%20Li%20and%20Yan-Qi%20Song%20and%20Su-Juan%20Qin%20and%20Qiao-Yan%20Wen%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Neural%20network%20is%20a%20powerful%20learning%20paradigm%20for%20data%20feature%20learning%20in%0Athe%20era%20of%20big%20data.%20However%2C%20most%20neural%20network%20models%20are%20deterministic%0Amodels%20that%20ignore%20the%20uncertainty%20of%20data.%20Fuzzy%20neural%20networks%20are%20proposed%0Ato%20address%20this%20problem.%20FDNN%20is%20a%20hierarchical%20deep%20neural%20network%20that%0Aderives%20information%20from%20both%20fuzzy%20and%20neural%20representations%2C%20the%0Arepresentations%20are%20then%20fused%20to%20form%20representation%20to%20be%20classified.%20FDNN%0Aperform%20well%20on%20uncertain%20data%20classification%20tasks.%20In%20this%20paper%2C%20we%20proposed%0Aa%20novel%20hierarchical%20fused%20quantum%20fuzzy%20neural%20network%20%28HQFNN%29.%20Different%20from%0Aclassical%20FDNN%2C%20HQFNN%20uses%20quantum%20neural%20networks%20to%20learn%20fuzzy%20membership%0Afunctions%20in%20fuzzy%20neural%20network.%20We%20conducted%20simulated%20experiment%20on%20two%0Atypes%20of%20datasets%20%28Dirty-MNIST%20and%2015-Scene%29%2C%20the%20results%20show%20that%20the%0Aproposed%20model%20can%20outperform%20several%20existing%20methods.%20In%20addition%2C%20we%0Ademonstrate%20the%20robustness%20of%20the%20proposed%20quantum%20circuit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09318v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hierarchical%20Fused%20Quantum%20Fuzzy%20Neural%20Network%20for%20Image%0A%20%20Classification&entry.906535625=Sheng-Yao%20Wu%20and%20Run-Ze%20Li%20and%20Yan-Qi%20Song%20and%20Su-Juan%20Qin%20and%20Qiao-Yan%20Wen%20and%20Fei%20Gao&entry.1292438233=%20%20Neural%20network%20is%20a%20powerful%20learning%20paradigm%20for%20data%20feature%20learning%20in%0Athe%20era%20of%20big%20data.%20However%2C%20most%20neural%20network%20models%20are%20deterministic%0Amodels%20that%20ignore%20the%20uncertainty%20of%20data.%20Fuzzy%20neural%20networks%20are%20proposed%0Ato%20address%20this%20problem.%20FDNN%20is%20a%20hierarchical%20deep%20neural%20network%20that%0Aderives%20information%20from%20both%20fuzzy%20and%20neural%20representations%2C%20the%0Arepresentations%20are%20then%20fused%20to%20form%20representation%20to%20be%20classified.%20FDNN%0Aperform%20well%20on%20uncertain%20data%20classification%20tasks.%20In%20this%20paper%2C%20we%20proposed%0Aa%20novel%20hierarchical%20fused%20quantum%20fuzzy%20neural%20network%20%28HQFNN%29.%20Different%20from%0Aclassical%20FDNN%2C%20HQFNN%20uses%20quantum%20neural%20networks%20to%20learn%20fuzzy%20membership%0Afunctions%20in%20fuzzy%20neural%20network.%20We%20conducted%20simulated%20experiment%20on%20two%0Atypes%20of%20datasets%20%28Dirty-MNIST%20and%2015-Scene%29%2C%20the%20results%20show%20that%20the%0Aproposed%20model%20can%20outperform%20several%20existing%20methods.%20In%20addition%2C%20we%0Ademonstrate%20the%20robustness%20of%20the%20proposed%20quantum%20circuit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09318v1&entry.124074799=Read"},
{"title": "Token Alignment via Character Matching for Subword Completion", "author": "Ben Athiwaratkun and Shiqi Wang and Mingyue Shang and Yuchen Tian and Zijian Wang and Sujan Kumar Gonugondla and Sanjay Krishna Gouda and Rob Kwiatowski and Ramesh Nallapati and Bing Xiang", "abstract": "  Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.\n", "link": "http://arxiv.org/abs/2403.08688v1", "date": "2024-03-13", "relevancy": 2.3998, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4691}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4524}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion&body=Title%3A%20Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion%0AAuthor%3A%20Ben%20Athiwaratkun%20and%20Shiqi%20Wang%20and%20Mingyue%20Shang%20and%20Yuchen%20Tian%20and%20Zijian%20Wang%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sanjay%20Krishna%20Gouda%20and%20Rob%20Kwiatowski%20and%20Ramesh%20Nallapati%20and%20Bing%20Xiang%0AAbstract%3A%20%20%20Generative%20models%2C%20widely%20utilized%20in%20various%20applications%2C%20can%20often%0Astruggle%20with%20prompts%20corresponding%20to%20partial%20tokens.%20This%20struggle%20stems%20from%0Atokenization%2C%20where%20partial%20tokens%20fall%20out%20of%20distribution%20during%20inference%2C%0Aleading%20to%20incorrect%20or%20nonsensical%20outputs.%20This%20paper%20examines%20a%20technique%20to%0Aalleviate%20the%20tokenization%20artifact%20on%20text%20completion%20in%20generative%20models%2C%0Amaintaining%20performance%20even%20in%20regular%20non-subword%20cases.%20The%20method%2C%20termed%0Atoken%20alignment%2C%20involves%20backtracking%20to%20the%20last%20complete%20tokens%20and%20ensuring%0Athe%20model%27s%20generation%20aligns%20with%20the%20prompt.%20This%20approach%20showcases%20marked%0Aimprovement%20across%20many%20partial%20token%20scenarios%2C%20including%20nuanced%20cases%20like%0Aspace-prefix%20and%20partial%20indentation%2C%20with%20only%20a%20minor%20time%20increase.%20The%0Atechnique%20and%20analysis%20detailed%20in%20this%20paper%20contribute%20to%20the%20continuous%0Aadvancement%20of%20generative%20models%20in%20handling%20partial%20inputs%2C%20bearing%20relevance%0Afor%20applications%20like%20code%20completion%20and%20text%20autocompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08688v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Alignment%20via%20Character%20Matching%20for%20Subword%20Completion&entry.906535625=Ben%20Athiwaratkun%20and%20Shiqi%20Wang%20and%20Mingyue%20Shang%20and%20Yuchen%20Tian%20and%20Zijian%20Wang%20and%20Sujan%20Kumar%20Gonugondla%20and%20Sanjay%20Krishna%20Gouda%20and%20Rob%20Kwiatowski%20and%20Ramesh%20Nallapati%20and%20Bing%20Xiang&entry.1292438233=%20%20Generative%20models%2C%20widely%20utilized%20in%20various%20applications%2C%20can%20often%0Astruggle%20with%20prompts%20corresponding%20to%20partial%20tokens.%20This%20struggle%20stems%20from%0Atokenization%2C%20where%20partial%20tokens%20fall%20out%20of%20distribution%20during%20inference%2C%0Aleading%20to%20incorrect%20or%20nonsensical%20outputs.%20This%20paper%20examines%20a%20technique%20to%0Aalleviate%20the%20tokenization%20artifact%20on%20text%20completion%20in%20generative%20models%2C%0Amaintaining%20performance%20even%20in%20regular%20non-subword%20cases.%20The%20method%2C%20termed%0Atoken%20alignment%2C%20involves%20backtracking%20to%20the%20last%20complete%20tokens%20and%20ensuring%0Athe%20model%27s%20generation%20aligns%20with%20the%20prompt.%20This%20approach%20showcases%20marked%0Aimprovement%20across%20many%20partial%20token%20scenarios%2C%20including%20nuanced%20cases%20like%0Aspace-prefix%20and%20partial%20indentation%2C%20with%20only%20a%20minor%20time%20increase.%20The%0Atechnique%20and%20analysis%20detailed%20in%20this%20paper%20contribute%20to%20the%20continuous%0Aadvancement%20of%20generative%20models%20in%20handling%20partial%20inputs%2C%20bearing%20relevance%0Afor%20applications%20like%20code%20completion%20and%20text%20autocompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08688v1&entry.124074799=Read"},
{"title": "CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot\n  Learning", "author": "Yanyi Zhang and Qi Jia and Xin Fan and Yu Liu and Ran He", "abstract": "  Attribute and object (A-O) disentanglement is a fundamental and critical\nproblem for Compositional Zero-shot Learning (CZSL), whose aim is to recognize\nnovel A-O compositions based on foregone knowledge. Existing methods based on\ndisentangled representation learning lose sight of the contextual dependency\nbetween the A-O primitive pairs. Inspired by this, we propose a novel A-O\ndisentangled framework for CZSL, namely Class-specified Cascaded Network\n(CSCNet). The key insight is to firstly classify one primitive and then\nspecifies the predicted class as a priori for guiding another primitive\nrecognition in a cascaded fashion. To this end, CSCNet constructs\nAttribute-to-Object and Object-to-Attribute cascaded branches, in addition to a\ncomposition branch modeling the two primitives as a whole. Notably, we devise a\nparametric classifier (ParamCls) to improve the matching between visual and\nsemantic embeddings. By improving the A-O disentanglement, our framework\nachieves superior results than previous competitive methods.\n", "link": "http://arxiv.org/abs/2403.05924v2", "date": "2024-03-13", "relevancy": 2.3979, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4505}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CSCNET%3A%20Class-Specified%20Cascaded%20Network%20for%20Compositional%20Zero-Shot%0A%20%20Learning&body=Title%3A%20CSCNET%3A%20Class-Specified%20Cascaded%20Network%20for%20Compositional%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20Yanyi%20Zhang%20and%20Qi%20Jia%20and%20Xin%20Fan%20and%20Yu%20Liu%20and%20Ran%20He%0AAbstract%3A%20%20%20Attribute%20and%20object%20%28A-O%29%20disentanglement%20is%20a%20fundamental%20and%20critical%0Aproblem%20for%20Compositional%20Zero-shot%20Learning%20%28CZSL%29%2C%20whose%20aim%20is%20to%20recognize%0Anovel%20A-O%20compositions%20based%20on%20foregone%20knowledge.%20Existing%20methods%20based%20on%0Adisentangled%20representation%20learning%20lose%20sight%20of%20the%20contextual%20dependency%0Abetween%20the%20A-O%20primitive%20pairs.%20Inspired%20by%20this%2C%20we%20propose%20a%20novel%20A-O%0Adisentangled%20framework%20for%20CZSL%2C%20namely%20Class-specified%20Cascaded%20Network%0A%28CSCNet%29.%20The%20key%20insight%20is%20to%20firstly%20classify%20one%20primitive%20and%20then%0Aspecifies%20the%20predicted%20class%20as%20a%20priori%20for%20guiding%20another%20primitive%0Arecognition%20in%20a%20cascaded%20fashion.%20To%20this%20end%2C%20CSCNet%20constructs%0AAttribute-to-Object%20and%20Object-to-Attribute%20cascaded%20branches%2C%20in%20addition%20to%20a%0Acomposition%20branch%20modeling%20the%20two%20primitives%20as%20a%20whole.%20Notably%2C%20we%20devise%20a%0Aparametric%20classifier%20%28ParamCls%29%20to%20improve%20the%20matching%20between%20visual%20and%0Asemantic%20embeddings.%20By%20improving%20the%20A-O%20disentanglement%2C%20our%20framework%0Aachieves%20superior%20results%20than%20previous%20competitive%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05924v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSCNET%3A%20Class-Specified%20Cascaded%20Network%20for%20Compositional%20Zero-Shot%0A%20%20Learning&entry.906535625=Yanyi%20Zhang%20and%20Qi%20Jia%20and%20Xin%20Fan%20and%20Yu%20Liu%20and%20Ran%20He&entry.1292438233=%20%20Attribute%20and%20object%20%28A-O%29%20disentanglement%20is%20a%20fundamental%20and%20critical%0Aproblem%20for%20Compositional%20Zero-shot%20Learning%20%28CZSL%29%2C%20whose%20aim%20is%20to%20recognize%0Anovel%20A-O%20compositions%20based%20on%20foregone%20knowledge.%20Existing%20methods%20based%20on%0Adisentangled%20representation%20learning%20lose%20sight%20of%20the%20contextual%20dependency%0Abetween%20the%20A-O%20primitive%20pairs.%20Inspired%20by%20this%2C%20we%20propose%20a%20novel%20A-O%0Adisentangled%20framework%20for%20CZSL%2C%20namely%20Class-specified%20Cascaded%20Network%0A%28CSCNet%29.%20The%20key%20insight%20is%20to%20firstly%20classify%20one%20primitive%20and%20then%0Aspecifies%20the%20predicted%20class%20as%20a%20priori%20for%20guiding%20another%20primitive%0Arecognition%20in%20a%20cascaded%20fashion.%20To%20this%20end%2C%20CSCNet%20constructs%0AAttribute-to-Object%20and%20Object-to-Attribute%20cascaded%20branches%2C%20in%20addition%20to%20a%0Acomposition%20branch%20modeling%20the%20two%20primitives%20as%20a%20whole.%20Notably%2C%20we%20devise%20a%0Aparametric%20classifier%20%28ParamCls%29%20to%20improve%20the%20matching%20between%20visual%20and%0Asemantic%20embeddings.%20By%20improving%20the%20A-O%20disentanglement%2C%20our%20framework%0Aachieves%20superior%20results%20than%20previous%20competitive%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05924v2&entry.124074799=Read"},
{"title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism", "author": "Megha Srivastava and Simran Arora and Dan Boneh", "abstract": "  The increasing compute demands of AI systems has led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning, poses challenges. Existing works\non verifiable training largely fall into two classes: proof-based systems,\nwhich struggle to scale due to requiring cryptographic techniques, and\n\"optimistic\" methods that consider a trusted third-party auditor who replicates\nthe training process. A key challenge with the latter is that hardware\nnondeterminism between GPU types during training prevents an auditor from\nreplicating the training process exactly, and such schemes are therefore\nnon-robust. We propose a method that combines training in a higher precision\nthan the target model, rounding after intermediate computation steps, and\nstoring rounding decisions based on an adaptive thresholding procedure, to\nsuccessfully control for nondeterminism. Across three different NVIDIA GPUs\n(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32\nprecision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2\n(117M) models. Our verifiable training scheme significantly decreases the\nstorage and time costs compared to proof-based systems.\n", "link": "http://arxiv.org/abs/2403.09603v1", "date": "2024-03-14", "relevancy": 2.3847, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4657}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4578}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism&body=Title%3A%20Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism%0AAuthor%3A%20Megha%20Srivastava%20and%20Simran%20Arora%20and%20Dan%20Boneh%0AAbstract%3A%20%20%20The%20increasing%20compute%20demands%20of%20AI%20systems%20has%20led%20to%20the%20emergence%20of%0Aservices%20that%20train%20models%20on%20behalf%20of%20clients%20lacking%20necessary%20resources.%0AHowever%2C%20ensuring%20correctness%20of%20training%20and%20guarding%20against%20potential%0Atraining-time%20attacks%2C%20such%20as%20data%20poisoning%2C%20poses%20challenges.%20Existing%20works%0Aon%20verifiable%20training%20largely%20fall%20into%20two%20classes%3A%20proof-based%20systems%2C%0Awhich%20struggle%20to%20scale%20due%20to%20requiring%20cryptographic%20techniques%2C%20and%0A%22optimistic%22%20methods%20that%20consider%20a%20trusted%20third-party%20auditor%20who%20replicates%0Athe%20training%20process.%20A%20key%20challenge%20with%20the%20latter%20is%20that%20hardware%0Anondeterminism%20between%20GPU%20types%20during%20training%20prevents%20an%20auditor%20from%0Areplicating%20the%20training%20process%20exactly%2C%20and%20such%20schemes%20are%20therefore%0Anon-robust.%20We%20propose%20a%20method%20that%20combines%20training%20in%20a%20higher%20precision%0Athan%20the%20target%20model%2C%20rounding%20after%20intermediate%20computation%20steps%2C%20and%0Astoring%20rounding%20decisions%20based%20on%20an%20adaptive%20thresholding%20procedure%2C%20to%0Asuccessfully%20control%20for%20nondeterminism.%20Across%20three%20different%20NVIDIA%20GPUs%0A%28A40%2C%20Titan%20XP%2C%20RTX%202080%20Ti%29%2C%20we%20achieve%20exact%20training%20replication%20at%20FP32%0Aprecision%20for%20both%20full-training%20and%20fine-tuning%20of%20ResNet-50%20%2823M%29%20and%20GPT-2%0A%28117M%29%20models.%20Our%20verifiable%20training%20scheme%20significantly%20decreases%20the%0Astorage%20and%20time%20costs%20compared%20to%20proof-based%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09603v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Verifiable%20Training%20by%20Controlling%20Hardware%20Nondeterminism&entry.906535625=Megha%20Srivastava%20and%20Simran%20Arora%20and%20Dan%20Boneh&entry.1292438233=%20%20The%20increasing%20compute%20demands%20of%20AI%20systems%20has%20led%20to%20the%20emergence%20of%0Aservices%20that%20train%20models%20on%20behalf%20of%20clients%20lacking%20necessary%20resources.%0AHowever%2C%20ensuring%20correctness%20of%20training%20and%20guarding%20against%20potential%0Atraining-time%20attacks%2C%20such%20as%20data%20poisoning%2C%20poses%20challenges.%20Existing%20works%0Aon%20verifiable%20training%20largely%20fall%20into%20two%20classes%3A%20proof-based%20systems%2C%0Awhich%20struggle%20to%20scale%20due%20to%20requiring%20cryptographic%20techniques%2C%20and%0A%22optimistic%22%20methods%20that%20consider%20a%20trusted%20third-party%20auditor%20who%20replicates%0Athe%20training%20process.%20A%20key%20challenge%20with%20the%20latter%20is%20that%20hardware%0Anondeterminism%20between%20GPU%20types%20during%20training%20prevents%20an%20auditor%20from%0Areplicating%20the%20training%20process%20exactly%2C%20and%20such%20schemes%20are%20therefore%0Anon-robust.%20We%20propose%20a%20method%20that%20combines%20training%20in%20a%20higher%20precision%0Athan%20the%20target%20model%2C%20rounding%20after%20intermediate%20computation%20steps%2C%20and%0Astoring%20rounding%20decisions%20based%20on%20an%20adaptive%20thresholding%20procedure%2C%20to%0Asuccessfully%20control%20for%20nondeterminism.%20Across%20three%20different%20NVIDIA%20GPUs%0A%28A40%2C%20Titan%20XP%2C%20RTX%202080%20Ti%29%2C%20we%20achieve%20exact%20training%20replication%20at%20FP32%0Aprecision%20for%20both%20full-training%20and%20fine-tuning%20of%20ResNet-50%20%2823M%29%20and%20GPT-2%0A%28117M%29%20models.%20Our%20verifiable%20training%20scheme%20significantly%20decreases%20the%0Astorage%20and%20time%20costs%20compared%20to%20proof-based%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09603v1&entry.124074799=Read"},
{"title": "When Semantic Segmentation Meets Frequency Aliasing", "author": "Linwei Chen and Lin Gu and Ying Fu", "abstract": "  Despite recent advancements in semantic segmentation, where and what pixels\nare hard to segment remains largely unexplored. Existing research only\nseparates an image into easy and hard regions and empirically observes the\nlatter are associated with object boundaries. In this paper, we conduct a\ncomprehensive analysis of hard pixel errors, categorizing them into three\ntypes: false responses, merging mistakes, and displacements. Our findings\nreveal a quantitative association between hard pixels and aliasing, which is\ndistortion caused by the overlapping of frequency components in the Fourier\ndomain during downsampling. To identify the frequencies responsible for\naliasing, we propose using the equivalent sampling rate to calculate the\nNyquist frequency, which marks the threshold for aliasing. Then, we introduce\nthe aliasing score as a metric to quantify the extent of aliasing. While\npositively correlated with the proposed aliasing score, three types of hard\npixels exhibit different patterns. Here, we propose two novel de-aliasing\nfilter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\ndegradation by accurately removing or adjusting frequencies higher than the\nNyquist frequency. The DAF precisely removes the frequencies responsible for\naliasing before downsampling, while the FreqMix dynamically selects\nhigh-frequency components within the encoder block. Experimental results\ndemonstrate consistent improvements in semantic segmentation and low-light\ninstance segmentation tasks. The code is available at:\n\\url{https://github.com/Linwei-Chen/Seg-Aliasing}.\n", "link": "http://arxiv.org/abs/2403.09065v1", "date": "2024-03-14", "relevancy": 2.3832, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4806}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4777}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4716}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20Semantic%20Segmentation%20Meets%20Frequency%20Aliasing&body=Title%3A%20When%20Semantic%20Segmentation%20Meets%20Frequency%20Aliasing%0AAuthor%3A%20Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Despite%20recent%20advancements%20in%20semantic%20segmentation%2C%20where%20and%20what%20pixels%0Aare%20hard%20to%20segment%20remains%20largely%20unexplored.%20Existing%20research%20only%0Aseparates%20an%20image%20into%20easy%20and%20hard%20regions%20and%20empirically%20observes%20the%0Alatter%20are%20associated%20with%20object%20boundaries.%20In%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20analysis%20of%20hard%20pixel%20errors%2C%20categorizing%20them%20into%20three%0Atypes%3A%20false%20responses%2C%20merging%20mistakes%2C%20and%20displacements.%20Our%20findings%0Areveal%20a%20quantitative%20association%20between%20hard%20pixels%20and%20aliasing%2C%20which%20is%0Adistortion%20caused%20by%20the%20overlapping%20of%20frequency%20components%20in%20the%20Fourier%0Adomain%20during%20downsampling.%20To%20identify%20the%20frequencies%20responsible%20for%0Aaliasing%2C%20we%20propose%20using%20the%20equivalent%20sampling%20rate%20to%20calculate%20the%0ANyquist%20frequency%2C%20which%20marks%20the%20threshold%20for%20aliasing.%20Then%2C%20we%20introduce%0Athe%20aliasing%20score%20as%20a%20metric%20to%20quantify%20the%20extent%20of%20aliasing.%20While%0Apositively%20correlated%20with%20the%20proposed%20aliasing%20score%2C%20three%20types%20of%20hard%0Apixels%20exhibit%20different%20patterns.%20Here%2C%20we%20propose%20two%20novel%20de-aliasing%0Afilter%20%28DAF%29%20and%20frequency%20mixing%20%28FreqMix%29%20modules%20to%20alleviate%20aliasing%0Adegradation%20by%20accurately%20removing%20or%20adjusting%20frequencies%20higher%20than%20the%0ANyquist%20frequency.%20The%20DAF%20precisely%20removes%20the%20frequencies%20responsible%20for%0Aaliasing%20before%20downsampling%2C%20while%20the%20FreqMix%20dynamically%20selects%0Ahigh-frequency%20components%20within%20the%20encoder%20block.%20Experimental%20results%0Ademonstrate%20consistent%20improvements%20in%20semantic%20segmentation%20and%20low-light%0Ainstance%20segmentation%20tasks.%20The%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Linwei-Chen/Seg-Aliasing%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09065v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Semantic%20Segmentation%20Meets%20Frequency%20Aliasing&entry.906535625=Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu&entry.1292438233=%20%20Despite%20recent%20advancements%20in%20semantic%20segmentation%2C%20where%20and%20what%20pixels%0Aare%20hard%20to%20segment%20remains%20largely%20unexplored.%20Existing%20research%20only%0Aseparates%20an%20image%20into%20easy%20and%20hard%20regions%20and%20empirically%20observes%20the%0Alatter%20are%20associated%20with%20object%20boundaries.%20In%20this%20paper%2C%20we%20conduct%20a%0Acomprehensive%20analysis%20of%20hard%20pixel%20errors%2C%20categorizing%20them%20into%20three%0Atypes%3A%20false%20responses%2C%20merging%20mistakes%2C%20and%20displacements.%20Our%20findings%0Areveal%20a%20quantitative%20association%20between%20hard%20pixels%20and%20aliasing%2C%20which%20is%0Adistortion%20caused%20by%20the%20overlapping%20of%20frequency%20components%20in%20the%20Fourier%0Adomain%20during%20downsampling.%20To%20identify%20the%20frequencies%20responsible%20for%0Aaliasing%2C%20we%20propose%20using%20the%20equivalent%20sampling%20rate%20to%20calculate%20the%0ANyquist%20frequency%2C%20which%20marks%20the%20threshold%20for%20aliasing.%20Then%2C%20we%20introduce%0Athe%20aliasing%20score%20as%20a%20metric%20to%20quantify%20the%20extent%20of%20aliasing.%20While%0Apositively%20correlated%20with%20the%20proposed%20aliasing%20score%2C%20three%20types%20of%20hard%0Apixels%20exhibit%20different%20patterns.%20Here%2C%20we%20propose%20two%20novel%20de-aliasing%0Afilter%20%28DAF%29%20and%20frequency%20mixing%20%28FreqMix%29%20modules%20to%20alleviate%20aliasing%0Adegradation%20by%20accurately%20removing%20or%20adjusting%20frequencies%20higher%20than%20the%0ANyquist%20frequency.%20The%20DAF%20precisely%20removes%20the%20frequencies%20responsible%20for%0Aaliasing%20before%20downsampling%2C%20while%20the%20FreqMix%20dynamically%20selects%0Ahigh-frequency%20components%20within%20the%20encoder%20block.%20Experimental%20results%0Ademonstrate%20consistent%20improvements%20in%20semantic%20segmentation%20and%20low-light%0Ainstance%20segmentation%20tasks.%20The%20code%20is%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/Linwei-Chen/Seg-Aliasing%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09065v1&entry.124074799=Read"},
{"title": "PFStorer: Personalized Face Restoration and Super-Resolution", "author": "Tuomas Varanka and Tapani Toivonen and Soumya Tripathy and Guoying Zhao and Erman Acar", "abstract": "  Recent developments in face restoration have achieved remarkable results in\nproducing high-quality and lifelike outputs. The stunning results however often\nfail to be faithful with respect to the identity of the person as the models\nlack necessary context. In this paper, we explore the potential of personalized\nface restoration with diffusion models. In our approach a restoration model is\npersonalized using a few images of the identity, leading to tailored\nrestoration with respect to the identity while retaining fine-grained details.\nBy using independent trainable blocks for personalization, the rich prior of a\nbase restoration model can be exploited to its fullest. To avoid the model\nrelying on parts of identity left in the conditioning low-quality images, a\ngenerative regularizer is employed. With a learnable parameter, the model\nlearns to balance between the details generated based on the input image and\nthe degree of personalization. Moreover, we improve the training pipeline of\nface restoration models to enable an alignment-free approach. We showcase the\nrobust capabilities of our approach in several real-world scenarios with\nmultiple identities, demonstrating our method's ability to generate\nfine-grained details with faithful restoration. In the user study we evaluate\nthe perceptual quality and faithfulness of the genereated details, with our\nmethod being voted best 61% of the time compared to the second best with 25% of\nthe votes.\n", "link": "http://arxiv.org/abs/2403.08436v1", "date": "2024-03-13", "relevancy": 2.3781, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6165}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5893}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5747}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PFStorer%3A%20Personalized%20Face%20Restoration%20and%20Super-Resolution&body=Title%3A%20PFStorer%3A%20Personalized%20Face%20Restoration%20and%20Super-Resolution%0AAuthor%3A%20Tuomas%20Varanka%20and%20Tapani%20Toivonen%20and%20Soumya%20Tripathy%20and%20Guoying%20Zhao%20and%20Erman%20Acar%0AAbstract%3A%20%20%20Recent%20developments%20in%20face%20restoration%20have%20achieved%20remarkable%20results%20in%0Aproducing%20high-quality%20and%20lifelike%20outputs.%20The%20stunning%20results%20however%20often%0Afail%20to%20be%20faithful%20with%20respect%20to%20the%20identity%20of%20the%20person%20as%20the%20models%0Alack%20necessary%20context.%20In%20this%20paper%2C%20we%20explore%20the%20potential%20of%20personalized%0Aface%20restoration%20with%20diffusion%20models.%20In%20our%20approach%20a%20restoration%20model%20is%0Apersonalized%20using%20a%20few%20images%20of%20the%20identity%2C%20leading%20to%20tailored%0Arestoration%20with%20respect%20to%20the%20identity%20while%20retaining%20fine-grained%20details.%0ABy%20using%20independent%20trainable%20blocks%20for%20personalization%2C%20the%20rich%20prior%20of%20a%0Abase%20restoration%20model%20can%20be%20exploited%20to%20its%20fullest.%20To%20avoid%20the%20model%0Arelying%20on%20parts%20of%20identity%20left%20in%20the%20conditioning%20low-quality%20images%2C%20a%0Agenerative%20regularizer%20is%20employed.%20With%20a%20learnable%20parameter%2C%20the%20model%0Alearns%20to%20balance%20between%20the%20details%20generated%20based%20on%20the%20input%20image%20and%0Athe%20degree%20of%20personalization.%20Moreover%2C%20we%20improve%20the%20training%20pipeline%20of%0Aface%20restoration%20models%20to%20enable%20an%20alignment-free%20approach.%20We%20showcase%20the%0Arobust%20capabilities%20of%20our%20approach%20in%20several%20real-world%20scenarios%20with%0Amultiple%20identities%2C%20demonstrating%20our%20method%27s%20ability%20to%20generate%0Afine-grained%20details%20with%20faithful%20restoration.%20In%20the%20user%20study%20we%20evaluate%0Athe%20perceptual%20quality%20and%20faithfulness%20of%20the%20genereated%20details%2C%20with%20our%0Amethod%20being%20voted%20best%2061%25%20of%20the%20time%20compared%20to%20the%20second%20best%20with%2025%25%20of%0Athe%20votes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08436v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PFStorer%3A%20Personalized%20Face%20Restoration%20and%20Super-Resolution&entry.906535625=Tuomas%20Varanka%20and%20Tapani%20Toivonen%20and%20Soumya%20Tripathy%20and%20Guoying%20Zhao%20and%20Erman%20Acar&entry.1292438233=%20%20Recent%20developments%20in%20face%20restoration%20have%20achieved%20remarkable%20results%20in%0Aproducing%20high-quality%20and%20lifelike%20outputs.%20The%20stunning%20results%20however%20often%0Afail%20to%20be%20faithful%20with%20respect%20to%20the%20identity%20of%20the%20person%20as%20the%20models%0Alack%20necessary%20context.%20In%20this%20paper%2C%20we%20explore%20the%20potential%20of%20personalized%0Aface%20restoration%20with%20diffusion%20models.%20In%20our%20approach%20a%20restoration%20model%20is%0Apersonalized%20using%20a%20few%20images%20of%20the%20identity%2C%20leading%20to%20tailored%0Arestoration%20with%20respect%20to%20the%20identity%20while%20retaining%20fine-grained%20details.%0ABy%20using%20independent%20trainable%20blocks%20for%20personalization%2C%20the%20rich%20prior%20of%20a%0Abase%20restoration%20model%20can%20be%20exploited%20to%20its%20fullest.%20To%20avoid%20the%20model%0Arelying%20on%20parts%20of%20identity%20left%20in%20the%20conditioning%20low-quality%20images%2C%20a%0Agenerative%20regularizer%20is%20employed.%20With%20a%20learnable%20parameter%2C%20the%20model%0Alearns%20to%20balance%20between%20the%20details%20generated%20based%20on%20the%20input%20image%20and%0Athe%20degree%20of%20personalization.%20Moreover%2C%20we%20improve%20the%20training%20pipeline%20of%0Aface%20restoration%20models%20to%20enable%20an%20alignment-free%20approach.%20We%20showcase%20the%0Arobust%20capabilities%20of%20our%20approach%20in%20several%20real-world%20scenarios%20with%0Amultiple%20identities%2C%20demonstrating%20our%20method%27s%20ability%20to%20generate%0Afine-grained%20details%20with%20faithful%20restoration.%20In%20the%20user%20study%20we%20evaluate%0Athe%20perceptual%20quality%20and%20faithfulness%20of%20the%20genereated%20details%2C%20with%20our%0Amethod%20being%20voted%20best%2061%25%20of%20the%20time%20compared%20to%20the%20second%20best%20with%2025%25%20of%0Athe%20votes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08436v1&entry.124074799=Read"},
{"title": "Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse\n  Mixture-of-Experts", "author": "Byeongjun Park and Hyojun Go and Jin-Young Kim and Sangmin Woo and Seokil Ham and Changick Kim", "abstract": "  Diffusion models have achieved remarkable success across a range of\ngenerative tasks. Recent efforts to enhance diffusion model architectures have\nreimagined them as a form of multi-task learning, where each task corresponds\nto a denoising task at a specific noise level. While these efforts have focused\non parameter isolation and task routing, they fall short of capturing detailed\ninter-task relationships and risk losing semantic information, respectively. In\nresponse, we introduce Switch Diffusion Transformer (Switch-DiT), which\nestablishes inter-task relationships between conflicting tasks without\ncompromising semantic information. To achieve this, we employ a sparse\nmixture-of-experts within each transformer block to utilize semantic\ninformation and facilitate handling conflicts in tasks through parameter\nisolation. Additionally, we propose a diffusion prior loss, encouraging similar\ntasks to share their denoising paths while isolating conflicting ones. Through\nthese, each transformer block contains a shared expert across all tasks, where\nthe common and task-specific denoising paths enable the diffusion model to\nconstruct its beneficial way of synergizing denoising tasks. Extensive\nexperiments validate the effectiveness of our approach in improving both image\nquality and convergence rate, and further analysis demonstrates that Switch-DiT\nconstructs tailored denoising paths across various generation scenarios.\n", "link": "http://arxiv.org/abs/2403.09176v1", "date": "2024-03-14", "relevancy": 2.3761, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.636}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5722}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Switch%20Diffusion%20Transformer%3A%20Synergizing%20Denoising%20Tasks%20with%20Sparse%0A%20%20Mixture-of-Experts&body=Title%3A%20Switch%20Diffusion%20Transformer%3A%20Synergizing%20Denoising%20Tasks%20with%20Sparse%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Byeongjun%20Park%20and%20Hyojun%20Go%20and%20Jin-Young%20Kim%20and%20Sangmin%20Woo%20and%20Seokil%20Ham%20and%20Changick%20Kim%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20remarkable%20success%20across%20a%20range%20of%0Agenerative%20tasks.%20Recent%20efforts%20to%20enhance%20diffusion%20model%20architectures%20have%0Areimagined%20them%20as%20a%20form%20of%20multi-task%20learning%2C%20where%20each%20task%20corresponds%0Ato%20a%20denoising%20task%20at%20a%20specific%20noise%20level.%20While%20these%20efforts%20have%20focused%0Aon%20parameter%20isolation%20and%20task%20routing%2C%20they%20fall%20short%20of%20capturing%20detailed%0Ainter-task%20relationships%20and%20risk%20losing%20semantic%20information%2C%20respectively.%20In%0Aresponse%2C%20we%20introduce%20Switch%20Diffusion%20Transformer%20%28Switch-DiT%29%2C%20which%0Aestablishes%20inter-task%20relationships%20between%20conflicting%20tasks%20without%0Acompromising%20semantic%20information.%20To%20achieve%20this%2C%20we%20employ%20a%20sparse%0Amixture-of-experts%20within%20each%20transformer%20block%20to%20utilize%20semantic%0Ainformation%20and%20facilitate%20handling%20conflicts%20in%20tasks%20through%20parameter%0Aisolation.%20Additionally%2C%20we%20propose%20a%20diffusion%20prior%20loss%2C%20encouraging%20similar%0Atasks%20to%20share%20their%20denoising%20paths%20while%20isolating%20conflicting%20ones.%20Through%0Athese%2C%20each%20transformer%20block%20contains%20a%20shared%20expert%20across%20all%20tasks%2C%20where%0Athe%20common%20and%20task-specific%20denoising%20paths%20enable%20the%20diffusion%20model%20to%0Aconstruct%20its%20beneficial%20way%20of%20synergizing%20denoising%20tasks.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20approach%20in%20improving%20both%20image%0Aquality%20and%20convergence%20rate%2C%20and%20further%20analysis%20demonstrates%20that%20Switch-DiT%0Aconstructs%20tailored%20denoising%20paths%20across%20various%20generation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09176v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switch%20Diffusion%20Transformer%3A%20Synergizing%20Denoising%20Tasks%20with%20Sparse%0A%20%20Mixture-of-Experts&entry.906535625=Byeongjun%20Park%20and%20Hyojun%20Go%20and%20Jin-Young%20Kim%20and%20Sangmin%20Woo%20and%20Seokil%20Ham%20and%20Changick%20Kim&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20remarkable%20success%20across%20a%20range%20of%0Agenerative%20tasks.%20Recent%20efforts%20to%20enhance%20diffusion%20model%20architectures%20have%0Areimagined%20them%20as%20a%20form%20of%20multi-task%20learning%2C%20where%20each%20task%20corresponds%0Ato%20a%20denoising%20task%20at%20a%20specific%20noise%20level.%20While%20these%20efforts%20have%20focused%0Aon%20parameter%20isolation%20and%20task%20routing%2C%20they%20fall%20short%20of%20capturing%20detailed%0Ainter-task%20relationships%20and%20risk%20losing%20semantic%20information%2C%20respectively.%20In%0Aresponse%2C%20we%20introduce%20Switch%20Diffusion%20Transformer%20%28Switch-DiT%29%2C%20which%0Aestablishes%20inter-task%20relationships%20between%20conflicting%20tasks%20without%0Acompromising%20semantic%20information.%20To%20achieve%20this%2C%20we%20employ%20a%20sparse%0Amixture-of-experts%20within%20each%20transformer%20block%20to%20utilize%20semantic%0Ainformation%20and%20facilitate%20handling%20conflicts%20in%20tasks%20through%20parameter%0Aisolation.%20Additionally%2C%20we%20propose%20a%20diffusion%20prior%20loss%2C%20encouraging%20similar%0Atasks%20to%20share%20their%20denoising%20paths%20while%20isolating%20conflicting%20ones.%20Through%0Athese%2C%20each%20transformer%20block%20contains%20a%20shared%20expert%20across%20all%20tasks%2C%20where%0Athe%20common%20and%20task-specific%20denoising%20paths%20enable%20the%20diffusion%20model%20to%0Aconstruct%20its%20beneficial%20way%20of%20synergizing%20denoising%20tasks.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20approach%20in%20improving%20both%20image%0Aquality%20and%20convergence%20rate%2C%20and%20further%20analysis%20demonstrates%20that%20Switch-DiT%0Aconstructs%20tailored%20denoising%20paths%20across%20various%20generation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09176v1&entry.124074799=Read"},
{"title": "Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution", "author": "Samuel Sze and Lars Kunze", "abstract": "  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2403.08748v1", "date": "2024-03-13", "relevancy": 2.3754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5989}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5795}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&body=Title%3A%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution%0AAuthor%3A%20Samuel%20Sze%20and%20Lars%20Kunze%0AAbstract%3A%20%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08748v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&entry.906535625=Samuel%20Sze%20and%20Lars%20Kunze&entry.1292438233=%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08748v1&entry.124074799=Read"},
{"title": "Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D\n  Prior", "author": "Cheng Chen and Xiaofeng Yang and Fan Yang and Chengzeng Feng and Zhoujie Fu and Chuan-Sheng Foo and Guosheng Lin and Fayao Liu", "abstract": "  Recent works on text-to-3d generation show that using only 2D diffusion\nsupervision for 3D generation tends to produce results with inconsistent\nappearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals\nwith extra legs). Existing methods mainly address this issue by retraining\ndiffusion models with images rendered from 3D data to ensure multi-view\nconsistency while struggling to balance 2D generation quality with 3D\nconsistency. In this paper, we present a new framework Sculpt3D that equips the\ncurrent pipeline with explicit injection of 3D priors from retrieved reference\nobjects without re-training the 2D diffusion model. Specifically, we\ndemonstrate that high-quality and diverse 3D geometry can be guaranteed by\nkeypoints supervision through a sparse ray sampling approach. Moreover, to\nensure accurate appearances of different views, we further modulate the output\nof the 2D diffusion model to the correct patterns of the template views without\naltering the generated object's style. These two decoupled designs effectively\nharness 3D information from reference objects to generate 3D objects while\npreserving the generation quality of the 2D diffusion model. Extensive\nexperiments show our method can largely improve the multi-view consistency\nwhile retaining fidelity and diversity. Our project page is available at:\nhttps://stellarcheng.github.io/Sculpt3D/.\n", "link": "http://arxiv.org/abs/2403.09140v1", "date": "2024-03-14", "relevancy": 2.3713, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.589}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5785}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sculpt3D%3A%20Multi-View%20Consistent%20Text-to-3D%20Generation%20with%20Sparse%203D%0A%20%20Prior&body=Title%3A%20Sculpt3D%3A%20Multi-View%20Consistent%20Text-to-3D%20Generation%20with%20Sparse%203D%0A%20%20Prior%0AAuthor%3A%20Cheng%20Chen%20and%20Xiaofeng%20Yang%20and%20Fan%20Yang%20and%20Chengzeng%20Feng%20and%20Zhoujie%20Fu%20and%20Chuan-Sheng%20Foo%20and%20Guosheng%20Lin%20and%20Fayao%20Liu%0AAbstract%3A%20%20%20Recent%20works%20on%20text-to-3d%20generation%20show%20that%20using%20only%202D%20diffusion%0Asupervision%20for%203D%20generation%20tends%20to%20produce%20results%20with%20inconsistent%0Aappearances%20%28e.g.%2C%20faces%20on%20the%20back%20view%29%20and%20inaccurate%20shapes%20%28e.g.%2C%20animals%0Awith%20extra%20legs%29.%20Existing%20methods%20mainly%20address%20this%20issue%20by%20retraining%0Adiffusion%20models%20with%20images%20rendered%20from%203D%20data%20to%20ensure%20multi-view%0Aconsistency%20while%20struggling%20to%20balance%202D%20generation%20quality%20with%203D%0Aconsistency.%20In%20this%20paper%2C%20we%20present%20a%20new%20framework%20Sculpt3D%20that%20equips%20the%0Acurrent%20pipeline%20with%20explicit%20injection%20of%203D%20priors%20from%20retrieved%20reference%0Aobjects%20without%20re-training%20the%202D%20diffusion%20model.%20Specifically%2C%20we%0Ademonstrate%20that%20high-quality%20and%20diverse%203D%20geometry%20can%20be%20guaranteed%20by%0Akeypoints%20supervision%20through%20a%20sparse%20ray%20sampling%20approach.%20Moreover%2C%20to%0Aensure%20accurate%20appearances%20of%20different%20views%2C%20we%20further%20modulate%20the%20output%0Aof%20the%202D%20diffusion%20model%20to%20the%20correct%20patterns%20of%20the%20template%20views%20without%0Aaltering%20the%20generated%20object%27s%20style.%20These%20two%20decoupled%20designs%20effectively%0Aharness%203D%20information%20from%20reference%20objects%20to%20generate%203D%20objects%20while%0Apreserving%20the%20generation%20quality%20of%20the%202D%20diffusion%20model.%20Extensive%0Aexperiments%20show%20our%20method%20can%20largely%20improve%20the%20multi-view%20consistency%0Awhile%20retaining%20fidelity%20and%20diversity.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//stellarcheng.github.io/Sculpt3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09140v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sculpt3D%3A%20Multi-View%20Consistent%20Text-to-3D%20Generation%20with%20Sparse%203D%0A%20%20Prior&entry.906535625=Cheng%20Chen%20and%20Xiaofeng%20Yang%20and%20Fan%20Yang%20and%20Chengzeng%20Feng%20and%20Zhoujie%20Fu%20and%20Chuan-Sheng%20Foo%20and%20Guosheng%20Lin%20and%20Fayao%20Liu&entry.1292438233=%20%20Recent%20works%20on%20text-to-3d%20generation%20show%20that%20using%20only%202D%20diffusion%0Asupervision%20for%203D%20generation%20tends%20to%20produce%20results%20with%20inconsistent%0Aappearances%20%28e.g.%2C%20faces%20on%20the%20back%20view%29%20and%20inaccurate%20shapes%20%28e.g.%2C%20animals%0Awith%20extra%20legs%29.%20Existing%20methods%20mainly%20address%20this%20issue%20by%20retraining%0Adiffusion%20models%20with%20images%20rendered%20from%203D%20data%20to%20ensure%20multi-view%0Aconsistency%20while%20struggling%20to%20balance%202D%20generation%20quality%20with%203D%0Aconsistency.%20In%20this%20paper%2C%20we%20present%20a%20new%20framework%20Sculpt3D%20that%20equips%20the%0Acurrent%20pipeline%20with%20explicit%20injection%20of%203D%20priors%20from%20retrieved%20reference%0Aobjects%20without%20re-training%20the%202D%20diffusion%20model.%20Specifically%2C%20we%0Ademonstrate%20that%20high-quality%20and%20diverse%203D%20geometry%20can%20be%20guaranteed%20by%0Akeypoints%20supervision%20through%20a%20sparse%20ray%20sampling%20approach.%20Moreover%2C%20to%0Aensure%20accurate%20appearances%20of%20different%20views%2C%20we%20further%20modulate%20the%20output%0Aof%20the%202D%20diffusion%20model%20to%20the%20correct%20patterns%20of%20the%20template%20views%20without%0Aaltering%20the%20generated%20object%27s%20style.%20These%20two%20decoupled%20designs%20effectively%0Aharness%203D%20information%20from%20reference%20objects%20to%20generate%203D%20objects%20while%0Apreserving%20the%20generation%20quality%20of%20the%202D%20diffusion%20model.%20Extensive%0Aexperiments%20show%20our%20method%20can%20largely%20improve%20the%20multi-view%20consistency%0Awhile%20retaining%20fidelity%20and%20diversity.%20Our%20project%20page%20is%20available%20at%3A%0Ahttps%3A//stellarcheng.github.io/Sculpt3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09140v1&entry.124074799=Read"},
{"title": "Historical Astronomical Diagrams Decomposition in Geometric Primitives", "author": "Syrine Kalleli and Scott Trigg and S\u00e9gol\u00e8ne Albouy and Mathieu Husson and Mathieu Aubry", "abstract": "  Automatically extracting the geometric content from the hundreds of thousands\nof diagrams drawn in historical manuscripts would enable historians to study\nthe diffusion of astronomical knowledge on a global scale. However,\nstate-of-the-art vectorization methods, often designed to tackle modern data,\nare not adapted to the complexity and diversity of historical astronomical\ndiagrams. Our contribution is thus twofold. First, we introduce a unique\ndataset of 303 astronomical diagrams from diverse traditions, ranging from the\nXIIth to the XVIIIth century, annotated with more than 3000 line segments,\ncircles and arcs. Second, we develop a model that builds on DINO-DETR to enable\nthe prediction of multiple geometric primitives. We show that it can be trained\nsolely on synthetic data and accurately predict primitives on our challenging\ndataset. Our approach widely improves over the LETR baseline, which is\nrestricted to lines, by introducing a meaningful parametrization for multiple\nprimitives, jointly training for detection and parameter refinement, using\ndeformable attention and training on rich synthetic data. Our dataset and code\nare available on our webpage.\n", "link": "http://arxiv.org/abs/2403.08721v1", "date": "2024-03-13", "relevancy": 1.8972, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4617}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4487}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives&body=Title%3A%20Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives%0AAuthor%3A%20Syrine%20Kalleli%20and%20Scott%20Trigg%20and%20S%C3%A9gol%C3%A8ne%20Albouy%20and%20Mathieu%20Husson%20and%20Mathieu%20Aubry%0AAbstract%3A%20%20%20Automatically%20extracting%20the%20geometric%20content%20from%20the%20hundreds%20of%20thousands%0Aof%20diagrams%20drawn%20in%20historical%20manuscripts%20would%20enable%20historians%20to%20study%0Athe%20diffusion%20of%20astronomical%20knowledge%20on%20a%20global%20scale.%20However%2C%0Astate-of-the-art%20vectorization%20methods%2C%20often%20designed%20to%20tackle%20modern%20data%2C%0Aare%20not%20adapted%20to%20the%20complexity%20and%20diversity%20of%20historical%20astronomical%0Adiagrams.%20Our%20contribution%20is%20thus%20twofold.%20First%2C%20we%20introduce%20a%20unique%0Adataset%20of%20303%20astronomical%20diagrams%20from%20diverse%20traditions%2C%20ranging%20from%20the%0AXIIth%20to%20the%20XVIIIth%20century%2C%20annotated%20with%20more%20than%203000%20line%20segments%2C%0Acircles%20and%20arcs.%20Second%2C%20we%20develop%20a%20model%20that%20builds%20on%20DINO-DETR%20to%20enable%0Athe%20prediction%20of%20multiple%20geometric%20primitives.%20We%20show%20that%20it%20can%20be%20trained%0Asolely%20on%20synthetic%20data%20and%20accurately%20predict%20primitives%20on%20our%20challenging%0Adataset.%20Our%20approach%20widely%20improves%20over%20the%20LETR%20baseline%2C%20which%20is%0Arestricted%20to%20lines%2C%20by%20introducing%20a%20meaningful%20parametrization%20for%20multiple%0Aprimitives%2C%20jointly%20training%20for%20detection%20and%20parameter%20refinement%2C%20using%0Adeformable%20attention%20and%20training%20on%20rich%20synthetic%20data.%20Our%20dataset%20and%20code%0Aare%20available%20on%20our%20webpage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08721v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Historical%20Astronomical%20Diagrams%20Decomposition%20in%20Geometric%20Primitives&entry.906535625=Syrine%20Kalleli%20and%20Scott%20Trigg%20and%20S%C3%A9gol%C3%A8ne%20Albouy%20and%20Mathieu%20Husson%20and%20Mathieu%20Aubry&entry.1292438233=%20%20Automatically%20extracting%20the%20geometric%20content%20from%20the%20hundreds%20of%20thousands%0Aof%20diagrams%20drawn%20in%20historical%20manuscripts%20would%20enable%20historians%20to%20study%0Athe%20diffusion%20of%20astronomical%20knowledge%20on%20a%20global%20scale.%20However%2C%0Astate-of-the-art%20vectorization%20methods%2C%20often%20designed%20to%20tackle%20modern%20data%2C%0Aare%20not%20adapted%20to%20the%20complexity%20and%20diversity%20of%20historical%20astronomical%0Adiagrams.%20Our%20contribution%20is%20thus%20twofold.%20First%2C%20we%20introduce%20a%20unique%0Adataset%20of%20303%20astronomical%20diagrams%20from%20diverse%20traditions%2C%20ranging%20from%20the%0AXIIth%20to%20the%20XVIIIth%20century%2C%20annotated%20with%20more%20than%203000%20line%20segments%2C%0Acircles%20and%20arcs.%20Second%2C%20we%20develop%20a%20model%20that%20builds%20on%20DINO-DETR%20to%20enable%0Athe%20prediction%20of%20multiple%20geometric%20primitives.%20We%20show%20that%20it%20can%20be%20trained%0Asolely%20on%20synthetic%20data%20and%20accurately%20predict%20primitives%20on%20our%20challenging%0Adataset.%20Our%20approach%20widely%20improves%20over%20the%20LETR%20baseline%2C%20which%20is%0Arestricted%20to%20lines%2C%20by%20introducing%20a%20meaningful%20parametrization%20for%20multiple%0Aprimitives%2C%20jointly%20training%20for%20detection%20and%20parameter%20refinement%2C%20using%0Adeformable%20attention%20and%20training%20on%20rich%20synthetic%20data.%20Our%20dataset%20and%20code%0Aare%20available%20on%20our%20webpage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08721v1&entry.124074799=Read"},
{"title": "To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation", "author": "Abdul Hameed Azeemi and Ihsan Ayyub Qazi and Agha Ali Raza", "abstract": "  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n", "link": "http://arxiv.org/abs/2403.09259v1", "date": "2024-03-14", "relevancy": 2.1633, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&body=Title%3A%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation%0AAuthor%3A%20Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20HUDS%2C%20a%20hybrid%20AL%20strategy%20for%20domain%20adaptation%20in%20NMT%20that%20combines%0Auncertainty%20and%20diversity%20for%20sentence%20selection.%20HUDS%20computes%20uncertainty%0Ascores%20for%20unlabeled%20sentences%20and%20subsequently%20stratifies%20them.%20It%20then%0Aclusters%20sentence%20embeddings%20within%20each%20stratum%20using%20k-MEANS%20and%20computes%0Adiversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%20hybrid%20score%20that%0Acombines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%20top%20instances%20for%0Aannotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%20German-English%0Adatasets%20demonstrate%20the%20better%20performance%20of%20HUDS%20over%20other%20strong%20AL%0Abaselines.%20We%20analyze%20the%20sentence%20selection%20with%20HUDS%20and%20show%20that%20it%0Aprioritizes%20diverse%20instances%20having%20high%20model%20uncertainty%20for%20annotation%20in%0Aearly%20AL%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09259v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&entry.906535625=Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza&entry.1292438233=%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20HUDS%2C%20a%20hybrid%20AL%20strategy%20for%20domain%20adaptation%20in%20NMT%20that%20combines%0Auncertainty%20and%20diversity%20for%20sentence%20selection.%20HUDS%20computes%20uncertainty%0Ascores%20for%20unlabeled%20sentences%20and%20subsequently%20stratifies%20them.%20It%20then%0Aclusters%20sentence%20embeddings%20within%20each%20stratum%20using%20k-MEANS%20and%20computes%0Adiversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%20hybrid%20score%20that%0Acombines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%20top%20instances%20for%0Aannotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%20German-English%0Adatasets%20demonstrate%20the%20better%20performance%20of%20HUDS%20over%20other%20strong%20AL%0Abaselines.%20We%20analyze%20the%20sentence%20selection%20with%20HUDS%20and%20show%20that%20it%0Aprioritizes%20diverse%20instances%20having%20high%20model%20uncertainty%20for%20annotation%20in%0Aearly%20AL%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09259v1&entry.124074799=Read"},
{"title": "CLOAF: CoLlisiOn-Aware Human Flow", "author": "Andrey Davydov and Martin Engilberge and Mathieu Salzmann and Pascal Fua", "abstract": "  Even the best current algorithms for estimating body 3D shape and pose yield\nresults that include body self-intersections. In this paper, we present CLOAF,\nwhich exploits the diffeomorphic nature of Ordinary Differential Equations to\neliminate such self-intersections while still imposing body shape constraints.\nWe show that, unlike earlier approaches to addressing this issue, ours\ncompletely eliminates the self-intersections without compromising the accuracy\nof the reconstructions. Being differentiable, CLOAF can be used to fine-tune\npose and shape estimation baselines to improve their overall performance and\neliminate self-intersections in their predictions. Furthermore, we demonstrate\nhow our CLOAF strategy can be applied to practically any motion field induced\nby the user. CLOAF also makes it possible to edit motion to interact with the\nenvironment without worrying about potential collision or loss of body-shape\nprior.\n", "link": "http://arxiv.org/abs/2403.09050v1", "date": "2024-03-14", "relevancy": 1.4929, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4842}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4803}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLOAF%3A%20CoLlisiOn-Aware%20Human%20Flow&body=Title%3A%20CLOAF%3A%20CoLlisiOn-Aware%20Human%20Flow%0AAuthor%3A%20Andrey%20Davydov%20and%20Martin%20Engilberge%20and%20Mathieu%20Salzmann%20and%20Pascal%20Fua%0AAbstract%3A%20%20%20Even%20the%20best%20current%20algorithms%20for%20estimating%20body%203D%20shape%20and%20pose%20yield%0Aresults%20that%20include%20body%20self-intersections.%20In%20this%20paper%2C%20we%20present%20CLOAF%2C%0Awhich%20exploits%20the%20diffeomorphic%20nature%20of%20Ordinary%20Differential%20Equations%20to%0Aeliminate%20such%20self-intersections%20while%20still%20imposing%20body%20shape%20constraints.%0AWe%20show%20that%2C%20unlike%20earlier%20approaches%20to%20addressing%20this%20issue%2C%20ours%0Acompletely%20eliminates%20the%20self-intersections%20without%20compromising%20the%20accuracy%0Aof%20the%20reconstructions.%20Being%20differentiable%2C%20CLOAF%20can%20be%20used%20to%20fine-tune%0Apose%20and%20shape%20estimation%20baselines%20to%20improve%20their%20overall%20performance%20and%0Aeliminate%20self-intersections%20in%20their%20predictions.%20Furthermore%2C%20we%20demonstrate%0Ahow%20our%20CLOAF%20strategy%20can%20be%20applied%20to%20practically%20any%20motion%20field%20induced%0Aby%20the%20user.%20CLOAF%20also%20makes%20it%20possible%20to%20edit%20motion%20to%20interact%20with%20the%0Aenvironment%20without%20worrying%20about%20potential%20collision%20or%20loss%20of%20body-shape%0Aprior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09050v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLOAF%3A%20CoLlisiOn-Aware%20Human%20Flow&entry.906535625=Andrey%20Davydov%20and%20Martin%20Engilberge%20and%20Mathieu%20Salzmann%20and%20Pascal%20Fua&entry.1292438233=%20%20Even%20the%20best%20current%20algorithms%20for%20estimating%20body%203D%20shape%20and%20pose%20yield%0Aresults%20that%20include%20body%20self-intersections.%20In%20this%20paper%2C%20we%20present%20CLOAF%2C%0Awhich%20exploits%20the%20diffeomorphic%20nature%20of%20Ordinary%20Differential%20Equations%20to%0Aeliminate%20such%20self-intersections%20while%20still%20imposing%20body%20shape%20constraints.%0AWe%20show%20that%2C%20unlike%20earlier%20approaches%20to%20addressing%20this%20issue%2C%20ours%0Acompletely%20eliminates%20the%20self-intersections%20without%20compromising%20the%20accuracy%0Aof%20the%20reconstructions.%20Being%20differentiable%2C%20CLOAF%20can%20be%20used%20to%20fine-tune%0Apose%20and%20shape%20estimation%20baselines%20to%20improve%20their%20overall%20performance%20and%0Aeliminate%20self-intersections%20in%20their%20predictions.%20Furthermore%2C%20we%20demonstrate%0Ahow%20our%20CLOAF%20strategy%20can%20be%20applied%20to%20practically%20any%20motion%20field%20induced%0Aby%20the%20user.%20CLOAF%20also%20makes%20it%20possible%20to%20edit%20motion%20to%20interact%20with%20the%0Aenvironment%20without%20worrying%20about%20potential%20collision%20or%20loss%20of%20body-shape%0Aprior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09050v1&entry.124074799=Read"},
{"title": "Assessing the Impact of Sequence Length Learning on Classification Tasks\n  for Transformer Encoder Models", "author": "Jean-Thomas Baillargeon and Luc Lamontagne", "abstract": "  Classification algorithms using Transformer architectures can be affected by\nthe sequence length learning problem whenever observations from different\nclasses have a different length distribution. This problem causes models to use\nsequence length as a predictive feature instead of relying on important textual\ninformation. Although most public datasets are not affected by this problem,\nprivately owned corpora for fields such as medicine and insurance may carry\nthis data bias. The exploitation of this sequence length feature poses\nchallenges throughout the value chain as these machine learning models can be\nused in critical applications. In this paper, we empirically expose this\nproblem and present approaches to minimize its impacts.\n", "link": "http://arxiv.org/abs/2212.08399v2", "date": "2024-03-14", "relevancy": 1.7828, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4759}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4334}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models&body=Title%3A%20Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models%0AAuthor%3A%20Jean-Thomas%20Baillargeon%20and%20Luc%20Lamontagne%0AAbstract%3A%20%20%20Classification%20algorithms%20using%20Transformer%20architectures%20can%20be%20affected%20by%0Athe%20sequence%20length%20learning%20problem%20whenever%20observations%20from%20different%0Aclasses%20have%20a%20different%20length%20distribution.%20This%20problem%20causes%20models%20to%20use%0Asequence%20length%20as%20a%20predictive%20feature%20instead%20of%20relying%20on%20important%20textual%0Ainformation.%20Although%20most%20public%20datasets%20are%20not%20affected%20by%20this%20problem%2C%0Aprivately%20owned%20corpora%20for%20fields%20such%20as%20medicine%20and%20insurance%20may%20carry%0Athis%20data%20bias.%20The%20exploitation%20of%20this%20sequence%20length%20feature%20poses%0Achallenges%20throughout%20the%20value%20chain%20as%20these%20machine%20learning%20models%20can%20be%0Aused%20in%20critical%20applications.%20In%20this%20paper%2C%20we%20empirically%20expose%20this%0Aproblem%20and%20present%20approaches%20to%20minimize%20its%20impacts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08399v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Impact%20of%20Sequence%20Length%20Learning%20on%20Classification%20Tasks%0A%20%20for%20Transformer%20Encoder%20Models&entry.906535625=Jean-Thomas%20Baillargeon%20and%20Luc%20Lamontagne&entry.1292438233=%20%20Classification%20algorithms%20using%20Transformer%20architectures%20can%20be%20affected%20by%0Athe%20sequence%20length%20learning%20problem%20whenever%20observations%20from%20different%0Aclasses%20have%20a%20different%20length%20distribution.%20This%20problem%20causes%20models%20to%20use%0Asequence%20length%20as%20a%20predictive%20feature%20instead%20of%20relying%20on%20important%20textual%0Ainformation.%20Although%20most%20public%20datasets%20are%20not%20affected%20by%20this%20problem%2C%0Aprivately%20owned%20corpora%20for%20fields%20such%20as%20medicine%20and%20insurance%20may%20carry%0Athis%20data%20bias.%20The%20exploitation%20of%20this%20sequence%20length%20feature%20poses%0Achallenges%20throughout%20the%20value%20chain%20as%20these%20machine%20learning%20models%20can%20be%0Aused%20in%20critical%20applications.%20In%20this%20paper%2C%20we%20empirically%20expose%20this%0Aproblem%20and%20present%20approaches%20to%20minimize%20its%20impacts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08399v2&entry.124074799=Read"},
{"title": "Discrete approximations of Gaussian smoothing and Gaussian derivatives", "author": "Tony Lindeberg", "abstract": "  This paper develops an in-depth treatment concerning the problem of\napproximating the Gaussian smoothing and Gaussian derivative computations in\nscale-space theory for application on discrete data. With close connections to\nprevious axiomatic treatments of continuous and discrete scale-space theory, we\nconsider three main ways discretizing these scale-space operations in terms of\nexplicit discrete convolutions, based on either (i) sampling the Gaussian\nkernels and the Gaussian derivative kernels, (ii) locally integrating the\nGaussian kernels and the Gaussian derivative kernels over each pixel support\nregion and (iii) basing the scale-space analysis on the discrete analogue of\nthe Gaussian kernel, and then computing derivative approximations by applying\nsmall-support central difference operators to the spatially smoothed image\ndata.\n  We study the properties of these three main discretization methods both\ntheoretically and experimentally, and characterize their performance by\nquantitative measures, including the results they give rise to with respect to\nthe task of scale selection, investigated for four different use cases, and\nwith emphasis on the behaviour at fine scales. The results show that the\nsampled Gaussian kernels and derivatives as well as the integrated Gaussian\nkernels and derivatives perform very poorly at very fine scales. At very fine\nscales, the discrete analogue of the Gaussian kernel with its corresponding\ndiscrete derivative approximations performs substantially better. The sampled\nGaussian kernel and the sampled Gaussian derivatives do, on the other hand,\nlead to numerically very good approximations of the corresponding continuous\nresults, when the scale parameter is sufficiently large, in the experiments\npresented in the paper, when the scale parameter is greater than a value of\nabout 1, in units of the grid spacing.\n", "link": "http://arxiv.org/abs/2311.11317v4", "date": "2024-03-14", "relevancy": 1.78, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4625}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives&body=Title%3A%20Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20%20%20This%20paper%20develops%20an%20in-depth%20treatment%20concerning%20the%20problem%20of%0Aapproximating%20the%20Gaussian%20smoothing%20and%20Gaussian%20derivative%20computations%20in%0Ascale-space%20theory%20for%20application%20on%20discrete%20data.%20With%20close%20connections%20to%0Aprevious%20axiomatic%20treatments%20of%20continuous%20and%20discrete%20scale-space%20theory%2C%20we%0Aconsider%20three%20main%20ways%20discretizing%20these%20scale-space%20operations%20in%20terms%20of%0Aexplicit%20discrete%20convolutions%2C%20based%20on%20either%20%28i%29%20sampling%20the%20Gaussian%0Akernels%20and%20the%20Gaussian%20derivative%20kernels%2C%20%28ii%29%20locally%20integrating%20the%0AGaussian%20kernels%20and%20the%20Gaussian%20derivative%20kernels%20over%20each%20pixel%20support%0Aregion%20and%20%28iii%29%20basing%20the%20scale-space%20analysis%20on%20the%20discrete%20analogue%20of%0Athe%20Gaussian%20kernel%2C%20and%20then%20computing%20derivative%20approximations%20by%20applying%0Asmall-support%20central%20difference%20operators%20to%20the%20spatially%20smoothed%20image%0Adata.%0A%20%20We%20study%20the%20properties%20of%20these%20three%20main%20discretization%20methods%20both%0Atheoretically%20and%20experimentally%2C%20and%20characterize%20their%20performance%20by%0Aquantitative%20measures%2C%20including%20the%20results%20they%20give%20rise%20to%20with%20respect%20to%0Athe%20task%20of%20scale%20selection%2C%20investigated%20for%20four%20different%20use%20cases%2C%20and%0Awith%20emphasis%20on%20the%20behaviour%20at%20fine%20scales.%20The%20results%20show%20that%20the%0Asampled%20Gaussian%20kernels%20and%20derivatives%20as%20well%20as%20the%20integrated%20Gaussian%0Akernels%20and%20derivatives%20perform%20very%20poorly%20at%20very%20fine%20scales.%20At%20very%20fine%0Ascales%2C%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20with%20its%20corresponding%0Adiscrete%20derivative%20approximations%20performs%20substantially%20better.%20The%20sampled%0AGaussian%20kernel%20and%20the%20sampled%20Gaussian%20derivatives%20do%2C%20on%20the%20other%20hand%2C%0Alead%20to%20numerically%20very%20good%20approximations%20of%20the%20corresponding%20continuous%0Aresults%2C%20when%20the%20scale%20parameter%20is%20sufficiently%20large%2C%20in%20the%20experiments%0Apresented%20in%20the%20paper%2C%20when%20the%20scale%20parameter%20is%20greater%20than%20a%20value%20of%0Aabout%201%2C%20in%20units%20of%20the%20grid%20spacing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11317v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives&entry.906535625=Tony%20Lindeberg&entry.1292438233=%20%20This%20paper%20develops%20an%20in-depth%20treatment%20concerning%20the%20problem%20of%0Aapproximating%20the%20Gaussian%20smoothing%20and%20Gaussian%20derivative%20computations%20in%0Ascale-space%20theory%20for%20application%20on%20discrete%20data.%20With%20close%20connections%20to%0Aprevious%20axiomatic%20treatments%20of%20continuous%20and%20discrete%20scale-space%20theory%2C%20we%0Aconsider%20three%20main%20ways%20discretizing%20these%20scale-space%20operations%20in%20terms%20of%0Aexplicit%20discrete%20convolutions%2C%20based%20on%20either%20%28i%29%20sampling%20the%20Gaussian%0Akernels%20and%20the%20Gaussian%20derivative%20kernels%2C%20%28ii%29%20locally%20integrating%20the%0AGaussian%20kernels%20and%20the%20Gaussian%20derivative%20kernels%20over%20each%20pixel%20support%0Aregion%20and%20%28iii%29%20basing%20the%20scale-space%20analysis%20on%20the%20discrete%20analogue%20of%0Athe%20Gaussian%20kernel%2C%20and%20then%20computing%20derivative%20approximations%20by%20applying%0Asmall-support%20central%20difference%20operators%20to%20the%20spatially%20smoothed%20image%0Adata.%0A%20%20We%20study%20the%20properties%20of%20these%20three%20main%20discretization%20methods%20both%0Atheoretically%20and%20experimentally%2C%20and%20characterize%20their%20performance%20by%0Aquantitative%20measures%2C%20including%20the%20results%20they%20give%20rise%20to%20with%20respect%20to%0Athe%20task%20of%20scale%20selection%2C%20investigated%20for%20four%20different%20use%20cases%2C%20and%0Awith%20emphasis%20on%20the%20behaviour%20at%20fine%20scales.%20The%20results%20show%20that%20the%0Asampled%20Gaussian%20kernels%20and%20derivatives%20as%20well%20as%20the%20integrated%20Gaussian%0Akernels%20and%20derivatives%20perform%20very%20poorly%20at%20very%20fine%20scales.%20At%20very%20fine%0Ascales%2C%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20with%20its%20corresponding%0Adiscrete%20derivative%20approximations%20performs%20substantially%20better.%20The%20sampled%0AGaussian%20kernel%20and%20the%20sampled%20Gaussian%20derivatives%20do%2C%20on%20the%20other%20hand%2C%0Alead%20to%20numerically%20very%20good%20approximations%20of%20the%20corresponding%20continuous%0Aresults%2C%20when%20the%20scale%20parameter%20is%20sufficiently%20large%2C%20in%20the%20experiments%0Apresented%20in%20the%20paper%2C%20when%20the%20scale%20parameter%20is%20greater%20than%20a%20value%20of%0Aabout%201%2C%20in%20units%20of%20the%20grid%20spacing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11317v4&entry.124074799=Read"},
{"title": "Computational Imaging for Machine Perception: Transferring Semantic\n  Segmentation beyond Aberrations", "author": "Qi Jiang and Hao Shi and Shaohua Gao and Jiaming Zhang and Kailun Yang and Lei Sun and Huajian Ni and Kaiwei Wang", "abstract": "  Semantic scene understanding with Minimalist Optical Systems (MOS) in mobile\nand wearable applications remains a challenge due to the corrupted imaging\nquality induced by optical aberrations. However, previous works only focus on\nimproving the subjective imaging quality through the Computational Imaging (CI)\ntechnique, ignoring the feasibility of advancing semantic segmentation. In this\npaper, we pioneer the investigation of Semantic Segmentation under Optical\nAberrations (SSOA) with MOS. To benchmark SSOA, we construct Virtual Prototype\nLens (VPL) groups through optical simulation, generating Cityscapes-ab and\nKITTI-360-ab datasets under different behaviors and levels of aberrations. We\nlook into SSOA via an unsupervised domain adaptation perspective to address the\nscarcity of labeled aberration data in real-world scenarios. Further, we\npropose Computational Imaging Assisted Domain Adaptation (CIADA) to leverage\nprior knowledge of CI for robust performance in SSOA. Based on our benchmark,\nwe conduct experiments on the robustness of classical segmenters against\naberrations. In addition, extensive evaluations of possible solutions to SSOA\nreveal that CIADA achieves superior performance under all aberration\ndistributions, bridging the gap between computational imaging and downstream\napplications for MOS. The project page is at\nhttps://github.com/zju-jiangqi/CIADA.\n", "link": "http://arxiv.org/abs/2211.11257v2", "date": "2024-03-14", "relevancy": 2.186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5574}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.529}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Computational%20Imaging%20for%20Machine%20Perception%3A%20Transferring%20Semantic%0A%20%20Segmentation%20beyond%20Aberrations&body=Title%3A%20Computational%20Imaging%20for%20Machine%20Perception%3A%20Transferring%20Semantic%0A%20%20Segmentation%20beyond%20Aberrations%0AAuthor%3A%20Qi%20Jiang%20and%20Hao%20Shi%20and%20Shaohua%20Gao%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Lei%20Sun%20and%20Huajian%20Ni%20and%20Kaiwei%20Wang%0AAbstract%3A%20%20%20Semantic%20scene%20understanding%20with%20Minimalist%20Optical%20Systems%20%28MOS%29%20in%20mobile%0Aand%20wearable%20applications%20remains%20a%20challenge%20due%20to%20the%20corrupted%20imaging%0Aquality%20induced%20by%20optical%20aberrations.%20However%2C%20previous%20works%20only%20focus%20on%0Aimproving%20the%20subjective%20imaging%20quality%20through%20the%20Computational%20Imaging%20%28CI%29%0Atechnique%2C%20ignoring%20the%20feasibility%20of%20advancing%20semantic%20segmentation.%20In%20this%0Apaper%2C%20we%20pioneer%20the%20investigation%20of%20Semantic%20Segmentation%20under%20Optical%0AAberrations%20%28SSOA%29%20with%20MOS.%20To%20benchmark%20SSOA%2C%20we%20construct%20Virtual%20Prototype%0ALens%20%28VPL%29%20groups%20through%20optical%20simulation%2C%20generating%20Cityscapes-ab%20and%0AKITTI-360-ab%20datasets%20under%20different%20behaviors%20and%20levels%20of%20aberrations.%20We%0Alook%20into%20SSOA%20via%20an%20unsupervised%20domain%20adaptation%20perspective%20to%20address%20the%0Ascarcity%20of%20labeled%20aberration%20data%20in%20real-world%20scenarios.%20Further%2C%20we%0Apropose%20Computational%20Imaging%20Assisted%20Domain%20Adaptation%20%28CIADA%29%20to%20leverage%0Aprior%20knowledge%20of%20CI%20for%20robust%20performance%20in%20SSOA.%20Based%20on%20our%20benchmark%2C%0Awe%20conduct%20experiments%20on%20the%20robustness%20of%20classical%20segmenters%20against%0Aaberrations.%20In%20addition%2C%20extensive%20evaluations%20of%20possible%20solutions%20to%20SSOA%0Areveal%20that%20CIADA%20achieves%20superior%20performance%20under%20all%20aberration%0Adistributions%2C%20bridging%20the%20gap%20between%20computational%20imaging%20and%20downstream%0Aapplications%20for%20MOS.%20The%20project%20page%20is%20at%0Ahttps%3A//github.com/zju-jiangqi/CIADA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.11257v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Imaging%20for%20Machine%20Perception%3A%20Transferring%20Semantic%0A%20%20Segmentation%20beyond%20Aberrations&entry.906535625=Qi%20Jiang%20and%20Hao%20Shi%20and%20Shaohua%20Gao%20and%20Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Lei%20Sun%20and%20Huajian%20Ni%20and%20Kaiwei%20Wang&entry.1292438233=%20%20Semantic%20scene%20understanding%20with%20Minimalist%20Optical%20Systems%20%28MOS%29%20in%20mobile%0Aand%20wearable%20applications%20remains%20a%20challenge%20due%20to%20the%20corrupted%20imaging%0Aquality%20induced%20by%20optical%20aberrations.%20However%2C%20previous%20works%20only%20focus%20on%0Aimproving%20the%20subjective%20imaging%20quality%20through%20the%20Computational%20Imaging%20%28CI%29%0Atechnique%2C%20ignoring%20the%20feasibility%20of%20advancing%20semantic%20segmentation.%20In%20this%0Apaper%2C%20we%20pioneer%20the%20investigation%20of%20Semantic%20Segmentation%20under%20Optical%0AAberrations%20%28SSOA%29%20with%20MOS.%20To%20benchmark%20SSOA%2C%20we%20construct%20Virtual%20Prototype%0ALens%20%28VPL%29%20groups%20through%20optical%20simulation%2C%20generating%20Cityscapes-ab%20and%0AKITTI-360-ab%20datasets%20under%20different%20behaviors%20and%20levels%20of%20aberrations.%20We%0Alook%20into%20SSOA%20via%20an%20unsupervised%20domain%20adaptation%20perspective%20to%20address%20the%0Ascarcity%20of%20labeled%20aberration%20data%20in%20real-world%20scenarios.%20Further%2C%20we%0Apropose%20Computational%20Imaging%20Assisted%20Domain%20Adaptation%20%28CIADA%29%20to%20leverage%0Aprior%20knowledge%20of%20CI%20for%20robust%20performance%20in%20SSOA.%20Based%20on%20our%20benchmark%2C%0Awe%20conduct%20experiments%20on%20the%20robustness%20of%20classical%20segmenters%20against%0Aaberrations.%20In%20addition%2C%20extensive%20evaluations%20of%20possible%20solutions%20to%20SSOA%0Areveal%20that%20CIADA%20achieves%20superior%20performance%20under%20all%20aberration%0Adistributions%2C%20bridging%20the%20gap%20between%20computational%20imaging%20and%20downstream%0Aapplications%20for%20MOS.%20The%20project%20page%20is%20at%0Ahttps%3A//github.com/zju-jiangqi/CIADA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.11257v2&entry.124074799=Read"},
{"title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis", "author": "Xiaoxiao Sun and Xingjian Leng and Zijian Wang and Yang Yang and Zi Huang and Liang Zheng", "abstract": "  Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.\n", "link": "http://arxiv.org/abs/2310.04414v3", "date": "2024-03-13", "relevancy": 1.5055, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5136}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CIFAR-10-Warehouse%3A%20Broad%20and%20More%20Realistic%20Testbeds%20in%20Model%0A%20%20Generalization%20Analysis&body=Title%3A%20CIFAR-10-Warehouse%3A%20Broad%20and%20More%20Realistic%20Testbeds%20in%20Model%0A%20%20Generalization%20Analysis%0AAuthor%3A%20Xiaoxiao%20Sun%20and%20Xingjian%20Leng%20and%20Zijian%20Wang%20and%20Yang%20Yang%20and%20Zi%20Huang%20and%20Liang%20Zheng%0AAbstract%3A%20%20%20Analyzing%20model%20performance%20in%20various%20unseen%20environments%20is%20a%20critical%0Aresearch%20problem%20in%20the%20machine%20learning%20community.%20To%20study%20this%20problem%2C%20it%0Ais%20important%20to%20construct%20a%20testbed%20with%20out-of-distribution%20test%20sets%20that%0Ahave%20broad%20coverage%20of%20environmental%20discrepancies.%20However%2C%20existing%20testbeds%0Atypically%20either%20have%20a%20small%20number%20of%20domains%20or%20are%20synthesized%20by%20image%0Acorruptions%2C%20hindering%20algorithm%20design%20that%20demonstrates%20real-world%0Aeffectiveness.%20In%20this%20paper%2C%20we%20introduce%20CIFAR-10-Warehouse%2C%20consisting%20of%0A180%20datasets%20collected%20by%20prompting%20image%20search%20engines%20and%20diffusion%20models%0Ain%20various%20ways.%20Generally%20sized%20between%20300%20and%208%2C000%20images%2C%20the%20datasets%0Acontain%20natural%20images%2C%20cartoons%2C%20certain%20colors%2C%20or%20objects%20that%20do%20not%0Anaturally%20appear.%20With%20CIFAR-10-W%2C%20we%20aim%20to%20enhance%20the%20evaluation%20and%20deepen%0Athe%20understanding%20of%20two%20generalization%20tasks%3A%20domain%20generalization%20and%20model%0Aaccuracy%20prediction%20in%20various%20out-of-distribution%20environments.%20We%20conduct%0Aextensive%20benchmarking%20and%20comparison%20experiments%20and%20show%20that%20CIFAR-10-W%0Aoffers%20new%20and%20interesting%20insights%20inherent%20to%20these%20tasks.%20We%20also%20discuss%0Aother%20fields%20that%20would%20benefit%20from%20CIFAR-10-W.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04414v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CIFAR-10-Warehouse%3A%20Broad%20and%20More%20Realistic%20Testbeds%20in%20Model%0A%20%20Generalization%20Analysis&entry.906535625=Xiaoxiao%20Sun%20and%20Xingjian%20Leng%20and%20Zijian%20Wang%20and%20Yang%20Yang%20and%20Zi%20Huang%20and%20Liang%20Zheng&entry.1292438233=%20%20Analyzing%20model%20performance%20in%20various%20unseen%20environments%20is%20a%20critical%0Aresearch%20problem%20in%20the%20machine%20learning%20community.%20To%20study%20this%20problem%2C%20it%0Ais%20important%20to%20construct%20a%20testbed%20with%20out-of-distribution%20test%20sets%20that%0Ahave%20broad%20coverage%20of%20environmental%20discrepancies.%20However%2C%20existing%20testbeds%0Atypically%20either%20have%20a%20small%20number%20of%20domains%20or%20are%20synthesized%20by%20image%0Acorruptions%2C%20hindering%20algorithm%20design%20that%20demonstrates%20real-world%0Aeffectiveness.%20In%20this%20paper%2C%20we%20introduce%20CIFAR-10-Warehouse%2C%20consisting%20of%0A180%20datasets%20collected%20by%20prompting%20image%20search%20engines%20and%20diffusion%20models%0Ain%20various%20ways.%20Generally%20sized%20between%20300%20and%208%2C000%20images%2C%20the%20datasets%0Acontain%20natural%20images%2C%20cartoons%2C%20certain%20colors%2C%20or%20objects%20that%20do%20not%0Anaturally%20appear.%20With%20CIFAR-10-W%2C%20we%20aim%20to%20enhance%20the%20evaluation%20and%20deepen%0Athe%20understanding%20of%20two%20generalization%20tasks%3A%20domain%20generalization%20and%20model%0Aaccuracy%20prediction%20in%20various%20out-of-distribution%20environments.%20We%20conduct%0Aextensive%20benchmarking%20and%20comparison%20experiments%20and%20show%20that%20CIFAR-10-W%0Aoffers%20new%20and%20interesting%20insights%20inherent%20to%20these%20tasks.%20We%20also%20discuss%0Aother%20fields%20that%20would%20benefit%20from%20CIFAR-10-W.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04414v3&entry.124074799=Read"},
{"title": "Actor-Critic Physics-informed Neural Lyapunov Control", "author": "Jiarui Wang and Mahyar Fazlyab", "abstract": "  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n", "link": "http://arxiv.org/abs/2403.08448v1", "date": "2024-03-13", "relevancy": 1.9742, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control&body=Title%3A%20Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control%0AAuthor%3A%20Jiarui%20Wang%20and%20Mahyar%20Fazlyab%0AAbstract%3A%20%20%20Designing%20control%20policies%20for%20stabilization%20tasks%20with%20provable%20guarantees%0Ais%20a%20long-standing%20problem%20in%20nonlinear%20control.%20A%20crucial%20performance%20metric%0Ais%20the%20size%20of%20the%20resulting%20region%20of%20attraction%2C%20which%20essentially%20serves%20as%0Aa%20robustness%20%22margin%22%20of%20the%20closed-loop%20system%20against%20uncertainties.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20to%20train%20a%20stabilizing%20neural%20network%20controller%0Aalong%20with%20its%20corresponding%20Lyapunov%20certificate%2C%20aiming%20to%20maximize%20the%0Aresulting%20region%20of%20attraction%20while%20respecting%20the%20actuation%20constraints.%0ACrucial%20to%20our%20approach%20is%20the%20use%20of%20Zubov%27s%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20precisely%20characterizes%20the%20true%20region%20of%20attraction%20of%20a%20given%0Acontrol%20policy.%20Our%20framework%20follows%20an%20actor-critic%20pattern%20where%20we%0Aalternate%20between%20improving%20the%20control%20policy%20%28actor%29%20and%20learning%20a%20Zubov%0Afunction%20%28critic%29.%20Finally%2C%20we%20compute%20the%20largest%20certifiable%20region%20of%0Aattraction%20by%20invoking%20an%20SMT%20solver%20after%20the%20training%20procedure.%20Our%0Anumerical%20experiments%20on%20several%20design%20problems%20show%20consistent%20and%0Asignificant%20improvements%20in%20the%20size%20of%20the%20resulting%20region%20of%20attraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08448v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control&entry.906535625=Jiarui%20Wang%20and%20Mahyar%20Fazlyab&entry.1292438233=%20%20Designing%20control%20policies%20for%20stabilization%20tasks%20with%20provable%20guarantees%0Ais%20a%20long-standing%20problem%20in%20nonlinear%20control.%20A%20crucial%20performance%20metric%0Ais%20the%20size%20of%20the%20resulting%20region%20of%20attraction%2C%20which%20essentially%20serves%20as%0Aa%20robustness%20%22margin%22%20of%20the%20closed-loop%20system%20against%20uncertainties.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20to%20train%20a%20stabilizing%20neural%20network%20controller%0Aalong%20with%20its%20corresponding%20Lyapunov%20certificate%2C%20aiming%20to%20maximize%20the%0Aresulting%20region%20of%20attraction%20while%20respecting%20the%20actuation%20constraints.%0ACrucial%20to%20our%20approach%20is%20the%20use%20of%20Zubov%27s%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20precisely%20characterizes%20the%20true%20region%20of%20attraction%20of%20a%20given%0Acontrol%20policy.%20Our%20framework%20follows%20an%20actor-critic%20pattern%20where%20we%0Aalternate%20between%20improving%20the%20control%20policy%20%28actor%29%20and%20learning%20a%20Zubov%0Afunction%20%28critic%29.%20Finally%2C%20we%20compute%20the%20largest%20certifiable%20region%20of%0Aattraction%20by%20invoking%20an%20SMT%20solver%20after%20the%20training%20procedure.%20Our%0Anumerical%20experiments%20on%20several%20design%20problems%20show%20consistent%20and%0Asignificant%20improvements%20in%20the%20size%20of%20the%20resulting%20region%20of%20attraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08448v1&entry.124074799=Read"},
{"title": "RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during\n  Robot Arm Movement with Neural Radiance Fields", "author": "Chang Liu and Kejian Shi and Kaichen Zhou and Haoxiao Wang and Jiyao Zhang and Hao Dong", "abstract": "  Robotic research encounters a significant hurdle when it comes to the\nintricate task of grasping objects that come in various shapes, materials, and\ntextures. Unlike many prior investigations that heavily leaned on specialized\npoint-cloud cameras or abundant RGB visual data to gather 3D insights for\nobject-grasping missions, this paper introduces a pioneering approach called\nRGBGrasp. This method depends on a limited set of RGB views to perceive the 3D\nsurroundings containing transparent and specular objects and achieve accurate\ngrasping. Our method utilizes pre-trained depth prediction models to establish\ngeometry constraints, enabling precise 3D structure estimation, even under\nlimited view conditions. Finally, we integrate hash encoding and a proposal\nsampler strategy to significantly accelerate the 3D reconstruction process.\nThese innovations significantly enhance the adaptability and effectiveness of\nour algorithm in real-world scenarios. Through comprehensive experimental\nvalidations, we demonstrate that RGBGrasp achieves remarkable success across a\nwide spectrum of object-grasping scenarios, establishing it as a promising\nsolution for real-world robotic manipulation tasks. The demonstrations of our\nmethod can be found on: https://sites.google.com/view/rgbgrasp\n", "link": "http://arxiv.org/abs/2311.16592v2", "date": "2024-03-14", "relevancy": 1.8319, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6275}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5962}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RGBGrasp%3A%20Image-based%20Object%20Grasping%20by%20Capturing%20Multiple%20Views%20during%0A%20%20Robot%20Arm%20Movement%20with%20Neural%20Radiance%20Fields&body=Title%3A%20RGBGrasp%3A%20Image-based%20Object%20Grasping%20by%20Capturing%20Multiple%20Views%20during%0A%20%20Robot%20Arm%20Movement%20with%20Neural%20Radiance%20Fields%0AAuthor%3A%20Chang%20Liu%20and%20Kejian%20Shi%20and%20Kaichen%20Zhou%20and%20Haoxiao%20Wang%20and%20Jiyao%20Zhang%20and%20Hao%20Dong%0AAbstract%3A%20%20%20Robotic%20research%20encounters%20a%20significant%20hurdle%20when%20it%20comes%20to%20the%0Aintricate%20task%20of%20grasping%20objects%20that%20come%20in%20various%20shapes%2C%20materials%2C%20and%0Atextures.%20Unlike%20many%20prior%20investigations%20that%20heavily%20leaned%20on%20specialized%0Apoint-cloud%20cameras%20or%20abundant%20RGB%20visual%20data%20to%20gather%203D%20insights%20for%0Aobject-grasping%20missions%2C%20this%20paper%20introduces%20a%20pioneering%20approach%20called%0ARGBGrasp.%20This%20method%20depends%20on%20a%20limited%20set%20of%20RGB%20views%20to%20perceive%20the%203D%0Asurroundings%20containing%20transparent%20and%20specular%20objects%20and%20achieve%20accurate%0Agrasping.%20Our%20method%20utilizes%20pre-trained%20depth%20prediction%20models%20to%20establish%0Ageometry%20constraints%2C%20enabling%20precise%203D%20structure%20estimation%2C%20even%20under%0Alimited%20view%20conditions.%20Finally%2C%20we%20integrate%20hash%20encoding%20and%20a%20proposal%0Asampler%20strategy%20to%20significantly%20accelerate%20the%203D%20reconstruction%20process.%0AThese%20innovations%20significantly%20enhance%20the%20adaptability%20and%20effectiveness%20of%0Aour%20algorithm%20in%20real-world%20scenarios.%20Through%20comprehensive%20experimental%0Avalidations%2C%20we%20demonstrate%20that%20RGBGrasp%20achieves%20remarkable%20success%20across%20a%0Awide%20spectrum%20of%20object-grasping%20scenarios%2C%20establishing%20it%20as%20a%20promising%0Asolution%20for%20real-world%20robotic%20manipulation%20tasks.%20The%20demonstrations%20of%20our%0Amethod%20can%20be%20found%20on%3A%20https%3A//sites.google.com/view/rgbgrasp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16592v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGBGrasp%3A%20Image-based%20Object%20Grasping%20by%20Capturing%20Multiple%20Views%20during%0A%20%20Robot%20Arm%20Movement%20with%20Neural%20Radiance%20Fields&entry.906535625=Chang%20Liu%20and%20Kejian%20Shi%20and%20Kaichen%20Zhou%20and%20Haoxiao%20Wang%20and%20Jiyao%20Zhang%20and%20Hao%20Dong&entry.1292438233=%20%20Robotic%20research%20encounters%20a%20significant%20hurdle%20when%20it%20comes%20to%20the%0Aintricate%20task%20of%20grasping%20objects%20that%20come%20in%20various%20shapes%2C%20materials%2C%20and%0Atextures.%20Unlike%20many%20prior%20investigations%20that%20heavily%20leaned%20on%20specialized%0Apoint-cloud%20cameras%20or%20abundant%20RGB%20visual%20data%20to%20gather%203D%20insights%20for%0Aobject-grasping%20missions%2C%20this%20paper%20introduces%20a%20pioneering%20approach%20called%0ARGBGrasp.%20This%20method%20depends%20on%20a%20limited%20set%20of%20RGB%20views%20to%20perceive%20the%203D%0Asurroundings%20containing%20transparent%20and%20specular%20objects%20and%20achieve%20accurate%0Agrasping.%20Our%20method%20utilizes%20pre-trained%20depth%20prediction%20models%20to%20establish%0Ageometry%20constraints%2C%20enabling%20precise%203D%20structure%20estimation%2C%20even%20under%0Alimited%20view%20conditions.%20Finally%2C%20we%20integrate%20hash%20encoding%20and%20a%20proposal%0Asampler%20strategy%20to%20significantly%20accelerate%20the%203D%20reconstruction%20process.%0AThese%20innovations%20significantly%20enhance%20the%20adaptability%20and%20effectiveness%20of%0Aour%20algorithm%20in%20real-world%20scenarios.%20Through%20comprehensive%20experimental%0Avalidations%2C%20we%20demonstrate%20that%20RGBGrasp%20achieves%20remarkable%20success%20across%20a%0Awide%20spectrum%20of%20object-grasping%20scenarios%2C%20establishing%20it%20as%20a%20promising%0Asolution%20for%20real-world%20robotic%20manipulation%20tasks.%20The%20demonstrations%20of%20our%0Amethod%20can%20be%20found%20on%3A%20https%3A//sites.google.com/view/rgbgrasp%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16592v2&entry.124074799=Read"},
{"title": "Semantic Residual Prompts for Continual Learning", "author": "Martin Menabue and Emanuele Frascaroli and Matteo Boschini and Enver Sangineto and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained\nmodel and focus training on a few parameter vectors termed prompts. Most of\nthese methods organize these vectors in a pool of key-value pairs, and use the\ninput image as query to retrieve the prompts (values). However, as keys are\nlearned while tasks progress, the prompting selection strategy is itself\nsubject to catastrophic forgetting, an issue often overlooked by existing\napproaches. For instance, prompts introduced to accommodate new tasks might end\nup interfering with previously learned prompts. To make the selection strategy\nmore stable, we ask a foundational model (CLIP) to select our prompt within a\ntwo-level adaptation mechanism. Specifically, the first level leverages\nstandard textual prompts for the CLIP textual encoder, leading to stable class\nprototypes. The second level, instead, uses these prototypes along with the\nquery image as keys to index a second pool. The retrieved prompts serve to\nadapt a pre-trained ViT, granting plasticity. In doing so, we also propose a\nnovel residual mechanism to transfer CLIP semantics to the ViT layers. Through\nextensive analysis on established CL benchmarks, we show that our method\nsignificantly outperforms both state-of-the-art CL approaches and the zero-shot\nCLIP test. Notably, our findings hold true even for datasets with a substantial\ndomain gap w.r.t. the pre-training knowledge of the backbone model, as\nshowcased by experiments on satellite imagery and medical datasets.\n", "link": "http://arxiv.org/abs/2403.06870v2", "date": "2024-03-14", "relevancy": 2.1455, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5823}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5059}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5027}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning&body=Title%3A%20Semantic%20Residual%20Prompts%20for%20Continual%20Learning%0AAuthor%3A%20Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20focus%20training%20on%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%0Athese%20methods%20organize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%2C%20and%20use%20the%0Ainput%20image%20as%20query%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%0Alearned%20while%20tasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%0Asubject%20to%20catastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%0Aapproaches.%20For%20instance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%0Aup%20interfering%20with%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%0Amore%20stable%2C%20we%20ask%20a%20foundational%20model%20%28CLIP%29%20to%20select%20our%20prompt%20within%20a%0Atwo-level%20adaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%0Astandard%20textual%20prompts%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06870v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Residual%20Prompts%20for%20Continual%20Learning&entry.906535625=Martin%20Menabue%20and%20Emanuele%20Frascaroli%20and%20Matteo%20Boschini%20and%20Enver%20Sangineto%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20Prompt-tuning%20methods%20for%20Continual%20Learning%20%28CL%29%20freeze%20a%20large%20pre-trained%0Amodel%20and%20focus%20training%20on%20a%20few%20parameter%20vectors%20termed%20prompts.%20Most%20of%0Athese%20methods%20organize%20these%20vectors%20in%20a%20pool%20of%20key-value%20pairs%2C%20and%20use%20the%0Ainput%20image%20as%20query%20to%20retrieve%20the%20prompts%20%28values%29.%20However%2C%20as%20keys%20are%0Alearned%20while%20tasks%20progress%2C%20the%20prompting%20selection%20strategy%20is%20itself%0Asubject%20to%20catastrophic%20forgetting%2C%20an%20issue%20often%20overlooked%20by%20existing%0Aapproaches.%20For%20instance%2C%20prompts%20introduced%20to%20accommodate%20new%20tasks%20might%20end%0Aup%20interfering%20with%20previously%20learned%20prompts.%20To%20make%20the%20selection%20strategy%0Amore%20stable%2C%20we%20ask%20a%20foundational%20model%20%28CLIP%29%20to%20select%20our%20prompt%20within%20a%0Atwo-level%20adaptation%20mechanism.%20Specifically%2C%20the%20first%20level%20leverages%0Astandard%20textual%20prompts%20for%20the%20CLIP%20textual%20encoder%2C%20leading%20to%20stable%20class%0Aprototypes.%20The%20second%20level%2C%20instead%2C%20uses%20these%20prototypes%20along%20with%20the%0Aquery%20image%20as%20keys%20to%20index%20a%20second%20pool.%20The%20retrieved%20prompts%20serve%20to%0Aadapt%20a%20pre-trained%20ViT%2C%20granting%20plasticity.%20In%20doing%20so%2C%20we%20also%20propose%20a%0Anovel%20residual%20mechanism%20to%20transfer%20CLIP%20semantics%20to%20the%20ViT%20layers.%20Through%0Aextensive%20analysis%20on%20established%20CL%20benchmarks%2C%20we%20show%20that%20our%20method%0Asignificantly%20outperforms%20both%20state-of-the-art%20CL%20approaches%20and%20the%20zero-shot%0ACLIP%20test.%20Notably%2C%20our%20findings%20hold%20true%20even%20for%20datasets%20with%20a%20substantial%0Adomain%20gap%20w.r.t.%20the%20pre-training%20knowledge%20of%20the%20backbone%20model%2C%20as%0Ashowcased%20by%20experiments%20on%20satellite%20imagery%20and%20medical%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06870v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


