<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241016.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage\n  Gaussian Splats", "author": "Chen Ziwen and Hao Tan and Kai Zhang and Sai Bi and Fujun Luan and Yicong Hong and Li Fuxin and Zexiang Xu", "abstract": "  We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is\ncapable of reconstructing a large scene from a long sequence of input images.\nSpecifically, our model can process 32 source images at 960x540 resolution\nwithin only 1.3 seconds on a single A100 80G GPU. Our architecture features a\nmixture of the recent Mamba2 blocks and the classical transformer blocks which\nallowed many more tokens to be processed than prior work, enhanced by efficient\ntoken merging and Gaussian pruning steps that balance between quality and\nefficiency. Unlike previous feed-forward models that are limited to processing\n1~4 input images and can only reconstruct a small portion of a large scene,\nLong-LRM reconstructs the entire scene in a single feed-forward step. On\nlarge-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method\nachieves performance comparable to optimization-based approaches while being\ntwo orders of magnitude more efficient. Project page:\nhttps://arthurhero.github.io/projects/llrm\n", "link": "http://arxiv.org/abs/2410.12781v1", "date": "2024-10-16", "relevancy": 3.2652, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7315}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6232}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-LRM%3A%20Long-sequence%20Large%20Reconstruction%20Model%20for%20Wide-coverage%0A%20%20Gaussian%20Splats&body=Title%3A%20Long-LRM%3A%20Long-sequence%20Large%20Reconstruction%20Model%20for%20Wide-coverage%0A%20%20Gaussian%20Splats%0AAuthor%3A%20Chen%20Ziwen%20and%20Hao%20Tan%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Fujun%20Luan%20and%20Yicong%20Hong%20and%20Li%20Fuxin%20and%20Zexiang%20Xu%0AAbstract%3A%20%20%20We%20propose%20Long-LRM%2C%20a%20generalizable%203D%20Gaussian%20reconstruction%20model%20that%20is%0Acapable%20of%20reconstructing%20a%20large%20scene%20from%20a%20long%20sequence%20of%20input%20images.%0ASpecifically%2C%20our%20model%20can%20process%2032%20source%20images%20at%20960x540%20resolution%0Awithin%20only%201.3%20seconds%20on%20a%20single%20A100%2080G%20GPU.%20Our%20architecture%20features%20a%0Amixture%20of%20the%20recent%20Mamba2%20blocks%20and%20the%20classical%20transformer%20blocks%20which%0Aallowed%20many%20more%20tokens%20to%20be%20processed%20than%20prior%20work%2C%20enhanced%20by%20efficient%0Atoken%20merging%20and%20Gaussian%20pruning%20steps%20that%20balance%20between%20quality%20and%0Aefficiency.%20Unlike%20previous%20feed-forward%20models%20that%20are%20limited%20to%20processing%0A1~4%20input%20images%20and%20can%20only%20reconstruct%20a%20small%20portion%20of%20a%20large%20scene%2C%0ALong-LRM%20reconstructs%20the%20entire%20scene%20in%20a%20single%20feed-forward%20step.%20On%0Alarge-scale%20scene%20datasets%20such%20as%20DL3DV-140%20and%20Tanks%20and%20Temples%2C%20our%20method%0Aachieves%20performance%20comparable%20to%20optimization-based%20approaches%20while%20being%0Atwo%20orders%20of%20magnitude%20more%20efficient.%20Project%20page%3A%0Ahttps%3A//arthurhero.github.io/projects/llrm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-LRM%253A%2520Long-sequence%2520Large%2520Reconstruction%2520Model%2520for%2520Wide-coverage%250A%2520%2520Gaussian%2520Splats%26entry.906535625%3DChen%2520Ziwen%2520and%2520Hao%2520Tan%2520and%2520Kai%2520Zhang%2520and%2520Sai%2520Bi%2520and%2520Fujun%2520Luan%2520and%2520Yicong%2520Hong%2520and%2520Li%2520Fuxin%2520and%2520Zexiang%2520Xu%26entry.1292438233%3D%2520%2520We%2520propose%2520Long-LRM%252C%2520a%2520generalizable%25203D%2520Gaussian%2520reconstruction%2520model%2520that%2520is%250Acapable%2520of%2520reconstructing%2520a%2520large%2520scene%2520from%2520a%2520long%2520sequence%2520of%2520input%2520images.%250ASpecifically%252C%2520our%2520model%2520can%2520process%252032%2520source%2520images%2520at%2520960x540%2520resolution%250Awithin%2520only%25201.3%2520seconds%2520on%2520a%2520single%2520A100%252080G%2520GPU.%2520Our%2520architecture%2520features%2520a%250Amixture%2520of%2520the%2520recent%2520Mamba2%2520blocks%2520and%2520the%2520classical%2520transformer%2520blocks%2520which%250Aallowed%2520many%2520more%2520tokens%2520to%2520be%2520processed%2520than%2520prior%2520work%252C%2520enhanced%2520by%2520efficient%250Atoken%2520merging%2520and%2520Gaussian%2520pruning%2520steps%2520that%2520balance%2520between%2520quality%2520and%250Aefficiency.%2520Unlike%2520previous%2520feed-forward%2520models%2520that%2520are%2520limited%2520to%2520processing%250A1~4%2520input%2520images%2520and%2520can%2520only%2520reconstruct%2520a%2520small%2520portion%2520of%2520a%2520large%2520scene%252C%250ALong-LRM%2520reconstructs%2520the%2520entire%2520scene%2520in%2520a%2520single%2520feed-forward%2520step.%2520On%250Alarge-scale%2520scene%2520datasets%2520such%2520as%2520DL3DV-140%2520and%2520Tanks%2520and%2520Temples%252C%2520our%2520method%250Aachieves%2520performance%2520comparable%2520to%2520optimization-based%2520approaches%2520while%2520being%250Atwo%2520orders%2520of%2520magnitude%2520more%2520efficient.%2520Project%2520page%253A%250Ahttps%253A//arthurhero.github.io/projects/llrm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-LRM%3A%20Long-sequence%20Large%20Reconstruction%20Model%20for%20Wide-coverage%0A%20%20Gaussian%20Splats&entry.906535625=Chen%20Ziwen%20and%20Hao%20Tan%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Fujun%20Luan%20and%20Yicong%20Hong%20and%20Li%20Fuxin%20and%20Zexiang%20Xu&entry.1292438233=%20%20We%20propose%20Long-LRM%2C%20a%20generalizable%203D%20Gaussian%20reconstruction%20model%20that%20is%0Acapable%20of%20reconstructing%20a%20large%20scene%20from%20a%20long%20sequence%20of%20input%20images.%0ASpecifically%2C%20our%20model%20can%20process%2032%20source%20images%20at%20960x540%20resolution%0Awithin%20only%201.3%20seconds%20on%20a%20single%20A100%2080G%20GPU.%20Our%20architecture%20features%20a%0Amixture%20of%20the%20recent%20Mamba2%20blocks%20and%20the%20classical%20transformer%20blocks%20which%0Aallowed%20many%20more%20tokens%20to%20be%20processed%20than%20prior%20work%2C%20enhanced%20by%20efficient%0Atoken%20merging%20and%20Gaussian%20pruning%20steps%20that%20balance%20between%20quality%20and%0Aefficiency.%20Unlike%20previous%20feed-forward%20models%20that%20are%20limited%20to%20processing%0A1~4%20input%20images%20and%20can%20only%20reconstruct%20a%20small%20portion%20of%20a%20large%20scene%2C%0ALong-LRM%20reconstructs%20the%20entire%20scene%20in%20a%20single%20feed-forward%20step.%20On%0Alarge-scale%20scene%20datasets%20such%20as%20DL3DV-140%20and%20Tanks%20and%20Temples%2C%20our%20method%0Aachieves%20performance%20comparable%20to%20optimization-based%20approaches%20while%20being%0Atwo%20orders%20of%20magnitude%20more%20efficient.%20Project%20page%3A%0Ahttps%3A//arthurhero.github.io/projects/llrm%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12781v1&entry.124074799=Read"},
{"title": "Developing Generalist Foundation Models from a Multimodal Dataset for 3D\n  Computed Tomography", "author": "Ibrahim Ethem Hamamci and Sezgin Er and Furkan Almas and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Irem Dogan and Muhammed Furkan Dasdelen and Omer Faruk Durugol and Bastian Wittmann and Tamaz Amiranashvili and Enis Simsar and Mehmet Simsar and Emine Bensu Erdemir and Abdullah Alanbay and Anjany Sekuboyina and Berkan Lafci and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze", "abstract": "  While computer vision has achieved tremendous success with multimodal\nencoding and direct textual interaction with images via chat-based large\nlanguage models, similar advancements in medical imaging AI, particularly in 3D\nimaging, have been limited due to the scarcity of comprehensive datasets. To\naddress this critical gap, we introduce CT-RATE, the first dataset that pairs\n3D medical images with corresponding textual reports. CT-RATE comprises 25,692\nnon-contrast 3D chest CT scans from 21,304 unique patients. Through various\nreconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3\nmillion 2D slices. Each scan is accompanied by its corresponding radiology\nreport. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive\nlanguage-image pretraining framework designed for broad applications without\nthe need for task-specific training. We demonstrate how CT-CLIP can be used in\ntwo tasks: multi-abnormality detection and case retrieval. Remarkably, in\nmulti-abnormality detection, CT-CLIP outperforms state-of-the-art fully\nsupervised models across all key metrics, effectively eliminating the need for\nmanual annotation. In case retrieval, it efficiently retrieves relevant cases\nusing either image or textual queries, thereby enhancing knowledge\ndissemination. By combining CT-CLIP's vision encoder with a pretrained large\nlanguage model, we create CT-CHAT, a vision-language foundational chat model\nfor 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs\nderived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI\nassistants, underscoring the necessity for specialized methods in 3D medical\nimaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT\nnot only addresses critical challenges in 3D medical imaging but also lays the\ngroundwork for future innovations in medical AI and improved patient care.\n", "link": "http://arxiv.org/abs/2403.17834v2", "date": "2024-10-16", "relevancy": 3.2365, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6638}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6391}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developing%20Generalist%20Foundation%20Models%20from%20a%20Multimodal%20Dataset%20for%203D%0A%20%20Computed%20Tomography&body=Title%3A%20Developing%20Generalist%20Foundation%20Models%20from%20a%20Multimodal%20Dataset%20for%203D%0A%20%20Computed%20Tomography%0AAuthor%3A%20Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Furkan%20Almas%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Omer%20Faruk%20Durugol%20and%20Bastian%20Wittmann%20and%20Tamaz%20Amiranashvili%20and%20Enis%20Simsar%20and%20Mehmet%20Simsar%20and%20Emine%20Bensu%20Erdemir%20and%20Abdullah%20Alanbay%20and%20Anjany%20Sekuboyina%20and%20Berkan%20Lafci%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20While%20computer%20vision%20has%20achieved%20tremendous%20success%20with%20multimodal%0Aencoding%20and%20direct%20textual%20interaction%20with%20images%20via%20chat-based%20large%0Alanguage%20models%2C%20similar%20advancements%20in%20medical%20imaging%20AI%2C%20particularly%20in%203D%0Aimaging%2C%20have%20been%20limited%20due%20to%20the%20scarcity%20of%20comprehensive%20datasets.%20To%0Aaddress%20this%20critical%20gap%2C%20we%20introduce%20CT-RATE%2C%20the%20first%20dataset%20that%20pairs%0A3D%20medical%20images%20with%20corresponding%20textual%20reports.%20CT-RATE%20comprises%2025%2C692%0Anon-contrast%203D%20chest%20CT%20scans%20from%2021%2C304%20unique%20patients.%20Through%20various%0Areconstructions%2C%20these%20scans%20are%20expanded%20to%2050%2C188%20volumes%2C%20totaling%20over%2014.3%0Amillion%202D%20slices.%20Each%20scan%20is%20accompanied%20by%20its%20corresponding%20radiology%0Areport.%20Leveraging%20CT-RATE%2C%20we%20develop%20CT-CLIP%2C%20a%20CT-focused%20contrastive%0Alanguage-image%20pretraining%20framework%20designed%20for%20broad%20applications%20without%0Athe%20need%20for%20task-specific%20training.%20We%20demonstrate%20how%20CT-CLIP%20can%20be%20used%20in%0Atwo%20tasks%3A%20multi-abnormality%20detection%20and%20case%20retrieval.%20Remarkably%2C%20in%0Amulti-abnormality%20detection%2C%20CT-CLIP%20outperforms%20state-of-the-art%20fully%0Asupervised%20models%20across%20all%20key%20metrics%2C%20effectively%20eliminating%20the%20need%20for%0Amanual%20annotation.%20In%20case%20retrieval%2C%20it%20efficiently%20retrieves%20relevant%20cases%0Ausing%20either%20image%20or%20textual%20queries%2C%20thereby%20enhancing%20knowledge%0Adissemination.%20By%20combining%20CT-CLIP%27s%20vision%20encoder%20with%20a%20pretrained%20large%0Alanguage%20model%2C%20we%20create%20CT-CHAT%2C%20a%20vision-language%20foundational%20chat%20model%0Afor%203D%20chest%20CT%20volumes.%20Finetuned%20on%20over%202.7%20million%20question-answer%20pairs%0Aderived%20from%20the%20CT-RATE%20dataset%2C%20CT-CHAT%20surpasses%20other%20multimodal%20AI%0Aassistants%2C%20underscoring%20the%20necessity%20for%20specialized%20methods%20in%203D%20medical%0Aimaging.%20Collectively%2C%20the%20open-source%20release%20of%20CT-RATE%2C%20CT-CLIP%2C%20and%20CT-CHAT%0Anot%20only%20addresses%20critical%20challenges%20in%203D%20medical%20imaging%20but%20also%20lays%20the%0Agroundwork%20for%20future%20innovations%20in%20medical%20AI%20and%20improved%20patient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloping%2520Generalist%2520Foundation%2520Models%2520from%2520a%2520Multimodal%2520Dataset%2520for%25203D%250A%2520%2520Computed%2520Tomography%26entry.906535625%3DIbrahim%2520Ethem%2520Hamamci%2520and%2520Sezgin%2520Er%2520and%2520Furkan%2520Almas%2520and%2520Ayse%2520Gulnihan%2520Simsek%2520and%2520Sevval%2520Nil%2520Esirgun%2520and%2520Irem%2520Dogan%2520and%2520Muhammed%2520Furkan%2520Dasdelen%2520and%2520Omer%2520Faruk%2520Durugol%2520and%2520Bastian%2520Wittmann%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Enis%2520Simsar%2520and%2520Mehmet%2520Simsar%2520and%2520Emine%2520Bensu%2520Erdemir%2520and%2520Abdullah%2520Alanbay%2520and%2520Anjany%2520Sekuboyina%2520and%2520Berkan%2520Lafci%2520and%2520Christian%2520Bluethgen%2520and%2520Mehmet%2520Kemal%2520Ozdemir%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520While%2520computer%2520vision%2520has%2520achieved%2520tremendous%2520success%2520with%2520multimodal%250Aencoding%2520and%2520direct%2520textual%2520interaction%2520with%2520images%2520via%2520chat-based%2520large%250Alanguage%2520models%252C%2520similar%2520advancements%2520in%2520medical%2520imaging%2520AI%252C%2520particularly%2520in%25203D%250Aimaging%252C%2520have%2520been%2520limited%2520due%2520to%2520the%2520scarcity%2520of%2520comprehensive%2520datasets.%2520To%250Aaddress%2520this%2520critical%2520gap%252C%2520we%2520introduce%2520CT-RATE%252C%2520the%2520first%2520dataset%2520that%2520pairs%250A3D%2520medical%2520images%2520with%2520corresponding%2520textual%2520reports.%2520CT-RATE%2520comprises%252025%252C692%250Anon-contrast%25203D%2520chest%2520CT%2520scans%2520from%252021%252C304%2520unique%2520patients.%2520Through%2520various%250Areconstructions%252C%2520these%2520scans%2520are%2520expanded%2520to%252050%252C188%2520volumes%252C%2520totaling%2520over%252014.3%250Amillion%25202D%2520slices.%2520Each%2520scan%2520is%2520accompanied%2520by%2520its%2520corresponding%2520radiology%250Areport.%2520Leveraging%2520CT-RATE%252C%2520we%2520develop%2520CT-CLIP%252C%2520a%2520CT-focused%2520contrastive%250Alanguage-image%2520pretraining%2520framework%2520designed%2520for%2520broad%2520applications%2520without%250Athe%2520need%2520for%2520task-specific%2520training.%2520We%2520demonstrate%2520how%2520CT-CLIP%2520can%2520be%2520used%2520in%250Atwo%2520tasks%253A%2520multi-abnormality%2520detection%2520and%2520case%2520retrieval.%2520Remarkably%252C%2520in%250Amulti-abnormality%2520detection%252C%2520CT-CLIP%2520outperforms%2520state-of-the-art%2520fully%250Asupervised%2520models%2520across%2520all%2520key%2520metrics%252C%2520effectively%2520eliminating%2520the%2520need%2520for%250Amanual%2520annotation.%2520In%2520case%2520retrieval%252C%2520it%2520efficiently%2520retrieves%2520relevant%2520cases%250Ausing%2520either%2520image%2520or%2520textual%2520queries%252C%2520thereby%2520enhancing%2520knowledge%250Adissemination.%2520By%2520combining%2520CT-CLIP%2527s%2520vision%2520encoder%2520with%2520a%2520pretrained%2520large%250Alanguage%2520model%252C%2520we%2520create%2520CT-CHAT%252C%2520a%2520vision-language%2520foundational%2520chat%2520model%250Afor%25203D%2520chest%2520CT%2520volumes.%2520Finetuned%2520on%2520over%25202.7%2520million%2520question-answer%2520pairs%250Aderived%2520from%2520the%2520CT-RATE%2520dataset%252C%2520CT-CHAT%2520surpasses%2520other%2520multimodal%2520AI%250Aassistants%252C%2520underscoring%2520the%2520necessity%2520for%2520specialized%2520methods%2520in%25203D%2520medical%250Aimaging.%2520Collectively%252C%2520the%2520open-source%2520release%2520of%2520CT-RATE%252C%2520CT-CLIP%252C%2520and%2520CT-CHAT%250Anot%2520only%2520addresses%2520critical%2520challenges%2520in%25203D%2520medical%2520imaging%2520but%2520also%2520lays%2520the%250Agroundwork%2520for%2520future%2520innovations%2520in%2520medical%2520AI%2520and%2520improved%2520patient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developing%20Generalist%20Foundation%20Models%20from%20a%20Multimodal%20Dataset%20for%203D%0A%20%20Computed%20Tomography&entry.906535625=Ibrahim%20Ethem%20Hamamci%20and%20Sezgin%20Er%20and%20Furkan%20Almas%20and%20Ayse%20Gulnihan%20Simsek%20and%20Sevval%20Nil%20Esirgun%20and%20Irem%20Dogan%20and%20Muhammed%20Furkan%20Dasdelen%20and%20Omer%20Faruk%20Durugol%20and%20Bastian%20Wittmann%20and%20Tamaz%20Amiranashvili%20and%20Enis%20Simsar%20and%20Mehmet%20Simsar%20and%20Emine%20Bensu%20Erdemir%20and%20Abdullah%20Alanbay%20and%20Anjany%20Sekuboyina%20and%20Berkan%20Lafci%20and%20Christian%20Bluethgen%20and%20Mehmet%20Kemal%20Ozdemir%20and%20Bjoern%20Menze&entry.1292438233=%20%20While%20computer%20vision%20has%20achieved%20tremendous%20success%20with%20multimodal%0Aencoding%20and%20direct%20textual%20interaction%20with%20images%20via%20chat-based%20large%0Alanguage%20models%2C%20similar%20advancements%20in%20medical%20imaging%20AI%2C%20particularly%20in%203D%0Aimaging%2C%20have%20been%20limited%20due%20to%20the%20scarcity%20of%20comprehensive%20datasets.%20To%0Aaddress%20this%20critical%20gap%2C%20we%20introduce%20CT-RATE%2C%20the%20first%20dataset%20that%20pairs%0A3D%20medical%20images%20with%20corresponding%20textual%20reports.%20CT-RATE%20comprises%2025%2C692%0Anon-contrast%203D%20chest%20CT%20scans%20from%2021%2C304%20unique%20patients.%20Through%20various%0Areconstructions%2C%20these%20scans%20are%20expanded%20to%2050%2C188%20volumes%2C%20totaling%20over%2014.3%0Amillion%202D%20slices.%20Each%20scan%20is%20accompanied%20by%20its%20corresponding%20radiology%0Areport.%20Leveraging%20CT-RATE%2C%20we%20develop%20CT-CLIP%2C%20a%20CT-focused%20contrastive%0Alanguage-image%20pretraining%20framework%20designed%20for%20broad%20applications%20without%0Athe%20need%20for%20task-specific%20training.%20We%20demonstrate%20how%20CT-CLIP%20can%20be%20used%20in%0Atwo%20tasks%3A%20multi-abnormality%20detection%20and%20case%20retrieval.%20Remarkably%2C%20in%0Amulti-abnormality%20detection%2C%20CT-CLIP%20outperforms%20state-of-the-art%20fully%0Asupervised%20models%20across%20all%20key%20metrics%2C%20effectively%20eliminating%20the%20need%20for%0Amanual%20annotation.%20In%20case%20retrieval%2C%20it%20efficiently%20retrieves%20relevant%20cases%0Ausing%20either%20image%20or%20textual%20queries%2C%20thereby%20enhancing%20knowledge%0Adissemination.%20By%20combining%20CT-CLIP%27s%20vision%20encoder%20with%20a%20pretrained%20large%0Alanguage%20model%2C%20we%20create%20CT-CHAT%2C%20a%20vision-language%20foundational%20chat%20model%0Afor%203D%20chest%20CT%20volumes.%20Finetuned%20on%20over%202.7%20million%20question-answer%20pairs%0Aderived%20from%20the%20CT-RATE%20dataset%2C%20CT-CHAT%20surpasses%20other%20multimodal%20AI%0Aassistants%2C%20underscoring%20the%20necessity%20for%20specialized%20methods%20in%203D%20medical%0Aimaging.%20Collectively%2C%20the%20open-source%20release%20of%20CT-RATE%2C%20CT-CLIP%2C%20and%20CT-CHAT%0Anot%20only%20addresses%20critical%20challenges%20in%203D%20medical%20imaging%20but%20also%20lays%20the%0Agroundwork%20for%20future%20innovations%20in%20medical%20AI%20and%20improved%20patient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17834v2&entry.124074799=Read"},
{"title": "Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural\n  Calibration", "author": "Yifan Zhang and Siyu Ren and Junhui Hou and Jinjian Wu and Yixuan Yuan and Guangming Shi", "abstract": "  This paper introduces a novel self-supervised learning framework for\nenhancing 3D perception in autonomous driving scenes. Specifically, our\napproach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext\ntask that estimates the rigid pose aligning camera and LiDAR coordinate\nsystems. First, we propose the learnable transformation alignment to bridge the\ndomain gap between image and point cloud data, converting features into a\nunified representation space for effective comparison and matching. Second, we\nidentify the overlapping area between the image and point cloud with the fused\nfeatures. Third, we establish dense 2D-3D correspondences to estimate the rigid\npose. The framework not only learns fine-grained matching from points to pixels\nbut also achieves alignment of the image and point cloud at a holistic level,\nunderstanding their relative pose. We demonstrate the efficacy of NCLR by\napplying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D\nsemantic segmentation, object detection, and panoptic segmentation.\nComprehensive experiments on various datasets illustrate the superiority of\nNCLR over existing self-supervised methods. The results confirm that joint\nlearning from different modalities significantly enhances the network's\nunderstanding abilities and effectiveness of learned representation. The code\nis publicly available at https://github.com/Eaphan/NCLR.\n", "link": "http://arxiv.org/abs/2401.12452v3", "date": "2024-10-16", "relevancy": 3.1639, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6573}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20of%20LiDAR%203D%20Point%20Clouds%20via%202D-3D%20Neural%0A%20%20Calibration&body=Title%3A%20Self-supervised%20Learning%20of%20LiDAR%203D%20Point%20Clouds%20via%202D-3D%20Neural%0A%20%20Calibration%0AAuthor%3A%20Yifan%20Zhang%20and%20Siyu%20Ren%20and%20Junhui%20Hou%20and%20Jinjian%20Wu%20and%20Yixuan%20Yuan%20and%20Guangming%20Shi%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20self-supervised%20learning%20framework%20for%0Aenhancing%203D%20perception%20in%20autonomous%20driving%20scenes.%20Specifically%2C%20our%0Aapproach%2C%20namely%20NCLR%2C%20focuses%20on%202D-3D%20neural%20calibration%2C%20a%20novel%20pretext%0Atask%20that%20estimates%20the%20rigid%20pose%20aligning%20camera%20and%20LiDAR%20coordinate%0Asystems.%20First%2C%20we%20propose%20the%20learnable%20transformation%20alignment%20to%20bridge%20the%0Adomain%20gap%20between%20image%20and%20point%20cloud%20data%2C%20converting%20features%20into%20a%0Aunified%20representation%20space%20for%20effective%20comparison%20and%20matching.%20Second%2C%20we%0Aidentify%20the%20overlapping%20area%20between%20the%20image%20and%20point%20cloud%20with%20the%20fused%0Afeatures.%20Third%2C%20we%20establish%20dense%202D-3D%20correspondences%20to%20estimate%20the%20rigid%0Apose.%20The%20framework%20not%20only%20learns%20fine-grained%20matching%20from%20points%20to%20pixels%0Abut%20also%20achieves%20alignment%20of%20the%20image%20and%20point%20cloud%20at%20a%20holistic%20level%2C%0Aunderstanding%20their%20relative%20pose.%20We%20demonstrate%20the%20efficacy%20of%20NCLR%20by%0Aapplying%20the%20pre-trained%20backbone%20to%20downstream%20tasks%2C%20such%20as%20LiDAR-based%203D%0Asemantic%20segmentation%2C%20object%20detection%2C%20and%20panoptic%20segmentation.%0AComprehensive%20experiments%20on%20various%20datasets%20illustrate%20the%20superiority%20of%0ANCLR%20over%20existing%20self-supervised%20methods.%20The%20results%20confirm%20that%20joint%0Alearning%20from%20different%20modalities%20significantly%20enhances%20the%20network%27s%0Aunderstanding%20abilities%20and%20effectiveness%20of%20learned%20representation.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/Eaphan/NCLR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12452v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520of%2520LiDAR%25203D%2520Point%2520Clouds%2520via%25202D-3D%2520Neural%250A%2520%2520Calibration%26entry.906535625%3DYifan%2520Zhang%2520and%2520Siyu%2520Ren%2520and%2520Junhui%2520Hou%2520and%2520Jinjian%2520Wu%2520and%2520Yixuan%2520Yuan%2520and%2520Guangming%2520Shi%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520self-supervised%2520learning%2520framework%2520for%250Aenhancing%25203D%2520perception%2520in%2520autonomous%2520driving%2520scenes.%2520Specifically%252C%2520our%250Aapproach%252C%2520namely%2520NCLR%252C%2520focuses%2520on%25202D-3D%2520neural%2520calibration%252C%2520a%2520novel%2520pretext%250Atask%2520that%2520estimates%2520the%2520rigid%2520pose%2520aligning%2520camera%2520and%2520LiDAR%2520coordinate%250Asystems.%2520First%252C%2520we%2520propose%2520the%2520learnable%2520transformation%2520alignment%2520to%2520bridge%2520the%250Adomain%2520gap%2520between%2520image%2520and%2520point%2520cloud%2520data%252C%2520converting%2520features%2520into%2520a%250Aunified%2520representation%2520space%2520for%2520effective%2520comparison%2520and%2520matching.%2520Second%252C%2520we%250Aidentify%2520the%2520overlapping%2520area%2520between%2520the%2520image%2520and%2520point%2520cloud%2520with%2520the%2520fused%250Afeatures.%2520Third%252C%2520we%2520establish%2520dense%25202D-3D%2520correspondences%2520to%2520estimate%2520the%2520rigid%250Apose.%2520The%2520framework%2520not%2520only%2520learns%2520fine-grained%2520matching%2520from%2520points%2520to%2520pixels%250Abut%2520also%2520achieves%2520alignment%2520of%2520the%2520image%2520and%2520point%2520cloud%2520at%2520a%2520holistic%2520level%252C%250Aunderstanding%2520their%2520relative%2520pose.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520NCLR%2520by%250Aapplying%2520the%2520pre-trained%2520backbone%2520to%2520downstream%2520tasks%252C%2520such%2520as%2520LiDAR-based%25203D%250Asemantic%2520segmentation%252C%2520object%2520detection%252C%2520and%2520panoptic%2520segmentation.%250AComprehensive%2520experiments%2520on%2520various%2520datasets%2520illustrate%2520the%2520superiority%2520of%250ANCLR%2520over%2520existing%2520self-supervised%2520methods.%2520The%2520results%2520confirm%2520that%2520joint%250Alearning%2520from%2520different%2520modalities%2520significantly%2520enhances%2520the%2520network%2527s%250Aunderstanding%2520abilities%2520and%2520effectiveness%2520of%2520learned%2520representation.%2520The%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/Eaphan/NCLR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12452v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20of%20LiDAR%203D%20Point%20Clouds%20via%202D-3D%20Neural%0A%20%20Calibration&entry.906535625=Yifan%20Zhang%20and%20Siyu%20Ren%20and%20Junhui%20Hou%20and%20Jinjian%20Wu%20and%20Yixuan%20Yuan%20and%20Guangming%20Shi&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20self-supervised%20learning%20framework%20for%0Aenhancing%203D%20perception%20in%20autonomous%20driving%20scenes.%20Specifically%2C%20our%0Aapproach%2C%20namely%20NCLR%2C%20focuses%20on%202D-3D%20neural%20calibration%2C%20a%20novel%20pretext%0Atask%20that%20estimates%20the%20rigid%20pose%20aligning%20camera%20and%20LiDAR%20coordinate%0Asystems.%20First%2C%20we%20propose%20the%20learnable%20transformation%20alignment%20to%20bridge%20the%0Adomain%20gap%20between%20image%20and%20point%20cloud%20data%2C%20converting%20features%20into%20a%0Aunified%20representation%20space%20for%20effective%20comparison%20and%20matching.%20Second%2C%20we%0Aidentify%20the%20overlapping%20area%20between%20the%20image%20and%20point%20cloud%20with%20the%20fused%0Afeatures.%20Third%2C%20we%20establish%20dense%202D-3D%20correspondences%20to%20estimate%20the%20rigid%0Apose.%20The%20framework%20not%20only%20learns%20fine-grained%20matching%20from%20points%20to%20pixels%0Abut%20also%20achieves%20alignment%20of%20the%20image%20and%20point%20cloud%20at%20a%20holistic%20level%2C%0Aunderstanding%20their%20relative%20pose.%20We%20demonstrate%20the%20efficacy%20of%20NCLR%20by%0Aapplying%20the%20pre-trained%20backbone%20to%20downstream%20tasks%2C%20such%20as%20LiDAR-based%203D%0Asemantic%20segmentation%2C%20object%20detection%2C%20and%20panoptic%20segmentation.%0AComprehensive%20experiments%20on%20various%20datasets%20illustrate%20the%20superiority%20of%0ANCLR%20over%20existing%20self-supervised%20methods.%20The%20results%20confirm%20that%20joint%0Alearning%20from%20different%20modalities%20significantly%20enhances%20the%20network%27s%0Aunderstanding%20abilities%20and%20effectiveness%20of%20learned%20representation.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/Eaphan/NCLR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12452v3&entry.124074799=Read"},
{"title": "3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image\n  Generation", "author": "Dewei Zhou and Ji Xie and Zongxin Yang and Yi Yang", "abstract": "  The increasing demand for controllable outputs in text-to-image generation\nhas spurred advancements in multi-instance generation (MIG), allowing users to\ndefine both instance layouts and attributes. However, unlike image-conditional\ngeneration methods such as ControlNet, MIG techniques have not been widely\nadopted in state-of-the-art models like SD2 and SDXL, primarily due to the\nchallenge of building robust renderers that simultaneously handle instance\npositioning and attribute rendering. In this paper, we introduce Depth-Driven\nDecoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG\nprocess into two stages: (i) generating a coarse scene depth map for accurate\ninstance positioning and scene composition, and (ii) rendering fine-grained\nattributes using pre-trained ControlNet on any foundational model, without\nadditional training. Our 3DIS framework integrates a custom adapter into LDM3D\nfor precise depth-based layouts and employs a finetuning-free method for\nenhanced instance-level attribute rendering. Extensive experiments on\nCOCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly\noutperforms existing methods in both layout precision and attribute rendering.\nNotably, 3DIS offers seamless compatibility with diverse foundational models,\nproviding a robust, adaptable solution for advanced multi-instance generation.\nThe code is available at: https://github.com/limuloo/3DIS.\n", "link": "http://arxiv.org/abs/2410.12669v1", "date": "2024-10-16", "relevancy": 3.134, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6354}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DIS%3A%20Depth-Driven%20Decoupled%20Instance%20Synthesis%20for%20Text-to-Image%0A%20%20Generation&body=Title%3A%203DIS%3A%20Depth-Driven%20Decoupled%20Instance%20Synthesis%20for%20Text-to-Image%0A%20%20Generation%0AAuthor%3A%20Dewei%20Zhou%20and%20Ji%20Xie%20and%20Zongxin%20Yang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20controllable%20outputs%20in%20text-to-image%20generation%0Ahas%20spurred%20advancements%20in%20multi-instance%20generation%20%28MIG%29%2C%20allowing%20users%20to%0Adefine%20both%20instance%20layouts%20and%20attributes.%20However%2C%20unlike%20image-conditional%0Ageneration%20methods%20such%20as%20ControlNet%2C%20MIG%20techniques%20have%20not%20been%20widely%0Aadopted%20in%20state-of-the-art%20models%20like%20SD2%20and%20SDXL%2C%20primarily%20due%20to%20the%0Achallenge%20of%20building%20robust%20renderers%20that%20simultaneously%20handle%20instance%0Apositioning%20and%20attribute%20rendering.%20In%20this%20paper%2C%20we%20introduce%20Depth-Driven%0ADecoupled%20Instance%20Synthesis%20%283DIS%29%2C%20a%20novel%20framework%20that%20decouples%20the%20MIG%0Aprocess%20into%20two%20stages%3A%20%28i%29%20generating%20a%20coarse%20scene%20depth%20map%20for%20accurate%0Ainstance%20positioning%20and%20scene%20composition%2C%20and%20%28ii%29%20rendering%20fine-grained%0Aattributes%20using%20pre-trained%20ControlNet%20on%20any%20foundational%20model%2C%20without%0Aadditional%20training.%20Our%203DIS%20framework%20integrates%20a%20custom%20adapter%20into%20LDM3D%0Afor%20precise%20depth-based%20layouts%20and%20employs%20a%20finetuning-free%20method%20for%0Aenhanced%20instance-level%20attribute%20rendering.%20Extensive%20experiments%20on%0ACOCO-Position%20and%20COCO-MIG%20benchmarks%20demonstrate%20that%203DIS%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20layout%20precision%20and%20attribute%20rendering.%0ANotably%2C%203DIS%20offers%20seamless%20compatibility%20with%20diverse%20foundational%20models%2C%0Aproviding%20a%20robust%2C%20adaptable%20solution%20for%20advanced%20multi-instance%20generation.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/limuloo/3DIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DIS%253A%2520Depth-Driven%2520Decoupled%2520Instance%2520Synthesis%2520for%2520Text-to-Image%250A%2520%2520Generation%26entry.906535625%3DDewei%2520Zhou%2520and%2520Ji%2520Xie%2520and%2520Zongxin%2520Yang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520controllable%2520outputs%2520in%2520text-to-image%2520generation%250Ahas%2520spurred%2520advancements%2520in%2520multi-instance%2520generation%2520%2528MIG%2529%252C%2520allowing%2520users%2520to%250Adefine%2520both%2520instance%2520layouts%2520and%2520attributes.%2520However%252C%2520unlike%2520image-conditional%250Ageneration%2520methods%2520such%2520as%2520ControlNet%252C%2520MIG%2520techniques%2520have%2520not%2520been%2520widely%250Aadopted%2520in%2520state-of-the-art%2520models%2520like%2520SD2%2520and%2520SDXL%252C%2520primarily%2520due%2520to%2520the%250Achallenge%2520of%2520building%2520robust%2520renderers%2520that%2520simultaneously%2520handle%2520instance%250Apositioning%2520and%2520attribute%2520rendering.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Depth-Driven%250ADecoupled%2520Instance%2520Synthesis%2520%25283DIS%2529%252C%2520a%2520novel%2520framework%2520that%2520decouples%2520the%2520MIG%250Aprocess%2520into%2520two%2520stages%253A%2520%2528i%2529%2520generating%2520a%2520coarse%2520scene%2520depth%2520map%2520for%2520accurate%250Ainstance%2520positioning%2520and%2520scene%2520composition%252C%2520and%2520%2528ii%2529%2520rendering%2520fine-grained%250Aattributes%2520using%2520pre-trained%2520ControlNet%2520on%2520any%2520foundational%2520model%252C%2520without%250Aadditional%2520training.%2520Our%25203DIS%2520framework%2520integrates%2520a%2520custom%2520adapter%2520into%2520LDM3D%250Afor%2520precise%2520depth-based%2520layouts%2520and%2520employs%2520a%2520finetuning-free%2520method%2520for%250Aenhanced%2520instance-level%2520attribute%2520rendering.%2520Extensive%2520experiments%2520on%250ACOCO-Position%2520and%2520COCO-MIG%2520benchmarks%2520demonstrate%2520that%25203DIS%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520both%2520layout%2520precision%2520and%2520attribute%2520rendering.%250ANotably%252C%25203DIS%2520offers%2520seamless%2520compatibility%2520with%2520diverse%2520foundational%2520models%252C%250Aproviding%2520a%2520robust%252C%2520adaptable%2520solution%2520for%2520advanced%2520multi-instance%2520generation.%250AThe%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/limuloo/3DIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DIS%3A%20Depth-Driven%20Decoupled%20Instance%20Synthesis%20for%20Text-to-Image%0A%20%20Generation&entry.906535625=Dewei%20Zhou%20and%20Ji%20Xie%20and%20Zongxin%20Yang%20and%20Yi%20Yang&entry.1292438233=%20%20The%20increasing%20demand%20for%20controllable%20outputs%20in%20text-to-image%20generation%0Ahas%20spurred%20advancements%20in%20multi-instance%20generation%20%28MIG%29%2C%20allowing%20users%20to%0Adefine%20both%20instance%20layouts%20and%20attributes.%20However%2C%20unlike%20image-conditional%0Ageneration%20methods%20such%20as%20ControlNet%2C%20MIG%20techniques%20have%20not%20been%20widely%0Aadopted%20in%20state-of-the-art%20models%20like%20SD2%20and%20SDXL%2C%20primarily%20due%20to%20the%0Achallenge%20of%20building%20robust%20renderers%20that%20simultaneously%20handle%20instance%0Apositioning%20and%20attribute%20rendering.%20In%20this%20paper%2C%20we%20introduce%20Depth-Driven%0ADecoupled%20Instance%20Synthesis%20%283DIS%29%2C%20a%20novel%20framework%20that%20decouples%20the%20MIG%0Aprocess%20into%20two%20stages%3A%20%28i%29%20generating%20a%20coarse%20scene%20depth%20map%20for%20accurate%0Ainstance%20positioning%20and%20scene%20composition%2C%20and%20%28ii%29%20rendering%20fine-grained%0Aattributes%20using%20pre-trained%20ControlNet%20on%20any%20foundational%20model%2C%20without%0Aadditional%20training.%20Our%203DIS%20framework%20integrates%20a%20custom%20adapter%20into%20LDM3D%0Afor%20precise%20depth-based%20layouts%20and%20employs%20a%20finetuning-free%20method%20for%0Aenhanced%20instance-level%20attribute%20rendering.%20Extensive%20experiments%20on%0ACOCO-Position%20and%20COCO-MIG%20benchmarks%20demonstrate%20that%203DIS%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20layout%20precision%20and%20attribute%20rendering.%0ANotably%2C%203DIS%20offers%20seamless%20compatibility%20with%20diverse%20foundational%20models%2C%0Aproviding%20a%20robust%2C%20adaptable%20solution%20for%20advanced%20multi-instance%20generation.%0AThe%20code%20is%20available%20at%3A%20https%3A//github.com/limuloo/3DIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12669v1&entry.124074799=Read"},
{"title": "CMAL: A Novel Cross-Modal Associative Learning Framework for\n  Vision-Language Pre-Training", "author": "Zhiyuan Ma and Jianjun Li and Guohui Li and Kaiyan Huang", "abstract": "  With the flourishing of social media platforms, vision-language pre-training\n(VLP) recently has received great attention and many remarkable progresses have\nbeen achieved. The success of VLP largely benefits from the information\ncomplementation and enhancement between different modalities. However, most of\nrecent studies focus on cross-modal contrastive learning (CMCL) to promote\nimage-text alignment by pulling embeddings of positive sample pairs together\nwhile pushing those of negative pairs apart, which ignores the natural\nasymmetry property between different modalities and requires large-scale\nimage-text corpus to achieve arduous progress. To mitigate this predicament, we\npropose CMAL, a Cross-Modal Associative Learning framework with anchor points\ndetection and cross-modal associative learning for VLP. Specifically, we first\nrespectively embed visual objects and textual tokens into separate hypersphere\nspaces to learn intra-modal hidden features, and then design a cross-modal\nassociative prompt layer to perform anchor point masking and swap feature\nfilling for constructing a hybrid cross-modal associative prompt. Afterwards,\nwe exploit a unified semantic encoder to learn their cross-modal interactive\nfeatures for context adaptation. Finally, we design an associative mapping\nclassification layer to learn potential associative mappings between modalities\nat anchor points, within which we develop a fresh self-supervised associative\nmapping classification task to boost CMAL's performance. Experimental results\nverify the effectiveness of CMAL, showing that it achieves competitive\nperformance against previous CMCL-based methods on four common downstream\nvision-and-language tasks, with significantly fewer corpus. Especially, CMAL\nobtains new state-of-the-art results on SNLI-VE and REC (testA).\n", "link": "http://arxiv.org/abs/2410.12595v1", "date": "2024-10-16", "relevancy": 3.0735, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMAL%3A%20A%20Novel%20Cross-Modal%20Associative%20Learning%20Framework%20for%0A%20%20Vision-Language%20Pre-Training&body=Title%3A%20CMAL%3A%20A%20Novel%20Cross-Modal%20Associative%20Learning%20Framework%20for%0A%20%20Vision-Language%20Pre-Training%0AAuthor%3A%20Zhiyuan%20Ma%20and%20Jianjun%20Li%20and%20Guohui%20Li%20and%20Kaiyan%20Huang%0AAbstract%3A%20%20%20With%20the%20flourishing%20of%20social%20media%20platforms%2C%20vision-language%20pre-training%0A%28VLP%29%20recently%20has%20received%20great%20attention%20and%20many%20remarkable%20progresses%20have%0Abeen%20achieved.%20The%20success%20of%20VLP%20largely%20benefits%20from%20the%20information%0Acomplementation%20and%20enhancement%20between%20different%20modalities.%20However%2C%20most%20of%0Arecent%20studies%20focus%20on%20cross-modal%20contrastive%20learning%20%28CMCL%29%20to%20promote%0Aimage-text%20alignment%20by%20pulling%20embeddings%20of%20positive%20sample%20pairs%20together%0Awhile%20pushing%20those%20of%20negative%20pairs%20apart%2C%20which%20ignores%20the%20natural%0Aasymmetry%20property%20between%20different%20modalities%20and%20requires%20large-scale%0Aimage-text%20corpus%20to%20achieve%20arduous%20progress.%20To%20mitigate%20this%20predicament%2C%20we%0Apropose%20CMAL%2C%20a%20Cross-Modal%20Associative%20Learning%20framework%20with%20anchor%20points%0Adetection%20and%20cross-modal%20associative%20learning%20for%20VLP.%20Specifically%2C%20we%20first%0Arespectively%20embed%20visual%20objects%20and%20textual%20tokens%20into%20separate%20hypersphere%0Aspaces%20to%20learn%20intra-modal%20hidden%20features%2C%20and%20then%20design%20a%20cross-modal%0Aassociative%20prompt%20layer%20to%20perform%20anchor%20point%20masking%20and%20swap%20feature%0Afilling%20for%20constructing%20a%20hybrid%20cross-modal%20associative%20prompt.%20Afterwards%2C%0Awe%20exploit%20a%20unified%20semantic%20encoder%20to%20learn%20their%20cross-modal%20interactive%0Afeatures%20for%20context%20adaptation.%20Finally%2C%20we%20design%20an%20associative%20mapping%0Aclassification%20layer%20to%20learn%20potential%20associative%20mappings%20between%20modalities%0Aat%20anchor%20points%2C%20within%20which%20we%20develop%20a%20fresh%20self-supervised%20associative%0Amapping%20classification%20task%20to%20boost%20CMAL%27s%20performance.%20Experimental%20results%0Averify%20the%20effectiveness%20of%20CMAL%2C%20showing%20that%20it%20achieves%20competitive%0Aperformance%20against%20previous%20CMCL-based%20methods%20on%20four%20common%20downstream%0Avision-and-language%20tasks%2C%20with%20significantly%20fewer%20corpus.%20Especially%2C%20CMAL%0Aobtains%20new%20state-of-the-art%20results%20on%20SNLI-VE%20and%20REC%20%28testA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMAL%253A%2520A%2520Novel%2520Cross-Modal%2520Associative%2520Learning%2520Framework%2520for%250A%2520%2520Vision-Language%2520Pre-Training%26entry.906535625%3DZhiyuan%2520Ma%2520and%2520Jianjun%2520Li%2520and%2520Guohui%2520Li%2520and%2520Kaiyan%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520flourishing%2520of%2520social%2520media%2520platforms%252C%2520vision-language%2520pre-training%250A%2528VLP%2529%2520recently%2520has%2520received%2520great%2520attention%2520and%2520many%2520remarkable%2520progresses%2520have%250Abeen%2520achieved.%2520The%2520success%2520of%2520VLP%2520largely%2520benefits%2520from%2520the%2520information%250Acomplementation%2520and%2520enhancement%2520between%2520different%2520modalities.%2520However%252C%2520most%2520of%250Arecent%2520studies%2520focus%2520on%2520cross-modal%2520contrastive%2520learning%2520%2528CMCL%2529%2520to%2520promote%250Aimage-text%2520alignment%2520by%2520pulling%2520embeddings%2520of%2520positive%2520sample%2520pairs%2520together%250Awhile%2520pushing%2520those%2520of%2520negative%2520pairs%2520apart%252C%2520which%2520ignores%2520the%2520natural%250Aasymmetry%2520property%2520between%2520different%2520modalities%2520and%2520requires%2520large-scale%250Aimage-text%2520corpus%2520to%2520achieve%2520arduous%2520progress.%2520To%2520mitigate%2520this%2520predicament%252C%2520we%250Apropose%2520CMAL%252C%2520a%2520Cross-Modal%2520Associative%2520Learning%2520framework%2520with%2520anchor%2520points%250Adetection%2520and%2520cross-modal%2520associative%2520learning%2520for%2520VLP.%2520Specifically%252C%2520we%2520first%250Arespectively%2520embed%2520visual%2520objects%2520and%2520textual%2520tokens%2520into%2520separate%2520hypersphere%250Aspaces%2520to%2520learn%2520intra-modal%2520hidden%2520features%252C%2520and%2520then%2520design%2520a%2520cross-modal%250Aassociative%2520prompt%2520layer%2520to%2520perform%2520anchor%2520point%2520masking%2520and%2520swap%2520feature%250Afilling%2520for%2520constructing%2520a%2520hybrid%2520cross-modal%2520associative%2520prompt.%2520Afterwards%252C%250Awe%2520exploit%2520a%2520unified%2520semantic%2520encoder%2520to%2520learn%2520their%2520cross-modal%2520interactive%250Afeatures%2520for%2520context%2520adaptation.%2520Finally%252C%2520we%2520design%2520an%2520associative%2520mapping%250Aclassification%2520layer%2520to%2520learn%2520potential%2520associative%2520mappings%2520between%2520modalities%250Aat%2520anchor%2520points%252C%2520within%2520which%2520we%2520develop%2520a%2520fresh%2520self-supervised%2520associative%250Amapping%2520classification%2520task%2520to%2520boost%2520CMAL%2527s%2520performance.%2520Experimental%2520results%250Averify%2520the%2520effectiveness%2520of%2520CMAL%252C%2520showing%2520that%2520it%2520achieves%2520competitive%250Aperformance%2520against%2520previous%2520CMCL-based%2520methods%2520on%2520four%2520common%2520downstream%250Avision-and-language%2520tasks%252C%2520with%2520significantly%2520fewer%2520corpus.%2520Especially%252C%2520CMAL%250Aobtains%2520new%2520state-of-the-art%2520results%2520on%2520SNLI-VE%2520and%2520REC%2520%2528testA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMAL%3A%20A%20Novel%20Cross-Modal%20Associative%20Learning%20Framework%20for%0A%20%20Vision-Language%20Pre-Training&entry.906535625=Zhiyuan%20Ma%20and%20Jianjun%20Li%20and%20Guohui%20Li%20and%20Kaiyan%20Huang&entry.1292438233=%20%20With%20the%20flourishing%20of%20social%20media%20platforms%2C%20vision-language%20pre-training%0A%28VLP%29%20recently%20has%20received%20great%20attention%20and%20many%20remarkable%20progresses%20have%0Abeen%20achieved.%20The%20success%20of%20VLP%20largely%20benefits%20from%20the%20information%0Acomplementation%20and%20enhancement%20between%20different%20modalities.%20However%2C%20most%20of%0Arecent%20studies%20focus%20on%20cross-modal%20contrastive%20learning%20%28CMCL%29%20to%20promote%0Aimage-text%20alignment%20by%20pulling%20embeddings%20of%20positive%20sample%20pairs%20together%0Awhile%20pushing%20those%20of%20negative%20pairs%20apart%2C%20which%20ignores%20the%20natural%0Aasymmetry%20property%20between%20different%20modalities%20and%20requires%20large-scale%0Aimage-text%20corpus%20to%20achieve%20arduous%20progress.%20To%20mitigate%20this%20predicament%2C%20we%0Apropose%20CMAL%2C%20a%20Cross-Modal%20Associative%20Learning%20framework%20with%20anchor%20points%0Adetection%20and%20cross-modal%20associative%20learning%20for%20VLP.%20Specifically%2C%20we%20first%0Arespectively%20embed%20visual%20objects%20and%20textual%20tokens%20into%20separate%20hypersphere%0Aspaces%20to%20learn%20intra-modal%20hidden%20features%2C%20and%20then%20design%20a%20cross-modal%0Aassociative%20prompt%20layer%20to%20perform%20anchor%20point%20masking%20and%20swap%20feature%0Afilling%20for%20constructing%20a%20hybrid%20cross-modal%20associative%20prompt.%20Afterwards%2C%0Awe%20exploit%20a%20unified%20semantic%20encoder%20to%20learn%20their%20cross-modal%20interactive%0Afeatures%20for%20context%20adaptation.%20Finally%2C%20we%20design%20an%20associative%20mapping%0Aclassification%20layer%20to%20learn%20potential%20associative%20mappings%20between%20modalities%0Aat%20anchor%20points%2C%20within%20which%20we%20develop%20a%20fresh%20self-supervised%20associative%0Amapping%20classification%20task%20to%20boost%20CMAL%27s%20performance.%20Experimental%20results%0Averify%20the%20effectiveness%20of%20CMAL%2C%20showing%20that%20it%20achieves%20competitive%0Aperformance%20against%20previous%20CMCL-based%20methods%20on%20four%20common%20downstream%0Avision-and-language%20tasks%2C%20with%20significantly%20fewer%20corpus.%20Especially%2C%20CMAL%0Aobtains%20new%20state-of-the-art%20results%20on%20SNLI-VE%20and%20REC%20%28testA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12595v1&entry.124074799=Read"},
{"title": "Gaussian Primitives for Deformable Image Registration", "author": "Jihe Li and Xiang Liu and Fabian Zhang and Xia Li and Xixin Cao and Ye Zhang and Joachim Buhmann", "abstract": "  Deformable Image Registration (DIR) is essential for aligning medical images\nthat exhibit anatomical variations, facilitating applications such as disease\ntracking and radiotherapy planning. While classical iterative methods and deep\nlearning approaches have achieved success in DIR, they are often hindered by\ncomputational inefficiency or poor generalization. In this paper, we introduce\nGaussianDIR, a novel, case-specific optimization DIR method inspired by 3D\nGaussian splatting. In general, GaussianDIR represents image deformations using\na sparse set of mobile and flexible Gaussian primitives, each defined by a\ncenter position, covariance, and local rigid transformation. This compact and\nexplicit representation reduces noise and computational overhead while\nimproving interpretability. Furthermore, the movement of individual voxel is\nderived via blending the local rigid transformation of the neighboring Gaussian\nprimitives. By this, GaussianDIR captures both global smoothness and local\nrigidity as well as reduces the computational burden. To address varying levels\nof deformation complexity, GaussianDIR also integrates an adaptive density\ncontrol mechanism that dynamically adjusts the density of Gaussian primitives.\nAdditionally, we employ multi-scale Gaussian primitives to capture both coarse\nand fine deformations, reducing optimization to local minima. Experimental\nresults on brain MRI, lung CT, and cardiac MRI datasets demonstrate that\nGaussianDIR outperforms existing DIR methods in both accuracy and efficiency,\nhighlighting its potential for clinical applications. Finally, as a\ntraining-free approach, it challenges the stereotype that iterative methods are\ninherently slow and transcend the limitations of poor generalization.\n", "link": "http://arxiv.org/abs/2406.03394v2", "date": "2024-10-16", "relevancy": 3.0144, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6215}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6091}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Primitives%20for%20Deformable%20Image%20Registration&body=Title%3A%20Gaussian%20Primitives%20for%20Deformable%20Image%20Registration%0AAuthor%3A%20Jihe%20Li%20and%20Xiang%20Liu%20and%20Fabian%20Zhang%20and%20Xia%20Li%20and%20Xixin%20Cao%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann%0AAbstract%3A%20%20%20Deformable%20Image%20Registration%20%28DIR%29%20is%20essential%20for%20aligning%20medical%20images%0Athat%20exhibit%20anatomical%20variations%2C%20facilitating%20applications%20such%20as%20disease%0Atracking%20and%20radiotherapy%20planning.%20While%20classical%20iterative%20methods%20and%20deep%0Alearning%20approaches%20have%20achieved%20success%20in%20DIR%2C%20they%20are%20often%20hindered%20by%0Acomputational%20inefficiency%20or%20poor%20generalization.%20In%20this%20paper%2C%20we%20introduce%0AGaussianDIR%2C%20a%20novel%2C%20case-specific%20optimization%20DIR%20method%20inspired%20by%203D%0AGaussian%20splatting.%20In%20general%2C%20GaussianDIR%20represents%20image%20deformations%20using%0Aa%20sparse%20set%20of%20mobile%20and%20flexible%20Gaussian%20primitives%2C%20each%20defined%20by%20a%0Acenter%20position%2C%20covariance%2C%20and%20local%20rigid%20transformation.%20This%20compact%20and%0Aexplicit%20representation%20reduces%20noise%20and%20computational%20overhead%20while%0Aimproving%20interpretability.%20Furthermore%2C%20the%20movement%20of%20individual%20voxel%20is%0Aderived%20via%20blending%20the%20local%20rigid%20transformation%20of%20the%20neighboring%20Gaussian%0Aprimitives.%20By%20this%2C%20GaussianDIR%20captures%20both%20global%20smoothness%20and%20local%0Arigidity%20as%20well%20as%20reduces%20the%20computational%20burden.%20To%20address%20varying%20levels%0Aof%20deformation%20complexity%2C%20GaussianDIR%20also%20integrates%20an%20adaptive%20density%0Acontrol%20mechanism%20that%20dynamically%20adjusts%20the%20density%20of%20Gaussian%20primitives.%0AAdditionally%2C%20we%20employ%20multi-scale%20Gaussian%20primitives%20to%20capture%20both%20coarse%0Aand%20fine%20deformations%2C%20reducing%20optimization%20to%20local%20minima.%20Experimental%0Aresults%20on%20brain%20MRI%2C%20lung%20CT%2C%20and%20cardiac%20MRI%20datasets%20demonstrate%20that%0AGaussianDIR%20outperforms%20existing%20DIR%20methods%20in%20both%20accuracy%20and%20efficiency%2C%0Ahighlighting%20its%20potential%20for%20clinical%20applications.%20Finally%2C%20as%20a%0Atraining-free%20approach%2C%20it%20challenges%20the%20stereotype%20that%20iterative%20methods%20are%0Ainherently%20slow%20and%20transcend%20the%20limitations%20of%20poor%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Primitives%2520for%2520Deformable%2520Image%2520Registration%26entry.906535625%3DJihe%2520Li%2520and%2520Xiang%2520Liu%2520and%2520Fabian%2520Zhang%2520and%2520Xia%2520Li%2520and%2520Xixin%2520Cao%2520and%2520Ye%2520Zhang%2520and%2520Joachim%2520Buhmann%26entry.1292438233%3D%2520%2520Deformable%2520Image%2520Registration%2520%2528DIR%2529%2520is%2520essential%2520for%2520aligning%2520medical%2520images%250Athat%2520exhibit%2520anatomical%2520variations%252C%2520facilitating%2520applications%2520such%2520as%2520disease%250Atracking%2520and%2520radiotherapy%2520planning.%2520While%2520classical%2520iterative%2520methods%2520and%2520deep%250Alearning%2520approaches%2520have%2520achieved%2520success%2520in%2520DIR%252C%2520they%2520are%2520often%2520hindered%2520by%250Acomputational%2520inefficiency%2520or%2520poor%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AGaussianDIR%252C%2520a%2520novel%252C%2520case-specific%2520optimization%2520DIR%2520method%2520inspired%2520by%25203D%250AGaussian%2520splatting.%2520In%2520general%252C%2520GaussianDIR%2520represents%2520image%2520deformations%2520using%250Aa%2520sparse%2520set%2520of%2520mobile%2520and%2520flexible%2520Gaussian%2520primitives%252C%2520each%2520defined%2520by%2520a%250Acenter%2520position%252C%2520covariance%252C%2520and%2520local%2520rigid%2520transformation.%2520This%2520compact%2520and%250Aexplicit%2520representation%2520reduces%2520noise%2520and%2520computational%2520overhead%2520while%250Aimproving%2520interpretability.%2520Furthermore%252C%2520the%2520movement%2520of%2520individual%2520voxel%2520is%250Aderived%2520via%2520blending%2520the%2520local%2520rigid%2520transformation%2520of%2520the%2520neighboring%2520Gaussian%250Aprimitives.%2520By%2520this%252C%2520GaussianDIR%2520captures%2520both%2520global%2520smoothness%2520and%2520local%250Arigidity%2520as%2520well%2520as%2520reduces%2520the%2520computational%2520burden.%2520To%2520address%2520varying%2520levels%250Aof%2520deformation%2520complexity%252C%2520GaussianDIR%2520also%2520integrates%2520an%2520adaptive%2520density%250Acontrol%2520mechanism%2520that%2520dynamically%2520adjusts%2520the%2520density%2520of%2520Gaussian%2520primitives.%250AAdditionally%252C%2520we%2520employ%2520multi-scale%2520Gaussian%2520primitives%2520to%2520capture%2520both%2520coarse%250Aand%2520fine%2520deformations%252C%2520reducing%2520optimization%2520to%2520local%2520minima.%2520Experimental%250Aresults%2520on%2520brain%2520MRI%252C%2520lung%2520CT%252C%2520and%2520cardiac%2520MRI%2520datasets%2520demonstrate%2520that%250AGaussianDIR%2520outperforms%2520existing%2520DIR%2520methods%2520in%2520both%2520accuracy%2520and%2520efficiency%252C%250Ahighlighting%2520its%2520potential%2520for%2520clinical%2520applications.%2520Finally%252C%2520as%2520a%250Atraining-free%2520approach%252C%2520it%2520challenges%2520the%2520stereotype%2520that%2520iterative%2520methods%2520are%250Ainherently%2520slow%2520and%2520transcend%2520the%2520limitations%2520of%2520poor%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Primitives%20for%20Deformable%20Image%20Registration&entry.906535625=Jihe%20Li%20and%20Xiang%20Liu%20and%20Fabian%20Zhang%20and%20Xia%20Li%20and%20Xixin%20Cao%20and%20Ye%20Zhang%20and%20Joachim%20Buhmann&entry.1292438233=%20%20Deformable%20Image%20Registration%20%28DIR%29%20is%20essential%20for%20aligning%20medical%20images%0Athat%20exhibit%20anatomical%20variations%2C%20facilitating%20applications%20such%20as%20disease%0Atracking%20and%20radiotherapy%20planning.%20While%20classical%20iterative%20methods%20and%20deep%0Alearning%20approaches%20have%20achieved%20success%20in%20DIR%2C%20they%20are%20often%20hindered%20by%0Acomputational%20inefficiency%20or%20poor%20generalization.%20In%20this%20paper%2C%20we%20introduce%0AGaussianDIR%2C%20a%20novel%2C%20case-specific%20optimization%20DIR%20method%20inspired%20by%203D%0AGaussian%20splatting.%20In%20general%2C%20GaussianDIR%20represents%20image%20deformations%20using%0Aa%20sparse%20set%20of%20mobile%20and%20flexible%20Gaussian%20primitives%2C%20each%20defined%20by%20a%0Acenter%20position%2C%20covariance%2C%20and%20local%20rigid%20transformation.%20This%20compact%20and%0Aexplicit%20representation%20reduces%20noise%20and%20computational%20overhead%20while%0Aimproving%20interpretability.%20Furthermore%2C%20the%20movement%20of%20individual%20voxel%20is%0Aderived%20via%20blending%20the%20local%20rigid%20transformation%20of%20the%20neighboring%20Gaussian%0Aprimitives.%20By%20this%2C%20GaussianDIR%20captures%20both%20global%20smoothness%20and%20local%0Arigidity%20as%20well%20as%20reduces%20the%20computational%20burden.%20To%20address%20varying%20levels%0Aof%20deformation%20complexity%2C%20GaussianDIR%20also%20integrates%20an%20adaptive%20density%0Acontrol%20mechanism%20that%20dynamically%20adjusts%20the%20density%20of%20Gaussian%20primitives.%0AAdditionally%2C%20we%20employ%20multi-scale%20Gaussian%20primitives%20to%20capture%20both%20coarse%0Aand%20fine%20deformations%2C%20reducing%20optimization%20to%20local%20minima.%20Experimental%0Aresults%20on%20brain%20MRI%2C%20lung%20CT%2C%20and%20cardiac%20MRI%20datasets%20demonstrate%20that%0AGaussianDIR%20outperforms%20existing%20DIR%20methods%20in%20both%20accuracy%20and%20efficiency%2C%0Ahighlighting%20its%20potential%20for%20clinical%20applications.%20Finally%2C%20as%20a%0Atraining-free%20approach%2C%20it%20challenges%20the%20stereotype%20that%20iterative%20methods%20are%0Ainherently%20slow%20and%20transcend%20the%20limitations%20of%20poor%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03394v2&entry.124074799=Read"},
{"title": "Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations", "author": "Shen Fan and Przemyslaw Musialski", "abstract": "  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n", "link": "http://arxiv.org/abs/2410.12725v1", "date": "2024-10-16", "relevancy": 3.0122, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.634}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6064}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%203D%20Geometry%20Reconstruction%20from%20Implicit%20Neural%0A%20%20Representations&body=Title%3A%20Optimizing%203D%20Geometry%20Reconstruction%20from%20Implicit%20Neural%0A%20%20Representations%0AAuthor%3A%20Shen%20Fan%20and%20Przemyslaw%20Musialski%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20have%20emerged%20as%20a%20powerful%20tool%20in%20learning%0A3D%20geometry%2C%20offering%20unparalleled%20advantages%20over%20conventional%20representations%0Alike%20mesh-based%20methods.%20A%20common%20type%20of%20INR%20implicitly%20encodes%20a%20shape%27s%0Aboundary%20as%20the%20zero-level%20set%20of%20the%20learned%20continuous%20function%20and%20learns%20a%0Amapping%20from%20a%20low-dimensional%20latent%20space%20to%20the%20space%20of%20all%20possible%20shapes%0Arepresented%20by%20its%20signed%20distance%20function.%20However%2C%20most%20INRs%20struggle%20to%0Aretain%20high-frequency%20details%2C%20which%20are%20crucial%20for%20accurate%20geometric%0Adepiction%2C%20and%20they%20are%20computationally%20expensive.%20To%20address%20these%0Alimitations%2C%20we%20present%20a%20novel%20approach%20that%20both%20reduces%20computational%0Aexpenses%20and%20enhances%20the%20capture%20of%20fine%20details.%20Our%20method%20integrates%0Aperiodic%20activation%20functions%2C%20positional%20encodings%2C%20and%20normals%20into%20the%0Aneural%20network%20architecture.%20This%20integration%20significantly%20enhances%20the%0Amodel%27s%20ability%20to%20learn%20the%20entire%20space%20of%203D%20shapes%20while%20preserving%0Aintricate%20details%20and%20sharp%20features%2C%20areas%20where%20conventional%20representations%0Aoften%20fall%20short.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%25203D%2520Geometry%2520Reconstruction%2520from%2520Implicit%2520Neural%250A%2520%2520Representations%26entry.906535625%3DShen%2520Fan%2520and%2520Przemyslaw%2520Musialski%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520in%2520learning%250A3D%2520geometry%252C%2520offering%2520unparalleled%2520advantages%2520over%2520conventional%2520representations%250Alike%2520mesh-based%2520methods.%2520A%2520common%2520type%2520of%2520INR%2520implicitly%2520encodes%2520a%2520shape%2527s%250Aboundary%2520as%2520the%2520zero-level%2520set%2520of%2520the%2520learned%2520continuous%2520function%2520and%2520learns%2520a%250Amapping%2520from%2520a%2520low-dimensional%2520latent%2520space%2520to%2520the%2520space%2520of%2520all%2520possible%2520shapes%250Arepresented%2520by%2520its%2520signed%2520distance%2520function.%2520However%252C%2520most%2520INRs%2520struggle%2520to%250Aretain%2520high-frequency%2520details%252C%2520which%2520are%2520crucial%2520for%2520accurate%2520geometric%250Adepiction%252C%2520and%2520they%2520are%2520computationally%2520expensive.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%2520both%2520reduces%2520computational%250Aexpenses%2520and%2520enhances%2520the%2520capture%2520of%2520fine%2520details.%2520Our%2520method%2520integrates%250Aperiodic%2520activation%2520functions%252C%2520positional%2520encodings%252C%2520and%2520normals%2520into%2520the%250Aneural%2520network%2520architecture.%2520This%2520integration%2520significantly%2520enhances%2520the%250Amodel%2527s%2520ability%2520to%2520learn%2520the%2520entire%2520space%2520of%25203D%2520shapes%2520while%2520preserving%250Aintricate%2520details%2520and%2520sharp%2520features%252C%2520areas%2520where%2520conventional%2520representations%250Aoften%2520fall%2520short.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%203D%20Geometry%20Reconstruction%20from%20Implicit%20Neural%0A%20%20Representations&entry.906535625=Shen%20Fan%20and%20Przemyslaw%20Musialski&entry.1292438233=%20%20Implicit%20neural%20representations%20have%20emerged%20as%20a%20powerful%20tool%20in%20learning%0A3D%20geometry%2C%20offering%20unparalleled%20advantages%20over%20conventional%20representations%0Alike%20mesh-based%20methods.%20A%20common%20type%20of%20INR%20implicitly%20encodes%20a%20shape%27s%0Aboundary%20as%20the%20zero-level%20set%20of%20the%20learned%20continuous%20function%20and%20learns%20a%0Amapping%20from%20a%20low-dimensional%20latent%20space%20to%20the%20space%20of%20all%20possible%20shapes%0Arepresented%20by%20its%20signed%20distance%20function.%20However%2C%20most%20INRs%20struggle%20to%0Aretain%20high-frequency%20details%2C%20which%20are%20crucial%20for%20accurate%20geometric%0Adepiction%2C%20and%20they%20are%20computationally%20expensive.%20To%20address%20these%0Alimitations%2C%20we%20present%20a%20novel%20approach%20that%20both%20reduces%20computational%0Aexpenses%20and%20enhances%20the%20capture%20of%20fine%20details.%20Our%20method%20integrates%0Aperiodic%20activation%20functions%2C%20positional%20encodings%2C%20and%20normals%20into%20the%0Aneural%20network%20architecture.%20This%20integration%20significantly%20enhances%20the%0Amodel%27s%20ability%20to%20learn%20the%20entire%20space%20of%203D%20shapes%20while%20preserving%0Aintricate%20details%20and%20sharp%20features%2C%20areas%20where%20conventional%20representations%0Aoften%20fall%20short.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12725v1&entry.124074799=Read"},
{"title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language\n  Models", "author": "Ce Zhang and Simon Stepputtis and Katia Sycara and Yaqi Xie", "abstract": "  Test-time adaptation, which enables models to generalize to diverse data with\nunlabeled test samples, holds significant value in real-world scenarios.\nRecently, researchers have applied this setting to advanced pre-trained\nvision-language models (VLMs), developing approaches such as test-time prompt\ntuning to further extend their practical applicability. However, these methods\ntypically focus solely on adapting VLMs from a single modality and fail to\naccumulate task-specific knowledge as more samples are processed. To address\nthis, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation\napproach for VLMs that effectively accumulates task-specific knowledge from\nmulti-modalities. Specifically, we create and evolve two sets of\nprototypes--textual and visual--to progressively capture more accurate\nmulti-modal representations for target classes during test time. Moreover, to\npromote consistent multi-modal representations, we introduce and optimize\nlearnable residuals for each test sample to align the prototypes from both\nmodalities. Extensive experimental results on 15 benchmark datasets demonstrate\nthat our proposed DPE consistently outperforms previous state-of-the-art\nmethods while also exhibiting competitive computational efficiency. Code is\navailable at https://github.com/zhangce01/DPE-CLIP.\n", "link": "http://arxiv.org/abs/2410.12790v1", "date": "2024-10-16", "relevancy": 3.0034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Prototype%20Evolving%20for%20Test-Time%20Generalization%20of%20Vision-Language%0A%20%20Models&body=Title%3A%20Dual%20Prototype%20Evolving%20for%20Test-Time%20Generalization%20of%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie%0AAbstract%3A%20%20%20Test-time%20adaptation%2C%20which%20enables%20models%20to%20generalize%20to%20diverse%20data%20with%0Aunlabeled%20test%20samples%2C%20holds%20significant%20value%20in%20real-world%20scenarios.%0ARecently%2C%20researchers%20have%20applied%20this%20setting%20to%20advanced%20pre-trained%0Avision-language%20models%20%28VLMs%29%2C%20developing%20approaches%20such%20as%20test-time%20prompt%0Atuning%20to%20further%20extend%20their%20practical%20applicability.%20However%2C%20these%20methods%0Atypically%20focus%20solely%20on%20adapting%20VLMs%20from%20a%20single%20modality%20and%20fail%20to%0Aaccumulate%20task-specific%20knowledge%20as%20more%20samples%20are%20processed.%20To%20address%0Athis%2C%20we%20introduce%20Dual%20Prototype%20Evolving%20%28DPE%29%2C%20a%20novel%20test-time%20adaptation%0Aapproach%20for%20VLMs%20that%20effectively%20accumulates%20task-specific%20knowledge%20from%0Amulti-modalities.%20Specifically%2C%20we%20create%20and%20evolve%20two%20sets%20of%0Aprototypes--textual%20and%20visual--to%20progressively%20capture%20more%20accurate%0Amulti-modal%20representations%20for%20target%20classes%20during%20test%20time.%20Moreover%2C%20to%0Apromote%20consistent%20multi-modal%20representations%2C%20we%20introduce%20and%20optimize%0Alearnable%20residuals%20for%20each%20test%20sample%20to%20align%20the%20prototypes%20from%20both%0Amodalities.%20Extensive%20experimental%20results%20on%2015%20benchmark%20datasets%20demonstrate%0Athat%20our%20proposed%20DPE%20consistently%20outperforms%20previous%20state-of-the-art%0Amethods%20while%20also%20exhibiting%20competitive%20computational%20efficiency.%20Code%20is%0Aavailable%20at%20https%3A//github.com/zhangce01/DPE-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Prototype%2520Evolving%2520for%2520Test-Time%2520Generalization%2520of%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DCe%2520Zhang%2520and%2520Simon%2520Stepputtis%2520and%2520Katia%2520Sycara%2520and%2520Yaqi%2520Xie%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%252C%2520which%2520enables%2520models%2520to%2520generalize%2520to%2520diverse%2520data%2520with%250Aunlabeled%2520test%2520samples%252C%2520holds%2520significant%2520value%2520in%2520real-world%2520scenarios.%250ARecently%252C%2520researchers%2520have%2520applied%2520this%2520setting%2520to%2520advanced%2520pre-trained%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520developing%2520approaches%2520such%2520as%2520test-time%2520prompt%250Atuning%2520to%2520further%2520extend%2520their%2520practical%2520applicability.%2520However%252C%2520these%2520methods%250Atypically%2520focus%2520solely%2520on%2520adapting%2520VLMs%2520from%2520a%2520single%2520modality%2520and%2520fail%2520to%250Aaccumulate%2520task-specific%2520knowledge%2520as%2520more%2520samples%2520are%2520processed.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520Dual%2520Prototype%2520Evolving%2520%2528DPE%2529%252C%2520a%2520novel%2520test-time%2520adaptation%250Aapproach%2520for%2520VLMs%2520that%2520effectively%2520accumulates%2520task-specific%2520knowledge%2520from%250Amulti-modalities.%2520Specifically%252C%2520we%2520create%2520and%2520evolve%2520two%2520sets%2520of%250Aprototypes--textual%2520and%2520visual--to%2520progressively%2520capture%2520more%2520accurate%250Amulti-modal%2520representations%2520for%2520target%2520classes%2520during%2520test%2520time.%2520Moreover%252C%2520to%250Apromote%2520consistent%2520multi-modal%2520representations%252C%2520we%2520introduce%2520and%2520optimize%250Alearnable%2520residuals%2520for%2520each%2520test%2520sample%2520to%2520align%2520the%2520prototypes%2520from%2520both%250Amodalities.%2520Extensive%2520experimental%2520results%2520on%252015%2520benchmark%2520datasets%2520demonstrate%250Athat%2520our%2520proposed%2520DPE%2520consistently%2520outperforms%2520previous%2520state-of-the-art%250Amethods%2520while%2520also%2520exhibiting%2520competitive%2520computational%2520efficiency.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/zhangce01/DPE-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Prototype%20Evolving%20for%20Test-Time%20Generalization%20of%20Vision-Language%0A%20%20Models&entry.906535625=Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie&entry.1292438233=%20%20Test-time%20adaptation%2C%20which%20enables%20models%20to%20generalize%20to%20diverse%20data%20with%0Aunlabeled%20test%20samples%2C%20holds%20significant%20value%20in%20real-world%20scenarios.%0ARecently%2C%20researchers%20have%20applied%20this%20setting%20to%20advanced%20pre-trained%0Avision-language%20models%20%28VLMs%29%2C%20developing%20approaches%20such%20as%20test-time%20prompt%0Atuning%20to%20further%20extend%20their%20practical%20applicability.%20However%2C%20these%20methods%0Atypically%20focus%20solely%20on%20adapting%20VLMs%20from%20a%20single%20modality%20and%20fail%20to%0Aaccumulate%20task-specific%20knowledge%20as%20more%20samples%20are%20processed.%20To%20address%0Athis%2C%20we%20introduce%20Dual%20Prototype%20Evolving%20%28DPE%29%2C%20a%20novel%20test-time%20adaptation%0Aapproach%20for%20VLMs%20that%20effectively%20accumulates%20task-specific%20knowledge%20from%0Amulti-modalities.%20Specifically%2C%20we%20create%20and%20evolve%20two%20sets%20of%0Aprototypes--textual%20and%20visual--to%20progressively%20capture%20more%20accurate%0Amulti-modal%20representations%20for%20target%20classes%20during%20test%20time.%20Moreover%2C%20to%0Apromote%20consistent%20multi-modal%20representations%2C%20we%20introduce%20and%20optimize%0Alearnable%20residuals%20for%20each%20test%20sample%20to%20align%20the%20prototypes%20from%20both%0Amodalities.%20Extensive%20experimental%20results%20on%2015%20benchmark%20datasets%20demonstrate%0Athat%20our%20proposed%20DPE%20consistently%20outperforms%20previous%20state-of-the-art%0Amethods%20while%20also%20exhibiting%20competitive%20computational%20efficiency.%20Code%20is%0Aavailable%20at%20https%3A//github.com/zhangce01/DPE-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12790v1&entry.124074799=Read"},
{"title": "A3D: Does Diffusion Dream about 3D Alignment?", "author": "Savva Ignatyev and Nina Konovalova and Daniil Selikhanovych and Oleg Voynov and Nikolay Patakin and Ilya Olkov and Dmitry Senushkin and Alexey Artemov and Anton Konushin and Alexander Filippov and Peter Wonka and Evgeny Burnaev", "abstract": "  We tackle the problem of text-driven 3D generation from a geometry alignment\nperspective. Given a set of text prompts, we aim to generate a collection of\nobjects with semantically corresponding parts aligned across them. Recent\nmethods based on Score Distillation have succeeded in distilling the knowledge\nfrom 2D diffusion models to high-quality representations of the 3D objects.\nThese methods handle multiple text queries separately, and therefore the\nresulting objects have a high variability in object pose and structure.\nHowever, in some applications, such as 3D asset design, it may be desirable to\nobtain a set of objects aligned with each other. In order to achieve the\nalignment of the corresponding parts of the generated objects, we propose to\nembed these objects into a common latent space and optimize the continuous\ntransitions between these objects. We enforce two kinds of properties of these\ntransitions: smoothness of the transition and plausibility of the intermediate\nobjects along the transition. We demonstrate that both of these properties are\nessential for good alignment. We provide several practical scenarios that\nbenefit from alignment between the objects, including 3D editing and object\nhybridization, and experimentally demonstrate the effectiveness of our method.\nhttps://voyleg.github.io/a3d/\n", "link": "http://arxiv.org/abs/2406.15020v3", "date": "2024-10-16", "relevancy": 2.9881, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F&body=Title%3A%20A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F%0AAuthor%3A%20Savva%20Ignatyev%20and%20Nina%20Konovalova%20and%20Daniil%20Selikhanovych%20and%20Oleg%20Voynov%20and%20Nikolay%20Patakin%20and%20Ilya%20Olkov%20and%20Dmitry%20Senushkin%20and%20Alexey%20Artemov%20and%20Anton%20Konushin%20and%20Alexander%20Filippov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20text-driven%203D%20generation%20from%20a%20geometry%20alignment%0Aperspective.%20Given%20a%20set%20of%20text%20prompts%2C%20we%20aim%20to%20generate%20a%20collection%20of%0Aobjects%20with%20semantically%20corresponding%20parts%20aligned%20across%20them.%20Recent%0Amethods%20based%20on%20Score%20Distillation%20have%20succeeded%20in%20distilling%20the%20knowledge%0Afrom%202D%20diffusion%20models%20to%20high-quality%20representations%20of%20the%203D%20objects.%0AThese%20methods%20handle%20multiple%20text%20queries%20separately%2C%20and%20therefore%20the%0Aresulting%20objects%20have%20a%20high%20variability%20in%20object%20pose%20and%20structure.%0AHowever%2C%20in%20some%20applications%2C%20such%20as%203D%20asset%20design%2C%20it%20may%20be%20desirable%20to%0Aobtain%20a%20set%20of%20objects%20aligned%20with%20each%20other.%20In%20order%20to%20achieve%20the%0Aalignment%20of%20the%20corresponding%20parts%20of%20the%20generated%20objects%2C%20we%20propose%20to%0Aembed%20these%20objects%20into%20a%20common%20latent%20space%20and%20optimize%20the%20continuous%0Atransitions%20between%20these%20objects.%20We%20enforce%20two%20kinds%20of%20properties%20of%20these%0Atransitions%3A%20smoothness%20of%20the%20transition%20and%20plausibility%20of%20the%20intermediate%0Aobjects%20along%20the%20transition.%20We%20demonstrate%20that%20both%20of%20these%20properties%20are%0Aessential%20for%20good%20alignment.%20We%20provide%20several%20practical%20scenarios%20that%0Abenefit%20from%20alignment%20between%20the%20objects%2C%20including%203D%20editing%20and%20object%0Ahybridization%2C%20and%20experimentally%20demonstrate%20the%20effectiveness%20of%20our%20method.%0Ahttps%3A//voyleg.github.io/a3d/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15020v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA3D%253A%2520Does%2520Diffusion%2520Dream%2520about%25203D%2520Alignment%253F%26entry.906535625%3DSavva%2520Ignatyev%2520and%2520Nina%2520Konovalova%2520and%2520Daniil%2520Selikhanovych%2520and%2520Oleg%2520Voynov%2520and%2520Nikolay%2520Patakin%2520and%2520Ilya%2520Olkov%2520and%2520Dmitry%2520Senushkin%2520and%2520Alexey%2520Artemov%2520and%2520Anton%2520Konushin%2520and%2520Alexander%2520Filippov%2520and%2520Peter%2520Wonka%2520and%2520Evgeny%2520Burnaev%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520text-driven%25203D%2520generation%2520from%2520a%2520geometry%2520alignment%250Aperspective.%2520Given%2520a%2520set%2520of%2520text%2520prompts%252C%2520we%2520aim%2520to%2520generate%2520a%2520collection%2520of%250Aobjects%2520with%2520semantically%2520corresponding%2520parts%2520aligned%2520across%2520them.%2520Recent%250Amethods%2520based%2520on%2520Score%2520Distillation%2520have%2520succeeded%2520in%2520distilling%2520the%2520knowledge%250Afrom%25202D%2520diffusion%2520models%2520to%2520high-quality%2520representations%2520of%2520the%25203D%2520objects.%250AThese%2520methods%2520handle%2520multiple%2520text%2520queries%2520separately%252C%2520and%2520therefore%2520the%250Aresulting%2520objects%2520have%2520a%2520high%2520variability%2520in%2520object%2520pose%2520and%2520structure.%250AHowever%252C%2520in%2520some%2520applications%252C%2520such%2520as%25203D%2520asset%2520design%252C%2520it%2520may%2520be%2520desirable%2520to%250Aobtain%2520a%2520set%2520of%2520objects%2520aligned%2520with%2520each%2520other.%2520In%2520order%2520to%2520achieve%2520the%250Aalignment%2520of%2520the%2520corresponding%2520parts%2520of%2520the%2520generated%2520objects%252C%2520we%2520propose%2520to%250Aembed%2520these%2520objects%2520into%2520a%2520common%2520latent%2520space%2520and%2520optimize%2520the%2520continuous%250Atransitions%2520between%2520these%2520objects.%2520We%2520enforce%2520two%2520kinds%2520of%2520properties%2520of%2520these%250Atransitions%253A%2520smoothness%2520of%2520the%2520transition%2520and%2520plausibility%2520of%2520the%2520intermediate%250Aobjects%2520along%2520the%2520transition.%2520We%2520demonstrate%2520that%2520both%2520of%2520these%2520properties%2520are%250Aessential%2520for%2520good%2520alignment.%2520We%2520provide%2520several%2520practical%2520scenarios%2520that%250Abenefit%2520from%2520alignment%2520between%2520the%2520objects%252C%2520including%25203D%2520editing%2520and%2520object%250Ahybridization%252C%2520and%2520experimentally%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%250Ahttps%253A//voyleg.github.io/a3d/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15020v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A3D%3A%20Does%20Diffusion%20Dream%20about%203D%20Alignment%3F&entry.906535625=Savva%20Ignatyev%20and%20Nina%20Konovalova%20and%20Daniil%20Selikhanovych%20and%20Oleg%20Voynov%20and%20Nikolay%20Patakin%20and%20Ilya%20Olkov%20and%20Dmitry%20Senushkin%20and%20Alexey%20Artemov%20and%20Anton%20Konushin%20and%20Alexander%20Filippov%20and%20Peter%20Wonka%20and%20Evgeny%20Burnaev&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20text-driven%203D%20generation%20from%20a%20geometry%20alignment%0Aperspective.%20Given%20a%20set%20of%20text%20prompts%2C%20we%20aim%20to%20generate%20a%20collection%20of%0Aobjects%20with%20semantically%20corresponding%20parts%20aligned%20across%20them.%20Recent%0Amethods%20based%20on%20Score%20Distillation%20have%20succeeded%20in%20distilling%20the%20knowledge%0Afrom%202D%20diffusion%20models%20to%20high-quality%20representations%20of%20the%203D%20objects.%0AThese%20methods%20handle%20multiple%20text%20queries%20separately%2C%20and%20therefore%20the%0Aresulting%20objects%20have%20a%20high%20variability%20in%20object%20pose%20and%20structure.%0AHowever%2C%20in%20some%20applications%2C%20such%20as%203D%20asset%20design%2C%20it%20may%20be%20desirable%20to%0Aobtain%20a%20set%20of%20objects%20aligned%20with%20each%20other.%20In%20order%20to%20achieve%20the%0Aalignment%20of%20the%20corresponding%20parts%20of%20the%20generated%20objects%2C%20we%20propose%20to%0Aembed%20these%20objects%20into%20a%20common%20latent%20space%20and%20optimize%20the%20continuous%0Atransitions%20between%20these%20objects.%20We%20enforce%20two%20kinds%20of%20properties%20of%20these%0Atransitions%3A%20smoothness%20of%20the%20transition%20and%20plausibility%20of%20the%20intermediate%0Aobjects%20along%20the%20transition.%20We%20demonstrate%20that%20both%20of%20these%20properties%20are%0Aessential%20for%20good%20alignment.%20We%20provide%20several%20practical%20scenarios%20that%0Abenefit%20from%20alignment%20between%20the%20objects%2C%20including%203D%20editing%20and%20object%0Ahybridization%2C%20and%20experimentally%20demonstrate%20the%20effectiveness%20of%20our%20method.%0Ahttps%3A//voyleg.github.io/a3d/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15020v3&entry.124074799=Read"},
{"title": "QueensCAMP: an RGB-D dataset for robust Visual SLAM", "author": "Hudson M. S. Bruno and Esther L. Colombini and Sidney N. Givigi Jr", "abstract": "  Visual Simultaneous Localization and Mapping (VSLAM) is a fundamental\ntechnology for robotics applications. While VSLAM research has achieved\nsignificant advancements, its robustness under challenging situations, such as\npoor lighting, dynamic environments, motion blur, and sensor failures, remains\na challenging issue. To address these challenges, we introduce a novel RGB-D\ndataset designed for evaluating the robustness of VSLAM systems. The dataset\ncomprises real-world indoor scenes with dynamic objects, motion blur, and\nvarying illumination, as well as emulated camera failures, including lens dirt,\ncondensation, underexposure, and overexposure. Additionally, we offer\nopen-source scripts for injecting camera failures into any images, enabling\nfurther customization by the research community. Our experiments demonstrate\nthat ORB-SLAM2, a traditional VSLAM algorithm, and TartanVO, a Deep\nLearning-based VO algorithm, can experience performance degradation under these\nchallenging conditions. Therefore, this dataset and the camera failure\nopen-source tools provide a valuable resource for developing more robust VSLAM\nsystems capable of handling real-world challenges.\n", "link": "http://arxiv.org/abs/2410.12520v1", "date": "2024-10-16", "relevancy": 2.9546, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6303}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueensCAMP%3A%20an%20RGB-D%20dataset%20for%20robust%20Visual%20SLAM&body=Title%3A%20QueensCAMP%3A%20an%20RGB-D%20dataset%20for%20robust%20Visual%20SLAM%0AAuthor%3A%20Hudson%20M.%20S.%20Bruno%20and%20Esther%20L.%20Colombini%20and%20Sidney%20N.%20Givigi%20Jr%0AAbstract%3A%20%20%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28VSLAM%29%20is%20a%20fundamental%0Atechnology%20for%20robotics%20applications.%20While%20VSLAM%20research%20has%20achieved%0Asignificant%20advancements%2C%20its%20robustness%20under%20challenging%20situations%2C%20such%20as%0Apoor%20lighting%2C%20dynamic%20environments%2C%20motion%20blur%2C%20and%20sensor%20failures%2C%20remains%0Aa%20challenging%20issue.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20RGB-D%0Adataset%20designed%20for%20evaluating%20the%20robustness%20of%20VSLAM%20systems.%20The%20dataset%0Acomprises%20real-world%20indoor%20scenes%20with%20dynamic%20objects%2C%20motion%20blur%2C%20and%0Avarying%20illumination%2C%20as%20well%20as%20emulated%20camera%20failures%2C%20including%20lens%20dirt%2C%0Acondensation%2C%20underexposure%2C%20and%20overexposure.%20Additionally%2C%20we%20offer%0Aopen-source%20scripts%20for%20injecting%20camera%20failures%20into%20any%20images%2C%20enabling%0Afurther%20customization%20by%20the%20research%20community.%20Our%20experiments%20demonstrate%0Athat%20ORB-SLAM2%2C%20a%20traditional%20VSLAM%20algorithm%2C%20and%20TartanVO%2C%20a%20Deep%0ALearning-based%20VO%20algorithm%2C%20can%20experience%20performance%20degradation%20under%20these%0Achallenging%20conditions.%20Therefore%2C%20this%20dataset%20and%20the%20camera%20failure%0Aopen-source%20tools%20provide%20a%20valuable%20resource%20for%20developing%20more%20robust%20VSLAM%0Asystems%20capable%20of%20handling%20real-world%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueensCAMP%253A%2520an%2520RGB-D%2520dataset%2520for%2520robust%2520Visual%2520SLAM%26entry.906535625%3DHudson%2520M.%2520S.%2520Bruno%2520and%2520Esther%2520L.%2520Colombini%2520and%2520Sidney%2520N.%2520Givigi%2520Jr%26entry.1292438233%3D%2520%2520Visual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528VSLAM%2529%2520is%2520a%2520fundamental%250Atechnology%2520for%2520robotics%2520applications.%2520While%2520VSLAM%2520research%2520has%2520achieved%250Asignificant%2520advancements%252C%2520its%2520robustness%2520under%2520challenging%2520situations%252C%2520such%2520as%250Apoor%2520lighting%252C%2520dynamic%2520environments%252C%2520motion%2520blur%252C%2520and%2520sensor%2520failures%252C%2520remains%250Aa%2520challenging%2520issue.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520RGB-D%250Adataset%2520designed%2520for%2520evaluating%2520the%2520robustness%2520of%2520VSLAM%2520systems.%2520The%2520dataset%250Acomprises%2520real-world%2520indoor%2520scenes%2520with%2520dynamic%2520objects%252C%2520motion%2520blur%252C%2520and%250Avarying%2520illumination%252C%2520as%2520well%2520as%2520emulated%2520camera%2520failures%252C%2520including%2520lens%2520dirt%252C%250Acondensation%252C%2520underexposure%252C%2520and%2520overexposure.%2520Additionally%252C%2520we%2520offer%250Aopen-source%2520scripts%2520for%2520injecting%2520camera%2520failures%2520into%2520any%2520images%252C%2520enabling%250Afurther%2520customization%2520by%2520the%2520research%2520community.%2520Our%2520experiments%2520demonstrate%250Athat%2520ORB-SLAM2%252C%2520a%2520traditional%2520VSLAM%2520algorithm%252C%2520and%2520TartanVO%252C%2520a%2520Deep%250ALearning-based%2520VO%2520algorithm%252C%2520can%2520experience%2520performance%2520degradation%2520under%2520these%250Achallenging%2520conditions.%2520Therefore%252C%2520this%2520dataset%2520and%2520the%2520camera%2520failure%250Aopen-source%2520tools%2520provide%2520a%2520valuable%2520resource%2520for%2520developing%2520more%2520robust%2520VSLAM%250Asystems%2520capable%2520of%2520handling%2520real-world%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueensCAMP%3A%20an%20RGB-D%20dataset%20for%20robust%20Visual%20SLAM&entry.906535625=Hudson%20M.%20S.%20Bruno%20and%20Esther%20L.%20Colombini%20and%20Sidney%20N.%20Givigi%20Jr&entry.1292438233=%20%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28VSLAM%29%20is%20a%20fundamental%0Atechnology%20for%20robotics%20applications.%20While%20VSLAM%20research%20has%20achieved%0Asignificant%20advancements%2C%20its%20robustness%20under%20challenging%20situations%2C%20such%20as%0Apoor%20lighting%2C%20dynamic%20environments%2C%20motion%20blur%2C%20and%20sensor%20failures%2C%20remains%0Aa%20challenging%20issue.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20RGB-D%0Adataset%20designed%20for%20evaluating%20the%20robustness%20of%20VSLAM%20systems.%20The%20dataset%0Acomprises%20real-world%20indoor%20scenes%20with%20dynamic%20objects%2C%20motion%20blur%2C%20and%0Avarying%20illumination%2C%20as%20well%20as%20emulated%20camera%20failures%2C%20including%20lens%20dirt%2C%0Acondensation%2C%20underexposure%2C%20and%20overexposure.%20Additionally%2C%20we%20offer%0Aopen-source%20scripts%20for%20injecting%20camera%20failures%20into%20any%20images%2C%20enabling%0Afurther%20customization%20by%20the%20research%20community.%20Our%20experiments%20demonstrate%0Athat%20ORB-SLAM2%2C%20a%20traditional%20VSLAM%20algorithm%2C%20and%20TartanVO%2C%20a%20Deep%0ALearning-based%20VO%20algorithm%2C%20can%20experience%20performance%20degradation%20under%20these%0Achallenging%20conditions.%20Therefore%2C%20this%20dataset%20and%20the%20camera%20failure%0Aopen-source%20tools%20provide%20a%20valuable%20resource%20for%20developing%20more%20robust%20VSLAM%0Asystems%20capable%20of%20handling%20real-world%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12520v1&entry.124074799=Read"},
{"title": "Gravity-aligned Rotation Averaging with Circular Regression", "author": "Linfei Pan and Marc Pollefeys and D\u00e1niel Bar\u00e1th", "abstract": "  Reconstructing a 3D scene from unordered images is pivotal in computer vision\nand robotics, with applications spanning crowd-sourced mapping and beyond.\nWhile global Structure-from-Motion (SfM) techniques are scalable and fast, they\noften compromise on accuracy. To address this, we introduce a principled\napproach that integrates gravity direction into the rotation averaging phase of\nglobal pipelines, enhancing camera orientation accuracy and reducing the\ndegrees of freedom. This additional information is commonly available in recent\nconsumer devices, such as smartphones, mixed-reality devices and drones, making\nthe proposed method readily accessible. Rooted in circular regression, our\nalgorithm has similar convergence guarantees as linear regression. It also\nsupports scenarios where only a subset of cameras have known gravity.\nAdditionally, we propose a mechanism to refine error-prone gravity. We achieve\nstate-of-the-art accuracy on four large-scale datasets. Particularly, the\nproposed method improves upon the SfM baseline by 13 AUC@$1^\\circ$ points, on\naverage, while running eight times faster. It also outperforms the standard\nplanar pose graph optimization technique by 23 AUC@$1^\\circ$ points. The code\nis at https://github.com/colmap/glomap.\n", "link": "http://arxiv.org/abs/2410.12763v1", "date": "2024-10-16", "relevancy": 2.8913, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6048}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5737}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gravity-aligned%20Rotation%20Averaging%20with%20Circular%20Regression&body=Title%3A%20Gravity-aligned%20Rotation%20Averaging%20with%20Circular%20Regression%0AAuthor%3A%20Linfei%20Pan%20and%20Marc%20Pollefeys%20and%20D%C3%A1niel%20Bar%C3%A1th%0AAbstract%3A%20%20%20Reconstructing%20a%203D%20scene%20from%20unordered%20images%20is%20pivotal%20in%20computer%20vision%0Aand%20robotics%2C%20with%20applications%20spanning%20crowd-sourced%20mapping%20and%20beyond.%0AWhile%20global%20Structure-from-Motion%20%28SfM%29%20techniques%20are%20scalable%20and%20fast%2C%20they%0Aoften%20compromise%20on%20accuracy.%20To%20address%20this%2C%20we%20introduce%20a%20principled%0Aapproach%20that%20integrates%20gravity%20direction%20into%20the%20rotation%20averaging%20phase%20of%0Aglobal%20pipelines%2C%20enhancing%20camera%20orientation%20accuracy%20and%20reducing%20the%0Adegrees%20of%20freedom.%20This%20additional%20information%20is%20commonly%20available%20in%20recent%0Aconsumer%20devices%2C%20such%20as%20smartphones%2C%20mixed-reality%20devices%20and%20drones%2C%20making%0Athe%20proposed%20method%20readily%20accessible.%20Rooted%20in%20circular%20regression%2C%20our%0Aalgorithm%20has%20similar%20convergence%20guarantees%20as%20linear%20regression.%20It%20also%0Asupports%20scenarios%20where%20only%20a%20subset%20of%20cameras%20have%20known%20gravity.%0AAdditionally%2C%20we%20propose%20a%20mechanism%20to%20refine%20error-prone%20gravity.%20We%20achieve%0Astate-of-the-art%20accuracy%20on%20four%20large-scale%20datasets.%20Particularly%2C%20the%0Aproposed%20method%20improves%20upon%20the%20SfM%20baseline%20by%2013%20AUC%40%241%5E%5Ccirc%24%20points%2C%20on%0Aaverage%2C%20while%20running%20eight%20times%20faster.%20It%20also%20outperforms%20the%20standard%0Aplanar%20pose%20graph%20optimization%20technique%20by%2023%20AUC%40%241%5E%5Ccirc%24%20points.%20The%20code%0Ais%20at%20https%3A//github.com/colmap/glomap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGravity-aligned%2520Rotation%2520Averaging%2520with%2520Circular%2520Regression%26entry.906535625%3DLinfei%2520Pan%2520and%2520Marc%2520Pollefeys%2520and%2520D%25C3%25A1niel%2520Bar%25C3%25A1th%26entry.1292438233%3D%2520%2520Reconstructing%2520a%25203D%2520scene%2520from%2520unordered%2520images%2520is%2520pivotal%2520in%2520computer%2520vision%250Aand%2520robotics%252C%2520with%2520applications%2520spanning%2520crowd-sourced%2520mapping%2520and%2520beyond.%250AWhile%2520global%2520Structure-from-Motion%2520%2528SfM%2529%2520techniques%2520are%2520scalable%2520and%2520fast%252C%2520they%250Aoften%2520compromise%2520on%2520accuracy.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520principled%250Aapproach%2520that%2520integrates%2520gravity%2520direction%2520into%2520the%2520rotation%2520averaging%2520phase%2520of%250Aglobal%2520pipelines%252C%2520enhancing%2520camera%2520orientation%2520accuracy%2520and%2520reducing%2520the%250Adegrees%2520of%2520freedom.%2520This%2520additional%2520information%2520is%2520commonly%2520available%2520in%2520recent%250Aconsumer%2520devices%252C%2520such%2520as%2520smartphones%252C%2520mixed-reality%2520devices%2520and%2520drones%252C%2520making%250Athe%2520proposed%2520method%2520readily%2520accessible.%2520Rooted%2520in%2520circular%2520regression%252C%2520our%250Aalgorithm%2520has%2520similar%2520convergence%2520guarantees%2520as%2520linear%2520regression.%2520It%2520also%250Asupports%2520scenarios%2520where%2520only%2520a%2520subset%2520of%2520cameras%2520have%2520known%2520gravity.%250AAdditionally%252C%2520we%2520propose%2520a%2520mechanism%2520to%2520refine%2520error-prone%2520gravity.%2520We%2520achieve%250Astate-of-the-art%2520accuracy%2520on%2520four%2520large-scale%2520datasets.%2520Particularly%252C%2520the%250Aproposed%2520method%2520improves%2520upon%2520the%2520SfM%2520baseline%2520by%252013%2520AUC%2540%25241%255E%255Ccirc%2524%2520points%252C%2520on%250Aaverage%252C%2520while%2520running%2520eight%2520times%2520faster.%2520It%2520also%2520outperforms%2520the%2520standard%250Aplanar%2520pose%2520graph%2520optimization%2520technique%2520by%252023%2520AUC%2540%25241%255E%255Ccirc%2524%2520points.%2520The%2520code%250Ais%2520at%2520https%253A//github.com/colmap/glomap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gravity-aligned%20Rotation%20Averaging%20with%20Circular%20Regression&entry.906535625=Linfei%20Pan%20and%20Marc%20Pollefeys%20and%20D%C3%A1niel%20Bar%C3%A1th&entry.1292438233=%20%20Reconstructing%20a%203D%20scene%20from%20unordered%20images%20is%20pivotal%20in%20computer%20vision%0Aand%20robotics%2C%20with%20applications%20spanning%20crowd-sourced%20mapping%20and%20beyond.%0AWhile%20global%20Structure-from-Motion%20%28SfM%29%20techniques%20are%20scalable%20and%20fast%2C%20they%0Aoften%20compromise%20on%20accuracy.%20To%20address%20this%2C%20we%20introduce%20a%20principled%0Aapproach%20that%20integrates%20gravity%20direction%20into%20the%20rotation%20averaging%20phase%20of%0Aglobal%20pipelines%2C%20enhancing%20camera%20orientation%20accuracy%20and%20reducing%20the%0Adegrees%20of%20freedom.%20This%20additional%20information%20is%20commonly%20available%20in%20recent%0Aconsumer%20devices%2C%20such%20as%20smartphones%2C%20mixed-reality%20devices%20and%20drones%2C%20making%0Athe%20proposed%20method%20readily%20accessible.%20Rooted%20in%20circular%20regression%2C%20our%0Aalgorithm%20has%20similar%20convergence%20guarantees%20as%20linear%20regression.%20It%20also%0Asupports%20scenarios%20where%20only%20a%20subset%20of%20cameras%20have%20known%20gravity.%0AAdditionally%2C%20we%20propose%20a%20mechanism%20to%20refine%20error-prone%20gravity.%20We%20achieve%0Astate-of-the-art%20accuracy%20on%20four%20large-scale%20datasets.%20Particularly%2C%20the%0Aproposed%20method%20improves%20upon%20the%20SfM%20baseline%20by%2013%20AUC%40%241%5E%5Ccirc%24%20points%2C%20on%0Aaverage%2C%20while%20running%20eight%20times%20faster.%20It%20also%20outperforms%20the%20standard%0Aplanar%20pose%20graph%20optimization%20technique%20by%2023%20AUC%40%241%5E%5Ccirc%24%20points.%20The%20code%0Ais%20at%20https%3A//github.com/colmap/glomap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12763v1&entry.124074799=Read"},
{"title": "On Large Uni- and Multi-modal Models for Unsupervised Classification of\n  Social Media Images: Nature's Contribution to People as a case study", "author": "Rohaifa Khaldi and Domingo Alcaraz-Segura and Ignacio S\u00e1nchez-Herrera and Javier Martinez-Lopez and Carlos Javier Navarro and Siham Tabik", "abstract": "  Social media images have proven to be a valuable source of information for\nunderstanding human interactions with important subjects such as cultural\nheritage, biodiversity, and nature, among others. The task of grouping such\nimages into a number of semantically meaningful clusters without labels is\nchallenging due to the high diversity and complex nature of the visual content\nin addition to their large volume. On the other hand, recent advances in Large\nVisual Models (LVMs), Large Language Models (LLMs), and Large Visual Language\nModels (LVLMs) provide an important opportunity to explore new productive and\nscalable solutions. This work proposes, analyzes, and compares various\napproaches based on one or more state-of-the-art LVM, LLM, and LVLM, for\nmapping social media images into a number of predefined classes. As a case\nstudy, we consider the problem of understanding the interactions between humans\nand nature, also known as Nature's Contribution to People or Cultural Ecosystem\nServices (CES). Our experiments show that the highest-performing approaches,\nwith accuracy above 95%, still require the creation of a small labeled dataset.\nThese include the fine-tuned LVM DINOv2 and the LVLM LLaVA-1.5 combined with a\nfine-tuned LLM. The top fully unsupervised approaches, achieving accuracy above\n84%, are the LVLMs, specifically the proprietary GPT-4 model and the public\nLLaVA-1.5 model. Additionally, the LVM DINOv2, when applied in a 10-shot\nlearning setup, delivered competitive results with an accuracy of 83.99%,\nclosely matching the performance of the LVLM LLaVA-1.5.\n", "link": "http://arxiv.org/abs/2410.00275v2", "date": "2024-10-16", "relevancy": 2.8622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.575}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Large%20Uni-%20and%20Multi-modal%20Models%20for%20Unsupervised%20Classification%20of%0A%20%20Social%20Media%20Images%3A%20Nature%27s%20Contribution%20to%20People%20as%20a%20case%20study&body=Title%3A%20On%20Large%20Uni-%20and%20Multi-modal%20Models%20for%20Unsupervised%20Classification%20of%0A%20%20Social%20Media%20Images%3A%20Nature%27s%20Contribution%20to%20People%20as%20a%20case%20study%0AAuthor%3A%20Rohaifa%20Khaldi%20and%20Domingo%20Alcaraz-Segura%20and%20Ignacio%20S%C3%A1nchez-Herrera%20and%20Javier%20Martinez-Lopez%20and%20Carlos%20Javier%20Navarro%20and%20Siham%20Tabik%0AAbstract%3A%20%20%20Social%20media%20images%20have%20proven%20to%20be%20a%20valuable%20source%20of%20information%20for%0Aunderstanding%20human%20interactions%20with%20important%20subjects%20such%20as%20cultural%0Aheritage%2C%20biodiversity%2C%20and%20nature%2C%20among%20others.%20The%20task%20of%20grouping%20such%0Aimages%20into%20a%20number%20of%20semantically%20meaningful%20clusters%20without%20labels%20is%0Achallenging%20due%20to%20the%20high%20diversity%20and%20complex%20nature%20of%20the%20visual%20content%0Ain%20addition%20to%20their%20large%20volume.%20On%20the%20other%20hand%2C%20recent%20advances%20in%20Large%0AVisual%20Models%20%28LVMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20Large%20Visual%20Language%0AModels%20%28LVLMs%29%20provide%20an%20important%20opportunity%20to%20explore%20new%20productive%20and%0Ascalable%20solutions.%20This%20work%20proposes%2C%20analyzes%2C%20and%20compares%20various%0Aapproaches%20based%20on%20one%20or%20more%20state-of-the-art%20LVM%2C%20LLM%2C%20and%20LVLM%2C%20for%0Amapping%20social%20media%20images%20into%20a%20number%20of%20predefined%20classes.%20As%20a%20case%0Astudy%2C%20we%20consider%20the%20problem%20of%20understanding%20the%20interactions%20between%20humans%0Aand%20nature%2C%20also%20known%20as%20Nature%27s%20Contribution%20to%20People%20or%20Cultural%20Ecosystem%0AServices%20%28CES%29.%20Our%20experiments%20show%20that%20the%20highest-performing%20approaches%2C%0Awith%20accuracy%20above%2095%25%2C%20still%20require%20the%20creation%20of%20a%20small%20labeled%20dataset.%0AThese%20include%20the%20fine-tuned%20LVM%20DINOv2%20and%20the%20LVLM%20LLaVA-1.5%20combined%20with%20a%0Afine-tuned%20LLM.%20The%20top%20fully%20unsupervised%20approaches%2C%20achieving%20accuracy%20above%0A84%25%2C%20are%20the%20LVLMs%2C%20specifically%20the%20proprietary%20GPT-4%20model%20and%20the%20public%0ALLaVA-1.5%20model.%20Additionally%2C%20the%20LVM%20DINOv2%2C%20when%20applied%20in%20a%2010-shot%0Alearning%20setup%2C%20delivered%20competitive%20results%20with%20an%20accuracy%20of%2083.99%25%2C%0Aclosely%20matching%20the%20performance%20of%20the%20LVLM%20LLaVA-1.5.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Large%2520Uni-%2520and%2520Multi-modal%2520Models%2520for%2520Unsupervised%2520Classification%2520of%250A%2520%2520Social%2520Media%2520Images%253A%2520Nature%2527s%2520Contribution%2520to%2520People%2520as%2520a%2520case%2520study%26entry.906535625%3DRohaifa%2520Khaldi%2520and%2520Domingo%2520Alcaraz-Segura%2520and%2520Ignacio%2520S%25C3%25A1nchez-Herrera%2520and%2520Javier%2520Martinez-Lopez%2520and%2520Carlos%2520Javier%2520Navarro%2520and%2520Siham%2520Tabik%26entry.1292438233%3D%2520%2520Social%2520media%2520images%2520have%2520proven%2520to%2520be%2520a%2520valuable%2520source%2520of%2520information%2520for%250Aunderstanding%2520human%2520interactions%2520with%2520important%2520subjects%2520such%2520as%2520cultural%250Aheritage%252C%2520biodiversity%252C%2520and%2520nature%252C%2520among%2520others.%2520The%2520task%2520of%2520grouping%2520such%250Aimages%2520into%2520a%2520number%2520of%2520semantically%2520meaningful%2520clusters%2520without%2520labels%2520is%250Achallenging%2520due%2520to%2520the%2520high%2520diversity%2520and%2520complex%2520nature%2520of%2520the%2520visual%2520content%250Ain%2520addition%2520to%2520their%2520large%2520volume.%2520On%2520the%2520other%2520hand%252C%2520recent%2520advances%2520in%2520Large%250AVisual%2520Models%2520%2528LVMs%2529%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520and%2520Large%2520Visual%2520Language%250AModels%2520%2528LVLMs%2529%2520provide%2520an%2520important%2520opportunity%2520to%2520explore%2520new%2520productive%2520and%250Ascalable%2520solutions.%2520This%2520work%2520proposes%252C%2520analyzes%252C%2520and%2520compares%2520various%250Aapproaches%2520based%2520on%2520one%2520or%2520more%2520state-of-the-art%2520LVM%252C%2520LLM%252C%2520and%2520LVLM%252C%2520for%250Amapping%2520social%2520media%2520images%2520into%2520a%2520number%2520of%2520predefined%2520classes.%2520As%2520a%2520case%250Astudy%252C%2520we%2520consider%2520the%2520problem%2520of%2520understanding%2520the%2520interactions%2520between%2520humans%250Aand%2520nature%252C%2520also%2520known%2520as%2520Nature%2527s%2520Contribution%2520to%2520People%2520or%2520Cultural%2520Ecosystem%250AServices%2520%2528CES%2529.%2520Our%2520experiments%2520show%2520that%2520the%2520highest-performing%2520approaches%252C%250Awith%2520accuracy%2520above%252095%2525%252C%2520still%2520require%2520the%2520creation%2520of%2520a%2520small%2520labeled%2520dataset.%250AThese%2520include%2520the%2520fine-tuned%2520LVM%2520DINOv2%2520and%2520the%2520LVLM%2520LLaVA-1.5%2520combined%2520with%2520a%250Afine-tuned%2520LLM.%2520The%2520top%2520fully%2520unsupervised%2520approaches%252C%2520achieving%2520accuracy%2520above%250A84%2525%252C%2520are%2520the%2520LVLMs%252C%2520specifically%2520the%2520proprietary%2520GPT-4%2520model%2520and%2520the%2520public%250ALLaVA-1.5%2520model.%2520Additionally%252C%2520the%2520LVM%2520DINOv2%252C%2520when%2520applied%2520in%2520a%252010-shot%250Alearning%2520setup%252C%2520delivered%2520competitive%2520results%2520with%2520an%2520accuracy%2520of%252083.99%2525%252C%250Aclosely%2520matching%2520the%2520performance%2520of%2520the%2520LVLM%2520LLaVA-1.5.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Large%20Uni-%20and%20Multi-modal%20Models%20for%20Unsupervised%20Classification%20of%0A%20%20Social%20Media%20Images%3A%20Nature%27s%20Contribution%20to%20People%20as%20a%20case%20study&entry.906535625=Rohaifa%20Khaldi%20and%20Domingo%20Alcaraz-Segura%20and%20Ignacio%20S%C3%A1nchez-Herrera%20and%20Javier%20Martinez-Lopez%20and%20Carlos%20Javier%20Navarro%20and%20Siham%20Tabik&entry.1292438233=%20%20Social%20media%20images%20have%20proven%20to%20be%20a%20valuable%20source%20of%20information%20for%0Aunderstanding%20human%20interactions%20with%20important%20subjects%20such%20as%20cultural%0Aheritage%2C%20biodiversity%2C%20and%20nature%2C%20among%20others.%20The%20task%20of%20grouping%20such%0Aimages%20into%20a%20number%20of%20semantically%20meaningful%20clusters%20without%20labels%20is%0Achallenging%20due%20to%20the%20high%20diversity%20and%20complex%20nature%20of%20the%20visual%20content%0Ain%20addition%20to%20their%20large%20volume.%20On%20the%20other%20hand%2C%20recent%20advances%20in%20Large%0AVisual%20Models%20%28LVMs%29%2C%20Large%20Language%20Models%20%28LLMs%29%2C%20and%20Large%20Visual%20Language%0AModels%20%28LVLMs%29%20provide%20an%20important%20opportunity%20to%20explore%20new%20productive%20and%0Ascalable%20solutions.%20This%20work%20proposes%2C%20analyzes%2C%20and%20compares%20various%0Aapproaches%20based%20on%20one%20or%20more%20state-of-the-art%20LVM%2C%20LLM%2C%20and%20LVLM%2C%20for%0Amapping%20social%20media%20images%20into%20a%20number%20of%20predefined%20classes.%20As%20a%20case%0Astudy%2C%20we%20consider%20the%20problem%20of%20understanding%20the%20interactions%20between%20humans%0Aand%20nature%2C%20also%20known%20as%20Nature%27s%20Contribution%20to%20People%20or%20Cultural%20Ecosystem%0AServices%20%28CES%29.%20Our%20experiments%20show%20that%20the%20highest-performing%20approaches%2C%0Awith%20accuracy%20above%2095%25%2C%20still%20require%20the%20creation%20of%20a%20small%20labeled%20dataset.%0AThese%20include%20the%20fine-tuned%20LVM%20DINOv2%20and%20the%20LVLM%20LLaVA-1.5%20combined%20with%20a%0Afine-tuned%20LLM.%20The%20top%20fully%20unsupervised%20approaches%2C%20achieving%20accuracy%20above%0A84%25%2C%20are%20the%20LVLMs%2C%20specifically%20the%20proprietary%20GPT-4%20model%20and%20the%20public%0ALLaVA-1.5%20model.%20Additionally%2C%20the%20LVM%20DINOv2%2C%20when%20applied%20in%20a%2010-shot%0Alearning%20setup%2C%20delivered%20competitive%20results%20with%20an%20accuracy%20of%2083.99%25%2C%0Aclosely%20matching%20the%20performance%20of%20the%20LVLM%20LLaVA-1.5.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00275v2&entry.124074799=Read"},
{"title": "Understanding Figurative Meaning through Explainable Visual Entailment", "author": "Arkadiy Saakyan and Shreyas Kulkarni and Tuhin Chakrabarty and Smaranda Muresan", "abstract": "  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n", "link": "http://arxiv.org/abs/2405.01474v2", "date": "2024-10-16", "relevancy": 2.8325, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment&body=Title%3A%20Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment%0AAuthor%3A%20Arkadiy%20Saakyan%20and%20Shreyas%20Kulkarni%20and%20Tuhin%20Chakrabarty%20and%20Smaranda%20Muresan%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Atasks%20requiring%20a%20fine-grained%20understanding%20of%20literal%20meaning%20in%20images%20and%0Atext%2C%20such%20as%20visual%20question-answering%20or%20visual%20entailment.%20However%2C%20there%0Ahas%20been%20little%20exploration%20of%20these%20models%27%20capabilities%20when%20presented%20with%0Aimages%20and%20captions%20containing%20figurative%20meaning%2C%20such%20as%20metaphors%20or%20humor.%0ATo%20close%20this%20gap%2C%20we%20propose%20a%20new%20task%20framing%20the%20figurative%20meaning%0Aunderstanding%20problem%20as%20an%20explainable%20visual%20entailment%20task%2C%20where%20the%20model%0Ahas%20to%20predict%20whether%20the%20image%20%28premise%29%20entails%20a%20caption%20%28hypothesis%29%20and%0Ajustify%20the%20predicted%20label%20with%20a%20textual%20explanation.%20The%20figurative%0Aphenomena%20can%20be%20present%20either%20in%20the%20image%2C%20the%20caption%2C%20or%20both.%20Utilizing%20a%0Ahuman-AI%20collaboration%20approach%2C%20we%20build%20the%20accompanying%20expert-verified%0Adataset%20V-FLUTE%2C%20containing%206%2C027%20%7Bimage%2C%20caption%2C%20label%2C%20explanation%7D%0Ainstances%20spanning%20five%20diverse%20figurative%20phenomena%3A%20metaphors%2C%20similes%2C%0Aidioms%2C%20sarcasm%2C%20and%20humor.%20Through%20automatic%20evaluation%2C%20we%20find%20that%20VLMs%0Astruggle%20to%20generalize%20from%20literal%20to%20figurative%20meaning%2C%20particularly%20when%20it%0Ais%20present%20in%20images.%20Further%2C%20we%20identify%20common%20types%20of%20errors%20in%20VLM%0Areasoning%20via%20human%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01474v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Figurative%2520Meaning%2520through%2520Explainable%2520Visual%2520Entailment%26entry.906535625%3DArkadiy%2520Saakyan%2520and%2520Shreyas%2520Kulkarni%2520and%2520Tuhin%2520Chakrabarty%2520and%2520Smaranda%2520Muresan%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%250Atasks%2520requiring%2520a%2520fine-grained%2520understanding%2520of%2520literal%2520meaning%2520in%2520images%2520and%250Atext%252C%2520such%2520as%2520visual%2520question-answering%2520or%2520visual%2520entailment.%2520However%252C%2520there%250Ahas%2520been%2520little%2520exploration%2520of%2520these%2520models%2527%2520capabilities%2520when%2520presented%2520with%250Aimages%2520and%2520captions%2520containing%2520figurative%2520meaning%252C%2520such%2520as%2520metaphors%2520or%2520humor.%250ATo%2520close%2520this%2520gap%252C%2520we%2520propose%2520a%2520new%2520task%2520framing%2520the%2520figurative%2520meaning%250Aunderstanding%2520problem%2520as%2520an%2520explainable%2520visual%2520entailment%2520task%252C%2520where%2520the%2520model%250Ahas%2520to%2520predict%2520whether%2520the%2520image%2520%2528premise%2529%2520entails%2520a%2520caption%2520%2528hypothesis%2529%2520and%250Ajustify%2520the%2520predicted%2520label%2520with%2520a%2520textual%2520explanation.%2520The%2520figurative%250Aphenomena%2520can%2520be%2520present%2520either%2520in%2520the%2520image%252C%2520the%2520caption%252C%2520or%2520both.%2520Utilizing%2520a%250Ahuman-AI%2520collaboration%2520approach%252C%2520we%2520build%2520the%2520accompanying%2520expert-verified%250Adataset%2520V-FLUTE%252C%2520containing%25206%252C027%2520%257Bimage%252C%2520caption%252C%2520label%252C%2520explanation%257D%250Ainstances%2520spanning%2520five%2520diverse%2520figurative%2520phenomena%253A%2520metaphors%252C%2520similes%252C%250Aidioms%252C%2520sarcasm%252C%2520and%2520humor.%2520Through%2520automatic%2520evaluation%252C%2520we%2520find%2520that%2520VLMs%250Astruggle%2520to%2520generalize%2520from%2520literal%2520to%2520figurative%2520meaning%252C%2520particularly%2520when%2520it%250Ais%2520present%2520in%2520images.%2520Further%252C%2520we%2520identify%2520common%2520types%2520of%2520errors%2520in%2520VLM%250Areasoning%2520via%2520human%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01474v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Figurative%20Meaning%20through%20Explainable%20Visual%20Entailment&entry.906535625=Arkadiy%20Saakyan%20and%20Shreyas%20Kulkarni%20and%20Tuhin%20Chakrabarty%20and%20Smaranda%20Muresan&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Atasks%20requiring%20a%20fine-grained%20understanding%20of%20literal%20meaning%20in%20images%20and%0Atext%2C%20such%20as%20visual%20question-answering%20or%20visual%20entailment.%20However%2C%20there%0Ahas%20been%20little%20exploration%20of%20these%20models%27%20capabilities%20when%20presented%20with%0Aimages%20and%20captions%20containing%20figurative%20meaning%2C%20such%20as%20metaphors%20or%20humor.%0ATo%20close%20this%20gap%2C%20we%20propose%20a%20new%20task%20framing%20the%20figurative%20meaning%0Aunderstanding%20problem%20as%20an%20explainable%20visual%20entailment%20task%2C%20where%20the%20model%0Ahas%20to%20predict%20whether%20the%20image%20%28premise%29%20entails%20a%20caption%20%28hypothesis%29%20and%0Ajustify%20the%20predicted%20label%20with%20a%20textual%20explanation.%20The%20figurative%0Aphenomena%20can%20be%20present%20either%20in%20the%20image%2C%20the%20caption%2C%20or%20both.%20Utilizing%20a%0Ahuman-AI%20collaboration%20approach%2C%20we%20build%20the%20accompanying%20expert-verified%0Adataset%20V-FLUTE%2C%20containing%206%2C027%20%7Bimage%2C%20caption%2C%20label%2C%20explanation%7D%0Ainstances%20spanning%20five%20diverse%20figurative%20phenomena%3A%20metaphors%2C%20similes%2C%0Aidioms%2C%20sarcasm%2C%20and%20humor.%20Through%20automatic%20evaluation%2C%20we%20find%20that%20VLMs%0Astruggle%20to%20generalize%20from%20literal%20to%20figurative%20meaning%2C%20particularly%20when%20it%0Ais%20present%20in%20images.%20Further%2C%20we%20identify%20common%20types%20of%20errors%20in%20VLM%0Areasoning%20via%20human%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01474v2&entry.124074799=Read"},
{"title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric\n  Learning and Generative Modeling on Data Manifolds", "author": "Xingzhi Sun and Danqi Liao and Kincaid MacDonald and Yanlei Zhang and Chen Liu and Guillaume Huguet and Guy Wolf and Ian Adelstein and Tim G. J. Rudner and Smita Krishnaswamy", "abstract": "  Rapid growth of high-dimensional datasets in fields such as single-cell RNA\nsequencing and spatial genomics has led to unprecedented opportunities for\nscientific discovery, but it also presents unique computational and statistical\nchallenges. Traditional methods struggle with geometry-aware data generation,\ninterpolation along meaningful trajectories, and transporting populations via\nfeasible paths. To address these issues, we introduce Geometry-Aware Generative\nAutoencoder (GAGA), a novel framework that combines extensible manifold\nlearning with generative modeling. GAGA constructs a neural network embedding\nspace that respects the intrinsic geometries discovered by manifold learning\nand learns a novel warped Riemannian metric on the data space. This warped\nmetric is derived from both the points on the data manifold and negative\nsamples off the manifold, allowing it to characterize a meaningful geometry\nacross the entire latent space. Using this metric, GAGA can uniformly sample\npoints on the manifold, generate points along geodesics, and interpolate\nbetween populations across the learned manifold. GAGA shows competitive\nperformance in simulated and real world datasets, including a 30% improvement\nover the state-of-the-art methods in single-cell population-level trajectory\ninference.\n", "link": "http://arxiv.org/abs/2410.12779v1", "date": "2024-10-16", "relevancy": 2.8285, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6002}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5506}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Generative%20Autoencoders%20for%20Warped%20Riemannian%20Metric%0A%20%20Learning%20and%20Generative%20Modeling%20on%20Data%20Manifolds&body=Title%3A%20Geometry-Aware%20Generative%20Autoencoders%20for%20Warped%20Riemannian%20Metric%0A%20%20Learning%20and%20Generative%20Modeling%20on%20Data%20Manifolds%0AAuthor%3A%20Xingzhi%20Sun%20and%20Danqi%20Liao%20and%20Kincaid%20MacDonald%20and%20Yanlei%20Zhang%20and%20Chen%20Liu%20and%20Guillaume%20Huguet%20and%20Guy%20Wolf%20and%20Ian%20Adelstein%20and%20Tim%20G.%20J.%20Rudner%20and%20Smita%20Krishnaswamy%0AAbstract%3A%20%20%20Rapid%20growth%20of%20high-dimensional%20datasets%20in%20fields%20such%20as%20single-cell%20RNA%0Asequencing%20and%20spatial%20genomics%20has%20led%20to%20unprecedented%20opportunities%20for%0Ascientific%20discovery%2C%20but%20it%20also%20presents%20unique%20computational%20and%20statistical%0Achallenges.%20Traditional%20methods%20struggle%20with%20geometry-aware%20data%20generation%2C%0Ainterpolation%20along%20meaningful%20trajectories%2C%20and%20transporting%20populations%20via%0Afeasible%20paths.%20To%20address%20these%20issues%2C%20we%20introduce%20Geometry-Aware%20Generative%0AAutoencoder%20%28GAGA%29%2C%20a%20novel%20framework%20that%20combines%20extensible%20manifold%0Alearning%20with%20generative%20modeling.%20GAGA%20constructs%20a%20neural%20network%20embedding%0Aspace%20that%20respects%20the%20intrinsic%20geometries%20discovered%20by%20manifold%20learning%0Aand%20learns%20a%20novel%20warped%20Riemannian%20metric%20on%20the%20data%20space.%20This%20warped%0Ametric%20is%20derived%20from%20both%20the%20points%20on%20the%20data%20manifold%20and%20negative%0Asamples%20off%20the%20manifold%2C%20allowing%20it%20to%20characterize%20a%20meaningful%20geometry%0Aacross%20the%20entire%20latent%20space.%20Using%20this%20metric%2C%20GAGA%20can%20uniformly%20sample%0Apoints%20on%20the%20manifold%2C%20generate%20points%20along%20geodesics%2C%20and%20interpolate%0Abetween%20populations%20across%20the%20learned%20manifold.%20GAGA%20shows%20competitive%0Aperformance%20in%20simulated%20and%20real%20world%20datasets%2C%20including%20a%2030%25%20improvement%0Aover%20the%20state-of-the-art%20methods%20in%20single-cell%20population-level%20trajectory%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Generative%2520Autoencoders%2520for%2520Warped%2520Riemannian%2520Metric%250A%2520%2520Learning%2520and%2520Generative%2520Modeling%2520on%2520Data%2520Manifolds%26entry.906535625%3DXingzhi%2520Sun%2520and%2520Danqi%2520Liao%2520and%2520Kincaid%2520MacDonald%2520and%2520Yanlei%2520Zhang%2520and%2520Chen%2520Liu%2520and%2520Guillaume%2520Huguet%2520and%2520Guy%2520Wolf%2520and%2520Ian%2520Adelstein%2520and%2520Tim%2520G.%2520J.%2520Rudner%2520and%2520Smita%2520Krishnaswamy%26entry.1292438233%3D%2520%2520Rapid%2520growth%2520of%2520high-dimensional%2520datasets%2520in%2520fields%2520such%2520as%2520single-cell%2520RNA%250Asequencing%2520and%2520spatial%2520genomics%2520has%2520led%2520to%2520unprecedented%2520opportunities%2520for%250Ascientific%2520discovery%252C%2520but%2520it%2520also%2520presents%2520unique%2520computational%2520and%2520statistical%250Achallenges.%2520Traditional%2520methods%2520struggle%2520with%2520geometry-aware%2520data%2520generation%252C%250Ainterpolation%2520along%2520meaningful%2520trajectories%252C%2520and%2520transporting%2520populations%2520via%250Afeasible%2520paths.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Geometry-Aware%2520Generative%250AAutoencoder%2520%2528GAGA%2529%252C%2520a%2520novel%2520framework%2520that%2520combines%2520extensible%2520manifold%250Alearning%2520with%2520generative%2520modeling.%2520GAGA%2520constructs%2520a%2520neural%2520network%2520embedding%250Aspace%2520that%2520respects%2520the%2520intrinsic%2520geometries%2520discovered%2520by%2520manifold%2520learning%250Aand%2520learns%2520a%2520novel%2520warped%2520Riemannian%2520metric%2520on%2520the%2520data%2520space.%2520This%2520warped%250Ametric%2520is%2520derived%2520from%2520both%2520the%2520points%2520on%2520the%2520data%2520manifold%2520and%2520negative%250Asamples%2520off%2520the%2520manifold%252C%2520allowing%2520it%2520to%2520characterize%2520a%2520meaningful%2520geometry%250Aacross%2520the%2520entire%2520latent%2520space.%2520Using%2520this%2520metric%252C%2520GAGA%2520can%2520uniformly%2520sample%250Apoints%2520on%2520the%2520manifold%252C%2520generate%2520points%2520along%2520geodesics%252C%2520and%2520interpolate%250Abetween%2520populations%2520across%2520the%2520learned%2520manifold.%2520GAGA%2520shows%2520competitive%250Aperformance%2520in%2520simulated%2520and%2520real%2520world%2520datasets%252C%2520including%2520a%252030%2525%2520improvement%250Aover%2520the%2520state-of-the-art%2520methods%2520in%2520single-cell%2520population-level%2520trajectory%250Ainference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Generative%20Autoencoders%20for%20Warped%20Riemannian%20Metric%0A%20%20Learning%20and%20Generative%20Modeling%20on%20Data%20Manifolds&entry.906535625=Xingzhi%20Sun%20and%20Danqi%20Liao%20and%20Kincaid%20MacDonald%20and%20Yanlei%20Zhang%20and%20Chen%20Liu%20and%20Guillaume%20Huguet%20and%20Guy%20Wolf%20and%20Ian%20Adelstein%20and%20Tim%20G.%20J.%20Rudner%20and%20Smita%20Krishnaswamy&entry.1292438233=%20%20Rapid%20growth%20of%20high-dimensional%20datasets%20in%20fields%20such%20as%20single-cell%20RNA%0Asequencing%20and%20spatial%20genomics%20has%20led%20to%20unprecedented%20opportunities%20for%0Ascientific%20discovery%2C%20but%20it%20also%20presents%20unique%20computational%20and%20statistical%0Achallenges.%20Traditional%20methods%20struggle%20with%20geometry-aware%20data%20generation%2C%0Ainterpolation%20along%20meaningful%20trajectories%2C%20and%20transporting%20populations%20via%0Afeasible%20paths.%20To%20address%20these%20issues%2C%20we%20introduce%20Geometry-Aware%20Generative%0AAutoencoder%20%28GAGA%29%2C%20a%20novel%20framework%20that%20combines%20extensible%20manifold%0Alearning%20with%20generative%20modeling.%20GAGA%20constructs%20a%20neural%20network%20embedding%0Aspace%20that%20respects%20the%20intrinsic%20geometries%20discovered%20by%20manifold%20learning%0Aand%20learns%20a%20novel%20warped%20Riemannian%20metric%20on%20the%20data%20space.%20This%20warped%0Ametric%20is%20derived%20from%20both%20the%20points%20on%20the%20data%20manifold%20and%20negative%0Asamples%20off%20the%20manifold%2C%20allowing%20it%20to%20characterize%20a%20meaningful%20geometry%0Aacross%20the%20entire%20latent%20space.%20Using%20this%20metric%2C%20GAGA%20can%20uniformly%20sample%0Apoints%20on%20the%20manifold%2C%20generate%20points%20along%20geodesics%2C%20and%20interpolate%0Abetween%20populations%20across%20the%20learned%20manifold.%20GAGA%20shows%20competitive%0Aperformance%20in%20simulated%20and%20real%20world%20datasets%2C%20including%20a%2030%25%20improvement%0Aover%20the%20state-of-the-art%20methods%20in%20single-cell%20population-level%20trajectory%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12779v1&entry.124074799=Read"},
{"title": "Adaptive Prompt Learning with SAM for Few-shot Scanning Probe Microscope\n  Image Segmentation", "author": "Yao Shen and Ziwei Wei and Chunmeng Liu and Shuming Wei and Qi Zhao and Kaiyang Zeng and Guangyao Li", "abstract": "  The Segment Anything Model (SAM) has demonstrated strong performance in image\nsegmentation of natural scene images. However, its effectiveness diminishes\nmarkedly when applied to specific scientific domains, such as Scanning Probe\nMicroscope (SPM) images. This decline in accuracy can be attributed to the\ndistinct data distribution and limited availability of the data inherent in the\nscientific images. On the other hand, the acquisition of adequate SPM datasets\nis both time-intensive and laborious as well as skill-dependent. To address\nthese challenges, we propose an Adaptive Prompt Learning with SAM (APL-SAM)\nframework tailored for few-shot SPM image segmentation. Our approach\nincorporates two key innovations to enhance SAM: 1) An Adaptive Prompt Learning\nmodule leverages few-shot embeddings derived from limited support set to learn\nadaptively central representatives, serving as visual prompts. This innovation\neliminates the need for time-consuming online user interactions for providing\nprompts, such as exhaustively marking points and bounding boxes slice by slice;\n2) A multi-source, multi-level mask decoder specifically designed for few-shot\nSPM image segmentation is introduced, which can effectively capture the\ncorrespondence between the support and query images. To facilitate\ncomprehensive training and evaluation, we introduce a new dataset, SPM-Seg,\ncurated for SPM image segmentation. Extensive experiments on this dataset\nreveal that the proposed APL-SAM framework significantly outperforms the\noriginal SAM, achieving over a 30% improvement in terms of Dice Similarity\nCoefficient with only one-shot guidance. Moreover, APL-SAM surpasses\nstate-of-the-art few-shot segmentation methods and even fully supervised\napproaches in performance. Code and dataset used in this study will be made\navailable upon acceptance.\n", "link": "http://arxiv.org/abs/2410.12562v1", "date": "2024-10-16", "relevancy": 2.8138, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Prompt%20Learning%20with%20SAM%20for%20Few-shot%20Scanning%20Probe%20Microscope%0A%20%20Image%20Segmentation&body=Title%3A%20Adaptive%20Prompt%20Learning%20with%20SAM%20for%20Few-shot%20Scanning%20Probe%20Microscope%0A%20%20Image%20Segmentation%0AAuthor%3A%20Yao%20Shen%20and%20Ziwei%20Wei%20and%20Chunmeng%20Liu%20and%20Shuming%20Wei%20and%20Qi%20Zhao%20and%20Kaiyang%20Zeng%20and%20Guangyao%20Li%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20strong%20performance%20in%20image%0Asegmentation%20of%20natural%20scene%20images.%20However%2C%20its%20effectiveness%20diminishes%0Amarkedly%20when%20applied%20to%20specific%20scientific%20domains%2C%20such%20as%20Scanning%20Probe%0AMicroscope%20%28SPM%29%20images.%20This%20decline%20in%20accuracy%20can%20be%20attributed%20to%20the%0Adistinct%20data%20distribution%20and%20limited%20availability%20of%20the%20data%20inherent%20in%20the%0Ascientific%20images.%20On%20the%20other%20hand%2C%20the%20acquisition%20of%20adequate%20SPM%20datasets%0Ais%20both%20time-intensive%20and%20laborious%20as%20well%20as%20skill-dependent.%20To%20address%0Athese%20challenges%2C%20we%20propose%20an%20Adaptive%20Prompt%20Learning%20with%20SAM%20%28APL-SAM%29%0Aframework%20tailored%20for%20few-shot%20SPM%20image%20segmentation.%20Our%20approach%0Aincorporates%20two%20key%20innovations%20to%20enhance%20SAM%3A%201%29%20An%20Adaptive%20Prompt%20Learning%0Amodule%20leverages%20few-shot%20embeddings%20derived%20from%20limited%20support%20set%20to%20learn%0Aadaptively%20central%20representatives%2C%20serving%20as%20visual%20prompts.%20This%20innovation%0Aeliminates%20the%20need%20for%20time-consuming%20online%20user%20interactions%20for%20providing%0Aprompts%2C%20such%20as%20exhaustively%20marking%20points%20and%20bounding%20boxes%20slice%20by%20slice%3B%0A2%29%20A%20multi-source%2C%20multi-level%20mask%20decoder%20specifically%20designed%20for%20few-shot%0ASPM%20image%20segmentation%20is%20introduced%2C%20which%20can%20effectively%20capture%20the%0Acorrespondence%20between%20the%20support%20and%20query%20images.%20To%20facilitate%0Acomprehensive%20training%20and%20evaluation%2C%20we%20introduce%20a%20new%20dataset%2C%20SPM-Seg%2C%0Acurated%20for%20SPM%20image%20segmentation.%20Extensive%20experiments%20on%20this%20dataset%0Areveal%20that%20the%20proposed%20APL-SAM%20framework%20significantly%20outperforms%20the%0Aoriginal%20SAM%2C%20achieving%20over%20a%2030%25%20improvement%20in%20terms%20of%20Dice%20Similarity%0ACoefficient%20with%20only%20one-shot%20guidance.%20Moreover%2C%20APL-SAM%20surpasses%0Astate-of-the-art%20few-shot%20segmentation%20methods%20and%20even%20fully%20supervised%0Aapproaches%20in%20performance.%20Code%20and%20dataset%20used%20in%20this%20study%20will%20be%20made%0Aavailable%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Prompt%2520Learning%2520with%2520SAM%2520for%2520Few-shot%2520Scanning%2520Probe%2520Microscope%250A%2520%2520Image%2520Segmentation%26entry.906535625%3DYao%2520Shen%2520and%2520Ziwei%2520Wei%2520and%2520Chunmeng%2520Liu%2520and%2520Shuming%2520Wei%2520and%2520Qi%2520Zhao%2520and%2520Kaiyang%2520Zeng%2520and%2520Guangyao%2520Li%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520demonstrated%2520strong%2520performance%2520in%2520image%250Asegmentation%2520of%2520natural%2520scene%2520images.%2520However%252C%2520its%2520effectiveness%2520diminishes%250Amarkedly%2520when%2520applied%2520to%2520specific%2520scientific%2520domains%252C%2520such%2520as%2520Scanning%2520Probe%250AMicroscope%2520%2528SPM%2529%2520images.%2520This%2520decline%2520in%2520accuracy%2520can%2520be%2520attributed%2520to%2520the%250Adistinct%2520data%2520distribution%2520and%2520limited%2520availability%2520of%2520the%2520data%2520inherent%2520in%2520the%250Ascientific%2520images.%2520On%2520the%2520other%2520hand%252C%2520the%2520acquisition%2520of%2520adequate%2520SPM%2520datasets%250Ais%2520both%2520time-intensive%2520and%2520laborious%2520as%2520well%2520as%2520skill-dependent.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520an%2520Adaptive%2520Prompt%2520Learning%2520with%2520SAM%2520%2528APL-SAM%2529%250Aframework%2520tailored%2520for%2520few-shot%2520SPM%2520image%2520segmentation.%2520Our%2520approach%250Aincorporates%2520two%2520key%2520innovations%2520to%2520enhance%2520SAM%253A%25201%2529%2520An%2520Adaptive%2520Prompt%2520Learning%250Amodule%2520leverages%2520few-shot%2520embeddings%2520derived%2520from%2520limited%2520support%2520set%2520to%2520learn%250Aadaptively%2520central%2520representatives%252C%2520serving%2520as%2520visual%2520prompts.%2520This%2520innovation%250Aeliminates%2520the%2520need%2520for%2520time-consuming%2520online%2520user%2520interactions%2520for%2520providing%250Aprompts%252C%2520such%2520as%2520exhaustively%2520marking%2520points%2520and%2520bounding%2520boxes%2520slice%2520by%2520slice%253B%250A2%2529%2520A%2520multi-source%252C%2520multi-level%2520mask%2520decoder%2520specifically%2520designed%2520for%2520few-shot%250ASPM%2520image%2520segmentation%2520is%2520introduced%252C%2520which%2520can%2520effectively%2520capture%2520the%250Acorrespondence%2520between%2520the%2520support%2520and%2520query%2520images.%2520To%2520facilitate%250Acomprehensive%2520training%2520and%2520evaluation%252C%2520we%2520introduce%2520a%2520new%2520dataset%252C%2520SPM-Seg%252C%250Acurated%2520for%2520SPM%2520image%2520segmentation.%2520Extensive%2520experiments%2520on%2520this%2520dataset%250Areveal%2520that%2520the%2520proposed%2520APL-SAM%2520framework%2520significantly%2520outperforms%2520the%250Aoriginal%2520SAM%252C%2520achieving%2520over%2520a%252030%2525%2520improvement%2520in%2520terms%2520of%2520Dice%2520Similarity%250ACoefficient%2520with%2520only%2520one-shot%2520guidance.%2520Moreover%252C%2520APL-SAM%2520surpasses%250Astate-of-the-art%2520few-shot%2520segmentation%2520methods%2520and%2520even%2520fully%2520supervised%250Aapproaches%2520in%2520performance.%2520Code%2520and%2520dataset%2520used%2520in%2520this%2520study%2520will%2520be%2520made%250Aavailable%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Prompt%20Learning%20with%20SAM%20for%20Few-shot%20Scanning%20Probe%20Microscope%0A%20%20Image%20Segmentation&entry.906535625=Yao%20Shen%20and%20Ziwei%20Wei%20and%20Chunmeng%20Liu%20and%20Shuming%20Wei%20and%20Qi%20Zhao%20and%20Kaiyang%20Zeng%20and%20Guangyao%20Li&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20strong%20performance%20in%20image%0Asegmentation%20of%20natural%20scene%20images.%20However%2C%20its%20effectiveness%20diminishes%0Amarkedly%20when%20applied%20to%20specific%20scientific%20domains%2C%20such%20as%20Scanning%20Probe%0AMicroscope%20%28SPM%29%20images.%20This%20decline%20in%20accuracy%20can%20be%20attributed%20to%20the%0Adistinct%20data%20distribution%20and%20limited%20availability%20of%20the%20data%20inherent%20in%20the%0Ascientific%20images.%20On%20the%20other%20hand%2C%20the%20acquisition%20of%20adequate%20SPM%20datasets%0Ais%20both%20time-intensive%20and%20laborious%20as%20well%20as%20skill-dependent.%20To%20address%0Athese%20challenges%2C%20we%20propose%20an%20Adaptive%20Prompt%20Learning%20with%20SAM%20%28APL-SAM%29%0Aframework%20tailored%20for%20few-shot%20SPM%20image%20segmentation.%20Our%20approach%0Aincorporates%20two%20key%20innovations%20to%20enhance%20SAM%3A%201%29%20An%20Adaptive%20Prompt%20Learning%0Amodule%20leverages%20few-shot%20embeddings%20derived%20from%20limited%20support%20set%20to%20learn%0Aadaptively%20central%20representatives%2C%20serving%20as%20visual%20prompts.%20This%20innovation%0Aeliminates%20the%20need%20for%20time-consuming%20online%20user%20interactions%20for%20providing%0Aprompts%2C%20such%20as%20exhaustively%20marking%20points%20and%20bounding%20boxes%20slice%20by%20slice%3B%0A2%29%20A%20multi-source%2C%20multi-level%20mask%20decoder%20specifically%20designed%20for%20few-shot%0ASPM%20image%20segmentation%20is%20introduced%2C%20which%20can%20effectively%20capture%20the%0Acorrespondence%20between%20the%20support%20and%20query%20images.%20To%20facilitate%0Acomprehensive%20training%20and%20evaluation%2C%20we%20introduce%20a%20new%20dataset%2C%20SPM-Seg%2C%0Acurated%20for%20SPM%20image%20segmentation.%20Extensive%20experiments%20on%20this%20dataset%0Areveal%20that%20the%20proposed%20APL-SAM%20framework%20significantly%20outperforms%20the%0Aoriginal%20SAM%2C%20achieving%20over%20a%2030%25%20improvement%20in%20terms%20of%20Dice%20Similarity%0ACoefficient%20with%20only%20one-shot%20guidance.%20Moreover%2C%20APL-SAM%20surpasses%0Astate-of-the-art%20few-shot%20segmentation%20methods%20and%20even%20fully%20supervised%0Aapproaches%20in%20performance.%20Code%20and%20dataset%20used%20in%20this%20study%20will%20be%20made%0Aavailable%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12562v1&entry.124074799=Read"},
{"title": "Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene\n  Parameters Approximation", "author": "Jiajie Yang", "abstract": "  Recent advancements in Radiance Fields have significantly improved novel-view\nsynthesis. However, in many real-world applications, the more advanced\nchallenge lies in inverse rendering, which seeks to derive the physical\nproperties of a scene, including light, geometry, textures, and materials.\nMeshes, as a traditional representation adopted by many simulation pipeline,\nhowever, still show limited influence in radiance field for inverse rendering.\nThis paper introduces a novel framework called Triangle Patchlet (abbr.\nTriplet), a mesh-based representation, to comprehensively approximate these\nscene parameters. We begin by assembling Triplets with either randomly\ngenerated points or sparse points obtained from camera calibration where all\nfaces are treated as an independent element. Next, we simulate the physical\ninteraction of light and optimize the scene parameters using traditional\ngraphics rendering techniques like rasterization and ray tracing, accompanying\nwith density control and propagation. An iterative mesh extracting process is\nalso suggested, where we continue to optimize on geometry and materials with\ngraph-based operation. We also introduce several regulation terms to enable\nbetter generalization of materials property. Our framework could precisely\nestimate the light, materials and geometry with mesh without prior of light,\nmaterials and geometry in a unified framework. Experiments demonstrate that our\napproach can achieve state-of-the-art visual quality while reconstructing\nhigh-quality geometry and accurate material properties.\n", "link": "http://arxiv.org/abs/2410.12414v1", "date": "2024-10-16", "relevancy": 2.7929, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5666}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5666}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triplet%3A%20Triangle%20Patchlet%20for%20Mesh-Based%20Inverse%20Rendering%20and%20Scene%0A%20%20Parameters%20Approximation&body=Title%3A%20Triplet%3A%20Triangle%20Patchlet%20for%20Mesh-Based%20Inverse%20Rendering%20and%20Scene%0A%20%20Parameters%20Approximation%0AAuthor%3A%20Jiajie%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Radiance%20Fields%20have%20significantly%20improved%20novel-view%0Asynthesis.%20However%2C%20in%20many%20real-world%20applications%2C%20the%20more%20advanced%0Achallenge%20lies%20in%20inverse%20rendering%2C%20which%20seeks%20to%20derive%20the%20physical%0Aproperties%20of%20a%20scene%2C%20including%20light%2C%20geometry%2C%20textures%2C%20and%20materials.%0AMeshes%2C%20as%20a%20traditional%20representation%20adopted%20by%20many%20simulation%20pipeline%2C%0Ahowever%2C%20still%20show%20limited%20influence%20in%20radiance%20field%20for%20inverse%20rendering.%0AThis%20paper%20introduces%20a%20novel%20framework%20called%20Triangle%20Patchlet%20%28abbr.%0ATriplet%29%2C%20a%20mesh-based%20representation%2C%20to%20comprehensively%20approximate%20these%0Ascene%20parameters.%20We%20begin%20by%20assembling%20Triplets%20with%20either%20randomly%0Agenerated%20points%20or%20sparse%20points%20obtained%20from%20camera%20calibration%20where%20all%0Afaces%20are%20treated%20as%20an%20independent%20element.%20Next%2C%20we%20simulate%20the%20physical%0Ainteraction%20of%20light%20and%20optimize%20the%20scene%20parameters%20using%20traditional%0Agraphics%20rendering%20techniques%20like%20rasterization%20and%20ray%20tracing%2C%20accompanying%0Awith%20density%20control%20and%20propagation.%20An%20iterative%20mesh%20extracting%20process%20is%0Aalso%20suggested%2C%20where%20we%20continue%20to%20optimize%20on%20geometry%20and%20materials%20with%0Agraph-based%20operation.%20We%20also%20introduce%20several%20regulation%20terms%20to%20enable%0Abetter%20generalization%20of%20materials%20property.%20Our%20framework%20could%20precisely%0Aestimate%20the%20light%2C%20materials%20and%20geometry%20with%20mesh%20without%20prior%20of%20light%2C%0Amaterials%20and%20geometry%20in%20a%20unified%20framework.%20Experiments%20demonstrate%20that%20our%0Aapproach%20can%20achieve%20state-of-the-art%20visual%20quality%20while%20reconstructing%0Ahigh-quality%20geometry%20and%20accurate%20material%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriplet%253A%2520Triangle%2520Patchlet%2520for%2520Mesh-Based%2520Inverse%2520Rendering%2520and%2520Scene%250A%2520%2520Parameters%2520Approximation%26entry.906535625%3DJiajie%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Radiance%2520Fields%2520have%2520significantly%2520improved%2520novel-view%250Asynthesis.%2520However%252C%2520in%2520many%2520real-world%2520applications%252C%2520the%2520more%2520advanced%250Achallenge%2520lies%2520in%2520inverse%2520rendering%252C%2520which%2520seeks%2520to%2520derive%2520the%2520physical%250Aproperties%2520of%2520a%2520scene%252C%2520including%2520light%252C%2520geometry%252C%2520textures%252C%2520and%2520materials.%250AMeshes%252C%2520as%2520a%2520traditional%2520representation%2520adopted%2520by%2520many%2520simulation%2520pipeline%252C%250Ahowever%252C%2520still%2520show%2520limited%2520influence%2520in%2520radiance%2520field%2520for%2520inverse%2520rendering.%250AThis%2520paper%2520introduces%2520a%2520novel%2520framework%2520called%2520Triangle%2520Patchlet%2520%2528abbr.%250ATriplet%2529%252C%2520a%2520mesh-based%2520representation%252C%2520to%2520comprehensively%2520approximate%2520these%250Ascene%2520parameters.%2520We%2520begin%2520by%2520assembling%2520Triplets%2520with%2520either%2520randomly%250Agenerated%2520points%2520or%2520sparse%2520points%2520obtained%2520from%2520camera%2520calibration%2520where%2520all%250Afaces%2520are%2520treated%2520as%2520an%2520independent%2520element.%2520Next%252C%2520we%2520simulate%2520the%2520physical%250Ainteraction%2520of%2520light%2520and%2520optimize%2520the%2520scene%2520parameters%2520using%2520traditional%250Agraphics%2520rendering%2520techniques%2520like%2520rasterization%2520and%2520ray%2520tracing%252C%2520accompanying%250Awith%2520density%2520control%2520and%2520propagation.%2520An%2520iterative%2520mesh%2520extracting%2520process%2520is%250Aalso%2520suggested%252C%2520where%2520we%2520continue%2520to%2520optimize%2520on%2520geometry%2520and%2520materials%2520with%250Agraph-based%2520operation.%2520We%2520also%2520introduce%2520several%2520regulation%2520terms%2520to%2520enable%250Abetter%2520generalization%2520of%2520materials%2520property.%2520Our%2520framework%2520could%2520precisely%250Aestimate%2520the%2520light%252C%2520materials%2520and%2520geometry%2520with%2520mesh%2520without%2520prior%2520of%2520light%252C%250Amaterials%2520and%2520geometry%2520in%2520a%2520unified%2520framework.%2520Experiments%2520demonstrate%2520that%2520our%250Aapproach%2520can%2520achieve%2520state-of-the-art%2520visual%2520quality%2520while%2520reconstructing%250Ahigh-quality%2520geometry%2520and%2520accurate%2520material%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triplet%3A%20Triangle%20Patchlet%20for%20Mesh-Based%20Inverse%20Rendering%20and%20Scene%0A%20%20Parameters%20Approximation&entry.906535625=Jiajie%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20Radiance%20Fields%20have%20significantly%20improved%20novel-view%0Asynthesis.%20However%2C%20in%20many%20real-world%20applications%2C%20the%20more%20advanced%0Achallenge%20lies%20in%20inverse%20rendering%2C%20which%20seeks%20to%20derive%20the%20physical%0Aproperties%20of%20a%20scene%2C%20including%20light%2C%20geometry%2C%20textures%2C%20and%20materials.%0AMeshes%2C%20as%20a%20traditional%20representation%20adopted%20by%20many%20simulation%20pipeline%2C%0Ahowever%2C%20still%20show%20limited%20influence%20in%20radiance%20field%20for%20inverse%20rendering.%0AThis%20paper%20introduces%20a%20novel%20framework%20called%20Triangle%20Patchlet%20%28abbr.%0ATriplet%29%2C%20a%20mesh-based%20representation%2C%20to%20comprehensively%20approximate%20these%0Ascene%20parameters.%20We%20begin%20by%20assembling%20Triplets%20with%20either%20randomly%0Agenerated%20points%20or%20sparse%20points%20obtained%20from%20camera%20calibration%20where%20all%0Afaces%20are%20treated%20as%20an%20independent%20element.%20Next%2C%20we%20simulate%20the%20physical%0Ainteraction%20of%20light%20and%20optimize%20the%20scene%20parameters%20using%20traditional%0Agraphics%20rendering%20techniques%20like%20rasterization%20and%20ray%20tracing%2C%20accompanying%0Awith%20density%20control%20and%20propagation.%20An%20iterative%20mesh%20extracting%20process%20is%0Aalso%20suggested%2C%20where%20we%20continue%20to%20optimize%20on%20geometry%20and%20materials%20with%0Agraph-based%20operation.%20We%20also%20introduce%20several%20regulation%20terms%20to%20enable%0Abetter%20generalization%20of%20materials%20property.%20Our%20framework%20could%20precisely%0Aestimate%20the%20light%2C%20materials%20and%20geometry%20with%20mesh%20without%20prior%20of%20light%2C%0Amaterials%20and%20geometry%20in%20a%20unified%20framework.%20Experiments%20demonstrate%20that%20our%0Aapproach%20can%20achieve%20state-of-the-art%20visual%20quality%20while%20reconstructing%0Ahigh-quality%20geometry%20and%20accurate%20material%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12414v1&entry.124074799=Read"},
{"title": "VrdONE: One-stage Video Visual Relation Detection", "author": "Xinjie Jiang and Chenxi Zheng and Xuemiao Xu and Bangzhen Liu and Weiying Zheng and Huaidong Zhang and Shengfeng He", "abstract": "  Video Visual Relation Detection (VidVRD) focuses on understanding how\nentities interact over time and space in videos, a key step for gaining deeper\ninsights into video scenes beyond basic visual tasks. Traditional methods for\nVidVRD, challenged by its complexity, typically split the task into two parts:\none for identifying what relation categories are present and another for\ndetermining their temporal boundaries. This split overlooks the inherent\nconnection between these elements. Addressing the need to recognize entity\npairs' spatiotemporal interactions across a range of durations, we propose\nVrdONE, a streamlined yet efficacious one-stage model. VrdONE combines the\nfeatures of subjects and objects, turning predicate detection into 1D instance\nsegmentation on their combined representations. This setup allows for both\nrelation category identification and binary mask generation in one go,\neliminating the need for extra steps like proposal generation or\npost-processing. VrdONE facilitates the interaction of features across various\nframes, adeptly capturing both short-lived and enduring relations.\nAdditionally, we introduce the Subject-Object Synergy (SOS) module, enhancing\nhow subjects and objects perceive each other before combining. VrdONE achieves\nstate-of-the-art performances on the VidOR benchmark and ImageNet-VidVRD,\nshowcasing its superior capability in discerning relations across different\ntemporal scales. The code is available at https://github.com/lucaspk512/vrdone.\n", "link": "http://arxiv.org/abs/2408.09408v2", "date": "2024-10-16", "relevancy": 2.7868, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VrdONE%3A%20One-stage%20Video%20Visual%20Relation%20Detection&body=Title%3A%20VrdONE%3A%20One-stage%20Video%20Visual%20Relation%20Detection%0AAuthor%3A%20Xinjie%20Jiang%20and%20Chenxi%20Zheng%20and%20Xuemiao%20Xu%20and%20Bangzhen%20Liu%20and%20Weiying%20Zheng%20and%20Huaidong%20Zhang%20and%20Shengfeng%20He%0AAbstract%3A%20%20%20Video%20Visual%20Relation%20Detection%20%28VidVRD%29%20focuses%20on%20understanding%20how%0Aentities%20interact%20over%20time%20and%20space%20in%20videos%2C%20a%20key%20step%20for%20gaining%20deeper%0Ainsights%20into%20video%20scenes%20beyond%20basic%20visual%20tasks.%20Traditional%20methods%20for%0AVidVRD%2C%20challenged%20by%20its%20complexity%2C%20typically%20split%20the%20task%20into%20two%20parts%3A%0Aone%20for%20identifying%20what%20relation%20categories%20are%20present%20and%20another%20for%0Adetermining%20their%20temporal%20boundaries.%20This%20split%20overlooks%20the%20inherent%0Aconnection%20between%20these%20elements.%20Addressing%20the%20need%20to%20recognize%20entity%0Apairs%27%20spatiotemporal%20interactions%20across%20a%20range%20of%20durations%2C%20we%20propose%0AVrdONE%2C%20a%20streamlined%20yet%20efficacious%20one-stage%20model.%20VrdONE%20combines%20the%0Afeatures%20of%20subjects%20and%20objects%2C%20turning%20predicate%20detection%20into%201D%20instance%0Asegmentation%20on%20their%20combined%20representations.%20This%20setup%20allows%20for%20both%0Arelation%20category%20identification%20and%20binary%20mask%20generation%20in%20one%20go%2C%0Aeliminating%20the%20need%20for%20extra%20steps%20like%20proposal%20generation%20or%0Apost-processing.%20VrdONE%20facilitates%20the%20interaction%20of%20features%20across%20various%0Aframes%2C%20adeptly%20capturing%20both%20short-lived%20and%20enduring%20relations.%0AAdditionally%2C%20we%20introduce%20the%20Subject-Object%20Synergy%20%28SOS%29%20module%2C%20enhancing%0Ahow%20subjects%20and%20objects%20perceive%20each%20other%20before%20combining.%20VrdONE%20achieves%0Astate-of-the-art%20performances%20on%20the%20VidOR%20benchmark%20and%20ImageNet-VidVRD%2C%0Ashowcasing%20its%20superior%20capability%20in%20discerning%20relations%20across%20different%0Atemporal%20scales.%20The%20code%20is%20available%20at%20https%3A//github.com/lucaspk512/vrdone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVrdONE%253A%2520One-stage%2520Video%2520Visual%2520Relation%2520Detection%26entry.906535625%3DXinjie%2520Jiang%2520and%2520Chenxi%2520Zheng%2520and%2520Xuemiao%2520Xu%2520and%2520Bangzhen%2520Liu%2520and%2520Weiying%2520Zheng%2520and%2520Huaidong%2520Zhang%2520and%2520Shengfeng%2520He%26entry.1292438233%3D%2520%2520Video%2520Visual%2520Relation%2520Detection%2520%2528VidVRD%2529%2520focuses%2520on%2520understanding%2520how%250Aentities%2520interact%2520over%2520time%2520and%2520space%2520in%2520videos%252C%2520a%2520key%2520step%2520for%2520gaining%2520deeper%250Ainsights%2520into%2520video%2520scenes%2520beyond%2520basic%2520visual%2520tasks.%2520Traditional%2520methods%2520for%250AVidVRD%252C%2520challenged%2520by%2520its%2520complexity%252C%2520typically%2520split%2520the%2520task%2520into%2520two%2520parts%253A%250Aone%2520for%2520identifying%2520what%2520relation%2520categories%2520are%2520present%2520and%2520another%2520for%250Adetermining%2520their%2520temporal%2520boundaries.%2520This%2520split%2520overlooks%2520the%2520inherent%250Aconnection%2520between%2520these%2520elements.%2520Addressing%2520the%2520need%2520to%2520recognize%2520entity%250Apairs%2527%2520spatiotemporal%2520interactions%2520across%2520a%2520range%2520of%2520durations%252C%2520we%2520propose%250AVrdONE%252C%2520a%2520streamlined%2520yet%2520efficacious%2520one-stage%2520model.%2520VrdONE%2520combines%2520the%250Afeatures%2520of%2520subjects%2520and%2520objects%252C%2520turning%2520predicate%2520detection%2520into%25201D%2520instance%250Asegmentation%2520on%2520their%2520combined%2520representations.%2520This%2520setup%2520allows%2520for%2520both%250Arelation%2520category%2520identification%2520and%2520binary%2520mask%2520generation%2520in%2520one%2520go%252C%250Aeliminating%2520the%2520need%2520for%2520extra%2520steps%2520like%2520proposal%2520generation%2520or%250Apost-processing.%2520VrdONE%2520facilitates%2520the%2520interaction%2520of%2520features%2520across%2520various%250Aframes%252C%2520adeptly%2520capturing%2520both%2520short-lived%2520and%2520enduring%2520relations.%250AAdditionally%252C%2520we%2520introduce%2520the%2520Subject-Object%2520Synergy%2520%2528SOS%2529%2520module%252C%2520enhancing%250Ahow%2520subjects%2520and%2520objects%2520perceive%2520each%2520other%2520before%2520combining.%2520VrdONE%2520achieves%250Astate-of-the-art%2520performances%2520on%2520the%2520VidOR%2520benchmark%2520and%2520ImageNet-VidVRD%252C%250Ashowcasing%2520its%2520superior%2520capability%2520in%2520discerning%2520relations%2520across%2520different%250Atemporal%2520scales.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/lucaspk512/vrdone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VrdONE%3A%20One-stage%20Video%20Visual%20Relation%20Detection&entry.906535625=Xinjie%20Jiang%20and%20Chenxi%20Zheng%20and%20Xuemiao%20Xu%20and%20Bangzhen%20Liu%20and%20Weiying%20Zheng%20and%20Huaidong%20Zhang%20and%20Shengfeng%20He&entry.1292438233=%20%20Video%20Visual%20Relation%20Detection%20%28VidVRD%29%20focuses%20on%20understanding%20how%0Aentities%20interact%20over%20time%20and%20space%20in%20videos%2C%20a%20key%20step%20for%20gaining%20deeper%0Ainsights%20into%20video%20scenes%20beyond%20basic%20visual%20tasks.%20Traditional%20methods%20for%0AVidVRD%2C%20challenged%20by%20its%20complexity%2C%20typically%20split%20the%20task%20into%20two%20parts%3A%0Aone%20for%20identifying%20what%20relation%20categories%20are%20present%20and%20another%20for%0Adetermining%20their%20temporal%20boundaries.%20This%20split%20overlooks%20the%20inherent%0Aconnection%20between%20these%20elements.%20Addressing%20the%20need%20to%20recognize%20entity%0Apairs%27%20spatiotemporal%20interactions%20across%20a%20range%20of%20durations%2C%20we%20propose%0AVrdONE%2C%20a%20streamlined%20yet%20efficacious%20one-stage%20model.%20VrdONE%20combines%20the%0Afeatures%20of%20subjects%20and%20objects%2C%20turning%20predicate%20detection%20into%201D%20instance%0Asegmentation%20on%20their%20combined%20representations.%20This%20setup%20allows%20for%20both%0Arelation%20category%20identification%20and%20binary%20mask%20generation%20in%20one%20go%2C%0Aeliminating%20the%20need%20for%20extra%20steps%20like%20proposal%20generation%20or%0Apost-processing.%20VrdONE%20facilitates%20the%20interaction%20of%20features%20across%20various%0Aframes%2C%20adeptly%20capturing%20both%20short-lived%20and%20enduring%20relations.%0AAdditionally%2C%20we%20introduce%20the%20Subject-Object%20Synergy%20%28SOS%29%20module%2C%20enhancing%0Ahow%20subjects%20and%20objects%20perceive%20each%20other%20before%20combining.%20VrdONE%20achieves%0Astate-of-the-art%20performances%20on%20the%20VidOR%20benchmark%20and%20ImageNet-VidVRD%2C%0Ashowcasing%20its%20superior%20capability%20in%20discerning%20relations%20across%20different%0Atemporal%20scales.%20The%20code%20is%20available%20at%20https%3A//github.com/lucaspk512/vrdone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09408v2&entry.124074799=Read"},
{"title": "Interpret Your Decision: Logical Reasoning Regularization for\n  Generalization in Visual Classification", "author": "Zhaorui Tan and Xi Yang and Qiufeng Wang and Anh Nguyen and Kaizhu Huang", "abstract": "  Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.\n", "link": "http://arxiv.org/abs/2410.04492v3", "date": "2024-10-16", "relevancy": 2.7781, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification&body=Title%3A%20Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification%0AAuthor%3A%20Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Kaizhu%20Huang%0AAbstract%3A%20%20%20Vision%20models%20excel%20in%20image%20classification%20but%20struggle%20to%20generalize%20to%0Aunseen%20data%2C%20such%20as%20classifying%20images%20from%20unseen%20domains%20or%20discovering%0Anovel%20categories.%20In%20this%20paper%2C%20we%20explore%20the%20relationship%20between%20logical%0Areasoning%20and%20deep%20learning%20generalization%20in%20visual%20classification.%20A%20logical%0Aregularization%20termed%20L-Reg%20is%20derived%20which%20bridges%20a%20logical%20analysis%0Aframework%20to%20image%20classification.%20Our%20work%20reveals%20that%20L-Reg%20reduces%20the%0Acomplexity%20of%20the%20model%20in%20terms%20of%20the%20feature%20distribution%20and%20classifier%0Aweights.%20Specifically%2C%20we%20unveil%20the%20interpretability%20brought%20by%20L-Reg%2C%20as%20it%0Aenables%20the%20model%20to%20extract%20the%20salient%20features%2C%20such%20as%20faces%20to%20persons%2C%0Afor%20classification.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20L-Reg%0Aenhances%20generalization%20across%20various%20scenarios%2C%20including%20multi-domain%0Ageneralization%20and%20generalized%20category%20discovery.%20In%20complex%20real-world%0Ascenarios%20where%20images%20span%20unknown%20classes%20and%20unseen%20domains%2C%20L-Reg%0Aconsistently%20improves%20generalization%2C%20highlighting%20its%20practical%20efficacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04492v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpret%2520Your%2520Decision%253A%2520Logical%2520Reasoning%2520Regularization%2520for%250A%2520%2520Generalization%2520in%2520Visual%2520Classification%26entry.906535625%3DZhaorui%2520Tan%2520and%2520Xi%2520Yang%2520and%2520Qiufeng%2520Wang%2520and%2520Anh%2520Nguyen%2520and%2520Kaizhu%2520Huang%26entry.1292438233%3D%2520%2520Vision%2520models%2520excel%2520in%2520image%2520classification%2520but%2520struggle%2520to%2520generalize%2520to%250Aunseen%2520data%252C%2520such%2520as%2520classifying%2520images%2520from%2520unseen%2520domains%2520or%2520discovering%250Anovel%2520categories.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520relationship%2520between%2520logical%250Areasoning%2520and%2520deep%2520learning%2520generalization%2520in%2520visual%2520classification.%2520A%2520logical%250Aregularization%2520termed%2520L-Reg%2520is%2520derived%2520which%2520bridges%2520a%2520logical%2520analysis%250Aframework%2520to%2520image%2520classification.%2520Our%2520work%2520reveals%2520that%2520L-Reg%2520reduces%2520the%250Acomplexity%2520of%2520the%2520model%2520in%2520terms%2520of%2520the%2520feature%2520distribution%2520and%2520classifier%250Aweights.%2520Specifically%252C%2520we%2520unveil%2520the%2520interpretability%2520brought%2520by%2520L-Reg%252C%2520as%2520it%250Aenables%2520the%2520model%2520to%2520extract%2520the%2520salient%2520features%252C%2520such%2520as%2520faces%2520to%2520persons%252C%250Afor%2520classification.%2520Theoretical%2520analysis%2520and%2520experiments%2520demonstrate%2520that%2520L-Reg%250Aenhances%2520generalization%2520across%2520various%2520scenarios%252C%2520including%2520multi-domain%250Ageneralization%2520and%2520generalized%2520category%2520discovery.%2520In%2520complex%2520real-world%250Ascenarios%2520where%2520images%2520span%2520unknown%2520classes%2520and%2520unseen%2520domains%252C%2520L-Reg%250Aconsistently%2520improves%2520generalization%252C%2520highlighting%2520its%2520practical%2520efficacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04492v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpret%20Your%20Decision%3A%20Logical%20Reasoning%20Regularization%20for%0A%20%20Generalization%20in%20Visual%20Classification&entry.906535625=Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Kaizhu%20Huang&entry.1292438233=%20%20Vision%20models%20excel%20in%20image%20classification%20but%20struggle%20to%20generalize%20to%0Aunseen%20data%2C%20such%20as%20classifying%20images%20from%20unseen%20domains%20or%20discovering%0Anovel%20categories.%20In%20this%20paper%2C%20we%20explore%20the%20relationship%20between%20logical%0Areasoning%20and%20deep%20learning%20generalization%20in%20visual%20classification.%20A%20logical%0Aregularization%20termed%20L-Reg%20is%20derived%20which%20bridges%20a%20logical%20analysis%0Aframework%20to%20image%20classification.%20Our%20work%20reveals%20that%20L-Reg%20reduces%20the%0Acomplexity%20of%20the%20model%20in%20terms%20of%20the%20feature%20distribution%20and%20classifier%0Aweights.%20Specifically%2C%20we%20unveil%20the%20interpretability%20brought%20by%20L-Reg%2C%20as%20it%0Aenables%20the%20model%20to%20extract%20the%20salient%20features%2C%20such%20as%20faces%20to%20persons%2C%0Afor%20classification.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20L-Reg%0Aenhances%20generalization%20across%20various%20scenarios%2C%20including%20multi-domain%0Ageneralization%20and%20generalized%20category%20discovery.%20In%20complex%20real-world%0Ascenarios%20where%20images%20span%20unknown%20classes%20and%20unseen%20domains%2C%20L-Reg%0Aconsistently%20improves%20generalization%2C%20highlighting%20its%20practical%20efficacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04492v3&entry.124074799=Read"},
{"title": "Dynamic Tuning Towards Parameter and Inference Efficiency for ViT\n  Adaptation", "author": "Wangbo Zhao and Jiasheng Tang and Yizeng Han and Yibing Song and Kai Wang and Gao Huang and Fan Wang and Yang You", "abstract": "  Existing parameter-efficient fine-tuning (PEFT) methods have achieved\nsignificant success on vision transformers (ViTs) adaptation by improving\nparameter efficiency. However, the exploration of enhancing inference\nefficiency during adaptation remains underexplored. This limits the broader\napplication of pre-trained ViT models, especially when the model is\ncomputationally extensive. In this paper, we propose Dynamic Tuning (DyT), a\nnovel approach to improve both parameter and inference efficiency for ViT\nadaptation. Specifically, besides using the lightweight adapter modules, we\npropose a token dispatcher to distinguish informative tokens from less\nimportant ones, allowing the latter to dynamically skip the original block,\nthereby reducing the redundant computation during inference. Additionally, we\nexplore multiple design variants to find the best practice of DyT. Finally,\ninspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced\nadapter to further boost the adaptation performance. We validate DyT across\nvarious tasks, including image/video recognition and semantic segmentation. For\ninstance, DyT achieves superior performance compared to existing PEFT methods\nwhile evoking only 71% of their FLOPs on the VTAB-1K benchmark.\n", "link": "http://arxiv.org/abs/2403.11808v2", "date": "2024-10-16", "relevancy": 2.7356, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5584}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5426}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Tuning%20Towards%20Parameter%20and%20Inference%20Efficiency%20for%20ViT%0A%20%20Adaptation&body=Title%3A%20Dynamic%20Tuning%20Towards%20Parameter%20and%20Inference%20Efficiency%20for%20ViT%0A%20%20Adaptation%0AAuthor%3A%20Wangbo%20Zhao%20and%20Jiasheng%20Tang%20and%20Yizeng%20Han%20and%20Yibing%20Song%20and%20Kai%20Wang%20and%20Gao%20Huang%20and%20Fan%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20Existing%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20achieved%0Asignificant%20success%20on%20vision%20transformers%20%28ViTs%29%20adaptation%20by%20improving%0Aparameter%20efficiency.%20However%2C%20the%20exploration%20of%20enhancing%20inference%0Aefficiency%20during%20adaptation%20remains%20underexplored.%20This%20limits%20the%20broader%0Aapplication%20of%20pre-trained%20ViT%20models%2C%20especially%20when%20the%20model%20is%0Acomputationally%20extensive.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Tuning%20%28DyT%29%2C%20a%0Anovel%20approach%20to%20improve%20both%20parameter%20and%20inference%20efficiency%20for%20ViT%0Aadaptation.%20Specifically%2C%20besides%20using%20the%20lightweight%20adapter%20modules%2C%20we%0Apropose%20a%20token%20dispatcher%20to%20distinguish%20informative%20tokens%20from%20less%0Aimportant%20ones%2C%20allowing%20the%20latter%20to%20dynamically%20skip%20the%20original%20block%2C%0Athereby%20reducing%20the%20redundant%20computation%20during%20inference.%20Additionally%2C%20we%0Aexplore%20multiple%20design%20variants%20to%20find%20the%20best%20practice%20of%20DyT.%20Finally%2C%0Ainspired%20by%20the%20mixture-of-experts%20%28MoE%29%20mechanism%2C%20we%20introduce%20an%20enhanced%0Aadapter%20to%20further%20boost%20the%20adaptation%20performance.%20We%20validate%20DyT%20across%0Avarious%20tasks%2C%20including%20image/video%20recognition%20and%20semantic%20segmentation.%20For%0Ainstance%2C%20DyT%20achieves%20superior%20performance%20compared%20to%20existing%20PEFT%20methods%0Awhile%20evoking%20only%2071%25%20of%20their%20FLOPs%20on%20the%20VTAB-1K%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Tuning%2520Towards%2520Parameter%2520and%2520Inference%2520Efficiency%2520for%2520ViT%250A%2520%2520Adaptation%26entry.906535625%3DWangbo%2520Zhao%2520and%2520Jiasheng%2520Tang%2520and%2520Yizeng%2520Han%2520and%2520Yibing%2520Song%2520and%2520Kai%2520Wang%2520and%2520Gao%2520Huang%2520and%2520Fan%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520Existing%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520have%2520achieved%250Asignificant%2520success%2520on%2520vision%2520transformers%2520%2528ViTs%2529%2520adaptation%2520by%2520improving%250Aparameter%2520efficiency.%2520However%252C%2520the%2520exploration%2520of%2520enhancing%2520inference%250Aefficiency%2520during%2520adaptation%2520remains%2520underexplored.%2520This%2520limits%2520the%2520broader%250Aapplication%2520of%2520pre-trained%2520ViT%2520models%252C%2520especially%2520when%2520the%2520model%2520is%250Acomputationally%2520extensive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dynamic%2520Tuning%2520%2528DyT%2529%252C%2520a%250Anovel%2520approach%2520to%2520improve%2520both%2520parameter%2520and%2520inference%2520efficiency%2520for%2520ViT%250Aadaptation.%2520Specifically%252C%2520besides%2520using%2520the%2520lightweight%2520adapter%2520modules%252C%2520we%250Apropose%2520a%2520token%2520dispatcher%2520to%2520distinguish%2520informative%2520tokens%2520from%2520less%250Aimportant%2520ones%252C%2520allowing%2520the%2520latter%2520to%2520dynamically%2520skip%2520the%2520original%2520block%252C%250Athereby%2520reducing%2520the%2520redundant%2520computation%2520during%2520inference.%2520Additionally%252C%2520we%250Aexplore%2520multiple%2520design%2520variants%2520to%2520find%2520the%2520best%2520practice%2520of%2520DyT.%2520Finally%252C%250Ainspired%2520by%2520the%2520mixture-of-experts%2520%2528MoE%2529%2520mechanism%252C%2520we%2520introduce%2520an%2520enhanced%250Aadapter%2520to%2520further%2520boost%2520the%2520adaptation%2520performance.%2520We%2520validate%2520DyT%2520across%250Avarious%2520tasks%252C%2520including%2520image/video%2520recognition%2520and%2520semantic%2520segmentation.%2520For%250Ainstance%252C%2520DyT%2520achieves%2520superior%2520performance%2520compared%2520to%2520existing%2520PEFT%2520methods%250Awhile%2520evoking%2520only%252071%2525%2520of%2520their%2520FLOPs%2520on%2520the%2520VTAB-1K%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Tuning%20Towards%20Parameter%20and%20Inference%20Efficiency%20for%20ViT%0A%20%20Adaptation&entry.906535625=Wangbo%20Zhao%20and%20Jiasheng%20Tang%20and%20Yizeng%20Han%20and%20Yibing%20Song%20and%20Kai%20Wang%20and%20Gao%20Huang%20and%20Fan%20Wang%20and%20Yang%20You&entry.1292438233=%20%20Existing%20parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20have%20achieved%0Asignificant%20success%20on%20vision%20transformers%20%28ViTs%29%20adaptation%20by%20improving%0Aparameter%20efficiency.%20However%2C%20the%20exploration%20of%20enhancing%20inference%0Aefficiency%20during%20adaptation%20remains%20underexplored.%20This%20limits%20the%20broader%0Aapplication%20of%20pre-trained%20ViT%20models%2C%20especially%20when%20the%20model%20is%0Acomputationally%20extensive.%20In%20this%20paper%2C%20we%20propose%20Dynamic%20Tuning%20%28DyT%29%2C%20a%0Anovel%20approach%20to%20improve%20both%20parameter%20and%20inference%20efficiency%20for%20ViT%0Aadaptation.%20Specifically%2C%20besides%20using%20the%20lightweight%20adapter%20modules%2C%20we%0Apropose%20a%20token%20dispatcher%20to%20distinguish%20informative%20tokens%20from%20less%0Aimportant%20ones%2C%20allowing%20the%20latter%20to%20dynamically%20skip%20the%20original%20block%2C%0Athereby%20reducing%20the%20redundant%20computation%20during%20inference.%20Additionally%2C%20we%0Aexplore%20multiple%20design%20variants%20to%20find%20the%20best%20practice%20of%20DyT.%20Finally%2C%0Ainspired%20by%20the%20mixture-of-experts%20%28MoE%29%20mechanism%2C%20we%20introduce%20an%20enhanced%0Aadapter%20to%20further%20boost%20the%20adaptation%20performance.%20We%20validate%20DyT%20across%0Avarious%20tasks%2C%20including%20image/video%20recognition%20and%20semantic%20segmentation.%20For%0Ainstance%2C%20DyT%20achieves%20superior%20performance%20compared%20to%20existing%20PEFT%20methods%0Awhile%20evoking%20only%2071%25%20of%20their%20FLOPs%20on%20the%20VTAB-1K%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11808v2&entry.124074799=Read"},
{"title": "Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP", "author": "Eunji Kim and Kyuhong Shim and Simyung Chang and Sungroh Yoon", "abstract": "  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n", "link": "http://arxiv.org/abs/2410.08469v2", "date": "2024-10-16", "relevancy": 2.711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Token%20Reweighting%20for%20Interpretable%20and%20Controllable%20Text%0A%20%20Embeddings%20in%20CLIP&body=Title%3A%20Semantic%20Token%20Reweighting%20for%20Interpretable%20and%20Controllable%20Text%0A%20%20Embeddings%20in%20CLIP%0AAuthor%3A%20Eunji%20Kim%20and%20Kyuhong%20Shim%20and%20Simyung%20Chang%20and%20Sungroh%20Yoon%0AAbstract%3A%20%20%20A%20text%20encoder%20within%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20plays%20a%20crucial%0Arole%20in%20translating%20textual%20input%20into%20an%20embedding%20space%20shared%20with%20images%2C%0Athereby%20facilitating%20the%20interpretative%20analysis%20of%20vision%20tasks%20through%0Anatural%20language.%20Despite%20the%20varying%20significance%20of%20different%20textual%0Aelements%20within%20a%20sentence%20depending%20on%20the%20context%2C%20efforts%20to%20account%20for%0Avariation%20of%20importance%20in%20constructing%20text%20embeddings%20have%20been%20lacking.%20We%0Apropose%20a%20framework%20of%20Semantic%20Token%20Reweighting%20to%20build%20Interpretable%20text%0Aembeddings%20%28SToRI%29%2C%20which%20incorporates%20controllability%20as%20well.%20SToRI%20refines%0Athe%20text%20encoding%20process%20in%20CLIP%20by%20differentially%20weighting%20semantic%20elements%0Abased%20on%20contextual%20importance%2C%20enabling%20finer%20control%20over%20emphasis%20responsive%0Ato%20data-driven%20insights%20and%20user%20preferences.%20The%20efficacy%20of%20SToRI%20is%0Ademonstrated%20through%20comprehensive%20experiments%20on%20few-shot%20image%20classification%0Aand%20image%20retrieval%20tailored%20to%20user%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Token%2520Reweighting%2520for%2520Interpretable%2520and%2520Controllable%2520Text%250A%2520%2520Embeddings%2520in%2520CLIP%26entry.906535625%3DEunji%2520Kim%2520and%2520Kyuhong%2520Shim%2520and%2520Simyung%2520Chang%2520and%2520Sungroh%2520Yoon%26entry.1292438233%3D%2520%2520A%2520text%2520encoder%2520within%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520plays%2520a%2520crucial%250Arole%2520in%2520translating%2520textual%2520input%2520into%2520an%2520embedding%2520space%2520shared%2520with%2520images%252C%250Athereby%2520facilitating%2520the%2520interpretative%2520analysis%2520of%2520vision%2520tasks%2520through%250Anatural%2520language.%2520Despite%2520the%2520varying%2520significance%2520of%2520different%2520textual%250Aelements%2520within%2520a%2520sentence%2520depending%2520on%2520the%2520context%252C%2520efforts%2520to%2520account%2520for%250Avariation%2520of%2520importance%2520in%2520constructing%2520text%2520embeddings%2520have%2520been%2520lacking.%2520We%250Apropose%2520a%2520framework%2520of%2520Semantic%2520Token%2520Reweighting%2520to%2520build%2520Interpretable%2520text%250Aembeddings%2520%2528SToRI%2529%252C%2520which%2520incorporates%2520controllability%2520as%2520well.%2520SToRI%2520refines%250Athe%2520text%2520encoding%2520process%2520in%2520CLIP%2520by%2520differentially%2520weighting%2520semantic%2520elements%250Abased%2520on%2520contextual%2520importance%252C%2520enabling%2520finer%2520control%2520over%2520emphasis%2520responsive%250Ato%2520data-driven%2520insights%2520and%2520user%2520preferences.%2520The%2520efficacy%2520of%2520SToRI%2520is%250Ademonstrated%2520through%2520comprehensive%2520experiments%2520on%2520few-shot%2520image%2520classification%250Aand%2520image%2520retrieval%2520tailored%2520to%2520user%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Token%20Reweighting%20for%20Interpretable%20and%20Controllable%20Text%0A%20%20Embeddings%20in%20CLIP&entry.906535625=Eunji%20Kim%20and%20Kyuhong%20Shim%20and%20Simyung%20Chang%20and%20Sungroh%20Yoon&entry.1292438233=%20%20A%20text%20encoder%20within%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20plays%20a%20crucial%0Arole%20in%20translating%20textual%20input%20into%20an%20embedding%20space%20shared%20with%20images%2C%0Athereby%20facilitating%20the%20interpretative%20analysis%20of%20vision%20tasks%20through%0Anatural%20language.%20Despite%20the%20varying%20significance%20of%20different%20textual%0Aelements%20within%20a%20sentence%20depending%20on%20the%20context%2C%20efforts%20to%20account%20for%0Avariation%20of%20importance%20in%20constructing%20text%20embeddings%20have%20been%20lacking.%20We%0Apropose%20a%20framework%20of%20Semantic%20Token%20Reweighting%20to%20build%20Interpretable%20text%0Aembeddings%20%28SToRI%29%2C%20which%20incorporates%20controllability%20as%20well.%20SToRI%20refines%0Athe%20text%20encoding%20process%20in%20CLIP%20by%20differentially%20weighting%20semantic%20elements%0Abased%20on%20contextual%20importance%2C%20enabling%20finer%20control%20over%20emphasis%20responsive%0Ato%20data-driven%20insights%20and%20user%20preferences.%20The%20efficacy%20of%20SToRI%20is%0Ademonstrated%20through%20comprehensive%20experiments%20on%20few-shot%20image%20classification%0Aand%20image%20retrieval%20tailored%20to%20user%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08469v2&entry.124074799=Read"},
{"title": "Rethinking Visual Counterfactual Explanations Through Region Constraint", "author": "Bartlomiej Sobieski and Jakub Grzywaczewski and Bartlomiej Sadlej and Matthew Tivnan and Przemyslaw Biecek", "abstract": "  Visual counterfactual explanations (VCEs) have recently gained immense\npopularity as a tool for clarifying the decision-making process of image\nclassifiers. This trend is largely motivated by what these explanations promise\nto deliver -- indicate semantically meaningful factors that change the\nclassifier's decision. However, we argue that current state-of-the-art\napproaches lack a crucial component -- the region constraint -- whose absence\nprevents from drawing explicit conclusions, and may even lead to faulty\nreasoning due to phenomenons like confirmation bias. To address the issue of\nprevious methods, which modify images in a very entangled and widely dispersed\nmanner, we propose region-constrained VCEs (RVCEs), which assume that only a\npredefined image region can be modified to influence the model's prediction. To\neffectively sample from this subclass of VCEs, we propose Region-Constrained\nCounterfactual Schr\\\"odinger Bridges (RCSB), an adaptation of a tractable\nsubclass of Schr\\\"odinger Bridges to the problem of conditional inpainting,\nwhere the conditioning signal originates from the classifier of interest. In\naddition to setting a new state-of-the-art by a large margin, we extend RCSB to\nallow for exact counterfactual reasoning, where the predefined region contains\nonly the factor of interest, and incorporating the user to actively interact\nwith the RVCE by predefining the regions manually.\n", "link": "http://arxiv.org/abs/2410.12591v1", "date": "2024-10-16", "relevancy": 2.7073, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Visual%20Counterfactual%20Explanations%20Through%20Region%20Constraint&body=Title%3A%20Rethinking%20Visual%20Counterfactual%20Explanations%20Through%20Region%20Constraint%0AAuthor%3A%20Bartlomiej%20Sobieski%20and%20Jakub%20Grzywaczewski%20and%20Bartlomiej%20Sadlej%20and%20Matthew%20Tivnan%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20Visual%20counterfactual%20explanations%20%28VCEs%29%20have%20recently%20gained%20immense%0Apopularity%20as%20a%20tool%20for%20clarifying%20the%20decision-making%20process%20of%20image%0Aclassifiers.%20This%20trend%20is%20largely%20motivated%20by%20what%20these%20explanations%20promise%0Ato%20deliver%20--%20indicate%20semantically%20meaningful%20factors%20that%20change%20the%0Aclassifier%27s%20decision.%20However%2C%20we%20argue%20that%20current%20state-of-the-art%0Aapproaches%20lack%20a%20crucial%20component%20--%20the%20region%20constraint%20--%20whose%20absence%0Aprevents%20from%20drawing%20explicit%20conclusions%2C%20and%20may%20even%20lead%20to%20faulty%0Areasoning%20due%20to%20phenomenons%20like%20confirmation%20bias.%20To%20address%20the%20issue%20of%0Aprevious%20methods%2C%20which%20modify%20images%20in%20a%20very%20entangled%20and%20widely%20dispersed%0Amanner%2C%20we%20propose%20region-constrained%20VCEs%20%28RVCEs%29%2C%20which%20assume%20that%20only%20a%0Apredefined%20image%20region%20can%20be%20modified%20to%20influence%20the%20model%27s%20prediction.%20To%0Aeffectively%20sample%20from%20this%20subclass%20of%20VCEs%2C%20we%20propose%20Region-Constrained%0ACounterfactual%20Schr%5C%22odinger%20Bridges%20%28RCSB%29%2C%20an%20adaptation%20of%20a%20tractable%0Asubclass%20of%20Schr%5C%22odinger%20Bridges%20to%20the%20problem%20of%20conditional%20inpainting%2C%0Awhere%20the%20conditioning%20signal%20originates%20from%20the%20classifier%20of%20interest.%20In%0Aaddition%20to%20setting%20a%20new%20state-of-the-art%20by%20a%20large%20margin%2C%20we%20extend%20RCSB%20to%0Aallow%20for%20exact%20counterfactual%20reasoning%2C%20where%20the%20predefined%20region%20contains%0Aonly%20the%20factor%20of%20interest%2C%20and%20incorporating%20the%20user%20to%20actively%20interact%0Awith%20the%20RVCE%20by%20predefining%20the%20regions%20manually.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Visual%2520Counterfactual%2520Explanations%2520Through%2520Region%2520Constraint%26entry.906535625%3DBartlomiej%2520Sobieski%2520and%2520Jakub%2520Grzywaczewski%2520and%2520Bartlomiej%2520Sadlej%2520and%2520Matthew%2520Tivnan%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3D%2520%2520Visual%2520counterfactual%2520explanations%2520%2528VCEs%2529%2520have%2520recently%2520gained%2520immense%250Apopularity%2520as%2520a%2520tool%2520for%2520clarifying%2520the%2520decision-making%2520process%2520of%2520image%250Aclassifiers.%2520This%2520trend%2520is%2520largely%2520motivated%2520by%2520what%2520these%2520explanations%2520promise%250Ato%2520deliver%2520--%2520indicate%2520semantically%2520meaningful%2520factors%2520that%2520change%2520the%250Aclassifier%2527s%2520decision.%2520However%252C%2520we%2520argue%2520that%2520current%2520state-of-the-art%250Aapproaches%2520lack%2520a%2520crucial%2520component%2520--%2520the%2520region%2520constraint%2520--%2520whose%2520absence%250Aprevents%2520from%2520drawing%2520explicit%2520conclusions%252C%2520and%2520may%2520even%2520lead%2520to%2520faulty%250Areasoning%2520due%2520to%2520phenomenons%2520like%2520confirmation%2520bias.%2520To%2520address%2520the%2520issue%2520of%250Aprevious%2520methods%252C%2520which%2520modify%2520images%2520in%2520a%2520very%2520entangled%2520and%2520widely%2520dispersed%250Amanner%252C%2520we%2520propose%2520region-constrained%2520VCEs%2520%2528RVCEs%2529%252C%2520which%2520assume%2520that%2520only%2520a%250Apredefined%2520image%2520region%2520can%2520be%2520modified%2520to%2520influence%2520the%2520model%2527s%2520prediction.%2520To%250Aeffectively%2520sample%2520from%2520this%2520subclass%2520of%2520VCEs%252C%2520we%2520propose%2520Region-Constrained%250ACounterfactual%2520Schr%255C%2522odinger%2520Bridges%2520%2528RCSB%2529%252C%2520an%2520adaptation%2520of%2520a%2520tractable%250Asubclass%2520of%2520Schr%255C%2522odinger%2520Bridges%2520to%2520the%2520problem%2520of%2520conditional%2520inpainting%252C%250Awhere%2520the%2520conditioning%2520signal%2520originates%2520from%2520the%2520classifier%2520of%2520interest.%2520In%250Aaddition%2520to%2520setting%2520a%2520new%2520state-of-the-art%2520by%2520a%2520large%2520margin%252C%2520we%2520extend%2520RCSB%2520to%250Aallow%2520for%2520exact%2520counterfactual%2520reasoning%252C%2520where%2520the%2520predefined%2520region%2520contains%250Aonly%2520the%2520factor%2520of%2520interest%252C%2520and%2520incorporating%2520the%2520user%2520to%2520actively%2520interact%250Awith%2520the%2520RVCE%2520by%2520predefining%2520the%2520regions%2520manually.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Visual%20Counterfactual%20Explanations%20Through%20Region%20Constraint&entry.906535625=Bartlomiej%20Sobieski%20and%20Jakub%20Grzywaczewski%20and%20Bartlomiej%20Sadlej%20and%20Matthew%20Tivnan%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20Visual%20counterfactual%20explanations%20%28VCEs%29%20have%20recently%20gained%20immense%0Apopularity%20as%20a%20tool%20for%20clarifying%20the%20decision-making%20process%20of%20image%0Aclassifiers.%20This%20trend%20is%20largely%20motivated%20by%20what%20these%20explanations%20promise%0Ato%20deliver%20--%20indicate%20semantically%20meaningful%20factors%20that%20change%20the%0Aclassifier%27s%20decision.%20However%2C%20we%20argue%20that%20current%20state-of-the-art%0Aapproaches%20lack%20a%20crucial%20component%20--%20the%20region%20constraint%20--%20whose%20absence%0Aprevents%20from%20drawing%20explicit%20conclusions%2C%20and%20may%20even%20lead%20to%20faulty%0Areasoning%20due%20to%20phenomenons%20like%20confirmation%20bias.%20To%20address%20the%20issue%20of%0Aprevious%20methods%2C%20which%20modify%20images%20in%20a%20very%20entangled%20and%20widely%20dispersed%0Amanner%2C%20we%20propose%20region-constrained%20VCEs%20%28RVCEs%29%2C%20which%20assume%20that%20only%20a%0Apredefined%20image%20region%20can%20be%20modified%20to%20influence%20the%20model%27s%20prediction.%20To%0Aeffectively%20sample%20from%20this%20subclass%20of%20VCEs%2C%20we%20propose%20Region-Constrained%0ACounterfactual%20Schr%5C%22odinger%20Bridges%20%28RCSB%29%2C%20an%20adaptation%20of%20a%20tractable%0Asubclass%20of%20Schr%5C%22odinger%20Bridges%20to%20the%20problem%20of%20conditional%20inpainting%2C%0Awhere%20the%20conditioning%20signal%20originates%20from%20the%20classifier%20of%20interest.%20In%0Aaddition%20to%20setting%20a%20new%20state-of-the-art%20by%20a%20large%20margin%2C%20we%20extend%20RCSB%20to%0Aallow%20for%20exact%20counterfactual%20reasoning%2C%20where%20the%20predefined%20region%20contains%0Aonly%20the%20factor%20of%20interest%2C%20and%20incorporating%20the%20user%20to%20actively%20interact%0Awith%20the%20RVCE%20by%20predefining%20the%20regions%20manually.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12591v1&entry.124074799=Read"},
{"title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models", "author": "Shicheng Xu and Liang Pang and Yunchang Zhu and Huawei Shen and Xueqi Cheng", "abstract": "  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n", "link": "http://arxiv.org/abs/2410.12662v1", "date": "2024-10-16", "relevancy": 2.6936, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Safety%20Mechanism%20Transfer%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Cross-Modal%20Safety%20Mechanism%20Transfer%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Shicheng%20Xu%20and%20Liang%20Pang%20and%20Yunchang%20Zhu%20and%20Huawei%20Shen%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20Vision-language%20alignment%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Asuccessfully%20enables%20LLMs%20to%20understand%20visual%20input.%20However%2C%20we%20find%20that%0Aexisting%20vision-language%20alignment%20methods%20fail%20to%20transfer%20the%20existing%20safety%0Amechanism%20for%20text%20in%20LLMs%20to%20vision%2C%20which%20leads%20to%20vulnerabilities%20in%20toxic%0Aimage.%20To%20explore%20the%20cause%20of%20this%20problem%2C%20we%20give%20the%20insightful%20explanation%0Aof%20where%20and%20how%20the%20safety%20mechanism%20of%20LVLMs%20operates%20and%20conduct%20comparative%0Aanalysis%20between%20text%20and%20vision.%20We%20find%20that%20the%20hidden%20states%20at%20the%0Aspecific%20transformer%20layers%20play%20a%20crucial%20role%20in%20the%20successful%20activation%20of%0Asafety%20mechanism%2C%20while%20the%20vision-language%20alignment%20at%20hidden%20states%20level%20in%0Acurrent%20methods%20is%20insufficient.%20This%20results%20in%20a%20semantic%20shift%20for%20input%0Aimages%20compared%20to%20text%20in%20hidden%20states%2C%20therefore%20misleads%20the%20safety%0Amechanism.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Text-Guided%20vision-language%0AAlignment%20method%20%28TGA%29%20for%20LVLMs.%20TGA%20retrieves%20the%20texts%20related%20to%20input%0Avision%20and%20uses%20them%20to%20guide%20the%20projection%20of%20vision%20into%20the%20hidden%20states%0Aspace%20in%20LLMs.%20Experiments%20show%20that%20TGA%20not%20only%20successfully%20transfers%20the%0Asafety%20mechanism%20for%20text%20in%20basic%20LLMs%20to%20vision%20in%20vision-language%20alignment%0Afor%20LVLMs%20without%20any%20safety%20fine-tuning%20on%20the%20visual%20modality%20but%20also%0Amaintains%20the%20general%20performance%20on%20various%20vision%20tasks%20%28Safe%20and%20Good%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Safety%2520Mechanism%2520Transfer%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DShicheng%2520Xu%2520and%2520Liang%2520Pang%2520and%2520Yunchang%2520Zhu%2520and%2520Huawei%2520Shen%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3D%2520%2520Vision-language%2520alignment%2520in%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%250Asuccessfully%2520enables%2520LLMs%2520to%2520understand%2520visual%2520input.%2520However%252C%2520we%2520find%2520that%250Aexisting%2520vision-language%2520alignment%2520methods%2520fail%2520to%2520transfer%2520the%2520existing%2520safety%250Amechanism%2520for%2520text%2520in%2520LLMs%2520to%2520vision%252C%2520which%2520leads%2520to%2520vulnerabilities%2520in%2520toxic%250Aimage.%2520To%2520explore%2520the%2520cause%2520of%2520this%2520problem%252C%2520we%2520give%2520the%2520insightful%2520explanation%250Aof%2520where%2520and%2520how%2520the%2520safety%2520mechanism%2520of%2520LVLMs%2520operates%2520and%2520conduct%2520comparative%250Aanalysis%2520between%2520text%2520and%2520vision.%2520We%2520find%2520that%2520the%2520hidden%2520states%2520at%2520the%250Aspecific%2520transformer%2520layers%2520play%2520a%2520crucial%2520role%2520in%2520the%2520successful%2520activation%2520of%250Asafety%2520mechanism%252C%2520while%2520the%2520vision-language%2520alignment%2520at%2520hidden%2520states%2520level%2520in%250Acurrent%2520methods%2520is%2520insufficient.%2520This%2520results%2520in%2520a%2520semantic%2520shift%2520for%2520input%250Aimages%2520compared%2520to%2520text%2520in%2520hidden%2520states%252C%2520therefore%2520misleads%2520the%2520safety%250Amechanism.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520Text-Guided%2520vision-language%250AAlignment%2520method%2520%2528TGA%2529%2520for%2520LVLMs.%2520TGA%2520retrieves%2520the%2520texts%2520related%2520to%2520input%250Avision%2520and%2520uses%2520them%2520to%2520guide%2520the%2520projection%2520of%2520vision%2520into%2520the%2520hidden%2520states%250Aspace%2520in%2520LLMs.%2520Experiments%2520show%2520that%2520TGA%2520not%2520only%2520successfully%2520transfers%2520the%250Asafety%2520mechanism%2520for%2520text%2520in%2520basic%2520LLMs%2520to%2520vision%2520in%2520vision-language%2520alignment%250Afor%2520LVLMs%2520without%2520any%2520safety%2520fine-tuning%2520on%2520the%2520visual%2520modality%2520but%2520also%250Amaintains%2520the%2520general%2520performance%2520on%2520various%2520vision%2520tasks%2520%2528Safe%2520and%2520Good%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Safety%20Mechanism%20Transfer%20in%20Large%20Vision-Language%20Models&entry.906535625=Shicheng%20Xu%20and%20Liang%20Pang%20and%20Yunchang%20Zhu%20and%20Huawei%20Shen%20and%20Xueqi%20Cheng&entry.1292438233=%20%20Vision-language%20alignment%20in%20Large%20Vision-Language%20Models%20%28LVLMs%29%0Asuccessfully%20enables%20LLMs%20to%20understand%20visual%20input.%20However%2C%20we%20find%20that%0Aexisting%20vision-language%20alignment%20methods%20fail%20to%20transfer%20the%20existing%20safety%0Amechanism%20for%20text%20in%20LLMs%20to%20vision%2C%20which%20leads%20to%20vulnerabilities%20in%20toxic%0Aimage.%20To%20explore%20the%20cause%20of%20this%20problem%2C%20we%20give%20the%20insightful%20explanation%0Aof%20where%20and%20how%20the%20safety%20mechanism%20of%20LVLMs%20operates%20and%20conduct%20comparative%0Aanalysis%20between%20text%20and%20vision.%20We%20find%20that%20the%20hidden%20states%20at%20the%0Aspecific%20transformer%20layers%20play%20a%20crucial%20role%20in%20the%20successful%20activation%20of%0Asafety%20mechanism%2C%20while%20the%20vision-language%20alignment%20at%20hidden%20states%20level%20in%0Acurrent%20methods%20is%20insufficient.%20This%20results%20in%20a%20semantic%20shift%20for%20input%0Aimages%20compared%20to%20text%20in%20hidden%20states%2C%20therefore%20misleads%20the%20safety%0Amechanism.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Text-Guided%20vision-language%0AAlignment%20method%20%28TGA%29%20for%20LVLMs.%20TGA%20retrieves%20the%20texts%20related%20to%20input%0Avision%20and%20uses%20them%20to%20guide%20the%20projection%20of%20vision%20into%20the%20hidden%20states%0Aspace%20in%20LLMs.%20Experiments%20show%20that%20TGA%20not%20only%20successfully%20transfers%20the%0Asafety%20mechanism%20for%20text%20in%20basic%20LLMs%20to%20vision%20in%20vision-language%20alignment%0Afor%20LVLMs%20without%20any%20safety%20fine-tuning%20on%20the%20visual%20modality%20but%20also%0Amaintains%20the%20general%20performance%20on%20various%20vision%20tasks%20%28Safe%20and%20Good%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12662v1&entry.124074799=Read"},
{"title": "ConLUX: Concept-Based Local Unified Explanations", "author": "Junhao Liu and Haonan Yu and Xin Zhang", "abstract": "  With the rapid advancements of various machine learning models, there is a\nsignificant demand for model-agnostic explanation techniques, which can explain\nthese models across different architectures. Mainstream model-agnostic\nexplanation techniques generate local explanations based on basic features\n(e.g., words for text models and (super-)pixels for image models). However,\nthese explanations often do not align with the decision-making processes of the\ntarget models and end-users, resulting in explanations that are unfaithful and\ndifficult for users to understand. On the other hand, concept-based techniques\nprovide explanations based on high-level features (e.g., topics for text models\nand objects for image models), but most are model-specific or require\nadditional pre-defined external concept knowledge. To address this limitation,\nwe propose \\toolname, a general framework to provide concept-based local\nexplanations for any machine learning models. Our key insight is that we can\nautomatically extract high-level concepts from large pre-trained models, and\nuniformly extend existing local model-agnostic techniques to provide unified\nconcept-based explanations. We have instantiated \\toolname on four different\ntypes of explanation techniques: LIME, Kernel SHAP, Anchor, and LORE, and\napplied these techniques to text and image models. Our evaluation results\ndemonstrate that 1) compared to the vanilla versions, \\toolname offers more\nfaithful explanations and makes them more understandable to users, and 2) by\noffering multiple forms of explanations, \\toolname outperforms state-of-the-art\nconcept-based explanation techniques specifically designed for text and image\nmodels, respectively.\n", "link": "http://arxiv.org/abs/2410.12439v1", "date": "2024-10-16", "relevancy": 2.6828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConLUX%3A%20Concept-Based%20Local%20Unified%20Explanations&body=Title%3A%20ConLUX%3A%20Concept-Based%20Local%20Unified%20Explanations%0AAuthor%3A%20Junhao%20Liu%20and%20Haonan%20Yu%20and%20Xin%20Zhang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20of%20various%20machine%20learning%20models%2C%20there%20is%20a%0Asignificant%20demand%20for%20model-agnostic%20explanation%20techniques%2C%20which%20can%20explain%0Athese%20models%20across%20different%20architectures.%20Mainstream%20model-agnostic%0Aexplanation%20techniques%20generate%20local%20explanations%20based%20on%20basic%20features%0A%28e.g.%2C%20words%20for%20text%20models%20and%20%28super-%29pixels%20for%20image%20models%29.%20However%2C%0Athese%20explanations%20often%20do%20not%20align%20with%20the%20decision-making%20processes%20of%20the%0Atarget%20models%20and%20end-users%2C%20resulting%20in%20explanations%20that%20are%20unfaithful%20and%0Adifficult%20for%20users%20to%20understand.%20On%20the%20other%20hand%2C%20concept-based%20techniques%0Aprovide%20explanations%20based%20on%20high-level%20features%20%28e.g.%2C%20topics%20for%20text%20models%0Aand%20objects%20for%20image%20models%29%2C%20but%20most%20are%20model-specific%20or%20require%0Aadditional%20pre-defined%20external%20concept%20knowledge.%20To%20address%20this%20limitation%2C%0Awe%20propose%20%5Ctoolname%2C%20a%20general%20framework%20to%20provide%20concept-based%20local%0Aexplanations%20for%20any%20machine%20learning%20models.%20Our%20key%20insight%20is%20that%20we%20can%0Aautomatically%20extract%20high-level%20concepts%20from%20large%20pre-trained%20models%2C%20and%0Auniformly%20extend%20existing%20local%20model-agnostic%20techniques%20to%20provide%20unified%0Aconcept-based%20explanations.%20We%20have%20instantiated%20%5Ctoolname%20on%20four%20different%0Atypes%20of%20explanation%20techniques%3A%20LIME%2C%20Kernel%20SHAP%2C%20Anchor%2C%20and%20LORE%2C%20and%0Aapplied%20these%20techniques%20to%20text%20and%20image%20models.%20Our%20evaluation%20results%0Ademonstrate%20that%201%29%20compared%20to%20the%20vanilla%20versions%2C%20%5Ctoolname%20offers%20more%0Afaithful%20explanations%20and%20makes%20them%20more%20understandable%20to%20users%2C%20and%202%29%20by%0Aoffering%20multiple%20forms%20of%20explanations%2C%20%5Ctoolname%20outperforms%20state-of-the-art%0Aconcept-based%20explanation%20techniques%20specifically%20designed%20for%20text%20and%20image%0Amodels%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConLUX%253A%2520Concept-Based%2520Local%2520Unified%2520Explanations%26entry.906535625%3DJunhao%2520Liu%2520and%2520Haonan%2520Yu%2520and%2520Xin%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520of%2520various%2520machine%2520learning%2520models%252C%2520there%2520is%2520a%250Asignificant%2520demand%2520for%2520model-agnostic%2520explanation%2520techniques%252C%2520which%2520can%2520explain%250Athese%2520models%2520across%2520different%2520architectures.%2520Mainstream%2520model-agnostic%250Aexplanation%2520techniques%2520generate%2520local%2520explanations%2520based%2520on%2520basic%2520features%250A%2528e.g.%252C%2520words%2520for%2520text%2520models%2520and%2520%2528super-%2529pixels%2520for%2520image%2520models%2529.%2520However%252C%250Athese%2520explanations%2520often%2520do%2520not%2520align%2520with%2520the%2520decision-making%2520processes%2520of%2520the%250Atarget%2520models%2520and%2520end-users%252C%2520resulting%2520in%2520explanations%2520that%2520are%2520unfaithful%2520and%250Adifficult%2520for%2520users%2520to%2520understand.%2520On%2520the%2520other%2520hand%252C%2520concept-based%2520techniques%250Aprovide%2520explanations%2520based%2520on%2520high-level%2520features%2520%2528e.g.%252C%2520topics%2520for%2520text%2520models%250Aand%2520objects%2520for%2520image%2520models%2529%252C%2520but%2520most%2520are%2520model-specific%2520or%2520require%250Aadditional%2520pre-defined%2520external%2520concept%2520knowledge.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520propose%2520%255Ctoolname%252C%2520a%2520general%2520framework%2520to%2520provide%2520concept-based%2520local%250Aexplanations%2520for%2520any%2520machine%2520learning%2520models.%2520Our%2520key%2520insight%2520is%2520that%2520we%2520can%250Aautomatically%2520extract%2520high-level%2520concepts%2520from%2520large%2520pre-trained%2520models%252C%2520and%250Auniformly%2520extend%2520existing%2520local%2520model-agnostic%2520techniques%2520to%2520provide%2520unified%250Aconcept-based%2520explanations.%2520We%2520have%2520instantiated%2520%255Ctoolname%2520on%2520four%2520different%250Atypes%2520of%2520explanation%2520techniques%253A%2520LIME%252C%2520Kernel%2520SHAP%252C%2520Anchor%252C%2520and%2520LORE%252C%2520and%250Aapplied%2520these%2520techniques%2520to%2520text%2520and%2520image%2520models.%2520Our%2520evaluation%2520results%250Ademonstrate%2520that%25201%2529%2520compared%2520to%2520the%2520vanilla%2520versions%252C%2520%255Ctoolname%2520offers%2520more%250Afaithful%2520explanations%2520and%2520makes%2520them%2520more%2520understandable%2520to%2520users%252C%2520and%25202%2529%2520by%250Aoffering%2520multiple%2520forms%2520of%2520explanations%252C%2520%255Ctoolname%2520outperforms%2520state-of-the-art%250Aconcept-based%2520explanation%2520techniques%2520specifically%2520designed%2520for%2520text%2520and%2520image%250Amodels%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConLUX%3A%20Concept-Based%20Local%20Unified%20Explanations&entry.906535625=Junhao%20Liu%20and%20Haonan%20Yu%20and%20Xin%20Zhang&entry.1292438233=%20%20With%20the%20rapid%20advancements%20of%20various%20machine%20learning%20models%2C%20there%20is%20a%0Asignificant%20demand%20for%20model-agnostic%20explanation%20techniques%2C%20which%20can%20explain%0Athese%20models%20across%20different%20architectures.%20Mainstream%20model-agnostic%0Aexplanation%20techniques%20generate%20local%20explanations%20based%20on%20basic%20features%0A%28e.g.%2C%20words%20for%20text%20models%20and%20%28super-%29pixels%20for%20image%20models%29.%20However%2C%0Athese%20explanations%20often%20do%20not%20align%20with%20the%20decision-making%20processes%20of%20the%0Atarget%20models%20and%20end-users%2C%20resulting%20in%20explanations%20that%20are%20unfaithful%20and%0Adifficult%20for%20users%20to%20understand.%20On%20the%20other%20hand%2C%20concept-based%20techniques%0Aprovide%20explanations%20based%20on%20high-level%20features%20%28e.g.%2C%20topics%20for%20text%20models%0Aand%20objects%20for%20image%20models%29%2C%20but%20most%20are%20model-specific%20or%20require%0Aadditional%20pre-defined%20external%20concept%20knowledge.%20To%20address%20this%20limitation%2C%0Awe%20propose%20%5Ctoolname%2C%20a%20general%20framework%20to%20provide%20concept-based%20local%0Aexplanations%20for%20any%20machine%20learning%20models.%20Our%20key%20insight%20is%20that%20we%20can%0Aautomatically%20extract%20high-level%20concepts%20from%20large%20pre-trained%20models%2C%20and%0Auniformly%20extend%20existing%20local%20model-agnostic%20techniques%20to%20provide%20unified%0Aconcept-based%20explanations.%20We%20have%20instantiated%20%5Ctoolname%20on%20four%20different%0Atypes%20of%20explanation%20techniques%3A%20LIME%2C%20Kernel%20SHAP%2C%20Anchor%2C%20and%20LORE%2C%20and%0Aapplied%20these%20techniques%20to%20text%20and%20image%20models.%20Our%20evaluation%20results%0Ademonstrate%20that%201%29%20compared%20to%20the%20vanilla%20versions%2C%20%5Ctoolname%20offers%20more%0Afaithful%20explanations%20and%20makes%20them%20more%20understandable%20to%20users%2C%20and%202%29%20by%0Aoffering%20multiple%20forms%20of%20explanations%2C%20%5Ctoolname%20outperforms%20state-of-the-art%0Aconcept-based%20explanation%20techniques%20specifically%20designed%20for%20text%20and%20image%0Amodels%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12439v1&entry.124074799=Read"},
{"title": "New Paradigm of Adversarial Training: Breaking Inherent Trade-Off\n  between Accuracy and Robustness via Dummy Classes", "author": "Yanyun Wang and Li Liu and Zi Liang and Qingqing Ye and Haibo Hu", "abstract": "  Adversarial Training (AT) is one of the most effective methods to enhance the\nrobustness of DNNs. However, existing AT methods suffer from an inherent\ntrade-off between adversarial robustness and clean accuracy, which seriously\nhinders their real-world deployment. While this problem has been widely studied\nwithin the current AT paradigm, existing AT methods still typically experience\na reduction in clean accuracy by over 10% to date, without significant\nimprovements in robustness compared with simple baselines like PGD-AT. This\ninherent trade-off raises a question: whether the current AT paradigm, which\nassumes to learn the corresponding benign and adversarial samples as the same\nclass, inappropriately combines clean and robust objectives that may be\nessentially inconsistent. In this work, we surprisingly reveal that up to 40%\nof CIFAR-10 adversarial samples always fail to satisfy such an assumption\nacross various AT methods and robust models, explicitly indicating the\nimprovement room for the current AT paradigm. Accordingly, to relax the tension\nbetween clean and robust learning derived from this overstrict assumption, we\npropose a new AT paradigm by introducing an additional dummy class for each\noriginal class, aiming to accommodate the hard adversarial samples with shifted\ndistribution after perturbation. The robustness w.r.t. these adversarial\nsamples can be achieved by runtime recovery from the predicted dummy classes to\ntheir corresponding original ones, eliminating the compromise with clean\nlearning. Building on this new paradigm, we propose a novel plug-and-play AT\ntechnology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the\nDUCAT concurrently improves clean accuracy and adversarial robustness compared\nwith state-of-the-art benchmarks, effectively breaking the existing inherent\ntrade-off.\n", "link": "http://arxiv.org/abs/2410.12671v1", "date": "2024-10-16", "relevancy": 2.6559, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5502}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Paradigm%20of%20Adversarial%20Training%3A%20Breaking%20Inherent%20Trade-Off%0A%20%20between%20Accuracy%20and%20Robustness%20via%20Dummy%20Classes&body=Title%3A%20New%20Paradigm%20of%20Adversarial%20Training%3A%20Breaking%20Inherent%20Trade-Off%0A%20%20between%20Accuracy%20and%20Robustness%20via%20Dummy%20Classes%0AAuthor%3A%20Yanyun%20Wang%20and%20Li%20Liu%20and%20Zi%20Liang%20and%20Qingqing%20Ye%20and%20Haibo%20Hu%0AAbstract%3A%20%20%20Adversarial%20Training%20%28AT%29%20is%20one%20of%20the%20most%20effective%20methods%20to%20enhance%20the%0Arobustness%20of%20DNNs.%20However%2C%20existing%20AT%20methods%20suffer%20from%20an%20inherent%0Atrade-off%20between%20adversarial%20robustness%20and%20clean%20accuracy%2C%20which%20seriously%0Ahinders%20their%20real-world%20deployment.%20While%20this%20problem%20has%20been%20widely%20studied%0Awithin%20the%20current%20AT%20paradigm%2C%20existing%20AT%20methods%20still%20typically%20experience%0Aa%20reduction%20in%20clean%20accuracy%20by%20over%2010%25%20to%20date%2C%20without%20significant%0Aimprovements%20in%20robustness%20compared%20with%20simple%20baselines%20like%20PGD-AT.%20This%0Ainherent%20trade-off%20raises%20a%20question%3A%20whether%20the%20current%20AT%20paradigm%2C%20which%0Aassumes%20to%20learn%20the%20corresponding%20benign%20and%20adversarial%20samples%20as%20the%20same%0Aclass%2C%20inappropriately%20combines%20clean%20and%20robust%20objectives%20that%20may%20be%0Aessentially%20inconsistent.%20In%20this%20work%2C%20we%20surprisingly%20reveal%20that%20up%20to%2040%25%0Aof%20CIFAR-10%20adversarial%20samples%20always%20fail%20to%20satisfy%20such%20an%20assumption%0Aacross%20various%20AT%20methods%20and%20robust%20models%2C%20explicitly%20indicating%20the%0Aimprovement%20room%20for%20the%20current%20AT%20paradigm.%20Accordingly%2C%20to%20relax%20the%20tension%0Abetween%20clean%20and%20robust%20learning%20derived%20from%20this%20overstrict%20assumption%2C%20we%0Apropose%20a%20new%20AT%20paradigm%20by%20introducing%20an%20additional%20dummy%20class%20for%20each%0Aoriginal%20class%2C%20aiming%20to%20accommodate%20the%20hard%20adversarial%20samples%20with%20shifted%0Adistribution%20after%20perturbation.%20The%20robustness%20w.r.t.%20these%20adversarial%0Asamples%20can%20be%20achieved%20by%20runtime%20recovery%20from%20the%20predicted%20dummy%20classes%20to%0Atheir%20corresponding%20original%20ones%2C%20eliminating%20the%20compromise%20with%20clean%0Alearning.%20Building%20on%20this%20new%20paradigm%2C%20we%20propose%20a%20novel%20plug-and-play%20AT%0Atechnology%20named%20DUmmy%20Classes-based%20Adversarial%20Training%20%28DUCAT%29.%20Extensive%0Aexperiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny-ImageNet%20demonstrate%20that%20the%0ADUCAT%20concurrently%20improves%20clean%20accuracy%20and%20adversarial%20robustness%20compared%0Awith%20state-of-the-art%20benchmarks%2C%20effectively%20breaking%20the%20existing%20inherent%0Atrade-off.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Paradigm%2520of%2520Adversarial%2520Training%253A%2520Breaking%2520Inherent%2520Trade-Off%250A%2520%2520between%2520Accuracy%2520and%2520Robustness%2520via%2520Dummy%2520Classes%26entry.906535625%3DYanyun%2520Wang%2520and%2520Li%2520Liu%2520and%2520Zi%2520Liang%2520and%2520Qingqing%2520Ye%2520and%2520Haibo%2520Hu%26entry.1292438233%3D%2520%2520Adversarial%2520Training%2520%2528AT%2529%2520is%2520one%2520of%2520the%2520most%2520effective%2520methods%2520to%2520enhance%2520the%250Arobustness%2520of%2520DNNs.%2520However%252C%2520existing%2520AT%2520methods%2520suffer%2520from%2520an%2520inherent%250Atrade-off%2520between%2520adversarial%2520robustness%2520and%2520clean%2520accuracy%252C%2520which%2520seriously%250Ahinders%2520their%2520real-world%2520deployment.%2520While%2520this%2520problem%2520has%2520been%2520widely%2520studied%250Awithin%2520the%2520current%2520AT%2520paradigm%252C%2520existing%2520AT%2520methods%2520still%2520typically%2520experience%250Aa%2520reduction%2520in%2520clean%2520accuracy%2520by%2520over%252010%2525%2520to%2520date%252C%2520without%2520significant%250Aimprovements%2520in%2520robustness%2520compared%2520with%2520simple%2520baselines%2520like%2520PGD-AT.%2520This%250Ainherent%2520trade-off%2520raises%2520a%2520question%253A%2520whether%2520the%2520current%2520AT%2520paradigm%252C%2520which%250Aassumes%2520to%2520learn%2520the%2520corresponding%2520benign%2520and%2520adversarial%2520samples%2520as%2520the%2520same%250Aclass%252C%2520inappropriately%2520combines%2520clean%2520and%2520robust%2520objectives%2520that%2520may%2520be%250Aessentially%2520inconsistent.%2520In%2520this%2520work%252C%2520we%2520surprisingly%2520reveal%2520that%2520up%2520to%252040%2525%250Aof%2520CIFAR-10%2520adversarial%2520samples%2520always%2520fail%2520to%2520satisfy%2520such%2520an%2520assumption%250Aacross%2520various%2520AT%2520methods%2520and%2520robust%2520models%252C%2520explicitly%2520indicating%2520the%250Aimprovement%2520room%2520for%2520the%2520current%2520AT%2520paradigm.%2520Accordingly%252C%2520to%2520relax%2520the%2520tension%250Abetween%2520clean%2520and%2520robust%2520learning%2520derived%2520from%2520this%2520overstrict%2520assumption%252C%2520we%250Apropose%2520a%2520new%2520AT%2520paradigm%2520by%2520introducing%2520an%2520additional%2520dummy%2520class%2520for%2520each%250Aoriginal%2520class%252C%2520aiming%2520to%2520accommodate%2520the%2520hard%2520adversarial%2520samples%2520with%2520shifted%250Adistribution%2520after%2520perturbation.%2520The%2520robustness%2520w.r.t.%2520these%2520adversarial%250Asamples%2520can%2520be%2520achieved%2520by%2520runtime%2520recovery%2520from%2520the%2520predicted%2520dummy%2520classes%2520to%250Atheir%2520corresponding%2520original%2520ones%252C%2520eliminating%2520the%2520compromise%2520with%2520clean%250Alearning.%2520Building%2520on%2520this%2520new%2520paradigm%252C%2520we%2520propose%2520a%2520novel%2520plug-and-play%2520AT%250Atechnology%2520named%2520DUmmy%2520Classes-based%2520Adversarial%2520Training%2520%2528DUCAT%2529.%2520Extensive%250Aexperiments%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520Tiny-ImageNet%2520demonstrate%2520that%2520the%250ADUCAT%2520concurrently%2520improves%2520clean%2520accuracy%2520and%2520adversarial%2520robustness%2520compared%250Awith%2520state-of-the-art%2520benchmarks%252C%2520effectively%2520breaking%2520the%2520existing%2520inherent%250Atrade-off.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Paradigm%20of%20Adversarial%20Training%3A%20Breaking%20Inherent%20Trade-Off%0A%20%20between%20Accuracy%20and%20Robustness%20via%20Dummy%20Classes&entry.906535625=Yanyun%20Wang%20and%20Li%20Liu%20and%20Zi%20Liang%20and%20Qingqing%20Ye%20and%20Haibo%20Hu&entry.1292438233=%20%20Adversarial%20Training%20%28AT%29%20is%20one%20of%20the%20most%20effective%20methods%20to%20enhance%20the%0Arobustness%20of%20DNNs.%20However%2C%20existing%20AT%20methods%20suffer%20from%20an%20inherent%0Atrade-off%20between%20adversarial%20robustness%20and%20clean%20accuracy%2C%20which%20seriously%0Ahinders%20their%20real-world%20deployment.%20While%20this%20problem%20has%20been%20widely%20studied%0Awithin%20the%20current%20AT%20paradigm%2C%20existing%20AT%20methods%20still%20typically%20experience%0Aa%20reduction%20in%20clean%20accuracy%20by%20over%2010%25%20to%20date%2C%20without%20significant%0Aimprovements%20in%20robustness%20compared%20with%20simple%20baselines%20like%20PGD-AT.%20This%0Ainherent%20trade-off%20raises%20a%20question%3A%20whether%20the%20current%20AT%20paradigm%2C%20which%0Aassumes%20to%20learn%20the%20corresponding%20benign%20and%20adversarial%20samples%20as%20the%20same%0Aclass%2C%20inappropriately%20combines%20clean%20and%20robust%20objectives%20that%20may%20be%0Aessentially%20inconsistent.%20In%20this%20work%2C%20we%20surprisingly%20reveal%20that%20up%20to%2040%25%0Aof%20CIFAR-10%20adversarial%20samples%20always%20fail%20to%20satisfy%20such%20an%20assumption%0Aacross%20various%20AT%20methods%20and%20robust%20models%2C%20explicitly%20indicating%20the%0Aimprovement%20room%20for%20the%20current%20AT%20paradigm.%20Accordingly%2C%20to%20relax%20the%20tension%0Abetween%20clean%20and%20robust%20learning%20derived%20from%20this%20overstrict%20assumption%2C%20we%0Apropose%20a%20new%20AT%20paradigm%20by%20introducing%20an%20additional%20dummy%20class%20for%20each%0Aoriginal%20class%2C%20aiming%20to%20accommodate%20the%20hard%20adversarial%20samples%20with%20shifted%0Adistribution%20after%20perturbation.%20The%20robustness%20w.r.t.%20these%20adversarial%0Asamples%20can%20be%20achieved%20by%20runtime%20recovery%20from%20the%20predicted%20dummy%20classes%20to%0Atheir%20corresponding%20original%20ones%2C%20eliminating%20the%20compromise%20with%20clean%0Alearning.%20Building%20on%20this%20new%20paradigm%2C%20we%20propose%20a%20novel%20plug-and-play%20AT%0Atechnology%20named%20DUmmy%20Classes-based%20Adversarial%20Training%20%28DUCAT%29.%20Extensive%0Aexperiments%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20Tiny-ImageNet%20demonstrate%20that%20the%0ADUCAT%20concurrently%20improves%20clean%20accuracy%20and%20adversarial%20robustness%20compared%0Awith%20state-of-the-art%20benchmarks%2C%20effectively%20breaking%20the%20existing%20inherent%0Atrade-off.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12671v1&entry.124074799=Read"},
{"title": "Local transfer learning Gaussian process modeling, with applications to\n  surrogate modeling of expensive computer simulators", "author": "Xinming Wang and Simon Mak and John Miller and Jianguo Wu", "abstract": "  A critical bottleneck for scientific progress is the costly nature of\ncomputer simulations for complex systems. Surrogate models provide an appealing\nsolution: such models are trained on simulator evaluations, then used to\nemulate and quantify uncertainty on the expensive simulator at unexplored\ninputs. In many applications, one often has available data on related systems.\nFor example, in designing a new jet turbine, there may be existing studies on\nturbines with similar configurations. A key question is how information from\nsuch \"source\" systems can be transferred for effective surrogate training on\nthe \"target\" system of interest. We thus propose a new LOcal transfer Learning\nGaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian\nprocess to transfer such information for surrogate modeling. The key novelty of\nthe LOL-GP is a latent regularization model, which identifies regions where\ntransfer should be performed and regions where it should be avoided. This\n\"local transfer\" property is desirable in scientific systems: at certain\nparameters, such systems may behave similarly and thus transfer is beneficial;\nat other parameters, they may behave differently and thus transfer is\ndetrimental. By accounting for local transfer, the LOL-GP can rectify a\ncritical limitation of \"negative transfer\" in existing transfer learning\nmodels, where the transfer of information worsens predictive performance. We\nderive a Gibbs sampling algorithm for efficient posterior predictive sampling\non the LOL-GP, for both the multi-source and multi-fidelity transfer settings.\nWe then show, via a suite of numerical experiments and an application for jet\nturbine design, the improved surrogate performance of the LOL-GP over existing\nmethods.\n", "link": "http://arxiv.org/abs/2410.12690v1", "date": "2024-10-16", "relevancy": 2.6495, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.532}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5296}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20transfer%20learning%20Gaussian%20process%20modeling%2C%20with%20applications%20to%0A%20%20surrogate%20modeling%20of%20expensive%20computer%20simulators&body=Title%3A%20Local%20transfer%20learning%20Gaussian%20process%20modeling%2C%20with%20applications%20to%0A%20%20surrogate%20modeling%20of%20expensive%20computer%20simulators%0AAuthor%3A%20Xinming%20Wang%20and%20Simon%20Mak%20and%20John%20Miller%20and%20Jianguo%20Wu%0AAbstract%3A%20%20%20A%20critical%20bottleneck%20for%20scientific%20progress%20is%20the%20costly%20nature%20of%0Acomputer%20simulations%20for%20complex%20systems.%20Surrogate%20models%20provide%20an%20appealing%0Asolution%3A%20such%20models%20are%20trained%20on%20simulator%20evaluations%2C%20then%20used%20to%0Aemulate%20and%20quantify%20uncertainty%20on%20the%20expensive%20simulator%20at%20unexplored%0Ainputs.%20In%20many%20applications%2C%20one%20often%20has%20available%20data%20on%20related%20systems.%0AFor%20example%2C%20in%20designing%20a%20new%20jet%20turbine%2C%20there%20may%20be%20existing%20studies%20on%0Aturbines%20with%20similar%20configurations.%20A%20key%20question%20is%20how%20information%20from%0Asuch%20%22source%22%20systems%20can%20be%20transferred%20for%20effective%20surrogate%20training%20on%0Athe%20%22target%22%20system%20of%20interest.%20We%20thus%20propose%20a%20new%20LOcal%20transfer%20Learning%0AGaussian%20Process%20%28LOL-GP%29%20model%2C%20which%20leverages%20a%20carefully-designed%20Gaussian%0Aprocess%20to%20transfer%20such%20information%20for%20surrogate%20modeling.%20The%20key%20novelty%20of%0Athe%20LOL-GP%20is%20a%20latent%20regularization%20model%2C%20which%20identifies%20regions%20where%0Atransfer%20should%20be%20performed%20and%20regions%20where%20it%20should%20be%20avoided.%20This%0A%22local%20transfer%22%20property%20is%20desirable%20in%20scientific%20systems%3A%20at%20certain%0Aparameters%2C%20such%20systems%20may%20behave%20similarly%20and%20thus%20transfer%20is%20beneficial%3B%0Aat%20other%20parameters%2C%20they%20may%20behave%20differently%20and%20thus%20transfer%20is%0Adetrimental.%20By%20accounting%20for%20local%20transfer%2C%20the%20LOL-GP%20can%20rectify%20a%0Acritical%20limitation%20of%20%22negative%20transfer%22%20in%20existing%20transfer%20learning%0Amodels%2C%20where%20the%20transfer%20of%20information%20worsens%20predictive%20performance.%20We%0Aderive%20a%20Gibbs%20sampling%20algorithm%20for%20efficient%20posterior%20predictive%20sampling%0Aon%20the%20LOL-GP%2C%20for%20both%20the%20multi-source%20and%20multi-fidelity%20transfer%20settings.%0AWe%20then%20show%2C%20via%20a%20suite%20of%20numerical%20experiments%20and%20an%20application%20for%20jet%0Aturbine%20design%2C%20the%20improved%20surrogate%20performance%20of%20the%20LOL-GP%20over%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520transfer%2520learning%2520Gaussian%2520process%2520modeling%252C%2520with%2520applications%2520to%250A%2520%2520surrogate%2520modeling%2520of%2520expensive%2520computer%2520simulators%26entry.906535625%3DXinming%2520Wang%2520and%2520Simon%2520Mak%2520and%2520John%2520Miller%2520and%2520Jianguo%2520Wu%26entry.1292438233%3D%2520%2520A%2520critical%2520bottleneck%2520for%2520scientific%2520progress%2520is%2520the%2520costly%2520nature%2520of%250Acomputer%2520simulations%2520for%2520complex%2520systems.%2520Surrogate%2520models%2520provide%2520an%2520appealing%250Asolution%253A%2520such%2520models%2520are%2520trained%2520on%2520simulator%2520evaluations%252C%2520then%2520used%2520to%250Aemulate%2520and%2520quantify%2520uncertainty%2520on%2520the%2520expensive%2520simulator%2520at%2520unexplored%250Ainputs.%2520In%2520many%2520applications%252C%2520one%2520often%2520has%2520available%2520data%2520on%2520related%2520systems.%250AFor%2520example%252C%2520in%2520designing%2520a%2520new%2520jet%2520turbine%252C%2520there%2520may%2520be%2520existing%2520studies%2520on%250Aturbines%2520with%2520similar%2520configurations.%2520A%2520key%2520question%2520is%2520how%2520information%2520from%250Asuch%2520%2522source%2522%2520systems%2520can%2520be%2520transferred%2520for%2520effective%2520surrogate%2520training%2520on%250Athe%2520%2522target%2522%2520system%2520of%2520interest.%2520We%2520thus%2520propose%2520a%2520new%2520LOcal%2520transfer%2520Learning%250AGaussian%2520Process%2520%2528LOL-GP%2529%2520model%252C%2520which%2520leverages%2520a%2520carefully-designed%2520Gaussian%250Aprocess%2520to%2520transfer%2520such%2520information%2520for%2520surrogate%2520modeling.%2520The%2520key%2520novelty%2520of%250Athe%2520LOL-GP%2520is%2520a%2520latent%2520regularization%2520model%252C%2520which%2520identifies%2520regions%2520where%250Atransfer%2520should%2520be%2520performed%2520and%2520regions%2520where%2520it%2520should%2520be%2520avoided.%2520This%250A%2522local%2520transfer%2522%2520property%2520is%2520desirable%2520in%2520scientific%2520systems%253A%2520at%2520certain%250Aparameters%252C%2520such%2520systems%2520may%2520behave%2520similarly%2520and%2520thus%2520transfer%2520is%2520beneficial%253B%250Aat%2520other%2520parameters%252C%2520they%2520may%2520behave%2520differently%2520and%2520thus%2520transfer%2520is%250Adetrimental.%2520By%2520accounting%2520for%2520local%2520transfer%252C%2520the%2520LOL-GP%2520can%2520rectify%2520a%250Acritical%2520limitation%2520of%2520%2522negative%2520transfer%2522%2520in%2520existing%2520transfer%2520learning%250Amodels%252C%2520where%2520the%2520transfer%2520of%2520information%2520worsens%2520predictive%2520performance.%2520We%250Aderive%2520a%2520Gibbs%2520sampling%2520algorithm%2520for%2520efficient%2520posterior%2520predictive%2520sampling%250Aon%2520the%2520LOL-GP%252C%2520for%2520both%2520the%2520multi-source%2520and%2520multi-fidelity%2520transfer%2520settings.%250AWe%2520then%2520show%252C%2520via%2520a%2520suite%2520of%2520numerical%2520experiments%2520and%2520an%2520application%2520for%2520jet%250Aturbine%2520design%252C%2520the%2520improved%2520surrogate%2520performance%2520of%2520the%2520LOL-GP%2520over%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20transfer%20learning%20Gaussian%20process%20modeling%2C%20with%20applications%20to%0A%20%20surrogate%20modeling%20of%20expensive%20computer%20simulators&entry.906535625=Xinming%20Wang%20and%20Simon%20Mak%20and%20John%20Miller%20and%20Jianguo%20Wu&entry.1292438233=%20%20A%20critical%20bottleneck%20for%20scientific%20progress%20is%20the%20costly%20nature%20of%0Acomputer%20simulations%20for%20complex%20systems.%20Surrogate%20models%20provide%20an%20appealing%0Asolution%3A%20such%20models%20are%20trained%20on%20simulator%20evaluations%2C%20then%20used%20to%0Aemulate%20and%20quantify%20uncertainty%20on%20the%20expensive%20simulator%20at%20unexplored%0Ainputs.%20In%20many%20applications%2C%20one%20often%20has%20available%20data%20on%20related%20systems.%0AFor%20example%2C%20in%20designing%20a%20new%20jet%20turbine%2C%20there%20may%20be%20existing%20studies%20on%0Aturbines%20with%20similar%20configurations.%20A%20key%20question%20is%20how%20information%20from%0Asuch%20%22source%22%20systems%20can%20be%20transferred%20for%20effective%20surrogate%20training%20on%0Athe%20%22target%22%20system%20of%20interest.%20We%20thus%20propose%20a%20new%20LOcal%20transfer%20Learning%0AGaussian%20Process%20%28LOL-GP%29%20model%2C%20which%20leverages%20a%20carefully-designed%20Gaussian%0Aprocess%20to%20transfer%20such%20information%20for%20surrogate%20modeling.%20The%20key%20novelty%20of%0Athe%20LOL-GP%20is%20a%20latent%20regularization%20model%2C%20which%20identifies%20regions%20where%0Atransfer%20should%20be%20performed%20and%20regions%20where%20it%20should%20be%20avoided.%20This%0A%22local%20transfer%22%20property%20is%20desirable%20in%20scientific%20systems%3A%20at%20certain%0Aparameters%2C%20such%20systems%20may%20behave%20similarly%20and%20thus%20transfer%20is%20beneficial%3B%0Aat%20other%20parameters%2C%20they%20may%20behave%20differently%20and%20thus%20transfer%20is%0Adetrimental.%20By%20accounting%20for%20local%20transfer%2C%20the%20LOL-GP%20can%20rectify%20a%0Acritical%20limitation%20of%20%22negative%20transfer%22%20in%20existing%20transfer%20learning%0Amodels%2C%20where%20the%20transfer%20of%20information%20worsens%20predictive%20performance.%20We%0Aderive%20a%20Gibbs%20sampling%20algorithm%20for%20efficient%20posterior%20predictive%20sampling%0Aon%20the%20LOL-GP%2C%20for%20both%20the%20multi-source%20and%20multi-fidelity%20transfer%20settings.%0AWe%20then%20show%2C%20via%20a%20suite%20of%20numerical%20experiments%20and%20an%20application%20for%20jet%0Aturbine%20design%2C%20the%20improved%20surrogate%20performance%20of%20the%20LOL-GP%20over%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12690v1&entry.124074799=Read"},
{"title": "Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex\n  Capabilities", "author": "Zhifei Xie and Changqiao Wu", "abstract": "  GPT-4o, an all-encompassing model, represents a milestone in the development\nof large multi-modal language models. It can understand visual, auditory, and\ntextual modalities, directly output audio, and support flexible duplex\ninteraction. Models from the open-source community often achieve some\nfunctionalities of GPT-4o, such as visual understanding and voice chat.\nNevertheless, training a unified model that incorporates all modalities is\nchallenging due to the complexities of multi-modal data, intricate model\narchitectures, and training processes. In this paper, we introduce Mini-Omni2,\na visual-audio assistant capable of providing real-time, end-to-end voice\nresponses to visoin and audio queries. By integrating pretrained visual and\nauditory encoders, Mini-Omni2 maintains performance in individual modalities.\nWe propose a three-stage training process to align modalities, allowing the\nlanguage model to handle multi-modal inputs and outputs after training on a\nlimited dataset. For interaction, we introduce a command-based interruption\nmechanism, enabling more flexible interaction with users. To the best of our\nknowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have\nsimilar form of functionality, and we hope it can offer valuable insights for\nsubsequent research.\n", "link": "http://arxiv.org/abs/2410.11190v2", "date": "2024-10-16", "relevancy": 2.6437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mini-Omni2%3A%20Towards%20Open-source%20GPT-4o%20with%20Vision%2C%20Speech%20and%20Duplex%0A%20%20Capabilities&body=Title%3A%20Mini-Omni2%3A%20Towards%20Open-source%20GPT-4o%20with%20Vision%2C%20Speech%20and%20Duplex%0A%20%20Capabilities%0AAuthor%3A%20Zhifei%20Xie%20and%20Changqiao%20Wu%0AAbstract%3A%20%20%20GPT-4o%2C%20an%20all-encompassing%20model%2C%20represents%20a%20milestone%20in%20the%20development%0Aof%20large%20multi-modal%20language%20models.%20It%20can%20understand%20visual%2C%20auditory%2C%20and%0Atextual%20modalities%2C%20directly%20output%20audio%2C%20and%20support%20flexible%20duplex%0Ainteraction.%20Models%20from%20the%20open-source%20community%20often%20achieve%20some%0Afunctionalities%20of%20GPT-4o%2C%20such%20as%20visual%20understanding%20and%20voice%20chat.%0ANevertheless%2C%20training%20a%20unified%20model%20that%20incorporates%20all%20modalities%20is%0Achallenging%20due%20to%20the%20complexities%20of%20multi-modal%20data%2C%20intricate%20model%0Aarchitectures%2C%20and%20training%20processes.%20In%20this%20paper%2C%20we%20introduce%20Mini-Omni2%2C%0Aa%20visual-audio%20assistant%20capable%20of%20providing%20real-time%2C%20end-to-end%20voice%0Aresponses%20to%20visoin%20and%20audio%20queries.%20By%20integrating%20pretrained%20visual%20and%0Aauditory%20encoders%2C%20Mini-Omni2%20maintains%20performance%20in%20individual%20modalities.%0AWe%20propose%20a%20three-stage%20training%20process%20to%20align%20modalities%2C%20allowing%20the%0Alanguage%20model%20to%20handle%20multi-modal%20inputs%20and%20outputs%20after%20training%20on%20a%0Alimited%20dataset.%20For%20interaction%2C%20we%20introduce%20a%20command-based%20interruption%0Amechanism%2C%20enabling%20more%20flexible%20interaction%20with%20users.%20To%20the%20best%20of%20our%0Aknowledge%2C%20Mini-Omni2%20is%20one%20of%20the%20closest%20reproductions%20of%20GPT-4o%2C%20which%20have%0Asimilar%20form%20of%20functionality%2C%20and%20we%20hope%20it%20can%20offer%20valuable%20insights%20for%0Asubsequent%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMini-Omni2%253A%2520Towards%2520Open-source%2520GPT-4o%2520with%2520Vision%252C%2520Speech%2520and%2520Duplex%250A%2520%2520Capabilities%26entry.906535625%3DZhifei%2520Xie%2520and%2520Changqiao%2520Wu%26entry.1292438233%3D%2520%2520GPT-4o%252C%2520an%2520all-encompassing%2520model%252C%2520represents%2520a%2520milestone%2520in%2520the%2520development%250Aof%2520large%2520multi-modal%2520language%2520models.%2520It%2520can%2520understand%2520visual%252C%2520auditory%252C%2520and%250Atextual%2520modalities%252C%2520directly%2520output%2520audio%252C%2520and%2520support%2520flexible%2520duplex%250Ainteraction.%2520Models%2520from%2520the%2520open-source%2520community%2520often%2520achieve%2520some%250Afunctionalities%2520of%2520GPT-4o%252C%2520such%2520as%2520visual%2520understanding%2520and%2520voice%2520chat.%250ANevertheless%252C%2520training%2520a%2520unified%2520model%2520that%2520incorporates%2520all%2520modalities%2520is%250Achallenging%2520due%2520to%2520the%2520complexities%2520of%2520multi-modal%2520data%252C%2520intricate%2520model%250Aarchitectures%252C%2520and%2520training%2520processes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Mini-Omni2%252C%250Aa%2520visual-audio%2520assistant%2520capable%2520of%2520providing%2520real-time%252C%2520end-to-end%2520voice%250Aresponses%2520to%2520visoin%2520and%2520audio%2520queries.%2520By%2520integrating%2520pretrained%2520visual%2520and%250Aauditory%2520encoders%252C%2520Mini-Omni2%2520maintains%2520performance%2520in%2520individual%2520modalities.%250AWe%2520propose%2520a%2520three-stage%2520training%2520process%2520to%2520align%2520modalities%252C%2520allowing%2520the%250Alanguage%2520model%2520to%2520handle%2520multi-modal%2520inputs%2520and%2520outputs%2520after%2520training%2520on%2520a%250Alimited%2520dataset.%2520For%2520interaction%252C%2520we%2520introduce%2520a%2520command-based%2520interruption%250Amechanism%252C%2520enabling%2520more%2520flexible%2520interaction%2520with%2520users.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520Mini-Omni2%2520is%2520one%2520of%2520the%2520closest%2520reproductions%2520of%2520GPT-4o%252C%2520which%2520have%250Asimilar%2520form%2520of%2520functionality%252C%2520and%2520we%2520hope%2520it%2520can%2520offer%2520valuable%2520insights%2520for%250Asubsequent%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mini-Omni2%3A%20Towards%20Open-source%20GPT-4o%20with%20Vision%2C%20Speech%20and%20Duplex%0A%20%20Capabilities&entry.906535625=Zhifei%20Xie%20and%20Changqiao%20Wu&entry.1292438233=%20%20GPT-4o%2C%20an%20all-encompassing%20model%2C%20represents%20a%20milestone%20in%20the%20development%0Aof%20large%20multi-modal%20language%20models.%20It%20can%20understand%20visual%2C%20auditory%2C%20and%0Atextual%20modalities%2C%20directly%20output%20audio%2C%20and%20support%20flexible%20duplex%0Ainteraction.%20Models%20from%20the%20open-source%20community%20often%20achieve%20some%0Afunctionalities%20of%20GPT-4o%2C%20such%20as%20visual%20understanding%20and%20voice%20chat.%0ANevertheless%2C%20training%20a%20unified%20model%20that%20incorporates%20all%20modalities%20is%0Achallenging%20due%20to%20the%20complexities%20of%20multi-modal%20data%2C%20intricate%20model%0Aarchitectures%2C%20and%20training%20processes.%20In%20this%20paper%2C%20we%20introduce%20Mini-Omni2%2C%0Aa%20visual-audio%20assistant%20capable%20of%20providing%20real-time%2C%20end-to-end%20voice%0Aresponses%20to%20visoin%20and%20audio%20queries.%20By%20integrating%20pretrained%20visual%20and%0Aauditory%20encoders%2C%20Mini-Omni2%20maintains%20performance%20in%20individual%20modalities.%0AWe%20propose%20a%20three-stage%20training%20process%20to%20align%20modalities%2C%20allowing%20the%0Alanguage%20model%20to%20handle%20multi-modal%20inputs%20and%20outputs%20after%20training%20on%20a%0Alimited%20dataset.%20For%20interaction%2C%20we%20introduce%20a%20command-based%20interruption%0Amechanism%2C%20enabling%20more%20flexible%20interaction%20with%20users.%20To%20the%20best%20of%20our%0Aknowledge%2C%20Mini-Omni2%20is%20one%20of%20the%20closest%20reproductions%20of%20GPT-4o%2C%20which%20have%0Asimilar%20form%20of%20functionality%2C%20and%20we%20hope%20it%20can%20offer%20valuable%20insights%20for%0Asubsequent%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11190v2&entry.124074799=Read"},
{"title": "Loss Landscape Characterization of Neural Networks without\n  Over-Parametrziation", "author": "Rustem Islamov and Niccol\u00f2 Ajroldi and Antonio Orvieto and Aurelien Lucchi", "abstract": "  Optimization methods play a crucial role in modern machine learning, powering\nthe remarkable empirical achievements of deep learning models. These successes\nare even more remarkable given the complex non-convex nature of the loss\nlandscape of these models. Yet, ensuring the convergence of optimization\nmethods requires specific structural conditions on the objective function that\nare rarely satisfied in practice. One prominent example is the widely\nrecognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable\nattention in recent years. However, validating such assumptions for deep neural\nnetworks entails substantial and often impractical levels of\nover-parametrization. In order to address this limitation, we propose a novel\nclass of functions that can characterize the loss landscape of modern deep\nmodels without requiring extensive over-parametrization and can also include\nsaddle points. Crucially, we prove that gradient-based optimizers possess\ntheoretical guarantees of convergence under this assumption. Finally, we\nvalidate the soundness of our new function class through both theoretical\nanalysis and empirical experimentation across a diverse range of deep learning\nmodels.\n", "link": "http://arxiv.org/abs/2410.12455v1", "date": "2024-10-16", "relevancy": 2.5882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5179}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loss%20Landscape%20Characterization%20of%20Neural%20Networks%20without%0A%20%20Over-Parametrziation&body=Title%3A%20Loss%20Landscape%20Characterization%20of%20Neural%20Networks%20without%0A%20%20Over-Parametrziation%0AAuthor%3A%20Rustem%20Islamov%20and%20Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Aurelien%20Lucchi%0AAbstract%3A%20%20%20Optimization%20methods%20play%20a%20crucial%20role%20in%20modern%20machine%20learning%2C%20powering%0Athe%20remarkable%20empirical%20achievements%20of%20deep%20learning%20models.%20These%20successes%0Aare%20even%20more%20remarkable%20given%20the%20complex%20non-convex%20nature%20of%20the%20loss%0Alandscape%20of%20these%20models.%20Yet%2C%20ensuring%20the%20convergence%20of%20optimization%0Amethods%20requires%20specific%20structural%20conditions%20on%20the%20objective%20function%20that%0Aare%20rarely%20satisfied%20in%20practice.%20One%20prominent%20example%20is%20the%20widely%0Arecognized%20Polyak-Lojasiewicz%20%28PL%29%20inequality%2C%20which%20has%20gained%20considerable%0Aattention%20in%20recent%20years.%20However%2C%20validating%20such%20assumptions%20for%20deep%20neural%0Anetworks%20entails%20substantial%20and%20often%20impractical%20levels%20of%0Aover-parametrization.%20In%20order%20to%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0Aclass%20of%20functions%20that%20can%20characterize%20the%20loss%20landscape%20of%20modern%20deep%0Amodels%20without%20requiring%20extensive%20over-parametrization%20and%20can%20also%20include%0Asaddle%20points.%20Crucially%2C%20we%20prove%20that%20gradient-based%20optimizers%20possess%0Atheoretical%20guarantees%20of%20convergence%20under%20this%20assumption.%20Finally%2C%20we%0Avalidate%20the%20soundness%20of%20our%20new%20function%20class%20through%20both%20theoretical%0Aanalysis%20and%20empirical%20experimentation%20across%20a%20diverse%20range%20of%20deep%20learning%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoss%2520Landscape%2520Characterization%2520of%2520Neural%2520Networks%2520without%250A%2520%2520Over-Parametrziation%26entry.906535625%3DRustem%2520Islamov%2520and%2520Niccol%25C3%25B2%2520Ajroldi%2520and%2520Antonio%2520Orvieto%2520and%2520Aurelien%2520Lucchi%26entry.1292438233%3D%2520%2520Optimization%2520methods%2520play%2520a%2520crucial%2520role%2520in%2520modern%2520machine%2520learning%252C%2520powering%250Athe%2520remarkable%2520empirical%2520achievements%2520of%2520deep%2520learning%2520models.%2520These%2520successes%250Aare%2520even%2520more%2520remarkable%2520given%2520the%2520complex%2520non-convex%2520nature%2520of%2520the%2520loss%250Alandscape%2520of%2520these%2520models.%2520Yet%252C%2520ensuring%2520the%2520convergence%2520of%2520optimization%250Amethods%2520requires%2520specific%2520structural%2520conditions%2520on%2520the%2520objective%2520function%2520that%250Aare%2520rarely%2520satisfied%2520in%2520practice.%2520One%2520prominent%2520example%2520is%2520the%2520widely%250Arecognized%2520Polyak-Lojasiewicz%2520%2528PL%2529%2520inequality%252C%2520which%2520has%2520gained%2520considerable%250Aattention%2520in%2520recent%2520years.%2520However%252C%2520validating%2520such%2520assumptions%2520for%2520deep%2520neural%250Anetworks%2520entails%2520substantial%2520and%2520often%2520impractical%2520levels%2520of%250Aover-parametrization.%2520In%2520order%2520to%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%250Aclass%2520of%2520functions%2520that%2520can%2520characterize%2520the%2520loss%2520landscape%2520of%2520modern%2520deep%250Amodels%2520without%2520requiring%2520extensive%2520over-parametrization%2520and%2520can%2520also%2520include%250Asaddle%2520points.%2520Crucially%252C%2520we%2520prove%2520that%2520gradient-based%2520optimizers%2520possess%250Atheoretical%2520guarantees%2520of%2520convergence%2520under%2520this%2520assumption.%2520Finally%252C%2520we%250Avalidate%2520the%2520soundness%2520of%2520our%2520new%2520function%2520class%2520through%2520both%2520theoretical%250Aanalysis%2520and%2520empirical%2520experimentation%2520across%2520a%2520diverse%2520range%2520of%2520deep%2520learning%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loss%20Landscape%20Characterization%20of%20Neural%20Networks%20without%0A%20%20Over-Parametrziation&entry.906535625=Rustem%20Islamov%20and%20Niccol%C3%B2%20Ajroldi%20and%20Antonio%20Orvieto%20and%20Aurelien%20Lucchi&entry.1292438233=%20%20Optimization%20methods%20play%20a%20crucial%20role%20in%20modern%20machine%20learning%2C%20powering%0Athe%20remarkable%20empirical%20achievements%20of%20deep%20learning%20models.%20These%20successes%0Aare%20even%20more%20remarkable%20given%20the%20complex%20non-convex%20nature%20of%20the%20loss%0Alandscape%20of%20these%20models.%20Yet%2C%20ensuring%20the%20convergence%20of%20optimization%0Amethods%20requires%20specific%20structural%20conditions%20on%20the%20objective%20function%20that%0Aare%20rarely%20satisfied%20in%20practice.%20One%20prominent%20example%20is%20the%20widely%0Arecognized%20Polyak-Lojasiewicz%20%28PL%29%20inequality%2C%20which%20has%20gained%20considerable%0Aattention%20in%20recent%20years.%20However%2C%20validating%20such%20assumptions%20for%20deep%20neural%0Anetworks%20entails%20substantial%20and%20often%20impractical%20levels%20of%0Aover-parametrization.%20In%20order%20to%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0Aclass%20of%20functions%20that%20can%20characterize%20the%20loss%20landscape%20of%20modern%20deep%0Amodels%20without%20requiring%20extensive%20over-parametrization%20and%20can%20also%20include%0Asaddle%20points.%20Crucially%2C%20we%20prove%20that%20gradient-based%20optimizers%20possess%0Atheoretical%20guarantees%20of%20convergence%20under%20this%20assumption.%20Finally%2C%20we%0Avalidate%20the%20soundness%20of%20our%20new%20function%20class%20through%20both%20theoretical%0Aanalysis%20and%20empirical%20experimentation%20across%20a%20diverse%20range%20of%20deep%20learning%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12455v1&entry.124074799=Read"},
{"title": "Towards Zero-Shot Camera Trap Image Categorization", "author": "Ji\u0159\u00ed Vysko\u010dil and Lukas Picek", "abstract": "  This paper describes the search for an alternative approach to the automatic\ncategorization of camera trap images. First, we benchmark state-of-the-art\nclassifiers using a single model for all images. Next, we evaluate methods\ncombining MegaDetector with one or more classifiers and Segment Anything to\nassess their impact on reducing location-specific overfitting. Last, we propose\nand test two approaches using large language and foundational models, such as\nDINOv2, BioCLIP, BLIP, and ChatGPT, in a zero-shot scenario. Evaluation carried\nout on two publicly available datasets (WCT from New Zealand, CCT20 from the\nSouthwestern US) and a private dataset (CEF from Central Europe) revealed that\ncombining MegaDetector with two separate classifiers achieves the highest\naccuracy. This approach reduced the relative error of a single BEiTV2\nclassifier by approximately 42\\% on CCT20, 48\\% on CEF, and 75\\% on WCT.\nBesides, as the background is removed, the error in terms of accuracy in new\nlocations is reduced to half. The proposed zero-shot pipeline based on DINOv2\nand FAISS achieved competitive results (1.0\\% and 4.7\\% smaller on CCT20, and\nCEF, respectively), which highlights the potential of zero-shot approaches for\ncamera trap image categorization.\n", "link": "http://arxiv.org/abs/2410.12769v1", "date": "2024-10-16", "relevancy": 2.5849, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5325}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5219}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Zero-Shot%20Camera%20Trap%20Image%20Categorization&body=Title%3A%20Towards%20Zero-Shot%20Camera%20Trap%20Image%20Categorization%0AAuthor%3A%20Ji%C5%99%C3%AD%20Vysko%C4%8Dil%20and%20Lukas%20Picek%0AAbstract%3A%20%20%20This%20paper%20describes%20the%20search%20for%20an%20alternative%20approach%20to%20the%20automatic%0Acategorization%20of%20camera%20trap%20images.%20First%2C%20we%20benchmark%20state-of-the-art%0Aclassifiers%20using%20a%20single%20model%20for%20all%20images.%20Next%2C%20we%20evaluate%20methods%0Acombining%20MegaDetector%20with%20one%20or%20more%20classifiers%20and%20Segment%20Anything%20to%0Aassess%20their%20impact%20on%20reducing%20location-specific%20overfitting.%20Last%2C%20we%20propose%0Aand%20test%20two%20approaches%20using%20large%20language%20and%20foundational%20models%2C%20such%20as%0ADINOv2%2C%20BioCLIP%2C%20BLIP%2C%20and%20ChatGPT%2C%20in%20a%20zero-shot%20scenario.%20Evaluation%20carried%0Aout%20on%20two%20publicly%20available%20datasets%20%28WCT%20from%20New%20Zealand%2C%20CCT20%20from%20the%0ASouthwestern%20US%29%20and%20a%20private%20dataset%20%28CEF%20from%20Central%20Europe%29%20revealed%20that%0Acombining%20MegaDetector%20with%20two%20separate%20classifiers%20achieves%20the%20highest%0Aaccuracy.%20This%20approach%20reduced%20the%20relative%20error%20of%20a%20single%20BEiTV2%0Aclassifier%20by%20approximately%2042%5C%25%20on%20CCT20%2C%2048%5C%25%20on%20CEF%2C%20and%2075%5C%25%20on%20WCT.%0ABesides%2C%20as%20the%20background%20is%20removed%2C%20the%20error%20in%20terms%20of%20accuracy%20in%20new%0Alocations%20is%20reduced%20to%20half.%20The%20proposed%20zero-shot%20pipeline%20based%20on%20DINOv2%0Aand%20FAISS%20achieved%20competitive%20results%20%281.0%5C%25%20and%204.7%5C%25%20smaller%20on%20CCT20%2C%20and%0ACEF%2C%20respectively%29%2C%20which%20highlights%20the%20potential%20of%20zero-shot%20approaches%20for%0Acamera%20trap%20image%20categorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Zero-Shot%2520Camera%2520Trap%2520Image%2520Categorization%26entry.906535625%3DJi%25C5%2599%25C3%25AD%2520Vysko%25C4%258Dil%2520and%2520Lukas%2520Picek%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520the%2520search%2520for%2520an%2520alternative%2520approach%2520to%2520the%2520automatic%250Acategorization%2520of%2520camera%2520trap%2520images.%2520First%252C%2520we%2520benchmark%2520state-of-the-art%250Aclassifiers%2520using%2520a%2520single%2520model%2520for%2520all%2520images.%2520Next%252C%2520we%2520evaluate%2520methods%250Acombining%2520MegaDetector%2520with%2520one%2520or%2520more%2520classifiers%2520and%2520Segment%2520Anything%2520to%250Aassess%2520their%2520impact%2520on%2520reducing%2520location-specific%2520overfitting.%2520Last%252C%2520we%2520propose%250Aand%2520test%2520two%2520approaches%2520using%2520large%2520language%2520and%2520foundational%2520models%252C%2520such%2520as%250ADINOv2%252C%2520BioCLIP%252C%2520BLIP%252C%2520and%2520ChatGPT%252C%2520in%2520a%2520zero-shot%2520scenario.%2520Evaluation%2520carried%250Aout%2520on%2520two%2520publicly%2520available%2520datasets%2520%2528WCT%2520from%2520New%2520Zealand%252C%2520CCT20%2520from%2520the%250ASouthwestern%2520US%2529%2520and%2520a%2520private%2520dataset%2520%2528CEF%2520from%2520Central%2520Europe%2529%2520revealed%2520that%250Acombining%2520MegaDetector%2520with%2520two%2520separate%2520classifiers%2520achieves%2520the%2520highest%250Aaccuracy.%2520This%2520approach%2520reduced%2520the%2520relative%2520error%2520of%2520a%2520single%2520BEiTV2%250Aclassifier%2520by%2520approximately%252042%255C%2525%2520on%2520CCT20%252C%252048%255C%2525%2520on%2520CEF%252C%2520and%252075%255C%2525%2520on%2520WCT.%250ABesides%252C%2520as%2520the%2520background%2520is%2520removed%252C%2520the%2520error%2520in%2520terms%2520of%2520accuracy%2520in%2520new%250Alocations%2520is%2520reduced%2520to%2520half.%2520The%2520proposed%2520zero-shot%2520pipeline%2520based%2520on%2520DINOv2%250Aand%2520FAISS%2520achieved%2520competitive%2520results%2520%25281.0%255C%2525%2520and%25204.7%255C%2525%2520smaller%2520on%2520CCT20%252C%2520and%250ACEF%252C%2520respectively%2529%252C%2520which%2520highlights%2520the%2520potential%2520of%2520zero-shot%2520approaches%2520for%250Acamera%2520trap%2520image%2520categorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Zero-Shot%20Camera%20Trap%20Image%20Categorization&entry.906535625=Ji%C5%99%C3%AD%20Vysko%C4%8Dil%20and%20Lukas%20Picek&entry.1292438233=%20%20This%20paper%20describes%20the%20search%20for%20an%20alternative%20approach%20to%20the%20automatic%0Acategorization%20of%20camera%20trap%20images.%20First%2C%20we%20benchmark%20state-of-the-art%0Aclassifiers%20using%20a%20single%20model%20for%20all%20images.%20Next%2C%20we%20evaluate%20methods%0Acombining%20MegaDetector%20with%20one%20or%20more%20classifiers%20and%20Segment%20Anything%20to%0Aassess%20their%20impact%20on%20reducing%20location-specific%20overfitting.%20Last%2C%20we%20propose%0Aand%20test%20two%20approaches%20using%20large%20language%20and%20foundational%20models%2C%20such%20as%0ADINOv2%2C%20BioCLIP%2C%20BLIP%2C%20and%20ChatGPT%2C%20in%20a%20zero-shot%20scenario.%20Evaluation%20carried%0Aout%20on%20two%20publicly%20available%20datasets%20%28WCT%20from%20New%20Zealand%2C%20CCT20%20from%20the%0ASouthwestern%20US%29%20and%20a%20private%20dataset%20%28CEF%20from%20Central%20Europe%29%20revealed%20that%0Acombining%20MegaDetector%20with%20two%20separate%20classifiers%20achieves%20the%20highest%0Aaccuracy.%20This%20approach%20reduced%20the%20relative%20error%20of%20a%20single%20BEiTV2%0Aclassifier%20by%20approximately%2042%5C%25%20on%20CCT20%2C%2048%5C%25%20on%20CEF%2C%20and%2075%5C%25%20on%20WCT.%0ABesides%2C%20as%20the%20background%20is%20removed%2C%20the%20error%20in%20terms%20of%20accuracy%20in%20new%0Alocations%20is%20reduced%20to%20half.%20The%20proposed%20zero-shot%20pipeline%20based%20on%20DINOv2%0Aand%20FAISS%20achieved%20competitive%20results%20%281.0%5C%25%20and%204.7%5C%25%20smaller%20on%20CCT20%2C%20and%0ACEF%2C%20respectively%29%2C%20which%20highlights%20the%20potential%20of%20zero-shot%20approaches%20for%0Acamera%20trap%20image%20categorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12769v1&entry.124074799=Read"},
{"title": "Sharing to learn and learning to share; Fitting together Meta-Learning,\n  Multi-Task Learning, and Transfer Learning: A meta review", "author": "Richa Upadhyay and Ronald Phlypo and Rajkumar Saini and Marcus Liwicki", "abstract": "  Integrating knowledge across different domains is an essential feature of\nhuman learning. Learning paradigms such as transfer learning, meta-learning,\nand multi-task learning reflect the human learning process by exploiting the\nprior knowledge for new tasks, encouraging faster learning and good\ngeneralization for new tasks. This article gives a detailed view of these\nlearning paradigms and their comparative analysis. The weakness of one learning\nalgorithm turns out to be a strength of another, and thus, merging them is a\nprevalent trait in the literature. Numerous research papers focus on each of\nthese learning paradigms separately and provide a comprehensive overview of\nthem. However, this article reviews research studies that combine (two of)\nthese learning algorithms. This survey describes how these techniques are\ncombined to solve problems in many different fields of research, including\ncomputer vision, natural language processing, hyper-spectral imaging, and many\nmore, in a supervised setting only. Based on the knowledge accumulated from the\nliterature, we hypothesize a generic task-agnostic and model-agnostic learning\nnetwork - an ensemble of meta-learning, transfer learning, and multi-task\nlearning, termed Multi-modal Multi-task Meta Transfer Learning. We also present\nsome open research questions, limitations, and future research directions for\nthis proposed network. The aim of this article is to spark interest among\nscholars in effectively merging existing learning algorithms with the intention\nof advancing research in this field. Instead of presenting experimental\nresults, we invite readers to explore and contemplate techniques for merging\nalgorithms while navigating through their limitations.\n", "link": "http://arxiv.org/abs/2111.12146v8", "date": "2024-10-16", "relevancy": 2.5832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20to%20learn%20and%20learning%20to%20share%3B%20Fitting%20together%20Meta-Learning%2C%0A%20%20Multi-Task%20Learning%2C%20and%20Transfer%20Learning%3A%20A%20meta%20review&body=Title%3A%20Sharing%20to%20learn%20and%20learning%20to%20share%3B%20Fitting%20together%20Meta-Learning%2C%0A%20%20Multi-Task%20Learning%2C%20and%20Transfer%20Learning%3A%20A%20meta%20review%0AAuthor%3A%20Richa%20Upadhyay%20and%20Ronald%20Phlypo%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki%0AAbstract%3A%20%20%20Integrating%20knowledge%20across%20different%20domains%20is%20an%20essential%20feature%20of%0Ahuman%20learning.%20Learning%20paradigms%20such%20as%20transfer%20learning%2C%20meta-learning%2C%0Aand%20multi-task%20learning%20reflect%20the%20human%20learning%20process%20by%20exploiting%20the%0Aprior%20knowledge%20for%20new%20tasks%2C%20encouraging%20faster%20learning%20and%20good%0Ageneralization%20for%20new%20tasks.%20This%20article%20gives%20a%20detailed%20view%20of%20these%0Alearning%20paradigms%20and%20their%20comparative%20analysis.%20The%20weakness%20of%20one%20learning%0Aalgorithm%20turns%20out%20to%20be%20a%20strength%20of%20another%2C%20and%20thus%2C%20merging%20them%20is%20a%0Aprevalent%20trait%20in%20the%20literature.%20Numerous%20research%20papers%20focus%20on%20each%20of%0Athese%20learning%20paradigms%20separately%20and%20provide%20a%20comprehensive%20overview%20of%0Athem.%20However%2C%20this%20article%20reviews%20research%20studies%20that%20combine%20%28two%20of%29%0Athese%20learning%20algorithms.%20This%20survey%20describes%20how%20these%20techniques%20are%0Acombined%20to%20solve%20problems%20in%20many%20different%20fields%20of%20research%2C%20including%0Acomputer%20vision%2C%20natural%20language%20processing%2C%20hyper-spectral%20imaging%2C%20and%20many%0Amore%2C%20in%20a%20supervised%20setting%20only.%20Based%20on%20the%20knowledge%20accumulated%20from%20the%0Aliterature%2C%20we%20hypothesize%20a%20generic%20task-agnostic%20and%20model-agnostic%20learning%0Anetwork%20-%20an%20ensemble%20of%20meta-learning%2C%20transfer%20learning%2C%20and%20multi-task%0Alearning%2C%20termed%20Multi-modal%20Multi-task%20Meta%20Transfer%20Learning.%20We%20also%20present%0Asome%20open%20research%20questions%2C%20limitations%2C%20and%20future%20research%20directions%20for%0Athis%20proposed%20network.%20The%20aim%20of%20this%20article%20is%20to%20spark%20interest%20among%0Ascholars%20in%20effectively%20merging%20existing%20learning%20algorithms%20with%20the%20intention%0Aof%20advancing%20research%20in%20this%20field.%20Instead%20of%20presenting%20experimental%0Aresults%2C%20we%20invite%20readers%20to%20explore%20and%20contemplate%20techniques%20for%20merging%0Aalgorithms%20while%20navigating%20through%20their%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.12146v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520to%2520learn%2520and%2520learning%2520to%2520share%253B%2520Fitting%2520together%2520Meta-Learning%252C%250A%2520%2520Multi-Task%2520Learning%252C%2520and%2520Transfer%2520Learning%253A%2520A%2520meta%2520review%26entry.906535625%3DRicha%2520Upadhyay%2520and%2520Ronald%2520Phlypo%2520and%2520Rajkumar%2520Saini%2520and%2520Marcus%2520Liwicki%26entry.1292438233%3D%2520%2520Integrating%2520knowledge%2520across%2520different%2520domains%2520is%2520an%2520essential%2520feature%2520of%250Ahuman%2520learning.%2520Learning%2520paradigms%2520such%2520as%2520transfer%2520learning%252C%2520meta-learning%252C%250Aand%2520multi-task%2520learning%2520reflect%2520the%2520human%2520learning%2520process%2520by%2520exploiting%2520the%250Aprior%2520knowledge%2520for%2520new%2520tasks%252C%2520encouraging%2520faster%2520learning%2520and%2520good%250Ageneralization%2520for%2520new%2520tasks.%2520This%2520article%2520gives%2520a%2520detailed%2520view%2520of%2520these%250Alearning%2520paradigms%2520and%2520their%2520comparative%2520analysis.%2520The%2520weakness%2520of%2520one%2520learning%250Aalgorithm%2520turns%2520out%2520to%2520be%2520a%2520strength%2520of%2520another%252C%2520and%2520thus%252C%2520merging%2520them%2520is%2520a%250Aprevalent%2520trait%2520in%2520the%2520literature.%2520Numerous%2520research%2520papers%2520focus%2520on%2520each%2520of%250Athese%2520learning%2520paradigms%2520separately%2520and%2520provide%2520a%2520comprehensive%2520overview%2520of%250Athem.%2520However%252C%2520this%2520article%2520reviews%2520research%2520studies%2520that%2520combine%2520%2528two%2520of%2529%250Athese%2520learning%2520algorithms.%2520This%2520survey%2520describes%2520how%2520these%2520techniques%2520are%250Acombined%2520to%2520solve%2520problems%2520in%2520many%2520different%2520fields%2520of%2520research%252C%2520including%250Acomputer%2520vision%252C%2520natural%2520language%2520processing%252C%2520hyper-spectral%2520imaging%252C%2520and%2520many%250Amore%252C%2520in%2520a%2520supervised%2520setting%2520only.%2520Based%2520on%2520the%2520knowledge%2520accumulated%2520from%2520the%250Aliterature%252C%2520we%2520hypothesize%2520a%2520generic%2520task-agnostic%2520and%2520model-agnostic%2520learning%250Anetwork%2520-%2520an%2520ensemble%2520of%2520meta-learning%252C%2520transfer%2520learning%252C%2520and%2520multi-task%250Alearning%252C%2520termed%2520Multi-modal%2520Multi-task%2520Meta%2520Transfer%2520Learning.%2520We%2520also%2520present%250Asome%2520open%2520research%2520questions%252C%2520limitations%252C%2520and%2520future%2520research%2520directions%2520for%250Athis%2520proposed%2520network.%2520The%2520aim%2520of%2520this%2520article%2520is%2520to%2520spark%2520interest%2520among%250Ascholars%2520in%2520effectively%2520merging%2520existing%2520learning%2520algorithms%2520with%2520the%2520intention%250Aof%2520advancing%2520research%2520in%2520this%2520field.%2520Instead%2520of%2520presenting%2520experimental%250Aresults%252C%2520we%2520invite%2520readers%2520to%2520explore%2520and%2520contemplate%2520techniques%2520for%2520merging%250Aalgorithms%2520while%2520navigating%2520through%2520their%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2111.12146v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20to%20learn%20and%20learning%20to%20share%3B%20Fitting%20together%20Meta-Learning%2C%0A%20%20Multi-Task%20Learning%2C%20and%20Transfer%20Learning%3A%20A%20meta%20review&entry.906535625=Richa%20Upadhyay%20and%20Ronald%20Phlypo%20and%20Rajkumar%20Saini%20and%20Marcus%20Liwicki&entry.1292438233=%20%20Integrating%20knowledge%20across%20different%20domains%20is%20an%20essential%20feature%20of%0Ahuman%20learning.%20Learning%20paradigms%20such%20as%20transfer%20learning%2C%20meta-learning%2C%0Aand%20multi-task%20learning%20reflect%20the%20human%20learning%20process%20by%20exploiting%20the%0Aprior%20knowledge%20for%20new%20tasks%2C%20encouraging%20faster%20learning%20and%20good%0Ageneralization%20for%20new%20tasks.%20This%20article%20gives%20a%20detailed%20view%20of%20these%0Alearning%20paradigms%20and%20their%20comparative%20analysis.%20The%20weakness%20of%20one%20learning%0Aalgorithm%20turns%20out%20to%20be%20a%20strength%20of%20another%2C%20and%20thus%2C%20merging%20them%20is%20a%0Aprevalent%20trait%20in%20the%20literature.%20Numerous%20research%20papers%20focus%20on%20each%20of%0Athese%20learning%20paradigms%20separately%20and%20provide%20a%20comprehensive%20overview%20of%0Athem.%20However%2C%20this%20article%20reviews%20research%20studies%20that%20combine%20%28two%20of%29%0Athese%20learning%20algorithms.%20This%20survey%20describes%20how%20these%20techniques%20are%0Acombined%20to%20solve%20problems%20in%20many%20different%20fields%20of%20research%2C%20including%0Acomputer%20vision%2C%20natural%20language%20processing%2C%20hyper-spectral%20imaging%2C%20and%20many%0Amore%2C%20in%20a%20supervised%20setting%20only.%20Based%20on%20the%20knowledge%20accumulated%20from%20the%0Aliterature%2C%20we%20hypothesize%20a%20generic%20task-agnostic%20and%20model-agnostic%20learning%0Anetwork%20-%20an%20ensemble%20of%20meta-learning%2C%20transfer%20learning%2C%20and%20multi-task%0Alearning%2C%20termed%20Multi-modal%20Multi-task%20Meta%20Transfer%20Learning.%20We%20also%20present%0Asome%20open%20research%20questions%2C%20limitations%2C%20and%20future%20research%20directions%20for%0Athis%20proposed%20network.%20The%20aim%20of%20this%20article%20is%20to%20spark%20interest%20among%0Ascholars%20in%20effectively%20merging%20existing%20learning%20algorithms%20with%20the%20intention%0Aof%20advancing%20research%20in%20this%20field.%20Instead%20of%20presenting%20experimental%0Aresults%2C%20we%20invite%20readers%20to%20explore%20and%20contemplate%20techniques%20for%20merging%0Aalgorithms%20while%20navigating%20through%20their%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.12146v8&entry.124074799=Read"},
{"title": "DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning", "author": "Jiabao Wei and Zhiyuan Ma", "abstract": "  Virtual Try-ON (VTON) aims to synthesis specific person images dressed in\ngiven garments, which recently receives numerous attention in online shopping\nscenarios. Currently, the core challenges of the VTON task mainly lie in the\nfine-grained semantic extraction (i.e.,deep semantics) of the given reference\ngarments during depth estimation and effective texture preservation when the\ngarments are synthesized and warped onto human body. To cope with these issues,\nwe propose DH-VTON, a deep text-driven virtual try-on model featuring a special\nhybrid attention learning strategy and deep garment semantic preservation\nmodule. By standing on the shoulder of a well-built pre-trained\npaint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this\nwork. Specifically, to extract the deep semantics of the garments, we first\nintroduce InternViT-6B as fine-grained feature learner, which can be trained to\nalign with the large-scale intrinsic knowledge with deep text semantics\n(e.g.,\"neckline\" or \"girdle\") to make up for the deficiency of the commonly\nadopted CLIP encoder. Based on this, to enhance the customized dressing\nabilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+)\nmodule and propose to leverage a fresh hybrid attention strategy for training,\nwhich can adaptively integrate fine-grained characteristics of the garments\ninto the different layers of the VTON model, so as to achieve multi-scale\nfeatures preservation effects. Extensive experiments on several representative\ndatasets demonstrate that our method outperforms previous diffusion-based and\nGAN-based approaches, showing competitive performance in preserving garment\ndetails and generating authentic human images.\n", "link": "http://arxiv.org/abs/2410.12501v1", "date": "2024-10-16", "relevancy": 2.5578, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6441}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6428}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DH-VTON%3A%20Deep%20Text-Driven%20Virtual%20Try-On%20via%20Hybrid%20Attention%20Learning&body=Title%3A%20DH-VTON%3A%20Deep%20Text-Driven%20Virtual%20Try-On%20via%20Hybrid%20Attention%20Learning%0AAuthor%3A%20Jiabao%20Wei%20and%20Zhiyuan%20Ma%0AAbstract%3A%20%20%20Virtual%20Try-ON%20%28VTON%29%20aims%20to%20synthesis%20specific%20person%20images%20dressed%20in%0Agiven%20garments%2C%20which%20recently%20receives%20numerous%20attention%20in%20online%20shopping%0Ascenarios.%20Currently%2C%20the%20core%20challenges%20of%20the%20VTON%20task%20mainly%20lie%20in%20the%0Afine-grained%20semantic%20extraction%20%28i.e.%2Cdeep%20semantics%29%20of%20the%20given%20reference%0Agarments%20during%20depth%20estimation%20and%20effective%20texture%20preservation%20when%20the%0Agarments%20are%20synthesized%20and%20warped%20onto%20human%20body.%20To%20cope%20with%20these%20issues%2C%0Awe%20propose%20DH-VTON%2C%20a%20deep%20text-driven%20virtual%20try-on%20model%20featuring%20a%20special%0Ahybrid%20attention%20learning%20strategy%20and%20deep%20garment%20semantic%20preservation%0Amodule.%20By%20standing%20on%20the%20shoulder%20of%20a%20well-built%20pre-trained%0Apaint-by-example%20%28abbr.%20PBE%29%20approach%2C%20we%20present%20our%20DH-VTON%20pipeline%20in%20this%0Awork.%20Specifically%2C%20to%20extract%20the%20deep%20semantics%20of%20the%20garments%2C%20we%20first%0Aintroduce%20InternViT-6B%20as%20fine-grained%20feature%20learner%2C%20which%20can%20be%20trained%20to%0Aalign%20with%20the%20large-scale%20intrinsic%20knowledge%20with%20deep%20text%20semantics%0A%28e.g.%2C%22neckline%22%20or%20%22girdle%22%29%20to%20make%20up%20for%20the%20deficiency%20of%20the%20commonly%0Aadopted%20CLIP%20encoder.%20Based%20on%20this%2C%20to%20enhance%20the%20customized%20dressing%0Aabilities%2C%20we%20further%20introduce%20Garment-Feature%20ControlNet%20Plus%20%28abbr.%20GFC%2B%29%0Amodule%20and%20propose%20to%20leverage%20a%20fresh%20hybrid%20attention%20strategy%20for%20training%2C%0Awhich%20can%20adaptively%20integrate%20fine-grained%20characteristics%20of%20the%20garments%0Ainto%20the%20different%20layers%20of%20the%20VTON%20model%2C%20so%20as%20to%20achieve%20multi-scale%0Afeatures%20preservation%20effects.%20Extensive%20experiments%20on%20several%20representative%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20previous%20diffusion-based%20and%0AGAN-based%20approaches%2C%20showing%20competitive%20performance%20in%20preserving%20garment%0Adetails%20and%20generating%20authentic%20human%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDH-VTON%253A%2520Deep%2520Text-Driven%2520Virtual%2520Try-On%2520via%2520Hybrid%2520Attention%2520Learning%26entry.906535625%3DJiabao%2520Wei%2520and%2520Zhiyuan%2520Ma%26entry.1292438233%3D%2520%2520Virtual%2520Try-ON%2520%2528VTON%2529%2520aims%2520to%2520synthesis%2520specific%2520person%2520images%2520dressed%2520in%250Agiven%2520garments%252C%2520which%2520recently%2520receives%2520numerous%2520attention%2520in%2520online%2520shopping%250Ascenarios.%2520Currently%252C%2520the%2520core%2520challenges%2520of%2520the%2520VTON%2520task%2520mainly%2520lie%2520in%2520the%250Afine-grained%2520semantic%2520extraction%2520%2528i.e.%252Cdeep%2520semantics%2529%2520of%2520the%2520given%2520reference%250Agarments%2520during%2520depth%2520estimation%2520and%2520effective%2520texture%2520preservation%2520when%2520the%250Agarments%2520are%2520synthesized%2520and%2520warped%2520onto%2520human%2520body.%2520To%2520cope%2520with%2520these%2520issues%252C%250Awe%2520propose%2520DH-VTON%252C%2520a%2520deep%2520text-driven%2520virtual%2520try-on%2520model%2520featuring%2520a%2520special%250Ahybrid%2520attention%2520learning%2520strategy%2520and%2520deep%2520garment%2520semantic%2520preservation%250Amodule.%2520By%2520standing%2520on%2520the%2520shoulder%2520of%2520a%2520well-built%2520pre-trained%250Apaint-by-example%2520%2528abbr.%2520PBE%2529%2520approach%252C%2520we%2520present%2520our%2520DH-VTON%2520pipeline%2520in%2520this%250Awork.%2520Specifically%252C%2520to%2520extract%2520the%2520deep%2520semantics%2520of%2520the%2520garments%252C%2520we%2520first%250Aintroduce%2520InternViT-6B%2520as%2520fine-grained%2520feature%2520learner%252C%2520which%2520can%2520be%2520trained%2520to%250Aalign%2520with%2520the%2520large-scale%2520intrinsic%2520knowledge%2520with%2520deep%2520text%2520semantics%250A%2528e.g.%252C%2522neckline%2522%2520or%2520%2522girdle%2522%2529%2520to%2520make%2520up%2520for%2520the%2520deficiency%2520of%2520the%2520commonly%250Aadopted%2520CLIP%2520encoder.%2520Based%2520on%2520this%252C%2520to%2520enhance%2520the%2520customized%2520dressing%250Aabilities%252C%2520we%2520further%2520introduce%2520Garment-Feature%2520ControlNet%2520Plus%2520%2528abbr.%2520GFC%252B%2529%250Amodule%2520and%2520propose%2520to%2520leverage%2520a%2520fresh%2520hybrid%2520attention%2520strategy%2520for%2520training%252C%250Awhich%2520can%2520adaptively%2520integrate%2520fine-grained%2520characteristics%2520of%2520the%2520garments%250Ainto%2520the%2520different%2520layers%2520of%2520the%2520VTON%2520model%252C%2520so%2520as%2520to%2520achieve%2520multi-scale%250Afeatures%2520preservation%2520effects.%2520Extensive%2520experiments%2520on%2520several%2520representative%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520previous%2520diffusion-based%2520and%250AGAN-based%2520approaches%252C%2520showing%2520competitive%2520performance%2520in%2520preserving%2520garment%250Adetails%2520and%2520generating%2520authentic%2520human%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DH-VTON%3A%20Deep%20Text-Driven%20Virtual%20Try-On%20via%20Hybrid%20Attention%20Learning&entry.906535625=Jiabao%20Wei%20and%20Zhiyuan%20Ma&entry.1292438233=%20%20Virtual%20Try-ON%20%28VTON%29%20aims%20to%20synthesis%20specific%20person%20images%20dressed%20in%0Agiven%20garments%2C%20which%20recently%20receives%20numerous%20attention%20in%20online%20shopping%0Ascenarios.%20Currently%2C%20the%20core%20challenges%20of%20the%20VTON%20task%20mainly%20lie%20in%20the%0Afine-grained%20semantic%20extraction%20%28i.e.%2Cdeep%20semantics%29%20of%20the%20given%20reference%0Agarments%20during%20depth%20estimation%20and%20effective%20texture%20preservation%20when%20the%0Agarments%20are%20synthesized%20and%20warped%20onto%20human%20body.%20To%20cope%20with%20these%20issues%2C%0Awe%20propose%20DH-VTON%2C%20a%20deep%20text-driven%20virtual%20try-on%20model%20featuring%20a%20special%0Ahybrid%20attention%20learning%20strategy%20and%20deep%20garment%20semantic%20preservation%0Amodule.%20By%20standing%20on%20the%20shoulder%20of%20a%20well-built%20pre-trained%0Apaint-by-example%20%28abbr.%20PBE%29%20approach%2C%20we%20present%20our%20DH-VTON%20pipeline%20in%20this%0Awork.%20Specifically%2C%20to%20extract%20the%20deep%20semantics%20of%20the%20garments%2C%20we%20first%0Aintroduce%20InternViT-6B%20as%20fine-grained%20feature%20learner%2C%20which%20can%20be%20trained%20to%0Aalign%20with%20the%20large-scale%20intrinsic%20knowledge%20with%20deep%20text%20semantics%0A%28e.g.%2C%22neckline%22%20or%20%22girdle%22%29%20to%20make%20up%20for%20the%20deficiency%20of%20the%20commonly%0Aadopted%20CLIP%20encoder.%20Based%20on%20this%2C%20to%20enhance%20the%20customized%20dressing%0Aabilities%2C%20we%20further%20introduce%20Garment-Feature%20ControlNet%20Plus%20%28abbr.%20GFC%2B%29%0Amodule%20and%20propose%20to%20leverage%20a%20fresh%20hybrid%20attention%20strategy%20for%20training%2C%0Awhich%20can%20adaptively%20integrate%20fine-grained%20characteristics%20of%20the%20garments%0Ainto%20the%20different%20layers%20of%20the%20VTON%20model%2C%20so%20as%20to%20achieve%20multi-scale%0Afeatures%20preservation%20effects.%20Extensive%20experiments%20on%20several%20representative%0Adatasets%20demonstrate%20that%20our%20method%20outperforms%20previous%20diffusion-based%20and%0AGAN-based%20approaches%2C%20showing%20competitive%20performance%20in%20preserving%20garment%0Adetails%20and%20generating%20authentic%20human%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12501v1&entry.124074799=Read"},
{"title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free\n  Way", "author": "Jiazi Bu and Pengyang Ling and Pan Zhang and Tong Wu and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  The text-to-video (T2V) generation models, offering convenient visual\ncreation, have recently garnered increasing attention. Despite their\nsubstantial potential, the generated videos may present artifacts, including\nstructural implausibility, temporal inconsistency, and a lack of motion, often\nresulting in near-static video. In this work, we have identified a correlation\nbetween the disparity of temporal attention maps across different blocks and\nthe occurrence of temporal inconsistencies. Additionally, we have observed that\nthe energy contained within the temporal attention maps is directly related to\nthe magnitude of motion amplitude in the generated videos. Based on these\nobservations, we present BroadWay, a training-free method to improve the\nquality of text-to-video generation without introducing additional parameters,\naugmenting memory or sampling time. Specifically, BroadWay is composed of two\nprincipal components: 1) Temporal Self-Guidance improves the structural\nplausibility and temporal consistency of generated videos by reducing the\ndisparity between the temporal attention maps across various decoder blocks. 2)\nFourier-based Motion Enhancement enhances the magnitude and richness of motion\nby amplifying the energy of the map. Extensive experiments demonstrate that\nBroadWay significantly improves the quality of text-to-video generation with\nnegligible additional cost.\n", "link": "http://arxiv.org/abs/2410.06241v2", "date": "2024-10-16", "relevancy": 2.5289, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.664}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6278}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BroadWay%3A%20Boost%20Your%20Text-to-Video%20Generation%20Model%20in%20a%20Training-free%0A%20%20Way&body=Title%3A%20BroadWay%3A%20Boost%20Your%20Text-to-Video%20Generation%20Model%20in%20a%20Training-free%0A%20%20Way%0AAuthor%3A%20Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20The%20text-to-video%20%28T2V%29%20generation%20models%2C%20offering%20convenient%20visual%0Acreation%2C%20have%20recently%20garnered%20increasing%20attention.%20Despite%20their%0Asubstantial%20potential%2C%20the%20generated%20videos%20may%20present%20artifacts%2C%20including%0Astructural%20implausibility%2C%20temporal%20inconsistency%2C%20and%20a%20lack%20of%20motion%2C%20often%0Aresulting%20in%20near-static%20video.%20In%20this%20work%2C%20we%20have%20identified%20a%20correlation%0Abetween%20the%20disparity%20of%20temporal%20attention%20maps%20across%20different%20blocks%20and%0Athe%20occurrence%20of%20temporal%20inconsistencies.%20Additionally%2C%20we%20have%20observed%20that%0Athe%20energy%20contained%20within%20the%20temporal%20attention%20maps%20is%20directly%20related%20to%0Athe%20magnitude%20of%20motion%20amplitude%20in%20the%20generated%20videos.%20Based%20on%20these%0Aobservations%2C%20we%20present%20BroadWay%2C%20a%20training-free%20method%20to%20improve%20the%0Aquality%20of%20text-to-video%20generation%20without%20introducing%20additional%20parameters%2C%0Aaugmenting%20memory%20or%20sampling%20time.%20Specifically%2C%20BroadWay%20is%20composed%20of%20two%0Aprincipal%20components%3A%201%29%20Temporal%20Self-Guidance%20improves%20the%20structural%0Aplausibility%20and%20temporal%20consistency%20of%20generated%20videos%20by%20reducing%20the%0Adisparity%20between%20the%20temporal%20attention%20maps%20across%20various%20decoder%20blocks.%202%29%0AFourier-based%20Motion%20Enhancement%20enhances%20the%20magnitude%20and%20richness%20of%20motion%0Aby%20amplifying%20the%20energy%20of%20the%20map.%20Extensive%20experiments%20demonstrate%20that%0ABroadWay%20significantly%20improves%20the%20quality%20of%20text-to-video%20generation%20with%0Anegligible%20additional%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBroadWay%253A%2520Boost%2520Your%2520Text-to-Video%2520Generation%2520Model%2520in%2520a%2520Training-free%250A%2520%2520Way%26entry.906535625%3DJiazi%2520Bu%2520and%2520Pengyang%2520Ling%2520and%2520Pan%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520The%2520text-to-video%2520%2528T2V%2529%2520generation%2520models%252C%2520offering%2520convenient%2520visual%250Acreation%252C%2520have%2520recently%2520garnered%2520increasing%2520attention.%2520Despite%2520their%250Asubstantial%2520potential%252C%2520the%2520generated%2520videos%2520may%2520present%2520artifacts%252C%2520including%250Astructural%2520implausibility%252C%2520temporal%2520inconsistency%252C%2520and%2520a%2520lack%2520of%2520motion%252C%2520often%250Aresulting%2520in%2520near-static%2520video.%2520In%2520this%2520work%252C%2520we%2520have%2520identified%2520a%2520correlation%250Abetween%2520the%2520disparity%2520of%2520temporal%2520attention%2520maps%2520across%2520different%2520blocks%2520and%250Athe%2520occurrence%2520of%2520temporal%2520inconsistencies.%2520Additionally%252C%2520we%2520have%2520observed%2520that%250Athe%2520energy%2520contained%2520within%2520the%2520temporal%2520attention%2520maps%2520is%2520directly%2520related%2520to%250Athe%2520magnitude%2520of%2520motion%2520amplitude%2520in%2520the%2520generated%2520videos.%2520Based%2520on%2520these%250Aobservations%252C%2520we%2520present%2520BroadWay%252C%2520a%2520training-free%2520method%2520to%2520improve%2520the%250Aquality%2520of%2520text-to-video%2520generation%2520without%2520introducing%2520additional%2520parameters%252C%250Aaugmenting%2520memory%2520or%2520sampling%2520time.%2520Specifically%252C%2520BroadWay%2520is%2520composed%2520of%2520two%250Aprincipal%2520components%253A%25201%2529%2520Temporal%2520Self-Guidance%2520improves%2520the%2520structural%250Aplausibility%2520and%2520temporal%2520consistency%2520of%2520generated%2520videos%2520by%2520reducing%2520the%250Adisparity%2520between%2520the%2520temporal%2520attention%2520maps%2520across%2520various%2520decoder%2520blocks.%25202%2529%250AFourier-based%2520Motion%2520Enhancement%2520enhances%2520the%2520magnitude%2520and%2520richness%2520of%2520motion%250Aby%2520amplifying%2520the%2520energy%2520of%2520the%2520map.%2520Extensive%2520experiments%2520demonstrate%2520that%250ABroadWay%2520significantly%2520improves%2520the%2520quality%2520of%2520text-to-video%2520generation%2520with%250Anegligible%2520additional%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BroadWay%3A%20Boost%20Your%20Text-to-Video%20Generation%20Model%20in%20a%20Training-free%0A%20%20Way&entry.906535625=Jiazi%20Bu%20and%20Pengyang%20Ling%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20The%20text-to-video%20%28T2V%29%20generation%20models%2C%20offering%20convenient%20visual%0Acreation%2C%20have%20recently%20garnered%20increasing%20attention.%20Despite%20their%0Asubstantial%20potential%2C%20the%20generated%20videos%20may%20present%20artifacts%2C%20including%0Astructural%20implausibility%2C%20temporal%20inconsistency%2C%20and%20a%20lack%20of%20motion%2C%20often%0Aresulting%20in%20near-static%20video.%20In%20this%20work%2C%20we%20have%20identified%20a%20correlation%0Abetween%20the%20disparity%20of%20temporal%20attention%20maps%20across%20different%20blocks%20and%0Athe%20occurrence%20of%20temporal%20inconsistencies.%20Additionally%2C%20we%20have%20observed%20that%0Athe%20energy%20contained%20within%20the%20temporal%20attention%20maps%20is%20directly%20related%20to%0Athe%20magnitude%20of%20motion%20amplitude%20in%20the%20generated%20videos.%20Based%20on%20these%0Aobservations%2C%20we%20present%20BroadWay%2C%20a%20training-free%20method%20to%20improve%20the%0Aquality%20of%20text-to-video%20generation%20without%20introducing%20additional%20parameters%2C%0Aaugmenting%20memory%20or%20sampling%20time.%20Specifically%2C%20BroadWay%20is%20composed%20of%20two%0Aprincipal%20components%3A%201%29%20Temporal%20Self-Guidance%20improves%20the%20structural%0Aplausibility%20and%20temporal%20consistency%20of%20generated%20videos%20by%20reducing%20the%0Adisparity%20between%20the%20temporal%20attention%20maps%20across%20various%20decoder%20blocks.%202%29%0AFourier-based%20Motion%20Enhancement%20enhances%20the%20magnitude%20and%20richness%20of%20motion%0Aby%20amplifying%20the%20energy%20of%20the%20map.%20Extensive%20experiments%20demonstrate%20that%0ABroadWay%20significantly%20improves%20the%20quality%20of%20text-to-video%20generation%20with%0Anegligible%20additional%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06241v2&entry.124074799=Read"},
{"title": "LoraMap: Harnessing the Power of LoRA Connections", "author": "Hyeryun Park and Jeongwon Kwak and Dongsuk Jang and Sumin Park and Jinwook Choi", "abstract": "  Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.\n", "link": "http://arxiv.org/abs/2408.16264v2", "date": "2024-10-16", "relevancy": 2.4921, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoraMap%3A%20Harnessing%20the%20Power%20of%20LoRA%20Connections&body=Title%3A%20LoraMap%3A%20Harnessing%20the%20Power%20of%20LoRA%20Connections%0AAuthor%3A%20Hyeryun%20Park%20and%20Jeongwon%20Kwak%20and%20Dongsuk%20Jang%20and%20Sumin%20Park%20and%20Jinwook%20Choi%0AAbstract%3A%20%20%20Fact-checking%20techniques%20can%20mitigate%20hallucinations%20in%20Large%20Language%20Models%0A%28LLMs%29%2C%20a%20prominent%20issue%20in%20specialized%20domains.%20As%20parameter-efficient%0Atechniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20can%20overcome%20substantial%0Acomputational%20overhead%2C%20some%20studies%20have%20explored%20the%20integration%20of%20multiple%0ALoRAs.%20While%20previous%20studies%20focus%20on%20parallel%20integration%2C%20this%20paper%0Ainvestigates%20methods%20to%20establish%20connections%20among%20multiple%20LoRAs.%20We%20create%0Athree%20reasoning%20datasets%20tailored%20to%20fact-checking%20and%20fine-tune%20individual%0ALoRAs%2C%20allowing%20them%20to%20view%20and%20reason%20from%20diverse%20perspectives.%20Then%2C%20we%0Aexplore%20strategies%20for%20allocating%20these%20reasoning%20LoRAs%20and%20introduce%20LoraMap%2C%0Aan%20approach%20to%20map%20connections%20between%20them.%20The%20results%20of%20the%20fact-checking%0Atask%20demonstrate%20that%20the%20performance%20of%20LoraMap%20is%20superior%20to%20LoraHub%2C%20an%0Aexisting%20method%20for%20integrating%20LoRAs.%20LoraMap%20also%20outperforms%20with%0Asignificantly%20fewer%20trainable%20parameters%20than%20LoraConcat%2C%20which%20concatenates%0ALoRAs%20and%20further%20fine-tunes%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoraMap%253A%2520Harnessing%2520the%2520Power%2520of%2520LoRA%2520Connections%26entry.906535625%3DHyeryun%2520Park%2520and%2520Jeongwon%2520Kwak%2520and%2520Dongsuk%2520Jang%2520and%2520Sumin%2520Park%2520and%2520Jinwook%2520Choi%26entry.1292438233%3D%2520%2520Fact-checking%2520techniques%2520can%2520mitigate%2520hallucinations%2520in%2520Large%2520Language%2520Models%250A%2528LLMs%2529%252C%2520a%2520prominent%2520issue%2520in%2520specialized%2520domains.%2520As%2520parameter-efficient%250Atechniques%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520can%2520overcome%2520substantial%250Acomputational%2520overhead%252C%2520some%2520studies%2520have%2520explored%2520the%2520integration%2520of%2520multiple%250ALoRAs.%2520While%2520previous%2520studies%2520focus%2520on%2520parallel%2520integration%252C%2520this%2520paper%250Ainvestigates%2520methods%2520to%2520establish%2520connections%2520among%2520multiple%2520LoRAs.%2520We%2520create%250Athree%2520reasoning%2520datasets%2520tailored%2520to%2520fact-checking%2520and%2520fine-tune%2520individual%250ALoRAs%252C%2520allowing%2520them%2520to%2520view%2520and%2520reason%2520from%2520diverse%2520perspectives.%2520Then%252C%2520we%250Aexplore%2520strategies%2520for%2520allocating%2520these%2520reasoning%2520LoRAs%2520and%2520introduce%2520LoraMap%252C%250Aan%2520approach%2520to%2520map%2520connections%2520between%2520them.%2520The%2520results%2520of%2520the%2520fact-checking%250Atask%2520demonstrate%2520that%2520the%2520performance%2520of%2520LoraMap%2520is%2520superior%2520to%2520LoraHub%252C%2520an%250Aexisting%2520method%2520for%2520integrating%2520LoRAs.%2520LoraMap%2520also%2520outperforms%2520with%250Asignificantly%2520fewer%2520trainable%2520parameters%2520than%2520LoraConcat%252C%2520which%2520concatenates%250ALoRAs%2520and%2520further%2520fine-tunes%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoraMap%3A%20Harnessing%20the%20Power%20of%20LoRA%20Connections&entry.906535625=Hyeryun%20Park%20and%20Jeongwon%20Kwak%20and%20Dongsuk%20Jang%20and%20Sumin%20Park%20and%20Jinwook%20Choi&entry.1292438233=%20%20Fact-checking%20techniques%20can%20mitigate%20hallucinations%20in%20Large%20Language%20Models%0A%28LLMs%29%2C%20a%20prominent%20issue%20in%20specialized%20domains.%20As%20parameter-efficient%0Atechniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20can%20overcome%20substantial%0Acomputational%20overhead%2C%20some%20studies%20have%20explored%20the%20integration%20of%20multiple%0ALoRAs.%20While%20previous%20studies%20focus%20on%20parallel%20integration%2C%20this%20paper%0Ainvestigates%20methods%20to%20establish%20connections%20among%20multiple%20LoRAs.%20We%20create%0Athree%20reasoning%20datasets%20tailored%20to%20fact-checking%20and%20fine-tune%20individual%0ALoRAs%2C%20allowing%20them%20to%20view%20and%20reason%20from%20diverse%20perspectives.%20Then%2C%20we%0Aexplore%20strategies%20for%20allocating%20these%20reasoning%20LoRAs%20and%20introduce%20LoraMap%2C%0Aan%20approach%20to%20map%20connections%20between%20them.%20The%20results%20of%20the%20fact-checking%0Atask%20demonstrate%20that%20the%20performance%20of%20LoraMap%20is%20superior%20to%20LoraHub%2C%20an%0Aexisting%20method%20for%20integrating%20LoRAs.%20LoraMap%20also%20outperforms%20with%0Asignificantly%20fewer%20trainable%20parameters%20than%20LoraConcat%2C%20which%20concatenates%0ALoRAs%20and%20further%20fine-tunes%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16264v2&entry.124074799=Read"},
{"title": "MultiCamCows2024 -- A Multi-view Image Dataset for AI-driven\n  Holstein-Friesian Cattle Re-Identification on a Working Farm", "author": "Phoenix Yu and Tilo Burghardt and Andrew W Dowsey and Neill W Campbell", "abstract": "  We present MultiCamCows2024, a farm-scale image dataset filmed across\nmultiple cameras for the biometric identification of individual\nHolstein-Friesian cattle exploiting their unique black and white coat-patterns.\nCaptured by three ceiling-mounted visual sensors covering adjacent barn areas\nover seven days on a working dairy farm, the dataset comprises 101, 329 images\nof 90 cows, plus the underlying original CCTV footage. The dataset is provided\nalongside full computer vision recognition baselines, that is both a supervised\nand self-supervised learning framework for individual cow identification\ntrained on cattle tracklets. We report a performance above 96% single image\nidentification accuracy from the dataset and demonstrate that combining data\nfrom multiple cameras during learning enhances self-supervised identification.\nWe show that our framework enables fully automatic cattle identification,\nbarring only the simple human verification of tracklet integrity during data\ncollection. Crucially, our study highlights that multi-camera, supervised and\nself-supervised components in tandem not only deliver highly accurate\nindividual cow identification but also achieve this efficiently with no\nlabelling of cattle identities by humans at all. We argue that this improvement\nin efficacy has practical implications for livestock management, behaviour\nanalysis, and agricultural monitoring. For full reproducibility and practical\nease of use, we publish all key software and code including re-identification\ncomponents and the species detector with this paper.\n", "link": "http://arxiv.org/abs/2410.12695v1", "date": "2024-10-16", "relevancy": 2.4825, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiCamCows2024%20--%20A%20Multi-view%20Image%20Dataset%20for%20AI-driven%0A%20%20Holstein-Friesian%20Cattle%20Re-Identification%20on%20a%20Working%20Farm&body=Title%3A%20MultiCamCows2024%20--%20A%20Multi-view%20Image%20Dataset%20for%20AI-driven%0A%20%20Holstein-Friesian%20Cattle%20Re-Identification%20on%20a%20Working%20Farm%0AAuthor%3A%20Phoenix%20Yu%20and%20Tilo%20Burghardt%20and%20Andrew%20W%20Dowsey%20and%20Neill%20W%20Campbell%0AAbstract%3A%20%20%20We%20present%20MultiCamCows2024%2C%20a%20farm-scale%20image%20dataset%20filmed%20across%0Amultiple%20cameras%20for%20the%20biometric%20identification%20of%20individual%0AHolstein-Friesian%20cattle%20exploiting%20their%20unique%20black%20and%20white%20coat-patterns.%0ACaptured%20by%20three%20ceiling-mounted%20visual%20sensors%20covering%20adjacent%20barn%20areas%0Aover%20seven%20days%20on%20a%20working%20dairy%20farm%2C%20the%20dataset%20comprises%20101%2C%20329%20images%0Aof%2090%20cows%2C%20plus%20the%20underlying%20original%20CCTV%20footage.%20The%20dataset%20is%20provided%0Aalongside%20full%20computer%20vision%20recognition%20baselines%2C%20that%20is%20both%20a%20supervised%0Aand%20self-supervised%20learning%20framework%20for%20individual%20cow%20identification%0Atrained%20on%20cattle%20tracklets.%20We%20report%20a%20performance%20above%2096%25%20single%20image%0Aidentification%20accuracy%20from%20the%20dataset%20and%20demonstrate%20that%20combining%20data%0Afrom%20multiple%20cameras%20during%20learning%20enhances%20self-supervised%20identification.%0AWe%20show%20that%20our%20framework%20enables%20fully%20automatic%20cattle%20identification%2C%0Abarring%20only%20the%20simple%20human%20verification%20of%20tracklet%20integrity%20during%20data%0Acollection.%20Crucially%2C%20our%20study%20highlights%20that%20multi-camera%2C%20supervised%20and%0Aself-supervised%20components%20in%20tandem%20not%20only%20deliver%20highly%20accurate%0Aindividual%20cow%20identification%20but%20also%20achieve%20this%20efficiently%20with%20no%0Alabelling%20of%20cattle%20identities%20by%20humans%20at%20all.%20We%20argue%20that%20this%20improvement%0Ain%20efficacy%20has%20practical%20implications%20for%20livestock%20management%2C%20behaviour%0Aanalysis%2C%20and%20agricultural%20monitoring.%20For%20full%20reproducibility%20and%20practical%0Aease%20of%20use%2C%20we%20publish%20all%20key%20software%20and%20code%20including%20re-identification%0Acomponents%20and%20the%20species%20detector%20with%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiCamCows2024%2520--%2520A%2520Multi-view%2520Image%2520Dataset%2520for%2520AI-driven%250A%2520%2520Holstein-Friesian%2520Cattle%2520Re-Identification%2520on%2520a%2520Working%2520Farm%26entry.906535625%3DPhoenix%2520Yu%2520and%2520Tilo%2520Burghardt%2520and%2520Andrew%2520W%2520Dowsey%2520and%2520Neill%2520W%2520Campbell%26entry.1292438233%3D%2520%2520We%2520present%2520MultiCamCows2024%252C%2520a%2520farm-scale%2520image%2520dataset%2520filmed%2520across%250Amultiple%2520cameras%2520for%2520the%2520biometric%2520identification%2520of%2520individual%250AHolstein-Friesian%2520cattle%2520exploiting%2520their%2520unique%2520black%2520and%2520white%2520coat-patterns.%250ACaptured%2520by%2520three%2520ceiling-mounted%2520visual%2520sensors%2520covering%2520adjacent%2520barn%2520areas%250Aover%2520seven%2520days%2520on%2520a%2520working%2520dairy%2520farm%252C%2520the%2520dataset%2520comprises%2520101%252C%2520329%2520images%250Aof%252090%2520cows%252C%2520plus%2520the%2520underlying%2520original%2520CCTV%2520footage.%2520The%2520dataset%2520is%2520provided%250Aalongside%2520full%2520computer%2520vision%2520recognition%2520baselines%252C%2520that%2520is%2520both%2520a%2520supervised%250Aand%2520self-supervised%2520learning%2520framework%2520for%2520individual%2520cow%2520identification%250Atrained%2520on%2520cattle%2520tracklets.%2520We%2520report%2520a%2520performance%2520above%252096%2525%2520single%2520image%250Aidentification%2520accuracy%2520from%2520the%2520dataset%2520and%2520demonstrate%2520that%2520combining%2520data%250Afrom%2520multiple%2520cameras%2520during%2520learning%2520enhances%2520self-supervised%2520identification.%250AWe%2520show%2520that%2520our%2520framework%2520enables%2520fully%2520automatic%2520cattle%2520identification%252C%250Abarring%2520only%2520the%2520simple%2520human%2520verification%2520of%2520tracklet%2520integrity%2520during%2520data%250Acollection.%2520Crucially%252C%2520our%2520study%2520highlights%2520that%2520multi-camera%252C%2520supervised%2520and%250Aself-supervised%2520components%2520in%2520tandem%2520not%2520only%2520deliver%2520highly%2520accurate%250Aindividual%2520cow%2520identification%2520but%2520also%2520achieve%2520this%2520efficiently%2520with%2520no%250Alabelling%2520of%2520cattle%2520identities%2520by%2520humans%2520at%2520all.%2520We%2520argue%2520that%2520this%2520improvement%250Ain%2520efficacy%2520has%2520practical%2520implications%2520for%2520livestock%2520management%252C%2520behaviour%250Aanalysis%252C%2520and%2520agricultural%2520monitoring.%2520For%2520full%2520reproducibility%2520and%2520practical%250Aease%2520of%2520use%252C%2520we%2520publish%2520all%2520key%2520software%2520and%2520code%2520including%2520re-identification%250Acomponents%2520and%2520the%2520species%2520detector%2520with%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiCamCows2024%20--%20A%20Multi-view%20Image%20Dataset%20for%20AI-driven%0A%20%20Holstein-Friesian%20Cattle%20Re-Identification%20on%20a%20Working%20Farm&entry.906535625=Phoenix%20Yu%20and%20Tilo%20Burghardt%20and%20Andrew%20W%20Dowsey%20and%20Neill%20W%20Campbell&entry.1292438233=%20%20We%20present%20MultiCamCows2024%2C%20a%20farm-scale%20image%20dataset%20filmed%20across%0Amultiple%20cameras%20for%20the%20biometric%20identification%20of%20individual%0AHolstein-Friesian%20cattle%20exploiting%20their%20unique%20black%20and%20white%20coat-patterns.%0ACaptured%20by%20three%20ceiling-mounted%20visual%20sensors%20covering%20adjacent%20barn%20areas%0Aover%20seven%20days%20on%20a%20working%20dairy%20farm%2C%20the%20dataset%20comprises%20101%2C%20329%20images%0Aof%2090%20cows%2C%20plus%20the%20underlying%20original%20CCTV%20footage.%20The%20dataset%20is%20provided%0Aalongside%20full%20computer%20vision%20recognition%20baselines%2C%20that%20is%20both%20a%20supervised%0Aand%20self-supervised%20learning%20framework%20for%20individual%20cow%20identification%0Atrained%20on%20cattle%20tracklets.%20We%20report%20a%20performance%20above%2096%25%20single%20image%0Aidentification%20accuracy%20from%20the%20dataset%20and%20demonstrate%20that%20combining%20data%0Afrom%20multiple%20cameras%20during%20learning%20enhances%20self-supervised%20identification.%0AWe%20show%20that%20our%20framework%20enables%20fully%20automatic%20cattle%20identification%2C%0Abarring%20only%20the%20simple%20human%20verification%20of%20tracklet%20integrity%20during%20data%0Acollection.%20Crucially%2C%20our%20study%20highlights%20that%20multi-camera%2C%20supervised%20and%0Aself-supervised%20components%20in%20tandem%20not%20only%20deliver%20highly%20accurate%0Aindividual%20cow%20identification%20but%20also%20achieve%20this%20efficiently%20with%20no%0Alabelling%20of%20cattle%20identities%20by%20humans%20at%20all.%20We%20argue%20that%20this%20improvement%0Ain%20efficacy%20has%20practical%20implications%20for%20livestock%20management%2C%20behaviour%0Aanalysis%2C%20and%20agricultural%20monitoring.%20For%20full%20reproducibility%20and%20practical%0Aease%20of%20use%2C%20we%20publish%20all%20key%20software%20and%20code%20including%20re-identification%0Acomponents%20and%20the%20species%20detector%20with%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12695v1&entry.124074799=Read"},
{"title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?", "author": "Yuyao Ge and Shenghua Liu and Baolong Bi and Yiwei Wang and Lingrui Mei and Wenjie Feng and Lizhe Chen and Xueqi Cheng", "abstract": "  Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.\n", "link": "http://arxiv.org/abs/2402.07140v4", "date": "2024-10-16", "relevancy": 2.4633, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Graph%20Descriptive%20Order%20Affect%20Solving%20Graph%20Problems%20with%20LLMs%3F&body=Title%3A%20Can%20Graph%20Descriptive%20Order%20Affect%20Solving%20Graph%20Problems%20with%20LLMs%3F%0AAuthor%3A%20Yuyao%20Ge%20and%20Shenghua%20Liu%20and%20Baolong%20Bi%20and%20Yiwei%20Wang%20and%20Lingrui%20Mei%20and%20Wenjie%20Feng%20and%20Lizhe%20Chen%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20success%20in%20reasoning%0Atasks%2C%20including%20mathematical%20reasoning%20and%20logical%20deduction.%20Among%20these%0Areasoning%20tasks%2C%20graph%20problems%20stand%20out%20due%20to%20their%20complexity%20and%20unique%0Astructural%20characteristics%2C%20attracting%20considerable%20attention%20from%20researchers.%0APrevious%20studies%20have%20explored%20LLMs%27%20graph%20reasoning%20abilities%20through%20various%0Atechniques%2C%20such%20as%20different%20encoding%20methods%20for%20graph%20structures%20and%20the%20use%0Aof%20carefully%20designed%20prompts.%20However%2C%20a%20critical%20factor%20has%20been%20mostly%0Aoverlooked%3A%20the%20prompt%20sequential%20order%20in%20which%20graph%20descriptions%20are%0Apresented%20to%20the%20models.%20In%20this%20study%2C%20we%20present%20the%20first%20comprehensive%0Aanalysis%20of%20how%20the%20order%20of%20graph%20descriptions%20impacts%20LLM%20performance.%0ASpecifically%2C%20we%20comprehensively%20evaluate%20four%20graph%20description%20orders%20across%0Asix%20graph%20problems%20using%20six%20mainstream%20LLMs.%20The%20results%20reveal%20that%3A%20%281%29%0Aordered%20graph%20descriptions%20significantly%20improve%20LLMs%27%20comprehension%20of%20graph%0Astructures%3B%20%282%29%20the%20robustness%20of%20LLMs%20to%20graph%20description%20order%20varies%20across%0Adifferent%20tasks%3B%20and%20%283%29%20the%20impact%20of%20graph%20order%20on%20performance%20is%20closely%0Arelated%20to%20the%20inherent%20characteristics%20of%20tasks.%20This%20study%20provides%20a%0Acritical%20advancement%20in%20the%20application%20of%20LLMs%20for%20solving%20graph-related%0Aproblems%2C%20paving%20the%20way%20for%20future%20research%20to%20optimize%20model%20performance%0Athrough%20strategic%20graph%20description%20ordering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07140v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Graph%2520Descriptive%2520Order%2520Affect%2520Solving%2520Graph%2520Problems%2520with%2520LLMs%253F%26entry.906535625%3DYuyao%2520Ge%2520and%2520Shenghua%2520Liu%2520and%2520Baolong%2520Bi%2520and%2520Yiwei%2520Wang%2520and%2520Lingrui%2520Mei%2520and%2520Wenjie%2520Feng%2520and%2520Lizhe%2520Chen%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520success%2520in%2520reasoning%250Atasks%252C%2520including%2520mathematical%2520reasoning%2520and%2520logical%2520deduction.%2520Among%2520these%250Areasoning%2520tasks%252C%2520graph%2520problems%2520stand%2520out%2520due%2520to%2520their%2520complexity%2520and%2520unique%250Astructural%2520characteristics%252C%2520attracting%2520considerable%2520attention%2520from%2520researchers.%250APrevious%2520studies%2520have%2520explored%2520LLMs%2527%2520graph%2520reasoning%2520abilities%2520through%2520various%250Atechniques%252C%2520such%2520as%2520different%2520encoding%2520methods%2520for%2520graph%2520structures%2520and%2520the%2520use%250Aof%2520carefully%2520designed%2520prompts.%2520However%252C%2520a%2520critical%2520factor%2520has%2520been%2520mostly%250Aoverlooked%253A%2520the%2520prompt%2520sequential%2520order%2520in%2520which%2520graph%2520descriptions%2520are%250Apresented%2520to%2520the%2520models.%2520In%2520this%2520study%252C%2520we%2520present%2520the%2520first%2520comprehensive%250Aanalysis%2520of%2520how%2520the%2520order%2520of%2520graph%2520descriptions%2520impacts%2520LLM%2520performance.%250ASpecifically%252C%2520we%2520comprehensively%2520evaluate%2520four%2520graph%2520description%2520orders%2520across%250Asix%2520graph%2520problems%2520using%2520six%2520mainstream%2520LLMs.%2520The%2520results%2520reveal%2520that%253A%2520%25281%2529%250Aordered%2520graph%2520descriptions%2520significantly%2520improve%2520LLMs%2527%2520comprehension%2520of%2520graph%250Astructures%253B%2520%25282%2529%2520the%2520robustness%2520of%2520LLMs%2520to%2520graph%2520description%2520order%2520varies%2520across%250Adifferent%2520tasks%253B%2520and%2520%25283%2529%2520the%2520impact%2520of%2520graph%2520order%2520on%2520performance%2520is%2520closely%250Arelated%2520to%2520the%2520inherent%2520characteristics%2520of%2520tasks.%2520This%2520study%2520provides%2520a%250Acritical%2520advancement%2520in%2520the%2520application%2520of%2520LLMs%2520for%2520solving%2520graph-related%250Aproblems%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520to%2520optimize%2520model%2520performance%250Athrough%2520strategic%2520graph%2520description%2520ordering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07140v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Graph%20Descriptive%20Order%20Affect%20Solving%20Graph%20Problems%20with%20LLMs%3F&entry.906535625=Yuyao%20Ge%20and%20Shenghua%20Liu%20and%20Baolong%20Bi%20and%20Yiwei%20Wang%20and%20Lingrui%20Mei%20and%20Wenjie%20Feng%20and%20Lizhe%20Chen%20and%20Xueqi%20Cheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20success%20in%20reasoning%0Atasks%2C%20including%20mathematical%20reasoning%20and%20logical%20deduction.%20Among%20these%0Areasoning%20tasks%2C%20graph%20problems%20stand%20out%20due%20to%20their%20complexity%20and%20unique%0Astructural%20characteristics%2C%20attracting%20considerable%20attention%20from%20researchers.%0APrevious%20studies%20have%20explored%20LLMs%27%20graph%20reasoning%20abilities%20through%20various%0Atechniques%2C%20such%20as%20different%20encoding%20methods%20for%20graph%20structures%20and%20the%20use%0Aof%20carefully%20designed%20prompts.%20However%2C%20a%20critical%20factor%20has%20been%20mostly%0Aoverlooked%3A%20the%20prompt%20sequential%20order%20in%20which%20graph%20descriptions%20are%0Apresented%20to%20the%20models.%20In%20this%20study%2C%20we%20present%20the%20first%20comprehensive%0Aanalysis%20of%20how%20the%20order%20of%20graph%20descriptions%20impacts%20LLM%20performance.%0ASpecifically%2C%20we%20comprehensively%20evaluate%20four%20graph%20description%20orders%20across%0Asix%20graph%20problems%20using%20six%20mainstream%20LLMs.%20The%20results%20reveal%20that%3A%20%281%29%0Aordered%20graph%20descriptions%20significantly%20improve%20LLMs%27%20comprehension%20of%20graph%0Astructures%3B%20%282%29%20the%20robustness%20of%20LLMs%20to%20graph%20description%20order%20varies%20across%0Adifferent%20tasks%3B%20and%20%283%29%20the%20impact%20of%20graph%20order%20on%20performance%20is%20closely%0Arelated%20to%20the%20inherent%20characteristics%20of%20tasks.%20This%20study%20provides%20a%0Acritical%20advancement%20in%20the%20application%20of%20LLMs%20for%20solving%20graph-related%0Aproblems%2C%20paving%20the%20way%20for%20future%20research%20to%20optimize%20model%20performance%0Athrough%20strategic%20graph%20description%20ordering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07140v4&entry.124074799=Read"},
{"title": "HELM: Hierarchical Encoding for mRNA Language Modeling", "author": "Mehdi Yazdani-Jahromi and Mangal Prakash and Tommaso Mansi and Artem Moskalev and Rui Liao", "abstract": "  Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its\ncodon structure directly impacting biological properties. While Language Models\n(LMs) have shown promise in analyzing biological sequences, existing approaches\nfail to account for the hierarchical nature of mRNA's codon structure. We\nintroduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel\npre-training strategy that incorporates codon-level hierarchical structure into\nlanguage model training. HELM modulates the loss function based on codon\nsynonymity, aligning the model's learning process with the biological reality\nof mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,\ndemonstrating that HELM outperforms standard language model pre-training as\nwell as existing foundation model baselines on six diverse downstream property\nprediction tasks and an antibody region annotation tasks on average by around\n8\\%. Additionally, HELM enhances the generative capabilities of language model,\nproducing diverse mRNA sequences that better align with the underlying true\ndata distribution compared to non-hierarchical baselines.\n", "link": "http://arxiv.org/abs/2410.12459v1", "date": "2024-10-16", "relevancy": 2.4525, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HELM%3A%20Hierarchical%20Encoding%20for%20mRNA%20Language%20Modeling&body=Title%3A%20HELM%3A%20Hierarchical%20Encoding%20for%20mRNA%20Language%20Modeling%0AAuthor%3A%20Mehdi%20Yazdani-Jahromi%20and%20Mangal%20Prakash%20and%20Tommaso%20Mansi%20and%20Artem%20Moskalev%20and%20Rui%20Liao%0AAbstract%3A%20%20%20Messenger%20RNA%20%28mRNA%29%20plays%20a%20crucial%20role%20in%20protein%20synthesis%2C%20with%20its%0Acodon%20structure%20directly%20impacting%20biological%20properties.%20While%20Language%20Models%0A%28LMs%29%20have%20shown%20promise%20in%20analyzing%20biological%20sequences%2C%20existing%20approaches%0Afail%20to%20account%20for%20the%20hierarchical%20nature%20of%20mRNA%27s%20codon%20structure.%20We%0Aintroduce%20Hierarchical%20Encoding%20for%20mRNA%20Language%20Modeling%20%28HELM%29%2C%20a%20novel%0Apre-training%20strategy%20that%20incorporates%20codon-level%20hierarchical%20structure%20into%0Alanguage%20model%20training.%20HELM%20modulates%20the%20loss%20function%20based%20on%20codon%0Asynonymity%2C%20aligning%20the%20model%27s%20learning%20process%20with%20the%20biological%20reality%0Aof%20mRNA%20sequences.%20We%20evaluate%20HELM%20on%20diverse%20mRNA%20datasets%20and%20tasks%2C%0Ademonstrating%20that%20HELM%20outperforms%20standard%20language%20model%20pre-training%20as%0Awell%20as%20existing%20foundation%20model%20baselines%20on%20six%20diverse%20downstream%20property%0Aprediction%20tasks%20and%20an%20antibody%20region%20annotation%20tasks%20on%20average%20by%20around%0A8%5C%25.%20Additionally%2C%20HELM%20enhances%20the%20generative%20capabilities%20of%20language%20model%2C%0Aproducing%20diverse%20mRNA%20sequences%20that%20better%20align%20with%20the%20underlying%20true%0Adata%20distribution%20compared%20to%20non-hierarchical%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHELM%253A%2520Hierarchical%2520Encoding%2520for%2520mRNA%2520Language%2520Modeling%26entry.906535625%3DMehdi%2520Yazdani-Jahromi%2520and%2520Mangal%2520Prakash%2520and%2520Tommaso%2520Mansi%2520and%2520Artem%2520Moskalev%2520and%2520Rui%2520Liao%26entry.1292438233%3D%2520%2520Messenger%2520RNA%2520%2528mRNA%2529%2520plays%2520a%2520crucial%2520role%2520in%2520protein%2520synthesis%252C%2520with%2520its%250Acodon%2520structure%2520directly%2520impacting%2520biological%2520properties.%2520While%2520Language%2520Models%250A%2528LMs%2529%2520have%2520shown%2520promise%2520in%2520analyzing%2520biological%2520sequences%252C%2520existing%2520approaches%250Afail%2520to%2520account%2520for%2520the%2520hierarchical%2520nature%2520of%2520mRNA%2527s%2520codon%2520structure.%2520We%250Aintroduce%2520Hierarchical%2520Encoding%2520for%2520mRNA%2520Language%2520Modeling%2520%2528HELM%2529%252C%2520a%2520novel%250Apre-training%2520strategy%2520that%2520incorporates%2520codon-level%2520hierarchical%2520structure%2520into%250Alanguage%2520model%2520training.%2520HELM%2520modulates%2520the%2520loss%2520function%2520based%2520on%2520codon%250Asynonymity%252C%2520aligning%2520the%2520model%2527s%2520learning%2520process%2520with%2520the%2520biological%2520reality%250Aof%2520mRNA%2520sequences.%2520We%2520evaluate%2520HELM%2520on%2520diverse%2520mRNA%2520datasets%2520and%2520tasks%252C%250Ademonstrating%2520that%2520HELM%2520outperforms%2520standard%2520language%2520model%2520pre-training%2520as%250Awell%2520as%2520existing%2520foundation%2520model%2520baselines%2520on%2520six%2520diverse%2520downstream%2520property%250Aprediction%2520tasks%2520and%2520an%2520antibody%2520region%2520annotation%2520tasks%2520on%2520average%2520by%2520around%250A8%255C%2525.%2520Additionally%252C%2520HELM%2520enhances%2520the%2520generative%2520capabilities%2520of%2520language%2520model%252C%250Aproducing%2520diverse%2520mRNA%2520sequences%2520that%2520better%2520align%2520with%2520the%2520underlying%2520true%250Adata%2520distribution%2520compared%2520to%2520non-hierarchical%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HELM%3A%20Hierarchical%20Encoding%20for%20mRNA%20Language%20Modeling&entry.906535625=Mehdi%20Yazdani-Jahromi%20and%20Mangal%20Prakash%20and%20Tommaso%20Mansi%20and%20Artem%20Moskalev%20and%20Rui%20Liao&entry.1292438233=%20%20Messenger%20RNA%20%28mRNA%29%20plays%20a%20crucial%20role%20in%20protein%20synthesis%2C%20with%20its%0Acodon%20structure%20directly%20impacting%20biological%20properties.%20While%20Language%20Models%0A%28LMs%29%20have%20shown%20promise%20in%20analyzing%20biological%20sequences%2C%20existing%20approaches%0Afail%20to%20account%20for%20the%20hierarchical%20nature%20of%20mRNA%27s%20codon%20structure.%20We%0Aintroduce%20Hierarchical%20Encoding%20for%20mRNA%20Language%20Modeling%20%28HELM%29%2C%20a%20novel%0Apre-training%20strategy%20that%20incorporates%20codon-level%20hierarchical%20structure%20into%0Alanguage%20model%20training.%20HELM%20modulates%20the%20loss%20function%20based%20on%20codon%0Asynonymity%2C%20aligning%20the%20model%27s%20learning%20process%20with%20the%20biological%20reality%0Aof%20mRNA%20sequences.%20We%20evaluate%20HELM%20on%20diverse%20mRNA%20datasets%20and%20tasks%2C%0Ademonstrating%20that%20HELM%20outperforms%20standard%20language%20model%20pre-training%20as%0Awell%20as%20existing%20foundation%20model%20baselines%20on%20six%20diverse%20downstream%20property%0Aprediction%20tasks%20and%20an%20antibody%20region%20annotation%20tasks%20on%20average%20by%20around%0A8%5C%25.%20Additionally%2C%20HELM%20enhances%20the%20generative%20capabilities%20of%20language%20model%2C%0Aproducing%20diverse%20mRNA%20sequences%20that%20better%20align%20with%20the%20underlying%20true%0Adata%20distribution%20compared%20to%20non-hierarchical%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12459v1&entry.124074799=Read"},
{"title": "RAFA-Net: Region Attention Network For Food Items And Agricultural\n  Stress Recognition", "author": "Asish Bera and Ondrej Krejcar and Debotosh Bhattacharjee", "abstract": "  Deep Convolutional Neural Networks (CNNs) have facilitated remarkable success\nin recognizing various food items and agricultural stress. A decent performance\nboost has been witnessed in solving the agro-food challenges by mining and\nanalyzing of region-based partial feature descriptors. Also, computationally\nexpensive ensemble learning schemes using multiple CNNs have been studied in\nearlier works. This work proposes a region attention scheme for modelling\nlong-range dependencies by building a correlation among different regions\nwithin an input image. The attention method enhances feature representation by\nlearning the usefulness of context information from complementary regions.\nSpatial pyramidal pooling and average pooling pair aggregate partial\ndescriptors into a holistic representation. Both pooling methods establish\nspatial and channel-wise relationships without incurring extra parameters. A\ncontext gating scheme is applied to refine the descriptiveness of weighted\nattentional features, which is relevant for classification. The proposed Region\nAttention network for Food items and Agricultural stress recognition method,\ndubbed RAFA-Net, has been experimented on three public food datasets, and has\nachieved state-of-the-art performances with distinct margins. The highest top-1\naccuracies of RAFA-Net are 91.69%, 91.56%, and 96.97% on the UECFood-100,\nUECFood-256, and MAFood-121 datasets, respectively. In addition, better\naccuracies have been achieved on two benchmark agricultural stress datasets.\nThe best top-1 accuracies on the Insect Pest (IP-102) and PlantDoc-27 plant\ndisease datasets are 92.36%, and 85.54%, respectively; implying RAFA-Net's\ngeneralization capability.\n", "link": "http://arxiv.org/abs/2410.12718v1", "date": "2024-10-16", "relevancy": 2.4517, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5366}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4689}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAFA-Net%3A%20Region%20Attention%20Network%20For%20Food%20Items%20And%20Agricultural%0A%20%20Stress%20Recognition&body=Title%3A%20RAFA-Net%3A%20Region%20Attention%20Network%20For%20Food%20Items%20And%20Agricultural%0A%20%20Stress%20Recognition%0AAuthor%3A%20Asish%20Bera%20and%20Ondrej%20Krejcar%20and%20Debotosh%20Bhattacharjee%0AAbstract%3A%20%20%20Deep%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20facilitated%20remarkable%20success%0Ain%20recognizing%20various%20food%20items%20and%20agricultural%20stress.%20A%20decent%20performance%0Aboost%20has%20been%20witnessed%20in%20solving%20the%20agro-food%20challenges%20by%20mining%20and%0Aanalyzing%20of%20region-based%20partial%20feature%20descriptors.%20Also%2C%20computationally%0Aexpensive%20ensemble%20learning%20schemes%20using%20multiple%20CNNs%20have%20been%20studied%20in%0Aearlier%20works.%20This%20work%20proposes%20a%20region%20attention%20scheme%20for%20modelling%0Along-range%20dependencies%20by%20building%20a%20correlation%20among%20different%20regions%0Awithin%20an%20input%20image.%20The%20attention%20method%20enhances%20feature%20representation%20by%0Alearning%20the%20usefulness%20of%20context%20information%20from%20complementary%20regions.%0ASpatial%20pyramidal%20pooling%20and%20average%20pooling%20pair%20aggregate%20partial%0Adescriptors%20into%20a%20holistic%20representation.%20Both%20pooling%20methods%20establish%0Aspatial%20and%20channel-wise%20relationships%20without%20incurring%20extra%20parameters.%20A%0Acontext%20gating%20scheme%20is%20applied%20to%20refine%20the%20descriptiveness%20of%20weighted%0Aattentional%20features%2C%20which%20is%20relevant%20for%20classification.%20The%20proposed%20Region%0AAttention%20network%20for%20Food%20items%20and%20Agricultural%20stress%20recognition%20method%2C%0Adubbed%20RAFA-Net%2C%20has%20been%20experimented%20on%20three%20public%20food%20datasets%2C%20and%20has%0Aachieved%20state-of-the-art%20performances%20with%20distinct%20margins.%20The%20highest%20top-1%0Aaccuracies%20of%20RAFA-Net%20are%2091.69%25%2C%2091.56%25%2C%20and%2096.97%25%20on%20the%20UECFood-100%2C%0AUECFood-256%2C%20and%20MAFood-121%20datasets%2C%20respectively.%20In%20addition%2C%20better%0Aaccuracies%20have%20been%20achieved%20on%20two%20benchmark%20agricultural%20stress%20datasets.%0AThe%20best%20top-1%20accuracies%20on%20the%20Insect%20Pest%20%28IP-102%29%20and%20PlantDoc-27%20plant%0Adisease%20datasets%20are%2092.36%25%2C%20and%2085.54%25%2C%20respectively%3B%20implying%20RAFA-Net%27s%0Ageneralization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAFA-Net%253A%2520Region%2520Attention%2520Network%2520For%2520Food%2520Items%2520And%2520Agricultural%250A%2520%2520Stress%2520Recognition%26entry.906535625%3DAsish%2520Bera%2520and%2520Ondrej%2520Krejcar%2520and%2520Debotosh%2520Bhattacharjee%26entry.1292438233%3D%2520%2520Deep%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520facilitated%2520remarkable%2520success%250Ain%2520recognizing%2520various%2520food%2520items%2520and%2520agricultural%2520stress.%2520A%2520decent%2520performance%250Aboost%2520has%2520been%2520witnessed%2520in%2520solving%2520the%2520agro-food%2520challenges%2520by%2520mining%2520and%250Aanalyzing%2520of%2520region-based%2520partial%2520feature%2520descriptors.%2520Also%252C%2520computationally%250Aexpensive%2520ensemble%2520learning%2520schemes%2520using%2520multiple%2520CNNs%2520have%2520been%2520studied%2520in%250Aearlier%2520works.%2520This%2520work%2520proposes%2520a%2520region%2520attention%2520scheme%2520for%2520modelling%250Along-range%2520dependencies%2520by%2520building%2520a%2520correlation%2520among%2520different%2520regions%250Awithin%2520an%2520input%2520image.%2520The%2520attention%2520method%2520enhances%2520feature%2520representation%2520by%250Alearning%2520the%2520usefulness%2520of%2520context%2520information%2520from%2520complementary%2520regions.%250ASpatial%2520pyramidal%2520pooling%2520and%2520average%2520pooling%2520pair%2520aggregate%2520partial%250Adescriptors%2520into%2520a%2520holistic%2520representation.%2520Both%2520pooling%2520methods%2520establish%250Aspatial%2520and%2520channel-wise%2520relationships%2520without%2520incurring%2520extra%2520parameters.%2520A%250Acontext%2520gating%2520scheme%2520is%2520applied%2520to%2520refine%2520the%2520descriptiveness%2520of%2520weighted%250Aattentional%2520features%252C%2520which%2520is%2520relevant%2520for%2520classification.%2520The%2520proposed%2520Region%250AAttention%2520network%2520for%2520Food%2520items%2520and%2520Agricultural%2520stress%2520recognition%2520method%252C%250Adubbed%2520RAFA-Net%252C%2520has%2520been%2520experimented%2520on%2520three%2520public%2520food%2520datasets%252C%2520and%2520has%250Aachieved%2520state-of-the-art%2520performances%2520with%2520distinct%2520margins.%2520The%2520highest%2520top-1%250Aaccuracies%2520of%2520RAFA-Net%2520are%252091.69%2525%252C%252091.56%2525%252C%2520and%252096.97%2525%2520on%2520the%2520UECFood-100%252C%250AUECFood-256%252C%2520and%2520MAFood-121%2520datasets%252C%2520respectively.%2520In%2520addition%252C%2520better%250Aaccuracies%2520have%2520been%2520achieved%2520on%2520two%2520benchmark%2520agricultural%2520stress%2520datasets.%250AThe%2520best%2520top-1%2520accuracies%2520on%2520the%2520Insect%2520Pest%2520%2528IP-102%2529%2520and%2520PlantDoc-27%2520plant%250Adisease%2520datasets%2520are%252092.36%2525%252C%2520and%252085.54%2525%252C%2520respectively%253B%2520implying%2520RAFA-Net%2527s%250Ageneralization%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAFA-Net%3A%20Region%20Attention%20Network%20For%20Food%20Items%20And%20Agricultural%0A%20%20Stress%20Recognition&entry.906535625=Asish%20Bera%20and%20Ondrej%20Krejcar%20and%20Debotosh%20Bhattacharjee&entry.1292438233=%20%20Deep%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20facilitated%20remarkable%20success%0Ain%20recognizing%20various%20food%20items%20and%20agricultural%20stress.%20A%20decent%20performance%0Aboost%20has%20been%20witnessed%20in%20solving%20the%20agro-food%20challenges%20by%20mining%20and%0Aanalyzing%20of%20region-based%20partial%20feature%20descriptors.%20Also%2C%20computationally%0Aexpensive%20ensemble%20learning%20schemes%20using%20multiple%20CNNs%20have%20been%20studied%20in%0Aearlier%20works.%20This%20work%20proposes%20a%20region%20attention%20scheme%20for%20modelling%0Along-range%20dependencies%20by%20building%20a%20correlation%20among%20different%20regions%0Awithin%20an%20input%20image.%20The%20attention%20method%20enhances%20feature%20representation%20by%0Alearning%20the%20usefulness%20of%20context%20information%20from%20complementary%20regions.%0ASpatial%20pyramidal%20pooling%20and%20average%20pooling%20pair%20aggregate%20partial%0Adescriptors%20into%20a%20holistic%20representation.%20Both%20pooling%20methods%20establish%0Aspatial%20and%20channel-wise%20relationships%20without%20incurring%20extra%20parameters.%20A%0Acontext%20gating%20scheme%20is%20applied%20to%20refine%20the%20descriptiveness%20of%20weighted%0Aattentional%20features%2C%20which%20is%20relevant%20for%20classification.%20The%20proposed%20Region%0AAttention%20network%20for%20Food%20items%20and%20Agricultural%20stress%20recognition%20method%2C%0Adubbed%20RAFA-Net%2C%20has%20been%20experimented%20on%20three%20public%20food%20datasets%2C%20and%20has%0Aachieved%20state-of-the-art%20performances%20with%20distinct%20margins.%20The%20highest%20top-1%0Aaccuracies%20of%20RAFA-Net%20are%2091.69%25%2C%2091.56%25%2C%20and%2096.97%25%20on%20the%20UECFood-100%2C%0AUECFood-256%2C%20and%20MAFood-121%20datasets%2C%20respectively.%20In%20addition%2C%20better%0Aaccuracies%20have%20been%20achieved%20on%20two%20benchmark%20agricultural%20stress%20datasets.%0AThe%20best%20top-1%20accuracies%20on%20the%20Insect%20Pest%20%28IP-102%29%20and%20PlantDoc-27%20plant%0Adisease%20datasets%20are%2092.36%25%2C%20and%2085.54%25%2C%20respectively%3B%20implying%20RAFA-Net%27s%0Ageneralization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12718v1&entry.124074799=Read"},
{"title": "Exploring Model Kinship for Merging Large Language Models", "author": "Yedi Hu and Yunzhi Yao and Ningyu Zhang and Shumin Deng and Huajun Chen", "abstract": "  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n", "link": "http://arxiv.org/abs/2410.12613v1", "date": "2024-10-16", "relevancy": 2.4367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models&body=Title%3A%20Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models%0AAuthor%3A%20Yedi%20Hu%20and%20Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Shumin%20Deng%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20Model%20merging%20has%20become%20one%20of%20the%20key%20technologies%20for%20enhancing%20the%0Acapabilities%20and%20efficiency%20of%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20our%0Aunderstanding%20of%20the%20expected%20performance%20gains%20and%20principles%20when%20merging%20any%0Atwo%20models%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20model%20kinship%2C%20the%0Adegree%20of%20similarity%20or%20relatedness%20between%20LLMs%2C%20analogous%20to%20biological%0Aevolution.%20With%20comprehensive%20empirical%20analysis%2C%20we%20find%20that%20there%20is%20a%0Acertain%20relationship%20between%20model%20kinship%20and%20the%20performance%20gains%20after%0Amodel%20merging%2C%20which%20can%20help%20guide%20our%20selection%20of%20candidate%20models.%20Inspired%0Aby%20this%2C%20we%20propose%20a%20new%20model%20merging%20strategy%3A%20Top-k%20Greedy%20Merging%20with%0AModel%20Kinship%2C%20which%20can%20yield%20better%20performance%20on%20benchmark%20datasets.%0ASpecifically%2C%20we%20discover%20that%20using%20model%20kinship%20as%20a%20criterion%20can%20assist%20us%0Ain%20continuously%20performing%20model%20merging%2C%20alleviating%20the%20degradation%20%28local%0Aoptima%29%20in%20model%20evolution%2C%20whereas%20model%20kinship%20can%20serve%20as%20a%20guide%20to%0Aescape%20these%20traps.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/ModelKinship.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Model%2520Kinship%2520for%2520Merging%2520Large%2520Language%2520Models%26entry.906535625%3DYedi%2520Hu%2520and%2520Yunzhi%2520Yao%2520and%2520Ningyu%2520Zhang%2520and%2520Shumin%2520Deng%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520become%2520one%2520of%2520the%2520key%2520technologies%2520for%2520enhancing%2520the%250Acapabilities%2520and%2520efficiency%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520our%250Aunderstanding%2520of%2520the%2520expected%2520performance%2520gains%2520and%2520principles%2520when%2520merging%2520any%250Atwo%2520models%2520remains%2520limited.%2520In%2520this%2520work%252C%2520we%2520introduce%2520model%2520kinship%252C%2520the%250Adegree%2520of%2520similarity%2520or%2520relatedness%2520between%2520LLMs%252C%2520analogous%2520to%2520biological%250Aevolution.%2520With%2520comprehensive%2520empirical%2520analysis%252C%2520we%2520find%2520that%2520there%2520is%2520a%250Acertain%2520relationship%2520between%2520model%2520kinship%2520and%2520the%2520performance%2520gains%2520after%250Amodel%2520merging%252C%2520which%2520can%2520help%2520guide%2520our%2520selection%2520of%2520candidate%2520models.%2520Inspired%250Aby%2520this%252C%2520we%2520propose%2520a%2520new%2520model%2520merging%2520strategy%253A%2520Top-k%2520Greedy%2520Merging%2520with%250AModel%2520Kinship%252C%2520which%2520can%2520yield%2520better%2520performance%2520on%2520benchmark%2520datasets.%250ASpecifically%252C%2520we%2520discover%2520that%2520using%2520model%2520kinship%2520as%2520a%2520criterion%2520can%2520assist%2520us%250Ain%2520continuously%2520performing%2520model%2520merging%252C%2520alleviating%2520the%2520degradation%2520%2528local%250Aoptima%2529%2520in%2520model%2520evolution%252C%2520whereas%2520model%2520kinship%2520can%2520serve%2520as%2520a%2520guide%2520to%250Aescape%2520these%2520traps.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjunlp/ModelKinship.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Model%20Kinship%20for%20Merging%20Large%20Language%20Models&entry.906535625=Yedi%20Hu%20and%20Yunzhi%20Yao%20and%20Ningyu%20Zhang%20and%20Shumin%20Deng%20and%20Huajun%20Chen&entry.1292438233=%20%20Model%20merging%20has%20become%20one%20of%20the%20key%20technologies%20for%20enhancing%20the%0Acapabilities%20and%20efficiency%20of%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20our%0Aunderstanding%20of%20the%20expected%20performance%20gains%20and%20principles%20when%20merging%20any%0Atwo%20models%20remains%20limited.%20In%20this%20work%2C%20we%20introduce%20model%20kinship%2C%20the%0Adegree%20of%20similarity%20or%20relatedness%20between%20LLMs%2C%20analogous%20to%20biological%0Aevolution.%20With%20comprehensive%20empirical%20analysis%2C%20we%20find%20that%20there%20is%20a%0Acertain%20relationship%20between%20model%20kinship%20and%20the%20performance%20gains%20after%0Amodel%20merging%2C%20which%20can%20help%20guide%20our%20selection%20of%20candidate%20models.%20Inspired%0Aby%20this%2C%20we%20propose%20a%20new%20model%20merging%20strategy%3A%20Top-k%20Greedy%20Merging%20with%0AModel%20Kinship%2C%20which%20can%20yield%20better%20performance%20on%20benchmark%20datasets.%0ASpecifically%2C%20we%20discover%20that%20using%20model%20kinship%20as%20a%20criterion%20can%20assist%20us%0Ain%20continuously%20performing%20model%20merging%2C%20alleviating%20the%20degradation%20%28local%0Aoptima%29%20in%20model%20evolution%2C%20whereas%20model%20kinship%20can%20serve%20as%20a%20guide%20to%0Aescape%20these%20traps.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zjunlp/ModelKinship.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12613v1&entry.124074799=Read"},
{"title": "Vaccinating Federated Learning for Robust Modulation Classification in\n  Distributed Wireless Networks", "author": "Hunmin Lee and Hongju Seong and Wonbin Kim and Hyeokchan Kwon and Daehee Seo", "abstract": "  Automatic modulation classification (AMC) serves a vital role in ensuring\nefficient and reliable communication services within distributed wireless\nnetworks. Recent developments have seen a surge in interest in deep neural\nnetwork (DNN)-based AMC models, with Federated Learning (FL) emerging as a\npromising framework. Despite these advancements, the presence of various noises\nwithin the signal exerts significant challenges while optimizing models to\ncapture salient features. Furthermore, existing FL-based AMC models commonly\nrely on linear aggregation strategies, which face notable difficulties in\nintegrating locally fine-tuned parameters within practical non-IID (Independent\nand Identically Distributed) environments, thereby hindering optimal learning\nconvergence. To address these challenges, we propose FedVaccine, a novel FL\nmodel aimed at improving generalizability across signals with varying noise\nlevels by deliberately introducing a balanced level of noise. This is\naccomplished through our proposed harmonic noise resilience approach, which\nidentifies an optimal noise tolerance for DNN models, thereby regulating the\ntraining process and mitigating overfitting. Additionally, FedVaccine overcomes\nthe limitations of existing FL-based AMC models' linear aggregation by\nemploying a split-learning strategy using structural clustering topology and\nlocal queue data structure, enabling adaptive and cumulative updates to local\nmodels. Our experimental results, including IID and non-IID datasets as well as\nablation studies, confirm FedVaccine's robust performance and superiority over\nexisting FL-based AMC approaches across different noise levels. These findings\nhighlight FedVaccine's potential to enhance the reliability and performance of\nAMC systems in practical wireless network environments.\n", "link": "http://arxiv.org/abs/2410.12772v1", "date": "2024-10-16", "relevancy": 2.435, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5114}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4777}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vaccinating%20Federated%20Learning%20for%20Robust%20Modulation%20Classification%20in%0A%20%20Distributed%20Wireless%20Networks&body=Title%3A%20Vaccinating%20Federated%20Learning%20for%20Robust%20Modulation%20Classification%20in%0A%20%20Distributed%20Wireless%20Networks%0AAuthor%3A%20Hunmin%20Lee%20and%20Hongju%20Seong%20and%20Wonbin%20Kim%20and%20Hyeokchan%20Kwon%20and%20Daehee%20Seo%0AAbstract%3A%20%20%20Automatic%20modulation%20classification%20%28AMC%29%20serves%20a%20vital%20role%20in%20ensuring%0Aefficient%20and%20reliable%20communication%20services%20within%20distributed%20wireless%0Anetworks.%20Recent%20developments%20have%20seen%20a%20surge%20in%20interest%20in%20deep%20neural%0Anetwork%20%28DNN%29-based%20AMC%20models%2C%20with%20Federated%20Learning%20%28FL%29%20emerging%20as%20a%0Apromising%20framework.%20Despite%20these%20advancements%2C%20the%20presence%20of%20various%20noises%0Awithin%20the%20signal%20exerts%20significant%20challenges%20while%20optimizing%20models%20to%0Acapture%20salient%20features.%20Furthermore%2C%20existing%20FL-based%20AMC%20models%20commonly%0Arely%20on%20linear%20aggregation%20strategies%2C%20which%20face%20notable%20difficulties%20in%0Aintegrating%20locally%20fine-tuned%20parameters%20within%20practical%20non-IID%20%28Independent%0Aand%20Identically%20Distributed%29%20environments%2C%20thereby%20hindering%20optimal%20learning%0Aconvergence.%20To%20address%20these%20challenges%2C%20we%20propose%20FedVaccine%2C%20a%20novel%20FL%0Amodel%20aimed%20at%20improving%20generalizability%20across%20signals%20with%20varying%20noise%0Alevels%20by%20deliberately%20introducing%20a%20balanced%20level%20of%20noise.%20This%20is%0Aaccomplished%20through%20our%20proposed%20harmonic%20noise%20resilience%20approach%2C%20which%0Aidentifies%20an%20optimal%20noise%20tolerance%20for%20DNN%20models%2C%20thereby%20regulating%20the%0Atraining%20process%20and%20mitigating%20overfitting.%20Additionally%2C%20FedVaccine%20overcomes%0Athe%20limitations%20of%20existing%20FL-based%20AMC%20models%27%20linear%20aggregation%20by%0Aemploying%20a%20split-learning%20strategy%20using%20structural%20clustering%20topology%20and%0Alocal%20queue%20data%20structure%2C%20enabling%20adaptive%20and%20cumulative%20updates%20to%20local%0Amodels.%20Our%20experimental%20results%2C%20including%20IID%20and%20non-IID%20datasets%20as%20well%20as%0Aablation%20studies%2C%20confirm%20FedVaccine%27s%20robust%20performance%20and%20superiority%20over%0Aexisting%20FL-based%20AMC%20approaches%20across%20different%20noise%20levels.%20These%20findings%0Ahighlight%20FedVaccine%27s%20potential%20to%20enhance%20the%20reliability%20and%20performance%20of%0AAMC%20systems%20in%20practical%20wireless%20network%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVaccinating%2520Federated%2520Learning%2520for%2520Robust%2520Modulation%2520Classification%2520in%250A%2520%2520Distributed%2520Wireless%2520Networks%26entry.906535625%3DHunmin%2520Lee%2520and%2520Hongju%2520Seong%2520and%2520Wonbin%2520Kim%2520and%2520Hyeokchan%2520Kwon%2520and%2520Daehee%2520Seo%26entry.1292438233%3D%2520%2520Automatic%2520modulation%2520classification%2520%2528AMC%2529%2520serves%2520a%2520vital%2520role%2520in%2520ensuring%250Aefficient%2520and%2520reliable%2520communication%2520services%2520within%2520distributed%2520wireless%250Anetworks.%2520Recent%2520developments%2520have%2520seen%2520a%2520surge%2520in%2520interest%2520in%2520deep%2520neural%250Anetwork%2520%2528DNN%2529-based%2520AMC%2520models%252C%2520with%2520Federated%2520Learning%2520%2528FL%2529%2520emerging%2520as%2520a%250Apromising%2520framework.%2520Despite%2520these%2520advancements%252C%2520the%2520presence%2520of%2520various%2520noises%250Awithin%2520the%2520signal%2520exerts%2520significant%2520challenges%2520while%2520optimizing%2520models%2520to%250Acapture%2520salient%2520features.%2520Furthermore%252C%2520existing%2520FL-based%2520AMC%2520models%2520commonly%250Arely%2520on%2520linear%2520aggregation%2520strategies%252C%2520which%2520face%2520notable%2520difficulties%2520in%250Aintegrating%2520locally%2520fine-tuned%2520parameters%2520within%2520practical%2520non-IID%2520%2528Independent%250Aand%2520Identically%2520Distributed%2529%2520environments%252C%2520thereby%2520hindering%2520optimal%2520learning%250Aconvergence.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FedVaccine%252C%2520a%2520novel%2520FL%250Amodel%2520aimed%2520at%2520improving%2520generalizability%2520across%2520signals%2520with%2520varying%2520noise%250Alevels%2520by%2520deliberately%2520introducing%2520a%2520balanced%2520level%2520of%2520noise.%2520This%2520is%250Aaccomplished%2520through%2520our%2520proposed%2520harmonic%2520noise%2520resilience%2520approach%252C%2520which%250Aidentifies%2520an%2520optimal%2520noise%2520tolerance%2520for%2520DNN%2520models%252C%2520thereby%2520regulating%2520the%250Atraining%2520process%2520and%2520mitigating%2520overfitting.%2520Additionally%252C%2520FedVaccine%2520overcomes%250Athe%2520limitations%2520of%2520existing%2520FL-based%2520AMC%2520models%2527%2520linear%2520aggregation%2520by%250Aemploying%2520a%2520split-learning%2520strategy%2520using%2520structural%2520clustering%2520topology%2520and%250Alocal%2520queue%2520data%2520structure%252C%2520enabling%2520adaptive%2520and%2520cumulative%2520updates%2520to%2520local%250Amodels.%2520Our%2520experimental%2520results%252C%2520including%2520IID%2520and%2520non-IID%2520datasets%2520as%2520well%2520as%250Aablation%2520studies%252C%2520confirm%2520FedVaccine%2527s%2520robust%2520performance%2520and%2520superiority%2520over%250Aexisting%2520FL-based%2520AMC%2520approaches%2520across%2520different%2520noise%2520levels.%2520These%2520findings%250Ahighlight%2520FedVaccine%2527s%2520potential%2520to%2520enhance%2520the%2520reliability%2520and%2520performance%2520of%250AAMC%2520systems%2520in%2520practical%2520wireless%2520network%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vaccinating%20Federated%20Learning%20for%20Robust%20Modulation%20Classification%20in%0A%20%20Distributed%20Wireless%20Networks&entry.906535625=Hunmin%20Lee%20and%20Hongju%20Seong%20and%20Wonbin%20Kim%20and%20Hyeokchan%20Kwon%20and%20Daehee%20Seo&entry.1292438233=%20%20Automatic%20modulation%20classification%20%28AMC%29%20serves%20a%20vital%20role%20in%20ensuring%0Aefficient%20and%20reliable%20communication%20services%20within%20distributed%20wireless%0Anetworks.%20Recent%20developments%20have%20seen%20a%20surge%20in%20interest%20in%20deep%20neural%0Anetwork%20%28DNN%29-based%20AMC%20models%2C%20with%20Federated%20Learning%20%28FL%29%20emerging%20as%20a%0Apromising%20framework.%20Despite%20these%20advancements%2C%20the%20presence%20of%20various%20noises%0Awithin%20the%20signal%20exerts%20significant%20challenges%20while%20optimizing%20models%20to%0Acapture%20salient%20features.%20Furthermore%2C%20existing%20FL-based%20AMC%20models%20commonly%0Arely%20on%20linear%20aggregation%20strategies%2C%20which%20face%20notable%20difficulties%20in%0Aintegrating%20locally%20fine-tuned%20parameters%20within%20practical%20non-IID%20%28Independent%0Aand%20Identically%20Distributed%29%20environments%2C%20thereby%20hindering%20optimal%20learning%0Aconvergence.%20To%20address%20these%20challenges%2C%20we%20propose%20FedVaccine%2C%20a%20novel%20FL%0Amodel%20aimed%20at%20improving%20generalizability%20across%20signals%20with%20varying%20noise%0Alevels%20by%20deliberately%20introducing%20a%20balanced%20level%20of%20noise.%20This%20is%0Aaccomplished%20through%20our%20proposed%20harmonic%20noise%20resilience%20approach%2C%20which%0Aidentifies%20an%20optimal%20noise%20tolerance%20for%20DNN%20models%2C%20thereby%20regulating%20the%0Atraining%20process%20and%20mitigating%20overfitting.%20Additionally%2C%20FedVaccine%20overcomes%0Athe%20limitations%20of%20existing%20FL-based%20AMC%20models%27%20linear%20aggregation%20by%0Aemploying%20a%20split-learning%20strategy%20using%20structural%20clustering%20topology%20and%0Alocal%20queue%20data%20structure%2C%20enabling%20adaptive%20and%20cumulative%20updates%20to%20local%0Amodels.%20Our%20experimental%20results%2C%20including%20IID%20and%20non-IID%20datasets%20as%20well%20as%0Aablation%20studies%2C%20confirm%20FedVaccine%27s%20robust%20performance%20and%20superiority%20over%0Aexisting%20FL-based%20AMC%20approaches%20across%20different%20noise%20levels.%20These%20findings%0Ahighlight%20FedVaccine%27s%20potential%20to%20enhance%20the%20reliability%20and%20performance%20of%0AAMC%20systems%20in%20practical%20wireless%20network%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12772v1&entry.124074799=Read"},
{"title": "Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation", "author": "Chenyu Zhang and Wenxue Guan and Xiaodan Xing and Guang Yang", "abstract": "  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n", "link": "http://arxiv.org/abs/2410.10551v2", "date": "2024-10-16", "relevancy": 2.4335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation&body=Title%3A%20Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation%0AAuthor%3A%20Chenyu%20Zhang%20and%20Wenxue%20Guan%20and%20Xiaodan%20Xing%20and%20Guang%20Yang%0AAbstract%3A%20%20%20Whole%20heart%20segmentation%20%28WHS%29%20supports%20cardiovascular%20disease%20%28CVD%29%0Adiagnosis%2C%20disease%20monitoring%2C%20treatment%20planning%2C%20and%20prognosis.%20Deep%20learning%0Ahas%20become%20the%20most%20widely%20used%20method%20for%20WHS%20applications%20in%20recent%20years.%0AHowever%2C%20segmentation%20of%20whole-heart%20structures%20faces%20numerous%20challenges%0Aincluding%20heart%20shape%20variability%20during%20the%20cardiac%20cycle%2C%20clinical%20artifacts%0Alike%20motion%20and%20poor%20contrast-to-noise%20ratio%2C%20domain%20shifts%20in%20multi-center%0Adata%2C%20and%20the%20distinct%20modalities%20of%20CT%20and%20MRI.%20To%20address%20these%20limitations%0Aand%20improve%20segmentation%20quality%2C%20this%20paper%20introduces%20a%20new%0Atopology-preserving%20module%20that%20is%20integrated%20into%20deep%20neural%20networks.%20The%0Aimplementation%20achieves%20anatomically%20plausible%20segmentation%20by%20using%20learned%0Atopology-preserving%20fields%2C%20which%20are%20based%20entirely%20on%203D%20convolution%20and%20are%0Atherefore%20very%20effective%20for%203D%20voxel%20data.%20We%20incorporate%20natural%20constraints%0Abetween%20structures%20into%20the%20end-to-end%20training%20and%20enrich%20the%20feature%0Arepresentation%20of%20the%20neural%20network.%20The%20effectiveness%20of%20the%20proposed%20method%0Ais%20validated%20on%20an%20open-source%20medical%20heart%20dataset%2C%20specifically%20using%20the%0AWHS%2B%2B%20data.%20The%20results%20demonstrate%20that%20the%20architecture%20performs%0Aexceptionally%20well%2C%20achieving%20a%20Dice%20coefficient%20of%200.939%20during%20testing.%20This%0Aindicates%20full%20topology%20preservation%20for%20individual%20structures%20and%0Asignificantly%20outperforms%20other%20baselines%20in%20preserving%20the%20overall%20scene%0Atopology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Cardiac%2520Integrity%253A%2520A%2520Topology-Infused%2520Approach%2520to%2520Whole%2520Heart%250A%2520%2520Segmentation%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Wenxue%2520Guan%2520and%2520Xiaodan%2520Xing%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520Whole%2520heart%2520segmentation%2520%2528WHS%2529%2520supports%2520cardiovascular%2520disease%2520%2528CVD%2529%250Adiagnosis%252C%2520disease%2520monitoring%252C%2520treatment%2520planning%252C%2520and%2520prognosis.%2520Deep%2520learning%250Ahas%2520become%2520the%2520most%2520widely%2520used%2520method%2520for%2520WHS%2520applications%2520in%2520recent%2520years.%250AHowever%252C%2520segmentation%2520of%2520whole-heart%2520structures%2520faces%2520numerous%2520challenges%250Aincluding%2520heart%2520shape%2520variability%2520during%2520the%2520cardiac%2520cycle%252C%2520clinical%2520artifacts%250Alike%2520motion%2520and%2520poor%2520contrast-to-noise%2520ratio%252C%2520domain%2520shifts%2520in%2520multi-center%250Adata%252C%2520and%2520the%2520distinct%2520modalities%2520of%2520CT%2520and%2520MRI.%2520To%2520address%2520these%2520limitations%250Aand%2520improve%2520segmentation%2520quality%252C%2520this%2520paper%2520introduces%2520a%2520new%250Atopology-preserving%2520module%2520that%2520is%2520integrated%2520into%2520deep%2520neural%2520networks.%2520The%250Aimplementation%2520achieves%2520anatomically%2520plausible%2520segmentation%2520by%2520using%2520learned%250Atopology-preserving%2520fields%252C%2520which%2520are%2520based%2520entirely%2520on%25203D%2520convolution%2520and%2520are%250Atherefore%2520very%2520effective%2520for%25203D%2520voxel%2520data.%2520We%2520incorporate%2520natural%2520constraints%250Abetween%2520structures%2520into%2520the%2520end-to-end%2520training%2520and%2520enrich%2520the%2520feature%250Arepresentation%2520of%2520the%2520neural%2520network.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%250Ais%2520validated%2520on%2520an%2520open-source%2520medical%2520heart%2520dataset%252C%2520specifically%2520using%2520the%250AWHS%252B%252B%2520data.%2520The%2520results%2520demonstrate%2520that%2520the%2520architecture%2520performs%250Aexceptionally%2520well%252C%2520achieving%2520a%2520Dice%2520coefficient%2520of%25200.939%2520during%2520testing.%2520This%250Aindicates%2520full%2520topology%2520preservation%2520for%2520individual%2520structures%2520and%250Asignificantly%2520outperforms%2520other%2520baselines%2520in%2520preserving%2520the%2520overall%2520scene%250Atopology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Cardiac%20Integrity%3A%20A%20Topology-Infused%20Approach%20to%20Whole%20Heart%0A%20%20Segmentation&entry.906535625=Chenyu%20Zhang%20and%20Wenxue%20Guan%20and%20Xiaodan%20Xing%20and%20Guang%20Yang&entry.1292438233=%20%20Whole%20heart%20segmentation%20%28WHS%29%20supports%20cardiovascular%20disease%20%28CVD%29%0Adiagnosis%2C%20disease%20monitoring%2C%20treatment%20planning%2C%20and%20prognosis.%20Deep%20learning%0Ahas%20become%20the%20most%20widely%20used%20method%20for%20WHS%20applications%20in%20recent%20years.%0AHowever%2C%20segmentation%20of%20whole-heart%20structures%20faces%20numerous%20challenges%0Aincluding%20heart%20shape%20variability%20during%20the%20cardiac%20cycle%2C%20clinical%20artifacts%0Alike%20motion%20and%20poor%20contrast-to-noise%20ratio%2C%20domain%20shifts%20in%20multi-center%0Adata%2C%20and%20the%20distinct%20modalities%20of%20CT%20and%20MRI.%20To%20address%20these%20limitations%0Aand%20improve%20segmentation%20quality%2C%20this%20paper%20introduces%20a%20new%0Atopology-preserving%20module%20that%20is%20integrated%20into%20deep%20neural%20networks.%20The%0Aimplementation%20achieves%20anatomically%20plausible%20segmentation%20by%20using%20learned%0Atopology-preserving%20fields%2C%20which%20are%20based%20entirely%20on%203D%20convolution%20and%20are%0Atherefore%20very%20effective%20for%203D%20voxel%20data.%20We%20incorporate%20natural%20constraints%0Abetween%20structures%20into%20the%20end-to-end%20training%20and%20enrich%20the%20feature%0Arepresentation%20of%20the%20neural%20network.%20The%20effectiveness%20of%20the%20proposed%20method%0Ais%20validated%20on%20an%20open-source%20medical%20heart%20dataset%2C%20specifically%20using%20the%0AWHS%2B%2B%20data.%20The%20results%20demonstrate%20that%20the%20architecture%20performs%0Aexceptionally%20well%2C%20achieving%20a%20Dice%20coefficient%20of%200.939%20during%20testing.%20This%0Aindicates%20full%20topology%20preservation%20for%20individual%20structures%20and%0Asignificantly%20outperforms%20other%20baselines%20in%20preserving%20the%20overall%20scene%0Atopology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10551v2&entry.124074799=Read"},
{"title": "VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine", "author": "Lingxiao Luo and Bingda Tang and Xuanzhong Chen and Rong Han and Ting Chen", "abstract": "  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n", "link": "http://arxiv.org/abs/2410.12694v1", "date": "2024-10-16", "relevancy": 2.4315, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VividMed%3A%20Vision%20Language%20Model%20with%20Versatile%20Visual%20Grounding%20for%0A%20%20Medicine&body=Title%3A%20VividMed%3A%20Vision%20Language%20Model%20with%20Versatile%20Visual%20Grounding%20for%0A%20%20Medicine%0AAuthor%3A%20Lingxiao%20Luo%20and%20Bingda%20Tang%20and%20Xuanzhong%20Chen%20and%20Rong%20Han%20and%20Ting%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%0Aremarkable%20promise%20in%20generating%20visually%20grounded%20responses.%20However%2C%20their%0Aapplication%20in%20the%20medical%20domain%20is%20hindered%20by%20unique%20challenges.%20For%0Ainstance%2C%20most%20VLMs%20rely%20on%20a%20single%20method%20of%20visual%20grounding%2C%20whereas%0Acomplex%20medical%20tasks%20demand%20more%20versatile%20approaches.%20Additionally%2C%20while%0Amost%20VLMs%20process%20only%202D%20images%2C%20a%20large%20portion%20of%20medical%20images%20are%203D.%20The%0Alack%20of%20medical%20data%20further%20compounds%20these%20obstacles.%20To%20address%20these%0Achallenges%2C%20we%20present%20VividMed%2C%20a%20vision%20language%20model%20with%20versatile%20visual%0Agrounding%20for%20medicine.%20Our%20model%20supports%20generating%20both%20semantic%0Asegmentation%20masks%20and%20instance-level%20bounding%20boxes%2C%20and%20accommodates%20various%0Aimaging%20modalities%2C%20including%20both%202D%20and%203D%20data.%20We%20design%20a%20three-stage%0Atraining%20procedure%20and%20an%20automatic%20data%20synthesis%20pipeline%20based%20on%20open%0Adatasets%20and%20models.%20Besides%20visual%20grounding%20tasks%2C%20VividMed%20also%20excels%20in%0Aother%20common%20downstream%20tasks%2C%20including%20Visual%20Question%20Answering%20%28VQA%29%20and%0Areport%20generation.%20Ablation%20studies%20empirically%20show%20that%20the%20integration%20of%0Avisual%20grounding%20ability%20leads%20to%20improved%20performance%20on%20these%20tasks.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/function2-llx/MMMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVividMed%253A%2520Vision%2520Language%2520Model%2520with%2520Versatile%2520Visual%2520Grounding%2520for%250A%2520%2520Medicine%26entry.906535625%3DLingxiao%2520Luo%2520and%2520Bingda%2520Tang%2520and%2520Xuanzhong%2520Chen%2520and%2520Rong%2520Han%2520and%2520Ting%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%250Aremarkable%2520promise%2520in%2520generating%2520visually%2520grounded%2520responses.%2520However%252C%2520their%250Aapplication%2520in%2520the%2520medical%2520domain%2520is%2520hindered%2520by%2520unique%2520challenges.%2520For%250Ainstance%252C%2520most%2520VLMs%2520rely%2520on%2520a%2520single%2520method%2520of%2520visual%2520grounding%252C%2520whereas%250Acomplex%2520medical%2520tasks%2520demand%2520more%2520versatile%2520approaches.%2520Additionally%252C%2520while%250Amost%2520VLMs%2520process%2520only%25202D%2520images%252C%2520a%2520large%2520portion%2520of%2520medical%2520images%2520are%25203D.%2520The%250Alack%2520of%2520medical%2520data%2520further%2520compounds%2520these%2520obstacles.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520present%2520VividMed%252C%2520a%2520vision%2520language%2520model%2520with%2520versatile%2520visual%250Agrounding%2520for%2520medicine.%2520Our%2520model%2520supports%2520generating%2520both%2520semantic%250Asegmentation%2520masks%2520and%2520instance-level%2520bounding%2520boxes%252C%2520and%2520accommodates%2520various%250Aimaging%2520modalities%252C%2520including%2520both%25202D%2520and%25203D%2520data.%2520We%2520design%2520a%2520three-stage%250Atraining%2520procedure%2520and%2520an%2520automatic%2520data%2520synthesis%2520pipeline%2520based%2520on%2520open%250Adatasets%2520and%2520models.%2520Besides%2520visual%2520grounding%2520tasks%252C%2520VividMed%2520also%2520excels%2520in%250Aother%2520common%2520downstream%2520tasks%252C%2520including%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520and%250Areport%2520generation.%2520Ablation%2520studies%2520empirically%2520show%2520that%2520the%2520integration%2520of%250Avisual%2520grounding%2520ability%2520leads%2520to%2520improved%2520performance%2520on%2520these%2520tasks.%2520Our%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/function2-llx/MMMM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VividMed%3A%20Vision%20Language%20Model%20with%20Versatile%20Visual%20Grounding%20for%0A%20%20Medicine&entry.906535625=Lingxiao%20Luo%20and%20Bingda%20Tang%20and%20Xuanzhong%20Chen%20and%20Rong%20Han%20and%20Ting%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20Vision%20Language%20Models%20%28VLMs%29%20have%20demonstrated%0Aremarkable%20promise%20in%20generating%20visually%20grounded%20responses.%20However%2C%20their%0Aapplication%20in%20the%20medical%20domain%20is%20hindered%20by%20unique%20challenges.%20For%0Ainstance%2C%20most%20VLMs%20rely%20on%20a%20single%20method%20of%20visual%20grounding%2C%20whereas%0Acomplex%20medical%20tasks%20demand%20more%20versatile%20approaches.%20Additionally%2C%20while%0Amost%20VLMs%20process%20only%202D%20images%2C%20a%20large%20portion%20of%20medical%20images%20are%203D.%20The%0Alack%20of%20medical%20data%20further%20compounds%20these%20obstacles.%20To%20address%20these%0Achallenges%2C%20we%20present%20VividMed%2C%20a%20vision%20language%20model%20with%20versatile%20visual%0Agrounding%20for%20medicine.%20Our%20model%20supports%20generating%20both%20semantic%0Asegmentation%20masks%20and%20instance-level%20bounding%20boxes%2C%20and%20accommodates%20various%0Aimaging%20modalities%2C%20including%20both%202D%20and%203D%20data.%20We%20design%20a%20three-stage%0Atraining%20procedure%20and%20an%20automatic%20data%20synthesis%20pipeline%20based%20on%20open%0Adatasets%20and%20models.%20Besides%20visual%20grounding%20tasks%2C%20VividMed%20also%20excels%20in%0Aother%20common%20downstream%20tasks%2C%20including%20Visual%20Question%20Answering%20%28VQA%29%20and%0Areport%20generation.%20Ablation%20studies%20empirically%20show%20that%20the%20integration%20of%0Avisual%20grounding%20ability%20leads%20to%20improved%20performance%20on%20these%20tasks.%20Our%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/function2-llx/MMMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12694v1&entry.124074799=Read"},
{"title": "Perseus: Leveraging Common Data Patterns with Curriculum Learning for\n  More Robust Graph Neural Networks", "author": "Kaiwen Xia and Huijun Wu and Duanyu Li and Min Xie and Ruibo Wang and Wenzhe Zhang", "abstract": "  Graph Neural Networks (GNNs) excel at handling graph data but remain\nvulnerable to adversarial attacks. Existing defense methods typically rely on\nassumptions like graph sparsity and homophily to either preprocess the graph or\nguide structure learning. However, preprocessing methods often struggle to\naccurately distinguish between normal edges and adversarial perturbations,\nleading to suboptimal results due to the loss of valuable edge information.\nRobust graph neural network models train directly on graph data affected by\nadversarial perturbations, without preprocessing. This can cause the model to\nget stuck in poor local optima, negatively affecting its performance. To\naddress these challenges, we propose Perseus, a novel adversarial defense\nmethod based on curriculum learning. Perseus assesses edge difficulty using\nglobal homophily and applies a curriculum learning strategy to adjust the\nlearning order, guiding the model to learn the full graph structure while\nadaptively focusing on common data patterns. This approach mitigates the impact\nof adversarial perturbations. Experiments show that models trained with Perseus\nachieve superior performance and are significantly more robust to adversarial\nattacks.\n", "link": "http://arxiv.org/abs/2410.12425v1", "date": "2024-10-16", "relevancy": 2.4178, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4904}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perseus%3A%20Leveraging%20Common%20Data%20Patterns%20with%20Curriculum%20Learning%20for%0A%20%20More%20Robust%20Graph%20Neural%20Networks&body=Title%3A%20Perseus%3A%20Leveraging%20Common%20Data%20Patterns%20with%20Curriculum%20Learning%20for%0A%20%20More%20Robust%20Graph%20Neural%20Networks%0AAuthor%3A%20Kaiwen%20Xia%20and%20Huijun%20Wu%20and%20Duanyu%20Li%20and%20Min%20Xie%20and%20Ruibo%20Wang%20and%20Wenzhe%20Zhang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20handling%20graph%20data%20but%20remain%0Avulnerable%20to%20adversarial%20attacks.%20Existing%20defense%20methods%20typically%20rely%20on%0Aassumptions%20like%20graph%20sparsity%20and%20homophily%20to%20either%20preprocess%20the%20graph%20or%0Aguide%20structure%20learning.%20However%2C%20preprocessing%20methods%20often%20struggle%20to%0Aaccurately%20distinguish%20between%20normal%20edges%20and%20adversarial%20perturbations%2C%0Aleading%20to%20suboptimal%20results%20due%20to%20the%20loss%20of%20valuable%20edge%20information.%0ARobust%20graph%20neural%20network%20models%20train%20directly%20on%20graph%20data%20affected%20by%0Aadversarial%20perturbations%2C%20without%20preprocessing.%20This%20can%20cause%20the%20model%20to%0Aget%20stuck%20in%20poor%20local%20optima%2C%20negatively%20affecting%20its%20performance.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Perseus%2C%20a%20novel%20adversarial%20defense%0Amethod%20based%20on%20curriculum%20learning.%20Perseus%20assesses%20edge%20difficulty%20using%0Aglobal%20homophily%20and%20applies%20a%20curriculum%20learning%20strategy%20to%20adjust%20the%0Alearning%20order%2C%20guiding%20the%20model%20to%20learn%20the%20full%20graph%20structure%20while%0Aadaptively%20focusing%20on%20common%20data%20patterns.%20This%20approach%20mitigates%20the%20impact%0Aof%20adversarial%20perturbations.%20Experiments%20show%20that%20models%20trained%20with%20Perseus%0Aachieve%20superior%20performance%20and%20are%20significantly%20more%20robust%20to%20adversarial%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerseus%253A%2520Leveraging%2520Common%2520Data%2520Patterns%2520with%2520Curriculum%2520Learning%2520for%250A%2520%2520More%2520Robust%2520Graph%2520Neural%2520Networks%26entry.906535625%3DKaiwen%2520Xia%2520and%2520Huijun%2520Wu%2520and%2520Duanyu%2520Li%2520and%2520Min%2520Xie%2520and%2520Ruibo%2520Wang%2520and%2520Wenzhe%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520at%2520handling%2520graph%2520data%2520but%2520remain%250Avulnerable%2520to%2520adversarial%2520attacks.%2520Existing%2520defense%2520methods%2520typically%2520rely%2520on%250Aassumptions%2520like%2520graph%2520sparsity%2520and%2520homophily%2520to%2520either%2520preprocess%2520the%2520graph%2520or%250Aguide%2520structure%2520learning.%2520However%252C%2520preprocessing%2520methods%2520often%2520struggle%2520to%250Aaccurately%2520distinguish%2520between%2520normal%2520edges%2520and%2520adversarial%2520perturbations%252C%250Aleading%2520to%2520suboptimal%2520results%2520due%2520to%2520the%2520loss%2520of%2520valuable%2520edge%2520information.%250ARobust%2520graph%2520neural%2520network%2520models%2520train%2520directly%2520on%2520graph%2520data%2520affected%2520by%250Aadversarial%2520perturbations%252C%2520without%2520preprocessing.%2520This%2520can%2520cause%2520the%2520model%2520to%250Aget%2520stuck%2520in%2520poor%2520local%2520optima%252C%2520negatively%2520affecting%2520its%2520performance.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Perseus%252C%2520a%2520novel%2520adversarial%2520defense%250Amethod%2520based%2520on%2520curriculum%2520learning.%2520Perseus%2520assesses%2520edge%2520difficulty%2520using%250Aglobal%2520homophily%2520and%2520applies%2520a%2520curriculum%2520learning%2520strategy%2520to%2520adjust%2520the%250Alearning%2520order%252C%2520guiding%2520the%2520model%2520to%2520learn%2520the%2520full%2520graph%2520structure%2520while%250Aadaptively%2520focusing%2520on%2520common%2520data%2520patterns.%2520This%2520approach%2520mitigates%2520the%2520impact%250Aof%2520adversarial%2520perturbations.%2520Experiments%2520show%2520that%2520models%2520trained%2520with%2520Perseus%250Aachieve%2520superior%2520performance%2520and%2520are%2520significantly%2520more%2520robust%2520to%2520adversarial%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perseus%3A%20Leveraging%20Common%20Data%20Patterns%20with%20Curriculum%20Learning%20for%0A%20%20More%20Robust%20Graph%20Neural%20Networks&entry.906535625=Kaiwen%20Xia%20and%20Huijun%20Wu%20and%20Duanyu%20Li%20and%20Min%20Xie%20and%20Ruibo%20Wang%20and%20Wenzhe%20Zhang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20at%20handling%20graph%20data%20but%20remain%0Avulnerable%20to%20adversarial%20attacks.%20Existing%20defense%20methods%20typically%20rely%20on%0Aassumptions%20like%20graph%20sparsity%20and%20homophily%20to%20either%20preprocess%20the%20graph%20or%0Aguide%20structure%20learning.%20However%2C%20preprocessing%20methods%20often%20struggle%20to%0Aaccurately%20distinguish%20between%20normal%20edges%20and%20adversarial%20perturbations%2C%0Aleading%20to%20suboptimal%20results%20due%20to%20the%20loss%20of%20valuable%20edge%20information.%0ARobust%20graph%20neural%20network%20models%20train%20directly%20on%20graph%20data%20affected%20by%0Aadversarial%20perturbations%2C%20without%20preprocessing.%20This%20can%20cause%20the%20model%20to%0Aget%20stuck%20in%20poor%20local%20optima%2C%20negatively%20affecting%20its%20performance.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Perseus%2C%20a%20novel%20adversarial%20defense%0Amethod%20based%20on%20curriculum%20learning.%20Perseus%20assesses%20edge%20difficulty%20using%0Aglobal%20homophily%20and%20applies%20a%20curriculum%20learning%20strategy%20to%20adjust%20the%0Alearning%20order%2C%20guiding%20the%20model%20to%20learn%20the%20full%20graph%20structure%20while%0Aadaptively%20focusing%20on%20common%20data%20patterns.%20This%20approach%20mitigates%20the%20impact%0Aof%20adversarial%20perturbations.%20Experiments%20show%20that%20models%20trained%20with%20Perseus%0Aachieve%20superior%20performance%20and%20are%20significantly%20more%20robust%20to%20adversarial%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12425v1&entry.124074799=Read"},
{"title": "Likelihood-based Differentiable Structure Learning", "author": "Chang Deng and Kevin Bello and Pradeep Ravikumar and Bryon Aragam", "abstract": "  Existing approaches to differentiable structure learning of directed acyclic\ngraphs (DAGs) rely on strong identifiability assumptions in order to guarantee\nthat global minimizers of the acyclicity-constrained optimization problem\nidentifies the true DAG. Moreover, it has been observed empirically that the\noptimizer may exploit undesirable artifacts in the loss function. We explain\nand remedy these issues by studying the behavior of differentiable\nacyclicity-constrained programs under general likelihoods with multiple global\nminimizers. By carefully regularizing the likelihood, it is possible to\nidentify the sparsest model in the Markov equivalence class, even in the\nabsence of an identifiable parametrization. We first study the Gaussian case in\ndetail, showing how proper regularization of the likelihood defines a score\nthat identifies the sparsest model. Assuming faithfulness, it also recovers the\nMarkov equivalence class. These results are then generalized to general models\nand likelihoods, where the same claims hold. These theoretical results are\nvalidated empirically, showing how this can be done using standard\ngradient-based optimizers, thus paving the way for differentiable structure\nlearning under general models and losses.\n", "link": "http://arxiv.org/abs/2410.06163v2", "date": "2024-10-16", "relevancy": 2.4129, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4995}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4761}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood-based%20Differentiable%20Structure%20Learning&body=Title%3A%20Likelihood-based%20Differentiable%20Structure%20Learning%0AAuthor%3A%20Chang%20Deng%20and%20Kevin%20Bello%20and%20Pradeep%20Ravikumar%20and%20Bryon%20Aragam%0AAbstract%3A%20%20%20Existing%20approaches%20to%20differentiable%20structure%20learning%20of%20directed%20acyclic%0Agraphs%20%28DAGs%29%20rely%20on%20strong%20identifiability%20assumptions%20in%20order%20to%20guarantee%0Athat%20global%20minimizers%20of%20the%20acyclicity-constrained%20optimization%20problem%0Aidentifies%20the%20true%20DAG.%20Moreover%2C%20it%20has%20been%20observed%20empirically%20that%20the%0Aoptimizer%20may%20exploit%20undesirable%20artifacts%20in%20the%20loss%20function.%20We%20explain%0Aand%20remedy%20these%20issues%20by%20studying%20the%20behavior%20of%20differentiable%0Aacyclicity-constrained%20programs%20under%20general%20likelihoods%20with%20multiple%20global%0Aminimizers.%20By%20carefully%20regularizing%20the%20likelihood%2C%20it%20is%20possible%20to%0Aidentify%20the%20sparsest%20model%20in%20the%20Markov%20equivalence%20class%2C%20even%20in%20the%0Aabsence%20of%20an%20identifiable%20parametrization.%20We%20first%20study%20the%20Gaussian%20case%20in%0Adetail%2C%20showing%20how%20proper%20regularization%20of%20the%20likelihood%20defines%20a%20score%0Athat%20identifies%20the%20sparsest%20model.%20Assuming%20faithfulness%2C%20it%20also%20recovers%20the%0AMarkov%20equivalence%20class.%20These%20results%20are%20then%20generalized%20to%20general%20models%0Aand%20likelihoods%2C%20where%20the%20same%20claims%20hold.%20These%20theoretical%20results%20are%0Avalidated%20empirically%2C%20showing%20how%20this%20can%20be%20done%20using%20standard%0Agradient-based%20optimizers%2C%20thus%20paving%20the%20way%20for%20differentiable%20structure%0Alearning%20under%20general%20models%20and%20losses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood-based%2520Differentiable%2520Structure%2520Learning%26entry.906535625%3DChang%2520Deng%2520and%2520Kevin%2520Bello%2520and%2520Pradeep%2520Ravikumar%2520and%2520Bryon%2520Aragam%26entry.1292438233%3D%2520%2520Existing%2520approaches%2520to%2520differentiable%2520structure%2520learning%2520of%2520directed%2520acyclic%250Agraphs%2520%2528DAGs%2529%2520rely%2520on%2520strong%2520identifiability%2520assumptions%2520in%2520order%2520to%2520guarantee%250Athat%2520global%2520minimizers%2520of%2520the%2520acyclicity-constrained%2520optimization%2520problem%250Aidentifies%2520the%2520true%2520DAG.%2520Moreover%252C%2520it%2520has%2520been%2520observed%2520empirically%2520that%2520the%250Aoptimizer%2520may%2520exploit%2520undesirable%2520artifacts%2520in%2520the%2520loss%2520function.%2520We%2520explain%250Aand%2520remedy%2520these%2520issues%2520by%2520studying%2520the%2520behavior%2520of%2520differentiable%250Aacyclicity-constrained%2520programs%2520under%2520general%2520likelihoods%2520with%2520multiple%2520global%250Aminimizers.%2520By%2520carefully%2520regularizing%2520the%2520likelihood%252C%2520it%2520is%2520possible%2520to%250Aidentify%2520the%2520sparsest%2520model%2520in%2520the%2520Markov%2520equivalence%2520class%252C%2520even%2520in%2520the%250Aabsence%2520of%2520an%2520identifiable%2520parametrization.%2520We%2520first%2520study%2520the%2520Gaussian%2520case%2520in%250Adetail%252C%2520showing%2520how%2520proper%2520regularization%2520of%2520the%2520likelihood%2520defines%2520a%2520score%250Athat%2520identifies%2520the%2520sparsest%2520model.%2520Assuming%2520faithfulness%252C%2520it%2520also%2520recovers%2520the%250AMarkov%2520equivalence%2520class.%2520These%2520results%2520are%2520then%2520generalized%2520to%2520general%2520models%250Aand%2520likelihoods%252C%2520where%2520the%2520same%2520claims%2520hold.%2520These%2520theoretical%2520results%2520are%250Avalidated%2520empirically%252C%2520showing%2520how%2520this%2520can%2520be%2520done%2520using%2520standard%250Agradient-based%2520optimizers%252C%2520thus%2520paving%2520the%2520way%2520for%2520differentiable%2520structure%250Alearning%2520under%2520general%2520models%2520and%2520losses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood-based%20Differentiable%20Structure%20Learning&entry.906535625=Chang%20Deng%20and%20Kevin%20Bello%20and%20Pradeep%20Ravikumar%20and%20Bryon%20Aragam&entry.1292438233=%20%20Existing%20approaches%20to%20differentiable%20structure%20learning%20of%20directed%20acyclic%0Agraphs%20%28DAGs%29%20rely%20on%20strong%20identifiability%20assumptions%20in%20order%20to%20guarantee%0Athat%20global%20minimizers%20of%20the%20acyclicity-constrained%20optimization%20problem%0Aidentifies%20the%20true%20DAG.%20Moreover%2C%20it%20has%20been%20observed%20empirically%20that%20the%0Aoptimizer%20may%20exploit%20undesirable%20artifacts%20in%20the%20loss%20function.%20We%20explain%0Aand%20remedy%20these%20issues%20by%20studying%20the%20behavior%20of%20differentiable%0Aacyclicity-constrained%20programs%20under%20general%20likelihoods%20with%20multiple%20global%0Aminimizers.%20By%20carefully%20regularizing%20the%20likelihood%2C%20it%20is%20possible%20to%0Aidentify%20the%20sparsest%20model%20in%20the%20Markov%20equivalence%20class%2C%20even%20in%20the%0Aabsence%20of%20an%20identifiable%20parametrization.%20We%20first%20study%20the%20Gaussian%20case%20in%0Adetail%2C%20showing%20how%20proper%20regularization%20of%20the%20likelihood%20defines%20a%20score%0Athat%20identifies%20the%20sparsest%20model.%20Assuming%20faithfulness%2C%20it%20also%20recovers%20the%0AMarkov%20equivalence%20class.%20These%20results%20are%20then%20generalized%20to%20general%20models%0Aand%20likelihoods%2C%20where%20the%20same%20claims%20hold.%20These%20theoretical%20results%20are%0Avalidated%20empirically%2C%20showing%20how%20this%20can%20be%20done%20using%20standard%0Agradient-based%20optimizers%2C%20thus%20paving%20the%20way%20for%20differentiable%20structure%0Alearning%20under%20general%20models%20and%20losses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06163v2&entry.124074799=Read"},
{"title": "SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation", "author": "Jaehong Yoon and Shoubin Yu and Vaidehi Patil and Huaxiu Yao and Mohit Bansal", "abstract": "  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n", "link": "http://arxiv.org/abs/2410.12761v1", "date": "2024-10-16", "relevancy": 2.4069, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6153}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.594}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFREE%3A%20Training-Free%20and%20Adaptive%20Guard%20for%20Safe%20Text-to-Image%20And%0A%20%20Video%20Generation&body=Title%3A%20SAFREE%3A%20Training-Free%20and%20Adaptive%20Guard%20for%20Safe%20Text-to-Image%20And%0A%20%20Video%20Generation%0AAuthor%3A%20Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Vaidehi%20Patil%20and%20Huaxiu%20Yao%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20enhanced%20their%20ability%0Ato%20generate%20high-quality%20images%20and%20videos%2C%20but%20they%20have%20also%20increased%20the%0Arisk%20of%20producing%20unsafe%20content.%20Existing%20unlearning/editing-based%20methods%20for%0Asafe%20generation%20remove%20harmful%20concepts%20from%20models%20but%20face%20several%0Achallenges%3A%20%281%29%20They%20cannot%20instantly%20remove%20harmful%20concepts%20without%20training.%0A%282%29%20Their%20safe%20generation%20capabilities%20depend%20on%20collected%20training%20data.%20%283%29%0AThey%20alter%20model%20weights%2C%20risking%20degradation%20in%20quality%20for%20content%20unrelated%0Ato%20toxic%20concepts.%20To%20address%20these%2C%20we%20propose%20SAFREE%2C%20a%20novel%2C%20training-free%0Aapproach%20for%20safe%20T2I%20and%20T2V%2C%20that%20does%20not%20alter%20the%20model%27s%20weights.%0ASpecifically%2C%20we%20detect%20a%20subspace%20corresponding%20to%20a%20set%20of%20toxic%20concepts%20in%0Athe%20text%20embedding%20space%20and%20steer%20prompt%20embeddings%20away%20from%20this%20subspace%2C%0Athereby%20filtering%20out%20harmful%20content%20while%20preserving%20intended%20semantics.%20To%0Abalance%20the%20trade-off%20between%20filtering%20toxicity%20and%20preserving%20safe%20concepts%2C%0ASAFREE%20incorporates%20a%20novel%20self-validating%20filtering%20mechanism%20that%0Adynamically%20adjusts%20the%20denoising%20steps%20when%20applying%20the%20filtered%20embeddings.%0AAdditionally%2C%20we%20incorporate%20adaptive%20re-attention%20mechanisms%20within%20the%0Adiffusion%20latent%20space%20to%20selectively%20diminish%20the%20influence%20of%20features%0Arelated%20to%20toxic%20concepts%20at%20the%20pixel%20level.%20In%20the%20end%2C%20SAFREE%20ensures%0Acoherent%20safety%20checking%2C%20preserving%20the%20fidelity%2C%20quality%2C%20and%20safety%20of%20the%0Aoutput.%20SAFREE%20achieves%20SOTA%20performance%20in%20suppressing%20unsafe%20content%20in%20T2I%0Ageneration%20compared%20to%20training-free%20baselines%20and%20effectively%20filters%20targeted%0Aconcepts%20while%20maintaining%20high-quality%20images.%20It%20also%20shows%20competitive%0Aresults%20against%20training-based%20methods.%20We%20extend%20SAFREE%20to%20various%20T2I%0Abackbones%20and%20T2V%20tasks%2C%20showcasing%20its%20flexibility%20and%20generalization.%20SAFREE%0Aprovides%20a%20robust%20and%20adaptable%20safeguard%20for%20ensuring%20safe%20visual%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFREE%253A%2520Training-Free%2520and%2520Adaptive%2520Guard%2520for%2520Safe%2520Text-to-Image%2520And%250A%2520%2520Video%2520Generation%26entry.906535625%3DJaehong%2520Yoon%2520and%2520Shoubin%2520Yu%2520and%2520Vaidehi%2520Patil%2520and%2520Huaxiu%2520Yao%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520significantly%2520enhanced%2520their%2520ability%250Ato%2520generate%2520high-quality%2520images%2520and%2520videos%252C%2520but%2520they%2520have%2520also%2520increased%2520the%250Arisk%2520of%2520producing%2520unsafe%2520content.%2520Existing%2520unlearning/editing-based%2520methods%2520for%250Asafe%2520generation%2520remove%2520harmful%2520concepts%2520from%2520models%2520but%2520face%2520several%250Achallenges%253A%2520%25281%2529%2520They%2520cannot%2520instantly%2520remove%2520harmful%2520concepts%2520without%2520training.%250A%25282%2529%2520Their%2520safe%2520generation%2520capabilities%2520depend%2520on%2520collected%2520training%2520data.%2520%25283%2529%250AThey%2520alter%2520model%2520weights%252C%2520risking%2520degradation%2520in%2520quality%2520for%2520content%2520unrelated%250Ato%2520toxic%2520concepts.%2520To%2520address%2520these%252C%2520we%2520propose%2520SAFREE%252C%2520a%2520novel%252C%2520training-free%250Aapproach%2520for%2520safe%2520T2I%2520and%2520T2V%252C%2520that%2520does%2520not%2520alter%2520the%2520model%2527s%2520weights.%250ASpecifically%252C%2520we%2520detect%2520a%2520subspace%2520corresponding%2520to%2520a%2520set%2520of%2520toxic%2520concepts%2520in%250Athe%2520text%2520embedding%2520space%2520and%2520steer%2520prompt%2520embeddings%2520away%2520from%2520this%2520subspace%252C%250Athereby%2520filtering%2520out%2520harmful%2520content%2520while%2520preserving%2520intended%2520semantics.%2520To%250Abalance%2520the%2520trade-off%2520between%2520filtering%2520toxicity%2520and%2520preserving%2520safe%2520concepts%252C%250ASAFREE%2520incorporates%2520a%2520novel%2520self-validating%2520filtering%2520mechanism%2520that%250Adynamically%2520adjusts%2520the%2520denoising%2520steps%2520when%2520applying%2520the%2520filtered%2520embeddings.%250AAdditionally%252C%2520we%2520incorporate%2520adaptive%2520re-attention%2520mechanisms%2520within%2520the%250Adiffusion%2520latent%2520space%2520to%2520selectively%2520diminish%2520the%2520influence%2520of%2520features%250Arelated%2520to%2520toxic%2520concepts%2520at%2520the%2520pixel%2520level.%2520In%2520the%2520end%252C%2520SAFREE%2520ensures%250Acoherent%2520safety%2520checking%252C%2520preserving%2520the%2520fidelity%252C%2520quality%252C%2520and%2520safety%2520of%2520the%250Aoutput.%2520SAFREE%2520achieves%2520SOTA%2520performance%2520in%2520suppressing%2520unsafe%2520content%2520in%2520T2I%250Ageneration%2520compared%2520to%2520training-free%2520baselines%2520and%2520effectively%2520filters%2520targeted%250Aconcepts%2520while%2520maintaining%2520high-quality%2520images.%2520It%2520also%2520shows%2520competitive%250Aresults%2520against%2520training-based%2520methods.%2520We%2520extend%2520SAFREE%2520to%2520various%2520T2I%250Abackbones%2520and%2520T2V%2520tasks%252C%2520showcasing%2520its%2520flexibility%2520and%2520generalization.%2520SAFREE%250Aprovides%2520a%2520robust%2520and%2520adaptable%2520safeguard%2520for%2520ensuring%2520safe%2520visual%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFREE%3A%20Training-Free%20and%20Adaptive%20Guard%20for%20Safe%20Text-to-Image%20And%0A%20%20Video%20Generation&entry.906535625=Jaehong%20Yoon%20and%20Shoubin%20Yu%20and%20Vaidehi%20Patil%20and%20Huaxiu%20Yao%20and%20Mohit%20Bansal&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20enhanced%20their%20ability%0Ato%20generate%20high-quality%20images%20and%20videos%2C%20but%20they%20have%20also%20increased%20the%0Arisk%20of%20producing%20unsafe%20content.%20Existing%20unlearning/editing-based%20methods%20for%0Asafe%20generation%20remove%20harmful%20concepts%20from%20models%20but%20face%20several%0Achallenges%3A%20%281%29%20They%20cannot%20instantly%20remove%20harmful%20concepts%20without%20training.%0A%282%29%20Their%20safe%20generation%20capabilities%20depend%20on%20collected%20training%20data.%20%283%29%0AThey%20alter%20model%20weights%2C%20risking%20degradation%20in%20quality%20for%20content%20unrelated%0Ato%20toxic%20concepts.%20To%20address%20these%2C%20we%20propose%20SAFREE%2C%20a%20novel%2C%20training-free%0Aapproach%20for%20safe%20T2I%20and%20T2V%2C%20that%20does%20not%20alter%20the%20model%27s%20weights.%0ASpecifically%2C%20we%20detect%20a%20subspace%20corresponding%20to%20a%20set%20of%20toxic%20concepts%20in%0Athe%20text%20embedding%20space%20and%20steer%20prompt%20embeddings%20away%20from%20this%20subspace%2C%0Athereby%20filtering%20out%20harmful%20content%20while%20preserving%20intended%20semantics.%20To%0Abalance%20the%20trade-off%20between%20filtering%20toxicity%20and%20preserving%20safe%20concepts%2C%0ASAFREE%20incorporates%20a%20novel%20self-validating%20filtering%20mechanism%20that%0Adynamically%20adjusts%20the%20denoising%20steps%20when%20applying%20the%20filtered%20embeddings.%0AAdditionally%2C%20we%20incorporate%20adaptive%20re-attention%20mechanisms%20within%20the%0Adiffusion%20latent%20space%20to%20selectively%20diminish%20the%20influence%20of%20features%0Arelated%20to%20toxic%20concepts%20at%20the%20pixel%20level.%20In%20the%20end%2C%20SAFREE%20ensures%0Acoherent%20safety%20checking%2C%20preserving%20the%20fidelity%2C%20quality%2C%20and%20safety%20of%20the%0Aoutput.%20SAFREE%20achieves%20SOTA%20performance%20in%20suppressing%20unsafe%20content%20in%20T2I%0Ageneration%20compared%20to%20training-free%20baselines%20and%20effectively%20filters%20targeted%0Aconcepts%20while%20maintaining%20high-quality%20images.%20It%20also%20shows%20competitive%0Aresults%20against%20training-based%20methods.%20We%20extend%20SAFREE%20to%20various%20T2I%0Abackbones%20and%20T2V%20tasks%2C%20showcasing%20its%20flexibility%20and%20generalization.%20SAFREE%0Aprovides%20a%20robust%20and%20adaptable%20safeguard%20for%20ensuring%20safe%20visual%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12761v1&entry.124074799=Read"},
{"title": "Efficient Optimization Algorithms for Linear Adversarial Training", "author": "Ant\u00f4nio H. RIbeiro and Thomas B. Sch\u00f6n and Dave Zahariah and Francis Bach", "abstract": "  Adversarial training can be used to learn models that are robust against\nperturbations. For linear models, it can be formulated as a convex optimization\nproblem. Compared to methods proposed in the context of deep learning,\nleveraging the optimization structure allows significantly faster convergence\nrates. Still, the use of generic convex solvers can be inefficient for\nlarge-scale problems. Here, we propose tailored optimization algorithms for the\nadversarial training of linear models, which render large-scale regression and\nclassification problems more tractable. For regression problems, we propose a\nfamily of solvers based on iterative ridge regression and, for classification,\na family of solvers based on projected gradient descent. The methods are based\non extended variable reformulations of the original problem. We illustrate\ntheir efficiency in numerical examples.\n", "link": "http://arxiv.org/abs/2410.12677v1", "date": "2024-10-16", "relevancy": 2.401, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4854}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Optimization%20Algorithms%20for%20Linear%20Adversarial%20Training&body=Title%3A%20Efficient%20Optimization%20Algorithms%20for%20Linear%20Adversarial%20Training%0AAuthor%3A%20Ant%C3%B4nio%20H.%20RIbeiro%20and%20Thomas%20B.%20Sch%C3%B6n%20and%20Dave%20Zahariah%20and%20Francis%20Bach%0AAbstract%3A%20%20%20Adversarial%20training%20can%20be%20used%20to%20learn%20models%20that%20are%20robust%20against%0Aperturbations.%20For%20linear%20models%2C%20it%20can%20be%20formulated%20as%20a%20convex%20optimization%0Aproblem.%20Compared%20to%20methods%20proposed%20in%20the%20context%20of%20deep%20learning%2C%0Aleveraging%20the%20optimization%20structure%20allows%20significantly%20faster%20convergence%0Arates.%20Still%2C%20the%20use%20of%20generic%20convex%20solvers%20can%20be%20inefficient%20for%0Alarge-scale%20problems.%20Here%2C%20we%20propose%20tailored%20optimization%20algorithms%20for%20the%0Aadversarial%20training%20of%20linear%20models%2C%20which%20render%20large-scale%20regression%20and%0Aclassification%20problems%20more%20tractable.%20For%20regression%20problems%2C%20we%20propose%20a%0Afamily%20of%20solvers%20based%20on%20iterative%20ridge%20regression%20and%2C%20for%20classification%2C%0Aa%20family%20of%20solvers%20based%20on%20projected%20gradient%20descent.%20The%20methods%20are%20based%0Aon%20extended%20variable%20reformulations%20of%20the%20original%20problem.%20We%20illustrate%0Atheir%20efficiency%20in%20numerical%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Optimization%2520Algorithms%2520for%2520Linear%2520Adversarial%2520Training%26entry.906535625%3DAnt%25C3%25B4nio%2520H.%2520RIbeiro%2520and%2520Thomas%2520B.%2520Sch%25C3%25B6n%2520and%2520Dave%2520Zahariah%2520and%2520Francis%2520Bach%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520can%2520be%2520used%2520to%2520learn%2520models%2520that%2520are%2520robust%2520against%250Aperturbations.%2520For%2520linear%2520models%252C%2520it%2520can%2520be%2520formulated%2520as%2520a%2520convex%2520optimization%250Aproblem.%2520Compared%2520to%2520methods%2520proposed%2520in%2520the%2520context%2520of%2520deep%2520learning%252C%250Aleveraging%2520the%2520optimization%2520structure%2520allows%2520significantly%2520faster%2520convergence%250Arates.%2520Still%252C%2520the%2520use%2520of%2520generic%2520convex%2520solvers%2520can%2520be%2520inefficient%2520for%250Alarge-scale%2520problems.%2520Here%252C%2520we%2520propose%2520tailored%2520optimization%2520algorithms%2520for%2520the%250Aadversarial%2520training%2520of%2520linear%2520models%252C%2520which%2520render%2520large-scale%2520regression%2520and%250Aclassification%2520problems%2520more%2520tractable.%2520For%2520regression%2520problems%252C%2520we%2520propose%2520a%250Afamily%2520of%2520solvers%2520based%2520on%2520iterative%2520ridge%2520regression%2520and%252C%2520for%2520classification%252C%250Aa%2520family%2520of%2520solvers%2520based%2520on%2520projected%2520gradient%2520descent.%2520The%2520methods%2520are%2520based%250Aon%2520extended%2520variable%2520reformulations%2520of%2520the%2520original%2520problem.%2520We%2520illustrate%250Atheir%2520efficiency%2520in%2520numerical%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Optimization%20Algorithms%20for%20Linear%20Adversarial%20Training&entry.906535625=Ant%C3%B4nio%20H.%20RIbeiro%20and%20Thomas%20B.%20Sch%C3%B6n%20and%20Dave%20Zahariah%20and%20Francis%20Bach&entry.1292438233=%20%20Adversarial%20training%20can%20be%20used%20to%20learn%20models%20that%20are%20robust%20against%0Aperturbations.%20For%20linear%20models%2C%20it%20can%20be%20formulated%20as%20a%20convex%20optimization%0Aproblem.%20Compared%20to%20methods%20proposed%20in%20the%20context%20of%20deep%20learning%2C%0Aleveraging%20the%20optimization%20structure%20allows%20significantly%20faster%20convergence%0Arates.%20Still%2C%20the%20use%20of%20generic%20convex%20solvers%20can%20be%20inefficient%20for%0Alarge-scale%20problems.%20Here%2C%20we%20propose%20tailored%20optimization%20algorithms%20for%20the%0Aadversarial%20training%20of%20linear%20models%2C%20which%20render%20large-scale%20regression%20and%0Aclassification%20problems%20more%20tractable.%20For%20regression%20problems%2C%20we%20propose%20a%0Afamily%20of%20solvers%20based%20on%20iterative%20ridge%20regression%20and%2C%20for%20classification%2C%0Aa%20family%20of%20solvers%20based%20on%20projected%20gradient%20descent.%20The%20methods%20are%20based%0Aon%20extended%20variable%20reformulations%20of%20the%20original%20problem.%20We%20illustrate%0Atheir%20efficiency%20in%20numerical%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12677v1&entry.124074799=Read"},
{"title": "Token-based Decision Criteria Are Suboptimal in In-context Learning", "author": "Hakaze Cho and Yoshihiro Sakai and Mariko Kato and Kenshiro Tanaka and Akira Ishii and Naoya Inoue", "abstract": "  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n", "link": "http://arxiv.org/abs/2406.16535v2", "date": "2024-10-16", "relevancy": 2.3893, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4846}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4762}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&body=Title%3A%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aoutput%20probabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%0Asuch%20token-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%0Aboundaries%2C%20despite%20delicate%20calibrations%20through%20translation%20and%20constrained%0Arotation%20applied.%20To%20address%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%0Arenounces%20token%20probabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%0ALM%27s%20last%20hidden%20states.%20In%20detail%2C%20we%20assign%20the%20label%20of%20the%20nearest%20centroid%0Apreviously%20estimated%20from%20a%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%0Alabel.%20Our%20experiments%20on%206%20models%20and%2010%20classification%20datasets%20indicate%20that%0AHidden%20Calibration%20consistently%20outperforms%20current%20token-based%20baselines%20by%0Aabout%2020%25~50%25%2C%20achieving%20a%20strong%20state-of-the-art%20in%20ICL.%20Our%20further%20analysis%0Ademonstrates%20that%20Hidden%20Calibration%20finds%20better%20classification%20criteria%20with%0Aless%20inter-class%20overlap%2C%20and%20LMs%20provide%20linearly%20separable%20intra-class%0Aclusters%20with%20the%20help%20of%20demonstrations%2C%20which%20supports%20Hidden%20Calibration%20and%0Agives%20new%20insights%20into%20the%20principle%20of%20ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-based%2520Decision%2520Criteria%2520Are%2520Suboptimal%2520in%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Yoshihiro%2520Sakai%2520and%2520Mariko%2520Kato%2520and%2520Kenshiro%2520Tanaka%2520and%2520Akira%2520Ishii%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520typically%2520utilizes%2520classification%2520criteria%2520from%250Aoutput%2520probabilities%2520of%2520manually%2520selected%2520label%2520tokens.%2520However%252C%2520we%2520argue%2520that%250Asuch%2520token-based%2520classification%2520criteria%2520lead%2520to%2520suboptimal%2520decision%250Aboundaries%252C%2520despite%2520delicate%2520calibrations%2520through%2520translation%2520and%2520constrained%250Arotation%2520applied.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Hidden%2520Calibration%252C%2520which%250Arenounces%2520token%2520probabilities%2520and%2520uses%2520the%2520nearest%2520centroid%2520classifier%2520on%2520the%250ALM%2527s%2520last%2520hidden%2520states.%2520In%2520detail%252C%2520we%2520assign%2520the%2520label%2520of%2520the%2520nearest%2520centroid%250Apreviously%2520estimated%2520from%2520a%2520calibration%2520set%2520to%2520the%2520test%2520sample%2520as%2520the%2520predicted%250Alabel.%2520Our%2520experiments%2520on%25206%2520models%2520and%252010%2520classification%2520datasets%2520indicate%2520that%250AHidden%2520Calibration%2520consistently%2520outperforms%2520current%2520token-based%2520baselines%2520by%250Aabout%252020%2525~50%2525%252C%2520achieving%2520a%2520strong%2520state-of-the-art%2520in%2520ICL.%2520Our%2520further%2520analysis%250Ademonstrates%2520that%2520Hidden%2520Calibration%2520finds%2520better%2520classification%2520criteria%2520with%250Aless%2520inter-class%2520overlap%252C%2520and%2520LMs%2520provide%2520linearly%2520separable%2520intra-class%250Aclusters%2520with%2520the%2520help%2520of%2520demonstrations%252C%2520which%2520supports%2520Hidden%2520Calibration%2520and%250Agives%2520new%2520insights%2520into%2520the%2520principle%2520of%2520ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aoutput%20probabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%0Asuch%20token-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%0Aboundaries%2C%20despite%20delicate%20calibrations%20through%20translation%20and%20constrained%0Arotation%20applied.%20To%20address%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%0Arenounces%20token%20probabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%0ALM%27s%20last%20hidden%20states.%20In%20detail%2C%20we%20assign%20the%20label%20of%20the%20nearest%20centroid%0Apreviously%20estimated%20from%20a%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%0Alabel.%20Our%20experiments%20on%206%20models%20and%2010%20classification%20datasets%20indicate%20that%0AHidden%20Calibration%20consistently%20outperforms%20current%20token-based%20baselines%20by%0Aabout%2020%25~50%25%2C%20achieving%20a%20strong%20state-of-the-art%20in%20ICL.%20Our%20further%20analysis%0Ademonstrates%20that%20Hidden%20Calibration%20finds%20better%20classification%20criteria%20with%0Aless%20inter-class%20overlap%2C%20and%20LMs%20provide%20linearly%20separable%20intra-class%0Aclusters%20with%20the%20help%20of%20demonstrations%2C%20which%20supports%20Hidden%20Calibration%20and%0Agives%20new%20insights%20into%20the%20principle%20of%20ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16535v2&entry.124074799=Read"},
{"title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning", "author": "Xiaowen Sun and Xufeng Zhao and Jae Hee Lee and Wenhao Lu and Matthias Kerzel and Stefan Wermter", "abstract": "  The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA\n", "link": "http://arxiv.org/abs/2406.09988v2", "date": "2024-10-16", "relevancy": 2.3836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.598}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Details%20Make%20a%20Difference%3A%20Object%20State-Sensitive%20Neurorobotic%20Task%0A%20%20Planning&body=Title%3A%20Details%20Make%20a%20Difference%3A%20Object%20State-Sensitive%20Neurorobotic%20Task%0A%20%20Planning%0AAuthor%3A%20Xiaowen%20Sun%20and%20Xufeng%20Zhao%20and%20Jae%20Hee%20Lee%20and%20Wenhao%20Lu%20and%20Matthias%20Kerzel%20and%20Stefan%20Wermter%0AAbstract%3A%20%20%20The%20state%20of%20an%20object%20reflects%20its%20current%20status%20or%20condition%20and%20is%0Aimportant%20for%20a%20robot%27s%20task%20planning%20and%20manipulation.%20However%2C%20detecting%20an%0Aobject%27s%20state%20and%20generating%20a%20state-sensitive%20plan%20for%20robots%20is%20challenging.%0ARecently%2C%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%20Models%0A%28VLMs%29%20have%20shown%20impressive%20capabilities%20in%20generating%20plans.%20However%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20there%20is%20hardly%20any%20investigation%20on%20whether%20LLMs%20or%0AVLMs%20can%20also%20generate%20object%20state-sensitive%20plans.%20To%20study%20this%2C%20we%0Aintroduce%20an%20Object%20State-Sensitive%20Agent%20%28OSSA%29%2C%20a%20task-planning%20agent%0Aempowered%20by%20pre-trained%20neural%20networks.%20We%20propose%20two%20methods%20for%20OSSA%3A%20%28i%29%0Aa%20modular%20model%20consisting%20of%20a%20pre-trained%20vision%20processing%20module%20%28dense%0Acaptioning%20model%2C%20DCM%29%20and%20a%20natural%20language%20processing%20model%20%28LLM%29%2C%20and%20%28ii%29%0Aa%20monolithic%20model%20consisting%20only%20of%20a%20VLM.%20To%20quantitatively%20evaluate%20the%0Aperformances%20of%20the%20two%20methods%2C%20we%20use%20tabletop%20scenarios%20where%20the%20task%20is%20to%0Aclear%20the%20table.%20We%20contribute%20a%20multimodal%20benchmark%20dataset%20that%20takes%20object%0Astates%20into%20consideration.%20Our%20results%20show%20that%20both%20methods%20can%20be%20used%20for%0Aobject%20state-sensitive%20tasks%2C%20but%20the%20monolithic%20approach%20outperforms%20the%0Amodular%20approach.%20The%20code%20for%20OSSA%20is%20available%20at%0Ahttps%3A//github.com/Xiao-wen-Sun/OSSA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetails%2520Make%2520a%2520Difference%253A%2520Object%2520State-Sensitive%2520Neurorobotic%2520Task%250A%2520%2520Planning%26entry.906535625%3DXiaowen%2520Sun%2520and%2520Xufeng%2520Zhao%2520and%2520Jae%2520Hee%2520Lee%2520and%2520Wenhao%2520Lu%2520and%2520Matthias%2520Kerzel%2520and%2520Stefan%2520Wermter%26entry.1292438233%3D%2520%2520The%2520state%2520of%2520an%2520object%2520reflects%2520its%2520current%2520status%2520or%2520condition%2520and%2520is%250Aimportant%2520for%2520a%2520robot%2527s%2520task%2520planning%2520and%2520manipulation.%2520However%252C%2520detecting%2520an%250Aobject%2527s%2520state%2520and%2520generating%2520a%2520state-sensitive%2520plan%2520for%2520robots%2520is%2520challenging.%250ARecently%252C%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520in%2520generating%2520plans.%2520However%252C%2520to%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520there%2520is%2520hardly%2520any%2520investigation%2520on%2520whether%2520LLMs%2520or%250AVLMs%2520can%2520also%2520generate%2520object%2520state-sensitive%2520plans.%2520To%2520study%2520this%252C%2520we%250Aintroduce%2520an%2520Object%2520State-Sensitive%2520Agent%2520%2528OSSA%2529%252C%2520a%2520task-planning%2520agent%250Aempowered%2520by%2520pre-trained%2520neural%2520networks.%2520We%2520propose%2520two%2520methods%2520for%2520OSSA%253A%2520%2528i%2529%250Aa%2520modular%2520model%2520consisting%2520of%2520a%2520pre-trained%2520vision%2520processing%2520module%2520%2528dense%250Acaptioning%2520model%252C%2520DCM%2529%2520and%2520a%2520natural%2520language%2520processing%2520model%2520%2528LLM%2529%252C%2520and%2520%2528ii%2529%250Aa%2520monolithic%2520model%2520consisting%2520only%2520of%2520a%2520VLM.%2520To%2520quantitatively%2520evaluate%2520the%250Aperformances%2520of%2520the%2520two%2520methods%252C%2520we%2520use%2520tabletop%2520scenarios%2520where%2520the%2520task%2520is%2520to%250Aclear%2520the%2520table.%2520We%2520contribute%2520a%2520multimodal%2520benchmark%2520dataset%2520that%2520takes%2520object%250Astates%2520into%2520consideration.%2520Our%2520results%2520show%2520that%2520both%2520methods%2520can%2520be%2520used%2520for%250Aobject%2520state-sensitive%2520tasks%252C%2520but%2520the%2520monolithic%2520approach%2520outperforms%2520the%250Amodular%2520approach.%2520The%2520code%2520for%2520OSSA%2520is%2520available%2520at%250Ahttps%253A//github.com/Xiao-wen-Sun/OSSA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Details%20Make%20a%20Difference%3A%20Object%20State-Sensitive%20Neurorobotic%20Task%0A%20%20Planning&entry.906535625=Xiaowen%20Sun%20and%20Xufeng%20Zhao%20and%20Jae%20Hee%20Lee%20and%20Wenhao%20Lu%20and%20Matthias%20Kerzel%20and%20Stefan%20Wermter&entry.1292438233=%20%20The%20state%20of%20an%20object%20reflects%20its%20current%20status%20or%20condition%20and%20is%0Aimportant%20for%20a%20robot%27s%20task%20planning%20and%20manipulation.%20However%2C%20detecting%20an%0Aobject%27s%20state%20and%20generating%20a%20state-sensitive%20plan%20for%20robots%20is%20challenging.%0ARecently%2C%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision-Language%20Models%0A%28VLMs%29%20have%20shown%20impressive%20capabilities%20in%20generating%20plans.%20However%2C%20to%20the%0Abest%20of%20our%20knowledge%2C%20there%20is%20hardly%20any%20investigation%20on%20whether%20LLMs%20or%0AVLMs%20can%20also%20generate%20object%20state-sensitive%20plans.%20To%20study%20this%2C%20we%0Aintroduce%20an%20Object%20State-Sensitive%20Agent%20%28OSSA%29%2C%20a%20task-planning%20agent%0Aempowered%20by%20pre-trained%20neural%20networks.%20We%20propose%20two%20methods%20for%20OSSA%3A%20%28i%29%0Aa%20modular%20model%20consisting%20of%20a%20pre-trained%20vision%20processing%20module%20%28dense%0Acaptioning%20model%2C%20DCM%29%20and%20a%20natural%20language%20processing%20model%20%28LLM%29%2C%20and%20%28ii%29%0Aa%20monolithic%20model%20consisting%20only%20of%20a%20VLM.%20To%20quantitatively%20evaluate%20the%0Aperformances%20of%20the%20two%20methods%2C%20we%20use%20tabletop%20scenarios%20where%20the%20task%20is%20to%0Aclear%20the%20table.%20We%20contribute%20a%20multimodal%20benchmark%20dataset%20that%20takes%20object%0Astates%20into%20consideration.%20Our%20results%20show%20that%20both%20methods%20can%20be%20used%20for%0Aobject%20state-sensitive%20tasks%2C%20but%20the%20monolithic%20approach%20outperforms%20the%0Amodular%20approach.%20The%20code%20for%20OSSA%20is%20available%20at%0Ahttps%3A//github.com/Xiao-wen-Sun/OSSA%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09988v2&entry.124074799=Read"},
{"title": "Topological reconstruction of sampled surfaces via Morse theory", "author": "Franco Coltraro and Jaume Amor\u00f3s and Maria Alberich-Carrami\u00f1ana and Carme Torras", "abstract": "  In this work, we study the perception problem for sampled surfaces (possibly\nwith boundary) using tools from computational topology, specifically, how to\nidentify their underlying topology starting from point-cloud samples in space,\nsuch as those obtained with 3D scanners. We present a reconstruction algorithm\nbased on a careful topological study of the point sample that allows us to\nobtain a cellular decomposition of it using a Morse function. No triangulation\nor local implicit equations are used as intermediate steps, avoiding in this\nway reconstruction-induced artifices. The algorithm can be run without any\nprior knowledge of the surface topology, density or regularity of the\npoint-sample. The results consist of a piece-wise decomposition of the given\nsurface as a union of Morse cells (i.e. topological disks), suitable for tasks\nsuch as mesh-independent reparametrization or noise-filtering, and a small-rank\ncellular complex determining the topology of the surface. The algorithm, which\nwe test with several real and synthetic surfaces, can be applied to smooth\nsurfaces with or without boundary, embedded in an ambient space of any\ndimension.\n", "link": "http://arxiv.org/abs/2405.17257v2", "date": "2024-10-16", "relevancy": 2.3716, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4753}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4753}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20reconstruction%20of%20sampled%20surfaces%20via%20Morse%20theory&body=Title%3A%20Topological%20reconstruction%20of%20sampled%20surfaces%20via%20Morse%20theory%0AAuthor%3A%20Franco%20Coltraro%20and%20Jaume%20Amor%C3%B3s%20and%20Maria%20Alberich-Carrami%C3%B1ana%20and%20Carme%20Torras%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20perception%20problem%20for%20sampled%20surfaces%20%28possibly%0Awith%20boundary%29%20using%20tools%20from%20computational%20topology%2C%20specifically%2C%20how%20to%0Aidentify%20their%20underlying%20topology%20starting%20from%20point-cloud%20samples%20in%20space%2C%0Asuch%20as%20those%20obtained%20with%203D%20scanners.%20We%20present%20a%20reconstruction%20algorithm%0Abased%20on%20a%20careful%20topological%20study%20of%20the%20point%20sample%20that%20allows%20us%20to%0Aobtain%20a%20cellular%20decomposition%20of%20it%20using%20a%20Morse%20function.%20No%20triangulation%0Aor%20local%20implicit%20equations%20are%20used%20as%20intermediate%20steps%2C%20avoiding%20in%20this%0Away%20reconstruction-induced%20artifices.%20The%20algorithm%20can%20be%20run%20without%20any%0Aprior%20knowledge%20of%20the%20surface%20topology%2C%20density%20or%20regularity%20of%20the%0Apoint-sample.%20The%20results%20consist%20of%20a%20piece-wise%20decomposition%20of%20the%20given%0Asurface%20as%20a%20union%20of%20Morse%20cells%20%28i.e.%20topological%20disks%29%2C%20suitable%20for%20tasks%0Asuch%20as%20mesh-independent%20reparametrization%20or%20noise-filtering%2C%20and%20a%20small-rank%0Acellular%20complex%20determining%20the%20topology%20of%20the%20surface.%20The%20algorithm%2C%20which%0Awe%20test%20with%20several%20real%20and%20synthetic%20surfaces%2C%20can%20be%20applied%20to%20smooth%0Asurfaces%20with%20or%20without%20boundary%2C%20embedded%20in%20an%20ambient%20space%20of%20any%0Adimension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17257v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520reconstruction%2520of%2520sampled%2520surfaces%2520via%2520Morse%2520theory%26entry.906535625%3DFranco%2520Coltraro%2520and%2520Jaume%2520Amor%25C3%25B3s%2520and%2520Maria%2520Alberich-Carrami%25C3%25B1ana%2520and%2520Carme%2520Torras%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520perception%2520problem%2520for%2520sampled%2520surfaces%2520%2528possibly%250Awith%2520boundary%2529%2520using%2520tools%2520from%2520computational%2520topology%252C%2520specifically%252C%2520how%2520to%250Aidentify%2520their%2520underlying%2520topology%2520starting%2520from%2520point-cloud%2520samples%2520in%2520space%252C%250Asuch%2520as%2520those%2520obtained%2520with%25203D%2520scanners.%2520We%2520present%2520a%2520reconstruction%2520algorithm%250Abased%2520on%2520a%2520careful%2520topological%2520study%2520of%2520the%2520point%2520sample%2520that%2520allows%2520us%2520to%250Aobtain%2520a%2520cellular%2520decomposition%2520of%2520it%2520using%2520a%2520Morse%2520function.%2520No%2520triangulation%250Aor%2520local%2520implicit%2520equations%2520are%2520used%2520as%2520intermediate%2520steps%252C%2520avoiding%2520in%2520this%250Away%2520reconstruction-induced%2520artifices.%2520The%2520algorithm%2520can%2520be%2520run%2520without%2520any%250Aprior%2520knowledge%2520of%2520the%2520surface%2520topology%252C%2520density%2520or%2520regularity%2520of%2520the%250Apoint-sample.%2520The%2520results%2520consist%2520of%2520a%2520piece-wise%2520decomposition%2520of%2520the%2520given%250Asurface%2520as%2520a%2520union%2520of%2520Morse%2520cells%2520%2528i.e.%2520topological%2520disks%2529%252C%2520suitable%2520for%2520tasks%250Asuch%2520as%2520mesh-independent%2520reparametrization%2520or%2520noise-filtering%252C%2520and%2520a%2520small-rank%250Acellular%2520complex%2520determining%2520the%2520topology%2520of%2520the%2520surface.%2520The%2520algorithm%252C%2520which%250Awe%2520test%2520with%2520several%2520real%2520and%2520synthetic%2520surfaces%252C%2520can%2520be%2520applied%2520to%2520smooth%250Asurfaces%2520with%2520or%2520without%2520boundary%252C%2520embedded%2520in%2520an%2520ambient%2520space%2520of%2520any%250Adimension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17257v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20reconstruction%20of%20sampled%20surfaces%20via%20Morse%20theory&entry.906535625=Franco%20Coltraro%20and%20Jaume%20Amor%C3%B3s%20and%20Maria%20Alberich-Carrami%C3%B1ana%20and%20Carme%20Torras&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20perception%20problem%20for%20sampled%20surfaces%20%28possibly%0Awith%20boundary%29%20using%20tools%20from%20computational%20topology%2C%20specifically%2C%20how%20to%0Aidentify%20their%20underlying%20topology%20starting%20from%20point-cloud%20samples%20in%20space%2C%0Asuch%20as%20those%20obtained%20with%203D%20scanners.%20We%20present%20a%20reconstruction%20algorithm%0Abased%20on%20a%20careful%20topological%20study%20of%20the%20point%20sample%20that%20allows%20us%20to%0Aobtain%20a%20cellular%20decomposition%20of%20it%20using%20a%20Morse%20function.%20No%20triangulation%0Aor%20local%20implicit%20equations%20are%20used%20as%20intermediate%20steps%2C%20avoiding%20in%20this%0Away%20reconstruction-induced%20artifices.%20The%20algorithm%20can%20be%20run%20without%20any%0Aprior%20knowledge%20of%20the%20surface%20topology%2C%20density%20or%20regularity%20of%20the%0Apoint-sample.%20The%20results%20consist%20of%20a%20piece-wise%20decomposition%20of%20the%20given%0Asurface%20as%20a%20union%20of%20Morse%20cells%20%28i.e.%20topological%20disks%29%2C%20suitable%20for%20tasks%0Asuch%20as%20mesh-independent%20reparametrization%20or%20noise-filtering%2C%20and%20a%20small-rank%0Acellular%20complex%20determining%20the%20topology%20of%20the%20surface.%20The%20algorithm%2C%20which%0Awe%20test%20with%20several%20real%20and%20synthetic%20surfaces%2C%20can%20be%20applied%20to%20smooth%0Asurfaces%20with%20or%20without%20boundary%2C%20embedded%20in%20an%20ambient%20space%20of%20any%0Adimension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17257v2&entry.124074799=Read"},
{"title": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse\n  Synthetic Data and Global-to-Local Adaptive Perception", "author": "Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He", "abstract": "  Document Layout Analysis is crucial for real-world document understanding\nsystems, but it encounters a challenging trade-off between speed and accuracy:\nmultimodal methods leveraging both text and visual features achieve higher\naccuracy but suffer from significant latency, whereas unimodal methods relying\nsolely on visual features offer faster processing speeds at the expense of\naccuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel\napproach that enhances accuracy while maintaining speed advantages through\ndocument-specific optimizations in both pre-training and model design. For\nrobust document pre-training, we introduce the Mesh-candidate BestFit\nalgorithm, which frames document synthesis as a two-dimensional bin packing\nproblem, generating the large-scale, diverse DocSynth-300K dataset.\nPre-training on the resulting DocSynth-300K dataset significantly improves\nfine-tuning performance across various document types. In terms of model\noptimization, we propose a Global-to-Local Controllable Receptive Module that\nis capable of better handling multi-scale variations of document elements.\nFurthermore, to validate performance across different document types, we\nintroduce a complex and challenging benchmark named DocStructBench. Extensive\nexperiments on downstream datasets demonstrate that DocLayout-YOLO excels in\nboth speed and accuracy. Code, data, and models are available at\nhttps://github.com/opendatalab/DocLayout-YOLO.\n", "link": "http://arxiv.org/abs/2410.12628v1", "date": "2024-10-16", "relevancy": 2.3651, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6405}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocLayout-YOLO%3A%20Enhancing%20Document%20Layout%20Analysis%20through%20Diverse%0A%20%20Synthetic%20Data%20and%20Global-to-Local%20Adaptive%20Perception&body=Title%3A%20DocLayout-YOLO%3A%20Enhancing%20Document%20Layout%20Analysis%20through%20Diverse%0A%20%20Synthetic%20Data%20and%20Global-to-Local%20Adaptive%20Perception%0AAuthor%3A%20Zhiyuan%20Zhao%20and%20Hengrui%20Kang%20and%20Bin%20Wang%20and%20Conghui%20He%0AAbstract%3A%20%20%20Document%20Layout%20Analysis%20is%20crucial%20for%20real-world%20document%20understanding%0Asystems%2C%20but%20it%20encounters%20a%20challenging%20trade-off%20between%20speed%20and%20accuracy%3A%0Amultimodal%20methods%20leveraging%20both%20text%20and%20visual%20features%20achieve%20higher%0Aaccuracy%20but%20suffer%20from%20significant%20latency%2C%20whereas%20unimodal%20methods%20relying%0Asolely%20on%20visual%20features%20offer%20faster%20processing%20speeds%20at%20the%20expense%20of%0Aaccuracy.%20To%20address%20this%20dilemma%2C%20we%20introduce%20DocLayout-YOLO%2C%20a%20novel%0Aapproach%20that%20enhances%20accuracy%20while%20maintaining%20speed%20advantages%20through%0Adocument-specific%20optimizations%20in%20both%20pre-training%20and%20model%20design.%20For%0Arobust%20document%20pre-training%2C%20we%20introduce%20the%20Mesh-candidate%20BestFit%0Aalgorithm%2C%20which%20frames%20document%20synthesis%20as%20a%20two-dimensional%20bin%20packing%0Aproblem%2C%20generating%20the%20large-scale%2C%20diverse%20DocSynth-300K%20dataset.%0APre-training%20on%20the%20resulting%20DocSynth-300K%20dataset%20significantly%20improves%0Afine-tuning%20performance%20across%20various%20document%20types.%20In%20terms%20of%20model%0Aoptimization%2C%20we%20propose%20a%20Global-to-Local%20Controllable%20Receptive%20Module%20that%0Ais%20capable%20of%20better%20handling%20multi-scale%20variations%20of%20document%20elements.%0AFurthermore%2C%20to%20validate%20performance%20across%20different%20document%20types%2C%20we%0Aintroduce%20a%20complex%20and%20challenging%20benchmark%20named%20DocStructBench.%20Extensive%0Aexperiments%20on%20downstream%20datasets%20demonstrate%20that%20DocLayout-YOLO%20excels%20in%0Aboth%20speed%20and%20accuracy.%20Code%2C%20data%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/DocLayout-YOLO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocLayout-YOLO%253A%2520Enhancing%2520Document%2520Layout%2520Analysis%2520through%2520Diverse%250A%2520%2520Synthetic%2520Data%2520and%2520Global-to-Local%2520Adaptive%2520Perception%26entry.906535625%3DZhiyuan%2520Zhao%2520and%2520Hengrui%2520Kang%2520and%2520Bin%2520Wang%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520Document%2520Layout%2520Analysis%2520is%2520crucial%2520for%2520real-world%2520document%2520understanding%250Asystems%252C%2520but%2520it%2520encounters%2520a%2520challenging%2520trade-off%2520between%2520speed%2520and%2520accuracy%253A%250Amultimodal%2520methods%2520leveraging%2520both%2520text%2520and%2520visual%2520features%2520achieve%2520higher%250Aaccuracy%2520but%2520suffer%2520from%2520significant%2520latency%252C%2520whereas%2520unimodal%2520methods%2520relying%250Asolely%2520on%2520visual%2520features%2520offer%2520faster%2520processing%2520speeds%2520at%2520the%2520expense%2520of%250Aaccuracy.%2520To%2520address%2520this%2520dilemma%252C%2520we%2520introduce%2520DocLayout-YOLO%252C%2520a%2520novel%250Aapproach%2520that%2520enhances%2520accuracy%2520while%2520maintaining%2520speed%2520advantages%2520through%250Adocument-specific%2520optimizations%2520in%2520both%2520pre-training%2520and%2520model%2520design.%2520For%250Arobust%2520document%2520pre-training%252C%2520we%2520introduce%2520the%2520Mesh-candidate%2520BestFit%250Aalgorithm%252C%2520which%2520frames%2520document%2520synthesis%2520as%2520a%2520two-dimensional%2520bin%2520packing%250Aproblem%252C%2520generating%2520the%2520large-scale%252C%2520diverse%2520DocSynth-300K%2520dataset.%250APre-training%2520on%2520the%2520resulting%2520DocSynth-300K%2520dataset%2520significantly%2520improves%250Afine-tuning%2520performance%2520across%2520various%2520document%2520types.%2520In%2520terms%2520of%2520model%250Aoptimization%252C%2520we%2520propose%2520a%2520Global-to-Local%2520Controllable%2520Receptive%2520Module%2520that%250Ais%2520capable%2520of%2520better%2520handling%2520multi-scale%2520variations%2520of%2520document%2520elements.%250AFurthermore%252C%2520to%2520validate%2520performance%2520across%2520different%2520document%2520types%252C%2520we%250Aintroduce%2520a%2520complex%2520and%2520challenging%2520benchmark%2520named%2520DocStructBench.%2520Extensive%250Aexperiments%2520on%2520downstream%2520datasets%2520demonstrate%2520that%2520DocLayout-YOLO%2520excels%2520in%250Aboth%2520speed%2520and%2520accuracy.%2520Code%252C%2520data%252C%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/opendatalab/DocLayout-YOLO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocLayout-YOLO%3A%20Enhancing%20Document%20Layout%20Analysis%20through%20Diverse%0A%20%20Synthetic%20Data%20and%20Global-to-Local%20Adaptive%20Perception&entry.906535625=Zhiyuan%20Zhao%20and%20Hengrui%20Kang%20and%20Bin%20Wang%20and%20Conghui%20He&entry.1292438233=%20%20Document%20Layout%20Analysis%20is%20crucial%20for%20real-world%20document%20understanding%0Asystems%2C%20but%20it%20encounters%20a%20challenging%20trade-off%20between%20speed%20and%20accuracy%3A%0Amultimodal%20methods%20leveraging%20both%20text%20and%20visual%20features%20achieve%20higher%0Aaccuracy%20but%20suffer%20from%20significant%20latency%2C%20whereas%20unimodal%20methods%20relying%0Asolely%20on%20visual%20features%20offer%20faster%20processing%20speeds%20at%20the%20expense%20of%0Aaccuracy.%20To%20address%20this%20dilemma%2C%20we%20introduce%20DocLayout-YOLO%2C%20a%20novel%0Aapproach%20that%20enhances%20accuracy%20while%20maintaining%20speed%20advantages%20through%0Adocument-specific%20optimizations%20in%20both%20pre-training%20and%20model%20design.%20For%0Arobust%20document%20pre-training%2C%20we%20introduce%20the%20Mesh-candidate%20BestFit%0Aalgorithm%2C%20which%20frames%20document%20synthesis%20as%20a%20two-dimensional%20bin%20packing%0Aproblem%2C%20generating%20the%20large-scale%2C%20diverse%20DocSynth-300K%20dataset.%0APre-training%20on%20the%20resulting%20DocSynth-300K%20dataset%20significantly%20improves%0Afine-tuning%20performance%20across%20various%20document%20types.%20In%20terms%20of%20model%0Aoptimization%2C%20we%20propose%20a%20Global-to-Local%20Controllable%20Receptive%20Module%20that%0Ais%20capable%20of%20better%20handling%20multi-scale%20variations%20of%20document%20elements.%0AFurthermore%2C%20to%20validate%20performance%20across%20different%20document%20types%2C%20we%0Aintroduce%20a%20complex%20and%20challenging%20benchmark%20named%20DocStructBench.%20Extensive%0Aexperiments%20on%20downstream%20datasets%20demonstrate%20that%20DocLayout-YOLO%20excels%20in%0Aboth%20speed%20and%20accuracy.%20Code%2C%20data%2C%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/DocLayout-YOLO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12628v1&entry.124074799=Read"},
{"title": "Task Aware Modulation using Representation Learning: An Approach for Few\n  Shot Learning in Environmental Systems", "author": "Arvind Renganathan and Rahul Ghosh and Ankush Khandelwal and Vipin Kumar", "abstract": "  We introduce TAM-RL (Task Aware Modulation using Representation Learning), a\nnovel multimodal meta-learning framework for few-shot learning in heterogeneous\nsystems, designed for science and engineering problems where entities share a\ncommon underlying forward model but exhibit heterogeneity due to\nentity-specific characteristics. TAM-RL leverages an amortized training process\nwith a modulation network and a base network to learn task-specific modulation\nparameters, enabling efficient adaptation to new tasks with limited data. We\nevaluate TAM-RL on two real-world environmental datasets: Gross Primary Product\n(GPP) prediction and streamflow forecasting, demonstrating significant\nimprovements over existing meta-learning methods. On the FLUXNET dataset,\nTAM-RL improves RMSE by 18.9\\% over MMAML with just one month of few-shot data,\nwhile for streamflow prediction, it achieves an 8.21\\% improvement with one\nyear of data. Synthetic data experiments further validate TAM-RL's superior\nperformance in heterogeneous task distributions, outperforming the baselines in\nthe most heterogeneous setting. Notably, TAM-RL offers substantial\ncomputational efficiency, with at least 3x faster training times compared to\ngradient-based meta-learning approaches while being much simpler to train due\nto reduced complexity. Ablation studies highlight the importance of pretraining\nand adaptation mechanisms in TAM-RL's performance.\n", "link": "http://arxiv.org/abs/2310.04727v2", "date": "2024-10-16", "relevancy": 2.3633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5422}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Aware%20Modulation%20using%20Representation%20Learning%3A%20An%20Approach%20for%20Few%0A%20%20Shot%20Learning%20in%20Environmental%20Systems&body=Title%3A%20Task%20Aware%20Modulation%20using%20Representation%20Learning%3A%20An%20Approach%20for%20Few%0A%20%20Shot%20Learning%20in%20Environmental%20Systems%0AAuthor%3A%20Arvind%20Renganathan%20and%20Rahul%20Ghosh%20and%20Ankush%20Khandelwal%20and%20Vipin%20Kumar%0AAbstract%3A%20%20%20We%20introduce%20TAM-RL%20%28Task%20Aware%20Modulation%20using%20Representation%20Learning%29%2C%20a%0Anovel%20multimodal%20meta-learning%20framework%20for%20few-shot%20learning%20in%20heterogeneous%0Asystems%2C%20designed%20for%20science%20and%20engineering%20problems%20where%20entities%20share%20a%0Acommon%20underlying%20forward%20model%20but%20exhibit%20heterogeneity%20due%20to%0Aentity-specific%20characteristics.%20TAM-RL%20leverages%20an%20amortized%20training%20process%0Awith%20a%20modulation%20network%20and%20a%20base%20network%20to%20learn%20task-specific%20modulation%0Aparameters%2C%20enabling%20efficient%20adaptation%20to%20new%20tasks%20with%20limited%20data.%20We%0Aevaluate%20TAM-RL%20on%20two%20real-world%20environmental%20datasets%3A%20Gross%20Primary%20Product%0A%28GPP%29%20prediction%20and%20streamflow%20forecasting%2C%20demonstrating%20significant%0Aimprovements%20over%20existing%20meta-learning%20methods.%20On%20the%20FLUXNET%20dataset%2C%0ATAM-RL%20improves%20RMSE%20by%2018.9%5C%25%20over%20MMAML%20with%20just%20one%20month%20of%20few-shot%20data%2C%0Awhile%20for%20streamflow%20prediction%2C%20it%20achieves%20an%208.21%5C%25%20improvement%20with%20one%0Ayear%20of%20data.%20Synthetic%20data%20experiments%20further%20validate%20TAM-RL%27s%20superior%0Aperformance%20in%20heterogeneous%20task%20distributions%2C%20outperforming%20the%20baselines%20in%0Athe%20most%20heterogeneous%20setting.%20Notably%2C%20TAM-RL%20offers%20substantial%0Acomputational%20efficiency%2C%20with%20at%20least%203x%20faster%20training%20times%20compared%20to%0Agradient-based%20meta-learning%20approaches%20while%20being%20much%20simpler%20to%20train%20due%0Ato%20reduced%20complexity.%20Ablation%20studies%20highlight%20the%20importance%20of%20pretraining%0Aand%20adaptation%20mechanisms%20in%20TAM-RL%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Aware%2520Modulation%2520using%2520Representation%2520Learning%253A%2520An%2520Approach%2520for%2520Few%250A%2520%2520Shot%2520Learning%2520in%2520Environmental%2520Systems%26entry.906535625%3DArvind%2520Renganathan%2520and%2520Rahul%2520Ghosh%2520and%2520Ankush%2520Khandelwal%2520and%2520Vipin%2520Kumar%26entry.1292438233%3D%2520%2520We%2520introduce%2520TAM-RL%2520%2528Task%2520Aware%2520Modulation%2520using%2520Representation%2520Learning%2529%252C%2520a%250Anovel%2520multimodal%2520meta-learning%2520framework%2520for%2520few-shot%2520learning%2520in%2520heterogeneous%250Asystems%252C%2520designed%2520for%2520science%2520and%2520engineering%2520problems%2520where%2520entities%2520share%2520a%250Acommon%2520underlying%2520forward%2520model%2520but%2520exhibit%2520heterogeneity%2520due%2520to%250Aentity-specific%2520characteristics.%2520TAM-RL%2520leverages%2520an%2520amortized%2520training%2520process%250Awith%2520a%2520modulation%2520network%2520and%2520a%2520base%2520network%2520to%2520learn%2520task-specific%2520modulation%250Aparameters%252C%2520enabling%2520efficient%2520adaptation%2520to%2520new%2520tasks%2520with%2520limited%2520data.%2520We%250Aevaluate%2520TAM-RL%2520on%2520two%2520real-world%2520environmental%2520datasets%253A%2520Gross%2520Primary%2520Product%250A%2528GPP%2529%2520prediction%2520and%2520streamflow%2520forecasting%252C%2520demonstrating%2520significant%250Aimprovements%2520over%2520existing%2520meta-learning%2520methods.%2520On%2520the%2520FLUXNET%2520dataset%252C%250ATAM-RL%2520improves%2520RMSE%2520by%252018.9%255C%2525%2520over%2520MMAML%2520with%2520just%2520one%2520month%2520of%2520few-shot%2520data%252C%250Awhile%2520for%2520streamflow%2520prediction%252C%2520it%2520achieves%2520an%25208.21%255C%2525%2520improvement%2520with%2520one%250Ayear%2520of%2520data.%2520Synthetic%2520data%2520experiments%2520further%2520validate%2520TAM-RL%2527s%2520superior%250Aperformance%2520in%2520heterogeneous%2520task%2520distributions%252C%2520outperforming%2520the%2520baselines%2520in%250Athe%2520most%2520heterogeneous%2520setting.%2520Notably%252C%2520TAM-RL%2520offers%2520substantial%250Acomputational%2520efficiency%252C%2520with%2520at%2520least%25203x%2520faster%2520training%2520times%2520compared%2520to%250Agradient-based%2520meta-learning%2520approaches%2520while%2520being%2520much%2520simpler%2520to%2520train%2520due%250Ato%2520reduced%2520complexity.%2520Ablation%2520studies%2520highlight%2520the%2520importance%2520of%2520pretraining%250Aand%2520adaptation%2520mechanisms%2520in%2520TAM-RL%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Aware%20Modulation%20using%20Representation%20Learning%3A%20An%20Approach%20for%20Few%0A%20%20Shot%20Learning%20in%20Environmental%20Systems&entry.906535625=Arvind%20Renganathan%20and%20Rahul%20Ghosh%20and%20Ankush%20Khandelwal%20and%20Vipin%20Kumar&entry.1292438233=%20%20We%20introduce%20TAM-RL%20%28Task%20Aware%20Modulation%20using%20Representation%20Learning%29%2C%20a%0Anovel%20multimodal%20meta-learning%20framework%20for%20few-shot%20learning%20in%20heterogeneous%0Asystems%2C%20designed%20for%20science%20and%20engineering%20problems%20where%20entities%20share%20a%0Acommon%20underlying%20forward%20model%20but%20exhibit%20heterogeneity%20due%20to%0Aentity-specific%20characteristics.%20TAM-RL%20leverages%20an%20amortized%20training%20process%0Awith%20a%20modulation%20network%20and%20a%20base%20network%20to%20learn%20task-specific%20modulation%0Aparameters%2C%20enabling%20efficient%20adaptation%20to%20new%20tasks%20with%20limited%20data.%20We%0Aevaluate%20TAM-RL%20on%20two%20real-world%20environmental%20datasets%3A%20Gross%20Primary%20Product%0A%28GPP%29%20prediction%20and%20streamflow%20forecasting%2C%20demonstrating%20significant%0Aimprovements%20over%20existing%20meta-learning%20methods.%20On%20the%20FLUXNET%20dataset%2C%0ATAM-RL%20improves%20RMSE%20by%2018.9%5C%25%20over%20MMAML%20with%20just%20one%20month%20of%20few-shot%20data%2C%0Awhile%20for%20streamflow%20prediction%2C%20it%20achieves%20an%208.21%5C%25%20improvement%20with%20one%0Ayear%20of%20data.%20Synthetic%20data%20experiments%20further%20validate%20TAM-RL%27s%20superior%0Aperformance%20in%20heterogeneous%20task%20distributions%2C%20outperforming%20the%20baselines%20in%0Athe%20most%20heterogeneous%20setting.%20Notably%2C%20TAM-RL%20offers%20substantial%0Acomputational%20efficiency%2C%20with%20at%20least%203x%20faster%20training%20times%20compared%20to%0Agradient-based%20meta-learning%20approaches%20while%20being%20much%20simpler%20to%20train%20due%0Ato%20reduced%20complexity.%20Ablation%20studies%20highlight%20the%20importance%20of%20pretraining%0Aand%20adaptation%20mechanisms%20in%20TAM-RL%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04727v2&entry.124074799=Read"},
{"title": "ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment", "author": "Tarek Naous and Michael J. Ryan and Anton Lavrouk and Mohit Chandra and Wei Xu", "abstract": "  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n", "link": "http://arxiv.org/abs/2305.14463v4", "date": "2024-10-16", "relevancy": 2.344, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReadMe%2B%2B%3A%20Benchmarking%20Multilingual%20Language%20Models%20for%20Multi-Domain%0A%20%20Readability%20Assessment&body=Title%3A%20ReadMe%2B%2B%3A%20Benchmarking%20Multilingual%20Language%20Models%20for%20Multi-Domain%0A%20%20Readability%20Assessment%0AAuthor%3A%20Tarek%20Naous%20and%20Michael%20J.%20Ryan%20and%20Anton%20Lavrouk%20and%20Mohit%20Chandra%20and%20Wei%20Xu%0AAbstract%3A%20%20%20We%20present%20a%20comprehensive%20evaluation%20of%20large%20language%20models%20for%0Amultilingual%20readability%20assessment.%20Existing%20evaluation%20resources%20lack%20domain%0Aand%20language%20diversity%2C%20limiting%20the%20ability%20for%20cross-domain%20and%20cross-lingual%0Aanalyses.%20This%20paper%20introduces%20ReadMe%2B%2B%2C%20a%20multilingual%20multi-domain%20dataset%0Awith%20human%20annotations%20of%209757%20sentences%20in%20Arabic%2C%20English%2C%20French%2C%20Hindi%2C%20and%0ARussian%2C%20collected%20from%20112%20different%20data%20sources.%20This%20benchmark%20will%0Aencourage%20research%20on%20developing%20robust%20multilingual%20readability%20assessment%0Amethods.%20Using%20ReadMe%2B%2B%2C%20we%20benchmark%20multilingual%20and%20monolingual%20language%0Amodels%20in%20the%20supervised%2C%20unsupervised%2C%20and%20few-shot%20prompting%20settings.%20The%0Adomain%20and%20language%20diversity%20in%20ReadMe%2B%2B%20enable%20us%20to%20test%20more%20effective%0Afew-shot%20prompting%2C%20and%20identify%20shortcomings%20in%20state-of-the-art%20unsupervised%0Amethods.%20Our%20experiments%20also%20reveal%20exciting%20results%20of%20superior%20domain%0Ageneralization%20and%20enhanced%20cross-lingual%20transfer%20capabilities%20by%20models%0Atrained%20on%20ReadMe%2B%2B.%20We%20will%20make%20our%20data%20publicly%20available%20and%20release%20a%0Apython%20package%20tool%20for%20multilingual%20sentence%20readability%20prediction%20using%20our%0Atrained%20models%20at%3A%20https%3A//github.com/tareknaous/readme%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14463v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReadMe%252B%252B%253A%2520Benchmarking%2520Multilingual%2520Language%2520Models%2520for%2520Multi-Domain%250A%2520%2520Readability%2520Assessment%26entry.906535625%3DTarek%2520Naous%2520and%2520Michael%2520J.%2520Ryan%2520and%2520Anton%2520Lavrouk%2520and%2520Mohit%2520Chandra%2520and%2520Wei%2520Xu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520comprehensive%2520evaluation%2520of%2520large%2520language%2520models%2520for%250Amultilingual%2520readability%2520assessment.%2520Existing%2520evaluation%2520resources%2520lack%2520domain%250Aand%2520language%2520diversity%252C%2520limiting%2520the%2520ability%2520for%2520cross-domain%2520and%2520cross-lingual%250Aanalyses.%2520This%2520paper%2520introduces%2520ReadMe%252B%252B%252C%2520a%2520multilingual%2520multi-domain%2520dataset%250Awith%2520human%2520annotations%2520of%25209757%2520sentences%2520in%2520Arabic%252C%2520English%252C%2520French%252C%2520Hindi%252C%2520and%250ARussian%252C%2520collected%2520from%2520112%2520different%2520data%2520sources.%2520This%2520benchmark%2520will%250Aencourage%2520research%2520on%2520developing%2520robust%2520multilingual%2520readability%2520assessment%250Amethods.%2520Using%2520ReadMe%252B%252B%252C%2520we%2520benchmark%2520multilingual%2520and%2520monolingual%2520language%250Amodels%2520in%2520the%2520supervised%252C%2520unsupervised%252C%2520and%2520few-shot%2520prompting%2520settings.%2520The%250Adomain%2520and%2520language%2520diversity%2520in%2520ReadMe%252B%252B%2520enable%2520us%2520to%2520test%2520more%2520effective%250Afew-shot%2520prompting%252C%2520and%2520identify%2520shortcomings%2520in%2520state-of-the-art%2520unsupervised%250Amethods.%2520Our%2520experiments%2520also%2520reveal%2520exciting%2520results%2520of%2520superior%2520domain%250Ageneralization%2520and%2520enhanced%2520cross-lingual%2520transfer%2520capabilities%2520by%2520models%250Atrained%2520on%2520ReadMe%252B%252B.%2520We%2520will%2520make%2520our%2520data%2520publicly%2520available%2520and%2520release%2520a%250Apython%2520package%2520tool%2520for%2520multilingual%2520sentence%2520readability%2520prediction%2520using%2520our%250Atrained%2520models%2520at%253A%2520https%253A//github.com/tareknaous/readme%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.14463v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReadMe%2B%2B%3A%20Benchmarking%20Multilingual%20Language%20Models%20for%20Multi-Domain%0A%20%20Readability%20Assessment&entry.906535625=Tarek%20Naous%20and%20Michael%20J.%20Ryan%20and%20Anton%20Lavrouk%20and%20Mohit%20Chandra%20and%20Wei%20Xu&entry.1292438233=%20%20We%20present%20a%20comprehensive%20evaluation%20of%20large%20language%20models%20for%0Amultilingual%20readability%20assessment.%20Existing%20evaluation%20resources%20lack%20domain%0Aand%20language%20diversity%2C%20limiting%20the%20ability%20for%20cross-domain%20and%20cross-lingual%0Aanalyses.%20This%20paper%20introduces%20ReadMe%2B%2B%2C%20a%20multilingual%20multi-domain%20dataset%0Awith%20human%20annotations%20of%209757%20sentences%20in%20Arabic%2C%20English%2C%20French%2C%20Hindi%2C%20and%0ARussian%2C%20collected%20from%20112%20different%20data%20sources.%20This%20benchmark%20will%0Aencourage%20research%20on%20developing%20robust%20multilingual%20readability%20assessment%0Amethods.%20Using%20ReadMe%2B%2B%2C%20we%20benchmark%20multilingual%20and%20monolingual%20language%0Amodels%20in%20the%20supervised%2C%20unsupervised%2C%20and%20few-shot%20prompting%20settings.%20The%0Adomain%20and%20language%20diversity%20in%20ReadMe%2B%2B%20enable%20us%20to%20test%20more%20effective%0Afew-shot%20prompting%2C%20and%20identify%20shortcomings%20in%20state-of-the-art%20unsupervised%0Amethods.%20Our%20experiments%20also%20reveal%20exciting%20results%20of%20superior%20domain%0Ageneralization%20and%20enhanced%20cross-lingual%20transfer%20capabilities%20by%20models%0Atrained%20on%20ReadMe%2B%2B.%20We%20will%20make%20our%20data%20publicly%20available%20and%20release%20a%0Apython%20package%20tool%20for%20multilingual%20sentence%20readability%20prediction%20using%20our%0Atrained%20models%20at%3A%20https%3A//github.com/tareknaous/readme%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14463v4&entry.124074799=Read"},
{"title": "CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot\n  Applications", "author": "Jan Blumenkamp and Steven Morad and Jennifer Gielis and Amanda Prorok", "abstract": "  Autonomous robot operation in unstructured environments is often underpinned\nby spatial understanding through vision. Systems composed of multiple\nconcurrently operating robots additionally require access to frequent, accurate\nand reliable pose estimates. In this work, we propose CoViS-Net, a\ndecentralized visual spatial foundation model that learns spatial priors from\ndata, enabling pose estimation as well as spatial comprehension. Our model is\nfully decentralized, platform-agnostic, executable in real-time using onboard\ncompute, and does not require existing networking infrastructure. CoViS-Net\nprovides relative pose estimates and a local bird's-eye-view (BEV)\nrepresentation, even without camera overlap between robots (in contrast to\nclassical methods). We demonstrate its use in a multi-robot formation control\ntask across various real-world settings. We provide code, models and\nsupplementary material online. https://proroklab.github.io/CoViS-Net/\n", "link": "http://arxiv.org/abs/2405.01107v3", "date": "2024-10-16", "relevancy": 2.3417, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoViS-Net%3A%20A%20Cooperative%20Visual%20Spatial%20Foundation%20Model%20for%20Multi-Robot%0A%20%20Applications&body=Title%3A%20CoViS-Net%3A%20A%20Cooperative%20Visual%20Spatial%20Foundation%20Model%20for%20Multi-Robot%0A%20%20Applications%0AAuthor%3A%20Jan%20Blumenkamp%20and%20Steven%20Morad%20and%20Jennifer%20Gielis%20and%20Amanda%20Prorok%0AAbstract%3A%20%20%20Autonomous%20robot%20operation%20in%20unstructured%20environments%20is%20often%20underpinned%0Aby%20spatial%20understanding%20through%20vision.%20Systems%20composed%20of%20multiple%0Aconcurrently%20operating%20robots%20additionally%20require%20access%20to%20frequent%2C%20accurate%0Aand%20reliable%20pose%20estimates.%20In%20this%20work%2C%20we%20propose%20CoViS-Net%2C%20a%0Adecentralized%20visual%20spatial%20foundation%20model%20that%20learns%20spatial%20priors%20from%0Adata%2C%20enabling%20pose%20estimation%20as%20well%20as%20spatial%20comprehension.%20Our%20model%20is%0Afully%20decentralized%2C%20platform-agnostic%2C%20executable%20in%20real-time%20using%20onboard%0Acompute%2C%20and%20does%20not%20require%20existing%20networking%20infrastructure.%20CoViS-Net%0Aprovides%20relative%20pose%20estimates%20and%20a%20local%20bird%27s-eye-view%20%28BEV%29%0Arepresentation%2C%20even%20without%20camera%20overlap%20between%20robots%20%28in%20contrast%20to%0Aclassical%20methods%29.%20We%20demonstrate%20its%20use%20in%20a%20multi-robot%20formation%20control%0Atask%20across%20various%20real-world%20settings.%20We%20provide%20code%2C%20models%20and%0Asupplementary%20material%20online.%20https%3A//proroklab.github.io/CoViS-Net/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01107v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoViS-Net%253A%2520A%2520Cooperative%2520Visual%2520Spatial%2520Foundation%2520Model%2520for%2520Multi-Robot%250A%2520%2520Applications%26entry.906535625%3DJan%2520Blumenkamp%2520and%2520Steven%2520Morad%2520and%2520Jennifer%2520Gielis%2520and%2520Amanda%2520Prorok%26entry.1292438233%3D%2520%2520Autonomous%2520robot%2520operation%2520in%2520unstructured%2520environments%2520is%2520often%2520underpinned%250Aby%2520spatial%2520understanding%2520through%2520vision.%2520Systems%2520composed%2520of%2520multiple%250Aconcurrently%2520operating%2520robots%2520additionally%2520require%2520access%2520to%2520frequent%252C%2520accurate%250Aand%2520reliable%2520pose%2520estimates.%2520In%2520this%2520work%252C%2520we%2520propose%2520CoViS-Net%252C%2520a%250Adecentralized%2520visual%2520spatial%2520foundation%2520model%2520that%2520learns%2520spatial%2520priors%2520from%250Adata%252C%2520enabling%2520pose%2520estimation%2520as%2520well%2520as%2520spatial%2520comprehension.%2520Our%2520model%2520is%250Afully%2520decentralized%252C%2520platform-agnostic%252C%2520executable%2520in%2520real-time%2520using%2520onboard%250Acompute%252C%2520and%2520does%2520not%2520require%2520existing%2520networking%2520infrastructure.%2520CoViS-Net%250Aprovides%2520relative%2520pose%2520estimates%2520and%2520a%2520local%2520bird%2527s-eye-view%2520%2528BEV%2529%250Arepresentation%252C%2520even%2520without%2520camera%2520overlap%2520between%2520robots%2520%2528in%2520contrast%2520to%250Aclassical%2520methods%2529.%2520We%2520demonstrate%2520its%2520use%2520in%2520a%2520multi-robot%2520formation%2520control%250Atask%2520across%2520various%2520real-world%2520settings.%2520We%2520provide%2520code%252C%2520models%2520and%250Asupplementary%2520material%2520online.%2520https%253A//proroklab.github.io/CoViS-Net/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01107v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoViS-Net%3A%20A%20Cooperative%20Visual%20Spatial%20Foundation%20Model%20for%20Multi-Robot%0A%20%20Applications&entry.906535625=Jan%20Blumenkamp%20and%20Steven%20Morad%20and%20Jennifer%20Gielis%20and%20Amanda%20Prorok&entry.1292438233=%20%20Autonomous%20robot%20operation%20in%20unstructured%20environments%20is%20often%20underpinned%0Aby%20spatial%20understanding%20through%20vision.%20Systems%20composed%20of%20multiple%0Aconcurrently%20operating%20robots%20additionally%20require%20access%20to%20frequent%2C%20accurate%0Aand%20reliable%20pose%20estimates.%20In%20this%20work%2C%20we%20propose%20CoViS-Net%2C%20a%0Adecentralized%20visual%20spatial%20foundation%20model%20that%20learns%20spatial%20priors%20from%0Adata%2C%20enabling%20pose%20estimation%20as%20well%20as%20spatial%20comprehension.%20Our%20model%20is%0Afully%20decentralized%2C%20platform-agnostic%2C%20executable%20in%20real-time%20using%20onboard%0Acompute%2C%20and%20does%20not%20require%20existing%20networking%20infrastructure.%20CoViS-Net%0Aprovides%20relative%20pose%20estimates%20and%20a%20local%20bird%27s-eye-view%20%28BEV%29%0Arepresentation%2C%20even%20without%20camera%20overlap%20between%20robots%20%28in%20contrast%20to%0Aclassical%20methods%29.%20We%20demonstrate%20its%20use%20in%20a%20multi-robot%20formation%20control%0Atask%20across%20various%20real-world%20settings.%20We%20provide%20code%2C%20models%20and%0Asupplementary%20material%20online.%20https%3A//proroklab.github.io/CoViS-Net/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01107v3&entry.124074799=Read"},
{"title": "Conformity in Large Language Models", "author": "Xiaochen Zhu and Caiqi Zhang and Tom Stafford and Nigel Collier and Andreas Vlachos", "abstract": "  The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.\n", "link": "http://arxiv.org/abs/2410.12428v1", "date": "2024-10-16", "relevancy": 2.306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformity%20in%20Large%20Language%20Models&body=Title%3A%20Conformity%20in%20Large%20Language%20Models%0AAuthor%3A%20Xiaochen%20Zhu%20and%20Caiqi%20Zhang%20and%20Tom%20Stafford%20and%20Nigel%20Collier%20and%20Andreas%20Vlachos%0AAbstract%3A%20%20%20The%20conformity%20effect%20describes%20the%20tendency%20of%20individuals%20to%20align%20their%0Aresponses%20with%20the%20majority.%20Studying%20this%20bias%20in%20large%20language%20models%20%28LLMs%29%0Ais%20crucial%2C%20as%20LLMs%20are%20increasingly%20used%20in%20various%20information-seeking%20and%0Adecision-making%20tasks%20as%20conversation%20partners%20to%20improve%20productivity.%20Thus%2C%0Aconformity%20to%20incorrect%20responses%20can%20compromise%20their%20effectiveness.%20In%20this%0Apaper%2C%20we%20adapt%20psychological%20experiments%20to%20examine%20the%20extent%20of%20conformity%0Ain%20state-of-the-art%20LLMs.%20Our%20findings%20reveal%20that%20all%20models%20tested%20exhibit%0Avarying%20levels%20of%20conformity%20toward%20the%20majority%2C%20regardless%20of%20their%20initial%0Achoice%20or%20correctness%2C%20across%20different%20knowledge%20domains.%20Notably%2C%20we%20are%20the%0Afirst%20to%20show%20that%20LLMs%20are%20more%20likely%20to%20conform%20when%20they%20are%20more%20uncertain%0Ain%20their%20own%20prediction.%20We%20further%20explore%20factors%20that%20influence%20conformity%2C%0Asuch%20as%20training%20paradigms%20and%20input%20characteristics%2C%20finding%20that%0Ainstruction-tuned%20models%20are%20less%20susceptible%20to%20conformity%2C%20while%20increasing%0Athe%20naturalness%20of%20majority%20tones%20amplifies%20conformity.%20Finally%2C%20we%20propose%20two%0Ainterventions--Devil%27s%20Advocate%20and%20Question%20Distillation--to%20mitigate%0Aconformity%2C%20providing%20insights%20into%20building%20more%20robust%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformity%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXiaochen%2520Zhu%2520and%2520Caiqi%2520Zhang%2520and%2520Tom%2520Stafford%2520and%2520Nigel%2520Collier%2520and%2520Andreas%2520Vlachos%26entry.1292438233%3D%2520%2520The%2520conformity%2520effect%2520describes%2520the%2520tendency%2520of%2520individuals%2520to%2520align%2520their%250Aresponses%2520with%2520the%2520majority.%2520Studying%2520this%2520bias%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Ais%2520crucial%252C%2520as%2520LLMs%2520are%2520increasingly%2520used%2520in%2520various%2520information-seeking%2520and%250Adecision-making%2520tasks%2520as%2520conversation%2520partners%2520to%2520improve%2520productivity.%2520Thus%252C%250Aconformity%2520to%2520incorrect%2520responses%2520can%2520compromise%2520their%2520effectiveness.%2520In%2520this%250Apaper%252C%2520we%2520adapt%2520psychological%2520experiments%2520to%2520examine%2520the%2520extent%2520of%2520conformity%250Ain%2520state-of-the-art%2520LLMs.%2520Our%2520findings%2520reveal%2520that%2520all%2520models%2520tested%2520exhibit%250Avarying%2520levels%2520of%2520conformity%2520toward%2520the%2520majority%252C%2520regardless%2520of%2520their%2520initial%250Achoice%2520or%2520correctness%252C%2520across%2520different%2520knowledge%2520domains.%2520Notably%252C%2520we%2520are%2520the%250Afirst%2520to%2520show%2520that%2520LLMs%2520are%2520more%2520likely%2520to%2520conform%2520when%2520they%2520are%2520more%2520uncertain%250Ain%2520their%2520own%2520prediction.%2520We%2520further%2520explore%2520factors%2520that%2520influence%2520conformity%252C%250Asuch%2520as%2520training%2520paradigms%2520and%2520input%2520characteristics%252C%2520finding%2520that%250Ainstruction-tuned%2520models%2520are%2520less%2520susceptible%2520to%2520conformity%252C%2520while%2520increasing%250Athe%2520naturalness%2520of%2520majority%2520tones%2520amplifies%2520conformity.%2520Finally%252C%2520we%2520propose%2520two%250Ainterventions--Devil%2527s%2520Advocate%2520and%2520Question%2520Distillation--to%2520mitigate%250Aconformity%252C%2520providing%2520insights%2520into%2520building%2520more%2520robust%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformity%20in%20Large%20Language%20Models&entry.906535625=Xiaochen%20Zhu%20and%20Caiqi%20Zhang%20and%20Tom%20Stafford%20and%20Nigel%20Collier%20and%20Andreas%20Vlachos&entry.1292438233=%20%20The%20conformity%20effect%20describes%20the%20tendency%20of%20individuals%20to%20align%20their%0Aresponses%20with%20the%20majority.%20Studying%20this%20bias%20in%20large%20language%20models%20%28LLMs%29%0Ais%20crucial%2C%20as%20LLMs%20are%20increasingly%20used%20in%20various%20information-seeking%20and%0Adecision-making%20tasks%20as%20conversation%20partners%20to%20improve%20productivity.%20Thus%2C%0Aconformity%20to%20incorrect%20responses%20can%20compromise%20their%20effectiveness.%20In%20this%0Apaper%2C%20we%20adapt%20psychological%20experiments%20to%20examine%20the%20extent%20of%20conformity%0Ain%20state-of-the-art%20LLMs.%20Our%20findings%20reveal%20that%20all%20models%20tested%20exhibit%0Avarying%20levels%20of%20conformity%20toward%20the%20majority%2C%20regardless%20of%20their%20initial%0Achoice%20or%20correctness%2C%20across%20different%20knowledge%20domains.%20Notably%2C%20we%20are%20the%0Afirst%20to%20show%20that%20LLMs%20are%20more%20likely%20to%20conform%20when%20they%20are%20more%20uncertain%0Ain%20their%20own%20prediction.%20We%20further%20explore%20factors%20that%20influence%20conformity%2C%0Asuch%20as%20training%20paradigms%20and%20input%20characteristics%2C%20finding%20that%0Ainstruction-tuned%20models%20are%20less%20susceptible%20to%20conformity%2C%20while%20increasing%0Athe%20naturalness%20of%20majority%20tones%20amplifies%20conformity.%20Finally%2C%20we%20propose%20two%0Ainterventions--Devil%27s%20Advocate%20and%20Question%20Distillation--to%20mitigate%0Aconformity%2C%20providing%20insights%20into%20building%20more%20robust%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12428v1&entry.124074799=Read"},
{"title": "STRUX: An LLM for Decision-Making with Structured Explanations", "author": "Yiming Lu and Yebowen Hu and Hassan Foroosh and Wei Jin and Fei Liu", "abstract": "  Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.\n", "link": "http://arxiv.org/abs/2410.12583v1", "date": "2024-10-16", "relevancy": 2.3006, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRUX%3A%20An%20LLM%20for%20Decision-Making%20with%20Structured%20Explanations&body=Title%3A%20STRUX%3A%20An%20LLM%20for%20Decision-Making%20with%20Structured%20Explanations%0AAuthor%3A%20Yiming%20Lu%20and%20Yebowen%20Hu%20and%20Hassan%20Foroosh%20and%20Wei%20Jin%20and%20Fei%20Liu%0AAbstract%3A%20%20%20Countless%20decisions%20shape%20our%20daily%20lives%2C%20and%20it%20is%20paramount%20to%20understand%0Athe%20how%20and%20why%20behind%20these%20choices.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20LLM%0Adecision-making%20framework%20called%20STRUX%2C%20which%20enhances%20LLM%20decision-making%20by%0Aproviding%20structured%20explanations.%20These%20include%20favorable%20and%20adverse%20facts%0Arelated%20to%20the%20decision%2C%20along%20with%20their%20respective%20strengths.%20STRUX%20begins%20by%0Adistilling%20lengthy%20information%20into%20a%20concise%20table%20of%20key%20facts.%20It%20then%0Aemploys%20a%20series%20of%20self-reflection%20steps%20to%20determine%20which%20of%20these%20facts%20are%0Apivotal%2C%20categorizing%20them%20as%20either%20favorable%20or%20adverse%20in%20relation%20to%20a%0Aspecific%20decision.%20Lastly%2C%20we%20fine-tune%20an%20LLM%20to%20identify%20and%20prioritize%20these%0Akey%20facts%20to%20optimize%20decision-making.%20STRUX%20has%20been%20evaluated%20on%20the%0Achallenging%20task%20of%20forecasting%20stock%20investment%20decisions%20based%20on%20earnings%0Acall%20transcripts%20and%20demonstrated%20superior%20performance%20against%20strong%0Abaselines.%20It%20enhances%20decision%20transparency%20by%20allowing%20users%20to%20understand%0Athe%20impact%20of%20different%20factors%2C%20representing%20a%20meaningful%20step%20towards%0Apractical%20decision-making%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRUX%253A%2520An%2520LLM%2520for%2520Decision-Making%2520with%2520Structured%2520Explanations%26entry.906535625%3DYiming%2520Lu%2520and%2520Yebowen%2520Hu%2520and%2520Hassan%2520Foroosh%2520and%2520Wei%2520Jin%2520and%2520Fei%2520Liu%26entry.1292438233%3D%2520%2520Countless%2520decisions%2520shape%2520our%2520daily%2520lives%252C%2520and%2520it%2520is%2520paramount%2520to%2520understand%250Athe%2520how%2520and%2520why%2520behind%2520these%2520choices.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520LLM%250Adecision-making%2520framework%2520called%2520STRUX%252C%2520which%2520enhances%2520LLM%2520decision-making%2520by%250Aproviding%2520structured%2520explanations.%2520These%2520include%2520favorable%2520and%2520adverse%2520facts%250Arelated%2520to%2520the%2520decision%252C%2520along%2520with%2520their%2520respective%2520strengths.%2520STRUX%2520begins%2520by%250Adistilling%2520lengthy%2520information%2520into%2520a%2520concise%2520table%2520of%2520key%2520facts.%2520It%2520then%250Aemploys%2520a%2520series%2520of%2520self-reflection%2520steps%2520to%2520determine%2520which%2520of%2520these%2520facts%2520are%250Apivotal%252C%2520categorizing%2520them%2520as%2520either%2520favorable%2520or%2520adverse%2520in%2520relation%2520to%2520a%250Aspecific%2520decision.%2520Lastly%252C%2520we%2520fine-tune%2520an%2520LLM%2520to%2520identify%2520and%2520prioritize%2520these%250Akey%2520facts%2520to%2520optimize%2520decision-making.%2520STRUX%2520has%2520been%2520evaluated%2520on%2520the%250Achallenging%2520task%2520of%2520forecasting%2520stock%2520investment%2520decisions%2520based%2520on%2520earnings%250Acall%2520transcripts%2520and%2520demonstrated%2520superior%2520performance%2520against%2520strong%250Abaselines.%2520It%2520enhances%2520decision%2520transparency%2520by%2520allowing%2520users%2520to%2520understand%250Athe%2520impact%2520of%2520different%2520factors%252C%2520representing%2520a%2520meaningful%2520step%2520towards%250Apractical%2520decision-making%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRUX%3A%20An%20LLM%20for%20Decision-Making%20with%20Structured%20Explanations&entry.906535625=Yiming%20Lu%20and%20Yebowen%20Hu%20and%20Hassan%20Foroosh%20and%20Wei%20Jin%20and%20Fei%20Liu&entry.1292438233=%20%20Countless%20decisions%20shape%20our%20daily%20lives%2C%20and%20it%20is%20paramount%20to%20understand%0Athe%20how%20and%20why%20behind%20these%20choices.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20LLM%0Adecision-making%20framework%20called%20STRUX%2C%20which%20enhances%20LLM%20decision-making%20by%0Aproviding%20structured%20explanations.%20These%20include%20favorable%20and%20adverse%20facts%0Arelated%20to%20the%20decision%2C%20along%20with%20their%20respective%20strengths.%20STRUX%20begins%20by%0Adistilling%20lengthy%20information%20into%20a%20concise%20table%20of%20key%20facts.%20It%20then%0Aemploys%20a%20series%20of%20self-reflection%20steps%20to%20determine%20which%20of%20these%20facts%20are%0Apivotal%2C%20categorizing%20them%20as%20either%20favorable%20or%20adverse%20in%20relation%20to%20a%0Aspecific%20decision.%20Lastly%2C%20we%20fine-tune%20an%20LLM%20to%20identify%20and%20prioritize%20these%0Akey%20facts%20to%20optimize%20decision-making.%20STRUX%20has%20been%20evaluated%20on%20the%0Achallenging%20task%20of%20forecasting%20stock%20investment%20decisions%20based%20on%20earnings%0Acall%20transcripts%20and%20demonstrated%20superior%20performance%20against%20strong%0Abaselines.%20It%20enhances%20decision%20transparency%20by%20allowing%20users%20to%20understand%0Athe%20impact%20of%20different%20factors%2C%20representing%20a%20meaningful%20step%20towards%0Apractical%20decision-making%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12583v1&entry.124074799=Read"},
{"title": "Nonlinear bayesian tomography of ion temperature and velocity for\n  Doppler coherence imaging spectroscopy in RT-1", "author": "Kenji Ueda and Masaki. Nishiura", "abstract": "  We present a novel Bayesian tomography approach for Coherence Imaging\nSpectroscopy (CIS) that simultaneously reconstructs ion temperature and\nvelocity distributions in plasmas. Utilizing nonlinear Gaussian Process\nTomography (GPT) with the Laplace approximation, we model prior distributions\nof log-emissivity, temperature, and velocity as Gaussian processes. This\nframework rigorously incorporates nonlinear effects and temperature\ndependencies often neglected in conventional CIS tomography, enabling robust\nreconstruction even in the region of high temperature and velocity. By applying\na log-Gaussian process, we also address issues like velocity divergence in\nlow-emissivity regions. Validated with phantom simulations and experimental\ndata from the RT-1 device, our method reveals detailed spatial structures of\nion temperature and toroidal ion flow characteristic of magnetospheric plasma.\nThis work significantly broadens the scope of CIS tomography, offering a robust\ntool for plasma diagnostics and facilitating integration with complementary\nmeasurement techniques.\n", "link": "http://arxiv.org/abs/2410.12424v1", "date": "2024-10-16", "relevancy": 2.2952, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5057}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4357}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20bayesian%20tomography%20of%20ion%20temperature%20and%20velocity%20for%0A%20%20Doppler%20coherence%20imaging%20spectroscopy%20in%20RT-1&body=Title%3A%20Nonlinear%20bayesian%20tomography%20of%20ion%20temperature%20and%20velocity%20for%0A%20%20Doppler%20coherence%20imaging%20spectroscopy%20in%20RT-1%0AAuthor%3A%20Kenji%20Ueda%20and%20Masaki.%20Nishiura%0AAbstract%3A%20%20%20We%20present%20a%20novel%20Bayesian%20tomography%20approach%20for%20Coherence%20Imaging%0ASpectroscopy%20%28CIS%29%20that%20simultaneously%20reconstructs%20ion%20temperature%20and%0Avelocity%20distributions%20in%20plasmas.%20Utilizing%20nonlinear%20Gaussian%20Process%0ATomography%20%28GPT%29%20with%20the%20Laplace%20approximation%2C%20we%20model%20prior%20distributions%0Aof%20log-emissivity%2C%20temperature%2C%20and%20velocity%20as%20Gaussian%20processes.%20This%0Aframework%20rigorously%20incorporates%20nonlinear%20effects%20and%20temperature%0Adependencies%20often%20neglected%20in%20conventional%20CIS%20tomography%2C%20enabling%20robust%0Areconstruction%20even%20in%20the%20region%20of%20high%20temperature%20and%20velocity.%20By%20applying%0Aa%20log-Gaussian%20process%2C%20we%20also%20address%20issues%20like%20velocity%20divergence%20in%0Alow-emissivity%20regions.%20Validated%20with%20phantom%20simulations%20and%20experimental%0Adata%20from%20the%20RT-1%20device%2C%20our%20method%20reveals%20detailed%20spatial%20structures%20of%0Aion%20temperature%20and%20toroidal%20ion%20flow%20characteristic%20of%20magnetospheric%20plasma.%0AThis%20work%20significantly%20broadens%20the%20scope%20of%20CIS%20tomography%2C%20offering%20a%20robust%0Atool%20for%20plasma%20diagnostics%20and%20facilitating%20integration%20with%20complementary%0Ameasurement%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520bayesian%2520tomography%2520of%2520ion%2520temperature%2520and%2520velocity%2520for%250A%2520%2520Doppler%2520coherence%2520imaging%2520spectroscopy%2520in%2520RT-1%26entry.906535625%3DKenji%2520Ueda%2520and%2520Masaki.%2520Nishiura%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520Bayesian%2520tomography%2520approach%2520for%2520Coherence%2520Imaging%250ASpectroscopy%2520%2528CIS%2529%2520that%2520simultaneously%2520reconstructs%2520ion%2520temperature%2520and%250Avelocity%2520distributions%2520in%2520plasmas.%2520Utilizing%2520nonlinear%2520Gaussian%2520Process%250ATomography%2520%2528GPT%2529%2520with%2520the%2520Laplace%2520approximation%252C%2520we%2520model%2520prior%2520distributions%250Aof%2520log-emissivity%252C%2520temperature%252C%2520and%2520velocity%2520as%2520Gaussian%2520processes.%2520This%250Aframework%2520rigorously%2520incorporates%2520nonlinear%2520effects%2520and%2520temperature%250Adependencies%2520often%2520neglected%2520in%2520conventional%2520CIS%2520tomography%252C%2520enabling%2520robust%250Areconstruction%2520even%2520in%2520the%2520region%2520of%2520high%2520temperature%2520and%2520velocity.%2520By%2520applying%250Aa%2520log-Gaussian%2520process%252C%2520we%2520also%2520address%2520issues%2520like%2520velocity%2520divergence%2520in%250Alow-emissivity%2520regions.%2520Validated%2520with%2520phantom%2520simulations%2520and%2520experimental%250Adata%2520from%2520the%2520RT-1%2520device%252C%2520our%2520method%2520reveals%2520detailed%2520spatial%2520structures%2520of%250Aion%2520temperature%2520and%2520toroidal%2520ion%2520flow%2520characteristic%2520of%2520magnetospheric%2520plasma.%250AThis%2520work%2520significantly%2520broadens%2520the%2520scope%2520of%2520CIS%2520tomography%252C%2520offering%2520a%2520robust%250Atool%2520for%2520plasma%2520diagnostics%2520and%2520facilitating%2520integration%2520with%2520complementary%250Ameasurement%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20bayesian%20tomography%20of%20ion%20temperature%20and%20velocity%20for%0A%20%20Doppler%20coherence%20imaging%20spectroscopy%20in%20RT-1&entry.906535625=Kenji%20Ueda%20and%20Masaki.%20Nishiura&entry.1292438233=%20%20We%20present%20a%20novel%20Bayesian%20tomography%20approach%20for%20Coherence%20Imaging%0ASpectroscopy%20%28CIS%29%20that%20simultaneously%20reconstructs%20ion%20temperature%20and%0Avelocity%20distributions%20in%20plasmas.%20Utilizing%20nonlinear%20Gaussian%20Process%0ATomography%20%28GPT%29%20with%20the%20Laplace%20approximation%2C%20we%20model%20prior%20distributions%0Aof%20log-emissivity%2C%20temperature%2C%20and%20velocity%20as%20Gaussian%20processes.%20This%0Aframework%20rigorously%20incorporates%20nonlinear%20effects%20and%20temperature%0Adependencies%20often%20neglected%20in%20conventional%20CIS%20tomography%2C%20enabling%20robust%0Areconstruction%20even%20in%20the%20region%20of%20high%20temperature%20and%20velocity.%20By%20applying%0Aa%20log-Gaussian%20process%2C%20we%20also%20address%20issues%20like%20velocity%20divergence%20in%0Alow-emissivity%20regions.%20Validated%20with%20phantom%20simulations%20and%20experimental%0Adata%20from%20the%20RT-1%20device%2C%20our%20method%20reveals%20detailed%20spatial%20structures%20of%0Aion%20temperature%20and%20toroidal%20ion%20flow%20characteristic%20of%20magnetospheric%20plasma.%0AThis%20work%20significantly%20broadens%20the%20scope%20of%20CIS%20tomography%2C%20offering%20a%20robust%0Atool%20for%20plasma%20diagnostics%20and%20facilitating%20integration%20with%20complementary%0Ameasurement%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12424v1&entry.124074799=Read"},
{"title": "Vector Field-Guided Learning Predictive Control for Motion Planning of\n  Mobile Robots with Uncertain Dynamics", "author": "Yang Lu and Weijia Yao and Yongqian Xiao and Xinglong Zhang and Xin Xu and Yaonan Wang and Dingbang Xiao", "abstract": "  In obstacle-dense scenarios, providing safe guidance for mobile robots is\ncritical to improve the safe maneuvering capability. However, the guidance\nprovided by standard guiding vector fields (GVFs) may limit the motion\ncapability due to the improper curvature of the integral curve when traversing\nobstacles. On the other hand, robotic system dynamics are often time-varying,\nuncertain, and even unknown during the motion planning process. Therefore, many\nexisting kinodynamic motion planning methods could not achieve satisfactory\nreliability in guaranteeing safety. To address these challenges, we propose a\ntwo-level Vector Field-guided Learning Predictive Control (VF-LPC) approach\nthat improves safe maneuverability. The first level, the guiding level,\ngenerates safe desired trajectories using the designed kinodynamic GVF,\nenabling safe motion in obstacle-dense environments. The second level, the\nIntegrated Motion Planning and Control (IMPC) level, first uses a deep Koopman\noperator to learn a nominal dynamics model offline and then updates the model\nuncertainties online using sparse Gaussian processes (GPs). The learned\ndynamics and a game-based safe barrier function are then incorporated into the\nLPC framework to generate near-optimal planning solutions. Extensive\nsimulations and real-world experiments were conducted on quadrotor unmanned\naerial vehicles and unmanned ground vehicles, demonstrating that VF-LPC enables\nrobots to maneuver safely.\n", "link": "http://arxiv.org/abs/2405.08283v3", "date": "2024-10-16", "relevancy": 2.2937, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5856}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5687}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics&body=Title%3A%20Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics%0AAuthor%3A%20Yang%20Lu%20and%20Weijia%20Yao%20and%20Yongqian%20Xiao%20and%20Xinglong%20Zhang%20and%20Xin%20Xu%20and%20Yaonan%20Wang%20and%20Dingbang%20Xiao%0AAbstract%3A%20%20%20In%20obstacle-dense%20scenarios%2C%20providing%20safe%20guidance%20for%20mobile%20robots%20is%0Acritical%20to%20improve%20the%20safe%20maneuvering%20capability.%20However%2C%20the%20guidance%0Aprovided%20by%20standard%20guiding%20vector%20fields%20%28GVFs%29%20may%20limit%20the%20motion%0Acapability%20due%20to%20the%20improper%20curvature%20of%20the%20integral%20curve%20when%20traversing%0Aobstacles.%20On%20the%20other%20hand%2C%20robotic%20system%20dynamics%20are%20often%20time-varying%2C%0Auncertain%2C%20and%20even%20unknown%20during%20the%20motion%20planning%20process.%20Therefore%2C%20many%0Aexisting%20kinodynamic%20motion%20planning%20methods%20could%20not%20achieve%20satisfactory%0Areliability%20in%20guaranteeing%20safety.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atwo-level%20Vector%20Field-guided%20Learning%20Predictive%20Control%20%28VF-LPC%29%20approach%0Athat%20improves%20safe%20maneuverability.%20The%20first%20level%2C%20the%20guiding%20level%2C%0Agenerates%20safe%20desired%20trajectories%20using%20the%20designed%20kinodynamic%20GVF%2C%0Aenabling%20safe%20motion%20in%20obstacle-dense%20environments.%20The%20second%20level%2C%20the%0AIntegrated%20Motion%20Planning%20and%20Control%20%28IMPC%29%20level%2C%20first%20uses%20a%20deep%20Koopman%0Aoperator%20to%20learn%20a%20nominal%20dynamics%20model%20offline%20and%20then%20updates%20the%20model%0Auncertainties%20online%20using%20sparse%20Gaussian%20processes%20%28GPs%29.%20The%20learned%0Adynamics%20and%20a%20game-based%20safe%20barrier%20function%20are%20then%20incorporated%20into%20the%0ALPC%20framework%20to%20generate%20near-optimal%20planning%20solutions.%20Extensive%0Asimulations%20and%20real-world%20experiments%20were%20conducted%20on%20quadrotor%20unmanned%0Aaerial%20vehicles%20and%20unmanned%20ground%20vehicles%2C%20demonstrating%20that%20VF-LPC%20enables%0Arobots%20to%20maneuver%20safely.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08283v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector%2520Field-Guided%2520Learning%2520Predictive%2520Control%2520for%2520Motion%2520Planning%2520of%250A%2520%2520Mobile%2520Robots%2520with%2520Uncertain%2520Dynamics%26entry.906535625%3DYang%2520Lu%2520and%2520Weijia%2520Yao%2520and%2520Yongqian%2520Xiao%2520and%2520Xinglong%2520Zhang%2520and%2520Xin%2520Xu%2520and%2520Yaonan%2520Wang%2520and%2520Dingbang%2520Xiao%26entry.1292438233%3D%2520%2520In%2520obstacle-dense%2520scenarios%252C%2520providing%2520safe%2520guidance%2520for%2520mobile%2520robots%2520is%250Acritical%2520to%2520improve%2520the%2520safe%2520maneuvering%2520capability.%2520However%252C%2520the%2520guidance%250Aprovided%2520by%2520standard%2520guiding%2520vector%2520fields%2520%2528GVFs%2529%2520may%2520limit%2520the%2520motion%250Acapability%2520due%2520to%2520the%2520improper%2520curvature%2520of%2520the%2520integral%2520curve%2520when%2520traversing%250Aobstacles.%2520On%2520the%2520other%2520hand%252C%2520robotic%2520system%2520dynamics%2520are%2520often%2520time-varying%252C%250Auncertain%252C%2520and%2520even%2520unknown%2520during%2520the%2520motion%2520planning%2520process.%2520Therefore%252C%2520many%250Aexisting%2520kinodynamic%2520motion%2520planning%2520methods%2520could%2520not%2520achieve%2520satisfactory%250Areliability%2520in%2520guaranteeing%2520safety.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Atwo-level%2520Vector%2520Field-guided%2520Learning%2520Predictive%2520Control%2520%2528VF-LPC%2529%2520approach%250Athat%2520improves%2520safe%2520maneuverability.%2520The%2520first%2520level%252C%2520the%2520guiding%2520level%252C%250Agenerates%2520safe%2520desired%2520trajectories%2520using%2520the%2520designed%2520kinodynamic%2520GVF%252C%250Aenabling%2520safe%2520motion%2520in%2520obstacle-dense%2520environments.%2520The%2520second%2520level%252C%2520the%250AIntegrated%2520Motion%2520Planning%2520and%2520Control%2520%2528IMPC%2529%2520level%252C%2520first%2520uses%2520a%2520deep%2520Koopman%250Aoperator%2520to%2520learn%2520a%2520nominal%2520dynamics%2520model%2520offline%2520and%2520then%2520updates%2520the%2520model%250Auncertainties%2520online%2520using%2520sparse%2520Gaussian%2520processes%2520%2528GPs%2529.%2520The%2520learned%250Adynamics%2520and%2520a%2520game-based%2520safe%2520barrier%2520function%2520are%2520then%2520incorporated%2520into%2520the%250ALPC%2520framework%2520to%2520generate%2520near-optimal%2520planning%2520solutions.%2520Extensive%250Asimulations%2520and%2520real-world%2520experiments%2520were%2520conducted%2520on%2520quadrotor%2520unmanned%250Aaerial%2520vehicles%2520and%2520unmanned%2520ground%2520vehicles%252C%2520demonstrating%2520that%2520VF-LPC%2520enables%250Arobots%2520to%2520maneuver%2520safely.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08283v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector%20Field-Guided%20Learning%20Predictive%20Control%20for%20Motion%20Planning%20of%0A%20%20Mobile%20Robots%20with%20Uncertain%20Dynamics&entry.906535625=Yang%20Lu%20and%20Weijia%20Yao%20and%20Yongqian%20Xiao%20and%20Xinglong%20Zhang%20and%20Xin%20Xu%20and%20Yaonan%20Wang%20and%20Dingbang%20Xiao&entry.1292438233=%20%20In%20obstacle-dense%20scenarios%2C%20providing%20safe%20guidance%20for%20mobile%20robots%20is%0Acritical%20to%20improve%20the%20safe%20maneuvering%20capability.%20However%2C%20the%20guidance%0Aprovided%20by%20standard%20guiding%20vector%20fields%20%28GVFs%29%20may%20limit%20the%20motion%0Acapability%20due%20to%20the%20improper%20curvature%20of%20the%20integral%20curve%20when%20traversing%0Aobstacles.%20On%20the%20other%20hand%2C%20robotic%20system%20dynamics%20are%20often%20time-varying%2C%0Auncertain%2C%20and%20even%20unknown%20during%20the%20motion%20planning%20process.%20Therefore%2C%20many%0Aexisting%20kinodynamic%20motion%20planning%20methods%20could%20not%20achieve%20satisfactory%0Areliability%20in%20guaranteeing%20safety.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Atwo-level%20Vector%20Field-guided%20Learning%20Predictive%20Control%20%28VF-LPC%29%20approach%0Athat%20improves%20safe%20maneuverability.%20The%20first%20level%2C%20the%20guiding%20level%2C%0Agenerates%20safe%20desired%20trajectories%20using%20the%20designed%20kinodynamic%20GVF%2C%0Aenabling%20safe%20motion%20in%20obstacle-dense%20environments.%20The%20second%20level%2C%20the%0AIntegrated%20Motion%20Planning%20and%20Control%20%28IMPC%29%20level%2C%20first%20uses%20a%20deep%20Koopman%0Aoperator%20to%20learn%20a%20nominal%20dynamics%20model%20offline%20and%20then%20updates%20the%20model%0Auncertainties%20online%20using%20sparse%20Gaussian%20processes%20%28GPs%29.%20The%20learned%0Adynamics%20and%20a%20game-based%20safe%20barrier%20function%20are%20then%20incorporated%20into%20the%0ALPC%20framework%20to%20generate%20near-optimal%20planning%20solutions.%20Extensive%0Asimulations%20and%20real-world%20experiments%20were%20conducted%20on%20quadrotor%20unmanned%0Aaerial%20vehicles%20and%20unmanned%20ground%20vehicles%2C%20demonstrating%20that%20VF-LPC%20enables%0Arobots%20to%20maneuver%20safely.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08283v3&entry.124074799=Read"},
{"title": "AdaCropFollow: Self-Supervised Online Adaptation for Visual Under-Canopy\n  Navigation", "author": "Arun N. Sivakumar and Federico Magistri and Mateus V. Gasparino and Jens Behley and Cyrill Stachniss and Girish Chowdhary", "abstract": "  Under-canopy agricultural robots can enable various applications like precise\nmonitoring, spraying, weeding, and plant manipulation tasks throughout the\ngrowing season. Autonomous navigation under the canopy is challenging due to\nthe degradation in accuracy of RTK-GPS and the large variability in the visual\nappearance of the scene over time. In prior work, we developed a supervised\nlearning-based perception system with semantic keypoint representation and\ndeployed this in various field conditions. A large number of failures of this\nsystem can be attributed to the inability of the perception model to adapt to\nthe domain shift encountered during deployment. In this paper, we propose a\nself-supervised online adaptation method for adapting the semantic keypoint\nrepresentation using a visual foundational model, geometric prior, and pseudo\nlabeling. Our preliminary experiments show that with minimal data and\nfine-tuning of parameters, the keypoint prediction model trained with labels on\nthe source domain can be adapted in a self-supervised manner to various\nchallenging target domains onboard the robot computer using our method. This\ncan enable fully autonomous row-following capability in under-canopy robots\nacross fields and crops without requiring human intervention.\n", "link": "http://arxiv.org/abs/2410.12411v1", "date": "2024-10-16", "relevancy": 2.2753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5865}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.57}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaCropFollow%3A%20Self-Supervised%20Online%20Adaptation%20for%20Visual%20Under-Canopy%0A%20%20Navigation&body=Title%3A%20AdaCropFollow%3A%20Self-Supervised%20Online%20Adaptation%20for%20Visual%20Under-Canopy%0A%20%20Navigation%0AAuthor%3A%20Arun%20N.%20Sivakumar%20and%20Federico%20Magistri%20and%20Mateus%20V.%20Gasparino%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%20and%20Girish%20Chowdhary%0AAbstract%3A%20%20%20Under-canopy%20agricultural%20robots%20can%20enable%20various%20applications%20like%20precise%0Amonitoring%2C%20spraying%2C%20weeding%2C%20and%20plant%20manipulation%20tasks%20throughout%20the%0Agrowing%20season.%20Autonomous%20navigation%20under%20the%20canopy%20is%20challenging%20due%20to%0Athe%20degradation%20in%20accuracy%20of%20RTK-GPS%20and%20the%20large%20variability%20in%20the%20visual%0Aappearance%20of%20the%20scene%20over%20time.%20In%20prior%20work%2C%20we%20developed%20a%20supervised%0Alearning-based%20perception%20system%20with%20semantic%20keypoint%20representation%20and%0Adeployed%20this%20in%20various%20field%20conditions.%20A%20large%20number%20of%20failures%20of%20this%0Asystem%20can%20be%20attributed%20to%20the%20inability%20of%20the%20perception%20model%20to%20adapt%20to%0Athe%20domain%20shift%20encountered%20during%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-supervised%20online%20adaptation%20method%20for%20adapting%20the%20semantic%20keypoint%0Arepresentation%20using%20a%20visual%20foundational%20model%2C%20geometric%20prior%2C%20and%20pseudo%0Alabeling.%20Our%20preliminary%20experiments%20show%20that%20with%20minimal%20data%20and%0Afine-tuning%20of%20parameters%2C%20the%20keypoint%20prediction%20model%20trained%20with%20labels%20on%0Athe%20source%20domain%20can%20be%20adapted%20in%20a%20self-supervised%20manner%20to%20various%0Achallenging%20target%20domains%20onboard%20the%20robot%20computer%20using%20our%20method.%20This%0Acan%20enable%20fully%20autonomous%20row-following%20capability%20in%20under-canopy%20robots%0Aacross%20fields%20and%20crops%20without%20requiring%20human%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaCropFollow%253A%2520Self-Supervised%2520Online%2520Adaptation%2520for%2520Visual%2520Under-Canopy%250A%2520%2520Navigation%26entry.906535625%3DArun%2520N.%2520Sivakumar%2520and%2520Federico%2520Magistri%2520and%2520Mateus%2520V.%2520Gasparino%2520and%2520Jens%2520Behley%2520and%2520Cyrill%2520Stachniss%2520and%2520Girish%2520Chowdhary%26entry.1292438233%3D%2520%2520Under-canopy%2520agricultural%2520robots%2520can%2520enable%2520various%2520applications%2520like%2520precise%250Amonitoring%252C%2520spraying%252C%2520weeding%252C%2520and%2520plant%2520manipulation%2520tasks%2520throughout%2520the%250Agrowing%2520season.%2520Autonomous%2520navigation%2520under%2520the%2520canopy%2520is%2520challenging%2520due%2520to%250Athe%2520degradation%2520in%2520accuracy%2520of%2520RTK-GPS%2520and%2520the%2520large%2520variability%2520in%2520the%2520visual%250Aappearance%2520of%2520the%2520scene%2520over%2520time.%2520In%2520prior%2520work%252C%2520we%2520developed%2520a%2520supervised%250Alearning-based%2520perception%2520system%2520with%2520semantic%2520keypoint%2520representation%2520and%250Adeployed%2520this%2520in%2520various%2520field%2520conditions.%2520A%2520large%2520number%2520of%2520failures%2520of%2520this%250Asystem%2520can%2520be%2520attributed%2520to%2520the%2520inability%2520of%2520the%2520perception%2520model%2520to%2520adapt%2520to%250Athe%2520domain%2520shift%2520encountered%2520during%2520deployment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aself-supervised%2520online%2520adaptation%2520method%2520for%2520adapting%2520the%2520semantic%2520keypoint%250Arepresentation%2520using%2520a%2520visual%2520foundational%2520model%252C%2520geometric%2520prior%252C%2520and%2520pseudo%250Alabeling.%2520Our%2520preliminary%2520experiments%2520show%2520that%2520with%2520minimal%2520data%2520and%250Afine-tuning%2520of%2520parameters%252C%2520the%2520keypoint%2520prediction%2520model%2520trained%2520with%2520labels%2520on%250Athe%2520source%2520domain%2520can%2520be%2520adapted%2520in%2520a%2520self-supervised%2520manner%2520to%2520various%250Achallenging%2520target%2520domains%2520onboard%2520the%2520robot%2520computer%2520using%2520our%2520method.%2520This%250Acan%2520enable%2520fully%2520autonomous%2520row-following%2520capability%2520in%2520under-canopy%2520robots%250Aacross%2520fields%2520and%2520crops%2520without%2520requiring%2520human%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaCropFollow%3A%20Self-Supervised%20Online%20Adaptation%20for%20Visual%20Under-Canopy%0A%20%20Navigation&entry.906535625=Arun%20N.%20Sivakumar%20and%20Federico%20Magistri%20and%20Mateus%20V.%20Gasparino%20and%20Jens%20Behley%20and%20Cyrill%20Stachniss%20and%20Girish%20Chowdhary&entry.1292438233=%20%20Under-canopy%20agricultural%20robots%20can%20enable%20various%20applications%20like%20precise%0Amonitoring%2C%20spraying%2C%20weeding%2C%20and%20plant%20manipulation%20tasks%20throughout%20the%0Agrowing%20season.%20Autonomous%20navigation%20under%20the%20canopy%20is%20challenging%20due%20to%0Athe%20degradation%20in%20accuracy%20of%20RTK-GPS%20and%20the%20large%20variability%20in%20the%20visual%0Aappearance%20of%20the%20scene%20over%20time.%20In%20prior%20work%2C%20we%20developed%20a%20supervised%0Alearning-based%20perception%20system%20with%20semantic%20keypoint%20representation%20and%0Adeployed%20this%20in%20various%20field%20conditions.%20A%20large%20number%20of%20failures%20of%20this%0Asystem%20can%20be%20attributed%20to%20the%20inability%20of%20the%20perception%20model%20to%20adapt%20to%0Athe%20domain%20shift%20encountered%20during%20deployment.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-supervised%20online%20adaptation%20method%20for%20adapting%20the%20semantic%20keypoint%0Arepresentation%20using%20a%20visual%20foundational%20model%2C%20geometric%20prior%2C%20and%20pseudo%0Alabeling.%20Our%20preliminary%20experiments%20show%20that%20with%20minimal%20data%20and%0Afine-tuning%20of%20parameters%2C%20the%20keypoint%20prediction%20model%20trained%20with%20labels%20on%0Athe%20source%20domain%20can%20be%20adapted%20in%20a%20self-supervised%20manner%20to%20various%0Achallenging%20target%20domains%20onboard%20the%20robot%20computer%20using%20our%20method.%20This%0Acan%20enable%20fully%20autonomous%20row-following%20capability%20in%20under-canopy%20robots%0Aacross%20fields%20and%20crops%20without%20requiring%20human%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12411v1&entry.124074799=Read"},
{"title": "Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models", "author": "Luis Barroso-Luque and Muhammed Shuaibi and Xiang Fu and Brandon M. Wood and Misko Dzamba and Meng Gao and Ammar Rizvi and C. Lawrence Zitnick and Zachary W. Ulissi", "abstract": "  The ability to discover new materials with desirable properties is critical\nfor numerous applications from helping mitigate climate change to advances in\nnext generation computing hardware. AI has the potential to accelerate\nmaterials discovery and design by more effectively exploring the chemical space\ncompared to other computational methods or by trial-and-error. While\nsubstantial progress has been made on AI for materials data, benchmarks, and\nmodels, a barrier that has emerged is the lack of publicly available training\ndata and open pre-trained models. To address this, we present a Meta FAIR\nrelease of the Open Materials 2024 (OMat24) large-scale open dataset and an\naccompanying set of pre-trained models. OMat24 contains over 110 million\ndensity functional theory (DFT) calculations focused on structural and\ncompositional diversity. Our EquiformerV2 models achieve state-of-the-art\nperformance on the Matbench Discovery leaderboard and are capable of predicting\nground-state stability and formation energies to an F1 score above 0.9 and an\naccuracy of 20 meV/atom, respectively. We explore the impact of model size,\nauxiliary denoising objectives, and fine-tuning on performance across a range\nof datasets including OMat24, MPtraj, and Alexandria. The open release of the\nOMat24 dataset and models enables the research community to build upon our\nefforts and drive further advancements in AI-assisted materials science.\n", "link": "http://arxiv.org/abs/2410.12771v1", "date": "2024-10-16", "relevancy": 2.2642, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4532}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Materials%202024%20%28OMat24%29%20Inorganic%20Materials%20Dataset%20and%20Models&body=Title%3A%20Open%20Materials%202024%20%28OMat24%29%20Inorganic%20Materials%20Dataset%20and%20Models%0AAuthor%3A%20Luis%20Barroso-Luque%20and%20Muhammed%20Shuaibi%20and%20Xiang%20Fu%20and%20Brandon%20M.%20Wood%20and%20Misko%20Dzamba%20and%20Meng%20Gao%20and%20Ammar%20Rizvi%20and%20C.%20Lawrence%20Zitnick%20and%20Zachary%20W.%20Ulissi%0AAbstract%3A%20%20%20The%20ability%20to%20discover%20new%20materials%20with%20desirable%20properties%20is%20critical%0Afor%20numerous%20applications%20from%20helping%20mitigate%20climate%20change%20to%20advances%20in%0Anext%20generation%20computing%20hardware.%20AI%20has%20the%20potential%20to%20accelerate%0Amaterials%20discovery%20and%20design%20by%20more%20effectively%20exploring%20the%20chemical%20space%0Acompared%20to%20other%20computational%20methods%20or%20by%20trial-and-error.%20While%0Asubstantial%20progress%20has%20been%20made%20on%20AI%20for%20materials%20data%2C%20benchmarks%2C%20and%0Amodels%2C%20a%20barrier%20that%20has%20emerged%20is%20the%20lack%20of%20publicly%20available%20training%0Adata%20and%20open%20pre-trained%20models.%20To%20address%20this%2C%20we%20present%20a%20Meta%20FAIR%0Arelease%20of%20the%20Open%20Materials%202024%20%28OMat24%29%20large-scale%20open%20dataset%20and%20an%0Aaccompanying%20set%20of%20pre-trained%20models.%20OMat24%20contains%20over%20110%20million%0Adensity%20functional%20theory%20%28DFT%29%20calculations%20focused%20on%20structural%20and%0Acompositional%20diversity.%20Our%20EquiformerV2%20models%20achieve%20state-of-the-art%0Aperformance%20on%20the%20Matbench%20Discovery%20leaderboard%20and%20are%20capable%20of%20predicting%0Aground-state%20stability%20and%20formation%20energies%20to%20an%20F1%20score%20above%200.9%20and%20an%0Aaccuracy%20of%2020%20meV/atom%2C%20respectively.%20We%20explore%20the%20impact%20of%20model%20size%2C%0Aauxiliary%20denoising%20objectives%2C%20and%20fine-tuning%20on%20performance%20across%20a%20range%0Aof%20datasets%20including%20OMat24%2C%20MPtraj%2C%20and%20Alexandria.%20The%20open%20release%20of%20the%0AOMat24%20dataset%20and%20models%20enables%20the%20research%20community%20to%20build%20upon%20our%0Aefforts%20and%20drive%20further%20advancements%20in%20AI-assisted%20materials%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Materials%25202024%2520%2528OMat24%2529%2520Inorganic%2520Materials%2520Dataset%2520and%2520Models%26entry.906535625%3DLuis%2520Barroso-Luque%2520and%2520Muhammed%2520Shuaibi%2520and%2520Xiang%2520Fu%2520and%2520Brandon%2520M.%2520Wood%2520and%2520Misko%2520Dzamba%2520and%2520Meng%2520Gao%2520and%2520Ammar%2520Rizvi%2520and%2520C.%2520Lawrence%2520Zitnick%2520and%2520Zachary%2520W.%2520Ulissi%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520discover%2520new%2520materials%2520with%2520desirable%2520properties%2520is%2520critical%250Afor%2520numerous%2520applications%2520from%2520helping%2520mitigate%2520climate%2520change%2520to%2520advances%2520in%250Anext%2520generation%2520computing%2520hardware.%2520AI%2520has%2520the%2520potential%2520to%2520accelerate%250Amaterials%2520discovery%2520and%2520design%2520by%2520more%2520effectively%2520exploring%2520the%2520chemical%2520space%250Acompared%2520to%2520other%2520computational%2520methods%2520or%2520by%2520trial-and-error.%2520While%250Asubstantial%2520progress%2520has%2520been%2520made%2520on%2520AI%2520for%2520materials%2520data%252C%2520benchmarks%252C%2520and%250Amodels%252C%2520a%2520barrier%2520that%2520has%2520emerged%2520is%2520the%2520lack%2520of%2520publicly%2520available%2520training%250Adata%2520and%2520open%2520pre-trained%2520models.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520Meta%2520FAIR%250Arelease%2520of%2520the%2520Open%2520Materials%25202024%2520%2528OMat24%2529%2520large-scale%2520open%2520dataset%2520and%2520an%250Aaccompanying%2520set%2520of%2520pre-trained%2520models.%2520OMat24%2520contains%2520over%2520110%2520million%250Adensity%2520functional%2520theory%2520%2528DFT%2529%2520calculations%2520focused%2520on%2520structural%2520and%250Acompositional%2520diversity.%2520Our%2520EquiformerV2%2520models%2520achieve%2520state-of-the-art%250Aperformance%2520on%2520the%2520Matbench%2520Discovery%2520leaderboard%2520and%2520are%2520capable%2520of%2520predicting%250Aground-state%2520stability%2520and%2520formation%2520energies%2520to%2520an%2520F1%2520score%2520above%25200.9%2520and%2520an%250Aaccuracy%2520of%252020%2520meV/atom%252C%2520respectively.%2520We%2520explore%2520the%2520impact%2520of%2520model%2520size%252C%250Aauxiliary%2520denoising%2520objectives%252C%2520and%2520fine-tuning%2520on%2520performance%2520across%2520a%2520range%250Aof%2520datasets%2520including%2520OMat24%252C%2520MPtraj%252C%2520and%2520Alexandria.%2520The%2520open%2520release%2520of%2520the%250AOMat24%2520dataset%2520and%2520models%2520enables%2520the%2520research%2520community%2520to%2520build%2520upon%2520our%250Aefforts%2520and%2520drive%2520further%2520advancements%2520in%2520AI-assisted%2520materials%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Materials%202024%20%28OMat24%29%20Inorganic%20Materials%20Dataset%20and%20Models&entry.906535625=Luis%20Barroso-Luque%20and%20Muhammed%20Shuaibi%20and%20Xiang%20Fu%20and%20Brandon%20M.%20Wood%20and%20Misko%20Dzamba%20and%20Meng%20Gao%20and%20Ammar%20Rizvi%20and%20C.%20Lawrence%20Zitnick%20and%20Zachary%20W.%20Ulissi&entry.1292438233=%20%20The%20ability%20to%20discover%20new%20materials%20with%20desirable%20properties%20is%20critical%0Afor%20numerous%20applications%20from%20helping%20mitigate%20climate%20change%20to%20advances%20in%0Anext%20generation%20computing%20hardware.%20AI%20has%20the%20potential%20to%20accelerate%0Amaterials%20discovery%20and%20design%20by%20more%20effectively%20exploring%20the%20chemical%20space%0Acompared%20to%20other%20computational%20methods%20or%20by%20trial-and-error.%20While%0Asubstantial%20progress%20has%20been%20made%20on%20AI%20for%20materials%20data%2C%20benchmarks%2C%20and%0Amodels%2C%20a%20barrier%20that%20has%20emerged%20is%20the%20lack%20of%20publicly%20available%20training%0Adata%20and%20open%20pre-trained%20models.%20To%20address%20this%2C%20we%20present%20a%20Meta%20FAIR%0Arelease%20of%20the%20Open%20Materials%202024%20%28OMat24%29%20large-scale%20open%20dataset%20and%20an%0Aaccompanying%20set%20of%20pre-trained%20models.%20OMat24%20contains%20over%20110%20million%0Adensity%20functional%20theory%20%28DFT%29%20calculations%20focused%20on%20structural%20and%0Acompositional%20diversity.%20Our%20EquiformerV2%20models%20achieve%20state-of-the-art%0Aperformance%20on%20the%20Matbench%20Discovery%20leaderboard%20and%20are%20capable%20of%20predicting%0Aground-state%20stability%20and%20formation%20energies%20to%20an%20F1%20score%20above%200.9%20and%20an%0Aaccuracy%20of%2020%20meV/atom%2C%20respectively.%20We%20explore%20the%20impact%20of%20model%20size%2C%0Aauxiliary%20denoising%20objectives%2C%20and%20fine-tuning%20on%20performance%20across%20a%20range%0Aof%20datasets%20including%20OMat24%2C%20MPtraj%2C%20and%20Alexandria.%20The%20open%20release%20of%20the%0AOMat24%20dataset%20and%20models%20enables%20the%20research%20community%20to%20build%20upon%20our%0Aefforts%20and%20drive%20further%20advancements%20in%20AI-assisted%20materials%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12771v1&entry.124074799=Read"},
{"title": "FairGLVQ: Fairness in Partition-Based Classification", "author": "Felix St\u00f6rck and Fabian Hinder and Johannes Brinkrolf and Benjamin Paassen and Valerie Vaquet and Barbara Hammer", "abstract": "  Fairness is an important objective throughout society. From the distribution\nof limited goods such as education, over hiring and payment, to taxes,\nlegislation, and jurisprudence. Due to the increasing importance of machine\nlearning approaches in all areas of daily life including those related to\nhealth, security, and equity, an increasing amount of research focuses on fair\nmachine learning. In this work, we focus on the fairness of partition- and\nprototype-based models. The contribution of this work is twofold: 1) we develop\na general framework for fair machine learning of partition-based models that\ndoes not depend on a specific fairness definition, and 2) we derive a fair\nversion of learning vector quantization (LVQ) as a specific instantiation. We\ncompare the resulting algorithm against other algorithms from the literature on\ntheoretical and real-world data showing its practical relevance.\n", "link": "http://arxiv.org/abs/2410.12452v1", "date": "2024-10-16", "relevancy": 2.2575, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.456}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairGLVQ%3A%20Fairness%20in%20Partition-Based%20Classification&body=Title%3A%20FairGLVQ%3A%20Fairness%20in%20Partition-Based%20Classification%0AAuthor%3A%20Felix%20St%C3%B6rck%20and%20Fabian%20Hinder%20and%20Johannes%20Brinkrolf%20and%20Benjamin%20Paassen%20and%20Valerie%20Vaquet%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20Fairness%20is%20an%20important%20objective%20throughout%20society.%20From%20the%20distribution%0Aof%20limited%20goods%20such%20as%20education%2C%20over%20hiring%20and%20payment%2C%20to%20taxes%2C%0Alegislation%2C%20and%20jurisprudence.%20Due%20to%20the%20increasing%20importance%20of%20machine%0Alearning%20approaches%20in%20all%20areas%20of%20daily%20life%20including%20those%20related%20to%0Ahealth%2C%20security%2C%20and%20equity%2C%20an%20increasing%20amount%20of%20research%20focuses%20on%20fair%0Amachine%20learning.%20In%20this%20work%2C%20we%20focus%20on%20the%20fairness%20of%20partition-%20and%0Aprototype-based%20models.%20The%20contribution%20of%20this%20work%20is%20twofold%3A%201%29%20we%20develop%0Aa%20general%20framework%20for%20fair%20machine%20learning%20of%20partition-based%20models%20that%0Adoes%20not%20depend%20on%20a%20specific%20fairness%20definition%2C%20and%202%29%20we%20derive%20a%20fair%0Aversion%20of%20learning%20vector%20quantization%20%28LVQ%29%20as%20a%20specific%20instantiation.%20We%0Acompare%20the%20resulting%20algorithm%20against%20other%20algorithms%20from%20the%20literature%20on%0Atheoretical%20and%20real-world%20data%20showing%20its%20practical%20relevance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairGLVQ%253A%2520Fairness%2520in%2520Partition-Based%2520Classification%26entry.906535625%3DFelix%2520St%25C3%25B6rck%2520and%2520Fabian%2520Hinder%2520and%2520Johannes%2520Brinkrolf%2520and%2520Benjamin%2520Paassen%2520and%2520Valerie%2520Vaquet%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520Fairness%2520is%2520an%2520important%2520objective%2520throughout%2520society.%2520From%2520the%2520distribution%250Aof%2520limited%2520goods%2520such%2520as%2520education%252C%2520over%2520hiring%2520and%2520payment%252C%2520to%2520taxes%252C%250Alegislation%252C%2520and%2520jurisprudence.%2520Due%2520to%2520the%2520increasing%2520importance%2520of%2520machine%250Alearning%2520approaches%2520in%2520all%2520areas%2520of%2520daily%2520life%2520including%2520those%2520related%2520to%250Ahealth%252C%2520security%252C%2520and%2520equity%252C%2520an%2520increasing%2520amount%2520of%2520research%2520focuses%2520on%2520fair%250Amachine%2520learning.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520fairness%2520of%2520partition-%2520and%250Aprototype-based%2520models.%2520The%2520contribution%2520of%2520this%2520work%2520is%2520twofold%253A%25201%2529%2520we%2520develop%250Aa%2520general%2520framework%2520for%2520fair%2520machine%2520learning%2520of%2520partition-based%2520models%2520that%250Adoes%2520not%2520depend%2520on%2520a%2520specific%2520fairness%2520definition%252C%2520and%25202%2529%2520we%2520derive%2520a%2520fair%250Aversion%2520of%2520learning%2520vector%2520quantization%2520%2528LVQ%2529%2520as%2520a%2520specific%2520instantiation.%2520We%250Acompare%2520the%2520resulting%2520algorithm%2520against%2520other%2520algorithms%2520from%2520the%2520literature%2520on%250Atheoretical%2520and%2520real-world%2520data%2520showing%2520its%2520practical%2520relevance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairGLVQ%3A%20Fairness%20in%20Partition-Based%20Classification&entry.906535625=Felix%20St%C3%B6rck%20and%20Fabian%20Hinder%20and%20Johannes%20Brinkrolf%20and%20Benjamin%20Paassen%20and%20Valerie%20Vaquet%20and%20Barbara%20Hammer&entry.1292438233=%20%20Fairness%20is%20an%20important%20objective%20throughout%20society.%20From%20the%20distribution%0Aof%20limited%20goods%20such%20as%20education%2C%20over%20hiring%20and%20payment%2C%20to%20taxes%2C%0Alegislation%2C%20and%20jurisprudence.%20Due%20to%20the%20increasing%20importance%20of%20machine%0Alearning%20approaches%20in%20all%20areas%20of%20daily%20life%20including%20those%20related%20to%0Ahealth%2C%20security%2C%20and%20equity%2C%20an%20increasing%20amount%20of%20research%20focuses%20on%20fair%0Amachine%20learning.%20In%20this%20work%2C%20we%20focus%20on%20the%20fairness%20of%20partition-%20and%0Aprototype-based%20models.%20The%20contribution%20of%20this%20work%20is%20twofold%3A%201%29%20we%20develop%0Aa%20general%20framework%20for%20fair%20machine%20learning%20of%20partition-based%20models%20that%0Adoes%20not%20depend%20on%20a%20specific%20fairness%20definition%2C%20and%202%29%20we%20derive%20a%20fair%0Aversion%20of%20learning%20vector%20quantization%20%28LVQ%29%20as%20a%20specific%20instantiation.%20We%0Acompare%20the%20resulting%20algorithm%20against%20other%20algorithms%20from%20the%20literature%20on%0Atheoretical%20and%20real-world%20data%20showing%20its%20practical%20relevance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12452v1&entry.124074799=Read"},
{"title": "An Exact Finite-dimensional Explicit Feature Map for Kernel Functions", "author": "Kamaledin Ghiasi-Shirazi and Mohammadreza Qaraei", "abstract": "  Kernel methods in machine learning use a kernel function that takes two data\npoints as input and returns their inner product after mapping them to a Hilbert\nspace, implicitly and without actually computing the mapping. For many kernel\nfunctions, such as Gaussian and Laplacian kernels, the feature space is known\nto be infinite-dimensional, making operations in this space possible only\nimplicitly. This implicit nature necessitates algorithms to be expressed using\ndual representations and the kernel trick. In this paper, given an arbitrary\nkernel function, we introduce an explicit, finite-dimensional feature map for\nany arbitrary kernel function that ensures the inner product of data points in\nthe feature space equals the kernel function value, during both training and\ntesting. The existence of this explicit mapping allows for kernelized\nalgorithms to be formulated in their primal form, without the need for the\nkernel trick or the dual representation. As a first application, we demonstrate\nhow to derive kernelized machine learning algorithms directly, without\nresorting to the dual representation, and apply this method specifically to\nPCA. As another application, without any changes to the t-SNE algorithm and its\nimplementation, we use it for visualizing the feature space of kernel\nfunctions.\n", "link": "http://arxiv.org/abs/2410.12635v1", "date": "2024-10-16", "relevancy": 2.2364, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4642}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.44}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Exact%20Finite-dimensional%20Explicit%20Feature%20Map%20for%20Kernel%20Functions&body=Title%3A%20An%20Exact%20Finite-dimensional%20Explicit%20Feature%20Map%20for%20Kernel%20Functions%0AAuthor%3A%20Kamaledin%20Ghiasi-Shirazi%20and%20Mohammadreza%20Qaraei%0AAbstract%3A%20%20%20Kernel%20methods%20in%20machine%20learning%20use%20a%20kernel%20function%20that%20takes%20two%20data%0Apoints%20as%20input%20and%20returns%20their%20inner%20product%20after%20mapping%20them%20to%20a%20Hilbert%0Aspace%2C%20implicitly%20and%20without%20actually%20computing%20the%20mapping.%20For%20many%20kernel%0Afunctions%2C%20such%20as%20Gaussian%20and%20Laplacian%20kernels%2C%20the%20feature%20space%20is%20known%0Ato%20be%20infinite-dimensional%2C%20making%20operations%20in%20this%20space%20possible%20only%0Aimplicitly.%20This%20implicit%20nature%20necessitates%20algorithms%20to%20be%20expressed%20using%0Adual%20representations%20and%20the%20kernel%20trick.%20In%20this%20paper%2C%20given%20an%20arbitrary%0Akernel%20function%2C%20we%20introduce%20an%20explicit%2C%20finite-dimensional%20feature%20map%20for%0Aany%20arbitrary%20kernel%20function%20that%20ensures%20the%20inner%20product%20of%20data%20points%20in%0Athe%20feature%20space%20equals%20the%20kernel%20function%20value%2C%20during%20both%20training%20and%0Atesting.%20The%20existence%20of%20this%20explicit%20mapping%20allows%20for%20kernelized%0Aalgorithms%20to%20be%20formulated%20in%20their%20primal%20form%2C%20without%20the%20need%20for%20the%0Akernel%20trick%20or%20the%20dual%20representation.%20As%20a%20first%20application%2C%20we%20demonstrate%0Ahow%20to%20derive%20kernelized%20machine%20learning%20algorithms%20directly%2C%20without%0Aresorting%20to%20the%20dual%20representation%2C%20and%20apply%20this%20method%20specifically%20to%0APCA.%20As%20another%20application%2C%20without%20any%20changes%20to%20the%20t-SNE%20algorithm%20and%20its%0Aimplementation%2C%20we%20use%20it%20for%20visualizing%20the%20feature%20space%20of%20kernel%0Afunctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Exact%2520Finite-dimensional%2520Explicit%2520Feature%2520Map%2520for%2520Kernel%2520Functions%26entry.906535625%3DKamaledin%2520Ghiasi-Shirazi%2520and%2520Mohammadreza%2520Qaraei%26entry.1292438233%3D%2520%2520Kernel%2520methods%2520in%2520machine%2520learning%2520use%2520a%2520kernel%2520function%2520that%2520takes%2520two%2520data%250Apoints%2520as%2520input%2520and%2520returns%2520their%2520inner%2520product%2520after%2520mapping%2520them%2520to%2520a%2520Hilbert%250Aspace%252C%2520implicitly%2520and%2520without%2520actually%2520computing%2520the%2520mapping.%2520For%2520many%2520kernel%250Afunctions%252C%2520such%2520as%2520Gaussian%2520and%2520Laplacian%2520kernels%252C%2520the%2520feature%2520space%2520is%2520known%250Ato%2520be%2520infinite-dimensional%252C%2520making%2520operations%2520in%2520this%2520space%2520possible%2520only%250Aimplicitly.%2520This%2520implicit%2520nature%2520necessitates%2520algorithms%2520to%2520be%2520expressed%2520using%250Adual%2520representations%2520and%2520the%2520kernel%2520trick.%2520In%2520this%2520paper%252C%2520given%2520an%2520arbitrary%250Akernel%2520function%252C%2520we%2520introduce%2520an%2520explicit%252C%2520finite-dimensional%2520feature%2520map%2520for%250Aany%2520arbitrary%2520kernel%2520function%2520that%2520ensures%2520the%2520inner%2520product%2520of%2520data%2520points%2520in%250Athe%2520feature%2520space%2520equals%2520the%2520kernel%2520function%2520value%252C%2520during%2520both%2520training%2520and%250Atesting.%2520The%2520existence%2520of%2520this%2520explicit%2520mapping%2520allows%2520for%2520kernelized%250Aalgorithms%2520to%2520be%2520formulated%2520in%2520their%2520primal%2520form%252C%2520without%2520the%2520need%2520for%2520the%250Akernel%2520trick%2520or%2520the%2520dual%2520representation.%2520As%2520a%2520first%2520application%252C%2520we%2520demonstrate%250Ahow%2520to%2520derive%2520kernelized%2520machine%2520learning%2520algorithms%2520directly%252C%2520without%250Aresorting%2520to%2520the%2520dual%2520representation%252C%2520and%2520apply%2520this%2520method%2520specifically%2520to%250APCA.%2520As%2520another%2520application%252C%2520without%2520any%2520changes%2520to%2520the%2520t-SNE%2520algorithm%2520and%2520its%250Aimplementation%252C%2520we%2520use%2520it%2520for%2520visualizing%2520the%2520feature%2520space%2520of%2520kernel%250Afunctions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Exact%20Finite-dimensional%20Explicit%20Feature%20Map%20for%20Kernel%20Functions&entry.906535625=Kamaledin%20Ghiasi-Shirazi%20and%20Mohammadreza%20Qaraei&entry.1292438233=%20%20Kernel%20methods%20in%20machine%20learning%20use%20a%20kernel%20function%20that%20takes%20two%20data%0Apoints%20as%20input%20and%20returns%20their%20inner%20product%20after%20mapping%20them%20to%20a%20Hilbert%0Aspace%2C%20implicitly%20and%20without%20actually%20computing%20the%20mapping.%20For%20many%20kernel%0Afunctions%2C%20such%20as%20Gaussian%20and%20Laplacian%20kernels%2C%20the%20feature%20space%20is%20known%0Ato%20be%20infinite-dimensional%2C%20making%20operations%20in%20this%20space%20possible%20only%0Aimplicitly.%20This%20implicit%20nature%20necessitates%20algorithms%20to%20be%20expressed%20using%0Adual%20representations%20and%20the%20kernel%20trick.%20In%20this%20paper%2C%20given%20an%20arbitrary%0Akernel%20function%2C%20we%20introduce%20an%20explicit%2C%20finite-dimensional%20feature%20map%20for%0Aany%20arbitrary%20kernel%20function%20that%20ensures%20the%20inner%20product%20of%20data%20points%20in%0Athe%20feature%20space%20equals%20the%20kernel%20function%20value%2C%20during%20both%20training%20and%0Atesting.%20The%20existence%20of%20this%20explicit%20mapping%20allows%20for%20kernelized%0Aalgorithms%20to%20be%20formulated%20in%20their%20primal%20form%2C%20without%20the%20need%20for%20the%0Akernel%20trick%20or%20the%20dual%20representation.%20As%20a%20first%20application%2C%20we%20demonstrate%0Ahow%20to%20derive%20kernelized%20machine%20learning%20algorithms%20directly%2C%20without%0Aresorting%20to%20the%20dual%20representation%2C%20and%20apply%20this%20method%20specifically%20to%0APCA.%20As%20another%20application%2C%20without%20any%20changes%20to%20the%20t-SNE%20algorithm%20and%20its%0Aimplementation%2C%20we%20use%20it%20for%20visualizing%20the%20feature%20space%20of%20kernel%0Afunctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12635v1&entry.124074799=Read"},
{"title": "Open-Source Conversational AI with SpeechBrain 1.0", "author": "Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve", "abstract": "  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n", "link": "http://arxiv.org/abs/2407.00463v5", "date": "2024-10-16", "relevancy": 2.2271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Conversational%20AI%20with%20SpeechBrain%201.0&body=Title%3A%20Open-Source%20Conversational%20AI%20with%20SpeechBrain%201.0%0AAuthor%3A%20Mirco%20Ravanelli%20and%20Titouan%20Parcollet%20and%20Adel%20Moumen%20and%20Sylvain%20de%20Langen%20and%20Cem%20Subakan%20and%20Peter%20Plantinga%20and%20Yingzhi%20Wang%20and%20Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Artem%20Ploujnikov%20and%20Francesco%20Paissan%20and%20Davide%20Borra%20and%20Salah%20Zaiem%20and%20Zeyu%20Zhao%20and%20Shucong%20Zhang%20and%20Georgios%20Karakasidis%20and%20Sung-Lin%20Yeh%20and%20Pierre%20Champion%20and%20Aku%20Rouhe%20and%20Rudolf%20Braun%20and%20Florian%20Mai%20and%20Juan%20Zuluaga-Gomez%20and%20Seyed%20Mahed%20Mousavi%20and%20Andreas%20Nautsch%20and%20Xuechen%20Liu%20and%20Sangeet%20Sagar%20and%20Jarod%20Duret%20and%20Salima%20Mdhaffar%20and%20Gaelle%20Laperriere%20and%20Mickael%20Rouvier%20and%20Renato%20De%20Mori%20and%20Yannick%20Esteve%0AAbstract%3A%20%20%20SpeechBrain%20is%20an%20open-source%20Conversational%20AI%20toolkit%20based%20on%20PyTorch%2C%0Afocused%20particularly%20on%20speech%20processing%20tasks%20such%20as%20speech%20recognition%2C%0Aspeech%20enhancement%2C%20speaker%20recognition%2C%20text-to-speech%2C%20and%20much%20more.%20It%0Apromotes%20transparency%20and%20replicability%20by%20releasing%20both%20the%20pre-trained%0Amodels%20and%20the%20complete%20%22recipes%22%20of%20code%20and%20algorithms%20required%20for%20training%0Athem.%20This%20paper%20presents%20SpeechBrain%201.0%2C%20a%20significant%20milestone%20in%20the%0Aevolution%20of%20the%20toolkit%2C%20which%20now%20has%20over%20200%20recipes%20for%20speech%2C%20audio%2C%20and%0Alanguage%20processing%20tasks%2C%20and%20more%20than%20100%20models%20available%20on%20Hugging%20Face.%0ASpeechBrain%201.0%20introduces%20new%20technologies%20to%20support%20diverse%20learning%0Amodalities%2C%20Large%20Language%20Model%20%28LLM%29%20integration%2C%20and%20advanced%20decoding%0Astrategies%2C%20along%20with%20novel%20models%2C%20tasks%2C%20and%20modalities.%20It%20also%20includes%20a%0Anew%20benchmark%20repository%2C%20offering%20researchers%20a%20unified%20platform%20for%0Aevaluating%20models%20across%20diverse%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00463v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Source%2520Conversational%2520AI%2520with%2520SpeechBrain%25201.0%26entry.906535625%3DMirco%2520Ravanelli%2520and%2520Titouan%2520Parcollet%2520and%2520Adel%2520Moumen%2520and%2520Sylvain%2520de%2520Langen%2520and%2520Cem%2520Subakan%2520and%2520Peter%2520Plantinga%2520and%2520Yingzhi%2520Wang%2520and%2520Pooneh%2520Mousavi%2520and%2520Luca%2520Della%2520Libera%2520and%2520Artem%2520Ploujnikov%2520and%2520Francesco%2520Paissan%2520and%2520Davide%2520Borra%2520and%2520Salah%2520Zaiem%2520and%2520Zeyu%2520Zhao%2520and%2520Shucong%2520Zhang%2520and%2520Georgios%2520Karakasidis%2520and%2520Sung-Lin%2520Yeh%2520and%2520Pierre%2520Champion%2520and%2520Aku%2520Rouhe%2520and%2520Rudolf%2520Braun%2520and%2520Florian%2520Mai%2520and%2520Juan%2520Zuluaga-Gomez%2520and%2520Seyed%2520Mahed%2520Mousavi%2520and%2520Andreas%2520Nautsch%2520and%2520Xuechen%2520Liu%2520and%2520Sangeet%2520Sagar%2520and%2520Jarod%2520Duret%2520and%2520Salima%2520Mdhaffar%2520and%2520Gaelle%2520Laperriere%2520and%2520Mickael%2520Rouvier%2520and%2520Renato%2520De%2520Mori%2520and%2520Yannick%2520Esteve%26entry.1292438233%3D%2520%2520SpeechBrain%2520is%2520an%2520open-source%2520Conversational%2520AI%2520toolkit%2520based%2520on%2520PyTorch%252C%250Afocused%2520particularly%2520on%2520speech%2520processing%2520tasks%2520such%2520as%2520speech%2520recognition%252C%250Aspeech%2520enhancement%252C%2520speaker%2520recognition%252C%2520text-to-speech%252C%2520and%2520much%2520more.%2520It%250Apromotes%2520transparency%2520and%2520replicability%2520by%2520releasing%2520both%2520the%2520pre-trained%250Amodels%2520and%2520the%2520complete%2520%2522recipes%2522%2520of%2520code%2520and%2520algorithms%2520required%2520for%2520training%250Athem.%2520This%2520paper%2520presents%2520SpeechBrain%25201.0%252C%2520a%2520significant%2520milestone%2520in%2520the%250Aevolution%2520of%2520the%2520toolkit%252C%2520which%2520now%2520has%2520over%2520200%2520recipes%2520for%2520speech%252C%2520audio%252C%2520and%250Alanguage%2520processing%2520tasks%252C%2520and%2520more%2520than%2520100%2520models%2520available%2520on%2520Hugging%2520Face.%250ASpeechBrain%25201.0%2520introduces%2520new%2520technologies%2520to%2520support%2520diverse%2520learning%250Amodalities%252C%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520integration%252C%2520and%2520advanced%2520decoding%250Astrategies%252C%2520along%2520with%2520novel%2520models%252C%2520tasks%252C%2520and%2520modalities.%2520It%2520also%2520includes%2520a%250Anew%2520benchmark%2520repository%252C%2520offering%2520researchers%2520a%2520unified%2520platform%2520for%250Aevaluating%2520models%2520across%2520diverse%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00463v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Conversational%20AI%20with%20SpeechBrain%201.0&entry.906535625=Mirco%20Ravanelli%20and%20Titouan%20Parcollet%20and%20Adel%20Moumen%20and%20Sylvain%20de%20Langen%20and%20Cem%20Subakan%20and%20Peter%20Plantinga%20and%20Yingzhi%20Wang%20and%20Pooneh%20Mousavi%20and%20Luca%20Della%20Libera%20and%20Artem%20Ploujnikov%20and%20Francesco%20Paissan%20and%20Davide%20Borra%20and%20Salah%20Zaiem%20and%20Zeyu%20Zhao%20and%20Shucong%20Zhang%20and%20Georgios%20Karakasidis%20and%20Sung-Lin%20Yeh%20and%20Pierre%20Champion%20and%20Aku%20Rouhe%20and%20Rudolf%20Braun%20and%20Florian%20Mai%20and%20Juan%20Zuluaga-Gomez%20and%20Seyed%20Mahed%20Mousavi%20and%20Andreas%20Nautsch%20and%20Xuechen%20Liu%20and%20Sangeet%20Sagar%20and%20Jarod%20Duret%20and%20Salima%20Mdhaffar%20and%20Gaelle%20Laperriere%20and%20Mickael%20Rouvier%20and%20Renato%20De%20Mori%20and%20Yannick%20Esteve&entry.1292438233=%20%20SpeechBrain%20is%20an%20open-source%20Conversational%20AI%20toolkit%20based%20on%20PyTorch%2C%0Afocused%20particularly%20on%20speech%20processing%20tasks%20such%20as%20speech%20recognition%2C%0Aspeech%20enhancement%2C%20speaker%20recognition%2C%20text-to-speech%2C%20and%20much%20more.%20It%0Apromotes%20transparency%20and%20replicability%20by%20releasing%20both%20the%20pre-trained%0Amodels%20and%20the%20complete%20%22recipes%22%20of%20code%20and%20algorithms%20required%20for%20training%0Athem.%20This%20paper%20presents%20SpeechBrain%201.0%2C%20a%20significant%20milestone%20in%20the%0Aevolution%20of%20the%20toolkit%2C%20which%20now%20has%20over%20200%20recipes%20for%20speech%2C%20audio%2C%20and%0Alanguage%20processing%20tasks%2C%20and%20more%20than%20100%20models%20available%20on%20Hugging%20Face.%0ASpeechBrain%201.0%20introduces%20new%20technologies%20to%20support%20diverse%20learning%0Amodalities%2C%20Large%20Language%20Model%20%28LLM%29%20integration%2C%20and%20advanced%20decoding%0Astrategies%2C%20along%20with%20novel%20models%2C%20tasks%2C%20and%20modalities.%20It%20also%20includes%20a%0Anew%20benchmark%20repository%2C%20offering%20researchers%20a%20unified%20platform%20for%0Aevaluating%20models%20across%20diverse%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00463v5&entry.124074799=Read"},
{"title": "Energy and Carbon Considerations of Fine-Tuning BERT", "author": "Xiaorong Wang and Clara Na and Emma Strubell and Sorelle Friedler and Sasha Luccioni", "abstract": "  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n", "link": "http://arxiv.org/abs/2311.10267v2", "date": "2024-10-16", "relevancy": 2.2232, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20and%20Carbon%20Considerations%20of%20Fine-Tuning%20BERT&body=Title%3A%20Energy%20and%20Carbon%20Considerations%20of%20Fine-Tuning%20BERT%0AAuthor%3A%20Xiaorong%20Wang%20and%20Clara%20Na%20and%20Emma%20Strubell%20and%20Sorelle%20Friedler%20and%20Sasha%20Luccioni%0AAbstract%3A%20%20%20Despite%20the%20popularity%20of%20the%20%60pre-train%20then%20fine-tune%27%20paradigm%20in%20the%20NLP%0Acommunity%2C%20existing%20work%20quantifying%20energy%20costs%20and%20associated%20carbon%0Aemissions%20has%20largely%20focused%20on%20language%20model%20pre-training.%20Although%20a%20single%0Apre-training%20run%20draws%20substantially%20more%20energy%20than%20fine-tuning%2C%20fine-tuning%0Ais%20performed%20more%20frequently%20by%20many%20more%20individual%20actors%2C%20and%20thus%20must%20be%0Aaccounted%20for%20when%20considering%20the%20energy%20and%20carbon%20footprint%20of%20NLP.%20In%20order%0Ato%20better%20characterize%20the%20role%20of%20fine-tuning%20in%20the%20landscape%20of%20energy%20and%0Acarbon%20emissions%20in%20NLP%2C%20we%20perform%20a%20careful%20empirical%20study%20of%20the%0Acomputational%20costs%20of%20fine-tuning%20across%20tasks%2C%20datasets%2C%20hardware%0Ainfrastructure%20and%20measurement%20modalities.%20Our%20experimental%20results%20allow%20us%20to%0Aplace%20fine-tuning%20energy%20and%20carbon%20costs%20into%20perspective%20with%20respect%20to%0Apre-training%20and%20inference%2C%20and%20outline%20recommendations%20to%20NLP%20researchers%20and%0Apractitioners%20who%20wish%20to%20improve%20their%20fine-tuning%20energy%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520and%2520Carbon%2520Considerations%2520of%2520Fine-Tuning%2520BERT%26entry.906535625%3DXiaorong%2520Wang%2520and%2520Clara%2520Na%2520and%2520Emma%2520Strubell%2520and%2520Sorelle%2520Friedler%2520and%2520Sasha%2520Luccioni%26entry.1292438233%3D%2520%2520Despite%2520the%2520popularity%2520of%2520the%2520%2560pre-train%2520then%2520fine-tune%2527%2520paradigm%2520in%2520the%2520NLP%250Acommunity%252C%2520existing%2520work%2520quantifying%2520energy%2520costs%2520and%2520associated%2520carbon%250Aemissions%2520has%2520largely%2520focused%2520on%2520language%2520model%2520pre-training.%2520Although%2520a%2520single%250Apre-training%2520run%2520draws%2520substantially%2520more%2520energy%2520than%2520fine-tuning%252C%2520fine-tuning%250Ais%2520performed%2520more%2520frequently%2520by%2520many%2520more%2520individual%2520actors%252C%2520and%2520thus%2520must%2520be%250Aaccounted%2520for%2520when%2520considering%2520the%2520energy%2520and%2520carbon%2520footprint%2520of%2520NLP.%2520In%2520order%250Ato%2520better%2520characterize%2520the%2520role%2520of%2520fine-tuning%2520in%2520the%2520landscape%2520of%2520energy%2520and%250Acarbon%2520emissions%2520in%2520NLP%252C%2520we%2520perform%2520a%2520careful%2520empirical%2520study%2520of%2520the%250Acomputational%2520costs%2520of%2520fine-tuning%2520across%2520tasks%252C%2520datasets%252C%2520hardware%250Ainfrastructure%2520and%2520measurement%2520modalities.%2520Our%2520experimental%2520results%2520allow%2520us%2520to%250Aplace%2520fine-tuning%2520energy%2520and%2520carbon%2520costs%2520into%2520perspective%2520with%2520respect%2520to%250Apre-training%2520and%2520inference%252C%2520and%2520outline%2520recommendations%2520to%2520NLP%2520researchers%2520and%250Apractitioners%2520who%2520wish%2520to%2520improve%2520their%2520fine-tuning%2520energy%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20and%20Carbon%20Considerations%20of%20Fine-Tuning%20BERT&entry.906535625=Xiaorong%20Wang%20and%20Clara%20Na%20and%20Emma%20Strubell%20and%20Sorelle%20Friedler%20and%20Sasha%20Luccioni&entry.1292438233=%20%20Despite%20the%20popularity%20of%20the%20%60pre-train%20then%20fine-tune%27%20paradigm%20in%20the%20NLP%0Acommunity%2C%20existing%20work%20quantifying%20energy%20costs%20and%20associated%20carbon%0Aemissions%20has%20largely%20focused%20on%20language%20model%20pre-training.%20Although%20a%20single%0Apre-training%20run%20draws%20substantially%20more%20energy%20than%20fine-tuning%2C%20fine-tuning%0Ais%20performed%20more%20frequently%20by%20many%20more%20individual%20actors%2C%20and%20thus%20must%20be%0Aaccounted%20for%20when%20considering%20the%20energy%20and%20carbon%20footprint%20of%20NLP.%20In%20order%0Ato%20better%20characterize%20the%20role%20of%20fine-tuning%20in%20the%20landscape%20of%20energy%20and%0Acarbon%20emissions%20in%20NLP%2C%20we%20perform%20a%20careful%20empirical%20study%20of%20the%0Acomputational%20costs%20of%20fine-tuning%20across%20tasks%2C%20datasets%2C%20hardware%0Ainfrastructure%20and%20measurement%20modalities.%20Our%20experimental%20results%20allow%20us%20to%0Aplace%20fine-tuning%20energy%20and%20carbon%20costs%20into%20perspective%20with%20respect%20to%0Apre-training%20and%20inference%2C%20and%20outline%20recommendations%20to%20NLP%20researchers%20and%0Apractitioners%20who%20wish%20to%20improve%20their%20fine-tuning%20energy%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10267v2&entry.124074799=Read"},
{"title": "Mind the Gap Between Prototypes and Images in Cross-domain Finetuning", "author": "Hongduan Tian and Feng Liu and Zhanke Zhou and Tongliang Liu and Chengqi Zhang and Bo Han", "abstract": "  In cross-domain few-shot classification (CFC), recent works mainly focus on\nadapting a simple transformation head on top of a frozen pre-trained backbone\nwith few labeled data to project embeddings into a task-specific metric space\nwhere classification can be performed by measuring similarities between image\ninstance and prototype representations. Technically, an assumption implicitly\nadopted in such a framework is that the prototype and image instance embeddings\nshare the same representation transformation. However, in this paper, we find\nthat there naturally exists a gap, which resembles the modality gap, between\nthe prototype and image instance embeddings extracted from the frozen\npre-trained backbone, and simply applying the same transformation during the\nadaptation phase constrains exploring the optimal representations and shrinks\nthe gap between prototype and image representations. To solve this problem, we\npropose a simple yet effective method, contrastive prototype-image adaptation\n(CoPA), to adapt different transformations respectively for prototypes and\nimages similarly to CLIP by treating prototypes as text prompts. Extensive\nexperiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art\nperformance more efficiently. Meanwhile, further analyses also indicate that\nCoPA can learn better representation clusters, enlarge the gap, and achieve\nminimal validation loss at the enlarged gap.\n", "link": "http://arxiv.org/abs/2410.12474v1", "date": "2024-10-16", "relevancy": 2.1971, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5894}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5247}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%20Between%20Prototypes%20and%20Images%20in%20Cross-domain%20Finetuning&body=Title%3A%20Mind%20the%20Gap%20Between%20Prototypes%20and%20Images%20in%20Cross-domain%20Finetuning%0AAuthor%3A%20Hongduan%20Tian%20and%20Feng%20Liu%20and%20Zhanke%20Zhou%20and%20Tongliang%20Liu%20and%20Chengqi%20Zhang%20and%20Bo%20Han%0AAbstract%3A%20%20%20In%20cross-domain%20few-shot%20classification%20%28CFC%29%2C%20recent%20works%20mainly%20focus%20on%0Aadapting%20a%20simple%20transformation%20head%20on%20top%20of%20a%20frozen%20pre-trained%20backbone%0Awith%20few%20labeled%20data%20to%20project%20embeddings%20into%20a%20task-specific%20metric%20space%0Awhere%20classification%20can%20be%20performed%20by%20measuring%20similarities%20between%20image%0Ainstance%20and%20prototype%20representations.%20Technically%2C%20an%20assumption%20implicitly%0Aadopted%20in%20such%20a%20framework%20is%20that%20the%20prototype%20and%20image%20instance%20embeddings%0Ashare%20the%20same%20representation%20transformation.%20However%2C%20in%20this%20paper%2C%20we%20find%0Athat%20there%20naturally%20exists%20a%20gap%2C%20which%20resembles%20the%20modality%20gap%2C%20between%0Athe%20prototype%20and%20image%20instance%20embeddings%20extracted%20from%20the%20frozen%0Apre-trained%20backbone%2C%20and%20simply%20applying%20the%20same%20transformation%20during%20the%0Aadaptation%20phase%20constrains%20exploring%20the%20optimal%20representations%20and%20shrinks%0Athe%20gap%20between%20prototype%20and%20image%20representations.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20simple%20yet%20effective%20method%2C%20contrastive%20prototype-image%20adaptation%0A%28CoPA%29%2C%20to%20adapt%20different%20transformations%20respectively%20for%20prototypes%20and%0Aimages%20similarly%20to%20CLIP%20by%20treating%20prototypes%20as%20text%20prompts.%20Extensive%0Aexperiments%20on%20Meta-Dataset%20demonstrate%20that%20CoPA%20achieves%20the%20state-of-the-art%0Aperformance%20more%20efficiently.%20Meanwhile%2C%20further%20analyses%20also%20indicate%20that%0ACoPA%20can%20learn%20better%20representation%20clusters%2C%20enlarge%20the%20gap%2C%20and%20achieve%0Aminimal%20validation%20loss%20at%20the%20enlarged%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%2520Between%2520Prototypes%2520and%2520Images%2520in%2520Cross-domain%2520Finetuning%26entry.906535625%3DHongduan%2520Tian%2520and%2520Feng%2520Liu%2520and%2520Zhanke%2520Zhou%2520and%2520Tongliang%2520Liu%2520and%2520Chengqi%2520Zhang%2520and%2520Bo%2520Han%26entry.1292438233%3D%2520%2520In%2520cross-domain%2520few-shot%2520classification%2520%2528CFC%2529%252C%2520recent%2520works%2520mainly%2520focus%2520on%250Aadapting%2520a%2520simple%2520transformation%2520head%2520on%2520top%2520of%2520a%2520frozen%2520pre-trained%2520backbone%250Awith%2520few%2520labeled%2520data%2520to%2520project%2520embeddings%2520into%2520a%2520task-specific%2520metric%2520space%250Awhere%2520classification%2520can%2520be%2520performed%2520by%2520measuring%2520similarities%2520between%2520image%250Ainstance%2520and%2520prototype%2520representations.%2520Technically%252C%2520an%2520assumption%2520implicitly%250Aadopted%2520in%2520such%2520a%2520framework%2520is%2520that%2520the%2520prototype%2520and%2520image%2520instance%2520embeddings%250Ashare%2520the%2520same%2520representation%2520transformation.%2520However%252C%2520in%2520this%2520paper%252C%2520we%2520find%250Athat%2520there%2520naturally%2520exists%2520a%2520gap%252C%2520which%2520resembles%2520the%2520modality%2520gap%252C%2520between%250Athe%2520prototype%2520and%2520image%2520instance%2520embeddings%2520extracted%2520from%2520the%2520frozen%250Apre-trained%2520backbone%252C%2520and%2520simply%2520applying%2520the%2520same%2520transformation%2520during%2520the%250Aadaptation%2520phase%2520constrains%2520exploring%2520the%2520optimal%2520representations%2520and%2520shrinks%250Athe%2520gap%2520between%2520prototype%2520and%2520image%2520representations.%2520To%2520solve%2520this%2520problem%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520contrastive%2520prototype-image%2520adaptation%250A%2528CoPA%2529%252C%2520to%2520adapt%2520different%2520transformations%2520respectively%2520for%2520prototypes%2520and%250Aimages%2520similarly%2520to%2520CLIP%2520by%2520treating%2520prototypes%2520as%2520text%2520prompts.%2520Extensive%250Aexperiments%2520on%2520Meta-Dataset%2520demonstrate%2520that%2520CoPA%2520achieves%2520the%2520state-of-the-art%250Aperformance%2520more%2520efficiently.%2520Meanwhile%252C%2520further%2520analyses%2520also%2520indicate%2520that%250ACoPA%2520can%2520learn%2520better%2520representation%2520clusters%252C%2520enlarge%2520the%2520gap%252C%2520and%2520achieve%250Aminimal%2520validation%2520loss%2520at%2520the%2520enlarged%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%20Between%20Prototypes%20and%20Images%20in%20Cross-domain%20Finetuning&entry.906535625=Hongduan%20Tian%20and%20Feng%20Liu%20and%20Zhanke%20Zhou%20and%20Tongliang%20Liu%20and%20Chengqi%20Zhang%20and%20Bo%20Han&entry.1292438233=%20%20In%20cross-domain%20few-shot%20classification%20%28CFC%29%2C%20recent%20works%20mainly%20focus%20on%0Aadapting%20a%20simple%20transformation%20head%20on%20top%20of%20a%20frozen%20pre-trained%20backbone%0Awith%20few%20labeled%20data%20to%20project%20embeddings%20into%20a%20task-specific%20metric%20space%0Awhere%20classification%20can%20be%20performed%20by%20measuring%20similarities%20between%20image%0Ainstance%20and%20prototype%20representations.%20Technically%2C%20an%20assumption%20implicitly%0Aadopted%20in%20such%20a%20framework%20is%20that%20the%20prototype%20and%20image%20instance%20embeddings%0Ashare%20the%20same%20representation%20transformation.%20However%2C%20in%20this%20paper%2C%20we%20find%0Athat%20there%20naturally%20exists%20a%20gap%2C%20which%20resembles%20the%20modality%20gap%2C%20between%0Athe%20prototype%20and%20image%20instance%20embeddings%20extracted%20from%20the%20frozen%0Apre-trained%20backbone%2C%20and%20simply%20applying%20the%20same%20transformation%20during%20the%0Aadaptation%20phase%20constrains%20exploring%20the%20optimal%20representations%20and%20shrinks%0Athe%20gap%20between%20prototype%20and%20image%20representations.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20simple%20yet%20effective%20method%2C%20contrastive%20prototype-image%20adaptation%0A%28CoPA%29%2C%20to%20adapt%20different%20transformations%20respectively%20for%20prototypes%20and%0Aimages%20similarly%20to%20CLIP%20by%20treating%20prototypes%20as%20text%20prompts.%20Extensive%0Aexperiments%20on%20Meta-Dataset%20demonstrate%20that%20CoPA%20achieves%20the%20state-of-the-art%0Aperformance%20more%20efficiently.%20Meanwhile%2C%20further%20analyses%20also%20indicate%20that%0ACoPA%20can%20learn%20better%20representation%20clusters%2C%20enlarge%20the%20gap%2C%20and%20achieve%0Aminimal%20validation%20loss%20at%20the%20enlarged%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12474v1&entry.124074799=Read"},
{"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models", "author": "Shengkang Wang and Hongzhan Lin and Ziyang Luo and Zhen Ye and Guang Chen and Jing Ma", "abstract": "  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n", "link": "http://arxiv.org/abs/2406.11288v2", "date": "2024-10-16", "relevancy": 2.1831, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFC-Bench%3A%20Benchmarking%20Multimodal%20Fact-Checking%20with%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20MFC-Bench%3A%20Benchmarking%20Multimodal%20Fact-Checking%20with%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Shengkang%20Wang%20and%20Hongzhan%20Lin%20and%20Ziyang%20Luo%20and%20Zhen%20Ye%20and%20Guang%20Chen%20and%20Jing%20Ma%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20significantly%20improved%20multimodal%0Areasoning%20tasks%2C%20such%20as%20visual%20question%20answering%20and%20image%20captioning.%20These%0Amodels%20embed%20multimodal%20facts%20within%20their%20parameters%2C%20rather%20than%20relying%20on%0Aexternal%20knowledge%20bases%20to%20store%20factual%20information%20explicitly.%20However%2C%20the%0Acontent%20discerned%20by%20LVLMs%20may%20deviate%20from%20actual%20facts%20due%20to%20inherent%20bias%0Aor%20incorrect%20inference.%20To%20address%20this%20issue%2C%20we%20introduce%20MFC-Bench%2C%20a%0Arigorous%20and%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20factual%20accuracy%0Aof%20LVLMs%20across%20three%20stages%20of%20verdict%20prediction%20for%20MFC%3A%20Manipulation%2C%0AOut-of-Context%2C%20and%20Veracity%20Classification.%20Through%20our%20evaluation%20on%0AMFC-Bench%2C%20we%20benchmarked%20a%20dozen%20diverse%20and%20representative%20LVLMs%2C%20uncovering%0Athat%20current%20models%20still%20fall%20short%20in%20multimodal%20fact-checking%20and%0Ademonstrate%20insensitivity%20to%20various%20forms%20of%20manipulated%20content.%20We%20hope%20that%0AMFC-Bench%20could%20raise%20attention%20to%20the%20trustworthy%20AI%20potentially%20assisted%20by%0ALVLMs%20in%20the%20future.%20The%20MFC-Bench%20and%20accompanying%20resources%20are%20publicly%0Aaccessible%20at%20https%3A//github.com/wskbest/MFC-Bench%2C%20contributing%20to%20ongoing%0Aresearch%20in%20the%20multimodal%20fact-checking%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFC-Bench%253A%2520Benchmarking%2520Multimodal%2520Fact-Checking%2520with%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DShengkang%2520Wang%2520and%2520Hongzhan%2520Lin%2520and%2520Ziyang%2520Luo%2520and%2520Zhen%2520Ye%2520and%2520Guang%2520Chen%2520and%2520Jing%2520Ma%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520significantly%2520improved%2520multimodal%250Areasoning%2520tasks%252C%2520such%2520as%2520visual%2520question%2520answering%2520and%2520image%2520captioning.%2520These%250Amodels%2520embed%2520multimodal%2520facts%2520within%2520their%2520parameters%252C%2520rather%2520than%2520relying%2520on%250Aexternal%2520knowledge%2520bases%2520to%2520store%2520factual%2520information%2520explicitly.%2520However%252C%2520the%250Acontent%2520discerned%2520by%2520LVLMs%2520may%2520deviate%2520from%2520actual%2520facts%2520due%2520to%2520inherent%2520bias%250Aor%2520incorrect%2520inference.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MFC-Bench%252C%2520a%250Arigorous%2520and%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520factual%2520accuracy%250Aof%2520LVLMs%2520across%2520three%2520stages%2520of%2520verdict%2520prediction%2520for%2520MFC%253A%2520Manipulation%252C%250AOut-of-Context%252C%2520and%2520Veracity%2520Classification.%2520Through%2520our%2520evaluation%2520on%250AMFC-Bench%252C%2520we%2520benchmarked%2520a%2520dozen%2520diverse%2520and%2520representative%2520LVLMs%252C%2520uncovering%250Athat%2520current%2520models%2520still%2520fall%2520short%2520in%2520multimodal%2520fact-checking%2520and%250Ademonstrate%2520insensitivity%2520to%2520various%2520forms%2520of%2520manipulated%2520content.%2520We%2520hope%2520that%250AMFC-Bench%2520could%2520raise%2520attention%2520to%2520the%2520trustworthy%2520AI%2520potentially%2520assisted%2520by%250ALVLMs%2520in%2520the%2520future.%2520The%2520MFC-Bench%2520and%2520accompanying%2520resources%2520are%2520publicly%250Aaccessible%2520at%2520https%253A//github.com/wskbest/MFC-Bench%252C%2520contributing%2520to%2520ongoing%250Aresearch%2520in%2520the%2520multimodal%2520fact-checking%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFC-Bench%3A%20Benchmarking%20Multimodal%20Fact-Checking%20with%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Shengkang%20Wang%20and%20Hongzhan%20Lin%20and%20Ziyang%20Luo%20and%20Zhen%20Ye%20and%20Guang%20Chen%20and%20Jing%20Ma&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20significantly%20improved%20multimodal%0Areasoning%20tasks%2C%20such%20as%20visual%20question%20answering%20and%20image%20captioning.%20These%0Amodels%20embed%20multimodal%20facts%20within%20their%20parameters%2C%20rather%20than%20relying%20on%0Aexternal%20knowledge%20bases%20to%20store%20factual%20information%20explicitly.%20However%2C%20the%0Acontent%20discerned%20by%20LVLMs%20may%20deviate%20from%20actual%20facts%20due%20to%20inherent%20bias%0Aor%20incorrect%20inference.%20To%20address%20this%20issue%2C%20we%20introduce%20MFC-Bench%2C%20a%0Arigorous%20and%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%20factual%20accuracy%0Aof%20LVLMs%20across%20three%20stages%20of%20verdict%20prediction%20for%20MFC%3A%20Manipulation%2C%0AOut-of-Context%2C%20and%20Veracity%20Classification.%20Through%20our%20evaluation%20on%0AMFC-Bench%2C%20we%20benchmarked%20a%20dozen%20diverse%20and%20representative%20LVLMs%2C%20uncovering%0Athat%20current%20models%20still%20fall%20short%20in%20multimodal%20fact-checking%20and%0Ademonstrate%20insensitivity%20to%20various%20forms%20of%20manipulated%20content.%20We%20hope%20that%0AMFC-Bench%20could%20raise%20attention%20to%20the%20trustworthy%20AI%20potentially%20assisted%20by%0ALVLMs%20in%20the%20future.%20The%20MFC-Bench%20and%20accompanying%20resources%20are%20publicly%0Aaccessible%20at%20https%3A//github.com/wskbest/MFC-Bench%2C%20contributing%20to%20ongoing%0Aresearch%20in%20the%20multimodal%20fact-checking%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11288v2&entry.124074799=Read"},
{"title": "Non-Conservative Obstacle Avoidance for Multi-Body Systems Leveraging\n  Convex Hulls and Predicted Closest Points", "author": "Lotte Rassaerts and Eke Suichies and Bram van de Vrande and Marco Alonso and Bas Meere and Michelle Chong and Elena Torta", "abstract": "  This paper introduces a novel approach that integrates future closest point\npredictions into the distance constraints of a collision avoidance controller,\nleveraging convex hulls with closest point distance calculations. By addressing\nabrupt shifts in closest points, this method effectively reduces collision\nrisks and enhances controller performance. Applied to an Image Guided Therapy\nrobot and validated through simulations and user experiments, the framework\ndemonstrates improved distance prediction accuracy, smoother trajectories, and\nsafer navigation near obstacles.\n", "link": "http://arxiv.org/abs/2410.12659v1", "date": "2024-10-16", "relevancy": 2.1752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5904}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Conservative%20Obstacle%20Avoidance%20for%20Multi-Body%20Systems%20Leveraging%0A%20%20Convex%20Hulls%20and%20Predicted%20Closest%20Points&body=Title%3A%20Non-Conservative%20Obstacle%20Avoidance%20for%20Multi-Body%20Systems%20Leveraging%0A%20%20Convex%20Hulls%20and%20Predicted%20Closest%20Points%0AAuthor%3A%20Lotte%20Rassaerts%20and%20Eke%20Suichies%20and%20Bram%20van%20de%20Vrande%20and%20Marco%20Alonso%20and%20Bas%20Meere%20and%20Michelle%20Chong%20and%20Elena%20Torta%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20approach%20that%20integrates%20future%20closest%20point%0Apredictions%20into%20the%20distance%20constraints%20of%20a%20collision%20avoidance%20controller%2C%0Aleveraging%20convex%20hulls%20with%20closest%20point%20distance%20calculations.%20By%20addressing%0Aabrupt%20shifts%20in%20closest%20points%2C%20this%20method%20effectively%20reduces%20collision%0Arisks%20and%20enhances%20controller%20performance.%20Applied%20to%20an%20Image%20Guided%20Therapy%0Arobot%20and%20validated%20through%20simulations%20and%20user%20experiments%2C%20the%20framework%0Ademonstrates%20improved%20distance%20prediction%20accuracy%2C%20smoother%20trajectories%2C%20and%0Asafer%20navigation%20near%20obstacles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Conservative%2520Obstacle%2520Avoidance%2520for%2520Multi-Body%2520Systems%2520Leveraging%250A%2520%2520Convex%2520Hulls%2520and%2520Predicted%2520Closest%2520Points%26entry.906535625%3DLotte%2520Rassaerts%2520and%2520Eke%2520Suichies%2520and%2520Bram%2520van%2520de%2520Vrande%2520and%2520Marco%2520Alonso%2520and%2520Bas%2520Meere%2520and%2520Michelle%2520Chong%2520and%2520Elena%2520Torta%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520that%2520integrates%2520future%2520closest%2520point%250Apredictions%2520into%2520the%2520distance%2520constraints%2520of%2520a%2520collision%2520avoidance%2520controller%252C%250Aleveraging%2520convex%2520hulls%2520with%2520closest%2520point%2520distance%2520calculations.%2520By%2520addressing%250Aabrupt%2520shifts%2520in%2520closest%2520points%252C%2520this%2520method%2520effectively%2520reduces%2520collision%250Arisks%2520and%2520enhances%2520controller%2520performance.%2520Applied%2520to%2520an%2520Image%2520Guided%2520Therapy%250Arobot%2520and%2520validated%2520through%2520simulations%2520and%2520user%2520experiments%252C%2520the%2520framework%250Ademonstrates%2520improved%2520distance%2520prediction%2520accuracy%252C%2520smoother%2520trajectories%252C%2520and%250Asafer%2520navigation%2520near%2520obstacles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Conservative%20Obstacle%20Avoidance%20for%20Multi-Body%20Systems%20Leveraging%0A%20%20Convex%20Hulls%20and%20Predicted%20Closest%20Points&entry.906535625=Lotte%20Rassaerts%20and%20Eke%20Suichies%20and%20Bram%20van%20de%20Vrande%20and%20Marco%20Alonso%20and%20Bas%20Meere%20and%20Michelle%20Chong%20and%20Elena%20Torta&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20approach%20that%20integrates%20future%20closest%20point%0Apredictions%20into%20the%20distance%20constraints%20of%20a%20collision%20avoidance%20controller%2C%0Aleveraging%20convex%20hulls%20with%20closest%20point%20distance%20calculations.%20By%20addressing%0Aabrupt%20shifts%20in%20closest%20points%2C%20this%20method%20effectively%20reduces%20collision%0Arisks%20and%20enhances%20controller%20performance.%20Applied%20to%20an%20Image%20Guided%20Therapy%0Arobot%20and%20validated%20through%20simulations%20and%20user%20experiments%2C%20the%20framework%0Ademonstrates%20improved%20distance%20prediction%20accuracy%2C%20smoother%20trajectories%2C%20and%0Asafer%20navigation%20near%20obstacles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12659v1&entry.124074799=Read"},
{"title": "Optimizing Multi-Task Learning for Accurate Spacecraft Pose Estimation", "author": "Francesco Evangelisti and Francesco Rossi and Tobia Giani and Ilaria Bloise and Mattia Varile", "abstract": "  Accurate satellite pose estimation is crucial for autonomous guidance,\nnavigation, and control (GNC) systems in in-orbit servicing (IOS) missions.\nThis paper explores the impact of different tasks within a multi-task learning\n(MTL) framework for satellite pose estimation using monocular images. By\nintegrating tasks such as direct pose estimation, keypoint prediction, object\nlocalization, and segmentation into a single network, the study aims to\nevaluate the reciprocal influence between tasks by testing different multi-task\nconfigurations thanks to the modularity of the convolutional neural network\n(CNN) used in this work. The trends of mutual bias between the analyzed tasks\nare found by employing different weighting strategies to further test the\nrobustness of the findings. A synthetic dataset was developed to train and test\nthe MTL network. Results indicate that direct pose estimation and heatmap-based\npose estimation positively influence each other in general, while both the\nbounding box and segmentation tasks do not provide significant contributions\nand tend to degrade the overall estimation accuracy.\n", "link": "http://arxiv.org/abs/2410.12679v1", "date": "2024-10-16", "relevancy": 2.1555, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5328}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Multi-Task%20Learning%20for%20Accurate%20Spacecraft%20Pose%20Estimation&body=Title%3A%20Optimizing%20Multi-Task%20Learning%20for%20Accurate%20Spacecraft%20Pose%20Estimation%0AAuthor%3A%20Francesco%20Evangelisti%20and%20Francesco%20Rossi%20and%20Tobia%20Giani%20and%20Ilaria%20Bloise%20and%20Mattia%20Varile%0AAbstract%3A%20%20%20Accurate%20satellite%20pose%20estimation%20is%20crucial%20for%20autonomous%20guidance%2C%0Anavigation%2C%20and%20control%20%28GNC%29%20systems%20in%20in-orbit%20servicing%20%28IOS%29%20missions.%0AThis%20paper%20explores%20the%20impact%20of%20different%20tasks%20within%20a%20multi-task%20learning%0A%28MTL%29%20framework%20for%20satellite%20pose%20estimation%20using%20monocular%20images.%20By%0Aintegrating%20tasks%20such%20as%20direct%20pose%20estimation%2C%20keypoint%20prediction%2C%20object%0Alocalization%2C%20and%20segmentation%20into%20a%20single%20network%2C%20the%20study%20aims%20to%0Aevaluate%20the%20reciprocal%20influence%20between%20tasks%20by%20testing%20different%20multi-task%0Aconfigurations%20thanks%20to%20the%20modularity%20of%20the%20convolutional%20neural%20network%0A%28CNN%29%20used%20in%20this%20work.%20The%20trends%20of%20mutual%20bias%20between%20the%20analyzed%20tasks%0Aare%20found%20by%20employing%20different%20weighting%20strategies%20to%20further%20test%20the%0Arobustness%20of%20the%20findings.%20A%20synthetic%20dataset%20was%20developed%20to%20train%20and%20test%0Athe%20MTL%20network.%20Results%20indicate%20that%20direct%20pose%20estimation%20and%20heatmap-based%0Apose%20estimation%20positively%20influence%20each%20other%20in%20general%2C%20while%20both%20the%0Abounding%20box%20and%20segmentation%20tasks%20do%20not%20provide%20significant%20contributions%0Aand%20tend%20to%20degrade%20the%20overall%20estimation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Multi-Task%2520Learning%2520for%2520Accurate%2520Spacecraft%2520Pose%2520Estimation%26entry.906535625%3DFrancesco%2520Evangelisti%2520and%2520Francesco%2520Rossi%2520and%2520Tobia%2520Giani%2520and%2520Ilaria%2520Bloise%2520and%2520Mattia%2520Varile%26entry.1292438233%3D%2520%2520Accurate%2520satellite%2520pose%2520estimation%2520is%2520crucial%2520for%2520autonomous%2520guidance%252C%250Anavigation%252C%2520and%2520control%2520%2528GNC%2529%2520systems%2520in%2520in-orbit%2520servicing%2520%2528IOS%2529%2520missions.%250AThis%2520paper%2520explores%2520the%2520impact%2520of%2520different%2520tasks%2520within%2520a%2520multi-task%2520learning%250A%2528MTL%2529%2520framework%2520for%2520satellite%2520pose%2520estimation%2520using%2520monocular%2520images.%2520By%250Aintegrating%2520tasks%2520such%2520as%2520direct%2520pose%2520estimation%252C%2520keypoint%2520prediction%252C%2520object%250Alocalization%252C%2520and%2520segmentation%2520into%2520a%2520single%2520network%252C%2520the%2520study%2520aims%2520to%250Aevaluate%2520the%2520reciprocal%2520influence%2520between%2520tasks%2520by%2520testing%2520different%2520multi-task%250Aconfigurations%2520thanks%2520to%2520the%2520modularity%2520of%2520the%2520convolutional%2520neural%2520network%250A%2528CNN%2529%2520used%2520in%2520this%2520work.%2520The%2520trends%2520of%2520mutual%2520bias%2520between%2520the%2520analyzed%2520tasks%250Aare%2520found%2520by%2520employing%2520different%2520weighting%2520strategies%2520to%2520further%2520test%2520the%250Arobustness%2520of%2520the%2520findings.%2520A%2520synthetic%2520dataset%2520was%2520developed%2520to%2520train%2520and%2520test%250Athe%2520MTL%2520network.%2520Results%2520indicate%2520that%2520direct%2520pose%2520estimation%2520and%2520heatmap-based%250Apose%2520estimation%2520positively%2520influence%2520each%2520other%2520in%2520general%252C%2520while%2520both%2520the%250Abounding%2520box%2520and%2520segmentation%2520tasks%2520do%2520not%2520provide%2520significant%2520contributions%250Aand%2520tend%2520to%2520degrade%2520the%2520overall%2520estimation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Multi-Task%20Learning%20for%20Accurate%20Spacecraft%20Pose%20Estimation&entry.906535625=Francesco%20Evangelisti%20and%20Francesco%20Rossi%20and%20Tobia%20Giani%20and%20Ilaria%20Bloise%20and%20Mattia%20Varile&entry.1292438233=%20%20Accurate%20satellite%20pose%20estimation%20is%20crucial%20for%20autonomous%20guidance%2C%0Anavigation%2C%20and%20control%20%28GNC%29%20systems%20in%20in-orbit%20servicing%20%28IOS%29%20missions.%0AThis%20paper%20explores%20the%20impact%20of%20different%20tasks%20within%20a%20multi-task%20learning%0A%28MTL%29%20framework%20for%20satellite%20pose%20estimation%20using%20monocular%20images.%20By%0Aintegrating%20tasks%20such%20as%20direct%20pose%20estimation%2C%20keypoint%20prediction%2C%20object%0Alocalization%2C%20and%20segmentation%20into%20a%20single%20network%2C%20the%20study%20aims%20to%0Aevaluate%20the%20reciprocal%20influence%20between%20tasks%20by%20testing%20different%20multi-task%0Aconfigurations%20thanks%20to%20the%20modularity%20of%20the%20convolutional%20neural%20network%0A%28CNN%29%20used%20in%20this%20work.%20The%20trends%20of%20mutual%20bias%20between%20the%20analyzed%20tasks%0Aare%20found%20by%20employing%20different%20weighting%20strategies%20to%20further%20test%20the%0Arobustness%20of%20the%20findings.%20A%20synthetic%20dataset%20was%20developed%20to%20train%20and%20test%0Athe%20MTL%20network.%20Results%20indicate%20that%20direct%20pose%20estimation%20and%20heatmap-based%0Apose%20estimation%20positively%20influence%20each%20other%20in%20general%2C%20while%20both%20the%0Abounding%20box%20and%20segmentation%20tasks%20do%20not%20provide%20significant%20contributions%0Aand%20tend%20to%20degrade%20the%20overall%20estimation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12679v1&entry.124074799=Read"},
{"title": "MING: A Functional Approach to Learning Molecular Generative Models", "author": "Van Khoa Nguyen and Maciej Falkiewicz and Giangiacomo Mercatali and Alexandros Kalousis", "abstract": "  Traditional molecule generation methods often rely on sequence or graph-based\nrepresentations, which can limit their expressive power or require complex\npermutation-equivariant architectures. This paper introduces a novel paradigm\nfor learning molecule generative models based on functional representations.\nSpecifically, we propose Molecular Implicit Neural Generation (MING), a\ndiffusion-based model that learns molecular distributions in function space.\nUnlike standard diffusion processes in data space, MING employs a novel\nfunctional denoising probabilistic process, which jointly denoises the\ninformation in both the function's input and output spaces by leveraging an\nexpectation-maximization procedure for latent implicit neural representations\nof data. This approach allows for a simple yet effective model design that\naccurately captures underlying function distributions. Experimental results on\nmolecule-related datasets demonstrate MING's superior performance and ability\nto generate plausible molecular samples, surpassing state-of-the-art data-space\nmethods while offering a more streamlined architecture and significantly faster\ngeneration times.\n", "link": "http://arxiv.org/abs/2410.12522v1", "date": "2024-10-16", "relevancy": 2.1449, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5509}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5364}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MING%3A%20A%20Functional%20Approach%20to%20Learning%20Molecular%20Generative%20Models&body=Title%3A%20MING%3A%20A%20Functional%20Approach%20to%20Learning%20Molecular%20Generative%20Models%0AAuthor%3A%20Van%20Khoa%20Nguyen%20and%20Maciej%20Falkiewicz%20and%20Giangiacomo%20Mercatali%20and%20Alexandros%20Kalousis%0AAbstract%3A%20%20%20Traditional%20molecule%20generation%20methods%20often%20rely%20on%20sequence%20or%20graph-based%0Arepresentations%2C%20which%20can%20limit%20their%20expressive%20power%20or%20require%20complex%0Apermutation-equivariant%20architectures.%20This%20paper%20introduces%20a%20novel%20paradigm%0Afor%20learning%20molecule%20generative%20models%20based%20on%20functional%20representations.%0ASpecifically%2C%20we%20propose%20Molecular%20Implicit%20Neural%20Generation%20%28MING%29%2C%20a%0Adiffusion-based%20model%20that%20learns%20molecular%20distributions%20in%20function%20space.%0AUnlike%20standard%20diffusion%20processes%20in%20data%20space%2C%20MING%20employs%20a%20novel%0Afunctional%20denoising%20probabilistic%20process%2C%20which%20jointly%20denoises%20the%0Ainformation%20in%20both%20the%20function%27s%20input%20and%20output%20spaces%20by%20leveraging%20an%0Aexpectation-maximization%20procedure%20for%20latent%20implicit%20neural%20representations%0Aof%20data.%20This%20approach%20allows%20for%20a%20simple%20yet%20effective%20model%20design%20that%0Aaccurately%20captures%20underlying%20function%20distributions.%20Experimental%20results%20on%0Amolecule-related%20datasets%20demonstrate%20MING%27s%20superior%20performance%20and%20ability%0Ato%20generate%20plausible%20molecular%20samples%2C%20surpassing%20state-of-the-art%20data-space%0Amethods%20while%20offering%20a%20more%20streamlined%20architecture%20and%20significantly%20faster%0Ageneration%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMING%253A%2520A%2520Functional%2520Approach%2520to%2520Learning%2520Molecular%2520Generative%2520Models%26entry.906535625%3DVan%2520Khoa%2520Nguyen%2520and%2520Maciej%2520Falkiewicz%2520and%2520Giangiacomo%2520Mercatali%2520and%2520Alexandros%2520Kalousis%26entry.1292438233%3D%2520%2520Traditional%2520molecule%2520generation%2520methods%2520often%2520rely%2520on%2520sequence%2520or%2520graph-based%250Arepresentations%252C%2520which%2520can%2520limit%2520their%2520expressive%2520power%2520or%2520require%2520complex%250Apermutation-equivariant%2520architectures.%2520This%2520paper%2520introduces%2520a%2520novel%2520paradigm%250Afor%2520learning%2520molecule%2520generative%2520models%2520based%2520on%2520functional%2520representations.%250ASpecifically%252C%2520we%2520propose%2520Molecular%2520Implicit%2520Neural%2520Generation%2520%2528MING%2529%252C%2520a%250Adiffusion-based%2520model%2520that%2520learns%2520molecular%2520distributions%2520in%2520function%2520space.%250AUnlike%2520standard%2520diffusion%2520processes%2520in%2520data%2520space%252C%2520MING%2520employs%2520a%2520novel%250Afunctional%2520denoising%2520probabilistic%2520process%252C%2520which%2520jointly%2520denoises%2520the%250Ainformation%2520in%2520both%2520the%2520function%2527s%2520input%2520and%2520output%2520spaces%2520by%2520leveraging%2520an%250Aexpectation-maximization%2520procedure%2520for%2520latent%2520implicit%2520neural%2520representations%250Aof%2520data.%2520This%2520approach%2520allows%2520for%2520a%2520simple%2520yet%2520effective%2520model%2520design%2520that%250Aaccurately%2520captures%2520underlying%2520function%2520distributions.%2520Experimental%2520results%2520on%250Amolecule-related%2520datasets%2520demonstrate%2520MING%2527s%2520superior%2520performance%2520and%2520ability%250Ato%2520generate%2520plausible%2520molecular%2520samples%252C%2520surpassing%2520state-of-the-art%2520data-space%250Amethods%2520while%2520offering%2520a%2520more%2520streamlined%2520architecture%2520and%2520significantly%2520faster%250Ageneration%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MING%3A%20A%20Functional%20Approach%20to%20Learning%20Molecular%20Generative%20Models&entry.906535625=Van%20Khoa%20Nguyen%20and%20Maciej%20Falkiewicz%20and%20Giangiacomo%20Mercatali%20and%20Alexandros%20Kalousis&entry.1292438233=%20%20Traditional%20molecule%20generation%20methods%20often%20rely%20on%20sequence%20or%20graph-based%0Arepresentations%2C%20which%20can%20limit%20their%20expressive%20power%20or%20require%20complex%0Apermutation-equivariant%20architectures.%20This%20paper%20introduces%20a%20novel%20paradigm%0Afor%20learning%20molecule%20generative%20models%20based%20on%20functional%20representations.%0ASpecifically%2C%20we%20propose%20Molecular%20Implicit%20Neural%20Generation%20%28MING%29%2C%20a%0Adiffusion-based%20model%20that%20learns%20molecular%20distributions%20in%20function%20space.%0AUnlike%20standard%20diffusion%20processes%20in%20data%20space%2C%20MING%20employs%20a%20novel%0Afunctional%20denoising%20probabilistic%20process%2C%20which%20jointly%20denoises%20the%0Ainformation%20in%20both%20the%20function%27s%20input%20and%20output%20spaces%20by%20leveraging%20an%0Aexpectation-maximization%20procedure%20for%20latent%20implicit%20neural%20representations%0Aof%20data.%20This%20approach%20allows%20for%20a%20simple%20yet%20effective%20model%20design%20that%0Aaccurately%20captures%20underlying%20function%20distributions.%20Experimental%20results%20on%0Amolecule-related%20datasets%20demonstrate%20MING%27s%20superior%20performance%20and%20ability%0Ato%20generate%20plausible%20molecular%20samples%2C%20surpassing%20state-of-the-art%20data-space%0Amethods%20while%20offering%20a%20more%20streamlined%20architecture%20and%20significantly%20faster%0Ageneration%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12522v1&entry.124074799=Read"},
{"title": "Automatic Mapping of Anatomical Landmarks from Free-Text Using Large\n  Language Models: Insights from Llama-2", "author": "Mohamad Abdi and Gerardo Hemosillo Valadez and Halid Ziya Yerebakan", "abstract": "  Anatomical landmarks are vital in medical imaging for navigation and anomaly\ndetection. Modern large language models (LLMs), like Llama-2, offer promise for\nautomating the mapping of these landmarks in free-text radiology reports to\ncorresponding positions in image data. Recent studies propose LLMs may develop\ncoherent representations of generative processes. Motivated by these insights,\nwe investigated whether LLMs accurately represent the spatial positions of\nanatomical landmarks. Through experiments with Llama-2 models, we found that\nthey can linearly represent anatomical landmarks in space with considerable\nrobustness to different prompts. These results underscore the potential of LLMs\nto enhance the efficiency and accuracy of medical imaging workflows.\n", "link": "http://arxiv.org/abs/2410.12686v1", "date": "2024-10-16", "relevancy": 2.1405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Mapping%20of%20Anatomical%20Landmarks%20from%20Free-Text%20Using%20Large%0A%20%20Language%20Models%3A%20Insights%20from%20Llama-2&body=Title%3A%20Automatic%20Mapping%20of%20Anatomical%20Landmarks%20from%20Free-Text%20Using%20Large%0A%20%20Language%20Models%3A%20Insights%20from%20Llama-2%0AAuthor%3A%20Mohamad%20Abdi%20and%20Gerardo%20Hemosillo%20Valadez%20and%20Halid%20Ziya%20Yerebakan%0AAbstract%3A%20%20%20Anatomical%20landmarks%20are%20vital%20in%20medical%20imaging%20for%20navigation%20and%20anomaly%0Adetection.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20like%20Llama-2%2C%20offer%20promise%20for%0Aautomating%20the%20mapping%20of%20these%20landmarks%20in%20free-text%20radiology%20reports%20to%0Acorresponding%20positions%20in%20image%20data.%20Recent%20studies%20propose%20LLMs%20may%20develop%0Acoherent%20representations%20of%20generative%20processes.%20Motivated%20by%20these%20insights%2C%0Awe%20investigated%20whether%20LLMs%20accurately%20represent%20the%20spatial%20positions%20of%0Aanatomical%20landmarks.%20Through%20experiments%20with%20Llama-2%20models%2C%20we%20found%20that%0Athey%20can%20linearly%20represent%20anatomical%20landmarks%20in%20space%20with%20considerable%0Arobustness%20to%20different%20prompts.%20These%20results%20underscore%20the%20potential%20of%20LLMs%0Ato%20enhance%20the%20efficiency%20and%20accuracy%20of%20medical%20imaging%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Mapping%2520of%2520Anatomical%2520Landmarks%2520from%2520Free-Text%2520Using%2520Large%250A%2520%2520Language%2520Models%253A%2520Insights%2520from%2520Llama-2%26entry.906535625%3DMohamad%2520Abdi%2520and%2520Gerardo%2520Hemosillo%2520Valadez%2520and%2520Halid%2520Ziya%2520Yerebakan%26entry.1292438233%3D%2520%2520Anatomical%2520landmarks%2520are%2520vital%2520in%2520medical%2520imaging%2520for%2520navigation%2520and%2520anomaly%250Adetection.%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520like%2520Llama-2%252C%2520offer%2520promise%2520for%250Aautomating%2520the%2520mapping%2520of%2520these%2520landmarks%2520in%2520free-text%2520radiology%2520reports%2520to%250Acorresponding%2520positions%2520in%2520image%2520data.%2520Recent%2520studies%2520propose%2520LLMs%2520may%2520develop%250Acoherent%2520representations%2520of%2520generative%2520processes.%2520Motivated%2520by%2520these%2520insights%252C%250Awe%2520investigated%2520whether%2520LLMs%2520accurately%2520represent%2520the%2520spatial%2520positions%2520of%250Aanatomical%2520landmarks.%2520Through%2520experiments%2520with%2520Llama-2%2520models%252C%2520we%2520found%2520that%250Athey%2520can%2520linearly%2520represent%2520anatomical%2520landmarks%2520in%2520space%2520with%2520considerable%250Arobustness%2520to%2520different%2520prompts.%2520These%2520results%2520underscore%2520the%2520potential%2520of%2520LLMs%250Ato%2520enhance%2520the%2520efficiency%2520and%2520accuracy%2520of%2520medical%2520imaging%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Mapping%20of%20Anatomical%20Landmarks%20from%20Free-Text%20Using%20Large%0A%20%20Language%20Models%3A%20Insights%20from%20Llama-2&entry.906535625=Mohamad%20Abdi%20and%20Gerardo%20Hemosillo%20Valadez%20and%20Halid%20Ziya%20Yerebakan&entry.1292438233=%20%20Anatomical%20landmarks%20are%20vital%20in%20medical%20imaging%20for%20navigation%20and%20anomaly%0Adetection.%20Modern%20large%20language%20models%20%28LLMs%29%2C%20like%20Llama-2%2C%20offer%20promise%20for%0Aautomating%20the%20mapping%20of%20these%20landmarks%20in%20free-text%20radiology%20reports%20to%0Acorresponding%20positions%20in%20image%20data.%20Recent%20studies%20propose%20LLMs%20may%20develop%0Acoherent%20representations%20of%20generative%20processes.%20Motivated%20by%20these%20insights%2C%0Awe%20investigated%20whether%20LLMs%20accurately%20represent%20the%20spatial%20positions%20of%0Aanatomical%20landmarks.%20Through%20experiments%20with%20Llama-2%20models%2C%20we%20found%20that%0Athey%20can%20linearly%20represent%20anatomical%20landmarks%20in%20space%20with%20considerable%0Arobustness%20to%20different%20prompts.%20These%20results%20underscore%20the%20potential%20of%20LLMs%0Ato%20enhance%20the%20efficiency%20and%20accuracy%20of%20medical%20imaging%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12686v1&entry.124074799=Read"},
{"title": "Towards Graph Foundation Models: The Perspective of Zero-shot Reasoning\n  on Knowledge Graphs", "author": "Kai Wang and Siqiang Luo", "abstract": "  Inspired by the success of artificial general intelligence, there is a trend\ntowards developing Graph Foundation Models that excel in generalization across\nvarious graph tasks and domains. However, current models often require\nextensive training or fine-tuning to capture structural and semantic insights\non new graphs, which limits their versatility. In this work, we explore graph\nfoundation models from the perspective of zero-shot reasoning on Knowledge\nGraphs (KGs). Our focus is on utilizing KGs as a unified topological structure\nto tackle diverse tasks, while addressing semantic isolation challenges in KG\nreasoning to effectively integrate diverse semantic and structural features.\nThis brings us new methodological insights into KG reasoning, as well as high\ngeneralizability towards foundation models in practice. Methodologically, we\nintroduce SCORE, a unified graph reasoning framework that effectively\ngeneralizes diverse graph tasks using zero-shot learning. At the core of SCORE\nis semantic conditional message passing, a technique designed to capture both\nstructural and semantic invariances in graphs, with theoretical backing for its\nexpressive power. Practically, we evaluate the zero-shot reasoning capability\nof SCORE using 38 diverse graph datasets, covering node-level, link-level, and\ngraph-level tasks across multiple domains. Our experiments reveal a substantial\nperformance improvement over prior foundation models and supervised baselines,\nhighlighting the efficacy and adaptability of our approach.\n", "link": "http://arxiv.org/abs/2410.12609v1", "date": "2024-10-16", "relevancy": 2.1367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Graph%20Foundation%20Models%3A%20The%20Perspective%20of%20Zero-shot%20Reasoning%0A%20%20on%20Knowledge%20Graphs&body=Title%3A%20Towards%20Graph%20Foundation%20Models%3A%20The%20Perspective%20of%20Zero-shot%20Reasoning%0A%20%20on%20Knowledge%20Graphs%0AAuthor%3A%20Kai%20Wang%20and%20Siqiang%20Luo%0AAbstract%3A%20%20%20Inspired%20by%20the%20success%20of%20artificial%20general%20intelligence%2C%20there%20is%20a%20trend%0Atowards%20developing%20Graph%20Foundation%20Models%20that%20excel%20in%20generalization%20across%0Avarious%20graph%20tasks%20and%20domains.%20However%2C%20current%20models%20often%20require%0Aextensive%20training%20or%20fine-tuning%20to%20capture%20structural%20and%20semantic%20insights%0Aon%20new%20graphs%2C%20which%20limits%20their%20versatility.%20In%20this%20work%2C%20we%20explore%20graph%0Afoundation%20models%20from%20the%20perspective%20of%20zero-shot%20reasoning%20on%20Knowledge%0AGraphs%20%28KGs%29.%20Our%20focus%20is%20on%20utilizing%20KGs%20as%20a%20unified%20topological%20structure%0Ato%20tackle%20diverse%20tasks%2C%20while%20addressing%20semantic%20isolation%20challenges%20in%20KG%0Areasoning%20to%20effectively%20integrate%20diverse%20semantic%20and%20structural%20features.%0AThis%20brings%20us%20new%20methodological%20insights%20into%20KG%20reasoning%2C%20as%20well%20as%20high%0Ageneralizability%20towards%20foundation%20models%20in%20practice.%20Methodologically%2C%20we%0Aintroduce%20SCORE%2C%20a%20unified%20graph%20reasoning%20framework%20that%20effectively%0Ageneralizes%20diverse%20graph%20tasks%20using%20zero-shot%20learning.%20At%20the%20core%20of%20SCORE%0Ais%20semantic%20conditional%20message%20passing%2C%20a%20technique%20designed%20to%20capture%20both%0Astructural%20and%20semantic%20invariances%20in%20graphs%2C%20with%20theoretical%20backing%20for%20its%0Aexpressive%20power.%20Practically%2C%20we%20evaluate%20the%20zero-shot%20reasoning%20capability%0Aof%20SCORE%20using%2038%20diverse%20graph%20datasets%2C%20covering%20node-level%2C%20link-level%2C%20and%0Agraph-level%20tasks%20across%20multiple%20domains.%20Our%20experiments%20reveal%20a%20substantial%0Aperformance%20improvement%20over%20prior%20foundation%20models%20and%20supervised%20baselines%2C%0Ahighlighting%20the%20efficacy%20and%20adaptability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Graph%2520Foundation%2520Models%253A%2520The%2520Perspective%2520of%2520Zero-shot%2520Reasoning%250A%2520%2520on%2520Knowledge%2520Graphs%26entry.906535625%3DKai%2520Wang%2520and%2520Siqiang%2520Luo%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520success%2520of%2520artificial%2520general%2520intelligence%252C%2520there%2520is%2520a%2520trend%250Atowards%2520developing%2520Graph%2520Foundation%2520Models%2520that%2520excel%2520in%2520generalization%2520across%250Avarious%2520graph%2520tasks%2520and%2520domains.%2520However%252C%2520current%2520models%2520often%2520require%250Aextensive%2520training%2520or%2520fine-tuning%2520to%2520capture%2520structural%2520and%2520semantic%2520insights%250Aon%2520new%2520graphs%252C%2520which%2520limits%2520their%2520versatility.%2520In%2520this%2520work%252C%2520we%2520explore%2520graph%250Afoundation%2520models%2520from%2520the%2520perspective%2520of%2520zero-shot%2520reasoning%2520on%2520Knowledge%250AGraphs%2520%2528KGs%2529.%2520Our%2520focus%2520is%2520on%2520utilizing%2520KGs%2520as%2520a%2520unified%2520topological%2520structure%250Ato%2520tackle%2520diverse%2520tasks%252C%2520while%2520addressing%2520semantic%2520isolation%2520challenges%2520in%2520KG%250Areasoning%2520to%2520effectively%2520integrate%2520diverse%2520semantic%2520and%2520structural%2520features.%250AThis%2520brings%2520us%2520new%2520methodological%2520insights%2520into%2520KG%2520reasoning%252C%2520as%2520well%2520as%2520high%250Ageneralizability%2520towards%2520foundation%2520models%2520in%2520practice.%2520Methodologically%252C%2520we%250Aintroduce%2520SCORE%252C%2520a%2520unified%2520graph%2520reasoning%2520framework%2520that%2520effectively%250Ageneralizes%2520diverse%2520graph%2520tasks%2520using%2520zero-shot%2520learning.%2520At%2520the%2520core%2520of%2520SCORE%250Ais%2520semantic%2520conditional%2520message%2520passing%252C%2520a%2520technique%2520designed%2520to%2520capture%2520both%250Astructural%2520and%2520semantic%2520invariances%2520in%2520graphs%252C%2520with%2520theoretical%2520backing%2520for%2520its%250Aexpressive%2520power.%2520Practically%252C%2520we%2520evaluate%2520the%2520zero-shot%2520reasoning%2520capability%250Aof%2520SCORE%2520using%252038%2520diverse%2520graph%2520datasets%252C%2520covering%2520node-level%252C%2520link-level%252C%2520and%250Agraph-level%2520tasks%2520across%2520multiple%2520domains.%2520Our%2520experiments%2520reveal%2520a%2520substantial%250Aperformance%2520improvement%2520over%2520prior%2520foundation%2520models%2520and%2520supervised%2520baselines%252C%250Ahighlighting%2520the%2520efficacy%2520and%2520adaptability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Graph%20Foundation%20Models%3A%20The%20Perspective%20of%20Zero-shot%20Reasoning%0A%20%20on%20Knowledge%20Graphs&entry.906535625=Kai%20Wang%20and%20Siqiang%20Luo&entry.1292438233=%20%20Inspired%20by%20the%20success%20of%20artificial%20general%20intelligence%2C%20there%20is%20a%20trend%0Atowards%20developing%20Graph%20Foundation%20Models%20that%20excel%20in%20generalization%20across%0Avarious%20graph%20tasks%20and%20domains.%20However%2C%20current%20models%20often%20require%0Aextensive%20training%20or%20fine-tuning%20to%20capture%20structural%20and%20semantic%20insights%0Aon%20new%20graphs%2C%20which%20limits%20their%20versatility.%20In%20this%20work%2C%20we%20explore%20graph%0Afoundation%20models%20from%20the%20perspective%20of%20zero-shot%20reasoning%20on%20Knowledge%0AGraphs%20%28KGs%29.%20Our%20focus%20is%20on%20utilizing%20KGs%20as%20a%20unified%20topological%20structure%0Ato%20tackle%20diverse%20tasks%2C%20while%20addressing%20semantic%20isolation%20challenges%20in%20KG%0Areasoning%20to%20effectively%20integrate%20diverse%20semantic%20and%20structural%20features.%0AThis%20brings%20us%20new%20methodological%20insights%20into%20KG%20reasoning%2C%20as%20well%20as%20high%0Ageneralizability%20towards%20foundation%20models%20in%20practice.%20Methodologically%2C%20we%0Aintroduce%20SCORE%2C%20a%20unified%20graph%20reasoning%20framework%20that%20effectively%0Ageneralizes%20diverse%20graph%20tasks%20using%20zero-shot%20learning.%20At%20the%20core%20of%20SCORE%0Ais%20semantic%20conditional%20message%20passing%2C%20a%20technique%20designed%20to%20capture%20both%0Astructural%20and%20semantic%20invariances%20in%20graphs%2C%20with%20theoretical%20backing%20for%20its%0Aexpressive%20power.%20Practically%2C%20we%20evaluate%20the%20zero-shot%20reasoning%20capability%0Aof%20SCORE%20using%2038%20diverse%20graph%20datasets%2C%20covering%20node-level%2C%20link-level%2C%20and%0Agraph-level%20tasks%20across%20multiple%20domains.%20Our%20experiments%20reveal%20a%20substantial%0Aperformance%20improvement%20over%20prior%20foundation%20models%20and%20supervised%20baselines%2C%0Ahighlighting%20the%20efficacy%20and%20adaptability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12609v1&entry.124074799=Read"},
{"title": "Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans", "author": "Luca Marsilio and Davide Marzorati and Matteo Rossi and Andrea Moglia and Luca Mainardi and Alfonso Manzotti and Pietro Cerveri", "abstract": "  Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions.\n", "link": "http://arxiv.org/abs/2410.12641v1", "date": "2024-10-16", "relevancy": 2.1366, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5358}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascade%20learning%20in%20multi-task%20encoder-decoder%20networks%20for%20concurrent%0A%20%20bone%20segmentation%20and%20glenohumeral%20joint%20assessment%20in%20shoulder%20CT%20scans&body=Title%3A%20Cascade%20learning%20in%20multi-task%20encoder-decoder%20networks%20for%20concurrent%0A%20%20bone%20segmentation%20and%20glenohumeral%20joint%20assessment%20in%20shoulder%20CT%20scans%0AAuthor%3A%20Luca%20Marsilio%20and%20Davide%20Marzorati%20and%20Matteo%20Rossi%20and%20Andrea%20Moglia%20and%20Luca%20Mainardi%20and%20Alfonso%20Manzotti%20and%20Pietro%20Cerveri%0AAbstract%3A%20%20%20Osteoarthritis%20is%20a%20degenerative%20condition%20affecting%20bones%20and%20cartilage%2C%0Aoften%20leading%20to%20osteophyte%20formation%2C%20bone%20density%20loss%2C%20and%20joint%20space%0Anarrowing.%20Treatment%20options%20to%20restore%20normal%20joint%20function%20vary%20depending%20on%0Athe%20severity%20of%20the%20condition.%20This%20work%20introduces%20an%20innovative%20deep-learning%0Aframework%20processing%20shoulder%20CT%20scans.%20It%20features%20the%20semantic%20segmentation%0Aof%20the%20proximal%20humerus%20and%20scapula%2C%20the%203D%20reconstruction%20of%20bone%20surfaces%2C%0Athe%20identification%20of%20the%20glenohumeral%20%28GH%29%20joint%20region%2C%20and%20the%20staging%20of%0Athree%20common%20osteoarthritic-related%20pathologies%3A%20osteophyte%20formation%20%28OS%29%2C%20GH%0Aspace%20reduction%20%28JS%29%2C%20and%20humeroscapular%20alignment%20%28HSA%29.%20The%20pipeline%0Acomprises%20two%20cascaded%20CNN%20architectures%3A%203D%20CEL-UNet%20for%20segmentation%20and%203D%0AArthro-Net%20for%20threefold%20classification.%20A%20retrospective%20dataset%20of%20571%20CT%0Ascans%20featuring%20patients%20with%20various%20degrees%20of%20GH%20osteoarthritic-related%0Apathologies%20was%20used%20to%20train%2C%20validate%2C%20and%20test%20the%20pipeline.%20Root%20mean%0Asquared%20error%20and%20Hausdorff%20distance%20median%20values%20for%203D%20reconstruction%20were%0A0.22mm%20and%201.48mm%20for%20the%20humerus%20and%200.24mm%20and%201.48mm%20for%20the%20scapula%2C%0Aoutperforming%20state-of-the-art%20architectures%20and%20making%20it%20potentially%20suitable%0Afor%20a%20PSI-based%20shoulder%20arthroplasty%20preoperative%20plan%20context.%20The%0Aclassification%20accuracy%20for%20OS%2C%20JS%2C%20and%20HSA%20consistently%20reached%20around%2090%25%0Aacross%20all%20three%20categories.%20The%20computational%20time%20for%20the%20inference%20pipeline%0Awas%20less%20than%2015s%2C%20showcasing%20the%20framework%27s%20efficiency%20and%20compatibility%20with%0Aorthopedic%20radiology%20practice.%20The%20outcomes%20represent%20a%20promising%20advancement%0Atoward%20the%20medical%20translation%20of%20artificial%20intelligence%20tools.%20This%20progress%0Aaims%20to%20streamline%20the%20preoperative%20planning%20pipeline%20delivering%20high-quality%0Abone%20surfaces%20and%20supporting%20surgeons%20in%20selecting%20the%20most%20suitable%20surgical%0Aapproach%20according%20to%20the%20unique%20patient%20joint%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascade%2520learning%2520in%2520multi-task%2520encoder-decoder%2520networks%2520for%2520concurrent%250A%2520%2520bone%2520segmentation%2520and%2520glenohumeral%2520joint%2520assessment%2520in%2520shoulder%2520CT%2520scans%26entry.906535625%3DLuca%2520Marsilio%2520and%2520Davide%2520Marzorati%2520and%2520Matteo%2520Rossi%2520and%2520Andrea%2520Moglia%2520and%2520Luca%2520Mainardi%2520and%2520Alfonso%2520Manzotti%2520and%2520Pietro%2520Cerveri%26entry.1292438233%3D%2520%2520Osteoarthritis%2520is%2520a%2520degenerative%2520condition%2520affecting%2520bones%2520and%2520cartilage%252C%250Aoften%2520leading%2520to%2520osteophyte%2520formation%252C%2520bone%2520density%2520loss%252C%2520and%2520joint%2520space%250Anarrowing.%2520Treatment%2520options%2520to%2520restore%2520normal%2520joint%2520function%2520vary%2520depending%2520on%250Athe%2520severity%2520of%2520the%2520condition.%2520This%2520work%2520introduces%2520an%2520innovative%2520deep-learning%250Aframework%2520processing%2520shoulder%2520CT%2520scans.%2520It%2520features%2520the%2520semantic%2520segmentation%250Aof%2520the%2520proximal%2520humerus%2520and%2520scapula%252C%2520the%25203D%2520reconstruction%2520of%2520bone%2520surfaces%252C%250Athe%2520identification%2520of%2520the%2520glenohumeral%2520%2528GH%2529%2520joint%2520region%252C%2520and%2520the%2520staging%2520of%250Athree%2520common%2520osteoarthritic-related%2520pathologies%253A%2520osteophyte%2520formation%2520%2528OS%2529%252C%2520GH%250Aspace%2520reduction%2520%2528JS%2529%252C%2520and%2520humeroscapular%2520alignment%2520%2528HSA%2529.%2520The%2520pipeline%250Acomprises%2520two%2520cascaded%2520CNN%2520architectures%253A%25203D%2520CEL-UNet%2520for%2520segmentation%2520and%25203D%250AArthro-Net%2520for%2520threefold%2520classification.%2520A%2520retrospective%2520dataset%2520of%2520571%2520CT%250Ascans%2520featuring%2520patients%2520with%2520various%2520degrees%2520of%2520GH%2520osteoarthritic-related%250Apathologies%2520was%2520used%2520to%2520train%252C%2520validate%252C%2520and%2520test%2520the%2520pipeline.%2520Root%2520mean%250Asquared%2520error%2520and%2520Hausdorff%2520distance%2520median%2520values%2520for%25203D%2520reconstruction%2520were%250A0.22mm%2520and%25201.48mm%2520for%2520the%2520humerus%2520and%25200.24mm%2520and%25201.48mm%2520for%2520the%2520scapula%252C%250Aoutperforming%2520state-of-the-art%2520architectures%2520and%2520making%2520it%2520potentially%2520suitable%250Afor%2520a%2520PSI-based%2520shoulder%2520arthroplasty%2520preoperative%2520plan%2520context.%2520The%250Aclassification%2520accuracy%2520for%2520OS%252C%2520JS%252C%2520and%2520HSA%2520consistently%2520reached%2520around%252090%2525%250Aacross%2520all%2520three%2520categories.%2520The%2520computational%2520time%2520for%2520the%2520inference%2520pipeline%250Awas%2520less%2520than%252015s%252C%2520showcasing%2520the%2520framework%2527s%2520efficiency%2520and%2520compatibility%2520with%250Aorthopedic%2520radiology%2520practice.%2520The%2520outcomes%2520represent%2520a%2520promising%2520advancement%250Atoward%2520the%2520medical%2520translation%2520of%2520artificial%2520intelligence%2520tools.%2520This%2520progress%250Aaims%2520to%2520streamline%2520the%2520preoperative%2520planning%2520pipeline%2520delivering%2520high-quality%250Abone%2520surfaces%2520and%2520supporting%2520surgeons%2520in%2520selecting%2520the%2520most%2520suitable%2520surgical%250Aapproach%2520according%2520to%2520the%2520unique%2520patient%2520joint%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascade%20learning%20in%20multi-task%20encoder-decoder%20networks%20for%20concurrent%0A%20%20bone%20segmentation%20and%20glenohumeral%20joint%20assessment%20in%20shoulder%20CT%20scans&entry.906535625=Luca%20Marsilio%20and%20Davide%20Marzorati%20and%20Matteo%20Rossi%20and%20Andrea%20Moglia%20and%20Luca%20Mainardi%20and%20Alfonso%20Manzotti%20and%20Pietro%20Cerveri&entry.1292438233=%20%20Osteoarthritis%20is%20a%20degenerative%20condition%20affecting%20bones%20and%20cartilage%2C%0Aoften%20leading%20to%20osteophyte%20formation%2C%20bone%20density%20loss%2C%20and%20joint%20space%0Anarrowing.%20Treatment%20options%20to%20restore%20normal%20joint%20function%20vary%20depending%20on%0Athe%20severity%20of%20the%20condition.%20This%20work%20introduces%20an%20innovative%20deep-learning%0Aframework%20processing%20shoulder%20CT%20scans.%20It%20features%20the%20semantic%20segmentation%0Aof%20the%20proximal%20humerus%20and%20scapula%2C%20the%203D%20reconstruction%20of%20bone%20surfaces%2C%0Athe%20identification%20of%20the%20glenohumeral%20%28GH%29%20joint%20region%2C%20and%20the%20staging%20of%0Athree%20common%20osteoarthritic-related%20pathologies%3A%20osteophyte%20formation%20%28OS%29%2C%20GH%0Aspace%20reduction%20%28JS%29%2C%20and%20humeroscapular%20alignment%20%28HSA%29.%20The%20pipeline%0Acomprises%20two%20cascaded%20CNN%20architectures%3A%203D%20CEL-UNet%20for%20segmentation%20and%203D%0AArthro-Net%20for%20threefold%20classification.%20A%20retrospective%20dataset%20of%20571%20CT%0Ascans%20featuring%20patients%20with%20various%20degrees%20of%20GH%20osteoarthritic-related%0Apathologies%20was%20used%20to%20train%2C%20validate%2C%20and%20test%20the%20pipeline.%20Root%20mean%0Asquared%20error%20and%20Hausdorff%20distance%20median%20values%20for%203D%20reconstruction%20were%0A0.22mm%20and%201.48mm%20for%20the%20humerus%20and%200.24mm%20and%201.48mm%20for%20the%20scapula%2C%0Aoutperforming%20state-of-the-art%20architectures%20and%20making%20it%20potentially%20suitable%0Afor%20a%20PSI-based%20shoulder%20arthroplasty%20preoperative%20plan%20context.%20The%0Aclassification%20accuracy%20for%20OS%2C%20JS%2C%20and%20HSA%20consistently%20reached%20around%2090%25%0Aacross%20all%20three%20categories.%20The%20computational%20time%20for%20the%20inference%20pipeline%0Awas%20less%20than%2015s%2C%20showcasing%20the%20framework%27s%20efficiency%20and%20compatibility%20with%0Aorthopedic%20radiology%20practice.%20The%20outcomes%20represent%20a%20promising%20advancement%0Atoward%20the%20medical%20translation%20of%20artificial%20intelligence%20tools.%20This%20progress%0Aaims%20to%20streamline%20the%20preoperative%20planning%20pipeline%20delivering%20high-quality%0Abone%20surfaces%20and%20supporting%20surgeons%20in%20selecting%20the%20most%20suitable%20surgical%0Aapproach%20according%20to%20the%20unique%20patient%20joint%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12641v1&entry.124074799=Read"},
{"title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines", "author": "Genta Indra Winata and Frederikus Hudi and Patrick Amadeus Irawan and David Anugraha and Rifki Afina Putri and Yutong Wang and Adam Nohejl and Ubaidillah Ariq Prathama and Nedjma Ousidhoum and Afifa Amriani and Anar Rzayev and Anirban Das and Ashmari Pramodya and Aulia Adila and Bryan Wilie and Candy Olivia Mawalim and Ching Lam Cheng and Daud Abolade and Emmanuele Chersoni and Enrico Santus and Fariz Ikhwantri and Garry Kuwanto and Hanyang Zhao and Haryo Akbarianto Wibowo and Holy Lovenia and Jan Christian Blaise Cruz and Jan Wira Gotama Putra and Junho Myung and Lucky Susanto and Maria Angelica Riera Machin and Marina Zhukova and Michael Anugraha and Muhammad Farid Adilazuarda and Natasha Santosa and Peerat Limkonchotiwat and Raj Dabre and Rio Alexander Audino and Samuel Cahyawijaya and Shi-Xiong Zhang and Stephanie Yulia Salim and Yi Zhou and Yinxuan Gui and David Ifeoluwa Adelani and En-Shiun Annie Lee and Shogo Okada and Ayu Purwarianti and Alham Fikri Aji and Taro Watanabe and Derry Tanti Wijaya and Alice Oh and Chong-Wah Ngo", "abstract": "  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n", "link": "http://arxiv.org/abs/2410.12705v1", "date": "2024-10-16", "relevancy": 2.1281, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldCuisines%3A%20A%20Massive-Scale%20Benchmark%20for%20Multilingual%20and%0A%20%20Multicultural%20Visual%20Question%20Answering%20on%20Global%20Cuisines&body=Title%3A%20WorldCuisines%3A%20A%20Massive-Scale%20Benchmark%20for%20Multilingual%20and%0A%20%20Multicultural%20Visual%20Question%20Answering%20on%20Global%20Cuisines%0AAuthor%3A%20Genta%20Indra%20Winata%20and%20Frederikus%20Hudi%20and%20Patrick%20Amadeus%20Irawan%20and%20David%20Anugraha%20and%20Rifki%20Afina%20Putri%20and%20Yutong%20Wang%20and%20Adam%20Nohejl%20and%20Ubaidillah%20Ariq%20Prathama%20and%20Nedjma%20Ousidhoum%20and%20Afifa%20Amriani%20and%20Anar%20Rzayev%20and%20Anirban%20Das%20and%20Ashmari%20Pramodya%20and%20Aulia%20Adila%20and%20Bryan%20Wilie%20and%20Candy%20Olivia%20Mawalim%20and%20Ching%20Lam%20Cheng%20and%20Daud%20Abolade%20and%20Emmanuele%20Chersoni%20and%20Enrico%20Santus%20and%20Fariz%20Ikhwantri%20and%20Garry%20Kuwanto%20and%20Hanyang%20Zhao%20and%20Haryo%20Akbarianto%20Wibowo%20and%20Holy%20Lovenia%20and%20Jan%20Christian%20Blaise%20Cruz%20and%20Jan%20Wira%20Gotama%20Putra%20and%20Junho%20Myung%20and%20Lucky%20Susanto%20and%20Maria%20Angelica%20Riera%20Machin%20and%20Marina%20Zhukova%20and%20Michael%20Anugraha%20and%20Muhammad%20Farid%20Adilazuarda%20and%20Natasha%20Santosa%20and%20Peerat%20Limkonchotiwat%20and%20Raj%20Dabre%20and%20Rio%20Alexander%20Audino%20and%20Samuel%20Cahyawijaya%20and%20Shi-Xiong%20Zhang%20and%20Stephanie%20Yulia%20Salim%20and%20Yi%20Zhou%20and%20Yinxuan%20Gui%20and%20David%20Ifeoluwa%20Adelani%20and%20En-Shiun%20Annie%20Lee%20and%20Shogo%20Okada%20and%20Ayu%20Purwarianti%20and%20Alham%20Fikri%20Aji%20and%20Taro%20Watanabe%20and%20Derry%20Tanti%20Wijaya%20and%20Alice%20Oh%20and%20Chong-Wah%20Ngo%0AAbstract%3A%20%20%20Vision%20Language%20Models%20%28VLMs%29%20often%20struggle%20with%20culture-specific%20knowledge%2C%0Aparticularly%20in%20languages%20other%20than%20English%20and%20in%20underrepresented%20cultural%0Acontexts.%20To%20evaluate%20their%20understanding%20of%20such%20knowledge%2C%20we%20introduce%0AWorldCuisines%2C%20a%20massive-scale%20benchmark%20for%20multilingual%20and%20multicultural%2C%0Avisually%20grounded%20language%20understanding.%20This%20benchmark%20includes%20a%20visual%0Aquestion%20answering%20%28VQA%29%20dataset%20with%20text-image%20pairs%20across%2030%20languages%20and%0Adialects%2C%20spanning%209%20language%20families%20and%20featuring%20over%201%20million%20data%0Apoints%2C%20making%20it%20the%20largest%20multicultural%20VQA%20benchmark%20to%20date.%20It%20includes%0Atasks%20for%20identifying%20dish%20names%20and%20their%20origins.%20We%20provide%20evaluation%0Adatasets%20in%20two%20sizes%20%2812k%20and%2060k%20instances%29%20alongside%20a%20training%20dataset%20%281%0Amillion%20instances%29.%20Our%20findings%20show%20that%20while%20VLMs%20perform%20better%20with%0Acorrect%20location%20context%2C%20they%20struggle%20with%20adversarial%20contexts%20and%0Apredicting%20specific%20regional%20cuisines%20and%20languages.%20To%20support%20future%0Aresearch%2C%20we%20release%20a%20knowledge%20base%20with%20annotated%20food%20entries%20and%20images%0Aalong%20with%20the%20VQA%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldCuisines%253A%2520A%2520Massive-Scale%2520Benchmark%2520for%2520Multilingual%2520and%250A%2520%2520Multicultural%2520Visual%2520Question%2520Answering%2520on%2520Global%2520Cuisines%26entry.906535625%3DGenta%2520Indra%2520Winata%2520and%2520Frederikus%2520Hudi%2520and%2520Patrick%2520Amadeus%2520Irawan%2520and%2520David%2520Anugraha%2520and%2520Rifki%2520Afina%2520Putri%2520and%2520Yutong%2520Wang%2520and%2520Adam%2520Nohejl%2520and%2520Ubaidillah%2520Ariq%2520Prathama%2520and%2520Nedjma%2520Ousidhoum%2520and%2520Afifa%2520Amriani%2520and%2520Anar%2520Rzayev%2520and%2520Anirban%2520Das%2520and%2520Ashmari%2520Pramodya%2520and%2520Aulia%2520Adila%2520and%2520Bryan%2520Wilie%2520and%2520Candy%2520Olivia%2520Mawalim%2520and%2520Ching%2520Lam%2520Cheng%2520and%2520Daud%2520Abolade%2520and%2520Emmanuele%2520Chersoni%2520and%2520Enrico%2520Santus%2520and%2520Fariz%2520Ikhwantri%2520and%2520Garry%2520Kuwanto%2520and%2520Hanyang%2520Zhao%2520and%2520Haryo%2520Akbarianto%2520Wibowo%2520and%2520Holy%2520Lovenia%2520and%2520Jan%2520Christian%2520Blaise%2520Cruz%2520and%2520Jan%2520Wira%2520Gotama%2520Putra%2520and%2520Junho%2520Myung%2520and%2520Lucky%2520Susanto%2520and%2520Maria%2520Angelica%2520Riera%2520Machin%2520and%2520Marina%2520Zhukova%2520and%2520Michael%2520Anugraha%2520and%2520Muhammad%2520Farid%2520Adilazuarda%2520and%2520Natasha%2520Santosa%2520and%2520Peerat%2520Limkonchotiwat%2520and%2520Raj%2520Dabre%2520and%2520Rio%2520Alexander%2520Audino%2520and%2520Samuel%2520Cahyawijaya%2520and%2520Shi-Xiong%2520Zhang%2520and%2520Stephanie%2520Yulia%2520Salim%2520and%2520Yi%2520Zhou%2520and%2520Yinxuan%2520Gui%2520and%2520David%2520Ifeoluwa%2520Adelani%2520and%2520En-Shiun%2520Annie%2520Lee%2520and%2520Shogo%2520Okada%2520and%2520Ayu%2520Purwarianti%2520and%2520Alham%2520Fikri%2520Aji%2520and%2520Taro%2520Watanabe%2520and%2520Derry%2520Tanti%2520Wijaya%2520and%2520Alice%2520Oh%2520and%2520Chong-Wah%2520Ngo%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520often%2520struggle%2520with%2520culture-specific%2520knowledge%252C%250Aparticularly%2520in%2520languages%2520other%2520than%2520English%2520and%2520in%2520underrepresented%2520cultural%250Acontexts.%2520To%2520evaluate%2520their%2520understanding%2520of%2520such%2520knowledge%252C%2520we%2520introduce%250AWorldCuisines%252C%2520a%2520massive-scale%2520benchmark%2520for%2520multilingual%2520and%2520multicultural%252C%250Avisually%2520grounded%2520language%2520understanding.%2520This%2520benchmark%2520includes%2520a%2520visual%250Aquestion%2520answering%2520%2528VQA%2529%2520dataset%2520with%2520text-image%2520pairs%2520across%252030%2520languages%2520and%250Adialects%252C%2520spanning%25209%2520language%2520families%2520and%2520featuring%2520over%25201%2520million%2520data%250Apoints%252C%2520making%2520it%2520the%2520largest%2520multicultural%2520VQA%2520benchmark%2520to%2520date.%2520It%2520includes%250Atasks%2520for%2520identifying%2520dish%2520names%2520and%2520their%2520origins.%2520We%2520provide%2520evaluation%250Adatasets%2520in%2520two%2520sizes%2520%252812k%2520and%252060k%2520instances%2529%2520alongside%2520a%2520training%2520dataset%2520%25281%250Amillion%2520instances%2529.%2520Our%2520findings%2520show%2520that%2520while%2520VLMs%2520perform%2520better%2520with%250Acorrect%2520location%2520context%252C%2520they%2520struggle%2520with%2520adversarial%2520contexts%2520and%250Apredicting%2520specific%2520regional%2520cuisines%2520and%2520languages.%2520To%2520support%2520future%250Aresearch%252C%2520we%2520release%2520a%2520knowledge%2520base%2520with%2520annotated%2520food%2520entries%2520and%2520images%250Aalong%2520with%2520the%2520VQA%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldCuisines%3A%20A%20Massive-Scale%20Benchmark%20for%20Multilingual%20and%0A%20%20Multicultural%20Visual%20Question%20Answering%20on%20Global%20Cuisines&entry.906535625=Genta%20Indra%20Winata%20and%20Frederikus%20Hudi%20and%20Patrick%20Amadeus%20Irawan%20and%20David%20Anugraha%20and%20Rifki%20Afina%20Putri%20and%20Yutong%20Wang%20and%20Adam%20Nohejl%20and%20Ubaidillah%20Ariq%20Prathama%20and%20Nedjma%20Ousidhoum%20and%20Afifa%20Amriani%20and%20Anar%20Rzayev%20and%20Anirban%20Das%20and%20Ashmari%20Pramodya%20and%20Aulia%20Adila%20and%20Bryan%20Wilie%20and%20Candy%20Olivia%20Mawalim%20and%20Ching%20Lam%20Cheng%20and%20Daud%20Abolade%20and%20Emmanuele%20Chersoni%20and%20Enrico%20Santus%20and%20Fariz%20Ikhwantri%20and%20Garry%20Kuwanto%20and%20Hanyang%20Zhao%20and%20Haryo%20Akbarianto%20Wibowo%20and%20Holy%20Lovenia%20and%20Jan%20Christian%20Blaise%20Cruz%20and%20Jan%20Wira%20Gotama%20Putra%20and%20Junho%20Myung%20and%20Lucky%20Susanto%20and%20Maria%20Angelica%20Riera%20Machin%20and%20Marina%20Zhukova%20and%20Michael%20Anugraha%20and%20Muhammad%20Farid%20Adilazuarda%20and%20Natasha%20Santosa%20and%20Peerat%20Limkonchotiwat%20and%20Raj%20Dabre%20and%20Rio%20Alexander%20Audino%20and%20Samuel%20Cahyawijaya%20and%20Shi-Xiong%20Zhang%20and%20Stephanie%20Yulia%20Salim%20and%20Yi%20Zhou%20and%20Yinxuan%20Gui%20and%20David%20Ifeoluwa%20Adelani%20and%20En-Shiun%20Annie%20Lee%20and%20Shogo%20Okada%20and%20Ayu%20Purwarianti%20and%20Alham%20Fikri%20Aji%20and%20Taro%20Watanabe%20and%20Derry%20Tanti%20Wijaya%20and%20Alice%20Oh%20and%20Chong-Wah%20Ngo&entry.1292438233=%20%20Vision%20Language%20Models%20%28VLMs%29%20often%20struggle%20with%20culture-specific%20knowledge%2C%0Aparticularly%20in%20languages%20other%20than%20English%20and%20in%20underrepresented%20cultural%0Acontexts.%20To%20evaluate%20their%20understanding%20of%20such%20knowledge%2C%20we%20introduce%0AWorldCuisines%2C%20a%20massive-scale%20benchmark%20for%20multilingual%20and%20multicultural%2C%0Avisually%20grounded%20language%20understanding.%20This%20benchmark%20includes%20a%20visual%0Aquestion%20answering%20%28VQA%29%20dataset%20with%20text-image%20pairs%20across%2030%20languages%20and%0Adialects%2C%20spanning%209%20language%20families%20and%20featuring%20over%201%20million%20data%0Apoints%2C%20making%20it%20the%20largest%20multicultural%20VQA%20benchmark%20to%20date.%20It%20includes%0Atasks%20for%20identifying%20dish%20names%20and%20their%20origins.%20We%20provide%20evaluation%0Adatasets%20in%20two%20sizes%20%2812k%20and%2060k%20instances%29%20alongside%20a%20training%20dataset%20%281%0Amillion%20instances%29.%20Our%20findings%20show%20that%20while%20VLMs%20perform%20better%20with%0Acorrect%20location%20context%2C%20they%20struggle%20with%20adversarial%20contexts%20and%0Apredicting%20specific%20regional%20cuisines%20and%20languages.%20To%20support%20future%0Aresearch%2C%20we%20release%20a%20knowledge%20base%20with%20annotated%20food%20entries%20and%20images%0Aalong%20with%20the%20VQA%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12705v1&entry.124074799=Read"},
{"title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs", "author": "Wanying Wang and Zeyu Ma and Pengfei Liu and Mingang Chen", "abstract": "  While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.\n", "link": "http://arxiv.org/abs/2410.11507v2", "date": "2024-10-16", "relevancy": 2.1214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Benchmark%20and%20Assessment%3A%20An%20Agent-based%20Exploratory%20Dynamic%0A%20%20Evaluation%20Framework%20for%20LLMs&body=Title%3A%20Revisiting%20Benchmark%20and%20Assessment%3A%20An%20Agent-based%20Exploratory%20Dynamic%0A%20%20Evaluation%20Framework%20for%20LLMs%0AAuthor%3A%20Wanying%20Wang%20and%20Zeyu%20Ma%20and%20Pengfei%20Liu%20and%20Mingang%20Chen%0AAbstract%3A%20%20%20While%20various%20vertical%20domain%20large%20language%20models%20%28LLMs%29%20have%20been%0Adeveloped%2C%20the%20challenge%20of%20automatically%20evaluating%20their%20performance%20across%0Adifferent%20domains%20remains%20significant.%20Current%20benchmark-based%20evaluation%0Amethods%20exhibit%20rigid%2C%20aimless%20interactions%20and%20rely%20on%20pre-collected%20static%0Adatasets%20that%20are%20costly%20to%20build%2C%20inflexible%20across%20domains%2C%20and%20misaligned%0Awith%20practical%20user%20needs.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20evaluation%0Acomponents%20and%20introduce%20two%20concepts%3A%20Benchmark%2B%2C%20which%20extends%20traditional%0Aquestion-answer%20benchmark%20into%20a%20more%20flexible%20%22strategy-criterion%22%20format%3B%20and%0AAssessment%2B%2C%20which%20enhances%20the%20interaction%20process%2C%20enabling%20deeper%0Aexploration%20and%20supporting%20both%20quantitative%20metrics%20and%20qualitative%20insights.%0AThese%20concepts%20capture%20the%20nuanced%20behaviors%20of%20LLMs%20through%20richer%2C%20multi-turn%0Ainteractions.%20We%20propose%20an%20agent-based%20evaluation%20framework%20called%20TestAgent%2C%0Awhich%20implements%20these%20concepts%20through%20retrieval%20augmented%20generation%20and%0Areinforcement%20learning.%20Experiments%20on%20tasks%20ranging%20from%20constructing%20vertical%0Adomain%20evaluation%20to%20activating%20existing%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20TestAgent%20across%20various%20scenarios.%20We%20believe%20this%20work%0Aoffers%20an%20interesting%20perspective%20on%20automatic%20evaluation%20for%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Benchmark%2520and%2520Assessment%253A%2520An%2520Agent-based%2520Exploratory%2520Dynamic%250A%2520%2520Evaluation%2520Framework%2520for%2520LLMs%26entry.906535625%3DWanying%2520Wang%2520and%2520Zeyu%2520Ma%2520and%2520Pengfei%2520Liu%2520and%2520Mingang%2520Chen%26entry.1292438233%3D%2520%2520While%2520various%2520vertical%2520domain%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%250Adeveloped%252C%2520the%2520challenge%2520of%2520automatically%2520evaluating%2520their%2520performance%2520across%250Adifferent%2520domains%2520remains%2520significant.%2520Current%2520benchmark-based%2520evaluation%250Amethods%2520exhibit%2520rigid%252C%2520aimless%2520interactions%2520and%2520rely%2520on%2520pre-collected%2520static%250Adatasets%2520that%2520are%2520costly%2520to%2520build%252C%2520inflexible%2520across%2520domains%252C%2520and%2520misaligned%250Awith%2520practical%2520user%2520needs.%2520To%2520address%2520this%2520issue%252C%2520we%2520revisit%2520the%2520evaluation%250Acomponents%2520and%2520introduce%2520two%2520concepts%253A%2520Benchmark%252B%252C%2520which%2520extends%2520traditional%250Aquestion-answer%2520benchmark%2520into%2520a%2520more%2520flexible%2520%2522strategy-criterion%2522%2520format%253B%2520and%250AAssessment%252B%252C%2520which%2520enhances%2520the%2520interaction%2520process%252C%2520enabling%2520deeper%250Aexploration%2520and%2520supporting%2520both%2520quantitative%2520metrics%2520and%2520qualitative%2520insights.%250AThese%2520concepts%2520capture%2520the%2520nuanced%2520behaviors%2520of%2520LLMs%2520through%2520richer%252C%2520multi-turn%250Ainteractions.%2520We%2520propose%2520an%2520agent-based%2520evaluation%2520framework%2520called%2520TestAgent%252C%250Awhich%2520implements%2520these%2520concepts%2520through%2520retrieval%2520augmented%2520generation%2520and%250Areinforcement%2520learning.%2520Experiments%2520on%2520tasks%2520ranging%2520from%2520constructing%2520vertical%250Adomain%2520evaluation%2520to%2520activating%2520existing%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520TestAgent%2520across%2520various%2520scenarios.%2520We%2520believe%2520this%2520work%250Aoffers%2520an%2520interesting%2520perspective%2520on%2520automatic%2520evaluation%2520for%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Benchmark%20and%20Assessment%3A%20An%20Agent-based%20Exploratory%20Dynamic%0A%20%20Evaluation%20Framework%20for%20LLMs&entry.906535625=Wanying%20Wang%20and%20Zeyu%20Ma%20and%20Pengfei%20Liu%20and%20Mingang%20Chen&entry.1292438233=%20%20While%20various%20vertical%20domain%20large%20language%20models%20%28LLMs%29%20have%20been%0Adeveloped%2C%20the%20challenge%20of%20automatically%20evaluating%20their%20performance%20across%0Adifferent%20domains%20remains%20significant.%20Current%20benchmark-based%20evaluation%0Amethods%20exhibit%20rigid%2C%20aimless%20interactions%20and%20rely%20on%20pre-collected%20static%0Adatasets%20that%20are%20costly%20to%20build%2C%20inflexible%20across%20domains%2C%20and%20misaligned%0Awith%20practical%20user%20needs.%20To%20address%20this%20issue%2C%20we%20revisit%20the%20evaluation%0Acomponents%20and%20introduce%20two%20concepts%3A%20Benchmark%2B%2C%20which%20extends%20traditional%0Aquestion-answer%20benchmark%20into%20a%20more%20flexible%20%22strategy-criterion%22%20format%3B%20and%0AAssessment%2B%2C%20which%20enhances%20the%20interaction%20process%2C%20enabling%20deeper%0Aexploration%20and%20supporting%20both%20quantitative%20metrics%20and%20qualitative%20insights.%0AThese%20concepts%20capture%20the%20nuanced%20behaviors%20of%20LLMs%20through%20richer%2C%20multi-turn%0Ainteractions.%20We%20propose%20an%20agent-based%20evaluation%20framework%20called%20TestAgent%2C%0Awhich%20implements%20these%20concepts%20through%20retrieval%20augmented%20generation%20and%0Areinforcement%20learning.%20Experiments%20on%20tasks%20ranging%20from%20constructing%20vertical%0Adomain%20evaluation%20to%20activating%20existing%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20TestAgent%20across%20various%20scenarios.%20We%20believe%20this%20work%0Aoffers%20an%20interesting%20perspective%20on%20automatic%20evaluation%20for%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11507v2&entry.124074799=Read"},
{"title": "DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training", "author": "Yu Xie and Qian Qiao and Jun Gao and Tianxiang Wu and Jiaqing Fan and Yue Zhang and Jielei Zhang and Huyang Sun", "abstract": "  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n", "link": "http://arxiv.org/abs/2408.00355v2", "date": "2024-10-16", "relevancy": 2.1088, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNTextSpotter%3A%20Arbitrary-Shaped%20Scene%20Text%20Spotting%20via%20Improved%0A%20%20Denoising%20Training&body=Title%3A%20DNTextSpotter%3A%20Arbitrary-Shaped%20Scene%20Text%20Spotting%20via%20Improved%0A%20%20Denoising%20Training%0AAuthor%3A%20Yu%20Xie%20and%20Qian%20Qiao%20and%20Jun%20Gao%20and%20Tianxiang%20Wu%20and%20Jiaqing%20Fan%20and%20Yue%20Zhang%20and%20Jielei%20Zhang%20and%20Huyang%20Sun%0AAbstract%3A%20%20%20More%20and%20more%20end-to-end%20text%20spotting%20methods%20based%20on%20Transformer%0Aarchitecture%20have%20demonstrated%20superior%20performance.%20These%20methods%20utilize%20a%0Abipartite%20graph%20matching%20algorithm%20to%20perform%20one-to-one%20optimal%20matching%0Abetween%20predicted%20objects%20and%20actual%20objects.%20However%2C%20the%20instability%20of%0Abipartite%20graph%20matching%20can%20lead%20to%20inconsistent%20optimization%20targets%2C%20thereby%0Aaffecting%20the%20training%20performance%20of%20the%20model.%20Existing%20literature%20applies%0Adenoising%20training%20to%20solve%20the%20problem%20of%20bipartite%20graph%20matching%20instability%0Ain%20object%20detection%20tasks.%20Unfortunately%2C%20this%20denoising%20training%20method%20cannot%0Abe%20directly%20applied%20to%20text%20spotting%20tasks%2C%20as%20these%20tasks%20need%20to%20perform%0Airregular%20shape%20detection%20tasks%20and%20more%20complex%20text%20recognition%20tasks%20than%0Aclassification.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20denoising%20training%0Amethod%20%28DNTextSpotter%29%20for%20arbitrary-shaped%20text%20spotting.%20Specifically%2C%20we%0Adecompose%20the%20queries%20of%20the%20denoising%20part%20into%20noised%20positional%20queries%20and%0Anoised%20content%20queries.%20We%20use%20the%20four%20Bezier%20control%20points%20of%20the%20Bezier%0Acenter%20curve%20to%20generate%20the%20noised%20positional%20queries.%20For%20the%20noised%20content%0Aqueries%2C%20considering%20that%20the%20output%20of%20the%20text%20in%20a%20fixed%20positional%20order%20is%0Anot%20conducive%20to%20aligning%20position%20with%20content%2C%20we%20employ%20a%20masked%20character%0Asliding%20method%20to%20initialize%20noised%20content%20queries%2C%20thereby%20assisting%20in%20the%0Aalignment%20of%20text%20content%20and%20position.%20To%20improve%20the%20model%27s%20perception%20of%0Athe%20background%2C%20we%20further%20utilize%20an%20additional%20loss%20function%20for%20background%0Acharacters%20classification%20in%20the%20denoising%20training%20part.Although%20DNTextSpotter%0Ais%20conceptually%20simple%2C%20it%20outperforms%20the%20state-of-the-art%20methods%20on%20four%0Abenchmarks%20%28Total-Text%2C%20SCUT-CTW1500%2C%20ICDAR15%2C%20and%20Inverse-Text%29%2C%20especially%0Ayielding%20an%20improvement%20of%2011.3%25%20against%20the%20best%20approach%20in%20Inverse-Text%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNTextSpotter%253A%2520Arbitrary-Shaped%2520Scene%2520Text%2520Spotting%2520via%2520Improved%250A%2520%2520Denoising%2520Training%26entry.906535625%3DYu%2520Xie%2520and%2520Qian%2520Qiao%2520and%2520Jun%2520Gao%2520and%2520Tianxiang%2520Wu%2520and%2520Jiaqing%2520Fan%2520and%2520Yue%2520Zhang%2520and%2520Jielei%2520Zhang%2520and%2520Huyang%2520Sun%26entry.1292438233%3D%2520%2520More%2520and%2520more%2520end-to-end%2520text%2520spotting%2520methods%2520based%2520on%2520Transformer%250Aarchitecture%2520have%2520demonstrated%2520superior%2520performance.%2520These%2520methods%2520utilize%2520a%250Abipartite%2520graph%2520matching%2520algorithm%2520to%2520perform%2520one-to-one%2520optimal%2520matching%250Abetween%2520predicted%2520objects%2520and%2520actual%2520objects.%2520However%252C%2520the%2520instability%2520of%250Abipartite%2520graph%2520matching%2520can%2520lead%2520to%2520inconsistent%2520optimization%2520targets%252C%2520thereby%250Aaffecting%2520the%2520training%2520performance%2520of%2520the%2520model.%2520Existing%2520literature%2520applies%250Adenoising%2520training%2520to%2520solve%2520the%2520problem%2520of%2520bipartite%2520graph%2520matching%2520instability%250Ain%2520object%2520detection%2520tasks.%2520Unfortunately%252C%2520this%2520denoising%2520training%2520method%2520cannot%250Abe%2520directly%2520applied%2520to%2520text%2520spotting%2520tasks%252C%2520as%2520these%2520tasks%2520need%2520to%2520perform%250Airregular%2520shape%2520detection%2520tasks%2520and%2520more%2520complex%2520text%2520recognition%2520tasks%2520than%250Aclassification.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520denoising%2520training%250Amethod%2520%2528DNTextSpotter%2529%2520for%2520arbitrary-shaped%2520text%2520spotting.%2520Specifically%252C%2520we%250Adecompose%2520the%2520queries%2520of%2520the%2520denoising%2520part%2520into%2520noised%2520positional%2520queries%2520and%250Anoised%2520content%2520queries.%2520We%2520use%2520the%2520four%2520Bezier%2520control%2520points%2520of%2520the%2520Bezier%250Acenter%2520curve%2520to%2520generate%2520the%2520noised%2520positional%2520queries.%2520For%2520the%2520noised%2520content%250Aqueries%252C%2520considering%2520that%2520the%2520output%2520of%2520the%2520text%2520in%2520a%2520fixed%2520positional%2520order%2520is%250Anot%2520conducive%2520to%2520aligning%2520position%2520with%2520content%252C%2520we%2520employ%2520a%2520masked%2520character%250Asliding%2520method%2520to%2520initialize%2520noised%2520content%2520queries%252C%2520thereby%2520assisting%2520in%2520the%250Aalignment%2520of%2520text%2520content%2520and%2520position.%2520To%2520improve%2520the%2520model%2527s%2520perception%2520of%250Athe%2520background%252C%2520we%2520further%2520utilize%2520an%2520additional%2520loss%2520function%2520for%2520background%250Acharacters%2520classification%2520in%2520the%2520denoising%2520training%2520part.Although%2520DNTextSpotter%250Ais%2520conceptually%2520simple%252C%2520it%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%2520four%250Abenchmarks%2520%2528Total-Text%252C%2520SCUT-CTW1500%252C%2520ICDAR15%252C%2520and%2520Inverse-Text%2529%252C%2520especially%250Ayielding%2520an%2520improvement%2520of%252011.3%2525%2520against%2520the%2520best%2520approach%2520in%2520Inverse-Text%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNTextSpotter%3A%20Arbitrary-Shaped%20Scene%20Text%20Spotting%20via%20Improved%0A%20%20Denoising%20Training&entry.906535625=Yu%20Xie%20and%20Qian%20Qiao%20and%20Jun%20Gao%20and%20Tianxiang%20Wu%20and%20Jiaqing%20Fan%20and%20Yue%20Zhang%20and%20Jielei%20Zhang%20and%20Huyang%20Sun&entry.1292438233=%20%20More%20and%20more%20end-to-end%20text%20spotting%20methods%20based%20on%20Transformer%0Aarchitecture%20have%20demonstrated%20superior%20performance.%20These%20methods%20utilize%20a%0Abipartite%20graph%20matching%20algorithm%20to%20perform%20one-to-one%20optimal%20matching%0Abetween%20predicted%20objects%20and%20actual%20objects.%20However%2C%20the%20instability%20of%0Abipartite%20graph%20matching%20can%20lead%20to%20inconsistent%20optimization%20targets%2C%20thereby%0Aaffecting%20the%20training%20performance%20of%20the%20model.%20Existing%20literature%20applies%0Adenoising%20training%20to%20solve%20the%20problem%20of%20bipartite%20graph%20matching%20instability%0Ain%20object%20detection%20tasks.%20Unfortunately%2C%20this%20denoising%20training%20method%20cannot%0Abe%20directly%20applied%20to%20text%20spotting%20tasks%2C%20as%20these%20tasks%20need%20to%20perform%0Airregular%20shape%20detection%20tasks%20and%20more%20complex%20text%20recognition%20tasks%20than%0Aclassification.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20denoising%20training%0Amethod%20%28DNTextSpotter%29%20for%20arbitrary-shaped%20text%20spotting.%20Specifically%2C%20we%0Adecompose%20the%20queries%20of%20the%20denoising%20part%20into%20noised%20positional%20queries%20and%0Anoised%20content%20queries.%20We%20use%20the%20four%20Bezier%20control%20points%20of%20the%20Bezier%0Acenter%20curve%20to%20generate%20the%20noised%20positional%20queries.%20For%20the%20noised%20content%0Aqueries%2C%20considering%20that%20the%20output%20of%20the%20text%20in%20a%20fixed%20positional%20order%20is%0Anot%20conducive%20to%20aligning%20position%20with%20content%2C%20we%20employ%20a%20masked%20character%0Asliding%20method%20to%20initialize%20noised%20content%20queries%2C%20thereby%20assisting%20in%20the%0Aalignment%20of%20text%20content%20and%20position.%20To%20improve%20the%20model%27s%20perception%20of%0Athe%20background%2C%20we%20further%20utilize%20an%20additional%20loss%20function%20for%20background%0Acharacters%20classification%20in%20the%20denoising%20training%20part.Although%20DNTextSpotter%0Ais%20conceptually%20simple%2C%20it%20outperforms%20the%20state-of-the-art%20methods%20on%20four%0Abenchmarks%20%28Total-Text%2C%20SCUT-CTW1500%2C%20ICDAR15%2C%20and%20Inverse-Text%29%2C%20especially%0Ayielding%20an%20improvement%20of%2011.3%25%20against%20the%20best%20approach%20in%20Inverse-Text%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00355v2&entry.124074799=Read"},
{"title": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs\n  with Adaptive Compression", "author": "Zhenheng Tang and Xueze Kang and Yiming Yin and Xinglin Pan and Yuxin Wang and Xin He and Qiang Wang and Rongfei Zeng and Kaiyong Zhao and Shaohuai Shi and Amelie Chi Zhou and Bo Li and Bingsheng He and Xiaowen Chu", "abstract": "  To alleviate hardware scarcity in training large deep neural networks (DNNs),\nparticularly large language models (LLMs), we present FusionLLM, a\ndecentralized training system designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or individual devices.\nDecentralized training faces significant challenges regarding system design and\nefficiency, including: 1) the need for remote automatic differentiation (RAD),\n2) support for flexible model definitions and heterogeneous software, 3)\nheterogeneous hardware leading to low resource utilization or the straggler\nproblem, and 4) slow network communication. To address these challenges, in the\nsystem design, we represent the model as a directed acyclic graph of operators\n(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the\nedge represents the data dependency between operators. Based on this design, 1)\nusers are allowed to customize any DNN without caring low-level operator\nimplementation; 2) we enable the task scheduling with the more fine-grained\nsub-tasks, offering more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level ML framework versions.\n  To enhance system efficiency, we implement a workload estimator and design an\nOP-Fence scheduler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we propose an AdaTopK\ncompressor to adaptively compress intermediate activations and gradients at the\nslowest communication links. To evaluate the convergence and efficiency of our\nsystem and algorithms, we train ResNet-101 and GPT-2 on three real-world\ntestbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental\nresults demonstrate that our system and method can achieve 1.45 - 9.39x speedup\ncompared to baseline methods while ensuring convergence.\n", "link": "http://arxiv.org/abs/2410.12707v1", "date": "2024-10-16", "relevancy": 2.1065, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5568}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5259}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionLLM%3A%20A%20Decentralized%20LLM%20Training%20System%20on%20Geo-distributed%20GPUs%0A%20%20with%20Adaptive%20Compression&body=Title%3A%20FusionLLM%3A%20A%20Decentralized%20LLM%20Training%20System%20on%20Geo-distributed%20GPUs%0A%20%20with%20Adaptive%20Compression%0AAuthor%3A%20Zhenheng%20Tang%20and%20Xueze%20Kang%20and%20Yiming%20Yin%20and%20Xinglin%20Pan%20and%20Yuxin%20Wang%20and%20Xin%20He%20and%20Qiang%20Wang%20and%20Rongfei%20Zeng%20and%20Kaiyong%20Zhao%20and%20Shaohuai%20Shi%20and%20Amelie%20Chi%20Zhou%20and%20Bo%20Li%20and%20Bingsheng%20He%20and%20Xiaowen%20Chu%0AAbstract%3A%20%20%20To%20alleviate%20hardware%20scarcity%20in%20training%20large%20deep%20neural%20networks%20%28DNNs%29%2C%0Aparticularly%20large%20language%20models%20%28LLMs%29%2C%20we%20present%20FusionLLM%2C%20a%0Adecentralized%20training%20system%20designed%20and%20implemented%20for%20training%20DNNs%20using%0Ageo-distributed%20GPUs%20across%20different%20computing%20clusters%20or%20individual%20devices.%0ADecentralized%20training%20faces%20significant%20challenges%20regarding%20system%20design%20and%0Aefficiency%2C%20including%3A%201%29%20the%20need%20for%20remote%20automatic%20differentiation%20%28RAD%29%2C%0A2%29%20support%20for%20flexible%20model%20definitions%20and%20heterogeneous%20software%2C%203%29%0Aheterogeneous%20hardware%20leading%20to%20low%20resource%20utilization%20or%20the%20straggler%0Aproblem%2C%20and%204%29%20slow%20network%20communication.%20To%20address%20these%20challenges%2C%20in%20the%0Asystem%20design%2C%20we%20represent%20the%20model%20as%20a%20directed%20acyclic%20graph%20of%20operators%0A%28OP-DAG%29.%20Each%20node%20in%20the%20DAG%20represents%20the%20operator%20in%20the%20DNNs%2C%20while%20the%0Aedge%20represents%20the%20data%20dependency%20between%20operators.%20Based%20on%20this%20design%2C%201%29%0Ausers%20are%20allowed%20to%20customize%20any%20DNN%20without%20caring%20low-level%20operator%0Aimplementation%3B%202%29%20we%20enable%20the%20task%20scheduling%20with%20the%20more%20fine-grained%0Asub-tasks%2C%20offering%20more%20optimization%20space%3B%203%29%20a%20DAG%20runtime%20executor%20can%0Aimplement%20RAD%20withour%20requiring%20the%20consistent%20low-level%20ML%20framework%20versions.%0A%20%20To%20enhance%20system%20efficiency%2C%20we%20implement%20a%20workload%20estimator%20and%20design%20an%0AOP-Fence%20scheduler%20to%20cluster%20devices%20with%20similar%20bandwidths%20together%20and%0Apartition%20the%20DAG%20to%20increase%20throughput.%20Additionally%2C%20we%20propose%20an%20AdaTopK%0Acompressor%20to%20adaptively%20compress%20intermediate%20activations%20and%20gradients%20at%20the%0Aslowest%20communication%20links.%20To%20evaluate%20the%20convergence%20and%20efficiency%20of%20our%0Asystem%20and%20algorithms%2C%20we%20train%20ResNet-101%20and%20GPT-2%20on%20three%20real-world%0Atestbeds%20using%2048%20GPUs%20connected%20with%208%20Mbps~10%20Gbps%20networks.%20Experimental%0Aresults%20demonstrate%20that%20our%20system%20and%20method%20can%20achieve%201.45%20-%209.39x%20speedup%0Acompared%20to%20baseline%20methods%20while%20ensuring%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionLLM%253A%2520A%2520Decentralized%2520LLM%2520Training%2520System%2520on%2520Geo-distributed%2520GPUs%250A%2520%2520with%2520Adaptive%2520Compression%26entry.906535625%3DZhenheng%2520Tang%2520and%2520Xueze%2520Kang%2520and%2520Yiming%2520Yin%2520and%2520Xinglin%2520Pan%2520and%2520Yuxin%2520Wang%2520and%2520Xin%2520He%2520and%2520Qiang%2520Wang%2520and%2520Rongfei%2520Zeng%2520and%2520Kaiyong%2520Zhao%2520and%2520Shaohuai%2520Shi%2520and%2520Amelie%2520Chi%2520Zhou%2520and%2520Bo%2520Li%2520and%2520Bingsheng%2520He%2520and%2520Xiaowen%2520Chu%26entry.1292438233%3D%2520%2520To%2520alleviate%2520hardware%2520scarcity%2520in%2520training%2520large%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%250Aparticularly%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%2520present%2520FusionLLM%252C%2520a%250Adecentralized%2520training%2520system%2520designed%2520and%2520implemented%2520for%2520training%2520DNNs%2520using%250Ageo-distributed%2520GPUs%2520across%2520different%2520computing%2520clusters%2520or%2520individual%2520devices.%250ADecentralized%2520training%2520faces%2520significant%2520challenges%2520regarding%2520system%2520design%2520and%250Aefficiency%252C%2520including%253A%25201%2529%2520the%2520need%2520for%2520remote%2520automatic%2520differentiation%2520%2528RAD%2529%252C%250A2%2529%2520support%2520for%2520flexible%2520model%2520definitions%2520and%2520heterogeneous%2520software%252C%25203%2529%250Aheterogeneous%2520hardware%2520leading%2520to%2520low%2520resource%2520utilization%2520or%2520the%2520straggler%250Aproblem%252C%2520and%25204%2529%2520slow%2520network%2520communication.%2520To%2520address%2520these%2520challenges%252C%2520in%2520the%250Asystem%2520design%252C%2520we%2520represent%2520the%2520model%2520as%2520a%2520directed%2520acyclic%2520graph%2520of%2520operators%250A%2528OP-DAG%2529.%2520Each%2520node%2520in%2520the%2520DAG%2520represents%2520the%2520operator%2520in%2520the%2520DNNs%252C%2520while%2520the%250Aedge%2520represents%2520the%2520data%2520dependency%2520between%2520operators.%2520Based%2520on%2520this%2520design%252C%25201%2529%250Ausers%2520are%2520allowed%2520to%2520customize%2520any%2520DNN%2520without%2520caring%2520low-level%2520operator%250Aimplementation%253B%25202%2529%2520we%2520enable%2520the%2520task%2520scheduling%2520with%2520the%2520more%2520fine-grained%250Asub-tasks%252C%2520offering%2520more%2520optimization%2520space%253B%25203%2529%2520a%2520DAG%2520runtime%2520executor%2520can%250Aimplement%2520RAD%2520withour%2520requiring%2520the%2520consistent%2520low-level%2520ML%2520framework%2520versions.%250A%2520%2520To%2520enhance%2520system%2520efficiency%252C%2520we%2520implement%2520a%2520workload%2520estimator%2520and%2520design%2520an%250AOP-Fence%2520scheduler%2520to%2520cluster%2520devices%2520with%2520similar%2520bandwidths%2520together%2520and%250Apartition%2520the%2520DAG%2520to%2520increase%2520throughput.%2520Additionally%252C%2520we%2520propose%2520an%2520AdaTopK%250Acompressor%2520to%2520adaptively%2520compress%2520intermediate%2520activations%2520and%2520gradients%2520at%2520the%250Aslowest%2520communication%2520links.%2520To%2520evaluate%2520the%2520convergence%2520and%2520efficiency%2520of%2520our%250Asystem%2520and%2520algorithms%252C%2520we%2520train%2520ResNet-101%2520and%2520GPT-2%2520on%2520three%2520real-world%250Atestbeds%2520using%252048%2520GPUs%2520connected%2520with%25208%2520Mbps~10%2520Gbps%2520networks.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520system%2520and%2520method%2520can%2520achieve%25201.45%2520-%25209.39x%2520speedup%250Acompared%2520to%2520baseline%2520methods%2520while%2520ensuring%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionLLM%3A%20A%20Decentralized%20LLM%20Training%20System%20on%20Geo-distributed%20GPUs%0A%20%20with%20Adaptive%20Compression&entry.906535625=Zhenheng%20Tang%20and%20Xueze%20Kang%20and%20Yiming%20Yin%20and%20Xinglin%20Pan%20and%20Yuxin%20Wang%20and%20Xin%20He%20and%20Qiang%20Wang%20and%20Rongfei%20Zeng%20and%20Kaiyong%20Zhao%20and%20Shaohuai%20Shi%20and%20Amelie%20Chi%20Zhou%20and%20Bo%20Li%20and%20Bingsheng%20He%20and%20Xiaowen%20Chu&entry.1292438233=%20%20To%20alleviate%20hardware%20scarcity%20in%20training%20large%20deep%20neural%20networks%20%28DNNs%29%2C%0Aparticularly%20large%20language%20models%20%28LLMs%29%2C%20we%20present%20FusionLLM%2C%20a%0Adecentralized%20training%20system%20designed%20and%20implemented%20for%20training%20DNNs%20using%0Ageo-distributed%20GPUs%20across%20different%20computing%20clusters%20or%20individual%20devices.%0ADecentralized%20training%20faces%20significant%20challenges%20regarding%20system%20design%20and%0Aefficiency%2C%20including%3A%201%29%20the%20need%20for%20remote%20automatic%20differentiation%20%28RAD%29%2C%0A2%29%20support%20for%20flexible%20model%20definitions%20and%20heterogeneous%20software%2C%203%29%0Aheterogeneous%20hardware%20leading%20to%20low%20resource%20utilization%20or%20the%20straggler%0Aproblem%2C%20and%204%29%20slow%20network%20communication.%20To%20address%20these%20challenges%2C%20in%20the%0Asystem%20design%2C%20we%20represent%20the%20model%20as%20a%20directed%20acyclic%20graph%20of%20operators%0A%28OP-DAG%29.%20Each%20node%20in%20the%20DAG%20represents%20the%20operator%20in%20the%20DNNs%2C%20while%20the%0Aedge%20represents%20the%20data%20dependency%20between%20operators.%20Based%20on%20this%20design%2C%201%29%0Ausers%20are%20allowed%20to%20customize%20any%20DNN%20without%20caring%20low-level%20operator%0Aimplementation%3B%202%29%20we%20enable%20the%20task%20scheduling%20with%20the%20more%20fine-grained%0Asub-tasks%2C%20offering%20more%20optimization%20space%3B%203%29%20a%20DAG%20runtime%20executor%20can%0Aimplement%20RAD%20withour%20requiring%20the%20consistent%20low-level%20ML%20framework%20versions.%0A%20%20To%20enhance%20system%20efficiency%2C%20we%20implement%20a%20workload%20estimator%20and%20design%20an%0AOP-Fence%20scheduler%20to%20cluster%20devices%20with%20similar%20bandwidths%20together%20and%0Apartition%20the%20DAG%20to%20increase%20throughput.%20Additionally%2C%20we%20propose%20an%20AdaTopK%0Acompressor%20to%20adaptively%20compress%20intermediate%20activations%20and%20gradients%20at%20the%0Aslowest%20communication%20links.%20To%20evaluate%20the%20convergence%20and%20efficiency%20of%20our%0Asystem%20and%20algorithms%2C%20we%20train%20ResNet-101%20and%20GPT-2%20on%20three%20real-world%0Atestbeds%20using%2048%20GPUs%20connected%20with%208%20Mbps~10%20Gbps%20networks.%20Experimental%0Aresults%20demonstrate%20that%20our%20system%20and%20method%20can%20achieve%201.45%20-%209.39x%20speedup%0Acompared%20to%20baseline%20methods%20while%20ensuring%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12707v1&entry.124074799=Read"},
{"title": "FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion", "author": "Jiacheng Ruan and Yebin Yang and Zehao Lin and Feiyu Xiong and Zeyun Tang and Zhiyu Li", "abstract": "  Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.\n", "link": "http://arxiv.org/abs/2410.12564v1", "date": "2024-10-16", "relevancy": 2.1064, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion&body=Title%3A%20FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion%0AAuthor%3A%20Jiacheng%20Ruan%20and%20Yebin%20Yang%20and%20Zehao%20Lin%20and%20Feiyu%20Xiong%20and%20Zeyun%20Tang%20and%20Zhiyu%20Li%0AAbstract%3A%20%20%20Benefiting%20from%20the%20revolutionary%20advances%20in%20large%20language%20models%20%28LLMs%29%0Aand%20foundational%20vision%20models%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20also%0Amade%20significant%20progress.%20However%2C%20current%20benchmarks%20focus%20on%20tasks%20that%0Aevaluating%20only%20a%20single%20aspect%20of%20LVLM%20capabilities%20%28e.g.%2C%20recognition%2C%0Adetection%2C%20understanding%29.%20These%20tasks%20fail%20to%20fully%20demonstrate%20LVLMs%27%0Apotential%20in%20complex%20application%20scenarios.%20To%20comprehensively%20assess%20the%0Aperformance%20of%20existing%20LVLMs%2C%20we%20propose%20a%20more%20challenging%20task%20called%20the%0AFlow%20Text%20with%20Image%20Insertion%20task%20%28FTII%29.%20This%20task%20requires%20LVLMs%20to%0Asimultaneously%20possess%20outstanding%20abilities%20in%20image%20comprehension%2C%0Ainstruction%20understanding%2C%20and%20long-text%20interpretation.%20Specifically%2C%20given%0Aseveral%20text%20paragraphs%20and%20a%20set%20of%20candidate%20images%2C%20as%20the%20text%20paragraphs%0Aaccumulate%2C%20the%20LVLMs%20are%20required%20to%20select%20the%20most%20suitable%20image%20from%20the%0Acandidates%20to%20insert%20after%20the%20corresponding%20paragraph.%20Constructing%20a%0Abenchmark%20for%20such%20a%20task%20is%20highly%20challenging%2C%20particularly%20in%20determining%0Athe%20sequence%20of%20flowing%20text%20and%20images.%20To%20address%20this%20challenge%2C%20we%20turn%20to%0Aprofessional%20news%20reports%2C%20which%20naturally%20contain%20a%20gold%20standard%20for%0Aimage-text%20sequences.%20Based%20on%20this%2C%20we%20introduce%20the%20Flow%20Text%20with%20Image%0AInsertion%20Benchmark%20%28FTII-Bench%29%2C%20which%20includes%20318%20high-quality%20Chinese%0Aimage-text%20news%20articles%20and%20307%20high-quality%20English%20image-text%20news%20articles%2C%0Acovering%2010%20different%20news%20domains.%20Using%20these%20625%20high-quality%20articles%2C%20we%0Aconstruct%20problems%20of%20two%20different%20types%20with%20multiple%20levels%20of%20difficulty.%0AFurthermore%2C%20we%20establish%20two%20different%20evaluation%20pipelines%20based%20on%20the%20CLIP%0Amodel%20and%20existing%20LVLMs.%20We%20evaluate%209%20open-source%20and%202%20closed-source%20LVLMs%0Aas%20well%20as%202%20CLIP-based%20models.%20Results%20indicate%20that%20even%20the%20most%20advanced%0Amodels%20%28e.g.%2C%20GPT-4o%29%20face%20significant%20challenges%20when%20tackling%20the%20FTII%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFTII-Bench%253A%2520A%2520Comprehensive%2520Multimodal%2520Benchmark%2520for%2520Flow%2520Text%2520with%250A%2520%2520Image%2520Insertion%26entry.906535625%3DJiacheng%2520Ruan%2520and%2520Yebin%2520Yang%2520and%2520Zehao%2520Lin%2520and%2520Feiyu%2520Xiong%2520and%2520Zeyun%2520Tang%2520and%2520Zhiyu%2520Li%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520the%2520revolutionary%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Aand%2520foundational%2520vision%2520models%252C%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520also%250Amade%2520significant%2520progress.%2520However%252C%2520current%2520benchmarks%2520focus%2520on%2520tasks%2520that%250Aevaluating%2520only%2520a%2520single%2520aspect%2520of%2520LVLM%2520capabilities%2520%2528e.g.%252C%2520recognition%252C%250Adetection%252C%2520understanding%2529.%2520These%2520tasks%2520fail%2520to%2520fully%2520demonstrate%2520LVLMs%2527%250Apotential%2520in%2520complex%2520application%2520scenarios.%2520To%2520comprehensively%2520assess%2520the%250Aperformance%2520of%2520existing%2520LVLMs%252C%2520we%2520propose%2520a%2520more%2520challenging%2520task%2520called%2520the%250AFlow%2520Text%2520with%2520Image%2520Insertion%2520task%2520%2528FTII%2529.%2520This%2520task%2520requires%2520LVLMs%2520to%250Asimultaneously%2520possess%2520outstanding%2520abilities%2520in%2520image%2520comprehension%252C%250Ainstruction%2520understanding%252C%2520and%2520long-text%2520interpretation.%2520Specifically%252C%2520given%250Aseveral%2520text%2520paragraphs%2520and%2520a%2520set%2520of%2520candidate%2520images%252C%2520as%2520the%2520text%2520paragraphs%250Aaccumulate%252C%2520the%2520LVLMs%2520are%2520required%2520to%2520select%2520the%2520most%2520suitable%2520image%2520from%2520the%250Acandidates%2520to%2520insert%2520after%2520the%2520corresponding%2520paragraph.%2520Constructing%2520a%250Abenchmark%2520for%2520such%2520a%2520task%2520is%2520highly%2520challenging%252C%2520particularly%2520in%2520determining%250Athe%2520sequence%2520of%2520flowing%2520text%2520and%2520images.%2520To%2520address%2520this%2520challenge%252C%2520we%2520turn%2520to%250Aprofessional%2520news%2520reports%252C%2520which%2520naturally%2520contain%2520a%2520gold%2520standard%2520for%250Aimage-text%2520sequences.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520the%2520Flow%2520Text%2520with%2520Image%250AInsertion%2520Benchmark%2520%2528FTII-Bench%2529%252C%2520which%2520includes%2520318%2520high-quality%2520Chinese%250Aimage-text%2520news%2520articles%2520and%2520307%2520high-quality%2520English%2520image-text%2520news%2520articles%252C%250Acovering%252010%2520different%2520news%2520domains.%2520Using%2520these%2520625%2520high-quality%2520articles%252C%2520we%250Aconstruct%2520problems%2520of%2520two%2520different%2520types%2520with%2520multiple%2520levels%2520of%2520difficulty.%250AFurthermore%252C%2520we%2520establish%2520two%2520different%2520evaluation%2520pipelines%2520based%2520on%2520the%2520CLIP%250Amodel%2520and%2520existing%2520LVLMs.%2520We%2520evaluate%25209%2520open-source%2520and%25202%2520closed-source%2520LVLMs%250Aas%2520well%2520as%25202%2520CLIP-based%2520models.%2520Results%2520indicate%2520that%2520even%2520the%2520most%2520advanced%250Amodels%2520%2528e.g.%252C%2520GPT-4o%2529%2520face%2520significant%2520challenges%2520when%2520tackling%2520the%2520FTII%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FTII-Bench%3A%20A%20Comprehensive%20Multimodal%20Benchmark%20for%20Flow%20Text%20with%0A%20%20Image%20Insertion&entry.906535625=Jiacheng%20Ruan%20and%20Yebin%20Yang%20and%20Zehao%20Lin%20and%20Feiyu%20Xiong%20and%20Zeyun%20Tang%20and%20Zhiyu%20Li&entry.1292438233=%20%20Benefiting%20from%20the%20revolutionary%20advances%20in%20large%20language%20models%20%28LLMs%29%0Aand%20foundational%20vision%20models%2C%20large%20vision-language%20models%20%28LVLMs%29%20have%20also%0Amade%20significant%20progress.%20However%2C%20current%20benchmarks%20focus%20on%20tasks%20that%0Aevaluating%20only%20a%20single%20aspect%20of%20LVLM%20capabilities%20%28e.g.%2C%20recognition%2C%0Adetection%2C%20understanding%29.%20These%20tasks%20fail%20to%20fully%20demonstrate%20LVLMs%27%0Apotential%20in%20complex%20application%20scenarios.%20To%20comprehensively%20assess%20the%0Aperformance%20of%20existing%20LVLMs%2C%20we%20propose%20a%20more%20challenging%20task%20called%20the%0AFlow%20Text%20with%20Image%20Insertion%20task%20%28FTII%29.%20This%20task%20requires%20LVLMs%20to%0Asimultaneously%20possess%20outstanding%20abilities%20in%20image%20comprehension%2C%0Ainstruction%20understanding%2C%20and%20long-text%20interpretation.%20Specifically%2C%20given%0Aseveral%20text%20paragraphs%20and%20a%20set%20of%20candidate%20images%2C%20as%20the%20text%20paragraphs%0Aaccumulate%2C%20the%20LVLMs%20are%20required%20to%20select%20the%20most%20suitable%20image%20from%20the%0Acandidates%20to%20insert%20after%20the%20corresponding%20paragraph.%20Constructing%20a%0Abenchmark%20for%20such%20a%20task%20is%20highly%20challenging%2C%20particularly%20in%20determining%0Athe%20sequence%20of%20flowing%20text%20and%20images.%20To%20address%20this%20challenge%2C%20we%20turn%20to%0Aprofessional%20news%20reports%2C%20which%20naturally%20contain%20a%20gold%20standard%20for%0Aimage-text%20sequences.%20Based%20on%20this%2C%20we%20introduce%20the%20Flow%20Text%20with%20Image%0AInsertion%20Benchmark%20%28FTII-Bench%29%2C%20which%20includes%20318%20high-quality%20Chinese%0Aimage-text%20news%20articles%20and%20307%20high-quality%20English%20image-text%20news%20articles%2C%0Acovering%2010%20different%20news%20domains.%20Using%20these%20625%20high-quality%20articles%2C%20we%0Aconstruct%20problems%20of%20two%20different%20types%20with%20multiple%20levels%20of%20difficulty.%0AFurthermore%2C%20we%20establish%20two%20different%20evaluation%20pipelines%20based%20on%20the%20CLIP%0Amodel%20and%20existing%20LVLMs.%20We%20evaluate%209%20open-source%20and%202%20closed-source%20LVLMs%0Aas%20well%20as%202%20CLIP-based%20models.%20Results%20indicate%20that%20even%20the%20most%20advanced%0Amodels%20%28e.g.%2C%20GPT-4o%29%20face%20significant%20challenges%20when%20tackling%20the%20FTII%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12564v1&entry.124074799=Read"},
{"title": "Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models", "author": "Wangtao Sun and Chenxiang Zhang and XueYou Zhang and Xuanqing Yu and Ziyang Huang and Pei Chen and Haotian Xu and Shizhu He and Jun Zhao and Kang Liu", "abstract": "  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n", "link": "http://arxiv.org/abs/2407.08440v3", "date": "2024-10-16", "relevancy": 2.0986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5293}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Instruction%20Following%3A%20Evaluating%20Inferential%20Rule%20Following%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Beyond%20Instruction%20Following%3A%20Evaluating%20Inferential%20Rule%20Following%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Wangtao%20Sun%20and%20Chenxiang%20Zhang%20and%20XueYou%20Zhang%20and%20Xuanqing%20Yu%20and%20Ziyang%20Huang%20and%20Pei%20Chen%20and%20Haotian%20Xu%20and%20Shizhu%20He%20and%20Jun%20Zhao%20and%20Kang%20Liu%0AAbstract%3A%20%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20ability%2C%20they%0Aare%20further%20supposed%20to%20be%20controlled%20and%20guided%20by%20in%20real-world%20scenarios%20to%0Abe%20safe%2C%20accurate%2C%20and%20intelligent.%20This%20demands%20the%20possession%20of%20capability%0Aof%20LLMs.%20However%2C%20no%20prior%20work%20has%20made%20a%20clear%20evaluation%20of%20the%20inferential%0Arule-following%20capability%20of%20LLMs.%20Previous%20studies%20that%20try%20to%20evaluate%20the%0Ainferential%20rule-following%20capability%20of%20LLMs%20fail%20to%20distinguish%20the%0Ainferential%20rule-following%20scenarios%20from%20the%20instruction-following%20scenarios.%0ATherefore%2C%20this%20paper%20first%20clarifies%20the%20concept%20of%20inferential%20rule-following%0Aand%20proposes%20a%20comprehensive%20benchmark%2C%20RuleBench%2C%20to%20evaluate%20a%20diversified%0Arange%20of%20inferential%20rule-following%20abilities.%20Our%20experimental%20results%20on%20a%0Avariety%20of%20LLMs%20show%20that%20they%20are%20still%20limited%20in%20following%20rules.%20Our%0Aanalysis%20based%20on%20the%20evaluation%20results%20provides%20insights%20into%20the%0Aimprovements%20for%20LLMs%20toward%20a%20better%20inferential%20rule-following%20intelligent%0Aagent.%20We%20further%20propose%20Inferential%20Rule-Following%20Tuning%20%28IRFT%29.%20The%0Aexperimental%20results%20show%20that%20through%20IRFT%2C%20LLMs%20can%20learn%20abstract%0Arule-following%20abilities%20from%20purely%20synthetic%20data%20and%20then%20generalize%20to%0ARuleBench.%20The%20data%20and%20code%20can%20be%20found%20at%3A%0Ahttps%3A//anonymous.4open.science/r/llm-rule-following-B3E3/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08440v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Instruction%2520Following%253A%2520Evaluating%2520Inferential%2520Rule%2520Following%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DWangtao%2520Sun%2520and%2520Chenxiang%2520Zhang%2520and%2520XueYou%2520Zhang%2520and%2520Xuanqing%2520Yu%2520and%2520Ziyang%2520Huang%2520and%2520Pei%2520Chen%2520and%2520Haotian%2520Xu%2520and%2520Shizhu%2520He%2520and%2520Jun%2520Zhao%2520and%2520Kang%2520Liu%26entry.1292438233%3D%2520%2520Although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520ability%252C%2520they%250Aare%2520further%2520supposed%2520to%2520be%2520controlled%2520and%2520guided%2520by%2520in%2520real-world%2520scenarios%2520to%250Abe%2520safe%252C%2520accurate%252C%2520and%2520intelligent.%2520This%2520demands%2520the%2520possession%2520of%2520capability%250Aof%2520LLMs.%2520However%252C%2520no%2520prior%2520work%2520has%2520made%2520a%2520clear%2520evaluation%2520of%2520the%2520inferential%250Arule-following%2520capability%2520of%2520LLMs.%2520Previous%2520studies%2520that%2520try%2520to%2520evaluate%2520the%250Ainferential%2520rule-following%2520capability%2520of%2520LLMs%2520fail%2520to%2520distinguish%2520the%250Ainferential%2520rule-following%2520scenarios%2520from%2520the%2520instruction-following%2520scenarios.%250ATherefore%252C%2520this%2520paper%2520first%2520clarifies%2520the%2520concept%2520of%2520inferential%2520rule-following%250Aand%2520proposes%2520a%2520comprehensive%2520benchmark%252C%2520RuleBench%252C%2520to%2520evaluate%2520a%2520diversified%250Arange%2520of%2520inferential%2520rule-following%2520abilities.%2520Our%2520experimental%2520results%2520on%2520a%250Avariety%2520of%2520LLMs%2520show%2520that%2520they%2520are%2520still%2520limited%2520in%2520following%2520rules.%2520Our%250Aanalysis%2520based%2520on%2520the%2520evaluation%2520results%2520provides%2520insights%2520into%2520the%250Aimprovements%2520for%2520LLMs%2520toward%2520a%2520better%2520inferential%2520rule-following%2520intelligent%250Aagent.%2520We%2520further%2520propose%2520Inferential%2520Rule-Following%2520Tuning%2520%2528IRFT%2529.%2520The%250Aexperimental%2520results%2520show%2520that%2520through%2520IRFT%252C%2520LLMs%2520can%2520learn%2520abstract%250Arule-following%2520abilities%2520from%2520purely%2520synthetic%2520data%2520and%2520then%2520generalize%2520to%250ARuleBench.%2520The%2520data%2520and%2520code%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/llm-rule-following-B3E3/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08440v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Instruction%20Following%3A%20Evaluating%20Inferential%20Rule%20Following%20of%0A%20%20Large%20Language%20Models&entry.906535625=Wangtao%20Sun%20and%20Chenxiang%20Zhang%20and%20XueYou%20Zhang%20and%20Xuanqing%20Yu%20and%20Ziyang%20Huang%20and%20Pei%20Chen%20and%20Haotian%20Xu%20and%20Shizhu%20He%20and%20Jun%20Zhao%20and%20Kang%20Liu&entry.1292438233=%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20ability%2C%20they%0Aare%20further%20supposed%20to%20be%20controlled%20and%20guided%20by%20in%20real-world%20scenarios%20to%0Abe%20safe%2C%20accurate%2C%20and%20intelligent.%20This%20demands%20the%20possession%20of%20capability%0Aof%20LLMs.%20However%2C%20no%20prior%20work%20has%20made%20a%20clear%20evaluation%20of%20the%20inferential%0Arule-following%20capability%20of%20LLMs.%20Previous%20studies%20that%20try%20to%20evaluate%20the%0Ainferential%20rule-following%20capability%20of%20LLMs%20fail%20to%20distinguish%20the%0Ainferential%20rule-following%20scenarios%20from%20the%20instruction-following%20scenarios.%0ATherefore%2C%20this%20paper%20first%20clarifies%20the%20concept%20of%20inferential%20rule-following%0Aand%20proposes%20a%20comprehensive%20benchmark%2C%20RuleBench%2C%20to%20evaluate%20a%20diversified%0Arange%20of%20inferential%20rule-following%20abilities.%20Our%20experimental%20results%20on%20a%0Avariety%20of%20LLMs%20show%20that%20they%20are%20still%20limited%20in%20following%20rules.%20Our%0Aanalysis%20based%20on%20the%20evaluation%20results%20provides%20insights%20into%20the%0Aimprovements%20for%20LLMs%20toward%20a%20better%20inferential%20rule-following%20intelligent%0Aagent.%20We%20further%20propose%20Inferential%20Rule-Following%20Tuning%20%28IRFT%29.%20The%0Aexperimental%20results%20show%20that%20through%20IRFT%2C%20LLMs%20can%20learn%20abstract%0Arule-following%20abilities%20from%20purely%20synthetic%20data%20and%20then%20generalize%20to%0ARuleBench.%20The%20data%20and%20code%20can%20be%20found%20at%3A%0Ahttps%3A//anonymous.4open.science/r/llm-rule-following-B3E3/%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08440v3&entry.124074799=Read"},
{"title": "Hamiltonian bridge: A physics-driven generative framework for targeted\n  pattern control", "author": "Vishaal Krishnan and Sumit Sinha and L. Mahadevan", "abstract": "  Patterns arise spontaneously in a range of systems spanning the sciences, and\ntheir study typically focuses on mechanisms to understand their evolution in\nspace-time. Increasingly, there has been a transition towards controlling these\npatterns in various functional settings, with implications for engineering.\nHere, we combine our knowledge of a general class of dynamical laws for pattern\nformation in non-equilibrium systems, and the power of stochastic optimal\ncontrol approaches to present a framework that allows us to control patterns at\nmultiple scales, which we dub the \"Hamiltonian bridge\". We use a mapping\nbetween stochastic many-body Lagrangian physics and deterministic Eulerian\npattern forming PDEs to leverage our recent approach utilizing the\nFeynman-Kac-based adjoint path integral formulation for the control of\ninteracting particles and generalize this to the active control of patterning\nfields. We demonstrate the applicability of our computational framework via\nnumerical experiments on the control of phase separation with and without a\nconserved order parameter, self-assembly of fluid droplets, coupled\nreaction-diffusion equations and finally a phenomenological model for\nspatio-temporal tissue differentiation. We interpret our numerical experiments\nin terms of a theoretical understanding of how the underlying physics shapes\nthe geometry of the pattern manifold, altering the transport paths of patterns\nand the nature of pattern interpolation. We finally conclude by showing how\noptimal control can be utilized to generate complex patterns via an iterative\ncontrol protocol over pattern forming pdes which can be casted as gradient\nflows. All together, our study shows how we can systematically build in\nphysical priors into a generative framework for pattern control in\nnon-equilibrium systems across multiple length and time scales.\n", "link": "http://arxiv.org/abs/2410.12665v1", "date": "2024-10-16", "relevancy": 2.0964, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5425}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5208}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hamiltonian%20bridge%3A%20A%20physics-driven%20generative%20framework%20for%20targeted%0A%20%20pattern%20control&body=Title%3A%20Hamiltonian%20bridge%3A%20A%20physics-driven%20generative%20framework%20for%20targeted%0A%20%20pattern%20control%0AAuthor%3A%20Vishaal%20Krishnan%20and%20Sumit%20Sinha%20and%20L.%20Mahadevan%0AAbstract%3A%20%20%20Patterns%20arise%20spontaneously%20in%20a%20range%20of%20systems%20spanning%20the%20sciences%2C%20and%0Atheir%20study%20typically%20focuses%20on%20mechanisms%20to%20understand%20their%20evolution%20in%0Aspace-time.%20Increasingly%2C%20there%20has%20been%20a%20transition%20towards%20controlling%20these%0Apatterns%20in%20various%20functional%20settings%2C%20with%20implications%20for%20engineering.%0AHere%2C%20we%20combine%20our%20knowledge%20of%20a%20general%20class%20of%20dynamical%20laws%20for%20pattern%0Aformation%20in%20non-equilibrium%20systems%2C%20and%20the%20power%20of%20stochastic%20optimal%0Acontrol%20approaches%20to%20present%20a%20framework%20that%20allows%20us%20to%20control%20patterns%20at%0Amultiple%20scales%2C%20which%20we%20dub%20the%20%22Hamiltonian%20bridge%22.%20We%20use%20a%20mapping%0Abetween%20stochastic%20many-body%20Lagrangian%20physics%20and%20deterministic%20Eulerian%0Apattern%20forming%20PDEs%20to%20leverage%20our%20recent%20approach%20utilizing%20the%0AFeynman-Kac-based%20adjoint%20path%20integral%20formulation%20for%20the%20control%20of%0Ainteracting%20particles%20and%20generalize%20this%20to%20the%20active%20control%20of%20patterning%0Afields.%20We%20demonstrate%20the%20applicability%20of%20our%20computational%20framework%20via%0Anumerical%20experiments%20on%20the%20control%20of%20phase%20separation%20with%20and%20without%20a%0Aconserved%20order%20parameter%2C%20self-assembly%20of%20fluid%20droplets%2C%20coupled%0Areaction-diffusion%20equations%20and%20finally%20a%20phenomenological%20model%20for%0Aspatio-temporal%20tissue%20differentiation.%20We%20interpret%20our%20numerical%20experiments%0Ain%20terms%20of%20a%20theoretical%20understanding%20of%20how%20the%20underlying%20physics%20shapes%0Athe%20geometry%20of%20the%20pattern%20manifold%2C%20altering%20the%20transport%20paths%20of%20patterns%0Aand%20the%20nature%20of%20pattern%20interpolation.%20We%20finally%20conclude%20by%20showing%20how%0Aoptimal%20control%20can%20be%20utilized%20to%20generate%20complex%20patterns%20via%20an%20iterative%0Acontrol%20protocol%20over%20pattern%20forming%20pdes%20which%20can%20be%20casted%20as%20gradient%0Aflows.%20All%20together%2C%20our%20study%20shows%20how%20we%20can%20systematically%20build%20in%0Aphysical%20priors%20into%20a%20generative%20framework%20for%20pattern%20control%20in%0Anon-equilibrium%20systems%20across%20multiple%20length%20and%20time%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHamiltonian%2520bridge%253A%2520A%2520physics-driven%2520generative%2520framework%2520for%2520targeted%250A%2520%2520pattern%2520control%26entry.906535625%3DVishaal%2520Krishnan%2520and%2520Sumit%2520Sinha%2520and%2520L.%2520Mahadevan%26entry.1292438233%3D%2520%2520Patterns%2520arise%2520spontaneously%2520in%2520a%2520range%2520of%2520systems%2520spanning%2520the%2520sciences%252C%2520and%250Atheir%2520study%2520typically%2520focuses%2520on%2520mechanisms%2520to%2520understand%2520their%2520evolution%2520in%250Aspace-time.%2520Increasingly%252C%2520there%2520has%2520been%2520a%2520transition%2520towards%2520controlling%2520these%250Apatterns%2520in%2520various%2520functional%2520settings%252C%2520with%2520implications%2520for%2520engineering.%250AHere%252C%2520we%2520combine%2520our%2520knowledge%2520of%2520a%2520general%2520class%2520of%2520dynamical%2520laws%2520for%2520pattern%250Aformation%2520in%2520non-equilibrium%2520systems%252C%2520and%2520the%2520power%2520of%2520stochastic%2520optimal%250Acontrol%2520approaches%2520to%2520present%2520a%2520framework%2520that%2520allows%2520us%2520to%2520control%2520patterns%2520at%250Amultiple%2520scales%252C%2520which%2520we%2520dub%2520the%2520%2522Hamiltonian%2520bridge%2522.%2520We%2520use%2520a%2520mapping%250Abetween%2520stochastic%2520many-body%2520Lagrangian%2520physics%2520and%2520deterministic%2520Eulerian%250Apattern%2520forming%2520PDEs%2520to%2520leverage%2520our%2520recent%2520approach%2520utilizing%2520the%250AFeynman-Kac-based%2520adjoint%2520path%2520integral%2520formulation%2520for%2520the%2520control%2520of%250Ainteracting%2520particles%2520and%2520generalize%2520this%2520to%2520the%2520active%2520control%2520of%2520patterning%250Afields.%2520We%2520demonstrate%2520the%2520applicability%2520of%2520our%2520computational%2520framework%2520via%250Anumerical%2520experiments%2520on%2520the%2520control%2520of%2520phase%2520separation%2520with%2520and%2520without%2520a%250Aconserved%2520order%2520parameter%252C%2520self-assembly%2520of%2520fluid%2520droplets%252C%2520coupled%250Areaction-diffusion%2520equations%2520and%2520finally%2520a%2520phenomenological%2520model%2520for%250Aspatio-temporal%2520tissue%2520differentiation.%2520We%2520interpret%2520our%2520numerical%2520experiments%250Ain%2520terms%2520of%2520a%2520theoretical%2520understanding%2520of%2520how%2520the%2520underlying%2520physics%2520shapes%250Athe%2520geometry%2520of%2520the%2520pattern%2520manifold%252C%2520altering%2520the%2520transport%2520paths%2520of%2520patterns%250Aand%2520the%2520nature%2520of%2520pattern%2520interpolation.%2520We%2520finally%2520conclude%2520by%2520showing%2520how%250Aoptimal%2520control%2520can%2520be%2520utilized%2520to%2520generate%2520complex%2520patterns%2520via%2520an%2520iterative%250Acontrol%2520protocol%2520over%2520pattern%2520forming%2520pdes%2520which%2520can%2520be%2520casted%2520as%2520gradient%250Aflows.%2520All%2520together%252C%2520our%2520study%2520shows%2520how%2520we%2520can%2520systematically%2520build%2520in%250Aphysical%2520priors%2520into%2520a%2520generative%2520framework%2520for%2520pattern%2520control%2520in%250Anon-equilibrium%2520systems%2520across%2520multiple%2520length%2520and%2520time%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hamiltonian%20bridge%3A%20A%20physics-driven%20generative%20framework%20for%20targeted%0A%20%20pattern%20control&entry.906535625=Vishaal%20Krishnan%20and%20Sumit%20Sinha%20and%20L.%20Mahadevan&entry.1292438233=%20%20Patterns%20arise%20spontaneously%20in%20a%20range%20of%20systems%20spanning%20the%20sciences%2C%20and%0Atheir%20study%20typically%20focuses%20on%20mechanisms%20to%20understand%20their%20evolution%20in%0Aspace-time.%20Increasingly%2C%20there%20has%20been%20a%20transition%20towards%20controlling%20these%0Apatterns%20in%20various%20functional%20settings%2C%20with%20implications%20for%20engineering.%0AHere%2C%20we%20combine%20our%20knowledge%20of%20a%20general%20class%20of%20dynamical%20laws%20for%20pattern%0Aformation%20in%20non-equilibrium%20systems%2C%20and%20the%20power%20of%20stochastic%20optimal%0Acontrol%20approaches%20to%20present%20a%20framework%20that%20allows%20us%20to%20control%20patterns%20at%0Amultiple%20scales%2C%20which%20we%20dub%20the%20%22Hamiltonian%20bridge%22.%20We%20use%20a%20mapping%0Abetween%20stochastic%20many-body%20Lagrangian%20physics%20and%20deterministic%20Eulerian%0Apattern%20forming%20PDEs%20to%20leverage%20our%20recent%20approach%20utilizing%20the%0AFeynman-Kac-based%20adjoint%20path%20integral%20formulation%20for%20the%20control%20of%0Ainteracting%20particles%20and%20generalize%20this%20to%20the%20active%20control%20of%20patterning%0Afields.%20We%20demonstrate%20the%20applicability%20of%20our%20computational%20framework%20via%0Anumerical%20experiments%20on%20the%20control%20of%20phase%20separation%20with%20and%20without%20a%0Aconserved%20order%20parameter%2C%20self-assembly%20of%20fluid%20droplets%2C%20coupled%0Areaction-diffusion%20equations%20and%20finally%20a%20phenomenological%20model%20for%0Aspatio-temporal%20tissue%20differentiation.%20We%20interpret%20our%20numerical%20experiments%0Ain%20terms%20of%20a%20theoretical%20understanding%20of%20how%20the%20underlying%20physics%20shapes%0Athe%20geometry%20of%20the%20pattern%20manifold%2C%20altering%20the%20transport%20paths%20of%20patterns%0Aand%20the%20nature%20of%20pattern%20interpolation.%20We%20finally%20conclude%20by%20showing%20how%0Aoptimal%20control%20can%20be%20utilized%20to%20generate%20complex%20patterns%20via%20an%20iterative%0Acontrol%20protocol%20over%20pattern%20forming%20pdes%20which%20can%20be%20casted%20as%20gradient%0Aflows.%20All%20together%2C%20our%20study%20shows%20how%20we%20can%20systematically%20build%20in%0Aphysical%20priors%20into%20a%20generative%20framework%20for%20pattern%20control%20in%0Anon-equilibrium%20systems%20across%20multiple%20length%20and%20time%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12665v1&entry.124074799=Read"},
{"title": "Scalable Structure Learning for Sparse Context-Specific Systems", "author": "Felix Leopoldo Rios and Alex Markham and Liam Solus", "abstract": "  Several approaches to graphically representing context-specific relations\namong jointly distributed categorical variables have been proposed, along with\nstructure learning algorithms. While existing optimization-based methods have\nlimited scalability due to the large number of context-specific models, the\nconstraint-based methods are more prone to error than even constraint-based\ndirected acyclic graph learning algorithms since more relations must be tested.\nWe present an algorithm for learning context-specific models that scales to\nhundreds of variables. Scalable learning is achieved through a combination of\nan order-based Markov chain Monte-Carlo search and a novel, context-specific\nsparsity assumption that is analogous to those typically invoked for directed\nacyclic graphical models. Unlike previous Markov chain Monte-Carlo search\nmethods, our Markov chain is guaranteed to have the true posterior of the\nvariable orderings as the stationary distribution. To implement the method, we\nsolve a first case of an open problem recently posed by Alon and Balogh. Future\nwork solving increasingly general instances of this problem would allow our\nmethods to learn increasingly dense models. The method is shown to perform well\non synthetic data and real world examples, in terms of both accuracy and\nscalability.\n", "link": "http://arxiv.org/abs/2402.07762v2", "date": "2024-10-16", "relevancy": 2.0909, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5412}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Structure%20Learning%20for%20Sparse%20Context-Specific%20Systems&body=Title%3A%20Scalable%20Structure%20Learning%20for%20Sparse%20Context-Specific%20Systems%0AAuthor%3A%20Felix%20Leopoldo%20Rios%20and%20Alex%20Markham%20and%20Liam%20Solus%0AAbstract%3A%20%20%20Several%20approaches%20to%20graphically%20representing%20context-specific%20relations%0Aamong%20jointly%20distributed%20categorical%20variables%20have%20been%20proposed%2C%20along%20with%0Astructure%20learning%20algorithms.%20While%20existing%20optimization-based%20methods%20have%0Alimited%20scalability%20due%20to%20the%20large%20number%20of%20context-specific%20models%2C%20the%0Aconstraint-based%20methods%20are%20more%20prone%20to%20error%20than%20even%20constraint-based%0Adirected%20acyclic%20graph%20learning%20algorithms%20since%20more%20relations%20must%20be%20tested.%0AWe%20present%20an%20algorithm%20for%20learning%20context-specific%20models%20that%20scales%20to%0Ahundreds%20of%20variables.%20Scalable%20learning%20is%20achieved%20through%20a%20combination%20of%0Aan%20order-based%20Markov%20chain%20Monte-Carlo%20search%20and%20a%20novel%2C%20context-specific%0Asparsity%20assumption%20that%20is%20analogous%20to%20those%20typically%20invoked%20for%20directed%0Aacyclic%20graphical%20models.%20Unlike%20previous%20Markov%20chain%20Monte-Carlo%20search%0Amethods%2C%20our%20Markov%20chain%20is%20guaranteed%20to%20have%20the%20true%20posterior%20of%20the%0Avariable%20orderings%20as%20the%20stationary%20distribution.%20To%20implement%20the%20method%2C%20we%0Asolve%20a%20first%20case%20of%20an%20open%20problem%20recently%20posed%20by%20Alon%20and%20Balogh.%20Future%0Awork%20solving%20increasingly%20general%20instances%20of%20this%20problem%20would%20allow%20our%0Amethods%20to%20learn%20increasingly%20dense%20models.%20The%20method%20is%20shown%20to%20perform%20well%0Aon%20synthetic%20data%20and%20real%20world%20examples%2C%20in%20terms%20of%20both%20accuracy%20and%0Ascalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Structure%2520Learning%2520for%2520Sparse%2520Context-Specific%2520Systems%26entry.906535625%3DFelix%2520Leopoldo%2520Rios%2520and%2520Alex%2520Markham%2520and%2520Liam%2520Solus%26entry.1292438233%3D%2520%2520Several%2520approaches%2520to%2520graphically%2520representing%2520context-specific%2520relations%250Aamong%2520jointly%2520distributed%2520categorical%2520variables%2520have%2520been%2520proposed%252C%2520along%2520with%250Astructure%2520learning%2520algorithms.%2520While%2520existing%2520optimization-based%2520methods%2520have%250Alimited%2520scalability%2520due%2520to%2520the%2520large%2520number%2520of%2520context-specific%2520models%252C%2520the%250Aconstraint-based%2520methods%2520are%2520more%2520prone%2520to%2520error%2520than%2520even%2520constraint-based%250Adirected%2520acyclic%2520graph%2520learning%2520algorithms%2520since%2520more%2520relations%2520must%2520be%2520tested.%250AWe%2520present%2520an%2520algorithm%2520for%2520learning%2520context-specific%2520models%2520that%2520scales%2520to%250Ahundreds%2520of%2520variables.%2520Scalable%2520learning%2520is%2520achieved%2520through%2520a%2520combination%2520of%250Aan%2520order-based%2520Markov%2520chain%2520Monte-Carlo%2520search%2520and%2520a%2520novel%252C%2520context-specific%250Asparsity%2520assumption%2520that%2520is%2520analogous%2520to%2520those%2520typically%2520invoked%2520for%2520directed%250Aacyclic%2520graphical%2520models.%2520Unlike%2520previous%2520Markov%2520chain%2520Monte-Carlo%2520search%250Amethods%252C%2520our%2520Markov%2520chain%2520is%2520guaranteed%2520to%2520have%2520the%2520true%2520posterior%2520of%2520the%250Avariable%2520orderings%2520as%2520the%2520stationary%2520distribution.%2520To%2520implement%2520the%2520method%252C%2520we%250Asolve%2520a%2520first%2520case%2520of%2520an%2520open%2520problem%2520recently%2520posed%2520by%2520Alon%2520and%2520Balogh.%2520Future%250Awork%2520solving%2520increasingly%2520general%2520instances%2520of%2520this%2520problem%2520would%2520allow%2520our%250Amethods%2520to%2520learn%2520increasingly%2520dense%2520models.%2520The%2520method%2520is%2520shown%2520to%2520perform%2520well%250Aon%2520synthetic%2520data%2520and%2520real%2520world%2520examples%252C%2520in%2520terms%2520of%2520both%2520accuracy%2520and%250Ascalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Structure%20Learning%20for%20Sparse%20Context-Specific%20Systems&entry.906535625=Felix%20Leopoldo%20Rios%20and%20Alex%20Markham%20and%20Liam%20Solus&entry.1292438233=%20%20Several%20approaches%20to%20graphically%20representing%20context-specific%20relations%0Aamong%20jointly%20distributed%20categorical%20variables%20have%20been%20proposed%2C%20along%20with%0Astructure%20learning%20algorithms.%20While%20existing%20optimization-based%20methods%20have%0Alimited%20scalability%20due%20to%20the%20large%20number%20of%20context-specific%20models%2C%20the%0Aconstraint-based%20methods%20are%20more%20prone%20to%20error%20than%20even%20constraint-based%0Adirected%20acyclic%20graph%20learning%20algorithms%20since%20more%20relations%20must%20be%20tested.%0AWe%20present%20an%20algorithm%20for%20learning%20context-specific%20models%20that%20scales%20to%0Ahundreds%20of%20variables.%20Scalable%20learning%20is%20achieved%20through%20a%20combination%20of%0Aan%20order-based%20Markov%20chain%20Monte-Carlo%20search%20and%20a%20novel%2C%20context-specific%0Asparsity%20assumption%20that%20is%20analogous%20to%20those%20typically%20invoked%20for%20directed%0Aacyclic%20graphical%20models.%20Unlike%20previous%20Markov%20chain%20Monte-Carlo%20search%0Amethods%2C%20our%20Markov%20chain%20is%20guaranteed%20to%20have%20the%20true%20posterior%20of%20the%0Avariable%20orderings%20as%20the%20stationary%20distribution.%20To%20implement%20the%20method%2C%20we%0Asolve%20a%20first%20case%20of%20an%20open%20problem%20recently%20posed%20by%20Alon%20and%20Balogh.%20Future%0Awork%20solving%20increasingly%20general%20instances%20of%20this%20problem%20would%20allow%20our%0Amethods%20to%20learn%20increasingly%20dense%20models.%20The%20method%20is%20shown%20to%20perform%20well%0Aon%20synthetic%20data%20and%20real%20world%20examples%2C%20in%20terms%20of%20both%20accuracy%20and%0Ascalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07762v2&entry.124074799=Read"},
{"title": "On the Utility of Domain Modeling Assistance with Large Language Models", "author": "Meriem Ben Chaaben and Lola Burgue\u00f1o and Istvan David and Houari Sahraoui", "abstract": "  Model-driven engineering (MDE) simplifies software development through\nabstraction, yet challenges such as time constraints, incomplete domain\nunderstanding, and adherence to syntactic constraints hinder the design\nprocess. This paper presents a study to evaluate the usefulness of a novel\napproach utilizing large language models (LLMs) and few-shot prompt learning to\nassist in domain modeling. The aim of this approach is to overcome the need for\nextensive training of AI-based completion models on scarce domain-specific\ndatasets and to offer versatile support for various modeling activities,\nproviding valuable recommendations to software modelers. To support this\napproach, we developed MAGDA, a user-friendly tool, through which we conduct a\nuser study and assess the real-world applicability of our approach in the\ncontext of domain modeling, offering valuable insights into its usability and\neffectiveness.\n", "link": "http://arxiv.org/abs/2410.12577v1", "date": "2024-10-16", "relevancy": 2.0822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Utility%20of%20Domain%20Modeling%20Assistance%20with%20Large%20Language%20Models&body=Title%3A%20On%20the%20Utility%20of%20Domain%20Modeling%20Assistance%20with%20Large%20Language%20Models%0AAuthor%3A%20Meriem%20Ben%20Chaaben%20and%20Lola%20Burgue%C3%B1o%20and%20Istvan%20David%20and%20Houari%20Sahraoui%0AAbstract%3A%20%20%20Model-driven%20engineering%20%28MDE%29%20simplifies%20software%20development%20through%0Aabstraction%2C%20yet%20challenges%20such%20as%20time%20constraints%2C%20incomplete%20domain%0Aunderstanding%2C%20and%20adherence%20to%20syntactic%20constraints%20hinder%20the%20design%0Aprocess.%20This%20paper%20presents%20a%20study%20to%20evaluate%20the%20usefulness%20of%20a%20novel%0Aapproach%20utilizing%20large%20language%20models%20%28LLMs%29%20and%20few-shot%20prompt%20learning%20to%0Aassist%20in%20domain%20modeling.%20The%20aim%20of%20this%20approach%20is%20to%20overcome%20the%20need%20for%0Aextensive%20training%20of%20AI-based%20completion%20models%20on%20scarce%20domain-specific%0Adatasets%20and%20to%20offer%20versatile%20support%20for%20various%20modeling%20activities%2C%0Aproviding%20valuable%20recommendations%20to%20software%20modelers.%20To%20support%20this%0Aapproach%2C%20we%20developed%20MAGDA%2C%20a%20user-friendly%20tool%2C%20through%20which%20we%20conduct%20a%0Auser%20study%20and%20assess%20the%20real-world%20applicability%20of%20our%20approach%20in%20the%0Acontext%20of%20domain%20modeling%2C%20offering%20valuable%20insights%20into%20its%20usability%20and%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Utility%2520of%2520Domain%2520Modeling%2520Assistance%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DMeriem%2520Ben%2520Chaaben%2520and%2520Lola%2520Burgue%25C3%25B1o%2520and%2520Istvan%2520David%2520and%2520Houari%2520Sahraoui%26entry.1292438233%3D%2520%2520Model-driven%2520engineering%2520%2528MDE%2529%2520simplifies%2520software%2520development%2520through%250Aabstraction%252C%2520yet%2520challenges%2520such%2520as%2520time%2520constraints%252C%2520incomplete%2520domain%250Aunderstanding%252C%2520and%2520adherence%2520to%2520syntactic%2520constraints%2520hinder%2520the%2520design%250Aprocess.%2520This%2520paper%2520presents%2520a%2520study%2520to%2520evaluate%2520the%2520usefulness%2520of%2520a%2520novel%250Aapproach%2520utilizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520few-shot%2520prompt%2520learning%2520to%250Aassist%2520in%2520domain%2520modeling.%2520The%2520aim%2520of%2520this%2520approach%2520is%2520to%2520overcome%2520the%2520need%2520for%250Aextensive%2520training%2520of%2520AI-based%2520completion%2520models%2520on%2520scarce%2520domain-specific%250Adatasets%2520and%2520to%2520offer%2520versatile%2520support%2520for%2520various%2520modeling%2520activities%252C%250Aproviding%2520valuable%2520recommendations%2520to%2520software%2520modelers.%2520To%2520support%2520this%250Aapproach%252C%2520we%2520developed%2520MAGDA%252C%2520a%2520user-friendly%2520tool%252C%2520through%2520which%2520we%2520conduct%2520a%250Auser%2520study%2520and%2520assess%2520the%2520real-world%2520applicability%2520of%2520our%2520approach%2520in%2520the%250Acontext%2520of%2520domain%2520modeling%252C%2520offering%2520valuable%2520insights%2520into%2520its%2520usability%2520and%250Aeffectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Utility%20of%20Domain%20Modeling%20Assistance%20with%20Large%20Language%20Models&entry.906535625=Meriem%20Ben%20Chaaben%20and%20Lola%20Burgue%C3%B1o%20and%20Istvan%20David%20and%20Houari%20Sahraoui&entry.1292438233=%20%20Model-driven%20engineering%20%28MDE%29%20simplifies%20software%20development%20through%0Aabstraction%2C%20yet%20challenges%20such%20as%20time%20constraints%2C%20incomplete%20domain%0Aunderstanding%2C%20and%20adherence%20to%20syntactic%20constraints%20hinder%20the%20design%0Aprocess.%20This%20paper%20presents%20a%20study%20to%20evaluate%20the%20usefulness%20of%20a%20novel%0Aapproach%20utilizing%20large%20language%20models%20%28LLMs%29%20and%20few-shot%20prompt%20learning%20to%0Aassist%20in%20domain%20modeling.%20The%20aim%20of%20this%20approach%20is%20to%20overcome%20the%20need%20for%0Aextensive%20training%20of%20AI-based%20completion%20models%20on%20scarce%20domain-specific%0Adatasets%20and%20to%20offer%20versatile%20support%20for%20various%20modeling%20activities%2C%0Aproviding%20valuable%20recommendations%20to%20software%20modelers.%20To%20support%20this%0Aapproach%2C%20we%20developed%20MAGDA%2C%20a%20user-friendly%20tool%2C%20through%20which%20we%20conduct%20a%0Auser%20study%20and%20assess%20the%20real-world%20applicability%20of%20our%20approach%20in%20the%0Acontext%20of%20domain%20modeling%2C%20offering%20valuable%20insights%20into%20its%20usability%20and%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12577v1&entry.124074799=Read"},
{"title": "Faster Algorithms for Growing Collision-Free Convex Polytopes in Robot\n  Configuration Space", "author": "Peter Werner and Thomas Cohn and Rebecca H. Jiang and Tim Seyde and Max Simchowitz and Russ Tedrake and Daniela Rus", "abstract": "  We propose two novel algorithms for constructing convex collision-free\npolytopes in robot configuration space. Finding these polytopes enables the\napplication of stronger motion-planning frameworks such as trajectory\noptimization with Graphs of Convex Sets [1] and is currently a major roadblock\nin the adoption of these approaches. In this paper, we build upon IRIS-NP\n(Iterative Regional Inflation by Semidefinite & Nonlinear Programming) [2] to\nsignificantly improve tunability, runtimes, and scaling to complex\nenvironments. IRIS-NP uses nonlinear programming paired with uniform random\ninitialization to find configurations on the boundary of the free configuration\nspace. Our key insight is that finding near-by configuration-space obstacles\nusing sampling is inexpensive and greatly accelerates region generation. We\npropose two algorithms using such samples to either employ nonlinear\nprogramming more efficiently (IRIS-NP2 ) or circumvent it altogether using a\nmassively-parallel zero-order optimization strategy (IRIS-ZO). We also propose\na termination condition that controls the probability of exceeding a\nuser-specified permissible fraction-in-collision, eliminating a significant\nsource of tuning difficulty in IRIS-NP. We compare performance across eight\nrobot environments, showing that IRIS-ZO achieves an order-of-magnitude speed\nadvantage over IRIS-NP. IRISNP2, also significantly faster than IRIS-NP, builds\nlarger polytopes using fewer hyperplanes, enabling faster downstream\ncomputation. Website: https://sites.google.com/view/fastiris\n", "link": "http://arxiv.org/abs/2410.12649v1", "date": "2024-10-16", "relevancy": 2.0746, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5249}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5181}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Algorithms%20for%20Growing%20Collision-Free%20Convex%20Polytopes%20in%20Robot%0A%20%20Configuration%20Space&body=Title%3A%20Faster%20Algorithms%20for%20Growing%20Collision-Free%20Convex%20Polytopes%20in%20Robot%0A%20%20Configuration%20Space%0AAuthor%3A%20Peter%20Werner%20and%20Thomas%20Cohn%20and%20Rebecca%20H.%20Jiang%20and%20Tim%20Seyde%20and%20Max%20Simchowitz%20and%20Russ%20Tedrake%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20We%20propose%20two%20novel%20algorithms%20for%20constructing%20convex%20collision-free%0Apolytopes%20in%20robot%20configuration%20space.%20Finding%20these%20polytopes%20enables%20the%0Aapplication%20of%20stronger%20motion-planning%20frameworks%20such%20as%20trajectory%0Aoptimization%20with%20Graphs%20of%20Convex%20Sets%20%5B1%5D%20and%20is%20currently%20a%20major%20roadblock%0Ain%20the%20adoption%20of%20these%20approaches.%20In%20this%20paper%2C%20we%20build%20upon%20IRIS-NP%0A%28Iterative%20Regional%20Inflation%20by%20Semidefinite%20%26%20Nonlinear%20Programming%29%20%5B2%5D%20to%0Asignificantly%20improve%20tunability%2C%20runtimes%2C%20and%20scaling%20to%20complex%0Aenvironments.%20IRIS-NP%20uses%20nonlinear%20programming%20paired%20with%20uniform%20random%0Ainitialization%20to%20find%20configurations%20on%20the%20boundary%20of%20the%20free%20configuration%0Aspace.%20Our%20key%20insight%20is%20that%20finding%20near-by%20configuration-space%20obstacles%0Ausing%20sampling%20is%20inexpensive%20and%20greatly%20accelerates%20region%20generation.%20We%0Apropose%20two%20algorithms%20using%20such%20samples%20to%20either%20employ%20nonlinear%0Aprogramming%20more%20efficiently%20%28IRIS-NP2%20%29%20or%20circumvent%20it%20altogether%20using%20a%0Amassively-parallel%20zero-order%20optimization%20strategy%20%28IRIS-ZO%29.%20We%20also%20propose%0Aa%20termination%20condition%20that%20controls%20the%20probability%20of%20exceeding%20a%0Auser-specified%20permissible%20fraction-in-collision%2C%20eliminating%20a%20significant%0Asource%20of%20tuning%20difficulty%20in%20IRIS-NP.%20We%20compare%20performance%20across%20eight%0Arobot%20environments%2C%20showing%20that%20IRIS-ZO%20achieves%20an%20order-of-magnitude%20speed%0Aadvantage%20over%20IRIS-NP.%20IRISNP2%2C%20also%20significantly%20faster%20than%20IRIS-NP%2C%20builds%0Alarger%20polytopes%20using%20fewer%20hyperplanes%2C%20enabling%20faster%20downstream%0Acomputation.%20Website%3A%20https%3A//sites.google.com/view/fastiris%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Algorithms%2520for%2520Growing%2520Collision-Free%2520Convex%2520Polytopes%2520in%2520Robot%250A%2520%2520Configuration%2520Space%26entry.906535625%3DPeter%2520Werner%2520and%2520Thomas%2520Cohn%2520and%2520Rebecca%2520H.%2520Jiang%2520and%2520Tim%2520Seyde%2520and%2520Max%2520Simchowitz%2520and%2520Russ%2520Tedrake%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520We%2520propose%2520two%2520novel%2520algorithms%2520for%2520constructing%2520convex%2520collision-free%250Apolytopes%2520in%2520robot%2520configuration%2520space.%2520Finding%2520these%2520polytopes%2520enables%2520the%250Aapplication%2520of%2520stronger%2520motion-planning%2520frameworks%2520such%2520as%2520trajectory%250Aoptimization%2520with%2520Graphs%2520of%2520Convex%2520Sets%2520%255B1%255D%2520and%2520is%2520currently%2520a%2520major%2520roadblock%250Ain%2520the%2520adoption%2520of%2520these%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520build%2520upon%2520IRIS-NP%250A%2528Iterative%2520Regional%2520Inflation%2520by%2520Semidefinite%2520%2526%2520Nonlinear%2520Programming%2529%2520%255B2%255D%2520to%250Asignificantly%2520improve%2520tunability%252C%2520runtimes%252C%2520and%2520scaling%2520to%2520complex%250Aenvironments.%2520IRIS-NP%2520uses%2520nonlinear%2520programming%2520paired%2520with%2520uniform%2520random%250Ainitialization%2520to%2520find%2520configurations%2520on%2520the%2520boundary%2520of%2520the%2520free%2520configuration%250Aspace.%2520Our%2520key%2520insight%2520is%2520that%2520finding%2520near-by%2520configuration-space%2520obstacles%250Ausing%2520sampling%2520is%2520inexpensive%2520and%2520greatly%2520accelerates%2520region%2520generation.%2520We%250Apropose%2520two%2520algorithms%2520using%2520such%2520samples%2520to%2520either%2520employ%2520nonlinear%250Aprogramming%2520more%2520efficiently%2520%2528IRIS-NP2%2520%2529%2520or%2520circumvent%2520it%2520altogether%2520using%2520a%250Amassively-parallel%2520zero-order%2520optimization%2520strategy%2520%2528IRIS-ZO%2529.%2520We%2520also%2520propose%250Aa%2520termination%2520condition%2520that%2520controls%2520the%2520probability%2520of%2520exceeding%2520a%250Auser-specified%2520permissible%2520fraction-in-collision%252C%2520eliminating%2520a%2520significant%250Asource%2520of%2520tuning%2520difficulty%2520in%2520IRIS-NP.%2520We%2520compare%2520performance%2520across%2520eight%250Arobot%2520environments%252C%2520showing%2520that%2520IRIS-ZO%2520achieves%2520an%2520order-of-magnitude%2520speed%250Aadvantage%2520over%2520IRIS-NP.%2520IRISNP2%252C%2520also%2520significantly%2520faster%2520than%2520IRIS-NP%252C%2520builds%250Alarger%2520polytopes%2520using%2520fewer%2520hyperplanes%252C%2520enabling%2520faster%2520downstream%250Acomputation.%2520Website%253A%2520https%253A//sites.google.com/view/fastiris%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Algorithms%20for%20Growing%20Collision-Free%20Convex%20Polytopes%20in%20Robot%0A%20%20Configuration%20Space&entry.906535625=Peter%20Werner%20and%20Thomas%20Cohn%20and%20Rebecca%20H.%20Jiang%20and%20Tim%20Seyde%20and%20Max%20Simchowitz%20and%20Russ%20Tedrake%20and%20Daniela%20Rus&entry.1292438233=%20%20We%20propose%20two%20novel%20algorithms%20for%20constructing%20convex%20collision-free%0Apolytopes%20in%20robot%20configuration%20space.%20Finding%20these%20polytopes%20enables%20the%0Aapplication%20of%20stronger%20motion-planning%20frameworks%20such%20as%20trajectory%0Aoptimization%20with%20Graphs%20of%20Convex%20Sets%20%5B1%5D%20and%20is%20currently%20a%20major%20roadblock%0Ain%20the%20adoption%20of%20these%20approaches.%20In%20this%20paper%2C%20we%20build%20upon%20IRIS-NP%0A%28Iterative%20Regional%20Inflation%20by%20Semidefinite%20%26%20Nonlinear%20Programming%29%20%5B2%5D%20to%0Asignificantly%20improve%20tunability%2C%20runtimes%2C%20and%20scaling%20to%20complex%0Aenvironments.%20IRIS-NP%20uses%20nonlinear%20programming%20paired%20with%20uniform%20random%0Ainitialization%20to%20find%20configurations%20on%20the%20boundary%20of%20the%20free%20configuration%0Aspace.%20Our%20key%20insight%20is%20that%20finding%20near-by%20configuration-space%20obstacles%0Ausing%20sampling%20is%20inexpensive%20and%20greatly%20accelerates%20region%20generation.%20We%0Apropose%20two%20algorithms%20using%20such%20samples%20to%20either%20employ%20nonlinear%0Aprogramming%20more%20efficiently%20%28IRIS-NP2%20%29%20or%20circumvent%20it%20altogether%20using%20a%0Amassively-parallel%20zero-order%20optimization%20strategy%20%28IRIS-ZO%29.%20We%20also%20propose%0Aa%20termination%20condition%20that%20controls%20the%20probability%20of%20exceeding%20a%0Auser-specified%20permissible%20fraction-in-collision%2C%20eliminating%20a%20significant%0Asource%20of%20tuning%20difficulty%20in%20IRIS-NP.%20We%20compare%20performance%20across%20eight%0Arobot%20environments%2C%20showing%20that%20IRIS-ZO%20achieves%20an%20order-of-magnitude%20speed%0Aadvantage%20over%20IRIS-NP.%20IRISNP2%2C%20also%20significantly%20faster%20than%20IRIS-NP%2C%20builds%0Alarger%20polytopes%20using%20fewer%20hyperplanes%2C%20enabling%20faster%20downstream%0Acomputation.%20Website%3A%20https%3A//sites.google.com/view/fastiris%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12649v1&entry.124074799=Read"},
{"title": "A Primal-dual algorithm for image reconstruction with ICNNs", "author": "Hok Shing Wong and Matthias J. Ehrhardt and Subhadip Mukherjee", "abstract": "  We address the optimization problem in a data-driven variational\nreconstruction framework, where the regularizer is parameterized by an\ninput-convex neural network (ICNN). While gradient-based methods are commonly\nused to solve such problems, they struggle to effectively handle non-smoothness\nwhich often leads to slow convergence. Moreover, the nested structure of the\nneural network complicates the application of standard non-smooth optimization\ntechniques, such as proximal algorithms. To overcome these challenges, we\nreformulate the problem and eliminate the network's nested structure. By\nrelating this reformulation to epigraphical projections of the activation\nfunctions, we transform the problem into a convex optimization problem that can\nbe efficiently solved using a primal-dual algorithm. We also prove that this\nreformulation is equivalent to the original variational problem. Through\nexperiments on several imaging tasks, we demonstrate that the proposed approach\noutperforms subgradient methods in terms of both speed and stability.\n", "link": "http://arxiv.org/abs/2410.12441v1", "date": "2024-10-16", "relevancy": 2.0739, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5227}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5158}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Primal-dual%20algorithm%20for%20image%20reconstruction%20with%20ICNNs&body=Title%3A%20A%20Primal-dual%20algorithm%20for%20image%20reconstruction%20with%20ICNNs%0AAuthor%3A%20Hok%20Shing%20Wong%20and%20Matthias%20J.%20Ehrhardt%20and%20Subhadip%20Mukherjee%0AAbstract%3A%20%20%20We%20address%20the%20optimization%20problem%20in%20a%20data-driven%20variational%0Areconstruction%20framework%2C%20where%20the%20regularizer%20is%20parameterized%20by%20an%0Ainput-convex%20neural%20network%20%28ICNN%29.%20While%20gradient-based%20methods%20are%20commonly%0Aused%20to%20solve%20such%20problems%2C%20they%20struggle%20to%20effectively%20handle%20non-smoothness%0Awhich%20often%20leads%20to%20slow%20convergence.%20Moreover%2C%20the%20nested%20structure%20of%20the%0Aneural%20network%20complicates%20the%20application%20of%20standard%20non-smooth%20optimization%0Atechniques%2C%20such%20as%20proximal%20algorithms.%20To%20overcome%20these%20challenges%2C%20we%0Areformulate%20the%20problem%20and%20eliminate%20the%20network%27s%20nested%20structure.%20By%0Arelating%20this%20reformulation%20to%20epigraphical%20projections%20of%20the%20activation%0Afunctions%2C%20we%20transform%20the%20problem%20into%20a%20convex%20optimization%20problem%20that%20can%0Abe%20efficiently%20solved%20using%20a%20primal-dual%20algorithm.%20We%20also%20prove%20that%20this%0Areformulation%20is%20equivalent%20to%20the%20original%20variational%20problem.%20Through%0Aexperiments%20on%20several%20imaging%20tasks%2C%20we%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20subgradient%20methods%20in%20terms%20of%20both%20speed%20and%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Primal-dual%2520algorithm%2520for%2520image%2520reconstruction%2520with%2520ICNNs%26entry.906535625%3DHok%2520Shing%2520Wong%2520and%2520Matthias%2520J.%2520Ehrhardt%2520and%2520Subhadip%2520Mukherjee%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520optimization%2520problem%2520in%2520a%2520data-driven%2520variational%250Areconstruction%2520framework%252C%2520where%2520the%2520regularizer%2520is%2520parameterized%2520by%2520an%250Ainput-convex%2520neural%2520network%2520%2528ICNN%2529.%2520While%2520gradient-based%2520methods%2520are%2520commonly%250Aused%2520to%2520solve%2520such%2520problems%252C%2520they%2520struggle%2520to%2520effectively%2520handle%2520non-smoothness%250Awhich%2520often%2520leads%2520to%2520slow%2520convergence.%2520Moreover%252C%2520the%2520nested%2520structure%2520of%2520the%250Aneural%2520network%2520complicates%2520the%2520application%2520of%2520standard%2520non-smooth%2520optimization%250Atechniques%252C%2520such%2520as%2520proximal%2520algorithms.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Areformulate%2520the%2520problem%2520and%2520eliminate%2520the%2520network%2527s%2520nested%2520structure.%2520By%250Arelating%2520this%2520reformulation%2520to%2520epigraphical%2520projections%2520of%2520the%2520activation%250Afunctions%252C%2520we%2520transform%2520the%2520problem%2520into%2520a%2520convex%2520optimization%2520problem%2520that%2520can%250Abe%2520efficiently%2520solved%2520using%2520a%2520primal-dual%2520algorithm.%2520We%2520also%2520prove%2520that%2520this%250Areformulation%2520is%2520equivalent%2520to%2520the%2520original%2520variational%2520problem.%2520Through%250Aexperiments%2520on%2520several%2520imaging%2520tasks%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520approach%250Aoutperforms%2520subgradient%2520methods%2520in%2520terms%2520of%2520both%2520speed%2520and%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Primal-dual%20algorithm%20for%20image%20reconstruction%20with%20ICNNs&entry.906535625=Hok%20Shing%20Wong%20and%20Matthias%20J.%20Ehrhardt%20and%20Subhadip%20Mukherjee&entry.1292438233=%20%20We%20address%20the%20optimization%20problem%20in%20a%20data-driven%20variational%0Areconstruction%20framework%2C%20where%20the%20regularizer%20is%20parameterized%20by%20an%0Ainput-convex%20neural%20network%20%28ICNN%29.%20While%20gradient-based%20methods%20are%20commonly%0Aused%20to%20solve%20such%20problems%2C%20they%20struggle%20to%20effectively%20handle%20non-smoothness%0Awhich%20often%20leads%20to%20slow%20convergence.%20Moreover%2C%20the%20nested%20structure%20of%20the%0Aneural%20network%20complicates%20the%20application%20of%20standard%20non-smooth%20optimization%0Atechniques%2C%20such%20as%20proximal%20algorithms.%20To%20overcome%20these%20challenges%2C%20we%0Areformulate%20the%20problem%20and%20eliminate%20the%20network%27s%20nested%20structure.%20By%0Arelating%20this%20reformulation%20to%20epigraphical%20projections%20of%20the%20activation%0Afunctions%2C%20we%20transform%20the%20problem%20into%20a%20convex%20optimization%20problem%20that%20can%0Abe%20efficiently%20solved%20using%20a%20primal-dual%20algorithm.%20We%20also%20prove%20that%20this%0Areformulation%20is%20equivalent%20to%20the%20original%20variational%20problem.%20Through%0Aexperiments%20on%20several%20imaging%20tasks%2C%20we%20demonstrate%20that%20the%20proposed%20approach%0Aoutperforms%20subgradient%20methods%20in%20terms%20of%20both%20speed%20and%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12441v1&entry.124074799=Read"},
{"title": "LLM-based Translation Inference with Iterative Bilingual Understanding", "author": "Andong Chen and Kehai Chen and Yang Xiang and Xuefeng Bai and Muyun Yang and Tiejun Zhao and Min zhang", "abstract": "  The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).\n", "link": "http://arxiv.org/abs/2410.12543v1", "date": "2024-10-16", "relevancy": 2.0636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-based%20Translation%20Inference%20with%20Iterative%20Bilingual%20Understanding&body=Title%3A%20LLM-based%20Translation%20Inference%20with%20Iterative%20Bilingual%20Understanding%0AAuthor%3A%20Andong%20Chen%20and%20Kehai%20Chen%20and%20Yang%20Xiang%20and%20Xuefeng%20Bai%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%20and%20Min%20zhang%0AAbstract%3A%20%20%20The%20remarkable%20understanding%20and%20generation%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20have%20greatly%20improved%20translation%20performance.%20However%2C%20incorrect%0Aunderstanding%20of%20the%20sentence%20to%20be%20translated%20can%20degrade%20translation%20quality.%0ATo%20address%20this%20issue%2C%20we%20proposed%20a%20novel%20Iterative%20Bilingual%20Understanding%0ATranslation%20%28IBUT%29%20method%20based%20on%20the%20cross-lingual%20capabilities%20of%20LLMs%20and%0Athe%20dual%20characteristics%20of%20translation%20tasks.%20The%20cross-lingual%20capability%20of%0ALLMs%20enables%20the%20generation%20of%20contextual%20understanding%20for%20both%20the%20source%20and%0Atarget%20languages%20separately.%20Furthermore%2C%20the%20dual%20characteristics%20allow%20IBUT%0Ato%20generate%20effective%20cross-lingual%20feedback%2C%20iteratively%20refining%20contextual%0Aunderstanding%2C%20thereby%20reducing%20errors%20and%20improving%20translation%20performance.%0AExperimental%20results%20showed%20that%20the%20proposed%20IBUT%20outperforms%20several%20strong%0Acomparison%20methods%2C%20especially%20being%20generalized%20to%20multiple%20domains%20%28e.g.%2C%0Anews%2C%20commonsense%2C%20and%20cultural%20translation%20benchmarks%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-based%2520Translation%2520Inference%2520with%2520Iterative%2520Bilingual%2520Understanding%26entry.906535625%3DAndong%2520Chen%2520and%2520Kehai%2520Chen%2520and%2520Yang%2520Xiang%2520and%2520Xuefeng%2520Bai%2520and%2520Muyun%2520Yang%2520and%2520Tiejun%2520Zhao%2520and%2520Min%2520zhang%26entry.1292438233%3D%2520%2520The%2520remarkable%2520understanding%2520and%2520generation%2520capabilities%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520greatly%2520improved%2520translation%2520performance.%2520However%252C%2520incorrect%250Aunderstanding%2520of%2520the%2520sentence%2520to%2520be%2520translated%2520can%2520degrade%2520translation%2520quality.%250ATo%2520address%2520this%2520issue%252C%2520we%2520proposed%2520a%2520novel%2520Iterative%2520Bilingual%2520Understanding%250ATranslation%2520%2528IBUT%2529%2520method%2520based%2520on%2520the%2520cross-lingual%2520capabilities%2520of%2520LLMs%2520and%250Athe%2520dual%2520characteristics%2520of%2520translation%2520tasks.%2520The%2520cross-lingual%2520capability%2520of%250ALLMs%2520enables%2520the%2520generation%2520of%2520contextual%2520understanding%2520for%2520both%2520the%2520source%2520and%250Atarget%2520languages%2520separately.%2520Furthermore%252C%2520the%2520dual%2520characteristics%2520allow%2520IBUT%250Ato%2520generate%2520effective%2520cross-lingual%2520feedback%252C%2520iteratively%2520refining%2520contextual%250Aunderstanding%252C%2520thereby%2520reducing%2520errors%2520and%2520improving%2520translation%2520performance.%250AExperimental%2520results%2520showed%2520that%2520the%2520proposed%2520IBUT%2520outperforms%2520several%2520strong%250Acomparison%2520methods%252C%2520especially%2520being%2520generalized%2520to%2520multiple%2520domains%2520%2528e.g.%252C%250Anews%252C%2520commonsense%252C%2520and%2520cultural%2520translation%2520benchmarks%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-based%20Translation%20Inference%20with%20Iterative%20Bilingual%20Understanding&entry.906535625=Andong%20Chen%20and%20Kehai%20Chen%20and%20Yang%20Xiang%20and%20Xuefeng%20Bai%20and%20Muyun%20Yang%20and%20Tiejun%20Zhao%20and%20Min%20zhang&entry.1292438233=%20%20The%20remarkable%20understanding%20and%20generation%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20have%20greatly%20improved%20translation%20performance.%20However%2C%20incorrect%0Aunderstanding%20of%20the%20sentence%20to%20be%20translated%20can%20degrade%20translation%20quality.%0ATo%20address%20this%20issue%2C%20we%20proposed%20a%20novel%20Iterative%20Bilingual%20Understanding%0ATranslation%20%28IBUT%29%20method%20based%20on%20the%20cross-lingual%20capabilities%20of%20LLMs%20and%0Athe%20dual%20characteristics%20of%20translation%20tasks.%20The%20cross-lingual%20capability%20of%0ALLMs%20enables%20the%20generation%20of%20contextual%20understanding%20for%20both%20the%20source%20and%0Atarget%20languages%20separately.%20Furthermore%2C%20the%20dual%20characteristics%20allow%20IBUT%0Ato%20generate%20effective%20cross-lingual%20feedback%2C%20iteratively%20refining%20contextual%0Aunderstanding%2C%20thereby%20reducing%20errors%20and%20improving%20translation%20performance.%0AExperimental%20results%20showed%20that%20the%20proposed%20IBUT%20outperforms%20several%20strong%0Acomparison%20methods%2C%20especially%20being%20generalized%20to%20multiple%20domains%20%28e.g.%2C%0Anews%2C%20commonsense%2C%20and%20cultural%20translation%20benchmarks%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12543v1&entry.124074799=Read"},
{"title": "Data Augmentation for Continual RL via Adversarial Gradient Episodic\n  Memory", "author": "Sihao Wu and Xingyu Zhao and Xiaowei Huang", "abstract": "  Data efficiency of learning, which plays a key role in the Reinforcement\nLearning (RL) training process, becomes even more important in continual RL\nwith sequential environments. In continual RL, the learner interacts with\nnon-stationary, sequential tasks and is required to learn new tasks without\nforgetting previous knowledge. However, there is little work on implementing\ndata augmentation for continual RL. In this paper, we investigate the efficacy\nof data augmentation for continual RL. Specifically, we provide benchmarking\ndata augmentations for continual RL, by (1) summarising existing data\naugmentation methods and (2) including a new augmentation method for continual\nRL: Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM). Extensive\nexperiments show that data augmentations, such as random amplitude scaling,\nstate-switch, mixup, adversarial augmentation, and Adv-GEM, can improve\nexisting continual RL algorithms in terms of their average performance,\ncatastrophic forgetting, and forward transfer, on robot control tasks. All data\naugmentation methods are implemented as plug-in modules for trivial integration\ninto continual RL methods.\n", "link": "http://arxiv.org/abs/2408.13452v3", "date": "2024-10-16", "relevancy": 2.0627, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5396}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Augmentation%20for%20Continual%20RL%20via%20Adversarial%20Gradient%20Episodic%0A%20%20Memory&body=Title%3A%20Data%20Augmentation%20for%20Continual%20RL%20via%20Adversarial%20Gradient%20Episodic%0A%20%20Memory%0AAuthor%3A%20Sihao%20Wu%20and%20Xingyu%20Zhao%20and%20Xiaowei%20Huang%0AAbstract%3A%20%20%20Data%20efficiency%20of%20learning%2C%20which%20plays%20a%20key%20role%20in%20the%20Reinforcement%0ALearning%20%28RL%29%20training%20process%2C%20becomes%20even%20more%20important%20in%20continual%20RL%0Awith%20sequential%20environments.%20In%20continual%20RL%2C%20the%20learner%20interacts%20with%0Anon-stationary%2C%20sequential%20tasks%20and%20is%20required%20to%20learn%20new%20tasks%20without%0Aforgetting%20previous%20knowledge.%20However%2C%20there%20is%20little%20work%20on%20implementing%0Adata%20augmentation%20for%20continual%20RL.%20In%20this%20paper%2C%20we%20investigate%20the%20efficacy%0Aof%20data%20augmentation%20for%20continual%20RL.%20Specifically%2C%20we%20provide%20benchmarking%0Adata%20augmentations%20for%20continual%20RL%2C%20by%20%281%29%20summarising%20existing%20data%0Aaugmentation%20methods%20and%20%282%29%20including%20a%20new%20augmentation%20method%20for%20continual%0ARL%3A%20Adversarial%20Augmentation%20with%20Gradient%20Episodic%20Memory%20%28Adv-GEM%29.%20Extensive%0Aexperiments%20show%20that%20data%20augmentations%2C%20such%20as%20random%20amplitude%20scaling%2C%0Astate-switch%2C%20mixup%2C%20adversarial%20augmentation%2C%20and%20Adv-GEM%2C%20can%20improve%0Aexisting%20continual%20RL%20algorithms%20in%20terms%20of%20their%20average%20performance%2C%0Acatastrophic%20forgetting%2C%20and%20forward%20transfer%2C%20on%20robot%20control%20tasks.%20All%20data%0Aaugmentation%20methods%20are%20implemented%20as%20plug-in%20modules%20for%20trivial%20integration%0Ainto%20continual%20RL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13452v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Augmentation%2520for%2520Continual%2520RL%2520via%2520Adversarial%2520Gradient%2520Episodic%250A%2520%2520Memory%26entry.906535625%3DSihao%2520Wu%2520and%2520Xingyu%2520Zhao%2520and%2520Xiaowei%2520Huang%26entry.1292438233%3D%2520%2520Data%2520efficiency%2520of%2520learning%252C%2520which%2520plays%2520a%2520key%2520role%2520in%2520the%2520Reinforcement%250ALearning%2520%2528RL%2529%2520training%2520process%252C%2520becomes%2520even%2520more%2520important%2520in%2520continual%2520RL%250Awith%2520sequential%2520environments.%2520In%2520continual%2520RL%252C%2520the%2520learner%2520interacts%2520with%250Anon-stationary%252C%2520sequential%2520tasks%2520and%2520is%2520required%2520to%2520learn%2520new%2520tasks%2520without%250Aforgetting%2520previous%2520knowledge.%2520However%252C%2520there%2520is%2520little%2520work%2520on%2520implementing%250Adata%2520augmentation%2520for%2520continual%2520RL.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520efficacy%250Aof%2520data%2520augmentation%2520for%2520continual%2520RL.%2520Specifically%252C%2520we%2520provide%2520benchmarking%250Adata%2520augmentations%2520for%2520continual%2520RL%252C%2520by%2520%25281%2529%2520summarising%2520existing%2520data%250Aaugmentation%2520methods%2520and%2520%25282%2529%2520including%2520a%2520new%2520augmentation%2520method%2520for%2520continual%250ARL%253A%2520Adversarial%2520Augmentation%2520with%2520Gradient%2520Episodic%2520Memory%2520%2528Adv-GEM%2529.%2520Extensive%250Aexperiments%2520show%2520that%2520data%2520augmentations%252C%2520such%2520as%2520random%2520amplitude%2520scaling%252C%250Astate-switch%252C%2520mixup%252C%2520adversarial%2520augmentation%252C%2520and%2520Adv-GEM%252C%2520can%2520improve%250Aexisting%2520continual%2520RL%2520algorithms%2520in%2520terms%2520of%2520their%2520average%2520performance%252C%250Acatastrophic%2520forgetting%252C%2520and%2520forward%2520transfer%252C%2520on%2520robot%2520control%2520tasks.%2520All%2520data%250Aaugmentation%2520methods%2520are%2520implemented%2520as%2520plug-in%2520modules%2520for%2520trivial%2520integration%250Ainto%2520continual%2520RL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13452v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Augmentation%20for%20Continual%20RL%20via%20Adversarial%20Gradient%20Episodic%0A%20%20Memory&entry.906535625=Sihao%20Wu%20and%20Xingyu%20Zhao%20and%20Xiaowei%20Huang&entry.1292438233=%20%20Data%20efficiency%20of%20learning%2C%20which%20plays%20a%20key%20role%20in%20the%20Reinforcement%0ALearning%20%28RL%29%20training%20process%2C%20becomes%20even%20more%20important%20in%20continual%20RL%0Awith%20sequential%20environments.%20In%20continual%20RL%2C%20the%20learner%20interacts%20with%0Anon-stationary%2C%20sequential%20tasks%20and%20is%20required%20to%20learn%20new%20tasks%20without%0Aforgetting%20previous%20knowledge.%20However%2C%20there%20is%20little%20work%20on%20implementing%0Adata%20augmentation%20for%20continual%20RL.%20In%20this%20paper%2C%20we%20investigate%20the%20efficacy%0Aof%20data%20augmentation%20for%20continual%20RL.%20Specifically%2C%20we%20provide%20benchmarking%0Adata%20augmentations%20for%20continual%20RL%2C%20by%20%281%29%20summarising%20existing%20data%0Aaugmentation%20methods%20and%20%282%29%20including%20a%20new%20augmentation%20method%20for%20continual%0ARL%3A%20Adversarial%20Augmentation%20with%20Gradient%20Episodic%20Memory%20%28Adv-GEM%29.%20Extensive%0Aexperiments%20show%20that%20data%20augmentations%2C%20such%20as%20random%20amplitude%20scaling%2C%0Astate-switch%2C%20mixup%2C%20adversarial%20augmentation%2C%20and%20Adv-GEM%2C%20can%20improve%0Aexisting%20continual%20RL%20algorithms%20in%20terms%20of%20their%20average%20performance%2C%0Acatastrophic%20forgetting%2C%20and%20forward%20transfer%2C%20on%20robot%20control%20tasks.%20All%20data%0Aaugmentation%20methods%20are%20implemented%20as%20plug-in%20modules%20for%20trivial%20integration%0Ainto%20continual%20RL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13452v3&entry.124074799=Read"},
{"title": "Adaptive Convolutional Neural Network for Image Super-resolution", "author": "Chunwei Tian and Xuanyu Zhang and Tao Wang and Yongjun Zhang and Qi Zhu and Chia-Wen Lin", "abstract": "  Convolutional neural networks can automatically learn features via deep\nnetwork architectures and given input samples. However, the robustness of\nobtained models may face challenges in varying scenes. Bigger differences in\nnetwork architecture are beneficial to extract more diversified structural\ninformation to strengthen the robustness of an obtained super-resolution model.\nIn this paper, we proposed a adaptive convolutional neural network for image\nsuper-resolution (ADSRNet). To capture more information, ADSRNet is implemented\nby a heterogeneous parallel network. The upper network can enhance relation of\ncontext information, salient information relation of a kernel mapping and\nrelations of shallow and deep layers to improve performance of image\nsuper-resolution. That can strengthen adaptability of an obtained\nsuper-resolution model for different scenes. The lower network utilizes a\nsymmetric architecture to enhance relations of different layers to mine more\nstructural information, which is complementary with a upper network for image\nsuper-resolution. The relevant experimental results show that the proposed\nADSRNet is effective to deal with image resolving. Codes are obtained at\nhttps://github.com/hellloxiaotian/ADSRNet.\n", "link": "http://arxiv.org/abs/2402.15704v4", "date": "2024-10-16", "relevancy": 2.0547, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5246}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Convolutional%20Neural%20Network%20for%20Image%20Super-resolution&body=Title%3A%20Adaptive%20Convolutional%20Neural%20Network%20for%20Image%20Super-resolution%0AAuthor%3A%20Chunwei%20Tian%20and%20Xuanyu%20Zhang%20and%20Tao%20Wang%20and%20Yongjun%20Zhang%20and%20Qi%20Zhu%20and%20Chia-Wen%20Lin%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20can%20automatically%20learn%20features%20via%20deep%0Anetwork%20architectures%20and%20given%20input%20samples.%20However%2C%20the%20robustness%20of%0Aobtained%20models%20may%20face%20challenges%20in%20varying%20scenes.%20Bigger%20differences%20in%0Anetwork%20architecture%20are%20beneficial%20to%20extract%20more%20diversified%20structural%0Ainformation%20to%20strengthen%20the%20robustness%20of%20an%20obtained%20super-resolution%20model.%0AIn%20this%20paper%2C%20we%20proposed%20a%20adaptive%20convolutional%20neural%20network%20for%20image%0Asuper-resolution%20%28ADSRNet%29.%20To%20capture%20more%20information%2C%20ADSRNet%20is%20implemented%0Aby%20a%20heterogeneous%20parallel%20network.%20The%20upper%20network%20can%20enhance%20relation%20of%0Acontext%20information%2C%20salient%20information%20relation%20of%20a%20kernel%20mapping%20and%0Arelations%20of%20shallow%20and%20deep%20layers%20to%20improve%20performance%20of%20image%0Asuper-resolution.%20That%20can%20strengthen%20adaptability%20of%20an%20obtained%0Asuper-resolution%20model%20for%20different%20scenes.%20The%20lower%20network%20utilizes%20a%0Asymmetric%20architecture%20to%20enhance%20relations%20of%20different%20layers%20to%20mine%20more%0Astructural%20information%2C%20which%20is%20complementary%20with%20a%20upper%20network%20for%20image%0Asuper-resolution.%20The%20relevant%20experimental%20results%20show%20that%20the%20proposed%0AADSRNet%20is%20effective%20to%20deal%20with%20image%20resolving.%20Codes%20are%20obtained%20at%0Ahttps%3A//github.com/hellloxiaotian/ADSRNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15704v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Convolutional%2520Neural%2520Network%2520for%2520Image%2520Super-resolution%26entry.906535625%3DChunwei%2520Tian%2520and%2520Xuanyu%2520Zhang%2520and%2520Tao%2520Wang%2520and%2520Yongjun%2520Zhang%2520and%2520Qi%2520Zhu%2520and%2520Chia-Wen%2520Lin%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520can%2520automatically%2520learn%2520features%2520via%2520deep%250Anetwork%2520architectures%2520and%2520given%2520input%2520samples.%2520However%252C%2520the%2520robustness%2520of%250Aobtained%2520models%2520may%2520face%2520challenges%2520in%2520varying%2520scenes.%2520Bigger%2520differences%2520in%250Anetwork%2520architecture%2520are%2520beneficial%2520to%2520extract%2520more%2520diversified%2520structural%250Ainformation%2520to%2520strengthen%2520the%2520robustness%2520of%2520an%2520obtained%2520super-resolution%2520model.%250AIn%2520this%2520paper%252C%2520we%2520proposed%2520a%2520adaptive%2520convolutional%2520neural%2520network%2520for%2520image%250Asuper-resolution%2520%2528ADSRNet%2529.%2520To%2520capture%2520more%2520information%252C%2520ADSRNet%2520is%2520implemented%250Aby%2520a%2520heterogeneous%2520parallel%2520network.%2520The%2520upper%2520network%2520can%2520enhance%2520relation%2520of%250Acontext%2520information%252C%2520salient%2520information%2520relation%2520of%2520a%2520kernel%2520mapping%2520and%250Arelations%2520of%2520shallow%2520and%2520deep%2520layers%2520to%2520improve%2520performance%2520of%2520image%250Asuper-resolution.%2520That%2520can%2520strengthen%2520adaptability%2520of%2520an%2520obtained%250Asuper-resolution%2520model%2520for%2520different%2520scenes.%2520The%2520lower%2520network%2520utilizes%2520a%250Asymmetric%2520architecture%2520to%2520enhance%2520relations%2520of%2520different%2520layers%2520to%2520mine%2520more%250Astructural%2520information%252C%2520which%2520is%2520complementary%2520with%2520a%2520upper%2520network%2520for%2520image%250Asuper-resolution.%2520The%2520relevant%2520experimental%2520results%2520show%2520that%2520the%2520proposed%250AADSRNet%2520is%2520effective%2520to%2520deal%2520with%2520image%2520resolving.%2520Codes%2520are%2520obtained%2520at%250Ahttps%253A//github.com/hellloxiaotian/ADSRNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15704v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Convolutional%20Neural%20Network%20for%20Image%20Super-resolution&entry.906535625=Chunwei%20Tian%20and%20Xuanyu%20Zhang%20and%20Tao%20Wang%20and%20Yongjun%20Zhang%20and%20Qi%20Zhu%20and%20Chia-Wen%20Lin&entry.1292438233=%20%20Convolutional%20neural%20networks%20can%20automatically%20learn%20features%20via%20deep%0Anetwork%20architectures%20and%20given%20input%20samples.%20However%2C%20the%20robustness%20of%0Aobtained%20models%20may%20face%20challenges%20in%20varying%20scenes.%20Bigger%20differences%20in%0Anetwork%20architecture%20are%20beneficial%20to%20extract%20more%20diversified%20structural%0Ainformation%20to%20strengthen%20the%20robustness%20of%20an%20obtained%20super-resolution%20model.%0AIn%20this%20paper%2C%20we%20proposed%20a%20adaptive%20convolutional%20neural%20network%20for%20image%0Asuper-resolution%20%28ADSRNet%29.%20To%20capture%20more%20information%2C%20ADSRNet%20is%20implemented%0Aby%20a%20heterogeneous%20parallel%20network.%20The%20upper%20network%20can%20enhance%20relation%20of%0Acontext%20information%2C%20salient%20information%20relation%20of%20a%20kernel%20mapping%20and%0Arelations%20of%20shallow%20and%20deep%20layers%20to%20improve%20performance%20of%20image%0Asuper-resolution.%20That%20can%20strengthen%20adaptability%20of%20an%20obtained%0Asuper-resolution%20model%20for%20different%20scenes.%20The%20lower%20network%20utilizes%20a%0Asymmetric%20architecture%20to%20enhance%20relations%20of%20different%20layers%20to%20mine%20more%0Astructural%20information%2C%20which%20is%20complementary%20with%20a%20upper%20network%20for%20image%0Asuper-resolution.%20The%20relevant%20experimental%20results%20show%20that%20the%20proposed%0AADSRNet%20is%20effective%20to%20deal%20with%20image%20resolving.%20Codes%20are%20obtained%20at%0Ahttps%3A//github.com/hellloxiaotian/ADSRNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15704v4&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Disentangled Representations for\n  Multivariate Time-Series", "author": "Ching Chang and Chiao-Tung Chan and Wei-Yao Wang and Wen-Chih Peng and Tien-Fu Chen", "abstract": "  Multivariate time-series data in fields like healthcare and industry are\ninformative but challenging due to high dimensionality and lack of labels.\nRecent self-supervised learning methods excel in learning rich representations\nwithout labels but struggle with disentangled embeddings and inductive bias\nissues like transformation-invariance. To address these challenges, we\nintroduce TimeDRL, a framework for multivariate time-series representation\nlearning with dual-level disentangled embeddings. TimeDRL features: (i)\ndisentangled timestamp-level and instance-level embeddings using a [CLS] token\nstrategy; (ii) timestamp-predictive and instance-contrastive tasks for\nrepresentation learning; and (iii) avoidance of augmentation methods to\neliminate inductive biases. Experiments on forecasting and classification\ndatasets show TimeDRL outperforms existing methods, with further validation in\nsemi-supervised settings with limited labeled data.\n", "link": "http://arxiv.org/abs/2410.12606v1", "date": "2024-10-16", "relevancy": 2.0475, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5168}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20of%20Disentangled%20Representations%20for%0A%20%20Multivariate%20Time-Series&body=Title%3A%20Self-Supervised%20Learning%20of%20Disentangled%20Representations%20for%0A%20%20Multivariate%20Time-Series%0AAuthor%3A%20Ching%20Chang%20and%20Chiao-Tung%20Chan%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen%0AAbstract%3A%20%20%20Multivariate%20time-series%20data%20in%20fields%20like%20healthcare%20and%20industry%20are%0Ainformative%20but%20challenging%20due%20to%20high%20dimensionality%20and%20lack%20of%20labels.%0ARecent%20self-supervised%20learning%20methods%20excel%20in%20learning%20rich%20representations%0Awithout%20labels%20but%20struggle%20with%20disentangled%20embeddings%20and%20inductive%20bias%0Aissues%20like%20transformation-invariance.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20TimeDRL%2C%20a%20framework%20for%20multivariate%20time-series%20representation%0Alearning%20with%20dual-level%20disentangled%20embeddings.%20TimeDRL%20features%3A%20%28i%29%0Adisentangled%20timestamp-level%20and%20instance-level%20embeddings%20using%20a%20%5BCLS%5D%20token%0Astrategy%3B%20%28ii%29%20timestamp-predictive%20and%20instance-contrastive%20tasks%20for%0Arepresentation%20learning%3B%20and%20%28iii%29%20avoidance%20of%20augmentation%20methods%20to%0Aeliminate%20inductive%20biases.%20Experiments%20on%20forecasting%20and%20classification%0Adatasets%20show%20TimeDRL%20outperforms%20existing%20methods%2C%20with%20further%20validation%20in%0Asemi-supervised%20settings%20with%20limited%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520of%2520Disentangled%2520Representations%2520for%250A%2520%2520Multivariate%2520Time-Series%26entry.906535625%3DChing%2520Chang%2520and%2520Chiao-Tung%2520Chan%2520and%2520Wei-Yao%2520Wang%2520and%2520Wen-Chih%2520Peng%2520and%2520Tien-Fu%2520Chen%26entry.1292438233%3D%2520%2520Multivariate%2520time-series%2520data%2520in%2520fields%2520like%2520healthcare%2520and%2520industry%2520are%250Ainformative%2520but%2520challenging%2520due%2520to%2520high%2520dimensionality%2520and%2520lack%2520of%2520labels.%250ARecent%2520self-supervised%2520learning%2520methods%2520excel%2520in%2520learning%2520rich%2520representations%250Awithout%2520labels%2520but%2520struggle%2520with%2520disentangled%2520embeddings%2520and%2520inductive%2520bias%250Aissues%2520like%2520transformation-invariance.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520TimeDRL%252C%2520a%2520framework%2520for%2520multivariate%2520time-series%2520representation%250Alearning%2520with%2520dual-level%2520disentangled%2520embeddings.%2520TimeDRL%2520features%253A%2520%2528i%2529%250Adisentangled%2520timestamp-level%2520and%2520instance-level%2520embeddings%2520using%2520a%2520%255BCLS%255D%2520token%250Astrategy%253B%2520%2528ii%2529%2520timestamp-predictive%2520and%2520instance-contrastive%2520tasks%2520for%250Arepresentation%2520learning%253B%2520and%2520%2528iii%2529%2520avoidance%2520of%2520augmentation%2520methods%2520to%250Aeliminate%2520inductive%2520biases.%2520Experiments%2520on%2520forecasting%2520and%2520classification%250Adatasets%2520show%2520TimeDRL%2520outperforms%2520existing%2520methods%252C%2520with%2520further%2520validation%2520in%250Asemi-supervised%2520settings%2520with%2520limited%2520labeled%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Disentangled%20Representations%20for%0A%20%20Multivariate%20Time-Series&entry.906535625=Ching%20Chang%20and%20Chiao-Tung%20Chan%20and%20Wei-Yao%20Wang%20and%20Wen-Chih%20Peng%20and%20Tien-Fu%20Chen&entry.1292438233=%20%20Multivariate%20time-series%20data%20in%20fields%20like%20healthcare%20and%20industry%20are%0Ainformative%20but%20challenging%20due%20to%20high%20dimensionality%20and%20lack%20of%20labels.%0ARecent%20self-supervised%20learning%20methods%20excel%20in%20learning%20rich%20representations%0Awithout%20labels%20but%20struggle%20with%20disentangled%20embeddings%20and%20inductive%20bias%0Aissues%20like%20transformation-invariance.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20TimeDRL%2C%20a%20framework%20for%20multivariate%20time-series%20representation%0Alearning%20with%20dual-level%20disentangled%20embeddings.%20TimeDRL%20features%3A%20%28i%29%0Adisentangled%20timestamp-level%20and%20instance-level%20embeddings%20using%20a%20%5BCLS%5D%20token%0Astrategy%3B%20%28ii%29%20timestamp-predictive%20and%20instance-contrastive%20tasks%20for%0Arepresentation%20learning%3B%20and%20%28iii%29%20avoidance%20of%20augmentation%20methods%20to%0Aeliminate%20inductive%20biases.%20Experiments%20on%20forecasting%20and%20classification%0Adatasets%20show%20TimeDRL%20outperforms%20existing%20methods%2C%20with%20further%20validation%20in%0Asemi-supervised%20settings%20with%20limited%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12606v1&entry.124074799=Read"},
{"title": "Counterfactual Generative Modeling with Variational Causal Inference", "author": "Yulun Wu and Louie McConnell and Claudia Iriondo", "abstract": "  Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in its high-dimensional observed outcome in addition to\nthe covariates. Prior works using variational inference in counterfactual\ngenerative modeling have been focusing on neural adaptations and model variants\nwithin the conditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage latent disentanglement that aids the correct identification of causal\neffect in counterfactual generations. In experiments, we demonstrate the\nadvantage of our framework compared to state-of-the-art models in\ncounterfactual generative modeling on multiple benchmarks.\n", "link": "http://arxiv.org/abs/2410.12730v1", "date": "2024-10-16", "relevancy": 2.0442, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5283}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.521}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Generative%20Modeling%20with%20Variational%20Causal%20Inference&body=Title%3A%20Counterfactual%20Generative%20Modeling%20with%20Variational%20Causal%20Inference%0AAuthor%3A%20Yulun%20Wu%20and%20Louie%20McConnell%20and%20Claudia%20Iriondo%0AAbstract%3A%20%20%20Estimating%20an%20individual%27s%20potential%20outcomes%20under%20counterfactual%20treatments%0Ais%20a%20challenging%20task%20for%20traditional%20causal%20inference%20and%20supervised%20learning%0Aapproaches%20when%20the%20outcome%20is%20high-dimensional%20%28e.g.%20gene%20expressions%2C%20facial%0Aimages%29%20and%20covariates%20are%20relatively%20limited.%20In%20this%20case%2C%20to%20predict%20one%27s%0Aoutcomes%20under%20counterfactual%20treatments%2C%20it%20is%20crucial%20to%20leverage%20individual%0Ainformation%20contained%20in%20its%20high-dimensional%20observed%20outcome%20in%20addition%20to%0Athe%20covariates.%20Prior%20works%20using%20variational%20inference%20in%20counterfactual%0Agenerative%20modeling%20have%20been%20focusing%20on%20neural%20adaptations%20and%20model%20variants%0Awithin%20the%20conditional%20variational%20autoencoder%20formulation%2C%20which%20we%20argue%20is%0Afundamentally%20ill-suited%20to%20the%20notion%20of%20counterfactual%20in%20causal%20inference.%0AIn%20this%20work%2C%20we%20present%20a%20novel%20variational%20Bayesian%20causal%20inference%0Aframework%20and%20its%20theoretical%20backings%20to%20properly%20handle%20counterfactual%0Agenerative%20modeling%20tasks%2C%20through%20which%20we%20are%20able%20to%20conduct%20counterfactual%0Asupervision%20end-to-end%20during%20training%20without%20any%20counterfactual%20samples%2C%20and%0Aencourage%20latent%20disentanglement%20that%20aids%20the%20correct%20identification%20of%20causal%0Aeffect%20in%20counterfactual%20generations.%20In%20experiments%2C%20we%20demonstrate%20the%0Aadvantage%20of%20our%20framework%20compared%20to%20state-of-the-art%20models%20in%0Acounterfactual%20generative%20modeling%20on%20multiple%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Generative%2520Modeling%2520with%2520Variational%2520Causal%2520Inference%26entry.906535625%3DYulun%2520Wu%2520and%2520Louie%2520McConnell%2520and%2520Claudia%2520Iriondo%26entry.1292438233%3D%2520%2520Estimating%2520an%2520individual%2527s%2520potential%2520outcomes%2520under%2520counterfactual%2520treatments%250Ais%2520a%2520challenging%2520task%2520for%2520traditional%2520causal%2520inference%2520and%2520supervised%2520learning%250Aapproaches%2520when%2520the%2520outcome%2520is%2520high-dimensional%2520%2528e.g.%2520gene%2520expressions%252C%2520facial%250Aimages%2529%2520and%2520covariates%2520are%2520relatively%2520limited.%2520In%2520this%2520case%252C%2520to%2520predict%2520one%2527s%250Aoutcomes%2520under%2520counterfactual%2520treatments%252C%2520it%2520is%2520crucial%2520to%2520leverage%2520individual%250Ainformation%2520contained%2520in%2520its%2520high-dimensional%2520observed%2520outcome%2520in%2520addition%2520to%250Athe%2520covariates.%2520Prior%2520works%2520using%2520variational%2520inference%2520in%2520counterfactual%250Agenerative%2520modeling%2520have%2520been%2520focusing%2520on%2520neural%2520adaptations%2520and%2520model%2520variants%250Awithin%2520the%2520conditional%2520variational%2520autoencoder%2520formulation%252C%2520which%2520we%2520argue%2520is%250Afundamentally%2520ill-suited%2520to%2520the%2520notion%2520of%2520counterfactual%2520in%2520causal%2520inference.%250AIn%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520variational%2520Bayesian%2520causal%2520inference%250Aframework%2520and%2520its%2520theoretical%2520backings%2520to%2520properly%2520handle%2520counterfactual%250Agenerative%2520modeling%2520tasks%252C%2520through%2520which%2520we%2520are%2520able%2520to%2520conduct%2520counterfactual%250Asupervision%2520end-to-end%2520during%2520training%2520without%2520any%2520counterfactual%2520samples%252C%2520and%250Aencourage%2520latent%2520disentanglement%2520that%2520aids%2520the%2520correct%2520identification%2520of%2520causal%250Aeffect%2520in%2520counterfactual%2520generations.%2520In%2520experiments%252C%2520we%2520demonstrate%2520the%250Aadvantage%2520of%2520our%2520framework%2520compared%2520to%2520state-of-the-art%2520models%2520in%250Acounterfactual%2520generative%2520modeling%2520on%2520multiple%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Generative%20Modeling%20with%20Variational%20Causal%20Inference&entry.906535625=Yulun%20Wu%20and%20Louie%20McConnell%20and%20Claudia%20Iriondo&entry.1292438233=%20%20Estimating%20an%20individual%27s%20potential%20outcomes%20under%20counterfactual%20treatments%0Ais%20a%20challenging%20task%20for%20traditional%20causal%20inference%20and%20supervised%20learning%0Aapproaches%20when%20the%20outcome%20is%20high-dimensional%20%28e.g.%20gene%20expressions%2C%20facial%0Aimages%29%20and%20covariates%20are%20relatively%20limited.%20In%20this%20case%2C%20to%20predict%20one%27s%0Aoutcomes%20under%20counterfactual%20treatments%2C%20it%20is%20crucial%20to%20leverage%20individual%0Ainformation%20contained%20in%20its%20high-dimensional%20observed%20outcome%20in%20addition%20to%0Athe%20covariates.%20Prior%20works%20using%20variational%20inference%20in%20counterfactual%0Agenerative%20modeling%20have%20been%20focusing%20on%20neural%20adaptations%20and%20model%20variants%0Awithin%20the%20conditional%20variational%20autoencoder%20formulation%2C%20which%20we%20argue%20is%0Afundamentally%20ill-suited%20to%20the%20notion%20of%20counterfactual%20in%20causal%20inference.%0AIn%20this%20work%2C%20we%20present%20a%20novel%20variational%20Bayesian%20causal%20inference%0Aframework%20and%20its%20theoretical%20backings%20to%20properly%20handle%20counterfactual%0Agenerative%20modeling%20tasks%2C%20through%20which%20we%20are%20able%20to%20conduct%20counterfactual%0Asupervision%20end-to-end%20during%20training%20without%20any%20counterfactual%20samples%2C%20and%0Aencourage%20latent%20disentanglement%20that%20aids%20the%20correct%20identification%20of%20causal%0Aeffect%20in%20counterfactual%20generations.%20In%20experiments%2C%20we%20demonstrate%20the%0Aadvantage%20of%20our%20framework%20compared%20to%20state-of-the-art%20models%20in%0Acounterfactual%20generative%20modeling%20on%20multiple%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12730v1&entry.124074799=Read"},
{"title": "Diffusion Language Models Are Versatile Protein Learners", "author": "Xinyou Wang and Zaixiang Zheng and Fei Ye and Dongyu Xue and Shujian Huang and Quanquan Gu", "abstract": "  This paper introduces diffusion protein language model (DPLM), a versatile\nprotein language model that demonstrates strong generative and predictive\ncapabilities for protein sequences. We first pre-train scalable DPLMs from\nevolutionary-scale protein sequences within a generative self-supervised\ndiscrete diffusion probabilistic framework, which generalizes language modeling\nfor proteins in a principled way. After pre-training, DPLM exhibits the ability\nto generate structurally plausible, novel, and diverse protein sequences for\nunconditional generation. We further demonstrate the proposed diffusion\ngenerative pre-training makes DPLM possess a better understanding of proteins,\nmaking it a superior representation learner, which can be fine-tuned for\nvarious predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).\nMoreover, DPLM can be tailored for various needs, which showcases its prowess\nof conditional generation in several ways: (1) conditioning on partial peptide\nsequences, e.g., generating scaffolds for functional motifs with high success\nrate; (2) incorporating other modalities as conditioner, e.g.,\nstructure-conditioned generation for inverse folding; and (3) steering sequence\ngeneration towards desired properties, e.g., satisfying specified secondary\nstructures, through a plug-and-play classifier guidance. Code is released at\n\\url{https://github.com/bytedance/dplm}.\n", "link": "http://arxiv.org/abs/2402.18567v2", "date": "2024-10-16", "relevancy": 2.0301, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5054}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Language%20Models%20Are%20Versatile%20Protein%20Learners&body=Title%3A%20Diffusion%20Language%20Models%20Are%20Versatile%20Protein%20Learners%0AAuthor%3A%20Xinyou%20Wang%20and%20Zaixiang%20Zheng%20and%20Fei%20Ye%20and%20Dongyu%20Xue%20and%20Shujian%20Huang%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20This%20paper%20introduces%20diffusion%20protein%20language%20model%20%28DPLM%29%2C%20a%20versatile%0Aprotein%20language%20model%20that%20demonstrates%20strong%20generative%20and%20predictive%0Acapabilities%20for%20protein%20sequences.%20We%20first%20pre-train%20scalable%20DPLMs%20from%0Aevolutionary-scale%20protein%20sequences%20within%20a%20generative%20self-supervised%0Adiscrete%20diffusion%20probabilistic%20framework%2C%20which%20generalizes%20language%20modeling%0Afor%20proteins%20in%20a%20principled%20way.%20After%20pre-training%2C%20DPLM%20exhibits%20the%20ability%0Ato%20generate%20structurally%20plausible%2C%20novel%2C%20and%20diverse%20protein%20sequences%20for%0Aunconditional%20generation.%20We%20further%20demonstrate%20the%20proposed%20diffusion%0Agenerative%20pre-training%20makes%20DPLM%20possess%20a%20better%20understanding%20of%20proteins%2C%0Amaking%20it%20a%20superior%20representation%20learner%2C%20which%20can%20be%20fine-tuned%20for%0Avarious%20predictive%20tasks%2C%20comparing%20favorably%20to%20ESM2%20%28Lin%20et%20al.%2C%202022%29.%0AMoreover%2C%20DPLM%20can%20be%20tailored%20for%20various%20needs%2C%20which%20showcases%20its%20prowess%0Aof%20conditional%20generation%20in%20several%20ways%3A%20%281%29%20conditioning%20on%20partial%20peptide%0Asequences%2C%20e.g.%2C%20generating%20scaffolds%20for%20functional%20motifs%20with%20high%20success%0Arate%3B%20%282%29%20incorporating%20other%20modalities%20as%20conditioner%2C%20e.g.%2C%0Astructure-conditioned%20generation%20for%20inverse%20folding%3B%20and%20%283%29%20steering%20sequence%0Ageneration%20towards%20desired%20properties%2C%20e.g.%2C%20satisfying%20specified%20secondary%0Astructures%2C%20through%20a%20plug-and-play%20classifier%20guidance.%20Code%20is%20released%20at%0A%5Curl%7Bhttps%3A//github.com/bytedance/dplm%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Language%2520Models%2520Are%2520Versatile%2520Protein%2520Learners%26entry.906535625%3DXinyou%2520Wang%2520and%2520Zaixiang%2520Zheng%2520and%2520Fei%2520Ye%2520and%2520Dongyu%2520Xue%2520and%2520Shujian%2520Huang%2520and%2520Quanquan%2520Gu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520diffusion%2520protein%2520language%2520model%2520%2528DPLM%2529%252C%2520a%2520versatile%250Aprotein%2520language%2520model%2520that%2520demonstrates%2520strong%2520generative%2520and%2520predictive%250Acapabilities%2520for%2520protein%2520sequences.%2520We%2520first%2520pre-train%2520scalable%2520DPLMs%2520from%250Aevolutionary-scale%2520protein%2520sequences%2520within%2520a%2520generative%2520self-supervised%250Adiscrete%2520diffusion%2520probabilistic%2520framework%252C%2520which%2520generalizes%2520language%2520modeling%250Afor%2520proteins%2520in%2520a%2520principled%2520way.%2520After%2520pre-training%252C%2520DPLM%2520exhibits%2520the%2520ability%250Ato%2520generate%2520structurally%2520plausible%252C%2520novel%252C%2520and%2520diverse%2520protein%2520sequences%2520for%250Aunconditional%2520generation.%2520We%2520further%2520demonstrate%2520the%2520proposed%2520diffusion%250Agenerative%2520pre-training%2520makes%2520DPLM%2520possess%2520a%2520better%2520understanding%2520of%2520proteins%252C%250Amaking%2520it%2520a%2520superior%2520representation%2520learner%252C%2520which%2520can%2520be%2520fine-tuned%2520for%250Avarious%2520predictive%2520tasks%252C%2520comparing%2520favorably%2520to%2520ESM2%2520%2528Lin%2520et%2520al.%252C%25202022%2529.%250AMoreover%252C%2520DPLM%2520can%2520be%2520tailored%2520for%2520various%2520needs%252C%2520which%2520showcases%2520its%2520prowess%250Aof%2520conditional%2520generation%2520in%2520several%2520ways%253A%2520%25281%2529%2520conditioning%2520on%2520partial%2520peptide%250Asequences%252C%2520e.g.%252C%2520generating%2520scaffolds%2520for%2520functional%2520motifs%2520with%2520high%2520success%250Arate%253B%2520%25282%2529%2520incorporating%2520other%2520modalities%2520as%2520conditioner%252C%2520e.g.%252C%250Astructure-conditioned%2520generation%2520for%2520inverse%2520folding%253B%2520and%2520%25283%2529%2520steering%2520sequence%250Ageneration%2520towards%2520desired%2520properties%252C%2520e.g.%252C%2520satisfying%2520specified%2520secondary%250Astructures%252C%2520through%2520a%2520plug-and-play%2520classifier%2520guidance.%2520Code%2520is%2520released%2520at%250A%255Curl%257Bhttps%253A//github.com/bytedance/dplm%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Language%20Models%20Are%20Versatile%20Protein%20Learners&entry.906535625=Xinyou%20Wang%20and%20Zaixiang%20Zheng%20and%20Fei%20Ye%20and%20Dongyu%20Xue%20and%20Shujian%20Huang%20and%20Quanquan%20Gu&entry.1292438233=%20%20This%20paper%20introduces%20diffusion%20protein%20language%20model%20%28DPLM%29%2C%20a%20versatile%0Aprotein%20language%20model%20that%20demonstrates%20strong%20generative%20and%20predictive%0Acapabilities%20for%20protein%20sequences.%20We%20first%20pre-train%20scalable%20DPLMs%20from%0Aevolutionary-scale%20protein%20sequences%20within%20a%20generative%20self-supervised%0Adiscrete%20diffusion%20probabilistic%20framework%2C%20which%20generalizes%20language%20modeling%0Afor%20proteins%20in%20a%20principled%20way.%20After%20pre-training%2C%20DPLM%20exhibits%20the%20ability%0Ato%20generate%20structurally%20plausible%2C%20novel%2C%20and%20diverse%20protein%20sequences%20for%0Aunconditional%20generation.%20We%20further%20demonstrate%20the%20proposed%20diffusion%0Agenerative%20pre-training%20makes%20DPLM%20possess%20a%20better%20understanding%20of%20proteins%2C%0Amaking%20it%20a%20superior%20representation%20learner%2C%20which%20can%20be%20fine-tuned%20for%0Avarious%20predictive%20tasks%2C%20comparing%20favorably%20to%20ESM2%20%28Lin%20et%20al.%2C%202022%29.%0AMoreover%2C%20DPLM%20can%20be%20tailored%20for%20various%20needs%2C%20which%20showcases%20its%20prowess%0Aof%20conditional%20generation%20in%20several%20ways%3A%20%281%29%20conditioning%20on%20partial%20peptide%0Asequences%2C%20e.g.%2C%20generating%20scaffolds%20for%20functional%20motifs%20with%20high%20success%0Arate%3B%20%282%29%20incorporating%20other%20modalities%20as%20conditioner%2C%20e.g.%2C%0Astructure-conditioned%20generation%20for%20inverse%20folding%3B%20and%20%283%29%20steering%20sequence%0Ageneration%20towards%20desired%20properties%2C%20e.g.%2C%20satisfying%20specified%20secondary%0Astructures%2C%20through%20a%20plug-and-play%20classifier%20guidance.%20Code%20is%20released%20at%0A%5Curl%7Bhttps%3A//github.com/bytedance/dplm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18567v2&entry.124074799=Read"},
{"title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in\n  Large Language Models", "author": "Yijiang River Dong and Hongzhou Lin and Mikhail Belkin and Ramon Huerta and Ivan Vuli\u0107", "abstract": "  Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.\n", "link": "http://arxiv.org/abs/2402.10052v2", "date": "2024-10-16", "relevancy": 2.0261, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5064}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNDIAL%3A%20Self-Distillation%20with%20Adjusted%20Logits%20for%20Robust%20Unlearning%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20UNDIAL%3A%20Self-Distillation%20with%20Adjusted%20Logits%20for%20Robust%20Unlearning%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yijiang%20River%20Dong%20and%20Hongzhou%20Lin%20and%20Mikhail%20Belkin%20and%20Ramon%20Huerta%20and%20Ivan%20Vuli%C4%87%0AAbstract%3A%20%20%20Mitigating%20the%20retention%20of%20sensitive%20or%20private%20information%20in%20large%0Alanguage%20models%20is%20essential%20for%20enhancing%20privacy%20and%20safety.%20Existing%0Aunlearning%20methods%2C%20like%20Gradient%20Ascent%20and%20Negative%20Preference%20Optimization%2C%0Adirectly%20tune%20models%20to%20remove%20unwanted%20information.%20However%2C%20these%20methods%0Aoften%20become%20unstable%20because%20they%20fine-tune%20by%20maximizing%20cross-entropy%20loss%2C%0Awhich%20is%20the%20opposite%20of%20traditional%20loss%20minimization%20in%20learning.%20This%0Areversal%20creates%20instability%2C%20especially%20on%20larger%20datasets%2C%20as%20the%20model%0Astruggles%20to%20balance%20unlearning%20with%20maintaining%20language%20capacity%2C%20leading%20to%0Aover-unlearning.%20In%20this%20paper%2C%20we%20introduce%20UnDIAL%20%28Unlearning%20via%0ASelf-Distillation%20on%20Adjusted%20Logits%29%2C%20a%20novel%20and%20robust%20unlearning%20method.%0AOur%20approach%20leverages%20self-distillation%20to%20adjust%20logits%20and%20selectively%0Areduce%20the%20influence%20of%20targeted%20tokens.%20This%20technique%20ensures%20smooth%0Aconvergence%20and%20avoids%20catastrophic%20forgetting%2C%20even%20in%20challenging%20unlearning%0Atasks%20with%20large%20datasets%20and%20sequential%20unlearning%20requests.%20Extensive%0Aexperiments%20show%20that%20UnDIAL%20can%20achieve%20both%20robustness%20in%20unlearning%20and%0Ascalability%20while%20maintaining%20stable%20training%20dynamics%20and%20resilience%20to%0Ahyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNDIAL%253A%2520Self-Distillation%2520with%2520Adjusted%2520Logits%2520for%2520Robust%2520Unlearning%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYijiang%2520River%2520Dong%2520and%2520Hongzhou%2520Lin%2520and%2520Mikhail%2520Belkin%2520and%2520Ramon%2520Huerta%2520and%2520Ivan%2520Vuli%25C4%2587%26entry.1292438233%3D%2520%2520Mitigating%2520the%2520retention%2520of%2520sensitive%2520or%2520private%2520information%2520in%2520large%250Alanguage%2520models%2520is%2520essential%2520for%2520enhancing%2520privacy%2520and%2520safety.%2520Existing%250Aunlearning%2520methods%252C%2520like%2520Gradient%2520Ascent%2520and%2520Negative%2520Preference%2520Optimization%252C%250Adirectly%2520tune%2520models%2520to%2520remove%2520unwanted%2520information.%2520However%252C%2520these%2520methods%250Aoften%2520become%2520unstable%2520because%2520they%2520fine-tune%2520by%2520maximizing%2520cross-entropy%2520loss%252C%250Awhich%2520is%2520the%2520opposite%2520of%2520traditional%2520loss%2520minimization%2520in%2520learning.%2520This%250Areversal%2520creates%2520instability%252C%2520especially%2520on%2520larger%2520datasets%252C%2520as%2520the%2520model%250Astruggles%2520to%2520balance%2520unlearning%2520with%2520maintaining%2520language%2520capacity%252C%2520leading%2520to%250Aover-unlearning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520UnDIAL%2520%2528Unlearning%2520via%250ASelf-Distillation%2520on%2520Adjusted%2520Logits%2529%252C%2520a%2520novel%2520and%2520robust%2520unlearning%2520method.%250AOur%2520approach%2520leverages%2520self-distillation%2520to%2520adjust%2520logits%2520and%2520selectively%250Areduce%2520the%2520influence%2520of%2520targeted%2520tokens.%2520This%2520technique%2520ensures%2520smooth%250Aconvergence%2520and%2520avoids%2520catastrophic%2520forgetting%252C%2520even%2520in%2520challenging%2520unlearning%250Atasks%2520with%2520large%2520datasets%2520and%2520sequential%2520unlearning%2520requests.%2520Extensive%250Aexperiments%2520show%2520that%2520UnDIAL%2520can%2520achieve%2520both%2520robustness%2520in%2520unlearning%2520and%250Ascalability%2520while%2520maintaining%2520stable%2520training%2520dynamics%2520and%2520resilience%2520to%250Ahyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNDIAL%3A%20Self-Distillation%20with%20Adjusted%20Logits%20for%20Robust%20Unlearning%20in%0A%20%20Large%20Language%20Models&entry.906535625=Yijiang%20River%20Dong%20and%20Hongzhou%20Lin%20and%20Mikhail%20Belkin%20and%20Ramon%20Huerta%20and%20Ivan%20Vuli%C4%87&entry.1292438233=%20%20Mitigating%20the%20retention%20of%20sensitive%20or%20private%20information%20in%20large%0Alanguage%20models%20is%20essential%20for%20enhancing%20privacy%20and%20safety.%20Existing%0Aunlearning%20methods%2C%20like%20Gradient%20Ascent%20and%20Negative%20Preference%20Optimization%2C%0Adirectly%20tune%20models%20to%20remove%20unwanted%20information.%20However%2C%20these%20methods%0Aoften%20become%20unstable%20because%20they%20fine-tune%20by%20maximizing%20cross-entropy%20loss%2C%0Awhich%20is%20the%20opposite%20of%20traditional%20loss%20minimization%20in%20learning.%20This%0Areversal%20creates%20instability%2C%20especially%20on%20larger%20datasets%2C%20as%20the%20model%0Astruggles%20to%20balance%20unlearning%20with%20maintaining%20language%20capacity%2C%20leading%20to%0Aover-unlearning.%20In%20this%20paper%2C%20we%20introduce%20UnDIAL%20%28Unlearning%20via%0ASelf-Distillation%20on%20Adjusted%20Logits%29%2C%20a%20novel%20and%20robust%20unlearning%20method.%0AOur%20approach%20leverages%20self-distillation%20to%20adjust%20logits%20and%20selectively%0Areduce%20the%20influence%20of%20targeted%20tokens.%20This%20technique%20ensures%20smooth%0Aconvergence%20and%20avoids%20catastrophic%20forgetting%2C%20even%20in%20challenging%20unlearning%0Atasks%20with%20large%20datasets%20and%20sequential%20unlearning%20requests.%20Extensive%0Aexperiments%20show%20that%20UnDIAL%20can%20achieve%20both%20robustness%20in%20unlearning%20and%0Ascalability%20while%20maintaining%20stable%20training%20dynamics%20and%20resilience%20to%0Ahyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10052v2&entry.124074799=Read"},
{"title": "CELL your Model: Contrastive Explanations for Large Language Models", "author": "Ronny Luss and Erik Miehling and Amit Dhurandhar", "abstract": "  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n", "link": "http://arxiv.org/abs/2406.11785v2", "date": "2024-10-16", "relevancy": 2.0259, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CELL%20your%20Model%3A%20Contrastive%20Explanations%20for%20Large%20Language%20Models&body=Title%3A%20CELL%20your%20Model%3A%20Contrastive%20Explanations%20for%20Large%20Language%20Models%0AAuthor%3A%20Ronny%20Luss%20and%20Erik%20Miehling%20and%20Amit%20Dhurandhar%0AAbstract%3A%20%20%20The%20advent%20of%20black-box%20deep%20neural%20network%20classification%20models%20has%20sparked%0Athe%20need%20to%20explain%20their%20decisions.%20However%2C%20in%20the%20case%20of%20generative%20AI%2C%0Asuch%20as%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20no%20class%20prediction%20to%20explain.%0ARather%2C%20one%20can%20ask%20why%20an%20LLM%20output%20a%20particular%20response%20to%20a%20given%20prompt.%0AIn%20this%20paper%2C%20we%20answer%20this%20question%20by%20proposing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20contrastive%20explanation%20methods%20requiring%20simply%0Ablack-box/query%20access.%20Our%20explanations%20suggest%20that%20an%20LLM%20outputs%20a%20reply%20to%0Aa%20given%20prompt%20because%20if%20the%20prompt%20was%20slightly%20modified%2C%20the%20LLM%20would%20have%0Agiven%20a%20different%20response%20that%20is%20either%20less%20preferable%20or%20contradicts%20the%0Aoriginal%20response.%20The%20key%20insight%20is%20that%20contrastive%20explanations%20simply%0Arequire%20a%20scoring%20function%20that%20has%20meaning%20to%20the%20user%20and%20not%20necessarily%20a%0Aspecific%20real%20valued%20quantity%20%28viz.%20class%20label%29.%20We%20offer%20two%20algorithms%20for%0Afinding%20contrastive%20explanations%3A%20i%29%20A%20myopic%20algorithm%2C%20which%20although%0Aeffective%20in%20creating%20contrasts%2C%20requires%20many%20model%20calls%20and%20ii%29%20A%20budgeted%0Aalgorithm%2C%20our%20main%20algorithmic%20contribution%2C%20which%20intelligently%20creates%0Acontrasts%20adhering%20to%20a%20query%20budget%2C%20necessary%20for%20longer%20contexts.%20We%20show%0Athe%20efficacy%20of%20these%20methods%20on%20diverse%20natural%20language%20tasks%20such%20as%0Aopen-text%20generation%2C%20automated%20red%20teaming%2C%20and%20explaining%20conversational%0Adegradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCELL%2520your%2520Model%253A%2520Contrastive%2520Explanations%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DRonny%2520Luss%2520and%2520Erik%2520Miehling%2520and%2520Amit%2520Dhurandhar%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520black-box%2520deep%2520neural%2520network%2520classification%2520models%2520has%2520sparked%250Athe%2520need%2520to%2520explain%2520their%2520decisions.%2520However%252C%2520in%2520the%2520case%2520of%2520generative%2520AI%252C%250Asuch%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%2520is%2520no%2520class%2520prediction%2520to%2520explain.%250ARather%252C%2520one%2520can%2520ask%2520why%2520an%2520LLM%2520output%2520a%2520particular%2520response%2520to%2520a%2520given%2520prompt.%250AIn%2520this%2520paper%252C%2520we%2520answer%2520this%2520question%2520by%2520proposing%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520contrastive%2520explanation%2520methods%2520requiring%2520simply%250Ablack-box/query%2520access.%2520Our%2520explanations%2520suggest%2520that%2520an%2520LLM%2520outputs%2520a%2520reply%2520to%250Aa%2520given%2520prompt%2520because%2520if%2520the%2520prompt%2520was%2520slightly%2520modified%252C%2520the%2520LLM%2520would%2520have%250Agiven%2520a%2520different%2520response%2520that%2520is%2520either%2520less%2520preferable%2520or%2520contradicts%2520the%250Aoriginal%2520response.%2520The%2520key%2520insight%2520is%2520that%2520contrastive%2520explanations%2520simply%250Arequire%2520a%2520scoring%2520function%2520that%2520has%2520meaning%2520to%2520the%2520user%2520and%2520not%2520necessarily%2520a%250Aspecific%2520real%2520valued%2520quantity%2520%2528viz.%2520class%2520label%2529.%2520We%2520offer%2520two%2520algorithms%2520for%250Afinding%2520contrastive%2520explanations%253A%2520i%2529%2520A%2520myopic%2520algorithm%252C%2520which%2520although%250Aeffective%2520in%2520creating%2520contrasts%252C%2520requires%2520many%2520model%2520calls%2520and%2520ii%2529%2520A%2520budgeted%250Aalgorithm%252C%2520our%2520main%2520algorithmic%2520contribution%252C%2520which%2520intelligently%2520creates%250Acontrasts%2520adhering%2520to%2520a%2520query%2520budget%252C%2520necessary%2520for%2520longer%2520contexts.%2520We%2520show%250Athe%2520efficacy%2520of%2520these%2520methods%2520on%2520diverse%2520natural%2520language%2520tasks%2520such%2520as%250Aopen-text%2520generation%252C%2520automated%2520red%2520teaming%252C%2520and%2520explaining%2520conversational%250Adegradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CELL%20your%20Model%3A%20Contrastive%20Explanations%20for%20Large%20Language%20Models&entry.906535625=Ronny%20Luss%20and%20Erik%20Miehling%20and%20Amit%20Dhurandhar&entry.1292438233=%20%20The%20advent%20of%20black-box%20deep%20neural%20network%20classification%20models%20has%20sparked%0Athe%20need%20to%20explain%20their%20decisions.%20However%2C%20in%20the%20case%20of%20generative%20AI%2C%0Asuch%20as%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20no%20class%20prediction%20to%20explain.%0ARather%2C%20one%20can%20ask%20why%20an%20LLM%20output%20a%20particular%20response%20to%20a%20given%20prompt.%0AIn%20this%20paper%2C%20we%20answer%20this%20question%20by%20proposing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20contrastive%20explanation%20methods%20requiring%20simply%0Ablack-box/query%20access.%20Our%20explanations%20suggest%20that%20an%20LLM%20outputs%20a%20reply%20to%0Aa%20given%20prompt%20because%20if%20the%20prompt%20was%20slightly%20modified%2C%20the%20LLM%20would%20have%0Agiven%20a%20different%20response%20that%20is%20either%20less%20preferable%20or%20contradicts%20the%0Aoriginal%20response.%20The%20key%20insight%20is%20that%20contrastive%20explanations%20simply%0Arequire%20a%20scoring%20function%20that%20has%20meaning%20to%20the%20user%20and%20not%20necessarily%20a%0Aspecific%20real%20valued%20quantity%20%28viz.%20class%20label%29.%20We%20offer%20two%20algorithms%20for%0Afinding%20contrastive%20explanations%3A%20i%29%20A%20myopic%20algorithm%2C%20which%20although%0Aeffective%20in%20creating%20contrasts%2C%20requires%20many%20model%20calls%20and%20ii%29%20A%20budgeted%0Aalgorithm%2C%20our%20main%20algorithmic%20contribution%2C%20which%20intelligently%20creates%0Acontrasts%20adhering%20to%20a%20query%20budget%2C%20necessary%20for%20longer%20contexts.%20We%20show%0Athe%20efficacy%20of%20these%20methods%20on%20diverse%20natural%20language%20tasks%20such%20as%0Aopen-text%20generation%2C%20automated%20red%20teaming%2C%20and%20explaining%20conversational%0Adegradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11785v2&entry.124074799=Read"},
{"title": "Explainable Moral Values: a neuro-symbolic approach to value\n  classification", "author": "Nicolas Lazzari and Stefano De Giorgis and Aldo Gangemi and Valentina Presutti", "abstract": "  This work explores the integration of ontology-based reasoning and Machine\nLearning techniques for explainable value classification. By relying on an\nontological formalization of moral values as in the Moral Foundations Theory,\nrelying on the DnS Ontology Design Pattern, the \\textit{sandra} neuro-symbolic\nreasoner is used to infer values (fomalized as descriptions) that are\n\\emph{satisfied by} a certain sentence. Sentences, alongside their structured\nrepresentation, are automatically generated using an open-source Large Language\nModel. The inferred descriptions are used to automatically detect the value\nassociated with a sentence. We show that only relying on the reasoner's\ninference results in explainable classification comparable to other more\ncomplex approaches. We show that combining the reasoner's inferences with\ndistributional semantics methods largely outperforms all the baselines,\nincluding complex models based on neural network architectures. Finally, we\nbuild a visualization tool to explore the potential of theory-based values\nclassification, which is publicly available at http://xmv.geomeaning.com/.\n", "link": "http://arxiv.org/abs/2410.12631v1", "date": "2024-10-16", "relevancy": 2.014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Moral%20Values%3A%20a%20neuro-symbolic%20approach%20to%20value%0A%20%20classification&body=Title%3A%20Explainable%20Moral%20Values%3A%20a%20neuro-symbolic%20approach%20to%20value%0A%20%20classification%0AAuthor%3A%20Nicolas%20Lazzari%20and%20Stefano%20De%20Giorgis%20and%20Aldo%20Gangemi%20and%20Valentina%20Presutti%0AAbstract%3A%20%20%20This%20work%20explores%20the%20integration%20of%20ontology-based%20reasoning%20and%20Machine%0ALearning%20techniques%20for%20explainable%20value%20classification.%20By%20relying%20on%20an%0Aontological%20formalization%20of%20moral%20values%20as%20in%20the%20Moral%20Foundations%20Theory%2C%0Arelying%20on%20the%20DnS%20Ontology%20Design%20Pattern%2C%20the%20%5Ctextit%7Bsandra%7D%20neuro-symbolic%0Areasoner%20is%20used%20to%20infer%20values%20%28fomalized%20as%20descriptions%29%20that%20are%0A%5Cemph%7Bsatisfied%20by%7D%20a%20certain%20sentence.%20Sentences%2C%20alongside%20their%20structured%0Arepresentation%2C%20are%20automatically%20generated%20using%20an%20open-source%20Large%20Language%0AModel.%20The%20inferred%20descriptions%20are%20used%20to%20automatically%20detect%20the%20value%0Aassociated%20with%20a%20sentence.%20We%20show%20that%20only%20relying%20on%20the%20reasoner%27s%0Ainference%20results%20in%20explainable%20classification%20comparable%20to%20other%20more%0Acomplex%20approaches.%20We%20show%20that%20combining%20the%20reasoner%27s%20inferences%20with%0Adistributional%20semantics%20methods%20largely%20outperforms%20all%20the%20baselines%2C%0Aincluding%20complex%20models%20based%20on%20neural%20network%20architectures.%20Finally%2C%20we%0Abuild%20a%20visualization%20tool%20to%20explore%20the%20potential%20of%20theory-based%20values%0Aclassification%2C%20which%20is%20publicly%20available%20at%20http%3A//xmv.geomeaning.com/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Moral%2520Values%253A%2520a%2520neuro-symbolic%2520approach%2520to%2520value%250A%2520%2520classification%26entry.906535625%3DNicolas%2520Lazzari%2520and%2520Stefano%2520De%2520Giorgis%2520and%2520Aldo%2520Gangemi%2520and%2520Valentina%2520Presutti%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520integration%2520of%2520ontology-based%2520reasoning%2520and%2520Machine%250ALearning%2520techniques%2520for%2520explainable%2520value%2520classification.%2520By%2520relying%2520on%2520an%250Aontological%2520formalization%2520of%2520moral%2520values%2520as%2520in%2520the%2520Moral%2520Foundations%2520Theory%252C%250Arelying%2520on%2520the%2520DnS%2520Ontology%2520Design%2520Pattern%252C%2520the%2520%255Ctextit%257Bsandra%257D%2520neuro-symbolic%250Areasoner%2520is%2520used%2520to%2520infer%2520values%2520%2528fomalized%2520as%2520descriptions%2529%2520that%2520are%250A%255Cemph%257Bsatisfied%2520by%257D%2520a%2520certain%2520sentence.%2520Sentences%252C%2520alongside%2520their%2520structured%250Arepresentation%252C%2520are%2520automatically%2520generated%2520using%2520an%2520open-source%2520Large%2520Language%250AModel.%2520The%2520inferred%2520descriptions%2520are%2520used%2520to%2520automatically%2520detect%2520the%2520value%250Aassociated%2520with%2520a%2520sentence.%2520We%2520show%2520that%2520only%2520relying%2520on%2520the%2520reasoner%2527s%250Ainference%2520results%2520in%2520explainable%2520classification%2520comparable%2520to%2520other%2520more%250Acomplex%2520approaches.%2520We%2520show%2520that%2520combining%2520the%2520reasoner%2527s%2520inferences%2520with%250Adistributional%2520semantics%2520methods%2520largely%2520outperforms%2520all%2520the%2520baselines%252C%250Aincluding%2520complex%2520models%2520based%2520on%2520neural%2520network%2520architectures.%2520Finally%252C%2520we%250Abuild%2520a%2520visualization%2520tool%2520to%2520explore%2520the%2520potential%2520of%2520theory-based%2520values%250Aclassification%252C%2520which%2520is%2520publicly%2520available%2520at%2520http%253A//xmv.geomeaning.com/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Moral%20Values%3A%20a%20neuro-symbolic%20approach%20to%20value%0A%20%20classification&entry.906535625=Nicolas%20Lazzari%20and%20Stefano%20De%20Giorgis%20and%20Aldo%20Gangemi%20and%20Valentina%20Presutti&entry.1292438233=%20%20This%20work%20explores%20the%20integration%20of%20ontology-based%20reasoning%20and%20Machine%0ALearning%20techniques%20for%20explainable%20value%20classification.%20By%20relying%20on%20an%0Aontological%20formalization%20of%20moral%20values%20as%20in%20the%20Moral%20Foundations%20Theory%2C%0Arelying%20on%20the%20DnS%20Ontology%20Design%20Pattern%2C%20the%20%5Ctextit%7Bsandra%7D%20neuro-symbolic%0Areasoner%20is%20used%20to%20infer%20values%20%28fomalized%20as%20descriptions%29%20that%20are%0A%5Cemph%7Bsatisfied%20by%7D%20a%20certain%20sentence.%20Sentences%2C%20alongside%20their%20structured%0Arepresentation%2C%20are%20automatically%20generated%20using%20an%20open-source%20Large%20Language%0AModel.%20The%20inferred%20descriptions%20are%20used%20to%20automatically%20detect%20the%20value%0Aassociated%20with%20a%20sentence.%20We%20show%20that%20only%20relying%20on%20the%20reasoner%27s%0Ainference%20results%20in%20explainable%20classification%20comparable%20to%20other%20more%0Acomplex%20approaches.%20We%20show%20that%20combining%20the%20reasoner%27s%20inferences%20with%0Adistributional%20semantics%20methods%20largely%20outperforms%20all%20the%20baselines%2C%0Aincluding%20complex%20models%20based%20on%20neural%20network%20architectures.%20Finally%2C%20we%0Abuild%20a%20visualization%20tool%20to%20explore%20the%20potential%20of%20theory-based%20values%0Aclassification%2C%20which%20is%20publicly%20available%20at%20http%3A//xmv.geomeaning.com/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12631v1&entry.124074799=Read"},
{"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "author": "Zhaoyang Wang and Weilei He and Zhiyuan Liang and Xuchao Zhang and Chetan Bansal and Ying Wei and Weitong Zhang and Huaxiu Yao", "abstract": "  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n", "link": "http://arxiv.org/abs/2410.12735v1", "date": "2024-10-16", "relevancy": 2.0131, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5065}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREAM%3A%20Consistency%20Regularized%20Self-Rewarding%20Language%20Models&body=Title%3A%20CREAM%3A%20Consistency%20Regularized%20Self-Rewarding%20Language%20Models%0AAuthor%3A%20Zhaoyang%20Wang%20and%20Weilei%20He%20and%20Zhiyuan%20Liang%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Ying%20Wei%20and%20Weitong%20Zhang%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20Recent%20self-rewarding%20large%20language%20models%20%28LLM%29%20have%20successfully%20applied%0ALLM-as-a-Judge%20to%20iteratively%20improve%20the%20alignment%20performance%20without%20the%0Aneed%20of%20human%20annotations%20for%20preference%20data.%20These%20methods%20commonly%20utilize%0Athe%20same%20LLM%20to%20act%20as%20both%20the%20policy%20model%20%28which%20generates%20responses%29%20and%0Athe%20reward%20model%20%28which%20scores%20and%20ranks%20those%20responses%29.%20The%20ranked%20responses%0Aare%20then%20used%20as%20preference%20pairs%20to%20train%20the%20LLM%20via%20direct%20alignment%0Atechnologies%20%28e.g.%20DPO%29.%20However%2C%20it%20is%20noteworthy%20that%20throughout%20this%0Aprocess%2C%20there%20is%20no%20guarantee%20of%20accuracy%20in%20the%20rewarding%20and%20ranking%2C%20which%0Ais%20critical%20for%20ensuring%20accurate%20rewards%20and%20high-quality%20preference%20data.%0AEmpirical%20results%20from%20relatively%20small%20LLMs%20%28e.g.%2C%207B%20parameters%29%20also%0Aindicate%20that%20improvements%20from%20self-rewarding%20may%20diminish%20after%20several%0Aiterations%20in%20certain%20situations%2C%20which%20we%20hypothesize%20is%20due%20to%20accumulated%0Abias%20in%20the%20reward%20system.%20This%20bias%20can%20lead%20to%20unreliable%20preference%20data%20for%0Atraining%20the%20LLM.%20To%20address%20this%20issue%2C%20we%20first%20formulate%20and%20analyze%20the%0Ageneralized%20iterative%20preference%20fine-tuning%20framework%20for%20self-rewarding%0Alanguage%20model.%20We%20then%20introduce%20the%20regularization%20to%20this%20generalized%0Aframework%20to%20mitigate%20the%20overconfident%20preference%20labeling%20in%20the%0Aself-rewarding%20process.%20Based%20on%20this%20theoretical%20insight%2C%20we%20propose%20a%0AConsistency%20Regularized%20sElf-rewarding%20lAnguage%20Model%20%28CREAM%29%20that%20leverages%0Athe%20rewarding%20consistency%20across%20different%20iterations%20to%20regularize%20the%0Aself-rewarding%20training%2C%20helping%20the%20model%20to%20learn%20from%20more%20reliable%0Apreference%20data.%20With%20this%20explicit%20regularization%2C%20our%20empirical%20results%0Ademonstrate%20the%20superiority%20of%20CREAM%20in%20improving%20both%20reward%20consistency%20and%0Aalignment%20performance.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Raibows/CREAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREAM%253A%2520Consistency%2520Regularized%2520Self-Rewarding%2520Language%2520Models%26entry.906535625%3DZhaoyang%2520Wang%2520and%2520Weilei%2520He%2520and%2520Zhiyuan%2520Liang%2520and%2520Xuchao%2520Zhang%2520and%2520Chetan%2520Bansal%2520and%2520Ying%2520Wei%2520and%2520Weitong%2520Zhang%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520Recent%2520self-rewarding%2520large%2520language%2520models%2520%2528LLM%2529%2520have%2520successfully%2520applied%250ALLM-as-a-Judge%2520to%2520iteratively%2520improve%2520the%2520alignment%2520performance%2520without%2520the%250Aneed%2520of%2520human%2520annotations%2520for%2520preference%2520data.%2520These%2520methods%2520commonly%2520utilize%250Athe%2520same%2520LLM%2520to%2520act%2520as%2520both%2520the%2520policy%2520model%2520%2528which%2520generates%2520responses%2529%2520and%250Athe%2520reward%2520model%2520%2528which%2520scores%2520and%2520ranks%2520those%2520responses%2529.%2520The%2520ranked%2520responses%250Aare%2520then%2520used%2520as%2520preference%2520pairs%2520to%2520train%2520the%2520LLM%2520via%2520direct%2520alignment%250Atechnologies%2520%2528e.g.%2520DPO%2529.%2520However%252C%2520it%2520is%2520noteworthy%2520that%2520throughout%2520this%250Aprocess%252C%2520there%2520is%2520no%2520guarantee%2520of%2520accuracy%2520in%2520the%2520rewarding%2520and%2520ranking%252C%2520which%250Ais%2520critical%2520for%2520ensuring%2520accurate%2520rewards%2520and%2520high-quality%2520preference%2520data.%250AEmpirical%2520results%2520from%2520relatively%2520small%2520LLMs%2520%2528e.g.%252C%25207B%2520parameters%2529%2520also%250Aindicate%2520that%2520improvements%2520from%2520self-rewarding%2520may%2520diminish%2520after%2520several%250Aiterations%2520in%2520certain%2520situations%252C%2520which%2520we%2520hypothesize%2520is%2520due%2520to%2520accumulated%250Abias%2520in%2520the%2520reward%2520system.%2520This%2520bias%2520can%2520lead%2520to%2520unreliable%2520preference%2520data%2520for%250Atraining%2520the%2520LLM.%2520To%2520address%2520this%2520issue%252C%2520we%2520first%2520formulate%2520and%2520analyze%2520the%250Ageneralized%2520iterative%2520preference%2520fine-tuning%2520framework%2520for%2520self-rewarding%250Alanguage%2520model.%2520We%2520then%2520introduce%2520the%2520regularization%2520to%2520this%2520generalized%250Aframework%2520to%2520mitigate%2520the%2520overconfident%2520preference%2520labeling%2520in%2520the%250Aself-rewarding%2520process.%2520Based%2520on%2520this%2520theoretical%2520insight%252C%2520we%2520propose%2520a%250AConsistency%2520Regularized%2520sElf-rewarding%2520lAnguage%2520Model%2520%2528CREAM%2529%2520that%2520leverages%250Athe%2520rewarding%2520consistency%2520across%2520different%2520iterations%2520to%2520regularize%2520the%250Aself-rewarding%2520training%252C%2520helping%2520the%2520model%2520to%2520learn%2520from%2520more%2520reliable%250Apreference%2520data.%2520With%2520this%2520explicit%2520regularization%252C%2520our%2520empirical%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520CREAM%2520in%2520improving%2520both%2520reward%2520consistency%2520and%250Aalignment%2520performance.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Raibows/CREAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREAM%3A%20Consistency%20Regularized%20Self-Rewarding%20Language%20Models&entry.906535625=Zhaoyang%20Wang%20and%20Weilei%20He%20and%20Zhiyuan%20Liang%20and%20Xuchao%20Zhang%20and%20Chetan%20Bansal%20and%20Ying%20Wei%20and%20Weitong%20Zhang%20and%20Huaxiu%20Yao&entry.1292438233=%20%20Recent%20self-rewarding%20large%20language%20models%20%28LLM%29%20have%20successfully%20applied%0ALLM-as-a-Judge%20to%20iteratively%20improve%20the%20alignment%20performance%20without%20the%0Aneed%20of%20human%20annotations%20for%20preference%20data.%20These%20methods%20commonly%20utilize%0Athe%20same%20LLM%20to%20act%20as%20both%20the%20policy%20model%20%28which%20generates%20responses%29%20and%0Athe%20reward%20model%20%28which%20scores%20and%20ranks%20those%20responses%29.%20The%20ranked%20responses%0Aare%20then%20used%20as%20preference%20pairs%20to%20train%20the%20LLM%20via%20direct%20alignment%0Atechnologies%20%28e.g.%20DPO%29.%20However%2C%20it%20is%20noteworthy%20that%20throughout%20this%0Aprocess%2C%20there%20is%20no%20guarantee%20of%20accuracy%20in%20the%20rewarding%20and%20ranking%2C%20which%0Ais%20critical%20for%20ensuring%20accurate%20rewards%20and%20high-quality%20preference%20data.%0AEmpirical%20results%20from%20relatively%20small%20LLMs%20%28e.g.%2C%207B%20parameters%29%20also%0Aindicate%20that%20improvements%20from%20self-rewarding%20may%20diminish%20after%20several%0Aiterations%20in%20certain%20situations%2C%20which%20we%20hypothesize%20is%20due%20to%20accumulated%0Abias%20in%20the%20reward%20system.%20This%20bias%20can%20lead%20to%20unreliable%20preference%20data%20for%0Atraining%20the%20LLM.%20To%20address%20this%20issue%2C%20we%20first%20formulate%20and%20analyze%20the%0Ageneralized%20iterative%20preference%20fine-tuning%20framework%20for%20self-rewarding%0Alanguage%20model.%20We%20then%20introduce%20the%20regularization%20to%20this%20generalized%0Aframework%20to%20mitigate%20the%20overconfident%20preference%20labeling%20in%20the%0Aself-rewarding%20process.%20Based%20on%20this%20theoretical%20insight%2C%20we%20propose%20a%0AConsistency%20Regularized%20sElf-rewarding%20lAnguage%20Model%20%28CREAM%29%20that%20leverages%0Athe%20rewarding%20consistency%20across%20different%20iterations%20to%20regularize%20the%0Aself-rewarding%20training%2C%20helping%20the%20model%20to%20learn%20from%20more%20reliable%0Apreference%20data.%20With%20this%20explicit%20regularization%2C%20our%20empirical%20results%0Ademonstrate%20the%20superiority%20of%20CREAM%20in%20improving%20both%20reward%20consistency%20and%0Aalignment%20performance.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Raibows/CREAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12735v1&entry.124074799=Read"},
{"title": "Context Matters: Leveraging Contextual Features for Time Series\n  Forecasting", "author": "Sameep Chattopadhyay and Pulkit Paliwal and Sai Shankar Narasimhan and Shubhankar Agarwal and Sandeep P. Chinchali", "abstract": "  Time series forecasts are often influenced by exogenous contextual features\nin addition to their corresponding history. For example, in financial settings,\nit is hard to accurately predict a stock price without considering public\nsentiments and policy decisions in the form of news articles, tweets, etc.\nThough this is common knowledge, the current state-of-the-art (SOTA)\nforecasting models fail to incorporate such contextual information, owing to\nits heterogeneity and multimodal nature. To address this, we introduce\nContextFormer, a novel plug-and-play method to surgically integrate multimodal\ncontextual information into existing pre-trained forecasting models.\nContextFormer effectively distills forecast-specific information from rich\nmultimodal contexts, including categorical, continuous, time-varying, and even\ntextual information, to significantly enhance the performance of existing base\nforecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on\na range of real-world datasets spanning energy, traffic, environmental, and\nfinancial domains.\n", "link": "http://arxiv.org/abs/2410.12672v1", "date": "2024-10-16", "relevancy": 1.9553, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Sameep%20Chattopadhyay%20and%20Pulkit%20Paliwal%20and%20Sai%20Shankar%20Narasimhan%20and%20Shubhankar%20Agarwal%20and%20Sandeep%20P.%20Chinchali%0AAbstract%3A%20%20%20Time%20series%20forecasts%20are%20often%20influenced%20by%20exogenous%20contextual%20features%0Ain%20addition%20to%20their%20corresponding%20history.%20For%20example%2C%20in%20financial%20settings%2C%0Ait%20is%20hard%20to%20accurately%20predict%20a%20stock%20price%20without%20considering%20public%0Asentiments%20and%20policy%20decisions%20in%20the%20form%20of%20news%20articles%2C%20tweets%2C%20etc.%0AThough%20this%20is%20common%20knowledge%2C%20the%20current%20state-of-the-art%20%28SOTA%29%0Aforecasting%20models%20fail%20to%20incorporate%20such%20contextual%20information%2C%20owing%20to%0Aits%20heterogeneity%20and%20multimodal%20nature.%20To%20address%20this%2C%20we%20introduce%0AContextFormer%2C%20a%20novel%20plug-and-play%20method%20to%20surgically%20integrate%20multimodal%0Acontextual%20information%20into%20existing%20pre-trained%20forecasting%20models.%0AContextFormer%20effectively%20distills%20forecast-specific%20information%20from%20rich%0Amultimodal%20contexts%2C%20including%20categorical%2C%20continuous%2C%20time-varying%2C%20and%20even%0Atextual%20information%2C%20to%20significantly%20enhance%20the%20performance%20of%20existing%20base%0Aforecasters.%20ContextFormer%20outperforms%20SOTA%20forecasting%20models%20by%20up%20to%2030%25%20on%0Aa%20range%20of%20real-world%20datasets%20spanning%20energy%2C%20traffic%2C%20environmental%2C%20and%0Afinancial%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Matters%253A%2520Leveraging%2520Contextual%2520Features%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DSameep%2520Chattopadhyay%2520and%2520Pulkit%2520Paliwal%2520and%2520Sai%2520Shankar%2520Narasimhan%2520and%2520Shubhankar%2520Agarwal%2520and%2520Sandeep%2520P.%2520Chinchali%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasts%2520are%2520often%2520influenced%2520by%2520exogenous%2520contextual%2520features%250Ain%2520addition%2520to%2520their%2520corresponding%2520history.%2520For%2520example%252C%2520in%2520financial%2520settings%252C%250Ait%2520is%2520hard%2520to%2520accurately%2520predict%2520a%2520stock%2520price%2520without%2520considering%2520public%250Asentiments%2520and%2520policy%2520decisions%2520in%2520the%2520form%2520of%2520news%2520articles%252C%2520tweets%252C%2520etc.%250AThough%2520this%2520is%2520common%2520knowledge%252C%2520the%2520current%2520state-of-the-art%2520%2528SOTA%2529%250Aforecasting%2520models%2520fail%2520to%2520incorporate%2520such%2520contextual%2520information%252C%2520owing%2520to%250Aits%2520heterogeneity%2520and%2520multimodal%2520nature.%2520To%2520address%2520this%252C%2520we%2520introduce%250AContextFormer%252C%2520a%2520novel%2520plug-and-play%2520method%2520to%2520surgically%2520integrate%2520multimodal%250Acontextual%2520information%2520into%2520existing%2520pre-trained%2520forecasting%2520models.%250AContextFormer%2520effectively%2520distills%2520forecast-specific%2520information%2520from%2520rich%250Amultimodal%2520contexts%252C%2520including%2520categorical%252C%2520continuous%252C%2520time-varying%252C%2520and%2520even%250Atextual%2520information%252C%2520to%2520significantly%2520enhance%2520the%2520performance%2520of%2520existing%2520base%250Aforecasters.%2520ContextFormer%2520outperforms%2520SOTA%2520forecasting%2520models%2520by%2520up%2520to%252030%2525%2520on%250Aa%2520range%2520of%2520real-world%2520datasets%2520spanning%2520energy%252C%2520traffic%252C%2520environmental%252C%2520and%250Afinancial%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Sameep%20Chattopadhyay%20and%20Pulkit%20Paliwal%20and%20Sai%20Shankar%20Narasimhan%20and%20Shubhankar%20Agarwal%20and%20Sandeep%20P.%20Chinchali&entry.1292438233=%20%20Time%20series%20forecasts%20are%20often%20influenced%20by%20exogenous%20contextual%20features%0Ain%20addition%20to%20their%20corresponding%20history.%20For%20example%2C%20in%20financial%20settings%2C%0Ait%20is%20hard%20to%20accurately%20predict%20a%20stock%20price%20without%20considering%20public%0Asentiments%20and%20policy%20decisions%20in%20the%20form%20of%20news%20articles%2C%20tweets%2C%20etc.%0AThough%20this%20is%20common%20knowledge%2C%20the%20current%20state-of-the-art%20%28SOTA%29%0Aforecasting%20models%20fail%20to%20incorporate%20such%20contextual%20information%2C%20owing%20to%0Aits%20heterogeneity%20and%20multimodal%20nature.%20To%20address%20this%2C%20we%20introduce%0AContextFormer%2C%20a%20novel%20plug-and-play%20method%20to%20surgically%20integrate%20multimodal%0Acontextual%20information%20into%20existing%20pre-trained%20forecasting%20models.%0AContextFormer%20effectively%20distills%20forecast-specific%20information%20from%20rich%0Amultimodal%20contexts%2C%20including%20categorical%2C%20continuous%2C%20time-varying%2C%20and%20even%0Atextual%20information%2C%20to%20significantly%20enhance%20the%20performance%20of%20existing%20base%0Aforecasters.%20ContextFormer%20outperforms%20SOTA%20forecasting%20models%20by%20up%20to%2030%25%20on%0Aa%20range%20of%20real-world%20datasets%20spanning%20energy%2C%20traffic%2C%20environmental%2C%20and%0Afinancial%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12672v1&entry.124074799=Read"},
{"title": "Bad Students Make Great Teachers: Active Learning Accelerates\n  Large-Scale Visual Understanding", "author": "Talfan Evans and Shreya Pathak and Hamza Merzic and Jonathan Schwarz and Ryutaro Tanno and Olivier J. Henaff", "abstract": "  Power-law scaling indicates that large-scale training with uniform sampling\nis prohibitively slow. Active learning methods aim to increase data efficiency\nby prioritizing learning on the most relevant examples. Despite their appeal,\nthese methods have yet to be widely adopted since no one algorithm has been\nshown to a) generalize across models and tasks b) scale to large datasets and\nc) yield overall FLOP savings when accounting for the overhead of data\nselection. In this work we propose a method which satisfies these three\nproperties, leveraging small, cheap proxy models to estimate \"learnability\"\nscores for datapoints, which are used to prioritize data for the training of\nmuch larger models. As a result, our models require 46% and 51% fewer training\nupdates and up to 25% less total computation to reach the same performance as\nuniformly trained visual classifiers on JFT and multimodal models on ALIGN.\nFinally, we find our data-prioritization scheme to be complementary with recent\ndata-curation and learning objectives, yielding a new state-of-the-art in\nseveral multimodal transfer tasks.\n", "link": "http://arxiv.org/abs/2312.05328v4", "date": "2024-10-16", "relevancy": 1.7262, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5858}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5643}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bad%20Students%20Make%20Great%20Teachers%3A%20Active%20Learning%20Accelerates%0A%20%20Large-Scale%20Visual%20Understanding&body=Title%3A%20Bad%20Students%20Make%20Great%20Teachers%3A%20Active%20Learning%20Accelerates%0A%20%20Large-Scale%20Visual%20Understanding%0AAuthor%3A%20Talfan%20Evans%20and%20Shreya%20Pathak%20and%20Hamza%20Merzic%20and%20Jonathan%20Schwarz%20and%20Ryutaro%20Tanno%20and%20Olivier%20J.%20Henaff%0AAbstract%3A%20%20%20Power-law%20scaling%20indicates%20that%20large-scale%20training%20with%20uniform%20sampling%0Ais%20prohibitively%20slow.%20Active%20learning%20methods%20aim%20to%20increase%20data%20efficiency%0Aby%20prioritizing%20learning%20on%20the%20most%20relevant%20examples.%20Despite%20their%20appeal%2C%0Athese%20methods%20have%20yet%20to%20be%20widely%20adopted%20since%20no%20one%20algorithm%20has%20been%0Ashown%20to%20a%29%20generalize%20across%20models%20and%20tasks%20b%29%20scale%20to%20large%20datasets%20and%0Ac%29%20yield%20overall%20FLOP%20savings%20when%20accounting%20for%20the%20overhead%20of%20data%0Aselection.%20In%20this%20work%20we%20propose%20a%20method%20which%20satisfies%20these%20three%0Aproperties%2C%20leveraging%20small%2C%20cheap%20proxy%20models%20to%20estimate%20%22learnability%22%0Ascores%20for%20datapoints%2C%20which%20are%20used%20to%20prioritize%20data%20for%20the%20training%20of%0Amuch%20larger%20models.%20As%20a%20result%2C%20our%20models%20require%2046%25%20and%2051%25%20fewer%20training%0Aupdates%20and%20up%20to%2025%25%20less%20total%20computation%20to%20reach%20the%20same%20performance%20as%0Auniformly%20trained%20visual%20classifiers%20on%20JFT%20and%20multimodal%20models%20on%20ALIGN.%0AFinally%2C%20we%20find%20our%20data-prioritization%20scheme%20to%20be%20complementary%20with%20recent%0Adata-curation%20and%20learning%20objectives%2C%20yielding%20a%20new%20state-of-the-art%20in%0Aseveral%20multimodal%20transfer%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05328v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBad%2520Students%2520Make%2520Great%2520Teachers%253A%2520Active%2520Learning%2520Accelerates%250A%2520%2520Large-Scale%2520Visual%2520Understanding%26entry.906535625%3DTalfan%2520Evans%2520and%2520Shreya%2520Pathak%2520and%2520Hamza%2520Merzic%2520and%2520Jonathan%2520Schwarz%2520and%2520Ryutaro%2520Tanno%2520and%2520Olivier%2520J.%2520Henaff%26entry.1292438233%3D%2520%2520Power-law%2520scaling%2520indicates%2520that%2520large-scale%2520training%2520with%2520uniform%2520sampling%250Ais%2520prohibitively%2520slow.%2520Active%2520learning%2520methods%2520aim%2520to%2520increase%2520data%2520efficiency%250Aby%2520prioritizing%2520learning%2520on%2520the%2520most%2520relevant%2520examples.%2520Despite%2520their%2520appeal%252C%250Athese%2520methods%2520have%2520yet%2520to%2520be%2520widely%2520adopted%2520since%2520no%2520one%2520algorithm%2520has%2520been%250Ashown%2520to%2520a%2529%2520generalize%2520across%2520models%2520and%2520tasks%2520b%2529%2520scale%2520to%2520large%2520datasets%2520and%250Ac%2529%2520yield%2520overall%2520FLOP%2520savings%2520when%2520accounting%2520for%2520the%2520overhead%2520of%2520data%250Aselection.%2520In%2520this%2520work%2520we%2520propose%2520a%2520method%2520which%2520satisfies%2520these%2520three%250Aproperties%252C%2520leveraging%2520small%252C%2520cheap%2520proxy%2520models%2520to%2520estimate%2520%2522learnability%2522%250Ascores%2520for%2520datapoints%252C%2520which%2520are%2520used%2520to%2520prioritize%2520data%2520for%2520the%2520training%2520of%250Amuch%2520larger%2520models.%2520As%2520a%2520result%252C%2520our%2520models%2520require%252046%2525%2520and%252051%2525%2520fewer%2520training%250Aupdates%2520and%2520up%2520to%252025%2525%2520less%2520total%2520computation%2520to%2520reach%2520the%2520same%2520performance%2520as%250Auniformly%2520trained%2520visual%2520classifiers%2520on%2520JFT%2520and%2520multimodal%2520models%2520on%2520ALIGN.%250AFinally%252C%2520we%2520find%2520our%2520data-prioritization%2520scheme%2520to%2520be%2520complementary%2520with%2520recent%250Adata-curation%2520and%2520learning%2520objectives%252C%2520yielding%2520a%2520new%2520state-of-the-art%2520in%250Aseveral%2520multimodal%2520transfer%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05328v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bad%20Students%20Make%20Great%20Teachers%3A%20Active%20Learning%20Accelerates%0A%20%20Large-Scale%20Visual%20Understanding&entry.906535625=Talfan%20Evans%20and%20Shreya%20Pathak%20and%20Hamza%20Merzic%20and%20Jonathan%20Schwarz%20and%20Ryutaro%20Tanno%20and%20Olivier%20J.%20Henaff&entry.1292438233=%20%20Power-law%20scaling%20indicates%20that%20large-scale%20training%20with%20uniform%20sampling%0Ais%20prohibitively%20slow.%20Active%20learning%20methods%20aim%20to%20increase%20data%20efficiency%0Aby%20prioritizing%20learning%20on%20the%20most%20relevant%20examples.%20Despite%20their%20appeal%2C%0Athese%20methods%20have%20yet%20to%20be%20widely%20adopted%20since%20no%20one%20algorithm%20has%20been%0Ashown%20to%20a%29%20generalize%20across%20models%20and%20tasks%20b%29%20scale%20to%20large%20datasets%20and%0Ac%29%20yield%20overall%20FLOP%20savings%20when%20accounting%20for%20the%20overhead%20of%20data%0Aselection.%20In%20this%20work%20we%20propose%20a%20method%20which%20satisfies%20these%20three%0Aproperties%2C%20leveraging%20small%2C%20cheap%20proxy%20models%20to%20estimate%20%22learnability%22%0Ascores%20for%20datapoints%2C%20which%20are%20used%20to%20prioritize%20data%20for%20the%20training%20of%0Amuch%20larger%20models.%20As%20a%20result%2C%20our%20models%20require%2046%25%20and%2051%25%20fewer%20training%0Aupdates%20and%20up%20to%2025%25%20less%20total%20computation%20to%20reach%20the%20same%20performance%20as%0Auniformly%20trained%20visual%20classifiers%20on%20JFT%20and%20multimodal%20models%20on%20ALIGN.%0AFinally%2C%20we%20find%20our%20data-prioritization%20scheme%20to%20be%20complementary%20with%20recent%0Adata-curation%20and%20learning%20objectives%2C%20yielding%20a%20new%20state-of-the-art%20in%0Aseveral%20multimodal%20transfer%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05328v4&entry.124074799=Read"},
{"title": "In-Context Learning Enables Robot Action Prediction in LLMs", "author": "Yida Yin and Zekai Wang and Yuvan Sharma and Dantong Niu and Trevor Darrell and Roei Herzig", "abstract": "  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.\n", "link": "http://arxiv.org/abs/2410.12782v1", "date": "2024-10-16", "relevancy": 1.6681, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.62}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5721}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20Enables%20Robot%20Action%20Prediction%20in%20LLMs&body=Title%3A%20In-Context%20Learning%20Enables%20Robot%20Action%20Prediction%20in%20LLMs%0AAuthor%3A%20Yida%20Yin%20and%20Zekai%20Wang%20and%20Yuvan%20Sharma%20and%20Dantong%20Niu%20and%20Trevor%20Darrell%20and%20Roei%20Herzig%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20using%0Ain-context%20learning%20%28ICL%29%20in%20the%20language%20domain.%20However%2C%20leveraging%20the%20ICL%0Acapabilities%20within%20LLMs%20to%20directly%20predict%20robot%20actions%20remains%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20introduce%20RoboPrompt%2C%20a%20framework%20that%20enables%0Aoff-the-shelf%20text-only%20LLMs%20to%20directly%20predict%20robot%20actions%20through%20ICL%0Awithout%20training.%20Our%20approach%20first%20heuristically%20identifies%20keyframes%20that%0Acapture%20important%20moments%20from%20an%20episode.%20Next%2C%20we%20extract%20end-effector%0Aactions%20from%20these%20keyframes%20as%20well%20as%20the%20estimated%20initial%20object%20poses%2C%20and%0Aboth%20are%20converted%20into%20textual%20descriptions.%20Finally%2C%20we%20construct%20a%0Astructured%20template%20to%20form%20ICL%20demonstrations%20from%20these%20textual%20descriptions%0Aand%20a%20task%20instruction.%20This%20enables%20an%20LLM%20to%20directly%20predict%20robot%20actions%0Aat%20test%20time.%20Through%20extensive%20experiments%20and%20analysis%2C%20RoboPrompt%20shows%0Astronger%20performance%20over%20zero-shot%20and%20ICL%20baselines%20in%20simulated%20and%0Areal-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Learning%2520Enables%2520Robot%2520Action%2520Prediction%2520in%2520LLMs%26entry.906535625%3DYida%2520Yin%2520and%2520Zekai%2520Wang%2520and%2520Yuvan%2520Sharma%2520and%2520Dantong%2520Niu%2520and%2520Trevor%2520Darrell%2520and%2520Roei%2520Herzig%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520success%2520using%250Ain-context%2520learning%2520%2528ICL%2529%2520in%2520the%2520language%2520domain.%2520However%252C%2520leveraging%2520the%2520ICL%250Acapabilities%2520within%2520LLMs%2520to%2520directly%2520predict%2520robot%2520actions%2520remains%2520largely%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RoboPrompt%252C%2520a%2520framework%2520that%2520enables%250Aoff-the-shelf%2520text-only%2520LLMs%2520to%2520directly%2520predict%2520robot%2520actions%2520through%2520ICL%250Awithout%2520training.%2520Our%2520approach%2520first%2520heuristically%2520identifies%2520keyframes%2520that%250Acapture%2520important%2520moments%2520from%2520an%2520episode.%2520Next%252C%2520we%2520extract%2520end-effector%250Aactions%2520from%2520these%2520keyframes%2520as%2520well%2520as%2520the%2520estimated%2520initial%2520object%2520poses%252C%2520and%250Aboth%2520are%2520converted%2520into%2520textual%2520descriptions.%2520Finally%252C%2520we%2520construct%2520a%250Astructured%2520template%2520to%2520form%2520ICL%2520demonstrations%2520from%2520these%2520textual%2520descriptions%250Aand%2520a%2520task%2520instruction.%2520This%2520enables%2520an%2520LLM%2520to%2520directly%2520predict%2520robot%2520actions%250Aat%2520test%2520time.%2520Through%2520extensive%2520experiments%2520and%2520analysis%252C%2520RoboPrompt%2520shows%250Astronger%2520performance%2520over%2520zero-shot%2520and%2520ICL%2520baselines%2520in%2520simulated%2520and%250Areal-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20Enables%20Robot%20Action%20Prediction%20in%20LLMs&entry.906535625=Yida%20Yin%20and%20Zekai%20Wang%20and%20Yuvan%20Sharma%20and%20Dantong%20Niu%20and%20Trevor%20Darrell%20and%20Roei%20Herzig&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20using%0Ain-context%20learning%20%28ICL%29%20in%20the%20language%20domain.%20However%2C%20leveraging%20the%20ICL%0Acapabilities%20within%20LLMs%20to%20directly%20predict%20robot%20actions%20remains%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20introduce%20RoboPrompt%2C%20a%20framework%20that%20enables%0Aoff-the-shelf%20text-only%20LLMs%20to%20directly%20predict%20robot%20actions%20through%20ICL%0Awithout%20training.%20Our%20approach%20first%20heuristically%20identifies%20keyframes%20that%0Acapture%20important%20moments%20from%20an%20episode.%20Next%2C%20we%20extract%20end-effector%0Aactions%20from%20these%20keyframes%20as%20well%20as%20the%20estimated%20initial%20object%20poses%2C%20and%0Aboth%20are%20converted%20into%20textual%20descriptions.%20Finally%2C%20we%20construct%20a%0Astructured%20template%20to%20form%20ICL%20demonstrations%20from%20these%20textual%20descriptions%0Aand%20a%20task%20instruction.%20This%20enables%20an%20LLM%20to%20directly%20predict%20robot%20actions%0Aat%20test%20time.%20Through%20extensive%20experiments%20and%20analysis%2C%20RoboPrompt%20shows%0Astronger%20performance%20over%20zero-shot%20and%20ICL%20baselines%20in%20simulated%20and%0Areal-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12782v1&entry.124074799=Read"},
{"title": "Training Neural Samplers with Reverse Diffusive KL Divergence", "author": "Jiajun He and Wenlin Chen and Mingtian Zhang and David Barber and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Training generative models to sample from unnormalized density functions is\nan important and challenging task in machine learning. Traditional training\nmethods often rely on the reverse Kullback-Leibler (KL) divergence due to its\ntractability. However, the mode-seeking behavior of reverse KL hinders\neffective approximation of multi-modal target distributions. To address this,\nwe propose to minimize the reverse KL along diffusion trajectories of both\nmodel and target densities. We refer to this objective as the reverse diffusive\nKL divergence, which allows the model to capture multiple modes. Leveraging\nthis objective, we train neural samplers that can efficiently generate samples\nfrom the target distribution in one step. We demonstrate that our method\nenhances sampling performance across various Boltzmann distributions, including\nboth synthetic multi-modal densities and n-body particle systems.\n", "link": "http://arxiv.org/abs/2410.12456v1", "date": "2024-10-16", "relevancy": 1.5772, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.544}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5263}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Neural%20Samplers%20with%20Reverse%20Diffusive%20KL%20Divergence&body=Title%3A%20Training%20Neural%20Samplers%20with%20Reverse%20Diffusive%20KL%20Divergence%0AAuthor%3A%20Jiajun%20He%20and%20Wenlin%20Chen%20and%20Mingtian%20Zhang%20and%20David%20Barber%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Training%20generative%20models%20to%20sample%20from%20unnormalized%20density%20functions%20is%0Aan%20important%20and%20challenging%20task%20in%20machine%20learning.%20Traditional%20training%0Amethods%20often%20rely%20on%20the%20reverse%20Kullback-Leibler%20%28KL%29%20divergence%20due%20to%20its%0Atractability.%20However%2C%20the%20mode-seeking%20behavior%20of%20reverse%20KL%20hinders%0Aeffective%20approximation%20of%20multi-modal%20target%20distributions.%20To%20address%20this%2C%0Awe%20propose%20to%20minimize%20the%20reverse%20KL%20along%20diffusion%20trajectories%20of%20both%0Amodel%20and%20target%20densities.%20We%20refer%20to%20this%20objective%20as%20the%20reverse%20diffusive%0AKL%20divergence%2C%20which%20allows%20the%20model%20to%20capture%20multiple%20modes.%20Leveraging%0Athis%20objective%2C%20we%20train%20neural%20samplers%20that%20can%20efficiently%20generate%20samples%0Afrom%20the%20target%20distribution%20in%20one%20step.%20We%20demonstrate%20that%20our%20method%0Aenhances%20sampling%20performance%20across%20various%20Boltzmann%20distributions%2C%20including%0Aboth%20synthetic%20multi-modal%20densities%20and%20n-body%20particle%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Neural%2520Samplers%2520with%2520Reverse%2520Diffusive%2520KL%2520Divergence%26entry.906535625%3DJiajun%2520He%2520and%2520Wenlin%2520Chen%2520and%2520Mingtian%2520Zhang%2520and%2520David%2520Barber%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Training%2520generative%2520models%2520to%2520sample%2520from%2520unnormalized%2520density%2520functions%2520is%250Aan%2520important%2520and%2520challenging%2520task%2520in%2520machine%2520learning.%2520Traditional%2520training%250Amethods%2520often%2520rely%2520on%2520the%2520reverse%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%2520due%2520to%2520its%250Atractability.%2520However%252C%2520the%2520mode-seeking%2520behavior%2520of%2520reverse%2520KL%2520hinders%250Aeffective%2520approximation%2520of%2520multi-modal%2520target%2520distributions.%2520To%2520address%2520this%252C%250Awe%2520propose%2520to%2520minimize%2520the%2520reverse%2520KL%2520along%2520diffusion%2520trajectories%2520of%2520both%250Amodel%2520and%2520target%2520densities.%2520We%2520refer%2520to%2520this%2520objective%2520as%2520the%2520reverse%2520diffusive%250AKL%2520divergence%252C%2520which%2520allows%2520the%2520model%2520to%2520capture%2520multiple%2520modes.%2520Leveraging%250Athis%2520objective%252C%2520we%2520train%2520neural%2520samplers%2520that%2520can%2520efficiently%2520generate%2520samples%250Afrom%2520the%2520target%2520distribution%2520in%2520one%2520step.%2520We%2520demonstrate%2520that%2520our%2520method%250Aenhances%2520sampling%2520performance%2520across%2520various%2520Boltzmann%2520distributions%252C%2520including%250Aboth%2520synthetic%2520multi-modal%2520densities%2520and%2520n-body%2520particle%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Neural%20Samplers%20with%20Reverse%20Diffusive%20KL%20Divergence&entry.906535625=Jiajun%20He%20and%20Wenlin%20Chen%20and%20Mingtian%20Zhang%20and%20David%20Barber%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Training%20generative%20models%20to%20sample%20from%20unnormalized%20density%20functions%20is%0Aan%20important%20and%20challenging%20task%20in%20machine%20learning.%20Traditional%20training%0Amethods%20often%20rely%20on%20the%20reverse%20Kullback-Leibler%20%28KL%29%20divergence%20due%20to%20its%0Atractability.%20However%2C%20the%20mode-seeking%20behavior%20of%20reverse%20KL%20hinders%0Aeffective%20approximation%20of%20multi-modal%20target%20distributions.%20To%20address%20this%2C%0Awe%20propose%20to%20minimize%20the%20reverse%20KL%20along%20diffusion%20trajectories%20of%20both%0Amodel%20and%20target%20densities.%20We%20refer%20to%20this%20objective%20as%20the%20reverse%20diffusive%0AKL%20divergence%2C%20which%20allows%20the%20model%20to%20capture%20multiple%20modes.%20Leveraging%0Athis%20objective%2C%20we%20train%20neural%20samplers%20that%20can%20efficiently%20generate%20samples%0Afrom%20the%20target%20distribution%20in%20one%20step.%20We%20demonstrate%20that%20our%20method%0Aenhances%20sampling%20performance%20across%20various%20Boltzmann%20distributions%2C%20including%0Aboth%20synthetic%20multi-modal%20densities%20and%20n-body%20particle%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12456v1&entry.124074799=Read"},
{"title": "Parsimony or Capability? Decomposition Delivers Both in Long-term Time\n  Series Forecasting", "author": "Jinliang Deng and Feiyang Ye and Du Yin and Xuan Song and Ivor W. Tsang and Hui Xiong", "abstract": "  Long-term time series forecasting (LTSF) represents a critical frontier in\ntime series analysis, characterized by extensive input sequences, as opposed to\nthe shorter spans typical of traditional approaches. While longer sequences\ninherently offer richer information for enhanced predictive precision,\nprevailing studies often respond by escalating model complexity. These\nintricate models can inflate into millions of parameters, resulting in\nprohibitive parameter scales. Our study demonstrates, through both analytical\nand empirical evidence, that decomposition is key to containing excessive model\ninflation while achieving uniformly superior and robust results across various\ndatasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of\ntime series data, our proposed model outperforms existing benchmarks, using\nover 99 \\% fewer parameters than the majority of competing methods. Through\nthis work, we aim to unleash the power of a restricted set of parameters by\ncapitalizing on domain characteristics--a timely reminder that in the realm of\nLTSF, bigger is not invariably better.\n", "link": "http://arxiv.org/abs/2401.11929v4", "date": "2024-10-16", "relevancy": 1.7731, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parsimony%20or%20Capability%3F%20Decomposition%20Delivers%20Both%20in%20Long-term%20Time%0A%20%20Series%20Forecasting&body=Title%3A%20Parsimony%20or%20Capability%3F%20Decomposition%20Delivers%20Both%20in%20Long-term%20Time%0A%20%20Series%20Forecasting%0AAuthor%3A%20Jinliang%20Deng%20and%20Feiyang%20Ye%20and%20Du%20Yin%20and%20Xuan%20Song%20and%20Ivor%20W.%20Tsang%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Long-term%20time%20series%20forecasting%20%28LTSF%29%20represents%20a%20critical%20frontier%20in%0Atime%20series%20analysis%2C%20characterized%20by%20extensive%20input%20sequences%2C%20as%20opposed%20to%0Athe%20shorter%20spans%20typical%20of%20traditional%20approaches.%20While%20longer%20sequences%0Ainherently%20offer%20richer%20information%20for%20enhanced%20predictive%20precision%2C%0Aprevailing%20studies%20often%20respond%20by%20escalating%20model%20complexity.%20These%0Aintricate%20models%20can%20inflate%20into%20millions%20of%20parameters%2C%20resulting%20in%0Aprohibitive%20parameter%20scales.%20Our%20study%20demonstrates%2C%20through%20both%20analytical%0Aand%20empirical%20evidence%2C%20that%20decomposition%20is%20key%20to%20containing%20excessive%20model%0Ainflation%20while%20achieving%20uniformly%20superior%20and%20robust%20results%20across%20various%0Adatasets.%20Remarkably%2C%20by%20tailoring%20decomposition%20to%20the%20intrinsic%20dynamics%20of%0Atime%20series%20data%2C%20our%20proposed%20model%20outperforms%20existing%20benchmarks%2C%20using%0Aover%2099%20%5C%25%20fewer%20parameters%20than%20the%20majority%20of%20competing%20methods.%20Through%0Athis%20work%2C%20we%20aim%20to%20unleash%20the%20power%20of%20a%20restricted%20set%20of%20parameters%20by%0Acapitalizing%20on%20domain%20characteristics--a%20timely%20reminder%20that%20in%20the%20realm%20of%0ALTSF%2C%20bigger%20is%20not%20invariably%20better.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11929v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParsimony%2520or%2520Capability%253F%2520Decomposition%2520Delivers%2520Both%2520in%2520Long-term%2520Time%250A%2520%2520Series%2520Forecasting%26entry.906535625%3DJinliang%2520Deng%2520and%2520Feiyang%2520Ye%2520and%2520Du%2520Yin%2520and%2520Xuan%2520Song%2520and%2520Ivor%2520W.%2520Tsang%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Long-term%2520time%2520series%2520forecasting%2520%2528LTSF%2529%2520represents%2520a%2520critical%2520frontier%2520in%250Atime%2520series%2520analysis%252C%2520characterized%2520by%2520extensive%2520input%2520sequences%252C%2520as%2520opposed%2520to%250Athe%2520shorter%2520spans%2520typical%2520of%2520traditional%2520approaches.%2520While%2520longer%2520sequences%250Ainherently%2520offer%2520richer%2520information%2520for%2520enhanced%2520predictive%2520precision%252C%250Aprevailing%2520studies%2520often%2520respond%2520by%2520escalating%2520model%2520complexity.%2520These%250Aintricate%2520models%2520can%2520inflate%2520into%2520millions%2520of%2520parameters%252C%2520resulting%2520in%250Aprohibitive%2520parameter%2520scales.%2520Our%2520study%2520demonstrates%252C%2520through%2520both%2520analytical%250Aand%2520empirical%2520evidence%252C%2520that%2520decomposition%2520is%2520key%2520to%2520containing%2520excessive%2520model%250Ainflation%2520while%2520achieving%2520uniformly%2520superior%2520and%2520robust%2520results%2520across%2520various%250Adatasets.%2520Remarkably%252C%2520by%2520tailoring%2520decomposition%2520to%2520the%2520intrinsic%2520dynamics%2520of%250Atime%2520series%2520data%252C%2520our%2520proposed%2520model%2520outperforms%2520existing%2520benchmarks%252C%2520using%250Aover%252099%2520%255C%2525%2520fewer%2520parameters%2520than%2520the%2520majority%2520of%2520competing%2520methods.%2520Through%250Athis%2520work%252C%2520we%2520aim%2520to%2520unleash%2520the%2520power%2520of%2520a%2520restricted%2520set%2520of%2520parameters%2520by%250Acapitalizing%2520on%2520domain%2520characteristics--a%2520timely%2520reminder%2520that%2520in%2520the%2520realm%2520of%250ALTSF%252C%2520bigger%2520is%2520not%2520invariably%2520better.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11929v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parsimony%20or%20Capability%3F%20Decomposition%20Delivers%20Both%20in%20Long-term%20Time%0A%20%20Series%20Forecasting&entry.906535625=Jinliang%20Deng%20and%20Feiyang%20Ye%20and%20Du%20Yin%20and%20Xuan%20Song%20and%20Ivor%20W.%20Tsang%20and%20Hui%20Xiong&entry.1292438233=%20%20Long-term%20time%20series%20forecasting%20%28LTSF%29%20represents%20a%20critical%20frontier%20in%0Atime%20series%20analysis%2C%20characterized%20by%20extensive%20input%20sequences%2C%20as%20opposed%20to%0Athe%20shorter%20spans%20typical%20of%20traditional%20approaches.%20While%20longer%20sequences%0Ainherently%20offer%20richer%20information%20for%20enhanced%20predictive%20precision%2C%0Aprevailing%20studies%20often%20respond%20by%20escalating%20model%20complexity.%20These%0Aintricate%20models%20can%20inflate%20into%20millions%20of%20parameters%2C%20resulting%20in%0Aprohibitive%20parameter%20scales.%20Our%20study%20demonstrates%2C%20through%20both%20analytical%0Aand%20empirical%20evidence%2C%20that%20decomposition%20is%20key%20to%20containing%20excessive%20model%0Ainflation%20while%20achieving%20uniformly%20superior%20and%20robust%20results%20across%20various%0Adatasets.%20Remarkably%2C%20by%20tailoring%20decomposition%20to%20the%20intrinsic%20dynamics%20of%0Atime%20series%20data%2C%20our%20proposed%20model%20outperforms%20existing%20benchmarks%2C%20using%0Aover%2099%20%5C%25%20fewer%20parameters%20than%20the%20majority%20of%20competing%20methods.%20Through%0Athis%20work%2C%20we%20aim%20to%20unleash%20the%20power%20of%20a%20restricted%20set%20of%20parameters%20by%0Acapitalizing%20on%20domain%20characteristics--a%20timely%20reminder%20that%20in%20the%20realm%20of%0ALTSF%2C%20bigger%20is%20not%20invariably%20better.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11929v4&entry.124074799=Read"},
{"title": "Initialization Method for Factorization Machine Based on Low-Rank\n  Approximation for Constructing a Corrected Approximate Ising Model", "author": "Yuya Seki and Hyakka Nakada and Shu Tanaka", "abstract": "  This paper presents an initialization method that can approximate a given\napproximate Ising model with a high degree of accuracy using the Factorization\nMachine (FM), a machine learning model. The construction of Ising models using\nFM is applied to the combinatorial optimization problem using the factorization\nmachine with quantum annealing. It is anticipated that the optimization\nperformance of FMQA will be enhanced through the implementation of the\nwarm-start method. Nevertheless, the optimal initialization method for\nleveraging the warm-start approach in FMQA remains undetermined. Consequently,\nthe present study compares a number of initialization methods and identifies\nthe most appropriate for use with a warm-start in FMQA through numerical\nexperimentation. Furthermore, the properties of the proposed FM initialization\nmethod are analyzed using random matrix theory, demonstrating that the\napproximation accuracy of the proposed method is not significantly influenced\nby the specific Ising model under consideration. The findings of this study\nwill facilitate the advancement of combinatorial optimization problem-solving\nthrough the use of Ising machines.\n", "link": "http://arxiv.org/abs/2410.12747v1", "date": "2024-10-16", "relevancy": 1.0986, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3728}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3677}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Initialization%20Method%20for%20Factorization%20Machine%20Based%20on%20Low-Rank%0A%20%20Approximation%20for%20Constructing%20a%20Corrected%20Approximate%20Ising%20Model&body=Title%3A%20Initialization%20Method%20for%20Factorization%20Machine%20Based%20on%20Low-Rank%0A%20%20Approximation%20for%20Constructing%20a%20Corrected%20Approximate%20Ising%20Model%0AAuthor%3A%20Yuya%20Seki%20and%20Hyakka%20Nakada%20and%20Shu%20Tanaka%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20initialization%20method%20that%20can%20approximate%20a%20given%0Aapproximate%20Ising%20model%20with%20a%20high%20degree%20of%20accuracy%20using%20the%20Factorization%0AMachine%20%28FM%29%2C%20a%20machine%20learning%20model.%20The%20construction%20of%20Ising%20models%20using%0AFM%20is%20applied%20to%20the%20combinatorial%20optimization%20problem%20using%20the%20factorization%0Amachine%20with%20quantum%20annealing.%20It%20is%20anticipated%20that%20the%20optimization%0Aperformance%20of%20FMQA%20will%20be%20enhanced%20through%20the%20implementation%20of%20the%0Awarm-start%20method.%20Nevertheless%2C%20the%20optimal%20initialization%20method%20for%0Aleveraging%20the%20warm-start%20approach%20in%20FMQA%20remains%20undetermined.%20Consequently%2C%0Athe%20present%20study%20compares%20a%20number%20of%20initialization%20methods%20and%20identifies%0Athe%20most%20appropriate%20for%20use%20with%20a%20warm-start%20in%20FMQA%20through%20numerical%0Aexperimentation.%20Furthermore%2C%20the%20properties%20of%20the%20proposed%20FM%20initialization%0Amethod%20are%20analyzed%20using%20random%20matrix%20theory%2C%20demonstrating%20that%20the%0Aapproximation%20accuracy%20of%20the%20proposed%20method%20is%20not%20significantly%20influenced%0Aby%20the%20specific%20Ising%20model%20under%20consideration.%20The%20findings%20of%20this%20study%0Awill%20facilitate%20the%20advancement%20of%20combinatorial%20optimization%20problem-solving%0Athrough%20the%20use%20of%20Ising%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInitialization%2520Method%2520for%2520Factorization%2520Machine%2520Based%2520on%2520Low-Rank%250A%2520%2520Approximation%2520for%2520Constructing%2520a%2520Corrected%2520Approximate%2520Ising%2520Model%26entry.906535625%3DYuya%2520Seki%2520and%2520Hyakka%2520Nakada%2520and%2520Shu%2520Tanaka%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520initialization%2520method%2520that%2520can%2520approximate%2520a%2520given%250Aapproximate%2520Ising%2520model%2520with%2520a%2520high%2520degree%2520of%2520accuracy%2520using%2520the%2520Factorization%250AMachine%2520%2528FM%2529%252C%2520a%2520machine%2520learning%2520model.%2520The%2520construction%2520of%2520Ising%2520models%2520using%250AFM%2520is%2520applied%2520to%2520the%2520combinatorial%2520optimization%2520problem%2520using%2520the%2520factorization%250Amachine%2520with%2520quantum%2520annealing.%2520It%2520is%2520anticipated%2520that%2520the%2520optimization%250Aperformance%2520of%2520FMQA%2520will%2520be%2520enhanced%2520through%2520the%2520implementation%2520of%2520the%250Awarm-start%2520method.%2520Nevertheless%252C%2520the%2520optimal%2520initialization%2520method%2520for%250Aleveraging%2520the%2520warm-start%2520approach%2520in%2520FMQA%2520remains%2520undetermined.%2520Consequently%252C%250Athe%2520present%2520study%2520compares%2520a%2520number%2520of%2520initialization%2520methods%2520and%2520identifies%250Athe%2520most%2520appropriate%2520for%2520use%2520with%2520a%2520warm-start%2520in%2520FMQA%2520through%2520numerical%250Aexperimentation.%2520Furthermore%252C%2520the%2520properties%2520of%2520the%2520proposed%2520FM%2520initialization%250Amethod%2520are%2520analyzed%2520using%2520random%2520matrix%2520theory%252C%2520demonstrating%2520that%2520the%250Aapproximation%2520accuracy%2520of%2520the%2520proposed%2520method%2520is%2520not%2520significantly%2520influenced%250Aby%2520the%2520specific%2520Ising%2520model%2520under%2520consideration.%2520The%2520findings%2520of%2520this%2520study%250Awill%2520facilitate%2520the%2520advancement%2520of%2520combinatorial%2520optimization%2520problem-solving%250Athrough%2520the%2520use%2520of%2520Ising%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Initialization%20Method%20for%20Factorization%20Machine%20Based%20on%20Low-Rank%0A%20%20Approximation%20for%20Constructing%20a%20Corrected%20Approximate%20Ising%20Model&entry.906535625=Yuya%20Seki%20and%20Hyakka%20Nakada%20and%20Shu%20Tanaka&entry.1292438233=%20%20This%20paper%20presents%20an%20initialization%20method%20that%20can%20approximate%20a%20given%0Aapproximate%20Ising%20model%20with%20a%20high%20degree%20of%20accuracy%20using%20the%20Factorization%0AMachine%20%28FM%29%2C%20a%20machine%20learning%20model.%20The%20construction%20of%20Ising%20models%20using%0AFM%20is%20applied%20to%20the%20combinatorial%20optimization%20problem%20using%20the%20factorization%0Amachine%20with%20quantum%20annealing.%20It%20is%20anticipated%20that%20the%20optimization%0Aperformance%20of%20FMQA%20will%20be%20enhanced%20through%20the%20implementation%20of%20the%0Awarm-start%20method.%20Nevertheless%2C%20the%20optimal%20initialization%20method%20for%0Aleveraging%20the%20warm-start%20approach%20in%20FMQA%20remains%20undetermined.%20Consequently%2C%0Athe%20present%20study%20compares%20a%20number%20of%20initialization%20methods%20and%20identifies%0Athe%20most%20appropriate%20for%20use%20with%20a%20warm-start%20in%20FMQA%20through%20numerical%0Aexperimentation.%20Furthermore%2C%20the%20properties%20of%20the%20proposed%20FM%20initialization%0Amethod%20are%20analyzed%20using%20random%20matrix%20theory%2C%20demonstrating%20that%20the%0Aapproximation%20accuracy%20of%20the%20proposed%20method%20is%20not%20significantly%20influenced%0Aby%20the%20specific%20Ising%20model%20under%20consideration.%20The%20findings%20of%20this%20study%0Awill%20facilitate%20the%20advancement%20of%20combinatorial%20optimization%20problem-solving%0Athrough%20the%20use%20of%20Ising%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12747v1&entry.124074799=Read"},
{"title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation", "author": "Haau-Sing Li and Patrick Fernandes and Iryna Gurevych and Andr\u00e9 F. T. Martins", "abstract": "  Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.\n", "link": "http://arxiv.org/abs/2408.13745v4", "date": "2024-10-16", "relevancy": 1.9549, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOCE%3A%20Finding%20the%20Sweet%20Spot%20for%20Execution-Based%20Code%20Generation&body=Title%3A%20DOCE%3A%20Finding%20the%20Sweet%20Spot%20for%20Execution-Based%20Code%20Generation%0AAuthor%3A%20Haau-Sing%20Li%20and%20Patrick%20Fernandes%20and%20Iryna%20Gurevych%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Recently%2C%20a%20diverse%20set%20of%20decoding%20and%20reranking%20procedures%20have%20been%20shown%0Aeffective%20for%20LLM-based%20code%20generation.%20However%2C%20a%20comprehensive%20framework%0Athat%20links%20and%20experimentally%20compares%20these%20methods%20is%20missing.%20We%20address%0Athis%20by%20proposing%20Decoding%20Objectives%20for%20Code%20Execution%2C%20a%20comprehensive%0Aframework%20that%20includes%20candidate%20generation%2C%20%24n%24-best%20reranking%2C%20minimum%20Bayes%0Arisk%20%28MBR%29%20decoding%2C%20and%20self-debugging%20as%20the%20core%20components.%20We%20then%20study%0Athe%20contributions%20of%20these%20components%20through%20execution-based%20evaluation%0Ametrics.%20Our%20findings%20highlight%20the%20importance%20of%20execution-based%20methods%20and%0Athe%20difference%20gap%20between%20execution-based%20and%20execution-free%20methods.%0AFurthermore%2C%20we%20assess%20the%20impact%20of%20filtering%20based%20on%20trial%20unit%20tests%2C%20a%0Asimple%20and%20effective%20strategy%20that%20has%20been%20often%20overlooked%20in%20prior%20works.%20We%0Aalso%20propose%20self-debugging%20on%20multiple%20candidates%2C%20obtaining%20state-of-the-art%0Aperformance%20on%20reranking%20for%20code%20generation.%20We%20expect%20our%20framework%20to%0Aprovide%20a%20solid%20guideline%20for%20future%20research%20on%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13745v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOCE%253A%2520Finding%2520the%2520Sweet%2520Spot%2520for%2520Execution-Based%2520Code%2520Generation%26entry.906535625%3DHaau-Sing%2520Li%2520and%2520Patrick%2520Fernandes%2520and%2520Iryna%2520Gurevych%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Recently%252C%2520a%2520diverse%2520set%2520of%2520decoding%2520and%2520reranking%2520procedures%2520have%2520been%2520shown%250Aeffective%2520for%2520LLM-based%2520code%2520generation.%2520However%252C%2520a%2520comprehensive%2520framework%250Athat%2520links%2520and%2520experimentally%2520compares%2520these%2520methods%2520is%2520missing.%2520We%2520address%250Athis%2520by%2520proposing%2520Decoding%2520Objectives%2520for%2520Code%2520Execution%252C%2520a%2520comprehensive%250Aframework%2520that%2520includes%2520candidate%2520generation%252C%2520%2524n%2524-best%2520reranking%252C%2520minimum%2520Bayes%250Arisk%2520%2528MBR%2529%2520decoding%252C%2520and%2520self-debugging%2520as%2520the%2520core%2520components.%2520We%2520then%2520study%250Athe%2520contributions%2520of%2520these%2520components%2520through%2520execution-based%2520evaluation%250Ametrics.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520execution-based%2520methods%2520and%250Athe%2520difference%2520gap%2520between%2520execution-based%2520and%2520execution-free%2520methods.%250AFurthermore%252C%2520we%2520assess%2520the%2520impact%2520of%2520filtering%2520based%2520on%2520trial%2520unit%2520tests%252C%2520a%250Asimple%2520and%2520effective%2520strategy%2520that%2520has%2520been%2520often%2520overlooked%2520in%2520prior%2520works.%2520We%250Aalso%2520propose%2520self-debugging%2520on%2520multiple%2520candidates%252C%2520obtaining%2520state-of-the-art%250Aperformance%2520on%2520reranking%2520for%2520code%2520generation.%2520We%2520expect%2520our%2520framework%2520to%250Aprovide%2520a%2520solid%2520guideline%2520for%2520future%2520research%2520on%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13745v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOCE%3A%20Finding%20the%20Sweet%20Spot%20for%20Execution-Based%20Code%20Generation&entry.906535625=Haau-Sing%20Li%20and%20Patrick%20Fernandes%20and%20Iryna%20Gurevych%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Recently%2C%20a%20diverse%20set%20of%20decoding%20and%20reranking%20procedures%20have%20been%20shown%0Aeffective%20for%20LLM-based%20code%20generation.%20However%2C%20a%20comprehensive%20framework%0Athat%20links%20and%20experimentally%20compares%20these%20methods%20is%20missing.%20We%20address%0Athis%20by%20proposing%20Decoding%20Objectives%20for%20Code%20Execution%2C%20a%20comprehensive%0Aframework%20that%20includes%20candidate%20generation%2C%20%24n%24-best%20reranking%2C%20minimum%20Bayes%0Arisk%20%28MBR%29%20decoding%2C%20and%20self-debugging%20as%20the%20core%20components.%20We%20then%20study%0Athe%20contributions%20of%20these%20components%20through%20execution-based%20evaluation%0Ametrics.%20Our%20findings%20highlight%20the%20importance%20of%20execution-based%20methods%20and%0Athe%20difference%20gap%20between%20execution-based%20and%20execution-free%20methods.%0AFurthermore%2C%20we%20assess%20the%20impact%20of%20filtering%20based%20on%20trial%20unit%20tests%2C%20a%0Asimple%20and%20effective%20strategy%20that%20has%20been%20often%20overlooked%20in%20prior%20works.%20We%0Aalso%20propose%20self-debugging%20on%20multiple%20candidates%2C%20obtaining%20state-of-the-art%0Aperformance%20on%20reranking%20for%20code%20generation.%20We%20expect%20our%20framework%20to%0Aprovide%20a%20solid%20guideline%20for%20future%20research%20on%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13745v4&entry.124074799=Read"},
{"title": "Energy-Efficient Computation with DVFS using Deep Reinforcement Learning\n  for Multi-Task Systems in Edge Computing", "author": "Xinyi Li and Ti Zhou and Haoyu Wang and Man Lin", "abstract": "  Periodic soft real-time systems have broad applications in many areas, such\nas IoT. Finding an optimal energy-efficient policy that is adaptable to\nunderlying edge devices while meeting deadlines for tasks has always been\nchallenging. This research studies generalized systems with multi-task,\nmulti-deadline scenarios with reinforcement learning-based DVFS for energy\nsaving. This work addresses the limitation of previous work that models a\nperiodic system as a single task and single-deadline scenario, which is too\nsimplified to cope with complex situations. The method encodes time series\ninformation in the Linux kernel into information that is easy to use for\nreinforcement learning, allowing the system to generate DVFS policies to adapt\nsystem patterns based on the general workload. For encoding, we present two\ndifferent methods for comparison. Both methods use only one performance\ncounter: system utilization and the kernel only needs minimal information from\nthe userspace. Our method is implemented on Jetson Nano Board (2GB) and is\ntested with three fixed multitask workloads, which are three, five, and eight\ntasks in the workload, respectively. For randomness and generalization, we also\ndesigned a random workload generator to build different multitask workloads to\ntest. Based on the test results, our method could save 3%-10% power compared to\nLinux built-in governors.\n", "link": "http://arxiv.org/abs/2409.19434v2", "date": "2024-10-16", "relevancy": 1.8991, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4948}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4752}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Computation%20with%20DVFS%20using%20Deep%20Reinforcement%20Learning%0A%20%20for%20Multi-Task%20Systems%20in%20Edge%20Computing&body=Title%3A%20Energy-Efficient%20Computation%20with%20DVFS%20using%20Deep%20Reinforcement%20Learning%0A%20%20for%20Multi-Task%20Systems%20in%20Edge%20Computing%0AAuthor%3A%20Xinyi%20Li%20and%20Ti%20Zhou%20and%20Haoyu%20Wang%20and%20Man%20Lin%0AAbstract%3A%20%20%20Periodic%20soft%20real-time%20systems%20have%20broad%20applications%20in%20many%20areas%2C%20such%0Aas%20IoT.%20Finding%20an%20optimal%20energy-efficient%20policy%20that%20is%20adaptable%20to%0Aunderlying%20edge%20devices%20while%20meeting%20deadlines%20for%20tasks%20has%20always%20been%0Achallenging.%20This%20research%20studies%20generalized%20systems%20with%20multi-task%2C%0Amulti-deadline%20scenarios%20with%20reinforcement%20learning-based%20DVFS%20for%20energy%0Asaving.%20This%20work%20addresses%20the%20limitation%20of%20previous%20work%20that%20models%20a%0Aperiodic%20system%20as%20a%20single%20task%20and%20single-deadline%20scenario%2C%20which%20is%20too%0Asimplified%20to%20cope%20with%20complex%20situations.%20The%20method%20encodes%20time%20series%0Ainformation%20in%20the%20Linux%20kernel%20into%20information%20that%20is%20easy%20to%20use%20for%0Areinforcement%20learning%2C%20allowing%20the%20system%20to%20generate%20DVFS%20policies%20to%20adapt%0Asystem%20patterns%20based%20on%20the%20general%20workload.%20For%20encoding%2C%20we%20present%20two%0Adifferent%20methods%20for%20comparison.%20Both%20methods%20use%20only%20one%20performance%0Acounter%3A%20system%20utilization%20and%20the%20kernel%20only%20needs%20minimal%20information%20from%0Athe%20userspace.%20Our%20method%20is%20implemented%20on%20Jetson%20Nano%20Board%20%282GB%29%20and%20is%0Atested%20with%20three%20fixed%20multitask%20workloads%2C%20which%20are%20three%2C%20five%2C%20and%20eight%0Atasks%20in%20the%20workload%2C%20respectively.%20For%20randomness%20and%20generalization%2C%20we%20also%0Adesigned%20a%20random%20workload%20generator%20to%20build%20different%20multitask%20workloads%20to%0Atest.%20Based%20on%20the%20test%20results%2C%20our%20method%20could%20save%203%25-10%25%20power%20compared%20to%0ALinux%20built-in%20governors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Efficient%2520Computation%2520with%2520DVFS%2520using%2520Deep%2520Reinforcement%2520Learning%250A%2520%2520for%2520Multi-Task%2520Systems%2520in%2520Edge%2520Computing%26entry.906535625%3DXinyi%2520Li%2520and%2520Ti%2520Zhou%2520and%2520Haoyu%2520Wang%2520and%2520Man%2520Lin%26entry.1292438233%3D%2520%2520Periodic%2520soft%2520real-time%2520systems%2520have%2520broad%2520applications%2520in%2520many%2520areas%252C%2520such%250Aas%2520IoT.%2520Finding%2520an%2520optimal%2520energy-efficient%2520policy%2520that%2520is%2520adaptable%2520to%250Aunderlying%2520edge%2520devices%2520while%2520meeting%2520deadlines%2520for%2520tasks%2520has%2520always%2520been%250Achallenging.%2520This%2520research%2520studies%2520generalized%2520systems%2520with%2520multi-task%252C%250Amulti-deadline%2520scenarios%2520with%2520reinforcement%2520learning-based%2520DVFS%2520for%2520energy%250Asaving.%2520This%2520work%2520addresses%2520the%2520limitation%2520of%2520previous%2520work%2520that%2520models%2520a%250Aperiodic%2520system%2520as%2520a%2520single%2520task%2520and%2520single-deadline%2520scenario%252C%2520which%2520is%2520too%250Asimplified%2520to%2520cope%2520with%2520complex%2520situations.%2520The%2520method%2520encodes%2520time%2520series%250Ainformation%2520in%2520the%2520Linux%2520kernel%2520into%2520information%2520that%2520is%2520easy%2520to%2520use%2520for%250Areinforcement%2520learning%252C%2520allowing%2520the%2520system%2520to%2520generate%2520DVFS%2520policies%2520to%2520adapt%250Asystem%2520patterns%2520based%2520on%2520the%2520general%2520workload.%2520For%2520encoding%252C%2520we%2520present%2520two%250Adifferent%2520methods%2520for%2520comparison.%2520Both%2520methods%2520use%2520only%2520one%2520performance%250Acounter%253A%2520system%2520utilization%2520and%2520the%2520kernel%2520only%2520needs%2520minimal%2520information%2520from%250Athe%2520userspace.%2520Our%2520method%2520is%2520implemented%2520on%2520Jetson%2520Nano%2520Board%2520%25282GB%2529%2520and%2520is%250Atested%2520with%2520three%2520fixed%2520multitask%2520workloads%252C%2520which%2520are%2520three%252C%2520five%252C%2520and%2520eight%250Atasks%2520in%2520the%2520workload%252C%2520respectively.%2520For%2520randomness%2520and%2520generalization%252C%2520we%2520also%250Adesigned%2520a%2520random%2520workload%2520generator%2520to%2520build%2520different%2520multitask%2520workloads%2520to%250Atest.%2520Based%2520on%2520the%2520test%2520results%252C%2520our%2520method%2520could%2520save%25203%2525-10%2525%2520power%2520compared%2520to%250ALinux%2520built-in%2520governors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Computation%20with%20DVFS%20using%20Deep%20Reinforcement%20Learning%0A%20%20for%20Multi-Task%20Systems%20in%20Edge%20Computing&entry.906535625=Xinyi%20Li%20and%20Ti%20Zhou%20and%20Haoyu%20Wang%20and%20Man%20Lin&entry.1292438233=%20%20Periodic%20soft%20real-time%20systems%20have%20broad%20applications%20in%20many%20areas%2C%20such%0Aas%20IoT.%20Finding%20an%20optimal%20energy-efficient%20policy%20that%20is%20adaptable%20to%0Aunderlying%20edge%20devices%20while%20meeting%20deadlines%20for%20tasks%20has%20always%20been%0Achallenging.%20This%20research%20studies%20generalized%20systems%20with%20multi-task%2C%0Amulti-deadline%20scenarios%20with%20reinforcement%20learning-based%20DVFS%20for%20energy%0Asaving.%20This%20work%20addresses%20the%20limitation%20of%20previous%20work%20that%20models%20a%0Aperiodic%20system%20as%20a%20single%20task%20and%20single-deadline%20scenario%2C%20which%20is%20too%0Asimplified%20to%20cope%20with%20complex%20situations.%20The%20method%20encodes%20time%20series%0Ainformation%20in%20the%20Linux%20kernel%20into%20information%20that%20is%20easy%20to%20use%20for%0Areinforcement%20learning%2C%20allowing%20the%20system%20to%20generate%20DVFS%20policies%20to%20adapt%0Asystem%20patterns%20based%20on%20the%20general%20workload.%20For%20encoding%2C%20we%20present%20two%0Adifferent%20methods%20for%20comparison.%20Both%20methods%20use%20only%20one%20performance%0Acounter%3A%20system%20utilization%20and%20the%20kernel%20only%20needs%20minimal%20information%20from%0Athe%20userspace.%20Our%20method%20is%20implemented%20on%20Jetson%20Nano%20Board%20%282GB%29%20and%20is%0Atested%20with%20three%20fixed%20multitask%20workloads%2C%20which%20are%20three%2C%20five%2C%20and%20eight%0Atasks%20in%20the%20workload%2C%20respectively.%20For%20randomness%20and%20generalization%2C%20we%20also%0Adesigned%20a%20random%20workload%20generator%20to%20build%20different%20multitask%20workloads%20to%0Atest.%20Based%20on%20the%20test%20results%2C%20our%20method%20could%20save%203%25-10%25%20power%20compared%20to%0ALinux%20built-in%20governors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19434v2&entry.124074799=Read"},
{"title": "Context-Scaling versus Task-Scaling in In-Context Learning", "author": "Amirhesam Abedsoltan and Adityanarayanan Radhakrishnan and Jingfeng Wu and Mikhail Belkin", "abstract": "  Transformers exhibit In-Context Learning (ICL), where these models solve new\ntasks by using examples in the prompt without additional training. In our work,\nwe identify and analyze two key components of ICL: (1) context-scaling, where\nmodel performance improves as the number of in-context examples increases and\n(2) task-scaling, where model performance improves as the number of\npre-training tasks increases. While transformers are capable of both\ncontext-scaling and task-scaling, we empirically show that standard Multi-Layer\nPerceptrons (MLPs) with vectorized input are only capable of task-scaling. To\nunderstand how transformers are capable of context-scaling, we first propose a\nsignificantly simplified transformer architecture without key, query, value\nweights. We show that it performs ICL comparably to the original GPT-2 model in\nvarious statistical learning tasks including linear regression, teacher-student\nsettings. Furthermore, a single block of our simplified transformer can be\nviewed as data dependent feature map followed by an MLP. This feature map on\nits own is a powerful predictor that is capable of context-scaling but is not\ncapable of task-scaling. We show empirically that concatenating the output of\nthis feature map with vectorized data as an input to MLPs enables both\ncontext-scaling and task-scaling. This finding provides a simple setting to\nstudy context and task-scaling for ICL.\n", "link": "http://arxiv.org/abs/2410.12783v1", "date": "2024-10-16", "relevancy": 1.9793, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Scaling%20versus%20Task-Scaling%20in%20In-Context%20Learning&body=Title%3A%20Context-Scaling%20versus%20Task-Scaling%20in%20In-Context%20Learning%0AAuthor%3A%20Amirhesam%20Abedsoltan%20and%20Adityanarayanan%20Radhakrishnan%20and%20Jingfeng%20Wu%20and%20Mikhail%20Belkin%0AAbstract%3A%20%20%20Transformers%20exhibit%20In-Context%20Learning%20%28ICL%29%2C%20where%20these%20models%20solve%20new%0Atasks%20by%20using%20examples%20in%20the%20prompt%20without%20additional%20training.%20In%20our%20work%2C%0Awe%20identify%20and%20analyze%20two%20key%20components%20of%20ICL%3A%20%281%29%20context-scaling%2C%20where%0Amodel%20performance%20improves%20as%20the%20number%20of%20in-context%20examples%20increases%20and%0A%282%29%20task-scaling%2C%20where%20model%20performance%20improves%20as%20the%20number%20of%0Apre-training%20tasks%20increases.%20While%20transformers%20are%20capable%20of%20both%0Acontext-scaling%20and%20task-scaling%2C%20we%20empirically%20show%20that%20standard%20Multi-Layer%0APerceptrons%20%28MLPs%29%20with%20vectorized%20input%20are%20only%20capable%20of%20task-scaling.%20To%0Aunderstand%20how%20transformers%20are%20capable%20of%20context-scaling%2C%20we%20first%20propose%20a%0Asignificantly%20simplified%20transformer%20architecture%20without%20key%2C%20query%2C%20value%0Aweights.%20We%20show%20that%20it%20performs%20ICL%20comparably%20to%20the%20original%20GPT-2%20model%20in%0Avarious%20statistical%20learning%20tasks%20including%20linear%20regression%2C%20teacher-student%0Asettings.%20Furthermore%2C%20a%20single%20block%20of%20our%20simplified%20transformer%20can%20be%0Aviewed%20as%20data%20dependent%20feature%20map%20followed%20by%20an%20MLP.%20This%20feature%20map%20on%0Aits%20own%20is%20a%20powerful%20predictor%20that%20is%20capable%20of%20context-scaling%20but%20is%20not%0Acapable%20of%20task-scaling.%20We%20show%20empirically%20that%20concatenating%20the%20output%20of%0Athis%20feature%20map%20with%20vectorized%20data%20as%20an%20input%20to%20MLPs%20enables%20both%0Acontext-scaling%20and%20task-scaling.%20This%20finding%20provides%20a%20simple%20setting%20to%0Astudy%20context%20and%20task-scaling%20for%20ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Scaling%2520versus%2520Task-Scaling%2520in%2520In-Context%2520Learning%26entry.906535625%3DAmirhesam%2520Abedsoltan%2520and%2520Adityanarayanan%2520Radhakrishnan%2520and%2520Jingfeng%2520Wu%2520and%2520Mikhail%2520Belkin%26entry.1292438233%3D%2520%2520Transformers%2520exhibit%2520In-Context%2520Learning%2520%2528ICL%2529%252C%2520where%2520these%2520models%2520solve%2520new%250Atasks%2520by%2520using%2520examples%2520in%2520the%2520prompt%2520without%2520additional%2520training.%2520In%2520our%2520work%252C%250Awe%2520identify%2520and%2520analyze%2520two%2520key%2520components%2520of%2520ICL%253A%2520%25281%2529%2520context-scaling%252C%2520where%250Amodel%2520performance%2520improves%2520as%2520the%2520number%2520of%2520in-context%2520examples%2520increases%2520and%250A%25282%2529%2520task-scaling%252C%2520where%2520model%2520performance%2520improves%2520as%2520the%2520number%2520of%250Apre-training%2520tasks%2520increases.%2520While%2520transformers%2520are%2520capable%2520of%2520both%250Acontext-scaling%2520and%2520task-scaling%252C%2520we%2520empirically%2520show%2520that%2520standard%2520Multi-Layer%250APerceptrons%2520%2528MLPs%2529%2520with%2520vectorized%2520input%2520are%2520only%2520capable%2520of%2520task-scaling.%2520To%250Aunderstand%2520how%2520transformers%2520are%2520capable%2520of%2520context-scaling%252C%2520we%2520first%2520propose%2520a%250Asignificantly%2520simplified%2520transformer%2520architecture%2520without%2520key%252C%2520query%252C%2520value%250Aweights.%2520We%2520show%2520that%2520it%2520performs%2520ICL%2520comparably%2520to%2520the%2520original%2520GPT-2%2520model%2520in%250Avarious%2520statistical%2520learning%2520tasks%2520including%2520linear%2520regression%252C%2520teacher-student%250Asettings.%2520Furthermore%252C%2520a%2520single%2520block%2520of%2520our%2520simplified%2520transformer%2520can%2520be%250Aviewed%2520as%2520data%2520dependent%2520feature%2520map%2520followed%2520by%2520an%2520MLP.%2520This%2520feature%2520map%2520on%250Aits%2520own%2520is%2520a%2520powerful%2520predictor%2520that%2520is%2520capable%2520of%2520context-scaling%2520but%2520is%2520not%250Acapable%2520of%2520task-scaling.%2520We%2520show%2520empirically%2520that%2520concatenating%2520the%2520output%2520of%250Athis%2520feature%2520map%2520with%2520vectorized%2520data%2520as%2520an%2520input%2520to%2520MLPs%2520enables%2520both%250Acontext-scaling%2520and%2520task-scaling.%2520This%2520finding%2520provides%2520a%2520simple%2520setting%2520to%250Astudy%2520context%2520and%2520task-scaling%2520for%2520ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Scaling%20versus%20Task-Scaling%20in%20In-Context%20Learning&entry.906535625=Amirhesam%20Abedsoltan%20and%20Adityanarayanan%20Radhakrishnan%20and%20Jingfeng%20Wu%20and%20Mikhail%20Belkin&entry.1292438233=%20%20Transformers%20exhibit%20In-Context%20Learning%20%28ICL%29%2C%20where%20these%20models%20solve%20new%0Atasks%20by%20using%20examples%20in%20the%20prompt%20without%20additional%20training.%20In%20our%20work%2C%0Awe%20identify%20and%20analyze%20two%20key%20components%20of%20ICL%3A%20%281%29%20context-scaling%2C%20where%0Amodel%20performance%20improves%20as%20the%20number%20of%20in-context%20examples%20increases%20and%0A%282%29%20task-scaling%2C%20where%20model%20performance%20improves%20as%20the%20number%20of%0Apre-training%20tasks%20increases.%20While%20transformers%20are%20capable%20of%20both%0Acontext-scaling%20and%20task-scaling%2C%20we%20empirically%20show%20that%20standard%20Multi-Layer%0APerceptrons%20%28MLPs%29%20with%20vectorized%20input%20are%20only%20capable%20of%20task-scaling.%20To%0Aunderstand%20how%20transformers%20are%20capable%20of%20context-scaling%2C%20we%20first%20propose%20a%0Asignificantly%20simplified%20transformer%20architecture%20without%20key%2C%20query%2C%20value%0Aweights.%20We%20show%20that%20it%20performs%20ICL%20comparably%20to%20the%20original%20GPT-2%20model%20in%0Avarious%20statistical%20learning%20tasks%20including%20linear%20regression%2C%20teacher-student%0Asettings.%20Furthermore%2C%20a%20single%20block%20of%20our%20simplified%20transformer%20can%20be%0Aviewed%20as%20data%20dependent%20feature%20map%20followed%20by%20an%20MLP.%20This%20feature%20map%20on%0Aits%20own%20is%20a%20powerful%20predictor%20that%20is%20capable%20of%20context-scaling%20but%20is%20not%0Acapable%20of%20task-scaling.%20We%20show%20empirically%20that%20concatenating%20the%20output%20of%0Athis%20feature%20map%20with%20vectorized%20data%20as%20an%20input%20to%20MLPs%20enables%20both%0Acontext-scaling%20and%20task-scaling.%20This%20finding%20provides%20a%20simple%20setting%20to%0Astudy%20context%20and%20task-scaling%20for%20ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12783v1&entry.124074799=Read"},
{"title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts", "author": "Hongcheng Gao and Tianyu Pang and Chao Du and Taihang Hu and Zhijie Deng and Min Lin", "abstract": "  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n", "link": "http://arxiv.org/abs/2410.12777v1", "date": "2024-10-16", "relevancy": 1.0532, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5336}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5282}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Unlearning%20on%20Diffusion%20Models%3A%20Preventing%20Relearning%20Unlearned%0A%20%20Concepts&body=Title%3A%20Meta-Unlearning%20on%20Diffusion%20Models%3A%20Preventing%20Relearning%20Unlearned%0A%20%20Concepts%0AAuthor%3A%20Hongcheng%20Gao%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Taihang%20Hu%20and%20Zhijie%20Deng%20and%20Min%20Lin%0AAbstract%3A%20%20%20With%20the%20rapid%20progress%20of%20diffusion-based%20content%20generation%2C%20significant%0Aefforts%20are%20being%20made%20to%20unlearn%20harmful%20or%20copyrighted%20concepts%20from%0Apretrained%20diffusion%20models%20%28DMs%29%20to%20prevent%20potential%20model%20misuse.%20However%2C%0Ait%20is%20observed%20that%20even%20when%20DMs%20are%20properly%20unlearned%20before%20release%2C%0Amalicious%20finetuning%20can%20compromise%20this%20process%2C%20causing%20DMs%20to%20relearn%20the%0Aunlearned%20concepts.%20This%20occurs%20partly%20because%20certain%20benign%20concepts%20%28e.g.%2C%0A%22skin%22%29%20retained%20in%20DMs%20are%20related%20to%20the%20unlearned%20ones%20%28e.g.%2C%20%22nudity%22%29%2C%0Afacilitating%20their%20relearning%20via%20finetuning.%20To%20address%20this%2C%20we%20propose%0Ameta-unlearning%20on%20DMs.%20Intuitively%2C%20a%20meta-unlearned%20DM%20should%20behave%20like%20an%0Aunlearned%20DM%20when%20used%20as%20is%3B%20moreover%2C%20if%20the%20meta-unlearned%20DM%20undergoes%0Amalicious%20finetuning%20on%20unlearned%20concepts%2C%20the%20related%20benign%20concepts%0Aretained%20within%20it%20will%20be%20triggered%20to%20self-destruct%2C%20hindering%20the%20relearning%0Aof%20unlearned%20concepts.%20Our%20meta-unlearning%20framework%20is%20compatible%20with%20most%0Aexisting%20unlearning%20methods%2C%20requiring%20only%20the%20addition%20of%20an%0Aeasy-to-implement%20meta%20objective.%20We%20validate%20our%20approach%20through%20empirical%0Aexperiments%20on%20meta-unlearning%20concepts%20from%20Stable%20Diffusion%20models%20%28SD-v1-4%0Aand%20SDXL%29%2C%20supported%20by%20extensive%20ablation%20studies.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/sail-sg/Meta-Unlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Unlearning%2520on%2520Diffusion%2520Models%253A%2520Preventing%2520Relearning%2520Unlearned%250A%2520%2520Concepts%26entry.906535625%3DHongcheng%2520Gao%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Taihang%2520Hu%2520and%2520Zhijie%2520Deng%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520progress%2520of%2520diffusion-based%2520content%2520generation%252C%2520significant%250Aefforts%2520are%2520being%2520made%2520to%2520unlearn%2520harmful%2520or%2520copyrighted%2520concepts%2520from%250Apretrained%2520diffusion%2520models%2520%2528DMs%2529%2520to%2520prevent%2520potential%2520model%2520misuse.%2520However%252C%250Ait%2520is%2520observed%2520that%2520even%2520when%2520DMs%2520are%2520properly%2520unlearned%2520before%2520release%252C%250Amalicious%2520finetuning%2520can%2520compromise%2520this%2520process%252C%2520causing%2520DMs%2520to%2520relearn%2520the%250Aunlearned%2520concepts.%2520This%2520occurs%2520partly%2520because%2520certain%2520benign%2520concepts%2520%2528e.g.%252C%250A%2522skin%2522%2529%2520retained%2520in%2520DMs%2520are%2520related%2520to%2520the%2520unlearned%2520ones%2520%2528e.g.%252C%2520%2522nudity%2522%2529%252C%250Afacilitating%2520their%2520relearning%2520via%2520finetuning.%2520To%2520address%2520this%252C%2520we%2520propose%250Ameta-unlearning%2520on%2520DMs.%2520Intuitively%252C%2520a%2520meta-unlearned%2520DM%2520should%2520behave%2520like%2520an%250Aunlearned%2520DM%2520when%2520used%2520as%2520is%253B%2520moreover%252C%2520if%2520the%2520meta-unlearned%2520DM%2520undergoes%250Amalicious%2520finetuning%2520on%2520unlearned%2520concepts%252C%2520the%2520related%2520benign%2520concepts%250Aretained%2520within%2520it%2520will%2520be%2520triggered%2520to%2520self-destruct%252C%2520hindering%2520the%2520relearning%250Aof%2520unlearned%2520concepts.%2520Our%2520meta-unlearning%2520framework%2520is%2520compatible%2520with%2520most%250Aexisting%2520unlearning%2520methods%252C%2520requiring%2520only%2520the%2520addition%2520of%2520an%250Aeasy-to-implement%2520meta%2520objective.%2520We%2520validate%2520our%2520approach%2520through%2520empirical%250Aexperiments%2520on%2520meta-unlearning%2520concepts%2520from%2520Stable%2520Diffusion%2520models%2520%2528SD-v1-4%250Aand%2520SDXL%2529%252C%2520supported%2520by%2520extensive%2520ablation%2520studies.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sail-sg/Meta-Unlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Unlearning%20on%20Diffusion%20Models%3A%20Preventing%20Relearning%20Unlearned%0A%20%20Concepts&entry.906535625=Hongcheng%20Gao%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Taihang%20Hu%20and%20Zhijie%20Deng%20and%20Min%20Lin&entry.1292438233=%20%20With%20the%20rapid%20progress%20of%20diffusion-based%20content%20generation%2C%20significant%0Aefforts%20are%20being%20made%20to%20unlearn%20harmful%20or%20copyrighted%20concepts%20from%0Apretrained%20diffusion%20models%20%28DMs%29%20to%20prevent%20potential%20model%20misuse.%20However%2C%0Ait%20is%20observed%20that%20even%20when%20DMs%20are%20properly%20unlearned%20before%20release%2C%0Amalicious%20finetuning%20can%20compromise%20this%20process%2C%20causing%20DMs%20to%20relearn%20the%0Aunlearned%20concepts.%20This%20occurs%20partly%20because%20certain%20benign%20concepts%20%28e.g.%2C%0A%22skin%22%29%20retained%20in%20DMs%20are%20related%20to%20the%20unlearned%20ones%20%28e.g.%2C%20%22nudity%22%29%2C%0Afacilitating%20their%20relearning%20via%20finetuning.%20To%20address%20this%2C%20we%20propose%0Ameta-unlearning%20on%20DMs.%20Intuitively%2C%20a%20meta-unlearned%20DM%20should%20behave%20like%20an%0Aunlearned%20DM%20when%20used%20as%20is%3B%20moreover%2C%20if%20the%20meta-unlearned%20DM%20undergoes%0Amalicious%20finetuning%20on%20unlearned%20concepts%2C%20the%20related%20benign%20concepts%0Aretained%20within%20it%20will%20be%20triggered%20to%20self-destruct%2C%20hindering%20the%20relearning%0Aof%20unlearned%20concepts.%20Our%20meta-unlearning%20framework%20is%20compatible%20with%20most%0Aexisting%20unlearning%20methods%2C%20requiring%20only%20the%20addition%20of%20an%0Aeasy-to-implement%20meta%20objective.%20We%20validate%20our%20approach%20through%20empirical%0Aexperiments%20on%20meta-unlearning%20concepts%20from%20Stable%20Diffusion%20models%20%28SD-v1-4%0Aand%20SDXL%29%2C%20supported%20by%20extensive%20ablation%20studies.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/sail-sg/Meta-Unlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12777v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


