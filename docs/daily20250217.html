<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250216.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding", "author": "Wenxuan Guo and Xiuwei Xu and Ziwei Wang and Jianjiang Feng and Jie Zhou and Jiwen Lu", "abstract": "  In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.\n", "link": "http://arxiv.org/abs/2502.10392v1", "date": "2025-02-14", "relevancy": 3.0316, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-guided%20Sparse%20Voxel%20Pruning%20for%20Efficient%203D%20Visual%20Grounding&body=Title%3A%20Text-guided%20Sparse%20Voxel%20Pruning%20for%20Efficient%203D%20Visual%20Grounding%0AAuthor%3A%20Wenxuan%20Guo%20and%20Xiuwei%20Xu%20and%20Ziwei%20Wang%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20multi-level%20convolution%20architecture%0Afor%203D%20visual%20grounding.%20Conventional%20methods%20are%20difficult%20to%20meet%20the%0Arequirements%20of%20real-time%20inference%20due%20to%20the%20two-stage%20or%20point-based%0Aarchitecture.%20Inspired%20by%20the%20success%20of%20multi-level%20fully%20sparse%20convolutional%0Aarchitecture%20in%203D%20object%20detection%2C%20we%20aim%20to%20build%20a%20new%203D%20visual%20grounding%0Aframework%20following%20this%20technical%20route.%20However%2C%20as%20in%203D%20visual%20grounding%0Atask%20the%203D%20scene%20representation%20should%20be%20deeply%20interacted%20with%20text%0Afeatures%2C%20sparse%20convolution-based%20architecture%20is%20inefficient%20for%20this%0Ainteraction%20due%20to%20the%20large%20amount%20of%20voxel%20features.%20To%20this%20end%2C%20we%20propose%0Atext-guided%20pruning%20%28TGP%29%20and%20completion-based%20addition%20%28CBA%29%20to%20deeply%20fuse%203D%0Ascene%20representation%20and%20text%20features%20in%20an%20efficient%20way%20by%20gradual%20region%0Apruning%20and%20target%20completion.%20Specifically%2C%20TGP%20iteratively%20sparsifies%20the%203D%0Ascene%20representation%20and%20thus%20efficiently%20interacts%20the%20voxel%20features%20with%0Atext%20features%20by%20cross-attention.%20To%20mitigate%20the%20affect%20of%20pruning%20on%20delicate%0Ageometric%20information%2C%20CBA%20adaptively%20fixes%20the%20over-pruned%20region%20by%20voxel%0Acompletion%20with%20negligible%20computational%20overhead.%20Compared%20with%20previous%0Asingle-stage%20methods%2C%20our%20method%20achieves%20top%20inference%20speed%20and%20surpasses%0Aprevious%20fastest%20method%20by%20100%5C%25%20FPS.%20Our%20method%20also%20achieves%20state-of-the-art%0Aaccuracy%20even%20compared%20with%20two-stage%20methods%2C%20with%20%24%2B1.13%24%20lead%20of%20Acc%400.5%20on%0AScanRefer%2C%20and%20%24%2B2.6%24%20and%20%24%2B3.2%24%20leads%20on%20NR3D%20and%20SR3D%20respectively.%20The%20code%0Ais%20available%20at%0A%5Chref%7Bhttps%3A//github.com/GWxuan/TSP3D%7D%7Bhttps%3A//github.com/GWxuan/TSP3D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-guided%2520Sparse%2520Voxel%2520Pruning%2520for%2520Efficient%25203D%2520Visual%2520Grounding%26entry.906535625%3DWenxuan%2520Guo%2520and%2520Xiuwei%2520Xu%2520and%2520Ziwei%2520Wang%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520efficient%2520multi-level%2520convolution%2520architecture%250Afor%25203D%2520visual%2520grounding.%2520Conventional%2520methods%2520are%2520difficult%2520to%2520meet%2520the%250Arequirements%2520of%2520real-time%2520inference%2520due%2520to%2520the%2520two-stage%2520or%2520point-based%250Aarchitecture.%2520Inspired%2520by%2520the%2520success%2520of%2520multi-level%2520fully%2520sparse%2520convolutional%250Aarchitecture%2520in%25203D%2520object%2520detection%252C%2520we%2520aim%2520to%2520build%2520a%2520new%25203D%2520visual%2520grounding%250Aframework%2520following%2520this%2520technical%2520route.%2520However%252C%2520as%2520in%25203D%2520visual%2520grounding%250Atask%2520the%25203D%2520scene%2520representation%2520should%2520be%2520deeply%2520interacted%2520with%2520text%250Afeatures%252C%2520sparse%2520convolution-based%2520architecture%2520is%2520inefficient%2520for%2520this%250Ainteraction%2520due%2520to%2520the%2520large%2520amount%2520of%2520voxel%2520features.%2520To%2520this%2520end%252C%2520we%2520propose%250Atext-guided%2520pruning%2520%2528TGP%2529%2520and%2520completion-based%2520addition%2520%2528CBA%2529%2520to%2520deeply%2520fuse%25203D%250Ascene%2520representation%2520and%2520text%2520features%2520in%2520an%2520efficient%2520way%2520by%2520gradual%2520region%250Apruning%2520and%2520target%2520completion.%2520Specifically%252C%2520TGP%2520iteratively%2520sparsifies%2520the%25203D%250Ascene%2520representation%2520and%2520thus%2520efficiently%2520interacts%2520the%2520voxel%2520features%2520with%250Atext%2520features%2520by%2520cross-attention.%2520To%2520mitigate%2520the%2520affect%2520of%2520pruning%2520on%2520delicate%250Ageometric%2520information%252C%2520CBA%2520adaptively%2520fixes%2520the%2520over-pruned%2520region%2520by%2520voxel%250Acompletion%2520with%2520negligible%2520computational%2520overhead.%2520Compared%2520with%2520previous%250Asingle-stage%2520methods%252C%2520our%2520method%2520achieves%2520top%2520inference%2520speed%2520and%2520surpasses%250Aprevious%2520fastest%2520method%2520by%2520100%255C%2525%2520FPS.%2520Our%2520method%2520also%2520achieves%2520state-of-the-art%250Aaccuracy%2520even%2520compared%2520with%2520two-stage%2520methods%252C%2520with%2520%2524%252B1.13%2524%2520lead%2520of%2520Acc%25400.5%2520on%250AScanRefer%252C%2520and%2520%2524%252B2.6%2524%2520and%2520%2524%252B3.2%2524%2520leads%2520on%2520NR3D%2520and%2520SR3D%2520respectively.%2520The%2520code%250Ais%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/GWxuan/TSP3D%257D%257Bhttps%253A//github.com/GWxuan/TSP3D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-guided%20Sparse%20Voxel%20Pruning%20for%20Efficient%203D%20Visual%20Grounding&entry.906535625=Wenxuan%20Guo%20and%20Xiuwei%20Xu%20and%20Ziwei%20Wang%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20efficient%20multi-level%20convolution%20architecture%0Afor%203D%20visual%20grounding.%20Conventional%20methods%20are%20difficult%20to%20meet%20the%0Arequirements%20of%20real-time%20inference%20due%20to%20the%20two-stage%20or%20point-based%0Aarchitecture.%20Inspired%20by%20the%20success%20of%20multi-level%20fully%20sparse%20convolutional%0Aarchitecture%20in%203D%20object%20detection%2C%20we%20aim%20to%20build%20a%20new%203D%20visual%20grounding%0Aframework%20following%20this%20technical%20route.%20However%2C%20as%20in%203D%20visual%20grounding%0Atask%20the%203D%20scene%20representation%20should%20be%20deeply%20interacted%20with%20text%0Afeatures%2C%20sparse%20convolution-based%20architecture%20is%20inefficient%20for%20this%0Ainteraction%20due%20to%20the%20large%20amount%20of%20voxel%20features.%20To%20this%20end%2C%20we%20propose%0Atext-guided%20pruning%20%28TGP%29%20and%20completion-based%20addition%20%28CBA%29%20to%20deeply%20fuse%203D%0Ascene%20representation%20and%20text%20features%20in%20an%20efficient%20way%20by%20gradual%20region%0Apruning%20and%20target%20completion.%20Specifically%2C%20TGP%20iteratively%20sparsifies%20the%203D%0Ascene%20representation%20and%20thus%20efficiently%20interacts%20the%20voxel%20features%20with%0Atext%20features%20by%20cross-attention.%20To%20mitigate%20the%20affect%20of%20pruning%20on%20delicate%0Ageometric%20information%2C%20CBA%20adaptively%20fixes%20the%20over-pruned%20region%20by%20voxel%0Acompletion%20with%20negligible%20computational%20overhead.%20Compared%20with%20previous%0Asingle-stage%20methods%2C%20our%20method%20achieves%20top%20inference%20speed%20and%20surpasses%0Aprevious%20fastest%20method%20by%20100%5C%25%20FPS.%20Our%20method%20also%20achieves%20state-of-the-art%0Aaccuracy%20even%20compared%20with%20two-stage%20methods%2C%20with%20%24%2B1.13%24%20lead%20of%20Acc%400.5%20on%0AScanRefer%2C%20and%20%24%2B2.6%24%20and%20%24%2B3.2%24%20leads%20on%20NR3D%20and%20SR3D%20respectively.%20The%20code%0Ais%20available%20at%0A%5Chref%7Bhttps%3A//github.com/GWxuan/TSP3D%7D%7Bhttps%3A//github.com/GWxuan/TSP3D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10392v1&entry.124074799=Read"},
{"title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering", "author": "Zeqing Wang and Wentao Wan and Qiqing Lao and Runmeng Chen and Minjie Lang and Xiao Wang and Keze Wang and Liang Lin", "abstract": "  Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost.\n", "link": "http://arxiv.org/abs/2311.17331v4", "date": "2025-02-14", "relevancy": 2.9009, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Top-Down%20Reasoning%3A%20An%20Explainable%20Multi-Agent%20Approach%20for%0A%20%20Visual%20Question%20Answering&body=Title%3A%20Towards%20Top-Down%20Reasoning%3A%20An%20Explainable%20Multi-Agent%20Approach%20for%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Zeqing%20Wang%20and%20Wentao%20Wan%20and%20Qiqing%20Lao%20and%20Runmeng%20Chen%20and%20Minjie%20Lang%20and%20Xiao%20Wang%20and%20Keze%20Wang%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Recently%2C%20to%20comprehensively%20improve%20Vision%20Language%20Models%20%28VLMs%29%20for%20Visual%0AQuestion%20Answering%20%28VQA%29%2C%20several%20methods%20have%20been%20proposed%20to%20further%0Areinforce%20the%20inference%20capabilities%20of%20VLMs%20to%20independently%20tackle%20VQA%20tasks%0Arather%20than%20some%20methods%20that%20only%20utilize%20VLMs%20as%20aids%20to%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20these%20methods%20ignore%20the%20rich%20common-sense%20knowledge%0Ainside%20the%20given%20VQA%20image%20sampled%20from%20the%20real%20world.%20Thus%2C%20they%20cannot%20fully%0Ause%20the%20powerful%20VLM%20for%20the%20given%20VQA%20question%20to%20achieve%20optimal%20performance.%0AAttempt%20to%20overcome%20this%20limitation%20and%20inspired%20by%20the%20human%20top-down%0Areasoning%20process%2C%20i.e.%2C%20systematically%20exploring%20relevant%20issues%20to%20derive%20a%0Acomprehensive%20answer%2C%20this%20work%20introduces%20a%20novel%2C%20explainable%20multi-agent%0Acollaboration%20framework%20by%20leveraging%20the%20expansive%20knowledge%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enhance%20the%20capabilities%20of%20VLMs%20themselves.%20Specifically%2C%20our%0Aframework%20comprises%20three%20agents%2C%20i.e.%2C%20Responder%2C%20Seeker%2C%20and%20Integrator%2C%20to%0Acollaboratively%20answer%20the%20given%20VQA%20question%20by%20seeking%20its%20relevant%20issues%0Aand%20generating%20the%20final%20answer%20in%20such%20a%20top-down%20reasoning%20process.%20The%0AVLM-based%20Responder%20agent%20generates%20the%20answer%20candidates%20for%20the%20question%20and%0Aresponds%20to%20other%20relevant%20issues.%20The%20Seeker%20agent%2C%20primarily%20based%20on%20LLM%2C%0Aidentifies%20relevant%20issues%20related%20to%20the%20question%20to%20inform%20the%20Responder%0Aagent%20and%20constructs%20a%20Multi-View%20Knowledge%20Base%20%28MVKB%29%20for%20the%20given%20visual%0Ascene%20by%20leveraging%20the%20build-in%20world%20knowledge%20of%20LLM.%20The%20Integrator%20agent%0Acombines%20knowledge%20from%20the%20Seeker%20agent%20and%20the%20Responder%20agent%20to%20produce%20the%0Afinal%20VQA%20answer.%20Extensive%20and%20comprehensive%20evaluations%20on%20diverse%20VQA%0Adatasets%20with%20a%20variety%20of%20VLMs%20demonstrate%20the%20superior%20performance%20and%0Ainterpretability%20of%20our%20framework%20over%20the%20baseline%20method%20in%20the%20zero-shot%0Asetting%20without%20extra%20training%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17331v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Top-Down%2520Reasoning%253A%2520An%2520Explainable%2520Multi-Agent%2520Approach%2520for%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DZeqing%2520Wang%2520and%2520Wentao%2520Wan%2520and%2520Qiqing%2520Lao%2520and%2520Runmeng%2520Chen%2520and%2520Minjie%2520Lang%2520and%2520Xiao%2520Wang%2520and%2520Keze%2520Wang%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Recently%252C%2520to%2520comprehensively%2520improve%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520for%2520Visual%250AQuestion%2520Answering%2520%2528VQA%2529%252C%2520several%2520methods%2520have%2520been%2520proposed%2520to%2520further%250Areinforce%2520the%2520inference%2520capabilities%2520of%2520VLMs%2520to%2520independently%2520tackle%2520VQA%2520tasks%250Arather%2520than%2520some%2520methods%2520that%2520only%2520utilize%2520VLMs%2520as%2520aids%2520to%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520However%252C%2520these%2520methods%2520ignore%2520the%2520rich%2520common-sense%2520knowledge%250Ainside%2520the%2520given%2520VQA%2520image%2520sampled%2520from%2520the%2520real%2520world.%2520Thus%252C%2520they%2520cannot%2520fully%250Ause%2520the%2520powerful%2520VLM%2520for%2520the%2520given%2520VQA%2520question%2520to%2520achieve%2520optimal%2520performance.%250AAttempt%2520to%2520overcome%2520this%2520limitation%2520and%2520inspired%2520by%2520the%2520human%2520top-down%250Areasoning%2520process%252C%2520i.e.%252C%2520systematically%2520exploring%2520relevant%2520issues%2520to%2520derive%2520a%250Acomprehensive%2520answer%252C%2520this%2520work%2520introduces%2520a%2520novel%252C%2520explainable%2520multi-agent%250Acollaboration%2520framework%2520by%2520leveraging%2520the%2520expansive%2520knowledge%2520of%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520enhance%2520the%2520capabilities%2520of%2520VLMs%2520themselves.%2520Specifically%252C%2520our%250Aframework%2520comprises%2520three%2520agents%252C%2520i.e.%252C%2520Responder%252C%2520Seeker%252C%2520and%2520Integrator%252C%2520to%250Acollaboratively%2520answer%2520the%2520given%2520VQA%2520question%2520by%2520seeking%2520its%2520relevant%2520issues%250Aand%2520generating%2520the%2520final%2520answer%2520in%2520such%2520a%2520top-down%2520reasoning%2520process.%2520The%250AVLM-based%2520Responder%2520agent%2520generates%2520the%2520answer%2520candidates%2520for%2520the%2520question%2520and%250Aresponds%2520to%2520other%2520relevant%2520issues.%2520The%2520Seeker%2520agent%252C%2520primarily%2520based%2520on%2520LLM%252C%250Aidentifies%2520relevant%2520issues%2520related%2520to%2520the%2520question%2520to%2520inform%2520the%2520Responder%250Aagent%2520and%2520constructs%2520a%2520Multi-View%2520Knowledge%2520Base%2520%2528MVKB%2529%2520for%2520the%2520given%2520visual%250Ascene%2520by%2520leveraging%2520the%2520build-in%2520world%2520knowledge%2520of%2520LLM.%2520The%2520Integrator%2520agent%250Acombines%2520knowledge%2520from%2520the%2520Seeker%2520agent%2520and%2520the%2520Responder%2520agent%2520to%2520produce%2520the%250Afinal%2520VQA%2520answer.%2520Extensive%2520and%2520comprehensive%2520evaluations%2520on%2520diverse%2520VQA%250Adatasets%2520with%2520a%2520variety%2520of%2520VLMs%2520demonstrate%2520the%2520superior%2520performance%2520and%250Ainterpretability%2520of%2520our%2520framework%2520over%2520the%2520baseline%2520method%2520in%2520the%2520zero-shot%250Asetting%2520without%2520extra%2520training%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17331v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Top-Down%20Reasoning%3A%20An%20Explainable%20Multi-Agent%20Approach%20for%0A%20%20Visual%20Question%20Answering&entry.906535625=Zeqing%20Wang%20and%20Wentao%20Wan%20and%20Qiqing%20Lao%20and%20Runmeng%20Chen%20and%20Minjie%20Lang%20and%20Xiao%20Wang%20and%20Keze%20Wang%20and%20Liang%20Lin&entry.1292438233=%20%20Recently%2C%20to%20comprehensively%20improve%20Vision%20Language%20Models%20%28VLMs%29%20for%20Visual%0AQuestion%20Answering%20%28VQA%29%2C%20several%20methods%20have%20been%20proposed%20to%20further%0Areinforce%20the%20inference%20capabilities%20of%20VLMs%20to%20independently%20tackle%20VQA%20tasks%0Arather%20than%20some%20methods%20that%20only%20utilize%20VLMs%20as%20aids%20to%20Large%20Language%0AModels%20%28LLMs%29.%20However%2C%20these%20methods%20ignore%20the%20rich%20common-sense%20knowledge%0Ainside%20the%20given%20VQA%20image%20sampled%20from%20the%20real%20world.%20Thus%2C%20they%20cannot%20fully%0Ause%20the%20powerful%20VLM%20for%20the%20given%20VQA%20question%20to%20achieve%20optimal%20performance.%0AAttempt%20to%20overcome%20this%20limitation%20and%20inspired%20by%20the%20human%20top-down%0Areasoning%20process%2C%20i.e.%2C%20systematically%20exploring%20relevant%20issues%20to%20derive%20a%0Acomprehensive%20answer%2C%20this%20work%20introduces%20a%20novel%2C%20explainable%20multi-agent%0Acollaboration%20framework%20by%20leveraging%20the%20expansive%20knowledge%20of%20Large%20Language%0AModels%20%28LLMs%29%20to%20enhance%20the%20capabilities%20of%20VLMs%20themselves.%20Specifically%2C%20our%0Aframework%20comprises%20three%20agents%2C%20i.e.%2C%20Responder%2C%20Seeker%2C%20and%20Integrator%2C%20to%0Acollaboratively%20answer%20the%20given%20VQA%20question%20by%20seeking%20its%20relevant%20issues%0Aand%20generating%20the%20final%20answer%20in%20such%20a%20top-down%20reasoning%20process.%20The%0AVLM-based%20Responder%20agent%20generates%20the%20answer%20candidates%20for%20the%20question%20and%0Aresponds%20to%20other%20relevant%20issues.%20The%20Seeker%20agent%2C%20primarily%20based%20on%20LLM%2C%0Aidentifies%20relevant%20issues%20related%20to%20the%20question%20to%20inform%20the%20Responder%0Aagent%20and%20constructs%20a%20Multi-View%20Knowledge%20Base%20%28MVKB%29%20for%20the%20given%20visual%0Ascene%20by%20leveraging%20the%20build-in%20world%20knowledge%20of%20LLM.%20The%20Integrator%20agent%0Acombines%20knowledge%20from%20the%20Seeker%20agent%20and%20the%20Responder%20agent%20to%20produce%20the%0Afinal%20VQA%20answer.%20Extensive%20and%20comprehensive%20evaluations%20on%20diverse%20VQA%0Adatasets%20with%20a%20variety%20of%20VLMs%20demonstrate%20the%20superior%20performance%20and%0Ainterpretability%20of%20our%20framework%20over%20the%20baseline%20method%20in%20the%20zero-shot%0Asetting%20without%20extra%20training%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17331v4&entry.124074799=Read"},
{"title": "Probing Perceptual Constancy in Large Vision Language Models", "author": "Haoran Sun and Suyang Yu and Yijiang Li and Qingying Gao and Haiyun Lyu and Hokin Deng and Dezhi Luo", "abstract": "  Perceptual constancy is the ability to maintain stable perceptions of objects\ndespite changes in sensory input, such as variations in distance, angle, or\nlighting. This ability is crucial for recognizing visual information in a\ndynamic world, making it essential for Vision-Language Models (VLMs). However,\nwhether VLMs are currently and theoretically capable of mastering this ability\nremains underexplored. In this study, we evaluated 33 VLMs using 253\nexperiments across three domains: color, size, and shape constancy. The\nexperiments included single-image and video adaptations of classic cognitive\ntasks, along with novel tasks in in-the-wild conditions, to evaluate the\nmodels' recognition of object properties under varying conditions. We found\nsignificant variability in VLM performance, with models performance in shape\nconstancy clearly dissociated from that of color and size constancy.\n", "link": "http://arxiv.org/abs/2502.10273v1", "date": "2025-02-14", "relevancy": 2.8711, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6255}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Perceptual%20Constancy%20in%20Large%20Vision%20Language%20Models&body=Title%3A%20Probing%20Perceptual%20Constancy%20in%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Haoran%20Sun%20and%20Suyang%20Yu%20and%20Yijiang%20Li%20and%20Qingying%20Gao%20and%20Haiyun%20Lyu%20and%20Hokin%20Deng%20and%20Dezhi%20Luo%0AAbstract%3A%20%20%20Perceptual%20constancy%20is%20the%20ability%20to%20maintain%20stable%20perceptions%20of%20objects%0Adespite%20changes%20in%20sensory%20input%2C%20such%20as%20variations%20in%20distance%2C%20angle%2C%20or%0Alighting.%20This%20ability%20is%20crucial%20for%20recognizing%20visual%20information%20in%20a%0Adynamic%20world%2C%20making%20it%20essential%20for%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%0Awhether%20VLMs%20are%20currently%20and%20theoretically%20capable%20of%20mastering%20this%20ability%0Aremains%20underexplored.%20In%20this%20study%2C%20we%20evaluated%2033%20VLMs%20using%20253%0Aexperiments%20across%20three%20domains%3A%20color%2C%20size%2C%20and%20shape%20constancy.%20The%0Aexperiments%20included%20single-image%20and%20video%20adaptations%20of%20classic%20cognitive%0Atasks%2C%20along%20with%20novel%20tasks%20in%20in-the-wild%20conditions%2C%20to%20evaluate%20the%0Amodels%27%20recognition%20of%20object%20properties%20under%20varying%20conditions.%20We%20found%0Asignificant%20variability%20in%20VLM%20performance%2C%20with%20models%20performance%20in%20shape%0Aconstancy%20clearly%20dissociated%20from%20that%20of%20color%20and%20size%20constancy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Perceptual%2520Constancy%2520in%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DHaoran%2520Sun%2520and%2520Suyang%2520Yu%2520and%2520Yijiang%2520Li%2520and%2520Qingying%2520Gao%2520and%2520Haiyun%2520Lyu%2520and%2520Hokin%2520Deng%2520and%2520Dezhi%2520Luo%26entry.1292438233%3D%2520%2520Perceptual%2520constancy%2520is%2520the%2520ability%2520to%2520maintain%2520stable%2520perceptions%2520of%2520objects%250Adespite%2520changes%2520in%2520sensory%2520input%252C%2520such%2520as%2520variations%2520in%2520distance%252C%2520angle%252C%2520or%250Alighting.%2520This%2520ability%2520is%2520crucial%2520for%2520recognizing%2520visual%2520information%2520in%2520a%250Adynamic%2520world%252C%2520making%2520it%2520essential%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520However%252C%250Awhether%2520VLMs%2520are%2520currently%2520and%2520theoretically%2520capable%2520of%2520mastering%2520this%2520ability%250Aremains%2520underexplored.%2520In%2520this%2520study%252C%2520we%2520evaluated%252033%2520VLMs%2520using%2520253%250Aexperiments%2520across%2520three%2520domains%253A%2520color%252C%2520size%252C%2520and%2520shape%2520constancy.%2520The%250Aexperiments%2520included%2520single-image%2520and%2520video%2520adaptations%2520of%2520classic%2520cognitive%250Atasks%252C%2520along%2520with%2520novel%2520tasks%2520in%2520in-the-wild%2520conditions%252C%2520to%2520evaluate%2520the%250Amodels%2527%2520recognition%2520of%2520object%2520properties%2520under%2520varying%2520conditions.%2520We%2520found%250Asignificant%2520variability%2520in%2520VLM%2520performance%252C%2520with%2520models%2520performance%2520in%2520shape%250Aconstancy%2520clearly%2520dissociated%2520from%2520that%2520of%2520color%2520and%2520size%2520constancy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Perceptual%20Constancy%20in%20Large%20Vision%20Language%20Models&entry.906535625=Haoran%20Sun%20and%20Suyang%20Yu%20and%20Yijiang%20Li%20and%20Qingying%20Gao%20and%20Haiyun%20Lyu%20and%20Hokin%20Deng%20and%20Dezhi%20Luo&entry.1292438233=%20%20Perceptual%20constancy%20is%20the%20ability%20to%20maintain%20stable%20perceptions%20of%20objects%0Adespite%20changes%20in%20sensory%20input%2C%20such%20as%20variations%20in%20distance%2C%20angle%2C%20or%0Alighting.%20This%20ability%20is%20crucial%20for%20recognizing%20visual%20information%20in%20a%0Adynamic%20world%2C%20making%20it%20essential%20for%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%0Awhether%20VLMs%20are%20currently%20and%20theoretically%20capable%20of%20mastering%20this%20ability%0Aremains%20underexplored.%20In%20this%20study%2C%20we%20evaluated%2033%20VLMs%20using%20253%0Aexperiments%20across%20three%20domains%3A%20color%2C%20size%2C%20and%20shape%20constancy.%20The%0Aexperiments%20included%20single-image%20and%20video%20adaptations%20of%20classic%20cognitive%0Atasks%2C%20along%20with%20novel%20tasks%20in%20in-the-wild%20conditions%2C%20to%20evaluate%20the%0Amodels%27%20recognition%20of%20object%20properties%20under%20varying%20conditions.%20We%20found%0Asignificant%20variability%20in%20VLM%20performance%2C%20with%20models%20performance%20in%20shape%0Aconstancy%20clearly%20dissociated%20from%20that%20of%20color%20and%20size%20constancy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10273v1&entry.124074799=Read"},
{"title": "ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences", "author": "Liyuan Zhu and Shengqu Cai and Shengyu Huang and Gordon Wetzstein and Naji Khosravan and Iro Armeni", "abstract": "  We introduce ReStyle3D, a novel framework for scene-level appearance transfer\nfrom a single style image to a real-world scene represented by multiple views.\nThe method combines explicit semantic correspondences with multi-view\nconsistency to achieve precise and coherent stylization. Unlike conventional\nstylization methods that apply a reference style globally, ReStyle3D uses\nopen-vocabulary segmentation to establish dense, instance-level correspondences\nbetween the style and real-world images. This ensures that each object is\nstylized with semantically matched textures. It first transfers the style to a\nsingle view using a training-free semantic-attention mechanism in a diffusion\nmodel. It then lifts the stylization to additional views via a learned\nwarp-and-refine network guided by monocular depth and pixel-wise\ncorrespondences. Experiments show that ReStyle3D consistently outperforms prior\nmethods in structure preservation, perceptual style similarity, and multi-view\ncoherence. User studies further validate its ability to produce\nphoto-realistic, semantically faithful results. Our code, pretrained models,\nand dataset will be publicly released, to support new applications in interior\ndesign, virtual staging, and 3D-consistent stylization.\n", "link": "http://arxiv.org/abs/2502.10377v1", "date": "2025-02-14", "relevancy": 2.8683, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.579}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReStyle3D%3A%20Scene-Level%20Appearance%20Transfer%20with%20Semantic%20Correspondences&body=Title%3A%20ReStyle3D%3A%20Scene-Level%20Appearance%20Transfer%20with%20Semantic%20Correspondences%0AAuthor%3A%20Liyuan%20Zhu%20and%20Shengqu%20Cai%20and%20Shengyu%20Huang%20and%20Gordon%20Wetzstein%20and%20Naji%20Khosravan%20and%20Iro%20Armeni%0AAbstract%3A%20%20%20We%20introduce%20ReStyle3D%2C%20a%20novel%20framework%20for%20scene-level%20appearance%20transfer%0Afrom%20a%20single%20style%20image%20to%20a%20real-world%20scene%20represented%20by%20multiple%20views.%0AThe%20method%20combines%20explicit%20semantic%20correspondences%20with%20multi-view%0Aconsistency%20to%20achieve%20precise%20and%20coherent%20stylization.%20Unlike%20conventional%0Astylization%20methods%20that%20apply%20a%20reference%20style%20globally%2C%20ReStyle3D%20uses%0Aopen-vocabulary%20segmentation%20to%20establish%20dense%2C%20instance-level%20correspondences%0Abetween%20the%20style%20and%20real-world%20images.%20This%20ensures%20that%20each%20object%20is%0Astylized%20with%20semantically%20matched%20textures.%20It%20first%20transfers%20the%20style%20to%20a%0Asingle%20view%20using%20a%20training-free%20semantic-attention%20mechanism%20in%20a%20diffusion%0Amodel.%20It%20then%20lifts%20the%20stylization%20to%20additional%20views%20via%20a%20learned%0Awarp-and-refine%20network%20guided%20by%20monocular%20depth%20and%20pixel-wise%0Acorrespondences.%20Experiments%20show%20that%20ReStyle3D%20consistently%20outperforms%20prior%0Amethods%20in%20structure%20preservation%2C%20perceptual%20style%20similarity%2C%20and%20multi-view%0Acoherence.%20User%20studies%20further%20validate%20its%20ability%20to%20produce%0Aphoto-realistic%2C%20semantically%20faithful%20results.%20Our%20code%2C%20pretrained%20models%2C%0Aand%20dataset%20will%20be%20publicly%20released%2C%20to%20support%20new%20applications%20in%20interior%0Adesign%2C%20virtual%20staging%2C%20and%203D-consistent%20stylization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReStyle3D%253A%2520Scene-Level%2520Appearance%2520Transfer%2520with%2520Semantic%2520Correspondences%26entry.906535625%3DLiyuan%2520Zhu%2520and%2520Shengqu%2520Cai%2520and%2520Shengyu%2520Huang%2520and%2520Gordon%2520Wetzstein%2520and%2520Naji%2520Khosravan%2520and%2520Iro%2520Armeni%26entry.1292438233%3D%2520%2520We%2520introduce%2520ReStyle3D%252C%2520a%2520novel%2520framework%2520for%2520scene-level%2520appearance%2520transfer%250Afrom%2520a%2520single%2520style%2520image%2520to%2520a%2520real-world%2520scene%2520represented%2520by%2520multiple%2520views.%250AThe%2520method%2520combines%2520explicit%2520semantic%2520correspondences%2520with%2520multi-view%250Aconsistency%2520to%2520achieve%2520precise%2520and%2520coherent%2520stylization.%2520Unlike%2520conventional%250Astylization%2520methods%2520that%2520apply%2520a%2520reference%2520style%2520globally%252C%2520ReStyle3D%2520uses%250Aopen-vocabulary%2520segmentation%2520to%2520establish%2520dense%252C%2520instance-level%2520correspondences%250Abetween%2520the%2520style%2520and%2520real-world%2520images.%2520This%2520ensures%2520that%2520each%2520object%2520is%250Astylized%2520with%2520semantically%2520matched%2520textures.%2520It%2520first%2520transfers%2520the%2520style%2520to%2520a%250Asingle%2520view%2520using%2520a%2520training-free%2520semantic-attention%2520mechanism%2520in%2520a%2520diffusion%250Amodel.%2520It%2520then%2520lifts%2520the%2520stylization%2520to%2520additional%2520views%2520via%2520a%2520learned%250Awarp-and-refine%2520network%2520guided%2520by%2520monocular%2520depth%2520and%2520pixel-wise%250Acorrespondences.%2520Experiments%2520show%2520that%2520ReStyle3D%2520consistently%2520outperforms%2520prior%250Amethods%2520in%2520structure%2520preservation%252C%2520perceptual%2520style%2520similarity%252C%2520and%2520multi-view%250Acoherence.%2520User%2520studies%2520further%2520validate%2520its%2520ability%2520to%2520produce%250Aphoto-realistic%252C%2520semantically%2520faithful%2520results.%2520Our%2520code%252C%2520pretrained%2520models%252C%250Aand%2520dataset%2520will%2520be%2520publicly%2520released%252C%2520to%2520support%2520new%2520applications%2520in%2520interior%250Adesign%252C%2520virtual%2520staging%252C%2520and%25203D-consistent%2520stylization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReStyle3D%3A%20Scene-Level%20Appearance%20Transfer%20with%20Semantic%20Correspondences&entry.906535625=Liyuan%20Zhu%20and%20Shengqu%20Cai%20and%20Shengyu%20Huang%20and%20Gordon%20Wetzstein%20and%20Naji%20Khosravan%20and%20Iro%20Armeni&entry.1292438233=%20%20We%20introduce%20ReStyle3D%2C%20a%20novel%20framework%20for%20scene-level%20appearance%20transfer%0Afrom%20a%20single%20style%20image%20to%20a%20real-world%20scene%20represented%20by%20multiple%20views.%0AThe%20method%20combines%20explicit%20semantic%20correspondences%20with%20multi-view%0Aconsistency%20to%20achieve%20precise%20and%20coherent%20stylization.%20Unlike%20conventional%0Astylization%20methods%20that%20apply%20a%20reference%20style%20globally%2C%20ReStyle3D%20uses%0Aopen-vocabulary%20segmentation%20to%20establish%20dense%2C%20instance-level%20correspondences%0Abetween%20the%20style%20and%20real-world%20images.%20This%20ensures%20that%20each%20object%20is%0Astylized%20with%20semantically%20matched%20textures.%20It%20first%20transfers%20the%20style%20to%20a%0Asingle%20view%20using%20a%20training-free%20semantic-attention%20mechanism%20in%20a%20diffusion%0Amodel.%20It%20then%20lifts%20the%20stylization%20to%20additional%20views%20via%20a%20learned%0Awarp-and-refine%20network%20guided%20by%20monocular%20depth%20and%20pixel-wise%0Acorrespondences.%20Experiments%20show%20that%20ReStyle3D%20consistently%20outperforms%20prior%0Amethods%20in%20structure%20preservation%2C%20perceptual%20style%20similarity%2C%20and%20multi-view%0Acoherence.%20User%20studies%20further%20validate%20its%20ability%20to%20produce%0Aphoto-realistic%2C%20semantically%20faithful%20results.%20Our%20code%2C%20pretrained%20models%2C%0Aand%20dataset%20will%20be%20publicly%20released%2C%20to%20support%20new%20applications%20in%20interior%0Adesign%2C%20virtual%20staging%2C%20and%203D-consistent%20stylization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10377v1&entry.124074799=Read"},
{"title": "Image Embedding Sampling Method for Diverse Captioning", "author": "Sania Waheed and Na Min An", "abstract": "  Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, smaller VLMs\nprioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.\n", "link": "http://arxiv.org/abs/2502.10118v1", "date": "2025-02-14", "relevancy": 2.828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning&body=Title%3A%20Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning%0AAuthor%3A%20Sania%20Waheed%20and%20Na%20Min%20An%0AAbstract%3A%20%20%20Image%20Captioning%20for%20state-of-the-art%20VLMs%20has%20significantly%20improved%20over%0Atime%3B%20however%2C%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity%2C%0Amaking%20them%20less%20accessible%20for%20resource-constrained%20applications%20such%20as%0Amobile%20devices%20and%20assistive%20technologies.%20Alternatively%2C%20smaller%20VLMs%0Aprioritize%20high-level%20scene%20descriptions%2C%20overlooking%20finer%20details%20that%0Acontribute%20to%20a%20richer%20understanding%20of%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%0Atraining-free%20framework%20that%20enhances%20caption%20diversity%20and%20informativeness%20by%0Aexplicitly%20attending%20to%20distinct%20image%20regions%20using%20a%20comparably%20small%20VLM%2C%0ABLIP%2C%20as%20the%20backbone.%20Our%20approach%20leverages%20structured%20segmentation%20to%0Aproduce%20hierarchical%20representations%20that%20capture%20both%20global%20and%20localized%0Asemantics.%20Without%20requiring%20additional%20model%20training%2C%20we%20demonstrate%20that%20our%0Amethod%20allows%20smaller%20VLMs%20to%20achieve%20performance%20comparable%20to%20larger%20models%0Ain%20terms%20of%20image-caption%20alignment%2C%20semantic%20integrity%2C%20and%20diversity.%20We%0Aevaluate%20our%20framework%20on%20MSCOCO%2C%20Flickr30k%2C%20and%20Nocaps%20test%20datasets%2C%0Aachieving%20a%20Div-2%20score%20of%200.735%2C%200.750%2C%20and%200.748%20for%20each%20dataset%0Arespectively%2C%20while%20maintaining%20strong%20image-caption%20relevancy%20and%20semantic%0Aintegrity%20with%20the%20human-annotated%20captions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Embedding%2520Sampling%2520Method%2520for%2520Diverse%2520Captioning%26entry.906535625%3DSania%2520Waheed%2520and%2520Na%2520Min%2520An%26entry.1292438233%3D%2520%2520Image%2520Captioning%2520for%2520state-of-the-art%2520VLMs%2520has%2520significantly%2520improved%2520over%250Atime%253B%2520however%252C%2520this%2520comes%2520at%2520the%2520cost%2520of%2520increased%2520computational%2520complexity%252C%250Amaking%2520them%2520less%2520accessible%2520for%2520resource-constrained%2520applications%2520such%2520as%250Amobile%2520devices%2520and%2520assistive%2520technologies.%2520Alternatively%252C%2520smaller%2520VLMs%250Aprioritize%2520high-level%2520scene%2520descriptions%252C%2520overlooking%2520finer%2520details%2520that%250Acontribute%2520to%2520a%2520richer%2520understanding%2520of%2520an%2520image.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Atraining-free%2520framework%2520that%2520enhances%2520caption%2520diversity%2520and%2520informativeness%2520by%250Aexplicitly%2520attending%2520to%2520distinct%2520image%2520regions%2520using%2520a%2520comparably%2520small%2520VLM%252C%250ABLIP%252C%2520as%2520the%2520backbone.%2520Our%2520approach%2520leverages%2520structured%2520segmentation%2520to%250Aproduce%2520hierarchical%2520representations%2520that%2520capture%2520both%2520global%2520and%2520localized%250Asemantics.%2520Without%2520requiring%2520additional%2520model%2520training%252C%2520we%2520demonstrate%2520that%2520our%250Amethod%2520allows%2520smaller%2520VLMs%2520to%2520achieve%2520performance%2520comparable%2520to%2520larger%2520models%250Ain%2520terms%2520of%2520image-caption%2520alignment%252C%2520semantic%2520integrity%252C%2520and%2520diversity.%2520We%250Aevaluate%2520our%2520framework%2520on%2520MSCOCO%252C%2520Flickr30k%252C%2520and%2520Nocaps%2520test%2520datasets%252C%250Aachieving%2520a%2520Div-2%2520score%2520of%25200.735%252C%25200.750%252C%2520and%25200.748%2520for%2520each%2520dataset%250Arespectively%252C%2520while%2520maintaining%2520strong%2520image-caption%2520relevancy%2520and%2520semantic%250Aintegrity%2520with%2520the%2520human-annotated%2520captions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Embedding%20Sampling%20Method%20for%20Diverse%20Captioning&entry.906535625=Sania%20Waheed%20and%20Na%20Min%20An&entry.1292438233=%20%20Image%20Captioning%20for%20state-of-the-art%20VLMs%20has%20significantly%20improved%20over%0Atime%3B%20however%2C%20this%20comes%20at%20the%20cost%20of%20increased%20computational%20complexity%2C%0Amaking%20them%20less%20accessible%20for%20resource-constrained%20applications%20such%20as%0Amobile%20devices%20and%20assistive%20technologies.%20Alternatively%2C%20smaller%20VLMs%0Aprioritize%20high-level%20scene%20descriptions%2C%20overlooking%20finer%20details%20that%0Acontribute%20to%20a%20richer%20understanding%20of%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%0Atraining-free%20framework%20that%20enhances%20caption%20diversity%20and%20informativeness%20by%0Aexplicitly%20attending%20to%20distinct%20image%20regions%20using%20a%20comparably%20small%20VLM%2C%0ABLIP%2C%20as%20the%20backbone.%20Our%20approach%20leverages%20structured%20segmentation%20to%0Aproduce%20hierarchical%20representations%20that%20capture%20both%20global%20and%20localized%0Asemantics.%20Without%20requiring%20additional%20model%20training%2C%20we%20demonstrate%20that%20our%0Amethod%20allows%20smaller%20VLMs%20to%20achieve%20performance%20comparable%20to%20larger%20models%0Ain%20terms%20of%20image-caption%20alignment%2C%20semantic%20integrity%2C%20and%20diversity.%20We%0Aevaluate%20our%20framework%20on%20MSCOCO%2C%20Flickr30k%2C%20and%20Nocaps%20test%20datasets%2C%0Aachieving%20a%20Div-2%20score%20of%200.735%2C%200.750%2C%20and%200.748%20for%20each%20dataset%0Arespectively%2C%20while%20maintaining%20strong%20image-caption%20relevancy%20and%20semantic%0Aintegrity%20with%20the%20human-annotated%20captions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10118v1&entry.124074799=Read"},
{"title": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds", "author": "Yui Lo and Yuqian Chen and Dongnan Liu and Jon Haitz Legarreta and Leo Zekelman and Fan Zhang and Jarrett Rushmore and Yogesh Rathi and Nikos Makris and Alexandra J. Golby and Weidong Cai and Lauren J. O'Donnell", "abstract": "  Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.\n", "link": "http://arxiv.org/abs/2410.22099v4", "date": "2025-02-14", "relevancy": 2.8264, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6017}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5485}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds&body=Title%3A%20TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds%0AAuthor%3A%20Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Jon%20Haitz%20Legarreta%20and%20Leo%20Zekelman%20and%20Fan%20Zhang%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell%0AAbstract%3A%20%20%20Brain%20imaging%20studies%20have%20demonstrated%20that%20diffusion%20MRI%20tractography%0Ageometric%20shape%20descriptors%20can%20inform%20the%20study%20of%20the%20brain%27s%20white%20matter%0Apathways%20and%20their%20relationship%20to%20brain%20function.%20In%20this%20work%2C%20we%20investigate%0Athe%20possibility%20of%20utilizing%20a%20deep%20learning%20model%20to%20compute%20shape%20measures%20of%0Athe%20brain%27s%20white%20matter%20connections.%20We%20introduce%20a%20novel%20framework%2C%0ATractShapeNet%2C%20that%20leverages%20a%20point%20cloud%20representation%20of%20tractography%20to%0Acompute%20five%20shape%20measures%3A%20length%2C%20span%2C%20volume%2C%20total%20surface%20area%2C%20and%0Airregularity.%20We%20assess%20the%20performance%20of%20the%20method%20on%20a%20large%20dataset%0Aincluding%201065%20healthy%20young%20adults.%20Experiments%20for%20shape%20measure%20computation%0Ademonstrate%20that%20our%20proposed%20TractShapeNet%20outperforms%20other%20point%20cloud-based%0Aneural%20network%20models%20in%20both%20the%20Pearson%20correlation%20coefficient%20and%0Anormalized%20error%20metrics.%20We%20compare%20the%20inference%20runtime%20results%20with%20the%0Aconventional%20shape%20computation%20tool%20DSI-Studio.%20Our%20results%20demonstrate%20that%20a%0Adeep%20learning%20approach%20enables%20faster%20and%20more%20efficient%20shape%20measure%0Acomputation.%20We%20also%20conduct%20experiments%20on%20two%20downstream%20language%20cognition%0Aprediction%20tasks%2C%20showing%20that%20shape%20measures%20from%20TractShapeNet%20perform%0Asimilarly%20to%20those%20computed%20by%20DSI-Studio.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SlicerDMRI/TractShapeNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22099v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTractShapeNet%253A%2520Efficient%2520Multi-Shape%2520Learning%2520with%25203D%2520Tractography%2520Point%250A%2520%2520Clouds%26entry.906535625%3DYui%2520Lo%2520and%2520Yuqian%2520Chen%2520and%2520Dongnan%2520Liu%2520and%2520Jon%2520Haitz%2520Legarreta%2520and%2520Leo%2520Zekelman%2520and%2520Fan%2520Zhang%2520and%2520Jarrett%2520Rushmore%2520and%2520Yogesh%2520Rathi%2520and%2520Nikos%2520Makris%2520and%2520Alexandra%2520J.%2520Golby%2520and%2520Weidong%2520Cai%2520and%2520Lauren%2520J.%2520O%2527Donnell%26entry.1292438233%3D%2520%2520Brain%2520imaging%2520studies%2520have%2520demonstrated%2520that%2520diffusion%2520MRI%2520tractography%250Ageometric%2520shape%2520descriptors%2520can%2520inform%2520the%2520study%2520of%2520the%2520brain%2527s%2520white%2520matter%250Apathways%2520and%2520their%2520relationship%2520to%2520brain%2520function.%2520In%2520this%2520work%252C%2520we%2520investigate%250Athe%2520possibility%2520of%2520utilizing%2520a%2520deep%2520learning%2520model%2520to%2520compute%2520shape%2520measures%2520of%250Athe%2520brain%2527s%2520white%2520matter%2520connections.%2520We%2520introduce%2520a%2520novel%2520framework%252C%250ATractShapeNet%252C%2520that%2520leverages%2520a%2520point%2520cloud%2520representation%2520of%2520tractography%2520to%250Acompute%2520five%2520shape%2520measures%253A%2520length%252C%2520span%252C%2520volume%252C%2520total%2520surface%2520area%252C%2520and%250Airregularity.%2520We%2520assess%2520the%2520performance%2520of%2520the%2520method%2520on%2520a%2520large%2520dataset%250Aincluding%25201065%2520healthy%2520young%2520adults.%2520Experiments%2520for%2520shape%2520measure%2520computation%250Ademonstrate%2520that%2520our%2520proposed%2520TractShapeNet%2520outperforms%2520other%2520point%2520cloud-based%250Aneural%2520network%2520models%2520in%2520both%2520the%2520Pearson%2520correlation%2520coefficient%2520and%250Anormalized%2520error%2520metrics.%2520We%2520compare%2520the%2520inference%2520runtime%2520results%2520with%2520the%250Aconventional%2520shape%2520computation%2520tool%2520DSI-Studio.%2520Our%2520results%2520demonstrate%2520that%2520a%250Adeep%2520learning%2520approach%2520enables%2520faster%2520and%2520more%2520efficient%2520shape%2520measure%250Acomputation.%2520We%2520also%2520conduct%2520experiments%2520on%2520two%2520downstream%2520language%2520cognition%250Aprediction%2520tasks%252C%2520showing%2520that%2520shape%2520measures%2520from%2520TractShapeNet%2520perform%250Asimilarly%2520to%2520those%2520computed%2520by%2520DSI-Studio.%2520Our%2520code%2520will%2520be%2520available%2520at%253A%250Ahttps%253A//github.com/SlicerDMRI/TractShapeNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22099v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TractShapeNet%3A%20Efficient%20Multi-Shape%20Learning%20with%203D%20Tractography%20Point%0A%20%20Clouds&entry.906535625=Yui%20Lo%20and%20Yuqian%20Chen%20and%20Dongnan%20Liu%20and%20Jon%20Haitz%20Legarreta%20and%20Leo%20Zekelman%20and%20Fan%20Zhang%20and%20Jarrett%20Rushmore%20and%20Yogesh%20Rathi%20and%20Nikos%20Makris%20and%20Alexandra%20J.%20Golby%20and%20Weidong%20Cai%20and%20Lauren%20J.%20O%27Donnell&entry.1292438233=%20%20Brain%20imaging%20studies%20have%20demonstrated%20that%20diffusion%20MRI%20tractography%0Ageometric%20shape%20descriptors%20can%20inform%20the%20study%20of%20the%20brain%27s%20white%20matter%0Apathways%20and%20their%20relationship%20to%20brain%20function.%20In%20this%20work%2C%20we%20investigate%0Athe%20possibility%20of%20utilizing%20a%20deep%20learning%20model%20to%20compute%20shape%20measures%20of%0Athe%20brain%27s%20white%20matter%20connections.%20We%20introduce%20a%20novel%20framework%2C%0ATractShapeNet%2C%20that%20leverages%20a%20point%20cloud%20representation%20of%20tractography%20to%0Acompute%20five%20shape%20measures%3A%20length%2C%20span%2C%20volume%2C%20total%20surface%20area%2C%20and%0Airregularity.%20We%20assess%20the%20performance%20of%20the%20method%20on%20a%20large%20dataset%0Aincluding%201065%20healthy%20young%20adults.%20Experiments%20for%20shape%20measure%20computation%0Ademonstrate%20that%20our%20proposed%20TractShapeNet%20outperforms%20other%20point%20cloud-based%0Aneural%20network%20models%20in%20both%20the%20Pearson%20correlation%20coefficient%20and%0Anormalized%20error%20metrics.%20We%20compare%20the%20inference%20runtime%20results%20with%20the%0Aconventional%20shape%20computation%20tool%20DSI-Studio.%20Our%20results%20demonstrate%20that%20a%0Adeep%20learning%20approach%20enables%20faster%20and%20more%20efficient%20shape%20measure%0Acomputation.%20We%20also%20conduct%20experiments%20on%20two%20downstream%20language%20cognition%0Aprediction%20tasks%2C%20showing%20that%20shape%20measures%20from%20TractShapeNet%20perform%0Asimilarly%20to%20those%20computed%20by%20DSI-Studio.%20Our%20code%20will%20be%20available%20at%3A%0Ahttps%3A//github.com/SlicerDMRI/TractShapeNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22099v4&entry.124074799=Read"},
{"title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control", "author": "Teng Li and Guangcong Zheng and Rui Jiang and  Shuigenzhan and Tao Wu and Yehao Lu and Yining Lin and Xi Li", "abstract": "  Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V.\n", "link": "http://arxiv.org/abs/2502.10059v1", "date": "2025-02-14", "relevancy": 2.7342, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7578}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6711}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealCam-I2V%3A%20Real-World%20Image-to-Video%20Generation%20with%20Interactive%0A%20%20Complex%20Camera%20Control&body=Title%3A%20RealCam-I2V%3A%20Real-World%20Image-to-Video%20Generation%20with%20Interactive%0A%20%20Complex%20Camera%20Control%0AAuthor%3A%20Teng%20Li%20and%20Guangcong%20Zheng%20and%20Rui%20Jiang%20and%20%20Shuigenzhan%20and%20Tao%20Wu%20and%20Yehao%20Lu%20and%20Yining%20Lin%20and%20Xi%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20camera-trajectory-guided%20image-to-video%20generation%0Aoffer%20higher%20precision%20and%20better%20support%20for%20complex%20camera%20control%20compared%0Ato%20text-based%20approaches.%20However%2C%20they%20also%20introduce%20significant%20usability%0Achallenges%2C%20as%20users%20often%20struggle%20to%20provide%20precise%20camera%20parameters%20when%0Aworking%20with%20arbitrary%20real-world%20images%20without%20knowledge%20of%20their%20depth%20nor%0Ascene%20scale.%20To%20address%20these%20real-world%20application%20issues%2C%20we%20propose%0ARealCam-I2V%2C%20a%20novel%20diffusion-based%20video%20generation%20framework%20that%20integrates%0Amonocular%20metric%20depth%20estimation%20to%20establish%203D%20scene%20reconstruction%20in%20a%0Apreprocessing%20step.%20During%20training%2C%20the%20reconstructed%203D%20scene%20enables%20scaling%0Acamera%20parameters%20from%20relative%20to%20absolute%20values%2C%20ensuring%20compatibility%20and%0Ascale%20consistency%20across%20diverse%20real-world%20images.%20In%20inference%2C%20RealCam-I2V%0Aoffers%20an%20intuitive%20interface%20where%20users%20can%20precisely%20draw%20camera%0Atrajectories%20by%20dragging%20within%20the%203D%20scene.%20To%20further%20enhance%20precise%20camera%0Acontrol%20and%20scene%20consistency%2C%20we%20propose%20scene-constrained%20noise%20shaping%2C%0Awhich%20shapes%20high-level%20noise%20and%20also%20allows%20the%20framework%20to%20maintain%0Adynamic%2C%20coherent%20video%20generation%20in%20lower%20noise%20stages.%20RealCam-I2V%20achieves%0Asignificant%20improvements%20in%20controllability%20and%20video%20quality%20on%20the%0ARealEstate10K%20and%20out-of-domain%20images.%20We%20further%20enables%20applications%20like%0Acamera-controlled%20looping%20video%20generation%20and%20generative%20frame%20interpolation.%0AWe%20will%20release%20our%20absolute-scale%20annotation%2C%20codes%2C%20and%20all%20checkpoints.%0APlease%20see%20dynamic%20results%20in%20https%3A//zgctroy.github.io/RealCam-I2V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealCam-I2V%253A%2520Real-World%2520Image-to-Video%2520Generation%2520with%2520Interactive%250A%2520%2520Complex%2520Camera%2520Control%26entry.906535625%3DTeng%2520Li%2520and%2520Guangcong%2520Zheng%2520and%2520Rui%2520Jiang%2520and%2520%2520Shuigenzhan%2520and%2520Tao%2520Wu%2520and%2520Yehao%2520Lu%2520and%2520Yining%2520Lin%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520camera-trajectory-guided%2520image-to-video%2520generation%250Aoffer%2520higher%2520precision%2520and%2520better%2520support%2520for%2520complex%2520camera%2520control%2520compared%250Ato%2520text-based%2520approaches.%2520However%252C%2520they%2520also%2520introduce%2520significant%2520usability%250Achallenges%252C%2520as%2520users%2520often%2520struggle%2520to%2520provide%2520precise%2520camera%2520parameters%2520when%250Aworking%2520with%2520arbitrary%2520real-world%2520images%2520without%2520knowledge%2520of%2520their%2520depth%2520nor%250Ascene%2520scale.%2520To%2520address%2520these%2520real-world%2520application%2520issues%252C%2520we%2520propose%250ARealCam-I2V%252C%2520a%2520novel%2520diffusion-based%2520video%2520generation%2520framework%2520that%2520integrates%250Amonocular%2520metric%2520depth%2520estimation%2520to%2520establish%25203D%2520scene%2520reconstruction%2520in%2520a%250Apreprocessing%2520step.%2520During%2520training%252C%2520the%2520reconstructed%25203D%2520scene%2520enables%2520scaling%250Acamera%2520parameters%2520from%2520relative%2520to%2520absolute%2520values%252C%2520ensuring%2520compatibility%2520and%250Ascale%2520consistency%2520across%2520diverse%2520real-world%2520images.%2520In%2520inference%252C%2520RealCam-I2V%250Aoffers%2520an%2520intuitive%2520interface%2520where%2520users%2520can%2520precisely%2520draw%2520camera%250Atrajectories%2520by%2520dragging%2520within%2520the%25203D%2520scene.%2520To%2520further%2520enhance%2520precise%2520camera%250Acontrol%2520and%2520scene%2520consistency%252C%2520we%2520propose%2520scene-constrained%2520noise%2520shaping%252C%250Awhich%2520shapes%2520high-level%2520noise%2520and%2520also%2520allows%2520the%2520framework%2520to%2520maintain%250Adynamic%252C%2520coherent%2520video%2520generation%2520in%2520lower%2520noise%2520stages.%2520RealCam-I2V%2520achieves%250Asignificant%2520improvements%2520in%2520controllability%2520and%2520video%2520quality%2520on%2520the%250ARealEstate10K%2520and%2520out-of-domain%2520images.%2520We%2520further%2520enables%2520applications%2520like%250Acamera-controlled%2520looping%2520video%2520generation%2520and%2520generative%2520frame%2520interpolation.%250AWe%2520will%2520release%2520our%2520absolute-scale%2520annotation%252C%2520codes%252C%2520and%2520all%2520checkpoints.%250APlease%2520see%2520dynamic%2520results%2520in%2520https%253A//zgctroy.github.io/RealCam-I2V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealCam-I2V%3A%20Real-World%20Image-to-Video%20Generation%20with%20Interactive%0A%20%20Complex%20Camera%20Control&entry.906535625=Teng%20Li%20and%20Guangcong%20Zheng%20and%20Rui%20Jiang%20and%20%20Shuigenzhan%20and%20Tao%20Wu%20and%20Yehao%20Lu%20and%20Yining%20Lin%20and%20Xi%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20camera-trajectory-guided%20image-to-video%20generation%0Aoffer%20higher%20precision%20and%20better%20support%20for%20complex%20camera%20control%20compared%0Ato%20text-based%20approaches.%20However%2C%20they%20also%20introduce%20significant%20usability%0Achallenges%2C%20as%20users%20often%20struggle%20to%20provide%20precise%20camera%20parameters%20when%0Aworking%20with%20arbitrary%20real-world%20images%20without%20knowledge%20of%20their%20depth%20nor%0Ascene%20scale.%20To%20address%20these%20real-world%20application%20issues%2C%20we%20propose%0ARealCam-I2V%2C%20a%20novel%20diffusion-based%20video%20generation%20framework%20that%20integrates%0Amonocular%20metric%20depth%20estimation%20to%20establish%203D%20scene%20reconstruction%20in%20a%0Apreprocessing%20step.%20During%20training%2C%20the%20reconstructed%203D%20scene%20enables%20scaling%0Acamera%20parameters%20from%20relative%20to%20absolute%20values%2C%20ensuring%20compatibility%20and%0Ascale%20consistency%20across%20diverse%20real-world%20images.%20In%20inference%2C%20RealCam-I2V%0Aoffers%20an%20intuitive%20interface%20where%20users%20can%20precisely%20draw%20camera%0Atrajectories%20by%20dragging%20within%20the%203D%20scene.%20To%20further%20enhance%20precise%20camera%0Acontrol%20and%20scene%20consistency%2C%20we%20propose%20scene-constrained%20noise%20shaping%2C%0Awhich%20shapes%20high-level%20noise%20and%20also%20allows%20the%20framework%20to%20maintain%0Adynamic%2C%20coherent%20video%20generation%20in%20lower%20noise%20stages.%20RealCam-I2V%20achieves%0Asignificant%20improvements%20in%20controllability%20and%20video%20quality%20on%20the%0ARealEstate10K%20and%20out-of-domain%20images.%20We%20further%20enables%20applications%20like%0Acamera-controlled%20looping%20video%20generation%20and%20generative%20frame%20interpolation.%0AWe%20will%20release%20our%20absolute-scale%20annotation%2C%20codes%2C%20and%20all%20checkpoints.%0APlease%20see%20dynamic%20results%20in%20https%3A//zgctroy.github.io/RealCam-I2V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10059v1&entry.124074799=Read"},
{"title": "Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation\n  Learning with Visual Observations", "author": "Minung Kim and Kawon Lee and Jungmo Kim and Sungho Choi and Seungyul Han", "abstract": "  Imitation learning (IL) enables agents to mimic expert behavior without\nreward signals but faces challenges in cross-domain scenarios with\nhigh-dimensional, noisy, and incomplete visual observations. To address this,\nwe propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning\n(DIFF-IL), a novel IL method that extracts domain-invariant features from\nindividual frames and adapts them into sequences to isolate and replicate\nexpert behaviors. We also introduce a frame-wise time labeling technique to\nsegment expert behaviors by timesteps and assign rewards aligned with temporal\ncontexts, enhancing task performance. Experiments across diverse visual\nenvironments demonstrate the effectiveness of DIFF-IL in addressing complex\nvisual tasks.\n", "link": "http://arxiv.org/abs/2502.02867v2", "date": "2025-02-14", "relevancy": 2.6596, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5276}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Invariant%20Per-Frame%20Feature%20Extraction%20for%20Cross-Domain%20Imitation%0A%20%20Learning%20with%20Visual%20Observations&body=Title%3A%20Domain-Invariant%20Per-Frame%20Feature%20Extraction%20for%20Cross-Domain%20Imitation%0A%20%20Learning%20with%20Visual%20Observations%0AAuthor%3A%20Minung%20Kim%20and%20Kawon%20Lee%20and%20Jungmo%20Kim%20and%20Sungho%20Choi%20and%20Seungyul%20Han%0AAbstract%3A%20%20%20Imitation%20learning%20%28IL%29%20enables%20agents%20to%20mimic%20expert%20behavior%20without%0Areward%20signals%20but%20faces%20challenges%20in%20cross-domain%20scenarios%20with%0Ahigh-dimensional%2C%20noisy%2C%20and%20incomplete%20visual%20observations.%20To%20address%20this%2C%0Awe%20propose%20Domain-Invariant%20Per-Frame%20Feature%20Extraction%20for%20Imitation%20Learning%0A%28DIFF-IL%29%2C%20a%20novel%20IL%20method%20that%20extracts%20domain-invariant%20features%20from%0Aindividual%20frames%20and%20adapts%20them%20into%20sequences%20to%20isolate%20and%20replicate%0Aexpert%20behaviors.%20We%20also%20introduce%20a%20frame-wise%20time%20labeling%20technique%20to%0Asegment%20expert%20behaviors%20by%20timesteps%20and%20assign%20rewards%20aligned%20with%20temporal%0Acontexts%2C%20enhancing%20task%20performance.%20Experiments%20across%20diverse%20visual%0Aenvironments%20demonstrate%20the%20effectiveness%20of%20DIFF-IL%20in%20addressing%20complex%0Avisual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02867v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Invariant%2520Per-Frame%2520Feature%2520Extraction%2520for%2520Cross-Domain%2520Imitation%250A%2520%2520Learning%2520with%2520Visual%2520Observations%26entry.906535625%3DMinung%2520Kim%2520and%2520Kawon%2520Lee%2520and%2520Jungmo%2520Kim%2520and%2520Sungho%2520Choi%2520and%2520Seungyul%2520Han%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520%2528IL%2529%2520enables%2520agents%2520to%2520mimic%2520expert%2520behavior%2520without%250Areward%2520signals%2520but%2520faces%2520challenges%2520in%2520cross-domain%2520scenarios%2520with%250Ahigh-dimensional%252C%2520noisy%252C%2520and%2520incomplete%2520visual%2520observations.%2520To%2520address%2520this%252C%250Awe%2520propose%2520Domain-Invariant%2520Per-Frame%2520Feature%2520Extraction%2520for%2520Imitation%2520Learning%250A%2528DIFF-IL%2529%252C%2520a%2520novel%2520IL%2520method%2520that%2520extracts%2520domain-invariant%2520features%2520from%250Aindividual%2520frames%2520and%2520adapts%2520them%2520into%2520sequences%2520to%2520isolate%2520and%2520replicate%250Aexpert%2520behaviors.%2520We%2520also%2520introduce%2520a%2520frame-wise%2520time%2520labeling%2520technique%2520to%250Asegment%2520expert%2520behaviors%2520by%2520timesteps%2520and%2520assign%2520rewards%2520aligned%2520with%2520temporal%250Acontexts%252C%2520enhancing%2520task%2520performance.%2520Experiments%2520across%2520diverse%2520visual%250Aenvironments%2520demonstrate%2520the%2520effectiveness%2520of%2520DIFF-IL%2520in%2520addressing%2520complex%250Avisual%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02867v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Invariant%20Per-Frame%20Feature%20Extraction%20for%20Cross-Domain%20Imitation%0A%20%20Learning%20with%20Visual%20Observations&entry.906535625=Minung%20Kim%20and%20Kawon%20Lee%20and%20Jungmo%20Kim%20and%20Sungho%20Choi%20and%20Seungyul%20Han&entry.1292438233=%20%20Imitation%20learning%20%28IL%29%20enables%20agents%20to%20mimic%20expert%20behavior%20without%0Areward%20signals%20but%20faces%20challenges%20in%20cross-domain%20scenarios%20with%0Ahigh-dimensional%2C%20noisy%2C%20and%20incomplete%20visual%20observations.%20To%20address%20this%2C%0Awe%20propose%20Domain-Invariant%20Per-Frame%20Feature%20Extraction%20for%20Imitation%20Learning%0A%28DIFF-IL%29%2C%20a%20novel%20IL%20method%20that%20extracts%20domain-invariant%20features%20from%0Aindividual%20frames%20and%20adapts%20them%20into%20sequences%20to%20isolate%20and%20replicate%0Aexpert%20behaviors.%20We%20also%20introduce%20a%20frame-wise%20time%20labeling%20technique%20to%0Asegment%20expert%20behaviors%20by%20timesteps%20and%20assign%20rewards%20aligned%20with%20temporal%0Acontexts%2C%20enhancing%20task%20performance.%20Experiments%20across%20diverse%20visual%0Aenvironments%20demonstrate%20the%20effectiveness%20of%20DIFF-IL%20in%20addressing%20complex%0Avisual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02867v2&entry.124074799=Read"},
{"title": "Exploring Representations and Interventions in Time Series Foundation\n  Models", "author": "Micha\u0142 Wili\u0144ski and Mononito Goswami and Nina \u017bukowska and Willa Potosnak and Artur Dubrawski", "abstract": "  Time series foundation models (TSFMs) promise to be powerful tools for a wide\nrange of applications. However, their internal representations and learned\nconcepts are still not well understood. In this study, we investigate the\nstructure and redundancy of representations across various TSFMs, examining the\nself-similarity of model layers within and across different model sizes. This\nanalysis reveals block-like redundancy in the representations, which can be\nutilized for informed pruning to improve inference speed and efficiency.\nAdditionally, we explore the concepts learned by these models - such as\nperiodicity and trends - and how these can be manipulated through latent space\nsteering to influence model behavior. Our experiments show that steering\ninterventions can introduce new features, e.g., adding periodicity or trends to\nsignals that initially lacked them. These findings underscore the value of\nrepresentational analysis for optimizing models and demonstrate how conceptual\nsteering offers new possibilities for more controlled and efficient time series\nanalysis with TSFMs.\n", "link": "http://arxiv.org/abs/2409.12915v3", "date": "2025-02-14", "relevancy": 2.6061, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5224}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Representations%20and%20Interventions%20in%20Time%20Series%20Foundation%0A%20%20Models&body=Title%3A%20Exploring%20Representations%20and%20Interventions%20in%20Time%20Series%20Foundation%0A%20%20Models%0AAuthor%3A%20Micha%C5%82%20Wili%C5%84ski%20and%20Mononito%20Goswami%20and%20Nina%20%C5%BBukowska%20and%20Willa%20Potosnak%20and%20Artur%20Dubrawski%0AAbstract%3A%20%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20promise%20to%20be%20powerful%20tools%20for%20a%20wide%0Arange%20of%20applications.%20However%2C%20their%20internal%20representations%20and%20learned%0Aconcepts%20are%20still%20not%20well%20understood.%20In%20this%20study%2C%20we%20investigate%20the%0Astructure%20and%20redundancy%20of%20representations%20across%20various%20TSFMs%2C%20examining%20the%0Aself-similarity%20of%20model%20layers%20within%20and%20across%20different%20model%20sizes.%20This%0Aanalysis%20reveals%20block-like%20redundancy%20in%20the%20representations%2C%20which%20can%20be%0Autilized%20for%20informed%20pruning%20to%20improve%20inference%20speed%20and%20efficiency.%0AAdditionally%2C%20we%20explore%20the%20concepts%20learned%20by%20these%20models%20-%20such%20as%0Aperiodicity%20and%20trends%20-%20and%20how%20these%20can%20be%20manipulated%20through%20latent%20space%0Asteering%20to%20influence%20model%20behavior.%20Our%20experiments%20show%20that%20steering%0Ainterventions%20can%20introduce%20new%20features%2C%20e.g.%2C%20adding%20periodicity%20or%20trends%20to%0Asignals%20that%20initially%20lacked%20them.%20These%20findings%20underscore%20the%20value%20of%0Arepresentational%20analysis%20for%20optimizing%20models%20and%20demonstrate%20how%20conceptual%0Asteering%20offers%20new%20possibilities%20for%20more%20controlled%20and%20efficient%20time%20series%0Aanalysis%20with%20TSFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12915v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Representations%2520and%2520Interventions%2520in%2520Time%2520Series%2520Foundation%250A%2520%2520Models%26entry.906535625%3DMicha%25C5%2582%2520Wili%25C5%2584ski%2520and%2520Mononito%2520Goswami%2520and%2520Nina%2520%25C5%25BBukowska%2520and%2520Willa%2520Potosnak%2520and%2520Artur%2520Dubrawski%26entry.1292438233%3D%2520%2520Time%2520series%2520foundation%2520models%2520%2528TSFMs%2529%2520promise%2520to%2520be%2520powerful%2520tools%2520for%2520a%2520wide%250Arange%2520of%2520applications.%2520However%252C%2520their%2520internal%2520representations%2520and%2520learned%250Aconcepts%2520are%2520still%2520not%2520well%2520understood.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%250Astructure%2520and%2520redundancy%2520of%2520representations%2520across%2520various%2520TSFMs%252C%2520examining%2520the%250Aself-similarity%2520of%2520model%2520layers%2520within%2520and%2520across%2520different%2520model%2520sizes.%2520This%250Aanalysis%2520reveals%2520block-like%2520redundancy%2520in%2520the%2520representations%252C%2520which%2520can%2520be%250Autilized%2520for%2520informed%2520pruning%2520to%2520improve%2520inference%2520speed%2520and%2520efficiency.%250AAdditionally%252C%2520we%2520explore%2520the%2520concepts%2520learned%2520by%2520these%2520models%2520-%2520such%2520as%250Aperiodicity%2520and%2520trends%2520-%2520and%2520how%2520these%2520can%2520be%2520manipulated%2520through%2520latent%2520space%250Asteering%2520to%2520influence%2520model%2520behavior.%2520Our%2520experiments%2520show%2520that%2520steering%250Ainterventions%2520can%2520introduce%2520new%2520features%252C%2520e.g.%252C%2520adding%2520periodicity%2520or%2520trends%2520to%250Asignals%2520that%2520initially%2520lacked%2520them.%2520These%2520findings%2520underscore%2520the%2520value%2520of%250Arepresentational%2520analysis%2520for%2520optimizing%2520models%2520and%2520demonstrate%2520how%2520conceptual%250Asteering%2520offers%2520new%2520possibilities%2520for%2520more%2520controlled%2520and%2520efficient%2520time%2520series%250Aanalysis%2520with%2520TSFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12915v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Representations%20and%20Interventions%20in%20Time%20Series%20Foundation%0A%20%20Models&entry.906535625=Micha%C5%82%20Wili%C5%84ski%20and%20Mononito%20Goswami%20and%20Nina%20%C5%BBukowska%20and%20Willa%20Potosnak%20and%20Artur%20Dubrawski&entry.1292438233=%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20promise%20to%20be%20powerful%20tools%20for%20a%20wide%0Arange%20of%20applications.%20However%2C%20their%20internal%20representations%20and%20learned%0Aconcepts%20are%20still%20not%20well%20understood.%20In%20this%20study%2C%20we%20investigate%20the%0Astructure%20and%20redundancy%20of%20representations%20across%20various%20TSFMs%2C%20examining%20the%0Aself-similarity%20of%20model%20layers%20within%20and%20across%20different%20model%20sizes.%20This%0Aanalysis%20reveals%20block-like%20redundancy%20in%20the%20representations%2C%20which%20can%20be%0Autilized%20for%20informed%20pruning%20to%20improve%20inference%20speed%20and%20efficiency.%0AAdditionally%2C%20we%20explore%20the%20concepts%20learned%20by%20these%20models%20-%20such%20as%0Aperiodicity%20and%20trends%20-%20and%20how%20these%20can%20be%20manipulated%20through%20latent%20space%0Asteering%20to%20influence%20model%20behavior.%20Our%20experiments%20show%20that%20steering%0Ainterventions%20can%20introduce%20new%20features%2C%20e.g.%2C%20adding%20periodicity%20or%20trends%20to%0Asignals%20that%20initially%20lacked%20them.%20These%20findings%20underscore%20the%20value%20of%0Arepresentational%20analysis%20for%20optimizing%20models%20and%20demonstrate%20how%20conceptual%0Asteering%20offers%20new%20possibilities%20for%20more%20controlled%20and%20efficient%20time%20series%0Aanalysis%20with%20TSFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12915v3&entry.124074799=Read"},
{"title": "Self-Training: A Survey", "author": "Massih-Reza Amini and Vasilii Feofanov and Loic Pauletto and Lies Hadjadj and Emilie Devijver and Yury Maximov", "abstract": "  Semi-supervised algorithms aim to learn prediction functions from a small set\nof labeled observations and a large set of unlabeled observations. Because this\nframework is relevant in many applications, they have received a lot of\ninterest in both academia and industry. Among the existing techniques,\nself-training methods have undoubtedly attracted greater attention in recent\nyears. These models are designed to find the decision boundary on low density\nregions without making additional assumptions about the data distribution, and\nuse the unsigned output score of a learned classifier, or its margin, as an\nindicator of confidence. The working principle of self-training algorithms is\nto learn a classifier iteratively by assigning pseudo-labels to the set of\nunlabeled training samples with a margin greater than a certain threshold. The\npseudo-labeled examples are then used to enrich the labeled training data and\nto train a new classifier in conjunction with the labeled training set. In this\npaper, we present self-training methods for binary and multi-class\nclassification; as well as their variants and two related approaches, namely\nconsistency-based approaches and transductive learning. We examine the impact\nof significant self-training features on various methods, using different\ngeneral and image classification benchmarks, and we discuss our ideas for\nfuture research in self-training. To the best of our knowledge, this is the\nfirst thorough and complete survey on this subject.\n", "link": "http://arxiv.org/abs/2202.12040v6", "date": "2025-02-14", "relevancy": 2.599, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5089}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Training%3A%20A%20Survey&body=Title%3A%20Self-Training%3A%20A%20Survey%0AAuthor%3A%20Massih-Reza%20Amini%20and%20Vasilii%20Feofanov%20and%20Loic%20Pauletto%20and%20Lies%20Hadjadj%20and%20Emilie%20Devijver%20and%20Yury%20Maximov%0AAbstract%3A%20%20%20Semi-supervised%20algorithms%20aim%20to%20learn%20prediction%20functions%20from%20a%20small%20set%0Aof%20labeled%20observations%20and%20a%20large%20set%20of%20unlabeled%20observations.%20Because%20this%0Aframework%20is%20relevant%20in%20many%20applications%2C%20they%20have%20received%20a%20lot%20of%0Ainterest%20in%20both%20academia%20and%20industry.%20Among%20the%20existing%20techniques%2C%0Aself-training%20methods%20have%20undoubtedly%20attracted%20greater%20attention%20in%20recent%0Ayears.%20These%20models%20are%20designed%20to%20find%20the%20decision%20boundary%20on%20low%20density%0Aregions%20without%20making%20additional%20assumptions%20about%20the%20data%20distribution%2C%20and%0Ause%20the%20unsigned%20output%20score%20of%20a%20learned%20classifier%2C%20or%20its%20margin%2C%20as%20an%0Aindicator%20of%20confidence.%20The%20working%20principle%20of%20self-training%20algorithms%20is%0Ato%20learn%20a%20classifier%20iteratively%20by%20assigning%20pseudo-labels%20to%20the%20set%20of%0Aunlabeled%20training%20samples%20with%20a%20margin%20greater%20than%20a%20certain%20threshold.%20The%0Apseudo-labeled%20examples%20are%20then%20used%20to%20enrich%20the%20labeled%20training%20data%20and%0Ato%20train%20a%20new%20classifier%20in%20conjunction%20with%20the%20labeled%20training%20set.%20In%20this%0Apaper%2C%20we%20present%20self-training%20methods%20for%20binary%20and%20multi-class%0Aclassification%3B%20as%20well%20as%20their%20variants%20and%20two%20related%20approaches%2C%20namely%0Aconsistency-based%20approaches%20and%20transductive%20learning.%20We%20examine%20the%20impact%0Aof%20significant%20self-training%20features%20on%20various%20methods%2C%20using%20different%0Ageneral%20and%20image%20classification%20benchmarks%2C%20and%20we%20discuss%20our%20ideas%20for%0Afuture%20research%20in%20self-training.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20thorough%20and%20complete%20survey%20on%20this%20subject.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.12040v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Training%253A%2520A%2520Survey%26entry.906535625%3DMassih-Reza%2520Amini%2520and%2520Vasilii%2520Feofanov%2520and%2520Loic%2520Pauletto%2520and%2520Lies%2520Hadjadj%2520and%2520Emilie%2520Devijver%2520and%2520Yury%2520Maximov%26entry.1292438233%3D%2520%2520Semi-supervised%2520algorithms%2520aim%2520to%2520learn%2520prediction%2520functions%2520from%2520a%2520small%2520set%250Aof%2520labeled%2520observations%2520and%2520a%2520large%2520set%2520of%2520unlabeled%2520observations.%2520Because%2520this%250Aframework%2520is%2520relevant%2520in%2520many%2520applications%252C%2520they%2520have%2520received%2520a%2520lot%2520of%250Ainterest%2520in%2520both%2520academia%2520and%2520industry.%2520Among%2520the%2520existing%2520techniques%252C%250Aself-training%2520methods%2520have%2520undoubtedly%2520attracted%2520greater%2520attention%2520in%2520recent%250Ayears.%2520These%2520models%2520are%2520designed%2520to%2520find%2520the%2520decision%2520boundary%2520on%2520low%2520density%250Aregions%2520without%2520making%2520additional%2520assumptions%2520about%2520the%2520data%2520distribution%252C%2520and%250Ause%2520the%2520unsigned%2520output%2520score%2520of%2520a%2520learned%2520classifier%252C%2520or%2520its%2520margin%252C%2520as%2520an%250Aindicator%2520of%2520confidence.%2520The%2520working%2520principle%2520of%2520self-training%2520algorithms%2520is%250Ato%2520learn%2520a%2520classifier%2520iteratively%2520by%2520assigning%2520pseudo-labels%2520to%2520the%2520set%2520of%250Aunlabeled%2520training%2520samples%2520with%2520a%2520margin%2520greater%2520than%2520a%2520certain%2520threshold.%2520The%250Apseudo-labeled%2520examples%2520are%2520then%2520used%2520to%2520enrich%2520the%2520labeled%2520training%2520data%2520and%250Ato%2520train%2520a%2520new%2520classifier%2520in%2520conjunction%2520with%2520the%2520labeled%2520training%2520set.%2520In%2520this%250Apaper%252C%2520we%2520present%2520self-training%2520methods%2520for%2520binary%2520and%2520multi-class%250Aclassification%253B%2520as%2520well%2520as%2520their%2520variants%2520and%2520two%2520related%2520approaches%252C%2520namely%250Aconsistency-based%2520approaches%2520and%2520transductive%2520learning.%2520We%2520examine%2520the%2520impact%250Aof%2520significant%2520self-training%2520features%2520on%2520various%2520methods%252C%2520using%2520different%250Ageneral%2520and%2520image%2520classification%2520benchmarks%252C%2520and%2520we%2520discuss%2520our%2520ideas%2520for%250Afuture%2520research%2520in%2520self-training.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520thorough%2520and%2520complete%2520survey%2520on%2520this%2520subject.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.12040v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Training%3A%20A%20Survey&entry.906535625=Massih-Reza%20Amini%20and%20Vasilii%20Feofanov%20and%20Loic%20Pauletto%20and%20Lies%20Hadjadj%20and%20Emilie%20Devijver%20and%20Yury%20Maximov&entry.1292438233=%20%20Semi-supervised%20algorithms%20aim%20to%20learn%20prediction%20functions%20from%20a%20small%20set%0Aof%20labeled%20observations%20and%20a%20large%20set%20of%20unlabeled%20observations.%20Because%20this%0Aframework%20is%20relevant%20in%20many%20applications%2C%20they%20have%20received%20a%20lot%20of%0Ainterest%20in%20both%20academia%20and%20industry.%20Among%20the%20existing%20techniques%2C%0Aself-training%20methods%20have%20undoubtedly%20attracted%20greater%20attention%20in%20recent%0Ayears.%20These%20models%20are%20designed%20to%20find%20the%20decision%20boundary%20on%20low%20density%0Aregions%20without%20making%20additional%20assumptions%20about%20the%20data%20distribution%2C%20and%0Ause%20the%20unsigned%20output%20score%20of%20a%20learned%20classifier%2C%20or%20its%20margin%2C%20as%20an%0Aindicator%20of%20confidence.%20The%20working%20principle%20of%20self-training%20algorithms%20is%0Ato%20learn%20a%20classifier%20iteratively%20by%20assigning%20pseudo-labels%20to%20the%20set%20of%0Aunlabeled%20training%20samples%20with%20a%20margin%20greater%20than%20a%20certain%20threshold.%20The%0Apseudo-labeled%20examples%20are%20then%20used%20to%20enrich%20the%20labeled%20training%20data%20and%0Ato%20train%20a%20new%20classifier%20in%20conjunction%20with%20the%20labeled%20training%20set.%20In%20this%0Apaper%2C%20we%20present%20self-training%20methods%20for%20binary%20and%20multi-class%0Aclassification%3B%20as%20well%20as%20their%20variants%20and%20two%20related%20approaches%2C%20namely%0Aconsistency-based%20approaches%20and%20transductive%20learning.%20We%20examine%20the%20impact%0Aof%20significant%20self-training%20features%20on%20various%20methods%2C%20using%20different%0Ageneral%20and%20image%20classification%20benchmarks%2C%20and%20we%20discuss%20our%20ideas%20for%0Afuture%20research%20in%20self-training.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20thorough%20and%20complete%20survey%20on%20this%20subject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.12040v6&entry.124074799=Read"},
{"title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model", "author": "Guoqing Ma and Haoyang Huang and Kun Yan and Liangyu Chen and Nan Duan and Shengming Yin and Changyi Wan and Ranchen Ming and Xiaoniu Song and Xing Chen and Yu Zhou and Deshan Sun and Deyu Zhou and Jian Zhou and Kaijun Tan and Kang An and Mei Chen and Wei Ji and Qiling Wu and Wen Sun and Xin Han and Yanan Wei and Zheng Ge and Aojie Li and Bin Wang and Bizhu Huang and Bo Wang and Brian Li and Changxing Miao and Chen Xu and Chenfei Wu and Chenguang Yu and Dapeng Shi and Dingyuan Hu and Enle Liu and Gang Yu and Ge Yang and Guanzhe Huang and Gulin Yan and Haiyang Feng and Hao Nie and Haonan Jia and Hanpeng Hu and Hanqi Chen and Haolong Yan and Heng Wang and Hongcheng Guo and Huilin Xiong and Huixin Xiong and Jiahao Gong and Jianchang Wu and Jiaoren Wu and Jie Wu and Jie Yang and Jiashuai Liu and Jiashuo Li and Jingyang Zhang and Junjing Guo and Junzhe Lin and Kaixiang Li and Lei Liu and Lei Xia and Liang Zhao and Liguo Tan and Liwen Huang and Liying Shi and Ming Li and Mingliang Li and Muhua Cheng and Na Wang and Qiaohui Chen and Qinglin He and Qiuyan Liang and Quan Sun and Ran Sun and Rui Wang and Shaoliang Pang and Shiliang Yang and Sitong Liu and Siqi Liu and Shuli Gao and Tiancheng Cao and Tianyu Wang and Weipeng Ming and Wenqing He and Xu Zhao and Xuelin Zhang and Xianfang Zeng and Xiaojia Liu and Xuan Yang and Yaqi Dai and Yanbo Yu and Yang Li and Yineng Deng and Yingming Wang and Yilei Wang and Yuanwei Lu and Yu Chen and Yu Luo and Yuchu Luo and Yuhe Yin and Yuheng Feng and Yuxiang Yang and Zecheng Tang and Zekai Zhang and Zidong Yang and Binxing Jiao and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu and Heung-Yeung Shum and Daxin Jiang", "abstract": "  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n", "link": "http://arxiv.org/abs/2502.10248v1", "date": "2025-02-14", "relevancy": 2.5914, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6651}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6571}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model&body=Title%3A%20Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model%0AAuthor%3A%20Guoqing%20Ma%20and%20Haoyang%20Huang%20and%20Kun%20Yan%20and%20Liangyu%20Chen%20and%20Nan%20Duan%20and%20Shengming%20Yin%20and%20Changyi%20Wan%20and%20Ranchen%20Ming%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Yu%20Zhou%20and%20Deshan%20Sun%20and%20Deyu%20Zhou%20and%20Jian%20Zhou%20and%20Kaijun%20Tan%20and%20Kang%20An%20and%20Mei%20Chen%20and%20Wei%20Ji%20and%20Qiling%20Wu%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Yanan%20Wei%20and%20Zheng%20Ge%20and%20Aojie%20Li%20and%20Bin%20Wang%20and%20Bizhu%20Huang%20and%20Bo%20Wang%20and%20Brian%20Li%20and%20Changxing%20Miao%20and%20Chen%20Xu%20and%20Chenfei%20Wu%20and%20Chenguang%20Yu%20and%20Dapeng%20Shi%20and%20Dingyuan%20Hu%20and%20Enle%20Liu%20and%20Gang%20Yu%20and%20Ge%20Yang%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Haiyang%20Feng%20and%20Hao%20Nie%20and%20Haonan%20Jia%20and%20Hanpeng%20Hu%20and%20Hanqi%20Chen%20and%20Haolong%20Yan%20and%20Heng%20Wang%20and%20Hongcheng%20Guo%20and%20Huilin%20Xiong%20and%20Huixin%20Xiong%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Wu%20and%20Jie%20Yang%20and%20Jiashuai%20Liu%20and%20Jiashuo%20Li%20and%20Jingyang%20Zhang%20and%20Junjing%20Guo%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Liu%20and%20Lei%20Xia%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Ming%20Li%20and%20Mingliang%20Li%20and%20Muhua%20Cheng%20and%20Na%20Wang%20and%20Qiaohui%20Chen%20and%20Qinglin%20He%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Ran%20Sun%20and%20Rui%20Wang%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Sitong%20Liu%20and%20Siqi%20Liu%20and%20Shuli%20Gao%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Weipeng%20Ming%20and%20Wenqing%20He%20and%20Xu%20Zhao%20and%20Xuelin%20Zhang%20and%20Xianfang%20Zeng%20and%20Xiaojia%20Liu%20and%20Xuan%20Yang%20and%20Yaqi%20Dai%20and%20Yanbo%20Yu%20and%20Yang%20Li%20and%20Yineng%20Deng%20and%20Yingming%20Wang%20and%20Yilei%20Wang%20and%20Yuanwei%20Lu%20and%20Yu%20Chen%20and%20Yu%20Luo%20and%20Yuchu%20Luo%20and%20Yuhe%20Yin%20and%20Yuheng%20Feng%20and%20Yuxiang%20Yang%20and%20Zecheng%20Tang%20and%20Zekai%20Zhang%20and%20Zidong%20Yang%20and%20Binxing%20Jiao%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu%20and%20Heung-Yeung%20Shum%20and%20Daxin%20Jiang%0AAbstract%3A%20%20%20We%20present%20Step-Video-T2V%2C%20a%20state-of-the-art%20text-to-video%20pre-trained%20model%0Awith%2030B%20parameters%20and%20the%20ability%20to%20generate%20videos%20up%20to%20204%20frames%20in%0Alength.%20A%20deep%20compression%20Variational%20Autoencoder%2C%20Video-VAE%2C%20is%20designed%20for%0Avideo%20generation%20tasks%2C%20achieving%2016x16%20spatial%20and%208x%20temporal%20compression%0Aratios%2C%20while%20maintaining%20exceptional%20video%20reconstruction%20quality.%20User%0Aprompts%20are%20encoded%20using%20two%20bilingual%20text%20encoders%20to%20handle%20both%20English%0Aand%20Chinese.%20A%20DiT%20with%203D%20full%20attention%20is%20trained%20using%20Flow%20Matching%20and%20is%0Aemployed%20to%20denoise%20input%20noise%20into%20latent%20frames.%20A%20video-based%20DPO%20approach%2C%0AVideo-DPO%2C%20is%20applied%20to%20reduce%20artifacts%20and%20improve%20the%20visual%20quality%20of%20the%0Agenerated%20videos.%20We%20also%20detail%20our%20training%20strategies%20and%20share%20key%0Aobservations%20and%20insights.%20Step-Video-T2V%27s%20performance%20is%20evaluated%20on%20a%20novel%0Avideo%20generation%20benchmark%2C%20Step-Video-T2V-Eval%2C%20demonstrating%20its%0Astate-of-the-art%20text-to-video%20quality%20when%20compared%20with%20both%20open-source%20and%0Acommercial%20engines.%20Additionally%2C%20we%20discuss%20the%20limitations%20of%20current%0Adiffusion-based%20model%20paradigm%20and%20outline%20future%20directions%20for%20video%0Afoundation%20models.%20We%20make%20both%20Step-Video-T2V%20and%20Step-Video-T2V-Eval%0Aavailable%20at%20https%3A//github.com/stepfun-ai/Step-Video-T2V.%20The%20online%20version%0Acan%20be%20accessed%20from%20https%3A//yuewen.cn/videos%20as%20well.%20Our%20goal%20is%20to%0Aaccelerate%20the%20innovation%20of%20video%20foundation%20models%20and%20empower%20video%20content%0Acreators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-Video-T2V%2520Technical%2520Report%253A%2520The%2520Practice%252C%2520Challenges%252C%2520and%2520Future%2520of%250A%2520%2520Video%2520Foundation%2520Model%26entry.906535625%3DGuoqing%2520Ma%2520and%2520Haoyang%2520Huang%2520and%2520Kun%2520Yan%2520and%2520Liangyu%2520Chen%2520and%2520Nan%2520Duan%2520and%2520Shengming%2520Yin%2520and%2520Changyi%2520Wan%2520and%2520Ranchen%2520Ming%2520and%2520Xiaoniu%2520Song%2520and%2520Xing%2520Chen%2520and%2520Yu%2520Zhou%2520and%2520Deshan%2520Sun%2520and%2520Deyu%2520Zhou%2520and%2520Jian%2520Zhou%2520and%2520Kaijun%2520Tan%2520and%2520Kang%2520An%2520and%2520Mei%2520Chen%2520and%2520Wei%2520Ji%2520and%2520Qiling%2520Wu%2520and%2520Wen%2520Sun%2520and%2520Xin%2520Han%2520and%2520Yanan%2520Wei%2520and%2520Zheng%2520Ge%2520and%2520Aojie%2520Li%2520and%2520Bin%2520Wang%2520and%2520Bizhu%2520Huang%2520and%2520Bo%2520Wang%2520and%2520Brian%2520Li%2520and%2520Changxing%2520Miao%2520and%2520Chen%2520Xu%2520and%2520Chenfei%2520Wu%2520and%2520Chenguang%2520Yu%2520and%2520Dapeng%2520Shi%2520and%2520Dingyuan%2520Hu%2520and%2520Enle%2520Liu%2520and%2520Gang%2520Yu%2520and%2520Ge%2520Yang%2520and%2520Guanzhe%2520Huang%2520and%2520Gulin%2520Yan%2520and%2520Haiyang%2520Feng%2520and%2520Hao%2520Nie%2520and%2520Haonan%2520Jia%2520and%2520Hanpeng%2520Hu%2520and%2520Hanqi%2520Chen%2520and%2520Haolong%2520Yan%2520and%2520Heng%2520Wang%2520and%2520Hongcheng%2520Guo%2520and%2520Huilin%2520Xiong%2520and%2520Huixin%2520Xiong%2520and%2520Jiahao%2520Gong%2520and%2520Jianchang%2520Wu%2520and%2520Jiaoren%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Jie%2520Yang%2520and%2520Jiashuai%2520Liu%2520and%2520Jiashuo%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Junjing%2520Guo%2520and%2520Junzhe%2520Lin%2520and%2520Kaixiang%2520Li%2520and%2520Lei%2520Liu%2520and%2520Lei%2520Xia%2520and%2520Liang%2520Zhao%2520and%2520Liguo%2520Tan%2520and%2520Liwen%2520Huang%2520and%2520Liying%2520Shi%2520and%2520Ming%2520Li%2520and%2520Mingliang%2520Li%2520and%2520Muhua%2520Cheng%2520and%2520Na%2520Wang%2520and%2520Qiaohui%2520Chen%2520and%2520Qinglin%2520He%2520and%2520Qiuyan%2520Liang%2520and%2520Quan%2520Sun%2520and%2520Ran%2520Sun%2520and%2520Rui%2520Wang%2520and%2520Shaoliang%2520Pang%2520and%2520Shiliang%2520Yang%2520and%2520Sitong%2520Liu%2520and%2520Siqi%2520Liu%2520and%2520Shuli%2520Gao%2520and%2520Tiancheng%2520Cao%2520and%2520Tianyu%2520Wang%2520and%2520Weipeng%2520Ming%2520and%2520Wenqing%2520He%2520and%2520Xu%2520Zhao%2520and%2520Xuelin%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Xiaojia%2520Liu%2520and%2520Xuan%2520Yang%2520and%2520Yaqi%2520Dai%2520and%2520Yanbo%2520Yu%2520and%2520Yang%2520Li%2520and%2520Yineng%2520Deng%2520and%2520Yingming%2520Wang%2520and%2520Yilei%2520Wang%2520and%2520Yuanwei%2520Lu%2520and%2520Yu%2520Chen%2520and%2520Yu%2520Luo%2520and%2520Yuchu%2520Luo%2520and%2520Yuhe%2520Yin%2520and%2520Yuheng%2520Feng%2520and%2520Yuxiang%2520Yang%2520and%2520Zecheng%2520Tang%2520and%2520Zekai%2520Zhang%2520and%2520Zidong%2520Yang%2520and%2520Binxing%2520Jiao%2520and%2520Jiansheng%2520Chen%2520and%2520Jing%2520Li%2520and%2520Shuchang%2520Zhou%2520and%2520Xiangyu%2520Zhang%2520and%2520Xinhao%2520Zhang%2520and%2520Yibo%2520Zhu%2520and%2520Heung-Yeung%2520Shum%2520and%2520Daxin%2520Jiang%26entry.1292438233%3D%2520%2520We%2520present%2520Step-Video-T2V%252C%2520a%2520state-of-the-art%2520text-to-video%2520pre-trained%2520model%250Awith%252030B%2520parameters%2520and%2520the%2520ability%2520to%2520generate%2520videos%2520up%2520to%2520204%2520frames%2520in%250Alength.%2520A%2520deep%2520compression%2520Variational%2520Autoencoder%252C%2520Video-VAE%252C%2520is%2520designed%2520for%250Avideo%2520generation%2520tasks%252C%2520achieving%252016x16%2520spatial%2520and%25208x%2520temporal%2520compression%250Aratios%252C%2520while%2520maintaining%2520exceptional%2520video%2520reconstruction%2520quality.%2520User%250Aprompts%2520are%2520encoded%2520using%2520two%2520bilingual%2520text%2520encoders%2520to%2520handle%2520both%2520English%250Aand%2520Chinese.%2520A%2520DiT%2520with%25203D%2520full%2520attention%2520is%2520trained%2520using%2520Flow%2520Matching%2520and%2520is%250Aemployed%2520to%2520denoise%2520input%2520noise%2520into%2520latent%2520frames.%2520A%2520video-based%2520DPO%2520approach%252C%250AVideo-DPO%252C%2520is%2520applied%2520to%2520reduce%2520artifacts%2520and%2520improve%2520the%2520visual%2520quality%2520of%2520the%250Agenerated%2520videos.%2520We%2520also%2520detail%2520our%2520training%2520strategies%2520and%2520share%2520key%250Aobservations%2520and%2520insights.%2520Step-Video-T2V%2527s%2520performance%2520is%2520evaluated%2520on%2520a%2520novel%250Avideo%2520generation%2520benchmark%252C%2520Step-Video-T2V-Eval%252C%2520demonstrating%2520its%250Astate-of-the-art%2520text-to-video%2520quality%2520when%2520compared%2520with%2520both%2520open-source%2520and%250Acommercial%2520engines.%2520Additionally%252C%2520we%2520discuss%2520the%2520limitations%2520of%2520current%250Adiffusion-based%2520model%2520paradigm%2520and%2520outline%2520future%2520directions%2520for%2520video%250Afoundation%2520models.%2520We%2520make%2520both%2520Step-Video-T2V%2520and%2520Step-Video-T2V-Eval%250Aavailable%2520at%2520https%253A//github.com/stepfun-ai/Step-Video-T2V.%2520The%2520online%2520version%250Acan%2520be%2520accessed%2520from%2520https%253A//yuewen.cn/videos%2520as%2520well.%2520Our%2520goal%2520is%2520to%250Aaccelerate%2520the%2520innovation%2520of%2520video%2520foundation%2520models%2520and%2520empower%2520video%2520content%250Acreators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model&entry.906535625=Guoqing%20Ma%20and%20Haoyang%20Huang%20and%20Kun%20Yan%20and%20Liangyu%20Chen%20and%20Nan%20Duan%20and%20Shengming%20Yin%20and%20Changyi%20Wan%20and%20Ranchen%20Ming%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Yu%20Zhou%20and%20Deshan%20Sun%20and%20Deyu%20Zhou%20and%20Jian%20Zhou%20and%20Kaijun%20Tan%20and%20Kang%20An%20and%20Mei%20Chen%20and%20Wei%20Ji%20and%20Qiling%20Wu%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Yanan%20Wei%20and%20Zheng%20Ge%20and%20Aojie%20Li%20and%20Bin%20Wang%20and%20Bizhu%20Huang%20and%20Bo%20Wang%20and%20Brian%20Li%20and%20Changxing%20Miao%20and%20Chen%20Xu%20and%20Chenfei%20Wu%20and%20Chenguang%20Yu%20and%20Dapeng%20Shi%20and%20Dingyuan%20Hu%20and%20Enle%20Liu%20and%20Gang%20Yu%20and%20Ge%20Yang%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Haiyang%20Feng%20and%20Hao%20Nie%20and%20Haonan%20Jia%20and%20Hanpeng%20Hu%20and%20Hanqi%20Chen%20and%20Haolong%20Yan%20and%20Heng%20Wang%20and%20Hongcheng%20Guo%20and%20Huilin%20Xiong%20and%20Huixin%20Xiong%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Wu%20and%20Jie%20Yang%20and%20Jiashuai%20Liu%20and%20Jiashuo%20Li%20and%20Jingyang%20Zhang%20and%20Junjing%20Guo%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Liu%20and%20Lei%20Xia%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Ming%20Li%20and%20Mingliang%20Li%20and%20Muhua%20Cheng%20and%20Na%20Wang%20and%20Qiaohui%20Chen%20and%20Qinglin%20He%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Ran%20Sun%20and%20Rui%20Wang%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Sitong%20Liu%20and%20Siqi%20Liu%20and%20Shuli%20Gao%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Weipeng%20Ming%20and%20Wenqing%20He%20and%20Xu%20Zhao%20and%20Xuelin%20Zhang%20and%20Xianfang%20Zeng%20and%20Xiaojia%20Liu%20and%20Xuan%20Yang%20and%20Yaqi%20Dai%20and%20Yanbo%20Yu%20and%20Yang%20Li%20and%20Yineng%20Deng%20and%20Yingming%20Wang%20and%20Yilei%20Wang%20and%20Yuanwei%20Lu%20and%20Yu%20Chen%20and%20Yu%20Luo%20and%20Yuchu%20Luo%20and%20Yuhe%20Yin%20and%20Yuheng%20Feng%20and%20Yuxiang%20Yang%20and%20Zecheng%20Tang%20and%20Zekai%20Zhang%20and%20Zidong%20Yang%20and%20Binxing%20Jiao%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu%20and%20Heung-Yeung%20Shum%20and%20Daxin%20Jiang&entry.1292438233=%20%20We%20present%20Step-Video-T2V%2C%20a%20state-of-the-art%20text-to-video%20pre-trained%20model%0Awith%2030B%20parameters%20and%20the%20ability%20to%20generate%20videos%20up%20to%20204%20frames%20in%0Alength.%20A%20deep%20compression%20Variational%20Autoencoder%2C%20Video-VAE%2C%20is%20designed%20for%0Avideo%20generation%20tasks%2C%20achieving%2016x16%20spatial%20and%208x%20temporal%20compression%0Aratios%2C%20while%20maintaining%20exceptional%20video%20reconstruction%20quality.%20User%0Aprompts%20are%20encoded%20using%20two%20bilingual%20text%20encoders%20to%20handle%20both%20English%0Aand%20Chinese.%20A%20DiT%20with%203D%20full%20attention%20is%20trained%20using%20Flow%20Matching%20and%20is%0Aemployed%20to%20denoise%20input%20noise%20into%20latent%20frames.%20A%20video-based%20DPO%20approach%2C%0AVideo-DPO%2C%20is%20applied%20to%20reduce%20artifacts%20and%20improve%20the%20visual%20quality%20of%20the%0Agenerated%20videos.%20We%20also%20detail%20our%20training%20strategies%20and%20share%20key%0Aobservations%20and%20insights.%20Step-Video-T2V%27s%20performance%20is%20evaluated%20on%20a%20novel%0Avideo%20generation%20benchmark%2C%20Step-Video-T2V-Eval%2C%20demonstrating%20its%0Astate-of-the-art%20text-to-video%20quality%20when%20compared%20with%20both%20open-source%20and%0Acommercial%20engines.%20Additionally%2C%20we%20discuss%20the%20limitations%20of%20current%0Adiffusion-based%20model%20paradigm%20and%20outline%20future%20directions%20for%20video%0Afoundation%20models.%20We%20make%20both%20Step-Video-T2V%20and%20Step-Video-T2V-Eval%0Aavailable%20at%20https%3A//github.com/stepfun-ai/Step-Video-T2V.%20The%20online%20version%0Acan%20be%20accessed%20from%20https%3A//yuewen.cn/videos%20as%20well.%20Our%20goal%20is%20to%0Aaccelerate%20the%20innovation%20of%20video%20foundation%20models%20and%20empower%20video%20content%0Acreators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10248v1&entry.124074799=Read"},
{"title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification", "author": "Yubo Wang and Haoyang Li and Fei Teng and Lei Chen", "abstract": "  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n", "link": "http://arxiv.org/abs/2501.02844v3", "date": "2025-02-14", "relevancy": 2.5794, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5277}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5222}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-based%20Retrieval%20Augmented%20Generation%20for%20Dynamic%20Few-shot%20Text%0A%20%20Classification&body=Title%3A%20Graph-based%20Retrieval%20Augmented%20Generation%20for%20Dynamic%20Few-shot%20Text%0A%20%20Classification%0AAuthor%3A%20Yubo%20Wang%20and%20Haoyang%20Li%20and%20Fei%20Teng%20and%20Lei%20Chen%0AAbstract%3A%20%20%20Text%20classification%20is%20a%20fundamental%20task%20in%20data%20mining%2C%20pivotal%20to%20various%0Aapplications%20such%20as%20tabular%20understanding%20and%20recommendation.%20Although%20neural%0Anetwork-based%20models%2C%20such%20as%20CNN%20and%20BERT%2C%20have%20demonstrated%20remarkable%0Aperformance%20in%20text%20classification%2C%20their%20effectiveness%20heavily%20relies%20on%0Aabundant%20labeled%20training%20data.%20This%20dependency%20makes%20these%20models%20less%0Aeffective%20in%20dynamic%20few-shot%20text%20classification%2C%20where%20labeled%20data%20is%0Ascarce%2C%20and%20new%20target%20labels%20frequently%20appear%20based%20on%20application%20needs.%0ARecently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20due%20to%20their%0Aextensive%20pretraining%20and%20contextual%20understanding%20ability.%20Current%20approaches%0Aprovide%20LLMs%20with%20text%20inputs%2C%20candidate%20labels%2C%20and%20additional%20side%0Ainformation%20%28e.g.%2C%20descriptions%29%20to%20classify%20texts.%20However%2C%20their%0Aeffectiveness%20is%20hindered%20by%20the%20increased%20input%20size%20and%20the%20noise%20introduced%0Athrough%20side%20information%20processing.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Agraph-based%20online%20retrieval-augmented%20generation%20framework%2C%20namely%20GORAG%2C%20for%0Adynamic%20few-shot%20text%20classification.%20Rather%20than%20treating%20each%20input%0Aindependently%2C%20GORAG%20constructs%20and%20maintains%20a%20weighted%20graph%20by%20extracting%0Aside%20information%20across%20all%20target%20texts.%20In%20this%20graph%2C%20text%20keywords%20and%0Alabels%20are%20represented%20as%20nodes%2C%20with%20edges%20indicating%20the%20correlations%20between%0Athem.%20To%20model%20these%20correlations%2C%20GORAG%20employs%20an%20edge%20weighting%20mechanism%20to%0Aprioritize%20the%20importance%20and%20reliability%20of%20extracted%20information%20and%0Adynamically%20retrieves%20relevant%20context%20using%20a%20minimum-cost%20spanning%20tree%0Atailored%20for%20each%20text%20input.%20Empirical%20evaluations%20demonstrate%20that%20GORAG%0Aoutperforms%20existing%20approaches%20by%20providing%20more%20comprehensive%20and%20precise%0Acontextual%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.02844v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-based%2520Retrieval%2520Augmented%2520Generation%2520for%2520Dynamic%2520Few-shot%2520Text%250A%2520%2520Classification%26entry.906535625%3DYubo%2520Wang%2520and%2520Haoyang%2520Li%2520and%2520Fei%2520Teng%2520and%2520Lei%2520Chen%26entry.1292438233%3D%2520%2520Text%2520classification%2520is%2520a%2520fundamental%2520task%2520in%2520data%2520mining%252C%2520pivotal%2520to%2520various%250Aapplications%2520such%2520as%2520tabular%2520understanding%2520and%2520recommendation.%2520Although%2520neural%250Anetwork-based%2520models%252C%2520such%2520as%2520CNN%2520and%2520BERT%252C%2520have%2520demonstrated%2520remarkable%250Aperformance%2520in%2520text%2520classification%252C%2520their%2520effectiveness%2520heavily%2520relies%2520on%250Aabundant%2520labeled%2520training%2520data.%2520This%2520dependency%2520makes%2520these%2520models%2520less%250Aeffective%2520in%2520dynamic%2520few-shot%2520text%2520classification%252C%2520where%2520labeled%2520data%2520is%250Ascarce%252C%2520and%2520new%2520target%2520labels%2520frequently%2520appear%2520based%2520on%2520application%2520needs.%250ARecently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520due%2520to%2520their%250Aextensive%2520pretraining%2520and%2520contextual%2520understanding%2520ability.%2520Current%2520approaches%250Aprovide%2520LLMs%2520with%2520text%2520inputs%252C%2520candidate%2520labels%252C%2520and%2520additional%2520side%250Ainformation%2520%2528e.g.%252C%2520descriptions%2529%2520to%2520classify%2520texts.%2520However%252C%2520their%250Aeffectiveness%2520is%2520hindered%2520by%2520the%2520increased%2520input%2520size%2520and%2520the%2520noise%2520introduced%250Athrough%2520side%2520information%2520processing.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Agraph-based%2520online%2520retrieval-augmented%2520generation%2520framework%252C%2520namely%2520GORAG%252C%2520for%250Adynamic%2520few-shot%2520text%2520classification.%2520Rather%2520than%2520treating%2520each%2520input%250Aindependently%252C%2520GORAG%2520constructs%2520and%2520maintains%2520a%2520weighted%2520graph%2520by%2520extracting%250Aside%2520information%2520across%2520all%2520target%2520texts.%2520In%2520this%2520graph%252C%2520text%2520keywords%2520and%250Alabels%2520are%2520represented%2520as%2520nodes%252C%2520with%2520edges%2520indicating%2520the%2520correlations%2520between%250Athem.%2520To%2520model%2520these%2520correlations%252C%2520GORAG%2520employs%2520an%2520edge%2520weighting%2520mechanism%2520to%250Aprioritize%2520the%2520importance%2520and%2520reliability%2520of%2520extracted%2520information%2520and%250Adynamically%2520retrieves%2520relevant%2520context%2520using%2520a%2520minimum-cost%2520spanning%2520tree%250Atailored%2520for%2520each%2520text%2520input.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520GORAG%250Aoutperforms%2520existing%2520approaches%2520by%2520providing%2520more%2520comprehensive%2520and%2520precise%250Acontextual%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.02844v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-based%20Retrieval%20Augmented%20Generation%20for%20Dynamic%20Few-shot%20Text%0A%20%20Classification&entry.906535625=Yubo%20Wang%20and%20Haoyang%20Li%20and%20Fei%20Teng%20and%20Lei%20Chen&entry.1292438233=%20%20Text%20classification%20is%20a%20fundamental%20task%20in%20data%20mining%2C%20pivotal%20to%20various%0Aapplications%20such%20as%20tabular%20understanding%20and%20recommendation.%20Although%20neural%0Anetwork-based%20models%2C%20such%20as%20CNN%20and%20BERT%2C%20have%20demonstrated%20remarkable%0Aperformance%20in%20text%20classification%2C%20their%20effectiveness%20heavily%20relies%20on%0Aabundant%20labeled%20training%20data.%20This%20dependency%20makes%20these%20models%20less%0Aeffective%20in%20dynamic%20few-shot%20text%20classification%2C%20where%20labeled%20data%20is%0Ascarce%2C%20and%20new%20target%20labels%20frequently%20appear%20based%20on%20application%20needs.%0ARecently%2C%20large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20due%20to%20their%0Aextensive%20pretraining%20and%20contextual%20understanding%20ability.%20Current%20approaches%0Aprovide%20LLMs%20with%20text%20inputs%2C%20candidate%20labels%2C%20and%20additional%20side%0Ainformation%20%28e.g.%2C%20descriptions%29%20to%20classify%20texts.%20However%2C%20their%0Aeffectiveness%20is%20hindered%20by%20the%20increased%20input%20size%20and%20the%20noise%20introduced%0Athrough%20side%20information%20processing.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Agraph-based%20online%20retrieval-augmented%20generation%20framework%2C%20namely%20GORAG%2C%20for%0Adynamic%20few-shot%20text%20classification.%20Rather%20than%20treating%20each%20input%0Aindependently%2C%20GORAG%20constructs%20and%20maintains%20a%20weighted%20graph%20by%20extracting%0Aside%20information%20across%20all%20target%20texts.%20In%20this%20graph%2C%20text%20keywords%20and%0Alabels%20are%20represented%20as%20nodes%2C%20with%20edges%20indicating%20the%20correlations%20between%0Athem.%20To%20model%20these%20correlations%2C%20GORAG%20employs%20an%20edge%20weighting%20mechanism%20to%0Aprioritize%20the%20importance%20and%20reliability%20of%20extracted%20information%20and%0Adynamically%20retrieves%20relevant%20context%20using%20a%20minimum-cost%20spanning%20tree%0Atailored%20for%20each%20text%20input.%20Empirical%20evaluations%20demonstrate%20that%20GORAG%0Aoutperforms%20existing%20approaches%20by%20providing%20more%20comprehensive%20and%20precise%0Acontextual%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.02844v3&entry.124074799=Read"},
{"title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection", "author": "Bettina Messmer and Vinko Sabol\u010dec and Martin Jaggi", "abstract": "  Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.\n", "link": "http://arxiv.org/abs/2502.10361v1", "date": "2025-02-14", "relevancy": 2.5659, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multilingual%20LLM%20Pretraining%20with%20Model-Based%20Data%20Selection&body=Title%3A%20Enhancing%20Multilingual%20LLM%20Pretraining%20with%20Model-Based%20Data%20Selection%0AAuthor%3A%20Bettina%20Messmer%20and%20Vinko%20Sabol%C4%8Dec%20and%20Martin%20Jaggi%0AAbstract%3A%20%20%20Dataset%20curation%20has%20become%20a%20basis%20for%20strong%20large%20language%20model%20%28LLM%29%0Aperformance.%20While%20various%20rule-based%20filtering%20heuristics%20exist%20for%20English%0Aand%20multilingual%20datasets%2C%20model-based%20filtering%20techniques%20have%20primarily%0Afocused%20on%20English.%20To%20address%20the%20disparity%20stemming%20from%20limited%20research%20on%0Anon-English%20languages%2C%20we%20propose%20a%20model-based%20filtering%20framework%20for%0Amultilingual%20datasets%20that%20aims%20to%20identify%20a%20diverse%20set%20of%20structured%20and%0Aknowledge-rich%20samples.%20Our%20approach%20emphasizes%20transparency%2C%20simplicity%2C%20and%0Aefficiency%2C%20leveraging%20Transformer-%20and%20FastText-based%20classifiers%20to%20ensure%0Athe%20broad%20accessibility%20of%20our%20technique%20and%20data.%20We%20conduct%20comprehensive%0Aablation%20studies%20on%20the%20FineWeb-2%20web%20crawl%20dataset%20across%20diverse%20language%0Afamilies%2C%20scripts%2C%20and%20resource%20availability%20to%20demonstrate%20the%20effectiveness%0Aof%20our%20method.%20Training%20a%201B-parameter%20Llama%20model%20for%2070B%20and%20119B%20tokens%2C%20our%0Aapproach%20can%20match%20the%20baseline%20MMLU%20score%20with%20as%20little%20as%2015%25%20of%20the%0Atraining%20tokens%2C%20while%20also%20improving%20across%20other%20benchmarks.%20These%20findings%0Aprovide%20strong%20evidence%20for%20the%20generalizability%20of%20our%20approach%20to%20other%0Alanguages.%20As%20a%20result%2C%20we%20extend%20our%20framework%20to%2020%20languages%20for%20which%20we%0Arelease%20the%20refined%20pretraining%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multilingual%2520LLM%2520Pretraining%2520with%2520Model-Based%2520Data%2520Selection%26entry.906535625%3DBettina%2520Messmer%2520and%2520Vinko%2520Sabol%25C4%258Dec%2520and%2520Martin%2520Jaggi%26entry.1292438233%3D%2520%2520Dataset%2520curation%2520has%2520become%2520a%2520basis%2520for%2520strong%2520large%2520language%2520model%2520%2528LLM%2529%250Aperformance.%2520While%2520various%2520rule-based%2520filtering%2520heuristics%2520exist%2520for%2520English%250Aand%2520multilingual%2520datasets%252C%2520model-based%2520filtering%2520techniques%2520have%2520primarily%250Afocused%2520on%2520English.%2520To%2520address%2520the%2520disparity%2520stemming%2520from%2520limited%2520research%2520on%250Anon-English%2520languages%252C%2520we%2520propose%2520a%2520model-based%2520filtering%2520framework%2520for%250Amultilingual%2520datasets%2520that%2520aims%2520to%2520identify%2520a%2520diverse%2520set%2520of%2520structured%2520and%250Aknowledge-rich%2520samples.%2520Our%2520approach%2520emphasizes%2520transparency%252C%2520simplicity%252C%2520and%250Aefficiency%252C%2520leveraging%2520Transformer-%2520and%2520FastText-based%2520classifiers%2520to%2520ensure%250Athe%2520broad%2520accessibility%2520of%2520our%2520technique%2520and%2520data.%2520We%2520conduct%2520comprehensive%250Aablation%2520studies%2520on%2520the%2520FineWeb-2%2520web%2520crawl%2520dataset%2520across%2520diverse%2520language%250Afamilies%252C%2520scripts%252C%2520and%2520resource%2520availability%2520to%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method.%2520Training%2520a%25201B-parameter%2520Llama%2520model%2520for%252070B%2520and%2520119B%2520tokens%252C%2520our%250Aapproach%2520can%2520match%2520the%2520baseline%2520MMLU%2520score%2520with%2520as%2520little%2520as%252015%2525%2520of%2520the%250Atraining%2520tokens%252C%2520while%2520also%2520improving%2520across%2520other%2520benchmarks.%2520These%2520findings%250Aprovide%2520strong%2520evidence%2520for%2520the%2520generalizability%2520of%2520our%2520approach%2520to%2520other%250Alanguages.%2520As%2520a%2520result%252C%2520we%2520extend%2520our%2520framework%2520to%252020%2520languages%2520for%2520which%2520we%250Arelease%2520the%2520refined%2520pretraining%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multilingual%20LLM%20Pretraining%20with%20Model-Based%20Data%20Selection&entry.906535625=Bettina%20Messmer%20and%20Vinko%20Sabol%C4%8Dec%20and%20Martin%20Jaggi&entry.1292438233=%20%20Dataset%20curation%20has%20become%20a%20basis%20for%20strong%20large%20language%20model%20%28LLM%29%0Aperformance.%20While%20various%20rule-based%20filtering%20heuristics%20exist%20for%20English%0Aand%20multilingual%20datasets%2C%20model-based%20filtering%20techniques%20have%20primarily%0Afocused%20on%20English.%20To%20address%20the%20disparity%20stemming%20from%20limited%20research%20on%0Anon-English%20languages%2C%20we%20propose%20a%20model-based%20filtering%20framework%20for%0Amultilingual%20datasets%20that%20aims%20to%20identify%20a%20diverse%20set%20of%20structured%20and%0Aknowledge-rich%20samples.%20Our%20approach%20emphasizes%20transparency%2C%20simplicity%2C%20and%0Aefficiency%2C%20leveraging%20Transformer-%20and%20FastText-based%20classifiers%20to%20ensure%0Athe%20broad%20accessibility%20of%20our%20technique%20and%20data.%20We%20conduct%20comprehensive%0Aablation%20studies%20on%20the%20FineWeb-2%20web%20crawl%20dataset%20across%20diverse%20language%0Afamilies%2C%20scripts%2C%20and%20resource%20availability%20to%20demonstrate%20the%20effectiveness%0Aof%20our%20method.%20Training%20a%201B-parameter%20Llama%20model%20for%2070B%20and%20119B%20tokens%2C%20our%0Aapproach%20can%20match%20the%20baseline%20MMLU%20score%20with%20as%20little%20as%2015%25%20of%20the%0Atraining%20tokens%2C%20while%20also%20improving%20across%20other%20benchmarks.%20These%20findings%0Aprovide%20strong%20evidence%20for%20the%20generalizability%20of%20our%20approach%20to%20other%0Alanguages.%20As%20a%20result%2C%20we%20extend%20our%20framework%20to%2020%20languages%20for%20which%20we%0Arelease%20the%20refined%20pretraining%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10361v1&entry.124074799=Read"},
{"title": "SGS-GNN: A Supervised Graph Sparsification method for Graph Neural\n  Networks", "author": "Siddhartha Shankar Das and Naheed Anjum Arafat and Muftiqur Rahman and S M Ferdous and Alex Pothen and Mahantesh M Halappanavar", "abstract": "  We propose SGS-GNN, a novel supervised graph sparsifier that learns the\nsampling probability distribution of edges and samples sparse subgraphs of a\nuser-specified size to reduce the computational costs required by GNNs for\ninference tasks on large graphs. SGS-GNN employs regularizers in the loss\nfunction to enhance homophily in sparse subgraphs, boosting the accuracy of\nGNNs on heterophilic graphs, where a significant number of the neighbors of a\nnode have dissimilar labels. SGS-GNN also supports conditional updates of the\nprobability distribution learning module based on a prior, which helps narrow\nthe search space for sparse graphs. SGS-GNN requires fewer epochs to obtain\nhigh accuracies since it learns the search space of subgraphs more effectively\nthan methods using fixed distributions such as random sampling. Extensive\nexperiments using 33 homophilic and heterophilic graphs demonstrate the\nfollowing: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN\nimproves the F1-scores by a geometric mean of 4% relative to the original\ngraph; on heterophilic graphs, the prediction accuracy is better up to 30%.\n(ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores\nof 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and\n(iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires\nabout half the number of epochs to converge.\n", "link": "http://arxiv.org/abs/2502.10208v1", "date": "2025-02-14", "relevancy": 2.5583, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5189}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5092}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGS-GNN%3A%20A%20Supervised%20Graph%20Sparsification%20method%20for%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20SGS-GNN%3A%20A%20Supervised%20Graph%20Sparsification%20method%20for%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Siddhartha%20Shankar%20Das%20and%20Naheed%20Anjum%20Arafat%20and%20Muftiqur%20Rahman%20and%20S%20M%20Ferdous%20and%20Alex%20Pothen%20and%20Mahantesh%20M%20Halappanavar%0AAbstract%3A%20%20%20We%20propose%20SGS-GNN%2C%20a%20novel%20supervised%20graph%20sparsifier%20that%20learns%20the%0Asampling%20probability%20distribution%20of%20edges%20and%20samples%20sparse%20subgraphs%20of%20a%0Auser-specified%20size%20to%20reduce%20the%20computational%20costs%20required%20by%20GNNs%20for%0Ainference%20tasks%20on%20large%20graphs.%20SGS-GNN%20employs%20regularizers%20in%20the%20loss%0Afunction%20to%20enhance%20homophily%20in%20sparse%20subgraphs%2C%20boosting%20the%20accuracy%20of%0AGNNs%20on%20heterophilic%20graphs%2C%20where%20a%20significant%20number%20of%20the%20neighbors%20of%20a%0Anode%20have%20dissimilar%20labels.%20SGS-GNN%20also%20supports%20conditional%20updates%20of%20the%0Aprobability%20distribution%20learning%20module%20based%20on%20a%20prior%2C%20which%20helps%20narrow%0Athe%20search%20space%20for%20sparse%20graphs.%20SGS-GNN%20requires%20fewer%20epochs%20to%20obtain%0Ahigh%20accuracies%20since%20it%20learns%20the%20search%20space%20of%20subgraphs%20more%20effectively%0Athan%20methods%20using%20fixed%20distributions%20such%20as%20random%20sampling.%20Extensive%0Aexperiments%20using%2033%20homophilic%20and%20heterophilic%20graphs%20demonstrate%20the%0Afollowing%3A%20%28i%29%20with%20only%2020%25%20of%20edges%20retained%20in%20the%20sparse%20subgraphs%2C%20SGS-GNN%0Aimproves%20the%20F1-scores%20by%20a%20geometric%20mean%20of%204%25%20relative%20to%20the%20original%0Agraph%3B%20on%20heterophilic%20graphs%2C%20the%20prediction%20accuracy%20is%20better%20up%20to%2030%25.%0A%28ii%29%20SGS-GNN%20outperforms%20state-of-the-art%20methods%20with%20improvement%20in%20F1-scores%0Aof%204-7%25%20in%20geometric%20mean%20with%20similar%20sparsities%20in%20the%20sampled%20subgraphs%2C%20and%0A%28iii%29%20compared%20to%20sparsifiers%20that%20employ%20fixed%20distributions%2C%20SGS-GNN%20requires%0Aabout%20half%20the%20number%20of%20epochs%20to%20converge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGS-GNN%253A%2520A%2520Supervised%2520Graph%2520Sparsification%2520method%2520for%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DSiddhartha%2520Shankar%2520Das%2520and%2520Naheed%2520Anjum%2520Arafat%2520and%2520Muftiqur%2520Rahman%2520and%2520S%2520M%2520Ferdous%2520and%2520Alex%2520Pothen%2520and%2520Mahantesh%2520M%2520Halappanavar%26entry.1292438233%3D%2520%2520We%2520propose%2520SGS-GNN%252C%2520a%2520novel%2520supervised%2520graph%2520sparsifier%2520that%2520learns%2520the%250Asampling%2520probability%2520distribution%2520of%2520edges%2520and%2520samples%2520sparse%2520subgraphs%2520of%2520a%250Auser-specified%2520size%2520to%2520reduce%2520the%2520computational%2520costs%2520required%2520by%2520GNNs%2520for%250Ainference%2520tasks%2520on%2520large%2520graphs.%2520SGS-GNN%2520employs%2520regularizers%2520in%2520the%2520loss%250Afunction%2520to%2520enhance%2520homophily%2520in%2520sparse%2520subgraphs%252C%2520boosting%2520the%2520accuracy%2520of%250AGNNs%2520on%2520heterophilic%2520graphs%252C%2520where%2520a%2520significant%2520number%2520of%2520the%2520neighbors%2520of%2520a%250Anode%2520have%2520dissimilar%2520labels.%2520SGS-GNN%2520also%2520supports%2520conditional%2520updates%2520of%2520the%250Aprobability%2520distribution%2520learning%2520module%2520based%2520on%2520a%2520prior%252C%2520which%2520helps%2520narrow%250Athe%2520search%2520space%2520for%2520sparse%2520graphs.%2520SGS-GNN%2520requires%2520fewer%2520epochs%2520to%2520obtain%250Ahigh%2520accuracies%2520since%2520it%2520learns%2520the%2520search%2520space%2520of%2520subgraphs%2520more%2520effectively%250Athan%2520methods%2520using%2520fixed%2520distributions%2520such%2520as%2520random%2520sampling.%2520Extensive%250Aexperiments%2520using%252033%2520homophilic%2520and%2520heterophilic%2520graphs%2520demonstrate%2520the%250Afollowing%253A%2520%2528i%2529%2520with%2520only%252020%2525%2520of%2520edges%2520retained%2520in%2520the%2520sparse%2520subgraphs%252C%2520SGS-GNN%250Aimproves%2520the%2520F1-scores%2520by%2520a%2520geometric%2520mean%2520of%25204%2525%2520relative%2520to%2520the%2520original%250Agraph%253B%2520on%2520heterophilic%2520graphs%252C%2520the%2520prediction%2520accuracy%2520is%2520better%2520up%2520to%252030%2525.%250A%2528ii%2529%2520SGS-GNN%2520outperforms%2520state-of-the-art%2520methods%2520with%2520improvement%2520in%2520F1-scores%250Aof%25204-7%2525%2520in%2520geometric%2520mean%2520with%2520similar%2520sparsities%2520in%2520the%2520sampled%2520subgraphs%252C%2520and%250A%2528iii%2529%2520compared%2520to%2520sparsifiers%2520that%2520employ%2520fixed%2520distributions%252C%2520SGS-GNN%2520requires%250Aabout%2520half%2520the%2520number%2520of%2520epochs%2520to%2520converge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGS-GNN%3A%20A%20Supervised%20Graph%20Sparsification%20method%20for%20Graph%20Neural%0A%20%20Networks&entry.906535625=Siddhartha%20Shankar%20Das%20and%20Naheed%20Anjum%20Arafat%20and%20Muftiqur%20Rahman%20and%20S%20M%20Ferdous%20and%20Alex%20Pothen%20and%20Mahantesh%20M%20Halappanavar&entry.1292438233=%20%20We%20propose%20SGS-GNN%2C%20a%20novel%20supervised%20graph%20sparsifier%20that%20learns%20the%0Asampling%20probability%20distribution%20of%20edges%20and%20samples%20sparse%20subgraphs%20of%20a%0Auser-specified%20size%20to%20reduce%20the%20computational%20costs%20required%20by%20GNNs%20for%0Ainference%20tasks%20on%20large%20graphs.%20SGS-GNN%20employs%20regularizers%20in%20the%20loss%0Afunction%20to%20enhance%20homophily%20in%20sparse%20subgraphs%2C%20boosting%20the%20accuracy%20of%0AGNNs%20on%20heterophilic%20graphs%2C%20where%20a%20significant%20number%20of%20the%20neighbors%20of%20a%0Anode%20have%20dissimilar%20labels.%20SGS-GNN%20also%20supports%20conditional%20updates%20of%20the%0Aprobability%20distribution%20learning%20module%20based%20on%20a%20prior%2C%20which%20helps%20narrow%0Athe%20search%20space%20for%20sparse%20graphs.%20SGS-GNN%20requires%20fewer%20epochs%20to%20obtain%0Ahigh%20accuracies%20since%20it%20learns%20the%20search%20space%20of%20subgraphs%20more%20effectively%0Athan%20methods%20using%20fixed%20distributions%20such%20as%20random%20sampling.%20Extensive%0Aexperiments%20using%2033%20homophilic%20and%20heterophilic%20graphs%20demonstrate%20the%0Afollowing%3A%20%28i%29%20with%20only%2020%25%20of%20edges%20retained%20in%20the%20sparse%20subgraphs%2C%20SGS-GNN%0Aimproves%20the%20F1-scores%20by%20a%20geometric%20mean%20of%204%25%20relative%20to%20the%20original%0Agraph%3B%20on%20heterophilic%20graphs%2C%20the%20prediction%20accuracy%20is%20better%20up%20to%2030%25.%0A%28ii%29%20SGS-GNN%20outperforms%20state-of-the-art%20methods%20with%20improvement%20in%20F1-scores%0Aof%204-7%25%20in%20geometric%20mean%20with%20similar%20sparsities%20in%20the%20sampled%20subgraphs%2C%20and%0A%28iii%29%20compared%20to%20sparsifiers%20that%20employ%20fixed%20distributions%2C%20SGS-GNN%20requires%0Aabout%20half%20the%20number%20of%20epochs%20to%20converge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10208v1&entry.124074799=Read"},
{"title": "Anti-Forgetting Adaptation for Unsupervised Person Re-identification", "author": "Hao Chen and Francois Bremond and Nicu Sebe and Shiliang Zhang", "abstract": "  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n", "link": "http://arxiv.org/abs/2411.14695v2", "date": "2025-02-14", "relevancy": 2.526, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5149}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anti-Forgetting%20Adaptation%20for%20Unsupervised%20Person%20Re-identification&body=Title%3A%20Anti-Forgetting%20Adaptation%20for%20Unsupervised%20Person%20Re-identification%0AAuthor%3A%20Hao%20Chen%20and%20Francois%20Bremond%20and%20Nicu%20Sebe%20and%20Shiliang%20Zhang%0AAbstract%3A%20%20%20Regular%20unsupervised%20domain%20adaptive%20person%20re-identification%20%28ReID%29%20focuses%0Aon%20adapting%20a%20model%20from%20a%20source%20domain%20to%20a%20fixed%20target%20domain.%20However%2C%20an%0Aadapted%20ReID%20model%20can%20hardly%20retain%20previously-acquired%20knowledge%20and%0Ageneralize%20to%20unseen%20data.%20In%20this%20paper%2C%20we%20propose%20a%20Dual-level%20Joint%0AAdaptation%20and%20Anti-forgetting%20%28DJAA%29%20framework%2C%20which%20incrementally%20adapts%20a%0Amodel%20to%20new%20domains%20without%20forgetting%20source%20domain%20and%20each%20adapted%20target%0Adomain.%20We%20explore%20the%20possibility%20of%20using%20prototype%20and%20instance-level%0Aconsistency%20to%20mitigate%20the%20forgetting%20during%20the%20adaptation.%20Specifically%2C%20we%0Astore%20a%20small%20number%20of%20representative%20image%20samples%20and%20corresponding%20cluster%0Aprototypes%20in%20a%20memory%20buffer%2C%20which%20is%20updated%20at%20each%20adaptation%20step.%20With%0Athe%20buffered%20images%20and%20prototypes%2C%20we%20regularize%20the%20image-to-image%20similarity%0Aand%20image-to-prototype%20similarity%20to%20rehearse%20old%20knowledge.%20After%20the%0Amulti-step%20adaptation%2C%20the%20model%20is%20tested%20on%20all%20seen%20domains%20and%20several%0Aunseen%20domains%20to%20validate%20the%20generalization%20ability%20of%20our%20method.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20method%20significantly%20improves%20the%0Aanti-forgetting%2C%20generalization%20and%20backward-compatible%20ability%20of%20an%0Aunsupervised%20person%20ReID%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnti-Forgetting%2520Adaptation%2520for%2520Unsupervised%2520Person%2520Re-identification%26entry.906535625%3DHao%2520Chen%2520and%2520Francois%2520Bremond%2520and%2520Nicu%2520Sebe%2520and%2520Shiliang%2520Zhang%26entry.1292438233%3D%2520%2520Regular%2520unsupervised%2520domain%2520adaptive%2520person%2520re-identification%2520%2528ReID%2529%2520focuses%250Aon%2520adapting%2520a%2520model%2520from%2520a%2520source%2520domain%2520to%2520a%2520fixed%2520target%2520domain.%2520However%252C%2520an%250Aadapted%2520ReID%2520model%2520can%2520hardly%2520retain%2520previously-acquired%2520knowledge%2520and%250Ageneralize%2520to%2520unseen%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Dual-level%2520Joint%250AAdaptation%2520and%2520Anti-forgetting%2520%2528DJAA%2529%2520framework%252C%2520which%2520incrementally%2520adapts%2520a%250Amodel%2520to%2520new%2520domains%2520without%2520forgetting%2520source%2520domain%2520and%2520each%2520adapted%2520target%250Adomain.%2520We%2520explore%2520the%2520possibility%2520of%2520using%2520prototype%2520and%2520instance-level%250Aconsistency%2520to%2520mitigate%2520the%2520forgetting%2520during%2520the%2520adaptation.%2520Specifically%252C%2520we%250Astore%2520a%2520small%2520number%2520of%2520representative%2520image%2520samples%2520and%2520corresponding%2520cluster%250Aprototypes%2520in%2520a%2520memory%2520buffer%252C%2520which%2520is%2520updated%2520at%2520each%2520adaptation%2520step.%2520With%250Athe%2520buffered%2520images%2520and%2520prototypes%252C%2520we%2520regularize%2520the%2520image-to-image%2520similarity%250Aand%2520image-to-prototype%2520similarity%2520to%2520rehearse%2520old%2520knowledge.%2520After%2520the%250Amulti-step%2520adaptation%252C%2520the%2520model%2520is%2520tested%2520on%2520all%2520seen%2520domains%2520and%2520several%250Aunseen%2520domains%2520to%2520validate%2520the%2520generalization%2520ability%2520of%2520our%2520method.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520method%2520significantly%2520improves%2520the%250Aanti-forgetting%252C%2520generalization%2520and%2520backward-compatible%2520ability%2520of%2520an%250Aunsupervised%2520person%2520ReID%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anti-Forgetting%20Adaptation%20for%20Unsupervised%20Person%20Re-identification&entry.906535625=Hao%20Chen%20and%20Francois%20Bremond%20and%20Nicu%20Sebe%20and%20Shiliang%20Zhang&entry.1292438233=%20%20Regular%20unsupervised%20domain%20adaptive%20person%20re-identification%20%28ReID%29%20focuses%0Aon%20adapting%20a%20model%20from%20a%20source%20domain%20to%20a%20fixed%20target%20domain.%20However%2C%20an%0Aadapted%20ReID%20model%20can%20hardly%20retain%20previously-acquired%20knowledge%20and%0Ageneralize%20to%20unseen%20data.%20In%20this%20paper%2C%20we%20propose%20a%20Dual-level%20Joint%0AAdaptation%20and%20Anti-forgetting%20%28DJAA%29%20framework%2C%20which%20incrementally%20adapts%20a%0Amodel%20to%20new%20domains%20without%20forgetting%20source%20domain%20and%20each%20adapted%20target%0Adomain.%20We%20explore%20the%20possibility%20of%20using%20prototype%20and%20instance-level%0Aconsistency%20to%20mitigate%20the%20forgetting%20during%20the%20adaptation.%20Specifically%2C%20we%0Astore%20a%20small%20number%20of%20representative%20image%20samples%20and%20corresponding%20cluster%0Aprototypes%20in%20a%20memory%20buffer%2C%20which%20is%20updated%20at%20each%20adaptation%20step.%20With%0Athe%20buffered%20images%20and%20prototypes%2C%20we%20regularize%20the%20image-to-image%20similarity%0Aand%20image-to-prototype%20similarity%20to%20rehearse%20old%20knowledge.%20After%20the%0Amulti-step%20adaptation%2C%20the%20model%20is%20tested%20on%20all%20seen%20domains%20and%20several%0Aunseen%20domains%20to%20validate%20the%20generalization%20ability%20of%20our%20method.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20method%20significantly%20improves%20the%0Aanti-forgetting%2C%20generalization%20and%20backward-compatible%20ability%20of%20an%0Aunsupervised%20person%20ReID%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14695v2&entry.124074799=Read"},
{"title": "SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis", "author": "Haoming Luo and Xiaotian Yu and Shengxuming Zhang and Jiabin Xia and Yang Jian and Yuning Sun and Liang Xue and Mingli Song and Jing Zhang and Xiuming Zhang and Zunlei Feng", "abstract": "  Pathology images are considered the ``gold standard\" for cancer diagnosis and\ntreatment, with gigapixel images providing extensive tissue and cellular\ninformation. Existing methods fail to simultaneously extract global structural\nand local detail features for comprehensive pathology image analysis\nefficiently. To address these limitations, we propose a self-calibration\nenhanced framework for whole slide pathology image analysis, comprising three\ncomponents: a global branch, a focus predictor, and a detailed branch. The\nglobal branch initially classifies using the pathological thumbnail, while the\nfocus predictor identifies relevant regions for classification based on the\nlast layer features of the global branch. The detailed extraction branch then\nassesses whether the magnified regions correspond to the lesion area. Finally,\na feature consistency constraint between the global and detail branches ensures\nthat the global branch focuses on the appropriate region and extracts\nsufficient discriminative features for final identification. These focused\ndiscriminative features prove invaluable for uncovering novel prognostic tumor\nmarkers from the perspective of feature cluster uniqueness and tissue spatial\ndistribution. Extensive experiment results demonstrate that the proposed\nframework can rapidly deliver accurate and explainable results for pathological\ngrading and prognosis tasks.\n", "link": "http://arxiv.org/abs/2412.10853v2", "date": "2025-02-14", "relevancy": 2.5017, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEW%3A%20Self-calibration%20Enhanced%20Whole%20Slide%20Pathology%20Image%20Analysis&body=Title%3A%20SEW%3A%20Self-calibration%20Enhanced%20Whole%20Slide%20Pathology%20Image%20Analysis%0AAuthor%3A%20Haoming%20Luo%20and%20Xiaotian%20Yu%20and%20Shengxuming%20Zhang%20and%20Jiabin%20Xia%20and%20Yang%20Jian%20and%20Yuning%20Sun%20and%20Liang%20Xue%20and%20Mingli%20Song%20and%20Jing%20Zhang%20and%20Xiuming%20Zhang%20and%20Zunlei%20Feng%0AAbstract%3A%20%20%20Pathology%20images%20are%20considered%20the%20%60%60gold%20standard%22%20for%20cancer%20diagnosis%20and%0Atreatment%2C%20with%20gigapixel%20images%20providing%20extensive%20tissue%20and%20cellular%0Ainformation.%20Existing%20methods%20fail%20to%20simultaneously%20extract%20global%20structural%0Aand%20local%20detail%20features%20for%20comprehensive%20pathology%20image%20analysis%0Aefficiently.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20self-calibration%0Aenhanced%20framework%20for%20whole%20slide%20pathology%20image%20analysis%2C%20comprising%20three%0Acomponents%3A%20a%20global%20branch%2C%20a%20focus%20predictor%2C%20and%20a%20detailed%20branch.%20The%0Aglobal%20branch%20initially%20classifies%20using%20the%20pathological%20thumbnail%2C%20while%20the%0Afocus%20predictor%20identifies%20relevant%20regions%20for%20classification%20based%20on%20the%0Alast%20layer%20features%20of%20the%20global%20branch.%20The%20detailed%20extraction%20branch%20then%0Aassesses%20whether%20the%20magnified%20regions%20correspond%20to%20the%20lesion%20area.%20Finally%2C%0Aa%20feature%20consistency%20constraint%20between%20the%20global%20and%20detail%20branches%20ensures%0Athat%20the%20global%20branch%20focuses%20on%20the%20appropriate%20region%20and%20extracts%0Asufficient%20discriminative%20features%20for%20final%20identification.%20These%20focused%0Adiscriminative%20features%20prove%20invaluable%20for%20uncovering%20novel%20prognostic%20tumor%0Amarkers%20from%20the%20perspective%20of%20feature%20cluster%20uniqueness%20and%20tissue%20spatial%0Adistribution.%20Extensive%20experiment%20results%20demonstrate%20that%20the%20proposed%0Aframework%20can%20rapidly%20deliver%20accurate%20and%20explainable%20results%20for%20pathological%0Agrading%20and%20prognosis%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEW%253A%2520Self-calibration%2520Enhanced%2520Whole%2520Slide%2520Pathology%2520Image%2520Analysis%26entry.906535625%3DHaoming%2520Luo%2520and%2520Xiaotian%2520Yu%2520and%2520Shengxuming%2520Zhang%2520and%2520Jiabin%2520Xia%2520and%2520Yang%2520Jian%2520and%2520Yuning%2520Sun%2520and%2520Liang%2520Xue%2520and%2520Mingli%2520Song%2520and%2520Jing%2520Zhang%2520and%2520Xiuming%2520Zhang%2520and%2520Zunlei%2520Feng%26entry.1292438233%3D%2520%2520Pathology%2520images%2520are%2520considered%2520the%2520%2560%2560gold%2520standard%2522%2520for%2520cancer%2520diagnosis%2520and%250Atreatment%252C%2520with%2520gigapixel%2520images%2520providing%2520extensive%2520tissue%2520and%2520cellular%250Ainformation.%2520Existing%2520methods%2520fail%2520to%2520simultaneously%2520extract%2520global%2520structural%250Aand%2520local%2520detail%2520features%2520for%2520comprehensive%2520pathology%2520image%2520analysis%250Aefficiently.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520self-calibration%250Aenhanced%2520framework%2520for%2520whole%2520slide%2520pathology%2520image%2520analysis%252C%2520comprising%2520three%250Acomponents%253A%2520a%2520global%2520branch%252C%2520a%2520focus%2520predictor%252C%2520and%2520a%2520detailed%2520branch.%2520The%250Aglobal%2520branch%2520initially%2520classifies%2520using%2520the%2520pathological%2520thumbnail%252C%2520while%2520the%250Afocus%2520predictor%2520identifies%2520relevant%2520regions%2520for%2520classification%2520based%2520on%2520the%250Alast%2520layer%2520features%2520of%2520the%2520global%2520branch.%2520The%2520detailed%2520extraction%2520branch%2520then%250Aassesses%2520whether%2520the%2520magnified%2520regions%2520correspond%2520to%2520the%2520lesion%2520area.%2520Finally%252C%250Aa%2520feature%2520consistency%2520constraint%2520between%2520the%2520global%2520and%2520detail%2520branches%2520ensures%250Athat%2520the%2520global%2520branch%2520focuses%2520on%2520the%2520appropriate%2520region%2520and%2520extracts%250Asufficient%2520discriminative%2520features%2520for%2520final%2520identification.%2520These%2520focused%250Adiscriminative%2520features%2520prove%2520invaluable%2520for%2520uncovering%2520novel%2520prognostic%2520tumor%250Amarkers%2520from%2520the%2520perspective%2520of%2520feature%2520cluster%2520uniqueness%2520and%2520tissue%2520spatial%250Adistribution.%2520Extensive%2520experiment%2520results%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520can%2520rapidly%2520deliver%2520accurate%2520and%2520explainable%2520results%2520for%2520pathological%250Agrading%2520and%2520prognosis%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEW%3A%20Self-calibration%20Enhanced%20Whole%20Slide%20Pathology%20Image%20Analysis&entry.906535625=Haoming%20Luo%20and%20Xiaotian%20Yu%20and%20Shengxuming%20Zhang%20and%20Jiabin%20Xia%20and%20Yang%20Jian%20and%20Yuning%20Sun%20and%20Liang%20Xue%20and%20Mingli%20Song%20and%20Jing%20Zhang%20and%20Xiuming%20Zhang%20and%20Zunlei%20Feng&entry.1292438233=%20%20Pathology%20images%20are%20considered%20the%20%60%60gold%20standard%22%20for%20cancer%20diagnosis%20and%0Atreatment%2C%20with%20gigapixel%20images%20providing%20extensive%20tissue%20and%20cellular%0Ainformation.%20Existing%20methods%20fail%20to%20simultaneously%20extract%20global%20structural%0Aand%20local%20detail%20features%20for%20comprehensive%20pathology%20image%20analysis%0Aefficiently.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20self-calibration%0Aenhanced%20framework%20for%20whole%20slide%20pathology%20image%20analysis%2C%20comprising%20three%0Acomponents%3A%20a%20global%20branch%2C%20a%20focus%20predictor%2C%20and%20a%20detailed%20branch.%20The%0Aglobal%20branch%20initially%20classifies%20using%20the%20pathological%20thumbnail%2C%20while%20the%0Afocus%20predictor%20identifies%20relevant%20regions%20for%20classification%20based%20on%20the%0Alast%20layer%20features%20of%20the%20global%20branch.%20The%20detailed%20extraction%20branch%20then%0Aassesses%20whether%20the%20magnified%20regions%20correspond%20to%20the%20lesion%20area.%20Finally%2C%0Aa%20feature%20consistency%20constraint%20between%20the%20global%20and%20detail%20branches%20ensures%0Athat%20the%20global%20branch%20focuses%20on%20the%20appropriate%20region%20and%20extracts%0Asufficient%20discriminative%20features%20for%20final%20identification.%20These%20focused%0Adiscriminative%20features%20prove%20invaluable%20for%20uncovering%20novel%20prognostic%20tumor%0Amarkers%20from%20the%20perspective%20of%20feature%20cluster%20uniqueness%20and%20tissue%20spatial%0Adistribution.%20Extensive%20experiment%20results%20demonstrate%20that%20the%20proposed%0Aframework%20can%20rapidly%20deliver%20accurate%20and%20explainable%20results%20for%20pathological%0Agrading%20and%20prognosis%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10853v2&entry.124074799=Read"},
{"title": "NeuroXVocal: Detection and Explanation of Alzheimer's Disease through\n  Non-invasive Analysis of Picture-prompted Speech", "author": "Nikolaos Ntampakis and Konstantinos Diamantaras and Ioanna Chouvarda and Magda Tsolaki and Vasileios Argyriou and Panagiotis Sarigianndis", "abstract": "  The early diagnosis of Alzheimer's Disease (AD) through non invasive methods\nremains a significant healthcare challenge. We present NeuroXVocal, a novel\ndual-component system that not only classifies but also explains potential AD\ncases through speech analysis. The classification component (Neuro) processes\nthree distinct data streams: acoustic features capturing speech patterns and\nvoice characteristics, textual features extracted from speech transcriptions,\nand precomputed embeddings representing linguistic patterns. These streams are\nfused through a custom transformer-based architecture that enables robust\ncross-modal interactions. The explainability component (XVocal) implements a\nRetrieval-Augmented Generation (RAG) approach, leveraging Large Language Models\ncombined with a domain-specific knowledge base of AD research literature. This\narchitecture enables XVocal to retrieve relevant clinical studies and research\nfindings to generate evidence-based context-sensitive explanations of the\nacoustic and linguistic markers identified in patient speech. Using the IS2021\nADReSSo Challenge benchmark dataset, our system achieved state-of-the-art\nperformance with 95.77% accuracy in AD classification, significantly\noutperforming previous approaches. The explainability component was\nqualitatively evaluated using a structured questionnaire completed by medical\nprofessionals, validating its clinical relevance. NeuroXVocal's unique\ncombination of high-accuracy classification and interpretable,\nliterature-grounded explanations demonstrates its potential as a practical tool\nfor supporting clinical AD diagnosis.\n", "link": "http://arxiv.org/abs/2502.10108v1", "date": "2025-02-14", "relevancy": 2.4779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5043}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroXVocal%3A%20Detection%20and%20Explanation%20of%20Alzheimer%27s%20Disease%20through%0A%20%20Non-invasive%20Analysis%20of%20Picture-prompted%20Speech&body=Title%3A%20NeuroXVocal%3A%20Detection%20and%20Explanation%20of%20Alzheimer%27s%20Disease%20through%0A%20%20Non-invasive%20Analysis%20of%20Picture-prompted%20Speech%0AAuthor%3A%20Nikolaos%20Ntampakis%20and%20Konstantinos%20Diamantaras%20and%20Ioanna%20Chouvarda%20and%20Magda%20Tsolaki%20and%20Vasileios%20Argyriou%20and%20Panagiotis%20Sarigianndis%0AAbstract%3A%20%20%20The%20early%20diagnosis%20of%20Alzheimer%27s%20Disease%20%28AD%29%20through%20non%20invasive%20methods%0Aremains%20a%20significant%20healthcare%20challenge.%20We%20present%20NeuroXVocal%2C%20a%20novel%0Adual-component%20system%20that%20not%20only%20classifies%20but%20also%20explains%20potential%20AD%0Acases%20through%20speech%20analysis.%20The%20classification%20component%20%28Neuro%29%20processes%0Athree%20distinct%20data%20streams%3A%20acoustic%20features%20capturing%20speech%20patterns%20and%0Avoice%20characteristics%2C%20textual%20features%20extracted%20from%20speech%20transcriptions%2C%0Aand%20precomputed%20embeddings%20representing%20linguistic%20patterns.%20These%20streams%20are%0Afused%20through%20a%20custom%20transformer-based%20architecture%20that%20enables%20robust%0Across-modal%20interactions.%20The%20explainability%20component%20%28XVocal%29%20implements%20a%0ARetrieval-Augmented%20Generation%20%28RAG%29%20approach%2C%20leveraging%20Large%20Language%20Models%0Acombined%20with%20a%20domain-specific%20knowledge%20base%20of%20AD%20research%20literature.%20This%0Aarchitecture%20enables%20XVocal%20to%20retrieve%20relevant%20clinical%20studies%20and%20research%0Afindings%20to%20generate%20evidence-based%20context-sensitive%20explanations%20of%20the%0Aacoustic%20and%20linguistic%20markers%20identified%20in%20patient%20speech.%20Using%20the%20IS2021%0AADReSSo%20Challenge%20benchmark%20dataset%2C%20our%20system%20achieved%20state-of-the-art%0Aperformance%20with%2095.77%25%20accuracy%20in%20AD%20classification%2C%20significantly%0Aoutperforming%20previous%20approaches.%20The%20explainability%20component%20was%0Aqualitatively%20evaluated%20using%20a%20structured%20questionnaire%20completed%20by%20medical%0Aprofessionals%2C%20validating%20its%20clinical%20relevance.%20NeuroXVocal%27s%20unique%0Acombination%20of%20high-accuracy%20classification%20and%20interpretable%2C%0Aliterature-grounded%20explanations%20demonstrates%20its%20potential%20as%20a%20practical%20tool%0Afor%20supporting%20clinical%20AD%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroXVocal%253A%2520Detection%2520and%2520Explanation%2520of%2520Alzheimer%2527s%2520Disease%2520through%250A%2520%2520Non-invasive%2520Analysis%2520of%2520Picture-prompted%2520Speech%26entry.906535625%3DNikolaos%2520Ntampakis%2520and%2520Konstantinos%2520Diamantaras%2520and%2520Ioanna%2520Chouvarda%2520and%2520Magda%2520Tsolaki%2520and%2520Vasileios%2520Argyriou%2520and%2520Panagiotis%2520Sarigianndis%26entry.1292438233%3D%2520%2520The%2520early%2520diagnosis%2520of%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520through%2520non%2520invasive%2520methods%250Aremains%2520a%2520significant%2520healthcare%2520challenge.%2520We%2520present%2520NeuroXVocal%252C%2520a%2520novel%250Adual-component%2520system%2520that%2520not%2520only%2520classifies%2520but%2520also%2520explains%2520potential%2520AD%250Acases%2520through%2520speech%2520analysis.%2520The%2520classification%2520component%2520%2528Neuro%2529%2520processes%250Athree%2520distinct%2520data%2520streams%253A%2520acoustic%2520features%2520capturing%2520speech%2520patterns%2520and%250Avoice%2520characteristics%252C%2520textual%2520features%2520extracted%2520from%2520speech%2520transcriptions%252C%250Aand%2520precomputed%2520embeddings%2520representing%2520linguistic%2520patterns.%2520These%2520streams%2520are%250Afused%2520through%2520a%2520custom%2520transformer-based%2520architecture%2520that%2520enables%2520robust%250Across-modal%2520interactions.%2520The%2520explainability%2520component%2520%2528XVocal%2529%2520implements%2520a%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520approach%252C%2520leveraging%2520Large%2520Language%2520Models%250Acombined%2520with%2520a%2520domain-specific%2520knowledge%2520base%2520of%2520AD%2520research%2520literature.%2520This%250Aarchitecture%2520enables%2520XVocal%2520to%2520retrieve%2520relevant%2520clinical%2520studies%2520and%2520research%250Afindings%2520to%2520generate%2520evidence-based%2520context-sensitive%2520explanations%2520of%2520the%250Aacoustic%2520and%2520linguistic%2520markers%2520identified%2520in%2520patient%2520speech.%2520Using%2520the%2520IS2021%250AADReSSo%2520Challenge%2520benchmark%2520dataset%252C%2520our%2520system%2520achieved%2520state-of-the-art%250Aperformance%2520with%252095.77%2525%2520accuracy%2520in%2520AD%2520classification%252C%2520significantly%250Aoutperforming%2520previous%2520approaches.%2520The%2520explainability%2520component%2520was%250Aqualitatively%2520evaluated%2520using%2520a%2520structured%2520questionnaire%2520completed%2520by%2520medical%250Aprofessionals%252C%2520validating%2520its%2520clinical%2520relevance.%2520NeuroXVocal%2527s%2520unique%250Acombination%2520of%2520high-accuracy%2520classification%2520and%2520interpretable%252C%250Aliterature-grounded%2520explanations%2520demonstrates%2520its%2520potential%2520as%2520a%2520practical%2520tool%250Afor%2520supporting%2520clinical%2520AD%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroXVocal%3A%20Detection%20and%20Explanation%20of%20Alzheimer%27s%20Disease%20through%0A%20%20Non-invasive%20Analysis%20of%20Picture-prompted%20Speech&entry.906535625=Nikolaos%20Ntampakis%20and%20Konstantinos%20Diamantaras%20and%20Ioanna%20Chouvarda%20and%20Magda%20Tsolaki%20and%20Vasileios%20Argyriou%20and%20Panagiotis%20Sarigianndis&entry.1292438233=%20%20The%20early%20diagnosis%20of%20Alzheimer%27s%20Disease%20%28AD%29%20through%20non%20invasive%20methods%0Aremains%20a%20significant%20healthcare%20challenge.%20We%20present%20NeuroXVocal%2C%20a%20novel%0Adual-component%20system%20that%20not%20only%20classifies%20but%20also%20explains%20potential%20AD%0Acases%20through%20speech%20analysis.%20The%20classification%20component%20%28Neuro%29%20processes%0Athree%20distinct%20data%20streams%3A%20acoustic%20features%20capturing%20speech%20patterns%20and%0Avoice%20characteristics%2C%20textual%20features%20extracted%20from%20speech%20transcriptions%2C%0Aand%20precomputed%20embeddings%20representing%20linguistic%20patterns.%20These%20streams%20are%0Afused%20through%20a%20custom%20transformer-based%20architecture%20that%20enables%20robust%0Across-modal%20interactions.%20The%20explainability%20component%20%28XVocal%29%20implements%20a%0ARetrieval-Augmented%20Generation%20%28RAG%29%20approach%2C%20leveraging%20Large%20Language%20Models%0Acombined%20with%20a%20domain-specific%20knowledge%20base%20of%20AD%20research%20literature.%20This%0Aarchitecture%20enables%20XVocal%20to%20retrieve%20relevant%20clinical%20studies%20and%20research%0Afindings%20to%20generate%20evidence-based%20context-sensitive%20explanations%20of%20the%0Aacoustic%20and%20linguistic%20markers%20identified%20in%20patient%20speech.%20Using%20the%20IS2021%0AADReSSo%20Challenge%20benchmark%20dataset%2C%20our%20system%20achieved%20state-of-the-art%0Aperformance%20with%2095.77%25%20accuracy%20in%20AD%20classification%2C%20significantly%0Aoutperforming%20previous%20approaches.%20The%20explainability%20component%20was%0Aqualitatively%20evaluated%20using%20a%20structured%20questionnaire%20completed%20by%20medical%0Aprofessionals%2C%20validating%20its%20clinical%20relevance.%20NeuroXVocal%27s%20unique%0Acombination%20of%20high-accuracy%20classification%20and%20interpretable%2C%0Aliterature-grounded%20explanations%20demonstrates%20its%20potential%20as%20a%20practical%20tool%0Afor%20supporting%20clinical%20AD%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10108v1&entry.124074799=Read"},
{"title": "Learning Relational Tabular Data without Shared Features", "author": "Zhaomin Wu and Shida Wang and Ziyang Wang and Bingsheng He", "abstract": "  Learning relational tabular data has gained significant attention recently,\nbut most studies focus on single tables, overlooking the potential of\ncross-table learning. Cross-table learning, especially in scenarios where\ntables lack shared features and pre-aligned data, offers vast opportunities but\nalso introduces substantial challenges. The alignment space is immense, and\ndetermining accurate alignments between tables is highly complex. We propose\nLatent Entity Alignment Learning (Leal), a novel framework enabling effective\ncross-table training without requiring shared features or pre-aligned data.\nLeal operates on the principle that properly aligned data yield lower loss than\nmisaligned data, a concept embodied in its soft alignment mechanism. This\nmechanism is coupled with a differentiable cluster sampler module, ensuring\nefficient scaling to large relational tables. Furthermore, we provide a\ntheoretical proof of the cluster sampler's approximation capacity. Extensive\nexperiments on five real-world and five synthetic datasets show that Leal\nachieves up to a 26.8% improvement in predictive performance compared to\nstate-of-the-art methods, demonstrating its effectiveness and scalability.\n", "link": "http://arxiv.org/abs/2502.10125v1", "date": "2025-02-14", "relevancy": 2.4582, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4951}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Relational%20Tabular%20Data%20without%20Shared%20Features&body=Title%3A%20Learning%20Relational%20Tabular%20Data%20without%20Shared%20Features%0AAuthor%3A%20Zhaomin%20Wu%20and%20Shida%20Wang%20and%20Ziyang%20Wang%20and%20Bingsheng%20He%0AAbstract%3A%20%20%20Learning%20relational%20tabular%20data%20has%20gained%20significant%20attention%20recently%2C%0Abut%20most%20studies%20focus%20on%20single%20tables%2C%20overlooking%20the%20potential%20of%0Across-table%20learning.%20Cross-table%20learning%2C%20especially%20in%20scenarios%20where%0Atables%20lack%20shared%20features%20and%20pre-aligned%20data%2C%20offers%20vast%20opportunities%20but%0Aalso%20introduces%20substantial%20challenges.%20The%20alignment%20space%20is%20immense%2C%20and%0Adetermining%20accurate%20alignments%20between%20tables%20is%20highly%20complex.%20We%20propose%0ALatent%20Entity%20Alignment%20Learning%20%28Leal%29%2C%20a%20novel%20framework%20enabling%20effective%0Across-table%20training%20without%20requiring%20shared%20features%20or%20pre-aligned%20data.%0ALeal%20operates%20on%20the%20principle%20that%20properly%20aligned%20data%20yield%20lower%20loss%20than%0Amisaligned%20data%2C%20a%20concept%20embodied%20in%20its%20soft%20alignment%20mechanism.%20This%0Amechanism%20is%20coupled%20with%20a%20differentiable%20cluster%20sampler%20module%2C%20ensuring%0Aefficient%20scaling%20to%20large%20relational%20tables.%20Furthermore%2C%20we%20provide%20a%0Atheoretical%20proof%20of%20the%20cluster%20sampler%27s%20approximation%20capacity.%20Extensive%0Aexperiments%20on%20five%20real-world%20and%20five%20synthetic%20datasets%20show%20that%20Leal%0Aachieves%20up%20to%20a%2026.8%25%20improvement%20in%20predictive%20performance%20compared%20to%0Astate-of-the-art%20methods%2C%20demonstrating%20its%20effectiveness%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Relational%2520Tabular%2520Data%2520without%2520Shared%2520Features%26entry.906535625%3DZhaomin%2520Wu%2520and%2520Shida%2520Wang%2520and%2520Ziyang%2520Wang%2520and%2520Bingsheng%2520He%26entry.1292438233%3D%2520%2520Learning%2520relational%2520tabular%2520data%2520has%2520gained%2520significant%2520attention%2520recently%252C%250Abut%2520most%2520studies%2520focus%2520on%2520single%2520tables%252C%2520overlooking%2520the%2520potential%2520of%250Across-table%2520learning.%2520Cross-table%2520learning%252C%2520especially%2520in%2520scenarios%2520where%250Atables%2520lack%2520shared%2520features%2520and%2520pre-aligned%2520data%252C%2520offers%2520vast%2520opportunities%2520but%250Aalso%2520introduces%2520substantial%2520challenges.%2520The%2520alignment%2520space%2520is%2520immense%252C%2520and%250Adetermining%2520accurate%2520alignments%2520between%2520tables%2520is%2520highly%2520complex.%2520We%2520propose%250ALatent%2520Entity%2520Alignment%2520Learning%2520%2528Leal%2529%252C%2520a%2520novel%2520framework%2520enabling%2520effective%250Across-table%2520training%2520without%2520requiring%2520shared%2520features%2520or%2520pre-aligned%2520data.%250ALeal%2520operates%2520on%2520the%2520principle%2520that%2520properly%2520aligned%2520data%2520yield%2520lower%2520loss%2520than%250Amisaligned%2520data%252C%2520a%2520concept%2520embodied%2520in%2520its%2520soft%2520alignment%2520mechanism.%2520This%250Amechanism%2520is%2520coupled%2520with%2520a%2520differentiable%2520cluster%2520sampler%2520module%252C%2520ensuring%250Aefficient%2520scaling%2520to%2520large%2520relational%2520tables.%2520Furthermore%252C%2520we%2520provide%2520a%250Atheoretical%2520proof%2520of%2520the%2520cluster%2520sampler%2527s%2520approximation%2520capacity.%2520Extensive%250Aexperiments%2520on%2520five%2520real-world%2520and%2520five%2520synthetic%2520datasets%2520show%2520that%2520Leal%250Aachieves%2520up%2520to%2520a%252026.8%2525%2520improvement%2520in%2520predictive%2520performance%2520compared%2520to%250Astate-of-the-art%2520methods%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Relational%20Tabular%20Data%20without%20Shared%20Features&entry.906535625=Zhaomin%20Wu%20and%20Shida%20Wang%20and%20Ziyang%20Wang%20and%20Bingsheng%20He&entry.1292438233=%20%20Learning%20relational%20tabular%20data%20has%20gained%20significant%20attention%20recently%2C%0Abut%20most%20studies%20focus%20on%20single%20tables%2C%20overlooking%20the%20potential%20of%0Across-table%20learning.%20Cross-table%20learning%2C%20especially%20in%20scenarios%20where%0Atables%20lack%20shared%20features%20and%20pre-aligned%20data%2C%20offers%20vast%20opportunities%20but%0Aalso%20introduces%20substantial%20challenges.%20The%20alignment%20space%20is%20immense%2C%20and%0Adetermining%20accurate%20alignments%20between%20tables%20is%20highly%20complex.%20We%20propose%0ALatent%20Entity%20Alignment%20Learning%20%28Leal%29%2C%20a%20novel%20framework%20enabling%20effective%0Across-table%20training%20without%20requiring%20shared%20features%20or%20pre-aligned%20data.%0ALeal%20operates%20on%20the%20principle%20that%20properly%20aligned%20data%20yield%20lower%20loss%20than%0Amisaligned%20data%2C%20a%20concept%20embodied%20in%20its%20soft%20alignment%20mechanism.%20This%0Amechanism%20is%20coupled%20with%20a%20differentiable%20cluster%20sampler%20module%2C%20ensuring%0Aefficient%20scaling%20to%20large%20relational%20tables.%20Furthermore%2C%20we%20provide%20a%0Atheoretical%20proof%20of%20the%20cluster%20sampler%27s%20approximation%20capacity.%20Extensive%0Aexperiments%20on%20five%20real-world%20and%20five%20synthetic%20datasets%20show%20that%20Leal%0Aachieves%20up%20to%20a%2026.8%25%20improvement%20in%20predictive%20performance%20compared%20to%0Astate-of-the-art%20methods%2C%20demonstrating%20its%20effectiveness%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10125v1&entry.124074799=Read"},
{"title": "VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated\n  Learning", "author": "Zilong Zhao and Han Wu and Aad Van Moorsel and Lydia Y. Chen", "abstract": "  This paper presents the application of Vertical Federated Learning (VFL) to\ngenerate synthetic tabular data using Generative Adversarial Networks (GANs).\nVFL is a collaborative approach to train machine learning models among distinct\ntabular data holders, such as financial institutions, who possess disjoint\nfeatures for the same group of customers. In this paper we introduce the VT-GAN\nframework, Vertical federated Tabular GAN, and demonstrate that VFL can be\nsuccessfully used to implement GANs for distributed tabular data in\nprivacy-preserving manner, with performance close to centralized GANs that\nassume shared data. We make design choices with respect to the distribution of\nGAN generator and discriminator models and introduce a training-with-shuffling\ntechnique so that no party can reconstruct training data from the GAN\nconditional vector. The paper presents (1) an implementation of VT-GAN, (2) a\ndetailed quality evaluation of the VT-GAN-generated synthetic data, (3) an\noverall scalability examination of VT-GAN framework, (4) a security analysis on\nVT-GAN's robustness against Membership Inference Attack with different settings\nof Differential Privacy, for a range of datasets with diverse distribution\ncharacteristics. Our results demonstrate that VT-GAN can consistently generate\nhigh-fidelity synthetic tabular data of comparable quality to that generated by\na centralized GAN algorithm. The difference in machine learning utility can be\nas low as 2.7%, even under extremely imbalanced data distributions across\nclients or with different numbers of clients.\n", "link": "http://arxiv.org/abs/2302.01706v2", "date": "2025-02-14", "relevancy": 2.4579, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4939}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4938}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VT-GAN%3A%20Cooperative%20Tabular%20Data%20Synthesis%20using%20Vertical%20Federated%0A%20%20Learning&body=Title%3A%20VT-GAN%3A%20Cooperative%20Tabular%20Data%20Synthesis%20using%20Vertical%20Federated%0A%20%20Learning%0AAuthor%3A%20Zilong%20Zhao%20and%20Han%20Wu%20and%20Aad%20Van%20Moorsel%20and%20Lydia%20Y.%20Chen%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20application%20of%20Vertical%20Federated%20Learning%20%28VFL%29%20to%0Agenerate%20synthetic%20tabular%20data%20using%20Generative%20Adversarial%20Networks%20%28GANs%29.%0AVFL%20is%20a%20collaborative%20approach%20to%20train%20machine%20learning%20models%20among%20distinct%0Atabular%20data%20holders%2C%20such%20as%20financial%20institutions%2C%20who%20possess%20disjoint%0Afeatures%20for%20the%20same%20group%20of%20customers.%20In%20this%20paper%20we%20introduce%20the%20VT-GAN%0Aframework%2C%20Vertical%20federated%20Tabular%20GAN%2C%20and%20demonstrate%20that%20VFL%20can%20be%0Asuccessfully%20used%20to%20implement%20GANs%20for%20distributed%20tabular%20data%20in%0Aprivacy-preserving%20manner%2C%20with%20performance%20close%20to%20centralized%20GANs%20that%0Aassume%20shared%20data.%20We%20make%20design%20choices%20with%20respect%20to%20the%20distribution%20of%0AGAN%20generator%20and%20discriminator%20models%20and%20introduce%20a%20training-with-shuffling%0Atechnique%20so%20that%20no%20party%20can%20reconstruct%20training%20data%20from%20the%20GAN%0Aconditional%20vector.%20The%20paper%20presents%20%281%29%20an%20implementation%20of%20VT-GAN%2C%20%282%29%20a%0Adetailed%20quality%20evaluation%20of%20the%20VT-GAN-generated%20synthetic%20data%2C%20%283%29%20an%0Aoverall%20scalability%20examination%20of%20VT-GAN%20framework%2C%20%284%29%20a%20security%20analysis%20on%0AVT-GAN%27s%20robustness%20against%20Membership%20Inference%20Attack%20with%20different%20settings%0Aof%20Differential%20Privacy%2C%20for%20a%20range%20of%20datasets%20with%20diverse%20distribution%0Acharacteristics.%20Our%20results%20demonstrate%20that%20VT-GAN%20can%20consistently%20generate%0Ahigh-fidelity%20synthetic%20tabular%20data%20of%20comparable%20quality%20to%20that%20generated%20by%0Aa%20centralized%20GAN%20algorithm.%20The%20difference%20in%20machine%20learning%20utility%20can%20be%0Aas%20low%20as%202.7%25%2C%20even%20under%20extremely%20imbalanced%20data%20distributions%20across%0Aclients%20or%20with%20different%20numbers%20of%20clients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.01706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVT-GAN%253A%2520Cooperative%2520Tabular%2520Data%2520Synthesis%2520using%2520Vertical%2520Federated%250A%2520%2520Learning%26entry.906535625%3DZilong%2520Zhao%2520and%2520Han%2520Wu%2520and%2520Aad%2520Van%2520Moorsel%2520and%2520Lydia%2520Y.%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520application%2520of%2520Vertical%2520Federated%2520Learning%2520%2528VFL%2529%2520to%250Agenerate%2520synthetic%2520tabular%2520data%2520using%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529.%250AVFL%2520is%2520a%2520collaborative%2520approach%2520to%2520train%2520machine%2520learning%2520models%2520among%2520distinct%250Atabular%2520data%2520holders%252C%2520such%2520as%2520financial%2520institutions%252C%2520who%2520possess%2520disjoint%250Afeatures%2520for%2520the%2520same%2520group%2520of%2520customers.%2520In%2520this%2520paper%2520we%2520introduce%2520the%2520VT-GAN%250Aframework%252C%2520Vertical%2520federated%2520Tabular%2520GAN%252C%2520and%2520demonstrate%2520that%2520VFL%2520can%2520be%250Asuccessfully%2520used%2520to%2520implement%2520GANs%2520for%2520distributed%2520tabular%2520data%2520in%250Aprivacy-preserving%2520manner%252C%2520with%2520performance%2520close%2520to%2520centralized%2520GANs%2520that%250Aassume%2520shared%2520data.%2520We%2520make%2520design%2520choices%2520with%2520respect%2520to%2520the%2520distribution%2520of%250AGAN%2520generator%2520and%2520discriminator%2520models%2520and%2520introduce%2520a%2520training-with-shuffling%250Atechnique%2520so%2520that%2520no%2520party%2520can%2520reconstruct%2520training%2520data%2520from%2520the%2520GAN%250Aconditional%2520vector.%2520The%2520paper%2520presents%2520%25281%2529%2520an%2520implementation%2520of%2520VT-GAN%252C%2520%25282%2529%2520a%250Adetailed%2520quality%2520evaluation%2520of%2520the%2520VT-GAN-generated%2520synthetic%2520data%252C%2520%25283%2529%2520an%250Aoverall%2520scalability%2520examination%2520of%2520VT-GAN%2520framework%252C%2520%25284%2529%2520a%2520security%2520analysis%2520on%250AVT-GAN%2527s%2520robustness%2520against%2520Membership%2520Inference%2520Attack%2520with%2520different%2520settings%250Aof%2520Differential%2520Privacy%252C%2520for%2520a%2520range%2520of%2520datasets%2520with%2520diverse%2520distribution%250Acharacteristics.%2520Our%2520results%2520demonstrate%2520that%2520VT-GAN%2520can%2520consistently%2520generate%250Ahigh-fidelity%2520synthetic%2520tabular%2520data%2520of%2520comparable%2520quality%2520to%2520that%2520generated%2520by%250Aa%2520centralized%2520GAN%2520algorithm.%2520The%2520difference%2520in%2520machine%2520learning%2520utility%2520can%2520be%250Aas%2520low%2520as%25202.7%2525%252C%2520even%2520under%2520extremely%2520imbalanced%2520data%2520distributions%2520across%250Aclients%2520or%2520with%2520different%2520numbers%2520of%2520clients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.01706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VT-GAN%3A%20Cooperative%20Tabular%20Data%20Synthesis%20using%20Vertical%20Federated%0A%20%20Learning&entry.906535625=Zilong%20Zhao%20and%20Han%20Wu%20and%20Aad%20Van%20Moorsel%20and%20Lydia%20Y.%20Chen&entry.1292438233=%20%20This%20paper%20presents%20the%20application%20of%20Vertical%20Federated%20Learning%20%28VFL%29%20to%0Agenerate%20synthetic%20tabular%20data%20using%20Generative%20Adversarial%20Networks%20%28GANs%29.%0AVFL%20is%20a%20collaborative%20approach%20to%20train%20machine%20learning%20models%20among%20distinct%0Atabular%20data%20holders%2C%20such%20as%20financial%20institutions%2C%20who%20possess%20disjoint%0Afeatures%20for%20the%20same%20group%20of%20customers.%20In%20this%20paper%20we%20introduce%20the%20VT-GAN%0Aframework%2C%20Vertical%20federated%20Tabular%20GAN%2C%20and%20demonstrate%20that%20VFL%20can%20be%0Asuccessfully%20used%20to%20implement%20GANs%20for%20distributed%20tabular%20data%20in%0Aprivacy-preserving%20manner%2C%20with%20performance%20close%20to%20centralized%20GANs%20that%0Aassume%20shared%20data.%20We%20make%20design%20choices%20with%20respect%20to%20the%20distribution%20of%0AGAN%20generator%20and%20discriminator%20models%20and%20introduce%20a%20training-with-shuffling%0Atechnique%20so%20that%20no%20party%20can%20reconstruct%20training%20data%20from%20the%20GAN%0Aconditional%20vector.%20The%20paper%20presents%20%281%29%20an%20implementation%20of%20VT-GAN%2C%20%282%29%20a%0Adetailed%20quality%20evaluation%20of%20the%20VT-GAN-generated%20synthetic%20data%2C%20%283%29%20an%0Aoverall%20scalability%20examination%20of%20VT-GAN%20framework%2C%20%284%29%20a%20security%20analysis%20on%0AVT-GAN%27s%20robustness%20against%20Membership%20Inference%20Attack%20with%20different%20settings%0Aof%20Differential%20Privacy%2C%20for%20a%20range%20of%20datasets%20with%20diverse%20distribution%0Acharacteristics.%20Our%20results%20demonstrate%20that%20VT-GAN%20can%20consistently%20generate%0Ahigh-fidelity%20synthetic%20tabular%20data%20of%20comparable%20quality%20to%20that%20generated%20by%0Aa%20centralized%20GAN%20algorithm.%20The%20difference%20in%20machine%20learning%20utility%20can%20be%0Aas%20low%20as%202.7%25%2C%20even%20under%20extremely%20imbalanced%20data%20distributions%20across%0Aclients%20or%20with%20different%20numbers%20of%20clients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01706v2&entry.124074799=Read"},
{"title": "Occupancy-SLAM: An Efficient and Robust Algorithm for Simultaneously\n  Optimizing Robot Poses and Occupancy Map", "author": "Yingyu Wang and Liang Zhao and Shoudong Huang", "abstract": "  Joint optimization of poses and features has been extensively studied and\ndemonstrated to yield more accurate results in feature-based SLAM problems.\nHowever, research on jointly optimizing poses and non-feature-based maps\nremains limited. Occupancy maps are widely used non-feature-based environment\nrepresentations because they effectively classify spaces into obstacles, free\nareas, and unknown regions, providing robots with spatial information for\nvarious tasks. In this paper, we propose Occupancy-SLAM, a novel\noptimization-based SLAM method that enables the joint optimization of robot\ntrajectory and the occupancy map through a parameterized map representation.\nThe key novelty lies in optimizing both robot poses and occupancy values at\ndifferent cell vertices simultaneously, a significant departure from existing\nmethods where the robot poses need to be optimized first before the map can be\nestimated. Evaluations using simulations and practical 2D laser datasets\ndemonstrate that the proposed approach can robustly obtain more accurate robot\ntrajectories and occupancy maps than state-of-the-art techniques with\ncomparable computational time. Preliminary results in the 3D case further\nconfirm the potential of the proposed method in practical 3D applications,\nachieving more accurate results than existing methods.\n", "link": "http://arxiv.org/abs/2502.06292v2", "date": "2025-02-14", "relevancy": 2.4252, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6174}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occupancy-SLAM%3A%20An%20Efficient%20and%20Robust%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Robot%20Poses%20and%20Occupancy%20Map&body=Title%3A%20Occupancy-SLAM%3A%20An%20Efficient%20and%20Robust%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Robot%20Poses%20and%20Occupancy%20Map%0AAuthor%3A%20Yingyu%20Wang%20and%20Liang%20Zhao%20and%20Shoudong%20Huang%0AAbstract%3A%20%20%20Joint%20optimization%20of%20poses%20and%20features%20has%20been%20extensively%20studied%20and%0Ademonstrated%20to%20yield%20more%20accurate%20results%20in%20feature-based%20SLAM%20problems.%0AHowever%2C%20research%20on%20jointly%20optimizing%20poses%20and%20non-feature-based%20maps%0Aremains%20limited.%20Occupancy%20maps%20are%20widely%20used%20non-feature-based%20environment%0Arepresentations%20because%20they%20effectively%20classify%20spaces%20into%20obstacles%2C%20free%0Aareas%2C%20and%20unknown%20regions%2C%20providing%20robots%20with%20spatial%20information%20for%0Avarious%20tasks.%20In%20this%20paper%2C%20we%20propose%20Occupancy-SLAM%2C%20a%20novel%0Aoptimization-based%20SLAM%20method%20that%20enables%20the%20joint%20optimization%20of%20robot%0Atrajectory%20and%20the%20occupancy%20map%20through%20a%20parameterized%20map%20representation.%0AThe%20key%20novelty%20lies%20in%20optimizing%20both%20robot%20poses%20and%20occupancy%20values%20at%0Adifferent%20cell%20vertices%20simultaneously%2C%20a%20significant%20departure%20from%20existing%0Amethods%20where%20the%20robot%20poses%20need%20to%20be%20optimized%20first%20before%20the%20map%20can%20be%0Aestimated.%20Evaluations%20using%20simulations%20and%20practical%202D%20laser%20datasets%0Ademonstrate%20that%20the%20proposed%20approach%20can%20robustly%20obtain%20more%20accurate%20robot%0Atrajectories%20and%20occupancy%20maps%20than%20state-of-the-art%20techniques%20with%0Acomparable%20computational%20time.%20Preliminary%20results%20in%20the%203D%20case%20further%0Aconfirm%20the%20potential%20of%20the%20proposed%20method%20in%20practical%203D%20applications%2C%0Aachieving%20more%20accurate%20results%20than%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccupancy-SLAM%253A%2520An%2520Efficient%2520and%2520Robust%2520Algorithm%2520for%2520Simultaneously%250A%2520%2520Optimizing%2520Robot%2520Poses%2520and%2520Occupancy%2520Map%26entry.906535625%3DYingyu%2520Wang%2520and%2520Liang%2520Zhao%2520and%2520Shoudong%2520Huang%26entry.1292438233%3D%2520%2520Joint%2520optimization%2520of%2520poses%2520and%2520features%2520has%2520been%2520extensively%2520studied%2520and%250Ademonstrated%2520to%2520yield%2520more%2520accurate%2520results%2520in%2520feature-based%2520SLAM%2520problems.%250AHowever%252C%2520research%2520on%2520jointly%2520optimizing%2520poses%2520and%2520non-feature-based%2520maps%250Aremains%2520limited.%2520Occupancy%2520maps%2520are%2520widely%2520used%2520non-feature-based%2520environment%250Arepresentations%2520because%2520they%2520effectively%2520classify%2520spaces%2520into%2520obstacles%252C%2520free%250Aareas%252C%2520and%2520unknown%2520regions%252C%2520providing%2520robots%2520with%2520spatial%2520information%2520for%250Avarious%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Occupancy-SLAM%252C%2520a%2520novel%250Aoptimization-based%2520SLAM%2520method%2520that%2520enables%2520the%2520joint%2520optimization%2520of%2520robot%250Atrajectory%2520and%2520the%2520occupancy%2520map%2520through%2520a%2520parameterized%2520map%2520representation.%250AThe%2520key%2520novelty%2520lies%2520in%2520optimizing%2520both%2520robot%2520poses%2520and%2520occupancy%2520values%2520at%250Adifferent%2520cell%2520vertices%2520simultaneously%252C%2520a%2520significant%2520departure%2520from%2520existing%250Amethods%2520where%2520the%2520robot%2520poses%2520need%2520to%2520be%2520optimized%2520first%2520before%2520the%2520map%2520can%2520be%250Aestimated.%2520Evaluations%2520using%2520simulations%2520and%2520practical%25202D%2520laser%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520can%2520robustly%2520obtain%2520more%2520accurate%2520robot%250Atrajectories%2520and%2520occupancy%2520maps%2520than%2520state-of-the-art%2520techniques%2520with%250Acomparable%2520computational%2520time.%2520Preliminary%2520results%2520in%2520the%25203D%2520case%2520further%250Aconfirm%2520the%2520potential%2520of%2520the%2520proposed%2520method%2520in%2520practical%25203D%2520applications%252C%250Aachieving%2520more%2520accurate%2520results%2520than%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occupancy-SLAM%3A%20An%20Efficient%20and%20Robust%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Robot%20Poses%20and%20Occupancy%20Map&entry.906535625=Yingyu%20Wang%20and%20Liang%20Zhao%20and%20Shoudong%20Huang&entry.1292438233=%20%20Joint%20optimization%20of%20poses%20and%20features%20has%20been%20extensively%20studied%20and%0Ademonstrated%20to%20yield%20more%20accurate%20results%20in%20feature-based%20SLAM%20problems.%0AHowever%2C%20research%20on%20jointly%20optimizing%20poses%20and%20non-feature-based%20maps%0Aremains%20limited.%20Occupancy%20maps%20are%20widely%20used%20non-feature-based%20environment%0Arepresentations%20because%20they%20effectively%20classify%20spaces%20into%20obstacles%2C%20free%0Aareas%2C%20and%20unknown%20regions%2C%20providing%20robots%20with%20spatial%20information%20for%0Avarious%20tasks.%20In%20this%20paper%2C%20we%20propose%20Occupancy-SLAM%2C%20a%20novel%0Aoptimization-based%20SLAM%20method%20that%20enables%20the%20joint%20optimization%20of%20robot%0Atrajectory%20and%20the%20occupancy%20map%20through%20a%20parameterized%20map%20representation.%0AThe%20key%20novelty%20lies%20in%20optimizing%20both%20robot%20poses%20and%20occupancy%20values%20at%0Adifferent%20cell%20vertices%20simultaneously%2C%20a%20significant%20departure%20from%20existing%0Amethods%20where%20the%20robot%20poses%20need%20to%20be%20optimized%20first%20before%20the%20map%20can%20be%0Aestimated.%20Evaluations%20using%20simulations%20and%20practical%202D%20laser%20datasets%0Ademonstrate%20that%20the%20proposed%20approach%20can%20robustly%20obtain%20more%20accurate%20robot%0Atrajectories%20and%20occupancy%20maps%20than%20state-of-the-art%20techniques%20with%0Acomparable%20computational%20time.%20Preliminary%20results%20in%20the%203D%20case%20further%0Aconfirm%20the%20potential%20of%20the%20proposed%20method%20in%20practical%203D%20applications%2C%0Aachieving%20more%20accurate%20results%20than%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06292v2&entry.124074799=Read"},
{"title": "MITO: Enabling Non-Line-of-Sight Perception using Millimeter-waves\n  through Real-World Datasets and Simulation Tools", "author": "Laura Dodds and Tara Boroushaki and Fadel Adib", "abstract": "  We present MITO, the first dataset of multi-spectral millimeter-wave (mmWave)\nimages of everyday objects. Unlike visible light, mmWave signals can image\nthrough everyday occlusions (e.g., cardboard boxes, fabric, plastic). However,\ndue to the dearth of publicly-available mmWave images and the interdisciplinary\nchallenges in collecting and processing mmWave signals, it remains difficult\ntoday for computer vision researchers to develop mmWave-based non-line-of-sight\nperception algorithms and models.\n  To overcome these challenges, we introduce a real-world dataset and\nopen-source simulation tool for mmWave imaging. The dataset is acquired using a\nUR5 robotic arm with two mmWave radars operating at different frequencies and\nan RGB-D camera. Through a signal processing pipeline, we capture and create\nover 580 real-world 3D mmWave images from over 76 different objects in the YCB\ndataset, a standard dataset for robotics manipulation. We provide real-world\nmmWave images in line-of-sight and non-line-of-sight, as well as RGB-D images\nand ground truth segmentation masks. We also develop an open-source simulation\ntool that can be used to generate synthetic mmWave images for any 3D triangle\nmesh, which achieves a median F-Score of 94% when compared to real-world mmWave\nimages.\n  We show the usefulness of this dataset and simulation tool in multiple CV\ntasks in non-line-of-sight. First, we perform object segmentation for mmWave\nimages using the segment anything model (SAM), and achieve a median precision\nand recall of 92.6% and 64%. Second, we train a classifier that can recognize\nobjects in non-line-of-sight. It is trained on synthetic images and can\nclassify real-world images with 85% accuracy.\n  We believe MITO will be a valuable resource for computer vision researchers\nin developing non-line-of-sight perception, similar to how early camera-based\ndatasets shaped the field.\n", "link": "http://arxiv.org/abs/2502.10259v1", "date": "2025-02-14", "relevancy": 2.4126, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6079}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MITO%3A%20Enabling%20Non-Line-of-Sight%20Perception%20using%20Millimeter-waves%0A%20%20through%20Real-World%20Datasets%20and%20Simulation%20Tools&body=Title%3A%20MITO%3A%20Enabling%20Non-Line-of-Sight%20Perception%20using%20Millimeter-waves%0A%20%20through%20Real-World%20Datasets%20and%20Simulation%20Tools%0AAuthor%3A%20Laura%20Dodds%20and%20Tara%20Boroushaki%20and%20Fadel%20Adib%0AAbstract%3A%20%20%20We%20present%20MITO%2C%20the%20first%20dataset%20of%20multi-spectral%20millimeter-wave%20%28mmWave%29%0Aimages%20of%20everyday%20objects.%20Unlike%20visible%20light%2C%20mmWave%20signals%20can%20image%0Athrough%20everyday%20occlusions%20%28e.g.%2C%20cardboard%20boxes%2C%20fabric%2C%20plastic%29.%20However%2C%0Adue%20to%20the%20dearth%20of%20publicly-available%20mmWave%20images%20and%20the%20interdisciplinary%0Achallenges%20in%20collecting%20and%20processing%20mmWave%20signals%2C%20it%20remains%20difficult%0Atoday%20for%20computer%20vision%20researchers%20to%20develop%20mmWave-based%20non-line-of-sight%0Aperception%20algorithms%20and%20models.%0A%20%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20real-world%20dataset%20and%0Aopen-source%20simulation%20tool%20for%20mmWave%20imaging.%20The%20dataset%20is%20acquired%20using%20a%0AUR5%20robotic%20arm%20with%20two%20mmWave%20radars%20operating%20at%20different%20frequencies%20and%0Aan%20RGB-D%20camera.%20Through%20a%20signal%20processing%20pipeline%2C%20we%20capture%20and%20create%0Aover%20580%20real-world%203D%20mmWave%20images%20from%20over%2076%20different%20objects%20in%20the%20YCB%0Adataset%2C%20a%20standard%20dataset%20for%20robotics%20manipulation.%20We%20provide%20real-world%0AmmWave%20images%20in%20line-of-sight%20and%20non-line-of-sight%2C%20as%20well%20as%20RGB-D%20images%0Aand%20ground%20truth%20segmentation%20masks.%20We%20also%20develop%20an%20open-source%20simulation%0Atool%20that%20can%20be%20used%20to%20generate%20synthetic%20mmWave%20images%20for%20any%203D%20triangle%0Amesh%2C%20which%20achieves%20a%20median%20F-Score%20of%2094%25%20when%20compared%20to%20real-world%20mmWave%0Aimages.%0A%20%20We%20show%20the%20usefulness%20of%20this%20dataset%20and%20simulation%20tool%20in%20multiple%20CV%0Atasks%20in%20non-line-of-sight.%20First%2C%20we%20perform%20object%20segmentation%20for%20mmWave%0Aimages%20using%20the%20segment%20anything%20model%20%28SAM%29%2C%20and%20achieve%20a%20median%20precision%0Aand%20recall%20of%2092.6%25%20and%2064%25.%20Second%2C%20we%20train%20a%20classifier%20that%20can%20recognize%0Aobjects%20in%20non-line-of-sight.%20It%20is%20trained%20on%20synthetic%20images%20and%20can%0Aclassify%20real-world%20images%20with%2085%25%20accuracy.%0A%20%20We%20believe%20MITO%20will%20be%20a%20valuable%20resource%20for%20computer%20vision%20researchers%0Ain%20developing%20non-line-of-sight%20perception%2C%20similar%20to%20how%20early%20camera-based%0Adatasets%20shaped%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMITO%253A%2520Enabling%2520Non-Line-of-Sight%2520Perception%2520using%2520Millimeter-waves%250A%2520%2520through%2520Real-World%2520Datasets%2520and%2520Simulation%2520Tools%26entry.906535625%3DLaura%2520Dodds%2520and%2520Tara%2520Boroushaki%2520and%2520Fadel%2520Adib%26entry.1292438233%3D%2520%2520We%2520present%2520MITO%252C%2520the%2520first%2520dataset%2520of%2520multi-spectral%2520millimeter-wave%2520%2528mmWave%2529%250Aimages%2520of%2520everyday%2520objects.%2520Unlike%2520visible%2520light%252C%2520mmWave%2520signals%2520can%2520image%250Athrough%2520everyday%2520occlusions%2520%2528e.g.%252C%2520cardboard%2520boxes%252C%2520fabric%252C%2520plastic%2529.%2520However%252C%250Adue%2520to%2520the%2520dearth%2520of%2520publicly-available%2520mmWave%2520images%2520and%2520the%2520interdisciplinary%250Achallenges%2520in%2520collecting%2520and%2520processing%2520mmWave%2520signals%252C%2520it%2520remains%2520difficult%250Atoday%2520for%2520computer%2520vision%2520researchers%2520to%2520develop%2520mmWave-based%2520non-line-of-sight%250Aperception%2520algorithms%2520and%2520models.%250A%2520%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520real-world%2520dataset%2520and%250Aopen-source%2520simulation%2520tool%2520for%2520mmWave%2520imaging.%2520The%2520dataset%2520is%2520acquired%2520using%2520a%250AUR5%2520robotic%2520arm%2520with%2520two%2520mmWave%2520radars%2520operating%2520at%2520different%2520frequencies%2520and%250Aan%2520RGB-D%2520camera.%2520Through%2520a%2520signal%2520processing%2520pipeline%252C%2520we%2520capture%2520and%2520create%250Aover%2520580%2520real-world%25203D%2520mmWave%2520images%2520from%2520over%252076%2520different%2520objects%2520in%2520the%2520YCB%250Adataset%252C%2520a%2520standard%2520dataset%2520for%2520robotics%2520manipulation.%2520We%2520provide%2520real-world%250AmmWave%2520images%2520in%2520line-of-sight%2520and%2520non-line-of-sight%252C%2520as%2520well%2520as%2520RGB-D%2520images%250Aand%2520ground%2520truth%2520segmentation%2520masks.%2520We%2520also%2520develop%2520an%2520open-source%2520simulation%250Atool%2520that%2520can%2520be%2520used%2520to%2520generate%2520synthetic%2520mmWave%2520images%2520for%2520any%25203D%2520triangle%250Amesh%252C%2520which%2520achieves%2520a%2520median%2520F-Score%2520of%252094%2525%2520when%2520compared%2520to%2520real-world%2520mmWave%250Aimages.%250A%2520%2520We%2520show%2520the%2520usefulness%2520of%2520this%2520dataset%2520and%2520simulation%2520tool%2520in%2520multiple%2520CV%250Atasks%2520in%2520non-line-of-sight.%2520First%252C%2520we%2520perform%2520object%2520segmentation%2520for%2520mmWave%250Aimages%2520using%2520the%2520segment%2520anything%2520model%2520%2528SAM%2529%252C%2520and%2520achieve%2520a%2520median%2520precision%250Aand%2520recall%2520of%252092.6%2525%2520and%252064%2525.%2520Second%252C%2520we%2520train%2520a%2520classifier%2520that%2520can%2520recognize%250Aobjects%2520in%2520non-line-of-sight.%2520It%2520is%2520trained%2520on%2520synthetic%2520images%2520and%2520can%250Aclassify%2520real-world%2520images%2520with%252085%2525%2520accuracy.%250A%2520%2520We%2520believe%2520MITO%2520will%2520be%2520a%2520valuable%2520resource%2520for%2520computer%2520vision%2520researchers%250Ain%2520developing%2520non-line-of-sight%2520perception%252C%2520similar%2520to%2520how%2520early%2520camera-based%250Adatasets%2520shaped%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MITO%3A%20Enabling%20Non-Line-of-Sight%20Perception%20using%20Millimeter-waves%0A%20%20through%20Real-World%20Datasets%20and%20Simulation%20Tools&entry.906535625=Laura%20Dodds%20and%20Tara%20Boroushaki%20and%20Fadel%20Adib&entry.1292438233=%20%20We%20present%20MITO%2C%20the%20first%20dataset%20of%20multi-spectral%20millimeter-wave%20%28mmWave%29%0Aimages%20of%20everyday%20objects.%20Unlike%20visible%20light%2C%20mmWave%20signals%20can%20image%0Athrough%20everyday%20occlusions%20%28e.g.%2C%20cardboard%20boxes%2C%20fabric%2C%20plastic%29.%20However%2C%0Adue%20to%20the%20dearth%20of%20publicly-available%20mmWave%20images%20and%20the%20interdisciplinary%0Achallenges%20in%20collecting%20and%20processing%20mmWave%20signals%2C%20it%20remains%20difficult%0Atoday%20for%20computer%20vision%20researchers%20to%20develop%20mmWave-based%20non-line-of-sight%0Aperception%20algorithms%20and%20models.%0A%20%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20real-world%20dataset%20and%0Aopen-source%20simulation%20tool%20for%20mmWave%20imaging.%20The%20dataset%20is%20acquired%20using%20a%0AUR5%20robotic%20arm%20with%20two%20mmWave%20radars%20operating%20at%20different%20frequencies%20and%0Aan%20RGB-D%20camera.%20Through%20a%20signal%20processing%20pipeline%2C%20we%20capture%20and%20create%0Aover%20580%20real-world%203D%20mmWave%20images%20from%20over%2076%20different%20objects%20in%20the%20YCB%0Adataset%2C%20a%20standard%20dataset%20for%20robotics%20manipulation.%20We%20provide%20real-world%0AmmWave%20images%20in%20line-of-sight%20and%20non-line-of-sight%2C%20as%20well%20as%20RGB-D%20images%0Aand%20ground%20truth%20segmentation%20masks.%20We%20also%20develop%20an%20open-source%20simulation%0Atool%20that%20can%20be%20used%20to%20generate%20synthetic%20mmWave%20images%20for%20any%203D%20triangle%0Amesh%2C%20which%20achieves%20a%20median%20F-Score%20of%2094%25%20when%20compared%20to%20real-world%20mmWave%0Aimages.%0A%20%20We%20show%20the%20usefulness%20of%20this%20dataset%20and%20simulation%20tool%20in%20multiple%20CV%0Atasks%20in%20non-line-of-sight.%20First%2C%20we%20perform%20object%20segmentation%20for%20mmWave%0Aimages%20using%20the%20segment%20anything%20model%20%28SAM%29%2C%20and%20achieve%20a%20median%20precision%0Aand%20recall%20of%2092.6%25%20and%2064%25.%20Second%2C%20we%20train%20a%20classifier%20that%20can%20recognize%0Aobjects%20in%20non-line-of-sight.%20It%20is%20trained%20on%20synthetic%20images%20and%20can%0Aclassify%20real-world%20images%20with%2085%25%20accuracy.%0A%20%20We%20believe%20MITO%20will%20be%20a%20valuable%20resource%20for%20computer%20vision%20researchers%0Ain%20developing%20non-line-of-sight%20perception%2C%20similar%20to%20how%20early%20camera-based%0Adatasets%20shaped%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10259v1&entry.124074799=Read"},
{"title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling", "author": "Theodoros Kouzelis and Ioannis Kakogeorgiou and Spyros Gidaris and Nikos Komodakis", "abstract": "  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n", "link": "http://arxiv.org/abs/2502.09509v2", "date": "2025-02-14", "relevancy": 2.4093, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6134}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6011}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&body=Title%3A%20EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling%0AAuthor%3A%20Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis%0AAbstract%3A%20%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEQ-VAE%253A%2520Equivariance%2520Regularized%2520Latent%2520Space%2520for%2520Improved%2520Generative%250A%2520%2520Image%2520Modeling%26entry.906535625%3DTheodoros%2520Kouzelis%2520and%2520Ioannis%2520Kakogeorgiou%2520and%2520Spyros%2520Gidaris%2520and%2520Nikos%2520Komodakis%26entry.1292438233%3D%2520%2520Latent%2520generative%2520models%2520have%2520emerged%2520as%2520a%2520leading%2520approach%2520for%2520high-quality%250Aimage%2520synthesis.%2520These%2520models%2520rely%2520on%2520an%2520autoencoder%2520to%2520compress%2520images%2520into%2520a%250Alatent%2520space%252C%2520followed%2520by%2520a%2520generative%2520model%2520to%2520learn%2520the%2520latent%2520distribution.%250AWe%2520identify%2520that%2520existing%2520autoencoders%2520lack%2520equivariance%2520to%2520semantic-preserving%250Atransformations%2520like%2520scaling%2520and%2520rotation%252C%2520resulting%2520in%2520complex%2520latent%2520spaces%250Athat%2520hinder%2520generative%2520performance.%2520To%2520address%2520this%252C%2520we%2520propose%2520EQ-VAE%252C%2520a%250Asimple%2520regularization%2520approach%2520that%2520enforces%2520equivariance%2520in%2520the%2520latent%2520space%252C%250Areducing%2520its%2520complexity%2520without%2520degrading%2520reconstruction%2520quality.%2520By%2520finetuning%250Apre-trained%2520autoencoders%2520with%2520EQ-VAE%252C%2520we%2520enhance%2520the%2520performance%2520of%2520several%250Astate-of-the-art%2520generative%2520models%252C%2520including%2520DiT%252C%2520SiT%252C%2520REPA%2520and%2520MaskGIT%252C%250Aachieving%2520a%25207%2520speedup%2520on%2520DiT-XL/2%2520with%2520only%2520five%2520epochs%2520of%2520SD-VAE%2520fine-tuning.%250AEQ-VAE%2520is%2520compatible%2520with%2520both%2520continuous%2520and%2520discrete%2520autoencoders%252C%2520thus%250Aoffering%2520a%2520versatile%2520enhancement%2520for%2520a%2520wide%2520range%2520of%2520latent%2520generative%2520models.%250AProject%2520page%2520and%2520code%253A%2520https%253A//eq-vae.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EQ-VAE%3A%20Equivariance%20Regularized%20Latent%20Space%20for%20Improved%20Generative%0A%20%20Image%20Modeling&entry.906535625=Theodoros%20Kouzelis%20and%20Ioannis%20Kakogeorgiou%20and%20Spyros%20Gidaris%20and%20Nikos%20Komodakis&entry.1292438233=%20%20Latent%20generative%20models%20have%20emerged%20as%20a%20leading%20approach%20for%20high-quality%0Aimage%20synthesis.%20These%20models%20rely%20on%20an%20autoencoder%20to%20compress%20images%20into%20a%0Alatent%20space%2C%20followed%20by%20a%20generative%20model%20to%20learn%20the%20latent%20distribution.%0AWe%20identify%20that%20existing%20autoencoders%20lack%20equivariance%20to%20semantic-preserving%0Atransformations%20like%20scaling%20and%20rotation%2C%20resulting%20in%20complex%20latent%20spaces%0Athat%20hinder%20generative%20performance.%20To%20address%20this%2C%20we%20propose%20EQ-VAE%2C%20a%0Asimple%20regularization%20approach%20that%20enforces%20equivariance%20in%20the%20latent%20space%2C%0Areducing%20its%20complexity%20without%20degrading%20reconstruction%20quality.%20By%20finetuning%0Apre-trained%20autoencoders%20with%20EQ-VAE%2C%20we%20enhance%20the%20performance%20of%20several%0Astate-of-the-art%20generative%20models%2C%20including%20DiT%2C%20SiT%2C%20REPA%20and%20MaskGIT%2C%0Aachieving%20a%207%20speedup%20on%20DiT-XL/2%20with%20only%20five%20epochs%20of%20SD-VAE%20fine-tuning.%0AEQ-VAE%20is%20compatible%20with%20both%20continuous%20and%20discrete%20autoencoders%2C%20thus%0Aoffering%20a%20versatile%20enhancement%20for%20a%20wide%20range%20of%20latent%20generative%20models.%0AProject%20page%20and%20code%3A%20https%3A//eq-vae.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09509v2&entry.124074799=Read"},
{"title": "CR-CTC: Consistency regularization on CTC for improved speech\n  recognition", "author": "Zengwei Yao and Wei Kang and Xiaoyu Yang and Fangjun Kuang and Liyong Guo and Han Zhu and Zengrui Jin and Zhaoqing Li and Long Lin and Daniel Povey", "abstract": "  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance. In this work, we propose the Consistency-Regularized CTC (CR-CTC),\nwhich enforces consistency between two CTC distributions obtained from\ndifferent augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC. It significantly improves\nthe CTC performance, achieving state-of-the-art results comparable to those\nattained by transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). We release our code at\nhttps://github.com/k2-fsa/icefall.\n", "link": "http://arxiv.org/abs/2410.05101v4", "date": "2025-02-14", "relevancy": 2.4019, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.476}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition&body=Title%3A%20CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition%0AAuthor%3A%20Zengwei%20Yao%20and%20Wei%20Kang%20and%20Xiaoyu%20Yang%20and%20Fangjun%20Kuang%20and%20Liyong%20Guo%20and%20Han%20Zhu%20and%20Zengrui%20Jin%20and%20Zhaoqing%20Li%20and%20Long%20Lin%20and%20Daniel%20Povey%0AAbstract%3A%20%20%20Connectionist%20Temporal%20Classification%20%28CTC%29%20is%20a%20widely%20used%20method%20for%0Aautomatic%20speech%20recognition%20%28ASR%29%2C%20renowned%20for%20its%20simplicity%20and%0Acomputational%20efficiency.%20However%2C%20it%20often%20falls%20short%20in%20recognition%0Aperformance.%20In%20this%20work%2C%20we%20propose%20the%20Consistency-Regularized%20CTC%20%28CR-CTC%29%2C%0Awhich%20enforces%20consistency%20between%20two%20CTC%20distributions%20obtained%20from%0Adifferent%20augmented%20views%20of%20the%20input%20speech%20mel-spectrogram.%20We%20provide%0Ain-depth%20insights%20into%20its%20essential%20behaviors%20from%20three%20perspectives%3A%201%29%20it%0Aconducts%20self-distillation%20between%20random%20pairs%20of%20sub-models%20that%20process%0Adifferent%20augmented%20views%3B%202%29%20it%20learns%20contextual%20representation%20through%0Amasked%20prediction%20for%20positions%20within%20time-masked%20regions%2C%20especially%20when%20we%0Aincrease%20the%20amount%20of%20time%20masking%3B%203%29%20it%20suppresses%20the%20extremely%20peaky%20CTC%0Adistributions%2C%20thereby%20reducing%20overfitting%20and%20improving%20the%20generalization%0Aability.%20Extensive%20experiments%20on%20LibriSpeech%2C%20Aishell-1%2C%20and%20GigaSpeech%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20CR-CTC.%20It%20significantly%20improves%0Athe%20CTC%20performance%2C%20achieving%20state-of-the-art%20results%20comparable%20to%20those%0Aattained%20by%20transducer%20or%20systems%20combining%20CTC%20and%20attention-based%0Aencoder-decoder%20%28CTC/AED%29.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/k2-fsa/icefall.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05101v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCR-CTC%253A%2520Consistency%2520regularization%2520on%2520CTC%2520for%2520improved%2520speech%250A%2520%2520recognition%26entry.906535625%3DZengwei%2520Yao%2520and%2520Wei%2520Kang%2520and%2520Xiaoyu%2520Yang%2520and%2520Fangjun%2520Kuang%2520and%2520Liyong%2520Guo%2520and%2520Han%2520Zhu%2520and%2520Zengrui%2520Jin%2520and%2520Zhaoqing%2520Li%2520and%2520Long%2520Lin%2520and%2520Daniel%2520Povey%26entry.1292438233%3D%2520%2520Connectionist%2520Temporal%2520Classification%2520%2528CTC%2529%2520is%2520a%2520widely%2520used%2520method%2520for%250Aautomatic%2520speech%2520recognition%2520%2528ASR%2529%252C%2520renowned%2520for%2520its%2520simplicity%2520and%250Acomputational%2520efficiency.%2520However%252C%2520it%2520often%2520falls%2520short%2520in%2520recognition%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Consistency-Regularized%2520CTC%2520%2528CR-CTC%2529%252C%250Awhich%2520enforces%2520consistency%2520between%2520two%2520CTC%2520distributions%2520obtained%2520from%250Adifferent%2520augmented%2520views%2520of%2520the%2520input%2520speech%2520mel-spectrogram.%2520We%2520provide%250Ain-depth%2520insights%2520into%2520its%2520essential%2520behaviors%2520from%2520three%2520perspectives%253A%25201%2529%2520it%250Aconducts%2520self-distillation%2520between%2520random%2520pairs%2520of%2520sub-models%2520that%2520process%250Adifferent%2520augmented%2520views%253B%25202%2529%2520it%2520learns%2520contextual%2520representation%2520through%250Amasked%2520prediction%2520for%2520positions%2520within%2520time-masked%2520regions%252C%2520especially%2520when%2520we%250Aincrease%2520the%2520amount%2520of%2520time%2520masking%253B%25203%2529%2520it%2520suppresses%2520the%2520extremely%2520peaky%2520CTC%250Adistributions%252C%2520thereby%2520reducing%2520overfitting%2520and%2520improving%2520the%2520generalization%250Aability.%2520Extensive%2520experiments%2520on%2520LibriSpeech%252C%2520Aishell-1%252C%2520and%2520GigaSpeech%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520CR-CTC.%2520It%2520significantly%2520improves%250Athe%2520CTC%2520performance%252C%2520achieving%2520state-of-the-art%2520results%2520comparable%2520to%2520those%250Aattained%2520by%2520transducer%2520or%2520systems%2520combining%2520CTC%2520and%2520attention-based%250Aencoder-decoder%2520%2528CTC/AED%2529.%2520We%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/k2-fsa/icefall.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05101v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CR-CTC%3A%20Consistency%20regularization%20on%20CTC%20for%20improved%20speech%0A%20%20recognition&entry.906535625=Zengwei%20Yao%20and%20Wei%20Kang%20and%20Xiaoyu%20Yang%20and%20Fangjun%20Kuang%20and%20Liyong%20Guo%20and%20Han%20Zhu%20and%20Zengrui%20Jin%20and%20Zhaoqing%20Li%20and%20Long%20Lin%20and%20Daniel%20Povey&entry.1292438233=%20%20Connectionist%20Temporal%20Classification%20%28CTC%29%20is%20a%20widely%20used%20method%20for%0Aautomatic%20speech%20recognition%20%28ASR%29%2C%20renowned%20for%20its%20simplicity%20and%0Acomputational%20efficiency.%20However%2C%20it%20often%20falls%20short%20in%20recognition%0Aperformance.%20In%20this%20work%2C%20we%20propose%20the%20Consistency-Regularized%20CTC%20%28CR-CTC%29%2C%0Awhich%20enforces%20consistency%20between%20two%20CTC%20distributions%20obtained%20from%0Adifferent%20augmented%20views%20of%20the%20input%20speech%20mel-spectrogram.%20We%20provide%0Ain-depth%20insights%20into%20its%20essential%20behaviors%20from%20three%20perspectives%3A%201%29%20it%0Aconducts%20self-distillation%20between%20random%20pairs%20of%20sub-models%20that%20process%0Adifferent%20augmented%20views%3B%202%29%20it%20learns%20contextual%20representation%20through%0Amasked%20prediction%20for%20positions%20within%20time-masked%20regions%2C%20especially%20when%20we%0Aincrease%20the%20amount%20of%20time%20masking%3B%203%29%20it%20suppresses%20the%20extremely%20peaky%20CTC%0Adistributions%2C%20thereby%20reducing%20overfitting%20and%20improving%20the%20generalization%0Aability.%20Extensive%20experiments%20on%20LibriSpeech%2C%20Aishell-1%2C%20and%20GigaSpeech%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20CR-CTC.%20It%20significantly%20improves%0Athe%20CTC%20performance%2C%20achieving%20state-of-the-art%20results%20comparable%20to%20those%0Aattained%20by%20transducer%20or%20systems%20combining%20CTC%20and%20attention-based%0Aencoder-decoder%20%28CTC/AED%29.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/k2-fsa/icefall.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05101v4&entry.124074799=Read"},
{"title": "Data-Adaptive Low-Rank Sparse Subspace Clustering", "author": "Ivica Kopriva", "abstract": "  Low-rank sparse subspace clustering (LRSSC) algorithms built on\nself-expressive model effectively capture both the global and local structure\nof the data. However, existing solutions, primarily based on proximal operators\nassociated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In\nthis work, we propose an LRSSC algorithm incorporating a data-adaptive\nsurrogate for the S0/L0 quasi-norm. We provide a numerical solution for the\ncorresponding proximal operator in cases where an analytical expression is\nunavailable. The proposed LRSSC algorithm is formulated within the proximal\nmapping framework, and we present theoretical proof of its global convergence\ntoward a stationary point. We evaluate the performance of the proposed method\non three well known datasets, comparing it against LRSSC algorithms constrained\nby Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.\n", "link": "http://arxiv.org/abs/2502.10106v1", "date": "2025-02-14", "relevancy": 2.3965, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5063}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4829}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Adaptive%20Low-Rank%20Sparse%20Subspace%20Clustering&body=Title%3A%20Data-Adaptive%20Low-Rank%20Sparse%20Subspace%20Clustering%0AAuthor%3A%20Ivica%20Kopriva%0AAbstract%3A%20%20%20Low-rank%20sparse%20subspace%20clustering%20%28LRSSC%29%20algorithms%20built%20on%0Aself-expressive%20model%20effectively%20capture%20both%20the%20global%20and%20local%20structure%0Aof%20the%20data.%20However%2C%20existing%20solutions%2C%20primarily%20based%20on%20proximal%20operators%0Aassociated%20with%20Sp/Lp%20%2C%20p%20e%20%7B0%2C%201/2%2C%202/3%2C%201%7D%2C%20norms%20are%20not%20data-adaptive.%20In%0Athis%20work%2C%20we%20propose%20an%20LRSSC%20algorithm%20incorporating%20a%20data-adaptive%0Asurrogate%20for%20the%20S0/L0%20quasi-norm.%20We%20provide%20a%20numerical%20solution%20for%20the%0Acorresponding%20proximal%20operator%20in%20cases%20where%20an%20analytical%20expression%20is%0Aunavailable.%20The%20proposed%20LRSSC%20algorithm%20is%20formulated%20within%20the%20proximal%0Amapping%20framework%2C%20and%20we%20present%20theoretical%20proof%20of%20its%20global%20convergence%0Atoward%20a%20stationary%20point.%20We%20evaluate%20the%20performance%20of%20the%20proposed%20method%0Aon%20three%20well%20known%20datasets%2C%20comparing%20it%20against%20LRSSC%20algorithms%20constrained%0Aby%20Sp/Lp%2C%20p%20e%20%7B0%2C%201/2%2C%202/3%2C%201%7D%2C%20norms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Adaptive%2520Low-Rank%2520Sparse%2520Subspace%2520Clustering%26entry.906535625%3DIvica%2520Kopriva%26entry.1292438233%3D%2520%2520Low-rank%2520sparse%2520subspace%2520clustering%2520%2528LRSSC%2529%2520algorithms%2520built%2520on%250Aself-expressive%2520model%2520effectively%2520capture%2520both%2520the%2520global%2520and%2520local%2520structure%250Aof%2520the%2520data.%2520However%252C%2520existing%2520solutions%252C%2520primarily%2520based%2520on%2520proximal%2520operators%250Aassociated%2520with%2520Sp/Lp%2520%252C%2520p%2520e%2520%257B0%252C%25201/2%252C%25202/3%252C%25201%257D%252C%2520norms%2520are%2520not%2520data-adaptive.%2520In%250Athis%2520work%252C%2520we%2520propose%2520an%2520LRSSC%2520algorithm%2520incorporating%2520a%2520data-adaptive%250Asurrogate%2520for%2520the%2520S0/L0%2520quasi-norm.%2520We%2520provide%2520a%2520numerical%2520solution%2520for%2520the%250Acorresponding%2520proximal%2520operator%2520in%2520cases%2520where%2520an%2520analytical%2520expression%2520is%250Aunavailable.%2520The%2520proposed%2520LRSSC%2520algorithm%2520is%2520formulated%2520within%2520the%2520proximal%250Amapping%2520framework%252C%2520and%2520we%2520present%2520theoretical%2520proof%2520of%2520its%2520global%2520convergence%250Atoward%2520a%2520stationary%2520point.%2520We%2520evaluate%2520the%2520performance%2520of%2520the%2520proposed%2520method%250Aon%2520three%2520well%2520known%2520datasets%252C%2520comparing%2520it%2520against%2520LRSSC%2520algorithms%2520constrained%250Aby%2520Sp/Lp%252C%2520p%2520e%2520%257B0%252C%25201/2%252C%25202/3%252C%25201%257D%252C%2520norms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Adaptive%20Low-Rank%20Sparse%20Subspace%20Clustering&entry.906535625=Ivica%20Kopriva&entry.1292438233=%20%20Low-rank%20sparse%20subspace%20clustering%20%28LRSSC%29%20algorithms%20built%20on%0Aself-expressive%20model%20effectively%20capture%20both%20the%20global%20and%20local%20structure%0Aof%20the%20data.%20However%2C%20existing%20solutions%2C%20primarily%20based%20on%20proximal%20operators%0Aassociated%20with%20Sp/Lp%20%2C%20p%20e%20%7B0%2C%201/2%2C%202/3%2C%201%7D%2C%20norms%20are%20not%20data-adaptive.%20In%0Athis%20work%2C%20we%20propose%20an%20LRSSC%20algorithm%20incorporating%20a%20data-adaptive%0Asurrogate%20for%20the%20S0/L0%20quasi-norm.%20We%20provide%20a%20numerical%20solution%20for%20the%0Acorresponding%20proximal%20operator%20in%20cases%20where%20an%20analytical%20expression%20is%0Aunavailable.%20The%20proposed%20LRSSC%20algorithm%20is%20formulated%20within%20the%20proximal%0Amapping%20framework%2C%20and%20we%20present%20theoretical%20proof%20of%20its%20global%20convergence%0Atoward%20a%20stationary%20point.%20We%20evaluate%20the%20performance%20of%20the%20proposed%20method%0Aon%20three%20well%20known%20datasets%2C%20comparing%20it%20against%20LRSSC%20algorithms%20constrained%0Aby%20Sp/Lp%2C%20p%20e%20%7B0%2C%201/2%2C%202/3%2C%201%7D%2C%20norms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10106v1&entry.124074799=Read"},
{"title": "Contrastive Federated Learning with Tabular Data Silos", "author": "Achmad Ginanjar and Xue Li and Wen Hua and Jiaming Pei", "abstract": "  Learning from vertical partitioned data silos is challenging due to the\nsegmented nature of data, sample misalignment, and strict privacy concerns.\nFederated learning has been proposed as a solution. However, sample\nmisalignment across silos often hinders optimal model performance and suggests\ndata sharing within the model, which breaks privacy. Our proposed solution is\nContrastive Federated Learning with Tabular Data Silos (CFL), which offers a\nsolution for data silos with sample misalignment without the need for sharing\noriginal or representative data to maintain privacy. CFL begins with local\nacquisition of contrastive representations of the data within each silo and\naggregates knowledge from other silos through the federated learning algorithm.\nOur experiments demonstrate that CFL solves the limitations of existing\nalgorithms for data silos and outperforms existing tabular contrastive\nlearning. CFL provides performance improvements without loosening privacy.\n", "link": "http://arxiv.org/abs/2409.06123v2", "date": "2025-02-14", "relevancy": 2.3772, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4659}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Federated%20Learning%20with%20Tabular%20Data%20Silos&body=Title%3A%20Contrastive%20Federated%20Learning%20with%20Tabular%20Data%20Silos%0AAuthor%3A%20Achmad%20Ginanjar%20and%20Xue%20Li%20and%20Wen%20Hua%20and%20Jiaming%20Pei%0AAbstract%3A%20%20%20Learning%20from%20vertical%20partitioned%20data%20silos%20is%20challenging%20due%20to%20the%0Asegmented%20nature%20of%20data%2C%20sample%20misalignment%2C%20and%20strict%20privacy%20concerns.%0AFederated%20learning%20has%20been%20proposed%20as%20a%20solution.%20However%2C%20sample%0Amisalignment%20across%20silos%20often%20hinders%20optimal%20model%20performance%20and%20suggests%0Adata%20sharing%20within%20the%20model%2C%20which%20breaks%20privacy.%20Our%20proposed%20solution%20is%0AContrastive%20Federated%20Learning%20with%20Tabular%20Data%20Silos%20%28CFL%29%2C%20which%20offers%20a%0Asolution%20for%20data%20silos%20with%20sample%20misalignment%20without%20the%20need%20for%20sharing%0Aoriginal%20or%20representative%20data%20to%20maintain%20privacy.%20CFL%20begins%20with%20local%0Aacquisition%20of%20contrastive%20representations%20of%20the%20data%20within%20each%20silo%20and%0Aaggregates%20knowledge%20from%20other%20silos%20through%20the%20federated%20learning%20algorithm.%0AOur%20experiments%20demonstrate%20that%20CFL%20solves%20the%20limitations%20of%20existing%0Aalgorithms%20for%20data%20silos%20and%20outperforms%20existing%20tabular%20contrastive%0Alearning.%20CFL%20provides%20performance%20improvements%20without%20loosening%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Federated%2520Learning%2520with%2520Tabular%2520Data%2520Silos%26entry.906535625%3DAchmad%2520Ginanjar%2520and%2520Xue%2520Li%2520and%2520Wen%2520Hua%2520and%2520Jiaming%2520Pei%26entry.1292438233%3D%2520%2520Learning%2520from%2520vertical%2520partitioned%2520data%2520silos%2520is%2520challenging%2520due%2520to%2520the%250Asegmented%2520nature%2520of%2520data%252C%2520sample%2520misalignment%252C%2520and%2520strict%2520privacy%2520concerns.%250AFederated%2520learning%2520has%2520been%2520proposed%2520as%2520a%2520solution.%2520However%252C%2520sample%250Amisalignment%2520across%2520silos%2520often%2520hinders%2520optimal%2520model%2520performance%2520and%2520suggests%250Adata%2520sharing%2520within%2520the%2520model%252C%2520which%2520breaks%2520privacy.%2520Our%2520proposed%2520solution%2520is%250AContrastive%2520Federated%2520Learning%2520with%2520Tabular%2520Data%2520Silos%2520%2528CFL%2529%252C%2520which%2520offers%2520a%250Asolution%2520for%2520data%2520silos%2520with%2520sample%2520misalignment%2520without%2520the%2520need%2520for%2520sharing%250Aoriginal%2520or%2520representative%2520data%2520to%2520maintain%2520privacy.%2520CFL%2520begins%2520with%2520local%250Aacquisition%2520of%2520contrastive%2520representations%2520of%2520the%2520data%2520within%2520each%2520silo%2520and%250Aaggregates%2520knowledge%2520from%2520other%2520silos%2520through%2520the%2520federated%2520learning%2520algorithm.%250AOur%2520experiments%2520demonstrate%2520that%2520CFL%2520solves%2520the%2520limitations%2520of%2520existing%250Aalgorithms%2520for%2520data%2520silos%2520and%2520outperforms%2520existing%2520tabular%2520contrastive%250Alearning.%2520CFL%2520provides%2520performance%2520improvements%2520without%2520loosening%2520privacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Federated%20Learning%20with%20Tabular%20Data%20Silos&entry.906535625=Achmad%20Ginanjar%20and%20Xue%20Li%20and%20Wen%20Hua%20and%20Jiaming%20Pei&entry.1292438233=%20%20Learning%20from%20vertical%20partitioned%20data%20silos%20is%20challenging%20due%20to%20the%0Asegmented%20nature%20of%20data%2C%20sample%20misalignment%2C%20and%20strict%20privacy%20concerns.%0AFederated%20learning%20has%20been%20proposed%20as%20a%20solution.%20However%2C%20sample%0Amisalignment%20across%20silos%20often%20hinders%20optimal%20model%20performance%20and%20suggests%0Adata%20sharing%20within%20the%20model%2C%20which%20breaks%20privacy.%20Our%20proposed%20solution%20is%0AContrastive%20Federated%20Learning%20with%20Tabular%20Data%20Silos%20%28CFL%29%2C%20which%20offers%20a%0Asolution%20for%20data%20silos%20with%20sample%20misalignment%20without%20the%20need%20for%20sharing%0Aoriginal%20or%20representative%20data%20to%20maintain%20privacy.%20CFL%20begins%20with%20local%0Aacquisition%20of%20contrastive%20representations%20of%20the%20data%20within%20each%20silo%20and%0Aaggregates%20knowledge%20from%20other%20silos%20through%20the%20federated%20learning%20algorithm.%0AOur%20experiments%20demonstrate%20that%20CFL%20solves%20the%20limitations%20of%20existing%0Aalgorithms%20for%20data%20silos%20and%20outperforms%20existing%20tabular%20contrastive%0Alearning.%20CFL%20provides%20performance%20improvements%20without%20loosening%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06123v2&entry.124074799=Read"},
{"title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving", "author": "Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin", "abstract": "  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.\n", "link": "http://arxiv.org/abs/2502.07640v2", "date": "2025-02-14", "relevancy": 2.3719, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4909}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goedel-Prover%3A%20A%20Frontier%20Model%20for%20Open-Source%20Automated%20Theorem%0A%20%20Proving&body=Title%3A%20Goedel-Prover%3A%20A%20Frontier%20Model%20for%20Open-Source%20Automated%20Theorem%0A%20%20Proving%0AAuthor%3A%20Yong%20Lin%20and%20Shange%20Tang%20and%20Bohan%20Lyu%20and%20Jiayun%20Wu%20and%20Hongzhou%20Lin%20and%20Kaiyu%20Yang%20and%20Jia%20Li%20and%20Mengzhou%20Xia%20and%20Danqi%20Chen%20and%20Sanjeev%20Arora%20and%20Chi%20Jin%0AAbstract%3A%20%20%20We%20introduce%20Goedel-Prover%2C%20an%20open-source%20large%20language%20model%20%28LLM%29%20that%0Aachieves%20the%20state-of-the-art%20%28SOTA%29%20performance%20in%20automated%20formal%20proof%0Ageneration%20for%20mathematical%20problems.%20The%20key%20challenge%20in%20this%20field%20is%20the%0Ascarcity%20of%20formalized%20math%20statements%20and%20proofs%2C%20which%20we%20tackle%20in%20the%0Afollowing%20ways.%20We%20train%20statement%20formalizers%20to%20translate%20the%20natural%0Alanguage%20math%20problems%20from%20Numina%20into%20formal%20language%20%28Lean%204%29%2C%20creating%20a%0Adataset%20of%201.64%20million%20formal%20statements.%20LLMs%20are%20used%20to%20check%20that%20the%0Aformal%20statements%20accurately%20preserve%20the%20content%20of%20the%20original%20natural%0Alanguage%20problems.%20We%20then%20iteratively%20build%20a%20large%20dataset%20of%20formal%20proofs%0Aby%20training%20a%20series%20of%20provers.%20Each%20prover%20succeeds%20in%20proving%20many%0Astatements%20that%20the%20previous%20ones%20could%20not%2C%20and%20these%20new%20proofs%20are%20added%20to%0Athe%20training%20set%20for%20the%20next%20prover.%20Despite%20using%20only%20supervised%0Afine-tuning%2C%20our%20final%20prover%20significantly%20outperforms%20the%20previous%20best%0Aopen-source%20model%2C%20DeepSeek-Prover-V1.5%2C%20which%20employs%20reinforcement%20learning.%0AOn%20the%20miniF2F%20benchmark%2C%20our%20model%20achieves%20a%20success%20rate%20of%2057.6%25%20%28Pass%4032%29%2C%0Asurpassing%20DeepSeek-Prover-V1.5%20by%207.6%25.%20On%20PutnamBench%2C%20Goedel-Prover%0Asuccessfully%20solves%207%20problems%20%28Pass%40512%29%2C%20ranking%20first%20on%20the%20leaderboard.%0AFurthermore%2C%20it%20generates%2029.7K%20formal%20proofs%20for%20Lean%20Workbook%20problems%2C%0Anearly%20doubling%20the%2015.7K%20produced%20by%20earlier%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoedel-Prover%253A%2520A%2520Frontier%2520Model%2520for%2520Open-Source%2520Automated%2520Theorem%250A%2520%2520Proving%26entry.906535625%3DYong%2520Lin%2520and%2520Shange%2520Tang%2520and%2520Bohan%2520Lyu%2520and%2520Jiayun%2520Wu%2520and%2520Hongzhou%2520Lin%2520and%2520Kaiyu%2520Yang%2520and%2520Jia%2520Li%2520and%2520Mengzhou%2520Xia%2520and%2520Danqi%2520Chen%2520and%2520Sanjeev%2520Arora%2520and%2520Chi%2520Jin%26entry.1292438233%3D%2520%2520We%2520introduce%2520Goedel-Prover%252C%2520an%2520open-source%2520large%2520language%2520model%2520%2528LLM%2529%2520that%250Aachieves%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520automated%2520formal%2520proof%250Ageneration%2520for%2520mathematical%2520problems.%2520The%2520key%2520challenge%2520in%2520this%2520field%2520is%2520the%250Ascarcity%2520of%2520formalized%2520math%2520statements%2520and%2520proofs%252C%2520which%2520we%2520tackle%2520in%2520the%250Afollowing%2520ways.%2520We%2520train%2520statement%2520formalizers%2520to%2520translate%2520the%2520natural%250Alanguage%2520math%2520problems%2520from%2520Numina%2520into%2520formal%2520language%2520%2528Lean%25204%2529%252C%2520creating%2520a%250Adataset%2520of%25201.64%2520million%2520formal%2520statements.%2520LLMs%2520are%2520used%2520to%2520check%2520that%2520the%250Aformal%2520statements%2520accurately%2520preserve%2520the%2520content%2520of%2520the%2520original%2520natural%250Alanguage%2520problems.%2520We%2520then%2520iteratively%2520build%2520a%2520large%2520dataset%2520of%2520formal%2520proofs%250Aby%2520training%2520a%2520series%2520of%2520provers.%2520Each%2520prover%2520succeeds%2520in%2520proving%2520many%250Astatements%2520that%2520the%2520previous%2520ones%2520could%2520not%252C%2520and%2520these%2520new%2520proofs%2520are%2520added%2520to%250Athe%2520training%2520set%2520for%2520the%2520next%2520prover.%2520Despite%2520using%2520only%2520supervised%250Afine-tuning%252C%2520our%2520final%2520prover%2520significantly%2520outperforms%2520the%2520previous%2520best%250Aopen-source%2520model%252C%2520DeepSeek-Prover-V1.5%252C%2520which%2520employs%2520reinforcement%2520learning.%250AOn%2520the%2520miniF2F%2520benchmark%252C%2520our%2520model%2520achieves%2520a%2520success%2520rate%2520of%252057.6%2525%2520%2528Pass%254032%2529%252C%250Asurpassing%2520DeepSeek-Prover-V1.5%2520by%25207.6%2525.%2520On%2520PutnamBench%252C%2520Goedel-Prover%250Asuccessfully%2520solves%25207%2520problems%2520%2528Pass%2540512%2529%252C%2520ranking%2520first%2520on%2520the%2520leaderboard.%250AFurthermore%252C%2520it%2520generates%252029.7K%2520formal%2520proofs%2520for%2520Lean%2520Workbook%2520problems%252C%250Anearly%2520doubling%2520the%252015.7K%2520produced%2520by%2520earlier%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goedel-Prover%3A%20A%20Frontier%20Model%20for%20Open-Source%20Automated%20Theorem%0A%20%20Proving&entry.906535625=Yong%20Lin%20and%20Shange%20Tang%20and%20Bohan%20Lyu%20and%20Jiayun%20Wu%20and%20Hongzhou%20Lin%20and%20Kaiyu%20Yang%20and%20Jia%20Li%20and%20Mengzhou%20Xia%20and%20Danqi%20Chen%20and%20Sanjeev%20Arora%20and%20Chi%20Jin&entry.1292438233=%20%20We%20introduce%20Goedel-Prover%2C%20an%20open-source%20large%20language%20model%20%28LLM%29%20that%0Aachieves%20the%20state-of-the-art%20%28SOTA%29%20performance%20in%20automated%20formal%20proof%0Ageneration%20for%20mathematical%20problems.%20The%20key%20challenge%20in%20this%20field%20is%20the%0Ascarcity%20of%20formalized%20math%20statements%20and%20proofs%2C%20which%20we%20tackle%20in%20the%0Afollowing%20ways.%20We%20train%20statement%20formalizers%20to%20translate%20the%20natural%0Alanguage%20math%20problems%20from%20Numina%20into%20formal%20language%20%28Lean%204%29%2C%20creating%20a%0Adataset%20of%201.64%20million%20formal%20statements.%20LLMs%20are%20used%20to%20check%20that%20the%0Aformal%20statements%20accurately%20preserve%20the%20content%20of%20the%20original%20natural%0Alanguage%20problems.%20We%20then%20iteratively%20build%20a%20large%20dataset%20of%20formal%20proofs%0Aby%20training%20a%20series%20of%20provers.%20Each%20prover%20succeeds%20in%20proving%20many%0Astatements%20that%20the%20previous%20ones%20could%20not%2C%20and%20these%20new%20proofs%20are%20added%20to%0Athe%20training%20set%20for%20the%20next%20prover.%20Despite%20using%20only%20supervised%0Afine-tuning%2C%20our%20final%20prover%20significantly%20outperforms%20the%20previous%20best%0Aopen-source%20model%2C%20DeepSeek-Prover-V1.5%2C%20which%20employs%20reinforcement%20learning.%0AOn%20the%20miniF2F%20benchmark%2C%20our%20model%20achieves%20a%20success%20rate%20of%2057.6%25%20%28Pass%4032%29%2C%0Asurpassing%20DeepSeek-Prover-V1.5%20by%207.6%25.%20On%20PutnamBench%2C%20Goedel-Prover%0Asuccessfully%20solves%207%20problems%20%28Pass%40512%29%2C%20ranking%20first%20on%20the%20leaderboard.%0AFurthermore%2C%20it%20generates%2029.7K%20formal%20proofs%20for%20Lean%20Workbook%20problems%2C%0Anearly%20doubling%20the%2015.7K%20produced%20by%20earlier%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07640v2&entry.124074799=Read"},
{"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models", "author": "Gokul Karthik Kumar and Iheb Chaabane and Kebin Wu", "abstract": "  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n", "link": "http://arxiv.org/abs/2502.10250v1", "date": "2025-02-14", "relevancy": 2.3329, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models&body=Title%3A%20VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Gokul%20Karthik%20Kumar%20and%20Iheb%20Chaabane%20and%20Kebin%20Wu%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20various%20visual%20benchmarks%20but%20are%0Aoften%20constrained%20by%20the%20lack%20of%20high-quality%20visual%20fine-tuning%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20VisCon-100K%2C%20a%20novel%20dataset%20derived%20from%0Ainterleaved%20image-text%20web%20documents.%20Our%20approach%20transforms%2045K%20web%20documents%0Afrom%20the%20OBELICS%20dataset%20into%20100K%20image%20conversation%20samples.%20We%20utilize%0AGPT-4V%20to%20generate%20image-contextual%20captions%20and%20OpenChat%203.5%20model%20to%20convert%0Athese%20captions%20into%20diverse%20free-form%20and%20multiple-choice%20question-answer%0Apairs.%20Integrating%20this%20dataset%20for%20fine-tuning%20considerably%20enhances%20VLM%0Aperformance%20across%20multiple%20benchmarks.%20Unlike%20methods%20that%20focus%20solely%20on%0Afine-grained%20visual%20content%2C%20our%20approach%20leverages%20accompanying%20web%20context%2C%0Ayielding%20superior%20results.%20We%20also%20discover%20that%20a%20%60leaky%20modality%20mix%2C%27%20where%0Aconversation%20samples%20contain%20questions%20answerable%20from%20both%20the%20image%20and%20its%0Acontextual%20caption%2C%20outperforms%20non-leaky%20combinations%20of%20captions%20and%20Q%5C%26A%0Apairs.%20VisCon-100k%20dataset%20shows%20strong%20performance%20with%20two%20popular%20VLM%0Aapproaches%3A%20text-only%20large%20language%20model%20%28LLM%29%20aligned%20with%20a%20vision%20encoder%0Ausing%20image%20captions%20data%20%28ShareGPT4V-7b%29%20and%20multimodally%20pretrained%20LLM%0A%28IDEFICS2-8b%29%20using%20interleaved%20image-text%20data.%20In%20addition%20to%20releasing%20the%0AVisCon-100K%20dataset%2C%20we%20provide%20a%20contextual%20captioner%20trained%20on%20this%20dataset%2C%0Afacilitating%20scalable%20fine-tuning%20data%20generation%20for%20future%20research%20and%0Aopen-source%20applications.%20Using%20the%20same%20pipeline%2C%20but%20substituting%20our%20trained%0Acontextual%20captioner%20for%20GPT-4V%2C%20we%20also%20release%20the%20larger%20VisCon-1M%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCon-100K%253A%2520Leveraging%2520Contextual%2520Web%2520Data%2520for%2520Fine-tuning%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DGokul%2520Karthik%2520Kumar%2520and%2520Iheb%2520Chaabane%2520and%2520Kebin%2520Wu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520excel%2520in%2520various%2520visual%2520benchmarks%2520but%2520are%250Aoften%2520constrained%2520by%2520the%2520lack%2520of%2520high-quality%2520visual%2520fine-tuning%2520data.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520VisCon-100K%252C%2520a%2520novel%2520dataset%2520derived%2520from%250Ainterleaved%2520image-text%2520web%2520documents.%2520Our%2520approach%2520transforms%252045K%2520web%2520documents%250Afrom%2520the%2520OBELICS%2520dataset%2520into%2520100K%2520image%2520conversation%2520samples.%2520We%2520utilize%250AGPT-4V%2520to%2520generate%2520image-contextual%2520captions%2520and%2520OpenChat%25203.5%2520model%2520to%2520convert%250Athese%2520captions%2520into%2520diverse%2520free-form%2520and%2520multiple-choice%2520question-answer%250Apairs.%2520Integrating%2520this%2520dataset%2520for%2520fine-tuning%2520considerably%2520enhances%2520VLM%250Aperformance%2520across%2520multiple%2520benchmarks.%2520Unlike%2520methods%2520that%2520focus%2520solely%2520on%250Afine-grained%2520visual%2520content%252C%2520our%2520approach%2520leverages%2520accompanying%2520web%2520context%252C%250Ayielding%2520superior%2520results.%2520We%2520also%2520discover%2520that%2520a%2520%2560leaky%2520modality%2520mix%252C%2527%2520where%250Aconversation%2520samples%2520contain%2520questions%2520answerable%2520from%2520both%2520the%2520image%2520and%2520its%250Acontextual%2520caption%252C%2520outperforms%2520non-leaky%2520combinations%2520of%2520captions%2520and%2520Q%255C%2526A%250Apairs.%2520VisCon-100k%2520dataset%2520shows%2520strong%2520performance%2520with%2520two%2520popular%2520VLM%250Aapproaches%253A%2520text-only%2520large%2520language%2520model%2520%2528LLM%2529%2520aligned%2520with%2520a%2520vision%2520encoder%250Ausing%2520image%2520captions%2520data%2520%2528ShareGPT4V-7b%2529%2520and%2520multimodally%2520pretrained%2520LLM%250A%2528IDEFICS2-8b%2529%2520using%2520interleaved%2520image-text%2520data.%2520In%2520addition%2520to%2520releasing%2520the%250AVisCon-100K%2520dataset%252C%2520we%2520provide%2520a%2520contextual%2520captioner%2520trained%2520on%2520this%2520dataset%252C%250Afacilitating%2520scalable%2520fine-tuning%2520data%2520generation%2520for%2520future%2520research%2520and%250Aopen-source%2520applications.%2520Using%2520the%2520same%2520pipeline%252C%2520but%2520substituting%2520our%2520trained%250Acontextual%2520captioner%2520for%2520GPT-4V%252C%2520we%2520also%2520release%2520the%2520larger%2520VisCon-1M%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models&entry.906535625=Gokul%20Karthik%20Kumar%20and%20Iheb%20Chaabane%20and%20Kebin%20Wu&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20various%20visual%20benchmarks%20but%20are%0Aoften%20constrained%20by%20the%20lack%20of%20high-quality%20visual%20fine-tuning%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20VisCon-100K%2C%20a%20novel%20dataset%20derived%20from%0Ainterleaved%20image-text%20web%20documents.%20Our%20approach%20transforms%2045K%20web%20documents%0Afrom%20the%20OBELICS%20dataset%20into%20100K%20image%20conversation%20samples.%20We%20utilize%0AGPT-4V%20to%20generate%20image-contextual%20captions%20and%20OpenChat%203.5%20model%20to%20convert%0Athese%20captions%20into%20diverse%20free-form%20and%20multiple-choice%20question-answer%0Apairs.%20Integrating%20this%20dataset%20for%20fine-tuning%20considerably%20enhances%20VLM%0Aperformance%20across%20multiple%20benchmarks.%20Unlike%20methods%20that%20focus%20solely%20on%0Afine-grained%20visual%20content%2C%20our%20approach%20leverages%20accompanying%20web%20context%2C%0Ayielding%20superior%20results.%20We%20also%20discover%20that%20a%20%60leaky%20modality%20mix%2C%27%20where%0Aconversation%20samples%20contain%20questions%20answerable%20from%20both%20the%20image%20and%20its%0Acontextual%20caption%2C%20outperforms%20non-leaky%20combinations%20of%20captions%20and%20Q%5C%26A%0Apairs.%20VisCon-100k%20dataset%20shows%20strong%20performance%20with%20two%20popular%20VLM%0Aapproaches%3A%20text-only%20large%20language%20model%20%28LLM%29%20aligned%20with%20a%20vision%20encoder%0Ausing%20image%20captions%20data%20%28ShareGPT4V-7b%29%20and%20multimodally%20pretrained%20LLM%0A%28IDEFICS2-8b%29%20using%20interleaved%20image-text%20data.%20In%20addition%20to%20releasing%20the%0AVisCon-100K%20dataset%2C%20we%20provide%20a%20contextual%20captioner%20trained%20on%20this%20dataset%2C%0Afacilitating%20scalable%20fine-tuning%20data%20generation%20for%20future%20research%20and%0Aopen-source%20applications.%20Using%20the%20same%20pipeline%2C%20but%20substituting%20our%20trained%0Acontextual%20captioner%20for%20GPT-4V%2C%20we%20also%20release%20the%20larger%20VisCon-1M%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10250v1&entry.124074799=Read"},
{"title": "Topological Neural Networks over the Air", "author": "Simone Fiorellino and Claudio Battiloro and Paolo Di Lorenzo", "abstract": "  Topological neural networks (TNNs) are information processing architectures\nthat model representations from data lying over topological spaces (e.g.,\nsimplicial or cell complexes) and allow for decentralized implementation\nthrough localized communications over different neighborhoods. Existing TNN\narchitectures have not yet been considered in realistic communication\nscenarios, where channel effects typically introduce disturbances such as\nfading and noise. This paper aims to propose a novel TNN design, operating on\nregular cell complexes, that performs over-the-air computation, incorporating\nthe wireless communication model into its architecture. Specifically, during\ntraining and inference, the proposed method considers channel impairments such\nas fading and noise in the topological convolutional filtering operation, which\ntakes place over different signal orders and neighborhoods. Numerical results\nillustrate the architecture's robustness to channel impairments during testing\nand the superior performance with respect to existing architectures, which are\neither communication-agnostic or graph-based.\n", "link": "http://arxiv.org/abs/2502.10070v1", "date": "2025-02-14", "relevancy": 2.3302, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4643}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20Neural%20Networks%20over%20the%20Air&body=Title%3A%20Topological%20Neural%20Networks%20over%20the%20Air%0AAuthor%3A%20Simone%20Fiorellino%20and%20Claudio%20Battiloro%20and%20Paolo%20Di%20Lorenzo%0AAbstract%3A%20%20%20Topological%20neural%20networks%20%28TNNs%29%20are%20information%20processing%20architectures%0Athat%20model%20representations%20from%20data%20lying%20over%20topological%20spaces%20%28e.g.%2C%0Asimplicial%20or%20cell%20complexes%29%20and%20allow%20for%20decentralized%20implementation%0Athrough%20localized%20communications%20over%20different%20neighborhoods.%20Existing%20TNN%0Aarchitectures%20have%20not%20yet%20been%20considered%20in%20realistic%20communication%0Ascenarios%2C%20where%20channel%20effects%20typically%20introduce%20disturbances%20such%20as%0Afading%20and%20noise.%20This%20paper%20aims%20to%20propose%20a%20novel%20TNN%20design%2C%20operating%20on%0Aregular%20cell%20complexes%2C%20that%20performs%20over-the-air%20computation%2C%20incorporating%0Athe%20wireless%20communication%20model%20into%20its%20architecture.%20Specifically%2C%20during%0Atraining%20and%20inference%2C%20the%20proposed%20method%20considers%20channel%20impairments%20such%0Aas%20fading%20and%20noise%20in%20the%20topological%20convolutional%20filtering%20operation%2C%20which%0Atakes%20place%20over%20different%20signal%20orders%20and%20neighborhoods.%20Numerical%20results%0Aillustrate%20the%20architecture%27s%20robustness%20to%20channel%20impairments%20during%20testing%0Aand%20the%20superior%20performance%20with%20respect%20to%20existing%20architectures%2C%20which%20are%0Aeither%20communication-agnostic%20or%20graph-based.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520Neural%2520Networks%2520over%2520the%2520Air%26entry.906535625%3DSimone%2520Fiorellino%2520and%2520Claudio%2520Battiloro%2520and%2520Paolo%2520Di%2520Lorenzo%26entry.1292438233%3D%2520%2520Topological%2520neural%2520networks%2520%2528TNNs%2529%2520are%2520information%2520processing%2520architectures%250Athat%2520model%2520representations%2520from%2520data%2520lying%2520over%2520topological%2520spaces%2520%2528e.g.%252C%250Asimplicial%2520or%2520cell%2520complexes%2529%2520and%2520allow%2520for%2520decentralized%2520implementation%250Athrough%2520localized%2520communications%2520over%2520different%2520neighborhoods.%2520Existing%2520TNN%250Aarchitectures%2520have%2520not%2520yet%2520been%2520considered%2520in%2520realistic%2520communication%250Ascenarios%252C%2520where%2520channel%2520effects%2520typically%2520introduce%2520disturbances%2520such%2520as%250Afading%2520and%2520noise.%2520This%2520paper%2520aims%2520to%2520propose%2520a%2520novel%2520TNN%2520design%252C%2520operating%2520on%250Aregular%2520cell%2520complexes%252C%2520that%2520performs%2520over-the-air%2520computation%252C%2520incorporating%250Athe%2520wireless%2520communication%2520model%2520into%2520its%2520architecture.%2520Specifically%252C%2520during%250Atraining%2520and%2520inference%252C%2520the%2520proposed%2520method%2520considers%2520channel%2520impairments%2520such%250Aas%2520fading%2520and%2520noise%2520in%2520the%2520topological%2520convolutional%2520filtering%2520operation%252C%2520which%250Atakes%2520place%2520over%2520different%2520signal%2520orders%2520and%2520neighborhoods.%2520Numerical%2520results%250Aillustrate%2520the%2520architecture%2527s%2520robustness%2520to%2520channel%2520impairments%2520during%2520testing%250Aand%2520the%2520superior%2520performance%2520with%2520respect%2520to%2520existing%2520architectures%252C%2520which%2520are%250Aeither%2520communication-agnostic%2520or%2520graph-based.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20Neural%20Networks%20over%20the%20Air&entry.906535625=Simone%20Fiorellino%20and%20Claudio%20Battiloro%20and%20Paolo%20Di%20Lorenzo&entry.1292438233=%20%20Topological%20neural%20networks%20%28TNNs%29%20are%20information%20processing%20architectures%0Athat%20model%20representations%20from%20data%20lying%20over%20topological%20spaces%20%28e.g.%2C%0Asimplicial%20or%20cell%20complexes%29%20and%20allow%20for%20decentralized%20implementation%0Athrough%20localized%20communications%20over%20different%20neighborhoods.%20Existing%20TNN%0Aarchitectures%20have%20not%20yet%20been%20considered%20in%20realistic%20communication%0Ascenarios%2C%20where%20channel%20effects%20typically%20introduce%20disturbances%20such%20as%0Afading%20and%20noise.%20This%20paper%20aims%20to%20propose%20a%20novel%20TNN%20design%2C%20operating%20on%0Aregular%20cell%20complexes%2C%20that%20performs%20over-the-air%20computation%2C%20incorporating%0Athe%20wireless%20communication%20model%20into%20its%20architecture.%20Specifically%2C%20during%0Atraining%20and%20inference%2C%20the%20proposed%20method%20considers%20channel%20impairments%20such%0Aas%20fading%20and%20noise%20in%20the%20topological%20convolutional%20filtering%20operation%2C%20which%0Atakes%20place%20over%20different%20signal%20orders%20and%20neighborhoods.%20Numerical%20results%0Aillustrate%20the%20architecture%27s%20robustness%20to%20channel%20impairments%20during%20testing%0Aand%20the%20superior%20performance%20with%20respect%20to%20existing%20architectures%2C%20which%20are%0Aeither%20communication-agnostic%20or%20graph-based.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10070v1&entry.124074799=Read"},
{"title": "CTE-MLO: Continuous-time and Efficient Multi-LiDAR Odometry with\n  Localizability-aware Point Cloud Sampling", "author": "Hongming Shen and Zhenyu Wu and Yulin Hui and Wei Wang and Qiyang Lyu and Tianchen Deng and Yeqing Zhu and Bailing Tian and Danwei Wang", "abstract": "  In recent years, LiDAR-based localization and mapping methods have achieved\nsignificant progress thanks to their reliable and real-time localization\ncapability. Considering single LiDAR odometry often faces hardware failures and\ndegeneracy in practical scenarios, Multi-LiDAR Odometry (MLO), as an emerging\ntechnology, is studied to enhance the performance of LiDAR-based localization\nand mapping systems. However, MLO can suffer from high computational complexity\nintroduced by dense point clouds that are fused from multiple LiDARs, and the\ncontinuous-time measurement characteristic is constantly neglected by existing\nLiDAR odometry. This motivates us to develop a Continuous-Time and Efficient\nMLO, namely CTE-MLO, which can achieve accurate and real-time estimation using\nmulti-LiDAR measurements through a continuous-time perspective. In this paper,\nthe Gaussian process estimation is naturally combined with the Kalman filter,\nwhich enables each LiDAR point in a point stream to query the corresponding\ncontinuous-time trajectory using its time instants. A decentralized multi-LiDAR\nsynchronization scheme is also devised to combine points from separate LiDARs\ninto a single point cloud without the primary LiDAR assignment. Moreover, with\nthe aim of improving the real-time performance of MLO without sacrificing\nrobustness, a point cloud sampling strategy is designed with the consideration\nof localizability. To this end, CTE-MLO integrates synchronization,\nlocalizability-aware sampling, continuous-time estimation, and voxel map\nmanagement within a Kalman filter framework, which can achieve high accuracy\nand robust continuous-time estimation within only a few linear iterations. The\neffectiveness of the proposed method is demonstrated through various scenarios,\nincluding public datasets and real-world applications. The code is available at\nhttps://github.com/shenhm516/CTE-MLO to benefit the community.\n", "link": "http://arxiv.org/abs/2408.04901v2", "date": "2025-02-14", "relevancy": 2.3143, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6103}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTE-MLO%3A%20Continuous-time%20and%20Efficient%20Multi-LiDAR%20Odometry%20with%0A%20%20Localizability-aware%20Point%20Cloud%20Sampling&body=Title%3A%20CTE-MLO%3A%20Continuous-time%20and%20Efficient%20Multi-LiDAR%20Odometry%20with%0A%20%20Localizability-aware%20Point%20Cloud%20Sampling%0AAuthor%3A%20Hongming%20Shen%20and%20Zhenyu%20Wu%20and%20Yulin%20Hui%20and%20Wei%20Wang%20and%20Qiyang%20Lyu%20and%20Tianchen%20Deng%20and%20Yeqing%20Zhu%20and%20Bailing%20Tian%20and%20Danwei%20Wang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20LiDAR-based%20localization%20and%20mapping%20methods%20have%20achieved%0Asignificant%20progress%20thanks%20to%20their%20reliable%20and%20real-time%20localization%0Acapability.%20Considering%20single%20LiDAR%20odometry%20often%20faces%20hardware%20failures%20and%0Adegeneracy%20in%20practical%20scenarios%2C%20Multi-LiDAR%20Odometry%20%28MLO%29%2C%20as%20an%20emerging%0Atechnology%2C%20is%20studied%20to%20enhance%20the%20performance%20of%20LiDAR-based%20localization%0Aand%20mapping%20systems.%20However%2C%20MLO%20can%20suffer%20from%20high%20computational%20complexity%0Aintroduced%20by%20dense%20point%20clouds%20that%20are%20fused%20from%20multiple%20LiDARs%2C%20and%20the%0Acontinuous-time%20measurement%20characteristic%20is%20constantly%20neglected%20by%20existing%0ALiDAR%20odometry.%20This%20motivates%20us%20to%20develop%20a%20Continuous-Time%20and%20Efficient%0AMLO%2C%20namely%20CTE-MLO%2C%20which%20can%20achieve%20accurate%20and%20real-time%20estimation%20using%0Amulti-LiDAR%20measurements%20through%20a%20continuous-time%20perspective.%20In%20this%20paper%2C%0Athe%20Gaussian%20process%20estimation%20is%20naturally%20combined%20with%20the%20Kalman%20filter%2C%0Awhich%20enables%20each%20LiDAR%20point%20in%20a%20point%20stream%20to%20query%20the%20corresponding%0Acontinuous-time%20trajectory%20using%20its%20time%20instants.%20A%20decentralized%20multi-LiDAR%0Asynchronization%20scheme%20is%20also%20devised%20to%20combine%20points%20from%20separate%20LiDARs%0Ainto%20a%20single%20point%20cloud%20without%20the%20primary%20LiDAR%20assignment.%20Moreover%2C%20with%0Athe%20aim%20of%20improving%20the%20real-time%20performance%20of%20MLO%20without%20sacrificing%0Arobustness%2C%20a%20point%20cloud%20sampling%20strategy%20is%20designed%20with%20the%20consideration%0Aof%20localizability.%20To%20this%20end%2C%20CTE-MLO%20integrates%20synchronization%2C%0Alocalizability-aware%20sampling%2C%20continuous-time%20estimation%2C%20and%20voxel%20map%0Amanagement%20within%20a%20Kalman%20filter%20framework%2C%20which%20can%20achieve%20high%20accuracy%0Aand%20robust%20continuous-time%20estimation%20within%20only%20a%20few%20linear%20iterations.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20demonstrated%20through%20various%20scenarios%2C%0Aincluding%20public%20datasets%20and%20real-world%20applications.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shenhm516/CTE-MLO%20to%20benefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTE-MLO%253A%2520Continuous-time%2520and%2520Efficient%2520Multi-LiDAR%2520Odometry%2520with%250A%2520%2520Localizability-aware%2520Point%2520Cloud%2520Sampling%26entry.906535625%3DHongming%2520Shen%2520and%2520Zhenyu%2520Wu%2520and%2520Yulin%2520Hui%2520and%2520Wei%2520Wang%2520and%2520Qiyang%2520Lyu%2520and%2520Tianchen%2520Deng%2520and%2520Yeqing%2520Zhu%2520and%2520Bailing%2520Tian%2520and%2520Danwei%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520LiDAR-based%2520localization%2520and%2520mapping%2520methods%2520have%2520achieved%250Asignificant%2520progress%2520thanks%2520to%2520their%2520reliable%2520and%2520real-time%2520localization%250Acapability.%2520Considering%2520single%2520LiDAR%2520odometry%2520often%2520faces%2520hardware%2520failures%2520and%250Adegeneracy%2520in%2520practical%2520scenarios%252C%2520Multi-LiDAR%2520Odometry%2520%2528MLO%2529%252C%2520as%2520an%2520emerging%250Atechnology%252C%2520is%2520studied%2520to%2520enhance%2520the%2520performance%2520of%2520LiDAR-based%2520localization%250Aand%2520mapping%2520systems.%2520However%252C%2520MLO%2520can%2520suffer%2520from%2520high%2520computational%2520complexity%250Aintroduced%2520by%2520dense%2520point%2520clouds%2520that%2520are%2520fused%2520from%2520multiple%2520LiDARs%252C%2520and%2520the%250Acontinuous-time%2520measurement%2520characteristic%2520is%2520constantly%2520neglected%2520by%2520existing%250ALiDAR%2520odometry.%2520This%2520motivates%2520us%2520to%2520develop%2520a%2520Continuous-Time%2520and%2520Efficient%250AMLO%252C%2520namely%2520CTE-MLO%252C%2520which%2520can%2520achieve%2520accurate%2520and%2520real-time%2520estimation%2520using%250Amulti-LiDAR%2520measurements%2520through%2520a%2520continuous-time%2520perspective.%2520In%2520this%2520paper%252C%250Athe%2520Gaussian%2520process%2520estimation%2520is%2520naturally%2520combined%2520with%2520the%2520Kalman%2520filter%252C%250Awhich%2520enables%2520each%2520LiDAR%2520point%2520in%2520a%2520point%2520stream%2520to%2520query%2520the%2520corresponding%250Acontinuous-time%2520trajectory%2520using%2520its%2520time%2520instants.%2520A%2520decentralized%2520multi-LiDAR%250Asynchronization%2520scheme%2520is%2520also%2520devised%2520to%2520combine%2520points%2520from%2520separate%2520LiDARs%250Ainto%2520a%2520single%2520point%2520cloud%2520without%2520the%2520primary%2520LiDAR%2520assignment.%2520Moreover%252C%2520with%250Athe%2520aim%2520of%2520improving%2520the%2520real-time%2520performance%2520of%2520MLO%2520without%2520sacrificing%250Arobustness%252C%2520a%2520point%2520cloud%2520sampling%2520strategy%2520is%2520designed%2520with%2520the%2520consideration%250Aof%2520localizability.%2520To%2520this%2520end%252C%2520CTE-MLO%2520integrates%2520synchronization%252C%250Alocalizability-aware%2520sampling%252C%2520continuous-time%2520estimation%252C%2520and%2520voxel%2520map%250Amanagement%2520within%2520a%2520Kalman%2520filter%2520framework%252C%2520which%2520can%2520achieve%2520high%2520accuracy%250Aand%2520robust%2520continuous-time%2520estimation%2520within%2520only%2520a%2520few%2520linear%2520iterations.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520is%2520demonstrated%2520through%2520various%2520scenarios%252C%250Aincluding%2520public%2520datasets%2520and%2520real-world%2520applications.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shenhm516/CTE-MLO%2520to%2520benefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTE-MLO%3A%20Continuous-time%20and%20Efficient%20Multi-LiDAR%20Odometry%20with%0A%20%20Localizability-aware%20Point%20Cloud%20Sampling&entry.906535625=Hongming%20Shen%20and%20Zhenyu%20Wu%20and%20Yulin%20Hui%20and%20Wei%20Wang%20and%20Qiyang%20Lyu%20and%20Tianchen%20Deng%20and%20Yeqing%20Zhu%20and%20Bailing%20Tian%20and%20Danwei%20Wang&entry.1292438233=%20%20In%20recent%20years%2C%20LiDAR-based%20localization%20and%20mapping%20methods%20have%20achieved%0Asignificant%20progress%20thanks%20to%20their%20reliable%20and%20real-time%20localization%0Acapability.%20Considering%20single%20LiDAR%20odometry%20often%20faces%20hardware%20failures%20and%0Adegeneracy%20in%20practical%20scenarios%2C%20Multi-LiDAR%20Odometry%20%28MLO%29%2C%20as%20an%20emerging%0Atechnology%2C%20is%20studied%20to%20enhance%20the%20performance%20of%20LiDAR-based%20localization%0Aand%20mapping%20systems.%20However%2C%20MLO%20can%20suffer%20from%20high%20computational%20complexity%0Aintroduced%20by%20dense%20point%20clouds%20that%20are%20fused%20from%20multiple%20LiDARs%2C%20and%20the%0Acontinuous-time%20measurement%20characteristic%20is%20constantly%20neglected%20by%20existing%0ALiDAR%20odometry.%20This%20motivates%20us%20to%20develop%20a%20Continuous-Time%20and%20Efficient%0AMLO%2C%20namely%20CTE-MLO%2C%20which%20can%20achieve%20accurate%20and%20real-time%20estimation%20using%0Amulti-LiDAR%20measurements%20through%20a%20continuous-time%20perspective.%20In%20this%20paper%2C%0Athe%20Gaussian%20process%20estimation%20is%20naturally%20combined%20with%20the%20Kalman%20filter%2C%0Awhich%20enables%20each%20LiDAR%20point%20in%20a%20point%20stream%20to%20query%20the%20corresponding%0Acontinuous-time%20trajectory%20using%20its%20time%20instants.%20A%20decentralized%20multi-LiDAR%0Asynchronization%20scheme%20is%20also%20devised%20to%20combine%20points%20from%20separate%20LiDARs%0Ainto%20a%20single%20point%20cloud%20without%20the%20primary%20LiDAR%20assignment.%20Moreover%2C%20with%0Athe%20aim%20of%20improving%20the%20real-time%20performance%20of%20MLO%20without%20sacrificing%0Arobustness%2C%20a%20point%20cloud%20sampling%20strategy%20is%20designed%20with%20the%20consideration%0Aof%20localizability.%20To%20this%20end%2C%20CTE-MLO%20integrates%20synchronization%2C%0Alocalizability-aware%20sampling%2C%20continuous-time%20estimation%2C%20and%20voxel%20map%0Amanagement%20within%20a%20Kalman%20filter%20framework%2C%20which%20can%20achieve%20high%20accuracy%0Aand%20robust%20continuous-time%20estimation%20within%20only%20a%20few%20linear%20iterations.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20demonstrated%20through%20various%20scenarios%2C%0Aincluding%20public%20datasets%20and%20real-world%20applications.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shenhm516/CTE-MLO%20to%20benefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04901v2&entry.124074799=Read"},
{"title": "Graph Foundation Models for Recommendation: A Comprehensive Survey", "author": "Bin Wu and Yihang Wang and Yuanhao Zeng and Jiawei Liu and Jiashu Zhao and Cheng Yang and Yawen Li and Long Xia and Dawei Yin and Chuan Shi", "abstract": "  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n", "link": "http://arxiv.org/abs/2502.08346v2", "date": "2025-02-14", "relevancy": 2.3113, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Foundation%20Models%20for%20Recommendation%3A%20A%20Comprehensive%20Survey&body=Title%3A%20Graph%20Foundation%20Models%20for%20Recommendation%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Bin%20Wu%20and%20Yihang%20Wang%20and%20Yuanhao%20Zeng%20and%20Jiawei%20Liu%20and%20Jiashu%20Zhao%20and%20Cheng%20Yang%20and%20Yawen%20Li%20and%20Long%20Xia%20and%20Dawei%20Yin%20and%20Chuan%20Shi%0AAbstract%3A%20%20%20Recommender%20systems%20%28RS%29%20serve%20as%20a%20fundamental%20tool%20for%20navigating%20the%20vast%0Aexpanse%20of%20online%20information%2C%20with%20deep%20learning%20advancements%20playing%20an%0Aincreasingly%20important%20role%20in%20improving%20ranking%20accuracy.%20Among%20these%2C%20graph%0Aneural%20networks%20%28GNNs%29%20excel%20at%20extracting%20higher-order%20structural%20information%2C%0Awhile%20large%20language%20models%20%28LLMs%29%20are%20designed%20to%20process%20and%20comprehend%0Anatural%20language%2C%20making%20both%20approaches%20highly%20effective%20and%20widely%20adopted.%0ARecent%20research%20has%20focused%20on%20graph%20foundation%20models%20%28GFMs%29%2C%20which%20integrate%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20to%20model%20complex%20RS%20problems%20more%20efficiently%20by%0Aleveraging%20the%20graph-based%20structure%20of%20user-item%20relationships%20alongside%0Atextual%20understanding.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%0AGFM-based%20RS%20technologies%20by%20introducing%20a%20clear%20taxonomy%20of%20current%0Aapproaches%2C%20diving%20into%20methodological%20details%2C%20and%20highlighting%20key%20challenges%0Aand%20future%20directions.%20By%20synthesizing%20recent%20advancements%2C%20we%20aim%20to%20offer%0Avaluable%20insights%20into%20the%20evolving%20landscape%20of%20GFM-based%20recommender%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Foundation%2520Models%2520for%2520Recommendation%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DBin%2520Wu%2520and%2520Yihang%2520Wang%2520and%2520Yuanhao%2520Zeng%2520and%2520Jiawei%2520Liu%2520and%2520Jiashu%2520Zhao%2520and%2520Cheng%2520Yang%2520and%2520Yawen%2520Li%2520and%2520Long%2520Xia%2520and%2520Dawei%2520Yin%2520and%2520Chuan%2520Shi%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520%2528RS%2529%2520serve%2520as%2520a%2520fundamental%2520tool%2520for%2520navigating%2520the%2520vast%250Aexpanse%2520of%2520online%2520information%252C%2520with%2520deep%2520learning%2520advancements%2520playing%2520an%250Aincreasingly%2520important%2520role%2520in%2520improving%2520ranking%2520accuracy.%2520Among%2520these%252C%2520graph%250Aneural%2520networks%2520%2528GNNs%2529%2520excel%2520at%2520extracting%2520higher-order%2520structural%2520information%252C%250Awhile%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520designed%2520to%2520process%2520and%2520comprehend%250Anatural%2520language%252C%2520making%2520both%2520approaches%2520highly%2520effective%2520and%2520widely%2520adopted.%250ARecent%2520research%2520has%2520focused%2520on%2520graph%2520foundation%2520models%2520%2528GFMs%2529%252C%2520which%2520integrate%250Athe%2520strengths%2520of%2520GNNs%2520and%2520LLMs%2520to%2520model%2520complex%2520RS%2520problems%2520more%2520efficiently%2520by%250Aleveraging%2520the%2520graph-based%2520structure%2520of%2520user-item%2520relationships%2520alongside%250Atextual%2520understanding.%2520In%2520this%2520survey%252C%2520we%2520provide%2520a%2520comprehensive%2520overview%2520of%250AGFM-based%2520RS%2520technologies%2520by%2520introducing%2520a%2520clear%2520taxonomy%2520of%2520current%250Aapproaches%252C%2520diving%2520into%2520methodological%2520details%252C%2520and%2520highlighting%2520key%2520challenges%250Aand%2520future%2520directions.%2520By%2520synthesizing%2520recent%2520advancements%252C%2520we%2520aim%2520to%2520offer%250Avaluable%2520insights%2520into%2520the%2520evolving%2520landscape%2520of%2520GFM-based%2520recommender%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Foundation%20Models%20for%20Recommendation%3A%20A%20Comprehensive%20Survey&entry.906535625=Bin%20Wu%20and%20Yihang%20Wang%20and%20Yuanhao%20Zeng%20and%20Jiawei%20Liu%20and%20Jiashu%20Zhao%20and%20Cheng%20Yang%20and%20Yawen%20Li%20and%20Long%20Xia%20and%20Dawei%20Yin%20and%20Chuan%20Shi&entry.1292438233=%20%20Recommender%20systems%20%28RS%29%20serve%20as%20a%20fundamental%20tool%20for%20navigating%20the%20vast%0Aexpanse%20of%20online%20information%2C%20with%20deep%20learning%20advancements%20playing%20an%0Aincreasingly%20important%20role%20in%20improving%20ranking%20accuracy.%20Among%20these%2C%20graph%0Aneural%20networks%20%28GNNs%29%20excel%20at%20extracting%20higher-order%20structural%20information%2C%0Awhile%20large%20language%20models%20%28LLMs%29%20are%20designed%20to%20process%20and%20comprehend%0Anatural%20language%2C%20making%20both%20approaches%20highly%20effective%20and%20widely%20adopted.%0ARecent%20research%20has%20focused%20on%20graph%20foundation%20models%20%28GFMs%29%2C%20which%20integrate%0Athe%20strengths%20of%20GNNs%20and%20LLMs%20to%20model%20complex%20RS%20problems%20more%20efficiently%20by%0Aleveraging%20the%20graph-based%20structure%20of%20user-item%20relationships%20alongside%0Atextual%20understanding.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20overview%20of%0AGFM-based%20RS%20technologies%20by%20introducing%20a%20clear%20taxonomy%20of%20current%0Aapproaches%2C%20diving%20into%20methodological%20details%2C%20and%20highlighting%20key%20challenges%0Aand%20future%20directions.%20By%20synthesizing%20recent%20advancements%2C%20we%20aim%20to%20offer%0Avaluable%20insights%20into%20the%20evolving%20landscape%20of%20GFM-based%20recommender%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08346v2&entry.124074799=Read"},
{"title": "Artificial Intelligence to Assess Dental Findings from Panoramic\n  Radiographs -- A Multinational Study", "author": "Yin-Chih Chelsea Wang and Tsao-Lun Chen and Shankeeth Vinayahalingam and Tai-Hsien Wu and Chu Wei Chang and Hsuan Hao Chang and Hung-Jen Wei and Mu-Hsiung Chen and Ching-Chang Ko and David Anssari Moin and Bram van Ginneken and Tong Xi and Hsiao-Cheng Tsai and Min-Huey Chen and Tzu-Ming Harry Hsu and Hye Chou", "abstract": "  Dental panoramic radiographs (DPRs) are widely used in clinical practice for\ncomprehensive oral assessment but present challenges due to overlapping\nstructures and time constraints in interpretation.\n  This study aimed to establish a solid baseline for the AI-automated\nassessment of findings in DPRs by developing, evaluating an AI system, and\ncomparing its performance with that of human readers across multinational data\nsets.\n  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and\nTaiwan), focusing on 8 types of dental findings. The AI system combined object\ndetection and semantic segmentation techniques for per-tooth finding\nidentification. Performance metrics included sensitivity, specificity, and area\nunder the receiver operating characteristic curve (AUC-ROC). AI\ngeneralizability was tested across data sets, and performance was compared with\nhuman dental practitioners.\n  The AI system demonstrated comparable or superior performance to human\nreaders, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for\nidentifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008)\nsensitivity for identifying missing teeth. The AI achieved a macro-averaged\nAUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with\nthe reference were comparable to inter-human agreements in 7 of 8 findings\nexcept for caries (p = .024). The AI system demonstrated robust generalization\nacross diverse imaging and demographic settings and processed images 79 times\nfaster (95% CI: 75-82) than human readers.\n  The AI system effectively assessed findings in DPRs, achieving performance on\npar with or better than human experts while significantly reducing\ninterpretation time. These results highlight the potential for integrating AI\ninto clinical workflows to improve diagnostic efficiency and accuracy, and\npatient management.\n", "link": "http://arxiv.org/abs/2502.10277v1", "date": "2025-02-14", "relevancy": 2.2988, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4624}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20to%20Assess%20Dental%20Findings%20from%20Panoramic%0A%20%20Radiographs%20--%20A%20Multinational%20Study&body=Title%3A%20Artificial%20Intelligence%20to%20Assess%20Dental%20Findings%20from%20Panoramic%0A%20%20Radiographs%20--%20A%20Multinational%20Study%0AAuthor%3A%20Yin-Chih%20Chelsea%20Wang%20and%20Tsao-Lun%20Chen%20and%20Shankeeth%20Vinayahalingam%20and%20Tai-Hsien%20Wu%20and%20Chu%20Wei%20Chang%20and%20Hsuan%20Hao%20Chang%20and%20Hung-Jen%20Wei%20and%20Mu-Hsiung%20Chen%20and%20Ching-Chang%20Ko%20and%20David%20Anssari%20Moin%20and%20Bram%20van%20Ginneken%20and%20Tong%20Xi%20and%20Hsiao-Cheng%20Tsai%20and%20Min-Huey%20Chen%20and%20Tzu-Ming%20Harry%20Hsu%20and%20Hye%20Chou%0AAbstract%3A%20%20%20Dental%20panoramic%20radiographs%20%28DPRs%29%20are%20widely%20used%20in%20clinical%20practice%20for%0Acomprehensive%20oral%20assessment%20but%20present%20challenges%20due%20to%20overlapping%0Astructures%20and%20time%20constraints%20in%20interpretation.%0A%20%20This%20study%20aimed%20to%20establish%20a%20solid%20baseline%20for%20the%20AI-automated%0Aassessment%20of%20findings%20in%20DPRs%20by%20developing%2C%20evaluating%20an%20AI%20system%2C%20and%0Acomparing%20its%20performance%20with%20that%20of%20human%20readers%20across%20multinational%20data%0Asets.%0A%20%20We%20analyzed%206%2C669%20DPRs%20from%20three%20data%20sets%20%28the%20Netherlands%2C%20Brazil%2C%20and%0ATaiwan%29%2C%20focusing%20on%208%20types%20of%20dental%20findings.%20The%20AI%20system%20combined%20object%0Adetection%20and%20semantic%20segmentation%20techniques%20for%20per-tooth%20finding%0Aidentification.%20Performance%20metrics%20included%20sensitivity%2C%20specificity%2C%20and%20area%0Aunder%20the%20receiver%20operating%20characteristic%20curve%20%28AUC-ROC%29.%20AI%0Ageneralizability%20was%20tested%20across%20data%20sets%2C%20and%20performance%20was%20compared%20with%0Ahuman%20dental%20practitioners.%0A%20%20The%20AI%20system%20demonstrated%20comparable%20or%20superior%20performance%20to%20human%0Areaders%2C%20particularly%20%2B67.9%25%20%2895%25%20CI%3A%2054.0%25-81.9%25%3B%20p%20%3C%20.001%29%20sensitivity%20for%0Aidentifying%20periapical%20radiolucencies%20and%20%2B4.7%25%20%2895%25%20CI%3A%201.4%25-8.0%25%3B%20p%20%3D%20.008%29%0Asensitivity%20for%20identifying%20missing%20teeth.%20The%20AI%20achieved%20a%20macro-averaged%0AAUC-ROC%20of%2096.2%25%20%2895%25%20CI%3A%2094.6%25-97.8%25%29%20across%208%20findings.%20AI%20agreements%20with%0Athe%20reference%20were%20comparable%20to%20inter-human%20agreements%20in%207%20of%208%20findings%0Aexcept%20for%20caries%20%28p%20%3D%20.024%29.%20The%20AI%20system%20demonstrated%20robust%20generalization%0Aacross%20diverse%20imaging%20and%20demographic%20settings%20and%20processed%20images%2079%20times%0Afaster%20%2895%25%20CI%3A%2075-82%29%20than%20human%20readers.%0A%20%20The%20AI%20system%20effectively%20assessed%20findings%20in%20DPRs%2C%20achieving%20performance%20on%0Apar%20with%20or%20better%20than%20human%20experts%20while%20significantly%20reducing%0Ainterpretation%20time.%20These%20results%20highlight%20the%20potential%20for%20integrating%20AI%0Ainto%20clinical%20workflows%20to%20improve%20diagnostic%20efficiency%20and%20accuracy%2C%20and%0Apatient%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520to%2520Assess%2520Dental%2520Findings%2520from%2520Panoramic%250A%2520%2520Radiographs%2520--%2520A%2520Multinational%2520Study%26entry.906535625%3DYin-Chih%2520Chelsea%2520Wang%2520and%2520Tsao-Lun%2520Chen%2520and%2520Shankeeth%2520Vinayahalingam%2520and%2520Tai-Hsien%2520Wu%2520and%2520Chu%2520Wei%2520Chang%2520and%2520Hsuan%2520Hao%2520Chang%2520and%2520Hung-Jen%2520Wei%2520and%2520Mu-Hsiung%2520Chen%2520and%2520Ching-Chang%2520Ko%2520and%2520David%2520Anssari%2520Moin%2520and%2520Bram%2520van%2520Ginneken%2520and%2520Tong%2520Xi%2520and%2520Hsiao-Cheng%2520Tsai%2520and%2520Min-Huey%2520Chen%2520and%2520Tzu-Ming%2520Harry%2520Hsu%2520and%2520Hye%2520Chou%26entry.1292438233%3D%2520%2520Dental%2520panoramic%2520radiographs%2520%2528DPRs%2529%2520are%2520widely%2520used%2520in%2520clinical%2520practice%2520for%250Acomprehensive%2520oral%2520assessment%2520but%2520present%2520challenges%2520due%2520to%2520overlapping%250Astructures%2520and%2520time%2520constraints%2520in%2520interpretation.%250A%2520%2520This%2520study%2520aimed%2520to%2520establish%2520a%2520solid%2520baseline%2520for%2520the%2520AI-automated%250Aassessment%2520of%2520findings%2520in%2520DPRs%2520by%2520developing%252C%2520evaluating%2520an%2520AI%2520system%252C%2520and%250Acomparing%2520its%2520performance%2520with%2520that%2520of%2520human%2520readers%2520across%2520multinational%2520data%250Asets.%250A%2520%2520We%2520analyzed%25206%252C669%2520DPRs%2520from%2520three%2520data%2520sets%2520%2528the%2520Netherlands%252C%2520Brazil%252C%2520and%250ATaiwan%2529%252C%2520focusing%2520on%25208%2520types%2520of%2520dental%2520findings.%2520The%2520AI%2520system%2520combined%2520object%250Adetection%2520and%2520semantic%2520segmentation%2520techniques%2520for%2520per-tooth%2520finding%250Aidentification.%2520Performance%2520metrics%2520included%2520sensitivity%252C%2520specificity%252C%2520and%2520area%250Aunder%2520the%2520receiver%2520operating%2520characteristic%2520curve%2520%2528AUC-ROC%2529.%2520AI%250Ageneralizability%2520was%2520tested%2520across%2520data%2520sets%252C%2520and%2520performance%2520was%2520compared%2520with%250Ahuman%2520dental%2520practitioners.%250A%2520%2520The%2520AI%2520system%2520demonstrated%2520comparable%2520or%2520superior%2520performance%2520to%2520human%250Areaders%252C%2520particularly%2520%252B67.9%2525%2520%252895%2525%2520CI%253A%252054.0%2525-81.9%2525%253B%2520p%2520%253C%2520.001%2529%2520sensitivity%2520for%250Aidentifying%2520periapical%2520radiolucencies%2520and%2520%252B4.7%2525%2520%252895%2525%2520CI%253A%25201.4%2525-8.0%2525%253B%2520p%2520%253D%2520.008%2529%250Asensitivity%2520for%2520identifying%2520missing%2520teeth.%2520The%2520AI%2520achieved%2520a%2520macro-averaged%250AAUC-ROC%2520of%252096.2%2525%2520%252895%2525%2520CI%253A%252094.6%2525-97.8%2525%2529%2520across%25208%2520findings.%2520AI%2520agreements%2520with%250Athe%2520reference%2520were%2520comparable%2520to%2520inter-human%2520agreements%2520in%25207%2520of%25208%2520findings%250Aexcept%2520for%2520caries%2520%2528p%2520%253D%2520.024%2529.%2520The%2520AI%2520system%2520demonstrated%2520robust%2520generalization%250Aacross%2520diverse%2520imaging%2520and%2520demographic%2520settings%2520and%2520processed%2520images%252079%2520times%250Afaster%2520%252895%2525%2520CI%253A%252075-82%2529%2520than%2520human%2520readers.%250A%2520%2520The%2520AI%2520system%2520effectively%2520assessed%2520findings%2520in%2520DPRs%252C%2520achieving%2520performance%2520on%250Apar%2520with%2520or%2520better%2520than%2520human%2520experts%2520while%2520significantly%2520reducing%250Ainterpretation%2520time.%2520These%2520results%2520highlight%2520the%2520potential%2520for%2520integrating%2520AI%250Ainto%2520clinical%2520workflows%2520to%2520improve%2520diagnostic%2520efficiency%2520and%2520accuracy%252C%2520and%250Apatient%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20to%20Assess%20Dental%20Findings%20from%20Panoramic%0A%20%20Radiographs%20--%20A%20Multinational%20Study&entry.906535625=Yin-Chih%20Chelsea%20Wang%20and%20Tsao-Lun%20Chen%20and%20Shankeeth%20Vinayahalingam%20and%20Tai-Hsien%20Wu%20and%20Chu%20Wei%20Chang%20and%20Hsuan%20Hao%20Chang%20and%20Hung-Jen%20Wei%20and%20Mu-Hsiung%20Chen%20and%20Ching-Chang%20Ko%20and%20David%20Anssari%20Moin%20and%20Bram%20van%20Ginneken%20and%20Tong%20Xi%20and%20Hsiao-Cheng%20Tsai%20and%20Min-Huey%20Chen%20and%20Tzu-Ming%20Harry%20Hsu%20and%20Hye%20Chou&entry.1292438233=%20%20Dental%20panoramic%20radiographs%20%28DPRs%29%20are%20widely%20used%20in%20clinical%20practice%20for%0Acomprehensive%20oral%20assessment%20but%20present%20challenges%20due%20to%20overlapping%0Astructures%20and%20time%20constraints%20in%20interpretation.%0A%20%20This%20study%20aimed%20to%20establish%20a%20solid%20baseline%20for%20the%20AI-automated%0Aassessment%20of%20findings%20in%20DPRs%20by%20developing%2C%20evaluating%20an%20AI%20system%2C%20and%0Acomparing%20its%20performance%20with%20that%20of%20human%20readers%20across%20multinational%20data%0Asets.%0A%20%20We%20analyzed%206%2C669%20DPRs%20from%20three%20data%20sets%20%28the%20Netherlands%2C%20Brazil%2C%20and%0ATaiwan%29%2C%20focusing%20on%208%20types%20of%20dental%20findings.%20The%20AI%20system%20combined%20object%0Adetection%20and%20semantic%20segmentation%20techniques%20for%20per-tooth%20finding%0Aidentification.%20Performance%20metrics%20included%20sensitivity%2C%20specificity%2C%20and%20area%0Aunder%20the%20receiver%20operating%20characteristic%20curve%20%28AUC-ROC%29.%20AI%0Ageneralizability%20was%20tested%20across%20data%20sets%2C%20and%20performance%20was%20compared%20with%0Ahuman%20dental%20practitioners.%0A%20%20The%20AI%20system%20demonstrated%20comparable%20or%20superior%20performance%20to%20human%0Areaders%2C%20particularly%20%2B67.9%25%20%2895%25%20CI%3A%2054.0%25-81.9%25%3B%20p%20%3C%20.001%29%20sensitivity%20for%0Aidentifying%20periapical%20radiolucencies%20and%20%2B4.7%25%20%2895%25%20CI%3A%201.4%25-8.0%25%3B%20p%20%3D%20.008%29%0Asensitivity%20for%20identifying%20missing%20teeth.%20The%20AI%20achieved%20a%20macro-averaged%0AAUC-ROC%20of%2096.2%25%20%2895%25%20CI%3A%2094.6%25-97.8%25%29%20across%208%20findings.%20AI%20agreements%20with%0Athe%20reference%20were%20comparable%20to%20inter-human%20agreements%20in%207%20of%208%20findings%0Aexcept%20for%20caries%20%28p%20%3D%20.024%29.%20The%20AI%20system%20demonstrated%20robust%20generalization%0Aacross%20diverse%20imaging%20and%20demographic%20settings%20and%20processed%20images%2079%20times%0Afaster%20%2895%25%20CI%3A%2075-82%29%20than%20human%20readers.%0A%20%20The%20AI%20system%20effectively%20assessed%20findings%20in%20DPRs%2C%20achieving%20performance%20on%0Apar%20with%20or%20better%20than%20human%20experts%20while%20significantly%20reducing%0Ainterpretation%20time.%20These%20results%20highlight%20the%20potential%20for%20integrating%20AI%0Ainto%20clinical%20workflows%20to%20improve%20diagnostic%20efficiency%20and%20accuracy%2C%20and%0Apatient%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10277v1&entry.124074799=Read"},
{"title": "LLM-Powered Preference Elicitation in Combinatorial Assignment", "author": "Ermis Soumalias and Yanchen Jiang and Kehang Zhu and Michael Curry and Sven Seuken and David C. Parkes", "abstract": "  We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.\n", "link": "http://arxiv.org/abs/2502.10308v1", "date": "2025-02-14", "relevancy": 2.2975, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Powered%20Preference%20Elicitation%20in%20Combinatorial%20Assignment&body=Title%3A%20LLM-Powered%20Preference%20Elicitation%20in%20Combinatorial%20Assignment%0AAuthor%3A%20Ermis%20Soumalias%20and%20Yanchen%20Jiang%20and%20Kehang%20Zhu%20and%20Michael%20Curry%20and%20Sven%20Seuken%20and%20David%20C.%20Parkes%0AAbstract%3A%20%20%20We%20study%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20as%20proxies%20for%20humans%0Ato%20simplify%20preference%20elicitation%20%28PE%29%20in%20combinatorial%20assignment.%20While%0Atraditional%20PE%20methods%20rely%20on%20iterative%20queries%20to%20capture%20preferences%2C%20LLMs%0Aoffer%20a%20one-shot%20alternative%20with%20reduced%20human%20effort.%20We%20propose%20a%20framework%0Afor%20LLM%20proxies%20that%20can%20work%20in%20tandem%20with%20SOTA%20ML-powered%20preference%0Aelicitation%20schemes.%20Our%20framework%20handles%20the%20novel%20challenges%20introduced%20by%0ALLMs%2C%20such%20as%20response%20variability%20and%20increased%20computational%20costs.%20We%0Aexperimentally%20evaluate%20the%20efficiency%20of%20LLM%20proxies%20against%20human%20queries%20in%0Athe%20well-studied%20course%20allocation%20domain%2C%20and%20we%20investigate%20the%20model%0Acapabilities%20required%20for%20success.%20We%20find%20that%20our%20approach%20improves%0Aallocative%20efficiency%20by%20up%20to%2020%25%2C%20and%20these%20results%20are%20robust%20across%0Adifferent%20LLMs%20and%20to%20differences%20in%20quality%20and%20accuracy%20of%20reporting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Powered%2520Preference%2520Elicitation%2520in%2520Combinatorial%2520Assignment%26entry.906535625%3DErmis%2520Soumalias%2520and%2520Yanchen%2520Jiang%2520and%2520Kehang%2520Zhu%2520and%2520Michael%2520Curry%2520and%2520Sven%2520Seuken%2520and%2520David%2520C.%2520Parkes%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520potential%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520proxies%2520for%2520humans%250Ato%2520simplify%2520preference%2520elicitation%2520%2528PE%2529%2520in%2520combinatorial%2520assignment.%2520While%250Atraditional%2520PE%2520methods%2520rely%2520on%2520iterative%2520queries%2520to%2520capture%2520preferences%252C%2520LLMs%250Aoffer%2520a%2520one-shot%2520alternative%2520with%2520reduced%2520human%2520effort.%2520We%2520propose%2520a%2520framework%250Afor%2520LLM%2520proxies%2520that%2520can%2520work%2520in%2520tandem%2520with%2520SOTA%2520ML-powered%2520preference%250Aelicitation%2520schemes.%2520Our%2520framework%2520handles%2520the%2520novel%2520challenges%2520introduced%2520by%250ALLMs%252C%2520such%2520as%2520response%2520variability%2520and%2520increased%2520computational%2520costs.%2520We%250Aexperimentally%2520evaluate%2520the%2520efficiency%2520of%2520LLM%2520proxies%2520against%2520human%2520queries%2520in%250Athe%2520well-studied%2520course%2520allocation%2520domain%252C%2520and%2520we%2520investigate%2520the%2520model%250Acapabilities%2520required%2520for%2520success.%2520We%2520find%2520that%2520our%2520approach%2520improves%250Aallocative%2520efficiency%2520by%2520up%2520to%252020%2525%252C%2520and%2520these%2520results%2520are%2520robust%2520across%250Adifferent%2520LLMs%2520and%2520to%2520differences%2520in%2520quality%2520and%2520accuracy%2520of%2520reporting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Powered%20Preference%20Elicitation%20in%20Combinatorial%20Assignment&entry.906535625=Ermis%20Soumalias%20and%20Yanchen%20Jiang%20and%20Kehang%20Zhu%20and%20Michael%20Curry%20and%20Sven%20Seuken%20and%20David%20C.%20Parkes&entry.1292438233=%20%20We%20study%20the%20potential%20of%20large%20language%20models%20%28LLMs%29%20as%20proxies%20for%20humans%0Ato%20simplify%20preference%20elicitation%20%28PE%29%20in%20combinatorial%20assignment.%20While%0Atraditional%20PE%20methods%20rely%20on%20iterative%20queries%20to%20capture%20preferences%2C%20LLMs%0Aoffer%20a%20one-shot%20alternative%20with%20reduced%20human%20effort.%20We%20propose%20a%20framework%0Afor%20LLM%20proxies%20that%20can%20work%20in%20tandem%20with%20SOTA%20ML-powered%20preference%0Aelicitation%20schemes.%20Our%20framework%20handles%20the%20novel%20challenges%20introduced%20by%0ALLMs%2C%20such%20as%20response%20variability%20and%20increased%20computational%20costs.%20We%0Aexperimentally%20evaluate%20the%20efficiency%20of%20LLM%20proxies%20against%20human%20queries%20in%0Athe%20well-studied%20course%20allocation%20domain%2C%20and%20we%20investigate%20the%20model%0Acapabilities%20required%20for%20success.%20We%20find%20that%20our%20approach%20improves%0Aallocative%20efficiency%20by%20up%20to%2020%25%2C%20and%20these%20results%20are%20robust%20across%0Adifferent%20LLMs%20and%20to%20differences%20in%20quality%20and%20accuracy%20of%20reporting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10308v1&entry.124074799=Read"},
{"title": "City-Scale Multi-Camera Vehicle Tracking System with Improved\n  Self-Supervised Camera Link Model", "author": "Yuqiang Lin and Sam Lockyer and Nic Zhang", "abstract": "  Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms\nthe basis for numerous future city-wide systems (e.g. traffic management, crash\ndetection, etc.). However, the challenge of matching vehicle trajectories\nacross different cameras based solely on feature extraction poses significant\ndifficulties. This article introduces an innovative multi-camera vehicle\ntracking system that utilizes a self-supervised camera link model. In contrast\nto related works that rely on manual spatial-temporal annotations, our model\nautomatically extracts crucial multi-camera relationships for vehicle matching.\nThe camera link is established through a pre-matching process that evaluates\nfeature similarities, pair numbers, and time variance for high-quality tracks.\nThis process calculates the probability of spatial linkage for all camera\ncombinations, selecting the highest scoring pairs to create camera links. Our\napproach significantly improves deployment times by eliminating the need for\nhuman annotation, offering substantial improvements in efficiency and\ncost-effectiveness when it comes to real-world application. This pairing\nprocess supports cross camera matching by setting spatial-temporal constraints,\nreducing the searching space for potential vehicle matches. According to our\nexperimental results, the proposed method achieves a new state-of-the-art among\nautomatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1\nScore.\n", "link": "http://arxiv.org/abs/2405.11345v3", "date": "2025-02-14", "relevancy": 2.2917, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6025}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5689}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model&body=Title%3A%20City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model%0AAuthor%3A%20Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Nic%20Zhang%0AAbstract%3A%20%20%20Multi-Target%20Multi-Camera%20Tracking%20%28MTMCT%29%20has%20broad%20applications%20and%20forms%0Athe%20basis%20for%20numerous%20future%20city-wide%20systems%20%28e.g.%20traffic%20management%2C%20crash%0Adetection%2C%20etc.%29.%20However%2C%20the%20challenge%20of%20matching%20vehicle%20trajectories%0Aacross%20different%20cameras%20based%20solely%20on%20feature%20extraction%20poses%20significant%0Adifficulties.%20This%20article%20introduces%20an%20innovative%20multi-camera%20vehicle%0Atracking%20system%20that%20utilizes%20a%20self-supervised%20camera%20link%20model.%20In%20contrast%0Ato%20related%20works%20that%20rely%20on%20manual%20spatial-temporal%20annotations%2C%20our%20model%0Aautomatically%20extracts%20crucial%20multi-camera%20relationships%20for%20vehicle%20matching.%0AThe%20camera%20link%20is%20established%20through%20a%20pre-matching%20process%20that%20evaluates%0Afeature%20similarities%2C%20pair%20numbers%2C%20and%20time%20variance%20for%20high-quality%20tracks.%0AThis%20process%20calculates%20the%20probability%20of%20spatial%20linkage%20for%20all%20camera%0Acombinations%2C%20selecting%20the%20highest%20scoring%20pairs%20to%20create%20camera%20links.%20Our%0Aapproach%20significantly%20improves%20deployment%20times%20by%20eliminating%20the%20need%20for%0Ahuman%20annotation%2C%20offering%20substantial%20improvements%20in%20efficiency%20and%0Acost-effectiveness%20when%20it%20comes%20to%20real-world%20application.%20This%20pairing%0Aprocess%20supports%20cross%20camera%20matching%20by%20setting%20spatial-temporal%20constraints%2C%0Areducing%20the%20searching%20space%20for%20potential%20vehicle%20matches.%20According%20to%20our%0Aexperimental%20results%2C%20the%20proposed%20method%20achieves%20a%20new%20state-of-the-art%20among%0Aautomatic%20camera-link%20based%20methods%20in%20CityFlow%20V2%20benchmarks%20with%2061.07%25%20IDF1%0AScore.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11345v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCity-Scale%2520Multi-Camera%2520Vehicle%2520Tracking%2520System%2520with%2520Improved%250A%2520%2520Self-Supervised%2520Camera%2520Link%2520Model%26entry.906535625%3DYuqiang%2520Lin%2520and%2520Sam%2520Lockyer%2520and%2520Nic%2520Zhang%26entry.1292438233%3D%2520%2520Multi-Target%2520Multi-Camera%2520Tracking%2520%2528MTMCT%2529%2520has%2520broad%2520applications%2520and%2520forms%250Athe%2520basis%2520for%2520numerous%2520future%2520city-wide%2520systems%2520%2528e.g.%2520traffic%2520management%252C%2520crash%250Adetection%252C%2520etc.%2529.%2520However%252C%2520the%2520challenge%2520of%2520matching%2520vehicle%2520trajectories%250Aacross%2520different%2520cameras%2520based%2520solely%2520on%2520feature%2520extraction%2520poses%2520significant%250Adifficulties.%2520This%2520article%2520introduces%2520an%2520innovative%2520multi-camera%2520vehicle%250Atracking%2520system%2520that%2520utilizes%2520a%2520self-supervised%2520camera%2520link%2520model.%2520In%2520contrast%250Ato%2520related%2520works%2520that%2520rely%2520on%2520manual%2520spatial-temporal%2520annotations%252C%2520our%2520model%250Aautomatically%2520extracts%2520crucial%2520multi-camera%2520relationships%2520for%2520vehicle%2520matching.%250AThe%2520camera%2520link%2520is%2520established%2520through%2520a%2520pre-matching%2520process%2520that%2520evaluates%250Afeature%2520similarities%252C%2520pair%2520numbers%252C%2520and%2520time%2520variance%2520for%2520high-quality%2520tracks.%250AThis%2520process%2520calculates%2520the%2520probability%2520of%2520spatial%2520linkage%2520for%2520all%2520camera%250Acombinations%252C%2520selecting%2520the%2520highest%2520scoring%2520pairs%2520to%2520create%2520camera%2520links.%2520Our%250Aapproach%2520significantly%2520improves%2520deployment%2520times%2520by%2520eliminating%2520the%2520need%2520for%250Ahuman%2520annotation%252C%2520offering%2520substantial%2520improvements%2520in%2520efficiency%2520and%250Acost-effectiveness%2520when%2520it%2520comes%2520to%2520real-world%2520application.%2520This%2520pairing%250Aprocess%2520supports%2520cross%2520camera%2520matching%2520by%2520setting%2520spatial-temporal%2520constraints%252C%250Areducing%2520the%2520searching%2520space%2520for%2520potential%2520vehicle%2520matches.%2520According%2520to%2520our%250Aexperimental%2520results%252C%2520the%2520proposed%2520method%2520achieves%2520a%2520new%2520state-of-the-art%2520among%250Aautomatic%2520camera-link%2520based%2520methods%2520in%2520CityFlow%2520V2%2520benchmarks%2520with%252061.07%2525%2520IDF1%250AScore.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11345v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=City-Scale%20Multi-Camera%20Vehicle%20Tracking%20System%20with%20Improved%0A%20%20Self-Supervised%20Camera%20Link%20Model&entry.906535625=Yuqiang%20Lin%20and%20Sam%20Lockyer%20and%20Nic%20Zhang&entry.1292438233=%20%20Multi-Target%20Multi-Camera%20Tracking%20%28MTMCT%29%20has%20broad%20applications%20and%20forms%0Athe%20basis%20for%20numerous%20future%20city-wide%20systems%20%28e.g.%20traffic%20management%2C%20crash%0Adetection%2C%20etc.%29.%20However%2C%20the%20challenge%20of%20matching%20vehicle%20trajectories%0Aacross%20different%20cameras%20based%20solely%20on%20feature%20extraction%20poses%20significant%0Adifficulties.%20This%20article%20introduces%20an%20innovative%20multi-camera%20vehicle%0Atracking%20system%20that%20utilizes%20a%20self-supervised%20camera%20link%20model.%20In%20contrast%0Ato%20related%20works%20that%20rely%20on%20manual%20spatial-temporal%20annotations%2C%20our%20model%0Aautomatically%20extracts%20crucial%20multi-camera%20relationships%20for%20vehicle%20matching.%0AThe%20camera%20link%20is%20established%20through%20a%20pre-matching%20process%20that%20evaluates%0Afeature%20similarities%2C%20pair%20numbers%2C%20and%20time%20variance%20for%20high-quality%20tracks.%0AThis%20process%20calculates%20the%20probability%20of%20spatial%20linkage%20for%20all%20camera%0Acombinations%2C%20selecting%20the%20highest%20scoring%20pairs%20to%20create%20camera%20links.%20Our%0Aapproach%20significantly%20improves%20deployment%20times%20by%20eliminating%20the%20need%20for%0Ahuman%20annotation%2C%20offering%20substantial%20improvements%20in%20efficiency%20and%0Acost-effectiveness%20when%20it%20comes%20to%20real-world%20application.%20This%20pairing%0Aprocess%20supports%20cross%20camera%20matching%20by%20setting%20spatial-temporal%20constraints%2C%0Areducing%20the%20searching%20space%20for%20potential%20vehicle%20matches.%20According%20to%20our%0Aexperimental%20results%2C%20the%20proposed%20method%20achieves%20a%20new%20state-of-the-art%20among%0Aautomatic%20camera-link%20based%20methods%20in%20CityFlow%20V2%20benchmarks%20with%2061.07%25%20IDF1%0AScore.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11345v3&entry.124074799=Read"},
{"title": "Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise\n  Sufficient Reasons", "author": "Shahaf Bassan and Ron Eliav and Shlomit Gur", "abstract": "  *Minimal sufficient reasons* represent a prevalent form of explanation - the\nsmallest subset of input features which, when held constant at their\ncorresponding values, ensure that the prediction remains unchanged. Previous\n*post-hoc* methods attempt to obtain such explanations but face two main\nlimitations: (1) Obtaining these subsets poses a computational challenge,\nleading most scalable methods to converge towards suboptimal, less meaningful\nsubsets; (2) These methods heavily rely on sampling out-of-distribution input\nassignments, potentially resulting in counterintuitive behaviors. To tackle\nthese limitations, we propose in this work a self-supervised training approach,\nwhich we term *sufficient subset training* (SST). Using SST, we train models to\ngenerate concise sufficient reasons for their predictions as an integral part\nof their output. Our results indicate that our framework produces succinct and\nfaithful subsets substantially more efficiently than competing post-hoc\nmethods, while maintaining comparable predictive performance.\n", "link": "http://arxiv.org/abs/2502.03391v2", "date": "2025-02-14", "relevancy": 2.288, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4693}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4562}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons&body=Title%3A%20Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons%0AAuthor%3A%20Shahaf%20Bassan%20and%20Ron%20Eliav%20and%20Shlomit%20Gur%0AAbstract%3A%20%20%20%2AMinimal%20sufficient%20reasons%2A%20represent%20a%20prevalent%20form%20of%20explanation%20-%20the%0Asmallest%20subset%20of%20input%20features%20which%2C%20when%20held%20constant%20at%20their%0Acorresponding%20values%2C%20ensure%20that%20the%20prediction%20remains%20unchanged.%20Previous%0A%2Apost-hoc%2A%20methods%20attempt%20to%20obtain%20such%20explanations%20but%20face%20two%20main%0Alimitations%3A%20%281%29%20Obtaining%20these%20subsets%20poses%20a%20computational%20challenge%2C%0Aleading%20most%20scalable%20methods%20to%20converge%20towards%20suboptimal%2C%20less%20meaningful%0Asubsets%3B%20%282%29%20These%20methods%20heavily%20rely%20on%20sampling%20out-of-distribution%20input%0Aassignments%2C%20potentially%20resulting%20in%20counterintuitive%20behaviors.%20To%20tackle%0Athese%20limitations%2C%20we%20propose%20in%20this%20work%20a%20self-supervised%20training%20approach%2C%0Awhich%20we%20term%20%2Asufficient%20subset%20training%2A%20%28SST%29.%20Using%20SST%2C%20we%20train%20models%20to%0Agenerate%20concise%20sufficient%20reasons%20for%20their%20predictions%20as%20an%20integral%20part%0Aof%20their%20output.%20Our%20results%20indicate%20that%20our%20framework%20produces%20succinct%20and%0Afaithful%20subsets%20substantially%20more%20efficiently%20than%20competing%20post-hoc%0Amethods%2C%20while%20maintaining%20comparable%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplain%2520Yourself%252C%2520Briefly%2521%2520Self-Explaining%2520Neural%2520Networks%2520with%2520Concise%250A%2520%2520Sufficient%2520Reasons%26entry.906535625%3DShahaf%2520Bassan%2520and%2520Ron%2520Eliav%2520and%2520Shlomit%2520Gur%26entry.1292438233%3D%2520%2520%252AMinimal%2520sufficient%2520reasons%252A%2520represent%2520a%2520prevalent%2520form%2520of%2520explanation%2520-%2520the%250Asmallest%2520subset%2520of%2520input%2520features%2520which%252C%2520when%2520held%2520constant%2520at%2520their%250Acorresponding%2520values%252C%2520ensure%2520that%2520the%2520prediction%2520remains%2520unchanged.%2520Previous%250A%252Apost-hoc%252A%2520methods%2520attempt%2520to%2520obtain%2520such%2520explanations%2520but%2520face%2520two%2520main%250Alimitations%253A%2520%25281%2529%2520Obtaining%2520these%2520subsets%2520poses%2520a%2520computational%2520challenge%252C%250Aleading%2520most%2520scalable%2520methods%2520to%2520converge%2520towards%2520suboptimal%252C%2520less%2520meaningful%250Asubsets%253B%2520%25282%2529%2520These%2520methods%2520heavily%2520rely%2520on%2520sampling%2520out-of-distribution%2520input%250Aassignments%252C%2520potentially%2520resulting%2520in%2520counterintuitive%2520behaviors.%2520To%2520tackle%250Athese%2520limitations%252C%2520we%2520propose%2520in%2520this%2520work%2520a%2520self-supervised%2520training%2520approach%252C%250Awhich%2520we%2520term%2520%252Asufficient%2520subset%2520training%252A%2520%2528SST%2529.%2520Using%2520SST%252C%2520we%2520train%2520models%2520to%250Agenerate%2520concise%2520sufficient%2520reasons%2520for%2520their%2520predictions%2520as%2520an%2520integral%2520part%250Aof%2520their%2520output.%2520Our%2520results%2520indicate%2520that%2520our%2520framework%2520produces%2520succinct%2520and%250Afaithful%2520subsets%2520substantially%2520more%2520efficiently%2520than%2520competing%2520post-hoc%250Amethods%252C%2520while%2520maintaining%2520comparable%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons&entry.906535625=Shahaf%20Bassan%20and%20Ron%20Eliav%20and%20Shlomit%20Gur&entry.1292438233=%20%20%2AMinimal%20sufficient%20reasons%2A%20represent%20a%20prevalent%20form%20of%20explanation%20-%20the%0Asmallest%20subset%20of%20input%20features%20which%2C%20when%20held%20constant%20at%20their%0Acorresponding%20values%2C%20ensure%20that%20the%20prediction%20remains%20unchanged.%20Previous%0A%2Apost-hoc%2A%20methods%20attempt%20to%20obtain%20such%20explanations%20but%20face%20two%20main%0Alimitations%3A%20%281%29%20Obtaining%20these%20subsets%20poses%20a%20computational%20challenge%2C%0Aleading%20most%20scalable%20methods%20to%20converge%20towards%20suboptimal%2C%20less%20meaningful%0Asubsets%3B%20%282%29%20These%20methods%20heavily%20rely%20on%20sampling%20out-of-distribution%20input%0Aassignments%2C%20potentially%20resulting%20in%20counterintuitive%20behaviors.%20To%20tackle%0Athese%20limitations%2C%20we%20propose%20in%20this%20work%20a%20self-supervised%20training%20approach%2C%0Awhich%20we%20term%20%2Asufficient%20subset%20training%2A%20%28SST%29.%20Using%20SST%2C%20we%20train%20models%20to%0Agenerate%20concise%20sufficient%20reasons%20for%20their%20predictions%20as%20an%20integral%20part%0Aof%20their%20output.%20Our%20results%20indicate%20that%20our%20framework%20produces%20succinct%20and%0Afaithful%20subsets%20substantially%20more%20efficiently%20than%20competing%20post-hoc%0Amethods%2C%20while%20maintaining%20comparable%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03391v2&entry.124074799=Read"},
{"title": "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated\n  Learning", "author": "Yuchen Liu and Chen Chen and Lingjuan Lyu and Yaochu Jin and Gang Chen", "abstract": "  Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack\n", "link": "http://arxiv.org/abs/2502.04890v2", "date": "2025-02-14", "relevancy": 2.2803, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.47}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4675}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning&body=Title%3A%20Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning%0AAuthor%3A%20Yuchen%20Liu%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Yaochu%20Jin%20and%20Gang%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20notorious%20for%20its%20vulnerability%20to%20Byzantine%0Aattacks.%20Most%20current%20Byzantine%20defenses%20share%20a%20common%20inductive%20bias%3A%20among%0Aall%20the%20gradients%2C%20the%20densely%20distributed%20ones%20are%20more%20likely%20to%20be%20honest.%0AHowever%2C%20such%20a%20bias%20is%20a%20poison%20to%20Byzantine%20robustness%20due%20to%20a%20newly%0Adiscovered%20phenomenon%20in%20this%20paper%20-%20gradient%20skew.%20We%20discover%20that%20a%20group%0Aof%20densely%20distributed%20honest%20gradients%20skew%20away%20from%20the%20optimal%20gradient%0A%28the%20average%20of%20honest%20gradients%29%20due%20to%20heterogeneous%20data.%20This%20gradient%20skew%0Aphenomenon%20allows%20Byzantine%20gradients%20to%20hide%20within%20the%20densely%20distributed%0Askewed%20gradients.%20As%20a%20result%2C%20Byzantine%20defenses%20are%20confused%20into%20believing%0Athat%20Byzantine%20gradients%20are%20honest.%20Motivated%20by%20this%20observation%2C%20we%20propose%0Aa%20novel%20skew-aware%20attack%20called%20STRIKE%3A%20first%2C%20we%20search%20for%20the%20skewed%0Agradients%3B%20then%2C%20we%20construct%20Byzantine%20gradients%20within%20the%20skewed%20gradients.%0AExperiments%20on%20three%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20our%0Aattack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04890v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploit%2520Gradient%2520Skewness%2520to%2520Circumvent%2520Byzantine%2520Defenses%2520for%2520Federated%250A%2520%2520Learning%26entry.906535625%3DYuchen%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Lingjuan%2520Lyu%2520and%2520Yaochu%2520Jin%2520and%2520Gang%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520notorious%2520for%2520its%2520vulnerability%2520to%2520Byzantine%250Aattacks.%2520Most%2520current%2520Byzantine%2520defenses%2520share%2520a%2520common%2520inductive%2520bias%253A%2520among%250Aall%2520the%2520gradients%252C%2520the%2520densely%2520distributed%2520ones%2520are%2520more%2520likely%2520to%2520be%2520honest.%250AHowever%252C%2520such%2520a%2520bias%2520is%2520a%2520poison%2520to%2520Byzantine%2520robustness%2520due%2520to%2520a%2520newly%250Adiscovered%2520phenomenon%2520in%2520this%2520paper%2520-%2520gradient%2520skew.%2520We%2520discover%2520that%2520a%2520group%250Aof%2520densely%2520distributed%2520honest%2520gradients%2520skew%2520away%2520from%2520the%2520optimal%2520gradient%250A%2528the%2520average%2520of%2520honest%2520gradients%2529%2520due%2520to%2520heterogeneous%2520data.%2520This%2520gradient%2520skew%250Aphenomenon%2520allows%2520Byzantine%2520gradients%2520to%2520hide%2520within%2520the%2520densely%2520distributed%250Askewed%2520gradients.%2520As%2520a%2520result%252C%2520Byzantine%2520defenses%2520are%2520confused%2520into%2520believing%250Athat%2520Byzantine%2520gradients%2520are%2520honest.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%250Aa%2520novel%2520skew-aware%2520attack%2520called%2520STRIKE%253A%2520first%252C%2520we%2520search%2520for%2520the%2520skewed%250Agradients%253B%2520then%252C%2520we%2520construct%2520Byzantine%2520gradients%2520within%2520the%2520skewed%2520gradients.%250AExperiments%2520on%2520three%2520benchmark%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520our%250Aattack%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04890v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploit%20Gradient%20Skewness%20to%20Circumvent%20Byzantine%20Defenses%20for%20Federated%0A%20%20Learning&entry.906535625=Yuchen%20Liu%20and%20Chen%20Chen%20and%20Lingjuan%20Lyu%20and%20Yaochu%20Jin%20and%20Gang%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20notorious%20for%20its%20vulnerability%20to%20Byzantine%0Aattacks.%20Most%20current%20Byzantine%20defenses%20share%20a%20common%20inductive%20bias%3A%20among%0Aall%20the%20gradients%2C%20the%20densely%20distributed%20ones%20are%20more%20likely%20to%20be%20honest.%0AHowever%2C%20such%20a%20bias%20is%20a%20poison%20to%20Byzantine%20robustness%20due%20to%20a%20newly%0Adiscovered%20phenomenon%20in%20this%20paper%20-%20gradient%20skew.%20We%20discover%20that%20a%20group%0Aof%20densely%20distributed%20honest%20gradients%20skew%20away%20from%20the%20optimal%20gradient%0A%28the%20average%20of%20honest%20gradients%29%20due%20to%20heterogeneous%20data.%20This%20gradient%20skew%0Aphenomenon%20allows%20Byzantine%20gradients%20to%20hide%20within%20the%20densely%20distributed%0Askewed%20gradients.%20As%20a%20result%2C%20Byzantine%20defenses%20are%20confused%20into%20believing%0Athat%20Byzantine%20gradients%20are%20honest.%20Motivated%20by%20this%20observation%2C%20we%20propose%0Aa%20novel%20skew-aware%20attack%20called%20STRIKE%3A%20first%2C%20we%20search%20for%20the%20skewed%0Agradients%3B%20then%2C%20we%20construct%20Byzantine%20gradients%20within%20the%20skewed%20gradients.%0AExperiments%20on%20three%20benchmark%20datasets%20validate%20the%20effectiveness%20of%20our%0Aattack%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04890v2&entry.124074799=Read"},
{"title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation", "author": "Rongzhao He and Weihao Zheng and Leilei Zhao and Ying Wang and Dalin Zhu and Dan Wu and Bin Hu", "abstract": "  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n", "link": "http://arxiv.org/abs/2501.14679v4", "date": "2025-02-14", "relevancy": 2.2524, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5637}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&body=Title%3A%20Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation%0AAuthor%3A%20Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu%0AAbstract%3A%20%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14679v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurface%2520Vision%2520Mamba%253A%2520Leveraging%2520Bidirectional%2520State%2520Space%2520Model%2520for%250A%2520%2520Efficient%2520Spherical%2520Manifold%2520Representation%26entry.906535625%3DRongzhao%2520He%2520and%2520Weihao%2520Zheng%2520and%2520Leilei%2520Zhao%2520and%2520Ying%2520Wang%2520and%2520Dalin%2520Zhu%2520and%2520Dan%2520Wu%2520and%2520Bin%2520Hu%26entry.1292438233%3D%2520%2520Attention-based%2520methods%2520have%2520demonstrated%2520exceptional%2520performance%2520in%250Amodelling%2520long-range%2520dependencies%2520on%2520spherical%2520cortical%2520surfaces%252C%2520surpassing%250Atraditional%2520Geometric%2520Deep%2520Learning%2520%2528GDL%2529%2520models.%2520However%252C%2520their%2520extensive%250Ainference%2520time%2520and%2520high%2520memory%2520demands%2520pose%2520challenges%2520for%2520application%2520to%2520large%250Adatasets%2520with%2520limited%2520computing%2520resources.%2520Inspired%2520by%2520the%2520state%2520space%2520model%2520in%250Acomputer%2520vision%252C%2520we%2520introduce%2520the%2520attention-free%2520Vision%2520Mamba%2520%2528Vim%2529%2520to%250Aspherical%2520surfaces%252C%2520presenting%2520a%2520domain-agnostic%2520architecture%2520for%2520analyzing%250Adata%2520on%2520spherical%2520manifolds.%2520Our%2520method%2520achieves%2520surface%2520patching%2520by%250Arepresenting%2520spherical%2520data%2520as%2520a%2520sequence%2520of%2520triangular%2520patches%2520derived%2520from%2520a%250Asubdivided%2520icosphere.%2520The%2520proposed%2520Surface%2520Vision%2520Mamba%2520%2528SiM%2529%2520is%2520evaluated%2520on%250Amultiple%2520neurodevelopmental%2520phenotype%2520regression%2520tasks%2520using%2520cortical%2520surface%250Ametrics%2520from%2520neonatal%2520brains.%2520Experimental%2520results%2520demonstrate%2520that%2520SiM%250Aoutperforms%2520both%2520attention-%2520and%2520GDL-based%2520methods%252C%2520delivering%25204.8%2520times%2520faster%250Ainference%2520and%2520achieving%252091.7%2525%2520lower%2520memory%2520consumption%2520compared%2520to%2520the%2520Surface%250AVision%2520Transformer%2520%2528SiT%2529%2520under%2520the%2520Ico-4%2520grid%2520partitioning.%2520Sensitivity%250Aanalysis%2520further%2520underscores%2520the%2520potential%2520of%2520SiM%2520to%2520identify%2520subtle%2520cognitive%250Adevelopmental%2520patterns.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Rongzhao-He/surface-vision-mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14679v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface%20Vision%20Mamba%3A%20Leveraging%20Bidirectional%20State%20Space%20Model%20for%0A%20%20Efficient%20Spherical%20Manifold%20Representation&entry.906535625=Rongzhao%20He%20and%20Weihao%20Zheng%20and%20Leilei%20Zhao%20and%20Ying%20Wang%20and%20Dalin%20Zhu%20and%20Dan%20Wu%20and%20Bin%20Hu&entry.1292438233=%20%20Attention-based%20methods%20have%20demonstrated%20exceptional%20performance%20in%0Amodelling%20long-range%20dependencies%20on%20spherical%20cortical%20surfaces%2C%20surpassing%0Atraditional%20Geometric%20Deep%20Learning%20%28GDL%29%20models.%20However%2C%20their%20extensive%0Ainference%20time%20and%20high%20memory%20demands%20pose%20challenges%20for%20application%20to%20large%0Adatasets%20with%20limited%20computing%20resources.%20Inspired%20by%20the%20state%20space%20model%20in%0Acomputer%20vision%2C%20we%20introduce%20the%20attention-free%20Vision%20Mamba%20%28Vim%29%20to%0Aspherical%20surfaces%2C%20presenting%20a%20domain-agnostic%20architecture%20for%20analyzing%0Adata%20on%20spherical%20manifolds.%20Our%20method%20achieves%20surface%20patching%20by%0Arepresenting%20spherical%20data%20as%20a%20sequence%20of%20triangular%20patches%20derived%20from%20a%0Asubdivided%20icosphere.%20The%20proposed%20Surface%20Vision%20Mamba%20%28SiM%29%20is%20evaluated%20on%0Amultiple%20neurodevelopmental%20phenotype%20regression%20tasks%20using%20cortical%20surface%0Ametrics%20from%20neonatal%20brains.%20Experimental%20results%20demonstrate%20that%20SiM%0Aoutperforms%20both%20attention-%20and%20GDL-based%20methods%2C%20delivering%204.8%20times%20faster%0Ainference%20and%20achieving%2091.7%25%20lower%20memory%20consumption%20compared%20to%20the%20Surface%0AVision%20Transformer%20%28SiT%29%20under%20the%20Ico-4%20grid%20partitioning.%20Sensitivity%0Aanalysis%20further%20underscores%20the%20potential%20of%20SiM%20to%20identify%20subtle%20cognitive%0Adevelopmental%20patterns.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Rongzhao-He/surface-vision-mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14679v4&entry.124074799=Read"},
{"title": "Federated Temporal Graph Clustering", "author": "Yang Liu and Zihao Zhou and Xianghong Xu and Qian Li", "abstract": "  Temporal graph clustering is a complex task that involves discovering\nmeaningful structures in dynamic graphs where relationships and entities change\nover time. Existing methods typically require centralized data collection,\nwhich poses significant privacy and communication challenges. In this work, we\nintroduce a novel Federated Temporal Graph Clustering (FTGC) framework that\nenables decentralized training of graph neural networks (GNNs) across multiple\nclients, ensuring data privacy throughout the process. Our approach\nincorporates a temporal aggregation mechanism to effectively capture the\nevolution of graph structures over time and a federated optimization strategy\nto collaboratively learn high-quality clustering representations. By preserving\ndata privacy and reducing communication overhead, our framework achieves\ncompetitive performance on temporal graph datasets, making it a promising\nsolution for privacy-sensitive, real-world applications involving dynamic data.\n", "link": "http://arxiv.org/abs/2410.12343v2", "date": "2025-02-14", "relevancy": 2.2336, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4761}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4385}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Temporal%20Graph%20Clustering&body=Title%3A%20Federated%20Temporal%20Graph%20Clustering%0AAuthor%3A%20Yang%20Liu%20and%20Zihao%20Zhou%20and%20Xianghong%20Xu%20and%20Qian%20Li%0AAbstract%3A%20%20%20Temporal%20graph%20clustering%20is%20a%20complex%20task%20that%20involves%20discovering%0Ameaningful%20structures%20in%20dynamic%20graphs%20where%20relationships%20and%20entities%20change%0Aover%20time.%20Existing%20methods%20typically%20require%20centralized%20data%20collection%2C%0Awhich%20poses%20significant%20privacy%20and%20communication%20challenges.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20Federated%20Temporal%20Graph%20Clustering%20%28FTGC%29%20framework%20that%0Aenables%20decentralized%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20across%20multiple%0Aclients%2C%20ensuring%20data%20privacy%20throughout%20the%20process.%20Our%20approach%0Aincorporates%20a%20temporal%20aggregation%20mechanism%20to%20effectively%20capture%20the%0Aevolution%20of%20graph%20structures%20over%20time%20and%20a%20federated%20optimization%20strategy%0Ato%20collaboratively%20learn%20high-quality%20clustering%20representations.%20By%20preserving%0Adata%20privacy%20and%20reducing%20communication%20overhead%2C%20our%20framework%20achieves%0Acompetitive%20performance%20on%20temporal%20graph%20datasets%2C%20making%20it%20a%20promising%0Asolution%20for%20privacy-sensitive%2C%20real-world%20applications%20involving%20dynamic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Temporal%2520Graph%2520Clustering%26entry.906535625%3DYang%2520Liu%2520and%2520Zihao%2520Zhou%2520and%2520Xianghong%2520Xu%2520and%2520Qian%2520Li%26entry.1292438233%3D%2520%2520Temporal%2520graph%2520clustering%2520is%2520a%2520complex%2520task%2520that%2520involves%2520discovering%250Ameaningful%2520structures%2520in%2520dynamic%2520graphs%2520where%2520relationships%2520and%2520entities%2520change%250Aover%2520time.%2520Existing%2520methods%2520typically%2520require%2520centralized%2520data%2520collection%252C%250Awhich%2520poses%2520significant%2520privacy%2520and%2520communication%2520challenges.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520novel%2520Federated%2520Temporal%2520Graph%2520Clustering%2520%2528FTGC%2529%2520framework%2520that%250Aenables%2520decentralized%2520training%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520across%2520multiple%250Aclients%252C%2520ensuring%2520data%2520privacy%2520throughout%2520the%2520process.%2520Our%2520approach%250Aincorporates%2520a%2520temporal%2520aggregation%2520mechanism%2520to%2520effectively%2520capture%2520the%250Aevolution%2520of%2520graph%2520structures%2520over%2520time%2520and%2520a%2520federated%2520optimization%2520strategy%250Ato%2520collaboratively%2520learn%2520high-quality%2520clustering%2520representations.%2520By%2520preserving%250Adata%2520privacy%2520and%2520reducing%2520communication%2520overhead%252C%2520our%2520framework%2520achieves%250Acompetitive%2520performance%2520on%2520temporal%2520graph%2520datasets%252C%2520making%2520it%2520a%2520promising%250Asolution%2520for%2520privacy-sensitive%252C%2520real-world%2520applications%2520involving%2520dynamic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Temporal%20Graph%20Clustering&entry.906535625=Yang%20Liu%20and%20Zihao%20Zhou%20and%20Xianghong%20Xu%20and%20Qian%20Li&entry.1292438233=%20%20Temporal%20graph%20clustering%20is%20a%20complex%20task%20that%20involves%20discovering%0Ameaningful%20structures%20in%20dynamic%20graphs%20where%20relationships%20and%20entities%20change%0Aover%20time.%20Existing%20methods%20typically%20require%20centralized%20data%20collection%2C%0Awhich%20poses%20significant%20privacy%20and%20communication%20challenges.%20In%20this%20work%2C%20we%0Aintroduce%20a%20novel%20Federated%20Temporal%20Graph%20Clustering%20%28FTGC%29%20framework%20that%0Aenables%20decentralized%20training%20of%20graph%20neural%20networks%20%28GNNs%29%20across%20multiple%0Aclients%2C%20ensuring%20data%20privacy%20throughout%20the%20process.%20Our%20approach%0Aincorporates%20a%20temporal%20aggregation%20mechanism%20to%20effectively%20capture%20the%0Aevolution%20of%20graph%20structures%20over%20time%20and%20a%20federated%20optimization%20strategy%0Ato%20collaboratively%20learn%20high-quality%20clustering%20representations.%20By%20preserving%0Adata%20privacy%20and%20reducing%20communication%20overhead%2C%20our%20framework%20achieves%0Acompetitive%20performance%20on%20temporal%20graph%20datasets%2C%20making%20it%20a%20promising%0Asolution%20for%20privacy-sensitive%2C%20real-world%20applications%20involving%20dynamic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12343v2&entry.124074799=Read"},
{"title": "Is What You Ask For What You Get? Investigating Concept Associations in\n  Text-to-Image Models", "author": "Salma Abdel Magid and Weiwei Pan and Simon Warchol and Grace Guo and Junsik Kim and Mahia Rahman and Hanspeter Pfister", "abstract": "  Text-to-image (T2I) models are increasingly used in impactful real-life\napplications. As such, there is a growing need to audit these models to ensure\nthat they generate desirable, task-appropriate images. However, systematically\ninspecting the associations between prompts and generated content in a\nhuman-understandable way remains challenging. To address this, we propose\nConcept2Concept, a framework where we characterize conditional distributions of\nvision language models using interpretable concepts and metrics that can be\ndefined in terms of these concepts. This characterization allows us to use our\nframework to audit models and prompt-datasets. To demonstrate, we investigate\nseveral case studies of conditional distributions of prompts, such as\nuser-defined distributions or empirical, real-world distributions. Lastly, we\nimplement Concept2Concept as an open-source interactive visualization tool to\nfacilitate use by non-technical end-users. A demo is available at\nhttps://tinyurl.com/Concept2ConceptDemo.\n", "link": "http://arxiv.org/abs/2410.04634v2", "date": "2025-02-14", "relevancy": 2.2282, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models&body=Title%3A%20Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models%0AAuthor%3A%20Salma%20Abdel%20Magid%20and%20Weiwei%20Pan%20and%20Simon%20Warchol%20and%20Grace%20Guo%20and%20Junsik%20Kim%20and%20Mahia%20Rahman%20and%20Hanspeter%20Pfister%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20models%20are%20increasingly%20used%20in%20impactful%20real-life%0Aapplications.%20As%20such%2C%20there%20is%20a%20growing%20need%20to%20audit%20these%20models%20to%20ensure%0Athat%20they%20generate%20desirable%2C%20task-appropriate%20images.%20However%2C%20systematically%0Ainspecting%20the%20associations%20between%20prompts%20and%20generated%20content%20in%20a%0Ahuman-understandable%20way%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AConcept2Concept%2C%20a%20framework%20where%20we%20characterize%20conditional%20distributions%20of%0Avision%20language%20models%20using%20interpretable%20concepts%20and%20metrics%20that%20can%20be%0Adefined%20in%20terms%20of%20these%20concepts.%20This%20characterization%20allows%20us%20to%20use%20our%0Aframework%20to%20audit%20models%20and%20prompt-datasets.%20To%20demonstrate%2C%20we%20investigate%0Aseveral%20case%20studies%20of%20conditional%20distributions%20of%20prompts%2C%20such%20as%0Auser-defined%20distributions%20or%20empirical%2C%20real-world%20distributions.%20Lastly%2C%20we%0Aimplement%20Concept2Concept%20as%20an%20open-source%20interactive%20visualization%20tool%20to%0Afacilitate%20use%20by%20non-technical%20end-users.%20A%20demo%20is%20available%20at%0Ahttps%3A//tinyurl.com/Concept2ConceptDemo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520What%2520You%2520Ask%2520For%2520What%2520You%2520Get%253F%2520Investigating%2520Concept%2520Associations%2520in%250A%2520%2520Text-to-Image%2520Models%26entry.906535625%3DSalma%2520Abdel%2520Magid%2520and%2520Weiwei%2520Pan%2520and%2520Simon%2520Warchol%2520and%2520Grace%2520Guo%2520and%2520Junsik%2520Kim%2520and%2520Mahia%2520Rahman%2520and%2520Hanspeter%2520Pfister%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520models%2520are%2520increasingly%2520used%2520in%2520impactful%2520real-life%250Aapplications.%2520As%2520such%252C%2520there%2520is%2520a%2520growing%2520need%2520to%2520audit%2520these%2520models%2520to%2520ensure%250Athat%2520they%2520generate%2520desirable%252C%2520task-appropriate%2520images.%2520However%252C%2520systematically%250Ainspecting%2520the%2520associations%2520between%2520prompts%2520and%2520generated%2520content%2520in%2520a%250Ahuman-understandable%2520way%2520remains%2520challenging.%2520To%2520address%2520this%252C%2520we%2520propose%250AConcept2Concept%252C%2520a%2520framework%2520where%2520we%2520characterize%2520conditional%2520distributions%2520of%250Avision%2520language%2520models%2520using%2520interpretable%2520concepts%2520and%2520metrics%2520that%2520can%2520be%250Adefined%2520in%2520terms%2520of%2520these%2520concepts.%2520This%2520characterization%2520allows%2520us%2520to%2520use%2520our%250Aframework%2520to%2520audit%2520models%2520and%2520prompt-datasets.%2520To%2520demonstrate%252C%2520we%2520investigate%250Aseveral%2520case%2520studies%2520of%2520conditional%2520distributions%2520of%2520prompts%252C%2520such%2520as%250Auser-defined%2520distributions%2520or%2520empirical%252C%2520real-world%2520distributions.%2520Lastly%252C%2520we%250Aimplement%2520Concept2Concept%2520as%2520an%2520open-source%2520interactive%2520visualization%2520tool%2520to%250Afacilitate%2520use%2520by%2520non-technical%2520end-users.%2520A%2520demo%2520is%2520available%2520at%250Ahttps%253A//tinyurl.com/Concept2ConceptDemo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models&entry.906535625=Salma%20Abdel%20Magid%20and%20Weiwei%20Pan%20and%20Simon%20Warchol%20and%20Grace%20Guo%20and%20Junsik%20Kim%20and%20Mahia%20Rahman%20and%20Hanspeter%20Pfister&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20models%20are%20increasingly%20used%20in%20impactful%20real-life%0Aapplications.%20As%20such%2C%20there%20is%20a%20growing%20need%20to%20audit%20these%20models%20to%20ensure%0Athat%20they%20generate%20desirable%2C%20task-appropriate%20images.%20However%2C%20systematically%0Ainspecting%20the%20associations%20between%20prompts%20and%20generated%20content%20in%20a%0Ahuman-understandable%20way%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AConcept2Concept%2C%20a%20framework%20where%20we%20characterize%20conditional%20distributions%20of%0Avision%20language%20models%20using%20interpretable%20concepts%20and%20metrics%20that%20can%20be%0Adefined%20in%20terms%20of%20these%20concepts.%20This%20characterization%20allows%20us%20to%20use%20our%0Aframework%20to%20audit%20models%20and%20prompt-datasets.%20To%20demonstrate%2C%20we%20investigate%0Aseveral%20case%20studies%20of%20conditional%20distributions%20of%20prompts%2C%20such%20as%0Auser-defined%20distributions%20or%20empirical%2C%20real-world%20distributions.%20Lastly%2C%20we%0Aimplement%20Concept2Concept%20as%20an%20open-source%20interactive%20visualization%20tool%20to%0Afacilitate%20use%20by%20non-technical%20end-users.%20A%20demo%20is%20available%20at%0Ahttps%3A//tinyurl.com/Concept2ConceptDemo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04634v2&entry.124074799=Read"},
{"title": "Probabilistic Super-Resolution for High-Fidelity Physical System\n  Simulations with Uncertainty Quantification", "author": "Pengyu Zhang and Connor Duffin and Alex Glyn-Davies and Arnaud Vadeboncoeur and Mark Girolami", "abstract": "  Super-resolution (SR) is a promising tool for generating high-fidelity\nsimulations of physical systems from low-resolution data, enabling fast and\naccurate predictions in engineering applications. However, existing\ndeep-learning based SR methods, require large labeled datasets and lack\nreliable uncertainty quantification (UQ), limiting their applicability in\nreal-world scenarios. To overcome these challenges, we propose a probabilistic\nSR framework that leverages the Statistical Finite Element Method and\nenergy-based generative modeling. Our method enables efficient high-resolution\npredictions with inherent UQ, while eliminating the need for extensive labeled\ndatasets. The method is validated on a 2D Poisson example and compared with\nbicubic interpolation upscaling. Results demonstrate a computational speed-up\nover high-resolution numerical solvers while providing reliable uncertainty\nestimates.\n", "link": "http://arxiv.org/abs/2502.10280v1", "date": "2025-02-14", "relevancy": 2.2241, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5627}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Super-Resolution%20for%20High-Fidelity%20Physical%20System%0A%20%20Simulations%20with%20Uncertainty%20Quantification&body=Title%3A%20Probabilistic%20Super-Resolution%20for%20High-Fidelity%20Physical%20System%0A%20%20Simulations%20with%20Uncertainty%20Quantification%0AAuthor%3A%20Pengyu%20Zhang%20and%20Connor%20Duffin%20and%20Alex%20Glyn-Davies%20and%20Arnaud%20Vadeboncoeur%20and%20Mark%20Girolami%0AAbstract%3A%20%20%20Super-resolution%20%28SR%29%20is%20a%20promising%20tool%20for%20generating%20high-fidelity%0Asimulations%20of%20physical%20systems%20from%20low-resolution%20data%2C%20enabling%20fast%20and%0Aaccurate%20predictions%20in%20engineering%20applications.%20However%2C%20existing%0Adeep-learning%20based%20SR%20methods%2C%20require%20large%20labeled%20datasets%20and%20lack%0Areliable%20uncertainty%20quantification%20%28UQ%29%2C%20limiting%20their%20applicability%20in%0Areal-world%20scenarios.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20probabilistic%0ASR%20framework%20that%20leverages%20the%20Statistical%20Finite%20Element%20Method%20and%0Aenergy-based%20generative%20modeling.%20Our%20method%20enables%20efficient%20high-resolution%0Apredictions%20with%20inherent%20UQ%2C%20while%20eliminating%20the%20need%20for%20extensive%20labeled%0Adatasets.%20The%20method%20is%20validated%20on%20a%202D%20Poisson%20example%20and%20compared%20with%0Abicubic%20interpolation%20upscaling.%20Results%20demonstrate%20a%20computational%20speed-up%0Aover%20high-resolution%20numerical%20solvers%20while%20providing%20reliable%20uncertainty%0Aestimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Super-Resolution%2520for%2520High-Fidelity%2520Physical%2520System%250A%2520%2520Simulations%2520with%2520Uncertainty%2520Quantification%26entry.906535625%3DPengyu%2520Zhang%2520and%2520Connor%2520Duffin%2520and%2520Alex%2520Glyn-Davies%2520and%2520Arnaud%2520Vadeboncoeur%2520and%2520Mark%2520Girolami%26entry.1292438233%3D%2520%2520Super-resolution%2520%2528SR%2529%2520is%2520a%2520promising%2520tool%2520for%2520generating%2520high-fidelity%250Asimulations%2520of%2520physical%2520systems%2520from%2520low-resolution%2520data%252C%2520enabling%2520fast%2520and%250Aaccurate%2520predictions%2520in%2520engineering%2520applications.%2520However%252C%2520existing%250Adeep-learning%2520based%2520SR%2520methods%252C%2520require%2520large%2520labeled%2520datasets%2520and%2520lack%250Areliable%2520uncertainty%2520quantification%2520%2528UQ%2529%252C%2520limiting%2520their%2520applicability%2520in%250Areal-world%2520scenarios.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520probabilistic%250ASR%2520framework%2520that%2520leverages%2520the%2520Statistical%2520Finite%2520Element%2520Method%2520and%250Aenergy-based%2520generative%2520modeling.%2520Our%2520method%2520enables%2520efficient%2520high-resolution%250Apredictions%2520with%2520inherent%2520UQ%252C%2520while%2520eliminating%2520the%2520need%2520for%2520extensive%2520labeled%250Adatasets.%2520The%2520method%2520is%2520validated%2520on%2520a%25202D%2520Poisson%2520example%2520and%2520compared%2520with%250Abicubic%2520interpolation%2520upscaling.%2520Results%2520demonstrate%2520a%2520computational%2520speed-up%250Aover%2520high-resolution%2520numerical%2520solvers%2520while%2520providing%2520reliable%2520uncertainty%250Aestimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Super-Resolution%20for%20High-Fidelity%20Physical%20System%0A%20%20Simulations%20with%20Uncertainty%20Quantification&entry.906535625=Pengyu%20Zhang%20and%20Connor%20Duffin%20and%20Alex%20Glyn-Davies%20and%20Arnaud%20Vadeboncoeur%20and%20Mark%20Girolami&entry.1292438233=%20%20Super-resolution%20%28SR%29%20is%20a%20promising%20tool%20for%20generating%20high-fidelity%0Asimulations%20of%20physical%20systems%20from%20low-resolution%20data%2C%20enabling%20fast%20and%0Aaccurate%20predictions%20in%20engineering%20applications.%20However%2C%20existing%0Adeep-learning%20based%20SR%20methods%2C%20require%20large%20labeled%20datasets%20and%20lack%0Areliable%20uncertainty%20quantification%20%28UQ%29%2C%20limiting%20their%20applicability%20in%0Areal-world%20scenarios.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%20probabilistic%0ASR%20framework%20that%20leverages%20the%20Statistical%20Finite%20Element%20Method%20and%0Aenergy-based%20generative%20modeling.%20Our%20method%20enables%20efficient%20high-resolution%0Apredictions%20with%20inherent%20UQ%2C%20while%20eliminating%20the%20need%20for%20extensive%20labeled%0Adatasets.%20The%20method%20is%20validated%20on%20a%202D%20Poisson%20example%20and%20compared%20with%0Abicubic%20interpolation%20upscaling.%20Results%20demonstrate%20a%20computational%20speed-up%0Aover%20high-resolution%20numerical%20solvers%20while%20providing%20reliable%20uncertainty%0Aestimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10280v1&entry.124074799=Read"},
{"title": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation", "author": "Raman Dutt", "abstract": "  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/\n", "link": "http://arxiv.org/abs/2502.07516v2", "date": "2025-02-14", "relevancy": 2.2157, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation&body=Title%3A%20The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation%0AAuthor%3A%20Raman%20Dutt%0AAbstract%3A%20%20%20Generative%20models%2C%20particularly%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20play%20a%0Acrucial%20role%20in%20medical%20image%20analysis.%20However%2C%20these%20models%20are%20prone%20to%0Atraining%20data%20memorization%2C%20posing%20significant%20risks%20to%20patient%20privacy.%0ASynthetic%20chest%20X-ray%20generation%20is%20one%20of%20the%20most%20common%20applications%20in%0Amedical%20image%20analysis%20with%20the%20MIMIC-CXR%20dataset%20serving%20as%20the%20primary%20data%0Arepository%20for%20this%20task.%20This%20study%20presents%20the%20first%20systematic%20attempt%20to%0Aidentify%20prompts%20and%20text%20tokens%20in%20MIMIC-CXR%20that%20contribute%20the%20most%20to%0Atraining%20data%20memorization.%20Our%20analysis%20reveals%20two%20unexpected%20findings%3A%20%281%29%0Aprompts%20containing%20traces%20of%20de-identification%20procedures%20%28markers%20introduced%0Ato%20hide%20Protected%20Health%20Information%29%20are%20the%20most%20memorized%2C%20and%20%282%29%20among%20all%0Atokens%2C%20de-identification%20markers%20contribute%20the%20most%20towards%20memorization.%0AThis%20highlights%20a%20broader%20issue%20with%20the%20standard%20anonymization%20practices%20and%0AT2I%20synthesis%20with%20MIMIC-CXR.%20To%20exacerbate%2C%20existing%20inference-time%0Amemorization%20mitigation%20strategies%20are%20ineffective%20and%20fail%20to%20sufficiently%0Areduce%20the%20model%27s%20reliance%20on%20memorized%20text%20tokens.%20On%20this%20front%2C%20we%20propose%0Aactionable%20strategies%20for%20different%20stakeholders%20to%20enhance%20privacy%20and%20improve%0Athe%20reliability%20of%20generative%20models%20in%20medical%20imaging.%20Finally%2C%20our%20results%0Aprovide%20a%20foundation%20for%20future%20work%20on%20developing%20and%20benchmarking%0Amemorization%20mitigation%20techniques%20for%20synthetic%20chest%20X-ray%20generation%20using%0Athe%20MIMIC-CXR%20dataset.%20The%20anonymized%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/diffusion_memorization-8011/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520the%2520Prompts%253A%2520De-Identification%2520Traces%2520Enhance%250A%2520%2520Memorization%2520Risks%2520in%2520Synthetic%2520Chest%2520X-Ray%2520Generation%26entry.906535625%3DRaman%2520Dutt%26entry.1292438233%3D%2520%2520Generative%2520models%252C%2520particularly%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%252C%2520play%2520a%250Acrucial%2520role%2520in%2520medical%2520image%2520analysis.%2520However%252C%2520these%2520models%2520are%2520prone%2520to%250Atraining%2520data%2520memorization%252C%2520posing%2520significant%2520risks%2520to%2520patient%2520privacy.%250ASynthetic%2520chest%2520X-ray%2520generation%2520is%2520one%2520of%2520the%2520most%2520common%2520applications%2520in%250Amedical%2520image%2520analysis%2520with%2520the%2520MIMIC-CXR%2520dataset%2520serving%2520as%2520the%2520primary%2520data%250Arepository%2520for%2520this%2520task.%2520This%2520study%2520presents%2520the%2520first%2520systematic%2520attempt%2520to%250Aidentify%2520prompts%2520and%2520text%2520tokens%2520in%2520MIMIC-CXR%2520that%2520contribute%2520the%2520most%2520to%250Atraining%2520data%2520memorization.%2520Our%2520analysis%2520reveals%2520two%2520unexpected%2520findings%253A%2520%25281%2529%250Aprompts%2520containing%2520traces%2520of%2520de-identification%2520procedures%2520%2528markers%2520introduced%250Ato%2520hide%2520Protected%2520Health%2520Information%2529%2520are%2520the%2520most%2520memorized%252C%2520and%2520%25282%2529%2520among%2520all%250Atokens%252C%2520de-identification%2520markers%2520contribute%2520the%2520most%2520towards%2520memorization.%250AThis%2520highlights%2520a%2520broader%2520issue%2520with%2520the%2520standard%2520anonymization%2520practices%2520and%250AT2I%2520synthesis%2520with%2520MIMIC-CXR.%2520To%2520exacerbate%252C%2520existing%2520inference-time%250Amemorization%2520mitigation%2520strategies%2520are%2520ineffective%2520and%2520fail%2520to%2520sufficiently%250Areduce%2520the%2520model%2527s%2520reliance%2520on%2520memorized%2520text%2520tokens.%2520On%2520this%2520front%252C%2520we%2520propose%250Aactionable%2520strategies%2520for%2520different%2520stakeholders%2520to%2520enhance%2520privacy%2520and%2520improve%250Athe%2520reliability%2520of%2520generative%2520models%2520in%2520medical%2520imaging.%2520Finally%252C%2520our%2520results%250Aprovide%2520a%2520foundation%2520for%2520future%2520work%2520on%2520developing%2520and%2520benchmarking%250Amemorization%2520mitigation%2520techniques%2520for%2520synthetic%2520chest%2520X-ray%2520generation%2520using%250Athe%2520MIMIC-CXR%2520dataset.%2520The%2520anonymized%2520code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/diffusion_memorization-8011/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20Prompts%3A%20De-Identification%20Traces%20Enhance%0A%20%20Memorization%20Risks%20in%20Synthetic%20Chest%20X-Ray%20Generation&entry.906535625=Raman%20Dutt&entry.1292438233=%20%20Generative%20models%2C%20particularly%20text-to-image%20%28T2I%29%20diffusion%20models%2C%20play%20a%0Acrucial%20role%20in%20medical%20image%20analysis.%20However%2C%20these%20models%20are%20prone%20to%0Atraining%20data%20memorization%2C%20posing%20significant%20risks%20to%20patient%20privacy.%0ASynthetic%20chest%20X-ray%20generation%20is%20one%20of%20the%20most%20common%20applications%20in%0Amedical%20image%20analysis%20with%20the%20MIMIC-CXR%20dataset%20serving%20as%20the%20primary%20data%0Arepository%20for%20this%20task.%20This%20study%20presents%20the%20first%20systematic%20attempt%20to%0Aidentify%20prompts%20and%20text%20tokens%20in%20MIMIC-CXR%20that%20contribute%20the%20most%20to%0Atraining%20data%20memorization.%20Our%20analysis%20reveals%20two%20unexpected%20findings%3A%20%281%29%0Aprompts%20containing%20traces%20of%20de-identification%20procedures%20%28markers%20introduced%0Ato%20hide%20Protected%20Health%20Information%29%20are%20the%20most%20memorized%2C%20and%20%282%29%20among%20all%0Atokens%2C%20de-identification%20markers%20contribute%20the%20most%20towards%20memorization.%0AThis%20highlights%20a%20broader%20issue%20with%20the%20standard%20anonymization%20practices%20and%0AT2I%20synthesis%20with%20MIMIC-CXR.%20To%20exacerbate%2C%20existing%20inference-time%0Amemorization%20mitigation%20strategies%20are%20ineffective%20and%20fail%20to%20sufficiently%0Areduce%20the%20model%27s%20reliance%20on%20memorized%20text%20tokens.%20On%20this%20front%2C%20we%20propose%0Aactionable%20strategies%20for%20different%20stakeholders%20to%20enhance%20privacy%20and%20improve%0Athe%20reliability%20of%20generative%20models%20in%20medical%20imaging.%20Finally%2C%20our%20results%0Aprovide%20a%20foundation%20for%20future%20work%20on%20developing%20and%20benchmarking%0Amemorization%20mitigation%20techniques%20for%20synthetic%20chest%20X-ray%20generation%20using%0Athe%20MIMIC-CXR%20dataset.%20The%20anonymized%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/diffusion_memorization-8011/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07516v2&entry.124074799=Read"},
{"title": "MassSpecGym: A benchmark for the discovery and identification of\n  molecules", "author": "Roman Bushuiev and Anton Bushuiev and Niek F. de Jonge and Adamo Young and Fleming Kretschmer and Raman Samusevich and Janne Heirman and Fei Wang and Luke Zhang and Kai D\u00fchrkop and Marcus Ludwig and Nils A. Haupt and Apurva Kalia and Corinna Brungs and Robin Schmid and Russell Greiner and Bo Wang and David S. Wishart and Li-Ping Liu and Juho Rousu and Wout Bittremieux and Hannes Rost and Tytus D. Mak and Soha Hassoun and Florian Huber and Justin J. J. van der Hooft and Michael A. Stravs and Sebastian B\u00f6cker and Josef Sivic and Tom\u00e1\u0161 Pluskal", "abstract": "  The discovery and identification of molecules in biological and environmental\nsamples is crucial for advancing biomedical and chemical sciences. Tandem mass\nspectrometry (MS/MS) is the leading technique for high-throughput elucidation\nof molecular structures. However, decoding a molecular structure from its mass\nspectrum is exceptionally challenging, even when performed by human experts. As\na result, the vast majority of acquired MS/MS spectra remain uninterpreted,\nthereby limiting our understanding of the underlying (bio)chemical processes.\nDespite decades of progress in machine learning applications for predicting\nmolecular structures from MS/MS spectra, the development of new methods is\nseverely hindered by the lack of standard datasets and evaluation protocols. To\naddress this problem, we propose MassSpecGym -- the first comprehensive\nbenchmark for the discovery and identification of molecules from MS/MS data.\nOur benchmark comprises the largest publicly available collection of\nhigh-quality labeled MS/MS spectra and defines three MS/MS annotation\nchallenges: de novo molecular structure generation, molecule retrieval, and\nspectrum simulation. It includes new evaluation metrics and a\ngeneralization-demanding data split, therefore standardizing the MS/MS\nannotation tasks and rendering the problem accessible to the broad machine\nlearning community. MassSpecGym is publicly available at\nhttps://github.com/pluskal-lab/MassSpecGym.\n", "link": "http://arxiv.org/abs/2410.23326v3", "date": "2025-02-14", "relevancy": 2.2095, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MassSpecGym%3A%20A%20benchmark%20for%20the%20discovery%20and%20identification%20of%0A%20%20molecules&body=Title%3A%20MassSpecGym%3A%20A%20benchmark%20for%20the%20discovery%20and%20identification%20of%0A%20%20molecules%0AAuthor%3A%20Roman%20Bushuiev%20and%20Anton%20Bushuiev%20and%20Niek%20F.%20de%20Jonge%20and%20Adamo%20Young%20and%20Fleming%20Kretschmer%20and%20Raman%20Samusevich%20and%20Janne%20Heirman%20and%20Fei%20Wang%20and%20Luke%20Zhang%20and%20Kai%20D%C3%BChrkop%20and%20Marcus%20Ludwig%20and%20Nils%20A.%20Haupt%20and%20Apurva%20Kalia%20and%20Corinna%20Brungs%20and%20Robin%20Schmid%20and%20Russell%20Greiner%20and%20Bo%20Wang%20and%20David%20S.%20Wishart%20and%20Li-Ping%20Liu%20and%20Juho%20Rousu%20and%20Wout%20Bittremieux%20and%20Hannes%20Rost%20and%20Tytus%20D.%20Mak%20and%20Soha%20Hassoun%20and%20Florian%20Huber%20and%20Justin%20J.%20J.%20van%20der%20Hooft%20and%20Michael%20A.%20Stravs%20and%20Sebastian%20B%C3%B6cker%20and%20Josef%20Sivic%20and%20Tom%C3%A1%C5%A1%20Pluskal%0AAbstract%3A%20%20%20The%20discovery%20and%20identification%20of%20molecules%20in%20biological%20and%20environmental%0Asamples%20is%20crucial%20for%20advancing%20biomedical%20and%20chemical%20sciences.%20Tandem%20mass%0Aspectrometry%20%28MS/MS%29%20is%20the%20leading%20technique%20for%20high-throughput%20elucidation%0Aof%20molecular%20structures.%20However%2C%20decoding%20a%20molecular%20structure%20from%20its%20mass%0Aspectrum%20is%20exceptionally%20challenging%2C%20even%20when%20performed%20by%20human%20experts.%20As%0Aa%20result%2C%20the%20vast%20majority%20of%20acquired%20MS/MS%20spectra%20remain%20uninterpreted%2C%0Athereby%20limiting%20our%20understanding%20of%20the%20underlying%20%28bio%29chemical%20processes.%0ADespite%20decades%20of%20progress%20in%20machine%20learning%20applications%20for%20predicting%0Amolecular%20structures%20from%20MS/MS%20spectra%2C%20the%20development%20of%20new%20methods%20is%0Aseverely%20hindered%20by%20the%20lack%20of%20standard%20datasets%20and%20evaluation%20protocols.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20MassSpecGym%20--%20the%20first%20comprehensive%0Abenchmark%20for%20the%20discovery%20and%20identification%20of%20molecules%20from%20MS/MS%20data.%0AOur%20benchmark%20comprises%20the%20largest%20publicly%20available%20collection%20of%0Ahigh-quality%20labeled%20MS/MS%20spectra%20and%20defines%20three%20MS/MS%20annotation%0Achallenges%3A%20de%20novo%20molecular%20structure%20generation%2C%20molecule%20retrieval%2C%20and%0Aspectrum%20simulation.%20It%20includes%20new%20evaluation%20metrics%20and%20a%0Ageneralization-demanding%20data%20split%2C%20therefore%20standardizing%20the%20MS/MS%0Aannotation%20tasks%20and%20rendering%20the%20problem%20accessible%20to%20the%20broad%20machine%0Alearning%20community.%20MassSpecGym%20is%20publicly%20available%20at%0Ahttps%3A//github.com/pluskal-lab/MassSpecGym.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23326v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMassSpecGym%253A%2520A%2520benchmark%2520for%2520the%2520discovery%2520and%2520identification%2520of%250A%2520%2520molecules%26entry.906535625%3DRoman%2520Bushuiev%2520and%2520Anton%2520Bushuiev%2520and%2520Niek%2520F.%2520de%2520Jonge%2520and%2520Adamo%2520Young%2520and%2520Fleming%2520Kretschmer%2520and%2520Raman%2520Samusevich%2520and%2520Janne%2520Heirman%2520and%2520Fei%2520Wang%2520and%2520Luke%2520Zhang%2520and%2520Kai%2520D%25C3%25BChrkop%2520and%2520Marcus%2520Ludwig%2520and%2520Nils%2520A.%2520Haupt%2520and%2520Apurva%2520Kalia%2520and%2520Corinna%2520Brungs%2520and%2520Robin%2520Schmid%2520and%2520Russell%2520Greiner%2520and%2520Bo%2520Wang%2520and%2520David%2520S.%2520Wishart%2520and%2520Li-Ping%2520Liu%2520and%2520Juho%2520Rousu%2520and%2520Wout%2520Bittremieux%2520and%2520Hannes%2520Rost%2520and%2520Tytus%2520D.%2520Mak%2520and%2520Soha%2520Hassoun%2520and%2520Florian%2520Huber%2520and%2520Justin%2520J.%2520J.%2520van%2520der%2520Hooft%2520and%2520Michael%2520A.%2520Stravs%2520and%2520Sebastian%2520B%25C3%25B6cker%2520and%2520Josef%2520Sivic%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Pluskal%26entry.1292438233%3D%2520%2520The%2520discovery%2520and%2520identification%2520of%2520molecules%2520in%2520biological%2520and%2520environmental%250Asamples%2520is%2520crucial%2520for%2520advancing%2520biomedical%2520and%2520chemical%2520sciences.%2520Tandem%2520mass%250Aspectrometry%2520%2528MS/MS%2529%2520is%2520the%2520leading%2520technique%2520for%2520high-throughput%2520elucidation%250Aof%2520molecular%2520structures.%2520However%252C%2520decoding%2520a%2520molecular%2520structure%2520from%2520its%2520mass%250Aspectrum%2520is%2520exceptionally%2520challenging%252C%2520even%2520when%2520performed%2520by%2520human%2520experts.%2520As%250Aa%2520result%252C%2520the%2520vast%2520majority%2520of%2520acquired%2520MS/MS%2520spectra%2520remain%2520uninterpreted%252C%250Athereby%2520limiting%2520our%2520understanding%2520of%2520the%2520underlying%2520%2528bio%2529chemical%2520processes.%250ADespite%2520decades%2520of%2520progress%2520in%2520machine%2520learning%2520applications%2520for%2520predicting%250Amolecular%2520structures%2520from%2520MS/MS%2520spectra%252C%2520the%2520development%2520of%2520new%2520methods%2520is%250Aseverely%2520hindered%2520by%2520the%2520lack%2520of%2520standard%2520datasets%2520and%2520evaluation%2520protocols.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520MassSpecGym%2520--%2520the%2520first%2520comprehensive%250Abenchmark%2520for%2520the%2520discovery%2520and%2520identification%2520of%2520molecules%2520from%2520MS/MS%2520data.%250AOur%2520benchmark%2520comprises%2520the%2520largest%2520publicly%2520available%2520collection%2520of%250Ahigh-quality%2520labeled%2520MS/MS%2520spectra%2520and%2520defines%2520three%2520MS/MS%2520annotation%250Achallenges%253A%2520de%2520novo%2520molecular%2520structure%2520generation%252C%2520molecule%2520retrieval%252C%2520and%250Aspectrum%2520simulation.%2520It%2520includes%2520new%2520evaluation%2520metrics%2520and%2520a%250Ageneralization-demanding%2520data%2520split%252C%2520therefore%2520standardizing%2520the%2520MS/MS%250Aannotation%2520tasks%2520and%2520rendering%2520the%2520problem%2520accessible%2520to%2520the%2520broad%2520machine%250Alearning%2520community.%2520MassSpecGym%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/pluskal-lab/MassSpecGym.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23326v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MassSpecGym%3A%20A%20benchmark%20for%20the%20discovery%20and%20identification%20of%0A%20%20molecules&entry.906535625=Roman%20Bushuiev%20and%20Anton%20Bushuiev%20and%20Niek%20F.%20de%20Jonge%20and%20Adamo%20Young%20and%20Fleming%20Kretschmer%20and%20Raman%20Samusevich%20and%20Janne%20Heirman%20and%20Fei%20Wang%20and%20Luke%20Zhang%20and%20Kai%20D%C3%BChrkop%20and%20Marcus%20Ludwig%20and%20Nils%20A.%20Haupt%20and%20Apurva%20Kalia%20and%20Corinna%20Brungs%20and%20Robin%20Schmid%20and%20Russell%20Greiner%20and%20Bo%20Wang%20and%20David%20S.%20Wishart%20and%20Li-Ping%20Liu%20and%20Juho%20Rousu%20and%20Wout%20Bittremieux%20and%20Hannes%20Rost%20and%20Tytus%20D.%20Mak%20and%20Soha%20Hassoun%20and%20Florian%20Huber%20and%20Justin%20J.%20J.%20van%20der%20Hooft%20and%20Michael%20A.%20Stravs%20and%20Sebastian%20B%C3%B6cker%20and%20Josef%20Sivic%20and%20Tom%C3%A1%C5%A1%20Pluskal&entry.1292438233=%20%20The%20discovery%20and%20identification%20of%20molecules%20in%20biological%20and%20environmental%0Asamples%20is%20crucial%20for%20advancing%20biomedical%20and%20chemical%20sciences.%20Tandem%20mass%0Aspectrometry%20%28MS/MS%29%20is%20the%20leading%20technique%20for%20high-throughput%20elucidation%0Aof%20molecular%20structures.%20However%2C%20decoding%20a%20molecular%20structure%20from%20its%20mass%0Aspectrum%20is%20exceptionally%20challenging%2C%20even%20when%20performed%20by%20human%20experts.%20As%0Aa%20result%2C%20the%20vast%20majority%20of%20acquired%20MS/MS%20spectra%20remain%20uninterpreted%2C%0Athereby%20limiting%20our%20understanding%20of%20the%20underlying%20%28bio%29chemical%20processes.%0ADespite%20decades%20of%20progress%20in%20machine%20learning%20applications%20for%20predicting%0Amolecular%20structures%20from%20MS/MS%20spectra%2C%20the%20development%20of%20new%20methods%20is%0Aseverely%20hindered%20by%20the%20lack%20of%20standard%20datasets%20and%20evaluation%20protocols.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20MassSpecGym%20--%20the%20first%20comprehensive%0Abenchmark%20for%20the%20discovery%20and%20identification%20of%20molecules%20from%20MS/MS%20data.%0AOur%20benchmark%20comprises%20the%20largest%20publicly%20available%20collection%20of%0Ahigh-quality%20labeled%20MS/MS%20spectra%20and%20defines%20three%20MS/MS%20annotation%0Achallenges%3A%20de%20novo%20molecular%20structure%20generation%2C%20molecule%20retrieval%2C%20and%0Aspectrum%20simulation.%20It%20includes%20new%20evaluation%20metrics%20and%20a%0Ageneralization-demanding%20data%20split%2C%20therefore%20standardizing%20the%20MS/MS%0Aannotation%20tasks%20and%20rendering%20the%20problem%20accessible%20to%20the%20broad%20machine%0Alearning%20community.%20MassSpecGym%20is%20publicly%20available%20at%0Ahttps%3A//github.com/pluskal-lab/MassSpecGym.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23326v3&entry.124074799=Read"},
{"title": "HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of\n  Human Head Motion in Diverse Conditions", "author": "Jianping Li and Qiutong Leng and Jinxing Liu and Xinhang Xu and Tongxin Jin and Muqing Cao and Thien-Minh Nguyen and Shenghai Yuan and Kun Cao and Lihua Xie", "abstract": "  Helmet-mounted wearable positioning systems are crucial for enhancing safety\nand facilitating coordination in industrial, construction, and emergency rescue\nenvironments. These systems, including LiDAR-Inertial Odometry (LIO) and\nVisual-Inertial Odometry (VIO), often face challenges in localization due to\nadverse environmental conditions such as dust, smoke, and limited visual\nfeatures. To address these limitations, we propose a novel head-mounted\nInertial Measurement Unit (IMU) dataset with ground truth, aimed at advancing\ndata-driven IMU pose estimation. Our dataset captures human head motion\npatterns using a helmet-mounted system, with data from ten participants\nperforming various activities. We explore the application of neural networks,\nspecifically Long Short-Term Memory (LSTM) and Transformer networks, to correct\nIMU biases and improve localization accuracy. Additionally, we evaluate the\nperformance of these methods across different IMU data window dimensions,\nmotion patterns, and sensor types. We release a publicly available dataset,\ndemonstrate the feasibility of advanced neural network approaches for\nhelmet-based localization, and provide evaluation metrics to establish a\nbaseline for future studies in this field. Data and code can be found at\nhttps://lqiutong.github.io/HelmetPoser.github.io/.\n", "link": "http://arxiv.org/abs/2409.05006v2", "date": "2025-02-14", "relevancy": 2.2004, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5719}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5355}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HelmetPoser%3A%20A%20Helmet-Mounted%20IMU%20Dataset%20for%20Data-Driven%20Estimation%20of%0A%20%20Human%20Head%20Motion%20in%20Diverse%20Conditions&body=Title%3A%20HelmetPoser%3A%20A%20Helmet-Mounted%20IMU%20Dataset%20for%20Data-Driven%20Estimation%20of%0A%20%20Human%20Head%20Motion%20in%20Diverse%20Conditions%0AAuthor%3A%20Jianping%20Li%20and%20Qiutong%20Leng%20and%20Jinxing%20Liu%20and%20Xinhang%20Xu%20and%20Tongxin%20Jin%20and%20Muqing%20Cao%20and%20Thien-Minh%20Nguyen%20and%20Shenghai%20Yuan%20and%20Kun%20Cao%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Helmet-mounted%20wearable%20positioning%20systems%20are%20crucial%20for%20enhancing%20safety%0Aand%20facilitating%20coordination%20in%20industrial%2C%20construction%2C%20and%20emergency%20rescue%0Aenvironments.%20These%20systems%2C%20including%20LiDAR-Inertial%20Odometry%20%28LIO%29%20and%0AVisual-Inertial%20Odometry%20%28VIO%29%2C%20often%20face%20challenges%20in%20localization%20due%20to%0Aadverse%20environmental%20conditions%20such%20as%20dust%2C%20smoke%2C%20and%20limited%20visual%0Afeatures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20head-mounted%0AInertial%20Measurement%20Unit%20%28IMU%29%20dataset%20with%20ground%20truth%2C%20aimed%20at%20advancing%0Adata-driven%20IMU%20pose%20estimation.%20Our%20dataset%20captures%20human%20head%20motion%0Apatterns%20using%20a%20helmet-mounted%20system%2C%20with%20data%20from%20ten%20participants%0Aperforming%20various%20activities.%20We%20explore%20the%20application%20of%20neural%20networks%2C%0Aspecifically%20Long%20Short-Term%20Memory%20%28LSTM%29%20and%20Transformer%20networks%2C%20to%20correct%0AIMU%20biases%20and%20improve%20localization%20accuracy.%20Additionally%2C%20we%20evaluate%20the%0Aperformance%20of%20these%20methods%20across%20different%20IMU%20data%20window%20dimensions%2C%0Amotion%20patterns%2C%20and%20sensor%20types.%20We%20release%20a%20publicly%20available%20dataset%2C%0Ademonstrate%20the%20feasibility%20of%20advanced%20neural%20network%20approaches%20for%0Ahelmet-based%20localization%2C%20and%20provide%20evaluation%20metrics%20to%20establish%20a%0Abaseline%20for%20future%20studies%20in%20this%20field.%20Data%20and%20code%20can%20be%20found%20at%0Ahttps%3A//lqiutong.github.io/HelmetPoser.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelmetPoser%253A%2520A%2520Helmet-Mounted%2520IMU%2520Dataset%2520for%2520Data-Driven%2520Estimation%2520of%250A%2520%2520Human%2520Head%2520Motion%2520in%2520Diverse%2520Conditions%26entry.906535625%3DJianping%2520Li%2520and%2520Qiutong%2520Leng%2520and%2520Jinxing%2520Liu%2520and%2520Xinhang%2520Xu%2520and%2520Tongxin%2520Jin%2520and%2520Muqing%2520Cao%2520and%2520Thien-Minh%2520Nguyen%2520and%2520Shenghai%2520Yuan%2520and%2520Kun%2520Cao%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Helmet-mounted%2520wearable%2520positioning%2520systems%2520are%2520crucial%2520for%2520enhancing%2520safety%250Aand%2520facilitating%2520coordination%2520in%2520industrial%252C%2520construction%252C%2520and%2520emergency%2520rescue%250Aenvironments.%2520These%2520systems%252C%2520including%2520LiDAR-Inertial%2520Odometry%2520%2528LIO%2529%2520and%250AVisual-Inertial%2520Odometry%2520%2528VIO%2529%252C%2520often%2520face%2520challenges%2520in%2520localization%2520due%2520to%250Aadverse%2520environmental%2520conditions%2520such%2520as%2520dust%252C%2520smoke%252C%2520and%2520limited%2520visual%250Afeatures.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520head-mounted%250AInertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520dataset%2520with%2520ground%2520truth%252C%2520aimed%2520at%2520advancing%250Adata-driven%2520IMU%2520pose%2520estimation.%2520Our%2520dataset%2520captures%2520human%2520head%2520motion%250Apatterns%2520using%2520a%2520helmet-mounted%2520system%252C%2520with%2520data%2520from%2520ten%2520participants%250Aperforming%2520various%2520activities.%2520We%2520explore%2520the%2520application%2520of%2520neural%2520networks%252C%250Aspecifically%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520and%2520Transformer%2520networks%252C%2520to%2520correct%250AIMU%2520biases%2520and%2520improve%2520localization%2520accuracy.%2520Additionally%252C%2520we%2520evaluate%2520the%250Aperformance%2520of%2520these%2520methods%2520across%2520different%2520IMU%2520data%2520window%2520dimensions%252C%250Amotion%2520patterns%252C%2520and%2520sensor%2520types.%2520We%2520release%2520a%2520publicly%2520available%2520dataset%252C%250Ademonstrate%2520the%2520feasibility%2520of%2520advanced%2520neural%2520network%2520approaches%2520for%250Ahelmet-based%2520localization%252C%2520and%2520provide%2520evaluation%2520metrics%2520to%2520establish%2520a%250Abaseline%2520for%2520future%2520studies%2520in%2520this%2520field.%2520Data%2520and%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//lqiutong.github.io/HelmetPoser.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HelmetPoser%3A%20A%20Helmet-Mounted%20IMU%20Dataset%20for%20Data-Driven%20Estimation%20of%0A%20%20Human%20Head%20Motion%20in%20Diverse%20Conditions&entry.906535625=Jianping%20Li%20and%20Qiutong%20Leng%20and%20Jinxing%20Liu%20and%20Xinhang%20Xu%20and%20Tongxin%20Jin%20and%20Muqing%20Cao%20and%20Thien-Minh%20Nguyen%20and%20Shenghai%20Yuan%20and%20Kun%20Cao%20and%20Lihua%20Xie&entry.1292438233=%20%20Helmet-mounted%20wearable%20positioning%20systems%20are%20crucial%20for%20enhancing%20safety%0Aand%20facilitating%20coordination%20in%20industrial%2C%20construction%2C%20and%20emergency%20rescue%0Aenvironments.%20These%20systems%2C%20including%20LiDAR-Inertial%20Odometry%20%28LIO%29%20and%0AVisual-Inertial%20Odometry%20%28VIO%29%2C%20often%20face%20challenges%20in%20localization%20due%20to%0Aadverse%20environmental%20conditions%20such%20as%20dust%2C%20smoke%2C%20and%20limited%20visual%0Afeatures.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20head-mounted%0AInertial%20Measurement%20Unit%20%28IMU%29%20dataset%20with%20ground%20truth%2C%20aimed%20at%20advancing%0Adata-driven%20IMU%20pose%20estimation.%20Our%20dataset%20captures%20human%20head%20motion%0Apatterns%20using%20a%20helmet-mounted%20system%2C%20with%20data%20from%20ten%20participants%0Aperforming%20various%20activities.%20We%20explore%20the%20application%20of%20neural%20networks%2C%0Aspecifically%20Long%20Short-Term%20Memory%20%28LSTM%29%20and%20Transformer%20networks%2C%20to%20correct%0AIMU%20biases%20and%20improve%20localization%20accuracy.%20Additionally%2C%20we%20evaluate%20the%0Aperformance%20of%20these%20methods%20across%20different%20IMU%20data%20window%20dimensions%2C%0Amotion%20patterns%2C%20and%20sensor%20types.%20We%20release%20a%20publicly%20available%20dataset%2C%0Ademonstrate%20the%20feasibility%20of%20advanced%20neural%20network%20approaches%20for%0Ahelmet-based%20localization%2C%20and%20provide%20evaluation%20metrics%20to%20establish%20a%0Abaseline%20for%20future%20studies%20in%20this%20field.%20Data%20and%20code%20can%20be%20found%20at%0Ahttps%3A//lqiutong.github.io/HelmetPoser.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05006v2&entry.124074799=Read"},
{"title": "Classification of Temporal Graphs using Persistent Homology", "author": "Siddharth Pritam and Rohit Roy and Madhav Cherupilil Sajeev", "abstract": "  Temporal graphs effectively model dynamic systems by representing\ninteractions as timestamped edges. However, analytical tools for temporal\ngraphs are limited compared to static graphs. We propose a novel method for\nanalyzing temporal graphs using Persistent Homology. Our approach leverages\n$\\delta$-temporal motifs (recurrent subgraphs) to capture temporal dynamics\n%without aggregation\n  . By evolving these motifs, we define the \\textit{average filtration} and\ncompute PH on the associated clique complex. This method captures both local\nand global temporal structures and is stable with respect to reference models.\nWe demonstrate the applicability of our approach to the temporal graph\nclassification task. Experiments verify the effectiveness of our approach,\nachieving over 92\\% accuracy, with some cases reaching 100\\%. Unlike existing\nmethods that require node classes, our approach is node class free, offering\nflexibility for a wide range of temporal graph analysis.\n", "link": "http://arxiv.org/abs/2502.10076v1", "date": "2025-02-14", "relevancy": 2.1964, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4862}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4189}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Temporal%20Graphs%20using%20Persistent%20Homology&body=Title%3A%20Classification%20of%20Temporal%20Graphs%20using%20Persistent%20Homology%0AAuthor%3A%20Siddharth%20Pritam%20and%20Rohit%20Roy%20and%20Madhav%20Cherupilil%20Sajeev%0AAbstract%3A%20%20%20Temporal%20graphs%20effectively%20model%20dynamic%20systems%20by%20representing%0Ainteractions%20as%20timestamped%20edges.%20However%2C%20analytical%20tools%20for%20temporal%0Agraphs%20are%20limited%20compared%20to%20static%20graphs.%20We%20propose%20a%20novel%20method%20for%0Aanalyzing%20temporal%20graphs%20using%20Persistent%20Homology.%20Our%20approach%20leverages%0A%24%5Cdelta%24-temporal%20motifs%20%28recurrent%20subgraphs%29%20to%20capture%20temporal%20dynamics%0A%25without%20aggregation%0A%20%20.%20By%20evolving%20these%20motifs%2C%20we%20define%20the%20%5Ctextit%7Baverage%20filtration%7D%20and%0Acompute%20PH%20on%20the%20associated%20clique%20complex.%20This%20method%20captures%20both%20local%0Aand%20global%20temporal%20structures%20and%20is%20stable%20with%20respect%20to%20reference%20models.%0AWe%20demonstrate%20the%20applicability%20of%20our%20approach%20to%20the%20temporal%20graph%0Aclassification%20task.%20Experiments%20verify%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20over%2092%5C%25%20accuracy%2C%20with%20some%20cases%20reaching%20100%5C%25.%20Unlike%20existing%0Amethods%20that%20require%20node%20classes%2C%20our%20approach%20is%20node%20class%20free%2C%20offering%0Aflexibility%20for%20a%20wide%20range%20of%20temporal%20graph%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Temporal%2520Graphs%2520using%2520Persistent%2520Homology%26entry.906535625%3DSiddharth%2520Pritam%2520and%2520Rohit%2520Roy%2520and%2520Madhav%2520Cherupilil%2520Sajeev%26entry.1292438233%3D%2520%2520Temporal%2520graphs%2520effectively%2520model%2520dynamic%2520systems%2520by%2520representing%250Ainteractions%2520as%2520timestamped%2520edges.%2520However%252C%2520analytical%2520tools%2520for%2520temporal%250Agraphs%2520are%2520limited%2520compared%2520to%2520static%2520graphs.%2520We%2520propose%2520a%2520novel%2520method%2520for%250Aanalyzing%2520temporal%2520graphs%2520using%2520Persistent%2520Homology.%2520Our%2520approach%2520leverages%250A%2524%255Cdelta%2524-temporal%2520motifs%2520%2528recurrent%2520subgraphs%2529%2520to%2520capture%2520temporal%2520dynamics%250A%2525without%2520aggregation%250A%2520%2520.%2520By%2520evolving%2520these%2520motifs%252C%2520we%2520define%2520the%2520%255Ctextit%257Baverage%2520filtration%257D%2520and%250Acompute%2520PH%2520on%2520the%2520associated%2520clique%2520complex.%2520This%2520method%2520captures%2520both%2520local%250Aand%2520global%2520temporal%2520structures%2520and%2520is%2520stable%2520with%2520respect%2520to%2520reference%2520models.%250AWe%2520demonstrate%2520the%2520applicability%2520of%2520our%2520approach%2520to%2520the%2520temporal%2520graph%250Aclassification%2520task.%2520Experiments%2520verify%2520the%2520effectiveness%2520of%2520our%2520approach%252C%250Aachieving%2520over%252092%255C%2525%2520accuracy%252C%2520with%2520some%2520cases%2520reaching%2520100%255C%2525.%2520Unlike%2520existing%250Amethods%2520that%2520require%2520node%2520classes%252C%2520our%2520approach%2520is%2520node%2520class%2520free%252C%2520offering%250Aflexibility%2520for%2520a%2520wide%2520range%2520of%2520temporal%2520graph%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Temporal%20Graphs%20using%20Persistent%20Homology&entry.906535625=Siddharth%20Pritam%20and%20Rohit%20Roy%20and%20Madhav%20Cherupilil%20Sajeev&entry.1292438233=%20%20Temporal%20graphs%20effectively%20model%20dynamic%20systems%20by%20representing%0Ainteractions%20as%20timestamped%20edges.%20However%2C%20analytical%20tools%20for%20temporal%0Agraphs%20are%20limited%20compared%20to%20static%20graphs.%20We%20propose%20a%20novel%20method%20for%0Aanalyzing%20temporal%20graphs%20using%20Persistent%20Homology.%20Our%20approach%20leverages%0A%24%5Cdelta%24-temporal%20motifs%20%28recurrent%20subgraphs%29%20to%20capture%20temporal%20dynamics%0A%25without%20aggregation%0A%20%20.%20By%20evolving%20these%20motifs%2C%20we%20define%20the%20%5Ctextit%7Baverage%20filtration%7D%20and%0Acompute%20PH%20on%20the%20associated%20clique%20complex.%20This%20method%20captures%20both%20local%0Aand%20global%20temporal%20structures%20and%20is%20stable%20with%20respect%20to%20reference%20models.%0AWe%20demonstrate%20the%20applicability%20of%20our%20approach%20to%20the%20temporal%20graph%0Aclassification%20task.%20Experiments%20verify%20the%20effectiveness%20of%20our%20approach%2C%0Aachieving%20over%2092%5C%25%20accuracy%2C%20with%20some%20cases%20reaching%20100%5C%25.%20Unlike%20existing%0Amethods%20that%20require%20node%20classes%2C%20our%20approach%20is%20node%20class%20free%2C%20offering%0Aflexibility%20for%20a%20wide%20range%20of%20temporal%20graph%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10076v1&entry.124074799=Read"},
{"title": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering", "author": "Nick Ferguson and Liane Guillou and Alan Bundy and Kwabena Nuamah", "abstract": "  Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.\n", "link": "http://arxiv.org/abs/2502.10338v1", "date": "2025-02-14", "relevancy": 2.1904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Meta-%20and%20Object-Level%20Reasoning%20of%20Large%20Language%20Models%0A%20%20for%20Question%20Answering&body=Title%3A%20Evaluating%20the%20Meta-%20and%20Object-Level%20Reasoning%20of%20Large%20Language%20Models%0A%20%20for%20Question%20Answering%0AAuthor%3A%20Nick%20Ferguson%20and%20Liane%20Guillou%20and%20Alan%20Bundy%20and%20Kwabena%20Nuamah%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20natural%20language%20tasks%20but%20still%20face%0Achallenges%20in%20Question%20Answering%20%28QA%29%20tasks%20requiring%20complex%2C%20multi-step%0Areasoning.%20We%20outline%20the%20types%20of%20reasoning%20required%20in%20some%20of%20these%20tasks%2C%0Aand%20reframe%20them%20in%20terms%20of%20meta-level%20reasoning%20%28akin%20to%20high-level%20strategic%0Areasoning%20or%20planning%29%20and%20object-level%20reasoning%20%28embodied%20in%20lower-level%0Atasks%20such%20as%20mathematical%20reasoning%29.%20Franklin%2C%20a%20novel%20dataset%20with%0Arequirements%20of%20meta-%20and%20object-level%20reasoning%2C%20is%20introduced%20and%20used%20along%0Awith%20three%20other%20datasets%20to%20evaluate%20four%20LLMs%20at%20question%20answering%20tasks%0Arequiring%20multiple%20steps%20of%20reasoning.%20Results%20from%20human%20annotation%20studies%0Asuggest%20LLMs%20demonstrate%20meta-level%20reasoning%20with%20high%20frequency%2C%20but%20struggle%0Awith%20object-level%20reasoning%20tasks%20in%20some%20of%20the%20datasets%20used.%20Additionally%2C%0Aevidence%20suggests%20that%20LLMs%20find%20the%20object-level%20reasoning%20required%20for%20the%0Aquestions%20in%20the%20Franklin%20dataset%20challenging%2C%20yet%20they%20do%20exhibit%20strong%0Aperformance%20with%20respect%20to%20the%20meta-level%20reasoning%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Meta-%2520and%2520Object-Level%2520Reasoning%2520of%2520Large%2520Language%2520Models%250A%2520%2520for%2520Question%2520Answering%26entry.906535625%3DNick%2520Ferguson%2520and%2520Liane%2520Guillou%2520and%2520Alan%2520Bundy%2520and%2520Kwabena%2520Nuamah%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520natural%2520language%2520tasks%2520but%2520still%2520face%250Achallenges%2520in%2520Question%2520Answering%2520%2528QA%2529%2520tasks%2520requiring%2520complex%252C%2520multi-step%250Areasoning.%2520We%2520outline%2520the%2520types%2520of%2520reasoning%2520required%2520in%2520some%2520of%2520these%2520tasks%252C%250Aand%2520reframe%2520them%2520in%2520terms%2520of%2520meta-level%2520reasoning%2520%2528akin%2520to%2520high-level%2520strategic%250Areasoning%2520or%2520planning%2529%2520and%2520object-level%2520reasoning%2520%2528embodied%2520in%2520lower-level%250Atasks%2520such%2520as%2520mathematical%2520reasoning%2529.%2520Franklin%252C%2520a%2520novel%2520dataset%2520with%250Arequirements%2520of%2520meta-%2520and%2520object-level%2520reasoning%252C%2520is%2520introduced%2520and%2520used%2520along%250Awith%2520three%2520other%2520datasets%2520to%2520evaluate%2520four%2520LLMs%2520at%2520question%2520answering%2520tasks%250Arequiring%2520multiple%2520steps%2520of%2520reasoning.%2520Results%2520from%2520human%2520annotation%2520studies%250Asuggest%2520LLMs%2520demonstrate%2520meta-level%2520reasoning%2520with%2520high%2520frequency%252C%2520but%2520struggle%250Awith%2520object-level%2520reasoning%2520tasks%2520in%2520some%2520of%2520the%2520datasets%2520used.%2520Additionally%252C%250Aevidence%2520suggests%2520that%2520LLMs%2520find%2520the%2520object-level%2520reasoning%2520required%2520for%2520the%250Aquestions%2520in%2520the%2520Franklin%2520dataset%2520challenging%252C%2520yet%2520they%2520do%2520exhibit%2520strong%250Aperformance%2520with%2520respect%2520to%2520the%2520meta-level%2520reasoning%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Meta-%20and%20Object-Level%20Reasoning%20of%20Large%20Language%20Models%0A%20%20for%20Question%20Answering&entry.906535625=Nick%20Ferguson%20and%20Liane%20Guillou%20and%20Alan%20Bundy%20and%20Kwabena%20Nuamah&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20natural%20language%20tasks%20but%20still%20face%0Achallenges%20in%20Question%20Answering%20%28QA%29%20tasks%20requiring%20complex%2C%20multi-step%0Areasoning.%20We%20outline%20the%20types%20of%20reasoning%20required%20in%20some%20of%20these%20tasks%2C%0Aand%20reframe%20them%20in%20terms%20of%20meta-level%20reasoning%20%28akin%20to%20high-level%20strategic%0Areasoning%20or%20planning%29%20and%20object-level%20reasoning%20%28embodied%20in%20lower-level%0Atasks%20such%20as%20mathematical%20reasoning%29.%20Franklin%2C%20a%20novel%20dataset%20with%0Arequirements%20of%20meta-%20and%20object-level%20reasoning%2C%20is%20introduced%20and%20used%20along%0Awith%20three%20other%20datasets%20to%20evaluate%20four%20LLMs%20at%20question%20answering%20tasks%0Arequiring%20multiple%20steps%20of%20reasoning.%20Results%20from%20human%20annotation%20studies%0Asuggest%20LLMs%20demonstrate%20meta-level%20reasoning%20with%20high%20frequency%2C%20but%20struggle%0Awith%20object-level%20reasoning%20tasks%20in%20some%20of%20the%20datasets%20used.%20Additionally%2C%0Aevidence%20suggests%20that%20LLMs%20find%20the%20object-level%20reasoning%20required%20for%20the%0Aquestions%20in%20the%20Franklin%20dataset%20challenging%2C%20yet%20they%20do%20exhibit%20strong%0Aperformance%20with%20respect%20to%20the%20meta-level%20reasoning%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10338v1&entry.124074799=Read"},
{"title": "Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph\n  Generation", "author": "Gamal Elghazaly and Raphael Frank", "abstract": "  High-Definition (HD) maps play a crucial role in autonomous vehicle\nnavigation, complementing onboard perception sensors for improved accuracy and\nsafety. Traditional HD map generation relies on dedicated mapping vehicles,\nwhich are costly and fail to capture real-time infrastructure changes. This\npaper presents HDMapLaneNet, a novel framework leveraging V2X communication and\nScene Graph Generation to collaboratively construct a localized geometric layer\nof HD maps. The approach extracts lane centerlines from front-facing camera\nimages, represents them as graphs, and transmits the data for global\naggregation to the cloud via V2X. Preliminary results on the nuScenes dataset\ndemonstrate superior association prediction performance compared to a\nstate-of-the-art method.\n", "link": "http://arxiv.org/abs/2502.10127v1", "date": "2025-02-14", "relevancy": 2.1701, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5582}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20V2X%20for%20Collaborative%20HD%20Maps%20Construction%20Using%20Scene%20Graph%0A%20%20Generation&body=Title%3A%20Leveraging%20V2X%20for%20Collaborative%20HD%20Maps%20Construction%20Using%20Scene%20Graph%0A%20%20Generation%0AAuthor%3A%20Gamal%20Elghazaly%20and%20Raphael%20Frank%0AAbstract%3A%20%20%20High-Definition%20%28HD%29%20maps%20play%20a%20crucial%20role%20in%20autonomous%20vehicle%0Anavigation%2C%20complementing%20onboard%20perception%20sensors%20for%20improved%20accuracy%20and%0Asafety.%20Traditional%20HD%20map%20generation%20relies%20on%20dedicated%20mapping%20vehicles%2C%0Awhich%20are%20costly%20and%20fail%20to%20capture%20real-time%20infrastructure%20changes.%20This%0Apaper%20presents%20HDMapLaneNet%2C%20a%20novel%20framework%20leveraging%20V2X%20communication%20and%0AScene%20Graph%20Generation%20to%20collaboratively%20construct%20a%20localized%20geometric%20layer%0Aof%20HD%20maps.%20The%20approach%20extracts%20lane%20centerlines%20from%20front-facing%20camera%0Aimages%2C%20represents%20them%20as%20graphs%2C%20and%20transmits%20the%20data%20for%20global%0Aaggregation%20to%20the%20cloud%20via%20V2X.%20Preliminary%20results%20on%20the%20nuScenes%20dataset%0Ademonstrate%20superior%20association%20prediction%20performance%20compared%20to%20a%0Astate-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520V2X%2520for%2520Collaborative%2520HD%2520Maps%2520Construction%2520Using%2520Scene%2520Graph%250A%2520%2520Generation%26entry.906535625%3DGamal%2520Elghazaly%2520and%2520Raphael%2520Frank%26entry.1292438233%3D%2520%2520High-Definition%2520%2528HD%2529%2520maps%2520play%2520a%2520crucial%2520role%2520in%2520autonomous%2520vehicle%250Anavigation%252C%2520complementing%2520onboard%2520perception%2520sensors%2520for%2520improved%2520accuracy%2520and%250Asafety.%2520Traditional%2520HD%2520map%2520generation%2520relies%2520on%2520dedicated%2520mapping%2520vehicles%252C%250Awhich%2520are%2520costly%2520and%2520fail%2520to%2520capture%2520real-time%2520infrastructure%2520changes.%2520This%250Apaper%2520presents%2520HDMapLaneNet%252C%2520a%2520novel%2520framework%2520leveraging%2520V2X%2520communication%2520and%250AScene%2520Graph%2520Generation%2520to%2520collaboratively%2520construct%2520a%2520localized%2520geometric%2520layer%250Aof%2520HD%2520maps.%2520The%2520approach%2520extracts%2520lane%2520centerlines%2520from%2520front-facing%2520camera%250Aimages%252C%2520represents%2520them%2520as%2520graphs%252C%2520and%2520transmits%2520the%2520data%2520for%2520global%250Aaggregation%2520to%2520the%2520cloud%2520via%2520V2X.%2520Preliminary%2520results%2520on%2520the%2520nuScenes%2520dataset%250Ademonstrate%2520superior%2520association%2520prediction%2520performance%2520compared%2520to%2520a%250Astate-of-the-art%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20V2X%20for%20Collaborative%20HD%20Maps%20Construction%20Using%20Scene%20Graph%0A%20%20Generation&entry.906535625=Gamal%20Elghazaly%20and%20Raphael%20Frank&entry.1292438233=%20%20High-Definition%20%28HD%29%20maps%20play%20a%20crucial%20role%20in%20autonomous%20vehicle%0Anavigation%2C%20complementing%20onboard%20perception%20sensors%20for%20improved%20accuracy%20and%0Asafety.%20Traditional%20HD%20map%20generation%20relies%20on%20dedicated%20mapping%20vehicles%2C%0Awhich%20are%20costly%20and%20fail%20to%20capture%20real-time%20infrastructure%20changes.%20This%0Apaper%20presents%20HDMapLaneNet%2C%20a%20novel%20framework%20leveraging%20V2X%20communication%20and%0AScene%20Graph%20Generation%20to%20collaboratively%20construct%20a%20localized%20geometric%20layer%0Aof%20HD%20maps.%20The%20approach%20extracts%20lane%20centerlines%20from%20front-facing%20camera%0Aimages%2C%20represents%20them%20as%20graphs%2C%20and%20transmits%20the%20data%20for%20global%0Aaggregation%20to%20the%20cloud%20via%20V2X.%20Preliminary%20results%20on%20the%20nuScenes%20dataset%0Ademonstrate%20superior%20association%20prediction%20performance%20compared%20to%20a%0Astate-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10127v1&entry.124074799=Read"},
{"title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges", "author": "Clinton J. Wang and Dean Lee and Cristina Menghini and Johannes Mols and Jack Doughty and Adam Khoja and Jayson Lynch and Sean Hendryx and Summer Yue and Dan Hendrycks", "abstract": "  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n", "link": "http://arxiv.org/abs/2502.08859v2", "date": "2025-02-14", "relevancy": 2.1659, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnigmaEval%3A%20A%20Benchmark%20of%20Long%20Multimodal%20Reasoning%20Challenges&body=Title%3A%20EnigmaEval%3A%20A%20Benchmark%20of%20Long%20Multimodal%20Reasoning%20Challenges%0AAuthor%3A%20Clinton%20J.%20Wang%20and%20Dean%20Lee%20and%20Cristina%20Menghini%20and%20Johannes%20Mols%20and%20Jack%20Doughty%20and%20Adam%20Khoja%20and%20Jayson%20Lynch%20and%20Sean%20Hendryx%20and%20Summer%20Yue%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20As%20language%20models%20master%20existing%20reasoning%20benchmarks%2C%20we%20need%20new%0Achallenges%20to%20evaluate%20their%20cognitive%20frontiers.%20Puzzle-solving%20events%20are%0Arich%20repositories%20of%20challenging%20multimodal%20problems%20that%20test%20a%20wide%20range%20of%0Aadvanced%20reasoning%20and%20knowledge%20capabilities%2C%20making%20them%20a%20unique%20testbed%20for%0Aevaluating%20frontier%20language%20models.%20We%20introduce%20EnigmaEval%2C%20a%20dataset%20of%0Aproblems%20and%20solutions%20derived%20from%20puzzle%20competitions%20and%20events%20that%20probes%0Amodels%27%20ability%20to%20perform%20implicit%20knowledge%20synthesis%20and%20multi-step%0Adeductive%20reasoning.%20Unlike%20existing%20reasoning%20and%20knowledge%20benchmarks%2C%20puzzle%0Asolving%20challenges%20models%20to%20discover%20hidden%20connections%20between%20seemingly%0Aunrelated%20pieces%20of%20information%20to%20uncover%20solution%20paths.%20The%20benchmark%0Acomprises%201184%20puzzles%20of%20varying%20complexity%20--%20each%20typically%20requiring%20teams%0Aof%20skilled%20solvers%20hours%20to%20days%20to%20complete%20--%20with%20unambiguous%2C%20verifiable%0Asolutions%20that%20enable%20efficient%20evaluation.%20State-of-the-art%20language%20models%0Aachieve%20extremely%20low%20accuracy%20on%20these%20puzzles%2C%20even%20lower%20than%20other%0Adifficult%20benchmarks%20such%20as%20Humanity%27s%20Last%20Exam%2C%20unveiling%20models%27%0Ashortcomings%20when%20challenged%20with%20problems%20requiring%20unstructured%20and%20lateral%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08859v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnigmaEval%253A%2520A%2520Benchmark%2520of%2520Long%2520Multimodal%2520Reasoning%2520Challenges%26entry.906535625%3DClinton%2520J.%2520Wang%2520and%2520Dean%2520Lee%2520and%2520Cristina%2520Menghini%2520and%2520Johannes%2520Mols%2520and%2520Jack%2520Doughty%2520and%2520Adam%2520Khoja%2520and%2520Jayson%2520Lynch%2520and%2520Sean%2520Hendryx%2520and%2520Summer%2520Yue%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520master%2520existing%2520reasoning%2520benchmarks%252C%2520we%2520need%2520new%250Achallenges%2520to%2520evaluate%2520their%2520cognitive%2520frontiers.%2520Puzzle-solving%2520events%2520are%250Arich%2520repositories%2520of%2520challenging%2520multimodal%2520problems%2520that%2520test%2520a%2520wide%2520range%2520of%250Aadvanced%2520reasoning%2520and%2520knowledge%2520capabilities%252C%2520making%2520them%2520a%2520unique%2520testbed%2520for%250Aevaluating%2520frontier%2520language%2520models.%2520We%2520introduce%2520EnigmaEval%252C%2520a%2520dataset%2520of%250Aproblems%2520and%2520solutions%2520derived%2520from%2520puzzle%2520competitions%2520and%2520events%2520that%2520probes%250Amodels%2527%2520ability%2520to%2520perform%2520implicit%2520knowledge%2520synthesis%2520and%2520multi-step%250Adeductive%2520reasoning.%2520Unlike%2520existing%2520reasoning%2520and%2520knowledge%2520benchmarks%252C%2520puzzle%250Asolving%2520challenges%2520models%2520to%2520discover%2520hidden%2520connections%2520between%2520seemingly%250Aunrelated%2520pieces%2520of%2520information%2520to%2520uncover%2520solution%2520paths.%2520The%2520benchmark%250Acomprises%25201184%2520puzzles%2520of%2520varying%2520complexity%2520--%2520each%2520typically%2520requiring%2520teams%250Aof%2520skilled%2520solvers%2520hours%2520to%2520days%2520to%2520complete%2520--%2520with%2520unambiguous%252C%2520verifiable%250Asolutions%2520that%2520enable%2520efficient%2520evaluation.%2520State-of-the-art%2520language%2520models%250Aachieve%2520extremely%2520low%2520accuracy%2520on%2520these%2520puzzles%252C%2520even%2520lower%2520than%2520other%250Adifficult%2520benchmarks%2520such%2520as%2520Humanity%2527s%2520Last%2520Exam%252C%2520unveiling%2520models%2527%250Ashortcomings%2520when%2520challenged%2520with%2520problems%2520requiring%2520unstructured%2520and%2520lateral%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08859v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnigmaEval%3A%20A%20Benchmark%20of%20Long%20Multimodal%20Reasoning%20Challenges&entry.906535625=Clinton%20J.%20Wang%20and%20Dean%20Lee%20and%20Cristina%20Menghini%20and%20Johannes%20Mols%20and%20Jack%20Doughty%20and%20Adam%20Khoja%20and%20Jayson%20Lynch%20and%20Sean%20Hendryx%20and%20Summer%20Yue%20and%20Dan%20Hendrycks&entry.1292438233=%20%20As%20language%20models%20master%20existing%20reasoning%20benchmarks%2C%20we%20need%20new%0Achallenges%20to%20evaluate%20their%20cognitive%20frontiers.%20Puzzle-solving%20events%20are%0Arich%20repositories%20of%20challenging%20multimodal%20problems%20that%20test%20a%20wide%20range%20of%0Aadvanced%20reasoning%20and%20knowledge%20capabilities%2C%20making%20them%20a%20unique%20testbed%20for%0Aevaluating%20frontier%20language%20models.%20We%20introduce%20EnigmaEval%2C%20a%20dataset%20of%0Aproblems%20and%20solutions%20derived%20from%20puzzle%20competitions%20and%20events%20that%20probes%0Amodels%27%20ability%20to%20perform%20implicit%20knowledge%20synthesis%20and%20multi-step%0Adeductive%20reasoning.%20Unlike%20existing%20reasoning%20and%20knowledge%20benchmarks%2C%20puzzle%0Asolving%20challenges%20models%20to%20discover%20hidden%20connections%20between%20seemingly%0Aunrelated%20pieces%20of%20information%20to%20uncover%20solution%20paths.%20The%20benchmark%0Acomprises%201184%20puzzles%20of%20varying%20complexity%20--%20each%20typically%20requiring%20teams%0Aof%20skilled%20solvers%20hours%20to%20days%20to%20complete%20--%20with%20unambiguous%2C%20verifiable%0Asolutions%20that%20enable%20efficient%20evaluation.%20State-of-the-art%20language%20models%0Aachieve%20extremely%20low%20accuracy%20on%20these%20puzzles%2C%20even%20lower%20than%20other%0Adifficult%20benchmarks%20such%20as%20Humanity%27s%20Last%20Exam%2C%20unveiling%20models%27%0Ashortcomings%20when%20challenged%20with%20problems%20requiring%20unstructured%20and%20lateral%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08859v2&entry.124074799=Read"},
{"title": "QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for\n  Scribble-Supervised Segmentation of Medical Images", "author": "Thien B. Nguyen-Tat and Hoang-An Vo and Phuoc-Sang Dang", "abstract": "  The deployment of advanced deep learning models for medical image\nsegmentation is often constrained by the requirement for extensively annotated\ndatasets. Weakly-supervised learning, which allows less precise labels, has\nbecome a promising solution to this challenge. Building on this approach, we\npropose QMaxViT-Unet+, a novel framework for scribble-supervised medical image\nsegmentation. This framework is built on the U-Net architecture, with the\nencoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks.\nThese blocks enhance the model's ability to learn local and global features\nefficiently. Additionally, our approach integrates a query-based Transformer\ndecoder to refine features and an edge enhancement module to compensate for the\nlimited boundary information in the scribble label. We evaluate the proposed\nQMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal\npolyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation\nmetrics include the Dice similarity coefficient (DSC) and the 95th percentile\nof Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+\nachieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on\nMS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm\nHD95 on BUSI. These results demonstrate that our method outperforms existing\napproaches in terms of accuracy, robustness, and efficiency while remaining\ncompetitive with fully-supervised learning approaches. This makes it ideal for\nmedical image analysis, where high-quality annotations are often scarce and\nrequire significant effort and expense. The code is available at:\nhttps://github.com/anpc849/QMaxViT-Unet\n", "link": "http://arxiv.org/abs/2502.10294v1", "date": "2025-02-14", "relevancy": 2.1584, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QMaxViT-Unet%2B%3A%20A%20Query-Based%20MaxViT-Unet%20with%20Edge%20Enhancement%20for%0A%20%20Scribble-Supervised%20Segmentation%20of%20Medical%20Images&body=Title%3A%20QMaxViT-Unet%2B%3A%20A%20Query-Based%20MaxViT-Unet%20with%20Edge%20Enhancement%20for%0A%20%20Scribble-Supervised%20Segmentation%20of%20Medical%20Images%0AAuthor%3A%20Thien%20B.%20Nguyen-Tat%20and%20Hoang-An%20Vo%20and%20Phuoc-Sang%20Dang%0AAbstract%3A%20%20%20The%20deployment%20of%20advanced%20deep%20learning%20models%20for%20medical%20image%0Asegmentation%20is%20often%20constrained%20by%20the%20requirement%20for%20extensively%20annotated%0Adatasets.%20Weakly-supervised%20learning%2C%20which%20allows%20less%20precise%20labels%2C%20has%0Abecome%20a%20promising%20solution%20to%20this%20challenge.%20Building%20on%20this%20approach%2C%20we%0Apropose%20QMaxViT-Unet%2B%2C%20a%20novel%20framework%20for%20scribble-supervised%20medical%20image%0Asegmentation.%20This%20framework%20is%20built%20on%20the%20U-Net%20architecture%2C%20with%20the%0Aencoder%20and%20decoder%20replaced%20by%20Multi-Axis%20Vision%20Transformer%20%28MaxViT%29%20blocks.%0AThese%20blocks%20enhance%20the%20model%27s%20ability%20to%20learn%20local%20and%20global%20features%0Aefficiently.%20Additionally%2C%20our%20approach%20integrates%20a%20query-based%20Transformer%0Adecoder%20to%20refine%20features%20and%20an%20edge%20enhancement%20module%20to%20compensate%20for%20the%0Alimited%20boundary%20information%20in%20the%20scribble%20label.%20We%20evaluate%20the%20proposed%0AQMaxViT-Unet%2B%20on%20four%20public%20datasets%20focused%20on%20cardiac%20structures%2C%20colorectal%0Apolyps%2C%20and%20breast%20cancer%3A%20ACDC%2C%20MS-CMRSeg%2C%20SUN-SEG%2C%20and%20BUSI.%20Evaluation%0Ametrics%20include%20the%20Dice%20similarity%20coefficient%20%28DSC%29%20and%20the%2095th%20percentile%0Aof%20Hausdorff%20distance%20%28HD95%29.%20Experimental%20results%20show%20that%20QMaxViT-Unet%2B%0Aachieves%2089.1%5C%25%20DSC%20and%201.316mm%20HD95%20on%20ACDC%2C%2088.4%5C%25%20DSC%20and%202.226mm%20HD95%20on%0AMS-CMRSeg%2C%2071.4%5C%25%20DSC%20and%204.996mm%20HD95%20on%20SUN-SEG%2C%20and%2069.4%5C%25%20DSC%20and%2050.122mm%0AHD95%20on%20BUSI.%20These%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20efficiency%20while%20remaining%0Acompetitive%20with%20fully-supervised%20learning%20approaches.%20This%20makes%20it%20ideal%20for%0Amedical%20image%20analysis%2C%20where%20high-quality%20annotations%20are%20often%20scarce%20and%0Arequire%20significant%20effort%20and%20expense.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/anpc849/QMaxViT-Unet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQMaxViT-Unet%252B%253A%2520A%2520Query-Based%2520MaxViT-Unet%2520with%2520Edge%2520Enhancement%2520for%250A%2520%2520Scribble-Supervised%2520Segmentation%2520of%2520Medical%2520Images%26entry.906535625%3DThien%2520B.%2520Nguyen-Tat%2520and%2520Hoang-An%2520Vo%2520and%2520Phuoc-Sang%2520Dang%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520advanced%2520deep%2520learning%2520models%2520for%2520medical%2520image%250Asegmentation%2520is%2520often%2520constrained%2520by%2520the%2520requirement%2520for%2520extensively%2520annotated%250Adatasets.%2520Weakly-supervised%2520learning%252C%2520which%2520allows%2520less%2520precise%2520labels%252C%2520has%250Abecome%2520a%2520promising%2520solution%2520to%2520this%2520challenge.%2520Building%2520on%2520this%2520approach%252C%2520we%250Apropose%2520QMaxViT-Unet%252B%252C%2520a%2520novel%2520framework%2520for%2520scribble-supervised%2520medical%2520image%250Asegmentation.%2520This%2520framework%2520is%2520built%2520on%2520the%2520U-Net%2520architecture%252C%2520with%2520the%250Aencoder%2520and%2520decoder%2520replaced%2520by%2520Multi-Axis%2520Vision%2520Transformer%2520%2528MaxViT%2529%2520blocks.%250AThese%2520blocks%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520learn%2520local%2520and%2520global%2520features%250Aefficiently.%2520Additionally%252C%2520our%2520approach%2520integrates%2520a%2520query-based%2520Transformer%250Adecoder%2520to%2520refine%2520features%2520and%2520an%2520edge%2520enhancement%2520module%2520to%2520compensate%2520for%2520the%250Alimited%2520boundary%2520information%2520in%2520the%2520scribble%2520label.%2520We%2520evaluate%2520the%2520proposed%250AQMaxViT-Unet%252B%2520on%2520four%2520public%2520datasets%2520focused%2520on%2520cardiac%2520structures%252C%2520colorectal%250Apolyps%252C%2520and%2520breast%2520cancer%253A%2520ACDC%252C%2520MS-CMRSeg%252C%2520SUN-SEG%252C%2520and%2520BUSI.%2520Evaluation%250Ametrics%2520include%2520the%2520Dice%2520similarity%2520coefficient%2520%2528DSC%2529%2520and%2520the%252095th%2520percentile%250Aof%2520Hausdorff%2520distance%2520%2528HD95%2529.%2520Experimental%2520results%2520show%2520that%2520QMaxViT-Unet%252B%250Aachieves%252089.1%255C%2525%2520DSC%2520and%25201.316mm%2520HD95%2520on%2520ACDC%252C%252088.4%255C%2525%2520DSC%2520and%25202.226mm%2520HD95%2520on%250AMS-CMRSeg%252C%252071.4%255C%2525%2520DSC%2520and%25204.996mm%2520HD95%2520on%2520SUN-SEG%252C%2520and%252069.4%255C%2525%2520DSC%2520and%252050.122mm%250AHD95%2520on%2520BUSI.%2520These%2520results%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%250Aapproaches%2520in%2520terms%2520of%2520accuracy%252C%2520robustness%252C%2520and%2520efficiency%2520while%2520remaining%250Acompetitive%2520with%2520fully-supervised%2520learning%2520approaches.%2520This%2520makes%2520it%2520ideal%2520for%250Amedical%2520image%2520analysis%252C%2520where%2520high-quality%2520annotations%2520are%2520often%2520scarce%2520and%250Arequire%2520significant%2520effort%2520and%2520expense.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/anpc849/QMaxViT-Unet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QMaxViT-Unet%2B%3A%20A%20Query-Based%20MaxViT-Unet%20with%20Edge%20Enhancement%20for%0A%20%20Scribble-Supervised%20Segmentation%20of%20Medical%20Images&entry.906535625=Thien%20B.%20Nguyen-Tat%20and%20Hoang-An%20Vo%20and%20Phuoc-Sang%20Dang&entry.1292438233=%20%20The%20deployment%20of%20advanced%20deep%20learning%20models%20for%20medical%20image%0Asegmentation%20is%20often%20constrained%20by%20the%20requirement%20for%20extensively%20annotated%0Adatasets.%20Weakly-supervised%20learning%2C%20which%20allows%20less%20precise%20labels%2C%20has%0Abecome%20a%20promising%20solution%20to%20this%20challenge.%20Building%20on%20this%20approach%2C%20we%0Apropose%20QMaxViT-Unet%2B%2C%20a%20novel%20framework%20for%20scribble-supervised%20medical%20image%0Asegmentation.%20This%20framework%20is%20built%20on%20the%20U-Net%20architecture%2C%20with%20the%0Aencoder%20and%20decoder%20replaced%20by%20Multi-Axis%20Vision%20Transformer%20%28MaxViT%29%20blocks.%0AThese%20blocks%20enhance%20the%20model%27s%20ability%20to%20learn%20local%20and%20global%20features%0Aefficiently.%20Additionally%2C%20our%20approach%20integrates%20a%20query-based%20Transformer%0Adecoder%20to%20refine%20features%20and%20an%20edge%20enhancement%20module%20to%20compensate%20for%20the%0Alimited%20boundary%20information%20in%20the%20scribble%20label.%20We%20evaluate%20the%20proposed%0AQMaxViT-Unet%2B%20on%20four%20public%20datasets%20focused%20on%20cardiac%20structures%2C%20colorectal%0Apolyps%2C%20and%20breast%20cancer%3A%20ACDC%2C%20MS-CMRSeg%2C%20SUN-SEG%2C%20and%20BUSI.%20Evaluation%0Ametrics%20include%20the%20Dice%20similarity%20coefficient%20%28DSC%29%20and%20the%2095th%20percentile%0Aof%20Hausdorff%20distance%20%28HD95%29.%20Experimental%20results%20show%20that%20QMaxViT-Unet%2B%0Aachieves%2089.1%5C%25%20DSC%20and%201.316mm%20HD95%20on%20ACDC%2C%2088.4%5C%25%20DSC%20and%202.226mm%20HD95%20on%0AMS-CMRSeg%2C%2071.4%5C%25%20DSC%20and%204.996mm%20HD95%20on%20SUN-SEG%2C%20and%2069.4%5C%25%20DSC%20and%2050.122mm%0AHD95%20on%20BUSI.%20These%20results%20demonstrate%20that%20our%20method%20outperforms%20existing%0Aapproaches%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20efficiency%20while%20remaining%0Acompetitive%20with%20fully-supervised%20learning%20approaches.%20This%20makes%20it%20ideal%20for%0Amedical%20image%20analysis%2C%20where%20high-quality%20annotations%20are%20often%20scarce%20and%0Arequire%20significant%20effort%20and%20expense.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/anpc849/QMaxViT-Unet%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10294v1&entry.124074799=Read"},
{"title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents", "author": "Hao Kang and Chenyan Xiong", "abstract": "  Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.\n", "link": "http://arxiv.org/abs/2406.10291v2", "date": "2025-02-14", "relevancy": 2.1579, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResearchArena%3A%20Benchmarking%20Large%20Language%20Models%27%20Ability%20to%20Collect%0A%20%20and%20Organize%20Information%20as%20Research%20Agents&body=Title%3A%20ResearchArena%3A%20Benchmarking%20Large%20Language%20Models%27%20Ability%20to%20Collect%0A%20%20and%20Organize%20Information%20as%20Research%20Agents%0AAuthor%3A%20Hao%20Kang%20and%20Chenyan%20Xiong%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20many%20natural%20language%20processing%0Atasks%20but%20face%20challenges%20in%20domain-specific%2C%20analytical%20tasks%20such%20as%0Aconducting%20research%20surveys.%20This%20study%20introduces%20ResearchArena%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20LLMs%27%20capabilities%20in%20conducting%20academic%0Asurveys%24%5Cunicode%7Bx2013%7D%24a%20foundational%20step%20in%20academic%20research.%20ResearchArena%0Amodels%20the%20process%20in%20three%20stages%3A%20%281%29%20information%20discovery%2C%20identifying%0Arelevant%20literature%3B%20%282%29%20information%20selection%2C%20evaluating%20papers%27%20relevance%0Aand%20impact%3B%20and%20%283%29%20information%20organization%2C%20structuring%20knowledge%20into%0Ahierarchical%20frameworks%20such%20as%20mind-maps.%20Notably%2C%20mind-map%20construction%20is%0Atreated%20as%20a%20bonus%20task%2C%20reflecting%20its%20supplementary%20role%20in%20survey-writing.%0ATo%20support%20these%20evaluations%2C%20we%20construct%20an%20offline%20environment%20of%2012M%0Afull-text%20academic%20papers%20and%207.9K%20survey%20papers.%20To%20ensure%20ethical%20compliance%2C%0Awe%20do%20not%20redistribute%20copyrighted%20materials%3B%20instead%2C%20we%20provide%20code%20to%0Aconstruct%20the%20environment%20from%20the%20Semantic%20Scholar%20Open%20Research%20Corpus%0A%28S2ORC%29.%20Preliminary%20evaluations%20reveal%20that%20LLM-based%20approaches%20underperform%0Acompared%20to%20simpler%20keyword-based%20retrieval%20methods%2C%20underscoring%20significant%0Aopportunities%20for%20advancing%20LLMs%20in%20autonomous%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearchArena%253A%2520Benchmarking%2520Large%2520Language%2520Models%2527%2520Ability%2520to%2520Collect%250A%2520%2520and%2520Organize%2520Information%2520as%2520Research%2520Agents%26entry.906535625%3DHao%2520Kang%2520and%2520Chenyan%2520Xiong%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520across%2520many%2520natural%2520language%2520processing%250Atasks%2520but%2520face%2520challenges%2520in%2520domain-specific%252C%2520analytical%2520tasks%2520such%2520as%250Aconducting%2520research%2520surveys.%2520This%2520study%2520introduces%2520ResearchArena%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520LLMs%2527%2520capabilities%2520in%2520conducting%2520academic%250Asurveys%2524%255Cunicode%257Bx2013%257D%2524a%2520foundational%2520step%2520in%2520academic%2520research.%2520ResearchArena%250Amodels%2520the%2520process%2520in%2520three%2520stages%253A%2520%25281%2529%2520information%2520discovery%252C%2520identifying%250Arelevant%2520literature%253B%2520%25282%2529%2520information%2520selection%252C%2520evaluating%2520papers%2527%2520relevance%250Aand%2520impact%253B%2520and%2520%25283%2529%2520information%2520organization%252C%2520structuring%2520knowledge%2520into%250Ahierarchical%2520frameworks%2520such%2520as%2520mind-maps.%2520Notably%252C%2520mind-map%2520construction%2520is%250Atreated%2520as%2520a%2520bonus%2520task%252C%2520reflecting%2520its%2520supplementary%2520role%2520in%2520survey-writing.%250ATo%2520support%2520these%2520evaluations%252C%2520we%2520construct%2520an%2520offline%2520environment%2520of%252012M%250Afull-text%2520academic%2520papers%2520and%25207.9K%2520survey%2520papers.%2520To%2520ensure%2520ethical%2520compliance%252C%250Awe%2520do%2520not%2520redistribute%2520copyrighted%2520materials%253B%2520instead%252C%2520we%2520provide%2520code%2520to%250Aconstruct%2520the%2520environment%2520from%2520the%2520Semantic%2520Scholar%2520Open%2520Research%2520Corpus%250A%2528S2ORC%2529.%2520Preliminary%2520evaluations%2520reveal%2520that%2520LLM-based%2520approaches%2520underperform%250Acompared%2520to%2520simpler%2520keyword-based%2520retrieval%2520methods%252C%2520underscoring%2520significant%250Aopportunities%2520for%2520advancing%2520LLMs%2520in%2520autonomous%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResearchArena%3A%20Benchmarking%20Large%20Language%20Models%27%20Ability%20to%20Collect%0A%20%20and%20Organize%20Information%20as%20Research%20Agents&entry.906535625=Hao%20Kang%20and%20Chenyan%20Xiong&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20many%20natural%20language%20processing%0Atasks%20but%20face%20challenges%20in%20domain-specific%2C%20analytical%20tasks%20such%20as%0Aconducting%20research%20surveys.%20This%20study%20introduces%20ResearchArena%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20LLMs%27%20capabilities%20in%20conducting%20academic%0Asurveys%24%5Cunicode%7Bx2013%7D%24a%20foundational%20step%20in%20academic%20research.%20ResearchArena%0Amodels%20the%20process%20in%20three%20stages%3A%20%281%29%20information%20discovery%2C%20identifying%0Arelevant%20literature%3B%20%282%29%20information%20selection%2C%20evaluating%20papers%27%20relevance%0Aand%20impact%3B%20and%20%283%29%20information%20organization%2C%20structuring%20knowledge%20into%0Ahierarchical%20frameworks%20such%20as%20mind-maps.%20Notably%2C%20mind-map%20construction%20is%0Atreated%20as%20a%20bonus%20task%2C%20reflecting%20its%20supplementary%20role%20in%20survey-writing.%0ATo%20support%20these%20evaluations%2C%20we%20construct%20an%20offline%20environment%20of%2012M%0Afull-text%20academic%20papers%20and%207.9K%20survey%20papers.%20To%20ensure%20ethical%20compliance%2C%0Awe%20do%20not%20redistribute%20copyrighted%20materials%3B%20instead%2C%20we%20provide%20code%20to%0Aconstruct%20the%20environment%20from%20the%20Semantic%20Scholar%20Open%20Research%20Corpus%0A%28S2ORC%29.%20Preliminary%20evaluations%20reveal%20that%20LLM-based%20approaches%20underperform%0Acompared%20to%20simpler%20keyword-based%20retrieval%20methods%2C%20underscoring%20significant%0Aopportunities%20for%20advancing%20LLMs%20in%20autonomous%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10291v2&entry.124074799=Read"},
{"title": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale", "author": "Fan Zhou and Zengzhi Wang and Qian Liu and Junlong Li and Pengfei Liu", "abstract": "  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX\n", "link": "http://arxiv.org/abs/2409.17115v2", "date": "2025-02-14", "relevancy": 2.144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20Like%0A%20%20Experts%20at%20Scale&body=Title%3A%20Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20Like%0A%20%20Experts%20at%20Scale%0AAuthor%3A%20Fan%20Zhou%20and%20Zengzhi%20Wang%20and%20Qian%20Liu%20and%20Junlong%20Li%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Large%20language%20model%20pre-training%20has%20traditionally%20relied%20on%20human%20experts%0Ato%20craft%20heuristics%20for%20improving%20the%20corpora%20quality%2C%20resulting%20in%20numerous%0Arules%20developed%20to%20date.%20However%2C%20these%20rules%20lack%20the%20flexibility%20to%20address%0Athe%20unique%20characteristics%20of%20individual%20example%20effectively.%20Meanwhile%2C%0Aapplying%20tailored%20rules%20to%20every%20example%20is%20impractical%20for%20human%20experts.%20In%0Athis%20paper%2C%20we%20demonstrate%20that%20even%20small%20language%20models%2C%20with%20as%20few%20as%200.3B%0Aparameters%2C%20can%20exhibit%20substantial%20data%20refining%20capabilities%20comparable%20to%0Athose%20of%20human%20experts.%20We%20introduce%20Programming%20Every%20Example%20%28ProX%29%2C%20a%20novel%0Aframework%20that%20treats%20data%20refinement%20as%20a%20programming%20task%2C%20enabling%20models%20to%0Arefine%20corpora%20by%20generating%20and%20executing%20fine-grained%20operations%2C%20such%20as%0Astring%20normalization%2C%20for%20each%20individual%20example%20at%20scale.%20Experimental%0Aresults%20show%20that%20models%20pre-trained%20on%20ProX-curated%20data%20outperform%20either%0Aoriginal%20data%20or%20data%20filtered%20by%20other%20selection%20methods%20by%20more%20than%202%25%0Aacross%20various%20downstream%20benchmarks.%20Its%20effectiveness%20spans%20various%20model%0Asizes%20and%20pre-training%20corpora%2C%20including%20C4%2C%20RedPajama-V2%2C%20FineWeb%2C%0AFineWeb-Edu%2C%20and%20DCLM.%20Furthermore%2C%20ProX%20exhibits%20significant%20potential%20in%0Adomain-specific%20continual%20pre-training%3A%20without%20domain%20specific%20design%2C%20models%0Atrained%20on%20OpenWebMath%20refined%20by%20ProX%20outperform%20human-crafted%20rule-based%0Amethods%2C%20improving%20average%20accuracy%20by%207.6%25%20over%20Mistral-7B%2C%20with%2014.6%25%20for%0ALlama-2-7B%20and%2020.3%25%20for%20CodeLlama-7B%2C%20all%20within%2010B%20tokens%20to%20be%20comparable%0Ato%20models%20like%20Llemma-7B%20trained%20on%20200B%20tokens.%20Further%20analysis%20highlights%0Athat%20ProX%20significantly%20saves%20training%20FLOPs%2C%20offering%20a%20promising%20path%20for%0Aefficient%20LLM%20pre-training.%20We%20are%20open-sourcing%20ProX%20with%20%3E500B%20corpus%2C%0Amodels%2C%20and%20sharing%20all%20training%20and%20implementation%20details%20for%20reproducible%0Aresearch%20and%20future%20innovation.%20Code%3A%20https%3A//github.com/GAIR-NLP/ProX%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgramming%2520Every%2520Example%253A%2520Lifting%2520Pre-training%2520Data%2520Quality%2520Like%250A%2520%2520Experts%2520at%2520Scale%26entry.906535625%3DFan%2520Zhou%2520and%2520Zengzhi%2520Wang%2520and%2520Qian%2520Liu%2520and%2520Junlong%2520Li%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520pre-training%2520has%2520traditionally%2520relied%2520on%2520human%2520experts%250Ato%2520craft%2520heuristics%2520for%2520improving%2520the%2520corpora%2520quality%252C%2520resulting%2520in%2520numerous%250Arules%2520developed%2520to%2520date.%2520However%252C%2520these%2520rules%2520lack%2520the%2520flexibility%2520to%2520address%250Athe%2520unique%2520characteristics%2520of%2520individual%2520example%2520effectively.%2520Meanwhile%252C%250Aapplying%2520tailored%2520rules%2520to%2520every%2520example%2520is%2520impractical%2520for%2520human%2520experts.%2520In%250Athis%2520paper%252C%2520we%2520demonstrate%2520that%2520even%2520small%2520language%2520models%252C%2520with%2520as%2520few%2520as%25200.3B%250Aparameters%252C%2520can%2520exhibit%2520substantial%2520data%2520refining%2520capabilities%2520comparable%2520to%250Athose%2520of%2520human%2520experts.%2520We%2520introduce%2520Programming%2520Every%2520Example%2520%2528ProX%2529%252C%2520a%2520novel%250Aframework%2520that%2520treats%2520data%2520refinement%2520as%2520a%2520programming%2520task%252C%2520enabling%2520models%2520to%250Arefine%2520corpora%2520by%2520generating%2520and%2520executing%2520fine-grained%2520operations%252C%2520such%2520as%250Astring%2520normalization%252C%2520for%2520each%2520individual%2520example%2520at%2520scale.%2520Experimental%250Aresults%2520show%2520that%2520models%2520pre-trained%2520on%2520ProX-curated%2520data%2520outperform%2520either%250Aoriginal%2520data%2520or%2520data%2520filtered%2520by%2520other%2520selection%2520methods%2520by%2520more%2520than%25202%2525%250Aacross%2520various%2520downstream%2520benchmarks.%2520Its%2520effectiveness%2520spans%2520various%2520model%250Asizes%2520and%2520pre-training%2520corpora%252C%2520including%2520C4%252C%2520RedPajama-V2%252C%2520FineWeb%252C%250AFineWeb-Edu%252C%2520and%2520DCLM.%2520Furthermore%252C%2520ProX%2520exhibits%2520significant%2520potential%2520in%250Adomain-specific%2520continual%2520pre-training%253A%2520without%2520domain%2520specific%2520design%252C%2520models%250Atrained%2520on%2520OpenWebMath%2520refined%2520by%2520ProX%2520outperform%2520human-crafted%2520rule-based%250Amethods%252C%2520improving%2520average%2520accuracy%2520by%25207.6%2525%2520over%2520Mistral-7B%252C%2520with%252014.6%2525%2520for%250ALlama-2-7B%2520and%252020.3%2525%2520for%2520CodeLlama-7B%252C%2520all%2520within%252010B%2520tokens%2520to%2520be%2520comparable%250Ato%2520models%2520like%2520Llemma-7B%2520trained%2520on%2520200B%2520tokens.%2520Further%2520analysis%2520highlights%250Athat%2520ProX%2520significantly%2520saves%2520training%2520FLOPs%252C%2520offering%2520a%2520promising%2520path%2520for%250Aefficient%2520LLM%2520pre-training.%2520We%2520are%2520open-sourcing%2520ProX%2520with%2520%253E500B%2520corpus%252C%250Amodels%252C%2520and%2520sharing%2520all%2520training%2520and%2520implementation%2520details%2520for%2520reproducible%250Aresearch%2520and%2520future%2520innovation.%2520Code%253A%2520https%253A//github.com/GAIR-NLP/ProX%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20Like%0A%20%20Experts%20at%20Scale&entry.906535625=Fan%20Zhou%20and%20Zengzhi%20Wang%20and%20Qian%20Liu%20and%20Junlong%20Li%20and%20Pengfei%20Liu&entry.1292438233=%20%20Large%20language%20model%20pre-training%20has%20traditionally%20relied%20on%20human%20experts%0Ato%20craft%20heuristics%20for%20improving%20the%20corpora%20quality%2C%20resulting%20in%20numerous%0Arules%20developed%20to%20date.%20However%2C%20these%20rules%20lack%20the%20flexibility%20to%20address%0Athe%20unique%20characteristics%20of%20individual%20example%20effectively.%20Meanwhile%2C%0Aapplying%20tailored%20rules%20to%20every%20example%20is%20impractical%20for%20human%20experts.%20In%0Athis%20paper%2C%20we%20demonstrate%20that%20even%20small%20language%20models%2C%20with%20as%20few%20as%200.3B%0Aparameters%2C%20can%20exhibit%20substantial%20data%20refining%20capabilities%20comparable%20to%0Athose%20of%20human%20experts.%20We%20introduce%20Programming%20Every%20Example%20%28ProX%29%2C%20a%20novel%0Aframework%20that%20treats%20data%20refinement%20as%20a%20programming%20task%2C%20enabling%20models%20to%0Arefine%20corpora%20by%20generating%20and%20executing%20fine-grained%20operations%2C%20such%20as%0Astring%20normalization%2C%20for%20each%20individual%20example%20at%20scale.%20Experimental%0Aresults%20show%20that%20models%20pre-trained%20on%20ProX-curated%20data%20outperform%20either%0Aoriginal%20data%20or%20data%20filtered%20by%20other%20selection%20methods%20by%20more%20than%202%25%0Aacross%20various%20downstream%20benchmarks.%20Its%20effectiveness%20spans%20various%20model%0Asizes%20and%20pre-training%20corpora%2C%20including%20C4%2C%20RedPajama-V2%2C%20FineWeb%2C%0AFineWeb-Edu%2C%20and%20DCLM.%20Furthermore%2C%20ProX%20exhibits%20significant%20potential%20in%0Adomain-specific%20continual%20pre-training%3A%20without%20domain%20specific%20design%2C%20models%0Atrained%20on%20OpenWebMath%20refined%20by%20ProX%20outperform%20human-crafted%20rule-based%0Amethods%2C%20improving%20average%20accuracy%20by%207.6%25%20over%20Mistral-7B%2C%20with%2014.6%25%20for%0ALlama-2-7B%20and%2020.3%25%20for%20CodeLlama-7B%2C%20all%20within%2010B%20tokens%20to%20be%20comparable%0Ato%20models%20like%20Llemma-7B%20trained%20on%20200B%20tokens.%20Further%20analysis%20highlights%0Athat%20ProX%20significantly%20saves%20training%20FLOPs%2C%20offering%20a%20promising%20path%20for%0Aefficient%20LLM%20pre-training.%20We%20are%20open-sourcing%20ProX%20with%20%3E500B%20corpus%2C%0Amodels%2C%20and%20sharing%20all%20training%20and%20implementation%20details%20for%20reproducible%0Aresearch%20and%20future%20innovation.%20Code%3A%20https%3A//github.com/GAIR-NLP/ProX%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17115v2&entry.124074799=Read"},
{"title": "Revisiting Generalization Power of a DNN in Terms of Symbolic\n  Interactions", "author": "Lei Cheng and Junpeng Zhang and Qihan Ren and Quanshi Zhang", "abstract": "  This paper aims to analyze the generalization power of deep neural networks\n(DNNs) from the perspective of interactions. Unlike previous analysis of a\nDNN's generalization power in a highdimensional feature space, we find that the\ngeneralization power of a DNN can be explained as the generalization power of\nthe interactions. We found that the generalizable interactions follow a\ndecay-shaped distribution, while non-generalizable interactions follow a\nspindle-shaped distribution. Furthermore, our theory can effectively\ndisentangle these two types of interactions from a DNN. We have verified that\nour theory can well match real interactions in a DNN in experiments.\n", "link": "http://arxiv.org/abs/2502.10162v1", "date": "2025-02-14", "relevancy": 2.1412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4349}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4266}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Generalization%20Power%20of%20a%20DNN%20in%20Terms%20of%20Symbolic%0A%20%20Interactions&body=Title%3A%20Revisiting%20Generalization%20Power%20of%20a%20DNN%20in%20Terms%20of%20Symbolic%0A%20%20Interactions%0AAuthor%3A%20Lei%20Cheng%20and%20Junpeng%20Zhang%20and%20Qihan%20Ren%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20analyze%20the%20generalization%20power%20of%20deep%20neural%20networks%0A%28DNNs%29%20from%20the%20perspective%20of%20interactions.%20Unlike%20previous%20analysis%20of%20a%0ADNN%27s%20generalization%20power%20in%20a%20highdimensional%20feature%20space%2C%20we%20find%20that%20the%0Ageneralization%20power%20of%20a%20DNN%20can%20be%20explained%20as%20the%20generalization%20power%20of%0Athe%20interactions.%20We%20found%20that%20the%20generalizable%20interactions%20follow%20a%0Adecay-shaped%20distribution%2C%20while%20non-generalizable%20interactions%20follow%20a%0Aspindle-shaped%20distribution.%20Furthermore%2C%20our%20theory%20can%20effectively%0Adisentangle%20these%20two%20types%20of%20interactions%20from%20a%20DNN.%20We%20have%20verified%20that%0Aour%20theory%20can%20well%20match%20real%20interactions%20in%20a%20DNN%20in%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Generalization%2520Power%2520of%2520a%2520DNN%2520in%2520Terms%2520of%2520Symbolic%250A%2520%2520Interactions%26entry.906535625%3DLei%2520Cheng%2520and%2520Junpeng%2520Zhang%2520and%2520Qihan%2520Ren%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520analyze%2520the%2520generalization%2520power%2520of%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520from%2520the%2520perspective%2520of%2520interactions.%2520Unlike%2520previous%2520analysis%2520of%2520a%250ADNN%2527s%2520generalization%2520power%2520in%2520a%2520highdimensional%2520feature%2520space%252C%2520we%2520find%2520that%2520the%250Ageneralization%2520power%2520of%2520a%2520DNN%2520can%2520be%2520explained%2520as%2520the%2520generalization%2520power%2520of%250Athe%2520interactions.%2520We%2520found%2520that%2520the%2520generalizable%2520interactions%2520follow%2520a%250Adecay-shaped%2520distribution%252C%2520while%2520non-generalizable%2520interactions%2520follow%2520a%250Aspindle-shaped%2520distribution.%2520Furthermore%252C%2520our%2520theory%2520can%2520effectively%250Adisentangle%2520these%2520two%2520types%2520of%2520interactions%2520from%2520a%2520DNN.%2520We%2520have%2520verified%2520that%250Aour%2520theory%2520can%2520well%2520match%2520real%2520interactions%2520in%2520a%2520DNN%2520in%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Generalization%20Power%20of%20a%20DNN%20in%20Terms%20of%20Symbolic%0A%20%20Interactions&entry.906535625=Lei%20Cheng%20and%20Junpeng%20Zhang%20and%20Qihan%20Ren%20and%20Quanshi%20Zhang&entry.1292438233=%20%20This%20paper%20aims%20to%20analyze%20the%20generalization%20power%20of%20deep%20neural%20networks%0A%28DNNs%29%20from%20the%20perspective%20of%20interactions.%20Unlike%20previous%20analysis%20of%20a%0ADNN%27s%20generalization%20power%20in%20a%20highdimensional%20feature%20space%2C%20we%20find%20that%20the%0Ageneralization%20power%20of%20a%20DNN%20can%20be%20explained%20as%20the%20generalization%20power%20of%0Athe%20interactions.%20We%20found%20that%20the%20generalizable%20interactions%20follow%20a%0Adecay-shaped%20distribution%2C%20while%20non-generalizable%20interactions%20follow%20a%0Aspindle-shaped%20distribution.%20Furthermore%2C%20our%20theory%20can%20effectively%0Adisentangle%20these%20two%20types%20of%20interactions%20from%20a%20DNN.%20We%20have%20verified%20that%0Aour%20theory%20can%20well%20match%20real%20interactions%20in%20a%20DNN%20in%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10162v1&entry.124074799=Read"},
{"title": "Object Detection and Tracking", "author": "Md Pranto and Omar Faruk", "abstract": "  Efficient and accurate object detection is an important topic in the\ndevelopment of computer vision systems. With the advent of deep learning\ntechniques, the accuracy of object detection has increased significantly. The\nproject aims to integrate a modern technique for object detection with the aim\nof achieving high accuracy with real-time performance. The reliance on other\ncomputer vision algorithms in many object identification systems, which results\nin poor and ineffective performance, is a significant obstacle. In this\nresearch, we solve the end-to-end object detection problem entirely using deep\nlearning techniques. The network is trained using the most difficult publicly\navailable dataset, which is used for an annual item detection challenge.\nApplications that need object detection can benefit the system's quick and\nprecise finding.\n", "link": "http://arxiv.org/abs/2502.10310v1", "date": "2025-02-14", "relevancy": 2.1362, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.558}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5184}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Detection%20and%20Tracking&body=Title%3A%20Object%20Detection%20and%20Tracking%0AAuthor%3A%20Md%20Pranto%20and%20Omar%20Faruk%0AAbstract%3A%20%20%20Efficient%20and%20accurate%20object%20detection%20is%20an%20important%20topic%20in%20the%0Adevelopment%20of%20computer%20vision%20systems.%20With%20the%20advent%20of%20deep%20learning%0Atechniques%2C%20the%20accuracy%20of%20object%20detection%20has%20increased%20significantly.%20The%0Aproject%20aims%20to%20integrate%20a%20modern%20technique%20for%20object%20detection%20with%20the%20aim%0Aof%20achieving%20high%20accuracy%20with%20real-time%20performance.%20The%20reliance%20on%20other%0Acomputer%20vision%20algorithms%20in%20many%20object%20identification%20systems%2C%20which%20results%0Ain%20poor%20and%20ineffective%20performance%2C%20is%20a%20significant%20obstacle.%20In%20this%0Aresearch%2C%20we%20solve%20the%20end-to-end%20object%20detection%20problem%20entirely%20using%20deep%0Alearning%20techniques.%20The%20network%20is%20trained%20using%20the%20most%20difficult%20publicly%0Aavailable%20dataset%2C%20which%20is%20used%20for%20an%20annual%20item%20detection%20challenge.%0AApplications%20that%20need%20object%20detection%20can%20benefit%20the%20system%27s%20quick%20and%0Aprecise%20finding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Detection%2520and%2520Tracking%26entry.906535625%3DMd%2520Pranto%2520and%2520Omar%2520Faruk%26entry.1292438233%3D%2520%2520Efficient%2520and%2520accurate%2520object%2520detection%2520is%2520an%2520important%2520topic%2520in%2520the%250Adevelopment%2520of%2520computer%2520vision%2520systems.%2520With%2520the%2520advent%2520of%2520deep%2520learning%250Atechniques%252C%2520the%2520accuracy%2520of%2520object%2520detection%2520has%2520increased%2520significantly.%2520The%250Aproject%2520aims%2520to%2520integrate%2520a%2520modern%2520technique%2520for%2520object%2520detection%2520with%2520the%2520aim%250Aof%2520achieving%2520high%2520accuracy%2520with%2520real-time%2520performance.%2520The%2520reliance%2520on%2520other%250Acomputer%2520vision%2520algorithms%2520in%2520many%2520object%2520identification%2520systems%252C%2520which%2520results%250Ain%2520poor%2520and%2520ineffective%2520performance%252C%2520is%2520a%2520significant%2520obstacle.%2520In%2520this%250Aresearch%252C%2520we%2520solve%2520the%2520end-to-end%2520object%2520detection%2520problem%2520entirely%2520using%2520deep%250Alearning%2520techniques.%2520The%2520network%2520is%2520trained%2520using%2520the%2520most%2520difficult%2520publicly%250Aavailable%2520dataset%252C%2520which%2520is%2520used%2520for%2520an%2520annual%2520item%2520detection%2520challenge.%250AApplications%2520that%2520need%2520object%2520detection%2520can%2520benefit%2520the%2520system%2527s%2520quick%2520and%250Aprecise%2520finding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Detection%20and%20Tracking&entry.906535625=Md%20Pranto%20and%20Omar%20Faruk&entry.1292438233=%20%20Efficient%20and%20accurate%20object%20detection%20is%20an%20important%20topic%20in%20the%0Adevelopment%20of%20computer%20vision%20systems.%20With%20the%20advent%20of%20deep%20learning%0Atechniques%2C%20the%20accuracy%20of%20object%20detection%20has%20increased%20significantly.%20The%0Aproject%20aims%20to%20integrate%20a%20modern%20technique%20for%20object%20detection%20with%20the%20aim%0Aof%20achieving%20high%20accuracy%20with%20real-time%20performance.%20The%20reliance%20on%20other%0Acomputer%20vision%20algorithms%20in%20many%20object%20identification%20systems%2C%20which%20results%0Ain%20poor%20and%20ineffective%20performance%2C%20is%20a%20significant%20obstacle.%20In%20this%0Aresearch%2C%20we%20solve%20the%20end-to-end%20object%20detection%20problem%20entirely%20using%20deep%0Alearning%20techniques.%20The%20network%20is%20trained%20using%20the%20most%20difficult%20publicly%0Aavailable%20dataset%2C%20which%20is%20used%20for%20an%20annual%20item%20detection%20challenge.%0AApplications%20that%20need%20object%20detection%20can%20benefit%20the%20system%27s%20quick%20and%0Aprecise%20finding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10310v1&entry.124074799=Read"},
{"title": "Balancing the Scales: A Theoretical and Algorithmic Framework for\n  Learning from Imbalanced Data", "author": "Corinna Cortes and Anqi Mao and Mehryar Mohri and Yutao Zhong", "abstract": "  Class imbalance remains a major challenge in machine learning, especially in\nmulti-class problems with long-tailed distributions. Existing methods, such as\ndata resampling, cost-sensitive techniques, and logistic loss modifications,\nthough popular and often effective, lack solid theoretical foundations. As an\nexample, we demonstrate that cost-sensitive methods are not Bayes consistent.\nThis paper introduces a novel theoretical framework for analyzing\ngeneralization in imbalanced classification. We propose a new class-imbalanced\nmargin loss function for both binary and multi-class settings, prove its strong\n$H$-consistency, and derive corresponding learning guarantees based on\nempirical loss and a new notion of class-sensitive Rademacher complexity.\nLeveraging these theoretical results, we devise novel and general learning\nalgorithms, IMMAX (Imbalanced Margin Maximization), which incorporate\nconfidence margins and are applicable to various hypothesis sets. While our\nfocus is theoretical, we also present extensive empirical results demonstrating\nthe effectiveness of our algorithms compared to existing baselines.\n", "link": "http://arxiv.org/abs/2502.10381v1", "date": "2025-02-14", "relevancy": 2.134, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20the%20Scales%3A%20A%20Theoretical%20and%20Algorithmic%20Framework%20for%0A%20%20Learning%20from%20Imbalanced%20Data&body=Title%3A%20Balancing%20the%20Scales%3A%20A%20Theoretical%20and%20Algorithmic%20Framework%20for%0A%20%20Learning%20from%20Imbalanced%20Data%0AAuthor%3A%20Corinna%20Cortes%20and%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20Class%20imbalance%20remains%20a%20major%20challenge%20in%20machine%20learning%2C%20especially%20in%0Amulti-class%20problems%20with%20long-tailed%20distributions.%20Existing%20methods%2C%20such%20as%0Adata%20resampling%2C%20cost-sensitive%20techniques%2C%20and%20logistic%20loss%20modifications%2C%0Athough%20popular%20and%20often%20effective%2C%20lack%20solid%20theoretical%20foundations.%20As%20an%0Aexample%2C%20we%20demonstrate%20that%20cost-sensitive%20methods%20are%20not%20Bayes%20consistent.%0AThis%20paper%20introduces%20a%20novel%20theoretical%20framework%20for%20analyzing%0Ageneralization%20in%20imbalanced%20classification.%20We%20propose%20a%20new%20class-imbalanced%0Amargin%20loss%20function%20for%20both%20binary%20and%20multi-class%20settings%2C%20prove%20its%20strong%0A%24H%24-consistency%2C%20and%20derive%20corresponding%20learning%20guarantees%20based%20on%0Aempirical%20loss%20and%20a%20new%20notion%20of%20class-sensitive%20Rademacher%20complexity.%0ALeveraging%20these%20theoretical%20results%2C%20we%20devise%20novel%20and%20general%20learning%0Aalgorithms%2C%20IMMAX%20%28Imbalanced%20Margin%20Maximization%29%2C%20which%20incorporate%0Aconfidence%20margins%20and%20are%20applicable%20to%20various%20hypothesis%20sets.%20While%20our%0Afocus%20is%20theoretical%2C%20we%20also%20present%20extensive%20empirical%20results%20demonstrating%0Athe%20effectiveness%20of%20our%20algorithms%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520the%2520Scales%253A%2520A%2520Theoretical%2520and%2520Algorithmic%2520Framework%2520for%250A%2520%2520Learning%2520from%2520Imbalanced%2520Data%26entry.906535625%3DCorinna%2520Cortes%2520and%2520Anqi%2520Mao%2520and%2520Mehryar%2520Mohri%2520and%2520Yutao%2520Zhong%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520remains%2520a%2520major%2520challenge%2520in%2520machine%2520learning%252C%2520especially%2520in%250Amulti-class%2520problems%2520with%2520long-tailed%2520distributions.%2520Existing%2520methods%252C%2520such%2520as%250Adata%2520resampling%252C%2520cost-sensitive%2520techniques%252C%2520and%2520logistic%2520loss%2520modifications%252C%250Athough%2520popular%2520and%2520often%2520effective%252C%2520lack%2520solid%2520theoretical%2520foundations.%2520As%2520an%250Aexample%252C%2520we%2520demonstrate%2520that%2520cost-sensitive%2520methods%2520are%2520not%2520Bayes%2520consistent.%250AThis%2520paper%2520introduces%2520a%2520novel%2520theoretical%2520framework%2520for%2520analyzing%250Ageneralization%2520in%2520imbalanced%2520classification.%2520We%2520propose%2520a%2520new%2520class-imbalanced%250Amargin%2520loss%2520function%2520for%2520both%2520binary%2520and%2520multi-class%2520settings%252C%2520prove%2520its%2520strong%250A%2524H%2524-consistency%252C%2520and%2520derive%2520corresponding%2520learning%2520guarantees%2520based%2520on%250Aempirical%2520loss%2520and%2520a%2520new%2520notion%2520of%2520class-sensitive%2520Rademacher%2520complexity.%250ALeveraging%2520these%2520theoretical%2520results%252C%2520we%2520devise%2520novel%2520and%2520general%2520learning%250Aalgorithms%252C%2520IMMAX%2520%2528Imbalanced%2520Margin%2520Maximization%2529%252C%2520which%2520incorporate%250Aconfidence%2520margins%2520and%2520are%2520applicable%2520to%2520various%2520hypothesis%2520sets.%2520While%2520our%250Afocus%2520is%2520theoretical%252C%2520we%2520also%2520present%2520extensive%2520empirical%2520results%2520demonstrating%250Athe%2520effectiveness%2520of%2520our%2520algorithms%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20the%20Scales%3A%20A%20Theoretical%20and%20Algorithmic%20Framework%20for%0A%20%20Learning%20from%20Imbalanced%20Data&entry.906535625=Corinna%20Cortes%20and%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20Class%20imbalance%20remains%20a%20major%20challenge%20in%20machine%20learning%2C%20especially%20in%0Amulti-class%20problems%20with%20long-tailed%20distributions.%20Existing%20methods%2C%20such%20as%0Adata%20resampling%2C%20cost-sensitive%20techniques%2C%20and%20logistic%20loss%20modifications%2C%0Athough%20popular%20and%20often%20effective%2C%20lack%20solid%20theoretical%20foundations.%20As%20an%0Aexample%2C%20we%20demonstrate%20that%20cost-sensitive%20methods%20are%20not%20Bayes%20consistent.%0AThis%20paper%20introduces%20a%20novel%20theoretical%20framework%20for%20analyzing%0Ageneralization%20in%20imbalanced%20classification.%20We%20propose%20a%20new%20class-imbalanced%0Amargin%20loss%20function%20for%20both%20binary%20and%20multi-class%20settings%2C%20prove%20its%20strong%0A%24H%24-consistency%2C%20and%20derive%20corresponding%20learning%20guarantees%20based%20on%0Aempirical%20loss%20and%20a%20new%20notion%20of%20class-sensitive%20Rademacher%20complexity.%0ALeveraging%20these%20theoretical%20results%2C%20we%20devise%20novel%20and%20general%20learning%0Aalgorithms%2C%20IMMAX%20%28Imbalanced%20Margin%20Maximization%29%2C%20which%20incorporate%0Aconfidence%20margins%20and%20are%20applicable%20to%20various%20hypothesis%20sets.%20While%20our%0Afocus%20is%20theoretical%2C%20we%20also%20present%20extensive%20empirical%20results%20demonstrating%0Athe%20effectiveness%20of%20our%20algorithms%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10381v1&entry.124074799=Read"},
{"title": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models", "author": "Tim Z. Xiao and Robert Bamler and Bernhard Sch\u00f6lkopf and Weiyang Liu", "abstract": "  Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.\n", "link": "http://arxiv.org/abs/2406.04344v3", "date": "2025-02-14", "relevancy": 2.133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbalized%20Machine%20Learning%3A%20Revisiting%20Machine%20Learning%20with%20Language%0A%20%20Models&body=Title%3A%20Verbalized%20Machine%20Learning%3A%20Revisiting%20Machine%20Learning%20with%20Language%0A%20%20Models%0AAuthor%3A%20Tim%20Z.%20Xiao%20and%20Robert%20Bamler%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Weiyang%20Liu%0AAbstract%3A%20%20%20Motivated%20by%20the%20progress%20made%20by%20large%20language%20models%20%28LLMs%29%2C%20we%20introduce%0Athe%20framework%20of%20verbalized%20machine%20learning%20%28VML%29.%20In%20contrast%20to%20conventional%0Amachine%20learning%20%28ML%29%20models%20that%20are%20typically%20optimized%20over%20a%20continuous%0Aparameter%20space%2C%20VML%20constrains%20the%20parameter%20space%20to%20be%20human-interpretable%0Anatural%20language.%20Such%20a%20constraint%20leads%20to%20a%20new%20perspective%20of%20function%0Aapproximation%2C%20where%20an%20LLM%20with%20a%20text%20prompt%20can%20be%20viewed%20as%20a%20function%0Aparameterized%20by%20the%20text%20prompt.%20Guided%20by%20this%20perspective%2C%20we%20revisit%0Aclassical%20ML%20problems%2C%20such%20as%20regression%20and%20classification%2C%20and%20find%20that%0Athese%20problems%20can%20be%20solved%20by%20an%20LLM-parameterized%20learner%20and%20optimizer.%20The%0Amajor%20advantages%20of%20VML%20include%20%281%29%20easy%20encoding%20of%20inductive%20bias%3A%20prior%0Aknowledge%20about%20the%20problem%20and%20hypothesis%20class%20can%20be%20encoded%20in%20natural%0Alanguage%20and%20fed%20into%20the%20LLM-parameterized%20learner%3B%20%282%29%20automatic%20model%20class%0Aselection%3A%20the%20optimizer%20can%20automatically%20select%20a%20model%20class%20based%20on%20data%0Aand%20verbalized%20prior%20knowledge%2C%20and%20it%20can%20update%20the%20model%20class%20during%0Atraining%3B%20and%20%283%29%20interpretable%20learner%20updates%3A%20the%20LLM-parameterized%0Aoptimizer%20can%20provide%20explanations%20for%20why%20an%20update%20is%20performed.%20We%0Aempirically%20verify%20the%20effectiveness%20of%20VML%2C%20and%20hope%20that%20VML%20can%20serve%20as%20a%0Astepping%20stone%20to%20stronger%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbalized%2520Machine%2520Learning%253A%2520Revisiting%2520Machine%2520Learning%2520with%2520Language%250A%2520%2520Models%26entry.906535625%3DTim%2520Z.%2520Xiao%2520and%2520Robert%2520Bamler%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%2520and%2520Weiyang%2520Liu%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520progress%2520made%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520we%2520introduce%250Athe%2520framework%2520of%2520verbalized%2520machine%2520learning%2520%2528VML%2529.%2520In%2520contrast%2520to%2520conventional%250Amachine%2520learning%2520%2528ML%2529%2520models%2520that%2520are%2520typically%2520optimized%2520over%2520a%2520continuous%250Aparameter%2520space%252C%2520VML%2520constrains%2520the%2520parameter%2520space%2520to%2520be%2520human-interpretable%250Anatural%2520language.%2520Such%2520a%2520constraint%2520leads%2520to%2520a%2520new%2520perspective%2520of%2520function%250Aapproximation%252C%2520where%2520an%2520LLM%2520with%2520a%2520text%2520prompt%2520can%2520be%2520viewed%2520as%2520a%2520function%250Aparameterized%2520by%2520the%2520text%2520prompt.%2520Guided%2520by%2520this%2520perspective%252C%2520we%2520revisit%250Aclassical%2520ML%2520problems%252C%2520such%2520as%2520regression%2520and%2520classification%252C%2520and%2520find%2520that%250Athese%2520problems%2520can%2520be%2520solved%2520by%2520an%2520LLM-parameterized%2520learner%2520and%2520optimizer.%2520The%250Amajor%2520advantages%2520of%2520VML%2520include%2520%25281%2529%2520easy%2520encoding%2520of%2520inductive%2520bias%253A%2520prior%250Aknowledge%2520about%2520the%2520problem%2520and%2520hypothesis%2520class%2520can%2520be%2520encoded%2520in%2520natural%250Alanguage%2520and%2520fed%2520into%2520the%2520LLM-parameterized%2520learner%253B%2520%25282%2529%2520automatic%2520model%2520class%250Aselection%253A%2520the%2520optimizer%2520can%2520automatically%2520select%2520a%2520model%2520class%2520based%2520on%2520data%250Aand%2520verbalized%2520prior%2520knowledge%252C%2520and%2520it%2520can%2520update%2520the%2520model%2520class%2520during%250Atraining%253B%2520and%2520%25283%2529%2520interpretable%2520learner%2520updates%253A%2520the%2520LLM-parameterized%250Aoptimizer%2520can%2520provide%2520explanations%2520for%2520why%2520an%2520update%2520is%2520performed.%2520We%250Aempirically%2520verify%2520the%2520effectiveness%2520of%2520VML%252C%2520and%2520hope%2520that%2520VML%2520can%2520serve%2520as%2520a%250Astepping%2520stone%2520to%2520stronger%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbalized%20Machine%20Learning%3A%20Revisiting%20Machine%20Learning%20with%20Language%0A%20%20Models&entry.906535625=Tim%20Z.%20Xiao%20and%20Robert%20Bamler%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Weiyang%20Liu&entry.1292438233=%20%20Motivated%20by%20the%20progress%20made%20by%20large%20language%20models%20%28LLMs%29%2C%20we%20introduce%0Athe%20framework%20of%20verbalized%20machine%20learning%20%28VML%29.%20In%20contrast%20to%20conventional%0Amachine%20learning%20%28ML%29%20models%20that%20are%20typically%20optimized%20over%20a%20continuous%0Aparameter%20space%2C%20VML%20constrains%20the%20parameter%20space%20to%20be%20human-interpretable%0Anatural%20language.%20Such%20a%20constraint%20leads%20to%20a%20new%20perspective%20of%20function%0Aapproximation%2C%20where%20an%20LLM%20with%20a%20text%20prompt%20can%20be%20viewed%20as%20a%20function%0Aparameterized%20by%20the%20text%20prompt.%20Guided%20by%20this%20perspective%2C%20we%20revisit%0Aclassical%20ML%20problems%2C%20such%20as%20regression%20and%20classification%2C%20and%20find%20that%0Athese%20problems%20can%20be%20solved%20by%20an%20LLM-parameterized%20learner%20and%20optimizer.%20The%0Amajor%20advantages%20of%20VML%20include%20%281%29%20easy%20encoding%20of%20inductive%20bias%3A%20prior%0Aknowledge%20about%20the%20problem%20and%20hypothesis%20class%20can%20be%20encoded%20in%20natural%0Alanguage%20and%20fed%20into%20the%20LLM-parameterized%20learner%3B%20%282%29%20automatic%20model%20class%0Aselection%3A%20the%20optimizer%20can%20automatically%20select%20a%20model%20class%20based%20on%20data%0Aand%20verbalized%20prior%20knowledge%2C%20and%20it%20can%20update%20the%20model%20class%20during%0Atraining%3B%20and%20%283%29%20interpretable%20learner%20updates%3A%20the%20LLM-parameterized%0Aoptimizer%20can%20provide%20explanations%20for%20why%20an%20update%20is%20performed.%20We%0Aempirically%20verify%20the%20effectiveness%20of%20VML%2C%20and%20hope%20that%20VML%20can%20serve%20as%20a%0Astepping%20stone%20to%20stronger%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04344v3&entry.124074799=Read"},
{"title": "Solving the enigma: Enhancing faithfulness and comprehensibility in\n  explanations of deep networks", "author": "Michail Mamalakis and Antonios Mamalakis and Ingrid Agartz and Lynn Egeland M\u00f8rch-Johnsen and Graham Murray and John Suckling and Pietro Lio", "abstract": "  The accelerated progress of artificial intelligence (AI) has popularized deep\nlearning models across various domains, yet their inherent opacity poses\nchallenges, particularly in critical fields like healthcare, medicine, and the\ngeosciences. Explainable AI (XAI) has emerged to shed light on these 'black\nbox' models, aiding in deciphering their decision-making processes. However,\ndifferent XAI methods often produce significantly different explanations,\nleading to high inter-method variability that increases uncertainty and\nundermines trust in deep networks' predictions. In this study, we address this\nchallenge by introducing a novel framework designed to enhance the\nexplainability of deep networks through a dual focus on maximizing both\naccuracy and comprehensibility in the explanations. Our framework integrates\noutputs from multiple established XAI methods and leverages a non-linear neural\nnetwork model, termed the 'explanation optimizer,' to construct a unified,\noptimal explanation. The optimizer evaluates explanations using two key\nmetrics: faithfulness (accuracy in reflecting the network's decisions) and\ncomplexity (comprehensibility). By balancing these, it provides accurate and\naccessible explanations, addressing a key XAI limitation. Experiments on\nmulti-class and binary classification in 2D object and 3D neuroscience imaging\nconfirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63%\nhigher than the best XAI methods in 3D and 2D tasks, respectively, while also\nreducing complexity for better understanding. These results demonstrate that\noptimal explanations based on specific quality criteria are achievable,\noffering a solution to the issue of inter-method variability in the current XAI\nliterature and supporting more trustworthy deep network predictions\n", "link": "http://arxiv.org/abs/2405.10008v2", "date": "2025-02-14", "relevancy": 2.1245, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5366}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.53}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20enigma%3A%20Enhancing%20faithfulness%20and%20comprehensibility%20in%0A%20%20explanations%20of%20deep%20networks&body=Title%3A%20Solving%20the%20enigma%3A%20Enhancing%20faithfulness%20and%20comprehensibility%20in%0A%20%20explanations%20of%20deep%20networks%0AAuthor%3A%20Michail%20Mamalakis%20and%20Antonios%20Mamalakis%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Graham%20Murray%20and%20John%20Suckling%20and%20Pietro%20Lio%0AAbstract%3A%20%20%20The%20accelerated%20progress%20of%20artificial%20intelligence%20%28AI%29%20has%20popularized%20deep%0Alearning%20models%20across%20various%20domains%2C%20yet%20their%20inherent%20opacity%20poses%0Achallenges%2C%20particularly%20in%20critical%20fields%20like%20healthcare%2C%20medicine%2C%20and%20the%0Ageosciences.%20Explainable%20AI%20%28XAI%29%20has%20emerged%20to%20shed%20light%20on%20these%20%27black%0Abox%27%20models%2C%20aiding%20in%20deciphering%20their%20decision-making%20processes.%20However%2C%0Adifferent%20XAI%20methods%20often%20produce%20significantly%20different%20explanations%2C%0Aleading%20to%20high%20inter-method%20variability%20that%20increases%20uncertainty%20and%0Aundermines%20trust%20in%20deep%20networks%27%20predictions.%20In%20this%20study%2C%20we%20address%20this%0Achallenge%20by%20introducing%20a%20novel%20framework%20designed%20to%20enhance%20the%0Aexplainability%20of%20deep%20networks%20through%20a%20dual%20focus%20on%20maximizing%20both%0Aaccuracy%20and%20comprehensibility%20in%20the%20explanations.%20Our%20framework%20integrates%0Aoutputs%20from%20multiple%20established%20XAI%20methods%20and%20leverages%20a%20non-linear%20neural%0Anetwork%20model%2C%20termed%20the%20%27explanation%20optimizer%2C%27%20to%20construct%20a%20unified%2C%0Aoptimal%20explanation.%20The%20optimizer%20evaluates%20explanations%20using%20two%20key%0Ametrics%3A%20faithfulness%20%28accuracy%20in%20reflecting%20the%20network%27s%20decisions%29%20and%0Acomplexity%20%28comprehensibility%29.%20By%20balancing%20these%2C%20it%20provides%20accurate%20and%0Aaccessible%20explanations%2C%20addressing%20a%20key%20XAI%20limitation.%20Experiments%20on%0Amulti-class%20and%20binary%20classification%20in%202D%20object%20and%203D%20neuroscience%20imaging%0Aconfirm%20its%20efficacy.%20Our%20optimizer%20achieved%20faithfulness%20scores%20155%25%20and%2063%25%0Ahigher%20than%20the%20best%20XAI%20methods%20in%203D%20and%202D%20tasks%2C%20respectively%2C%20while%20also%0Areducing%20complexity%20for%20better%20understanding.%20These%20results%20demonstrate%20that%0Aoptimal%20explanations%20based%20on%20specific%20quality%20criteria%20are%20achievable%2C%0Aoffering%20a%20solution%20to%20the%20issue%20of%20inter-method%20variability%20in%20the%20current%20XAI%0Aliterature%20and%20supporting%20more%20trustworthy%20deep%20network%20predictions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520enigma%253A%2520Enhancing%2520faithfulness%2520and%2520comprehensibility%2520in%250A%2520%2520explanations%2520of%2520deep%2520networks%26entry.906535625%3DMichail%2520Mamalakis%2520and%2520Antonios%2520Mamalakis%2520and%2520Ingrid%2520Agartz%2520and%2520Lynn%2520Egeland%2520M%25C3%25B8rch-Johnsen%2520and%2520Graham%2520Murray%2520and%2520John%2520Suckling%2520and%2520Pietro%2520Lio%26entry.1292438233%3D%2520%2520The%2520accelerated%2520progress%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520popularized%2520deep%250Alearning%2520models%2520across%2520various%2520domains%252C%2520yet%2520their%2520inherent%2520opacity%2520poses%250Achallenges%252C%2520particularly%2520in%2520critical%2520fields%2520like%2520healthcare%252C%2520medicine%252C%2520and%2520the%250Ageosciences.%2520Explainable%2520AI%2520%2528XAI%2529%2520has%2520emerged%2520to%2520shed%2520light%2520on%2520these%2520%2527black%250Abox%2527%2520models%252C%2520aiding%2520in%2520deciphering%2520their%2520decision-making%2520processes.%2520However%252C%250Adifferent%2520XAI%2520methods%2520often%2520produce%2520significantly%2520different%2520explanations%252C%250Aleading%2520to%2520high%2520inter-method%2520variability%2520that%2520increases%2520uncertainty%2520and%250Aundermines%2520trust%2520in%2520deep%2520networks%2527%2520predictions.%2520In%2520this%2520study%252C%2520we%2520address%2520this%250Achallenge%2520by%2520introducing%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520the%250Aexplainability%2520of%2520deep%2520networks%2520through%2520a%2520dual%2520focus%2520on%2520maximizing%2520both%250Aaccuracy%2520and%2520comprehensibility%2520in%2520the%2520explanations.%2520Our%2520framework%2520integrates%250Aoutputs%2520from%2520multiple%2520established%2520XAI%2520methods%2520and%2520leverages%2520a%2520non-linear%2520neural%250Anetwork%2520model%252C%2520termed%2520the%2520%2527explanation%2520optimizer%252C%2527%2520to%2520construct%2520a%2520unified%252C%250Aoptimal%2520explanation.%2520The%2520optimizer%2520evaluates%2520explanations%2520using%2520two%2520key%250Ametrics%253A%2520faithfulness%2520%2528accuracy%2520in%2520reflecting%2520the%2520network%2527s%2520decisions%2529%2520and%250Acomplexity%2520%2528comprehensibility%2529.%2520By%2520balancing%2520these%252C%2520it%2520provides%2520accurate%2520and%250Aaccessible%2520explanations%252C%2520addressing%2520a%2520key%2520XAI%2520limitation.%2520Experiments%2520on%250Amulti-class%2520and%2520binary%2520classification%2520in%25202D%2520object%2520and%25203D%2520neuroscience%2520imaging%250Aconfirm%2520its%2520efficacy.%2520Our%2520optimizer%2520achieved%2520faithfulness%2520scores%2520155%2525%2520and%252063%2525%250Ahigher%2520than%2520the%2520best%2520XAI%2520methods%2520in%25203D%2520and%25202D%2520tasks%252C%2520respectively%252C%2520while%2520also%250Areducing%2520complexity%2520for%2520better%2520understanding.%2520These%2520results%2520demonstrate%2520that%250Aoptimal%2520explanations%2520based%2520on%2520specific%2520quality%2520criteria%2520are%2520achievable%252C%250Aoffering%2520a%2520solution%2520to%2520the%2520issue%2520of%2520inter-method%2520variability%2520in%2520the%2520current%2520XAI%250Aliterature%2520and%2520supporting%2520more%2520trustworthy%2520deep%2520network%2520predictions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20enigma%3A%20Enhancing%20faithfulness%20and%20comprehensibility%20in%0A%20%20explanations%20of%20deep%20networks&entry.906535625=Michail%20Mamalakis%20and%20Antonios%20Mamalakis%20and%20Ingrid%20Agartz%20and%20Lynn%20Egeland%20M%C3%B8rch-Johnsen%20and%20Graham%20Murray%20and%20John%20Suckling%20and%20Pietro%20Lio&entry.1292438233=%20%20The%20accelerated%20progress%20of%20artificial%20intelligence%20%28AI%29%20has%20popularized%20deep%0Alearning%20models%20across%20various%20domains%2C%20yet%20their%20inherent%20opacity%20poses%0Achallenges%2C%20particularly%20in%20critical%20fields%20like%20healthcare%2C%20medicine%2C%20and%20the%0Ageosciences.%20Explainable%20AI%20%28XAI%29%20has%20emerged%20to%20shed%20light%20on%20these%20%27black%0Abox%27%20models%2C%20aiding%20in%20deciphering%20their%20decision-making%20processes.%20However%2C%0Adifferent%20XAI%20methods%20often%20produce%20significantly%20different%20explanations%2C%0Aleading%20to%20high%20inter-method%20variability%20that%20increases%20uncertainty%20and%0Aundermines%20trust%20in%20deep%20networks%27%20predictions.%20In%20this%20study%2C%20we%20address%20this%0Achallenge%20by%20introducing%20a%20novel%20framework%20designed%20to%20enhance%20the%0Aexplainability%20of%20deep%20networks%20through%20a%20dual%20focus%20on%20maximizing%20both%0Aaccuracy%20and%20comprehensibility%20in%20the%20explanations.%20Our%20framework%20integrates%0Aoutputs%20from%20multiple%20established%20XAI%20methods%20and%20leverages%20a%20non-linear%20neural%0Anetwork%20model%2C%20termed%20the%20%27explanation%20optimizer%2C%27%20to%20construct%20a%20unified%2C%0Aoptimal%20explanation.%20The%20optimizer%20evaluates%20explanations%20using%20two%20key%0Ametrics%3A%20faithfulness%20%28accuracy%20in%20reflecting%20the%20network%27s%20decisions%29%20and%0Acomplexity%20%28comprehensibility%29.%20By%20balancing%20these%2C%20it%20provides%20accurate%20and%0Aaccessible%20explanations%2C%20addressing%20a%20key%20XAI%20limitation.%20Experiments%20on%0Amulti-class%20and%20binary%20classification%20in%202D%20object%20and%203D%20neuroscience%20imaging%0Aconfirm%20its%20efficacy.%20Our%20optimizer%20achieved%20faithfulness%20scores%20155%25%20and%2063%25%0Ahigher%20than%20the%20best%20XAI%20methods%20in%203D%20and%202D%20tasks%2C%20respectively%2C%20while%20also%0Areducing%20complexity%20for%20better%20understanding.%20These%20results%20demonstrate%20that%0Aoptimal%20explanations%20based%20on%20specific%20quality%20criteria%20are%20achievable%2C%0Aoffering%20a%20solution%20to%20the%20issue%20of%20inter-method%20variability%20in%20the%20current%20XAI%0Aliterature%20and%20supporting%20more%20trustworthy%20deep%20network%20predictions%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10008v2&entry.124074799=Read"},
{"title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery", "author": "Utkarsh Mall and Cheng Perng Phoo and Mia Chiquier and Bharath Hariharan and Kavita Bala and Carl Vondrick", "abstract": "  Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation.\n", "link": "http://arxiv.org/abs/2502.10060v1", "date": "2025-02-14", "relevancy": 2.1199, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5337}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiSciPLE%3A%20Learning%20Interpretable%20Programs%20for%20Scientific%20Visual%0A%20%20Discovery&body=Title%3A%20DiSciPLE%3A%20Learning%20Interpretable%20Programs%20for%20Scientific%20Visual%0A%20%20Discovery%0AAuthor%3A%20Utkarsh%20Mall%20and%20Cheng%20Perng%20Phoo%20and%20Mia%20Chiquier%20and%20Bharath%20Hariharan%20and%20Kavita%20Bala%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20Visual%20data%20is%20used%20in%20numerous%20different%20scientific%20workflows%20ranging%20from%0Aremote%20sensing%20to%20ecology.%20As%20the%20amount%20of%20observation%20data%20increases%2C%20the%0Achallenge%20is%20not%20just%20to%20make%20accurate%20predictions%20but%20also%20to%20understand%20the%0Aunderlying%20mechanisms%20for%20those%20predictions.%20Good%20interpretation%20is%20important%0Ain%20scientific%20workflows%2C%20as%20it%20allows%20for%20better%20decision-making%20by%20providing%0Ainsights%20into%20the%20data.%20This%20paper%20introduces%20an%20automatic%20way%20of%20obtaining%0Asuch%20interpretable-by-design%20models%2C%20by%20learning%20programs%20that%20interleave%0Aneural%20networks.%20We%20propose%20DiSciPLE%20%28Discovering%20Scientific%20Programs%20using%0ALLMs%20and%20Evolution%29%20an%20evolutionary%20algorithm%20that%20leverages%20common%20sense%20and%0Aprior%20knowledge%20of%20large%20language%20models%20%28LLMs%29%20to%20create%20Python%20programs%0Aexplaining%20visual%20data.%20Additionally%2C%20we%20propose%20two%20improvements%3A%20a%20program%0Acritic%20and%20a%20program%20simplifier%20to%20improve%20our%20method%20further%20to%20synthesize%0Agood%20programs.%20On%20three%20different%20real-world%20problems%2C%20DiSciPLE%20learns%0Astate-of-the-art%20programs%20on%20novel%20tasks%20with%20no%20prior%20literature.%20For%20example%2C%0Awe%20can%20learn%20programs%20with%2035%25%20lower%20error%20than%20the%20closest%20non-interpretable%0Abaseline%20for%20population%20density%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiSciPLE%253A%2520Learning%2520Interpretable%2520Programs%2520for%2520Scientific%2520Visual%250A%2520%2520Discovery%26entry.906535625%3DUtkarsh%2520Mall%2520and%2520Cheng%2520Perng%2520Phoo%2520and%2520Mia%2520Chiquier%2520and%2520Bharath%2520Hariharan%2520and%2520Kavita%2520Bala%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520Visual%2520data%2520is%2520used%2520in%2520numerous%2520different%2520scientific%2520workflows%2520ranging%2520from%250Aremote%2520sensing%2520to%2520ecology.%2520As%2520the%2520amount%2520of%2520observation%2520data%2520increases%252C%2520the%250Achallenge%2520is%2520not%2520just%2520to%2520make%2520accurate%2520predictions%2520but%2520also%2520to%2520understand%2520the%250Aunderlying%2520mechanisms%2520for%2520those%2520predictions.%2520Good%2520interpretation%2520is%2520important%250Ain%2520scientific%2520workflows%252C%2520as%2520it%2520allows%2520for%2520better%2520decision-making%2520by%2520providing%250Ainsights%2520into%2520the%2520data.%2520This%2520paper%2520introduces%2520an%2520automatic%2520way%2520of%2520obtaining%250Asuch%2520interpretable-by-design%2520models%252C%2520by%2520learning%2520programs%2520that%2520interleave%250Aneural%2520networks.%2520We%2520propose%2520DiSciPLE%2520%2528Discovering%2520Scientific%2520Programs%2520using%250ALLMs%2520and%2520Evolution%2529%2520an%2520evolutionary%2520algorithm%2520that%2520leverages%2520common%2520sense%2520and%250Aprior%2520knowledge%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520create%2520Python%2520programs%250Aexplaining%2520visual%2520data.%2520Additionally%252C%2520we%2520propose%2520two%2520improvements%253A%2520a%2520program%250Acritic%2520and%2520a%2520program%2520simplifier%2520to%2520improve%2520our%2520method%2520further%2520to%2520synthesize%250Agood%2520programs.%2520On%2520three%2520different%2520real-world%2520problems%252C%2520DiSciPLE%2520learns%250Astate-of-the-art%2520programs%2520on%2520novel%2520tasks%2520with%2520no%2520prior%2520literature.%2520For%2520example%252C%250Awe%2520can%2520learn%2520programs%2520with%252035%2525%2520lower%2520error%2520than%2520the%2520closest%2520non-interpretable%250Abaseline%2520for%2520population%2520density%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiSciPLE%3A%20Learning%20Interpretable%20Programs%20for%20Scientific%20Visual%0A%20%20Discovery&entry.906535625=Utkarsh%20Mall%20and%20Cheng%20Perng%20Phoo%20and%20Mia%20Chiquier%20and%20Bharath%20Hariharan%20and%20Kavita%20Bala%20and%20Carl%20Vondrick&entry.1292438233=%20%20Visual%20data%20is%20used%20in%20numerous%20different%20scientific%20workflows%20ranging%20from%0Aremote%20sensing%20to%20ecology.%20As%20the%20amount%20of%20observation%20data%20increases%2C%20the%0Achallenge%20is%20not%20just%20to%20make%20accurate%20predictions%20but%20also%20to%20understand%20the%0Aunderlying%20mechanisms%20for%20those%20predictions.%20Good%20interpretation%20is%20important%0Ain%20scientific%20workflows%2C%20as%20it%20allows%20for%20better%20decision-making%20by%20providing%0Ainsights%20into%20the%20data.%20This%20paper%20introduces%20an%20automatic%20way%20of%20obtaining%0Asuch%20interpretable-by-design%20models%2C%20by%20learning%20programs%20that%20interleave%0Aneural%20networks.%20We%20propose%20DiSciPLE%20%28Discovering%20Scientific%20Programs%20using%0ALLMs%20and%20Evolution%29%20an%20evolutionary%20algorithm%20that%20leverages%20common%20sense%20and%0Aprior%20knowledge%20of%20large%20language%20models%20%28LLMs%29%20to%20create%20Python%20programs%0Aexplaining%20visual%20data.%20Additionally%2C%20we%20propose%20two%20improvements%3A%20a%20program%0Acritic%20and%20a%20program%20simplifier%20to%20improve%20our%20method%20further%20to%20synthesize%0Agood%20programs.%20On%20three%20different%20real-world%20problems%2C%20DiSciPLE%20learns%0Astate-of-the-art%20programs%20on%20novel%20tasks%20with%20no%20prior%20literature.%20For%20example%2C%0Awe%20can%20learn%20programs%20with%2035%25%20lower%20error%20than%20the%20closest%20non-interpretable%0Abaseline%20for%20population%20density%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10060v1&entry.124074799=Read"},
{"title": "Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution\n  Detection", "author": "Fanhu Zeng and Zhen Cheng and Fei Zhu and Hongxin Wei and Xu-Yao Zhang", "abstract": "  Out-of-Distribution (OOD) detection, aiming to distinguish outliers from\nknown categories, has gained prominence in practical scenarios. Recently, the\nadvent of vision-language models (VLM) has heightened interest in enhancing OOD\ndetection for VLM through few-shot tuning. However, existing methods mainly\nfocus on optimizing global prompts, ignoring refined utilization of local\ninformation with regard to outliers. Motivated by this, we freeze global\nprompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to\nemphasize regional enhancement with local prompts. Our method comprises two\nintegral components: global prompt guided negative augmentation and local\nprompt enhanced regional regularization. The former utilizes frozen, coarse\nglobal prompts as guiding cues to incorporate negative augmentation, thereby\nleveraging local outlier knowledge. The latter employs trainable local prompts\nand a regional regularization to capture local information effectively, aiding\nin outlier identification. We also propose regional-related metric to empower\nthe enrichment of OOD detection. Moreover, since our approach explores\nenhancing local prompts only, it can be seamlessly integrated with trained\nglobal prompts during inference to boost the performance. Comprehensive\nexperiments demonstrate the effectiveness and potential of our method. Notably,\nour method reduces average FPR95 by 5.17% against state-of-the-art method in\n4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot\nresults of previous methods. Code is released at\nhttps://github.com/AuroraZengfh/Local-Prompt.\n", "link": "http://arxiv.org/abs/2409.04796v2", "date": "2025-02-14", "relevancy": 2.088, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5329}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5143}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-Prompt%3A%20Extensible%20Local%20Prompts%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Detection&body=Title%3A%20Local-Prompt%3A%20Extensible%20Local%20Prompts%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Detection%0AAuthor%3A%20Fanhu%20Zeng%20and%20Zhen%20Cheng%20and%20Fei%20Zhu%20and%20Hongxin%20Wei%20and%20Xu-Yao%20Zhang%0AAbstract%3A%20%20%20Out-of-Distribution%20%28OOD%29%20detection%2C%20aiming%20to%20distinguish%20outliers%20from%0Aknown%20categories%2C%20has%20gained%20prominence%20in%20practical%20scenarios.%20Recently%2C%20the%0Aadvent%20of%20vision-language%20models%20%28VLM%29%20has%20heightened%20interest%20in%20enhancing%20OOD%0Adetection%20for%20VLM%20through%20few-shot%20tuning.%20However%2C%20existing%20methods%20mainly%0Afocus%20on%20optimizing%20global%20prompts%2C%20ignoring%20refined%20utilization%20of%20local%0Ainformation%20with%20regard%20to%20outliers.%20Motivated%20by%20this%2C%20we%20freeze%20global%0Aprompts%20and%20introduce%20Local-Prompt%2C%20a%20novel%20coarse-to-fine%20tuning%20paradigm%20to%0Aemphasize%20regional%20enhancement%20with%20local%20prompts.%20Our%20method%20comprises%20two%0Aintegral%20components%3A%20global%20prompt%20guided%20negative%20augmentation%20and%20local%0Aprompt%20enhanced%20regional%20regularization.%20The%20former%20utilizes%20frozen%2C%20coarse%0Aglobal%20prompts%20as%20guiding%20cues%20to%20incorporate%20negative%20augmentation%2C%20thereby%0Aleveraging%20local%20outlier%20knowledge.%20The%20latter%20employs%20trainable%20local%20prompts%0Aand%20a%20regional%20regularization%20to%20capture%20local%20information%20effectively%2C%20aiding%0Ain%20outlier%20identification.%20We%20also%20propose%20regional-related%20metric%20to%20empower%0Athe%20enrichment%20of%20OOD%20detection.%20Moreover%2C%20since%20our%20approach%20explores%0Aenhancing%20local%20prompts%20only%2C%20it%20can%20be%20seamlessly%20integrated%20with%20trained%0Aglobal%20prompts%20during%20inference%20to%20boost%20the%20performance.%20Comprehensive%0Aexperiments%20demonstrate%20the%20effectiveness%20and%20potential%20of%20our%20method.%20Notably%2C%0Aour%20method%20reduces%20average%20FPR95%20by%205.17%25%20against%20state-of-the-art%20method%20in%0A4-shot%20tuning%20on%20challenging%20ImageNet-1k%20dataset%2C%20even%20outperforming%2016-shot%0Aresults%20of%20previous%20methods.%20Code%20is%20released%20at%0Ahttps%3A//github.com/AuroraZengfh/Local-Prompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-Prompt%253A%2520Extensible%2520Local%2520Prompts%2520for%2520Few-Shot%2520Out-of-Distribution%250A%2520%2520Detection%26entry.906535625%3DFanhu%2520Zeng%2520and%2520Zhen%2520Cheng%2520and%2520Fei%2520Zhu%2520and%2520Hongxin%2520Wei%2520and%2520Xu-Yao%2520Zhang%26entry.1292438233%3D%2520%2520Out-of-Distribution%2520%2528OOD%2529%2520detection%252C%2520aiming%2520to%2520distinguish%2520outliers%2520from%250Aknown%2520categories%252C%2520has%2520gained%2520prominence%2520in%2520practical%2520scenarios.%2520Recently%252C%2520the%250Aadvent%2520of%2520vision-language%2520models%2520%2528VLM%2529%2520has%2520heightened%2520interest%2520in%2520enhancing%2520OOD%250Adetection%2520for%2520VLM%2520through%2520few-shot%2520tuning.%2520However%252C%2520existing%2520methods%2520mainly%250Afocus%2520on%2520optimizing%2520global%2520prompts%252C%2520ignoring%2520refined%2520utilization%2520of%2520local%250Ainformation%2520with%2520regard%2520to%2520outliers.%2520Motivated%2520by%2520this%252C%2520we%2520freeze%2520global%250Aprompts%2520and%2520introduce%2520Local-Prompt%252C%2520a%2520novel%2520coarse-to-fine%2520tuning%2520paradigm%2520to%250Aemphasize%2520regional%2520enhancement%2520with%2520local%2520prompts.%2520Our%2520method%2520comprises%2520two%250Aintegral%2520components%253A%2520global%2520prompt%2520guided%2520negative%2520augmentation%2520and%2520local%250Aprompt%2520enhanced%2520regional%2520regularization.%2520The%2520former%2520utilizes%2520frozen%252C%2520coarse%250Aglobal%2520prompts%2520as%2520guiding%2520cues%2520to%2520incorporate%2520negative%2520augmentation%252C%2520thereby%250Aleveraging%2520local%2520outlier%2520knowledge.%2520The%2520latter%2520employs%2520trainable%2520local%2520prompts%250Aand%2520a%2520regional%2520regularization%2520to%2520capture%2520local%2520information%2520effectively%252C%2520aiding%250Ain%2520outlier%2520identification.%2520We%2520also%2520propose%2520regional-related%2520metric%2520to%2520empower%250Athe%2520enrichment%2520of%2520OOD%2520detection.%2520Moreover%252C%2520since%2520our%2520approach%2520explores%250Aenhancing%2520local%2520prompts%2520only%252C%2520it%2520can%2520be%2520seamlessly%2520integrated%2520with%2520trained%250Aglobal%2520prompts%2520during%2520inference%2520to%2520boost%2520the%2520performance.%2520Comprehensive%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520and%2520potential%2520of%2520our%2520method.%2520Notably%252C%250Aour%2520method%2520reduces%2520average%2520FPR95%2520by%25205.17%2525%2520against%2520state-of-the-art%2520method%2520in%250A4-shot%2520tuning%2520on%2520challenging%2520ImageNet-1k%2520dataset%252C%2520even%2520outperforming%252016-shot%250Aresults%2520of%2520previous%2520methods.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/AuroraZengfh/Local-Prompt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-Prompt%3A%20Extensible%20Local%20Prompts%20for%20Few-Shot%20Out-of-Distribution%0A%20%20Detection&entry.906535625=Fanhu%20Zeng%20and%20Zhen%20Cheng%20and%20Fei%20Zhu%20and%20Hongxin%20Wei%20and%20Xu-Yao%20Zhang&entry.1292438233=%20%20Out-of-Distribution%20%28OOD%29%20detection%2C%20aiming%20to%20distinguish%20outliers%20from%0Aknown%20categories%2C%20has%20gained%20prominence%20in%20practical%20scenarios.%20Recently%2C%20the%0Aadvent%20of%20vision-language%20models%20%28VLM%29%20has%20heightened%20interest%20in%20enhancing%20OOD%0Adetection%20for%20VLM%20through%20few-shot%20tuning.%20However%2C%20existing%20methods%20mainly%0Afocus%20on%20optimizing%20global%20prompts%2C%20ignoring%20refined%20utilization%20of%20local%0Ainformation%20with%20regard%20to%20outliers.%20Motivated%20by%20this%2C%20we%20freeze%20global%0Aprompts%20and%20introduce%20Local-Prompt%2C%20a%20novel%20coarse-to-fine%20tuning%20paradigm%20to%0Aemphasize%20regional%20enhancement%20with%20local%20prompts.%20Our%20method%20comprises%20two%0Aintegral%20components%3A%20global%20prompt%20guided%20negative%20augmentation%20and%20local%0Aprompt%20enhanced%20regional%20regularization.%20The%20former%20utilizes%20frozen%2C%20coarse%0Aglobal%20prompts%20as%20guiding%20cues%20to%20incorporate%20negative%20augmentation%2C%20thereby%0Aleveraging%20local%20outlier%20knowledge.%20The%20latter%20employs%20trainable%20local%20prompts%0Aand%20a%20regional%20regularization%20to%20capture%20local%20information%20effectively%2C%20aiding%0Ain%20outlier%20identification.%20We%20also%20propose%20regional-related%20metric%20to%20empower%0Athe%20enrichment%20of%20OOD%20detection.%20Moreover%2C%20since%20our%20approach%20explores%0Aenhancing%20local%20prompts%20only%2C%20it%20can%20be%20seamlessly%20integrated%20with%20trained%0Aglobal%20prompts%20during%20inference%20to%20boost%20the%20performance.%20Comprehensive%0Aexperiments%20demonstrate%20the%20effectiveness%20and%20potential%20of%20our%20method.%20Notably%2C%0Aour%20method%20reduces%20average%20FPR95%20by%205.17%25%20against%20state-of-the-art%20method%20in%0A4-shot%20tuning%20on%20challenging%20ImageNet-1k%20dataset%2C%20even%20outperforming%2016-shot%0Aresults%20of%20previous%20methods.%20Code%20is%20released%20at%0Ahttps%3A//github.com/AuroraZengfh/Local-Prompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04796v2&entry.124074799=Read"},
{"title": "Looking around you: external information enhances representations for\n  event sequences", "author": "Maria Kovaleva and Petr Sokerin and Sofia Krehova and Alexey Zaytsev", "abstract": "  Representation learning produces models in different domains, such as store\npurchases, client transactions, and general people's behaviour. However, such\nmodels for sequential data usually process a single sequence, ignoring context\nfrom other relevant ones, even in domains with rapidly changing external\nenvironments like finance or misguiding the prediction for a user with no\nrecent events.\n  We are the first to propose a method that aggregates information from\nmultiple user representations augmenting a specific user one for a scenario of\nmultiple co-occurring event sequences. Our study considers diverse aggregation\napproaches, ranging from simple pooling techniques to trainable attention-based\napproaches, especially Kernel attention aggregation, that can highlight more\ncomplex information flow from other users. The proposed method operates atop an\nexisting encoder and supports its efficient fine-tuning. Across considered\ndatasets of financial transactions and downstream tasks, Kernel attention\nimproves ROC AUC scores, both with and without fine-tuning, while mean pooling\nyields a smaller but still significant gain.\n", "link": "http://arxiv.org/abs/2502.10205v1", "date": "2025-02-14", "relevancy": 2.0869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences&body=Title%3A%20Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences%0AAuthor%3A%20Maria%20Kovaleva%20and%20Petr%20Sokerin%20and%20Sofia%20Krehova%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Representation%20learning%20produces%20models%20in%20different%20domains%2C%20such%20as%20store%0Apurchases%2C%20client%20transactions%2C%20and%20general%20people%27s%20behaviour.%20However%2C%20such%0Amodels%20for%20sequential%20data%20usually%20process%20a%20single%20sequence%2C%20ignoring%20context%0Afrom%20other%20relevant%20ones%2C%20even%20in%20domains%20with%20rapidly%20changing%20external%0Aenvironments%20like%20finance%20or%20misguiding%20the%20prediction%20for%20a%20user%20with%20no%0Arecent%20events.%0A%20%20We%20are%20the%20first%20to%20propose%20a%20method%20that%20aggregates%20information%20from%0Amultiple%20user%20representations%20augmenting%20a%20specific%20user%20one%20for%20a%20scenario%20of%0Amultiple%20co-occurring%20event%20sequences.%20Our%20study%20considers%20diverse%20aggregation%0Aapproaches%2C%20ranging%20from%20simple%20pooling%20techniques%20to%20trainable%20attention-based%0Aapproaches%2C%20especially%20Kernel%20attention%20aggregation%2C%20that%20can%20highlight%20more%0Acomplex%20information%20flow%20from%20other%20users.%20The%20proposed%20method%20operates%20atop%20an%0Aexisting%20encoder%20and%20supports%20its%20efficient%20fine-tuning.%20Across%20considered%0Adatasets%20of%20financial%20transactions%20and%20downstream%20tasks%2C%20Kernel%20attention%0Aimproves%20ROC%20AUC%20scores%2C%20both%20with%20and%20without%20fine-tuning%2C%20while%20mean%20pooling%0Ayields%20a%20smaller%20but%20still%20significant%20gain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520around%2520you%253A%2520external%2520information%2520enhances%2520representations%2520for%250A%2520%2520event%2520sequences%26entry.906535625%3DMaria%2520Kovaleva%2520and%2520Petr%2520Sokerin%2520and%2520Sofia%2520Krehova%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Representation%2520learning%2520produces%2520models%2520in%2520different%2520domains%252C%2520such%2520as%2520store%250Apurchases%252C%2520client%2520transactions%252C%2520and%2520general%2520people%2527s%2520behaviour.%2520However%252C%2520such%250Amodels%2520for%2520sequential%2520data%2520usually%2520process%2520a%2520single%2520sequence%252C%2520ignoring%2520context%250Afrom%2520other%2520relevant%2520ones%252C%2520even%2520in%2520domains%2520with%2520rapidly%2520changing%2520external%250Aenvironments%2520like%2520finance%2520or%2520misguiding%2520the%2520prediction%2520for%2520a%2520user%2520with%2520no%250Arecent%2520events.%250A%2520%2520We%2520are%2520the%2520first%2520to%2520propose%2520a%2520method%2520that%2520aggregates%2520information%2520from%250Amultiple%2520user%2520representations%2520augmenting%2520a%2520specific%2520user%2520one%2520for%2520a%2520scenario%2520of%250Amultiple%2520co-occurring%2520event%2520sequences.%2520Our%2520study%2520considers%2520diverse%2520aggregation%250Aapproaches%252C%2520ranging%2520from%2520simple%2520pooling%2520techniques%2520to%2520trainable%2520attention-based%250Aapproaches%252C%2520especially%2520Kernel%2520attention%2520aggregation%252C%2520that%2520can%2520highlight%2520more%250Acomplex%2520information%2520flow%2520from%2520other%2520users.%2520The%2520proposed%2520method%2520operates%2520atop%2520an%250Aexisting%2520encoder%2520and%2520supports%2520its%2520efficient%2520fine-tuning.%2520Across%2520considered%250Adatasets%2520of%2520financial%2520transactions%2520and%2520downstream%2520tasks%252C%2520Kernel%2520attention%250Aimproves%2520ROC%2520AUC%2520scores%252C%2520both%2520with%2520and%2520without%2520fine-tuning%252C%2520while%2520mean%2520pooling%250Ayields%2520a%2520smaller%2520but%2520still%2520significant%2520gain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20around%20you%3A%20external%20information%20enhances%20representations%20for%0A%20%20event%20sequences&entry.906535625=Maria%20Kovaleva%20and%20Petr%20Sokerin%20and%20Sofia%20Krehova%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Representation%20learning%20produces%20models%20in%20different%20domains%2C%20such%20as%20store%0Apurchases%2C%20client%20transactions%2C%20and%20general%20people%27s%20behaviour.%20However%2C%20such%0Amodels%20for%20sequential%20data%20usually%20process%20a%20single%20sequence%2C%20ignoring%20context%0Afrom%20other%20relevant%20ones%2C%20even%20in%20domains%20with%20rapidly%20changing%20external%0Aenvironments%20like%20finance%20or%20misguiding%20the%20prediction%20for%20a%20user%20with%20no%0Arecent%20events.%0A%20%20We%20are%20the%20first%20to%20propose%20a%20method%20that%20aggregates%20information%20from%0Amultiple%20user%20representations%20augmenting%20a%20specific%20user%20one%20for%20a%20scenario%20of%0Amultiple%20co-occurring%20event%20sequences.%20Our%20study%20considers%20diverse%20aggregation%0Aapproaches%2C%20ranging%20from%20simple%20pooling%20techniques%20to%20trainable%20attention-based%0Aapproaches%2C%20especially%20Kernel%20attention%20aggregation%2C%20that%20can%20highlight%20more%0Acomplex%20information%20flow%20from%20other%20users.%20The%20proposed%20method%20operates%20atop%20an%0Aexisting%20encoder%20and%20supports%20its%20efficient%20fine-tuning.%20Across%20considered%0Adatasets%20of%20financial%20transactions%20and%20downstream%20tasks%2C%20Kernel%20attention%0Aimproves%20ROC%20AUC%20scores%2C%20both%20with%20and%20without%20fine-tuning%2C%20while%20mean%20pooling%0Ayields%20a%20smaller%20but%20still%20significant%20gain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10205v1&entry.124074799=Read"},
{"title": "Realistic Evaluation of Deep Partial-Label Learning Algorithms", "author": "Wei Wang and Dong-Dong Wu and Jindong Wang and Gang Niu and Min-Ling Zhang and Masashi Sugiyama", "abstract": "  Partial-label learning (PLL) is a weakly supervised learning problem in which\neach example is associated with multiple candidate labels and only one is the\ntrue label. In recent years, many deep PLL algorithms have been developed to\nimprove model performance. However, we find that some early developed\nalgorithms are often underestimated and can outperform many later algorithms\nwith complicated designs. In this paper, we delve into the empirical\nperspective of PLL and identify several critical but previously overlooked\nissues. First, model selection for PLL is non-trivial, but has never been\nsystematically studied. Second, the experimental settings are highly\ninconsistent, making it difficult to evaluate the effectiveness of the\nalgorithms. Third, there is a lack of real-world image datasets that can be\ncompatible with modern network architectures. Based on these findings, we\npropose PLENCH, the first Partial-Label learning bENCHmark to systematically\ncompare state-of-the-art deep PLL algorithms. We investigate the model\nselection problem for PLL for the first time, and propose novel model selection\ncriteria with theoretical guarantees. We also create Partial-Label CIFAR-10\n(PLCIFAR10), an image dataset of human-annotated partial labels collected from\nAmazon Mechanical Turk, to provide a testbed for evaluating the performance of\nPLL algorithms in more realistic scenarios. Researchers can quickly and\nconveniently perform a comprehensive and fair evaluation and verify the\neffectiveness of newly developed algorithms based on PLENCH. We hope that\nPLENCH will facilitate standardized, fair, and practical evaluation of PLL\nalgorithms in the future.\n", "link": "http://arxiv.org/abs/2502.10184v1", "date": "2025-02-14", "relevancy": 2.0868, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5574}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5205}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Evaluation%20of%20Deep%20Partial-Label%20Learning%20Algorithms&body=Title%3A%20Realistic%20Evaluation%20of%20Deep%20Partial-Label%20Learning%20Algorithms%0AAuthor%3A%20Wei%20Wang%20and%20Dong-Dong%20Wu%20and%20Jindong%20Wang%20and%20Gang%20Niu%20and%20Min-Ling%20Zhang%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20Partial-label%20learning%20%28PLL%29%20is%20a%20weakly%20supervised%20learning%20problem%20in%20which%0Aeach%20example%20is%20associated%20with%20multiple%20candidate%20labels%20and%20only%20one%20is%20the%0Atrue%20label.%20In%20recent%20years%2C%20many%20deep%20PLL%20algorithms%20have%20been%20developed%20to%0Aimprove%20model%20performance.%20However%2C%20we%20find%20that%20some%20early%20developed%0Aalgorithms%20are%20often%20underestimated%20and%20can%20outperform%20many%20later%20algorithms%0Awith%20complicated%20designs.%20In%20this%20paper%2C%20we%20delve%20into%20the%20empirical%0Aperspective%20of%20PLL%20and%20identify%20several%20critical%20but%20previously%20overlooked%0Aissues.%20First%2C%20model%20selection%20for%20PLL%20is%20non-trivial%2C%20but%20has%20never%20been%0Asystematically%20studied.%20Second%2C%20the%20experimental%20settings%20are%20highly%0Ainconsistent%2C%20making%20it%20difficult%20to%20evaluate%20the%20effectiveness%20of%20the%0Aalgorithms.%20Third%2C%20there%20is%20a%20lack%20of%20real-world%20image%20datasets%20that%20can%20be%0Acompatible%20with%20modern%20network%20architectures.%20Based%20on%20these%20findings%2C%20we%0Apropose%20PLENCH%2C%20the%20first%20Partial-Label%20learning%20bENCHmark%20to%20systematically%0Acompare%20state-of-the-art%20deep%20PLL%20algorithms.%20We%20investigate%20the%20model%0Aselection%20problem%20for%20PLL%20for%20the%20first%20time%2C%20and%20propose%20novel%20model%20selection%0Acriteria%20with%20theoretical%20guarantees.%20We%20also%20create%20Partial-Label%20CIFAR-10%0A%28PLCIFAR10%29%2C%20an%20image%20dataset%20of%20human-annotated%20partial%20labels%20collected%20from%0AAmazon%20Mechanical%20Turk%2C%20to%20provide%20a%20testbed%20for%20evaluating%20the%20performance%20of%0APLL%20algorithms%20in%20more%20realistic%20scenarios.%20Researchers%20can%20quickly%20and%0Aconveniently%20perform%20a%20comprehensive%20and%20fair%20evaluation%20and%20verify%20the%0Aeffectiveness%20of%20newly%20developed%20algorithms%20based%20on%20PLENCH.%20We%20hope%20that%0APLENCH%20will%20facilitate%20standardized%2C%20fair%2C%20and%20practical%20evaluation%20of%20PLL%0Aalgorithms%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Evaluation%2520of%2520Deep%2520Partial-Label%2520Learning%2520Algorithms%26entry.906535625%3DWei%2520Wang%2520and%2520Dong-Dong%2520Wu%2520and%2520Jindong%2520Wang%2520and%2520Gang%2520Niu%2520and%2520Min-Ling%2520Zhang%2520and%2520Masashi%2520Sugiyama%26entry.1292438233%3D%2520%2520Partial-label%2520learning%2520%2528PLL%2529%2520is%2520a%2520weakly%2520supervised%2520learning%2520problem%2520in%2520which%250Aeach%2520example%2520is%2520associated%2520with%2520multiple%2520candidate%2520labels%2520and%2520only%2520one%2520is%2520the%250Atrue%2520label.%2520In%2520recent%2520years%252C%2520many%2520deep%2520PLL%2520algorithms%2520have%2520been%2520developed%2520to%250Aimprove%2520model%2520performance.%2520However%252C%2520we%2520find%2520that%2520some%2520early%2520developed%250Aalgorithms%2520are%2520often%2520underestimated%2520and%2520can%2520outperform%2520many%2520later%2520algorithms%250Awith%2520complicated%2520designs.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%2520empirical%250Aperspective%2520of%2520PLL%2520and%2520identify%2520several%2520critical%2520but%2520previously%2520overlooked%250Aissues.%2520First%252C%2520model%2520selection%2520for%2520PLL%2520is%2520non-trivial%252C%2520but%2520has%2520never%2520been%250Asystematically%2520studied.%2520Second%252C%2520the%2520experimental%2520settings%2520are%2520highly%250Ainconsistent%252C%2520making%2520it%2520difficult%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520the%250Aalgorithms.%2520Third%252C%2520there%2520is%2520a%2520lack%2520of%2520real-world%2520image%2520datasets%2520that%2520can%2520be%250Acompatible%2520with%2520modern%2520network%2520architectures.%2520Based%2520on%2520these%2520findings%252C%2520we%250Apropose%2520PLENCH%252C%2520the%2520first%2520Partial-Label%2520learning%2520bENCHmark%2520to%2520systematically%250Acompare%2520state-of-the-art%2520deep%2520PLL%2520algorithms.%2520We%2520investigate%2520the%2520model%250Aselection%2520problem%2520for%2520PLL%2520for%2520the%2520first%2520time%252C%2520and%2520propose%2520novel%2520model%2520selection%250Acriteria%2520with%2520theoretical%2520guarantees.%2520We%2520also%2520create%2520Partial-Label%2520CIFAR-10%250A%2528PLCIFAR10%2529%252C%2520an%2520image%2520dataset%2520of%2520human-annotated%2520partial%2520labels%2520collected%2520from%250AAmazon%2520Mechanical%2520Turk%252C%2520to%2520provide%2520a%2520testbed%2520for%2520evaluating%2520the%2520performance%2520of%250APLL%2520algorithms%2520in%2520more%2520realistic%2520scenarios.%2520Researchers%2520can%2520quickly%2520and%250Aconveniently%2520perform%2520a%2520comprehensive%2520and%2520fair%2520evaluation%2520and%2520verify%2520the%250Aeffectiveness%2520of%2520newly%2520developed%2520algorithms%2520based%2520on%2520PLENCH.%2520We%2520hope%2520that%250APLENCH%2520will%2520facilitate%2520standardized%252C%2520fair%252C%2520and%2520practical%2520evaluation%2520of%2520PLL%250Aalgorithms%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Evaluation%20of%20Deep%20Partial-Label%20Learning%20Algorithms&entry.906535625=Wei%20Wang%20and%20Dong-Dong%20Wu%20and%20Jindong%20Wang%20and%20Gang%20Niu%20and%20Min-Ling%20Zhang%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20Partial-label%20learning%20%28PLL%29%20is%20a%20weakly%20supervised%20learning%20problem%20in%20which%0Aeach%20example%20is%20associated%20with%20multiple%20candidate%20labels%20and%20only%20one%20is%20the%0Atrue%20label.%20In%20recent%20years%2C%20many%20deep%20PLL%20algorithms%20have%20been%20developed%20to%0Aimprove%20model%20performance.%20However%2C%20we%20find%20that%20some%20early%20developed%0Aalgorithms%20are%20often%20underestimated%20and%20can%20outperform%20many%20later%20algorithms%0Awith%20complicated%20designs.%20In%20this%20paper%2C%20we%20delve%20into%20the%20empirical%0Aperspective%20of%20PLL%20and%20identify%20several%20critical%20but%20previously%20overlooked%0Aissues.%20First%2C%20model%20selection%20for%20PLL%20is%20non-trivial%2C%20but%20has%20never%20been%0Asystematically%20studied.%20Second%2C%20the%20experimental%20settings%20are%20highly%0Ainconsistent%2C%20making%20it%20difficult%20to%20evaluate%20the%20effectiveness%20of%20the%0Aalgorithms.%20Third%2C%20there%20is%20a%20lack%20of%20real-world%20image%20datasets%20that%20can%20be%0Acompatible%20with%20modern%20network%20architectures.%20Based%20on%20these%20findings%2C%20we%0Apropose%20PLENCH%2C%20the%20first%20Partial-Label%20learning%20bENCHmark%20to%20systematically%0Acompare%20state-of-the-art%20deep%20PLL%20algorithms.%20We%20investigate%20the%20model%0Aselection%20problem%20for%20PLL%20for%20the%20first%20time%2C%20and%20propose%20novel%20model%20selection%0Acriteria%20with%20theoretical%20guarantees.%20We%20also%20create%20Partial-Label%20CIFAR-10%0A%28PLCIFAR10%29%2C%20an%20image%20dataset%20of%20human-annotated%20partial%20labels%20collected%20from%0AAmazon%20Mechanical%20Turk%2C%20to%20provide%20a%20testbed%20for%20evaluating%20the%20performance%20of%0APLL%20algorithms%20in%20more%20realistic%20scenarios.%20Researchers%20can%20quickly%20and%0Aconveniently%20perform%20a%20comprehensive%20and%20fair%20evaluation%20and%20verify%20the%0Aeffectiveness%20of%20newly%20developed%20algorithms%20based%20on%20PLENCH.%20We%20hope%20that%0APLENCH%20will%20facilitate%20standardized%2C%20fair%2C%20and%20practical%20evaluation%20of%20PLL%0Aalgorithms%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10184v1&entry.124074799=Read"},
{"title": "Bag of Tricks for Inference-time Computation of LLM Reasoning", "author": "Fan Liu and Wenshuo Chao and Naiqiang Tan and Hao Liu", "abstract": "  With the advancement of large language models (LLMs), solving complex\nreasoning tasks has gained increasing attention. Inference-time computation\nmethods (e.g., Best-of-N, beam search, et al.) are particularly valuable as\nthey can enhance reasoning performance without modifying model parameters or\nrequiring additional training. However, these techniques come with\nimplementation challenges, and most existing methods remain at the\nproof-of-concept stage with limited practical adoption due to their\ncomputational complexity and varying effectiveness across different tasks. In\nthis paper, we investigate and benchmark diverse inference-time computation\nstrategies across reasoning tasks of varying complexity. Since most current\nmethods rely on a proposer-verifier pipeline that first generates candidate\nsolutions (e.g., reasoning solutions) and then selects the best one based on\nreward signals (e.g., RLHF rewards, process rewards), our research focuses on\noptimizing both candidate solution generation (e.g., instructing prompts,\nhyperparameters such as temperature and top-p) and reward mechanisms (e.g.,\nself-evaluation, reward types). Through extensive experiments (more than 20,000\nA100-80G GPU hours with over 1,000 experiments) across a variety of models\n(e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation\nstudies reveal that previously overlooked strategies can significantly enhance\nperformance (e.g., tuning temperature can improve reasoning task performance by\nup to 5%). Furthermore, we establish a standardized benchmark for\ninference-time computation by systematically evaluating six representative\nmethods across eight reasoning tasks. These findings provide a stronger\nfoundation for future research. The code is available at\nhttps://github.com/usail-hkust/benchmark_inference_time_computation_LL\n", "link": "http://arxiv.org/abs/2502.07191v3", "date": "2025-02-14", "relevancy": 2.0824, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bag%20of%20Tricks%20for%20Inference-time%20Computation%20of%20LLM%20Reasoning&body=Title%3A%20Bag%20of%20Tricks%20for%20Inference-time%20Computation%20of%20LLM%20Reasoning%0AAuthor%3A%20Fan%20Liu%20and%20Wenshuo%20Chao%20and%20Naiqiang%20Tan%20and%20Hao%20Liu%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20solving%20complex%0Areasoning%20tasks%20has%20gained%20increasing%20attention.%20Inference-time%20computation%0Amethods%20%28e.g.%2C%20Best-of-N%2C%20beam%20search%2C%20et%20al.%29%20are%20particularly%20valuable%20as%0Athey%20can%20enhance%20reasoning%20performance%20without%20modifying%20model%20parameters%20or%0Arequiring%20additional%20training.%20However%2C%20these%20techniques%20come%20with%0Aimplementation%20challenges%2C%20and%20most%20existing%20methods%20remain%20at%20the%0Aproof-of-concept%20stage%20with%20limited%20practical%20adoption%20due%20to%20their%0Acomputational%20complexity%20and%20varying%20effectiveness%20across%20different%20tasks.%20In%0Athis%20paper%2C%20we%20investigate%20and%20benchmark%20diverse%20inference-time%20computation%0Astrategies%20across%20reasoning%20tasks%20of%20varying%20complexity.%20Since%20most%20current%0Amethods%20rely%20on%20a%20proposer-verifier%20pipeline%20that%20first%20generates%20candidate%0Asolutions%20%28e.g.%2C%20reasoning%20solutions%29%20and%20then%20selects%20the%20best%20one%20based%20on%0Areward%20signals%20%28e.g.%2C%20RLHF%20rewards%2C%20process%20rewards%29%2C%20our%20research%20focuses%20on%0Aoptimizing%20both%20candidate%20solution%20generation%20%28e.g.%2C%20instructing%20prompts%2C%0Ahyperparameters%20such%20as%20temperature%20and%20top-p%29%20and%20reward%20mechanisms%20%28e.g.%2C%0Aself-evaluation%2C%20reward%20types%29.%20Through%20extensive%20experiments%20%28more%20than%2020%2C000%0AA100-80G%20GPU%20hours%20with%20over%201%2C000%20experiments%29%20across%20a%20variety%20of%20models%0A%28e.g.%2C%20Llama%2C%20Qwen%2C%20and%20Mistral%20families%29%20of%20various%20sizes%2C%20our%20ablation%0Astudies%20reveal%20that%20previously%20overlooked%20strategies%20can%20significantly%20enhance%0Aperformance%20%28e.g.%2C%20tuning%20temperature%20can%20improve%20reasoning%20task%20performance%20by%0Aup%20to%205%25%29.%20Furthermore%2C%20we%20establish%20a%20standardized%20benchmark%20for%0Ainference-time%20computation%20by%20systematically%20evaluating%20six%20representative%0Amethods%20across%20eight%20reasoning%20tasks.%20These%20findings%20provide%20a%20stronger%0Afoundation%20for%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/usail-hkust/benchmark_inference_time_computation_LL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07191v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBag%2520of%2520Tricks%2520for%2520Inference-time%2520Computation%2520of%2520LLM%2520Reasoning%26entry.906535625%3DFan%2520Liu%2520and%2520Wenshuo%2520Chao%2520and%2520Naiqiang%2520Tan%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520solving%2520complex%250Areasoning%2520tasks%2520has%2520gained%2520increasing%2520attention.%2520Inference-time%2520computation%250Amethods%2520%2528e.g.%252C%2520Best-of-N%252C%2520beam%2520search%252C%2520et%2520al.%2529%2520are%2520particularly%2520valuable%2520as%250Athey%2520can%2520enhance%2520reasoning%2520performance%2520without%2520modifying%2520model%2520parameters%2520or%250Arequiring%2520additional%2520training.%2520However%252C%2520these%2520techniques%2520come%2520with%250Aimplementation%2520challenges%252C%2520and%2520most%2520existing%2520methods%2520remain%2520at%2520the%250Aproof-of-concept%2520stage%2520with%2520limited%2520practical%2520adoption%2520due%2520to%2520their%250Acomputational%2520complexity%2520and%2520varying%2520effectiveness%2520across%2520different%2520tasks.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520and%2520benchmark%2520diverse%2520inference-time%2520computation%250Astrategies%2520across%2520reasoning%2520tasks%2520of%2520varying%2520complexity.%2520Since%2520most%2520current%250Amethods%2520rely%2520on%2520a%2520proposer-verifier%2520pipeline%2520that%2520first%2520generates%2520candidate%250Asolutions%2520%2528e.g.%252C%2520reasoning%2520solutions%2529%2520and%2520then%2520selects%2520the%2520best%2520one%2520based%2520on%250Areward%2520signals%2520%2528e.g.%252C%2520RLHF%2520rewards%252C%2520process%2520rewards%2529%252C%2520our%2520research%2520focuses%2520on%250Aoptimizing%2520both%2520candidate%2520solution%2520generation%2520%2528e.g.%252C%2520instructing%2520prompts%252C%250Ahyperparameters%2520such%2520as%2520temperature%2520and%2520top-p%2529%2520and%2520reward%2520mechanisms%2520%2528e.g.%252C%250Aself-evaluation%252C%2520reward%2520types%2529.%2520Through%2520extensive%2520experiments%2520%2528more%2520than%252020%252C000%250AA100-80G%2520GPU%2520hours%2520with%2520over%25201%252C000%2520experiments%2529%2520across%2520a%2520variety%2520of%2520models%250A%2528e.g.%252C%2520Llama%252C%2520Qwen%252C%2520and%2520Mistral%2520families%2529%2520of%2520various%2520sizes%252C%2520our%2520ablation%250Astudies%2520reveal%2520that%2520previously%2520overlooked%2520strategies%2520can%2520significantly%2520enhance%250Aperformance%2520%2528e.g.%252C%2520tuning%2520temperature%2520can%2520improve%2520reasoning%2520task%2520performance%2520by%250Aup%2520to%25205%2525%2529.%2520Furthermore%252C%2520we%2520establish%2520a%2520standardized%2520benchmark%2520for%250Ainference-time%2520computation%2520by%2520systematically%2520evaluating%2520six%2520representative%250Amethods%2520across%2520eight%2520reasoning%2520tasks.%2520These%2520findings%2520provide%2520a%2520stronger%250Afoundation%2520for%2520future%2520research.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/usail-hkust/benchmark_inference_time_computation_LL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07191v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bag%20of%20Tricks%20for%20Inference-time%20Computation%20of%20LLM%20Reasoning&entry.906535625=Fan%20Liu%20and%20Wenshuo%20Chao%20and%20Naiqiang%20Tan%20and%20Hao%20Liu&entry.1292438233=%20%20With%20the%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20solving%20complex%0Areasoning%20tasks%20has%20gained%20increasing%20attention.%20Inference-time%20computation%0Amethods%20%28e.g.%2C%20Best-of-N%2C%20beam%20search%2C%20et%20al.%29%20are%20particularly%20valuable%20as%0Athey%20can%20enhance%20reasoning%20performance%20without%20modifying%20model%20parameters%20or%0Arequiring%20additional%20training.%20However%2C%20these%20techniques%20come%20with%0Aimplementation%20challenges%2C%20and%20most%20existing%20methods%20remain%20at%20the%0Aproof-of-concept%20stage%20with%20limited%20practical%20adoption%20due%20to%20their%0Acomputational%20complexity%20and%20varying%20effectiveness%20across%20different%20tasks.%20In%0Athis%20paper%2C%20we%20investigate%20and%20benchmark%20diverse%20inference-time%20computation%0Astrategies%20across%20reasoning%20tasks%20of%20varying%20complexity.%20Since%20most%20current%0Amethods%20rely%20on%20a%20proposer-verifier%20pipeline%20that%20first%20generates%20candidate%0Asolutions%20%28e.g.%2C%20reasoning%20solutions%29%20and%20then%20selects%20the%20best%20one%20based%20on%0Areward%20signals%20%28e.g.%2C%20RLHF%20rewards%2C%20process%20rewards%29%2C%20our%20research%20focuses%20on%0Aoptimizing%20both%20candidate%20solution%20generation%20%28e.g.%2C%20instructing%20prompts%2C%0Ahyperparameters%20such%20as%20temperature%20and%20top-p%29%20and%20reward%20mechanisms%20%28e.g.%2C%0Aself-evaluation%2C%20reward%20types%29.%20Through%20extensive%20experiments%20%28more%20than%2020%2C000%0AA100-80G%20GPU%20hours%20with%20over%201%2C000%20experiments%29%20across%20a%20variety%20of%20models%0A%28e.g.%2C%20Llama%2C%20Qwen%2C%20and%20Mistral%20families%29%20of%20various%20sizes%2C%20our%20ablation%0Astudies%20reveal%20that%20previously%20overlooked%20strategies%20can%20significantly%20enhance%0Aperformance%20%28e.g.%2C%20tuning%20temperature%20can%20improve%20reasoning%20task%20performance%20by%0Aup%20to%205%25%29.%20Furthermore%2C%20we%20establish%20a%20standardized%20benchmark%20for%0Ainference-time%20computation%20by%20systematically%20evaluating%20six%20representative%0Amethods%20across%20eight%20reasoning%20tasks.%20These%20findings%20provide%20a%20stronger%0Afoundation%20for%20future%20research.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/usail-hkust/benchmark_inference_time_computation_LL%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07191v3&entry.124074799=Read"},
{"title": "Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a\n  Language Diffusion Model", "author": "Bo Ni and Markus J. Buehler", "abstract": "  Proteins are dynamic molecular machines whose biological functions, spanning\nenzymatic catalysis, signal transduction, and structural adaptation, are\nintrinsically linked to their motions. Designing proteins with targeted dynamic\nproperties, however, remains a challenge due to the complex, degenerate\nrelationships between sequence, structure, and molecular motion. Here, we\nintroduce VibeGen, a generative AI framework that enables end-to-end de novo\nprotein design conditioned on normal mode vibrations. VibeGen employs an\nagentic dual-model architecture, comprising a protein designer that generates\nsequence candidates based on specified vibrational modes and a protein\npredictor that evaluates their dynamic accuracy. This approach synergizes\ndiversity, accuracy, and novelty during the design process. Via full-atom\nmolecular simulations as direct validation, we demonstrate that the designed\nproteins accurately reproduce the prescribed normal mode amplitudes across the\nbackbone while adopting various stable, functionally relevant structures.\nNotably, generated sequences are de novo, exhibiting no significant similarity\nto natural proteins, thereby expanding the accessible protein space beyond\nevolutionary constraints. Our work integrates protein dynamics into generative\nprotein design, and establishes a direct, bidirectional link between sequence\nand vibrational behavior, unlocking new pathways for engineering biomolecules\nwith tailored dynamical and functional properties. This framework holds broad\nimplications for the rational design of flexible enzymes, dynamic scaffolds,\nand biomaterials, paving the way toward dynamics-informed AI-driven protein\nengineering.\n", "link": "http://arxiv.org/abs/2502.10173v1", "date": "2025-02-14", "relevancy": 2.0747, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5295}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5236}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20End-to-End%20De%20Novo%20Protein%20Design%20for%20Tailored%20Dynamics%20Using%20a%0A%20%20Language%20Diffusion%20Model&body=Title%3A%20Agentic%20End-to-End%20De%20Novo%20Protein%20Design%20for%20Tailored%20Dynamics%20Using%20a%0A%20%20Language%20Diffusion%20Model%0AAuthor%3A%20Bo%20Ni%20and%20Markus%20J.%20Buehler%0AAbstract%3A%20%20%20Proteins%20are%20dynamic%20molecular%20machines%20whose%20biological%20functions%2C%20spanning%0Aenzymatic%20catalysis%2C%20signal%20transduction%2C%20and%20structural%20adaptation%2C%20are%0Aintrinsically%20linked%20to%20their%20motions.%20Designing%20proteins%20with%20targeted%20dynamic%0Aproperties%2C%20however%2C%20remains%20a%20challenge%20due%20to%20the%20complex%2C%20degenerate%0Arelationships%20between%20sequence%2C%20structure%2C%20and%20molecular%20motion.%20Here%2C%20we%0Aintroduce%20VibeGen%2C%20a%20generative%20AI%20framework%20that%20enables%20end-to-end%20de%20novo%0Aprotein%20design%20conditioned%20on%20normal%20mode%20vibrations.%20VibeGen%20employs%20an%0Aagentic%20dual-model%20architecture%2C%20comprising%20a%20protein%20designer%20that%20generates%0Asequence%20candidates%20based%20on%20specified%20vibrational%20modes%20and%20a%20protein%0Apredictor%20that%20evaluates%20their%20dynamic%20accuracy.%20This%20approach%20synergizes%0Adiversity%2C%20accuracy%2C%20and%20novelty%20during%20the%20design%20process.%20Via%20full-atom%0Amolecular%20simulations%20as%20direct%20validation%2C%20we%20demonstrate%20that%20the%20designed%0Aproteins%20accurately%20reproduce%20the%20prescribed%20normal%20mode%20amplitudes%20across%20the%0Abackbone%20while%20adopting%20various%20stable%2C%20functionally%20relevant%20structures.%0ANotably%2C%20generated%20sequences%20are%20de%20novo%2C%20exhibiting%20no%20significant%20similarity%0Ato%20natural%20proteins%2C%20thereby%20expanding%20the%20accessible%20protein%20space%20beyond%0Aevolutionary%20constraints.%20Our%20work%20integrates%20protein%20dynamics%20into%20generative%0Aprotein%20design%2C%20and%20establishes%20a%20direct%2C%20bidirectional%20link%20between%20sequence%0Aand%20vibrational%20behavior%2C%20unlocking%20new%20pathways%20for%20engineering%20biomolecules%0Awith%20tailored%20dynamical%20and%20functional%20properties.%20This%20framework%20holds%20broad%0Aimplications%20for%20the%20rational%20design%20of%20flexible%20enzymes%2C%20dynamic%20scaffolds%2C%0Aand%20biomaterials%2C%20paving%20the%20way%20toward%20dynamics-informed%20AI-driven%20protein%0Aengineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520End-to-End%2520De%2520Novo%2520Protein%2520Design%2520for%2520Tailored%2520Dynamics%2520Using%2520a%250A%2520%2520Language%2520Diffusion%2520Model%26entry.906535625%3DBo%2520Ni%2520and%2520Markus%2520J.%2520Buehler%26entry.1292438233%3D%2520%2520Proteins%2520are%2520dynamic%2520molecular%2520machines%2520whose%2520biological%2520functions%252C%2520spanning%250Aenzymatic%2520catalysis%252C%2520signal%2520transduction%252C%2520and%2520structural%2520adaptation%252C%2520are%250Aintrinsically%2520linked%2520to%2520their%2520motions.%2520Designing%2520proteins%2520with%2520targeted%2520dynamic%250Aproperties%252C%2520however%252C%2520remains%2520a%2520challenge%2520due%2520to%2520the%2520complex%252C%2520degenerate%250Arelationships%2520between%2520sequence%252C%2520structure%252C%2520and%2520molecular%2520motion.%2520Here%252C%2520we%250Aintroduce%2520VibeGen%252C%2520a%2520generative%2520AI%2520framework%2520that%2520enables%2520end-to-end%2520de%2520novo%250Aprotein%2520design%2520conditioned%2520on%2520normal%2520mode%2520vibrations.%2520VibeGen%2520employs%2520an%250Aagentic%2520dual-model%2520architecture%252C%2520comprising%2520a%2520protein%2520designer%2520that%2520generates%250Asequence%2520candidates%2520based%2520on%2520specified%2520vibrational%2520modes%2520and%2520a%2520protein%250Apredictor%2520that%2520evaluates%2520their%2520dynamic%2520accuracy.%2520This%2520approach%2520synergizes%250Adiversity%252C%2520accuracy%252C%2520and%2520novelty%2520during%2520the%2520design%2520process.%2520Via%2520full-atom%250Amolecular%2520simulations%2520as%2520direct%2520validation%252C%2520we%2520demonstrate%2520that%2520the%2520designed%250Aproteins%2520accurately%2520reproduce%2520the%2520prescribed%2520normal%2520mode%2520amplitudes%2520across%2520the%250Abackbone%2520while%2520adopting%2520various%2520stable%252C%2520functionally%2520relevant%2520structures.%250ANotably%252C%2520generated%2520sequences%2520are%2520de%2520novo%252C%2520exhibiting%2520no%2520significant%2520similarity%250Ato%2520natural%2520proteins%252C%2520thereby%2520expanding%2520the%2520accessible%2520protein%2520space%2520beyond%250Aevolutionary%2520constraints.%2520Our%2520work%2520integrates%2520protein%2520dynamics%2520into%2520generative%250Aprotein%2520design%252C%2520and%2520establishes%2520a%2520direct%252C%2520bidirectional%2520link%2520between%2520sequence%250Aand%2520vibrational%2520behavior%252C%2520unlocking%2520new%2520pathways%2520for%2520engineering%2520biomolecules%250Awith%2520tailored%2520dynamical%2520and%2520functional%2520properties.%2520This%2520framework%2520holds%2520broad%250Aimplications%2520for%2520the%2520rational%2520design%2520of%2520flexible%2520enzymes%252C%2520dynamic%2520scaffolds%252C%250Aand%2520biomaterials%252C%2520paving%2520the%2520way%2520toward%2520dynamics-informed%2520AI-driven%2520protein%250Aengineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20End-to-End%20De%20Novo%20Protein%20Design%20for%20Tailored%20Dynamics%20Using%20a%0A%20%20Language%20Diffusion%20Model&entry.906535625=Bo%20Ni%20and%20Markus%20J.%20Buehler&entry.1292438233=%20%20Proteins%20are%20dynamic%20molecular%20machines%20whose%20biological%20functions%2C%20spanning%0Aenzymatic%20catalysis%2C%20signal%20transduction%2C%20and%20structural%20adaptation%2C%20are%0Aintrinsically%20linked%20to%20their%20motions.%20Designing%20proteins%20with%20targeted%20dynamic%0Aproperties%2C%20however%2C%20remains%20a%20challenge%20due%20to%20the%20complex%2C%20degenerate%0Arelationships%20between%20sequence%2C%20structure%2C%20and%20molecular%20motion.%20Here%2C%20we%0Aintroduce%20VibeGen%2C%20a%20generative%20AI%20framework%20that%20enables%20end-to-end%20de%20novo%0Aprotein%20design%20conditioned%20on%20normal%20mode%20vibrations.%20VibeGen%20employs%20an%0Aagentic%20dual-model%20architecture%2C%20comprising%20a%20protein%20designer%20that%20generates%0Asequence%20candidates%20based%20on%20specified%20vibrational%20modes%20and%20a%20protein%0Apredictor%20that%20evaluates%20their%20dynamic%20accuracy.%20This%20approach%20synergizes%0Adiversity%2C%20accuracy%2C%20and%20novelty%20during%20the%20design%20process.%20Via%20full-atom%0Amolecular%20simulations%20as%20direct%20validation%2C%20we%20demonstrate%20that%20the%20designed%0Aproteins%20accurately%20reproduce%20the%20prescribed%20normal%20mode%20amplitudes%20across%20the%0Abackbone%20while%20adopting%20various%20stable%2C%20functionally%20relevant%20structures.%0ANotably%2C%20generated%20sequences%20are%20de%20novo%2C%20exhibiting%20no%20significant%20similarity%0Ato%20natural%20proteins%2C%20thereby%20expanding%20the%20accessible%20protein%20space%20beyond%0Aevolutionary%20constraints.%20Our%20work%20integrates%20protein%20dynamics%20into%20generative%0Aprotein%20design%2C%20and%20establishes%20a%20direct%2C%20bidirectional%20link%20between%20sequence%0Aand%20vibrational%20behavior%2C%20unlocking%20new%20pathways%20for%20engineering%20biomolecules%0Awith%20tailored%20dynamical%20and%20functional%20properties.%20This%20framework%20holds%20broad%0Aimplications%20for%20the%20rational%20design%20of%20flexible%20enzymes%2C%20dynamic%20scaffolds%2C%0Aand%20biomaterials%2C%20paving%20the%20way%20toward%20dynamics-informed%20AI-driven%20protein%0Aengineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10173v1&entry.124074799=Read"},
{"title": "A Regularized Newton Method for Nonconvex Optimization with Global and\n  Local Complexity Guarantees", "author": "Yuhao Zhou and Jintao Xu and Chenglong Bao and Chao Ding and Jun Zhu", "abstract": "  We consider the problem of finding an $\\epsilon$-stationary point of a\nnonconvex function with a Lipschitz continuous Hessian and propose a quadratic\nregularized Newton method incorporating a new class of regularizers constructed\nfrom the current and previous gradients. The method leverages a recently\ndeveloped linear conjugate gradient approach with a negative curvature monitor\nto solve the regularized Newton equation. Notably, our algorithm is adaptive,\nrequiring no prior knowledge of the Lipschitz constant of the Hessian, and\nachieves a global complexity of $O(\\epsilon^{-\\frac{3}{2}}) + \\tilde O(1)$ in\nterms of the second-order oracle calls, and $\\tilde O(\\epsilon^{-\\frac{7}{4}})$\nfor Hessian-vector products, respectively. Moreover, when the iterates converge\nto a point where the Hessian is positive definite, the method exhibits\nquadratic local convergence. Preliminary numerical results illustrate the\ncompetitiveness of our algorithm.\n", "link": "http://arxiv.org/abs/2502.04799v2", "date": "2025-02-14", "relevancy": 2.0716, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4305}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4188}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&body=Title%3A%20A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees%0AAuthor%3A%20Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%0Anonconvex%20function%20with%20a%20Lipschitz%20continuous%20Hessian%20and%20propose%20a%20quadratic%0Aregularized%20Newton%20method%20incorporating%20a%20new%20class%20of%20regularizers%20constructed%0Afrom%20the%20current%20and%20previous%20gradients.%20The%20method%20leverages%20a%20recently%0Adeveloped%20linear%20conjugate%20gradient%20approach%20with%20a%20negative%20curvature%20monitor%0Ato%20solve%20the%20regularized%20Newton%20equation.%20Notably%2C%20our%20algorithm%20is%20adaptive%2C%0Arequiring%20no%20prior%20knowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian%2C%20and%0Aachieves%20a%20global%20complexity%20of%20%24O%28%5Cepsilon%5E%7B-%5Cfrac%7B3%7D%7B2%7D%7D%29%20%2B%20%5Ctilde%20O%281%29%24%20in%0Aterms%20of%20the%20second-order%20oracle%20calls%2C%20and%20%24%5Ctilde%20O%28%5Cepsilon%5E%7B-%5Cfrac%7B7%7D%7B4%7D%7D%29%24%0Afor%20Hessian-vector%20products%2C%20respectively.%20Moreover%2C%20when%20the%20iterates%20converge%0Ato%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%20method%20exhibits%0Aquadratic%20local%20convergence.%20Preliminary%20numerical%20results%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Regularized%2520Newton%2520Method%2520for%2520Nonconvex%2520Optimization%2520with%2520Global%2520and%250A%2520%2520Local%2520Complexity%2520Guarantees%26entry.906535625%3DYuhao%2520Zhou%2520and%2520Jintao%2520Xu%2520and%2520Chenglong%2520Bao%2520and%2520Chao%2520Ding%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520finding%2520an%2520%2524%255Cepsilon%2524-stationary%2520point%2520of%2520a%250Anonconvex%2520function%2520with%2520a%2520Lipschitz%2520continuous%2520Hessian%2520and%2520propose%2520a%2520quadratic%250Aregularized%2520Newton%2520method%2520incorporating%2520a%2520new%2520class%2520of%2520regularizers%2520constructed%250Afrom%2520the%2520current%2520and%2520previous%2520gradients.%2520The%2520method%2520leverages%2520a%2520recently%250Adeveloped%2520linear%2520conjugate%2520gradient%2520approach%2520with%2520a%2520negative%2520curvature%2520monitor%250Ato%2520solve%2520the%2520regularized%2520Newton%2520equation.%2520Notably%252C%2520our%2520algorithm%2520is%2520adaptive%252C%250Arequiring%2520no%2520prior%2520knowledge%2520of%2520the%2520Lipschitz%2520constant%2520of%2520the%2520Hessian%252C%2520and%250Aachieves%2520a%2520global%2520complexity%2520of%2520%2524O%2528%255Cepsilon%255E%257B-%255Cfrac%257B3%257D%257B2%257D%257D%2529%2520%252B%2520%255Ctilde%2520O%25281%2529%2524%2520in%250Aterms%2520of%2520the%2520second-order%2520oracle%2520calls%252C%2520and%2520%2524%255Ctilde%2520O%2528%255Cepsilon%255E%257B-%255Cfrac%257B7%257D%257B4%257D%257D%2529%2524%250Afor%2520Hessian-vector%2520products%252C%2520respectively.%2520Moreover%252C%2520when%2520the%2520iterates%2520converge%250Ato%2520a%2520point%2520where%2520the%2520Hessian%2520is%2520positive%2520definite%252C%2520the%2520method%2520exhibits%250Aquadratic%2520local%2520convergence.%2520Preliminary%2520numerical%2520results%2520illustrate%2520the%250Acompetitiveness%2520of%2520our%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Regularized%20Newton%20Method%20for%20Nonconvex%20Optimization%20with%20Global%20and%0A%20%20Local%20Complexity%20Guarantees&entry.906535625=Yuhao%20Zhou%20and%20Jintao%20Xu%20and%20Chenglong%20Bao%20and%20Chao%20Ding%20and%20Jun%20Zhu&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20finding%20an%20%24%5Cepsilon%24-stationary%20point%20of%20a%0Anonconvex%20function%20with%20a%20Lipschitz%20continuous%20Hessian%20and%20propose%20a%20quadratic%0Aregularized%20Newton%20method%20incorporating%20a%20new%20class%20of%20regularizers%20constructed%0Afrom%20the%20current%20and%20previous%20gradients.%20The%20method%20leverages%20a%20recently%0Adeveloped%20linear%20conjugate%20gradient%20approach%20with%20a%20negative%20curvature%20monitor%0Ato%20solve%20the%20regularized%20Newton%20equation.%20Notably%2C%20our%20algorithm%20is%20adaptive%2C%0Arequiring%20no%20prior%20knowledge%20of%20the%20Lipschitz%20constant%20of%20the%20Hessian%2C%20and%0Aachieves%20a%20global%20complexity%20of%20%24O%28%5Cepsilon%5E%7B-%5Cfrac%7B3%7D%7B2%7D%7D%29%20%2B%20%5Ctilde%20O%281%29%24%20in%0Aterms%20of%20the%20second-order%20oracle%20calls%2C%20and%20%24%5Ctilde%20O%28%5Cepsilon%5E%7B-%5Cfrac%7B7%7D%7B4%7D%7D%29%24%0Afor%20Hessian-vector%20products%2C%20respectively.%20Moreover%2C%20when%20the%20iterates%20converge%0Ato%20a%20point%20where%20the%20Hessian%20is%20positive%20definite%2C%20the%20method%20exhibits%0Aquadratic%20local%20convergence.%20Preliminary%20numerical%20results%20illustrate%20the%0Acompetitiveness%20of%20our%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04799v2&entry.124074799=Read"},
{"title": "Representation Learning on Out of Distribution in Tabular Data", "author": "Achmad Ginanjar and Xue Li and Priyanka Singh and Wen Hua", "abstract": "  The open-world assumption in model development suggests that a model might\nlack sufficient information to adequately handle data that is entirely distinct\nor out of distribution (OOD). While deep learning methods have shown promising\nresults in handling OOD data through generalization techniques, they often\nrequire specialized hardware that may not be accessible to all users. We\npresent TCL, a lightweight yet effective solution that operates efficiently on\nstandard CPU hardware. Our approach adapts contrastive learning principles\nspecifically for tabular data structures, incorporating full matrix\naugmentation and simplified loss calculation. Through comprehensive experiments\nacross 10 diverse datasets, we demonstrate that TCL outperforms existing\nmodels, including FT-Transformer and ResNet, particularly in classification\ntasks, while maintaining competitive performance in regression problems. TCL\nachieves these results with significantly reduced computational requirements,\nmaking it accessible to users with limited hardware capabilities. This study\nalso provides practical guidance for detecting and evaluating OOD data through\nstraightforward experiments and visualizations. Our findings show that TCL\noffers a promising balance between performance and efficiency in handling OOD\nprediction tasks, which is particularly beneficial for general machine learning\npractitioners working with computational constraints.\n", "link": "http://arxiv.org/abs/2502.10095v1", "date": "2025-02-14", "relevancy": 2.0706, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5302}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation%20Learning%20on%20Out%20of%20Distribution%20in%20Tabular%20Data&body=Title%3A%20Representation%20Learning%20on%20Out%20of%20Distribution%20in%20Tabular%20Data%0AAuthor%3A%20Achmad%20Ginanjar%20and%20Xue%20Li%20and%20Priyanka%20Singh%20and%20Wen%20Hua%0AAbstract%3A%20%20%20The%20open-world%20assumption%20in%20model%20development%20suggests%20that%20a%20model%20might%0Alack%20sufficient%20information%20to%20adequately%20handle%20data%20that%20is%20entirely%20distinct%0Aor%20out%20of%20distribution%20%28OOD%29.%20While%20deep%20learning%20methods%20have%20shown%20promising%0Aresults%20in%20handling%20OOD%20data%20through%20generalization%20techniques%2C%20they%20often%0Arequire%20specialized%20hardware%20that%20may%20not%20be%20accessible%20to%20all%20users.%20We%0Apresent%20TCL%2C%20a%20lightweight%20yet%20effective%20solution%20that%20operates%20efficiently%20on%0Astandard%20CPU%20hardware.%20Our%20approach%20adapts%20contrastive%20learning%20principles%0Aspecifically%20for%20tabular%20data%20structures%2C%20incorporating%20full%20matrix%0Aaugmentation%20and%20simplified%20loss%20calculation.%20Through%20comprehensive%20experiments%0Aacross%2010%20diverse%20datasets%2C%20we%20demonstrate%20that%20TCL%20outperforms%20existing%0Amodels%2C%20including%20FT-Transformer%20and%20ResNet%2C%20particularly%20in%20classification%0Atasks%2C%20while%20maintaining%20competitive%20performance%20in%20regression%20problems.%20TCL%0Aachieves%20these%20results%20with%20significantly%20reduced%20computational%20requirements%2C%0Amaking%20it%20accessible%20to%20users%20with%20limited%20hardware%20capabilities.%20This%20study%0Aalso%20provides%20practical%20guidance%20for%20detecting%20and%20evaluating%20OOD%20data%20through%0Astraightforward%20experiments%20and%20visualizations.%20Our%20findings%20show%20that%20TCL%0Aoffers%20a%20promising%20balance%20between%20performance%20and%20efficiency%20in%20handling%20OOD%0Aprediction%20tasks%2C%20which%20is%20particularly%20beneficial%20for%20general%20machine%20learning%0Apractitioners%20working%20with%20computational%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation%2520Learning%2520on%2520Out%2520of%2520Distribution%2520in%2520Tabular%2520Data%26entry.906535625%3DAchmad%2520Ginanjar%2520and%2520Xue%2520Li%2520and%2520Priyanka%2520Singh%2520and%2520Wen%2520Hua%26entry.1292438233%3D%2520%2520The%2520open-world%2520assumption%2520in%2520model%2520development%2520suggests%2520that%2520a%2520model%2520might%250Alack%2520sufficient%2520information%2520to%2520adequately%2520handle%2520data%2520that%2520is%2520entirely%2520distinct%250Aor%2520out%2520of%2520distribution%2520%2528OOD%2529.%2520While%2520deep%2520learning%2520methods%2520have%2520shown%2520promising%250Aresults%2520in%2520handling%2520OOD%2520data%2520through%2520generalization%2520techniques%252C%2520they%2520often%250Arequire%2520specialized%2520hardware%2520that%2520may%2520not%2520be%2520accessible%2520to%2520all%2520users.%2520We%250Apresent%2520TCL%252C%2520a%2520lightweight%2520yet%2520effective%2520solution%2520that%2520operates%2520efficiently%2520on%250Astandard%2520CPU%2520hardware.%2520Our%2520approach%2520adapts%2520contrastive%2520learning%2520principles%250Aspecifically%2520for%2520tabular%2520data%2520structures%252C%2520incorporating%2520full%2520matrix%250Aaugmentation%2520and%2520simplified%2520loss%2520calculation.%2520Through%2520comprehensive%2520experiments%250Aacross%252010%2520diverse%2520datasets%252C%2520we%2520demonstrate%2520that%2520TCL%2520outperforms%2520existing%250Amodels%252C%2520including%2520FT-Transformer%2520and%2520ResNet%252C%2520particularly%2520in%2520classification%250Atasks%252C%2520while%2520maintaining%2520competitive%2520performance%2520in%2520regression%2520problems.%2520TCL%250Aachieves%2520these%2520results%2520with%2520significantly%2520reduced%2520computational%2520requirements%252C%250Amaking%2520it%2520accessible%2520to%2520users%2520with%2520limited%2520hardware%2520capabilities.%2520This%2520study%250Aalso%2520provides%2520practical%2520guidance%2520for%2520detecting%2520and%2520evaluating%2520OOD%2520data%2520through%250Astraightforward%2520experiments%2520and%2520visualizations.%2520Our%2520findings%2520show%2520that%2520TCL%250Aoffers%2520a%2520promising%2520balance%2520between%2520performance%2520and%2520efficiency%2520in%2520handling%2520OOD%250Aprediction%2520tasks%252C%2520which%2520is%2520particularly%2520beneficial%2520for%2520general%2520machine%2520learning%250Apractitioners%2520working%2520with%2520computational%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation%20Learning%20on%20Out%20of%20Distribution%20in%20Tabular%20Data&entry.906535625=Achmad%20Ginanjar%20and%20Xue%20Li%20and%20Priyanka%20Singh%20and%20Wen%20Hua&entry.1292438233=%20%20The%20open-world%20assumption%20in%20model%20development%20suggests%20that%20a%20model%20might%0Alack%20sufficient%20information%20to%20adequately%20handle%20data%20that%20is%20entirely%20distinct%0Aor%20out%20of%20distribution%20%28OOD%29.%20While%20deep%20learning%20methods%20have%20shown%20promising%0Aresults%20in%20handling%20OOD%20data%20through%20generalization%20techniques%2C%20they%20often%0Arequire%20specialized%20hardware%20that%20may%20not%20be%20accessible%20to%20all%20users.%20We%0Apresent%20TCL%2C%20a%20lightweight%20yet%20effective%20solution%20that%20operates%20efficiently%20on%0Astandard%20CPU%20hardware.%20Our%20approach%20adapts%20contrastive%20learning%20principles%0Aspecifically%20for%20tabular%20data%20structures%2C%20incorporating%20full%20matrix%0Aaugmentation%20and%20simplified%20loss%20calculation.%20Through%20comprehensive%20experiments%0Aacross%2010%20diverse%20datasets%2C%20we%20demonstrate%20that%20TCL%20outperforms%20existing%0Amodels%2C%20including%20FT-Transformer%20and%20ResNet%2C%20particularly%20in%20classification%0Atasks%2C%20while%20maintaining%20competitive%20performance%20in%20regression%20problems.%20TCL%0Aachieves%20these%20results%20with%20significantly%20reduced%20computational%20requirements%2C%0Amaking%20it%20accessible%20to%20users%20with%20limited%20hardware%20capabilities.%20This%20study%0Aalso%20provides%20practical%20guidance%20for%20detecting%20and%20evaluating%20OOD%20data%20through%0Astraightforward%20experiments%20and%20visualizations.%20Our%20findings%20show%20that%20TCL%0Aoffers%20a%20promising%20balance%20between%20performance%20and%20efficiency%20in%20handling%20OOD%0Aprediction%20tasks%2C%20which%20is%20particularly%20beneficial%20for%20general%20machine%20learning%0Apractitioners%20working%20with%20computational%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10095v1&entry.124074799=Read"},
{"title": "A Critical Look At Tokenwise Reward-Guided Text Generation", "author": "Ahmad Rashid and Ruotian Wu and Julia Grosse and Agustinus Kristiadi and Pascal Poupart", "abstract": "  Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.\n", "link": "http://arxiv.org/abs/2406.07780v2", "date": "2025-02-14", "relevancy": 2.0683, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5344}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5234}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Critical%20Look%20At%20Tokenwise%20Reward-Guided%20Text%20Generation&body=Title%3A%20A%20Critical%20Look%20At%20Tokenwise%20Reward-Guided%20Text%20Generation%0AAuthor%3A%20Ahmad%20Rashid%20and%20Ruotian%20Wu%20and%20Julia%20Grosse%20and%20Agustinus%20Kristiadi%20and%20Pascal%20Poupart%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20improved%20by%20aligning%20with%20human%0Apreferences%20through%20fine-tuning%20--%20the%20so-called%20reinforcement%20learning%20from%0Ahuman%20feedback%20%28RLHF%29.%20However%2C%20the%20cost%20of%20fine-tuning%20an%20LLM%20is%20prohibitive%0Afor%20many%20users.%20Due%20to%20their%20ability%20to%20bypass%20LLM%20fine-tuning%2C%20prediction-time%0Atokenwise%20reward-guided%20text%20generation%20%28RGTG%29%20methods%20have%20recently%20been%0Aproposed.%20They%20use%20a%20reward%20model%20trained%20on%20full%20sequences%20to%20score%20partial%0Asequences%20during%20decoding%20in%20a%20bid%20to%20steer%20the%20generation%20towards%20sequences%0Awith%20high%20rewards.%20However%2C%20these%20methods%20have%20so%20far%20been%20only%20heuristically%0Amotivated%20and%20poorly%20analyzed.%20In%20this%20work%2C%20we%20show%20that%20reward%20models%20trained%0Aon%20full%20sequences%20are%20not%20compatible%20with%20scoring%20partial%20sequences.%20To%0Aalleviate%20this%20issue%2C%20we%20propose%20to%20train%20a%20Bradley-Terry%20reward%20model%20on%0Apartial%20sequences%20explicitly%2C%20and%20autoregressively%20sample%20from%20the%20implied%0Atokenwise%20policy%20during%20decoding%20time.%20We%20study%20the%20properties%20of%20this%20reward%0Amodel%20and%20the%20resulting%20policy%3A%20we%20show%20that%20this%20policy%20is%20proportional%20to%20the%0Aratio%20of%20two%20distinct%20RLHF%20policies.%20Our%20simple%20approach%20outperforms%20previous%0ARGTG%20methods%20and%20performs%20similarly%20to%20strong%20offline%20baselines%20without%0Alarge-scale%20LLM%20finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Critical%2520Look%2520At%2520Tokenwise%2520Reward-Guided%2520Text%2520Generation%26entry.906535625%3DAhmad%2520Rashid%2520and%2520Ruotian%2520Wu%2520and%2520Julia%2520Grosse%2520and%2520Agustinus%2520Kristiadi%2520and%2520Pascal%2520Poupart%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%2520improved%2520by%2520aligning%2520with%2520human%250Apreferences%2520through%2520fine-tuning%2520--%2520the%2520so-called%2520reinforcement%2520learning%2520from%250Ahuman%2520feedback%2520%2528RLHF%2529.%2520However%252C%2520the%2520cost%2520of%2520fine-tuning%2520an%2520LLM%2520is%2520prohibitive%250Afor%2520many%2520users.%2520Due%2520to%2520their%2520ability%2520to%2520bypass%2520LLM%2520fine-tuning%252C%2520prediction-time%250Atokenwise%2520reward-guided%2520text%2520generation%2520%2528RGTG%2529%2520methods%2520have%2520recently%2520been%250Aproposed.%2520They%2520use%2520a%2520reward%2520model%2520trained%2520on%2520full%2520sequences%2520to%2520score%2520partial%250Asequences%2520during%2520decoding%2520in%2520a%2520bid%2520to%2520steer%2520the%2520generation%2520towards%2520sequences%250Awith%2520high%2520rewards.%2520However%252C%2520these%2520methods%2520have%2520so%2520far%2520been%2520only%2520heuristically%250Amotivated%2520and%2520poorly%2520analyzed.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520reward%2520models%2520trained%250Aon%2520full%2520sequences%2520are%2520not%2520compatible%2520with%2520scoring%2520partial%2520sequences.%2520To%250Aalleviate%2520this%2520issue%252C%2520we%2520propose%2520to%2520train%2520a%2520Bradley-Terry%2520reward%2520model%2520on%250Apartial%2520sequences%2520explicitly%252C%2520and%2520autoregressively%2520sample%2520from%2520the%2520implied%250Atokenwise%2520policy%2520during%2520decoding%2520time.%2520We%2520study%2520the%2520properties%2520of%2520this%2520reward%250Amodel%2520and%2520the%2520resulting%2520policy%253A%2520we%2520show%2520that%2520this%2520policy%2520is%2520proportional%2520to%2520the%250Aratio%2520of%2520two%2520distinct%2520RLHF%2520policies.%2520Our%2520simple%2520approach%2520outperforms%2520previous%250ARGTG%2520methods%2520and%2520performs%2520similarly%2520to%2520strong%2520offline%2520baselines%2520without%250Alarge-scale%2520LLM%2520finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Critical%20Look%20At%20Tokenwise%20Reward-Guided%20Text%20Generation&entry.906535625=Ahmad%20Rashid%20and%20Ruotian%20Wu%20and%20Julia%20Grosse%20and%20Agustinus%20Kristiadi%20and%20Pascal%20Poupart&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20improved%20by%20aligning%20with%20human%0Apreferences%20through%20fine-tuning%20--%20the%20so-called%20reinforcement%20learning%20from%0Ahuman%20feedback%20%28RLHF%29.%20However%2C%20the%20cost%20of%20fine-tuning%20an%20LLM%20is%20prohibitive%0Afor%20many%20users.%20Due%20to%20their%20ability%20to%20bypass%20LLM%20fine-tuning%2C%20prediction-time%0Atokenwise%20reward-guided%20text%20generation%20%28RGTG%29%20methods%20have%20recently%20been%0Aproposed.%20They%20use%20a%20reward%20model%20trained%20on%20full%20sequences%20to%20score%20partial%0Asequences%20during%20decoding%20in%20a%20bid%20to%20steer%20the%20generation%20towards%20sequences%0Awith%20high%20rewards.%20However%2C%20these%20methods%20have%20so%20far%20been%20only%20heuristically%0Amotivated%20and%20poorly%20analyzed.%20In%20this%20work%2C%20we%20show%20that%20reward%20models%20trained%0Aon%20full%20sequences%20are%20not%20compatible%20with%20scoring%20partial%20sequences.%20To%0Aalleviate%20this%20issue%2C%20we%20propose%20to%20train%20a%20Bradley-Terry%20reward%20model%20on%0Apartial%20sequences%20explicitly%2C%20and%20autoregressively%20sample%20from%20the%20implied%0Atokenwise%20policy%20during%20decoding%20time.%20We%20study%20the%20properties%20of%20this%20reward%0Amodel%20and%20the%20resulting%20policy%3A%20we%20show%20that%20this%20policy%20is%20proportional%20to%20the%0Aratio%20of%20two%20distinct%20RLHF%20policies.%20Our%20simple%20approach%20outperforms%20previous%0ARGTG%20methods%20and%20performs%20similarly%20to%20strong%20offline%20baselines%20without%0Alarge-scale%20LLM%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07780v2&entry.124074799=Read"},
{"title": "Are Large Language Models the future crowd workers of Linguistics?", "author": "Iris Ferrazzo", "abstract": "  Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.\n", "link": "http://arxiv.org/abs/2502.10266v1", "date": "2025-02-14", "relevancy": 2.0628, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20the%20future%20crowd%20workers%20of%20Linguistics%3F&body=Title%3A%20Are%20Large%20Language%20Models%20the%20future%20crowd%20workers%20of%20Linguistics%3F%0AAuthor%3A%20Iris%20Ferrazzo%0AAbstract%3A%20%20%20Data%20elicitation%20from%20human%20participants%20is%20one%20of%20the%20core%20data%20collection%0Astrategies%20used%20in%20empirical%20linguistic%20research.%20The%20amount%20of%20participants%20in%0Asuch%20studies%20may%20vary%20considerably%2C%20ranging%20from%20a%20handful%20to%20crowdsourcing%0Adimensions.%20Even%20if%20they%20provide%20resourceful%20extensive%20data%2C%20both%20of%20these%0Asettings%20come%20alongside%20many%20disadvantages%2C%20such%20as%20low%20control%20of%0Aparticipants%27%20attention%20during%20task%20completion%2C%20precarious%20working%20conditions%0Ain%20crowdsourcing%20environments%2C%20and%20time-consuming%20experimental%20designs.%20For%0Athese%20reasons%2C%20this%20research%20aims%20to%20answer%20the%20question%20of%20whether%20Large%0ALanguage%20Models%20%28LLMs%29%20may%20overcome%20those%20obstacles%20if%20included%20in%20empirical%0Alinguistic%20pipelines.%20Two%20reproduction%20case%20studies%20are%20conducted%20to%20gain%0Aclarity%20into%20this%20matter%3A%20Cruz%20%282023%29%20and%20Lombard%20et%20al.%20%282021%29.%20The%20two%20forced%0Aelicitation%20tasks%2C%20originally%20designed%20for%20human%20participants%2C%20are%20reproduced%0Ain%20the%20proposed%20framework%20with%20the%20help%20of%20OpenAI%27s%20GPT-4o-mini%20model.%20Its%0Aperformance%20with%20our%20zero-shot%20prompting%20baseline%20shows%20the%20effectiveness%20and%0Ahigh%20versatility%20of%20LLMs%2C%20that%20tend%20to%20outperform%20human%20informants%20in%0Alinguistic%20tasks.%20The%20findings%20of%20the%20second%20replication%20further%20highlight%20the%0Aneed%20to%20explore%20additional%20prompting%20techniques%2C%20such%20as%20Chain-of-Thought%20%28CoT%29%0Aprompting%2C%20which%2C%20in%20a%20second%20follow-up%20experiment%2C%20demonstrates%20higher%0Aalignment%20to%20human%20performance%20on%20both%20critical%20and%20filler%20items.%20Given%20the%0Alimited%20scale%20of%20this%20study%2C%20it%20is%20worthwhile%20to%20further%20explore%20the%0Aperformance%20of%20LLMs%20in%20empirical%20Linguistics%20and%20in%20other%20future%20applications%0Ain%20the%20humanities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520the%2520future%2520crowd%2520workers%2520of%2520Linguistics%253F%26entry.906535625%3DIris%2520Ferrazzo%26entry.1292438233%3D%2520%2520Data%2520elicitation%2520from%2520human%2520participants%2520is%2520one%2520of%2520the%2520core%2520data%2520collection%250Astrategies%2520used%2520in%2520empirical%2520linguistic%2520research.%2520The%2520amount%2520of%2520participants%2520in%250Asuch%2520studies%2520may%2520vary%2520considerably%252C%2520ranging%2520from%2520a%2520handful%2520to%2520crowdsourcing%250Adimensions.%2520Even%2520if%2520they%2520provide%2520resourceful%2520extensive%2520data%252C%2520both%2520of%2520these%250Asettings%2520come%2520alongside%2520many%2520disadvantages%252C%2520such%2520as%2520low%2520control%2520of%250Aparticipants%2527%2520attention%2520during%2520task%2520completion%252C%2520precarious%2520working%2520conditions%250Ain%2520crowdsourcing%2520environments%252C%2520and%2520time-consuming%2520experimental%2520designs.%2520For%250Athese%2520reasons%252C%2520this%2520research%2520aims%2520to%2520answer%2520the%2520question%2520of%2520whether%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520may%2520overcome%2520those%2520obstacles%2520if%2520included%2520in%2520empirical%250Alinguistic%2520pipelines.%2520Two%2520reproduction%2520case%2520studies%2520are%2520conducted%2520to%2520gain%250Aclarity%2520into%2520this%2520matter%253A%2520Cruz%2520%25282023%2529%2520and%2520Lombard%2520et%2520al.%2520%25282021%2529.%2520The%2520two%2520forced%250Aelicitation%2520tasks%252C%2520originally%2520designed%2520for%2520human%2520participants%252C%2520are%2520reproduced%250Ain%2520the%2520proposed%2520framework%2520with%2520the%2520help%2520of%2520OpenAI%2527s%2520GPT-4o-mini%2520model.%2520Its%250Aperformance%2520with%2520our%2520zero-shot%2520prompting%2520baseline%2520shows%2520the%2520effectiveness%2520and%250Ahigh%2520versatility%2520of%2520LLMs%252C%2520that%2520tend%2520to%2520outperform%2520human%2520informants%2520in%250Alinguistic%2520tasks.%2520The%2520findings%2520of%2520the%2520second%2520replication%2520further%2520highlight%2520the%250Aneed%2520to%2520explore%2520additional%2520prompting%2520techniques%252C%2520such%2520as%2520Chain-of-Thought%2520%2528CoT%2529%250Aprompting%252C%2520which%252C%2520in%2520a%2520second%2520follow-up%2520experiment%252C%2520demonstrates%2520higher%250Aalignment%2520to%2520human%2520performance%2520on%2520both%2520critical%2520and%2520filler%2520items.%2520Given%2520the%250Alimited%2520scale%2520of%2520this%2520study%252C%2520it%2520is%2520worthwhile%2520to%2520further%2520explore%2520the%250Aperformance%2520of%2520LLMs%2520in%2520empirical%2520Linguistics%2520and%2520in%2520other%2520future%2520applications%250Ain%2520the%2520humanities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20the%20future%20crowd%20workers%20of%20Linguistics%3F&entry.906535625=Iris%20Ferrazzo&entry.1292438233=%20%20Data%20elicitation%20from%20human%20participants%20is%20one%20of%20the%20core%20data%20collection%0Astrategies%20used%20in%20empirical%20linguistic%20research.%20The%20amount%20of%20participants%20in%0Asuch%20studies%20may%20vary%20considerably%2C%20ranging%20from%20a%20handful%20to%20crowdsourcing%0Adimensions.%20Even%20if%20they%20provide%20resourceful%20extensive%20data%2C%20both%20of%20these%0Asettings%20come%20alongside%20many%20disadvantages%2C%20such%20as%20low%20control%20of%0Aparticipants%27%20attention%20during%20task%20completion%2C%20precarious%20working%20conditions%0Ain%20crowdsourcing%20environments%2C%20and%20time-consuming%20experimental%20designs.%20For%0Athese%20reasons%2C%20this%20research%20aims%20to%20answer%20the%20question%20of%20whether%20Large%0ALanguage%20Models%20%28LLMs%29%20may%20overcome%20those%20obstacles%20if%20included%20in%20empirical%0Alinguistic%20pipelines.%20Two%20reproduction%20case%20studies%20are%20conducted%20to%20gain%0Aclarity%20into%20this%20matter%3A%20Cruz%20%282023%29%20and%20Lombard%20et%20al.%20%282021%29.%20The%20two%20forced%0Aelicitation%20tasks%2C%20originally%20designed%20for%20human%20participants%2C%20are%20reproduced%0Ain%20the%20proposed%20framework%20with%20the%20help%20of%20OpenAI%27s%20GPT-4o-mini%20model.%20Its%0Aperformance%20with%20our%20zero-shot%20prompting%20baseline%20shows%20the%20effectiveness%20and%0Ahigh%20versatility%20of%20LLMs%2C%20that%20tend%20to%20outperform%20human%20informants%20in%0Alinguistic%20tasks.%20The%20findings%20of%20the%20second%20replication%20further%20highlight%20the%0Aneed%20to%20explore%20additional%20prompting%20techniques%2C%20such%20as%20Chain-of-Thought%20%28CoT%29%0Aprompting%2C%20which%2C%20in%20a%20second%20follow-up%20experiment%2C%20demonstrates%20higher%0Aalignment%20to%20human%20performance%20on%20both%20critical%20and%20filler%20items.%20Given%20the%0Alimited%20scale%20of%20this%20study%2C%20it%20is%20worthwhile%20to%20further%20explore%20the%0Aperformance%20of%20LLMs%20in%20empirical%20Linguistics%20and%20in%20other%20future%20applications%0Ain%20the%20humanities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10266v1&entry.124074799=Read"},
{"title": "HaSPeR: An Image Repository for Hand Shadow Puppet Recognition", "author": "Syed Rifat Raiyan and Zibran Zarif Amio and Sabbir Ahmed", "abstract": "  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.\n", "link": "http://arxiv.org/abs/2408.10360v5", "date": "2025-02-14", "relevancy": 2.0579, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5206}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5171}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HaSPeR%3A%20An%20Image%20Repository%20for%20Hand%20Shadow%20Puppet%20Recognition&body=Title%3A%20HaSPeR%3A%20An%20Image%20Repository%20for%20Hand%20Shadow%20Puppet%20Recognition%0AAuthor%3A%20Syed%20Rifat%20Raiyan%20and%20Zibran%20Zarif%20Amio%20and%20Sabbir%20Ahmed%0AAbstract%3A%20%20%20Hand%20shadow%20puppetry%2C%20also%20known%20as%20shadowgraphy%20or%20ombromanie%2C%20is%20a%20form%20of%0Atheatrical%20art%20and%20storytelling%20where%20hand%20shadows%20are%20projected%20onto%20flat%0Asurfaces%20to%20create%20illusions%20of%20living%20creatures.%20The%20skilled%20performers%20create%0Athese%20silhouettes%20by%20hand%20positioning%2C%20finger%20movements%2C%20and%20dexterous%20gestures%0Ato%20resemble%20shadows%20of%20animals%20and%20objects.%20Due%20to%20the%20lack%20of%20practitioners%0Aand%20a%20seismic%20shift%20in%20people%27s%20entertainment%20standards%2C%20this%20art%20form%20is%20on%0Athe%20verge%20of%20extinction.%20To%20facilitate%20its%20preservation%20and%20proliferate%20it%20to%20a%0Awider%20audience%2C%20we%20introduce%20%24%7B%5Crm%20H%7B%5Csmall%20A%7DSP%7B%5Csmall%20E%7DR%7D%24%2C%20a%20novel%20dataset%0Aconsisting%20of%2015%2C000%20images%20of%20hand%20shadow%20puppets%20across%2015%20classes%20extracted%0Afrom%20both%20professional%20and%20amateur%20hand%20shadow%20puppeteer%20clips.%20We%20provide%20a%0Adetailed%20statistical%20analysis%20of%20the%20dataset%20and%20employ%20a%20range%20of%20pretrained%0Aimage%20classification%20models%20to%20establish%20baselines.%20Our%20findings%20show%20a%0Asubstantial%20performance%20superiority%20of%20skip-connected%20convolutional%20models%20over%0Aattention-based%20transformer%20architectures.%20We%20also%20find%20that%20lightweight%0Amodels%2C%20such%20as%20MobileNetV2%2C%20suited%20for%20mobile%20applications%20and%20embedded%0Adevices%2C%20perform%20comparatively%20well.%20We%20surmise%20that%20such%20low-latency%0Aarchitectures%20can%20be%20useful%20in%20developing%20ombromanie%20teaching%20tools%2C%20and%20we%0Acreate%20a%20prototype%20application%20to%20explore%20this%20surmission.%20Keeping%20the%0Abest-performing%20model%20ResNet34%20under%20the%20limelight%2C%20we%20conduct%20comprehensive%0Afeature-spatial%2C%20explainability%2C%20and%20error%20analyses%20to%20gain%20insights%20into%20its%0Adecision-making%20process.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Adocumented%20dataset%20and%20research%20endeavor%20to%20preserve%20this%20dying%20art%20for%20future%0Agenerations%2C%20with%20computer%20vision%20approaches.%20Our%20code%20and%20data%20will%20be%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10360v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaSPeR%253A%2520An%2520Image%2520Repository%2520for%2520Hand%2520Shadow%2520Puppet%2520Recognition%26entry.906535625%3DSyed%2520Rifat%2520Raiyan%2520and%2520Zibran%2520Zarif%2520Amio%2520and%2520Sabbir%2520Ahmed%26entry.1292438233%3D%2520%2520Hand%2520shadow%2520puppetry%252C%2520also%2520known%2520as%2520shadowgraphy%2520or%2520ombromanie%252C%2520is%2520a%2520form%2520of%250Atheatrical%2520art%2520and%2520storytelling%2520where%2520hand%2520shadows%2520are%2520projected%2520onto%2520flat%250Asurfaces%2520to%2520create%2520illusions%2520of%2520living%2520creatures.%2520The%2520skilled%2520performers%2520create%250Athese%2520silhouettes%2520by%2520hand%2520positioning%252C%2520finger%2520movements%252C%2520and%2520dexterous%2520gestures%250Ato%2520resemble%2520shadows%2520of%2520animals%2520and%2520objects.%2520Due%2520to%2520the%2520lack%2520of%2520practitioners%250Aand%2520a%2520seismic%2520shift%2520in%2520people%2527s%2520entertainment%2520standards%252C%2520this%2520art%2520form%2520is%2520on%250Athe%2520verge%2520of%2520extinction.%2520To%2520facilitate%2520its%2520preservation%2520and%2520proliferate%2520it%2520to%2520a%250Awider%2520audience%252C%2520we%2520introduce%2520%2524%257B%255Crm%2520H%257B%255Csmall%2520A%257DSP%257B%255Csmall%2520E%257DR%257D%2524%252C%2520a%2520novel%2520dataset%250Aconsisting%2520of%252015%252C000%2520images%2520of%2520hand%2520shadow%2520puppets%2520across%252015%2520classes%2520extracted%250Afrom%2520both%2520professional%2520and%2520amateur%2520hand%2520shadow%2520puppeteer%2520clips.%2520We%2520provide%2520a%250Adetailed%2520statistical%2520analysis%2520of%2520the%2520dataset%2520and%2520employ%2520a%2520range%2520of%2520pretrained%250Aimage%2520classification%2520models%2520to%2520establish%2520baselines.%2520Our%2520findings%2520show%2520a%250Asubstantial%2520performance%2520superiority%2520of%2520skip-connected%2520convolutional%2520models%2520over%250Aattention-based%2520transformer%2520architectures.%2520We%2520also%2520find%2520that%2520lightweight%250Amodels%252C%2520such%2520as%2520MobileNetV2%252C%2520suited%2520for%2520mobile%2520applications%2520and%2520embedded%250Adevices%252C%2520perform%2520comparatively%2520well.%2520We%2520surmise%2520that%2520such%2520low-latency%250Aarchitectures%2520can%2520be%2520useful%2520in%2520developing%2520ombromanie%2520teaching%2520tools%252C%2520and%2520we%250Acreate%2520a%2520prototype%2520application%2520to%2520explore%2520this%2520surmission.%2520Keeping%2520the%250Abest-performing%2520model%2520ResNet34%2520under%2520the%2520limelight%252C%2520we%2520conduct%2520comprehensive%250Afeature-spatial%252C%2520explainability%252C%2520and%2520error%2520analyses%2520to%2520gain%2520insights%2520into%2520its%250Adecision-making%2520process.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Adocumented%2520dataset%2520and%2520research%2520endeavor%2520to%2520preserve%2520this%2520dying%2520art%2520for%2520future%250Agenerations%252C%2520with%2520computer%2520vision%2520approaches.%2520Our%2520code%2520and%2520data%2520will%2520be%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10360v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HaSPeR%3A%20An%20Image%20Repository%20for%20Hand%20Shadow%20Puppet%20Recognition&entry.906535625=Syed%20Rifat%20Raiyan%20and%20Zibran%20Zarif%20Amio%20and%20Sabbir%20Ahmed&entry.1292438233=%20%20Hand%20shadow%20puppetry%2C%20also%20known%20as%20shadowgraphy%20or%20ombromanie%2C%20is%20a%20form%20of%0Atheatrical%20art%20and%20storytelling%20where%20hand%20shadows%20are%20projected%20onto%20flat%0Asurfaces%20to%20create%20illusions%20of%20living%20creatures.%20The%20skilled%20performers%20create%0Athese%20silhouettes%20by%20hand%20positioning%2C%20finger%20movements%2C%20and%20dexterous%20gestures%0Ato%20resemble%20shadows%20of%20animals%20and%20objects.%20Due%20to%20the%20lack%20of%20practitioners%0Aand%20a%20seismic%20shift%20in%20people%27s%20entertainment%20standards%2C%20this%20art%20form%20is%20on%0Athe%20verge%20of%20extinction.%20To%20facilitate%20its%20preservation%20and%20proliferate%20it%20to%20a%0Awider%20audience%2C%20we%20introduce%20%24%7B%5Crm%20H%7B%5Csmall%20A%7DSP%7B%5Csmall%20E%7DR%7D%24%2C%20a%20novel%20dataset%0Aconsisting%20of%2015%2C000%20images%20of%20hand%20shadow%20puppets%20across%2015%20classes%20extracted%0Afrom%20both%20professional%20and%20amateur%20hand%20shadow%20puppeteer%20clips.%20We%20provide%20a%0Adetailed%20statistical%20analysis%20of%20the%20dataset%20and%20employ%20a%20range%20of%20pretrained%0Aimage%20classification%20models%20to%20establish%20baselines.%20Our%20findings%20show%20a%0Asubstantial%20performance%20superiority%20of%20skip-connected%20convolutional%20models%20over%0Aattention-based%20transformer%20architectures.%20We%20also%20find%20that%20lightweight%0Amodels%2C%20such%20as%20MobileNetV2%2C%20suited%20for%20mobile%20applications%20and%20embedded%0Adevices%2C%20perform%20comparatively%20well.%20We%20surmise%20that%20such%20low-latency%0Aarchitectures%20can%20be%20useful%20in%20developing%20ombromanie%20teaching%20tools%2C%20and%20we%0Acreate%20a%20prototype%20application%20to%20explore%20this%20surmission.%20Keeping%20the%0Abest-performing%20model%20ResNet34%20under%20the%20limelight%2C%20we%20conduct%20comprehensive%0Afeature-spatial%2C%20explainability%2C%20and%20error%20analyses%20to%20gain%20insights%20into%20its%0Adecision-making%20process.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Adocumented%20dataset%20and%20research%20endeavor%20to%20preserve%20this%20dying%20art%20for%20future%0Agenerations%2C%20with%20computer%20vision%20approaches.%20Our%20code%20and%20data%20will%20be%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10360v5&entry.124074799=Read"},
{"title": "Simplifying DINO via Coding Rate Regularization", "author": "Ziyang Wu and Jingyuan Zhang and Druv Pai and XuDong Wang and Chandan Singh and Jianwei Yang and Jianfeng Gao and Yi Ma", "abstract": "  DINO and DINOv2 are two model families being widely used to learn\nrepresentations from unlabeled imagery data at large scales. Their learned\nrepresentations often enable state-of-the-art performance for downstream tasks,\nsuch as image classification and segmentation. However, they employ many\nempirically motivated design choices and their training pipelines are highly\ncomplex and unstable -- many hyperparameters need to be carefully tuned to\nensure that the representations do not collapse -- which poses considerable\ndifficulty to improving them or adapting them to new domains. In this work, we\nposit that we can remove most such-motivated idiosyncrasies in the pre-training\npipelines, and only need to add an explicit coding rate term in the loss\nfunction to avoid collapse of the representations. As a result, we obtain\nhighly simplified variants of the DINO and DINOv2 which we call SimDINO and\nSimDINOv2, respectively. Remarkably, these simplified models are more robust to\ndifferent design choices, such as network architecture and hyperparameters, and\nthey learn even higher-quality representations, measured by performance on\ndownstream tasks, offering a Pareto improvement over the corresponding DINO and\nDINOv2 models. This work highlights the potential of using simplifying design\nprinciples to improve the empirical practice of deep learning.\n", "link": "http://arxiv.org/abs/2502.10385v1", "date": "2025-02-14", "relevancy": 2.0568, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.522}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20DINO%20via%20Coding%20Rate%20Regularization&body=Title%3A%20Simplifying%20DINO%20via%20Coding%20Rate%20Regularization%0AAuthor%3A%20Ziyang%20Wu%20and%20Jingyuan%20Zhang%20and%20Druv%20Pai%20and%20XuDong%20Wang%20and%20Chandan%20Singh%20and%20Jianwei%20Yang%20and%20Jianfeng%20Gao%20and%20Yi%20Ma%0AAbstract%3A%20%20%20DINO%20and%20DINOv2%20are%20two%20model%20families%20being%20widely%20used%20to%20learn%0Arepresentations%20from%20unlabeled%20imagery%20data%20at%20large%20scales.%20Their%20learned%0Arepresentations%20often%20enable%20state-of-the-art%20performance%20for%20downstream%20tasks%2C%0Asuch%20as%20image%20classification%20and%20segmentation.%20However%2C%20they%20employ%20many%0Aempirically%20motivated%20design%20choices%20and%20their%20training%20pipelines%20are%20highly%0Acomplex%20and%20unstable%20--%20many%20hyperparameters%20need%20to%20be%20carefully%20tuned%20to%0Aensure%20that%20the%20representations%20do%20not%20collapse%20--%20which%20poses%20considerable%0Adifficulty%20to%20improving%20them%20or%20adapting%20them%20to%20new%20domains.%20In%20this%20work%2C%20we%0Aposit%20that%20we%20can%20remove%20most%20such-motivated%20idiosyncrasies%20in%20the%20pre-training%0Apipelines%2C%20and%20only%20need%20to%20add%20an%20explicit%20coding%20rate%20term%20in%20the%20loss%0Afunction%20to%20avoid%20collapse%20of%20the%20representations.%20As%20a%20result%2C%20we%20obtain%0Ahighly%20simplified%20variants%20of%20the%20DINO%20and%20DINOv2%20which%20we%20call%20SimDINO%20and%0ASimDINOv2%2C%20respectively.%20Remarkably%2C%20these%20simplified%20models%20are%20more%20robust%20to%0Adifferent%20design%20choices%2C%20such%20as%20network%20architecture%20and%20hyperparameters%2C%20and%0Athey%20learn%20even%20higher-quality%20representations%2C%20measured%20by%20performance%20on%0Adownstream%20tasks%2C%20offering%20a%20Pareto%20improvement%20over%20the%20corresponding%20DINO%20and%0ADINOv2%20models.%20This%20work%20highlights%20the%20potential%20of%20using%20simplifying%20design%0Aprinciples%20to%20improve%20the%20empirical%20practice%20of%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520DINO%2520via%2520Coding%2520Rate%2520Regularization%26entry.906535625%3DZiyang%2520Wu%2520and%2520Jingyuan%2520Zhang%2520and%2520Druv%2520Pai%2520and%2520XuDong%2520Wang%2520and%2520Chandan%2520Singh%2520and%2520Jianwei%2520Yang%2520and%2520Jianfeng%2520Gao%2520and%2520Yi%2520Ma%26entry.1292438233%3D%2520%2520DINO%2520and%2520DINOv2%2520are%2520two%2520model%2520families%2520being%2520widely%2520used%2520to%2520learn%250Arepresentations%2520from%2520unlabeled%2520imagery%2520data%2520at%2520large%2520scales.%2520Their%2520learned%250Arepresentations%2520often%2520enable%2520state-of-the-art%2520performance%2520for%2520downstream%2520tasks%252C%250Asuch%2520as%2520image%2520classification%2520and%2520segmentation.%2520However%252C%2520they%2520employ%2520many%250Aempirically%2520motivated%2520design%2520choices%2520and%2520their%2520training%2520pipelines%2520are%2520highly%250Acomplex%2520and%2520unstable%2520--%2520many%2520hyperparameters%2520need%2520to%2520be%2520carefully%2520tuned%2520to%250Aensure%2520that%2520the%2520representations%2520do%2520not%2520collapse%2520--%2520which%2520poses%2520considerable%250Adifficulty%2520to%2520improving%2520them%2520or%2520adapting%2520them%2520to%2520new%2520domains.%2520In%2520this%2520work%252C%2520we%250Aposit%2520that%2520we%2520can%2520remove%2520most%2520such-motivated%2520idiosyncrasies%2520in%2520the%2520pre-training%250Apipelines%252C%2520and%2520only%2520need%2520to%2520add%2520an%2520explicit%2520coding%2520rate%2520term%2520in%2520the%2520loss%250Afunction%2520to%2520avoid%2520collapse%2520of%2520the%2520representations.%2520As%2520a%2520result%252C%2520we%2520obtain%250Ahighly%2520simplified%2520variants%2520of%2520the%2520DINO%2520and%2520DINOv2%2520which%2520we%2520call%2520SimDINO%2520and%250ASimDINOv2%252C%2520respectively.%2520Remarkably%252C%2520these%2520simplified%2520models%2520are%2520more%2520robust%2520to%250Adifferent%2520design%2520choices%252C%2520such%2520as%2520network%2520architecture%2520and%2520hyperparameters%252C%2520and%250Athey%2520learn%2520even%2520higher-quality%2520representations%252C%2520measured%2520by%2520performance%2520on%250Adownstream%2520tasks%252C%2520offering%2520a%2520Pareto%2520improvement%2520over%2520the%2520corresponding%2520DINO%2520and%250ADINOv2%2520models.%2520This%2520work%2520highlights%2520the%2520potential%2520of%2520using%2520simplifying%2520design%250Aprinciples%2520to%2520improve%2520the%2520empirical%2520practice%2520of%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20DINO%20via%20Coding%20Rate%20Regularization&entry.906535625=Ziyang%20Wu%20and%20Jingyuan%20Zhang%20and%20Druv%20Pai%20and%20XuDong%20Wang%20and%20Chandan%20Singh%20and%20Jianwei%20Yang%20and%20Jianfeng%20Gao%20and%20Yi%20Ma&entry.1292438233=%20%20DINO%20and%20DINOv2%20are%20two%20model%20families%20being%20widely%20used%20to%20learn%0Arepresentations%20from%20unlabeled%20imagery%20data%20at%20large%20scales.%20Their%20learned%0Arepresentations%20often%20enable%20state-of-the-art%20performance%20for%20downstream%20tasks%2C%0Asuch%20as%20image%20classification%20and%20segmentation.%20However%2C%20they%20employ%20many%0Aempirically%20motivated%20design%20choices%20and%20their%20training%20pipelines%20are%20highly%0Acomplex%20and%20unstable%20--%20many%20hyperparameters%20need%20to%20be%20carefully%20tuned%20to%0Aensure%20that%20the%20representations%20do%20not%20collapse%20--%20which%20poses%20considerable%0Adifficulty%20to%20improving%20them%20or%20adapting%20them%20to%20new%20domains.%20In%20this%20work%2C%20we%0Aposit%20that%20we%20can%20remove%20most%20such-motivated%20idiosyncrasies%20in%20the%20pre-training%0Apipelines%2C%20and%20only%20need%20to%20add%20an%20explicit%20coding%20rate%20term%20in%20the%20loss%0Afunction%20to%20avoid%20collapse%20of%20the%20representations.%20As%20a%20result%2C%20we%20obtain%0Ahighly%20simplified%20variants%20of%20the%20DINO%20and%20DINOv2%20which%20we%20call%20SimDINO%20and%0ASimDINOv2%2C%20respectively.%20Remarkably%2C%20these%20simplified%20models%20are%20more%20robust%20to%0Adifferent%20design%20choices%2C%20such%20as%20network%20architecture%20and%20hyperparameters%2C%20and%0Athey%20learn%20even%20higher-quality%20representations%2C%20measured%20by%20performance%20on%0Adownstream%20tasks%2C%20offering%20a%20Pareto%20improvement%20over%20the%20corresponding%20DINO%20and%0ADINOv2%20models.%20This%20work%20highlights%20the%20potential%20of%20using%20simplifying%20design%0Aprinciples%20to%20improve%20the%20empirical%20practice%20of%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10385v1&entry.124074799=Read"},
{"title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress", "author": "Dong Wang and Haris \u0160iki\u0107 and Lothar Thiele and Olga Saukh", "abstract": "  We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.\n", "link": "http://arxiv.org/abs/2502.10216v1", "date": "2025-02-14", "relevancy": 2.0525, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5196}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20the%20Data%20and%20Fine-Tuning%21%20Just%20Fold%20the%20Network%20to%20Compress&body=Title%3A%20Forget%20the%20Data%20and%20Fine-Tuning%21%20Just%20Fold%20the%20Network%20to%20Compress%0AAuthor%3A%20Dong%20Wang%20and%20Haris%20%C5%A0iki%C4%87%20and%20Lothar%20Thiele%20and%20Olga%20Saukh%0AAbstract%3A%20%20%20We%20introduce%20model%20folding%2C%20a%20novel%20data-free%20model%20compression%20technique%0Athat%20merges%20structurally%20similar%20neurons%20across%20layers%2C%20significantly%20reducing%0Athe%20model%20size%20without%20the%20need%20for%20fine-tuning%20or%20access%20to%20training%20data.%0AUnlike%20existing%20methods%2C%20model%20folding%20preserves%20data%20statistics%20during%0Acompression%20by%20leveraging%20k-means%20clustering%2C%20and%20using%20novel%20data-free%0Atechniques%20to%20prevent%20variance%20collapse%20or%20explosion.%20Our%20theoretical%20framework%0Aand%20experiments%20across%20standard%20benchmarks%2C%20including%20ResNet18%20and%20LLaMA-7B%2C%0Ademonstrate%20that%20model%20folding%20achieves%20comparable%20performance%20to%20data-driven%0Acompression%20techniques%20and%20outperforms%20recently%20proposed%20data-free%20methods%2C%0Aespecially%20at%20high%20sparsity%20levels.%20This%20approach%20is%20particularly%20effective%20for%0Acompressing%20large-scale%20models%2C%20making%20it%20suitable%20for%20deployment%20in%0Aresource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520the%2520Data%2520and%2520Fine-Tuning%2521%2520Just%2520Fold%2520the%2520Network%2520to%2520Compress%26entry.906535625%3DDong%2520Wang%2520and%2520Haris%2520%25C5%25A0iki%25C4%2587%2520and%2520Lothar%2520Thiele%2520and%2520Olga%2520Saukh%26entry.1292438233%3D%2520%2520We%2520introduce%2520model%2520folding%252C%2520a%2520novel%2520data-free%2520model%2520compression%2520technique%250Athat%2520merges%2520structurally%2520similar%2520neurons%2520across%2520layers%252C%2520significantly%2520reducing%250Athe%2520model%2520size%2520without%2520the%2520need%2520for%2520fine-tuning%2520or%2520access%2520to%2520training%2520data.%250AUnlike%2520existing%2520methods%252C%2520model%2520folding%2520preserves%2520data%2520statistics%2520during%250Acompression%2520by%2520leveraging%2520k-means%2520clustering%252C%2520and%2520using%2520novel%2520data-free%250Atechniques%2520to%2520prevent%2520variance%2520collapse%2520or%2520explosion.%2520Our%2520theoretical%2520framework%250Aand%2520experiments%2520across%2520standard%2520benchmarks%252C%2520including%2520ResNet18%2520and%2520LLaMA-7B%252C%250Ademonstrate%2520that%2520model%2520folding%2520achieves%2520comparable%2520performance%2520to%2520data-driven%250Acompression%2520techniques%2520and%2520outperforms%2520recently%2520proposed%2520data-free%2520methods%252C%250Aespecially%2520at%2520high%2520sparsity%2520levels.%2520This%2520approach%2520is%2520particularly%2520effective%2520for%250Acompressing%2520large-scale%2520models%252C%2520making%2520it%2520suitable%2520for%2520deployment%2520in%250Aresource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20the%20Data%20and%20Fine-Tuning%21%20Just%20Fold%20the%20Network%20to%20Compress&entry.906535625=Dong%20Wang%20and%20Haris%20%C5%A0iki%C4%87%20and%20Lothar%20Thiele%20and%20Olga%20Saukh&entry.1292438233=%20%20We%20introduce%20model%20folding%2C%20a%20novel%20data-free%20model%20compression%20technique%0Athat%20merges%20structurally%20similar%20neurons%20across%20layers%2C%20significantly%20reducing%0Athe%20model%20size%20without%20the%20need%20for%20fine-tuning%20or%20access%20to%20training%20data.%0AUnlike%20existing%20methods%2C%20model%20folding%20preserves%20data%20statistics%20during%0Acompression%20by%20leveraging%20k-means%20clustering%2C%20and%20using%20novel%20data-free%0Atechniques%20to%20prevent%20variance%20collapse%20or%20explosion.%20Our%20theoretical%20framework%0Aand%20experiments%20across%20standard%20benchmarks%2C%20including%20ResNet18%20and%20LLaMA-7B%2C%0Ademonstrate%20that%20model%20folding%20achieves%20comparable%20performance%20to%20data-driven%0Acompression%20techniques%20and%20outperforms%20recently%20proposed%20data-free%20methods%2C%0Aespecially%20at%20high%20sparsity%20levels.%20This%20approach%20is%20particularly%20effective%20for%0Acompressing%20large-scale%20models%2C%20making%20it%20suitable%20for%20deployment%20in%0Aresource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10216v1&entry.124074799=Read"},
{"title": "Exploring the Camera Bias of Person Re-identification", "author": "Myungseo Song and Jin-Woo Park and Jong-Seok Lee", "abstract": "  We empirically investigate the camera bias of person re-identification (ReID)\nmodels. Previously, camera-aware methods have been proposed to address this\nissue, but they are largely confined to training domains of the models. We\nmeasure the camera bias of ReID models on unseen domains and reveal that camera\nbias becomes more pronounced under data distribution shifts. As a debiasing\nmethod for unseen domain data, we revisit feature normalization on embedding\nvectors. While the normalization has been used as a straightforward solution,\nits underlying causes and broader applicability remain unexplored. We analyze\nwhy this simple method is effective at reducing bias and show that it can be\napplied to detailed bias factors such as low-level image properties and body\nangle. Furthermore, we validate its generalizability across various models and\nbenchmarks, highlighting its potential as a simple yet effective test-time\npostprocessing method for ReID. In addition, we explore the inherent risk of\ncamera bias in unsupervised learning of ReID models. The unsupervised models\nremain highly biased towards camera labels even for seen domain data,\nindicating substantial room for improvement. Based on observations of the\nnegative impact of camera-biased pseudo labels on training, we suggest simple\ntraining strategies to mitigate the bias. By applying these strategies to\nexisting unsupervised learning algorithms, we show that significant performance\nimprovements can be achieved with minor modifications.\n", "link": "http://arxiv.org/abs/2502.10195v1", "date": "2025-02-14", "relevancy": 2.0495, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.51}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Camera%20Bias%20of%20Person%20Re-identification&body=Title%3A%20Exploring%20the%20Camera%20Bias%20of%20Person%20Re-identification%0AAuthor%3A%20Myungseo%20Song%20and%20Jin-Woo%20Park%20and%20Jong-Seok%20Lee%0AAbstract%3A%20%20%20We%20empirically%20investigate%20the%20camera%20bias%20of%20person%20re-identification%20%28ReID%29%0Amodels.%20Previously%2C%20camera-aware%20methods%20have%20been%20proposed%20to%20address%20this%0Aissue%2C%20but%20they%20are%20largely%20confined%20to%20training%20domains%20of%20the%20models.%20We%0Ameasure%20the%20camera%20bias%20of%20ReID%20models%20on%20unseen%20domains%20and%20reveal%20that%20camera%0Abias%20becomes%20more%20pronounced%20under%20data%20distribution%20shifts.%20As%20a%20debiasing%0Amethod%20for%20unseen%20domain%20data%2C%20we%20revisit%20feature%20normalization%20on%20embedding%0Avectors.%20While%20the%20normalization%20has%20been%20used%20as%20a%20straightforward%20solution%2C%0Aits%20underlying%20causes%20and%20broader%20applicability%20remain%20unexplored.%20We%20analyze%0Awhy%20this%20simple%20method%20is%20effective%20at%20reducing%20bias%20and%20show%20that%20it%20can%20be%0Aapplied%20to%20detailed%20bias%20factors%20such%20as%20low-level%20image%20properties%20and%20body%0Aangle.%20Furthermore%2C%20we%20validate%20its%20generalizability%20across%20various%20models%20and%0Abenchmarks%2C%20highlighting%20its%20potential%20as%20a%20simple%20yet%20effective%20test-time%0Apostprocessing%20method%20for%20ReID.%20In%20addition%2C%20we%20explore%20the%20inherent%20risk%20of%0Acamera%20bias%20in%20unsupervised%20learning%20of%20ReID%20models.%20The%20unsupervised%20models%0Aremain%20highly%20biased%20towards%20camera%20labels%20even%20for%20seen%20domain%20data%2C%0Aindicating%20substantial%20room%20for%20improvement.%20Based%20on%20observations%20of%20the%0Anegative%20impact%20of%20camera-biased%20pseudo%20labels%20on%20training%2C%20we%20suggest%20simple%0Atraining%20strategies%20to%20mitigate%20the%20bias.%20By%20applying%20these%20strategies%20to%0Aexisting%20unsupervised%20learning%20algorithms%2C%20we%20show%20that%20significant%20performance%0Aimprovements%20can%20be%20achieved%20with%20minor%20modifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Camera%2520Bias%2520of%2520Person%2520Re-identification%26entry.906535625%3DMyungseo%2520Song%2520and%2520Jin-Woo%2520Park%2520and%2520Jong-Seok%2520Lee%26entry.1292438233%3D%2520%2520We%2520empirically%2520investigate%2520the%2520camera%2520bias%2520of%2520person%2520re-identification%2520%2528ReID%2529%250Amodels.%2520Previously%252C%2520camera-aware%2520methods%2520have%2520been%2520proposed%2520to%2520address%2520this%250Aissue%252C%2520but%2520they%2520are%2520largely%2520confined%2520to%2520training%2520domains%2520of%2520the%2520models.%2520We%250Ameasure%2520the%2520camera%2520bias%2520of%2520ReID%2520models%2520on%2520unseen%2520domains%2520and%2520reveal%2520that%2520camera%250Abias%2520becomes%2520more%2520pronounced%2520under%2520data%2520distribution%2520shifts.%2520As%2520a%2520debiasing%250Amethod%2520for%2520unseen%2520domain%2520data%252C%2520we%2520revisit%2520feature%2520normalization%2520on%2520embedding%250Avectors.%2520While%2520the%2520normalization%2520has%2520been%2520used%2520as%2520a%2520straightforward%2520solution%252C%250Aits%2520underlying%2520causes%2520and%2520broader%2520applicability%2520remain%2520unexplored.%2520We%2520analyze%250Awhy%2520this%2520simple%2520method%2520is%2520effective%2520at%2520reducing%2520bias%2520and%2520show%2520that%2520it%2520can%2520be%250Aapplied%2520to%2520detailed%2520bias%2520factors%2520such%2520as%2520low-level%2520image%2520properties%2520and%2520body%250Aangle.%2520Furthermore%252C%2520we%2520validate%2520its%2520generalizability%2520across%2520various%2520models%2520and%250Abenchmarks%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520simple%2520yet%2520effective%2520test-time%250Apostprocessing%2520method%2520for%2520ReID.%2520In%2520addition%252C%2520we%2520explore%2520the%2520inherent%2520risk%2520of%250Acamera%2520bias%2520in%2520unsupervised%2520learning%2520of%2520ReID%2520models.%2520The%2520unsupervised%2520models%250Aremain%2520highly%2520biased%2520towards%2520camera%2520labels%2520even%2520for%2520seen%2520domain%2520data%252C%250Aindicating%2520substantial%2520room%2520for%2520improvement.%2520Based%2520on%2520observations%2520of%2520the%250Anegative%2520impact%2520of%2520camera-biased%2520pseudo%2520labels%2520on%2520training%252C%2520we%2520suggest%2520simple%250Atraining%2520strategies%2520to%2520mitigate%2520the%2520bias.%2520By%2520applying%2520these%2520strategies%2520to%250Aexisting%2520unsupervised%2520learning%2520algorithms%252C%2520we%2520show%2520that%2520significant%2520performance%250Aimprovements%2520can%2520be%2520achieved%2520with%2520minor%2520modifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Camera%20Bias%20of%20Person%20Re-identification&entry.906535625=Myungseo%20Song%20and%20Jin-Woo%20Park%20and%20Jong-Seok%20Lee&entry.1292438233=%20%20We%20empirically%20investigate%20the%20camera%20bias%20of%20person%20re-identification%20%28ReID%29%0Amodels.%20Previously%2C%20camera-aware%20methods%20have%20been%20proposed%20to%20address%20this%0Aissue%2C%20but%20they%20are%20largely%20confined%20to%20training%20domains%20of%20the%20models.%20We%0Ameasure%20the%20camera%20bias%20of%20ReID%20models%20on%20unseen%20domains%20and%20reveal%20that%20camera%0Abias%20becomes%20more%20pronounced%20under%20data%20distribution%20shifts.%20As%20a%20debiasing%0Amethod%20for%20unseen%20domain%20data%2C%20we%20revisit%20feature%20normalization%20on%20embedding%0Avectors.%20While%20the%20normalization%20has%20been%20used%20as%20a%20straightforward%20solution%2C%0Aits%20underlying%20causes%20and%20broader%20applicability%20remain%20unexplored.%20We%20analyze%0Awhy%20this%20simple%20method%20is%20effective%20at%20reducing%20bias%20and%20show%20that%20it%20can%20be%0Aapplied%20to%20detailed%20bias%20factors%20such%20as%20low-level%20image%20properties%20and%20body%0Aangle.%20Furthermore%2C%20we%20validate%20its%20generalizability%20across%20various%20models%20and%0Abenchmarks%2C%20highlighting%20its%20potential%20as%20a%20simple%20yet%20effective%20test-time%0Apostprocessing%20method%20for%20ReID.%20In%20addition%2C%20we%20explore%20the%20inherent%20risk%20of%0Acamera%20bias%20in%20unsupervised%20learning%20of%20ReID%20models.%20The%20unsupervised%20models%0Aremain%20highly%20biased%20towards%20camera%20labels%20even%20for%20seen%20domain%20data%2C%0Aindicating%20substantial%20room%20for%20improvement.%20Based%20on%20observations%20of%20the%0Anegative%20impact%20of%20camera-biased%20pseudo%20labels%20on%20training%2C%20we%20suggest%20simple%0Atraining%20strategies%20to%20mitigate%20the%20bias.%20By%20applying%20these%20strategies%20to%0Aexisting%20unsupervised%20learning%20algorithms%2C%20we%20show%20that%20significant%20performance%0Aimprovements%20can%20be%20achieved%20with%20minor%20modifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10195v1&entry.124074799=Read"},
{"title": "SuperMerge: An Approach For Gradient-Based Model Merging", "author": "Haoyu Yang and Zheng Zhang and Saket Sathe", "abstract": "  Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic,\nmonolithic, and possess the superpower to simultaneously support thousands of\ntasks. However, high-throughput applications often prefer smaller task-specific\nmodels because of their lower latency and cost. One challenge of using\ntask-specific models is the incremental need for solving newer tasks after the\nmodel is already deployed for existing tasks. A straightforward solution\nrequires fine-tuning the model again for both existing and new tasks, which is\ncomputationally expensive and time-consuming. To address this issue, we propose\na model merging based approach called SUPERMERGE. SUPERMERGE is a\ngradient-based method to systematically merge several fine-tuned models trained\non existing and new tasks. SUPERMERGE is designed to be lightweight and fast,\nand the merged model achieves similar performance to fully fine-tuned models on\nall tasks. Furthermore, we proposed a hierarchical model merging strategy to\nreduce the peak space requirement without sacrificing the performance of the\nmerged model. We experimentally demonstrate that SUPERMERGE outperforms\nexisting model merging methods on common natural language processing and\ncomputer vision tasks.\n", "link": "http://arxiv.org/abs/2412.10416v2", "date": "2025-02-14", "relevancy": 2.0476, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5111}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperMerge%3A%20An%20Approach%20For%20Gradient-Based%20Model%20Merging&body=Title%3A%20SuperMerge%3A%20An%20Approach%20For%20Gradient-Based%20Model%20Merging%0AAuthor%3A%20Haoyu%20Yang%20and%20Zheng%20Zhang%20and%20Saket%20Sathe%0AAbstract%3A%20%20%20Large%20language%20models%2C%20such%20as%20ChatGPT%2C%20Claude%2C%20or%20LLaMA%2C%20are%20gigantic%2C%0Amonolithic%2C%20and%20possess%20the%20superpower%20to%20simultaneously%20support%20thousands%20of%0Atasks.%20However%2C%20high-throughput%20applications%20often%20prefer%20smaller%20task-specific%0Amodels%20because%20of%20their%20lower%20latency%20and%20cost.%20One%20challenge%20of%20using%0Atask-specific%20models%20is%20the%20incremental%20need%20for%20solving%20newer%20tasks%20after%20the%0Amodel%20is%20already%20deployed%20for%20existing%20tasks.%20A%20straightforward%20solution%0Arequires%20fine-tuning%20the%20model%20again%20for%20both%20existing%20and%20new%20tasks%2C%20which%20is%0Acomputationally%20expensive%20and%20time-consuming.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20model%20merging%20based%20approach%20called%20SUPERMERGE.%20SUPERMERGE%20is%20a%0Agradient-based%20method%20to%20systematically%20merge%20several%20fine-tuned%20models%20trained%0Aon%20existing%20and%20new%20tasks.%20SUPERMERGE%20is%20designed%20to%20be%20lightweight%20and%20fast%2C%0Aand%20the%20merged%20model%20achieves%20similar%20performance%20to%20fully%20fine-tuned%20models%20on%0Aall%20tasks.%20Furthermore%2C%20we%20proposed%20a%20hierarchical%20model%20merging%20strategy%20to%0Areduce%20the%20peak%20space%20requirement%20without%20sacrificing%20the%20performance%20of%20the%0Amerged%20model.%20We%20experimentally%20demonstrate%20that%20SUPERMERGE%20outperforms%0Aexisting%20model%20merging%20methods%20on%20common%20natural%20language%20processing%20and%0Acomputer%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperMerge%253A%2520An%2520Approach%2520For%2520Gradient-Based%2520Model%2520Merging%26entry.906535625%3DHaoyu%2520Yang%2520and%2520Zheng%2520Zhang%2520and%2520Saket%2520Sathe%26entry.1292438233%3D%2520%2520Large%2520language%2520models%252C%2520such%2520as%2520ChatGPT%252C%2520Claude%252C%2520or%2520LLaMA%252C%2520are%2520gigantic%252C%250Amonolithic%252C%2520and%2520possess%2520the%2520superpower%2520to%2520simultaneously%2520support%2520thousands%2520of%250Atasks.%2520However%252C%2520high-throughput%2520applications%2520often%2520prefer%2520smaller%2520task-specific%250Amodels%2520because%2520of%2520their%2520lower%2520latency%2520and%2520cost.%2520One%2520challenge%2520of%2520using%250Atask-specific%2520models%2520is%2520the%2520incremental%2520need%2520for%2520solving%2520newer%2520tasks%2520after%2520the%250Amodel%2520is%2520already%2520deployed%2520for%2520existing%2520tasks.%2520A%2520straightforward%2520solution%250Arequires%2520fine-tuning%2520the%2520model%2520again%2520for%2520both%2520existing%2520and%2520new%2520tasks%252C%2520which%2520is%250Acomputationally%2520expensive%2520and%2520time-consuming.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250Aa%2520model%2520merging%2520based%2520approach%2520called%2520SUPERMERGE.%2520SUPERMERGE%2520is%2520a%250Agradient-based%2520method%2520to%2520systematically%2520merge%2520several%2520fine-tuned%2520models%2520trained%250Aon%2520existing%2520and%2520new%2520tasks.%2520SUPERMERGE%2520is%2520designed%2520to%2520be%2520lightweight%2520and%2520fast%252C%250Aand%2520the%2520merged%2520model%2520achieves%2520similar%2520performance%2520to%2520fully%2520fine-tuned%2520models%2520on%250Aall%2520tasks.%2520Furthermore%252C%2520we%2520proposed%2520a%2520hierarchical%2520model%2520merging%2520strategy%2520to%250Areduce%2520the%2520peak%2520space%2520requirement%2520without%2520sacrificing%2520the%2520performance%2520of%2520the%250Amerged%2520model.%2520We%2520experimentally%2520demonstrate%2520that%2520SUPERMERGE%2520outperforms%250Aexisting%2520model%2520merging%2520methods%2520on%2520common%2520natural%2520language%2520processing%2520and%250Acomputer%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperMerge%3A%20An%20Approach%20For%20Gradient-Based%20Model%20Merging&entry.906535625=Haoyu%20Yang%20and%20Zheng%20Zhang%20and%20Saket%20Sathe&entry.1292438233=%20%20Large%20language%20models%2C%20such%20as%20ChatGPT%2C%20Claude%2C%20or%20LLaMA%2C%20are%20gigantic%2C%0Amonolithic%2C%20and%20possess%20the%20superpower%20to%20simultaneously%20support%20thousands%20of%0Atasks.%20However%2C%20high-throughput%20applications%20often%20prefer%20smaller%20task-specific%0Amodels%20because%20of%20their%20lower%20latency%20and%20cost.%20One%20challenge%20of%20using%0Atask-specific%20models%20is%20the%20incremental%20need%20for%20solving%20newer%20tasks%20after%20the%0Amodel%20is%20already%20deployed%20for%20existing%20tasks.%20A%20straightforward%20solution%0Arequires%20fine-tuning%20the%20model%20again%20for%20both%20existing%20and%20new%20tasks%2C%20which%20is%0Acomputationally%20expensive%20and%20time-consuming.%20To%20address%20this%20issue%2C%20we%20propose%0Aa%20model%20merging%20based%20approach%20called%20SUPERMERGE.%20SUPERMERGE%20is%20a%0Agradient-based%20method%20to%20systematically%20merge%20several%20fine-tuned%20models%20trained%0Aon%20existing%20and%20new%20tasks.%20SUPERMERGE%20is%20designed%20to%20be%20lightweight%20and%20fast%2C%0Aand%20the%20merged%20model%20achieves%20similar%20performance%20to%20fully%20fine-tuned%20models%20on%0Aall%20tasks.%20Furthermore%2C%20we%20proposed%20a%20hierarchical%20model%20merging%20strategy%20to%0Areduce%20the%20peak%20space%20requirement%20without%20sacrificing%20the%20performance%20of%20the%0Amerged%20model.%20We%20experimentally%20demonstrate%20that%20SUPERMERGE%20outperforms%0Aexisting%20model%20merging%20methods%20on%20common%20natural%20language%20processing%20and%0Acomputer%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10416v2&entry.124074799=Read"},
{"title": "Task-Aware Virtual Training: Enhancing Generalization in\n  Meta-Reinforcement Learning for Out-of-Distribution Tasks", "author": "Jeongmo Kim and Yisak Park and Minung Kim and Seungyul Han", "abstract": "  Meta reinforcement learning aims to develop policies that generalize to\nunseen tasks sampled from a task distribution. While context-based meta-RL\nmethods improve task representation using task latents, they often struggle\nwith out-of-distribution (OOD) tasks. To address this, we propose Task-Aware\nVirtual Training (TAVT), a novel algorithm that accurately captures task\ncharacteristics for both training and OOD scenarios using metric-based\nrepresentation learning. Our method successfully preserves task characteristics\nin virtual tasks and employs a state regularization technique to mitigate\noverestimation errors in state-varying environments. Numerical results\ndemonstrate that TAVT significantly enhances generalization to OOD tasks across\nvarious MuJoCo and MetaWorld environments.\n", "link": "http://arxiv.org/abs/2502.02834v2", "date": "2025-02-14", "relevancy": 2.0359, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5115}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Aware%20Virtual%20Training%3A%20Enhancing%20Generalization%20in%0A%20%20Meta-Reinforcement%20Learning%20for%20Out-of-Distribution%20Tasks&body=Title%3A%20Task-Aware%20Virtual%20Training%3A%20Enhancing%20Generalization%20in%0A%20%20Meta-Reinforcement%20Learning%20for%20Out-of-Distribution%20Tasks%0AAuthor%3A%20Jeongmo%20Kim%20and%20Yisak%20Park%20and%20Minung%20Kim%20and%20Seungyul%20Han%0AAbstract%3A%20%20%20Meta%20reinforcement%20learning%20aims%20to%20develop%20policies%20that%20generalize%20to%0Aunseen%20tasks%20sampled%20from%20a%20task%20distribution.%20While%20context-based%20meta-RL%0Amethods%20improve%20task%20representation%20using%20task%20latents%2C%20they%20often%20struggle%0Awith%20out-of-distribution%20%28OOD%29%20tasks.%20To%20address%20this%2C%20we%20propose%20Task-Aware%0AVirtual%20Training%20%28TAVT%29%2C%20a%20novel%20algorithm%20that%20accurately%20captures%20task%0Acharacteristics%20for%20both%20training%20and%20OOD%20scenarios%20using%20metric-based%0Arepresentation%20learning.%20Our%20method%20successfully%20preserves%20task%20characteristics%0Ain%20virtual%20tasks%20and%20employs%20a%20state%20regularization%20technique%20to%20mitigate%0Aoverestimation%20errors%20in%20state-varying%20environments.%20Numerical%20results%0Ademonstrate%20that%20TAVT%20significantly%20enhances%20generalization%20to%20OOD%20tasks%20across%0Avarious%20MuJoCo%20and%20MetaWorld%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Aware%2520Virtual%2520Training%253A%2520Enhancing%2520Generalization%2520in%250A%2520%2520Meta-Reinforcement%2520Learning%2520for%2520Out-of-Distribution%2520Tasks%26entry.906535625%3DJeongmo%2520Kim%2520and%2520Yisak%2520Park%2520and%2520Minung%2520Kim%2520and%2520Seungyul%2520Han%26entry.1292438233%3D%2520%2520Meta%2520reinforcement%2520learning%2520aims%2520to%2520develop%2520policies%2520that%2520generalize%2520to%250Aunseen%2520tasks%2520sampled%2520from%2520a%2520task%2520distribution.%2520While%2520context-based%2520meta-RL%250Amethods%2520improve%2520task%2520representation%2520using%2520task%2520latents%252C%2520they%2520often%2520struggle%250Awith%2520out-of-distribution%2520%2528OOD%2529%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520Task-Aware%250AVirtual%2520Training%2520%2528TAVT%2529%252C%2520a%2520novel%2520algorithm%2520that%2520accurately%2520captures%2520task%250Acharacteristics%2520for%2520both%2520training%2520and%2520OOD%2520scenarios%2520using%2520metric-based%250Arepresentation%2520learning.%2520Our%2520method%2520successfully%2520preserves%2520task%2520characteristics%250Ain%2520virtual%2520tasks%2520and%2520employs%2520a%2520state%2520regularization%2520technique%2520to%2520mitigate%250Aoverestimation%2520errors%2520in%2520state-varying%2520environments.%2520Numerical%2520results%250Ademonstrate%2520that%2520TAVT%2520significantly%2520enhances%2520generalization%2520to%2520OOD%2520tasks%2520across%250Avarious%2520MuJoCo%2520and%2520MetaWorld%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Aware%20Virtual%20Training%3A%20Enhancing%20Generalization%20in%0A%20%20Meta-Reinforcement%20Learning%20for%20Out-of-Distribution%20Tasks&entry.906535625=Jeongmo%20Kim%20and%20Yisak%20Park%20and%20Minung%20Kim%20and%20Seungyul%20Han&entry.1292438233=%20%20Meta%20reinforcement%20learning%20aims%20to%20develop%20policies%20that%20generalize%20to%0Aunseen%20tasks%20sampled%20from%20a%20task%20distribution.%20While%20context-based%20meta-RL%0Amethods%20improve%20task%20representation%20using%20task%20latents%2C%20they%20often%20struggle%0Awith%20out-of-distribution%20%28OOD%29%20tasks.%20To%20address%20this%2C%20we%20propose%20Task-Aware%0AVirtual%20Training%20%28TAVT%29%2C%20a%20novel%20algorithm%20that%20accurately%20captures%20task%0Acharacteristics%20for%20both%20training%20and%20OOD%20scenarios%20using%20metric-based%0Arepresentation%20learning.%20Our%20method%20successfully%20preserves%20task%20characteristics%0Ain%20virtual%20tasks%20and%20employs%20a%20state%20regularization%20technique%20to%20mitigate%0Aoverestimation%20errors%20in%20state-varying%20environments.%20Numerical%20results%0Ademonstrate%20that%20TAVT%20significantly%20enhances%20generalization%20to%20OOD%20tasks%20across%0Avarious%20MuJoCo%20and%20MetaWorld%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02834v2&entry.124074799=Read"},
{"title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers", "author": "Tiberiu Musat", "abstract": "  In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.\n", "link": "http://arxiv.org/abs/2411.12118v2", "date": "2025-02-14", "relevancy": 2.0346, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5114}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5105}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanism%20and%20Emergence%20of%20Stacked%20Attention%20Heads%20in%20Multi-Layer%0A%20%20Transformers&body=Title%3A%20Mechanism%20and%20Emergence%20of%20Stacked%20Attention%20Heads%20in%20Multi-Layer%0A%20%20Transformers%0AAuthor%3A%20Tiberiu%20Musat%0AAbstract%3A%20%20%20In%20this%20paper%2C%20I%20introduce%20the%20retrieval%20problem%2C%20a%20simple%20yet%20common%0Areasoning%20task%20that%20can%20be%20solved%20only%20by%20transformers%20with%20a%20minimum%20number%20of%0Alayers%2C%20which%20grows%20logarithmically%20with%20the%20input%20size.%20I%20empirically%20show%0Athat%20large%20language%20models%20can%20solve%20the%20task%20under%20different%20prompting%0Aformulations%20without%20any%20fine-tuning.%20To%20understand%20how%20transformers%20solve%20the%0Aretrieval%20problem%2C%20I%20train%20several%20transformers%20on%20a%20minimal%20formulation.%0ASuccessful%20learning%20occurs%20only%20under%20the%20presence%20of%20an%20implicit%20curriculum.%20I%0Auncover%20the%20learned%20mechanisms%20by%20studying%20the%20attention%20maps%20in%20the%20trained%0Atransformers.%20I%20also%20study%20the%20training%20process%2C%20uncovering%20that%20attention%0Aheads%20always%20emerge%20in%20a%20specific%20sequence%20guided%20by%20the%20implicit%20curriculum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanism%2520and%2520Emergence%2520of%2520Stacked%2520Attention%2520Heads%2520in%2520Multi-Layer%250A%2520%2520Transformers%26entry.906535625%3DTiberiu%2520Musat%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520I%2520introduce%2520the%2520retrieval%2520problem%252C%2520a%2520simple%2520yet%2520common%250Areasoning%2520task%2520that%2520can%2520be%2520solved%2520only%2520by%2520transformers%2520with%2520a%2520minimum%2520number%2520of%250Alayers%252C%2520which%2520grows%2520logarithmically%2520with%2520the%2520input%2520size.%2520I%2520empirically%2520show%250Athat%2520large%2520language%2520models%2520can%2520solve%2520the%2520task%2520under%2520different%2520prompting%250Aformulations%2520without%2520any%2520fine-tuning.%2520To%2520understand%2520how%2520transformers%2520solve%2520the%250Aretrieval%2520problem%252C%2520I%2520train%2520several%2520transformers%2520on%2520a%2520minimal%2520formulation.%250ASuccessful%2520learning%2520occurs%2520only%2520under%2520the%2520presence%2520of%2520an%2520implicit%2520curriculum.%2520I%250Auncover%2520the%2520learned%2520mechanisms%2520by%2520studying%2520the%2520attention%2520maps%2520in%2520the%2520trained%250Atransformers.%2520I%2520also%2520study%2520the%2520training%2520process%252C%2520uncovering%2520that%2520attention%250Aheads%2520always%2520emerge%2520in%2520a%2520specific%2520sequence%2520guided%2520by%2520the%2520implicit%2520curriculum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanism%20and%20Emergence%20of%20Stacked%20Attention%20Heads%20in%20Multi-Layer%0A%20%20Transformers&entry.906535625=Tiberiu%20Musat&entry.1292438233=%20%20In%20this%20paper%2C%20I%20introduce%20the%20retrieval%20problem%2C%20a%20simple%20yet%20common%0Areasoning%20task%20that%20can%20be%20solved%20only%20by%20transformers%20with%20a%20minimum%20number%20of%0Alayers%2C%20which%20grows%20logarithmically%20with%20the%20input%20size.%20I%20empirically%20show%0Athat%20large%20language%20models%20can%20solve%20the%20task%20under%20different%20prompting%0Aformulations%20without%20any%20fine-tuning.%20To%20understand%20how%20transformers%20solve%20the%0Aretrieval%20problem%2C%20I%20train%20several%20transformers%20on%20a%20minimal%20formulation.%0ASuccessful%20learning%20occurs%20only%20under%20the%20presence%20of%20an%20implicit%20curriculum.%20I%0Auncover%20the%20learned%20mechanisms%20by%20studying%20the%20attention%20maps%20in%20the%20trained%0Atransformers.%20I%20also%20study%20the%20training%20process%2C%20uncovering%20that%20attention%0Aheads%20always%20emerge%20in%20a%20specific%20sequence%20guided%20by%20the%20implicit%20curriculum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12118v2&entry.124074799=Read"},
{"title": "Prompt-based Depth Pruning of Large Language Models", "author": "Juyun Wee and Minjae Park and Jaeho Lee", "abstract": "  Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines.\n", "link": "http://arxiv.org/abs/2502.04348v2", "date": "2025-02-14", "relevancy": 2.0338, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-based%20Depth%20Pruning%20of%20Large%20Language%20Models&body=Title%3A%20Prompt-based%20Depth%20Pruning%20of%20Large%20Language%20Models%0AAuthor%3A%20Juyun%20Wee%20and%20Minjae%20Park%20and%20Jaeho%20Lee%0AAbstract%3A%20%20%20Depth%20pruning%20aims%20to%20reduce%20the%20inference%20cost%20of%20a%20large%20language%20model%0Awithout%20any%20hardware-specific%20complications%2C%20by%20simply%20removing%20several%20less%0Aimportant%20transformer%20blocks.%20However%2C%20our%20empirical%20findings%20suggest%20that%20the%0Aimportance%20of%20a%20transformer%20block%20may%20be%20highly%20task-dependent%20--%20a%20block%20that%0Ais%20crucial%20for%20a%20task%20can%20be%20removed%20without%20degrading%20the%20accuracy%20on%20another%0Atask.%20Based%20on%20this%20observation%2C%20we%20develop%20a%20dynamic%20depth%20pruning%20algorithm%2C%0Acoined%20PuDDing%20%28Prompt-routed%20Dynamic%20Depth%20Pruning%29%2C%20which%20determines%20which%0Ablocks%20to%20omit%20from%20the%20model%20based%20on%20the%20input%20prompt.%20PuDDing%20operates%20by%0Atraining%20a%20lightweight%20router%20to%20predict%20the%20best%20omission%20set%20among%20a%20set%20of%0Aoptions%2C%20where%20this%20option%20set%20has%20also%20been%20constructed%20in%20a%20data-driven%0Amanner.%20Empirical%20results%20on%20commonsense%20reasoning%20benchmarks%20demonstrate%20that%0APuDDing%20effectively%20accelerates%20the%20inference%20language%20models%2C%20and%20achieves%0Abetter%20on-task%20performance%20than%20static%20depth%20pruning%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-based%2520Depth%2520Pruning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DJuyun%2520Wee%2520and%2520Minjae%2520Park%2520and%2520Jaeho%2520Lee%26entry.1292438233%3D%2520%2520Depth%2520pruning%2520aims%2520to%2520reduce%2520the%2520inference%2520cost%2520of%2520a%2520large%2520language%2520model%250Awithout%2520any%2520hardware-specific%2520complications%252C%2520by%2520simply%2520removing%2520several%2520less%250Aimportant%2520transformer%2520blocks.%2520However%252C%2520our%2520empirical%2520findings%2520suggest%2520that%2520the%250Aimportance%2520of%2520a%2520transformer%2520block%2520may%2520be%2520highly%2520task-dependent%2520--%2520a%2520block%2520that%250Ais%2520crucial%2520for%2520a%2520task%2520can%2520be%2520removed%2520without%2520degrading%2520the%2520accuracy%2520on%2520another%250Atask.%2520Based%2520on%2520this%2520observation%252C%2520we%2520develop%2520a%2520dynamic%2520depth%2520pruning%2520algorithm%252C%250Acoined%2520PuDDing%2520%2528Prompt-routed%2520Dynamic%2520Depth%2520Pruning%2529%252C%2520which%2520determines%2520which%250Ablocks%2520to%2520omit%2520from%2520the%2520model%2520based%2520on%2520the%2520input%2520prompt.%2520PuDDing%2520operates%2520by%250Atraining%2520a%2520lightweight%2520router%2520to%2520predict%2520the%2520best%2520omission%2520set%2520among%2520a%2520set%2520of%250Aoptions%252C%2520where%2520this%2520option%2520set%2520has%2520also%2520been%2520constructed%2520in%2520a%2520data-driven%250Amanner.%2520Empirical%2520results%2520on%2520commonsense%2520reasoning%2520benchmarks%2520demonstrate%2520that%250APuDDing%2520effectively%2520accelerates%2520the%2520inference%2520language%2520models%252C%2520and%2520achieves%250Abetter%2520on-task%2520performance%2520than%2520static%2520depth%2520pruning%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-based%20Depth%20Pruning%20of%20Large%20Language%20Models&entry.906535625=Juyun%20Wee%20and%20Minjae%20Park%20and%20Jaeho%20Lee&entry.1292438233=%20%20Depth%20pruning%20aims%20to%20reduce%20the%20inference%20cost%20of%20a%20large%20language%20model%0Awithout%20any%20hardware-specific%20complications%2C%20by%20simply%20removing%20several%20less%0Aimportant%20transformer%20blocks.%20However%2C%20our%20empirical%20findings%20suggest%20that%20the%0Aimportance%20of%20a%20transformer%20block%20may%20be%20highly%20task-dependent%20--%20a%20block%20that%0Ais%20crucial%20for%20a%20task%20can%20be%20removed%20without%20degrading%20the%20accuracy%20on%20another%0Atask.%20Based%20on%20this%20observation%2C%20we%20develop%20a%20dynamic%20depth%20pruning%20algorithm%2C%0Acoined%20PuDDing%20%28Prompt-routed%20Dynamic%20Depth%20Pruning%29%2C%20which%20determines%20which%0Ablocks%20to%20omit%20from%20the%20model%20based%20on%20the%20input%20prompt.%20PuDDing%20operates%20by%0Atraining%20a%20lightweight%20router%20to%20predict%20the%20best%20omission%20set%20among%20a%20set%20of%0Aoptions%2C%20where%20this%20option%20set%20has%20also%20been%20constructed%20in%20a%20data-driven%0Amanner.%20Empirical%20results%20on%20commonsense%20reasoning%20benchmarks%20demonstrate%20that%0APuDDing%20effectively%20accelerates%20the%20inference%20language%20models%2C%20and%20achieves%0Abetter%20on-task%20performance%20than%20static%20depth%20pruning%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04348v2&entry.124074799=Read"},
{"title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning", "author": "Zhenni Bi and Kai Han and Chuanjian Liu and Yehui Tang and Yunhe Wang", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.\n", "link": "http://arxiv.org/abs/2412.09078v3", "date": "2025-02-14", "relevancy": 2.0314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forest-of-Thought%3A%20Scaling%20Test-Time%20Compute%20for%20Enhancing%20LLM%20Reasoning&body=Title%3A%20Forest-of-Thought%3A%20Scaling%20Test-Time%20Compute%20for%20Enhancing%20LLM%20Reasoning%0AAuthor%3A%20Zhenni%20Bi%20and%20Kai%20Han%20and%20Chuanjian%20Liu%20and%20Yehui%20Tang%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20abilities%20across%0Avarious%20language%20tasks%2C%20but%20solving%20complex%20reasoning%20problems%20remains%20a%0Asignificant%20challenge.%20While%20existing%20methods%2C%20such%20as%20Chain-of-Thought%20%28CoT%29%0Aand%20Tree-of-Thought%20%28ToT%29%2C%20enhance%20reasoning%20by%20decomposing%20problems%20or%0Astructuring%20prompts%2C%20they%20typically%20perform%20a%20single%20pass%20of%20reasoning%20and%20may%0Afail%20to%20revisit%20flawed%20paths%2C%20compromising%20accuracy.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20reasoning%20framework%20called%20Forest-of-Thought%0A%28FoT%29%2C%20which%20integrates%20multiple%20reasoning%20trees%20to%20leverage%20collective%0Adecision-making%20for%20solving%20complex%20logical%20problems.%20FoT%20employs%20sparse%0Aactivation%20strategies%20to%20select%20the%20most%20relevant%20reasoning%20paths%2C%20improving%0Aboth%20efficiency%20and%20accuracy.%20Additionally%2C%20we%20introduce%20a%20dynamic%0Aself-correction%20strategy%20that%20enables%20real-time%20error%20correction%2C%20along%20with%0Aconsensus-guided%20decision-making%20strategies%20to%20optimize%20both%20correctness%20and%0Acomputational%20resources.%20Experimental%20results%20demonstrate%20that%20the%20FoT%0Aframework%2C%20combined%20with%20these%20strategies%2C%20significantly%20enhances%20the%20reasoning%0Acapabilities%20of%20LLMs%2C%20enabling%20them%20to%20solve%20complex%20tasks%20with%20greater%0Aprecision%20and%20efficiency.Code%20will%20be%20available%20at%0Ahttps%3A//github.com/iamhankai/Forest-of-Thought.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09078v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForest-of-Thought%253A%2520Scaling%2520Test-Time%2520Compute%2520for%2520Enhancing%2520LLM%2520Reasoning%26entry.906535625%3DZhenni%2520Bi%2520and%2520Kai%2520Han%2520and%2520Chuanjian%2520Liu%2520and%2520Yehui%2520Tang%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520abilities%2520across%250Avarious%2520language%2520tasks%252C%2520but%2520solving%2520complex%2520reasoning%2520problems%2520remains%2520a%250Asignificant%2520challenge.%2520While%2520existing%2520methods%252C%2520such%2520as%2520Chain-of-Thought%2520%2528CoT%2529%250Aand%2520Tree-of-Thought%2520%2528ToT%2529%252C%2520enhance%2520reasoning%2520by%2520decomposing%2520problems%2520or%250Astructuring%2520prompts%252C%2520they%2520typically%2520perform%2520a%2520single%2520pass%2520of%2520reasoning%2520and%2520may%250Afail%2520to%2520revisit%2520flawed%2520paths%252C%2520compromising%2520accuracy.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520novel%2520reasoning%2520framework%2520called%2520Forest-of-Thought%250A%2528FoT%2529%252C%2520which%2520integrates%2520multiple%2520reasoning%2520trees%2520to%2520leverage%2520collective%250Adecision-making%2520for%2520solving%2520complex%2520logical%2520problems.%2520FoT%2520employs%2520sparse%250Aactivation%2520strategies%2520to%2520select%2520the%2520most%2520relevant%2520reasoning%2520paths%252C%2520improving%250Aboth%2520efficiency%2520and%2520accuracy.%2520Additionally%252C%2520we%2520introduce%2520a%2520dynamic%250Aself-correction%2520strategy%2520that%2520enables%2520real-time%2520error%2520correction%252C%2520along%2520with%250Aconsensus-guided%2520decision-making%2520strategies%2520to%2520optimize%2520both%2520correctness%2520and%250Acomputational%2520resources.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520FoT%250Aframework%252C%2520combined%2520with%2520these%2520strategies%252C%2520significantly%2520enhances%2520the%2520reasoning%250Acapabilities%2520of%2520LLMs%252C%2520enabling%2520them%2520to%2520solve%2520complex%2520tasks%2520with%2520greater%250Aprecision%2520and%2520efficiency.Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/iamhankai/Forest-of-Thought.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09078v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forest-of-Thought%3A%20Scaling%20Test-Time%20Compute%20for%20Enhancing%20LLM%20Reasoning&entry.906535625=Zhenni%20Bi%20and%20Kai%20Han%20and%20Chuanjian%20Liu%20and%20Yehui%20Tang%20and%20Yunhe%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20abilities%20across%0Avarious%20language%20tasks%2C%20but%20solving%20complex%20reasoning%20problems%20remains%20a%0Asignificant%20challenge.%20While%20existing%20methods%2C%20such%20as%20Chain-of-Thought%20%28CoT%29%0Aand%20Tree-of-Thought%20%28ToT%29%2C%20enhance%20reasoning%20by%20decomposing%20problems%20or%0Astructuring%20prompts%2C%20they%20typically%20perform%20a%20single%20pass%20of%20reasoning%20and%20may%0Afail%20to%20revisit%20flawed%20paths%2C%20compromising%20accuracy.%20To%20address%20this%0Alimitation%2C%20we%20propose%20a%20novel%20reasoning%20framework%20called%20Forest-of-Thought%0A%28FoT%29%2C%20which%20integrates%20multiple%20reasoning%20trees%20to%20leverage%20collective%0Adecision-making%20for%20solving%20complex%20logical%20problems.%20FoT%20employs%20sparse%0Aactivation%20strategies%20to%20select%20the%20most%20relevant%20reasoning%20paths%2C%20improving%0Aboth%20efficiency%20and%20accuracy.%20Additionally%2C%20we%20introduce%20a%20dynamic%0Aself-correction%20strategy%20that%20enables%20real-time%20error%20correction%2C%20along%20with%0Aconsensus-guided%20decision-making%20strategies%20to%20optimize%20both%20correctness%20and%0Acomputational%20resources.%20Experimental%20results%20demonstrate%20that%20the%20FoT%0Aframework%2C%20combined%20with%20these%20strategies%2C%20significantly%20enhances%20the%20reasoning%0Acapabilities%20of%20LLMs%2C%20enabling%20them%20to%20solve%20complex%20tasks%20with%20greater%0Aprecision%20and%20efficiency.Code%20will%20be%20available%20at%0Ahttps%3A//github.com/iamhankai/Forest-of-Thought.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09078v3&entry.124074799=Read"},
{"title": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting", "author": "Nicholas Dronen and Randall Balestriero", "abstract": "  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\nhttps://github.com/amazon-science/eideticnet-training.\n", "link": "http://arxiv.org/abs/2502.09500v2", "date": "2025-02-14", "relevancy": 2.0256, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5251}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting&body=Title%3A%20Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting%0AAuthor%3A%20Nicholas%20Dronen%20and%20Randall%20Balestriero%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20--%20the%20phenomenon%20of%20a%20neural%20network%20learning%20a%20task%0At1%20and%20losing%20the%20ability%20to%20perform%20it%20after%20being%20trained%20on%20some%20other%20task%0At2%20--%20is%20a%20long-standing%20problem%20for%20neural%20networks%20%5BMcCloskey%20and%20Cohen%2C%0A1989%5D.%20We%20present%20a%20method%2C%20Eidetic%20Learning%2C%20that%20provably%20solves%20catastrophic%0Aforgetting.%20A%20network%20trained%20with%20Eidetic%20Learning%20--%20here%2C%20an%20EideticNet%20--%0Arequires%20no%20rehearsal%20or%20replay.%20We%20consider%20successive%20discrete%20tasks%20and%20show%0Ahow%20at%20inference%20time%20an%20EideticNet%20automatically%20routes%20new%20instances%20without%0Aauxiliary%20task%20information.%20An%20EideticNet%20bears%20a%20family%20resemblance%20to%20the%0Asparsely-gated%20Mixture-of-Experts%20layer%20Shazeer%20et%20al.%20%5B2016%5D%20in%20that%20network%0Acapacity%20is%20partitioned%20across%20tasks%20and%20the%20network%20itself%20performs%0Adata-conditional%20routing.%20An%20EideticNet%20is%20easy%20to%20implement%20and%20train%2C%20is%0Aefficient%2C%20and%20has%20time%20and%20space%20complexity%20linear%20in%20the%20number%20of%0Aparameters.%20The%20guarantee%20of%20our%20method%20holds%20for%20normalization%20layers%20of%0Amodern%20neural%20networks%20during%20both%20pre-training%20and%20fine-tuning.%20We%20show%20with%20a%0Avariety%20of%20network%20architectures%20and%20sets%20of%20tasks%20that%20EideticNets%20are%20immune%0Ato%20forgetting.%20While%20the%20practical%20benefits%20of%20EideticNets%20are%20substantial%2C%20we%0Abelieve%20they%20can%20be%20benefit%20practitioners%20and%20theorists%20alike.%20The%20code%20for%0Atraining%20EideticNets%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/eideticnet-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEidetic%2520Learning%253A%2520an%2520Efficient%2520and%2520Provable%2520Solution%2520to%2520Catastrophic%250A%2520%2520Forgetting%26entry.906535625%3DNicholas%2520Dronen%2520and%2520Randall%2520Balestriero%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520--%2520the%2520phenomenon%2520of%2520a%2520neural%2520network%2520learning%2520a%2520task%250At1%2520and%2520losing%2520the%2520ability%2520to%2520perform%2520it%2520after%2520being%2520trained%2520on%2520some%2520other%2520task%250At2%2520--%2520is%2520a%2520long-standing%2520problem%2520for%2520neural%2520networks%2520%255BMcCloskey%2520and%2520Cohen%252C%250A1989%255D.%2520We%2520present%2520a%2520method%252C%2520Eidetic%2520Learning%252C%2520that%2520provably%2520solves%2520catastrophic%250Aforgetting.%2520A%2520network%2520trained%2520with%2520Eidetic%2520Learning%2520--%2520here%252C%2520an%2520EideticNet%2520--%250Arequires%2520no%2520rehearsal%2520or%2520replay.%2520We%2520consider%2520successive%2520discrete%2520tasks%2520and%2520show%250Ahow%2520at%2520inference%2520time%2520an%2520EideticNet%2520automatically%2520routes%2520new%2520instances%2520without%250Aauxiliary%2520task%2520information.%2520An%2520EideticNet%2520bears%2520a%2520family%2520resemblance%2520to%2520the%250Asparsely-gated%2520Mixture-of-Experts%2520layer%2520Shazeer%2520et%2520al.%2520%255B2016%255D%2520in%2520that%2520network%250Acapacity%2520is%2520partitioned%2520across%2520tasks%2520and%2520the%2520network%2520itself%2520performs%250Adata-conditional%2520routing.%2520An%2520EideticNet%2520is%2520easy%2520to%2520implement%2520and%2520train%252C%2520is%250Aefficient%252C%2520and%2520has%2520time%2520and%2520space%2520complexity%2520linear%2520in%2520the%2520number%2520of%250Aparameters.%2520The%2520guarantee%2520of%2520our%2520method%2520holds%2520for%2520normalization%2520layers%2520of%250Amodern%2520neural%2520networks%2520during%2520both%2520pre-training%2520and%2520fine-tuning.%2520We%2520show%2520with%2520a%250Avariety%2520of%2520network%2520architectures%2520and%2520sets%2520of%2520tasks%2520that%2520EideticNets%2520are%2520immune%250Ato%2520forgetting.%2520While%2520the%2520practical%2520benefits%2520of%2520EideticNets%2520are%2520substantial%252C%2520we%250Abelieve%2520they%2520can%2520be%2520benefit%2520practitioners%2520and%2520theorists%2520alike.%2520The%2520code%2520for%250Atraining%2520EideticNets%2520is%2520available%2520at%250Ahttps%253A//github.com/amazon-science/eideticnet-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eidetic%20Learning%3A%20an%20Efficient%20and%20Provable%20Solution%20to%20Catastrophic%0A%20%20Forgetting&entry.906535625=Nicholas%20Dronen%20and%20Randall%20Balestriero&entry.1292438233=%20%20Catastrophic%20forgetting%20--%20the%20phenomenon%20of%20a%20neural%20network%20learning%20a%20task%0At1%20and%20losing%20the%20ability%20to%20perform%20it%20after%20being%20trained%20on%20some%20other%20task%0At2%20--%20is%20a%20long-standing%20problem%20for%20neural%20networks%20%5BMcCloskey%20and%20Cohen%2C%0A1989%5D.%20We%20present%20a%20method%2C%20Eidetic%20Learning%2C%20that%20provably%20solves%20catastrophic%0Aforgetting.%20A%20network%20trained%20with%20Eidetic%20Learning%20--%20here%2C%20an%20EideticNet%20--%0Arequires%20no%20rehearsal%20or%20replay.%20We%20consider%20successive%20discrete%20tasks%20and%20show%0Ahow%20at%20inference%20time%20an%20EideticNet%20automatically%20routes%20new%20instances%20without%0Aauxiliary%20task%20information.%20An%20EideticNet%20bears%20a%20family%20resemblance%20to%20the%0Asparsely-gated%20Mixture-of-Experts%20layer%20Shazeer%20et%20al.%20%5B2016%5D%20in%20that%20network%0Acapacity%20is%20partitioned%20across%20tasks%20and%20the%20network%20itself%20performs%0Adata-conditional%20routing.%20An%20EideticNet%20is%20easy%20to%20implement%20and%20train%2C%20is%0Aefficient%2C%20and%20has%20time%20and%20space%20complexity%20linear%20in%20the%20number%20of%0Aparameters.%20The%20guarantee%20of%20our%20method%20holds%20for%20normalization%20layers%20of%0Amodern%20neural%20networks%20during%20both%20pre-training%20and%20fine-tuning.%20We%20show%20with%20a%0Avariety%20of%20network%20architectures%20and%20sets%20of%20tasks%20that%20EideticNets%20are%20immune%0Ato%20forgetting.%20While%20the%20practical%20benefits%20of%20EideticNets%20are%20substantial%2C%20we%0Abelieve%20they%20can%20be%20benefit%20practitioners%20and%20theorists%20alike.%20The%20code%20for%0Atraining%20EideticNets%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/eideticnet-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09500v2&entry.124074799=Read"},
{"title": "Modern Hopfield Networks with Continuous-Time Memories", "author": "Saul Santos and Ant\u00f3nio Farinhas and Daniel C. McNamee and Andr\u00e9 F. T. Martins", "abstract": "  Recent research has established a connection between modern Hopfield networks\n(HNs) and transformer attention heads, with guarantees of exponential storage\ncapacity. However, these models still face challenges scaling storage\nefficiently. Inspired by psychological theories of continuous neural resource\nallocation in working memory, we propose an approach that compresses large\ndiscrete Hopfield memories into smaller, continuous-time memories. Leveraging\ncontinuous attention, our new energy function modifies the update rule of HNs,\nreplacing the traditional softmax-based probability mass function with a\nprobability density, over the continuous memory. This formulation aligns with\nmodern perspectives on human executive function, offering a principled link\nbetween attractor dynamics in working memory and resource-efficient memory\nallocation. Our framework maintains competitive performance with HNs while\nleveraging a compressed memory, reducing computational costs across synthetic\nand video datasets.\n", "link": "http://arxiv.org/abs/2502.10122v1", "date": "2025-02-14", "relevancy": 2.0131, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5121}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modern%20Hopfield%20Networks%20with%20Continuous-Time%20Memories&body=Title%3A%20Modern%20Hopfield%20Networks%20with%20Continuous-Time%20Memories%0AAuthor%3A%20Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Recent%20research%20has%20established%20a%20connection%20between%20modern%20Hopfield%20networks%0A%28HNs%29%20and%20transformer%20attention%20heads%2C%20with%20guarantees%20of%20exponential%20storage%0Acapacity.%20However%2C%20these%20models%20still%20face%20challenges%20scaling%20storage%0Aefficiently.%20Inspired%20by%20psychological%20theories%20of%20continuous%20neural%20resource%0Aallocation%20in%20working%20memory%2C%20we%20propose%20an%20approach%20that%20compresses%20large%0Adiscrete%20Hopfield%20memories%20into%20smaller%2C%20continuous-time%20memories.%20Leveraging%0Acontinuous%20attention%2C%20our%20new%20energy%20function%20modifies%20the%20update%20rule%20of%20HNs%2C%0Areplacing%20the%20traditional%20softmax-based%20probability%20mass%20function%20with%20a%0Aprobability%20density%2C%20over%20the%20continuous%20memory.%20This%20formulation%20aligns%20with%0Amodern%20perspectives%20on%20human%20executive%20function%2C%20offering%20a%20principled%20link%0Abetween%20attractor%20dynamics%20in%20working%20memory%20and%20resource-efficient%20memory%0Aallocation.%20Our%20framework%20maintains%20competitive%20performance%20with%20HNs%20while%0Aleveraging%20a%20compressed%20memory%2C%20reducing%20computational%20costs%20across%20synthetic%0Aand%20video%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModern%2520Hopfield%2520Networks%2520with%2520Continuous-Time%2520Memories%26entry.906535625%3DSaul%2520Santos%2520and%2520Ant%25C3%25B3nio%2520Farinhas%2520and%2520Daniel%2520C.%2520McNamee%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520established%2520a%2520connection%2520between%2520modern%2520Hopfield%2520networks%250A%2528HNs%2529%2520and%2520transformer%2520attention%2520heads%252C%2520with%2520guarantees%2520of%2520exponential%2520storage%250Acapacity.%2520However%252C%2520these%2520models%2520still%2520face%2520challenges%2520scaling%2520storage%250Aefficiently.%2520Inspired%2520by%2520psychological%2520theories%2520of%2520continuous%2520neural%2520resource%250Aallocation%2520in%2520working%2520memory%252C%2520we%2520propose%2520an%2520approach%2520that%2520compresses%2520large%250Adiscrete%2520Hopfield%2520memories%2520into%2520smaller%252C%2520continuous-time%2520memories.%2520Leveraging%250Acontinuous%2520attention%252C%2520our%2520new%2520energy%2520function%2520modifies%2520the%2520update%2520rule%2520of%2520HNs%252C%250Areplacing%2520the%2520traditional%2520softmax-based%2520probability%2520mass%2520function%2520with%2520a%250Aprobability%2520density%252C%2520over%2520the%2520continuous%2520memory.%2520This%2520formulation%2520aligns%2520with%250Amodern%2520perspectives%2520on%2520human%2520executive%2520function%252C%2520offering%2520a%2520principled%2520link%250Abetween%2520attractor%2520dynamics%2520in%2520working%2520memory%2520and%2520resource-efficient%2520memory%250Aallocation.%2520Our%2520framework%2520maintains%2520competitive%2520performance%2520with%2520HNs%2520while%250Aleveraging%2520a%2520compressed%2520memory%252C%2520reducing%2520computational%2520costs%2520across%2520synthetic%250Aand%2520video%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modern%20Hopfield%20Networks%20with%20Continuous-Time%20Memories&entry.906535625=Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Recent%20research%20has%20established%20a%20connection%20between%20modern%20Hopfield%20networks%0A%28HNs%29%20and%20transformer%20attention%20heads%2C%20with%20guarantees%20of%20exponential%20storage%0Acapacity.%20However%2C%20these%20models%20still%20face%20challenges%20scaling%20storage%0Aefficiently.%20Inspired%20by%20psychological%20theories%20of%20continuous%20neural%20resource%0Aallocation%20in%20working%20memory%2C%20we%20propose%20an%20approach%20that%20compresses%20large%0Adiscrete%20Hopfield%20memories%20into%20smaller%2C%20continuous-time%20memories.%20Leveraging%0Acontinuous%20attention%2C%20our%20new%20energy%20function%20modifies%20the%20update%20rule%20of%20HNs%2C%0Areplacing%20the%20traditional%20softmax-based%20probability%20mass%20function%20with%20a%0Aprobability%20density%2C%20over%20the%20continuous%20memory.%20This%20formulation%20aligns%20with%0Amodern%20perspectives%20on%20human%20executive%20function%2C%20offering%20a%20principled%20link%0Abetween%20attractor%20dynamics%20in%20working%20memory%20and%20resource-efficient%20memory%0Aallocation.%20Our%20framework%20maintains%20competitive%20performance%20with%20HNs%20while%0Aleveraging%20a%20compressed%20memory%2C%20reducing%20computational%20costs%20across%20synthetic%0Aand%20video%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10122v1&entry.124074799=Read"},
{"title": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers", "author": "Aivin V. Solatorio and Rafael Macalaba and James Liounis", "abstract": "  Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.\n", "link": "http://arxiv.org/abs/2502.10263v1", "date": "2025-02-14", "relevancy": 2.0106, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20and%20Synthetic%20Data%20for%20Monitoring%20Dataset%20Mentions%0A%20%20in%20Research%20Papers&body=Title%3A%20Large%20Language%20Models%20and%20Synthetic%20Data%20for%20Monitoring%20Dataset%20Mentions%0A%20%20in%20Research%20Papers%0AAuthor%3A%20Aivin%20V.%20Solatorio%20and%20Rafael%20Macalaba%20and%20James%20Liounis%0AAbstract%3A%20%20%20Tracking%20how%20data%20is%20mentioned%20and%20used%20in%20research%20papers%20provides%20critical%0Ainsights%20for%20improving%20data%20discoverability%2C%20quality%2C%20and%20production.%20However%2C%0Amanually%20identifying%20and%20classifying%20dataset%20mentions%20across%20vast%20academic%0Aliterature%20is%20resource-intensive%20and%20not%20scalable.%20This%20paper%20presents%20a%0Amachine%20learning%20framework%20that%20automates%20dataset%20mention%20detection%20across%0Aresearch%20domains%20by%20leveraging%20large%20language%20models%20%28LLMs%29%2C%20synthetic%20data%2C%0Aand%20a%20two-stage%20fine-tuning%20process.%20We%20employ%20zero-shot%20extraction%20from%0Aresearch%20papers%2C%20an%20LLM-as-a-Judge%20for%20quality%20assessment%2C%20and%20a%20reasoning%0Aagent%20for%20refinement%20to%20generate%20a%20weakly%20supervised%20synthetic%20dataset.%20The%0APhi-3.5-mini%20instruct%20model%20is%20pre-fine-tuned%20on%20this%20dataset%2C%20followed%20by%0Afine-tuning%20on%20a%20manually%20annotated%20subset.%20At%20inference%2C%20a%20ModernBERT-based%0Aclassifier%20efficiently%20filters%20dataset%20mentions%2C%20reducing%20computational%0Aoverhead%20while%20maintaining%20high%20recall.%20Evaluated%20on%20a%20held-out%20manually%0Aannotated%20sample%2C%20our%20fine-tuned%20model%20outperforms%20NuExtract-v1.5%20and%0AGLiNER-large-v2.1%20in%20dataset%20extraction%20accuracy.%20Our%20results%20highlight%20how%0ALLM-generated%20synthetic%20data%20can%20effectively%20address%20training%20data%20scarcity%2C%0Aimproving%20generalization%20in%20low-resource%20settings.%20This%20framework%20offers%20a%0Apathway%20toward%20scalable%20monitoring%20of%20dataset%20usage%2C%20enhancing%20transparency%2C%0Aand%20supporting%20researchers%2C%20funders%2C%20and%20policymakers%20in%20identifying%20data%20gaps%0Aand%20strengthening%20data%20accessibility%20for%20informed%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520and%2520Synthetic%2520Data%2520for%2520Monitoring%2520Dataset%2520Mentions%250A%2520%2520in%2520Research%2520Papers%26entry.906535625%3DAivin%2520V.%2520Solatorio%2520and%2520Rafael%2520Macalaba%2520and%2520James%2520Liounis%26entry.1292438233%3D%2520%2520Tracking%2520how%2520data%2520is%2520mentioned%2520and%2520used%2520in%2520research%2520papers%2520provides%2520critical%250Ainsights%2520for%2520improving%2520data%2520discoverability%252C%2520quality%252C%2520and%2520production.%2520However%252C%250Amanually%2520identifying%2520and%2520classifying%2520dataset%2520mentions%2520across%2520vast%2520academic%250Aliterature%2520is%2520resource-intensive%2520and%2520not%2520scalable.%2520This%2520paper%2520presents%2520a%250Amachine%2520learning%2520framework%2520that%2520automates%2520dataset%2520mention%2520detection%2520across%250Aresearch%2520domains%2520by%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520synthetic%2520data%252C%250Aand%2520a%2520two-stage%2520fine-tuning%2520process.%2520We%2520employ%2520zero-shot%2520extraction%2520from%250Aresearch%2520papers%252C%2520an%2520LLM-as-a-Judge%2520for%2520quality%2520assessment%252C%2520and%2520a%2520reasoning%250Aagent%2520for%2520refinement%2520to%2520generate%2520a%2520weakly%2520supervised%2520synthetic%2520dataset.%2520The%250APhi-3.5-mini%2520instruct%2520model%2520is%2520pre-fine-tuned%2520on%2520this%2520dataset%252C%2520followed%2520by%250Afine-tuning%2520on%2520a%2520manually%2520annotated%2520subset.%2520At%2520inference%252C%2520a%2520ModernBERT-based%250Aclassifier%2520efficiently%2520filters%2520dataset%2520mentions%252C%2520reducing%2520computational%250Aoverhead%2520while%2520maintaining%2520high%2520recall.%2520Evaluated%2520on%2520a%2520held-out%2520manually%250Aannotated%2520sample%252C%2520our%2520fine-tuned%2520model%2520outperforms%2520NuExtract-v1.5%2520and%250AGLiNER-large-v2.1%2520in%2520dataset%2520extraction%2520accuracy.%2520Our%2520results%2520highlight%2520how%250ALLM-generated%2520synthetic%2520data%2520can%2520effectively%2520address%2520training%2520data%2520scarcity%252C%250Aimproving%2520generalization%2520in%2520low-resource%2520settings.%2520This%2520framework%2520offers%2520a%250Apathway%2520toward%2520scalable%2520monitoring%2520of%2520dataset%2520usage%252C%2520enhancing%2520transparency%252C%250Aand%2520supporting%2520researchers%252C%2520funders%252C%2520and%2520policymakers%2520in%2520identifying%2520data%2520gaps%250Aand%2520strengthening%2520data%2520accessibility%2520for%2520informed%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20and%20Synthetic%20Data%20for%20Monitoring%20Dataset%20Mentions%0A%20%20in%20Research%20Papers&entry.906535625=Aivin%20V.%20Solatorio%20and%20Rafael%20Macalaba%20and%20James%20Liounis&entry.1292438233=%20%20Tracking%20how%20data%20is%20mentioned%20and%20used%20in%20research%20papers%20provides%20critical%0Ainsights%20for%20improving%20data%20discoverability%2C%20quality%2C%20and%20production.%20However%2C%0Amanually%20identifying%20and%20classifying%20dataset%20mentions%20across%20vast%20academic%0Aliterature%20is%20resource-intensive%20and%20not%20scalable.%20This%20paper%20presents%20a%0Amachine%20learning%20framework%20that%20automates%20dataset%20mention%20detection%20across%0Aresearch%20domains%20by%20leveraging%20large%20language%20models%20%28LLMs%29%2C%20synthetic%20data%2C%0Aand%20a%20two-stage%20fine-tuning%20process.%20We%20employ%20zero-shot%20extraction%20from%0Aresearch%20papers%2C%20an%20LLM-as-a-Judge%20for%20quality%20assessment%2C%20and%20a%20reasoning%0Aagent%20for%20refinement%20to%20generate%20a%20weakly%20supervised%20synthetic%20dataset.%20The%0APhi-3.5-mini%20instruct%20model%20is%20pre-fine-tuned%20on%20this%20dataset%2C%20followed%20by%0Afine-tuning%20on%20a%20manually%20annotated%20subset.%20At%20inference%2C%20a%20ModernBERT-based%0Aclassifier%20efficiently%20filters%20dataset%20mentions%2C%20reducing%20computational%0Aoverhead%20while%20maintaining%20high%20recall.%20Evaluated%20on%20a%20held-out%20manually%0Aannotated%20sample%2C%20our%20fine-tuned%20model%20outperforms%20NuExtract-v1.5%20and%0AGLiNER-large-v2.1%20in%20dataset%20extraction%20accuracy.%20Our%20results%20highlight%20how%0ALLM-generated%20synthetic%20data%20can%20effectively%20address%20training%20data%20scarcity%2C%0Aimproving%20generalization%20in%20low-resource%20settings.%20This%20framework%20offers%20a%0Apathway%20toward%20scalable%20monitoring%20of%20dataset%20usage%2C%20enhancing%20transparency%2C%0Aand%20supporting%20researchers%2C%20funders%2C%20and%20policymakers%20in%20identifying%20data%20gaps%0Aand%20strengthening%20data%20accessibility%20for%20informed%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10263v1&entry.124074799=Read"},
{"title": "Compress image to patches for Vision Transformer", "author": "Xinfeng Zhao and Yaoru Sun", "abstract": "  The Vision Transformer (ViT) has made significant strides in the field of\ncomputer vision. However, as the depth of the model and the resolution of the\ninput images increase, the computational cost associated with training and\nrunning ViT models has surged dramatically.This paper proposes a hybrid model\nbased on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a\nmodule called CI2P, which utilizes the CompressAI encoder to compress images\nand subsequently generates a sequence of patches through a series of\nconvolutions. CI2P can replace the Patch Embedding component in the ViT model,\nenabling seamless integration into existing ViT models.Compared to ViT-B/16,\nCI2P-ViT has the number of patches input to the self-attention layer reduced to\na quarter of the original.This design not only significantly reduces the\ncomputational cost of the ViT model but also effectively enhances the model's\naccuracy by introducing the inductive bias properties of CNN.The ViT model's\nprecision is markedly enhanced.When trained from the ground up on the\nAnimals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing\na 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's\ncomputational operations, measured in floating-point operations per second\n(FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in\ntraining velocity on identical hardware configurations.\n", "link": "http://arxiv.org/abs/2502.10120v1", "date": "2025-02-14", "relevancy": 2.0033, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5019}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compress%20image%20to%20patches%20for%20Vision%20Transformer&body=Title%3A%20Compress%20image%20to%20patches%20for%20Vision%20Transformer%0AAuthor%3A%20Xinfeng%20Zhao%20and%20Yaoru%20Sun%0AAbstract%3A%20%20%20The%20Vision%20Transformer%20%28ViT%29%20has%20made%20significant%20strides%20in%20the%20field%20of%0Acomputer%20vision.%20However%2C%20as%20the%20depth%20of%20the%20model%20and%20the%20resolution%20of%20the%0Ainput%20images%20increase%2C%20the%20computational%20cost%20associated%20with%20training%20and%0Arunning%20ViT%20models%20has%20surged%20dramatically.This%20paper%20proposes%20a%20hybrid%20model%0Abased%20on%20CNN%20and%20Vision%20Transformer%2C%20named%20CI2P-ViT.%20The%20model%20incorporates%20a%0Amodule%20called%20CI2P%2C%20which%20utilizes%20the%20CompressAI%20encoder%20to%20compress%20images%0Aand%20subsequently%20generates%20a%20sequence%20of%20patches%20through%20a%20series%20of%0Aconvolutions.%20CI2P%20can%20replace%20the%20Patch%20Embedding%20component%20in%20the%20ViT%20model%2C%0Aenabling%20seamless%20integration%20into%20existing%20ViT%20models.Compared%20to%20ViT-B/16%2C%0ACI2P-ViT%20has%20the%20number%20of%20patches%20input%20to%20the%20self-attention%20layer%20reduced%20to%0Aa%20quarter%20of%20the%20original.This%20design%20not%20only%20significantly%20reduces%20the%0Acomputational%20cost%20of%20the%20ViT%20model%20but%20also%20effectively%20enhances%20the%20model%27s%0Aaccuracy%20by%20introducing%20the%20inductive%20bias%20properties%20of%20CNN.The%20ViT%20model%27s%0Aprecision%20is%20markedly%20enhanced.When%20trained%20from%20the%20ground%20up%20on%20the%0AAnimals-10%20dataset%2C%20CI2P-ViT%20achieved%20an%20accuracy%20rate%20of%2092.37%25%2C%20representing%0Aa%203.3%25%20improvement%20over%20the%20ViT-B/16%20baseline.%20Additionally%2C%20the%20model%27s%0Acomputational%20operations%2C%20measured%20in%20floating-point%20operations%20per%20second%0A%28FLOPs%29%2C%20were%20diminished%20by%2063.35%25%2C%20and%20it%20exhibited%20a%202-fold%20increase%20in%0Atraining%20velocity%20on%20identical%20hardware%20configurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompress%2520image%2520to%2520patches%2520for%2520Vision%2520Transformer%26entry.906535625%3DXinfeng%2520Zhao%2520and%2520Yaoru%2520Sun%26entry.1292438233%3D%2520%2520The%2520Vision%2520Transformer%2520%2528ViT%2529%2520has%2520made%2520significant%2520strides%2520in%2520the%2520field%2520of%250Acomputer%2520vision.%2520However%252C%2520as%2520the%2520depth%2520of%2520the%2520model%2520and%2520the%2520resolution%2520of%2520the%250Ainput%2520images%2520increase%252C%2520the%2520computational%2520cost%2520associated%2520with%2520training%2520and%250Arunning%2520ViT%2520models%2520has%2520surged%2520dramatically.This%2520paper%2520proposes%2520a%2520hybrid%2520model%250Abased%2520on%2520CNN%2520and%2520Vision%2520Transformer%252C%2520named%2520CI2P-ViT.%2520The%2520model%2520incorporates%2520a%250Amodule%2520called%2520CI2P%252C%2520which%2520utilizes%2520the%2520CompressAI%2520encoder%2520to%2520compress%2520images%250Aand%2520subsequently%2520generates%2520a%2520sequence%2520of%2520patches%2520through%2520a%2520series%2520of%250Aconvolutions.%2520CI2P%2520can%2520replace%2520the%2520Patch%2520Embedding%2520component%2520in%2520the%2520ViT%2520model%252C%250Aenabling%2520seamless%2520integration%2520into%2520existing%2520ViT%2520models.Compared%2520to%2520ViT-B/16%252C%250ACI2P-ViT%2520has%2520the%2520number%2520of%2520patches%2520input%2520to%2520the%2520self-attention%2520layer%2520reduced%2520to%250Aa%2520quarter%2520of%2520the%2520original.This%2520design%2520not%2520only%2520significantly%2520reduces%2520the%250Acomputational%2520cost%2520of%2520the%2520ViT%2520model%2520but%2520also%2520effectively%2520enhances%2520the%2520model%2527s%250Aaccuracy%2520by%2520introducing%2520the%2520inductive%2520bias%2520properties%2520of%2520CNN.The%2520ViT%2520model%2527s%250Aprecision%2520is%2520markedly%2520enhanced.When%2520trained%2520from%2520the%2520ground%2520up%2520on%2520the%250AAnimals-10%2520dataset%252C%2520CI2P-ViT%2520achieved%2520an%2520accuracy%2520rate%2520of%252092.37%2525%252C%2520representing%250Aa%25203.3%2525%2520improvement%2520over%2520the%2520ViT-B/16%2520baseline.%2520Additionally%252C%2520the%2520model%2527s%250Acomputational%2520operations%252C%2520measured%2520in%2520floating-point%2520operations%2520per%2520second%250A%2528FLOPs%2529%252C%2520were%2520diminished%2520by%252063.35%2525%252C%2520and%2520it%2520exhibited%2520a%25202-fold%2520increase%2520in%250Atraining%2520velocity%2520on%2520identical%2520hardware%2520configurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compress%20image%20to%20patches%20for%20Vision%20Transformer&entry.906535625=Xinfeng%20Zhao%20and%20Yaoru%20Sun&entry.1292438233=%20%20The%20Vision%20Transformer%20%28ViT%29%20has%20made%20significant%20strides%20in%20the%20field%20of%0Acomputer%20vision.%20However%2C%20as%20the%20depth%20of%20the%20model%20and%20the%20resolution%20of%20the%0Ainput%20images%20increase%2C%20the%20computational%20cost%20associated%20with%20training%20and%0Arunning%20ViT%20models%20has%20surged%20dramatically.This%20paper%20proposes%20a%20hybrid%20model%0Abased%20on%20CNN%20and%20Vision%20Transformer%2C%20named%20CI2P-ViT.%20The%20model%20incorporates%20a%0Amodule%20called%20CI2P%2C%20which%20utilizes%20the%20CompressAI%20encoder%20to%20compress%20images%0Aand%20subsequently%20generates%20a%20sequence%20of%20patches%20through%20a%20series%20of%0Aconvolutions.%20CI2P%20can%20replace%20the%20Patch%20Embedding%20component%20in%20the%20ViT%20model%2C%0Aenabling%20seamless%20integration%20into%20existing%20ViT%20models.Compared%20to%20ViT-B/16%2C%0ACI2P-ViT%20has%20the%20number%20of%20patches%20input%20to%20the%20self-attention%20layer%20reduced%20to%0Aa%20quarter%20of%20the%20original.This%20design%20not%20only%20significantly%20reduces%20the%0Acomputational%20cost%20of%20the%20ViT%20model%20but%20also%20effectively%20enhances%20the%20model%27s%0Aaccuracy%20by%20introducing%20the%20inductive%20bias%20properties%20of%20CNN.The%20ViT%20model%27s%0Aprecision%20is%20markedly%20enhanced.When%20trained%20from%20the%20ground%20up%20on%20the%0AAnimals-10%20dataset%2C%20CI2P-ViT%20achieved%20an%20accuracy%20rate%20of%2092.37%25%2C%20representing%0Aa%203.3%25%20improvement%20over%20the%20ViT-B/16%20baseline.%20Additionally%2C%20the%20model%27s%0Acomputational%20operations%2C%20measured%20in%20floating-point%20operations%20per%20second%0A%28FLOPs%29%2C%20were%20diminished%20by%2063.35%25%2C%20and%20it%20exhibited%20a%202-fold%20increase%20in%0Atraining%20velocity%20on%20identical%20hardware%20configurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10120v1&entry.124074799=Read"},
{"title": "STAR: Spectral Truncation and Rescale for Model Merging", "author": "Yu-Ang Lee and Ching-Yun Ko and Tejaswini Pedapati and I-Hsin Chung and Mi-Yen Yeh and Pin-Yu Chen", "abstract": "  Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.\n", "link": "http://arxiv.org/abs/2502.10339v1", "date": "2025-02-14", "relevancy": 2.0004, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4953}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR%3A%20Spectral%20Truncation%20and%20Rescale%20for%20Model%20Merging&body=Title%3A%20STAR%3A%20Spectral%20Truncation%20and%20Rescale%20for%20Model%20Merging%0AAuthor%3A%20Yu-Ang%20Lee%20and%20Ching-Yun%20Ko%20and%20Tejaswini%20Pedapati%20and%20I-Hsin%20Chung%20and%20Mi-Yen%20Yeh%20and%20Pin-Yu%20Chen%0AAbstract%3A%20%20%20Model%20merging%20is%20an%20efficient%20way%20of%20obtaining%20a%20multi-task%20model%20from%0Aseveral%20pretrained%20models%20without%20further%20fine-tuning%2C%20and%20it%20has%20gained%0Aattention%20in%20various%20domains%2C%20including%20natural%20language%20processing%20%28NLP%29.%0ADespite%20the%20efficiency%2C%20a%20key%20challenge%20in%20model%20merging%20is%20the%20seemingly%0Ainevitable%20decrease%20in%20task%20performance%20as%20the%20number%20of%20models%20increases.%20In%0Athis%20paper%2C%20we%20propose%20%24%5Cmathbf%7BS%7D%24pectral%20%24%5Cmathbf%7BT%7D%24runcation%20%24%5Cmathbf%7BA%7D%24nd%0A%24%5Cmathbf%7BR%7D%24escale%20%28STAR%29%20that%20aims%20at%20mitigating%20%60%60merging%20conflicts%27%27%20by%0Atruncating%20small%20components%20in%20the%20respective%20spectral%20spaces%2C%20which%20is%0Afollowed%20by%20an%20automatic%20parameter%20rescaling%20scheme%20to%20retain%20the%20nuclear%20norm%0Aof%20the%20original%20matrix.%20STAR%20requires%20no%20additional%20inference%20on%20original%0Atraining%20data%20and%20is%20robust%20to%20hyperparamater%20choice.%20We%20demonstrate%20the%0Aeffectiveness%20of%20STAR%20through%20extensive%20model%20merging%20cases%20on%20diverse%20NLP%0Atasks.%20Specifically%2C%20STAR%20works%20robustly%20across%20varying%20model%20sizes%2C%20and%20can%0Aoutperform%20baselines%20by%204.2%24%5C%25%24%20when%20merging%2012%20models%20on%20Flan-T5.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/IBM/STAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR%253A%2520Spectral%2520Truncation%2520and%2520Rescale%2520for%2520Model%2520Merging%26entry.906535625%3DYu-Ang%2520Lee%2520and%2520Ching-Yun%2520Ko%2520and%2520Tejaswini%2520Pedapati%2520and%2520I-Hsin%2520Chung%2520and%2520Mi-Yen%2520Yeh%2520and%2520Pin-Yu%2520Chen%26entry.1292438233%3D%2520%2520Model%2520merging%2520is%2520an%2520efficient%2520way%2520of%2520obtaining%2520a%2520multi-task%2520model%2520from%250Aseveral%2520pretrained%2520models%2520without%2520further%2520fine-tuning%252C%2520and%2520it%2520has%2520gained%250Aattention%2520in%2520various%2520domains%252C%2520including%2520natural%2520language%2520processing%2520%2528NLP%2529.%250ADespite%2520the%2520efficiency%252C%2520a%2520key%2520challenge%2520in%2520model%2520merging%2520is%2520the%2520seemingly%250Ainevitable%2520decrease%2520in%2520task%2520performance%2520as%2520the%2520number%2520of%2520models%2520increases.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520%2524%255Cmathbf%257BS%257D%2524pectral%2520%2524%255Cmathbf%257BT%257D%2524runcation%2520%2524%255Cmathbf%257BA%257D%2524nd%250A%2524%255Cmathbf%257BR%257D%2524escale%2520%2528STAR%2529%2520that%2520aims%2520at%2520mitigating%2520%2560%2560merging%2520conflicts%2527%2527%2520by%250Atruncating%2520small%2520components%2520in%2520the%2520respective%2520spectral%2520spaces%252C%2520which%2520is%250Afollowed%2520by%2520an%2520automatic%2520parameter%2520rescaling%2520scheme%2520to%2520retain%2520the%2520nuclear%2520norm%250Aof%2520the%2520original%2520matrix.%2520STAR%2520requires%2520no%2520additional%2520inference%2520on%2520original%250Atraining%2520data%2520and%2520is%2520robust%2520to%2520hyperparamater%2520choice.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520STAR%2520through%2520extensive%2520model%2520merging%2520cases%2520on%2520diverse%2520NLP%250Atasks.%2520Specifically%252C%2520STAR%2520works%2520robustly%2520across%2520varying%2520model%2520sizes%252C%2520and%2520can%250Aoutperform%2520baselines%2520by%25204.2%2524%255C%2525%2524%2520when%2520merging%252012%2520models%2520on%2520Flan-T5.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/IBM/STAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR%3A%20Spectral%20Truncation%20and%20Rescale%20for%20Model%20Merging&entry.906535625=Yu-Ang%20Lee%20and%20Ching-Yun%20Ko%20and%20Tejaswini%20Pedapati%20and%20I-Hsin%20Chung%20and%20Mi-Yen%20Yeh%20and%20Pin-Yu%20Chen&entry.1292438233=%20%20Model%20merging%20is%20an%20efficient%20way%20of%20obtaining%20a%20multi-task%20model%20from%0Aseveral%20pretrained%20models%20without%20further%20fine-tuning%2C%20and%20it%20has%20gained%0Aattention%20in%20various%20domains%2C%20including%20natural%20language%20processing%20%28NLP%29.%0ADespite%20the%20efficiency%2C%20a%20key%20challenge%20in%20model%20merging%20is%20the%20seemingly%0Ainevitable%20decrease%20in%20task%20performance%20as%20the%20number%20of%20models%20increases.%20In%0Athis%20paper%2C%20we%20propose%20%24%5Cmathbf%7BS%7D%24pectral%20%24%5Cmathbf%7BT%7D%24runcation%20%24%5Cmathbf%7BA%7D%24nd%0A%24%5Cmathbf%7BR%7D%24escale%20%28STAR%29%20that%20aims%20at%20mitigating%20%60%60merging%20conflicts%27%27%20by%0Atruncating%20small%20components%20in%20the%20respective%20spectral%20spaces%2C%20which%20is%0Afollowed%20by%20an%20automatic%20parameter%20rescaling%20scheme%20to%20retain%20the%20nuclear%20norm%0Aof%20the%20original%20matrix.%20STAR%20requires%20no%20additional%20inference%20on%20original%0Atraining%20data%20and%20is%20robust%20to%20hyperparamater%20choice.%20We%20demonstrate%20the%0Aeffectiveness%20of%20STAR%20through%20extensive%20model%20merging%20cases%20on%20diverse%20NLP%0Atasks.%20Specifically%2C%20STAR%20works%20robustly%20across%20varying%20model%20sizes%2C%20and%20can%0Aoutperform%20baselines%20by%204.2%24%5C%25%24%20when%20merging%2012%20models%20on%20Flan-T5.%20Our%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/IBM/STAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10339v1&entry.124074799=Read"},
{"title": "Process Reward Models for LLM Agents: Practical Framework and Directions", "author": "Sanjiban Choudhury", "abstract": "  We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.\n", "link": "http://arxiv.org/abs/2502.10325v1", "date": "2025-02-14", "relevancy": 1.9986, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5059}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process%20Reward%20Models%20for%20LLM%20Agents%3A%20Practical%20Framework%20and%20Directions&body=Title%3A%20Process%20Reward%20Models%20for%20LLM%20Agents%3A%20Practical%20Framework%20and%20Directions%0AAuthor%3A%20Sanjiban%20Choudhury%0AAbstract%3A%20%20%20We%20introduce%20Agent%20Process%20Reward%20Models%20%28AgentPRM%29%2C%20a%20simple%20and%20scalable%0Aframework%20for%20training%20LLM%20agents%20to%20continually%20improve%20through%20interactions.%0AAgentPRM%20follows%20a%20lightweight%20actor-critic%20paradigm%2C%20using%20Monte%20Carlo%0Arollouts%20to%20compute%20reward%20targets%20and%20optimize%20policies.%20It%20requires%20minimal%0Amodifications%20to%20existing%20RLHF%20pipelines%2C%20making%20it%20easy%20to%20integrate%20at%20scale.%0ABeyond%20AgentPRM%2C%20we%20propose%20InversePRM%2C%20which%20learns%20process%20rewards%20directly%0Afrom%20demonstrations%20without%20explicit%20outcome%20supervision.%20We%20also%20explore%20key%0Achallenges%20and%20opportunities%2C%20including%20exploration%2C%20process%20reward%20shaping%2C%0Aand%20model-predictive%20reasoning.%20We%20evaluate%20on%20ALFWorld%20benchmark%2C%20show%20that%0Asmall%203B%20models%20trained%20with%20AgentPRM%20and%20InversePRM%20outperform%20strong%20GPT-4o%0Abaselines%2C%20and%20analyze%20test-time%20scaling%2C%20reward%20hacking%2C%20and%20more.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/sanjibanc/agent_prm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess%2520Reward%2520Models%2520for%2520LLM%2520Agents%253A%2520Practical%2520Framework%2520and%2520Directions%26entry.906535625%3DSanjiban%2520Choudhury%26entry.1292438233%3D%2520%2520We%2520introduce%2520Agent%2520Process%2520Reward%2520Models%2520%2528AgentPRM%2529%252C%2520a%2520simple%2520and%2520scalable%250Aframework%2520for%2520training%2520LLM%2520agents%2520to%2520continually%2520improve%2520through%2520interactions.%250AAgentPRM%2520follows%2520a%2520lightweight%2520actor-critic%2520paradigm%252C%2520using%2520Monte%2520Carlo%250Arollouts%2520to%2520compute%2520reward%2520targets%2520and%2520optimize%2520policies.%2520It%2520requires%2520minimal%250Amodifications%2520to%2520existing%2520RLHF%2520pipelines%252C%2520making%2520it%2520easy%2520to%2520integrate%2520at%2520scale.%250ABeyond%2520AgentPRM%252C%2520we%2520propose%2520InversePRM%252C%2520which%2520learns%2520process%2520rewards%2520directly%250Afrom%2520demonstrations%2520without%2520explicit%2520outcome%2520supervision.%2520We%2520also%2520explore%2520key%250Achallenges%2520and%2520opportunities%252C%2520including%2520exploration%252C%2520process%2520reward%2520shaping%252C%250Aand%2520model-predictive%2520reasoning.%2520We%2520evaluate%2520on%2520ALFWorld%2520benchmark%252C%2520show%2520that%250Asmall%25203B%2520models%2520trained%2520with%2520AgentPRM%2520and%2520InversePRM%2520outperform%2520strong%2520GPT-4o%250Abaselines%252C%2520and%2520analyze%2520test-time%2520scaling%252C%2520reward%2520hacking%252C%2520and%2520more.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/sanjibanc/agent_prm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process%20Reward%20Models%20for%20LLM%20Agents%3A%20Practical%20Framework%20and%20Directions&entry.906535625=Sanjiban%20Choudhury&entry.1292438233=%20%20We%20introduce%20Agent%20Process%20Reward%20Models%20%28AgentPRM%29%2C%20a%20simple%20and%20scalable%0Aframework%20for%20training%20LLM%20agents%20to%20continually%20improve%20through%20interactions.%0AAgentPRM%20follows%20a%20lightweight%20actor-critic%20paradigm%2C%20using%20Monte%20Carlo%0Arollouts%20to%20compute%20reward%20targets%20and%20optimize%20policies.%20It%20requires%20minimal%0Amodifications%20to%20existing%20RLHF%20pipelines%2C%20making%20it%20easy%20to%20integrate%20at%20scale.%0ABeyond%20AgentPRM%2C%20we%20propose%20InversePRM%2C%20which%20learns%20process%20rewards%20directly%0Afrom%20demonstrations%20without%20explicit%20outcome%20supervision.%20We%20also%20explore%20key%0Achallenges%20and%20opportunities%2C%20including%20exploration%2C%20process%20reward%20shaping%2C%0Aand%20model-predictive%20reasoning.%20We%20evaluate%20on%20ALFWorld%20benchmark%2C%20show%20that%0Asmall%203B%20models%20trained%20with%20AgentPRM%20and%20InversePRM%20outperform%20strong%20GPT-4o%0Abaselines%2C%20and%20analyze%20test-time%20scaling%2C%20reward%20hacking%2C%20and%20more.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/sanjibanc/agent_prm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10325v1&entry.124074799=Read"},
{"title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute", "author": "Hongwei Yi and Shitong Shao and Tian Ye and Jiantong Zhao and Qingyu Yin and Michael Lingelbach and Li Yuan and Yonghong Tian and Enze Xie and Daquan Zhou", "abstract": "  In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.\n", "link": "http://arxiv.org/abs/2502.07701v2", "date": "2025-02-14", "relevancy": 1.9981, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7014}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6419}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Magic%201-For-1%3A%20Generating%20One%20Minute%20Video%20Clips%20within%20One%20Minute&body=Title%3A%20Magic%201-For-1%3A%20Generating%20One%20Minute%20Video%20Clips%20within%20One%20Minute%0AAuthor%3A%20Hongwei%20Yi%20and%20Shitong%20Shao%20and%20Tian%20Ye%20and%20Jiantong%20Zhao%20and%20Qingyu%20Yin%20and%20Michael%20Lingelbach%20and%20Li%20Yuan%20and%20Yonghong%20Tian%20and%20Enze%20Xie%20and%20Daquan%20Zhou%0AAbstract%3A%20%20%20In%20this%20technical%20report%2C%20we%20present%20Magic%201-For-1%20%28Magic141%29%2C%20an%20efficient%0Avideo%20generation%20model%20with%20optimized%20memory%20consumption%20and%20inference%20latency.%0AThe%20key%20idea%20is%20simple%3A%20factorize%20the%20text-to-video%20generation%20task%20into%20two%0Aseparate%20easier%20tasks%20for%20diffusion%20step%20distillation%2C%20namely%20text-to-image%0Ageneration%20and%20image-to-video%20generation.%20We%20verify%20that%20with%20the%20same%0Aoptimization%20algorithm%2C%20the%20image-to-video%20task%20is%20indeed%20easier%20to%20converge%0Aover%20the%20text-to-video%20task.%20We%20also%20explore%20a%20bag%20of%20optimization%20tricks%20to%0Areduce%20the%20computational%20cost%20of%20training%20the%20image-to-video%20%28I2V%29%20models%20from%0Athree%20aspects%3A%201%29%20model%20convergence%20speedup%20by%20using%20a%20multi-modal%20prior%0Acondition%20injection%3B%202%29%20inference%20latency%20speed%20up%20by%20applying%20an%20adversarial%0Astep%20distillation%2C%20and%203%29%20inference%20memory%20cost%20optimization%20with%20parameter%0Asparsification.%20With%20those%20techniques%2C%20we%20are%20able%20to%20generate%205-second%20video%0Aclips%20within%203%20seconds.%20By%20applying%20a%20test%20time%20sliding%20window%2C%20we%20are%20able%20to%0Agenerate%20a%20minute-long%20video%20within%20one%20minute%20with%20significantly%20improved%0Avisual%20quality%20and%20motion%20dynamics%2C%20spending%20less%20than%201%20second%20for%20generating%0A1%20second%20video%20clips%20on%20average.%20We%20conduct%20a%20series%20of%20preliminary%0Aexplorations%20to%20find%20out%20the%20optimal%20tradeoff%20between%20computational%20cost%20and%0Avideo%20quality%20during%20diffusion%20step%20distillation%20and%20hope%20this%20could%20be%20a%20good%0Afoundation%20model%20for%20open-source%20explorations.%20The%20code%20and%20the%20model%20weights%0Aare%20available%20at%20https%3A//github.com/DA-Group-PKU/Magic-1-For-1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagic%25201-For-1%253A%2520Generating%2520One%2520Minute%2520Video%2520Clips%2520within%2520One%2520Minute%26entry.906535625%3DHongwei%2520Yi%2520and%2520Shitong%2520Shao%2520and%2520Tian%2520Ye%2520and%2520Jiantong%2520Zhao%2520and%2520Qingyu%2520Yin%2520and%2520Michael%2520Lingelbach%2520and%2520Li%2520Yuan%2520and%2520Yonghong%2520Tian%2520and%2520Enze%2520Xie%2520and%2520Daquan%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520technical%2520report%252C%2520we%2520present%2520Magic%25201-For-1%2520%2528Magic141%2529%252C%2520an%2520efficient%250Avideo%2520generation%2520model%2520with%2520optimized%2520memory%2520consumption%2520and%2520inference%2520latency.%250AThe%2520key%2520idea%2520is%2520simple%253A%2520factorize%2520the%2520text-to-video%2520generation%2520task%2520into%2520two%250Aseparate%2520easier%2520tasks%2520for%2520diffusion%2520step%2520distillation%252C%2520namely%2520text-to-image%250Ageneration%2520and%2520image-to-video%2520generation.%2520We%2520verify%2520that%2520with%2520the%2520same%250Aoptimization%2520algorithm%252C%2520the%2520image-to-video%2520task%2520is%2520indeed%2520easier%2520to%2520converge%250Aover%2520the%2520text-to-video%2520task.%2520We%2520also%2520explore%2520a%2520bag%2520of%2520optimization%2520tricks%2520to%250Areduce%2520the%2520computational%2520cost%2520of%2520training%2520the%2520image-to-video%2520%2528I2V%2529%2520models%2520from%250Athree%2520aspects%253A%25201%2529%2520model%2520convergence%2520speedup%2520by%2520using%2520a%2520multi-modal%2520prior%250Acondition%2520injection%253B%25202%2529%2520inference%2520latency%2520speed%2520up%2520by%2520applying%2520an%2520adversarial%250Astep%2520distillation%252C%2520and%25203%2529%2520inference%2520memory%2520cost%2520optimization%2520with%2520parameter%250Asparsification.%2520With%2520those%2520techniques%252C%2520we%2520are%2520able%2520to%2520generate%25205-second%2520video%250Aclips%2520within%25203%2520seconds.%2520By%2520applying%2520a%2520test%2520time%2520sliding%2520window%252C%2520we%2520are%2520able%2520to%250Agenerate%2520a%2520minute-long%2520video%2520within%2520one%2520minute%2520with%2520significantly%2520improved%250Avisual%2520quality%2520and%2520motion%2520dynamics%252C%2520spending%2520less%2520than%25201%2520second%2520for%2520generating%250A1%2520second%2520video%2520clips%2520on%2520average.%2520We%2520conduct%2520a%2520series%2520of%2520preliminary%250Aexplorations%2520to%2520find%2520out%2520the%2520optimal%2520tradeoff%2520between%2520computational%2520cost%2520and%250Avideo%2520quality%2520during%2520diffusion%2520step%2520distillation%2520and%2520hope%2520this%2520could%2520be%2520a%2520good%250Afoundation%2520model%2520for%2520open-source%2520explorations.%2520The%2520code%2520and%2520the%2520model%2520weights%250Aare%2520available%2520at%2520https%253A//github.com/DA-Group-PKU/Magic-1-For-1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magic%201-For-1%3A%20Generating%20One%20Minute%20Video%20Clips%20within%20One%20Minute&entry.906535625=Hongwei%20Yi%20and%20Shitong%20Shao%20and%20Tian%20Ye%20and%20Jiantong%20Zhao%20and%20Qingyu%20Yin%20and%20Michael%20Lingelbach%20and%20Li%20Yuan%20and%20Yonghong%20Tian%20and%20Enze%20Xie%20and%20Daquan%20Zhou&entry.1292438233=%20%20In%20this%20technical%20report%2C%20we%20present%20Magic%201-For-1%20%28Magic141%29%2C%20an%20efficient%0Avideo%20generation%20model%20with%20optimized%20memory%20consumption%20and%20inference%20latency.%0AThe%20key%20idea%20is%20simple%3A%20factorize%20the%20text-to-video%20generation%20task%20into%20two%0Aseparate%20easier%20tasks%20for%20diffusion%20step%20distillation%2C%20namely%20text-to-image%0Ageneration%20and%20image-to-video%20generation.%20We%20verify%20that%20with%20the%20same%0Aoptimization%20algorithm%2C%20the%20image-to-video%20task%20is%20indeed%20easier%20to%20converge%0Aover%20the%20text-to-video%20task.%20We%20also%20explore%20a%20bag%20of%20optimization%20tricks%20to%0Areduce%20the%20computational%20cost%20of%20training%20the%20image-to-video%20%28I2V%29%20models%20from%0Athree%20aspects%3A%201%29%20model%20convergence%20speedup%20by%20using%20a%20multi-modal%20prior%0Acondition%20injection%3B%202%29%20inference%20latency%20speed%20up%20by%20applying%20an%20adversarial%0Astep%20distillation%2C%20and%203%29%20inference%20memory%20cost%20optimization%20with%20parameter%0Asparsification.%20With%20those%20techniques%2C%20we%20are%20able%20to%20generate%205-second%20video%0Aclips%20within%203%20seconds.%20By%20applying%20a%20test%20time%20sliding%20window%2C%20we%20are%20able%20to%0Agenerate%20a%20minute-long%20video%20within%20one%20minute%20with%20significantly%20improved%0Avisual%20quality%20and%20motion%20dynamics%2C%20spending%20less%20than%201%20second%20for%20generating%0A1%20second%20video%20clips%20on%20average.%20We%20conduct%20a%20series%20of%20preliminary%0Aexplorations%20to%20find%20out%20the%20optimal%20tradeoff%20between%20computational%20cost%20and%0Avideo%20quality%20during%20diffusion%20step%20distillation%20and%20hope%20this%20could%20be%20a%20good%0Afoundation%20model%20for%20open-source%20explorations.%20The%20code%20and%20the%20model%20weights%0Aare%20available%20at%20https%3A//github.com/DA-Group-PKU/Magic-1-For-1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07701v2&entry.124074799=Read"},
{"title": "Enhancing anomaly detection with topology-aware autoencoders", "author": "Vishal S. Ngairangbam and B\u0142a\u017cej Rozwoda and Kazuki Sakurai and Michael Spannowsky", "abstract": "  Anomaly detection in high-energy physics is essential for identifying new\nphysics beyond the Standard Model. Autoencoders provide a signal-agnostic\napproach but are limited by the topology of their latent space. This work\nexplores topology-aware autoencoders, embedding phase-space distributions onto\ncompact manifolds that reflect energy-momentum conservation. We construct\nautoencoders with spherical ($S^n$), product ($S^2 \\otimes S^2$), and\nprojective ($\\mathbb{RP}^2$) latent spaces and compare their anomaly detection\nperformance against conventional Euclidean embeddings. Our results show that\nautoencoders with topological priors significantly improve anomaly separation\nby preserving the global structure of the data manifold and reducing spurious\nreconstruction errors. Applying our approach to simulated hadronic top-quark\ndecays, we show that latent spaces with appropriate topological constraints\nenhance sensitivity and robustness in detecting anomalous events. This study\nestablishes topology-aware autoencoders as a powerful tool for unsupervised\nsearches for new physics in particle-collision data.\n", "link": "http://arxiv.org/abs/2502.10163v1", "date": "2025-02-14", "relevancy": 1.9959, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4879}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20anomaly%20detection%20with%20topology-aware%20autoencoders&body=Title%3A%20Enhancing%20anomaly%20detection%20with%20topology-aware%20autoencoders%0AAuthor%3A%20Vishal%20S.%20Ngairangbam%20and%20B%C5%82a%C5%BCej%20Rozwoda%20and%20Kazuki%20Sakurai%20and%20Michael%20Spannowsky%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20high-energy%20physics%20is%20essential%20for%20identifying%20new%0Aphysics%20beyond%20the%20Standard%20Model.%20Autoencoders%20provide%20a%20signal-agnostic%0Aapproach%20but%20are%20limited%20by%20the%20topology%20of%20their%20latent%20space.%20This%20work%0Aexplores%20topology-aware%20autoencoders%2C%20embedding%20phase-space%20distributions%20onto%0Acompact%20manifolds%20that%20reflect%20energy-momentum%20conservation.%20We%20construct%0Aautoencoders%20with%20spherical%20%28%24S%5En%24%29%2C%20product%20%28%24S%5E2%20%5Cotimes%20S%5E2%24%29%2C%20and%0Aprojective%20%28%24%5Cmathbb%7BRP%7D%5E2%24%29%20latent%20spaces%20and%20compare%20their%20anomaly%20detection%0Aperformance%20against%20conventional%20Euclidean%20embeddings.%20Our%20results%20show%20that%0Aautoencoders%20with%20topological%20priors%20significantly%20improve%20anomaly%20separation%0Aby%20preserving%20the%20global%20structure%20of%20the%20data%20manifold%20and%20reducing%20spurious%0Areconstruction%20errors.%20Applying%20our%20approach%20to%20simulated%20hadronic%20top-quark%0Adecays%2C%20we%20show%20that%20latent%20spaces%20with%20appropriate%20topological%20constraints%0Aenhance%20sensitivity%20and%20robustness%20in%20detecting%20anomalous%20events.%20This%20study%0Aestablishes%20topology-aware%20autoencoders%20as%20a%20powerful%20tool%20for%20unsupervised%0Asearches%20for%20new%20physics%20in%20particle-collision%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520anomaly%2520detection%2520with%2520topology-aware%2520autoencoders%26entry.906535625%3DVishal%2520S.%2520Ngairangbam%2520and%2520B%25C5%2582a%25C5%25BCej%2520Rozwoda%2520and%2520Kazuki%2520Sakurai%2520and%2520Michael%2520Spannowsky%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520high-energy%2520physics%2520is%2520essential%2520for%2520identifying%2520new%250Aphysics%2520beyond%2520the%2520Standard%2520Model.%2520Autoencoders%2520provide%2520a%2520signal-agnostic%250Aapproach%2520but%2520are%2520limited%2520by%2520the%2520topology%2520of%2520their%2520latent%2520space.%2520This%2520work%250Aexplores%2520topology-aware%2520autoencoders%252C%2520embedding%2520phase-space%2520distributions%2520onto%250Acompact%2520manifolds%2520that%2520reflect%2520energy-momentum%2520conservation.%2520We%2520construct%250Aautoencoders%2520with%2520spherical%2520%2528%2524S%255En%2524%2529%252C%2520product%2520%2528%2524S%255E2%2520%255Cotimes%2520S%255E2%2524%2529%252C%2520and%250Aprojective%2520%2528%2524%255Cmathbb%257BRP%257D%255E2%2524%2529%2520latent%2520spaces%2520and%2520compare%2520their%2520anomaly%2520detection%250Aperformance%2520against%2520conventional%2520Euclidean%2520embeddings.%2520Our%2520results%2520show%2520that%250Aautoencoders%2520with%2520topological%2520priors%2520significantly%2520improve%2520anomaly%2520separation%250Aby%2520preserving%2520the%2520global%2520structure%2520of%2520the%2520data%2520manifold%2520and%2520reducing%2520spurious%250Areconstruction%2520errors.%2520Applying%2520our%2520approach%2520to%2520simulated%2520hadronic%2520top-quark%250Adecays%252C%2520we%2520show%2520that%2520latent%2520spaces%2520with%2520appropriate%2520topological%2520constraints%250Aenhance%2520sensitivity%2520and%2520robustness%2520in%2520detecting%2520anomalous%2520events.%2520This%2520study%250Aestablishes%2520topology-aware%2520autoencoders%2520as%2520a%2520powerful%2520tool%2520for%2520unsupervised%250Asearches%2520for%2520new%2520physics%2520in%2520particle-collision%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20anomaly%20detection%20with%20topology-aware%20autoencoders&entry.906535625=Vishal%20S.%20Ngairangbam%20and%20B%C5%82a%C5%BCej%20Rozwoda%20and%20Kazuki%20Sakurai%20and%20Michael%20Spannowsky&entry.1292438233=%20%20Anomaly%20detection%20in%20high-energy%20physics%20is%20essential%20for%20identifying%20new%0Aphysics%20beyond%20the%20Standard%20Model.%20Autoencoders%20provide%20a%20signal-agnostic%0Aapproach%20but%20are%20limited%20by%20the%20topology%20of%20their%20latent%20space.%20This%20work%0Aexplores%20topology-aware%20autoencoders%2C%20embedding%20phase-space%20distributions%20onto%0Acompact%20manifolds%20that%20reflect%20energy-momentum%20conservation.%20We%20construct%0Aautoencoders%20with%20spherical%20%28%24S%5En%24%29%2C%20product%20%28%24S%5E2%20%5Cotimes%20S%5E2%24%29%2C%20and%0Aprojective%20%28%24%5Cmathbb%7BRP%7D%5E2%24%29%20latent%20spaces%20and%20compare%20their%20anomaly%20detection%0Aperformance%20against%20conventional%20Euclidean%20embeddings.%20Our%20results%20show%20that%0Aautoencoders%20with%20topological%20priors%20significantly%20improve%20anomaly%20separation%0Aby%20preserving%20the%20global%20structure%20of%20the%20data%20manifold%20and%20reducing%20spurious%0Areconstruction%20errors.%20Applying%20our%20approach%20to%20simulated%20hadronic%20top-quark%0Adecays%2C%20we%20show%20that%20latent%20spaces%20with%20appropriate%20topological%20constraints%0Aenhance%20sensitivity%20and%20robustness%20in%20detecting%20anomalous%20events.%20This%20study%0Aestablishes%20topology-aware%20autoencoders%20as%20a%20powerful%20tool%20for%20unsupervised%0Asearches%20for%20new%20physics%20in%20particle-collision%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10163v1&entry.124074799=Read"},
{"title": "Willingness to Read AI-Generated News Is Not Driven by Their Perceived\n  Quality", "author": "Fabrizio Gilardi and Sabrina Di Lorenzo and Juri Ezzaini and Beryl Santa and Benjamin Streiff and Eric Zurfluh and Emma Hoes", "abstract": "  The advancement of artificial intelligence has led to its application in many\nareas, including news media, which makes it crucial to understand public\nreception of AI-generated news. This preregistered study investigates (i) the\nperceived quality of AI-assisted and AI-generated versus human-generated news\narticles, (ii) whether disclosure of AI's involvement in generating these news\narticles influences engagement with them, and (iii) whether such awareness\naffects the willingness to read AI-generated articles in the future. We\nconducted a survey experiment with 599 Swiss participants, who evaluated the\ncredibility, readability, and expertise of news articles either written by\njournalists (control group), rewritten by AI (AI-assisted group), or entirely\nwritten by AI (AI-generated group). Our results indicate that all articles were\nperceived to be of equal quality. When participants in the treatment groups\nwere subsequently made aware of AI's role, they expressed a higher willingness\nto continue reading the articles than participants in the control group.\nHowever, they were not more willing to read AI-generated news in the future.\nThese results suggest that aversion to AI usage in news media is not primarily\nrooted in a perceived lack of quality, and that by disclosing using AI,\njournalists could induce more short-term engagement.\n", "link": "http://arxiv.org/abs/2409.03500v3", "date": "2025-02-14", "relevancy": 1.986, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3989}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.398}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Willingness%20to%20Read%20AI-Generated%20News%20Is%20Not%20Driven%20by%20Their%20Perceived%0A%20%20Quality&body=Title%3A%20Willingness%20to%20Read%20AI-Generated%20News%20Is%20Not%20Driven%20by%20Their%20Perceived%0A%20%20Quality%0AAuthor%3A%20Fabrizio%20Gilardi%20and%20Sabrina%20Di%20Lorenzo%20and%20Juri%20Ezzaini%20and%20Beryl%20Santa%20and%20Benjamin%20Streiff%20and%20Eric%20Zurfluh%20and%20Emma%20Hoes%0AAbstract%3A%20%20%20The%20advancement%20of%20artificial%20intelligence%20has%20led%20to%20its%20application%20in%20many%0Aareas%2C%20including%20news%20media%2C%20which%20makes%20it%20crucial%20to%20understand%20public%0Areception%20of%20AI-generated%20news.%20This%20preregistered%20study%20investigates%20%28i%29%20the%0Aperceived%20quality%20of%20AI-assisted%20and%20AI-generated%20versus%20human-generated%20news%0Aarticles%2C%20%28ii%29%20whether%20disclosure%20of%20AI%27s%20involvement%20in%20generating%20these%20news%0Aarticles%20influences%20engagement%20with%20them%2C%20and%20%28iii%29%20whether%20such%20awareness%0Aaffects%20the%20willingness%20to%20read%20AI-generated%20articles%20in%20the%20future.%20We%0Aconducted%20a%20survey%20experiment%20with%20599%20Swiss%20participants%2C%20who%20evaluated%20the%0Acredibility%2C%20readability%2C%20and%20expertise%20of%20news%20articles%20either%20written%20by%0Ajournalists%20%28control%20group%29%2C%20rewritten%20by%20AI%20%28AI-assisted%20group%29%2C%20or%20entirely%0Awritten%20by%20AI%20%28AI-generated%20group%29.%20Our%20results%20indicate%20that%20all%20articles%20were%0Aperceived%20to%20be%20of%20equal%20quality.%20When%20participants%20in%20the%20treatment%20groups%0Awere%20subsequently%20made%20aware%20of%20AI%27s%20role%2C%20they%20expressed%20a%20higher%20willingness%0Ato%20continue%20reading%20the%20articles%20than%20participants%20in%20the%20control%20group.%0AHowever%2C%20they%20were%20not%20more%20willing%20to%20read%20AI-generated%20news%20in%20the%20future.%0AThese%20results%20suggest%20that%20aversion%20to%20AI%20usage%20in%20news%20media%20is%20not%20primarily%0Arooted%20in%20a%20perceived%20lack%20of%20quality%2C%20and%20that%20by%20disclosing%20using%20AI%2C%0Ajournalists%20could%20induce%20more%20short-term%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03500v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWillingness%2520to%2520Read%2520AI-Generated%2520News%2520Is%2520Not%2520Driven%2520by%2520Their%2520Perceived%250A%2520%2520Quality%26entry.906535625%3DFabrizio%2520Gilardi%2520and%2520Sabrina%2520Di%2520Lorenzo%2520and%2520Juri%2520Ezzaini%2520and%2520Beryl%2520Santa%2520and%2520Benjamin%2520Streiff%2520and%2520Eric%2520Zurfluh%2520and%2520Emma%2520Hoes%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520artificial%2520intelligence%2520has%2520led%2520to%2520its%2520application%2520in%2520many%250Aareas%252C%2520including%2520news%2520media%252C%2520which%2520makes%2520it%2520crucial%2520to%2520understand%2520public%250Areception%2520of%2520AI-generated%2520news.%2520This%2520preregistered%2520study%2520investigates%2520%2528i%2529%2520the%250Aperceived%2520quality%2520of%2520AI-assisted%2520and%2520AI-generated%2520versus%2520human-generated%2520news%250Aarticles%252C%2520%2528ii%2529%2520whether%2520disclosure%2520of%2520AI%2527s%2520involvement%2520in%2520generating%2520these%2520news%250Aarticles%2520influences%2520engagement%2520with%2520them%252C%2520and%2520%2528iii%2529%2520whether%2520such%2520awareness%250Aaffects%2520the%2520willingness%2520to%2520read%2520AI-generated%2520articles%2520in%2520the%2520future.%2520We%250Aconducted%2520a%2520survey%2520experiment%2520with%2520599%2520Swiss%2520participants%252C%2520who%2520evaluated%2520the%250Acredibility%252C%2520readability%252C%2520and%2520expertise%2520of%2520news%2520articles%2520either%2520written%2520by%250Ajournalists%2520%2528control%2520group%2529%252C%2520rewritten%2520by%2520AI%2520%2528AI-assisted%2520group%2529%252C%2520or%2520entirely%250Awritten%2520by%2520AI%2520%2528AI-generated%2520group%2529.%2520Our%2520results%2520indicate%2520that%2520all%2520articles%2520were%250Aperceived%2520to%2520be%2520of%2520equal%2520quality.%2520When%2520participants%2520in%2520the%2520treatment%2520groups%250Awere%2520subsequently%2520made%2520aware%2520of%2520AI%2527s%2520role%252C%2520they%2520expressed%2520a%2520higher%2520willingness%250Ato%2520continue%2520reading%2520the%2520articles%2520than%2520participants%2520in%2520the%2520control%2520group.%250AHowever%252C%2520they%2520were%2520not%2520more%2520willing%2520to%2520read%2520AI-generated%2520news%2520in%2520the%2520future.%250AThese%2520results%2520suggest%2520that%2520aversion%2520to%2520AI%2520usage%2520in%2520news%2520media%2520is%2520not%2520primarily%250Arooted%2520in%2520a%2520perceived%2520lack%2520of%2520quality%252C%2520and%2520that%2520by%2520disclosing%2520using%2520AI%252C%250Ajournalists%2520could%2520induce%2520more%2520short-term%2520engagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03500v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Willingness%20to%20Read%20AI-Generated%20News%20Is%20Not%20Driven%20by%20Their%20Perceived%0A%20%20Quality&entry.906535625=Fabrizio%20Gilardi%20and%20Sabrina%20Di%20Lorenzo%20and%20Juri%20Ezzaini%20and%20Beryl%20Santa%20and%20Benjamin%20Streiff%20and%20Eric%20Zurfluh%20and%20Emma%20Hoes&entry.1292438233=%20%20The%20advancement%20of%20artificial%20intelligence%20has%20led%20to%20its%20application%20in%20many%0Aareas%2C%20including%20news%20media%2C%20which%20makes%20it%20crucial%20to%20understand%20public%0Areception%20of%20AI-generated%20news.%20This%20preregistered%20study%20investigates%20%28i%29%20the%0Aperceived%20quality%20of%20AI-assisted%20and%20AI-generated%20versus%20human-generated%20news%0Aarticles%2C%20%28ii%29%20whether%20disclosure%20of%20AI%27s%20involvement%20in%20generating%20these%20news%0Aarticles%20influences%20engagement%20with%20them%2C%20and%20%28iii%29%20whether%20such%20awareness%0Aaffects%20the%20willingness%20to%20read%20AI-generated%20articles%20in%20the%20future.%20We%0Aconducted%20a%20survey%20experiment%20with%20599%20Swiss%20participants%2C%20who%20evaluated%20the%0Acredibility%2C%20readability%2C%20and%20expertise%20of%20news%20articles%20either%20written%20by%0Ajournalists%20%28control%20group%29%2C%20rewritten%20by%20AI%20%28AI-assisted%20group%29%2C%20or%20entirely%0Awritten%20by%20AI%20%28AI-generated%20group%29.%20Our%20results%20indicate%20that%20all%20articles%20were%0Aperceived%20to%20be%20of%20equal%20quality.%20When%20participants%20in%20the%20treatment%20groups%0Awere%20subsequently%20made%20aware%20of%20AI%27s%20role%2C%20they%20expressed%20a%20higher%20willingness%0Ato%20continue%20reading%20the%20articles%20than%20participants%20in%20the%20control%20group.%0AHowever%2C%20they%20were%20not%20more%20willing%20to%20read%20AI-generated%20news%20in%20the%20future.%0AThese%20results%20suggest%20that%20aversion%20to%20AI%20usage%20in%20news%20media%20is%20not%20primarily%0Arooted%20in%20a%20perceived%20lack%20of%20quality%2C%20and%20that%20by%20disclosing%20using%20AI%2C%0Ajournalists%20could%20induce%20more%20short-term%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03500v3&entry.124074799=Read"},
{"title": "OWLS: Scaling Laws for Multilingual Speech Recognition and Translation\n  Models", "author": "William Chen and Jinchuan Tian and Yifan Peng and Brian Yan and Chao-Han Huck Yang and Shinji Watanabe", "abstract": "  Neural scaling laws offer valuable insights for designing robust sequence\nprocessing architectures. While these laws have been extensively characterized\nin other modalities, their behavior in speech remains comparatively\nunderexplored. In this work, we introduce OWLS, an open-access, reproducible\nsuite of multilingual speech recognition and translation models spanning 0.25B\nto 18B parameters, with the 18B version being the largest speech model, to the\nbest of our knowledge. OWLS leverages up to 360K hours of public speech data\nacross 150 languages, enabling a systematic investigation into how data, model,\nand compute scaling each influence performance in multilingual speech tasks. We\nuse OWLS to derive neural scaling laws, showing how final performance can be\nreliably predicted when scaling. One of our key findings is that scaling\nenhances performance on low-resource languages/dialects, helping to mitigate\nbias and improve the accessibility of speech technologies. Finally, we show how\nOWLS can be used to power new research directions by discovering emergent\nabilities in large-scale speech models. Model checkpoints will be released on\nhttps://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d\nfor future studies.\n", "link": "http://arxiv.org/abs/2502.10373v1", "date": "2025-02-14", "relevancy": 1.9844, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OWLS%3A%20Scaling%20Laws%20for%20Multilingual%20Speech%20Recognition%20and%20Translation%0A%20%20Models&body=Title%3A%20OWLS%3A%20Scaling%20Laws%20for%20Multilingual%20Speech%20Recognition%20and%20Translation%0A%20%20Models%0AAuthor%3A%20William%20Chen%20and%20Jinchuan%20Tian%20and%20Yifan%20Peng%20and%20Brian%20Yan%20and%20Chao-Han%20Huck%20Yang%20and%20Shinji%20Watanabe%0AAbstract%3A%20%20%20Neural%20scaling%20laws%20offer%20valuable%20insights%20for%20designing%20robust%20sequence%0Aprocessing%20architectures.%20While%20these%20laws%20have%20been%20extensively%20characterized%0Ain%20other%20modalities%2C%20their%20behavior%20in%20speech%20remains%20comparatively%0Aunderexplored.%20In%20this%20work%2C%20we%20introduce%20OWLS%2C%20an%20open-access%2C%20reproducible%0Asuite%20of%20multilingual%20speech%20recognition%20and%20translation%20models%20spanning%200.25B%0Ato%2018B%20parameters%2C%20with%20the%2018B%20version%20being%20the%20largest%20speech%20model%2C%20to%20the%0Abest%20of%20our%20knowledge.%20OWLS%20leverages%20up%20to%20360K%20hours%20of%20public%20speech%20data%0Aacross%20150%20languages%2C%20enabling%20a%20systematic%20investigation%20into%20how%20data%2C%20model%2C%0Aand%20compute%20scaling%20each%20influence%20performance%20in%20multilingual%20speech%20tasks.%20We%0Ause%20OWLS%20to%20derive%20neural%20scaling%20laws%2C%20showing%20how%20final%20performance%20can%20be%0Areliably%20predicted%20when%20scaling.%20One%20of%20our%20key%20findings%20is%20that%20scaling%0Aenhances%20performance%20on%20low-resource%20languages/dialects%2C%20helping%20to%20mitigate%0Abias%20and%20improve%20the%20accessibility%20of%20speech%20technologies.%20Finally%2C%20we%20show%20how%0AOWLS%20can%20be%20used%20to%20power%20new%20research%20directions%20by%20discovering%20emergent%0Aabilities%20in%20large-scale%20speech%20models.%20Model%20checkpoints%20will%20be%20released%20on%0Ahttps%3A//huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d%0Afor%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOWLS%253A%2520Scaling%2520Laws%2520for%2520Multilingual%2520Speech%2520Recognition%2520and%2520Translation%250A%2520%2520Models%26entry.906535625%3DWilliam%2520Chen%2520and%2520Jinchuan%2520Tian%2520and%2520Yifan%2520Peng%2520and%2520Brian%2520Yan%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3D%2520%2520Neural%2520scaling%2520laws%2520offer%2520valuable%2520insights%2520for%2520designing%2520robust%2520sequence%250Aprocessing%2520architectures.%2520While%2520these%2520laws%2520have%2520been%2520extensively%2520characterized%250Ain%2520other%2520modalities%252C%2520their%2520behavior%2520in%2520speech%2520remains%2520comparatively%250Aunderexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%2520OWLS%252C%2520an%2520open-access%252C%2520reproducible%250Asuite%2520of%2520multilingual%2520speech%2520recognition%2520and%2520translation%2520models%2520spanning%25200.25B%250Ato%252018B%2520parameters%252C%2520with%2520the%252018B%2520version%2520being%2520the%2520largest%2520speech%2520model%252C%2520to%2520the%250Abest%2520of%2520our%2520knowledge.%2520OWLS%2520leverages%2520up%2520to%2520360K%2520hours%2520of%2520public%2520speech%2520data%250Aacross%2520150%2520languages%252C%2520enabling%2520a%2520systematic%2520investigation%2520into%2520how%2520data%252C%2520model%252C%250Aand%2520compute%2520scaling%2520each%2520influence%2520performance%2520in%2520multilingual%2520speech%2520tasks.%2520We%250Ause%2520OWLS%2520to%2520derive%2520neural%2520scaling%2520laws%252C%2520showing%2520how%2520final%2520performance%2520can%2520be%250Areliably%2520predicted%2520when%2520scaling.%2520One%2520of%2520our%2520key%2520findings%2520is%2520that%2520scaling%250Aenhances%2520performance%2520on%2520low-resource%2520languages/dialects%252C%2520helping%2520to%2520mitigate%250Abias%2520and%2520improve%2520the%2520accessibility%2520of%2520speech%2520technologies.%2520Finally%252C%2520we%2520show%2520how%250AOWLS%2520can%2520be%2520used%2520to%2520power%2520new%2520research%2520directions%2520by%2520discovering%2520emergent%250Aabilities%2520in%2520large-scale%2520speech%2520models.%2520Model%2520checkpoints%2520will%2520be%2520released%2520on%250Ahttps%253A//huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d%250Afor%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OWLS%3A%20Scaling%20Laws%20for%20Multilingual%20Speech%20Recognition%20and%20Translation%0A%20%20Models&entry.906535625=William%20Chen%20and%20Jinchuan%20Tian%20and%20Yifan%20Peng%20and%20Brian%20Yan%20and%20Chao-Han%20Huck%20Yang%20and%20Shinji%20Watanabe&entry.1292438233=%20%20Neural%20scaling%20laws%20offer%20valuable%20insights%20for%20designing%20robust%20sequence%0Aprocessing%20architectures.%20While%20these%20laws%20have%20been%20extensively%20characterized%0Ain%20other%20modalities%2C%20their%20behavior%20in%20speech%20remains%20comparatively%0Aunderexplored.%20In%20this%20work%2C%20we%20introduce%20OWLS%2C%20an%20open-access%2C%20reproducible%0Asuite%20of%20multilingual%20speech%20recognition%20and%20translation%20models%20spanning%200.25B%0Ato%2018B%20parameters%2C%20with%20the%2018B%20version%20being%20the%20largest%20speech%20model%2C%20to%20the%0Abest%20of%20our%20knowledge.%20OWLS%20leverages%20up%20to%20360K%20hours%20of%20public%20speech%20data%0Aacross%20150%20languages%2C%20enabling%20a%20systematic%20investigation%20into%20how%20data%2C%20model%2C%0Aand%20compute%20scaling%20each%20influence%20performance%20in%20multilingual%20speech%20tasks.%20We%0Ause%20OWLS%20to%20derive%20neural%20scaling%20laws%2C%20showing%20how%20final%20performance%20can%20be%0Areliably%20predicted%20when%20scaling.%20One%20of%20our%20key%20findings%20is%20that%20scaling%0Aenhances%20performance%20on%20low-resource%20languages/dialects%2C%20helping%20to%20mitigate%0Abias%20and%20improve%20the%20accessibility%20of%20speech%20technologies.%20Finally%2C%20we%20show%20how%0AOWLS%20can%20be%20used%20to%20power%20new%20research%20directions%20by%20discovering%20emergent%0Aabilities%20in%20large-scale%20speech%20models.%20Model%20checkpoints%20will%20be%20released%20on%0Ahttps%3A//huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d%0Afor%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10373v1&entry.124074799=Read"},
{"title": "Dynamic Reinforcement Learning for Actors", "author": "Katsunari Shibata", "abstract": "  Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly\ncontrols system dynamics, instead of the actor (action-generating neural\nnetwork) outputs at each moment, bringing about a major qualitative shift in\nreinforcement learning (RL) from static to dynamic. The actor is initially\ndesigned to generate chaotic dynamics through the loop with its environment,\nenabling the agent to perform flexible and deterministic exploration. Dynamic\nRL controls global system dynamics using a local index called \"sensitivity,\"\nwhich indicates how much the input neighborhood contracts or expands into the\ncorresponding output neighborhood through each neuron's processing. While\nsensitivity adjustment learning (SAL) prevents excessive convergence of the\ndynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them --\nto converge more to improve reproducibility around better state transitions\nwith positive TD error and to diverge more to enhance exploration around worse\ntransitions with negative TD error. Dynamic RL was applied only to the actor in\nan Actor-Critic RL architecture while applying it to the critic remains a\nchallenge. It was tested on two dynamic tasks and functioned effectively\nwithout external exploration noise or backward computation through time.\nMoreover, it exhibited excellent adaptability to new environments, although\nsome problems remain. Drawing parallels between 'exploration' and 'thinking,'\nthe author hypothesizes that \"exploration grows into thinking through learning\"\nand believes this RL could be a key technique for the emergence of thinking,\nincluding inspiration that cannot be reconstructed from massive existing text\ndata. Finally, despite being presumptuous, the author presents the argument\nthat this research should not proceed due to its potentially fatal risks,\naiming to encourage discussion.\n", "link": "http://arxiv.org/abs/2502.10200v1", "date": "2025-02-14", "relevancy": 1.9837, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5608}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4861}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Reinforcement%20Learning%20for%20Actors&body=Title%3A%20Dynamic%20Reinforcement%20Learning%20for%20Actors%0AAuthor%3A%20Katsunari%20Shibata%0AAbstract%3A%20%20%20Dynamic%20Reinforcement%20Learning%20%28Dynamic%20RL%29%2C%20proposed%20in%20this%20paper%2C%20directly%0Acontrols%20system%20dynamics%2C%20instead%20of%20the%20actor%20%28action-generating%20neural%0Anetwork%29%20outputs%20at%20each%20moment%2C%20bringing%20about%20a%20major%20qualitative%20shift%20in%0Areinforcement%20learning%20%28RL%29%20from%20static%20to%20dynamic.%20The%20actor%20is%20initially%0Adesigned%20to%20generate%20chaotic%20dynamics%20through%20the%20loop%20with%20its%20environment%2C%0Aenabling%20the%20agent%20to%20perform%20flexible%20and%20deterministic%20exploration.%20Dynamic%0ARL%20controls%20global%20system%20dynamics%20using%20a%20local%20index%20called%20%22sensitivity%2C%22%0Awhich%20indicates%20how%20much%20the%20input%20neighborhood%20contracts%20or%20expands%20into%20the%0Acorresponding%20output%20neighborhood%20through%20each%20neuron%27s%20processing.%20While%0Asensitivity%20adjustment%20learning%20%28SAL%29%20prevents%20excessive%20convergence%20of%20the%0Adynamics%2C%20sensitivity-controlled%20reinforcement%20learning%20%28SRL%29%20adjusts%20them%20--%0Ato%20converge%20more%20to%20improve%20reproducibility%20around%20better%20state%20transitions%0Awith%20positive%20TD%20error%20and%20to%20diverge%20more%20to%20enhance%20exploration%20around%20worse%0Atransitions%20with%20negative%20TD%20error.%20Dynamic%20RL%20was%20applied%20only%20to%20the%20actor%20in%0Aan%20Actor-Critic%20RL%20architecture%20while%20applying%20it%20to%20the%20critic%20remains%20a%0Achallenge.%20It%20was%20tested%20on%20two%20dynamic%20tasks%20and%20functioned%20effectively%0Awithout%20external%20exploration%20noise%20or%20backward%20computation%20through%20time.%0AMoreover%2C%20it%20exhibited%20excellent%20adaptability%20to%20new%20environments%2C%20although%0Asome%20problems%20remain.%20Drawing%20parallels%20between%20%27exploration%27%20and%20%27thinking%2C%27%0Athe%20author%20hypothesizes%20that%20%22exploration%20grows%20into%20thinking%20through%20learning%22%0Aand%20believes%20this%20RL%20could%20be%20a%20key%20technique%20for%20the%20emergence%20of%20thinking%2C%0Aincluding%20inspiration%20that%20cannot%20be%20reconstructed%20from%20massive%20existing%20text%0Adata.%20Finally%2C%20despite%20being%20presumptuous%2C%20the%20author%20presents%20the%20argument%0Athat%20this%20research%20should%20not%20proceed%20due%20to%20its%20potentially%20fatal%20risks%2C%0Aaiming%20to%20encourage%20discussion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Reinforcement%2520Learning%2520for%2520Actors%26entry.906535625%3DKatsunari%2520Shibata%26entry.1292438233%3D%2520%2520Dynamic%2520Reinforcement%2520Learning%2520%2528Dynamic%2520RL%2529%252C%2520proposed%2520in%2520this%2520paper%252C%2520directly%250Acontrols%2520system%2520dynamics%252C%2520instead%2520of%2520the%2520actor%2520%2528action-generating%2520neural%250Anetwork%2529%2520outputs%2520at%2520each%2520moment%252C%2520bringing%2520about%2520a%2520major%2520qualitative%2520shift%2520in%250Areinforcement%2520learning%2520%2528RL%2529%2520from%2520static%2520to%2520dynamic.%2520The%2520actor%2520is%2520initially%250Adesigned%2520to%2520generate%2520chaotic%2520dynamics%2520through%2520the%2520loop%2520with%2520its%2520environment%252C%250Aenabling%2520the%2520agent%2520to%2520perform%2520flexible%2520and%2520deterministic%2520exploration.%2520Dynamic%250ARL%2520controls%2520global%2520system%2520dynamics%2520using%2520a%2520local%2520index%2520called%2520%2522sensitivity%252C%2522%250Awhich%2520indicates%2520how%2520much%2520the%2520input%2520neighborhood%2520contracts%2520or%2520expands%2520into%2520the%250Acorresponding%2520output%2520neighborhood%2520through%2520each%2520neuron%2527s%2520processing.%2520While%250Asensitivity%2520adjustment%2520learning%2520%2528SAL%2529%2520prevents%2520excessive%2520convergence%2520of%2520the%250Adynamics%252C%2520sensitivity-controlled%2520reinforcement%2520learning%2520%2528SRL%2529%2520adjusts%2520them%2520--%250Ato%2520converge%2520more%2520to%2520improve%2520reproducibility%2520around%2520better%2520state%2520transitions%250Awith%2520positive%2520TD%2520error%2520and%2520to%2520diverge%2520more%2520to%2520enhance%2520exploration%2520around%2520worse%250Atransitions%2520with%2520negative%2520TD%2520error.%2520Dynamic%2520RL%2520was%2520applied%2520only%2520to%2520the%2520actor%2520in%250Aan%2520Actor-Critic%2520RL%2520architecture%2520while%2520applying%2520it%2520to%2520the%2520critic%2520remains%2520a%250Achallenge.%2520It%2520was%2520tested%2520on%2520two%2520dynamic%2520tasks%2520and%2520functioned%2520effectively%250Awithout%2520external%2520exploration%2520noise%2520or%2520backward%2520computation%2520through%2520time.%250AMoreover%252C%2520it%2520exhibited%2520excellent%2520adaptability%2520to%2520new%2520environments%252C%2520although%250Asome%2520problems%2520remain.%2520Drawing%2520parallels%2520between%2520%2527exploration%2527%2520and%2520%2527thinking%252C%2527%250Athe%2520author%2520hypothesizes%2520that%2520%2522exploration%2520grows%2520into%2520thinking%2520through%2520learning%2522%250Aand%2520believes%2520this%2520RL%2520could%2520be%2520a%2520key%2520technique%2520for%2520the%2520emergence%2520of%2520thinking%252C%250Aincluding%2520inspiration%2520that%2520cannot%2520be%2520reconstructed%2520from%2520massive%2520existing%2520text%250Adata.%2520Finally%252C%2520despite%2520being%2520presumptuous%252C%2520the%2520author%2520presents%2520the%2520argument%250Athat%2520this%2520research%2520should%2520not%2520proceed%2520due%2520to%2520its%2520potentially%2520fatal%2520risks%252C%250Aaiming%2520to%2520encourage%2520discussion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Reinforcement%20Learning%20for%20Actors&entry.906535625=Katsunari%20Shibata&entry.1292438233=%20%20Dynamic%20Reinforcement%20Learning%20%28Dynamic%20RL%29%2C%20proposed%20in%20this%20paper%2C%20directly%0Acontrols%20system%20dynamics%2C%20instead%20of%20the%20actor%20%28action-generating%20neural%0Anetwork%29%20outputs%20at%20each%20moment%2C%20bringing%20about%20a%20major%20qualitative%20shift%20in%0Areinforcement%20learning%20%28RL%29%20from%20static%20to%20dynamic.%20The%20actor%20is%20initially%0Adesigned%20to%20generate%20chaotic%20dynamics%20through%20the%20loop%20with%20its%20environment%2C%0Aenabling%20the%20agent%20to%20perform%20flexible%20and%20deterministic%20exploration.%20Dynamic%0ARL%20controls%20global%20system%20dynamics%20using%20a%20local%20index%20called%20%22sensitivity%2C%22%0Awhich%20indicates%20how%20much%20the%20input%20neighborhood%20contracts%20or%20expands%20into%20the%0Acorresponding%20output%20neighborhood%20through%20each%20neuron%27s%20processing.%20While%0Asensitivity%20adjustment%20learning%20%28SAL%29%20prevents%20excessive%20convergence%20of%20the%0Adynamics%2C%20sensitivity-controlled%20reinforcement%20learning%20%28SRL%29%20adjusts%20them%20--%0Ato%20converge%20more%20to%20improve%20reproducibility%20around%20better%20state%20transitions%0Awith%20positive%20TD%20error%20and%20to%20diverge%20more%20to%20enhance%20exploration%20around%20worse%0Atransitions%20with%20negative%20TD%20error.%20Dynamic%20RL%20was%20applied%20only%20to%20the%20actor%20in%0Aan%20Actor-Critic%20RL%20architecture%20while%20applying%20it%20to%20the%20critic%20remains%20a%0Achallenge.%20It%20was%20tested%20on%20two%20dynamic%20tasks%20and%20functioned%20effectively%0Awithout%20external%20exploration%20noise%20or%20backward%20computation%20through%20time.%0AMoreover%2C%20it%20exhibited%20excellent%20adaptability%20to%20new%20environments%2C%20although%0Asome%20problems%20remain.%20Drawing%20parallels%20between%20%27exploration%27%20and%20%27thinking%2C%27%0Athe%20author%20hypothesizes%20that%20%22exploration%20grows%20into%20thinking%20through%20learning%22%0Aand%20believes%20this%20RL%20could%20be%20a%20key%20technique%20for%20the%20emergence%20of%20thinking%2C%0Aincluding%20inspiration%20that%20cannot%20be%20reconstructed%20from%20massive%20existing%20text%0Adata.%20Finally%2C%20despite%20being%20presumptuous%2C%20the%20author%20presents%20the%20argument%0Athat%20this%20research%20should%20not%20proceed%20due%20to%20its%20potentially%20fatal%20risks%2C%0Aaiming%20to%20encourage%20discussion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10200v1&entry.124074799=Read"},
{"title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting", "author": "Abdelhakim Benechehab and Vasilii Feofanov and Giuseppe Paolo and Albert Thomas and Maurizio Filippone and Bal\u00e1zs K\u00e9gl", "abstract": "  Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https://github.com/abenechehab/AdaPTS.\n", "link": "http://arxiv.org/abs/2502.10235v1", "date": "2025-02-14", "relevancy": 1.983, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5417}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5094}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaPTS%3A%20Adapting%20Univariate%20Foundation%20Models%20to%20Probabilistic%0A%20%20Multivariate%20Time%20Series%20Forecasting&body=Title%3A%20AdaPTS%3A%20Adapting%20Univariate%20Foundation%20Models%20to%20Probabilistic%0A%20%20Multivariate%20Time%20Series%20Forecasting%0AAuthor%3A%20Abdelhakim%20Benechehab%20and%20Vasilii%20Feofanov%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Maurizio%20Filippone%20and%20Bal%C3%A1zs%20K%C3%A9gl%0AAbstract%3A%20%20%20Pre-trained%20foundation%20models%20%28FMs%29%20have%20shown%20exceptional%20performance%20in%0Aunivariate%20time%20series%20forecasting%20tasks.%20However%2C%20several%20practical%20challenges%0Apersist%2C%20including%20managing%20intricate%20dependencies%20among%20features%20and%0Aquantifying%20uncertainty%20in%20predictions.%20This%20study%20aims%20to%20tackle%20these%0Acritical%20limitations%20by%20introducing%20adapters%3B%20feature-space%20transformations%0Athat%20facilitate%20the%20effective%20use%20of%20pre-trained%20univariate%20time%20series%20FMs%20for%0Amultivariate%20tasks.%20Adapters%20operate%20by%20projecting%20multivariate%20inputs%20into%20a%0Asuitable%20latent%20space%20and%20applying%20the%20FM%20independently%20to%20each%20dimension.%0AInspired%20by%20the%20literature%20on%20representation%20learning%20and%20partially%20stochastic%0ABayesian%20neural%20networks%2C%20we%20present%20a%20range%20of%20adapters%20and%0Aoptimization/inference%20strategies.%20Experiments%20conducted%20on%20both%20synthetic%20and%0Areal-world%20datasets%20confirm%20the%20efficacy%20of%20adapters%2C%20demonstrating%20substantial%0Aenhancements%20in%20forecasting%20accuracy%20and%20uncertainty%20quantification%20compared%20to%0Abaseline%20methods.%20Our%20framework%2C%20AdaPTS%2C%20positions%20adapters%20as%20a%20modular%2C%0Ascalable%2C%20and%20effective%20solution%20for%20leveraging%20time%20series%20FMs%20in%20multivariate%0Acontexts%2C%20thereby%20promoting%20their%20wider%20adoption%20in%20real-world%20applications.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/abenechehab/AdaPTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaPTS%253A%2520Adapting%2520Univariate%2520Foundation%2520Models%2520to%2520Probabilistic%250A%2520%2520Multivariate%2520Time%2520Series%2520Forecasting%26entry.906535625%3DAbdelhakim%2520Benechehab%2520and%2520Vasilii%2520Feofanov%2520and%2520Giuseppe%2520Paolo%2520and%2520Albert%2520Thomas%2520and%2520Maurizio%2520Filippone%2520and%2520Bal%25C3%25A1zs%2520K%25C3%25A9gl%26entry.1292438233%3D%2520%2520Pre-trained%2520foundation%2520models%2520%2528FMs%2529%2520have%2520shown%2520exceptional%2520performance%2520in%250Aunivariate%2520time%2520series%2520forecasting%2520tasks.%2520However%252C%2520several%2520practical%2520challenges%250Apersist%252C%2520including%2520managing%2520intricate%2520dependencies%2520among%2520features%2520and%250Aquantifying%2520uncertainty%2520in%2520predictions.%2520This%2520study%2520aims%2520to%2520tackle%2520these%250Acritical%2520limitations%2520by%2520introducing%2520adapters%253B%2520feature-space%2520transformations%250Athat%2520facilitate%2520the%2520effective%2520use%2520of%2520pre-trained%2520univariate%2520time%2520series%2520FMs%2520for%250Amultivariate%2520tasks.%2520Adapters%2520operate%2520by%2520projecting%2520multivariate%2520inputs%2520into%2520a%250Asuitable%2520latent%2520space%2520and%2520applying%2520the%2520FM%2520independently%2520to%2520each%2520dimension.%250AInspired%2520by%2520the%2520literature%2520on%2520representation%2520learning%2520and%2520partially%2520stochastic%250ABayesian%2520neural%2520networks%252C%2520we%2520present%2520a%2520range%2520of%2520adapters%2520and%250Aoptimization/inference%2520strategies.%2520Experiments%2520conducted%2520on%2520both%2520synthetic%2520and%250Areal-world%2520datasets%2520confirm%2520the%2520efficacy%2520of%2520adapters%252C%2520demonstrating%2520substantial%250Aenhancements%2520in%2520forecasting%2520accuracy%2520and%2520uncertainty%2520quantification%2520compared%2520to%250Abaseline%2520methods.%2520Our%2520framework%252C%2520AdaPTS%252C%2520positions%2520adapters%2520as%2520a%2520modular%252C%250Ascalable%252C%2520and%2520effective%2520solution%2520for%2520leveraging%2520time%2520series%2520FMs%2520in%2520multivariate%250Acontexts%252C%2520thereby%2520promoting%2520their%2520wider%2520adoption%2520in%2520real-world%2520applications.%2520We%250Arelease%2520the%2520code%2520at%2520https%253A//github.com/abenechehab/AdaPTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaPTS%3A%20Adapting%20Univariate%20Foundation%20Models%20to%20Probabilistic%0A%20%20Multivariate%20Time%20Series%20Forecasting&entry.906535625=Abdelhakim%20Benechehab%20and%20Vasilii%20Feofanov%20and%20Giuseppe%20Paolo%20and%20Albert%20Thomas%20and%20Maurizio%20Filippone%20and%20Bal%C3%A1zs%20K%C3%A9gl&entry.1292438233=%20%20Pre-trained%20foundation%20models%20%28FMs%29%20have%20shown%20exceptional%20performance%20in%0Aunivariate%20time%20series%20forecasting%20tasks.%20However%2C%20several%20practical%20challenges%0Apersist%2C%20including%20managing%20intricate%20dependencies%20among%20features%20and%0Aquantifying%20uncertainty%20in%20predictions.%20This%20study%20aims%20to%20tackle%20these%0Acritical%20limitations%20by%20introducing%20adapters%3B%20feature-space%20transformations%0Athat%20facilitate%20the%20effective%20use%20of%20pre-trained%20univariate%20time%20series%20FMs%20for%0Amultivariate%20tasks.%20Adapters%20operate%20by%20projecting%20multivariate%20inputs%20into%20a%0Asuitable%20latent%20space%20and%20applying%20the%20FM%20independently%20to%20each%20dimension.%0AInspired%20by%20the%20literature%20on%20representation%20learning%20and%20partially%20stochastic%0ABayesian%20neural%20networks%2C%20we%20present%20a%20range%20of%20adapters%20and%0Aoptimization/inference%20strategies.%20Experiments%20conducted%20on%20both%20synthetic%20and%0Areal-world%20datasets%20confirm%20the%20efficacy%20of%20adapters%2C%20demonstrating%20substantial%0Aenhancements%20in%20forecasting%20accuracy%20and%20uncertainty%20quantification%20compared%20to%0Abaseline%20methods.%20Our%20framework%2C%20AdaPTS%2C%20positions%20adapters%20as%20a%20modular%2C%0Ascalable%2C%20and%20effective%20solution%20for%20leveraging%20time%20series%20FMs%20in%20multivariate%0Acontexts%2C%20thereby%20promoting%20their%20wider%20adoption%20in%20real-world%20applications.%20We%0Arelease%20the%20code%20at%20https%3A//github.com/abenechehab/AdaPTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10235v1&entry.124074799=Read"},
{"title": "Region-Adaptive Sampling for Diffusion Transformers", "author": "Ziming Liu and Yifan Yang and Chengruidong Zhang and Yiqi Zhang and Lili Qiu and Yang You and Yuqing Yang", "abstract": "  Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.\n", "link": "http://arxiv.org/abs/2502.10389v1", "date": "2025-02-14", "relevancy": 1.9827, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6921}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6775}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Region-Adaptive%20Sampling%20for%20Diffusion%20Transformers&body=Title%3A%20Region-Adaptive%20Sampling%20for%20Diffusion%20Transformers%0AAuthor%3A%20Ziming%20Liu%20and%20Yifan%20Yang%20and%20Chengruidong%20Zhang%20and%20Yiqi%20Zhang%20and%20Lili%20Qiu%20and%20Yang%20You%20and%20Yuqing%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20become%20the%20leading%20choice%20for%20generative%20tasks%0Aacross%20diverse%20domains.%20However%2C%20their%20reliance%20on%20multiple%20sequential%20forward%0Apasses%20significantly%20limits%20real-time%20performance.%20Previous%20acceleration%0Amethods%20have%20primarily%20focused%20on%20reducing%20the%20number%20of%20sampling%20steps%20or%0Areusing%20intermediate%20results%2C%20failing%20to%20leverage%20variations%20across%20spatial%0Aregions%20within%20the%20image%20due%20to%20the%20constraints%20of%20convolutional%20U-Net%0Astructures.%20By%20harnessing%20the%20flexibility%20of%20Diffusion%20Transformers%20%28DiTs%29%20in%0Ahandling%20variable%20number%20of%20tokens%2C%20we%20introduce%20RAS%2C%20a%20novel%2C%20training-free%0Asampling%20strategy%20that%20dynamically%20assigns%20different%20sampling%20ratios%20to%20regions%0Awithin%20an%20image%20based%20on%20the%20focus%20of%20the%20DiT%20model.%20Our%20key%20observation%20is%0Athat%20during%20each%20sampling%20step%2C%20the%20model%20concentrates%20on%20semantically%0Ameaningful%20regions%2C%20and%20these%20areas%20of%20focus%20exhibit%20strong%20continuity%20across%0Aconsecutive%20steps.%20Leveraging%20this%20insight%2C%20RAS%20updates%20only%20the%20regions%0Acurrently%20in%20focus%2C%20while%20other%20regions%20are%20updated%20using%20cached%20noise%20from%20the%0Aprevious%20step.%20The%20model%27s%20focus%20is%20determined%20based%20on%20the%20output%20from%20the%0Apreceding%20step%2C%20capitalizing%20on%20the%20temporal%20consistency%20we%20observed.%20We%0Aevaluate%20RAS%20on%20Stable%20Diffusion%203%20and%20Lumina-Next-T2I%2C%20achieving%20speedups%20up%0Ato%202.36x%20and%202.51x%2C%20respectively%2C%20with%20minimal%20degradation%20in%20generation%0Aquality.%20Additionally%2C%20a%20user%20study%20reveals%20that%20RAS%20delivers%20comparable%0Aqualities%20under%20human%20evaluation%20while%20achieving%20a%201.6x%20speedup.%20Our%20approach%0Amakes%20a%20significant%20step%20towards%20more%20efficient%20diffusion%20transformers%2C%0Aenhancing%20their%20potential%20for%20real-time%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegion-Adaptive%2520Sampling%2520for%2520Diffusion%2520Transformers%26entry.906535625%3DZiming%2520Liu%2520and%2520Yifan%2520Yang%2520and%2520Chengruidong%2520Zhang%2520and%2520Yiqi%2520Zhang%2520and%2520Lili%2520Qiu%2520and%2520Yang%2520You%2520and%2520Yuqing%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520become%2520the%2520leading%2520choice%2520for%2520generative%2520tasks%250Aacross%2520diverse%2520domains.%2520However%252C%2520their%2520reliance%2520on%2520multiple%2520sequential%2520forward%250Apasses%2520significantly%2520limits%2520real-time%2520performance.%2520Previous%2520acceleration%250Amethods%2520have%2520primarily%2520focused%2520on%2520reducing%2520the%2520number%2520of%2520sampling%2520steps%2520or%250Areusing%2520intermediate%2520results%252C%2520failing%2520to%2520leverage%2520variations%2520across%2520spatial%250Aregions%2520within%2520the%2520image%2520due%2520to%2520the%2520constraints%2520of%2520convolutional%2520U-Net%250Astructures.%2520By%2520harnessing%2520the%2520flexibility%2520of%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520in%250Ahandling%2520variable%2520number%2520of%2520tokens%252C%2520we%2520introduce%2520RAS%252C%2520a%2520novel%252C%2520training-free%250Asampling%2520strategy%2520that%2520dynamically%2520assigns%2520different%2520sampling%2520ratios%2520to%2520regions%250Awithin%2520an%2520image%2520based%2520on%2520the%2520focus%2520of%2520the%2520DiT%2520model.%2520Our%2520key%2520observation%2520is%250Athat%2520during%2520each%2520sampling%2520step%252C%2520the%2520model%2520concentrates%2520on%2520semantically%250Ameaningful%2520regions%252C%2520and%2520these%2520areas%2520of%2520focus%2520exhibit%2520strong%2520continuity%2520across%250Aconsecutive%2520steps.%2520Leveraging%2520this%2520insight%252C%2520RAS%2520updates%2520only%2520the%2520regions%250Acurrently%2520in%2520focus%252C%2520while%2520other%2520regions%2520are%2520updated%2520using%2520cached%2520noise%2520from%2520the%250Aprevious%2520step.%2520The%2520model%2527s%2520focus%2520is%2520determined%2520based%2520on%2520the%2520output%2520from%2520the%250Apreceding%2520step%252C%2520capitalizing%2520on%2520the%2520temporal%2520consistency%2520we%2520observed.%2520We%250Aevaluate%2520RAS%2520on%2520Stable%2520Diffusion%25203%2520and%2520Lumina-Next-T2I%252C%2520achieving%2520speedups%2520up%250Ato%25202.36x%2520and%25202.51x%252C%2520respectively%252C%2520with%2520minimal%2520degradation%2520in%2520generation%250Aquality.%2520Additionally%252C%2520a%2520user%2520study%2520reveals%2520that%2520RAS%2520delivers%2520comparable%250Aqualities%2520under%2520human%2520evaluation%2520while%2520achieving%2520a%25201.6x%2520speedup.%2520Our%2520approach%250Amakes%2520a%2520significant%2520step%2520towards%2520more%2520efficient%2520diffusion%2520transformers%252C%250Aenhancing%2520their%2520potential%2520for%2520real-time%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Region-Adaptive%20Sampling%20for%20Diffusion%20Transformers&entry.906535625=Ziming%20Liu%20and%20Yifan%20Yang%20and%20Chengruidong%20Zhang%20and%20Yiqi%20Zhang%20and%20Lili%20Qiu%20and%20Yang%20You%20and%20Yuqing%20Yang&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20become%20the%20leading%20choice%20for%20generative%20tasks%0Aacross%20diverse%20domains.%20However%2C%20their%20reliance%20on%20multiple%20sequential%20forward%0Apasses%20significantly%20limits%20real-time%20performance.%20Previous%20acceleration%0Amethods%20have%20primarily%20focused%20on%20reducing%20the%20number%20of%20sampling%20steps%20or%0Areusing%20intermediate%20results%2C%20failing%20to%20leverage%20variations%20across%20spatial%0Aregions%20within%20the%20image%20due%20to%20the%20constraints%20of%20convolutional%20U-Net%0Astructures.%20By%20harnessing%20the%20flexibility%20of%20Diffusion%20Transformers%20%28DiTs%29%20in%0Ahandling%20variable%20number%20of%20tokens%2C%20we%20introduce%20RAS%2C%20a%20novel%2C%20training-free%0Asampling%20strategy%20that%20dynamically%20assigns%20different%20sampling%20ratios%20to%20regions%0Awithin%20an%20image%20based%20on%20the%20focus%20of%20the%20DiT%20model.%20Our%20key%20observation%20is%0Athat%20during%20each%20sampling%20step%2C%20the%20model%20concentrates%20on%20semantically%0Ameaningful%20regions%2C%20and%20these%20areas%20of%20focus%20exhibit%20strong%20continuity%20across%0Aconsecutive%20steps.%20Leveraging%20this%20insight%2C%20RAS%20updates%20only%20the%20regions%0Acurrently%20in%20focus%2C%20while%20other%20regions%20are%20updated%20using%20cached%20noise%20from%20the%0Aprevious%20step.%20The%20model%27s%20focus%20is%20determined%20based%20on%20the%20output%20from%20the%0Apreceding%20step%2C%20capitalizing%20on%20the%20temporal%20consistency%20we%20observed.%20We%0Aevaluate%20RAS%20on%20Stable%20Diffusion%203%20and%20Lumina-Next-T2I%2C%20achieving%20speedups%20up%0Ato%202.36x%20and%202.51x%2C%20respectively%2C%20with%20minimal%20degradation%20in%20generation%0Aquality.%20Additionally%2C%20a%20user%20study%20reveals%20that%20RAS%20delivers%20comparable%0Aqualities%20under%20human%20evaluation%20while%20achieving%20a%201.6x%20speedup.%20Our%20approach%0Amakes%20a%20significant%20step%20towards%20more%20efficient%20diffusion%20transformers%2C%0Aenhancing%20their%20potential%20for%20real-time%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10389v1&entry.124074799=Read"},
{"title": "RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing\n  Applications", "author": "Shyam Venkatasubramanian and Bosung Kang and Ali Pezeshki and Muralidhar Rangaswamy and Vahid Tarokh", "abstract": "  We present a large-scale dataset for radar adaptive signal processing (RASP)\napplications to support the development of data-driven models within the\nadaptive radar community. The dataset, RASPNet, exceeds 16 TB in size and\ncomprises 100 realistic scenarios compiled over a variety of topographies and\nland types from across the contiguous United States. For each scenario, RASPNet\nconsists of 10,000 clutter realizations from an airborne radar setting, which\ncan be used to benchmark radar and complex-valued learning algorithms. RASPNet\nintends to fill a prominent gap in the availability of a large-scale, realistic\ndataset that standardizes the evaluation of adaptive radar processing\ntechniques and complex-valued neural networks. We outline its construction,\norganization, and several applications, including a transfer learning example\nto demonstrate how RASPNet can be used for realistic adaptive radar processing\nscenarios.\n", "link": "http://arxiv.org/abs/2406.09638v2", "date": "2025-02-14", "relevancy": 1.972, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5248}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RASPNet%3A%20A%20Benchmark%20Dataset%20for%20Radar%20Adaptive%20Signal%20Processing%0A%20%20Applications&body=Title%3A%20RASPNet%3A%20A%20Benchmark%20Dataset%20for%20Radar%20Adaptive%20Signal%20Processing%0A%20%20Applications%0AAuthor%3A%20Shyam%20Venkatasubramanian%20and%20Bosung%20Kang%20and%20Ali%20Pezeshki%20and%20Muralidhar%20Rangaswamy%20and%20Vahid%20Tarokh%0AAbstract%3A%20%20%20We%20present%20a%20large-scale%20dataset%20for%20radar%20adaptive%20signal%20processing%20%28RASP%29%0Aapplications%20to%20support%20the%20development%20of%20data-driven%20models%20within%20the%0Aadaptive%20radar%20community.%20The%20dataset%2C%20RASPNet%2C%20exceeds%2016%20TB%20in%20size%20and%0Acomprises%20100%20realistic%20scenarios%20compiled%20over%20a%20variety%20of%20topographies%20and%0Aland%20types%20from%20across%20the%20contiguous%20United%20States.%20For%20each%20scenario%2C%20RASPNet%0Aconsists%20of%2010%2C000%20clutter%20realizations%20from%20an%20airborne%20radar%20setting%2C%20which%0Acan%20be%20used%20to%20benchmark%20radar%20and%20complex-valued%20learning%20algorithms.%20RASPNet%0Aintends%20to%20fill%20a%20prominent%20gap%20in%20the%20availability%20of%20a%20large-scale%2C%20realistic%0Adataset%20that%20standardizes%20the%20evaluation%20of%20adaptive%20radar%20processing%0Atechniques%20and%20complex-valued%20neural%20networks.%20We%20outline%20its%20construction%2C%0Aorganization%2C%20and%20several%20applications%2C%20including%20a%20transfer%20learning%20example%0Ato%20demonstrate%20how%20RASPNet%20can%20be%20used%20for%20realistic%20adaptive%20radar%20processing%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09638v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRASPNet%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Radar%2520Adaptive%2520Signal%2520Processing%250A%2520%2520Applications%26entry.906535625%3DShyam%2520Venkatasubramanian%2520and%2520Bosung%2520Kang%2520and%2520Ali%2520Pezeshki%2520and%2520Muralidhar%2520Rangaswamy%2520and%2520Vahid%2520Tarokh%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520large-scale%2520dataset%2520for%2520radar%2520adaptive%2520signal%2520processing%2520%2528RASP%2529%250Aapplications%2520to%2520support%2520the%2520development%2520of%2520data-driven%2520models%2520within%2520the%250Aadaptive%2520radar%2520community.%2520The%2520dataset%252C%2520RASPNet%252C%2520exceeds%252016%2520TB%2520in%2520size%2520and%250Acomprises%2520100%2520realistic%2520scenarios%2520compiled%2520over%2520a%2520variety%2520of%2520topographies%2520and%250Aland%2520types%2520from%2520across%2520the%2520contiguous%2520United%2520States.%2520For%2520each%2520scenario%252C%2520RASPNet%250Aconsists%2520of%252010%252C000%2520clutter%2520realizations%2520from%2520an%2520airborne%2520radar%2520setting%252C%2520which%250Acan%2520be%2520used%2520to%2520benchmark%2520radar%2520and%2520complex-valued%2520learning%2520algorithms.%2520RASPNet%250Aintends%2520to%2520fill%2520a%2520prominent%2520gap%2520in%2520the%2520availability%2520of%2520a%2520large-scale%252C%2520realistic%250Adataset%2520that%2520standardizes%2520the%2520evaluation%2520of%2520adaptive%2520radar%2520processing%250Atechniques%2520and%2520complex-valued%2520neural%2520networks.%2520We%2520outline%2520its%2520construction%252C%250Aorganization%252C%2520and%2520several%2520applications%252C%2520including%2520a%2520transfer%2520learning%2520example%250Ato%2520demonstrate%2520how%2520RASPNet%2520can%2520be%2520used%2520for%2520realistic%2520adaptive%2520radar%2520processing%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09638v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RASPNet%3A%20A%20Benchmark%20Dataset%20for%20Radar%20Adaptive%20Signal%20Processing%0A%20%20Applications&entry.906535625=Shyam%20Venkatasubramanian%20and%20Bosung%20Kang%20and%20Ali%20Pezeshki%20and%20Muralidhar%20Rangaswamy%20and%20Vahid%20Tarokh&entry.1292438233=%20%20We%20present%20a%20large-scale%20dataset%20for%20radar%20adaptive%20signal%20processing%20%28RASP%29%0Aapplications%20to%20support%20the%20development%20of%20data-driven%20models%20within%20the%0Aadaptive%20radar%20community.%20The%20dataset%2C%20RASPNet%2C%20exceeds%2016%20TB%20in%20size%20and%0Acomprises%20100%20realistic%20scenarios%20compiled%20over%20a%20variety%20of%20topographies%20and%0Aland%20types%20from%20across%20the%20contiguous%20United%20States.%20For%20each%20scenario%2C%20RASPNet%0Aconsists%20of%2010%2C000%20clutter%20realizations%20from%20an%20airborne%20radar%20setting%2C%20which%0Acan%20be%20used%20to%20benchmark%20radar%20and%20complex-valued%20learning%20algorithms.%20RASPNet%0Aintends%20to%20fill%20a%20prominent%20gap%20in%20the%20availability%20of%20a%20large-scale%2C%20realistic%0Adataset%20that%20standardizes%20the%20evaluation%20of%20adaptive%20radar%20processing%0Atechniques%20and%20complex-valued%20neural%20networks.%20We%20outline%20its%20construction%2C%0Aorganization%2C%20and%20several%20applications%2C%20including%20a%20transfer%20learning%20example%0Ato%20demonstrate%20how%20RASPNet%20can%20be%20used%20for%20realistic%20adaptive%20radar%20processing%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09638v2&entry.124074799=Read"},
{"title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey", "author": "Ruiyao Xu and Kaize Ding", "abstract": "  Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers.\n", "link": "http://arxiv.org/abs/2409.01980v3", "date": "2025-02-14", "relevancy": 1.9709, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5094}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Anomaly%20and%20Out-of-Distribution%20Detection%3A%20A%0A%20%20Survey&body=Title%3A%20Large%20Language%20Models%20for%20Anomaly%20and%20Out-of-Distribution%20Detection%3A%20A%0A%20%20Survey%0AAuthor%3A%20Ruiyao%20Xu%20and%20Kaize%20Ding%0AAbstract%3A%20%20%20Detecting%20anomalies%20or%20out-of-distribution%20%28OOD%29%20samples%20is%20critical%20for%0Amaintaining%20the%20reliability%20and%20trustworthiness%20of%20machine%20learning%20systems.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20effectiveness%0Anot%20only%20in%20natural%20language%20processing%20but%20also%20in%20broader%20applications%20due%20to%0Atheir%20advanced%20comprehension%20and%20generative%20capabilities.%20The%20integration%20of%0ALLMs%20into%20anomaly%20and%20OOD%20detection%20marks%20a%20significant%20shift%20from%20the%0Atraditional%20paradigm%20in%20the%20field.%20This%20survey%20focuses%20on%20the%20problem%20of%0Aanomaly%20and%20OOD%20detection%20under%20the%20context%20of%20LLMs.%20We%20propose%20a%20new%20taxonomy%0Ato%20categorize%20existing%20approaches%20into%20two%20classes%20based%20on%20the%20role%20played%20by%0ALLMs.%20Following%20our%20proposed%20taxonomy%2C%20we%20further%20discuss%20the%20related%20work%0Aunder%20each%20of%20the%20categories%20and%20finally%20discuss%20potential%20challenges%20and%0Adirections%20for%20future%20research%20in%20this%20field.%20We%20also%20provide%20an%20up-to-date%0Areading%20list%20of%20relevant%20papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01980v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Anomaly%2520and%2520Out-of-Distribution%2520Detection%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DRuiyao%2520Xu%2520and%2520Kaize%2520Ding%26entry.1292438233%3D%2520%2520Detecting%2520anomalies%2520or%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520critical%2520for%250Amaintaining%2520the%2520reliability%2520and%2520trustworthiness%2520of%2520machine%2520learning%2520systems.%250ARecently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520their%2520effectiveness%250Anot%2520only%2520in%2520natural%2520language%2520processing%2520but%2520also%2520in%2520broader%2520applications%2520due%2520to%250Atheir%2520advanced%2520comprehension%2520and%2520generative%2520capabilities.%2520The%2520integration%2520of%250ALLMs%2520into%2520anomaly%2520and%2520OOD%2520detection%2520marks%2520a%2520significant%2520shift%2520from%2520the%250Atraditional%2520paradigm%2520in%2520the%2520field.%2520This%2520survey%2520focuses%2520on%2520the%2520problem%2520of%250Aanomaly%2520and%2520OOD%2520detection%2520under%2520the%2520context%2520of%2520LLMs.%2520We%2520propose%2520a%2520new%2520taxonomy%250Ato%2520categorize%2520existing%2520approaches%2520into%2520two%2520classes%2520based%2520on%2520the%2520role%2520played%2520by%250ALLMs.%2520Following%2520our%2520proposed%2520taxonomy%252C%2520we%2520further%2520discuss%2520the%2520related%2520work%250Aunder%2520each%2520of%2520the%2520categories%2520and%2520finally%2520discuss%2520potential%2520challenges%2520and%250Adirections%2520for%2520future%2520research%2520in%2520this%2520field.%2520We%2520also%2520provide%2520an%2520up-to-date%250Areading%2520list%2520of%2520relevant%2520papers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01980v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Anomaly%20and%20Out-of-Distribution%20Detection%3A%20A%0A%20%20Survey&entry.906535625=Ruiyao%20Xu%20and%20Kaize%20Ding&entry.1292438233=%20%20Detecting%20anomalies%20or%20out-of-distribution%20%28OOD%29%20samples%20is%20critical%20for%0Amaintaining%20the%20reliability%20and%20trustworthiness%20of%20machine%20learning%20systems.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20their%20effectiveness%0Anot%20only%20in%20natural%20language%20processing%20but%20also%20in%20broader%20applications%20due%20to%0Atheir%20advanced%20comprehension%20and%20generative%20capabilities.%20The%20integration%20of%0ALLMs%20into%20anomaly%20and%20OOD%20detection%20marks%20a%20significant%20shift%20from%20the%0Atraditional%20paradigm%20in%20the%20field.%20This%20survey%20focuses%20on%20the%20problem%20of%0Aanomaly%20and%20OOD%20detection%20under%20the%20context%20of%20LLMs.%20We%20propose%20a%20new%20taxonomy%0Ato%20categorize%20existing%20approaches%20into%20two%20classes%20based%20on%20the%20role%20played%20by%0ALLMs.%20Following%20our%20proposed%20taxonomy%2C%20we%20further%20discuss%20the%20related%20work%0Aunder%20each%20of%20the%20categories%20and%20finally%20discuss%20potential%20challenges%20and%0Adirections%20for%20future%20research%20in%20this%20field.%20We%20also%20provide%20an%20up-to-date%0Areading%20list%20of%20relevant%20papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01980v3&entry.124074799=Read"},
{"title": "Strada-LLM: Graph LLM for traffic prediction", "author": "Seyed Mohamad Moghadas and Yangxintong Lyu and Bruno Cornelis and Alexandre Alahi and Adrian Munteanu", "abstract": "  Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.\n", "link": "http://arxiv.org/abs/2410.20856v2", "date": "2025-02-14", "relevancy": 1.9669, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.509}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4886}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strada-LLM%3A%20Graph%20LLM%20for%20traffic%20prediction&body=Title%3A%20Strada-LLM%3A%20Graph%20LLM%20for%20traffic%20prediction%0AAuthor%3A%20Seyed%20Mohamad%20Moghadas%20and%20Yangxintong%20Lyu%20and%20Bruno%20Cornelis%20and%20Alexandre%20Alahi%20and%20Adrian%20Munteanu%0AAbstract%3A%20%20%20Traffic%20prediction%20is%20a%20vital%20component%20of%20intelligent%20transportation%0Asystems.%20By%20reasoning%20about%20traffic%20patterns%20in%20both%20the%20spatial%20and%20temporal%0Adimensions%2C%20accurate%20and%20interpretable%20predictions%20can%20be%20provided.%20A%0Aconsiderable%20challenge%20in%20traffic%20prediction%20lies%20in%20handling%20the%20diverse%20data%0Adistributions%20caused%20by%20vastly%20different%20traffic%20conditions%20occurring%20at%0Adifferent%20locations.%20LLMs%20have%20been%20a%20dominant%20solution%20due%20to%20their%20remarkable%0Acapacity%20to%20adapt%20to%20new%20datasets%20with%20very%20few%20labeled%20data%20samples%2C%20i.e.%2C%0Afew-shot%20adaptability.%20However%2C%20existing%20forecasting%20techniques%20mainly%20focus%20on%0Aextracting%20local%20graph%20information%20and%20forming%20a%20text-like%20prompt%2C%20leaving%20LLM-%0Abased%20traffic%20prediction%20an%20open%20problem.%20This%20work%20presents%20a%20probabilistic%0ALLM%20for%20traffic%20forecasting%20with%20three%20highlights.%20We%20propose%20a%20graph-aware%20LLM%0Afor%20traffic%20prediction%20that%20considers%20proximal%20traffic%20information.%0ASpecifically%2C%20by%20considering%20the%20traffic%20of%20neighboring%20nodes%20as%20covariates%2C%0Aour%20model%20outperforms%20the%20corresponding%20time-series%20LLM.%20Furthermore%2C%20we%20adopt%0Aa%20lightweight%20approach%20for%20efficient%20domain%20adaptation%20when%20facing%20new%20data%0Adistributions%20in%20few-shot%20fashion.%20The%20comparative%20experiment%20demonstrates%20the%0Aproposed%20method%20outperforms%20the%20state-of-the-art%20LLM-based%20methods%20and%20the%0Atraditional%20GNN-%20based%20supervised%20approaches.%20Furthermore%2C%20Strada-LLM%20can%20be%0Aeasily%20adapted%20to%20different%20LLM%20backbones%20without%20a%20noticeable%20performance%0Adrop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrada-LLM%253A%2520Graph%2520LLM%2520for%2520traffic%2520prediction%26entry.906535625%3DSeyed%2520Mohamad%2520Moghadas%2520and%2520Yangxintong%2520Lyu%2520and%2520Bruno%2520Cornelis%2520and%2520Alexandre%2520Alahi%2520and%2520Adrian%2520Munteanu%26entry.1292438233%3D%2520%2520Traffic%2520prediction%2520is%2520a%2520vital%2520component%2520of%2520intelligent%2520transportation%250Asystems.%2520By%2520reasoning%2520about%2520traffic%2520patterns%2520in%2520both%2520the%2520spatial%2520and%2520temporal%250Adimensions%252C%2520accurate%2520and%2520interpretable%2520predictions%2520can%2520be%2520provided.%2520A%250Aconsiderable%2520challenge%2520in%2520traffic%2520prediction%2520lies%2520in%2520handling%2520the%2520diverse%2520data%250Adistributions%2520caused%2520by%2520vastly%2520different%2520traffic%2520conditions%2520occurring%2520at%250Adifferent%2520locations.%2520LLMs%2520have%2520been%2520a%2520dominant%2520solution%2520due%2520to%2520their%2520remarkable%250Acapacity%2520to%2520adapt%2520to%2520new%2520datasets%2520with%2520very%2520few%2520labeled%2520data%2520samples%252C%2520i.e.%252C%250Afew-shot%2520adaptability.%2520However%252C%2520existing%2520forecasting%2520techniques%2520mainly%2520focus%2520on%250Aextracting%2520local%2520graph%2520information%2520and%2520forming%2520a%2520text-like%2520prompt%252C%2520leaving%2520LLM-%250Abased%2520traffic%2520prediction%2520an%2520open%2520problem.%2520This%2520work%2520presents%2520a%2520probabilistic%250ALLM%2520for%2520traffic%2520forecasting%2520with%2520three%2520highlights.%2520We%2520propose%2520a%2520graph-aware%2520LLM%250Afor%2520traffic%2520prediction%2520that%2520considers%2520proximal%2520traffic%2520information.%250ASpecifically%252C%2520by%2520considering%2520the%2520traffic%2520of%2520neighboring%2520nodes%2520as%2520covariates%252C%250Aour%2520model%2520outperforms%2520the%2520corresponding%2520time-series%2520LLM.%2520Furthermore%252C%2520we%2520adopt%250Aa%2520lightweight%2520approach%2520for%2520efficient%2520domain%2520adaptation%2520when%2520facing%2520new%2520data%250Adistributions%2520in%2520few-shot%2520fashion.%2520The%2520comparative%2520experiment%2520demonstrates%2520the%250Aproposed%2520method%2520outperforms%2520the%2520state-of-the-art%2520LLM-based%2520methods%2520and%2520the%250Atraditional%2520GNN-%2520based%2520supervised%2520approaches.%2520Furthermore%252C%2520Strada-LLM%2520can%2520be%250Aeasily%2520adapted%2520to%2520different%2520LLM%2520backbones%2520without%2520a%2520noticeable%2520performance%250Adrop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strada-LLM%3A%20Graph%20LLM%20for%20traffic%20prediction&entry.906535625=Seyed%20Mohamad%20Moghadas%20and%20Yangxintong%20Lyu%20and%20Bruno%20Cornelis%20and%20Alexandre%20Alahi%20and%20Adrian%20Munteanu&entry.1292438233=%20%20Traffic%20prediction%20is%20a%20vital%20component%20of%20intelligent%20transportation%0Asystems.%20By%20reasoning%20about%20traffic%20patterns%20in%20both%20the%20spatial%20and%20temporal%0Adimensions%2C%20accurate%20and%20interpretable%20predictions%20can%20be%20provided.%20A%0Aconsiderable%20challenge%20in%20traffic%20prediction%20lies%20in%20handling%20the%20diverse%20data%0Adistributions%20caused%20by%20vastly%20different%20traffic%20conditions%20occurring%20at%0Adifferent%20locations.%20LLMs%20have%20been%20a%20dominant%20solution%20due%20to%20their%20remarkable%0Acapacity%20to%20adapt%20to%20new%20datasets%20with%20very%20few%20labeled%20data%20samples%2C%20i.e.%2C%0Afew-shot%20adaptability.%20However%2C%20existing%20forecasting%20techniques%20mainly%20focus%20on%0Aextracting%20local%20graph%20information%20and%20forming%20a%20text-like%20prompt%2C%20leaving%20LLM-%0Abased%20traffic%20prediction%20an%20open%20problem.%20This%20work%20presents%20a%20probabilistic%0ALLM%20for%20traffic%20forecasting%20with%20three%20highlights.%20We%20propose%20a%20graph-aware%20LLM%0Afor%20traffic%20prediction%20that%20considers%20proximal%20traffic%20information.%0ASpecifically%2C%20by%20considering%20the%20traffic%20of%20neighboring%20nodes%20as%20covariates%2C%0Aour%20model%20outperforms%20the%20corresponding%20time-series%20LLM.%20Furthermore%2C%20we%20adopt%0Aa%20lightweight%20approach%20for%20efficient%20domain%20adaptation%20when%20facing%20new%20data%0Adistributions%20in%20few-shot%20fashion.%20The%20comparative%20experiment%20demonstrates%20the%0Aproposed%20method%20outperforms%20the%20state-of-the-art%20LLM-based%20methods%20and%20the%0Atraditional%20GNN-%20based%20supervised%20approaches.%20Furthermore%2C%20Strada-LLM%20can%20be%0Aeasily%20adapted%20to%20different%20LLM%20backbones%20without%20a%20noticeable%20performance%0Adrop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20856v2&entry.124074799=Read"},
{"title": "Continual Learning with Strategic Selection and Forgetting for Network\n  Intrusion Detection", "author": "Xinchen Zhang and Running Zhao and Zhihan Jiang and Handi Chen and Yulong Ding and Edith C. H. Ngai and Shuang-Hua Yang", "abstract": "  Intrusion Detection Systems (IDS) are crucial for safeguarding digital\ninfrastructure. In dynamic network environments, both threat landscapes and\nnormal operational behaviors are constantly changing, resulting in concept\ndrift. While continuous learning mitigates the adverse effects of concept\ndrift, insufficient attention to drift patterns and excessive preservation of\noutdated knowledge can still hinder the IDS's adaptability. In this paper, we\npropose SSF (Strategic Selection and Forgetting), a novel continual learning\nmethod for IDS, providing continuous model updates with a constantly refreshed\nmemory buffer. Our approach features a strategic sample selection algorithm to\nselect representative new samples and a strategic forgetting mechanism to drop\noutdated samples. The proposed strategic sample selection algorithm prioritizes\nnew samples that cause the `drifted' pattern, enabling the model to better\nunderstand the evolving landscape. Additionally, we introduce strategic\nforgetting upon detecting significant drift by discarding outdated samples to\nfree up memory, allowing the incorporation of more recent data. SSF captures\nevolving patterns effectively and ensures the model is aligned with the change\nof data patterns, significantly enhancing the IDS's adaptability to concept\ndrift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15\ndatasets demonstrates its superior adaptability to concept drift for network\nintrusion detection. The code is released at\nhttps://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.\n", "link": "http://arxiv.org/abs/2412.16264v3", "date": "2025-02-14", "relevancy": 1.9635, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5013}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4995}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20with%20Strategic%20Selection%20and%20Forgetting%20for%20Network%0A%20%20Intrusion%20Detection&body=Title%3A%20Continual%20Learning%20with%20Strategic%20Selection%20and%20Forgetting%20for%20Network%0A%20%20Intrusion%20Detection%0AAuthor%3A%20Xinchen%20Zhang%20and%20Running%20Zhao%20and%20Zhihan%20Jiang%20and%20Handi%20Chen%20and%20Yulong%20Ding%20and%20Edith%20C.%20H.%20Ngai%20and%20Shuang-Hua%20Yang%0AAbstract%3A%20%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20are%20crucial%20for%20safeguarding%20digital%0Ainfrastructure.%20In%20dynamic%20network%20environments%2C%20both%20threat%20landscapes%20and%0Anormal%20operational%20behaviors%20are%20constantly%20changing%2C%20resulting%20in%20concept%0Adrift.%20While%20continuous%20learning%20mitigates%20the%20adverse%20effects%20of%20concept%0Adrift%2C%20insufficient%20attention%20to%20drift%20patterns%20and%20excessive%20preservation%20of%0Aoutdated%20knowledge%20can%20still%20hinder%20the%20IDS%27s%20adaptability.%20In%20this%20paper%2C%20we%0Apropose%20SSF%20%28Strategic%20Selection%20and%20Forgetting%29%2C%20a%20novel%20continual%20learning%0Amethod%20for%20IDS%2C%20providing%20continuous%20model%20updates%20with%20a%20constantly%20refreshed%0Amemory%20buffer.%20Our%20approach%20features%20a%20strategic%20sample%20selection%20algorithm%20to%0Aselect%20representative%20new%20samples%20and%20a%20strategic%20forgetting%20mechanism%20to%20drop%0Aoutdated%20samples.%20The%20proposed%20strategic%20sample%20selection%20algorithm%20prioritizes%0Anew%20samples%20that%20cause%20the%20%60drifted%27%20pattern%2C%20enabling%20the%20model%20to%20better%0Aunderstand%20the%20evolving%20landscape.%20Additionally%2C%20we%20introduce%20strategic%0Aforgetting%20upon%20detecting%20significant%20drift%20by%20discarding%20outdated%20samples%20to%0Afree%20up%20memory%2C%20allowing%20the%20incorporation%20of%20more%20recent%20data.%20SSF%20captures%0Aevolving%20patterns%20effectively%20and%20ensures%20the%20model%20is%20aligned%20with%20the%20change%0Aof%20data%20patterns%2C%20significantly%20enhancing%20the%20IDS%27s%20adaptability%20to%20concept%0Adrift.%20The%20state-of-the-art%20performance%20of%20SSF%20on%20NSL-KDD%20and%20UNSW-NB15%0Adatasets%20demonstrates%20its%20superior%20adaptability%20to%20concept%20drift%20for%20network%0Aintrusion%20detection.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16264v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520with%2520Strategic%2520Selection%2520and%2520Forgetting%2520for%2520Network%250A%2520%2520Intrusion%2520Detection%26entry.906535625%3DXinchen%2520Zhang%2520and%2520Running%2520Zhao%2520and%2520Zhihan%2520Jiang%2520and%2520Handi%2520Chen%2520and%2520Yulong%2520Ding%2520and%2520Edith%2520C.%2520H.%2520Ngai%2520and%2520Shuang-Hua%2520Yang%26entry.1292438233%3D%2520%2520Intrusion%2520Detection%2520Systems%2520%2528IDS%2529%2520are%2520crucial%2520for%2520safeguarding%2520digital%250Ainfrastructure.%2520In%2520dynamic%2520network%2520environments%252C%2520both%2520threat%2520landscapes%2520and%250Anormal%2520operational%2520behaviors%2520are%2520constantly%2520changing%252C%2520resulting%2520in%2520concept%250Adrift.%2520While%2520continuous%2520learning%2520mitigates%2520the%2520adverse%2520effects%2520of%2520concept%250Adrift%252C%2520insufficient%2520attention%2520to%2520drift%2520patterns%2520and%2520excessive%2520preservation%2520of%250Aoutdated%2520knowledge%2520can%2520still%2520hinder%2520the%2520IDS%2527s%2520adaptability.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520SSF%2520%2528Strategic%2520Selection%2520and%2520Forgetting%2529%252C%2520a%2520novel%2520continual%2520learning%250Amethod%2520for%2520IDS%252C%2520providing%2520continuous%2520model%2520updates%2520with%2520a%2520constantly%2520refreshed%250Amemory%2520buffer.%2520Our%2520approach%2520features%2520a%2520strategic%2520sample%2520selection%2520algorithm%2520to%250Aselect%2520representative%2520new%2520samples%2520and%2520a%2520strategic%2520forgetting%2520mechanism%2520to%2520drop%250Aoutdated%2520samples.%2520The%2520proposed%2520strategic%2520sample%2520selection%2520algorithm%2520prioritizes%250Anew%2520samples%2520that%2520cause%2520the%2520%2560drifted%2527%2520pattern%252C%2520enabling%2520the%2520model%2520to%2520better%250Aunderstand%2520the%2520evolving%2520landscape.%2520Additionally%252C%2520we%2520introduce%2520strategic%250Aforgetting%2520upon%2520detecting%2520significant%2520drift%2520by%2520discarding%2520outdated%2520samples%2520to%250Afree%2520up%2520memory%252C%2520allowing%2520the%2520incorporation%2520of%2520more%2520recent%2520data.%2520SSF%2520captures%250Aevolving%2520patterns%2520effectively%2520and%2520ensures%2520the%2520model%2520is%2520aligned%2520with%2520the%2520change%250Aof%2520data%2520patterns%252C%2520significantly%2520enhancing%2520the%2520IDS%2527s%2520adaptability%2520to%2520concept%250Adrift.%2520The%2520state-of-the-art%2520performance%2520of%2520SSF%2520on%2520NSL-KDD%2520and%2520UNSW-NB15%250Adatasets%2520demonstrates%2520its%2520superior%2520adaptability%2520to%2520concept%2520drift%2520for%2520network%250Aintrusion%2520detection.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16264v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20with%20Strategic%20Selection%20and%20Forgetting%20for%20Network%0A%20%20Intrusion%20Detection&entry.906535625=Xinchen%20Zhang%20and%20Running%20Zhao%20and%20Zhihan%20Jiang%20and%20Handi%20Chen%20and%20Yulong%20Ding%20and%20Edith%20C.%20H.%20Ngai%20and%20Shuang-Hua%20Yang&entry.1292438233=%20%20Intrusion%20Detection%20Systems%20%28IDS%29%20are%20crucial%20for%20safeguarding%20digital%0Ainfrastructure.%20In%20dynamic%20network%20environments%2C%20both%20threat%20landscapes%20and%0Anormal%20operational%20behaviors%20are%20constantly%20changing%2C%20resulting%20in%20concept%0Adrift.%20While%20continuous%20learning%20mitigates%20the%20adverse%20effects%20of%20concept%0Adrift%2C%20insufficient%20attention%20to%20drift%20patterns%20and%20excessive%20preservation%20of%0Aoutdated%20knowledge%20can%20still%20hinder%20the%20IDS%27s%20adaptability.%20In%20this%20paper%2C%20we%0Apropose%20SSF%20%28Strategic%20Selection%20and%20Forgetting%29%2C%20a%20novel%20continual%20learning%0Amethod%20for%20IDS%2C%20providing%20continuous%20model%20updates%20with%20a%20constantly%20refreshed%0Amemory%20buffer.%20Our%20approach%20features%20a%20strategic%20sample%20selection%20algorithm%20to%0Aselect%20representative%20new%20samples%20and%20a%20strategic%20forgetting%20mechanism%20to%20drop%0Aoutdated%20samples.%20The%20proposed%20strategic%20sample%20selection%20algorithm%20prioritizes%0Anew%20samples%20that%20cause%20the%20%60drifted%27%20pattern%2C%20enabling%20the%20model%20to%20better%0Aunderstand%20the%20evolving%20landscape.%20Additionally%2C%20we%20introduce%20strategic%0Aforgetting%20upon%20detecting%20significant%20drift%20by%20discarding%20outdated%20samples%20to%0Afree%20up%20memory%2C%20allowing%20the%20incorporation%20of%20more%20recent%20data.%20SSF%20captures%0Aevolving%20patterns%20effectively%20and%20ensures%20the%20model%20is%20aligned%20with%20the%20change%0Aof%20data%20patterns%2C%20significantly%20enhancing%20the%20IDS%27s%20adaptability%20to%20concept%0Adrift.%20The%20state-of-the-art%20performance%20of%20SSF%20on%20NSL-KDD%20and%20UNSW-NB15%0Adatasets%20demonstrates%20its%20superior%20adaptability%20to%20concept%20drift%20for%20network%0Aintrusion%20detection.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16264v3&entry.124074799=Read"},
{"title": "InfoPos: A ML-Assisted Solution Design Support Framework for Industrial\n  Cyber-Physical Systems", "author": "Uraz Odyurt and Richard Loendersloot and Tiedo Tinga", "abstract": "  The variety of building blocks and algorithms incorporated in data-centric\nand ML-assisted solutions is high, contributing to two challenges: selection of\nmost effective set and order of building blocks, as well as achieving such a\nselection with minimum cost. Considering that ML-assisted solution design is\ninfluenced by the extent of available data, as well as available knowledge of\nthe target system, it is advantageous to be able to select matching building\nblocks. We introduce the first iteration of our InfoPos framework, allowing the\nplacement of use-cases considering the available positions (levels), i.e., from\npoor to rich, of knowledge and data dimensions. With that input, designers and\ndevelopers can reveal the most effective corresponding choice(s), streamlining\nthe solution design process. The results from our demonstrator, an anomaly\nidentification use-case for industrial Cyber-Physical Systems, reflects\nachieved effects upon the use of different building blocks throughout knowledge\nand data positions. The achieved ML model performance is considered as the\nindicator. Our data processing code and the composed data sets are publicly\navailable.\n", "link": "http://arxiv.org/abs/2502.10331v1", "date": "2025-02-14", "relevancy": 0.9994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4884}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoPos%3A%20A%20ML-Assisted%20Solution%20Design%20Support%20Framework%20for%20Industrial%0A%20%20Cyber-Physical%20Systems&body=Title%3A%20InfoPos%3A%20A%20ML-Assisted%20Solution%20Design%20Support%20Framework%20for%20Industrial%0A%20%20Cyber-Physical%20Systems%0AAuthor%3A%20Uraz%20Odyurt%20and%20Richard%20Loendersloot%20and%20Tiedo%20Tinga%0AAbstract%3A%20%20%20The%20variety%20of%20building%20blocks%20and%20algorithms%20incorporated%20in%20data-centric%0Aand%20ML-assisted%20solutions%20is%20high%2C%20contributing%20to%20two%20challenges%3A%20selection%20of%0Amost%20effective%20set%20and%20order%20of%20building%20blocks%2C%20as%20well%20as%20achieving%20such%20a%0Aselection%20with%20minimum%20cost.%20Considering%20that%20ML-assisted%20solution%20design%20is%0Ainfluenced%20by%20the%20extent%20of%20available%20data%2C%20as%20well%20as%20available%20knowledge%20of%0Athe%20target%20system%2C%20it%20is%20advantageous%20to%20be%20able%20to%20select%20matching%20building%0Ablocks.%20We%20introduce%20the%20first%20iteration%20of%20our%20InfoPos%20framework%2C%20allowing%20the%0Aplacement%20of%20use-cases%20considering%20the%20available%20positions%20%28levels%29%2C%20i.e.%2C%20from%0Apoor%20to%20rich%2C%20of%20knowledge%20and%20data%20dimensions.%20With%20that%20input%2C%20designers%20and%0Adevelopers%20can%20reveal%20the%20most%20effective%20corresponding%20choice%28s%29%2C%20streamlining%0Athe%20solution%20design%20process.%20The%20results%20from%20our%20demonstrator%2C%20an%20anomaly%0Aidentification%20use-case%20for%20industrial%20Cyber-Physical%20Systems%2C%20reflects%0Aachieved%20effects%20upon%20the%20use%20of%20different%20building%20blocks%20throughout%20knowledge%0Aand%20data%20positions.%20The%20achieved%20ML%20model%20performance%20is%20considered%20as%20the%0Aindicator.%20Our%20data%20processing%20code%20and%20the%20composed%20data%20sets%20are%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoPos%253A%2520A%2520ML-Assisted%2520Solution%2520Design%2520Support%2520Framework%2520for%2520Industrial%250A%2520%2520Cyber-Physical%2520Systems%26entry.906535625%3DUraz%2520Odyurt%2520and%2520Richard%2520Loendersloot%2520and%2520Tiedo%2520Tinga%26entry.1292438233%3D%2520%2520The%2520variety%2520of%2520building%2520blocks%2520and%2520algorithms%2520incorporated%2520in%2520data-centric%250Aand%2520ML-assisted%2520solutions%2520is%2520high%252C%2520contributing%2520to%2520two%2520challenges%253A%2520selection%2520of%250Amost%2520effective%2520set%2520and%2520order%2520of%2520building%2520blocks%252C%2520as%2520well%2520as%2520achieving%2520such%2520a%250Aselection%2520with%2520minimum%2520cost.%2520Considering%2520that%2520ML-assisted%2520solution%2520design%2520is%250Ainfluenced%2520by%2520the%2520extent%2520of%2520available%2520data%252C%2520as%2520well%2520as%2520available%2520knowledge%2520of%250Athe%2520target%2520system%252C%2520it%2520is%2520advantageous%2520to%2520be%2520able%2520to%2520select%2520matching%2520building%250Ablocks.%2520We%2520introduce%2520the%2520first%2520iteration%2520of%2520our%2520InfoPos%2520framework%252C%2520allowing%2520the%250Aplacement%2520of%2520use-cases%2520considering%2520the%2520available%2520positions%2520%2528levels%2529%252C%2520i.e.%252C%2520from%250Apoor%2520to%2520rich%252C%2520of%2520knowledge%2520and%2520data%2520dimensions.%2520With%2520that%2520input%252C%2520designers%2520and%250Adevelopers%2520can%2520reveal%2520the%2520most%2520effective%2520corresponding%2520choice%2528s%2529%252C%2520streamlining%250Athe%2520solution%2520design%2520process.%2520The%2520results%2520from%2520our%2520demonstrator%252C%2520an%2520anomaly%250Aidentification%2520use-case%2520for%2520industrial%2520Cyber-Physical%2520Systems%252C%2520reflects%250Aachieved%2520effects%2520upon%2520the%2520use%2520of%2520different%2520building%2520blocks%2520throughout%2520knowledge%250Aand%2520data%2520positions.%2520The%2520achieved%2520ML%2520model%2520performance%2520is%2520considered%2520as%2520the%250Aindicator.%2520Our%2520data%2520processing%2520code%2520and%2520the%2520composed%2520data%2520sets%2520are%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoPos%3A%20A%20ML-Assisted%20Solution%20Design%20Support%20Framework%20for%20Industrial%0A%20%20Cyber-Physical%20Systems&entry.906535625=Uraz%20Odyurt%20and%20Richard%20Loendersloot%20and%20Tiedo%20Tinga&entry.1292438233=%20%20The%20variety%20of%20building%20blocks%20and%20algorithms%20incorporated%20in%20data-centric%0Aand%20ML-assisted%20solutions%20is%20high%2C%20contributing%20to%20two%20challenges%3A%20selection%20of%0Amost%20effective%20set%20and%20order%20of%20building%20blocks%2C%20as%20well%20as%20achieving%20such%20a%0Aselection%20with%20minimum%20cost.%20Considering%20that%20ML-assisted%20solution%20design%20is%0Ainfluenced%20by%20the%20extent%20of%20available%20data%2C%20as%20well%20as%20available%20knowledge%20of%0Athe%20target%20system%2C%20it%20is%20advantageous%20to%20be%20able%20to%20select%20matching%20building%0Ablocks.%20We%20introduce%20the%20first%20iteration%20of%20our%20InfoPos%20framework%2C%20allowing%20the%0Aplacement%20of%20use-cases%20considering%20the%20available%20positions%20%28levels%29%2C%20i.e.%2C%20from%0Apoor%20to%20rich%2C%20of%20knowledge%20and%20data%20dimensions.%20With%20that%20input%2C%20designers%20and%0Adevelopers%20can%20reveal%20the%20most%20effective%20corresponding%20choice%28s%29%2C%20streamlining%0Athe%20solution%20design%20process.%20The%20results%20from%20our%20demonstrator%2C%20an%20anomaly%0Aidentification%20use-case%20for%20industrial%20Cyber-Physical%20Systems%2C%20reflects%0Aachieved%20effects%20upon%20the%20use%20of%20different%20building%20blocks%20throughout%20knowledge%0Aand%20data%20positions.%20The%20achieved%20ML%20model%20performance%20is%20considered%20as%20the%0Aindicator.%20Our%20data%20processing%20code%20and%20the%20composed%20data%20sets%20are%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10331v1&entry.124074799=Read"},
{"title": "Fenchel-Young Variational Learning", "author": "Sophia Sklaviadis and Sweta Agrawal and Antonio Farinhas and Andre Martins and Mario Figueiredo", "abstract": "  From a variational perspective, many statistical learning criteria involve\nseeking a distribution that balances empirical risk and regularization. In this\npaper, we broaden this perspective by introducing a new general class of\nvariational methods based on Fenchel-Young (FY) losses, treated as divergences\nthat generalize (and encompass) the familiar Kullback-Leibler divergence at the\ncore of classical variational learning. Our proposed formulation -- FY\nvariational learning -- includes as key ingredients new notions of FY free\nenergy, FY evidence, FY evidence lower bound, and FY posterior. We derive\nalternating minimization and gradient backpropagation algorithms to compute (or\nlower bound) the FY evidence, which enables learning a wider class of models\nthan previous variational formulations. This leads to generalized FY variants\nof classical algorithms, such as an FY expectation-maximization (FYEM)\nalgorithm, and latent-variable models, such as an FY variational autoencoder\n(FYVAE). Our new methods are shown to be empirically competitive, often\noutperforming their classical counterparts, and most importantly, to have\nqualitatively novel features. For example, FYEM has an adaptively sparse\nE-step, while the FYVAE can support models with sparse observations and sparse\nposteriors.\n", "link": "http://arxiv.org/abs/2502.10295v1", "date": "2025-02-14", "relevancy": 1.5378, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.54}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fenchel-Young%20Variational%20Learning&body=Title%3A%20Fenchel-Young%20Variational%20Learning%0AAuthor%3A%20Sophia%20Sklaviadis%20and%20Sweta%20Agrawal%20and%20Antonio%20Farinhas%20and%20Andre%20Martins%20and%20Mario%20Figueiredo%0AAbstract%3A%20%20%20From%20a%20variational%20perspective%2C%20many%20statistical%20learning%20criteria%20involve%0Aseeking%20a%20distribution%20that%20balances%20empirical%20risk%20and%20regularization.%20In%20this%0Apaper%2C%20we%20broaden%20this%20perspective%20by%20introducing%20a%20new%20general%20class%20of%0Avariational%20methods%20based%20on%20Fenchel-Young%20%28FY%29%20losses%2C%20treated%20as%20divergences%0Athat%20generalize%20%28and%20encompass%29%20the%20familiar%20Kullback-Leibler%20divergence%20at%20the%0Acore%20of%20classical%20variational%20learning.%20Our%20proposed%20formulation%20--%20FY%0Avariational%20learning%20--%20includes%20as%20key%20ingredients%20new%20notions%20of%20FY%20free%0Aenergy%2C%20FY%20evidence%2C%20FY%20evidence%20lower%20bound%2C%20and%20FY%20posterior.%20We%20derive%0Aalternating%20minimization%20and%20gradient%20backpropagation%20algorithms%20to%20compute%20%28or%0Alower%20bound%29%20the%20FY%20evidence%2C%20which%20enables%20learning%20a%20wider%20class%20of%20models%0Athan%20previous%20variational%20formulations.%20This%20leads%20to%20generalized%20FY%20variants%0Aof%20classical%20algorithms%2C%20such%20as%20an%20FY%20expectation-maximization%20%28FYEM%29%0Aalgorithm%2C%20and%20latent-variable%20models%2C%20such%20as%20an%20FY%20variational%20autoencoder%0A%28FYVAE%29.%20Our%20new%20methods%20are%20shown%20to%20be%20empirically%20competitive%2C%20often%0Aoutperforming%20their%20classical%20counterparts%2C%20and%20most%20importantly%2C%20to%20have%0Aqualitatively%20novel%20features.%20For%20example%2C%20FYEM%20has%20an%20adaptively%20sparse%0AE-step%2C%20while%20the%20FYVAE%20can%20support%20models%20with%20sparse%20observations%20and%20sparse%0Aposteriors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFenchel-Young%2520Variational%2520Learning%26entry.906535625%3DSophia%2520Sklaviadis%2520and%2520Sweta%2520Agrawal%2520and%2520Antonio%2520Farinhas%2520and%2520Andre%2520Martins%2520and%2520Mario%2520Figueiredo%26entry.1292438233%3D%2520%2520From%2520a%2520variational%2520perspective%252C%2520many%2520statistical%2520learning%2520criteria%2520involve%250Aseeking%2520a%2520distribution%2520that%2520balances%2520empirical%2520risk%2520and%2520regularization.%2520In%2520this%250Apaper%252C%2520we%2520broaden%2520this%2520perspective%2520by%2520introducing%2520a%2520new%2520general%2520class%2520of%250Avariational%2520methods%2520based%2520on%2520Fenchel-Young%2520%2528FY%2529%2520losses%252C%2520treated%2520as%2520divergences%250Athat%2520generalize%2520%2528and%2520encompass%2529%2520the%2520familiar%2520Kullback-Leibler%2520divergence%2520at%2520the%250Acore%2520of%2520classical%2520variational%2520learning.%2520Our%2520proposed%2520formulation%2520--%2520FY%250Avariational%2520learning%2520--%2520includes%2520as%2520key%2520ingredients%2520new%2520notions%2520of%2520FY%2520free%250Aenergy%252C%2520FY%2520evidence%252C%2520FY%2520evidence%2520lower%2520bound%252C%2520and%2520FY%2520posterior.%2520We%2520derive%250Aalternating%2520minimization%2520and%2520gradient%2520backpropagation%2520algorithms%2520to%2520compute%2520%2528or%250Alower%2520bound%2529%2520the%2520FY%2520evidence%252C%2520which%2520enables%2520learning%2520a%2520wider%2520class%2520of%2520models%250Athan%2520previous%2520variational%2520formulations.%2520This%2520leads%2520to%2520generalized%2520FY%2520variants%250Aof%2520classical%2520algorithms%252C%2520such%2520as%2520an%2520FY%2520expectation-maximization%2520%2528FYEM%2529%250Aalgorithm%252C%2520and%2520latent-variable%2520models%252C%2520such%2520as%2520an%2520FY%2520variational%2520autoencoder%250A%2528FYVAE%2529.%2520Our%2520new%2520methods%2520are%2520shown%2520to%2520be%2520empirically%2520competitive%252C%2520often%250Aoutperforming%2520their%2520classical%2520counterparts%252C%2520and%2520most%2520importantly%252C%2520to%2520have%250Aqualitatively%2520novel%2520features.%2520For%2520example%252C%2520FYEM%2520has%2520an%2520adaptively%2520sparse%250AE-step%252C%2520while%2520the%2520FYVAE%2520can%2520support%2520models%2520with%2520sparse%2520observations%2520and%2520sparse%250Aposteriors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fenchel-Young%20Variational%20Learning&entry.906535625=Sophia%20Sklaviadis%20and%20Sweta%20Agrawal%20and%20Antonio%20Farinhas%20and%20Andre%20Martins%20and%20Mario%20Figueiredo&entry.1292438233=%20%20From%20a%20variational%20perspective%2C%20many%20statistical%20learning%20criteria%20involve%0Aseeking%20a%20distribution%20that%20balances%20empirical%20risk%20and%20regularization.%20In%20this%0Apaper%2C%20we%20broaden%20this%20perspective%20by%20introducing%20a%20new%20general%20class%20of%0Avariational%20methods%20based%20on%20Fenchel-Young%20%28FY%29%20losses%2C%20treated%20as%20divergences%0Athat%20generalize%20%28and%20encompass%29%20the%20familiar%20Kullback-Leibler%20divergence%20at%20the%0Acore%20of%20classical%20variational%20learning.%20Our%20proposed%20formulation%20--%20FY%0Avariational%20learning%20--%20includes%20as%20key%20ingredients%20new%20notions%20of%20FY%20free%0Aenergy%2C%20FY%20evidence%2C%20FY%20evidence%20lower%20bound%2C%20and%20FY%20posterior.%20We%20derive%0Aalternating%20minimization%20and%20gradient%20backpropagation%20algorithms%20to%20compute%20%28or%0Alower%20bound%29%20the%20FY%20evidence%2C%20which%20enables%20learning%20a%20wider%20class%20of%20models%0Athan%20previous%20variational%20formulations.%20This%20leads%20to%20generalized%20FY%20variants%0Aof%20classical%20algorithms%2C%20such%20as%20an%20FY%20expectation-maximization%20%28FYEM%29%0Aalgorithm%2C%20and%20latent-variable%20models%2C%20such%20as%20an%20FY%20variational%20autoencoder%0A%28FYVAE%29.%20Our%20new%20methods%20are%20shown%20to%20be%20empirically%20competitive%2C%20often%0Aoutperforming%20their%20classical%20counterparts%2C%20and%20most%20importantly%2C%20to%20have%0Aqualitatively%20novel%20features.%20For%20example%2C%20FYEM%20has%20an%20adaptively%20sparse%0AE-step%2C%20while%20the%20FYVAE%20can%20support%20models%20with%20sparse%20observations%20and%20sparse%0Aposteriors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10295v1&entry.124074799=Read"},
{"title": "Analog In-memory Training on General Non-ideal Resistive Elements: The\n  Impact of Response Functions", "author": "Zhaoxian Wu and Quan Xiao and Tayfun Gokmen and Omobayode Fagbohungbe and Tianyi Chen", "abstract": "  As the economic and environmental costs of training and deploying large\nvision or language models increase dramatically, analog in-memory computing\n(AIMC) emerges as a promising energy-efficient solution. However, the training\nperspective, especially its training dynamic, is underexplored. In AIMC\nhardware, the trainable weights are represented by the conductance of resistive\nelements and updated using consecutive electrical pulses. Among all the\nphysical properties of resistive elements, the response to the pulses directly\naffects the training dynamics. This paper first provides a theoretical\nfoundation for gradient-based training on AIMC hardware and studies the impact\nof response functions. We demonstrate that noisy update and asymmetric response\nfunctions negatively impact Analog SGD by imposing an implicit penalty term on\nthe objective. To overcome the issue, Tiki-Taka, a residual learning algorithm,\nconverges exactly to a critical point by optimizing a main array and a residual\narray bilevelly. The conclusion is supported by simulations validating our\ntheoretical insights.\n", "link": "http://arxiv.org/abs/2502.06309v2", "date": "2025-02-14", "relevancy": 1.5401, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5178}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5163}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analog%20In-memory%20Training%20on%20General%20Non-ideal%20Resistive%20Elements%3A%20The%0A%20%20Impact%20of%20Response%20Functions&body=Title%3A%20Analog%20In-memory%20Training%20on%20General%20Non-ideal%20Resistive%20Elements%3A%20The%0A%20%20Impact%20of%20Response%20Functions%0AAuthor%3A%20Zhaoxian%20Wu%20and%20Quan%20Xiao%20and%20Tayfun%20Gokmen%20and%20Omobayode%20Fagbohungbe%20and%20Tianyi%20Chen%0AAbstract%3A%20%20%20As%20the%20economic%20and%20environmental%20costs%20of%20training%20and%20deploying%20large%0Avision%20or%20language%20models%20increase%20dramatically%2C%20analog%20in-memory%20computing%0A%28AIMC%29%20emerges%20as%20a%20promising%20energy-efficient%20solution.%20However%2C%20the%20training%0Aperspective%2C%20especially%20its%20training%20dynamic%2C%20is%20underexplored.%20In%20AIMC%0Ahardware%2C%20the%20trainable%20weights%20are%20represented%20by%20the%20conductance%20of%20resistive%0Aelements%20and%20updated%20using%20consecutive%20electrical%20pulses.%20Among%20all%20the%0Aphysical%20properties%20of%20resistive%20elements%2C%20the%20response%20to%20the%20pulses%20directly%0Aaffects%20the%20training%20dynamics.%20This%20paper%20first%20provides%20a%20theoretical%0Afoundation%20for%20gradient-based%20training%20on%20AIMC%20hardware%20and%20studies%20the%20impact%0Aof%20response%20functions.%20We%20demonstrate%20that%20noisy%20update%20and%20asymmetric%20response%0Afunctions%20negatively%20impact%20Analog%20SGD%20by%20imposing%20an%20implicit%20penalty%20term%20on%0Athe%20objective.%20To%20overcome%20the%20issue%2C%20Tiki-Taka%2C%20a%20residual%20learning%20algorithm%2C%0Aconverges%20exactly%20to%20a%20critical%20point%20by%20optimizing%20a%20main%20array%20and%20a%20residual%0Aarray%20bilevelly.%20The%20conclusion%20is%20supported%20by%20simulations%20validating%20our%0Atheoretical%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalog%2520In-memory%2520Training%2520on%2520General%2520Non-ideal%2520Resistive%2520Elements%253A%2520The%250A%2520%2520Impact%2520of%2520Response%2520Functions%26entry.906535625%3DZhaoxian%2520Wu%2520and%2520Quan%2520Xiao%2520and%2520Tayfun%2520Gokmen%2520and%2520Omobayode%2520Fagbohungbe%2520and%2520Tianyi%2520Chen%26entry.1292438233%3D%2520%2520As%2520the%2520economic%2520and%2520environmental%2520costs%2520of%2520training%2520and%2520deploying%2520large%250Avision%2520or%2520language%2520models%2520increase%2520dramatically%252C%2520analog%2520in-memory%2520computing%250A%2528AIMC%2529%2520emerges%2520as%2520a%2520promising%2520energy-efficient%2520solution.%2520However%252C%2520the%2520training%250Aperspective%252C%2520especially%2520its%2520training%2520dynamic%252C%2520is%2520underexplored.%2520In%2520AIMC%250Ahardware%252C%2520the%2520trainable%2520weights%2520are%2520represented%2520by%2520the%2520conductance%2520of%2520resistive%250Aelements%2520and%2520updated%2520using%2520consecutive%2520electrical%2520pulses.%2520Among%2520all%2520the%250Aphysical%2520properties%2520of%2520resistive%2520elements%252C%2520the%2520response%2520to%2520the%2520pulses%2520directly%250Aaffects%2520the%2520training%2520dynamics.%2520This%2520paper%2520first%2520provides%2520a%2520theoretical%250Afoundation%2520for%2520gradient-based%2520training%2520on%2520AIMC%2520hardware%2520and%2520studies%2520the%2520impact%250Aof%2520response%2520functions.%2520We%2520demonstrate%2520that%2520noisy%2520update%2520and%2520asymmetric%2520response%250Afunctions%2520negatively%2520impact%2520Analog%2520SGD%2520by%2520imposing%2520an%2520implicit%2520penalty%2520term%2520on%250Athe%2520objective.%2520To%2520overcome%2520the%2520issue%252C%2520Tiki-Taka%252C%2520a%2520residual%2520learning%2520algorithm%252C%250Aconverges%2520exactly%2520to%2520a%2520critical%2520point%2520by%2520optimizing%2520a%2520main%2520array%2520and%2520a%2520residual%250Aarray%2520bilevelly.%2520The%2520conclusion%2520is%2520supported%2520by%2520simulations%2520validating%2520our%250Atheoretical%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analog%20In-memory%20Training%20on%20General%20Non-ideal%20Resistive%20Elements%3A%20The%0A%20%20Impact%20of%20Response%20Functions&entry.906535625=Zhaoxian%20Wu%20and%20Quan%20Xiao%20and%20Tayfun%20Gokmen%20and%20Omobayode%20Fagbohungbe%20and%20Tianyi%20Chen&entry.1292438233=%20%20As%20the%20economic%20and%20environmental%20costs%20of%20training%20and%20deploying%20large%0Avision%20or%20language%20models%20increase%20dramatically%2C%20analog%20in-memory%20computing%0A%28AIMC%29%20emerges%20as%20a%20promising%20energy-efficient%20solution.%20However%2C%20the%20training%0Aperspective%2C%20especially%20its%20training%20dynamic%2C%20is%20underexplored.%20In%20AIMC%0Ahardware%2C%20the%20trainable%20weights%20are%20represented%20by%20the%20conductance%20of%20resistive%0Aelements%20and%20updated%20using%20consecutive%20electrical%20pulses.%20Among%20all%20the%0Aphysical%20properties%20of%20resistive%20elements%2C%20the%20response%20to%20the%20pulses%20directly%0Aaffects%20the%20training%20dynamics.%20This%20paper%20first%20provides%20a%20theoretical%0Afoundation%20for%20gradient-based%20training%20on%20AIMC%20hardware%20and%20studies%20the%20impact%0Aof%20response%20functions.%20We%20demonstrate%20that%20noisy%20update%20and%20asymmetric%20response%0Afunctions%20negatively%20impact%20Analog%20SGD%20by%20imposing%20an%20implicit%20penalty%20term%20on%0Athe%20objective.%20To%20overcome%20the%20issue%2C%20Tiki-Taka%2C%20a%20residual%20learning%20algorithm%2C%0Aconverges%20exactly%20to%20a%20critical%20point%20by%20optimizing%20a%20main%20array%20and%20a%20residual%0Aarray%20bilevelly.%20The%20conclusion%20is%20supported%20by%20simulations%20validating%20our%0Atheoretical%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06309v2&entry.124074799=Read"},
{"title": "Learning Euler Factors of Elliptic Curves", "author": "Angelica Babei and Fran\u00e7ois Charton and Edgar Costa and Xiaoyu Huang and Kyu-Hwan Lee and David Lowry-Duda and Ashvni Narayanan and Alexey Pozdnyakov", "abstract": "  We apply transformer models and feedforward neural networks to predict\nFrobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train\nfurther models to predict $a_p \\bmod 2$ from $a_q \\bmod 2$, and cross-analysis\nsuch as $a_p \\bmod 2$ from $a_q$. Our experiments reveal that these models\nachieve high accuracy, even in the absence of explicit number-theoretic tools\nlike functional equations of $L$-functions. We also present partial\ninterpretability findings.\n", "link": "http://arxiv.org/abs/2502.10357v1", "date": "2025-02-14", "relevancy": 1.313, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4898}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4283}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Euler%20Factors%20of%20Elliptic%20Curves&body=Title%3A%20Learning%20Euler%20Factors%20of%20Elliptic%20Curves%0AAuthor%3A%20Angelica%20Babei%20and%20Fran%C3%A7ois%20Charton%20and%20Edgar%20Costa%20and%20Xiaoyu%20Huang%20and%20Kyu-Hwan%20Lee%20and%20David%20Lowry-Duda%20and%20Ashvni%20Narayanan%20and%20Alexey%20Pozdnyakov%0AAbstract%3A%20%20%20We%20apply%20transformer%20models%20and%20feedforward%20neural%20networks%20to%20predict%0AFrobenius%20traces%20%24a_p%24%20from%20elliptic%20curves%20given%20other%20traces%20%24a_q%24.%20We%20train%0Afurther%20models%20to%20predict%20%24a_p%20%5Cbmod%202%24%20from%20%24a_q%20%5Cbmod%202%24%2C%20and%20cross-analysis%0Asuch%20as%20%24a_p%20%5Cbmod%202%24%20from%20%24a_q%24.%20Our%20experiments%20reveal%20that%20these%20models%0Aachieve%20high%20accuracy%2C%20even%20in%20the%20absence%20of%20explicit%20number-theoretic%20tools%0Alike%20functional%20equations%20of%20%24L%24-functions.%20We%20also%20present%20partial%0Ainterpretability%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Euler%2520Factors%2520of%2520Elliptic%2520Curves%26entry.906535625%3DAngelica%2520Babei%2520and%2520Fran%25C3%25A7ois%2520Charton%2520and%2520Edgar%2520Costa%2520and%2520Xiaoyu%2520Huang%2520and%2520Kyu-Hwan%2520Lee%2520and%2520David%2520Lowry-Duda%2520and%2520Ashvni%2520Narayanan%2520and%2520Alexey%2520Pozdnyakov%26entry.1292438233%3D%2520%2520We%2520apply%2520transformer%2520models%2520and%2520feedforward%2520neural%2520networks%2520to%2520predict%250AFrobenius%2520traces%2520%2524a_p%2524%2520from%2520elliptic%2520curves%2520given%2520other%2520traces%2520%2524a_q%2524.%2520We%2520train%250Afurther%2520models%2520to%2520predict%2520%2524a_p%2520%255Cbmod%25202%2524%2520from%2520%2524a_q%2520%255Cbmod%25202%2524%252C%2520and%2520cross-analysis%250Asuch%2520as%2520%2524a_p%2520%255Cbmod%25202%2524%2520from%2520%2524a_q%2524.%2520Our%2520experiments%2520reveal%2520that%2520these%2520models%250Aachieve%2520high%2520accuracy%252C%2520even%2520in%2520the%2520absence%2520of%2520explicit%2520number-theoretic%2520tools%250Alike%2520functional%2520equations%2520of%2520%2524L%2524-functions.%2520We%2520also%2520present%2520partial%250Ainterpretability%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Euler%20Factors%20of%20Elliptic%20Curves&entry.906535625=Angelica%20Babei%20and%20Fran%C3%A7ois%20Charton%20and%20Edgar%20Costa%20and%20Xiaoyu%20Huang%20and%20Kyu-Hwan%20Lee%20and%20David%20Lowry-Duda%20and%20Ashvni%20Narayanan%20and%20Alexey%20Pozdnyakov&entry.1292438233=%20%20We%20apply%20transformer%20models%20and%20feedforward%20neural%20networks%20to%20predict%0AFrobenius%20traces%20%24a_p%24%20from%20elliptic%20curves%20given%20other%20traces%20%24a_q%24.%20We%20train%0Afurther%20models%20to%20predict%20%24a_p%20%5Cbmod%202%24%20from%20%24a_q%20%5Cbmod%202%24%2C%20and%20cross-analysis%0Asuch%20as%20%24a_p%20%5Cbmod%202%24%20from%20%24a_q%24.%20Our%20experiments%20reveal%20that%20these%20models%0Aachieve%20high%20accuracy%2C%20even%20in%20the%20absence%20of%20explicit%20number-theoretic%20tools%0Alike%20functional%20equations%20of%20%24L%24-functions.%20We%20also%20present%20partial%0Ainterpretability%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10357v1&entry.124074799=Read"},
{"title": "Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via\n  Hierarchical and Parallel Decoding", "author": "Laurin Luttmann and Lin Xie", "abstract": "  The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge\nin warehouse logistics, where pickers must navigate a mixed-shelves environment\nto retrieve SKUs efficiently. Traditional heuristics and optimization-based\napproaches struggle with scalability, while recent machine learning methods\noften rely on sequential decision-making, leading to high solution latency and\nsuboptimal agent coordination. In this work, we propose a novel hierarchical\nand parallel decoding approach for solving the min-max variant of the MSPRP via\nmulti-agent reinforcement learning. While our approach generates a joint\ndistribution over agent actions, allowing for fast decoding and effective\npicker coordination, our method introduces a sequential action selection to\navoid conflicts in the multi-dimensional action space. Experiments show\nstate-of-the-art performance in both solution quality and inference speed,\nparticularly for large-scale and out-of-distribution instances. Our code is\npublicly available at http://github.com/LTluttmann/marl4msprp.\n", "link": "http://arxiv.org/abs/2502.10233v1", "date": "2025-02-14", "relevancy": 1.4822, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5322}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Solve%20the%20Min-Max%20Mixed-Shelves%20Picker-Routing%20Problem%20via%0A%20%20Hierarchical%20and%20Parallel%20Decoding&body=Title%3A%20Learning%20to%20Solve%20the%20Min-Max%20Mixed-Shelves%20Picker-Routing%20Problem%20via%0A%20%20Hierarchical%20and%20Parallel%20Decoding%0AAuthor%3A%20Laurin%20Luttmann%20and%20Lin%20Xie%0AAbstract%3A%20%20%20The%20Mixed-Shelves%20Picker%20Routing%20Problem%20%28MSPRP%29%20is%20a%20fundamental%20challenge%0Ain%20warehouse%20logistics%2C%20where%20pickers%20must%20navigate%20a%20mixed-shelves%20environment%0Ato%20retrieve%20SKUs%20efficiently.%20Traditional%20heuristics%20and%20optimization-based%0Aapproaches%20struggle%20with%20scalability%2C%20while%20recent%20machine%20learning%20methods%0Aoften%20rely%20on%20sequential%20decision-making%2C%20leading%20to%20high%20solution%20latency%20and%0Asuboptimal%20agent%20coordination.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hierarchical%0Aand%20parallel%20decoding%20approach%20for%20solving%20the%20min-max%20variant%20of%20the%20MSPRP%20via%0Amulti-agent%20reinforcement%20learning.%20While%20our%20approach%20generates%20a%20joint%0Adistribution%20over%20agent%20actions%2C%20allowing%20for%20fast%20decoding%20and%20effective%0Apicker%20coordination%2C%20our%20method%20introduces%20a%20sequential%20action%20selection%20to%0Aavoid%20conflicts%20in%20the%20multi-dimensional%20action%20space.%20Experiments%20show%0Astate-of-the-art%20performance%20in%20both%20solution%20quality%20and%20inference%20speed%2C%0Aparticularly%20for%20large-scale%20and%20out-of-distribution%20instances.%20Our%20code%20is%0Apublicly%20available%20at%20http%3A//github.com/LTluttmann/marl4msprp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Solve%2520the%2520Min-Max%2520Mixed-Shelves%2520Picker-Routing%2520Problem%2520via%250A%2520%2520Hierarchical%2520and%2520Parallel%2520Decoding%26entry.906535625%3DLaurin%2520Luttmann%2520and%2520Lin%2520Xie%26entry.1292438233%3D%2520%2520The%2520Mixed-Shelves%2520Picker%2520Routing%2520Problem%2520%2528MSPRP%2529%2520is%2520a%2520fundamental%2520challenge%250Ain%2520warehouse%2520logistics%252C%2520where%2520pickers%2520must%2520navigate%2520a%2520mixed-shelves%2520environment%250Ato%2520retrieve%2520SKUs%2520efficiently.%2520Traditional%2520heuristics%2520and%2520optimization-based%250Aapproaches%2520struggle%2520with%2520scalability%252C%2520while%2520recent%2520machine%2520learning%2520methods%250Aoften%2520rely%2520on%2520sequential%2520decision-making%252C%2520leading%2520to%2520high%2520solution%2520latency%2520and%250Asuboptimal%2520agent%2520coordination.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%250Aand%2520parallel%2520decoding%2520approach%2520for%2520solving%2520the%2520min-max%2520variant%2520of%2520the%2520MSPRP%2520via%250Amulti-agent%2520reinforcement%2520learning.%2520While%2520our%2520approach%2520generates%2520a%2520joint%250Adistribution%2520over%2520agent%2520actions%252C%2520allowing%2520for%2520fast%2520decoding%2520and%2520effective%250Apicker%2520coordination%252C%2520our%2520method%2520introduces%2520a%2520sequential%2520action%2520selection%2520to%250Aavoid%2520conflicts%2520in%2520the%2520multi-dimensional%2520action%2520space.%2520Experiments%2520show%250Astate-of-the-art%2520performance%2520in%2520both%2520solution%2520quality%2520and%2520inference%2520speed%252C%250Aparticularly%2520for%2520large-scale%2520and%2520out-of-distribution%2520instances.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%2520http%253A//github.com/LTluttmann/marl4msprp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Solve%20the%20Min-Max%20Mixed-Shelves%20Picker-Routing%20Problem%20via%0A%20%20Hierarchical%20and%20Parallel%20Decoding&entry.906535625=Laurin%20Luttmann%20and%20Lin%20Xie&entry.1292438233=%20%20The%20Mixed-Shelves%20Picker%20Routing%20Problem%20%28MSPRP%29%20is%20a%20fundamental%20challenge%0Ain%20warehouse%20logistics%2C%20where%20pickers%20must%20navigate%20a%20mixed-shelves%20environment%0Ato%20retrieve%20SKUs%20efficiently.%20Traditional%20heuristics%20and%20optimization-based%0Aapproaches%20struggle%20with%20scalability%2C%20while%20recent%20machine%20learning%20methods%0Aoften%20rely%20on%20sequential%20decision-making%2C%20leading%20to%20high%20solution%20latency%20and%0Asuboptimal%20agent%20coordination.%20In%20this%20work%2C%20we%20propose%20a%20novel%20hierarchical%0Aand%20parallel%20decoding%20approach%20for%20solving%20the%20min-max%20variant%20of%20the%20MSPRP%20via%0Amulti-agent%20reinforcement%20learning.%20While%20our%20approach%20generates%20a%20joint%0Adistribution%20over%20agent%20actions%2C%20allowing%20for%20fast%20decoding%20and%20effective%0Apicker%20coordination%2C%20our%20method%20introduces%20a%20sequential%20action%20selection%20to%0Aavoid%20conflicts%20in%20the%20multi-dimensional%20action%20space.%20Experiments%20show%0Astate-of-the-art%20performance%20in%20both%20solution%20quality%20and%20inference%20speed%2C%0Aparticularly%20for%20large-scale%20and%20out-of-distribution%20instances.%20Our%20code%20is%0Apublicly%20available%20at%20http%3A//github.com/LTluttmann/marl4msprp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10233v1&entry.124074799=Read"},
{"title": "Adversarial Mixup Unlearning", "author": "Zhuoyi Peng and Yixuan Tang and Yi Yang", "abstract": "  Machine unlearning is a critical area of research aimed at safeguarding data\nprivacy by enabling the removal of sensitive information from machine learning\nmodels. One unique challenge in this field is catastrophic unlearning, where\nerasing specific data from a well-trained model unintentionally removes\nessential knowledge, causing the model to deviate significantly from a\nretrained one. To address this, we introduce a novel approach that regularizes\nthe unlearning process by utilizing synthesized mixup samples, which simulate\nthe data susceptible to catastrophic effects. At the core of our approach is a\ngenerator-unlearner framework, MixUnlearn, where a generator adversarially\nproduces challenging mixup examples, and the unlearner effectively forgets\ntarget information based on these synthesized data. Specifically, we first\nintroduce a novel contrastive objective to train the generator in an\nadversarial direction: generating examples that prompt the unlearner to reveal\ninformation that should be forgotten, while losing essential knowledge. Then\nthe unlearner, guided by two other contrastive loss terms, processes the\nsynthesized and real data jointly to ensure accurate unlearning without losing\ncritical knowledge, overcoming catastrophic effects. Extensive evaluations\nacross benchmark datasets demonstrate that our method significantly outperforms\nstate-of-the-art approaches, offering a robust solution to machine unlearning.\nThis work not only deepens understanding of unlearning mechanisms but also lays\nthe foundation for effective machine unlearning with mixup augmentation.\n", "link": "http://arxiv.org/abs/2502.10288v1", "date": "2025-02-14", "relevancy": 1.4861, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5154}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Mixup%20Unlearning&body=Title%3A%20Adversarial%20Mixup%20Unlearning%0AAuthor%3A%20Zhuoyi%20Peng%20and%20Yixuan%20Tang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Machine%20unlearning%20is%20a%20critical%20area%20of%20research%20aimed%20at%20safeguarding%20data%0Aprivacy%20by%20enabling%20the%20removal%20of%20sensitive%20information%20from%20machine%20learning%0Amodels.%20One%20unique%20challenge%20in%20this%20field%20is%20catastrophic%20unlearning%2C%20where%0Aerasing%20specific%20data%20from%20a%20well-trained%20model%20unintentionally%20removes%0Aessential%20knowledge%2C%20causing%20the%20model%20to%20deviate%20significantly%20from%20a%0Aretrained%20one.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20approach%20that%20regularizes%0Athe%20unlearning%20process%20by%20utilizing%20synthesized%20mixup%20samples%2C%20which%20simulate%0Athe%20data%20susceptible%20to%20catastrophic%20effects.%20At%20the%20core%20of%20our%20approach%20is%20a%0Agenerator-unlearner%20framework%2C%20MixUnlearn%2C%20where%20a%20generator%20adversarially%0Aproduces%20challenging%20mixup%20examples%2C%20and%20the%20unlearner%20effectively%20forgets%0Atarget%20information%20based%20on%20these%20synthesized%20data.%20Specifically%2C%20we%20first%0Aintroduce%20a%20novel%20contrastive%20objective%20to%20train%20the%20generator%20in%20an%0Aadversarial%20direction%3A%20generating%20examples%20that%20prompt%20the%20unlearner%20to%20reveal%0Ainformation%20that%20should%20be%20forgotten%2C%20while%20losing%20essential%20knowledge.%20Then%0Athe%20unlearner%2C%20guided%20by%20two%20other%20contrastive%20loss%20terms%2C%20processes%20the%0Asynthesized%20and%20real%20data%20jointly%20to%20ensure%20accurate%20unlearning%20without%20losing%0Acritical%20knowledge%2C%20overcoming%20catastrophic%20effects.%20Extensive%20evaluations%0Aacross%20benchmark%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20approaches%2C%20offering%20a%20robust%20solution%20to%20machine%20unlearning.%0AThis%20work%20not%20only%20deepens%20understanding%20of%20unlearning%20mechanisms%20but%20also%20lays%0Athe%20foundation%20for%20effective%20machine%20unlearning%20with%20mixup%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Mixup%2520Unlearning%26entry.906535625%3DZhuoyi%2520Peng%2520and%2520Yixuan%2520Tang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520is%2520a%2520critical%2520area%2520of%2520research%2520aimed%2520at%2520safeguarding%2520data%250Aprivacy%2520by%2520enabling%2520the%2520removal%2520of%2520sensitive%2520information%2520from%2520machine%2520learning%250Amodels.%2520One%2520unique%2520challenge%2520in%2520this%2520field%2520is%2520catastrophic%2520unlearning%252C%2520where%250Aerasing%2520specific%2520data%2520from%2520a%2520well-trained%2520model%2520unintentionally%2520removes%250Aessential%2520knowledge%252C%2520causing%2520the%2520model%2520to%2520deviate%2520significantly%2520from%2520a%250Aretrained%2520one.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520that%2520regularizes%250Athe%2520unlearning%2520process%2520by%2520utilizing%2520synthesized%2520mixup%2520samples%252C%2520which%2520simulate%250Athe%2520data%2520susceptible%2520to%2520catastrophic%2520effects.%2520At%2520the%2520core%2520of%2520our%2520approach%2520is%2520a%250Agenerator-unlearner%2520framework%252C%2520MixUnlearn%252C%2520where%2520a%2520generator%2520adversarially%250Aproduces%2520challenging%2520mixup%2520examples%252C%2520and%2520the%2520unlearner%2520effectively%2520forgets%250Atarget%2520information%2520based%2520on%2520these%2520synthesized%2520data.%2520Specifically%252C%2520we%2520first%250Aintroduce%2520a%2520novel%2520contrastive%2520objective%2520to%2520train%2520the%2520generator%2520in%2520an%250Aadversarial%2520direction%253A%2520generating%2520examples%2520that%2520prompt%2520the%2520unlearner%2520to%2520reveal%250Ainformation%2520that%2520should%2520be%2520forgotten%252C%2520while%2520losing%2520essential%2520knowledge.%2520Then%250Athe%2520unlearner%252C%2520guided%2520by%2520two%2520other%2520contrastive%2520loss%2520terms%252C%2520processes%2520the%250Asynthesized%2520and%2520real%2520data%2520jointly%2520to%2520ensure%2520accurate%2520unlearning%2520without%2520losing%250Acritical%2520knowledge%252C%2520overcoming%2520catastrophic%2520effects.%2520Extensive%2520evaluations%250Aacross%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%250Astate-of-the-art%2520approaches%252C%2520offering%2520a%2520robust%2520solution%2520to%2520machine%2520unlearning.%250AThis%2520work%2520not%2520only%2520deepens%2520understanding%2520of%2520unlearning%2520mechanisms%2520but%2520also%2520lays%250Athe%2520foundation%2520for%2520effective%2520machine%2520unlearning%2520with%2520mixup%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Mixup%20Unlearning&entry.906535625=Zhuoyi%20Peng%20and%20Yixuan%20Tang%20and%20Yi%20Yang&entry.1292438233=%20%20Machine%20unlearning%20is%20a%20critical%20area%20of%20research%20aimed%20at%20safeguarding%20data%0Aprivacy%20by%20enabling%20the%20removal%20of%20sensitive%20information%20from%20machine%20learning%0Amodels.%20One%20unique%20challenge%20in%20this%20field%20is%20catastrophic%20unlearning%2C%20where%0Aerasing%20specific%20data%20from%20a%20well-trained%20model%20unintentionally%20removes%0Aessential%20knowledge%2C%20causing%20the%20model%20to%20deviate%20significantly%20from%20a%0Aretrained%20one.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20approach%20that%20regularizes%0Athe%20unlearning%20process%20by%20utilizing%20synthesized%20mixup%20samples%2C%20which%20simulate%0Athe%20data%20susceptible%20to%20catastrophic%20effects.%20At%20the%20core%20of%20our%20approach%20is%20a%0Agenerator-unlearner%20framework%2C%20MixUnlearn%2C%20where%20a%20generator%20adversarially%0Aproduces%20challenging%20mixup%20examples%2C%20and%20the%20unlearner%20effectively%20forgets%0Atarget%20information%20based%20on%20these%20synthesized%20data.%20Specifically%2C%20we%20first%0Aintroduce%20a%20novel%20contrastive%20objective%20to%20train%20the%20generator%20in%20an%0Aadversarial%20direction%3A%20generating%20examples%20that%20prompt%20the%20unlearner%20to%20reveal%0Ainformation%20that%20should%20be%20forgotten%2C%20while%20losing%20essential%20knowledge.%20Then%0Athe%20unlearner%2C%20guided%20by%20two%20other%20contrastive%20loss%20terms%2C%20processes%20the%0Asynthesized%20and%20real%20data%20jointly%20to%20ensure%20accurate%20unlearning%20without%20losing%0Acritical%20knowledge%2C%20overcoming%20catastrophic%20effects.%20Extensive%20evaluations%0Aacross%20benchmark%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%0Astate-of-the-art%20approaches%2C%20offering%20a%20robust%20solution%20to%20machine%20unlearning.%0AThis%20work%20not%20only%20deepens%20understanding%20of%20unlearning%20mechanisms%20but%20also%20lays%0Athe%20foundation%20for%20effective%20machine%20unlearning%20with%20mixup%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10288v1&entry.124074799=Read"},
{"title": "Strassen Multisystolic Array Hardware Architectures", "author": "Trevor E. Pogue and Nicola Nicolici", "abstract": "  While Strassen's matrix multiplication algorithm reduces the complexity of\nnaive matrix multiplication, general-purpose hardware is not suitable for\nachieving the algorithm's promised theoretical speedups. This leaves the\nquestion of if it could be better exploited in custom hardware architectures\ndesigned specifically for executing the algorithm. However, there is limited\nprior work on this and it is not immediately clear how to derive such\narchitectures or if they can ultimately lead to real improvements. We bridge\nthis gap, presenting and evaluating new systolic array architectures that\nefficiently translate the theoretical complexity reductions of Strassen's\nalgorithm directly into hardware resource savings. Furthermore, the\narchitectures are multisystolic array designs that can multiply smaller\nmatrices with higher utilization than single-systolic array designs. The\nproposed designs implemented on FPGA reduce DSP requirements by a factor of\n$1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require\noverall similar soft logic resources when instantiated to support matrix sizes\ndown to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We\nevaluate the proposed designs both in isolation and in an end-to-end machine\nlearning accelerator compared to baseline designs and prior works, achieving\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2502.10063v1", "date": "2025-02-14", "relevancy": 1.7745, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4812}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Strassen%20Multisystolic%20Array%20Hardware%20Architectures&body=Title%3A%20Strassen%20Multisystolic%20Array%20Hardware%20Architectures%0AAuthor%3A%20Trevor%20E.%20Pogue%20and%20Nicola%20Nicolici%0AAbstract%3A%20%20%20While%20Strassen%27s%20matrix%20multiplication%20algorithm%20reduces%20the%20complexity%20of%0Anaive%20matrix%20multiplication%2C%20general-purpose%20hardware%20is%20not%20suitable%20for%0Aachieving%20the%20algorithm%27s%20promised%20theoretical%20speedups.%20This%20leaves%20the%0Aquestion%20of%20if%20it%20could%20be%20better%20exploited%20in%20custom%20hardware%20architectures%0Adesigned%20specifically%20for%20executing%20the%20algorithm.%20However%2C%20there%20is%20limited%0Aprior%20work%20on%20this%20and%20it%20is%20not%20immediately%20clear%20how%20to%20derive%20such%0Aarchitectures%20or%20if%20they%20can%20ultimately%20lead%20to%20real%20improvements.%20We%20bridge%0Athis%20gap%2C%20presenting%20and%20evaluating%20new%20systolic%20array%20architectures%20that%0Aefficiently%20translate%20the%20theoretical%20complexity%20reductions%20of%20Strassen%27s%0Aalgorithm%20directly%20into%20hardware%20resource%20savings.%20Furthermore%2C%20the%0Aarchitectures%20are%20multisystolic%20array%20designs%20that%20can%20multiply%20smaller%0Amatrices%20with%20higher%20utilization%20than%20single-systolic%20array%20designs.%20The%0Aproposed%20designs%20implemented%20on%20FPGA%20reduce%20DSP%20requirements%20by%20a%20factor%20of%0A%241.14%5Er%24%20for%20%24r%24%20implemented%20Strassen%20recursion%20levels%2C%20and%20otherwise%20require%0Aoverall%20similar%20soft%20logic%20resources%20when%20instantiated%20to%20support%20matrix%20sizes%0Adown%20to%2032x32%20and%2024x24%20at%201-2%20levels%20of%20Strassen%20recursion%2C%20respectively.%20We%0Aevaluate%20the%20proposed%20designs%20both%20in%20isolation%20and%20in%20an%20end-to-end%20machine%0Alearning%20accelerator%20compared%20to%20baseline%20designs%20and%20prior%20works%2C%20achieving%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStrassen%2520Multisystolic%2520Array%2520Hardware%2520Architectures%26entry.906535625%3DTrevor%2520E.%2520Pogue%2520and%2520Nicola%2520Nicolici%26entry.1292438233%3D%2520%2520While%2520Strassen%2527s%2520matrix%2520multiplication%2520algorithm%2520reduces%2520the%2520complexity%2520of%250Anaive%2520matrix%2520multiplication%252C%2520general-purpose%2520hardware%2520is%2520not%2520suitable%2520for%250Aachieving%2520the%2520algorithm%2527s%2520promised%2520theoretical%2520speedups.%2520This%2520leaves%2520the%250Aquestion%2520of%2520if%2520it%2520could%2520be%2520better%2520exploited%2520in%2520custom%2520hardware%2520architectures%250Adesigned%2520specifically%2520for%2520executing%2520the%2520algorithm.%2520However%252C%2520there%2520is%2520limited%250Aprior%2520work%2520on%2520this%2520and%2520it%2520is%2520not%2520immediately%2520clear%2520how%2520to%2520derive%2520such%250Aarchitectures%2520or%2520if%2520they%2520can%2520ultimately%2520lead%2520to%2520real%2520improvements.%2520We%2520bridge%250Athis%2520gap%252C%2520presenting%2520and%2520evaluating%2520new%2520systolic%2520array%2520architectures%2520that%250Aefficiently%2520translate%2520the%2520theoretical%2520complexity%2520reductions%2520of%2520Strassen%2527s%250Aalgorithm%2520directly%2520into%2520hardware%2520resource%2520savings.%2520Furthermore%252C%2520the%250Aarchitectures%2520are%2520multisystolic%2520array%2520designs%2520that%2520can%2520multiply%2520smaller%250Amatrices%2520with%2520higher%2520utilization%2520than%2520single-systolic%2520array%2520designs.%2520The%250Aproposed%2520designs%2520implemented%2520on%2520FPGA%2520reduce%2520DSP%2520requirements%2520by%2520a%2520factor%2520of%250A%25241.14%255Er%2524%2520for%2520%2524r%2524%2520implemented%2520Strassen%2520recursion%2520levels%252C%2520and%2520otherwise%2520require%250Aoverall%2520similar%2520soft%2520logic%2520resources%2520when%2520instantiated%2520to%2520support%2520matrix%2520sizes%250Adown%2520to%252032x32%2520and%252024x24%2520at%25201-2%2520levels%2520of%2520Strassen%2520recursion%252C%2520respectively.%2520We%250Aevaluate%2520the%2520proposed%2520designs%2520both%2520in%2520isolation%2520and%2520in%2520an%2520end-to-end%2520machine%250Alearning%2520accelerator%2520compared%2520to%2520baseline%2520designs%2520and%2520prior%2520works%252C%2520achieving%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Strassen%20Multisystolic%20Array%20Hardware%20Architectures&entry.906535625=Trevor%20E.%20Pogue%20and%20Nicola%20Nicolici&entry.1292438233=%20%20While%20Strassen%27s%20matrix%20multiplication%20algorithm%20reduces%20the%20complexity%20of%0Anaive%20matrix%20multiplication%2C%20general-purpose%20hardware%20is%20not%20suitable%20for%0Aachieving%20the%20algorithm%27s%20promised%20theoretical%20speedups.%20This%20leaves%20the%0Aquestion%20of%20if%20it%20could%20be%20better%20exploited%20in%20custom%20hardware%20architectures%0Adesigned%20specifically%20for%20executing%20the%20algorithm.%20However%2C%20there%20is%20limited%0Aprior%20work%20on%20this%20and%20it%20is%20not%20immediately%20clear%20how%20to%20derive%20such%0Aarchitectures%20or%20if%20they%20can%20ultimately%20lead%20to%20real%20improvements.%20We%20bridge%0Athis%20gap%2C%20presenting%20and%20evaluating%20new%20systolic%20array%20architectures%20that%0Aefficiently%20translate%20the%20theoretical%20complexity%20reductions%20of%20Strassen%27s%0Aalgorithm%20directly%20into%20hardware%20resource%20savings.%20Furthermore%2C%20the%0Aarchitectures%20are%20multisystolic%20array%20designs%20that%20can%20multiply%20smaller%0Amatrices%20with%20higher%20utilization%20than%20single-systolic%20array%20designs.%20The%0Aproposed%20designs%20implemented%20on%20FPGA%20reduce%20DSP%20requirements%20by%20a%20factor%20of%0A%241.14%5Er%24%20for%20%24r%24%20implemented%20Strassen%20recursion%20levels%2C%20and%20otherwise%20require%0Aoverall%20similar%20soft%20logic%20resources%20when%20instantiated%20to%20support%20matrix%20sizes%0Adown%20to%2032x32%20and%2024x24%20at%201-2%20levels%20of%20Strassen%20recursion%2C%20respectively.%20We%0Aevaluate%20the%20proposed%20designs%20both%20in%20isolation%20and%20in%20an%20end-to-end%20machine%0Alearning%20accelerator%20compared%20to%20baseline%20designs%20and%20prior%20works%2C%20achieving%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10063v1&entry.124074799=Read"},
{"title": "Integrated Multi-Simulation Environments for Aerial Robotics Research", "author": "Pascal Goldschmid and Aamir Ahmad", "abstract": "  Simulation frameworks play a pivotal role in the safe development of robotic\napplications. However, often different components of an envisioned robotic\nsystem are best simulated in different environments/simulators. This poses a\nsignificant challenge in simulating the entire project into a single integrated\nrobotic framework. Specifically, for partially-open or closed-source\nsimulators, often two core limitations arise. i) Actors in the scene other than\nthe designated robots cannot be controlled during runtime via interfaces such\nas ROS and ii) retrieving real-time state information (such as pose, velocity\netc.) of objects in the scene is prevented. In this work, we address these\nlimitations and describe our solution for the use case of integrating aerial\ndrones simulated by the powerful simulator Sphinx (provided by Parrot Drone)\ninto the Gazebo simulator. We achieve this by means of a mirrored instance of a\ndrone that is included into existing Gazebo-based environments. A promising\napplication of our integrated simulation environment is the task of target\ntracking that is common in aerial multi-robot scenarios. Therefore, to\ndemonstrate the effectiveness our our integrated simulation, we also implement\na model predictive controller (MPC) that outperforms the default PID-based\ncontroller framework provided with the Parrot's popular Anafi drone in various\ndynamic tracking scenarios thus enhancing the utility of the overall system. We\ntest our solution by including the Anafi drone in an existing Gazebo-based\nsimulation and evaluate the performance of the MPC through rigorous testing in\nsimulated and real-world tracking experiments against a customized PID\ncontroller baseline. Source code is published on\nhttps://github.com/robot-perception-group/anafi_sim.\n", "link": "http://arxiv.org/abs/2502.10218v1", "date": "2025-02-14", "relevancy": 1.0788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.546}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Multi-Simulation%20Environments%20for%20Aerial%20Robotics%20Research&body=Title%3A%20Integrated%20Multi-Simulation%20Environments%20for%20Aerial%20Robotics%20Research%0AAuthor%3A%20Pascal%20Goldschmid%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Simulation%20frameworks%20play%20a%20pivotal%20role%20in%20the%20safe%20development%20of%20robotic%0Aapplications.%20However%2C%20often%20different%20components%20of%20an%20envisioned%20robotic%0Asystem%20are%20best%20simulated%20in%20different%20environments/simulators.%20This%20poses%20a%0Asignificant%20challenge%20in%20simulating%20the%20entire%20project%20into%20a%20single%20integrated%0Arobotic%20framework.%20Specifically%2C%20for%20partially-open%20or%20closed-source%0Asimulators%2C%20often%20two%20core%20limitations%20arise.%20i%29%20Actors%20in%20the%20scene%20other%20than%0Athe%20designated%20robots%20cannot%20be%20controlled%20during%20runtime%20via%20interfaces%20such%0Aas%20ROS%20and%20ii%29%20retrieving%20real-time%20state%20information%20%28such%20as%20pose%2C%20velocity%0Aetc.%29%20of%20objects%20in%20the%20scene%20is%20prevented.%20In%20this%20work%2C%20we%20address%20these%0Alimitations%20and%20describe%20our%20solution%20for%20the%20use%20case%20of%20integrating%20aerial%0Adrones%20simulated%20by%20the%20powerful%20simulator%20Sphinx%20%28provided%20by%20Parrot%20Drone%29%0Ainto%20the%20Gazebo%20simulator.%20We%20achieve%20this%20by%20means%20of%20a%20mirrored%20instance%20of%20a%0Adrone%20that%20is%20included%20into%20existing%20Gazebo-based%20environments.%20A%20promising%0Aapplication%20of%20our%20integrated%20simulation%20environment%20is%20the%20task%20of%20target%0Atracking%20that%20is%20common%20in%20aerial%20multi-robot%20scenarios.%20Therefore%2C%20to%0Ademonstrate%20the%20effectiveness%20our%20our%20integrated%20simulation%2C%20we%20also%20implement%0Aa%20model%20predictive%20controller%20%28MPC%29%20that%20outperforms%20the%20default%20PID-based%0Acontroller%20framework%20provided%20with%20the%20Parrot%27s%20popular%20Anafi%20drone%20in%20various%0Adynamic%20tracking%20scenarios%20thus%20enhancing%20the%20utility%20of%20the%20overall%20system.%20We%0Atest%20our%20solution%20by%20including%20the%20Anafi%20drone%20in%20an%20existing%20Gazebo-based%0Asimulation%20and%20evaluate%20the%20performance%20of%20the%20MPC%20through%20rigorous%20testing%20in%0Asimulated%20and%20real-world%20tracking%20experiments%20against%20a%20customized%20PID%0Acontroller%20baseline.%20Source%20code%20is%20published%20on%0Ahttps%3A//github.com/robot-perception-group/anafi_sim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Multi-Simulation%2520Environments%2520for%2520Aerial%2520Robotics%2520Research%26entry.906535625%3DPascal%2520Goldschmid%2520and%2520Aamir%2520Ahmad%26entry.1292438233%3D%2520%2520Simulation%2520frameworks%2520play%2520a%2520pivotal%2520role%2520in%2520the%2520safe%2520development%2520of%2520robotic%250Aapplications.%2520However%252C%2520often%2520different%2520components%2520of%2520an%2520envisioned%2520robotic%250Asystem%2520are%2520best%2520simulated%2520in%2520different%2520environments/simulators.%2520This%2520poses%2520a%250Asignificant%2520challenge%2520in%2520simulating%2520the%2520entire%2520project%2520into%2520a%2520single%2520integrated%250Arobotic%2520framework.%2520Specifically%252C%2520for%2520partially-open%2520or%2520closed-source%250Asimulators%252C%2520often%2520two%2520core%2520limitations%2520arise.%2520i%2529%2520Actors%2520in%2520the%2520scene%2520other%2520than%250Athe%2520designated%2520robots%2520cannot%2520be%2520controlled%2520during%2520runtime%2520via%2520interfaces%2520such%250Aas%2520ROS%2520and%2520ii%2529%2520retrieving%2520real-time%2520state%2520information%2520%2528such%2520as%2520pose%252C%2520velocity%250Aetc.%2529%2520of%2520objects%2520in%2520the%2520scene%2520is%2520prevented.%2520In%2520this%2520work%252C%2520we%2520address%2520these%250Alimitations%2520and%2520describe%2520our%2520solution%2520for%2520the%2520use%2520case%2520of%2520integrating%2520aerial%250Adrones%2520simulated%2520by%2520the%2520powerful%2520simulator%2520Sphinx%2520%2528provided%2520by%2520Parrot%2520Drone%2529%250Ainto%2520the%2520Gazebo%2520simulator.%2520We%2520achieve%2520this%2520by%2520means%2520of%2520a%2520mirrored%2520instance%2520of%2520a%250Adrone%2520that%2520is%2520included%2520into%2520existing%2520Gazebo-based%2520environments.%2520A%2520promising%250Aapplication%2520of%2520our%2520integrated%2520simulation%2520environment%2520is%2520the%2520task%2520of%2520target%250Atracking%2520that%2520is%2520common%2520in%2520aerial%2520multi-robot%2520scenarios.%2520Therefore%252C%2520to%250Ademonstrate%2520the%2520effectiveness%2520our%2520our%2520integrated%2520simulation%252C%2520we%2520also%2520implement%250Aa%2520model%2520predictive%2520controller%2520%2528MPC%2529%2520that%2520outperforms%2520the%2520default%2520PID-based%250Acontroller%2520framework%2520provided%2520with%2520the%2520Parrot%2527s%2520popular%2520Anafi%2520drone%2520in%2520various%250Adynamic%2520tracking%2520scenarios%2520thus%2520enhancing%2520the%2520utility%2520of%2520the%2520overall%2520system.%2520We%250Atest%2520our%2520solution%2520by%2520including%2520the%2520Anafi%2520drone%2520in%2520an%2520existing%2520Gazebo-based%250Asimulation%2520and%2520evaluate%2520the%2520performance%2520of%2520the%2520MPC%2520through%2520rigorous%2520testing%2520in%250Asimulated%2520and%2520real-world%2520tracking%2520experiments%2520against%2520a%2520customized%2520PID%250Acontroller%2520baseline.%2520Source%2520code%2520is%2520published%2520on%250Ahttps%253A//github.com/robot-perception-group/anafi_sim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Multi-Simulation%20Environments%20for%20Aerial%20Robotics%20Research&entry.906535625=Pascal%20Goldschmid%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Simulation%20frameworks%20play%20a%20pivotal%20role%20in%20the%20safe%20development%20of%20robotic%0Aapplications.%20However%2C%20often%20different%20components%20of%20an%20envisioned%20robotic%0Asystem%20are%20best%20simulated%20in%20different%20environments/simulators.%20This%20poses%20a%0Asignificant%20challenge%20in%20simulating%20the%20entire%20project%20into%20a%20single%20integrated%0Arobotic%20framework.%20Specifically%2C%20for%20partially-open%20or%20closed-source%0Asimulators%2C%20often%20two%20core%20limitations%20arise.%20i%29%20Actors%20in%20the%20scene%20other%20than%0Athe%20designated%20robots%20cannot%20be%20controlled%20during%20runtime%20via%20interfaces%20such%0Aas%20ROS%20and%20ii%29%20retrieving%20real-time%20state%20information%20%28such%20as%20pose%2C%20velocity%0Aetc.%29%20of%20objects%20in%20the%20scene%20is%20prevented.%20In%20this%20work%2C%20we%20address%20these%0Alimitations%20and%20describe%20our%20solution%20for%20the%20use%20case%20of%20integrating%20aerial%0Adrones%20simulated%20by%20the%20powerful%20simulator%20Sphinx%20%28provided%20by%20Parrot%20Drone%29%0Ainto%20the%20Gazebo%20simulator.%20We%20achieve%20this%20by%20means%20of%20a%20mirrored%20instance%20of%20a%0Adrone%20that%20is%20included%20into%20existing%20Gazebo-based%20environments.%20A%20promising%0Aapplication%20of%20our%20integrated%20simulation%20environment%20is%20the%20task%20of%20target%0Atracking%20that%20is%20common%20in%20aerial%20multi-robot%20scenarios.%20Therefore%2C%20to%0Ademonstrate%20the%20effectiveness%20our%20our%20integrated%20simulation%2C%20we%20also%20implement%0Aa%20model%20predictive%20controller%20%28MPC%29%20that%20outperforms%20the%20default%20PID-based%0Acontroller%20framework%20provided%20with%20the%20Parrot%27s%20popular%20Anafi%20drone%20in%20various%0Adynamic%20tracking%20scenarios%20thus%20enhancing%20the%20utility%20of%20the%20overall%20system.%20We%0Atest%20our%20solution%20by%20including%20the%20Anafi%20drone%20in%20an%20existing%20Gazebo-based%0Asimulation%20and%20evaluate%20the%20performance%20of%20the%20MPC%20through%20rigorous%20testing%20in%0Asimulated%20and%20real-world%20tracking%20experiments%20against%20a%20customized%20PID%0Acontroller%20baseline.%20Source%20code%20is%20published%20on%0Ahttps%3A//github.com/robot-perception-group/anafi_sim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10218v1&entry.124074799=Read"},
{"title": "A Survey on LLM-powered Agents for Recommender Systems", "author": "Qiyao Peng and Hongtao Liu and Hua Huang and Qing Yang and Minglai Shao", "abstract": "  Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field.\n", "link": "http://arxiv.org/abs/2502.10050v1", "date": "2025-02-14", "relevancy": 1.8582, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20LLM-powered%20Agents%20for%20Recommender%20Systems&body=Title%3A%20A%20Survey%20on%20LLM-powered%20Agents%20for%20Recommender%20Systems%0AAuthor%3A%20Qiyao%20Peng%20and%20Hongtao%20Liu%20and%20Hua%20Huang%20and%20Qing%20Yang%20and%20Minglai%20Shao%0AAbstract%3A%20%20%20Recommender%20systems%20are%20essential%20components%20of%20many%20online%20platforms%2C%20yet%0Atraditional%20approaches%20still%20struggle%20with%20understanding%20complex%20user%0Apreferences%20and%20providing%20explainable%20recommendations.%20The%20emergence%20of%20Large%0ALanguage%20Model%20%28LLM%29-powered%20agents%20offers%20a%20promising%20approach%20by%20enabling%0Anatural%20language%20interactions%20and%20interpretable%20reasoning%2C%20potentially%0Atransforming%20research%20in%20recommender%20systems.%20This%20survey%20provides%20a%20systematic%0Areview%20of%20the%20emerging%20applications%20of%20LLM-powered%20agents%20in%20recommender%0Asystems.%20We%20identify%20and%20analyze%20three%20key%20paradigms%20in%20current%20research%3A%20%281%29%0ARecommender-oriented%20approaches%2C%20which%20leverage%20intelligent%20agents%20to%20enhance%0Athe%20fundamental%20recommendation%20mechanisms%3B%20%282%29%20Interaction-oriented%20approaches%2C%0Awhich%20facilitate%20dynamic%20user%20engagement%20through%20natural%20dialogue%20and%0Ainterpretable%20suggestions%3B%20and%20%283%29%20Simulation-oriented%20approaches%2C%20which%20employ%0Amulti-agent%20frameworks%20to%20model%20complex%20user-item%20interactions%20and%20system%0Adynamics.%20Beyond%20paradigm%20categorization%2C%20we%20analyze%20the%20architectural%0Afoundations%20of%20LLM-powered%20recommendation%20agents%2C%20examining%20their%20essential%0Acomponents%3A%20profile%20construction%2C%20memory%20management%2C%20strategic%20planning%2C%20and%0Aaction%20execution.%20Our%20investigation%20extends%20to%20a%20comprehensive%20analysis%20of%0Abenchmark%20datasets%20and%20evaluation%20frameworks%20in%20this%20domain.%20This%20systematic%0Aexamination%20not%20only%20illuminates%20the%20current%20state%20of%20LLM-powered%20agent%0Arecommender%20systems%20but%20also%20charts%20critical%20challenges%20and%20promising%20research%0Adirections%20in%20this%20transformative%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520LLM-powered%2520Agents%2520for%2520Recommender%2520Systems%26entry.906535625%3DQiyao%2520Peng%2520and%2520Hongtao%2520Liu%2520and%2520Hua%2520Huang%2520and%2520Qing%2520Yang%2520and%2520Minglai%2520Shao%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520essential%2520components%2520of%2520many%2520online%2520platforms%252C%2520yet%250Atraditional%2520approaches%2520still%2520struggle%2520with%2520understanding%2520complex%2520user%250Apreferences%2520and%2520providing%2520explainable%2520recommendations.%2520The%2520emergence%2520of%2520Large%250ALanguage%2520Model%2520%2528LLM%2529-powered%2520agents%2520offers%2520a%2520promising%2520approach%2520by%2520enabling%250Anatural%2520language%2520interactions%2520and%2520interpretable%2520reasoning%252C%2520potentially%250Atransforming%2520research%2520in%2520recommender%2520systems.%2520This%2520survey%2520provides%2520a%2520systematic%250Areview%2520of%2520the%2520emerging%2520applications%2520of%2520LLM-powered%2520agents%2520in%2520recommender%250Asystems.%2520We%2520identify%2520and%2520analyze%2520three%2520key%2520paradigms%2520in%2520current%2520research%253A%2520%25281%2529%250ARecommender-oriented%2520approaches%252C%2520which%2520leverage%2520intelligent%2520agents%2520to%2520enhance%250Athe%2520fundamental%2520recommendation%2520mechanisms%253B%2520%25282%2529%2520Interaction-oriented%2520approaches%252C%250Awhich%2520facilitate%2520dynamic%2520user%2520engagement%2520through%2520natural%2520dialogue%2520and%250Ainterpretable%2520suggestions%253B%2520and%2520%25283%2529%2520Simulation-oriented%2520approaches%252C%2520which%2520employ%250Amulti-agent%2520frameworks%2520to%2520model%2520complex%2520user-item%2520interactions%2520and%2520system%250Adynamics.%2520Beyond%2520paradigm%2520categorization%252C%2520we%2520analyze%2520the%2520architectural%250Afoundations%2520of%2520LLM-powered%2520recommendation%2520agents%252C%2520examining%2520their%2520essential%250Acomponents%253A%2520profile%2520construction%252C%2520memory%2520management%252C%2520strategic%2520planning%252C%2520and%250Aaction%2520execution.%2520Our%2520investigation%2520extends%2520to%2520a%2520comprehensive%2520analysis%2520of%250Abenchmark%2520datasets%2520and%2520evaluation%2520frameworks%2520in%2520this%2520domain.%2520This%2520systematic%250Aexamination%2520not%2520only%2520illuminates%2520the%2520current%2520state%2520of%2520LLM-powered%2520agent%250Arecommender%2520systems%2520but%2520also%2520charts%2520critical%2520challenges%2520and%2520promising%2520research%250Adirections%2520in%2520this%2520transformative%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20LLM-powered%20Agents%20for%20Recommender%20Systems&entry.906535625=Qiyao%20Peng%20and%20Hongtao%20Liu%20and%20Hua%20Huang%20and%20Qing%20Yang%20and%20Minglai%20Shao&entry.1292438233=%20%20Recommender%20systems%20are%20essential%20components%20of%20many%20online%20platforms%2C%20yet%0Atraditional%20approaches%20still%20struggle%20with%20understanding%20complex%20user%0Apreferences%20and%20providing%20explainable%20recommendations.%20The%20emergence%20of%20Large%0ALanguage%20Model%20%28LLM%29-powered%20agents%20offers%20a%20promising%20approach%20by%20enabling%0Anatural%20language%20interactions%20and%20interpretable%20reasoning%2C%20potentially%0Atransforming%20research%20in%20recommender%20systems.%20This%20survey%20provides%20a%20systematic%0Areview%20of%20the%20emerging%20applications%20of%20LLM-powered%20agents%20in%20recommender%0Asystems.%20We%20identify%20and%20analyze%20three%20key%20paradigms%20in%20current%20research%3A%20%281%29%0ARecommender-oriented%20approaches%2C%20which%20leverage%20intelligent%20agents%20to%20enhance%0Athe%20fundamental%20recommendation%20mechanisms%3B%20%282%29%20Interaction-oriented%20approaches%2C%0Awhich%20facilitate%20dynamic%20user%20engagement%20through%20natural%20dialogue%20and%0Ainterpretable%20suggestions%3B%20and%20%283%29%20Simulation-oriented%20approaches%2C%20which%20employ%0Amulti-agent%20frameworks%20to%20model%20complex%20user-item%20interactions%20and%20system%0Adynamics.%20Beyond%20paradigm%20categorization%2C%20we%20analyze%20the%20architectural%0Afoundations%20of%20LLM-powered%20recommendation%20agents%2C%20examining%20their%20essential%0Acomponents%3A%20profile%20construction%2C%20memory%20management%2C%20strategic%20planning%2C%20and%0Aaction%20execution.%20Our%20investigation%20extends%20to%20a%20comprehensive%20analysis%20of%0Abenchmark%20datasets%20and%20evaluation%20frameworks%20in%20this%20domain.%20This%20systematic%0Aexamination%20not%20only%20illuminates%20the%20current%20state%20of%20LLM-powered%20agent%0Arecommender%20systems%20but%20also%20charts%20critical%20challenges%20and%20promising%20research%0Adirections%20in%20this%20transformative%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10050v1&entry.124074799=Read"},
{"title": "Is Deep Learning finally better than Decision Trees on Tabular Data?", "author": "Guri Zab\u00ebrgja and Arlind Kadra and Christian M. M. Frey and Josif Grabocka", "abstract": "  Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands.\n", "link": "http://arxiv.org/abs/2402.03970v2", "date": "2025-02-14", "relevancy": 1.8002, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4818}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4282}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Deep%20Learning%20finally%20better%20than%20Decision%20Trees%20on%20Tabular%20Data%3F&body=Title%3A%20Is%20Deep%20Learning%20finally%20better%20than%20Decision%20Trees%20on%20Tabular%20Data%3F%0AAuthor%3A%20Guri%20Zab%C3%ABrgja%20and%20Arlind%20Kadra%20and%20Christian%20M.%20M.%20Frey%20and%20Josif%20Grabocka%0AAbstract%3A%20%20%20Tabular%20data%20is%20a%20ubiquitous%20data%20modality%20due%20to%20its%20versatility%20and%20ease%20of%0Ause%20in%20many%20real-world%20applications.%20The%20predominant%20heuristics%20for%20handling%0Aclassification%20tasks%20on%20tabular%20data%20rely%20on%20classical%20machine%20learning%0Atechniques%2C%20as%20the%20superiority%20of%20deep%20learning%20models%20has%20not%20yet%20been%0Ademonstrated.%20This%20raises%20the%20question%20of%20whether%20new%20deep%20learning%20paradigms%0Acan%20surpass%20classical%20approaches.%20Recent%20studies%20on%20tabular%20data%20offer%20a%20unique%0Aperspective%20on%20the%20limitations%20of%20neural%20networks%20in%20this%20domain%20and%20highlight%0Athe%20superiority%20of%20gradient%20boosted%20decision%20trees%20%28GBDTs%29%20in%20terms%20of%0Ascalability%20and%20robustness%20across%20various%20datasets.%20However%2C%20novel%20foundation%0Amodels%20have%20not%20been%20thoroughly%20assessed%20regarding%20quality%20or%20fairly%20compared%0Ato%20existing%20methods%20for%20tabular%20classification.%20Our%20study%20categorizes%20ten%0Astate-of-the-art%20neural%20models%20based%20on%20their%20underlying%20learning%20paradigm%2C%0Ademonstrating%20specifically%20that%20meta-learned%20foundation%20models%20outperform%20GBDTs%0Ain%20small%20data%20regimes.%20Although%20dataset-specific%20neural%20networks%20generally%0Aoutperform%20LLM-based%20tabular%20classifiers%2C%20they%20are%20surpassed%20by%20an%20AutoML%0Alibrary%20which%20exhibits%20the%20best%20performance%20but%20at%20the%20cost%20of%20higher%0Acomputational%20demands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Deep%2520Learning%2520finally%2520better%2520than%2520Decision%2520Trees%2520on%2520Tabular%2520Data%253F%26entry.906535625%3DGuri%2520Zab%25C3%25ABrgja%2520and%2520Arlind%2520Kadra%2520and%2520Christian%2520M.%2520M.%2520Frey%2520and%2520Josif%2520Grabocka%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520a%2520ubiquitous%2520data%2520modality%2520due%2520to%2520its%2520versatility%2520and%2520ease%2520of%250Ause%2520in%2520many%2520real-world%2520applications.%2520The%2520predominant%2520heuristics%2520for%2520handling%250Aclassification%2520tasks%2520on%2520tabular%2520data%2520rely%2520on%2520classical%2520machine%2520learning%250Atechniques%252C%2520as%2520the%2520superiority%2520of%2520deep%2520learning%2520models%2520has%2520not%2520yet%2520been%250Ademonstrated.%2520This%2520raises%2520the%2520question%2520of%2520whether%2520new%2520deep%2520learning%2520paradigms%250Acan%2520surpass%2520classical%2520approaches.%2520Recent%2520studies%2520on%2520tabular%2520data%2520offer%2520a%2520unique%250Aperspective%2520on%2520the%2520limitations%2520of%2520neural%2520networks%2520in%2520this%2520domain%2520and%2520highlight%250Athe%2520superiority%2520of%2520gradient%2520boosted%2520decision%2520trees%2520%2528GBDTs%2529%2520in%2520terms%2520of%250Ascalability%2520and%2520robustness%2520across%2520various%2520datasets.%2520However%252C%2520novel%2520foundation%250Amodels%2520have%2520not%2520been%2520thoroughly%2520assessed%2520regarding%2520quality%2520or%2520fairly%2520compared%250Ato%2520existing%2520methods%2520for%2520tabular%2520classification.%2520Our%2520study%2520categorizes%2520ten%250Astate-of-the-art%2520neural%2520models%2520based%2520on%2520their%2520underlying%2520learning%2520paradigm%252C%250Ademonstrating%2520specifically%2520that%2520meta-learned%2520foundation%2520models%2520outperform%2520GBDTs%250Ain%2520small%2520data%2520regimes.%2520Although%2520dataset-specific%2520neural%2520networks%2520generally%250Aoutperform%2520LLM-based%2520tabular%2520classifiers%252C%2520they%2520are%2520surpassed%2520by%2520an%2520AutoML%250Alibrary%2520which%2520exhibits%2520the%2520best%2520performance%2520but%2520at%2520the%2520cost%2520of%2520higher%250Acomputational%2520demands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Deep%20Learning%20finally%20better%20than%20Decision%20Trees%20on%20Tabular%20Data%3F&entry.906535625=Guri%20Zab%C3%ABrgja%20and%20Arlind%20Kadra%20and%20Christian%20M.%20M.%20Frey%20and%20Josif%20Grabocka&entry.1292438233=%20%20Tabular%20data%20is%20a%20ubiquitous%20data%20modality%20due%20to%20its%20versatility%20and%20ease%20of%0Ause%20in%20many%20real-world%20applications.%20The%20predominant%20heuristics%20for%20handling%0Aclassification%20tasks%20on%20tabular%20data%20rely%20on%20classical%20machine%20learning%0Atechniques%2C%20as%20the%20superiority%20of%20deep%20learning%20models%20has%20not%20yet%20been%0Ademonstrated.%20This%20raises%20the%20question%20of%20whether%20new%20deep%20learning%20paradigms%0Acan%20surpass%20classical%20approaches.%20Recent%20studies%20on%20tabular%20data%20offer%20a%20unique%0Aperspective%20on%20the%20limitations%20of%20neural%20networks%20in%20this%20domain%20and%20highlight%0Athe%20superiority%20of%20gradient%20boosted%20decision%20trees%20%28GBDTs%29%20in%20terms%20of%0Ascalability%20and%20robustness%20across%20various%20datasets.%20However%2C%20novel%20foundation%0Amodels%20have%20not%20been%20thoroughly%20assessed%20regarding%20quality%20or%20fairly%20compared%0Ato%20existing%20methods%20for%20tabular%20classification.%20Our%20study%20categorizes%20ten%0Astate-of-the-art%20neural%20models%20based%20on%20their%20underlying%20learning%20paradigm%2C%0Ademonstrating%20specifically%20that%20meta-learned%20foundation%20models%20outperform%20GBDTs%0Ain%20small%20data%20regimes.%20Although%20dataset-specific%20neural%20networks%20generally%0Aoutperform%20LLM-based%20tabular%20classifiers%2C%20they%20are%20surpassed%20by%20an%20AutoML%0Alibrary%20which%20exhibits%20the%20best%20performance%20but%20at%20the%20cost%20of%20higher%0Acomputational%20demands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03970v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


