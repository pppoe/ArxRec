<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250811.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SAGOnline: Segment Any Gaussians Online", "author": "Wentao Sun and Quanyun Wu and Hanqing Xu and Kyle Gao and Zhengsen Xu and Yiping Chen and Dedong Zhang and Lingfei Ma and John S. Zelek and Jonathan Li", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications.\n", "link": "http://arxiv.org/abs/2508.08219v1", "date": "2025-08-11", "relevancy": 3.4781, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7409}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6739}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAGOnline%3A%20Segment%20Any%20Gaussians%20Online&body=Title%3A%20SAGOnline%3A%20Segment%20Any%20Gaussians%20Online%0AAuthor%3A%20Wentao%20Sun%20and%20Quanyun%20Wu%20and%20Hanqing%20Xu%20and%20Kyle%20Gao%20and%20Zhengsen%20Xu%20and%20Yiping%20Chen%20and%20Dedong%20Zhang%20and%20Lingfei%20Ma%20and%20John%20S.%20Zelek%20and%20Jonathan%20Li%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20explicit%0A3D%20scene%20representation%2C%20yet%20achieving%20efficient%20and%20consistent%203D%20segmentation%0Aremains%20challenging.%20Current%20methods%20suffer%20from%20prohibitive%20computational%0Acosts%2C%20limited%203D%20spatial%20reasoning%2C%20and%20an%20inability%20to%20track%20multiple%20objects%0Asimultaneously.%20We%20present%20Segment%20Any%20Gaussians%20Online%20%28SAGOnline%29%2C%20a%0Alightweight%20and%20zero-shot%20framework%20for%20real-time%203D%20segmentation%20in%20Gaussian%0Ascenes%20that%20addresses%20these%20limitations%20through%20two%20key%20innovations%3A%20%281%29%20a%0Adecoupled%20strategy%20that%20integrates%20video%20foundation%20models%20%28e.g.%2C%20SAM2%29%20for%0Aview-consistent%202D%20mask%20propagation%20across%20synthesized%20views%3B%20and%20%282%29%20a%0AGPU-accelerated%203D%20mask%20generation%20and%20Gaussian-level%20instance%20labeling%0Aalgorithm%20that%20assigns%20unique%20identifiers%20to%203D%20primitives%2C%20enabling%20lossless%0Amulti-object%20tracking%20and%20segmentation%20across%20views.%20SAGOnline%20achieves%0Astate-of-the-art%20performance%20on%20NVOS%20%2892.7%25%20mIoU%29%20and%20Spin-NeRF%20%2895.2%25%20mIoU%29%0Abenchmarks%2C%20outperforming%20Feature3DGS%2C%20OmniSeg3D-gs%2C%20and%20SA3D%20by%2015--1500%20times%0Ain%20inference%20speed%20%2827%20ms/frame%29.%20Qualitative%20results%20demonstrate%20robust%0Amulti-object%20segmentation%20and%20tracking%20in%20complex%20scenes.%20Our%20contributions%0Ainclude%3A%20%28i%29%20a%20lightweight%20and%20zero-shot%20framework%20for%203D%20segmentation%20in%0AGaussian%20scenes%2C%20%28ii%29%20explicit%20labeling%20of%20Gaussian%20primitives%20enabling%0Asimultaneous%20segmentation%20and%20tracking%2C%20and%20%28iii%29%20the%20effective%20adaptation%20of%0A2D%20video%20foundation%20models%20to%20the%203D%20domain.%20This%20work%20allows%20real-time%0Arendering%20and%203D%20scene%20understanding%2C%20paving%20the%20way%20for%20practical%20AR/VR%20and%0Arobotic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAGOnline%253A%2520Segment%2520Any%2520Gaussians%2520Online%26entry.906535625%3DWentao%2520Sun%2520and%2520Quanyun%2520Wu%2520and%2520Hanqing%2520Xu%2520and%2520Kyle%2520Gao%2520and%2520Zhengsen%2520Xu%2520and%2520Yiping%2520Chen%2520and%2520Dedong%2520Zhang%2520and%2520Lingfei%2520Ma%2520and%2520John%2520S.%2520Zelek%2520and%2520Jonathan%2520Li%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520explicit%250A3D%2520scene%2520representation%252C%2520yet%2520achieving%2520efficient%2520and%2520consistent%25203D%2520segmentation%250Aremains%2520challenging.%2520Current%2520methods%2520suffer%2520from%2520prohibitive%2520computational%250Acosts%252C%2520limited%25203D%2520spatial%2520reasoning%252C%2520and%2520an%2520inability%2520to%2520track%2520multiple%2520objects%250Asimultaneously.%2520We%2520present%2520Segment%2520Any%2520Gaussians%2520Online%2520%2528SAGOnline%2529%252C%2520a%250Alightweight%2520and%2520zero-shot%2520framework%2520for%2520real-time%25203D%2520segmentation%2520in%2520Gaussian%250Ascenes%2520that%2520addresses%2520these%2520limitations%2520through%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%250Adecoupled%2520strategy%2520that%2520integrates%2520video%2520foundation%2520models%2520%2528e.g.%252C%2520SAM2%2529%2520for%250Aview-consistent%25202D%2520mask%2520propagation%2520across%2520synthesized%2520views%253B%2520and%2520%25282%2529%2520a%250AGPU-accelerated%25203D%2520mask%2520generation%2520and%2520Gaussian-level%2520instance%2520labeling%250Aalgorithm%2520that%2520assigns%2520unique%2520identifiers%2520to%25203D%2520primitives%252C%2520enabling%2520lossless%250Amulti-object%2520tracking%2520and%2520segmentation%2520across%2520views.%2520SAGOnline%2520achieves%250Astate-of-the-art%2520performance%2520on%2520NVOS%2520%252892.7%2525%2520mIoU%2529%2520and%2520Spin-NeRF%2520%252895.2%2525%2520mIoU%2529%250Abenchmarks%252C%2520outperforming%2520Feature3DGS%252C%2520OmniSeg3D-gs%252C%2520and%2520SA3D%2520by%252015--1500%2520times%250Ain%2520inference%2520speed%2520%252827%2520ms/frame%2529.%2520Qualitative%2520results%2520demonstrate%2520robust%250Amulti-object%2520segmentation%2520and%2520tracking%2520in%2520complex%2520scenes.%2520Our%2520contributions%250Ainclude%253A%2520%2528i%2529%2520a%2520lightweight%2520and%2520zero-shot%2520framework%2520for%25203D%2520segmentation%2520in%250AGaussian%2520scenes%252C%2520%2528ii%2529%2520explicit%2520labeling%2520of%2520Gaussian%2520primitives%2520enabling%250Asimultaneous%2520segmentation%2520and%2520tracking%252C%2520and%2520%2528iii%2529%2520the%2520effective%2520adaptation%2520of%250A2D%2520video%2520foundation%2520models%2520to%2520the%25203D%2520domain.%2520This%2520work%2520allows%2520real-time%250Arendering%2520and%25203D%2520scene%2520understanding%252C%2520paving%2520the%2520way%2520for%2520practical%2520AR/VR%2520and%250Arobotic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAGOnline%3A%20Segment%20Any%20Gaussians%20Online&entry.906535625=Wentao%20Sun%20and%20Quanyun%20Wu%20and%20Hanqing%20Xu%20and%20Kyle%20Gao%20and%20Zhengsen%20Xu%20and%20Yiping%20Chen%20and%20Dedong%20Zhang%20and%20Lingfei%20Ma%20and%20John%20S.%20Zelek%20and%20Jonathan%20Li&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20explicit%0A3D%20scene%20representation%2C%20yet%20achieving%20efficient%20and%20consistent%203D%20segmentation%0Aremains%20challenging.%20Current%20methods%20suffer%20from%20prohibitive%20computational%0Acosts%2C%20limited%203D%20spatial%20reasoning%2C%20and%20an%20inability%20to%20track%20multiple%20objects%0Asimultaneously.%20We%20present%20Segment%20Any%20Gaussians%20Online%20%28SAGOnline%29%2C%20a%0Alightweight%20and%20zero-shot%20framework%20for%20real-time%203D%20segmentation%20in%20Gaussian%0Ascenes%20that%20addresses%20these%20limitations%20through%20two%20key%20innovations%3A%20%281%29%20a%0Adecoupled%20strategy%20that%20integrates%20video%20foundation%20models%20%28e.g.%2C%20SAM2%29%20for%0Aview-consistent%202D%20mask%20propagation%20across%20synthesized%20views%3B%20and%20%282%29%20a%0AGPU-accelerated%203D%20mask%20generation%20and%20Gaussian-level%20instance%20labeling%0Aalgorithm%20that%20assigns%20unique%20identifiers%20to%203D%20primitives%2C%20enabling%20lossless%0Amulti-object%20tracking%20and%20segmentation%20across%20views.%20SAGOnline%20achieves%0Astate-of-the-art%20performance%20on%20NVOS%20%2892.7%25%20mIoU%29%20and%20Spin-NeRF%20%2895.2%25%20mIoU%29%0Abenchmarks%2C%20outperforming%20Feature3DGS%2C%20OmniSeg3D-gs%2C%20and%20SA3D%20by%2015--1500%20times%0Ain%20inference%20speed%20%2827%20ms/frame%29.%20Qualitative%20results%20demonstrate%20robust%0Amulti-object%20segmentation%20and%20tracking%20in%20complex%20scenes.%20Our%20contributions%0Ainclude%3A%20%28i%29%20a%20lightweight%20and%20zero-shot%20framework%20for%203D%20segmentation%20in%0AGaussian%20scenes%2C%20%28ii%29%20explicit%20labeling%20of%20Gaussian%20primitives%20enabling%0Asimultaneous%20segmentation%20and%20tracking%2C%20and%20%28iii%29%20the%20effective%20adaptation%20of%0A2D%20video%20foundation%20models%20to%20the%203D%20domain.%20This%20work%20allows%20real-time%0Arendering%20and%203D%20scene%20understanding%2C%20paving%20the%20way%20for%20practical%20AR/VR%20and%0Arobotic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08219v1&entry.124074799=Read"},
{"title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation", "author": "Bowen Xue and Qixin Yan and Wenjing Wang and Hao Liu and Chen Li", "abstract": "  Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.\n", "link": "http://arxiv.org/abs/2508.07901v1", "date": "2025-08-11", "relevancy": 3.2291, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7251}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6104}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%0A%20%20Generation&body=Title%3A%20Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%0A%20%20Generation%0AAuthor%3A%20Bowen%20Xue%20and%20Qixin%20Yan%20and%20Wenjing%20Wang%20and%20Hao%20Liu%20and%20Chen%20Li%0AAbstract%3A%20%20%20Generating%20high-fidelity%20human%20videos%20that%20match%20user-specified%20identities%20is%0Aimportant%20yet%20challenging%20in%20the%20field%20of%20generative%20AI.%20Existing%20methods%20often%0Arely%20on%20an%20excessive%20number%20of%20training%20parameters%20and%20lack%20compatibility%20with%0Aother%20AIGC%20tools.%20In%20this%20paper%2C%20we%20propose%20Stand-In%2C%20a%20lightweight%20and%0Aplug-and-play%20framework%20for%20identity%20preservation%20in%20video%20generation.%0ASpecifically%2C%20we%20introduce%20a%20conditional%20image%20branch%20into%20the%20pre-trained%0Avideo%20generation%20model.%20Identity%20control%20is%20achieved%20through%20restricted%0Aself-attentions%20with%20conditional%20position%20mapping%2C%20and%20can%20be%20learned%20quickly%0Awith%20only%202000%20pairs.%20Despite%20incorporating%20and%20training%20just%20%24%5Csim%241%5C%25%0Aadditional%20parameters%2C%20our%20framework%20achieves%20excellent%20results%20in%20video%0Aquality%20and%20identity%20preservation%2C%20outperforming%20other%20full-parameter%20training%0Amethods.%20Moreover%2C%20our%20framework%20can%20be%20seamlessly%20integrated%20for%20other%20tasks%2C%0Asuch%20as%20subject-driven%20video%20generation%2C%20pose-referenced%20video%20generation%2C%0Astylization%2C%20and%20face%20swapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStand-In%253A%2520A%2520Lightweight%2520and%2520Plug-and-Play%2520Identity%2520Control%2520for%2520Video%250A%2520%2520Generation%26entry.906535625%3DBowen%2520Xue%2520and%2520Qixin%2520Yan%2520and%2520Wenjing%2520Wang%2520and%2520Hao%2520Liu%2520and%2520Chen%2520Li%26entry.1292438233%3D%2520%2520Generating%2520high-fidelity%2520human%2520videos%2520that%2520match%2520user-specified%2520identities%2520is%250Aimportant%2520yet%2520challenging%2520in%2520the%2520field%2520of%2520generative%2520AI.%2520Existing%2520methods%2520often%250Arely%2520on%2520an%2520excessive%2520number%2520of%2520training%2520parameters%2520and%2520lack%2520compatibility%2520with%250Aother%2520AIGC%2520tools.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Stand-In%252C%2520a%2520lightweight%2520and%250Aplug-and-play%2520framework%2520for%2520identity%2520preservation%2520in%2520video%2520generation.%250ASpecifically%252C%2520we%2520introduce%2520a%2520conditional%2520image%2520branch%2520into%2520the%2520pre-trained%250Avideo%2520generation%2520model.%2520Identity%2520control%2520is%2520achieved%2520through%2520restricted%250Aself-attentions%2520with%2520conditional%2520position%2520mapping%252C%2520and%2520can%2520be%2520learned%2520quickly%250Awith%2520only%25202000%2520pairs.%2520Despite%2520incorporating%2520and%2520training%2520just%2520%2524%255Csim%25241%255C%2525%250Aadditional%2520parameters%252C%2520our%2520framework%2520achieves%2520excellent%2520results%2520in%2520video%250Aquality%2520and%2520identity%2520preservation%252C%2520outperforming%2520other%2520full-parameter%2520training%250Amethods.%2520Moreover%252C%2520our%2520framework%2520can%2520be%2520seamlessly%2520integrated%2520for%2520other%2520tasks%252C%250Asuch%2520as%2520subject-driven%2520video%2520generation%252C%2520pose-referenced%2520video%2520generation%252C%250Astylization%252C%2520and%2520face%2520swapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stand-In%3A%20A%20Lightweight%20and%20Plug-and-Play%20Identity%20Control%20for%20Video%0A%20%20Generation&entry.906535625=Bowen%20Xue%20and%20Qixin%20Yan%20and%20Wenjing%20Wang%20and%20Hao%20Liu%20and%20Chen%20Li&entry.1292438233=%20%20Generating%20high-fidelity%20human%20videos%20that%20match%20user-specified%20identities%20is%0Aimportant%20yet%20challenging%20in%20the%20field%20of%20generative%20AI.%20Existing%20methods%20often%0Arely%20on%20an%20excessive%20number%20of%20training%20parameters%20and%20lack%20compatibility%20with%0Aother%20AIGC%20tools.%20In%20this%20paper%2C%20we%20propose%20Stand-In%2C%20a%20lightweight%20and%0Aplug-and-play%20framework%20for%20identity%20preservation%20in%20video%20generation.%0ASpecifically%2C%20we%20introduce%20a%20conditional%20image%20branch%20into%20the%20pre-trained%0Avideo%20generation%20model.%20Identity%20control%20is%20achieved%20through%20restricted%0Aself-attentions%20with%20conditional%20position%20mapping%2C%20and%20can%20be%20learned%20quickly%0Awith%20only%202000%20pairs.%20Despite%20incorporating%20and%20training%20just%20%24%5Csim%241%5C%25%0Aadditional%20parameters%2C%20our%20framework%20achieves%20excellent%20results%20in%20video%0Aquality%20and%20identity%20preservation%2C%20outperforming%20other%20full-parameter%20training%0Amethods.%20Moreover%2C%20our%20framework%20can%20be%20seamlessly%20integrated%20for%20other%20tasks%2C%0Asuch%20as%20subject-driven%20video%20generation%2C%20pose-referenced%20video%20generation%2C%0Astylization%2C%20and%20face%20swapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07901v1&entry.124074799=Read"},
{"title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting", "author": "Shuting He and Guangquan Jie and Changshuo Wang and Yun Zhou and Shuming Hu and Guanbin Li and Henghui Ding", "abstract": "  We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.\n", "link": "http://arxiv.org/abs/2508.08252v1", "date": "2025-08-11", "relevancy": 3.2194, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6897}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.662}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReferSplat%3A%20Referring%20Segmentation%20in%203D%20Gaussian%20Splatting&body=Title%3A%20ReferSplat%3A%20Referring%20Segmentation%20in%203D%20Gaussian%20Splatting%0AAuthor%3A%20Shuting%20He%20and%20Guangquan%20Jie%20and%20Changshuo%20Wang%20and%20Yun%20Zhou%20and%20Shuming%20Hu%20and%20Guanbin%20Li%20and%20Henghui%20Ding%0AAbstract%3A%20%20%20We%20introduce%20Referring%203D%20Gaussian%20Splatting%20Segmentation%20%28R3DGS%29%2C%20a%20new%20task%0Athat%20aims%20to%20segment%20target%20objects%20in%20a%203D%20Gaussian%20scene%20based%20on%20natural%0Alanguage%20descriptions%2C%20which%20often%20contain%20spatial%20relationships%20or%20object%0Aattributes.%20This%20task%20requires%20the%20model%20to%20identify%20newly%20described%20objects%0Athat%20may%20be%20occluded%20or%20not%20directly%20visible%20in%20a%20novel%20view%2C%20posing%20a%0Asignificant%20challenge%20for%203D%20multi-modal%20understanding.%20Developing%20this%0Acapability%20is%20crucial%20for%20advancing%20embodied%20AI.%20To%20support%20research%20in%20this%0Aarea%2C%20we%20construct%20the%20first%20R3DGS%20dataset%2C%20Ref-LERF.%20Our%20analysis%20reveals%20that%0A3D%20multi-modal%20understanding%20and%20spatial%20relationship%20modeling%20are%20key%0Achallenges%20for%20R3DGS.%20To%20address%20these%20challenges%2C%20we%20propose%20ReferSplat%2C%20a%0Aframework%20that%20explicitly%20models%203D%20Gaussian%20points%20with%20natural%20language%0Aexpressions%20in%20a%20spatially%20aware%20paradigm.%20ReferSplat%20achieves%20state-of-the-art%0Aperformance%20on%20both%20the%20newly%20proposed%20R3DGS%20task%20and%203D%20open-vocabulary%0Asegmentation%20benchmarks.%20Dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/heshuting555/ReferSplat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReferSplat%253A%2520Referring%2520Segmentation%2520in%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DShuting%2520He%2520and%2520Guangquan%2520Jie%2520and%2520Changshuo%2520Wang%2520and%2520Yun%2520Zhou%2520and%2520Shuming%2520Hu%2520and%2520Guanbin%2520Li%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%2520We%2520introduce%2520Referring%25203D%2520Gaussian%2520Splatting%2520Segmentation%2520%2528R3DGS%2529%252C%2520a%2520new%2520task%250Athat%2520aims%2520to%2520segment%2520target%2520objects%2520in%2520a%25203D%2520Gaussian%2520scene%2520based%2520on%2520natural%250Alanguage%2520descriptions%252C%2520which%2520often%2520contain%2520spatial%2520relationships%2520or%2520object%250Aattributes.%2520This%2520task%2520requires%2520the%2520model%2520to%2520identify%2520newly%2520described%2520objects%250Athat%2520may%2520be%2520occluded%2520or%2520not%2520directly%2520visible%2520in%2520a%2520novel%2520view%252C%2520posing%2520a%250Asignificant%2520challenge%2520for%25203D%2520multi-modal%2520understanding.%2520Developing%2520this%250Acapability%2520is%2520crucial%2520for%2520advancing%2520embodied%2520AI.%2520To%2520support%2520research%2520in%2520this%250Aarea%252C%2520we%2520construct%2520the%2520first%2520R3DGS%2520dataset%252C%2520Ref-LERF.%2520Our%2520analysis%2520reveals%2520that%250A3D%2520multi-modal%2520understanding%2520and%2520spatial%2520relationship%2520modeling%2520are%2520key%250Achallenges%2520for%2520R3DGS.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520ReferSplat%252C%2520a%250Aframework%2520that%2520explicitly%2520models%25203D%2520Gaussian%2520points%2520with%2520natural%2520language%250Aexpressions%2520in%2520a%2520spatially%2520aware%2520paradigm.%2520ReferSplat%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520both%2520the%2520newly%2520proposed%2520R3DGS%2520task%2520and%25203D%2520open-vocabulary%250Asegmentation%2520benchmarks.%2520Dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/heshuting555/ReferSplat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReferSplat%3A%20Referring%20Segmentation%20in%203D%20Gaussian%20Splatting&entry.906535625=Shuting%20He%20and%20Guangquan%20Jie%20and%20Changshuo%20Wang%20and%20Yun%20Zhou%20and%20Shuming%20Hu%20and%20Guanbin%20Li%20and%20Henghui%20Ding&entry.1292438233=%20%20We%20introduce%20Referring%203D%20Gaussian%20Splatting%20Segmentation%20%28R3DGS%29%2C%20a%20new%20task%0Athat%20aims%20to%20segment%20target%20objects%20in%20a%203D%20Gaussian%20scene%20based%20on%20natural%0Alanguage%20descriptions%2C%20which%20often%20contain%20spatial%20relationships%20or%20object%0Aattributes.%20This%20task%20requires%20the%20model%20to%20identify%20newly%20described%20objects%0Athat%20may%20be%20occluded%20or%20not%20directly%20visible%20in%20a%20novel%20view%2C%20posing%20a%0Asignificant%20challenge%20for%203D%20multi-modal%20understanding.%20Developing%20this%0Acapability%20is%20crucial%20for%20advancing%20embodied%20AI.%20To%20support%20research%20in%20this%0Aarea%2C%20we%20construct%20the%20first%20R3DGS%20dataset%2C%20Ref-LERF.%20Our%20analysis%20reveals%20that%0A3D%20multi-modal%20understanding%20and%20spatial%20relationship%20modeling%20are%20key%0Achallenges%20for%20R3DGS.%20To%20address%20these%20challenges%2C%20we%20propose%20ReferSplat%2C%20a%0Aframework%20that%20explicitly%20models%203D%20Gaussian%20points%20with%20natural%20language%0Aexpressions%20in%20a%20spatially%20aware%20paradigm.%20ReferSplat%20achieves%20state-of-the-art%0Aperformance%20on%20both%20the%20newly%20proposed%20R3DGS%20task%20and%203D%20open-vocabulary%0Asegmentation%20benchmarks.%20Dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/heshuting555/ReferSplat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08252v1&entry.124074799=Read"},
{"title": "Learning an Implicit Physics Model for Image-based Fluid Simulation", "author": "Emily Yue-Ting Jia and Jiageng Mao and Zhiyuan Gao and Yajie Zhao and Yue Wang", "abstract": "  Humans possess an exceptional ability to imagine 4D scenes, encompassing both\nmotion and 3D geometry, from a single still image. This ability is rooted in\nour accumulated observations of similar scenes and an intuitive understanding\nof physics. In this paper, we aim to replicate this capacity in neural\nnetworks, specifically focusing on natural fluid imagery. Existing methods for\nthis task typically employ simplistic 2D motion estimators to animate the\nimage, leading to motion predictions that often defy physical principles,\nresulting in unrealistic animations. Our approach introduces a novel method for\ngenerating 4D scenes with physics-consistent animation from a single image. We\npropose the use of a physics-informed neural network that predicts motion for\neach surface point, guided by a loss term derived from fundamental physical\nprinciples, including the Navier-Stokes equations. To capture appearance, we\npredict feature-based 3D Gaussians from the input image and its estimated\ndepth, which are then animated using the predicted motions and rendered from\nany desired camera perspective. Experimental results highlight the\neffectiveness of our method in producing physically plausible animations,\nshowcasing significant performance improvements over existing methods. Our\nproject page is https://physfluid.github.io/ .\n", "link": "http://arxiv.org/abs/2508.08254v1", "date": "2025-08-11", "relevancy": 3.1885, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6704}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.661}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20an%20Implicit%20Physics%20Model%20for%20Image-based%20Fluid%20Simulation&body=Title%3A%20Learning%20an%20Implicit%20Physics%20Model%20for%20Image-based%20Fluid%20Simulation%0AAuthor%3A%20Emily%20Yue-Ting%20Jia%20and%20Jiageng%20Mao%20and%20Zhiyuan%20Gao%20and%20Yajie%20Zhao%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Humans%20possess%20an%20exceptional%20ability%20to%20imagine%204D%20scenes%2C%20encompassing%20both%0Amotion%20and%203D%20geometry%2C%20from%20a%20single%20still%20image.%20This%20ability%20is%20rooted%20in%0Aour%20accumulated%20observations%20of%20similar%20scenes%20and%20an%20intuitive%20understanding%0Aof%20physics.%20In%20this%20paper%2C%20we%20aim%20to%20replicate%20this%20capacity%20in%20neural%0Anetworks%2C%20specifically%20focusing%20on%20natural%20fluid%20imagery.%20Existing%20methods%20for%0Athis%20task%20typically%20employ%20simplistic%202D%20motion%20estimators%20to%20animate%20the%0Aimage%2C%20leading%20to%20motion%20predictions%20that%20often%20defy%20physical%20principles%2C%0Aresulting%20in%20unrealistic%20animations.%20Our%20approach%20introduces%20a%20novel%20method%20for%0Agenerating%204D%20scenes%20with%20physics-consistent%20animation%20from%20a%20single%20image.%20We%0Apropose%20the%20use%20of%20a%20physics-informed%20neural%20network%20that%20predicts%20motion%20for%0Aeach%20surface%20point%2C%20guided%20by%20a%20loss%20term%20derived%20from%20fundamental%20physical%0Aprinciples%2C%20including%20the%20Navier-Stokes%20equations.%20To%20capture%20appearance%2C%20we%0Apredict%20feature-based%203D%20Gaussians%20from%20the%20input%20image%20and%20its%20estimated%0Adepth%2C%20which%20are%20then%20animated%20using%20the%20predicted%20motions%20and%20rendered%20from%0Aany%20desired%20camera%20perspective.%20Experimental%20results%20highlight%20the%0Aeffectiveness%20of%20our%20method%20in%20producing%20physically%20plausible%20animations%2C%0Ashowcasing%20significant%20performance%20improvements%20over%20existing%20methods.%20Our%0Aproject%20page%20is%20https%3A//physfluid.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520an%2520Implicit%2520Physics%2520Model%2520for%2520Image-based%2520Fluid%2520Simulation%26entry.906535625%3DEmily%2520Yue-Ting%2520Jia%2520and%2520Jiageng%2520Mao%2520and%2520Zhiyuan%2520Gao%2520and%2520Yajie%2520Zhao%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Humans%2520possess%2520an%2520exceptional%2520ability%2520to%2520imagine%25204D%2520scenes%252C%2520encompassing%2520both%250Amotion%2520and%25203D%2520geometry%252C%2520from%2520a%2520single%2520still%2520image.%2520This%2520ability%2520is%2520rooted%2520in%250Aour%2520accumulated%2520observations%2520of%2520similar%2520scenes%2520and%2520an%2520intuitive%2520understanding%250Aof%2520physics.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520replicate%2520this%2520capacity%2520in%2520neural%250Anetworks%252C%2520specifically%2520focusing%2520on%2520natural%2520fluid%2520imagery.%2520Existing%2520methods%2520for%250Athis%2520task%2520typically%2520employ%2520simplistic%25202D%2520motion%2520estimators%2520to%2520animate%2520the%250Aimage%252C%2520leading%2520to%2520motion%2520predictions%2520that%2520often%2520defy%2520physical%2520principles%252C%250Aresulting%2520in%2520unrealistic%2520animations.%2520Our%2520approach%2520introduces%2520a%2520novel%2520method%2520for%250Agenerating%25204D%2520scenes%2520with%2520physics-consistent%2520animation%2520from%2520a%2520single%2520image.%2520We%250Apropose%2520the%2520use%2520of%2520a%2520physics-informed%2520neural%2520network%2520that%2520predicts%2520motion%2520for%250Aeach%2520surface%2520point%252C%2520guided%2520by%2520a%2520loss%2520term%2520derived%2520from%2520fundamental%2520physical%250Aprinciples%252C%2520including%2520the%2520Navier-Stokes%2520equations.%2520To%2520capture%2520appearance%252C%2520we%250Apredict%2520feature-based%25203D%2520Gaussians%2520from%2520the%2520input%2520image%2520and%2520its%2520estimated%250Adepth%252C%2520which%2520are%2520then%2520animated%2520using%2520the%2520predicted%2520motions%2520and%2520rendered%2520from%250Aany%2520desired%2520camera%2520perspective.%2520Experimental%2520results%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520producing%2520physically%2520plausible%2520animations%252C%250Ashowcasing%2520significant%2520performance%2520improvements%2520over%2520existing%2520methods.%2520Our%250Aproject%2520page%2520is%2520https%253A//physfluid.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20an%20Implicit%20Physics%20Model%20for%20Image-based%20Fluid%20Simulation&entry.906535625=Emily%20Yue-Ting%20Jia%20and%20Jiageng%20Mao%20and%20Zhiyuan%20Gao%20and%20Yajie%20Zhao%20and%20Yue%20Wang&entry.1292438233=%20%20Humans%20possess%20an%20exceptional%20ability%20to%20imagine%204D%20scenes%2C%20encompassing%20both%0Amotion%20and%203D%20geometry%2C%20from%20a%20single%20still%20image.%20This%20ability%20is%20rooted%20in%0Aour%20accumulated%20observations%20of%20similar%20scenes%20and%20an%20intuitive%20understanding%0Aof%20physics.%20In%20this%20paper%2C%20we%20aim%20to%20replicate%20this%20capacity%20in%20neural%0Anetworks%2C%20specifically%20focusing%20on%20natural%20fluid%20imagery.%20Existing%20methods%20for%0Athis%20task%20typically%20employ%20simplistic%202D%20motion%20estimators%20to%20animate%20the%0Aimage%2C%20leading%20to%20motion%20predictions%20that%20often%20defy%20physical%20principles%2C%0Aresulting%20in%20unrealistic%20animations.%20Our%20approach%20introduces%20a%20novel%20method%20for%0Agenerating%204D%20scenes%20with%20physics-consistent%20animation%20from%20a%20single%20image.%20We%0Apropose%20the%20use%20of%20a%20physics-informed%20neural%20network%20that%20predicts%20motion%20for%0Aeach%20surface%20point%2C%20guided%20by%20a%20loss%20term%20derived%20from%20fundamental%20physical%0Aprinciples%2C%20including%20the%20Navier-Stokes%20equations.%20To%20capture%20appearance%2C%20we%0Apredict%20feature-based%203D%20Gaussians%20from%20the%20input%20image%20and%20its%20estimated%0Adepth%2C%20which%20are%20then%20animated%20using%20the%20predicted%20motions%20and%20rendered%20from%0Aany%20desired%20camera%20perspective.%20Experimental%20results%20highlight%20the%0Aeffectiveness%20of%20our%20method%20in%20producing%20physically%20plausible%20animations%2C%0Ashowcasing%20significant%20performance%20improvements%20over%20existing%20methods.%20Our%0Aproject%20page%20is%20https%3A//physfluid.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08254v1&entry.124074799=Read"},
{"title": "3D Human Mesh Estimation from Single View RGBD", "author": "Ozhan Suat and Bedirhan Uguz and Batuhan Karagoz and Muhammed Can Keles and Emre Akbas", "abstract": "  Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.\n", "link": "http://arxiv.org/abs/2508.08178v1", "date": "2025-08-11", "relevancy": 3.1873, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6663}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6396}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD&body=Title%3A%203D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD%0AAuthor%3A%20Ozhan%20Suat%20and%20Bedirhan%20Uguz%20and%20Batuhan%20Karagoz%20and%20Muhammed%20Can%20Keles%20and%20Emre%20Akbas%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%203D%20human%20mesh%20estimation%20from%20RGB%20images%3B%0ARGBD%20cameras%2C%20offering%20additional%20depth%20data%2C%20remain%20underutilized.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20accurate%203D%20human%20mesh%20estimation%20from%20a%20single%0ARGBD%20view%2C%20leveraging%20the%20affordability%20and%20widespread%20adoption%20of%20RGBD%20cameras%0Afor%20real-world%20applications.%20A%20fully%20supervised%20approach%20for%20this%20problem%2C%0Arequires%20a%20dataset%20with%20RGBD%20image%20and%203D%20mesh%20label%20pairs.%20However%2C%20collecting%0Asuch%20a%20dataset%20is%20costly%20and%20challenging%2C%20hence%2C%20existing%20datasets%20are%20small%2C%0Aand%20limited%20in%20pose%20and%20shape%20diversity.%20To%20overcome%20this%20data%20scarcity%2C%20we%0Aleverage%20existing%20Motion%20Capture%20%28MoCap%29%20datasets.%20We%20first%20obtain%20complete%203D%0Ameshes%20from%20the%20body%20models%20found%20in%20MoCap%20datasets%2C%20and%20create%20partial%2C%0Asingle-view%20versions%20of%20them%20by%20projection%20to%20a%20virtual%20camera.%20This%20simulates%0Athe%20depth%20data%20provided%20by%20an%20RGBD%20camera%20from%20a%20single%20viewpoint.%20Then%2C%20we%0Atrain%20a%20masked%20autoencoder%20to%20complete%20the%20partial%2C%20single-view%20mesh.%20During%0Ainference%2C%20our%20method%2C%20which%20we%20name%20as%20M%24%5E3%24%20for%20%60%60Masked%20Mesh%20Modeling%27%27%2C%0Amatches%20the%20depth%20values%20coming%20from%20the%20sensor%20to%20vertices%20of%20a%20template%20human%0Amesh%2C%20which%20creates%20a%20partial%2C%20single-view%20mesh.%20We%20effectively%20recover%20parts%0Aof%20the%203D%20human%20body%20mesh%20model%20that%20are%20not%20visible%2C%20resulting%20in%20a%20full%20body%0Amesh.%20M%24%5E3%24%20achieves%2016.8%20mm%20and%2022.0%20mm%20per-vertex-error%20%28PVE%29%20on%20the%20SURREAL%0Aand%20CAPE%20datasets%2C%20respectively%3B%20outperforming%20existing%20methods%20that%20use%0Afull-body%20point%20clouds%20as%20input.%20We%20obtain%20a%20competitive%2070.9%20PVE%20on%20the%20BEHAVE%0Adataset%2C%20outperforming%20a%20recently%20published%20RGB%20based%20method%20by%2018.4%20mm%2C%0Ahighlighting%20the%20usefulness%20of%20depth%20data.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human%2520Mesh%2520Estimation%2520from%2520Single%2520View%2520RGBD%26entry.906535625%3DOzhan%2520Suat%2520and%2520Bedirhan%2520Uguz%2520and%2520Batuhan%2520Karagoz%2520and%2520Muhammed%2520Can%2520Keles%2520and%2520Emre%2520Akbas%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%25203D%2520human%2520mesh%2520estimation%2520from%2520RGB%2520images%253B%250ARGBD%2520cameras%252C%2520offering%2520additional%2520depth%2520data%252C%2520remain%2520underutilized.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520method%2520for%2520accurate%25203D%2520human%2520mesh%2520estimation%2520from%2520a%2520single%250ARGBD%2520view%252C%2520leveraging%2520the%2520affordability%2520and%2520widespread%2520adoption%2520of%2520RGBD%2520cameras%250Afor%2520real-world%2520applications.%2520A%2520fully%2520supervised%2520approach%2520for%2520this%2520problem%252C%250Arequires%2520a%2520dataset%2520with%2520RGBD%2520image%2520and%25203D%2520mesh%2520label%2520pairs.%2520However%252C%2520collecting%250Asuch%2520a%2520dataset%2520is%2520costly%2520and%2520challenging%252C%2520hence%252C%2520existing%2520datasets%2520are%2520small%252C%250Aand%2520limited%2520in%2520pose%2520and%2520shape%2520diversity.%2520To%2520overcome%2520this%2520data%2520scarcity%252C%2520we%250Aleverage%2520existing%2520Motion%2520Capture%2520%2528MoCap%2529%2520datasets.%2520We%2520first%2520obtain%2520complete%25203D%250Ameshes%2520from%2520the%2520body%2520models%2520found%2520in%2520MoCap%2520datasets%252C%2520and%2520create%2520partial%252C%250Asingle-view%2520versions%2520of%2520them%2520by%2520projection%2520to%2520a%2520virtual%2520camera.%2520This%2520simulates%250Athe%2520depth%2520data%2520provided%2520by%2520an%2520RGBD%2520camera%2520from%2520a%2520single%2520viewpoint.%2520Then%252C%2520we%250Atrain%2520a%2520masked%2520autoencoder%2520to%2520complete%2520the%2520partial%252C%2520single-view%2520mesh.%2520During%250Ainference%252C%2520our%2520method%252C%2520which%2520we%2520name%2520as%2520M%2524%255E3%2524%2520for%2520%2560%2560Masked%2520Mesh%2520Modeling%2527%2527%252C%250Amatches%2520the%2520depth%2520values%2520coming%2520from%2520the%2520sensor%2520to%2520vertices%2520of%2520a%2520template%2520human%250Amesh%252C%2520which%2520creates%2520a%2520partial%252C%2520single-view%2520mesh.%2520We%2520effectively%2520recover%2520parts%250Aof%2520the%25203D%2520human%2520body%2520mesh%2520model%2520that%2520are%2520not%2520visible%252C%2520resulting%2520in%2520a%2520full%2520body%250Amesh.%2520M%2524%255E3%2524%2520achieves%252016.8%2520mm%2520and%252022.0%2520mm%2520per-vertex-error%2520%2528PVE%2529%2520on%2520the%2520SURREAL%250Aand%2520CAPE%2520datasets%252C%2520respectively%253B%2520outperforming%2520existing%2520methods%2520that%2520use%250Afull-body%2520point%2520clouds%2520as%2520input.%2520We%2520obtain%2520a%2520competitive%252070.9%2520PVE%2520on%2520the%2520BEHAVE%250Adataset%252C%2520outperforming%2520a%2520recently%2520published%2520RGB%2520based%2520method%2520by%252018.4%2520mm%252C%250Ahighlighting%2520the%2520usefulness%2520of%2520depth%2520data.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Mesh%20Estimation%20from%20Single%20View%20RGBD&entry.906535625=Ozhan%20Suat%20and%20Bedirhan%20Uguz%20and%20Batuhan%20Karagoz%20and%20Muhammed%20Can%20Keles%20and%20Emre%20Akbas&entry.1292438233=%20%20Despite%20significant%20progress%20in%203D%20human%20mesh%20estimation%20from%20RGB%20images%3B%0ARGBD%20cameras%2C%20offering%20additional%20depth%20data%2C%20remain%20underutilized.%20In%20this%0Apaper%2C%20we%20present%20a%20method%20for%20accurate%203D%20human%20mesh%20estimation%20from%20a%20single%0ARGBD%20view%2C%20leveraging%20the%20affordability%20and%20widespread%20adoption%20of%20RGBD%20cameras%0Afor%20real-world%20applications.%20A%20fully%20supervised%20approach%20for%20this%20problem%2C%0Arequires%20a%20dataset%20with%20RGBD%20image%20and%203D%20mesh%20label%20pairs.%20However%2C%20collecting%0Asuch%20a%20dataset%20is%20costly%20and%20challenging%2C%20hence%2C%20existing%20datasets%20are%20small%2C%0Aand%20limited%20in%20pose%20and%20shape%20diversity.%20To%20overcome%20this%20data%20scarcity%2C%20we%0Aleverage%20existing%20Motion%20Capture%20%28MoCap%29%20datasets.%20We%20first%20obtain%20complete%203D%0Ameshes%20from%20the%20body%20models%20found%20in%20MoCap%20datasets%2C%20and%20create%20partial%2C%0Asingle-view%20versions%20of%20them%20by%20projection%20to%20a%20virtual%20camera.%20This%20simulates%0Athe%20depth%20data%20provided%20by%20an%20RGBD%20camera%20from%20a%20single%20viewpoint.%20Then%2C%20we%0Atrain%20a%20masked%20autoencoder%20to%20complete%20the%20partial%2C%20single-view%20mesh.%20During%0Ainference%2C%20our%20method%2C%20which%20we%20name%20as%20M%24%5E3%24%20for%20%60%60Masked%20Mesh%20Modeling%27%27%2C%0Amatches%20the%20depth%20values%20coming%20from%20the%20sensor%20to%20vertices%20of%20a%20template%20human%0Amesh%2C%20which%20creates%20a%20partial%2C%20single-view%20mesh.%20We%20effectively%20recover%20parts%0Aof%20the%203D%20human%20body%20mesh%20model%20that%20are%20not%20visible%2C%20resulting%20in%20a%20full%20body%0Amesh.%20M%24%5E3%24%20achieves%2016.8%20mm%20and%2022.0%20mm%20per-vertex-error%20%28PVE%29%20on%20the%20SURREAL%0Aand%20CAPE%20datasets%2C%20respectively%3B%20outperforming%20existing%20methods%20that%20use%0Afull-body%20point%20clouds%20as%20input.%20We%20obtain%20a%20competitive%2070.9%20PVE%20on%20the%20BEHAVE%0Adataset%2C%20outperforming%20a%20recently%20published%20RGB%20based%20method%20by%2018.4%20mm%2C%0Ahighlighting%20the%20usefulness%20of%20depth%20data.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08178v1&entry.124074799=Read"},
{"title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and\n  Deformable 3D Gaussian Reconstruction", "author": "Tianle Zeng and Junlei Hu and Gerardo Loza Galindo and Sharib Ali and Duygu Sarikaya and Pietro Valdastri and Dominic Jones", "abstract": "  Computer vision-based technologies significantly enhance surgical automation\nby advancing tool tracking, detection, and localization. However, Current\ndata-driven approaches are data-voracious, requiring large, high-quality\nlabeled image datasets, which limits their application in surgical data\nscience. Our Work introduces a novel dynamic Gaussian Splatting technique to\naddress the data scarcity in surgical image datasets. We propose a dynamic\nGaussian model to represent dynamic surgical scenes, enabling the rendering of\nsurgical instruments from unseen viewpoints and deformations with real tissue\nbackgrounds. We utilize a dynamic training adjustment strategy to address\nchallenges posed by poorly calibrated camera poses from real-world scenarios.\nAdditionally, we propose a method based on dynamic Gaussians for automatically\ngenerating annotations for our synthetic data. For evaluation, we constructed a\nnew dataset featuring seven scenes with 14,000 frames of tool and camera motion\nand tool jaw articulation, with a background of an ex-vivo porcine model. Using\nthis dataset, we synthetically replicate the scene deformation from the ground\ntruth data, allowing direct comparisons of synthetic image quality.\nExperimental results illustrate that our method generates photo-realistic\nlabeled image datasets with the highest values in Peak-Signal-to-Noise Ratio\n(29.87). We further evaluate the performance of medical-specific neural\nnetworks trained on real and synthetic images using an unseen real-world image\ndataset. Our results show that the performance of models trained on synthetic\nimages generated by the proposed method outperforms those trained with\nstate-of-the-art standard data augmentation by 10%, leading to an overall\nimprovement in model performances by nearly 15%.\n", "link": "http://arxiv.org/abs/2508.07897v1", "date": "2025-08-11", "relevancy": 3.1345, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6405}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6264}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeeCo%3A%20Image%20Synthesis%20of%20Novel%20Instrument%20States%20Based%20on%20Dynamic%20and%0A%20%20Deformable%203D%20Gaussian%20Reconstruction&body=Title%3A%20NeeCo%3A%20Image%20Synthesis%20of%20Novel%20Instrument%20States%20Based%20on%20Dynamic%20and%0A%20%20Deformable%203D%20Gaussian%20Reconstruction%0AAuthor%3A%20Tianle%20Zeng%20and%20Junlei%20Hu%20and%20Gerardo%20Loza%20Galindo%20and%20Sharib%20Ali%20and%20Duygu%20Sarikaya%20and%20Pietro%20Valdastri%20and%20Dominic%20Jones%0AAbstract%3A%20%20%20Computer%20vision-based%20technologies%20significantly%20enhance%20surgical%20automation%0Aby%20advancing%20tool%20tracking%2C%20detection%2C%20and%20localization.%20However%2C%20Current%0Adata-driven%20approaches%20are%20data-voracious%2C%20requiring%20large%2C%20high-quality%0Alabeled%20image%20datasets%2C%20which%20limits%20their%20application%20in%20surgical%20data%0Ascience.%20Our%20Work%20introduces%20a%20novel%20dynamic%20Gaussian%20Splatting%20technique%20to%0Aaddress%20the%20data%20scarcity%20in%20surgical%20image%20datasets.%20We%20propose%20a%20dynamic%0AGaussian%20model%20to%20represent%20dynamic%20surgical%20scenes%2C%20enabling%20the%20rendering%20of%0Asurgical%20instruments%20from%20unseen%20viewpoints%20and%20deformations%20with%20real%20tissue%0Abackgrounds.%20We%20utilize%20a%20dynamic%20training%20adjustment%20strategy%20to%20address%0Achallenges%20posed%20by%20poorly%20calibrated%20camera%20poses%20from%20real-world%20scenarios.%0AAdditionally%2C%20we%20propose%20a%20method%20based%20on%20dynamic%20Gaussians%20for%20automatically%0Agenerating%20annotations%20for%20our%20synthetic%20data.%20For%20evaluation%2C%20we%20constructed%20a%0Anew%20dataset%20featuring%20seven%20scenes%20with%2014%2C000%20frames%20of%20tool%20and%20camera%20motion%0Aand%20tool%20jaw%20articulation%2C%20with%20a%20background%20of%20an%20ex-vivo%20porcine%20model.%20Using%0Athis%20dataset%2C%20we%20synthetically%20replicate%20the%20scene%20deformation%20from%20the%20ground%0Atruth%20data%2C%20allowing%20direct%20comparisons%20of%20synthetic%20image%20quality.%0AExperimental%20results%20illustrate%20that%20our%20method%20generates%20photo-realistic%0Alabeled%20image%20datasets%20with%20the%20highest%20values%20in%20Peak-Signal-to-Noise%20Ratio%0A%2829.87%29.%20We%20further%20evaluate%20the%20performance%20of%20medical-specific%20neural%0Anetworks%20trained%20on%20real%20and%20synthetic%20images%20using%20an%20unseen%20real-world%20image%0Adataset.%20Our%20results%20show%20that%20the%20performance%20of%20models%20trained%20on%20synthetic%0Aimages%20generated%20by%20the%20proposed%20method%20outperforms%20those%20trained%20with%0Astate-of-the-art%20standard%20data%20augmentation%20by%2010%25%2C%20leading%20to%20an%20overall%0Aimprovement%20in%20model%20performances%20by%20nearly%2015%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeeCo%253A%2520Image%2520Synthesis%2520of%2520Novel%2520Instrument%2520States%2520Based%2520on%2520Dynamic%2520and%250A%2520%2520Deformable%25203D%2520Gaussian%2520Reconstruction%26entry.906535625%3DTianle%2520Zeng%2520and%2520Junlei%2520Hu%2520and%2520Gerardo%2520Loza%2520Galindo%2520and%2520Sharib%2520Ali%2520and%2520Duygu%2520Sarikaya%2520and%2520Pietro%2520Valdastri%2520and%2520Dominic%2520Jones%26entry.1292438233%3D%2520%2520Computer%2520vision-based%2520technologies%2520significantly%2520enhance%2520surgical%2520automation%250Aby%2520advancing%2520tool%2520tracking%252C%2520detection%252C%2520and%2520localization.%2520However%252C%2520Current%250Adata-driven%2520approaches%2520are%2520data-voracious%252C%2520requiring%2520large%252C%2520high-quality%250Alabeled%2520image%2520datasets%252C%2520which%2520limits%2520their%2520application%2520in%2520surgical%2520data%250Ascience.%2520Our%2520Work%2520introduces%2520a%2520novel%2520dynamic%2520Gaussian%2520Splatting%2520technique%2520to%250Aaddress%2520the%2520data%2520scarcity%2520in%2520surgical%2520image%2520datasets.%2520We%2520propose%2520a%2520dynamic%250AGaussian%2520model%2520to%2520represent%2520dynamic%2520surgical%2520scenes%252C%2520enabling%2520the%2520rendering%2520of%250Asurgical%2520instruments%2520from%2520unseen%2520viewpoints%2520and%2520deformations%2520with%2520real%2520tissue%250Abackgrounds.%2520We%2520utilize%2520a%2520dynamic%2520training%2520adjustment%2520strategy%2520to%2520address%250Achallenges%2520posed%2520by%2520poorly%2520calibrated%2520camera%2520poses%2520from%2520real-world%2520scenarios.%250AAdditionally%252C%2520we%2520propose%2520a%2520method%2520based%2520on%2520dynamic%2520Gaussians%2520for%2520automatically%250Agenerating%2520annotations%2520for%2520our%2520synthetic%2520data.%2520For%2520evaluation%252C%2520we%2520constructed%2520a%250Anew%2520dataset%2520featuring%2520seven%2520scenes%2520with%252014%252C000%2520frames%2520of%2520tool%2520and%2520camera%2520motion%250Aand%2520tool%2520jaw%2520articulation%252C%2520with%2520a%2520background%2520of%2520an%2520ex-vivo%2520porcine%2520model.%2520Using%250Athis%2520dataset%252C%2520we%2520synthetically%2520replicate%2520the%2520scene%2520deformation%2520from%2520the%2520ground%250Atruth%2520data%252C%2520allowing%2520direct%2520comparisons%2520of%2520synthetic%2520image%2520quality.%250AExperimental%2520results%2520illustrate%2520that%2520our%2520method%2520generates%2520photo-realistic%250Alabeled%2520image%2520datasets%2520with%2520the%2520highest%2520values%2520in%2520Peak-Signal-to-Noise%2520Ratio%250A%252829.87%2529.%2520We%2520further%2520evaluate%2520the%2520performance%2520of%2520medical-specific%2520neural%250Anetworks%2520trained%2520on%2520real%2520and%2520synthetic%2520images%2520using%2520an%2520unseen%2520real-world%2520image%250Adataset.%2520Our%2520results%2520show%2520that%2520the%2520performance%2520of%2520models%2520trained%2520on%2520synthetic%250Aimages%2520generated%2520by%2520the%2520proposed%2520method%2520outperforms%2520those%2520trained%2520with%250Astate-of-the-art%2520standard%2520data%2520augmentation%2520by%252010%2525%252C%2520leading%2520to%2520an%2520overall%250Aimprovement%2520in%2520model%2520performances%2520by%2520nearly%252015%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeeCo%3A%20Image%20Synthesis%20of%20Novel%20Instrument%20States%20Based%20on%20Dynamic%20and%0A%20%20Deformable%203D%20Gaussian%20Reconstruction&entry.906535625=Tianle%20Zeng%20and%20Junlei%20Hu%20and%20Gerardo%20Loza%20Galindo%20and%20Sharib%20Ali%20and%20Duygu%20Sarikaya%20and%20Pietro%20Valdastri%20and%20Dominic%20Jones&entry.1292438233=%20%20Computer%20vision-based%20technologies%20significantly%20enhance%20surgical%20automation%0Aby%20advancing%20tool%20tracking%2C%20detection%2C%20and%20localization.%20However%2C%20Current%0Adata-driven%20approaches%20are%20data-voracious%2C%20requiring%20large%2C%20high-quality%0Alabeled%20image%20datasets%2C%20which%20limits%20their%20application%20in%20surgical%20data%0Ascience.%20Our%20Work%20introduces%20a%20novel%20dynamic%20Gaussian%20Splatting%20technique%20to%0Aaddress%20the%20data%20scarcity%20in%20surgical%20image%20datasets.%20We%20propose%20a%20dynamic%0AGaussian%20model%20to%20represent%20dynamic%20surgical%20scenes%2C%20enabling%20the%20rendering%20of%0Asurgical%20instruments%20from%20unseen%20viewpoints%20and%20deformations%20with%20real%20tissue%0Abackgrounds.%20We%20utilize%20a%20dynamic%20training%20adjustment%20strategy%20to%20address%0Achallenges%20posed%20by%20poorly%20calibrated%20camera%20poses%20from%20real-world%20scenarios.%0AAdditionally%2C%20we%20propose%20a%20method%20based%20on%20dynamic%20Gaussians%20for%20automatically%0Agenerating%20annotations%20for%20our%20synthetic%20data.%20For%20evaluation%2C%20we%20constructed%20a%0Anew%20dataset%20featuring%20seven%20scenes%20with%2014%2C000%20frames%20of%20tool%20and%20camera%20motion%0Aand%20tool%20jaw%20articulation%2C%20with%20a%20background%20of%20an%20ex-vivo%20porcine%20model.%20Using%0Athis%20dataset%2C%20we%20synthetically%20replicate%20the%20scene%20deformation%20from%20the%20ground%0Atruth%20data%2C%20allowing%20direct%20comparisons%20of%20synthetic%20image%20quality.%0AExperimental%20results%20illustrate%20that%20our%20method%20generates%20photo-realistic%0Alabeled%20image%20datasets%20with%20the%20highest%20values%20in%20Peak-Signal-to-Noise%20Ratio%0A%2829.87%29.%20We%20further%20evaluate%20the%20performance%20of%20medical-specific%20neural%0Anetworks%20trained%20on%20real%20and%20synthetic%20images%20using%20an%20unseen%20real-world%20image%0Adataset.%20Our%20results%20show%20that%20the%20performance%20of%20models%20trained%20on%20synthetic%0Aimages%20generated%20by%20the%20proposed%20method%20outperforms%20those%20trained%20with%0Astate-of-the-art%20standard%20data%20augmentation%20by%2010%25%2C%20leading%20to%20an%20overall%0Aimprovement%20in%20model%20performances%20by%20nearly%2015%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07897v1&entry.124074799=Read"},
{"title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene\n  Reconstruction", "author": "Xudong Cai and Shuo Wang and Peng Wang and Yongcai Wang and Zhaoxin Fan and Wanting Li and Tianbao Zhang and Jianrong Tao and Yeying Jin and Deying Li", "abstract": "  Reconstructing dense geometry for dynamic scenes from a monocular video is a\ncritical yet challenging task. Recent memory-based methods enable efficient\nonline reconstruction, but they fundamentally suffer from a Memory Demand\nDilemma: The memory representation faces an inherent conflict between the\nlong-term stability required for static structures and the rapid, high-fidelity\ndetail retention needed for dynamic motion. This conflict forces existing\nmethods into a compromise, leading to either geometric drift in static\nstructures or blurred, inaccurate reconstructions of dynamic objects. To\naddress this dilemma, we propose Mem4D, a novel framework that decouples the\nmodeling of static geometry and dynamic motion. Guided by this insight, we\ndesign a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)\nfocuses on capturing high-frequency motion details from recent frames, enabling\naccurate and fine-grained modeling of dynamic content; 2) The Persistent\nStructure Memory (PSM) compresses and preserves long-term spatial information,\nensuring global consistency and drift-free reconstruction for static elements.\nBy alternating queries to these specialized memories, Mem4D simultaneously\nmaintains static geometry with global consistency and reconstructs dynamic\nelements with high fidelity. Experiments on challenging benchmarks demonstrate\nthat our method achieves state-of-the-art or competitive performance while\nmaintaining high efficiency. Codes will be publicly available.\n", "link": "http://arxiv.org/abs/2508.07908v1", "date": "2025-08-11", "relevancy": 3.0699, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6624}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5902}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mem4D%3A%20Decoupling%20Static%20and%20Dynamic%20Memory%20for%20Dynamic%20Scene%0A%20%20Reconstruction&body=Title%3A%20Mem4D%3A%20Decoupling%20Static%20and%20Dynamic%20Memory%20for%20Dynamic%20Scene%0A%20%20Reconstruction%0AAuthor%3A%20Xudong%20Cai%20and%20Shuo%20Wang%20and%20Peng%20Wang%20and%20Yongcai%20Wang%20and%20Zhaoxin%20Fan%20and%20Wanting%20Li%20and%20Tianbao%20Zhang%20and%20Jianrong%20Tao%20and%20Yeying%20Jin%20and%20Deying%20Li%0AAbstract%3A%20%20%20Reconstructing%20dense%20geometry%20for%20dynamic%20scenes%20from%20a%20monocular%20video%20is%20a%0Acritical%20yet%20challenging%20task.%20Recent%20memory-based%20methods%20enable%20efficient%0Aonline%20reconstruction%2C%20but%20they%20fundamentally%20suffer%20from%20a%20Memory%20Demand%0ADilemma%3A%20The%20memory%20representation%20faces%20an%20inherent%20conflict%20between%20the%0Along-term%20stability%20required%20for%20static%20structures%20and%20the%20rapid%2C%20high-fidelity%0Adetail%20retention%20needed%20for%20dynamic%20motion.%20This%20conflict%20forces%20existing%0Amethods%20into%20a%20compromise%2C%20leading%20to%20either%20geometric%20drift%20in%20static%0Astructures%20or%20blurred%2C%20inaccurate%20reconstructions%20of%20dynamic%20objects.%20To%0Aaddress%20this%20dilemma%2C%20we%20propose%20Mem4D%2C%20a%20novel%20framework%20that%20decouples%20the%0Amodeling%20of%20static%20geometry%20and%20dynamic%20motion.%20Guided%20by%20this%20insight%2C%20we%0Adesign%20a%20dual-memory%20architecture%3A%201%29%20The%20Transient%20Dynamics%20Memory%20%28TDM%29%0Afocuses%20on%20capturing%20high-frequency%20motion%20details%20from%20recent%20frames%2C%20enabling%0Aaccurate%20and%20fine-grained%20modeling%20of%20dynamic%20content%3B%202%29%20The%20Persistent%0AStructure%20Memory%20%28PSM%29%20compresses%20and%20preserves%20long-term%20spatial%20information%2C%0Aensuring%20global%20consistency%20and%20drift-free%20reconstruction%20for%20static%20elements.%0ABy%20alternating%20queries%20to%20these%20specialized%20memories%2C%20Mem4D%20simultaneously%0Amaintains%20static%20geometry%20with%20global%20consistency%20and%20reconstructs%20dynamic%0Aelements%20with%20high%20fidelity.%20Experiments%20on%20challenging%20benchmarks%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20or%20competitive%20performance%20while%0Amaintaining%20high%20efficiency.%20Codes%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMem4D%253A%2520Decoupling%2520Static%2520and%2520Dynamic%2520Memory%2520for%2520Dynamic%2520Scene%250A%2520%2520Reconstruction%26entry.906535625%3DXudong%2520Cai%2520and%2520Shuo%2520Wang%2520and%2520Peng%2520Wang%2520and%2520Yongcai%2520Wang%2520and%2520Zhaoxin%2520Fan%2520and%2520Wanting%2520Li%2520and%2520Tianbao%2520Zhang%2520and%2520Jianrong%2520Tao%2520and%2520Yeying%2520Jin%2520and%2520Deying%2520Li%26entry.1292438233%3D%2520%2520Reconstructing%2520dense%2520geometry%2520for%2520dynamic%2520scenes%2520from%2520a%2520monocular%2520video%2520is%2520a%250Acritical%2520yet%2520challenging%2520task.%2520Recent%2520memory-based%2520methods%2520enable%2520efficient%250Aonline%2520reconstruction%252C%2520but%2520they%2520fundamentally%2520suffer%2520from%2520a%2520Memory%2520Demand%250ADilemma%253A%2520The%2520memory%2520representation%2520faces%2520an%2520inherent%2520conflict%2520between%2520the%250Along-term%2520stability%2520required%2520for%2520static%2520structures%2520and%2520the%2520rapid%252C%2520high-fidelity%250Adetail%2520retention%2520needed%2520for%2520dynamic%2520motion.%2520This%2520conflict%2520forces%2520existing%250Amethods%2520into%2520a%2520compromise%252C%2520leading%2520to%2520either%2520geometric%2520drift%2520in%2520static%250Astructures%2520or%2520blurred%252C%2520inaccurate%2520reconstructions%2520of%2520dynamic%2520objects.%2520To%250Aaddress%2520this%2520dilemma%252C%2520we%2520propose%2520Mem4D%252C%2520a%2520novel%2520framework%2520that%2520decouples%2520the%250Amodeling%2520of%2520static%2520geometry%2520and%2520dynamic%2520motion.%2520Guided%2520by%2520this%2520insight%252C%2520we%250Adesign%2520a%2520dual-memory%2520architecture%253A%25201%2529%2520The%2520Transient%2520Dynamics%2520Memory%2520%2528TDM%2529%250Afocuses%2520on%2520capturing%2520high-frequency%2520motion%2520details%2520from%2520recent%2520frames%252C%2520enabling%250Aaccurate%2520and%2520fine-grained%2520modeling%2520of%2520dynamic%2520content%253B%25202%2529%2520The%2520Persistent%250AStructure%2520Memory%2520%2528PSM%2529%2520compresses%2520and%2520preserves%2520long-term%2520spatial%2520information%252C%250Aensuring%2520global%2520consistency%2520and%2520drift-free%2520reconstruction%2520for%2520static%2520elements.%250ABy%2520alternating%2520queries%2520to%2520these%2520specialized%2520memories%252C%2520Mem4D%2520simultaneously%250Amaintains%2520static%2520geometry%2520with%2520global%2520consistency%2520and%2520reconstructs%2520dynamic%250Aelements%2520with%2520high%2520fidelity.%2520Experiments%2520on%2520challenging%2520benchmarks%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520state-of-the-art%2520or%2520competitive%2520performance%2520while%250Amaintaining%2520high%2520efficiency.%2520Codes%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mem4D%3A%20Decoupling%20Static%20and%20Dynamic%20Memory%20for%20Dynamic%20Scene%0A%20%20Reconstruction&entry.906535625=Xudong%20Cai%20and%20Shuo%20Wang%20and%20Peng%20Wang%20and%20Yongcai%20Wang%20and%20Zhaoxin%20Fan%20and%20Wanting%20Li%20and%20Tianbao%20Zhang%20and%20Jianrong%20Tao%20and%20Yeying%20Jin%20and%20Deying%20Li&entry.1292438233=%20%20Reconstructing%20dense%20geometry%20for%20dynamic%20scenes%20from%20a%20monocular%20video%20is%20a%0Acritical%20yet%20challenging%20task.%20Recent%20memory-based%20methods%20enable%20efficient%0Aonline%20reconstruction%2C%20but%20they%20fundamentally%20suffer%20from%20a%20Memory%20Demand%0ADilemma%3A%20The%20memory%20representation%20faces%20an%20inherent%20conflict%20between%20the%0Along-term%20stability%20required%20for%20static%20structures%20and%20the%20rapid%2C%20high-fidelity%0Adetail%20retention%20needed%20for%20dynamic%20motion.%20This%20conflict%20forces%20existing%0Amethods%20into%20a%20compromise%2C%20leading%20to%20either%20geometric%20drift%20in%20static%0Astructures%20or%20blurred%2C%20inaccurate%20reconstructions%20of%20dynamic%20objects.%20To%0Aaddress%20this%20dilemma%2C%20we%20propose%20Mem4D%2C%20a%20novel%20framework%20that%20decouples%20the%0Amodeling%20of%20static%20geometry%20and%20dynamic%20motion.%20Guided%20by%20this%20insight%2C%20we%0Adesign%20a%20dual-memory%20architecture%3A%201%29%20The%20Transient%20Dynamics%20Memory%20%28TDM%29%0Afocuses%20on%20capturing%20high-frequency%20motion%20details%20from%20recent%20frames%2C%20enabling%0Aaccurate%20and%20fine-grained%20modeling%20of%20dynamic%20content%3B%202%29%20The%20Persistent%0AStructure%20Memory%20%28PSM%29%20compresses%20and%20preserves%20long-term%20spatial%20information%2C%0Aensuring%20global%20consistency%20and%20drift-free%20reconstruction%20for%20static%20elements.%0ABy%20alternating%20queries%20to%20these%20specialized%20memories%2C%20Mem4D%20simultaneously%0Amaintains%20static%20geometry%20with%20global%20consistency%20and%20reconstructs%20dynamic%0Aelements%20with%20high%20fidelity.%20Experiments%20on%20challenging%20benchmarks%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20or%20competitive%20performance%20while%0Amaintaining%20high%20efficiency.%20Codes%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07908v1&entry.124074799=Read"},
{"title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating\n  Room with Multimodal Large Language Model", "author": "Peiqi He and Zhenhao Zhang and Yixiang Zhang and Xiongjun Zhao and Shaoliang Peng", "abstract": "  Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.\n", "link": "http://arxiv.org/abs/2508.08199v1", "date": "2025-08-11", "relevancy": 3.0385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-ORMLLM%3A%20Improve%20Spatial%20Relation%20Understanding%20in%20the%20Operating%0A%20%20Room%20with%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Spatial-ORMLLM%3A%20Improve%20Spatial%20Relation%20Understanding%20in%20the%20Operating%0A%20%20Room%20with%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Peiqi%20He%20and%20Zhenhao%20Zhang%20and%20Yixiang%20Zhang%20and%20Xiongjun%20Zhao%20and%20Shaoliang%20Peng%0AAbstract%3A%20%20%20Precise%20spatial%20modeling%20in%20the%20operating%20room%20%28OR%29%20is%20foundational%20to%20many%0Aclinical%20tasks%2C%20supporting%20intraoperative%20awareness%2C%20hazard%20avoidance%2C%20and%0Asurgical%20decision-making.%20While%20existing%20approaches%20leverage%20large-scale%0Amultimodal%20datasets%20for%20latent-space%20alignment%20to%20implicitly%20learn%20spatial%0Arelationships%2C%20they%20overlook%20the%203D%20capabilities%20of%20MLLMs.%20However%2C%20this%0Aapproach%20raises%20two%20issues%3A%20%281%29%20Operating%20rooms%20typically%20lack%20multiple%20video%0Aand%20audio%20sensors%2C%20making%20multimodal%203D%20data%20difficult%20to%20obtain%3B%20%282%29%20Training%0Asolely%20on%20readily%20available%202D%20data%20fails%20to%20capture%20fine-grained%20details%20in%0Acomplex%20scenes.%20To%20address%20this%20gap%2C%20we%20introduce%20Spatial-ORMLLM%2C%20the%20first%0Alarge%20vision-language%20model%20for%203D%20spatial%20reasoning%20in%20operating%20rooms%20using%0Aonly%20RGB%20modality%20to%20infer%20volumetric%20and%20semantic%20cues%2C%20enabling%20downstream%0Amedical%20tasks%20with%20detailed%20and%20holistic%20spatial%20context.%20Spatial-ORMLLM%0Aincorporates%20a%20Spatial-Enhanced%20Feature%20Fusion%20Block%2C%20which%20integrates%202D%0Amodality%20inputs%20with%20rich%203D%20spatial%20knowledge%20extracted%20by%20the%20estimation%0Aalgorithm%20and%20then%20feeds%20the%20combined%20features%20into%20the%20visual%20tower.%20By%0Aemploying%20a%20unified%20end-to-end%20MLLM%20framework%2C%20it%20combines%20powerful%20spatial%0Afeatures%20with%20textual%20features%20to%20deliver%20robust%203D%20scene%20reasoning%20without%20any%0Aadditional%20expert%20annotations%20or%20sensor%20inputs.%20Experiments%20on%20multiple%0Abenchmark%20clinical%20datasets%20demonstrate%20that%20Spatial-ORMLLM%20achieves%0Astate-of-the-art%20performance%20and%20generalizes%20robustly%20to%20previously%20unseen%0Asurgical%20scenarios%20and%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-ORMLLM%253A%2520Improve%2520Spatial%2520Relation%2520Understanding%2520in%2520the%2520Operating%250A%2520%2520Room%2520with%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DPeiqi%2520He%2520and%2520Zhenhao%2520Zhang%2520and%2520Yixiang%2520Zhang%2520and%2520Xiongjun%2520Zhao%2520and%2520Shaoliang%2520Peng%26entry.1292438233%3D%2520%2520Precise%2520spatial%2520modeling%2520in%2520the%2520operating%2520room%2520%2528OR%2529%2520is%2520foundational%2520to%2520many%250Aclinical%2520tasks%252C%2520supporting%2520intraoperative%2520awareness%252C%2520hazard%2520avoidance%252C%2520and%250Asurgical%2520decision-making.%2520While%2520existing%2520approaches%2520leverage%2520large-scale%250Amultimodal%2520datasets%2520for%2520latent-space%2520alignment%2520to%2520implicitly%2520learn%2520spatial%250Arelationships%252C%2520they%2520overlook%2520the%25203D%2520capabilities%2520of%2520MLLMs.%2520However%252C%2520this%250Aapproach%2520raises%2520two%2520issues%253A%2520%25281%2529%2520Operating%2520rooms%2520typically%2520lack%2520multiple%2520video%250Aand%2520audio%2520sensors%252C%2520making%2520multimodal%25203D%2520data%2520difficult%2520to%2520obtain%253B%2520%25282%2529%2520Training%250Asolely%2520on%2520readily%2520available%25202D%2520data%2520fails%2520to%2520capture%2520fine-grained%2520details%2520in%250Acomplex%2520scenes.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Spatial-ORMLLM%252C%2520the%2520first%250Alarge%2520vision-language%2520model%2520for%25203D%2520spatial%2520reasoning%2520in%2520operating%2520rooms%2520using%250Aonly%2520RGB%2520modality%2520to%2520infer%2520volumetric%2520and%2520semantic%2520cues%252C%2520enabling%2520downstream%250Amedical%2520tasks%2520with%2520detailed%2520and%2520holistic%2520spatial%2520context.%2520Spatial-ORMLLM%250Aincorporates%2520a%2520Spatial-Enhanced%2520Feature%2520Fusion%2520Block%252C%2520which%2520integrates%25202D%250Amodality%2520inputs%2520with%2520rich%25203D%2520spatial%2520knowledge%2520extracted%2520by%2520the%2520estimation%250Aalgorithm%2520and%2520then%2520feeds%2520the%2520combined%2520features%2520into%2520the%2520visual%2520tower.%2520By%250Aemploying%2520a%2520unified%2520end-to-end%2520MLLM%2520framework%252C%2520it%2520combines%2520powerful%2520spatial%250Afeatures%2520with%2520textual%2520features%2520to%2520deliver%2520robust%25203D%2520scene%2520reasoning%2520without%2520any%250Aadditional%2520expert%2520annotations%2520or%2520sensor%2520inputs.%2520Experiments%2520on%2520multiple%250Abenchmark%2520clinical%2520datasets%2520demonstrate%2520that%2520Spatial-ORMLLM%2520achieves%250Astate-of-the-art%2520performance%2520and%2520generalizes%2520robustly%2520to%2520previously%2520unseen%250Asurgical%2520scenarios%2520and%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-ORMLLM%3A%20Improve%20Spatial%20Relation%20Understanding%20in%20the%20Operating%0A%20%20Room%20with%20Multimodal%20Large%20Language%20Model&entry.906535625=Peiqi%20He%20and%20Zhenhao%20Zhang%20and%20Yixiang%20Zhang%20and%20Xiongjun%20Zhao%20and%20Shaoliang%20Peng&entry.1292438233=%20%20Precise%20spatial%20modeling%20in%20the%20operating%20room%20%28OR%29%20is%20foundational%20to%20many%0Aclinical%20tasks%2C%20supporting%20intraoperative%20awareness%2C%20hazard%20avoidance%2C%20and%0Asurgical%20decision-making.%20While%20existing%20approaches%20leverage%20large-scale%0Amultimodal%20datasets%20for%20latent-space%20alignment%20to%20implicitly%20learn%20spatial%0Arelationships%2C%20they%20overlook%20the%203D%20capabilities%20of%20MLLMs.%20However%2C%20this%0Aapproach%20raises%20two%20issues%3A%20%281%29%20Operating%20rooms%20typically%20lack%20multiple%20video%0Aand%20audio%20sensors%2C%20making%20multimodal%203D%20data%20difficult%20to%20obtain%3B%20%282%29%20Training%0Asolely%20on%20readily%20available%202D%20data%20fails%20to%20capture%20fine-grained%20details%20in%0Acomplex%20scenes.%20To%20address%20this%20gap%2C%20we%20introduce%20Spatial-ORMLLM%2C%20the%20first%0Alarge%20vision-language%20model%20for%203D%20spatial%20reasoning%20in%20operating%20rooms%20using%0Aonly%20RGB%20modality%20to%20infer%20volumetric%20and%20semantic%20cues%2C%20enabling%20downstream%0Amedical%20tasks%20with%20detailed%20and%20holistic%20spatial%20context.%20Spatial-ORMLLM%0Aincorporates%20a%20Spatial-Enhanced%20Feature%20Fusion%20Block%2C%20which%20integrates%202D%0Amodality%20inputs%20with%20rich%203D%20spatial%20knowledge%20extracted%20by%20the%20estimation%0Aalgorithm%20and%20then%20feeds%20the%20combined%20features%20into%20the%20visual%20tower.%20By%0Aemploying%20a%20unified%20end-to-end%20MLLM%20framework%2C%20it%20combines%20powerful%20spatial%0Afeatures%20with%20textual%20features%20to%20deliver%20robust%203D%20scene%20reasoning%20without%20any%0Aadditional%20expert%20annotations%20or%20sensor%20inputs.%20Experiments%20on%20multiple%0Abenchmark%20clinical%20datasets%20demonstrate%20that%20Spatial-ORMLLM%20achieves%0Astate-of-the-art%20performance%20and%20generalizes%20robustly%20to%20previously%20unseen%0Asurgical%20scenarios%20and%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08199v1&entry.124074799=Read"},
{"title": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems", "author": "Qihao Yuan and Kailai Li and Jiaming Zhang", "abstract": "  3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods utilize large language models (LLMs) to handle natural language\ndescriptions, where the LLM either produces grounding results directly or\ngenerates programs that compute results (symbolically). In this work, we\npropose a zero-shot method that reformulates the 3DVG task as a Constraint\nSatisfaction Problem (CSP), where the variables and constraints represent\nobjects and their spatial relations, respectively. This allows a global\nsymbolic reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is available\nat https://asig-x.github.io/csvg_web.\n", "link": "http://arxiv.org/abs/2411.14594v2", "date": "2025-08-11", "relevancy": 2.9725, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6143}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Zero-Shot%203D%20Visual%20Grounding%20as%20Constraint%20Satisfaction%0A%20%20Problems&body=Title%3A%20Solving%20Zero-Shot%203D%20Visual%20Grounding%20as%20Constraint%20Satisfaction%0A%20%20Problems%0AAuthor%3A%20Qihao%20Yuan%20and%20Kailai%20Li%20and%20Jiaming%20Zhang%0AAbstract%3A%20%20%203D%20visual%20grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%20a%203D%20scene%20with%20natural%0Alanguage%20descriptions.%20Supervised%20methods%20have%20achieved%20decent%20accuracy%2C%20but%0Ahave%20a%20closed%20vocabulary%20and%20limited%20language%20understanding%20ability.%20Zero-shot%0Amethods%20utilize%20large%20language%20models%20%28LLMs%29%20to%20handle%20natural%20language%0Adescriptions%2C%20where%20the%20LLM%20either%20produces%20grounding%20results%20directly%20or%0Agenerates%20programs%20that%20compute%20results%20%28symbolically%29.%20In%20this%20work%2C%20we%0Apropose%20a%20zero-shot%20method%20that%20reformulates%20the%203DVG%20task%20as%20a%20Constraint%0ASatisfaction%20Problem%20%28CSP%29%2C%20where%20the%20variables%20and%20constraints%20represent%0Aobjects%20and%20their%20spatial%20relations%2C%20respectively.%20This%20allows%20a%20global%0Asymbolic%20reasoning%20of%20all%20relevant%20objects%2C%20producing%20grounding%20results%20of%20both%0Athe%20target%20and%20anchor%20objects.%20Moreover%2C%20we%20demonstrate%20the%20flexibility%20of%20our%0Aframework%20by%20handling%20negation-%20and%20counting-based%20queries%20with%20only%20minor%0Aextra%20coding%20efforts.%20Our%20system%2C%20Constraint%20Satisfaction%20Visual%20Grounding%0A%28CSVG%29%2C%20has%20been%20extensively%20evaluated%20on%20the%20public%20datasets%20ScanRefer%20and%0ANr3D%20datasets%20using%20only%20open-source%20LLMs.%20Results%20show%20the%20effectiveness%20of%0ACSVG%20and%20superior%20grounding%20accuracy%20over%20current%20state-of-the-art%20zero-shot%0A3DVG%20methods%20with%20improvements%20of%20%24%2B7.0%5C%25%24%20%28Acc%400.5%20score%29%20and%20%24%2B11.2%5C%25%24%20on%20the%0AScanRefer%20and%20Nr3D%20datasets%2C%20respectively.%20The%20code%20of%20our%20system%20is%20available%0Aat%20https%3A//asig-x.github.io/csvg_web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14594v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Zero-Shot%25203D%2520Visual%2520Grounding%2520as%2520Constraint%2520Satisfaction%250A%2520%2520Problems%26entry.906535625%3DQihao%2520Yuan%2520and%2520Kailai%2520Li%2520and%2520Jiaming%2520Zhang%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520%25283DVG%2529%2520aims%2520to%2520locate%2520objects%2520in%2520a%25203D%2520scene%2520with%2520natural%250Alanguage%2520descriptions.%2520Supervised%2520methods%2520have%2520achieved%2520decent%2520accuracy%252C%2520but%250Ahave%2520a%2520closed%2520vocabulary%2520and%2520limited%2520language%2520understanding%2520ability.%2520Zero-shot%250Amethods%2520utilize%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520handle%2520natural%2520language%250Adescriptions%252C%2520where%2520the%2520LLM%2520either%2520produces%2520grounding%2520results%2520directly%2520or%250Agenerates%2520programs%2520that%2520compute%2520results%2520%2528symbolically%2529.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520zero-shot%2520method%2520that%2520reformulates%2520the%25203DVG%2520task%2520as%2520a%2520Constraint%250ASatisfaction%2520Problem%2520%2528CSP%2529%252C%2520where%2520the%2520variables%2520and%2520constraints%2520represent%250Aobjects%2520and%2520their%2520spatial%2520relations%252C%2520respectively.%2520This%2520allows%2520a%2520global%250Asymbolic%2520reasoning%2520of%2520all%2520relevant%2520objects%252C%2520producing%2520grounding%2520results%2520of%2520both%250Athe%2520target%2520and%2520anchor%2520objects.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520flexibility%2520of%2520our%250Aframework%2520by%2520handling%2520negation-%2520and%2520counting-based%2520queries%2520with%2520only%2520minor%250Aextra%2520coding%2520efforts.%2520Our%2520system%252C%2520Constraint%2520Satisfaction%2520Visual%2520Grounding%250A%2528CSVG%2529%252C%2520has%2520been%2520extensively%2520evaluated%2520on%2520the%2520public%2520datasets%2520ScanRefer%2520and%250ANr3D%2520datasets%2520using%2520only%2520open-source%2520LLMs.%2520Results%2520show%2520the%2520effectiveness%2520of%250ACSVG%2520and%2520superior%2520grounding%2520accuracy%2520over%2520current%2520state-of-the-art%2520zero-shot%250A3DVG%2520methods%2520with%2520improvements%2520of%2520%2524%252B7.0%255C%2525%2524%2520%2528Acc%25400.5%2520score%2529%2520and%2520%2524%252B11.2%255C%2525%2524%2520on%2520the%250AScanRefer%2520and%2520Nr3D%2520datasets%252C%2520respectively.%2520The%2520code%2520of%2520our%2520system%2520is%2520available%250Aat%2520https%253A//asig-x.github.io/csvg_web.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14594v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Zero-Shot%203D%20Visual%20Grounding%20as%20Constraint%20Satisfaction%0A%20%20Problems&entry.906535625=Qihao%20Yuan%20and%20Kailai%20Li%20and%20Jiaming%20Zhang&entry.1292438233=%20%203D%20visual%20grounding%20%283DVG%29%20aims%20to%20locate%20objects%20in%20a%203D%20scene%20with%20natural%0Alanguage%20descriptions.%20Supervised%20methods%20have%20achieved%20decent%20accuracy%2C%20but%0Ahave%20a%20closed%20vocabulary%20and%20limited%20language%20understanding%20ability.%20Zero-shot%0Amethods%20utilize%20large%20language%20models%20%28LLMs%29%20to%20handle%20natural%20language%0Adescriptions%2C%20where%20the%20LLM%20either%20produces%20grounding%20results%20directly%20or%0Agenerates%20programs%20that%20compute%20results%20%28symbolically%29.%20In%20this%20work%2C%20we%0Apropose%20a%20zero-shot%20method%20that%20reformulates%20the%203DVG%20task%20as%20a%20Constraint%0ASatisfaction%20Problem%20%28CSP%29%2C%20where%20the%20variables%20and%20constraints%20represent%0Aobjects%20and%20their%20spatial%20relations%2C%20respectively.%20This%20allows%20a%20global%0Asymbolic%20reasoning%20of%20all%20relevant%20objects%2C%20producing%20grounding%20results%20of%20both%0Athe%20target%20and%20anchor%20objects.%20Moreover%2C%20we%20demonstrate%20the%20flexibility%20of%20our%0Aframework%20by%20handling%20negation-%20and%20counting-based%20queries%20with%20only%20minor%0Aextra%20coding%20efforts.%20Our%20system%2C%20Constraint%20Satisfaction%20Visual%20Grounding%0A%28CSVG%29%2C%20has%20been%20extensively%20evaluated%20on%20the%20public%20datasets%20ScanRefer%20and%0ANr3D%20datasets%20using%20only%20open-source%20LLMs.%20Results%20show%20the%20effectiveness%20of%0ACSVG%20and%20superior%20grounding%20accuracy%20over%20current%20state-of-the-art%20zero-shot%0A3DVG%20methods%20with%20improvements%20of%20%24%2B7.0%5C%25%24%20%28Acc%400.5%20score%29%20and%20%24%2B11.2%5C%25%24%20on%20the%0AScanRefer%20and%20Nr3D%20datasets%2C%20respectively.%20The%20code%20of%20our%20system%20is%20available%0Aat%20https%3A//asig-x.github.io/csvg_web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14594v2&entry.124074799=Read"},
{"title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding", "author": "Xiaoxing Hu and Kaicheng Yang and Jun Wang and Haoran Xu and Ziyong Feng and Yupei Wang", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA\n", "link": "http://arxiv.org/abs/2504.16801v2", "date": "2025-08-11", "relevancy": 2.9549, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.598}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5965}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding&body=Title%3A%20Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding%0AAuthor%3A%20Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Jun%20Wang%20and%20Haoran%20Xu%20and%20Ziyong%20Feng%20and%20Yupei%20Wang%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20success%20on%0Amultiple%20downstream%20tasks%20by%20aligning%20image%20and%20text%20modalities.%20However%2C%20the%0Anature%20of%20global%20contrastive%20learning%20limits%20CLIP%27s%20ability%20to%20comprehend%0Acompositional%20concepts%2C%20such%20as%20relations%20and%20attributes.%20Although%20recent%0Astudies%20employ%20global%20hard%20negative%20samples%20to%20improve%20compositional%0Aunderstanding%2C%20these%20methods%20significantly%20compromise%20the%20model%27s%20inherent%0Ageneral%20capabilities%20by%20forcibly%20distancing%20textual%20negative%20samples%20from%0Aimages%20in%20the%20embedding%20space.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0ADecoupled%20Global-Local%20Alignment%20%28DeGLA%29%20framework%20that%20improves%20compositional%0Aunderstanding%20while%20substantially%20mitigating%20losses%20in%20general%20capabilities.%20To%0Aoptimize%20the%20retention%20of%20the%20model%27s%20inherent%20capabilities%2C%20we%20incorporate%20a%0Aself-distillation%20mechanism%20within%20the%20global%20alignment%20process%2C%20aligning%20the%0Alearnable%20image-text%20encoder%20with%20a%20frozen%20teacher%20model%20derived%20from%20an%0Aexponential%20moving%20average.%20Under%20the%20constraint%20of%20self-distillation%2C%20it%0Aeffectively%20mitigates%20the%20catastrophic%20forgetting%20of%20pretrained%20knowledge%0Aduring%20fine-tuning.%20To%20improve%20compositional%20understanding%2C%20we%20first%20leverage%0Athe%20in-context%20learning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20construct%0Aabout%202M%20high-quality%20negative%20captions%20across%20five%20types.%20Subsequently%2C%20we%0Apropose%20the%20Image-Grounded%20Contrast%20%28IGC%29%20loss%20and%20Text-Grounded%20Contrast%20%28TGC%29%0Aloss%20to%20enhance%20vision-language%20compositionally.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20DeGLA%20framework.%20Compared%20to%20previous%0Astate-of-the-art%20methods%2C%20DeGLA%20achieves%20an%20average%20enhancement%20of%203.5%25%20across%0Athe%20VALSE%2C%20SugarCrepe%2C%20and%20ARO%20benchmarks.%20Concurrently%2C%20it%20obtains%20an%20average%0Aperformance%20improvement%20of%2013.0%25%20on%20zero-shot%20classification%20tasks%20across%0Aeleven%20datasets.%20Our%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/xiaoxing2001/DeGLA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Global-Local%2520Alignment%2520for%2520Improving%2520Compositional%250A%2520%2520Understanding%26entry.906535625%3DXiaoxing%2520Hu%2520and%2520Kaicheng%2520Yang%2520and%2520Jun%2520Wang%2520and%2520Haoran%2520Xu%2520and%2520Ziyong%2520Feng%2520and%2520Yupei%2520Wang%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520achieved%2520success%2520on%250Amultiple%2520downstream%2520tasks%2520by%2520aligning%2520image%2520and%2520text%2520modalities.%2520However%252C%2520the%250Anature%2520of%2520global%2520contrastive%2520learning%2520limits%2520CLIP%2527s%2520ability%2520to%2520comprehend%250Acompositional%2520concepts%252C%2520such%2520as%2520relations%2520and%2520attributes.%2520Although%2520recent%250Astudies%2520employ%2520global%2520hard%2520negative%2520samples%2520to%2520improve%2520compositional%250Aunderstanding%252C%2520these%2520methods%2520significantly%2520compromise%2520the%2520model%2527s%2520inherent%250Ageneral%2520capabilities%2520by%2520forcibly%2520distancing%2520textual%2520negative%2520samples%2520from%250Aimages%2520in%2520the%2520embedding%2520space.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%250ADecoupled%2520Global-Local%2520Alignment%2520%2528DeGLA%2529%2520framework%2520that%2520improves%2520compositional%250Aunderstanding%2520while%2520substantially%2520mitigating%2520losses%2520in%2520general%2520capabilities.%2520To%250Aoptimize%2520the%2520retention%2520of%2520the%2520model%2527s%2520inherent%2520capabilities%252C%2520we%2520incorporate%2520a%250Aself-distillation%2520mechanism%2520within%2520the%2520global%2520alignment%2520process%252C%2520aligning%2520the%250Alearnable%2520image-text%2520encoder%2520with%2520a%2520frozen%2520teacher%2520model%2520derived%2520from%2520an%250Aexponential%2520moving%2520average.%2520Under%2520the%2520constraint%2520of%2520self-distillation%252C%2520it%250Aeffectively%2520mitigates%2520the%2520catastrophic%2520forgetting%2520of%2520pretrained%2520knowledge%250Aduring%2520fine-tuning.%2520To%2520improve%2520compositional%2520understanding%252C%2520we%2520first%2520leverage%250Athe%2520in-context%2520learning%2520capability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520construct%250Aabout%25202M%2520high-quality%2520negative%2520captions%2520across%2520five%2520types.%2520Subsequently%252C%2520we%250Apropose%2520the%2520Image-Grounded%2520Contrast%2520%2528IGC%2529%2520loss%2520and%2520Text-Grounded%2520Contrast%2520%2528TGC%2529%250Aloss%2520to%2520enhance%2520vision-language%2520compositionally.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520DeGLA%2520framework.%2520Compared%2520to%2520previous%250Astate-of-the-art%2520methods%252C%2520DeGLA%2520achieves%2520an%2520average%2520enhancement%2520of%25203.5%2525%2520across%250Athe%2520VALSE%252C%2520SugarCrepe%252C%2520and%2520ARO%2520benchmarks.%2520Concurrently%252C%2520it%2520obtains%2520an%2520average%250Aperformance%2520improvement%2520of%252013.0%2525%2520on%2520zero-shot%2520classification%2520tasks%2520across%250Aeleven%2520datasets.%2520Our%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/xiaoxing2001/DeGLA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Global-Local%20Alignment%20for%20Improving%20Compositional%0A%20%20Understanding&entry.906535625=Xiaoxing%20Hu%20and%20Kaicheng%20Yang%20and%20Jun%20Wang%20and%20Haoran%20Xu%20and%20Ziyong%20Feng%20and%20Yupei%20Wang&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20success%20on%0Amultiple%20downstream%20tasks%20by%20aligning%20image%20and%20text%20modalities.%20However%2C%20the%0Anature%20of%20global%20contrastive%20learning%20limits%20CLIP%27s%20ability%20to%20comprehend%0Acompositional%20concepts%2C%20such%20as%20relations%20and%20attributes.%20Although%20recent%0Astudies%20employ%20global%20hard%20negative%20samples%20to%20improve%20compositional%0Aunderstanding%2C%20these%20methods%20significantly%20compromise%20the%20model%27s%20inherent%0Ageneral%20capabilities%20by%20forcibly%20distancing%20textual%20negative%20samples%20from%0Aimages%20in%20the%20embedding%20space.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%0ADecoupled%20Global-Local%20Alignment%20%28DeGLA%29%20framework%20that%20improves%20compositional%0Aunderstanding%20while%20substantially%20mitigating%20losses%20in%20general%20capabilities.%20To%0Aoptimize%20the%20retention%20of%20the%20model%27s%20inherent%20capabilities%2C%20we%20incorporate%20a%0Aself-distillation%20mechanism%20within%20the%20global%20alignment%20process%2C%20aligning%20the%0Alearnable%20image-text%20encoder%20with%20a%20frozen%20teacher%20model%20derived%20from%20an%0Aexponential%20moving%20average.%20Under%20the%20constraint%20of%20self-distillation%2C%20it%0Aeffectively%20mitigates%20the%20catastrophic%20forgetting%20of%20pretrained%20knowledge%0Aduring%20fine-tuning.%20To%20improve%20compositional%20understanding%2C%20we%20first%20leverage%0Athe%20in-context%20learning%20capability%20of%20Large%20Language%20Models%20%28LLMs%29%20to%20construct%0Aabout%202M%20high-quality%20negative%20captions%20across%20five%20types.%20Subsequently%2C%20we%0Apropose%20the%20Image-Grounded%20Contrast%20%28IGC%29%20loss%20and%20Text-Grounded%20Contrast%20%28TGC%29%0Aloss%20to%20enhance%20vision-language%20compositionally.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20DeGLA%20framework.%20Compared%20to%20previous%0Astate-of-the-art%20methods%2C%20DeGLA%20achieves%20an%20average%20enhancement%20of%203.5%25%20across%0Athe%20VALSE%2C%20SugarCrepe%2C%20and%20ARO%20benchmarks.%20Concurrently%2C%20it%20obtains%20an%20average%0Aperformance%20improvement%20of%2013.0%25%20on%20zero-shot%20classification%20tasks%20across%0Aeleven%20datasets.%20Our%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/xiaoxing2001/DeGLA%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16801v2&entry.124074799=Read"},
{"title": "VGGSounder: Audio-Visual Evaluations for Foundation Models", "author": "Daniil Zverev and Thadd\u00e4us Wiedemer and Ameya Prabhu and Matthias Bethge and Wieland Brendel and A. Sophia Koepke", "abstract": "  The emergence of audio-visual foundation models underscores the importance of\nreliably assessing their multi-modal understanding. The VGGSounder dataset is\ncommonly used as a benchmark for evaluation audio-visual classification.\nHowever, our analysis identifies several limitations of VGGSounder, including\nincomplete labelling, partially overlapping classes, and misaligned modalities.\nThese lead to distorted evaluations of auditory and visual capabilities. To\naddress these limitations, we introduce VGGSounder, a comprehensively\nre-annotated, multi-label test set that extends VGGSound and is specifically\ndesigned to evaluate audio-visual foundation models. VGGSounder features\ndetailed modality annotations, enabling precise analyses of modality-specific\nperformance. Furthermore, we reveal model limitations by analysing performance\ndegradation when adding another input modality with our new modality confusion\nmetric.\n", "link": "http://arxiv.org/abs/2508.08237v1", "date": "2025-08-11", "relevancy": 2.9499, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGSounder%3A%20Audio-Visual%20Evaluations%20for%20Foundation%20Models&body=Title%3A%20VGGSounder%3A%20Audio-Visual%20Evaluations%20for%20Foundation%20Models%0AAuthor%3A%20Daniil%20Zverev%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Ameya%20Prabhu%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel%20and%20A.%20Sophia%20Koepke%0AAbstract%3A%20%20%20The%20emergence%20of%20audio-visual%20foundation%20models%20underscores%20the%20importance%20of%0Areliably%20assessing%20their%20multi-modal%20understanding.%20The%20VGGSounder%20dataset%20is%0Acommonly%20used%20as%20a%20benchmark%20for%20evaluation%20audio-visual%20classification.%0AHowever%2C%20our%20analysis%20identifies%20several%20limitations%20of%20VGGSounder%2C%20including%0Aincomplete%20labelling%2C%20partially%20overlapping%20classes%2C%20and%20misaligned%20modalities.%0AThese%20lead%20to%20distorted%20evaluations%20of%20auditory%20and%20visual%20capabilities.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20VGGSounder%2C%20a%20comprehensively%0Are-annotated%2C%20multi-label%20test%20set%20that%20extends%20VGGSound%20and%20is%20specifically%0Adesigned%20to%20evaluate%20audio-visual%20foundation%20models.%20VGGSounder%20features%0Adetailed%20modality%20annotations%2C%20enabling%20precise%20analyses%20of%20modality-specific%0Aperformance.%20Furthermore%2C%20we%20reveal%20model%20limitations%20by%20analysing%20performance%0Adegradation%20when%20adding%20another%20input%20modality%20with%20our%20new%20modality%20confusion%0Ametric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGSounder%253A%2520Audio-Visual%2520Evaluations%2520for%2520Foundation%2520Models%26entry.906535625%3DDaniil%2520Zverev%2520and%2520Thadd%25C3%25A4us%2520Wiedemer%2520and%2520Ameya%2520Prabhu%2520and%2520Matthias%2520Bethge%2520and%2520Wieland%2520Brendel%2520and%2520A.%2520Sophia%2520Koepke%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520audio-visual%2520foundation%2520models%2520underscores%2520the%2520importance%2520of%250Areliably%2520assessing%2520their%2520multi-modal%2520understanding.%2520The%2520VGGSounder%2520dataset%2520is%250Acommonly%2520used%2520as%2520a%2520benchmark%2520for%2520evaluation%2520audio-visual%2520classification.%250AHowever%252C%2520our%2520analysis%2520identifies%2520several%2520limitations%2520of%2520VGGSounder%252C%2520including%250Aincomplete%2520labelling%252C%2520partially%2520overlapping%2520classes%252C%2520and%2520misaligned%2520modalities.%250AThese%2520lead%2520to%2520distorted%2520evaluations%2520of%2520auditory%2520and%2520visual%2520capabilities.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520VGGSounder%252C%2520a%2520comprehensively%250Are-annotated%252C%2520multi-label%2520test%2520set%2520that%2520extends%2520VGGSound%2520and%2520is%2520specifically%250Adesigned%2520to%2520evaluate%2520audio-visual%2520foundation%2520models.%2520VGGSounder%2520features%250Adetailed%2520modality%2520annotations%252C%2520enabling%2520precise%2520analyses%2520of%2520modality-specific%250Aperformance.%2520Furthermore%252C%2520we%2520reveal%2520model%2520limitations%2520by%2520analysing%2520performance%250Adegradation%2520when%2520adding%2520another%2520input%2520modality%2520with%2520our%2520new%2520modality%2520confusion%250Ametric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGSounder%3A%20Audio-Visual%20Evaluations%20for%20Foundation%20Models&entry.906535625=Daniil%20Zverev%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Ameya%20Prabhu%20and%20Matthias%20Bethge%20and%20Wieland%20Brendel%20and%20A.%20Sophia%20Koepke&entry.1292438233=%20%20The%20emergence%20of%20audio-visual%20foundation%20models%20underscores%20the%20importance%20of%0Areliably%20assessing%20their%20multi-modal%20understanding.%20The%20VGGSounder%20dataset%20is%0Acommonly%20used%20as%20a%20benchmark%20for%20evaluation%20audio-visual%20classification.%0AHowever%2C%20our%20analysis%20identifies%20several%20limitations%20of%20VGGSounder%2C%20including%0Aincomplete%20labelling%2C%20partially%20overlapping%20classes%2C%20and%20misaligned%20modalities.%0AThese%20lead%20to%20distorted%20evaluations%20of%20auditory%20and%20visual%20capabilities.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20VGGSounder%2C%20a%20comprehensively%0Are-annotated%2C%20multi-label%20test%20set%20that%20extends%20VGGSound%20and%20is%20specifically%0Adesigned%20to%20evaluate%20audio-visual%20foundation%20models.%20VGGSounder%20features%0Adetailed%20modality%20annotations%2C%20enabling%20precise%20analyses%20of%20modality-specific%0Aperformance.%20Furthermore%2C%20we%20reveal%20model%20limitations%20by%20analysing%20performance%0Adegradation%20when%20adding%20another%20input%20modality%20with%20our%20new%20modality%20confusion%0Ametric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08237v1&entry.124074799=Read"},
{"title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback", "author": "Chenhan Jiang and Yihan Zeng and Dit-Yan Yeung", "abstract": "  Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks.\n", "link": "http://arxiv.org/abs/2504.19860v2", "date": "2025-08-11", "relevancy": 2.9378, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback&body=Title%3A%20CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback%0AAuthor%3A%20Chenhan%20Jiang%20and%20Yihan%20Zeng%20and%20Dit-Yan%20Yeung%0AAbstract%3A%20%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20achieved%20remarkable%20success%20in%0Atext-to-3D%20content%20generation.%20However%2C%20SDS-based%20methods%20struggle%20to%20maintain%0Asemantic%20fidelity%20for%20user%20prompts%2C%20particularly%20when%20involving%20multiple%0Aobjects%20with%20intricate%20interactions.%20While%20existing%20approaches%20often%20address%203D%0Aconsistency%20through%20multiview%20diffusion%20model%20fine-tuning%20on%203D%20datasets%2C%20this%0Astrategy%20inadvertently%20exacerbates%20text-3D%20alignment%20degradation.%20The%0Alimitation%20stems%20from%20SDS%27s%20inherent%20accumulation%20of%20view-independent%20biases%0Aduring%20optimization%2C%20which%20progressively%20diverges%20from%20the%20ideal%20text%20alignment%0Adirection.%20To%20alleviate%20this%20limitation%2C%20we%20propose%20a%20novel%20SDS%20objective%2C%0Adubbed%20as%20Textual%20Coherent%20Score%20Distillation%20%28TCSD%29%2C%20which%20integrates%0Aalignment%20feedback%20from%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20TCSD%0Aleverages%20cross-modal%20understanding%20capabilities%20of%20MLLMs%20to%20assess%20and%20guide%0Athe%20text-3D%20correspondence%20during%20the%20optimization.%20We%20further%20develop%0A3DLLaVA-CRITIC%20-%20a%20fine-tuned%20MLLM%20specialized%20for%20evaluating%20multiview%20text%0Aalignment%20in%203D%20generations.%20Additionally%2C%20we%20introduce%20an%20LLM-layout%0Ainitialization%20that%20significantly%20accelerates%20optimization%20convergence%20through%0Asemantic-aware%20spatial%20configuration.%20Our%20framework%2C%20CoherenDream%2C%20achieves%0Aconsistent%20improvement%20across%20multiple%20metrics%20on%20TIFA%20subset.As%20the%20first%0Astudy%20to%20incorporate%20MLLMs%20into%20SDS%20optimization%2C%20we%20also%20conduct%20extensive%0Aablation%20studies%20to%20explore%20optimal%20MLLM%20adaptations%20for%203D%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19860v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoherenDream%253A%2520Boosting%2520Holistic%2520Text%2520Coherence%2520in%25203D%2520Generation%2520via%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%2520Feedback%26entry.906535625%3DChenhan%2520Jiang%2520and%2520Yihan%2520Zeng%2520and%2520Dit-Yan%2520Yeung%26entry.1292438233%3D%2520%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%2520has%2520achieved%2520remarkable%2520success%2520in%250Atext-to-3D%2520content%2520generation.%2520However%252C%2520SDS-based%2520methods%2520struggle%2520to%2520maintain%250Asemantic%2520fidelity%2520for%2520user%2520prompts%252C%2520particularly%2520when%2520involving%2520multiple%250Aobjects%2520with%2520intricate%2520interactions.%2520While%2520existing%2520approaches%2520often%2520address%25203D%250Aconsistency%2520through%2520multiview%2520diffusion%2520model%2520fine-tuning%2520on%25203D%2520datasets%252C%2520this%250Astrategy%2520inadvertently%2520exacerbates%2520text-3D%2520alignment%2520degradation.%2520The%250Alimitation%2520stems%2520from%2520SDS%2527s%2520inherent%2520accumulation%2520of%2520view-independent%2520biases%250Aduring%2520optimization%252C%2520which%2520progressively%2520diverges%2520from%2520the%2520ideal%2520text%2520alignment%250Adirection.%2520To%2520alleviate%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520SDS%2520objective%252C%250Adubbed%2520as%2520Textual%2520Coherent%2520Score%2520Distillation%2520%2528TCSD%2529%252C%2520which%2520integrates%250Aalignment%2520feedback%2520from%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Our%2520TCSD%250Aleverages%2520cross-modal%2520understanding%2520capabilities%2520of%2520MLLMs%2520to%2520assess%2520and%2520guide%250Athe%2520text-3D%2520correspondence%2520during%2520the%2520optimization.%2520We%2520further%2520develop%250A3DLLaVA-CRITIC%2520-%2520a%2520fine-tuned%2520MLLM%2520specialized%2520for%2520evaluating%2520multiview%2520text%250Aalignment%2520in%25203D%2520generations.%2520Additionally%252C%2520we%2520introduce%2520an%2520LLM-layout%250Ainitialization%2520that%2520significantly%2520accelerates%2520optimization%2520convergence%2520through%250Asemantic-aware%2520spatial%2520configuration.%2520Our%2520framework%252C%2520CoherenDream%252C%2520achieves%250Aconsistent%2520improvement%2520across%2520multiple%2520metrics%2520on%2520TIFA%2520subset.As%2520the%2520first%250Astudy%2520to%2520incorporate%2520MLLMs%2520into%2520SDS%2520optimization%252C%2520we%2520also%2520conduct%2520extensive%250Aablation%2520studies%2520to%2520explore%2520optimal%2520MLLM%2520adaptations%2520for%25203D%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19860v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoherenDream%3A%20Boosting%20Holistic%20Text%20Coherence%20in%203D%20Generation%20via%0A%20%20Multimodal%20Large%20Language%20Models%20Feedback&entry.906535625=Chenhan%20Jiang%20and%20Yihan%20Zeng%20and%20Dit-Yan%20Yeung&entry.1292438233=%20%20Score%20Distillation%20Sampling%20%28SDS%29%20has%20achieved%20remarkable%20success%20in%0Atext-to-3D%20content%20generation.%20However%2C%20SDS-based%20methods%20struggle%20to%20maintain%0Asemantic%20fidelity%20for%20user%20prompts%2C%20particularly%20when%20involving%20multiple%0Aobjects%20with%20intricate%20interactions.%20While%20existing%20approaches%20often%20address%203D%0Aconsistency%20through%20multiview%20diffusion%20model%20fine-tuning%20on%203D%20datasets%2C%20this%0Astrategy%20inadvertently%20exacerbates%20text-3D%20alignment%20degradation.%20The%0Alimitation%20stems%20from%20SDS%27s%20inherent%20accumulation%20of%20view-independent%20biases%0Aduring%20optimization%2C%20which%20progressively%20diverges%20from%20the%20ideal%20text%20alignment%0Adirection.%20To%20alleviate%20this%20limitation%2C%20we%20propose%20a%20novel%20SDS%20objective%2C%0Adubbed%20as%20Textual%20Coherent%20Score%20Distillation%20%28TCSD%29%2C%20which%20integrates%0Aalignment%20feedback%20from%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Our%20TCSD%0Aleverages%20cross-modal%20understanding%20capabilities%20of%20MLLMs%20to%20assess%20and%20guide%0Athe%20text-3D%20correspondence%20during%20the%20optimization.%20We%20further%20develop%0A3DLLaVA-CRITIC%20-%20a%20fine-tuned%20MLLM%20specialized%20for%20evaluating%20multiview%20text%0Aalignment%20in%203D%20generations.%20Additionally%2C%20we%20introduce%20an%20LLM-layout%0Ainitialization%20that%20significantly%20accelerates%20optimization%20convergence%20through%0Asemantic-aware%20spatial%20configuration.%20Our%20framework%2C%20CoherenDream%2C%20achieves%0Aconsistent%20improvement%20across%20multiple%20metrics%20on%20TIFA%20subset.As%20the%20first%0Astudy%20to%20incorporate%20MLLMs%20into%20SDS%20optimization%2C%20we%20also%20conduct%20extensive%0Aablation%20studies%20to%20explore%20optimal%20MLLM%20adaptations%20for%203D%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19860v2&entry.124074799=Read"},
{"title": "A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction\n  based on Content/Style Modeling", "author": "Chinmay Rao and Matthias van Osch and Nicola Pezzotti and Jeroen de Bresser and Mark van Buchem and Laurens Beljaards and Jakob Meineke and Elwin de Weerdt and Huangling Lu and Mariya Doneva and Marius Staring", "abstract": "  Since multiple MRI contrasts of the same anatomy contain redundant\ninformation, one contrast can guide the reconstruction of an undersampled\nsubsequent contrast. To this end, several end-to-end learning-based guided\nreconstruction methods have been proposed. However, a key challenge is the\nrequirement of large paired training datasets comprising raw data and aligned\nreference images. We propose a modular two-stage approach addressing this\nissue, additionally providing an explanatory framework for the multi-contrast\nproblem based on the shared and non-shared generative factors underlying two\ngiven contrasts. A content/style model of two-contrast image data is learned\nfrom a largely unpaired image-domain dataset and is subsequently applied as a\nplug-and-play operator in iterative reconstruction. The disentanglement of\ncontent and style allows explicit representation of contrast-independent and\ncontrast-specific factors. Consequently, incorporating prior information into\nthe reconstruction reduces to a simple replacement of the aliased content of\nthe reconstruction iterate with high-quality content derived from the reference\nscan. Combining this component with a data consistency step and introducing a\ngeneral corrective process for the content yields an iterative scheme. We name\nthis novel approach PnP-CoSMo. Various aspects like interpretability and\nconvergence are explored via simulations. Furthermore, its practicality is\ndemonstrated on the public NYU fastMRI DICOM dataset, showing improved\ngeneralizability compared to end-to-end methods, and on two in-house multi-coil\nraw datasets, offering up to 32.6% more acceleration over learning-based\nnon-guided reconstruction for a given SSIM.\n", "link": "http://arxiv.org/abs/2409.13477v4", "date": "2025-08-11", "relevancy": 2.8871, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.586}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling&body=Title%3A%20A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling%0AAuthor%3A%20Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Mark%20van%20Buchem%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring%0AAbstract%3A%20%20%20Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%0Ainformation%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%0Asubsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%0Areconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%0Arequirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%0Areference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20addressing%20this%0Aissue%2C%20additionally%20providing%20an%20explanatory%20framework%20for%20the%20multi-contrast%0Aproblem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%0Agiven%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%0Afrom%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%0Aplug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%0Acontent%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%0Acontrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%0Athe%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%0Athe%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%0Ascan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%0Ageneral%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%0Athis%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%0Aconvergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%0Ademonstrated%20on%20the%20public%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%0Ageneralizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%0Araw%20datasets%2C%20offering%20up%20to%2032.6%25%20more%20acceleration%20over%20learning-based%0Anon-guided%20reconstruction%20for%20a%20given%20SSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13477v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Plug-and-Play%2520Method%2520for%2520Guided%2520Multi-contrast%2520MRI%2520Reconstruction%250A%2520%2520based%2520on%2520Content/Style%2520Modeling%26entry.906535625%3DChinmay%2520Rao%2520and%2520Matthias%2520van%2520Osch%2520and%2520Nicola%2520Pezzotti%2520and%2520Jeroen%2520de%2520Bresser%2520and%2520Mark%2520van%2520Buchem%2520and%2520Laurens%2520Beljaards%2520and%2520Jakob%2520Meineke%2520and%2520Elwin%2520de%2520Weerdt%2520and%2520Huangling%2520Lu%2520and%2520Mariya%2520Doneva%2520and%2520Marius%2520Staring%26entry.1292438233%3D%2520%2520Since%2520multiple%2520MRI%2520contrasts%2520of%2520the%2520same%2520anatomy%2520contain%2520redundant%250Ainformation%252C%2520one%2520contrast%2520can%2520guide%2520the%2520reconstruction%2520of%2520an%2520undersampled%250Asubsequent%2520contrast.%2520To%2520this%2520end%252C%2520several%2520end-to-end%2520learning-based%2520guided%250Areconstruction%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520a%2520key%2520challenge%2520is%2520the%250Arequirement%2520of%2520large%2520paired%2520training%2520datasets%2520comprising%2520raw%2520data%2520and%2520aligned%250Areference%2520images.%2520We%2520propose%2520a%2520modular%2520two-stage%2520approach%2520addressing%2520this%250Aissue%252C%2520additionally%2520providing%2520an%2520explanatory%2520framework%2520for%2520the%2520multi-contrast%250Aproblem%2520based%2520on%2520the%2520shared%2520and%2520non-shared%2520generative%2520factors%2520underlying%2520two%250Agiven%2520contrasts.%2520A%2520content/style%2520model%2520of%2520two-contrast%2520image%2520data%2520is%2520learned%250Afrom%2520a%2520largely%2520unpaired%2520image-domain%2520dataset%2520and%2520is%2520subsequently%2520applied%2520as%2520a%250Aplug-and-play%2520operator%2520in%2520iterative%2520reconstruction.%2520The%2520disentanglement%2520of%250Acontent%2520and%2520style%2520allows%2520explicit%2520representation%2520of%2520contrast-independent%2520and%250Acontrast-specific%2520factors.%2520Consequently%252C%2520incorporating%2520prior%2520information%2520into%250Athe%2520reconstruction%2520reduces%2520to%2520a%2520simple%2520replacement%2520of%2520the%2520aliased%2520content%2520of%250Athe%2520reconstruction%2520iterate%2520with%2520high-quality%2520content%2520derived%2520from%2520the%2520reference%250Ascan.%2520Combining%2520this%2520component%2520with%2520a%2520data%2520consistency%2520step%2520and%2520introducing%2520a%250Ageneral%2520corrective%2520process%2520for%2520the%2520content%2520yields%2520an%2520iterative%2520scheme.%2520We%2520name%250Athis%2520novel%2520approach%2520PnP-CoSMo.%2520Various%2520aspects%2520like%2520interpretability%2520and%250Aconvergence%2520are%2520explored%2520via%2520simulations.%2520Furthermore%252C%2520its%2520practicality%2520is%250Ademonstrated%2520on%2520the%2520public%2520NYU%2520fastMRI%2520DICOM%2520dataset%252C%2520showing%2520improved%250Ageneralizability%2520compared%2520to%2520end-to-end%2520methods%252C%2520and%2520on%2520two%2520in-house%2520multi-coil%250Araw%2520datasets%252C%2520offering%2520up%2520to%252032.6%2525%2520more%2520acceleration%2520over%2520learning-based%250Anon-guided%2520reconstruction%2520for%2520a%2520given%2520SSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13477v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Plug-and-Play%20Method%20for%20Guided%20Multi-contrast%20MRI%20Reconstruction%0A%20%20based%20on%20Content/Style%20Modeling&entry.906535625=Chinmay%20Rao%20and%20Matthias%20van%20Osch%20and%20Nicola%20Pezzotti%20and%20Jeroen%20de%20Bresser%20and%20Mark%20van%20Buchem%20and%20Laurens%20Beljaards%20and%20Jakob%20Meineke%20and%20Elwin%20de%20Weerdt%20and%20Huangling%20Lu%20and%20Mariya%20Doneva%20and%20Marius%20Staring&entry.1292438233=%20%20Since%20multiple%20MRI%20contrasts%20of%20the%20same%20anatomy%20contain%20redundant%0Ainformation%2C%20one%20contrast%20can%20guide%20the%20reconstruction%20of%20an%20undersampled%0Asubsequent%20contrast.%20To%20this%20end%2C%20several%20end-to-end%20learning-based%20guided%0Areconstruction%20methods%20have%20been%20proposed.%20However%2C%20a%20key%20challenge%20is%20the%0Arequirement%20of%20large%20paired%20training%20datasets%20comprising%20raw%20data%20and%20aligned%0Areference%20images.%20We%20propose%20a%20modular%20two-stage%20approach%20addressing%20this%0Aissue%2C%20additionally%20providing%20an%20explanatory%20framework%20for%20the%20multi-contrast%0Aproblem%20based%20on%20the%20shared%20and%20non-shared%20generative%20factors%20underlying%20two%0Agiven%20contrasts.%20A%20content/style%20model%20of%20two-contrast%20image%20data%20is%20learned%0Afrom%20a%20largely%20unpaired%20image-domain%20dataset%20and%20is%20subsequently%20applied%20as%20a%0Aplug-and-play%20operator%20in%20iterative%20reconstruction.%20The%20disentanglement%20of%0Acontent%20and%20style%20allows%20explicit%20representation%20of%20contrast-independent%20and%0Acontrast-specific%20factors.%20Consequently%2C%20incorporating%20prior%20information%20into%0Athe%20reconstruction%20reduces%20to%20a%20simple%20replacement%20of%20the%20aliased%20content%20of%0Athe%20reconstruction%20iterate%20with%20high-quality%20content%20derived%20from%20the%20reference%0Ascan.%20Combining%20this%20component%20with%20a%20data%20consistency%20step%20and%20introducing%20a%0Ageneral%20corrective%20process%20for%20the%20content%20yields%20an%20iterative%20scheme.%20We%20name%0Athis%20novel%20approach%20PnP-CoSMo.%20Various%20aspects%20like%20interpretability%20and%0Aconvergence%20are%20explored%20via%20simulations.%20Furthermore%2C%20its%20practicality%20is%0Ademonstrated%20on%20the%20public%20NYU%20fastMRI%20DICOM%20dataset%2C%20showing%20improved%0Ageneralizability%20compared%20to%20end-to-end%20methods%2C%20and%20on%20two%20in-house%20multi-coil%0Araw%20datasets%2C%20offering%20up%20to%2032.6%25%20more%20acceleration%20over%20learning-based%0Anon-guided%20reconstruction%20for%20a%20given%20SSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13477v4&entry.124074799=Read"},
{"title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal\n  Tokens", "author": "Zhuqiang Lu and Zhenfei Yin and Mengwei He and Zhihui Wang and Zicheng Liu and Zhiyong Wang and Kun Hu", "abstract": "  Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.\n", "link": "http://arxiv.org/abs/2412.09919v2", "date": "2025-08-11", "relevancy": 2.8739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5846}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B-VLLM%3A%20A%20Vision%20Large%20Language%20Model%20with%20Balanced%20Spatio-Temporal%0A%20%20Tokens&body=Title%3A%20B-VLLM%3A%20A%20Vision%20Large%20Language%20Model%20with%20Balanced%20Spatio-Temporal%0A%20%20Tokens%0AAuthor%3A%20Zhuqiang%20Lu%20and%20Zhenfei%20Yin%20and%20Mengwei%20He%20and%20Zhihui%20Wang%20and%20Zicheng%20Liu%20and%20Zhiyong%20Wang%20and%20Kun%20Hu%0AAbstract%3A%20%20%20Recently%2C%20Vision%20Large%20Language%20Models%20%28VLLMs%29%20integrated%20with%20vision%0Aencoders%20have%20shown%20promising%20performance%20in%20vision%20understanding.%20The%20key%20of%0AVLLMs%20is%20to%20encode%20visual%20content%20into%20sequences%20of%20visual%20tokens%2C%20enabling%0AVLLMs%20to%20simultaneously%20process%20both%20visual%20and%20textual%20content.%20However%2C%0Aunderstanding%20videos%2C%20especially%20long%20videos%2C%20remain%20a%20challenge%20to%20VLLMs%20as%0Athe%20number%20of%20visual%20tokens%20grows%20rapidly%20when%20encoding%20videos%2C%20resulting%20in%0Athe%20risk%20of%20exceeding%20the%20context%20window%20of%20VLLMs%20and%20introducing%20heavy%0Acomputation%20burden.%20To%20restrict%20the%20number%20of%20visual%20tokens%2C%20existing%20VLLMs%0Aeither%3A%20%281%29%20uniformly%20downsample%20videos%20into%20a%20fixed%20number%20of%20frames%20or%20%282%29%0Areducing%20the%20number%20of%20visual%20tokens%20encoded%20from%20each%20frame.%20We%20argue%20the%0Aformer%20solution%20neglects%20the%20rich%20temporal%20cue%20in%20videos%20and%20the%20later%0Aoverlooks%20the%20spatial%20details%20in%20each%20frame.%20In%20this%20work%2C%20we%20present%0ABalanced-VLLM%20%28B-VLLM%29%3A%20a%20novel%20VLLM%20framework%20that%20aims%20to%20effectively%0Aleverage%20task%20relevant%20spatio-temporal%20cues%20while%20restricting%20the%20number%20of%0Avisual%20tokens%20under%20the%20VLLM%20context%20window%20length.%20At%20the%20core%20of%20our%20method%2C%0Awe%20devise%20a%20text-conditioned%20adaptive%20frame%20selection%20module%20to%20identify%20frames%0Arelevant%20to%20the%20visual%20understanding%20task.%20The%20selected%20frames%20are%20then%0Ade-duplicated%20using%20a%20temporal%20frame%20token%20merging%20technique.%20The%20visual%20tokens%0Aof%20the%20selected%20frames%20are%20processed%20through%20a%20spatial%20token%20sampling%20module%0Aand%20an%20optional%20spatial%20token%20merging%20strategy%20to%20achieve%20precise%20control%20over%0Athe%20token%20count.%20Experimental%20results%20show%20that%20B-VLLM%20is%20effective%20in%0Abalancing%20the%20number%20of%20frames%20and%20visual%20tokens%20in%20video%20understanding%2C%0Ayielding%20superior%20performance%20on%20various%20video%20understanding%20benchmarks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/zhuqiangLu/B-VLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB-VLLM%253A%2520A%2520Vision%2520Large%2520Language%2520Model%2520with%2520Balanced%2520Spatio-Temporal%250A%2520%2520Tokens%26entry.906535625%3DZhuqiang%2520Lu%2520and%2520Zhenfei%2520Yin%2520and%2520Mengwei%2520He%2520and%2520Zhihui%2520Wang%2520and%2520Zicheng%2520Liu%2520and%2520Zhiyong%2520Wang%2520and%2520Kun%2520Hu%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520integrated%2520with%2520vision%250Aencoders%2520have%2520shown%2520promising%2520performance%2520in%2520vision%2520understanding.%2520The%2520key%2520of%250AVLLMs%2520is%2520to%2520encode%2520visual%2520content%2520into%2520sequences%2520of%2520visual%2520tokens%252C%2520enabling%250AVLLMs%2520to%2520simultaneously%2520process%2520both%2520visual%2520and%2520textual%2520content.%2520However%252C%250Aunderstanding%2520videos%252C%2520especially%2520long%2520videos%252C%2520remain%2520a%2520challenge%2520to%2520VLLMs%2520as%250Athe%2520number%2520of%2520visual%2520tokens%2520grows%2520rapidly%2520when%2520encoding%2520videos%252C%2520resulting%2520in%250Athe%2520risk%2520of%2520exceeding%2520the%2520context%2520window%2520of%2520VLLMs%2520and%2520introducing%2520heavy%250Acomputation%2520burden.%2520To%2520restrict%2520the%2520number%2520of%2520visual%2520tokens%252C%2520existing%2520VLLMs%250Aeither%253A%2520%25281%2529%2520uniformly%2520downsample%2520videos%2520into%2520a%2520fixed%2520number%2520of%2520frames%2520or%2520%25282%2529%250Areducing%2520the%2520number%2520of%2520visual%2520tokens%2520encoded%2520from%2520each%2520frame.%2520We%2520argue%2520the%250Aformer%2520solution%2520neglects%2520the%2520rich%2520temporal%2520cue%2520in%2520videos%2520and%2520the%2520later%250Aoverlooks%2520the%2520spatial%2520details%2520in%2520each%2520frame.%2520In%2520this%2520work%252C%2520we%2520present%250ABalanced-VLLM%2520%2528B-VLLM%2529%253A%2520a%2520novel%2520VLLM%2520framework%2520that%2520aims%2520to%2520effectively%250Aleverage%2520task%2520relevant%2520spatio-temporal%2520cues%2520while%2520restricting%2520the%2520number%2520of%250Avisual%2520tokens%2520under%2520the%2520VLLM%2520context%2520window%2520length.%2520At%2520the%2520core%2520of%2520our%2520method%252C%250Awe%2520devise%2520a%2520text-conditioned%2520adaptive%2520frame%2520selection%2520module%2520to%2520identify%2520frames%250Arelevant%2520to%2520the%2520visual%2520understanding%2520task.%2520The%2520selected%2520frames%2520are%2520then%250Ade-duplicated%2520using%2520a%2520temporal%2520frame%2520token%2520merging%2520technique.%2520The%2520visual%2520tokens%250Aof%2520the%2520selected%2520frames%2520are%2520processed%2520through%2520a%2520spatial%2520token%2520sampling%2520module%250Aand%2520an%2520optional%2520spatial%2520token%2520merging%2520strategy%2520to%2520achieve%2520precise%2520control%2520over%250Athe%2520token%2520count.%2520Experimental%2520results%2520show%2520that%2520B-VLLM%2520is%2520effective%2520in%250Abalancing%2520the%2520number%2520of%2520frames%2520and%2520visual%2520tokens%2520in%2520video%2520understanding%252C%250Ayielding%2520superior%2520performance%2520on%2520various%2520video%2520understanding%2520benchmarks.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/zhuqiangLu/B-VLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B-VLLM%3A%20A%20Vision%20Large%20Language%20Model%20with%20Balanced%20Spatio-Temporal%0A%20%20Tokens&entry.906535625=Zhuqiang%20Lu%20and%20Zhenfei%20Yin%20and%20Mengwei%20He%20and%20Zhihui%20Wang%20and%20Zicheng%20Liu%20and%20Zhiyong%20Wang%20and%20Kun%20Hu&entry.1292438233=%20%20Recently%2C%20Vision%20Large%20Language%20Models%20%28VLLMs%29%20integrated%20with%20vision%0Aencoders%20have%20shown%20promising%20performance%20in%20vision%20understanding.%20The%20key%20of%0AVLLMs%20is%20to%20encode%20visual%20content%20into%20sequences%20of%20visual%20tokens%2C%20enabling%0AVLLMs%20to%20simultaneously%20process%20both%20visual%20and%20textual%20content.%20However%2C%0Aunderstanding%20videos%2C%20especially%20long%20videos%2C%20remain%20a%20challenge%20to%20VLLMs%20as%0Athe%20number%20of%20visual%20tokens%20grows%20rapidly%20when%20encoding%20videos%2C%20resulting%20in%0Athe%20risk%20of%20exceeding%20the%20context%20window%20of%20VLLMs%20and%20introducing%20heavy%0Acomputation%20burden.%20To%20restrict%20the%20number%20of%20visual%20tokens%2C%20existing%20VLLMs%0Aeither%3A%20%281%29%20uniformly%20downsample%20videos%20into%20a%20fixed%20number%20of%20frames%20or%20%282%29%0Areducing%20the%20number%20of%20visual%20tokens%20encoded%20from%20each%20frame.%20We%20argue%20the%0Aformer%20solution%20neglects%20the%20rich%20temporal%20cue%20in%20videos%20and%20the%20later%0Aoverlooks%20the%20spatial%20details%20in%20each%20frame.%20In%20this%20work%2C%20we%20present%0ABalanced-VLLM%20%28B-VLLM%29%3A%20a%20novel%20VLLM%20framework%20that%20aims%20to%20effectively%0Aleverage%20task%20relevant%20spatio-temporal%20cues%20while%20restricting%20the%20number%20of%0Avisual%20tokens%20under%20the%20VLLM%20context%20window%20length.%20At%20the%20core%20of%20our%20method%2C%0Awe%20devise%20a%20text-conditioned%20adaptive%20frame%20selection%20module%20to%20identify%20frames%0Arelevant%20to%20the%20visual%20understanding%20task.%20The%20selected%20frames%20are%20then%0Ade-duplicated%20using%20a%20temporal%20frame%20token%20merging%20technique.%20The%20visual%20tokens%0Aof%20the%20selected%20frames%20are%20processed%20through%20a%20spatial%20token%20sampling%20module%0Aand%20an%20optional%20spatial%20token%20merging%20strategy%20to%20achieve%20precise%20control%20over%0Athe%20token%20count.%20Experimental%20results%20show%20that%20B-VLLM%20is%20effective%20in%0Abalancing%20the%20number%20of%20frames%20and%20visual%20tokens%20in%20video%20understanding%2C%0Ayielding%20superior%20performance%20on%20various%20video%20understanding%20benchmarks.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/zhuqiangLu/B-VLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09919v2&entry.124074799=Read"},
{"title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding\n  with Vision Foundation Models", "author": "Thinesh Thiyakesan Ponbagavathi and Chengzheng Yang and Alina Roitberg", "abstract": "  Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.\n", "link": "http://arxiv.org/abs/2508.07996v1", "date": "2025-08-11", "relevancy": 2.8672, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Guided%20Relational%20Reasoning%20for%20Social%20Behavior%20Understanding%0A%20%20with%20Vision%20Foundation%20Models&body=Title%3A%20Prompt-Guided%20Relational%20Reasoning%20for%20Social%20Behavior%20Understanding%0A%20%20with%20Vision%20Foundation%20Models%0AAuthor%3A%20Thinesh%20Thiyakesan%20Ponbagavathi%20and%20Chengzheng%20Yang%20and%20Alina%20Roitberg%0AAbstract%3A%20%20%20Group%20Activity%20Detection%20%28GAD%29%20involves%20recognizing%20social%20groups%20and%20their%0Acollective%20behaviors%20in%20videos.%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20like%20DinoV2%2C%0Aoffer%20excellent%20features%2C%20but%20are%20pretrained%20primarily%20on%20object-centric%20data%0Aand%20remain%20underexplored%20for%20modeling%20group%20dynamics.%20While%20they%20are%20a%0Apromising%20alternative%20to%20highly%20task-specific%20GAD%20architectures%20that%20require%0Afull%20fine-tuning%2C%20our%20initial%20investigation%20reveals%20that%20simply%20swapping%20CNN%0Abackbones%20used%20in%20these%20methods%20with%20VFMs%20brings%20little%20gain%2C%20underscoring%20the%0Aneed%20for%20structured%2C%20group-aware%20reasoning%20on%20top.%0A%20%20We%20introduce%20Prompt-driven%20Group%20Activity%20Detection%20%28ProGraD%29%20--%20a%20method%0Athat%20bridges%20this%20gap%20through%201%29%20learnable%20group%20prompts%20to%20guide%20the%20VFM%0Aattention%20toward%20social%20configurations%2C%20and%202%29%20a%20lightweight%20two-layer%0AGroupContext%20Transformer%20that%20infers%20actor-group%20associations%20and%20collective%0Abehavior.%20We%20evaluate%20our%20approach%20on%20two%20recent%20GAD%20benchmarks%3A%20Cafe%2C%20which%0Afeatures%20multiple%20concurrent%20social%20groups%2C%20and%20Social-CAD%2C%20which%20focuses%20on%0Asingle-group%20interactions.%20While%20we%20surpass%20state-of-the-art%20in%20both%20settings%2C%0Aour%20method%20is%20especially%20effective%20in%20complex%20multi-group%20scenarios%2C%20where%20we%0Ayield%20a%20gain%20of%206.5%5C%25%20%28Group%20mAP%5C%401.0%29%20and%208.2%5C%25%20%28Group%20mAP%5C%400.5%29%20using%20only%0A10M%20trainable%20parameters.%20Furthermore%2C%20our%20experiments%20reveal%20that%20ProGraD%0Aproduces%20interpretable%20attention%20maps%2C%20offering%20insights%20into%20actor-group%0Areasoning.%20Code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Guided%2520Relational%2520Reasoning%2520for%2520Social%2520Behavior%2520Understanding%250A%2520%2520with%2520Vision%2520Foundation%2520Models%26entry.906535625%3DThinesh%2520Thiyakesan%2520Ponbagavathi%2520and%2520Chengzheng%2520Yang%2520and%2520Alina%2520Roitberg%26entry.1292438233%3D%2520%2520Group%2520Activity%2520Detection%2520%2528GAD%2529%2520involves%2520recognizing%2520social%2520groups%2520and%2520their%250Acollective%2520behaviors%2520in%2520videos.%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%252C%2520like%2520DinoV2%252C%250Aoffer%2520excellent%2520features%252C%2520but%2520are%2520pretrained%2520primarily%2520on%2520object-centric%2520data%250Aand%2520remain%2520underexplored%2520for%2520modeling%2520group%2520dynamics.%2520While%2520they%2520are%2520a%250Apromising%2520alternative%2520to%2520highly%2520task-specific%2520GAD%2520architectures%2520that%2520require%250Afull%2520fine-tuning%252C%2520our%2520initial%2520investigation%2520reveals%2520that%2520simply%2520swapping%2520CNN%250Abackbones%2520used%2520in%2520these%2520methods%2520with%2520VFMs%2520brings%2520little%2520gain%252C%2520underscoring%2520the%250Aneed%2520for%2520structured%252C%2520group-aware%2520reasoning%2520on%2520top.%250A%2520%2520We%2520introduce%2520Prompt-driven%2520Group%2520Activity%2520Detection%2520%2528ProGraD%2529%2520--%2520a%2520method%250Athat%2520bridges%2520this%2520gap%2520through%25201%2529%2520learnable%2520group%2520prompts%2520to%2520guide%2520the%2520VFM%250Aattention%2520toward%2520social%2520configurations%252C%2520and%25202%2529%2520a%2520lightweight%2520two-layer%250AGroupContext%2520Transformer%2520that%2520infers%2520actor-group%2520associations%2520and%2520collective%250Abehavior.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520recent%2520GAD%2520benchmarks%253A%2520Cafe%252C%2520which%250Afeatures%2520multiple%2520concurrent%2520social%2520groups%252C%2520and%2520Social-CAD%252C%2520which%2520focuses%2520on%250Asingle-group%2520interactions.%2520While%2520we%2520surpass%2520state-of-the-art%2520in%2520both%2520settings%252C%250Aour%2520method%2520is%2520especially%2520effective%2520in%2520complex%2520multi-group%2520scenarios%252C%2520where%2520we%250Ayield%2520a%2520gain%2520of%25206.5%255C%2525%2520%2528Group%2520mAP%255C%25401.0%2529%2520and%25208.2%255C%2525%2520%2528Group%2520mAP%255C%25400.5%2529%2520using%2520only%250A10M%2520trainable%2520parameters.%2520Furthermore%252C%2520our%2520experiments%2520reveal%2520that%2520ProGraD%250Aproduces%2520interpretable%2520attention%2520maps%252C%2520offering%2520insights%2520into%2520actor-group%250Areasoning.%2520Code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Guided%20Relational%20Reasoning%20for%20Social%20Behavior%20Understanding%0A%20%20with%20Vision%20Foundation%20Models&entry.906535625=Thinesh%20Thiyakesan%20Ponbagavathi%20and%20Chengzheng%20Yang%20and%20Alina%20Roitberg&entry.1292438233=%20%20Group%20Activity%20Detection%20%28GAD%29%20involves%20recognizing%20social%20groups%20and%20their%0Acollective%20behaviors%20in%20videos.%20Vision%20Foundation%20Models%20%28VFMs%29%2C%20like%20DinoV2%2C%0Aoffer%20excellent%20features%2C%20but%20are%20pretrained%20primarily%20on%20object-centric%20data%0Aand%20remain%20underexplored%20for%20modeling%20group%20dynamics.%20While%20they%20are%20a%0Apromising%20alternative%20to%20highly%20task-specific%20GAD%20architectures%20that%20require%0Afull%20fine-tuning%2C%20our%20initial%20investigation%20reveals%20that%20simply%20swapping%20CNN%0Abackbones%20used%20in%20these%20methods%20with%20VFMs%20brings%20little%20gain%2C%20underscoring%20the%0Aneed%20for%20structured%2C%20group-aware%20reasoning%20on%20top.%0A%20%20We%20introduce%20Prompt-driven%20Group%20Activity%20Detection%20%28ProGraD%29%20--%20a%20method%0Athat%20bridges%20this%20gap%20through%201%29%20learnable%20group%20prompts%20to%20guide%20the%20VFM%0Aattention%20toward%20social%20configurations%2C%20and%202%29%20a%20lightweight%20two-layer%0AGroupContext%20Transformer%20that%20infers%20actor-group%20associations%20and%20collective%0Abehavior.%20We%20evaluate%20our%20approach%20on%20two%20recent%20GAD%20benchmarks%3A%20Cafe%2C%20which%0Afeatures%20multiple%20concurrent%20social%20groups%2C%20and%20Social-CAD%2C%20which%20focuses%20on%0Asingle-group%20interactions.%20While%20we%20surpass%20state-of-the-art%20in%20both%20settings%2C%0Aour%20method%20is%20especially%20effective%20in%20complex%20multi-group%20scenarios%2C%20where%20we%0Ayield%20a%20gain%20of%206.5%5C%25%20%28Group%20mAP%5C%401.0%29%20and%208.2%5C%25%20%28Group%20mAP%5C%400.5%29%20using%20only%0A10M%20trainable%20parameters.%20Furthermore%2C%20our%20experiments%20reveal%20that%20ProGraD%0Aproduces%20interpretable%20attention%20maps%2C%20offering%20insights%20into%20actor-group%0Areasoning.%20Code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07996v1&entry.124074799=Read"},
{"title": "LL3M: Large Language 3D Modelers", "author": "Sining Lu and Guan Chen and Nam Anh Dinh and Itai Lang and Ari Holtzman and Rana Hanocka", "abstract": "  We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.\n", "link": "http://arxiv.org/abs/2508.08228v1", "date": "2025-08-11", "relevancy": 2.8607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5795}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LL3M%3A%20Large%20Language%203D%20Modelers&body=Title%3A%20LL3M%3A%20Large%20Language%203D%20Modelers%0AAuthor%3A%20Sining%20Lu%20and%20Guan%20Chen%20and%20Nam%20Anh%20Dinh%20and%20Itai%20Lang%20and%20Ari%20Holtzman%20and%20Rana%20Hanocka%0AAbstract%3A%20%20%20We%20present%20LL3M%2C%20a%20multi-agent%20system%20that%20leverages%20pretrained%20large%0Alanguage%20models%20%28LLMs%29%20to%20generate%203D%20assets%20by%20writing%20interpretable%20Python%0Acode%20in%20Blender.%20We%20break%20away%20from%20the%20typical%20generative%20approach%20that%20learns%0Afrom%20a%20collection%20of%203D%20data.%20Instead%2C%20we%20reformulate%20shape%20generation%20as%20a%0Acode-writing%20task%2C%20enabling%20greater%20modularity%2C%20editability%2C%20and%20integration%0Awith%20artist%20workflows.%20Given%20a%20text%20prompt%2C%20LL3M%20coordinates%20a%20team%20of%0Aspecialized%20LLM%20agents%20to%20plan%2C%20retrieve%2C%20write%2C%20debug%2C%20and%20refine%20Blender%0Ascripts%20that%20generate%20and%20edit%20geometry%20and%20appearance.%20The%20generated%20code%0Aworks%20as%20a%20high-level%2C%20interpretable%2C%20human-readable%2C%20well-documented%0Arepresentation%20of%20scenes%20and%20objects%2C%20making%20full%20use%20of%20sophisticated%20Blender%0Aconstructs%20%28e.g.%20B-meshes%2C%20geometry%20modifiers%2C%20shader%20nodes%29%20for%20diverse%2C%0Aunconstrained%20shapes%2C%20materials%2C%20and%20scenes.%20This%20code%20presents%20many%20avenues%0Afor%20further%20agent%20and%20human%20editing%20and%20experimentation%20via%20code%20tweaks%20or%0Aprocedural%20parameters.%20This%20medium%20naturally%20enables%20a%20co-creative%20loop%20in%20our%0Asystem%3A%20agents%20can%20automatically%20self-critique%20using%20code%20and%20visuals%2C%20while%0Aiterative%20user%20instructions%20provide%20an%20intuitive%20way%20to%20refine%20assets.%20A%20shared%0Acode%20context%20across%20agents%20enables%20awareness%20of%20previous%20attempts%2C%20and%20a%0Aretrieval-augmented%20generation%20knowledge%20base%20built%20from%20Blender%20API%0Adocumentation%2C%20BlenderRAG%2C%20equips%20agents%20with%20examples%2C%20types%2C%20and%20functions%0Aempowering%20advanced%20modeling%20operations%20and%20code%20correctness.%20We%20demonstrate%0Athe%20effectiveness%20of%20LL3M%20across%20diverse%20shape%20categories%2C%20style%20and%20material%0Aedits%2C%20and%20user-driven%20refinements.%20Our%20experiments%20showcase%20the%20power%20of%20code%0Aas%20a%20generative%20and%20interpretable%20medium%20for%203D%20asset%20creation.%20Our%20project%0Apage%20is%20at%20https%3A//threedle.github.io/ll3m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLL3M%253A%2520Large%2520Language%25203D%2520Modelers%26entry.906535625%3DSining%2520Lu%2520and%2520Guan%2520Chen%2520and%2520Nam%2520Anh%2520Dinh%2520and%2520Itai%2520Lang%2520and%2520Ari%2520Holtzman%2520and%2520Rana%2520Hanocka%26entry.1292438233%3D%2520%2520We%2520present%2520LL3M%252C%2520a%2520multi-agent%2520system%2520that%2520leverages%2520pretrained%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520generate%25203D%2520assets%2520by%2520writing%2520interpretable%2520Python%250Acode%2520in%2520Blender.%2520We%2520break%2520away%2520from%2520the%2520typical%2520generative%2520approach%2520that%2520learns%250Afrom%2520a%2520collection%2520of%25203D%2520data.%2520Instead%252C%2520we%2520reformulate%2520shape%2520generation%2520as%2520a%250Acode-writing%2520task%252C%2520enabling%2520greater%2520modularity%252C%2520editability%252C%2520and%2520integration%250Awith%2520artist%2520workflows.%2520Given%2520a%2520text%2520prompt%252C%2520LL3M%2520coordinates%2520a%2520team%2520of%250Aspecialized%2520LLM%2520agents%2520to%2520plan%252C%2520retrieve%252C%2520write%252C%2520debug%252C%2520and%2520refine%2520Blender%250Ascripts%2520that%2520generate%2520and%2520edit%2520geometry%2520and%2520appearance.%2520The%2520generated%2520code%250Aworks%2520as%2520a%2520high-level%252C%2520interpretable%252C%2520human-readable%252C%2520well-documented%250Arepresentation%2520of%2520scenes%2520and%2520objects%252C%2520making%2520full%2520use%2520of%2520sophisticated%2520Blender%250Aconstructs%2520%2528e.g.%2520B-meshes%252C%2520geometry%2520modifiers%252C%2520shader%2520nodes%2529%2520for%2520diverse%252C%250Aunconstrained%2520shapes%252C%2520materials%252C%2520and%2520scenes.%2520This%2520code%2520presents%2520many%2520avenues%250Afor%2520further%2520agent%2520and%2520human%2520editing%2520and%2520experimentation%2520via%2520code%2520tweaks%2520or%250Aprocedural%2520parameters.%2520This%2520medium%2520naturally%2520enables%2520a%2520co-creative%2520loop%2520in%2520our%250Asystem%253A%2520agents%2520can%2520automatically%2520self-critique%2520using%2520code%2520and%2520visuals%252C%2520while%250Aiterative%2520user%2520instructions%2520provide%2520an%2520intuitive%2520way%2520to%2520refine%2520assets.%2520A%2520shared%250Acode%2520context%2520across%2520agents%2520enables%2520awareness%2520of%2520previous%2520attempts%252C%2520and%2520a%250Aretrieval-augmented%2520generation%2520knowledge%2520base%2520built%2520from%2520Blender%2520API%250Adocumentation%252C%2520BlenderRAG%252C%2520equips%2520agents%2520with%2520examples%252C%2520types%252C%2520and%2520functions%250Aempowering%2520advanced%2520modeling%2520operations%2520and%2520code%2520correctness.%2520We%2520demonstrate%250Athe%2520effectiveness%2520of%2520LL3M%2520across%2520diverse%2520shape%2520categories%252C%2520style%2520and%2520material%250Aedits%252C%2520and%2520user-driven%2520refinements.%2520Our%2520experiments%2520showcase%2520the%2520power%2520of%2520code%250Aas%2520a%2520generative%2520and%2520interpretable%2520medium%2520for%25203D%2520asset%2520creation.%2520Our%2520project%250Apage%2520is%2520at%2520https%253A//threedle.github.io/ll3m.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LL3M%3A%20Large%20Language%203D%20Modelers&entry.906535625=Sining%20Lu%20and%20Guan%20Chen%20and%20Nam%20Anh%20Dinh%20and%20Itai%20Lang%20and%20Ari%20Holtzman%20and%20Rana%20Hanocka&entry.1292438233=%20%20We%20present%20LL3M%2C%20a%20multi-agent%20system%20that%20leverages%20pretrained%20large%0Alanguage%20models%20%28LLMs%29%20to%20generate%203D%20assets%20by%20writing%20interpretable%20Python%0Acode%20in%20Blender.%20We%20break%20away%20from%20the%20typical%20generative%20approach%20that%20learns%0Afrom%20a%20collection%20of%203D%20data.%20Instead%2C%20we%20reformulate%20shape%20generation%20as%20a%0Acode-writing%20task%2C%20enabling%20greater%20modularity%2C%20editability%2C%20and%20integration%0Awith%20artist%20workflows.%20Given%20a%20text%20prompt%2C%20LL3M%20coordinates%20a%20team%20of%0Aspecialized%20LLM%20agents%20to%20plan%2C%20retrieve%2C%20write%2C%20debug%2C%20and%20refine%20Blender%0Ascripts%20that%20generate%20and%20edit%20geometry%20and%20appearance.%20The%20generated%20code%0Aworks%20as%20a%20high-level%2C%20interpretable%2C%20human-readable%2C%20well-documented%0Arepresentation%20of%20scenes%20and%20objects%2C%20making%20full%20use%20of%20sophisticated%20Blender%0Aconstructs%20%28e.g.%20B-meshes%2C%20geometry%20modifiers%2C%20shader%20nodes%29%20for%20diverse%2C%0Aunconstrained%20shapes%2C%20materials%2C%20and%20scenes.%20This%20code%20presents%20many%20avenues%0Afor%20further%20agent%20and%20human%20editing%20and%20experimentation%20via%20code%20tweaks%20or%0Aprocedural%20parameters.%20This%20medium%20naturally%20enables%20a%20co-creative%20loop%20in%20our%0Asystem%3A%20agents%20can%20automatically%20self-critique%20using%20code%20and%20visuals%2C%20while%0Aiterative%20user%20instructions%20provide%20an%20intuitive%20way%20to%20refine%20assets.%20A%20shared%0Acode%20context%20across%20agents%20enables%20awareness%20of%20previous%20attempts%2C%20and%20a%0Aretrieval-augmented%20generation%20knowledge%20base%20built%20from%20Blender%20API%0Adocumentation%2C%20BlenderRAG%2C%20equips%20agents%20with%20examples%2C%20types%2C%20and%20functions%0Aempowering%20advanced%20modeling%20operations%20and%20code%20correctness.%20We%20demonstrate%0Athe%20effectiveness%20of%20LL3M%20across%20diverse%20shape%20categories%2C%20style%20and%20material%0Aedits%2C%20and%20user-driven%20refinements.%20Our%20experiments%20showcase%20the%20power%20of%20code%0Aas%20a%20generative%20and%20interpretable%20medium%20for%203D%20asset%20creation.%20Our%20project%0Apage%20is%20at%20https%3A//threedle.github.io/ll3m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08228v1&entry.124074799=Read"},
{"title": "CLGRPO: Reasoning Ability Enhancement for Small VLMs", "author": "Fanyi Wang and Binzhi Dong and Haotian Hu and Jinjin Xu and Zhiwang Zhang", "abstract": "  Small Vision Language Models (SVLMs) generally refer to models with parameter\nsizes less than or equal to 2B. Their low cost and power consumption\ncharacteristics confer high commercial value. However, their reasoning\nabilities are limited by the number of parameters. To address this issue, this\npaper proposes a post-training optimization paradigm called the Incremental\nTraining Strategy to enhance the reasoning ability of SVLMs. Firstly, we\nconstructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,\nwhich leverages multiple LVLMs with 7B parameters or more to transform original\ndata into COT data in a self-supervised manner. Our proposed Incremental\nTraining Strategy consists of four stages. Stage 1 injects domain knowledge by\nperforming Supervised Fine-Tuning (SFT) to the pretrained model on the COT\ndata. Stage 2 aligns the COT data format by conducting a small amount of Group\nRelative Policy Optimization (GRPO) training constrained only by format rewards\non the COT data. Stage 3 enhances reasoning ability by applying GRPO training\non the COT data with constraints on both format and accuracy rewards. The\nresulting model shows significant improvement compared to the baseline. Stage 4\naddresses the limited capacity of the SVLMs and the weak ability to capture\ncomplex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture\nspace of the training process. We conducted extensive comparative and ablation\nexperiments on the abstract semantic recognition dataset EMOSet-118K.\nExperimental results demonstrate that our method significantly improves the\nreasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the\noriginal data, accuracy increased by 2.77 and recall by 0.69, achieving\nperformance comparable to that of 8B models.\n", "link": "http://arxiv.org/abs/2506.18048v2", "date": "2025-08-11", "relevancy": 2.8297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLGRPO%3A%20Reasoning%20Ability%20Enhancement%20for%20Small%20VLMs&body=Title%3A%20CLGRPO%3A%20Reasoning%20Ability%20Enhancement%20for%20Small%20VLMs%0AAuthor%3A%20Fanyi%20Wang%20and%20Binzhi%20Dong%20and%20Haotian%20Hu%20and%20Jinjin%20Xu%20and%20Zhiwang%20Zhang%0AAbstract%3A%20%20%20Small%20Vision%20Language%20Models%20%28SVLMs%29%20generally%20refer%20to%20models%20with%20parameter%0Asizes%20less%20than%20or%20equal%20to%202B.%20Their%20low%20cost%20and%20power%20consumption%0Acharacteristics%20confer%20high%20commercial%20value.%20However%2C%20their%20reasoning%0Aabilities%20are%20limited%20by%20the%20number%20of%20parameters.%20To%20address%20this%20issue%2C%20this%0Apaper%20proposes%20a%20post-training%20optimization%20paradigm%20called%20the%20Incremental%0ATraining%20Strategy%20to%20enhance%20the%20reasoning%20ability%20of%20SVLMs.%20Firstly%2C%20we%0Aconstructed%20a%20Self-Supervised%20Chain-of-Thought%20%28COT%29%20Data%20Construction%20System%2C%0Awhich%20leverages%20multiple%20LVLMs%20with%207B%20parameters%20or%20more%20to%20transform%20original%0Adata%20into%20COT%20data%20in%20a%20self-supervised%20manner.%20Our%20proposed%20Incremental%0ATraining%20Strategy%20consists%20of%20four%20stages.%20Stage%201%20injects%20domain%20knowledge%20by%0Aperforming%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20the%20pretrained%20model%20on%20the%20COT%0Adata.%20Stage%202%20aligns%20the%20COT%20data%20format%20by%20conducting%20a%20small%20amount%20of%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20training%20constrained%20only%20by%20format%20rewards%0Aon%20the%20COT%20data.%20Stage%203%20enhances%20reasoning%20ability%20by%20applying%20GRPO%20training%0Aon%20the%20COT%20data%20with%20constraints%20on%20both%20format%20and%20accuracy%20rewards.%20The%0Aresulting%20model%20shows%20significant%20improvement%20compared%20to%20the%20baseline.%20Stage%204%0Aaddresses%20the%20limited%20capacity%20of%20the%20SVLMs%20and%20the%20weak%20ability%20to%20capture%0Acomplex%20patterns%20by%20proposing%20ClipLow%20GRPO%20%28CLGRPO%29%20to%20constrain%20the%20capture%0Aspace%20of%20the%20training%20process.%20We%20conducted%20extensive%20comparative%20and%20ablation%0Aexperiments%20on%20the%20abstract%20semantic%20recognition%20dataset%20EMOSet-118K.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%201B%20SVLM.%20Compared%20to%20the%20baseline%20model%20fine-tuned%20on%20the%0Aoriginal%20data%2C%20accuracy%20increased%20by%202.77%20and%20recall%20by%200.69%2C%20achieving%0Aperformance%20comparable%20to%20that%20of%208B%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLGRPO%253A%2520Reasoning%2520Ability%2520Enhancement%2520for%2520Small%2520VLMs%26entry.906535625%3DFanyi%2520Wang%2520and%2520Binzhi%2520Dong%2520and%2520Haotian%2520Hu%2520and%2520Jinjin%2520Xu%2520and%2520Zhiwang%2520Zhang%26entry.1292438233%3D%2520%2520Small%2520Vision%2520Language%2520Models%2520%2528SVLMs%2529%2520generally%2520refer%2520to%2520models%2520with%2520parameter%250Asizes%2520less%2520than%2520or%2520equal%2520to%25202B.%2520Their%2520low%2520cost%2520and%2520power%2520consumption%250Acharacteristics%2520confer%2520high%2520commercial%2520value.%2520However%252C%2520their%2520reasoning%250Aabilities%2520are%2520limited%2520by%2520the%2520number%2520of%2520parameters.%2520To%2520address%2520this%2520issue%252C%2520this%250Apaper%2520proposes%2520a%2520post-training%2520optimization%2520paradigm%2520called%2520the%2520Incremental%250ATraining%2520Strategy%2520to%2520enhance%2520the%2520reasoning%2520ability%2520of%2520SVLMs.%2520Firstly%252C%2520we%250Aconstructed%2520a%2520Self-Supervised%2520Chain-of-Thought%2520%2528COT%2529%2520Data%2520Construction%2520System%252C%250Awhich%2520leverages%2520multiple%2520LVLMs%2520with%25207B%2520parameters%2520or%2520more%2520to%2520transform%2520original%250Adata%2520into%2520COT%2520data%2520in%2520a%2520self-supervised%2520manner.%2520Our%2520proposed%2520Incremental%250ATraining%2520Strategy%2520consists%2520of%2520four%2520stages.%2520Stage%25201%2520injects%2520domain%2520knowledge%2520by%250Aperforming%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520to%2520the%2520pretrained%2520model%2520on%2520the%2520COT%250Adata.%2520Stage%25202%2520aligns%2520the%2520COT%2520data%2520format%2520by%2520conducting%2520a%2520small%2520amount%2520of%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520training%2520constrained%2520only%2520by%2520format%2520rewards%250Aon%2520the%2520COT%2520data.%2520Stage%25203%2520enhances%2520reasoning%2520ability%2520by%2520applying%2520GRPO%2520training%250Aon%2520the%2520COT%2520data%2520with%2520constraints%2520on%2520both%2520format%2520and%2520accuracy%2520rewards.%2520The%250Aresulting%2520model%2520shows%2520significant%2520improvement%2520compared%2520to%2520the%2520baseline.%2520Stage%25204%250Aaddresses%2520the%2520limited%2520capacity%2520of%2520the%2520SVLMs%2520and%2520the%2520weak%2520ability%2520to%2520capture%250Acomplex%2520patterns%2520by%2520proposing%2520ClipLow%2520GRPO%2520%2528CLGRPO%2529%2520to%2520constrain%2520the%2520capture%250Aspace%2520of%2520the%2520training%2520process.%2520We%2520conducted%2520extensive%2520comparative%2520and%2520ablation%250Aexperiments%2520on%2520the%2520abstract%2520semantic%2520recognition%2520dataset%2520EMOSet-118K.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520the%250Areasoning%2520ability%2520of%25201B%2520SVLM.%2520Compared%2520to%2520the%2520baseline%2520model%2520fine-tuned%2520on%2520the%250Aoriginal%2520data%252C%2520accuracy%2520increased%2520by%25202.77%2520and%2520recall%2520by%25200.69%252C%2520achieving%250Aperformance%2520comparable%2520to%2520that%2520of%25208B%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLGRPO%3A%20Reasoning%20Ability%20Enhancement%20for%20Small%20VLMs&entry.906535625=Fanyi%20Wang%20and%20Binzhi%20Dong%20and%20Haotian%20Hu%20and%20Jinjin%20Xu%20and%20Zhiwang%20Zhang&entry.1292438233=%20%20Small%20Vision%20Language%20Models%20%28SVLMs%29%20generally%20refer%20to%20models%20with%20parameter%0Asizes%20less%20than%20or%20equal%20to%202B.%20Their%20low%20cost%20and%20power%20consumption%0Acharacteristics%20confer%20high%20commercial%20value.%20However%2C%20their%20reasoning%0Aabilities%20are%20limited%20by%20the%20number%20of%20parameters.%20To%20address%20this%20issue%2C%20this%0Apaper%20proposes%20a%20post-training%20optimization%20paradigm%20called%20the%20Incremental%0ATraining%20Strategy%20to%20enhance%20the%20reasoning%20ability%20of%20SVLMs.%20Firstly%2C%20we%0Aconstructed%20a%20Self-Supervised%20Chain-of-Thought%20%28COT%29%20Data%20Construction%20System%2C%0Awhich%20leverages%20multiple%20LVLMs%20with%207B%20parameters%20or%20more%20to%20transform%20original%0Adata%20into%20COT%20data%20in%20a%20self-supervised%20manner.%20Our%20proposed%20Incremental%0ATraining%20Strategy%20consists%20of%20four%20stages.%20Stage%201%20injects%20domain%20knowledge%20by%0Aperforming%20Supervised%20Fine-Tuning%20%28SFT%29%20to%20the%20pretrained%20model%20on%20the%20COT%0Adata.%20Stage%202%20aligns%20the%20COT%20data%20format%20by%20conducting%20a%20small%20amount%20of%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20training%20constrained%20only%20by%20format%20rewards%0Aon%20the%20COT%20data.%20Stage%203%20enhances%20reasoning%20ability%20by%20applying%20GRPO%20training%0Aon%20the%20COT%20data%20with%20constraints%20on%20both%20format%20and%20accuracy%20rewards.%20The%0Aresulting%20model%20shows%20significant%20improvement%20compared%20to%20the%20baseline.%20Stage%204%0Aaddresses%20the%20limited%20capacity%20of%20the%20SVLMs%20and%20the%20weak%20ability%20to%20capture%0Acomplex%20patterns%20by%20proposing%20ClipLow%20GRPO%20%28CLGRPO%29%20to%20constrain%20the%20capture%0Aspace%20of%20the%20training%20process.%20We%20conducted%20extensive%20comparative%20and%20ablation%0Aexperiments%20on%20the%20abstract%20semantic%20recognition%20dataset%20EMOSet-118K.%0AExperimental%20results%20demonstrate%20that%20our%20method%20significantly%20improves%20the%0Areasoning%20ability%20of%201B%20SVLM.%20Compared%20to%20the%20baseline%20model%20fine-tuned%20on%20the%0Aoriginal%20data%2C%20accuracy%20increased%20by%202.77%20and%20recall%20by%200.69%2C%20achieving%0Aperformance%20comparable%20to%20that%20of%208B%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18048v2&entry.124074799=Read"},
{"title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation", "author": "Qing Wang and Xiaohang Yang and Yilan Dong and Naveen Raj Govindaraj and Gregory Slabaugh and Shanxin Yuan", "abstract": "  Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2506.10574v2", "date": "2025-08-11", "relevancy": 2.7524, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5773}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5429}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation&body=Title%3A%20DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation%0AAuthor%3A%20Qing%20Wang%20and%20Xiaohang%20Yang%20and%20Yilan%20Dong%20and%20Naveen%20Raj%20Govindaraj%20and%20Gregory%20Slabaugh%20and%20Shanxin%20Yuan%0AAbstract%3A%20%20%20Music-to-dance%20generation%20aims%20to%20synthesize%20human%20dance%20motion%20conditioned%0Aon%20musical%20input.%20Despite%20recent%20progress%2C%20significant%20challenges%20remain%20due%20to%0Athe%20semantic%20gap%20between%20music%20and%20dance%20motion%2C%20as%20music%20offers%20only%20abstract%0Acues%2C%20such%20as%20melody%2C%20groove%2C%20and%20emotion%2C%20without%20explicitly%20specifying%20the%0Aphysical%20movements.%20Moreover%2C%20a%20single%20piece%20of%20music%20can%20produce%20multiple%0Aplausible%20dance%20interpretations.%20This%20one-to-many%20mapping%20demands%20additional%0Aguidance%2C%20as%20music%20alone%20provides%20limited%20information%20for%20generating%20diverse%0Adance%20movements.%20The%20challenge%20is%20further%20amplified%20by%20the%20scarcity%20of%20paired%0Amusic%20and%20dance%20data%2C%20which%20restricts%20the%20model%5C%5Ea%5Cu%7BA%7D%5C%27Zs%20ability%20to%20learn%0Adiverse%20dance%20patterns.%20In%20this%20paper%2C%20we%20introduce%20DanceChat%2C%20a%20Large%20Language%0AModel%20%28LLM%29-guided%20music-to-dance%20generation%20approach.%20We%20use%20an%20LLM%20as%20a%0Achoreographer%20that%20provides%20textual%20motion%20instructions%2C%20offering%20explicit%2C%0Ahigh-level%20guidance%20for%20dance%20generation.%20This%20approach%20goes%20beyond%20implicit%0Alearning%20from%20music%20alone%2C%20enabling%20the%20model%20to%20generate%20dance%20that%20is%20both%0Amore%20diverse%20and%20better%20aligned%20with%20musical%20styles.%20Our%20approach%20consists%20of%0Athree%20components%3A%20%281%29%20an%20LLM-based%20pseudo%20instruction%20generation%20module%20that%0Aproduces%20textual%20dance%20guidance%20based%20on%20music%20style%20and%20structure%2C%20%282%29%20a%0Amulti-modal%20feature%20extraction%20and%20fusion%20module%20that%20integrates%20music%2C%20rhythm%2C%0Aand%20textual%20guidance%20into%20a%20shared%20representation%2C%20and%20%283%29%20a%20diffusion-based%0Amotion%20synthesis%20module%20together%20with%20a%20multi-modal%20alignment%20loss%2C%20which%0Aensures%20that%20the%20generated%20dance%20is%20aligned%20with%20both%20musical%20and%20textual%20cues.%0AExtensive%20experiments%20on%20AIST%2B%2B%20and%20human%20evaluations%20show%20that%20DanceChat%0Aoutperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10574v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanceChat%253A%2520Large%2520Language%2520Model-Guided%2520Music-to-Dance%2520Generation%26entry.906535625%3DQing%2520Wang%2520and%2520Xiaohang%2520Yang%2520and%2520Yilan%2520Dong%2520and%2520Naveen%2520Raj%2520Govindaraj%2520and%2520Gregory%2520Slabaugh%2520and%2520Shanxin%2520Yuan%26entry.1292438233%3D%2520%2520Music-to-dance%2520generation%2520aims%2520to%2520synthesize%2520human%2520dance%2520motion%2520conditioned%250Aon%2520musical%2520input.%2520Despite%2520recent%2520progress%252C%2520significant%2520challenges%2520remain%2520due%2520to%250Athe%2520semantic%2520gap%2520between%2520music%2520and%2520dance%2520motion%252C%2520as%2520music%2520offers%2520only%2520abstract%250Acues%252C%2520such%2520as%2520melody%252C%2520groove%252C%2520and%2520emotion%252C%2520without%2520explicitly%2520specifying%2520the%250Aphysical%2520movements.%2520Moreover%252C%2520a%2520single%2520piece%2520of%2520music%2520can%2520produce%2520multiple%250Aplausible%2520dance%2520interpretations.%2520This%2520one-to-many%2520mapping%2520demands%2520additional%250Aguidance%252C%2520as%2520music%2520alone%2520provides%2520limited%2520information%2520for%2520generating%2520diverse%250Adance%2520movements.%2520The%2520challenge%2520is%2520further%2520amplified%2520by%2520the%2520scarcity%2520of%2520paired%250Amusic%2520and%2520dance%2520data%252C%2520which%2520restricts%2520the%2520model%255C%255Ea%255Cu%257BA%257D%255C%2527Zs%2520ability%2520to%2520learn%250Adiverse%2520dance%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DanceChat%252C%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529-guided%2520music-to-dance%2520generation%2520approach.%2520We%2520use%2520an%2520LLM%2520as%2520a%250Achoreographer%2520that%2520provides%2520textual%2520motion%2520instructions%252C%2520offering%2520explicit%252C%250Ahigh-level%2520guidance%2520for%2520dance%2520generation.%2520This%2520approach%2520goes%2520beyond%2520implicit%250Alearning%2520from%2520music%2520alone%252C%2520enabling%2520the%2520model%2520to%2520generate%2520dance%2520that%2520is%2520both%250Amore%2520diverse%2520and%2520better%2520aligned%2520with%2520musical%2520styles.%2520Our%2520approach%2520consists%2520of%250Athree%2520components%253A%2520%25281%2529%2520an%2520LLM-based%2520pseudo%2520instruction%2520generation%2520module%2520that%250Aproduces%2520textual%2520dance%2520guidance%2520based%2520on%2520music%2520style%2520and%2520structure%252C%2520%25282%2529%2520a%250Amulti-modal%2520feature%2520extraction%2520and%2520fusion%2520module%2520that%2520integrates%2520music%252C%2520rhythm%252C%250Aand%2520textual%2520guidance%2520into%2520a%2520shared%2520representation%252C%2520and%2520%25283%2529%2520a%2520diffusion-based%250Amotion%2520synthesis%2520module%2520together%2520with%2520a%2520multi-modal%2520alignment%2520loss%252C%2520which%250Aensures%2520that%2520the%2520generated%2520dance%2520is%2520aligned%2520with%2520both%2520musical%2520and%2520textual%2520cues.%250AExtensive%2520experiments%2520on%2520AIST%252B%252B%2520and%2520human%2520evaluations%2520show%2520that%2520DanceChat%250Aoutperforms%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10574v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanceChat%3A%20Large%20Language%20Model-Guided%20Music-to-Dance%20Generation&entry.906535625=Qing%20Wang%20and%20Xiaohang%20Yang%20and%20Yilan%20Dong%20and%20Naveen%20Raj%20Govindaraj%20and%20Gregory%20Slabaugh%20and%20Shanxin%20Yuan&entry.1292438233=%20%20Music-to-dance%20generation%20aims%20to%20synthesize%20human%20dance%20motion%20conditioned%0Aon%20musical%20input.%20Despite%20recent%20progress%2C%20significant%20challenges%20remain%20due%20to%0Athe%20semantic%20gap%20between%20music%20and%20dance%20motion%2C%20as%20music%20offers%20only%20abstract%0Acues%2C%20such%20as%20melody%2C%20groove%2C%20and%20emotion%2C%20without%20explicitly%20specifying%20the%0Aphysical%20movements.%20Moreover%2C%20a%20single%20piece%20of%20music%20can%20produce%20multiple%0Aplausible%20dance%20interpretations.%20This%20one-to-many%20mapping%20demands%20additional%0Aguidance%2C%20as%20music%20alone%20provides%20limited%20information%20for%20generating%20diverse%0Adance%20movements.%20The%20challenge%20is%20further%20amplified%20by%20the%20scarcity%20of%20paired%0Amusic%20and%20dance%20data%2C%20which%20restricts%20the%20model%5C%5Ea%5Cu%7BA%7D%5C%27Zs%20ability%20to%20learn%0Adiverse%20dance%20patterns.%20In%20this%20paper%2C%20we%20introduce%20DanceChat%2C%20a%20Large%20Language%0AModel%20%28LLM%29-guided%20music-to-dance%20generation%20approach.%20We%20use%20an%20LLM%20as%20a%0Achoreographer%20that%20provides%20textual%20motion%20instructions%2C%20offering%20explicit%2C%0Ahigh-level%20guidance%20for%20dance%20generation.%20This%20approach%20goes%20beyond%20implicit%0Alearning%20from%20music%20alone%2C%20enabling%20the%20model%20to%20generate%20dance%20that%20is%20both%0Amore%20diverse%20and%20better%20aligned%20with%20musical%20styles.%20Our%20approach%20consists%20of%0Athree%20components%3A%20%281%29%20an%20LLM-based%20pseudo%20instruction%20generation%20module%20that%0Aproduces%20textual%20dance%20guidance%20based%20on%20music%20style%20and%20structure%2C%20%282%29%20a%0Amulti-modal%20feature%20extraction%20and%20fusion%20module%20that%20integrates%20music%2C%20rhythm%2C%0Aand%20textual%20guidance%20into%20a%20shared%20representation%2C%20and%20%283%29%20a%20diffusion-based%0Amotion%20synthesis%20module%20together%20with%20a%20multi-modal%20alignment%20loss%2C%20which%0Aensures%20that%20the%20generated%20dance%20is%20aligned%20with%20both%20musical%20and%20textual%20cues.%0AExtensive%20experiments%20on%20AIST%2B%2B%20and%20human%20evaluations%20show%20that%20DanceChat%0Aoutperforms%20state-of-the-art%20methods%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10574v2&entry.124074799=Read"},
{"title": "Mitigating Biases in Surgical Operating Rooms with Geometry", "author": "Tony Danjun Wang and Tobias Czempiel and Nassir Navab and Lennart Bastian", "abstract": "  Deep neural networks are prone to learning spurious correlations, exploiting\ndataset-specific artifacts rather than meaningful features for prediction. In\nsurgical operating rooms (OR), these manifest through the standardization of\nsmocks and gowns that obscure robust identifying landmarks, introducing model\nbias for tasks related to modeling OR personnel. Through gradient-based\nsaliency analysis on two public OR datasets, we reveal that CNN models succumb\nto such shortcuts, fixating on incidental visual cues such as footwear beneath\nsurgical gowns, distinctive eyewear, or other role-specific identifiers.\nAvoiding such biases is essential for the next generation of intelligent\nassistance systems in the OR, which should accurately recognize personalized\nworkflow traits, such as surgical skill level or coordination with other staff\nmembers. We address this problem by encoding personnel as 3D point cloud\nsequences, disentangling identity-relevant shape and motion patterns from\nappearance-based confounders. Our experiments demonstrate that while RGB and\ngeometric methods achieve comparable performance on datasets with apparent\nsimulation artifacts, RGB models suffer a 12% accuracy drop in realistic\nclinical settings with decreased visual diversity due to standardizations. This\nperformance gap confirms that geometric representations capture more meaningful\nbiometric features, providing an avenue to developing robust methods of\nmodeling humans in the OR.\n", "link": "http://arxiv.org/abs/2508.08028v1", "date": "2025-08-11", "relevancy": 2.7231, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5539}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5489}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Biases%20in%20Surgical%20Operating%20Rooms%20with%20Geometry&body=Title%3A%20Mitigating%20Biases%20in%20Surgical%20Operating%20Rooms%20with%20Geometry%0AAuthor%3A%20Tony%20Danjun%20Wang%20and%20Tobias%20Czempiel%20and%20Nassir%20Navab%20and%20Lennart%20Bastian%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20prone%20to%20learning%20spurious%20correlations%2C%20exploiting%0Adataset-specific%20artifacts%20rather%20than%20meaningful%20features%20for%20prediction.%20In%0Asurgical%20operating%20rooms%20%28OR%29%2C%20these%20manifest%20through%20the%20standardization%20of%0Asmocks%20and%20gowns%20that%20obscure%20robust%20identifying%20landmarks%2C%20introducing%20model%0Abias%20for%20tasks%20related%20to%20modeling%20OR%20personnel.%20Through%20gradient-based%0Asaliency%20analysis%20on%20two%20public%20OR%20datasets%2C%20we%20reveal%20that%20CNN%20models%20succumb%0Ato%20such%20shortcuts%2C%20fixating%20on%20incidental%20visual%20cues%20such%20as%20footwear%20beneath%0Asurgical%20gowns%2C%20distinctive%20eyewear%2C%20or%20other%20role-specific%20identifiers.%0AAvoiding%20such%20biases%20is%20essential%20for%20the%20next%20generation%20of%20intelligent%0Aassistance%20systems%20in%20the%20OR%2C%20which%20should%20accurately%20recognize%20personalized%0Aworkflow%20traits%2C%20such%20as%20surgical%20skill%20level%20or%20coordination%20with%20other%20staff%0Amembers.%20We%20address%20this%20problem%20by%20encoding%20personnel%20as%203D%20point%20cloud%0Asequences%2C%20disentangling%20identity-relevant%20shape%20and%20motion%20patterns%20from%0Aappearance-based%20confounders.%20Our%20experiments%20demonstrate%20that%20while%20RGB%20and%0Ageometric%20methods%20achieve%20comparable%20performance%20on%20datasets%20with%20apparent%0Asimulation%20artifacts%2C%20RGB%20models%20suffer%20a%2012%25%20accuracy%20drop%20in%20realistic%0Aclinical%20settings%20with%20decreased%20visual%20diversity%20due%20to%20standardizations.%20This%0Aperformance%20gap%20confirms%20that%20geometric%20representations%20capture%20more%20meaningful%0Abiometric%20features%2C%20providing%20an%20avenue%20to%20developing%20robust%20methods%20of%0Amodeling%20humans%20in%20the%20OR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Biases%2520in%2520Surgical%2520Operating%2520Rooms%2520with%2520Geometry%26entry.906535625%3DTony%2520Danjun%2520Wang%2520and%2520Tobias%2520Czempiel%2520and%2520Nassir%2520Navab%2520and%2520Lennart%2520Bastian%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520prone%2520to%2520learning%2520spurious%2520correlations%252C%2520exploiting%250Adataset-specific%2520artifacts%2520rather%2520than%2520meaningful%2520features%2520for%2520prediction.%2520In%250Asurgical%2520operating%2520rooms%2520%2528OR%2529%252C%2520these%2520manifest%2520through%2520the%2520standardization%2520of%250Asmocks%2520and%2520gowns%2520that%2520obscure%2520robust%2520identifying%2520landmarks%252C%2520introducing%2520model%250Abias%2520for%2520tasks%2520related%2520to%2520modeling%2520OR%2520personnel.%2520Through%2520gradient-based%250Asaliency%2520analysis%2520on%2520two%2520public%2520OR%2520datasets%252C%2520we%2520reveal%2520that%2520CNN%2520models%2520succumb%250Ato%2520such%2520shortcuts%252C%2520fixating%2520on%2520incidental%2520visual%2520cues%2520such%2520as%2520footwear%2520beneath%250Asurgical%2520gowns%252C%2520distinctive%2520eyewear%252C%2520or%2520other%2520role-specific%2520identifiers.%250AAvoiding%2520such%2520biases%2520is%2520essential%2520for%2520the%2520next%2520generation%2520of%2520intelligent%250Aassistance%2520systems%2520in%2520the%2520OR%252C%2520which%2520should%2520accurately%2520recognize%2520personalized%250Aworkflow%2520traits%252C%2520such%2520as%2520surgical%2520skill%2520level%2520or%2520coordination%2520with%2520other%2520staff%250Amembers.%2520We%2520address%2520this%2520problem%2520by%2520encoding%2520personnel%2520as%25203D%2520point%2520cloud%250Asequences%252C%2520disentangling%2520identity-relevant%2520shape%2520and%2520motion%2520patterns%2520from%250Aappearance-based%2520confounders.%2520Our%2520experiments%2520demonstrate%2520that%2520while%2520RGB%2520and%250Ageometric%2520methods%2520achieve%2520comparable%2520performance%2520on%2520datasets%2520with%2520apparent%250Asimulation%2520artifacts%252C%2520RGB%2520models%2520suffer%2520a%252012%2525%2520accuracy%2520drop%2520in%2520realistic%250Aclinical%2520settings%2520with%2520decreased%2520visual%2520diversity%2520due%2520to%2520standardizations.%2520This%250Aperformance%2520gap%2520confirms%2520that%2520geometric%2520representations%2520capture%2520more%2520meaningful%250Abiometric%2520features%252C%2520providing%2520an%2520avenue%2520to%2520developing%2520robust%2520methods%2520of%250Amodeling%2520humans%2520in%2520the%2520OR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Biases%20in%20Surgical%20Operating%20Rooms%20with%20Geometry&entry.906535625=Tony%20Danjun%20Wang%20and%20Tobias%20Czempiel%20and%20Nassir%20Navab%20and%20Lennart%20Bastian&entry.1292438233=%20%20Deep%20neural%20networks%20are%20prone%20to%20learning%20spurious%20correlations%2C%20exploiting%0Adataset-specific%20artifacts%20rather%20than%20meaningful%20features%20for%20prediction.%20In%0Asurgical%20operating%20rooms%20%28OR%29%2C%20these%20manifest%20through%20the%20standardization%20of%0Asmocks%20and%20gowns%20that%20obscure%20robust%20identifying%20landmarks%2C%20introducing%20model%0Abias%20for%20tasks%20related%20to%20modeling%20OR%20personnel.%20Through%20gradient-based%0Asaliency%20analysis%20on%20two%20public%20OR%20datasets%2C%20we%20reveal%20that%20CNN%20models%20succumb%0Ato%20such%20shortcuts%2C%20fixating%20on%20incidental%20visual%20cues%20such%20as%20footwear%20beneath%0Asurgical%20gowns%2C%20distinctive%20eyewear%2C%20or%20other%20role-specific%20identifiers.%0AAvoiding%20such%20biases%20is%20essential%20for%20the%20next%20generation%20of%20intelligent%0Aassistance%20systems%20in%20the%20OR%2C%20which%20should%20accurately%20recognize%20personalized%0Aworkflow%20traits%2C%20such%20as%20surgical%20skill%20level%20or%20coordination%20with%20other%20staff%0Amembers.%20We%20address%20this%20problem%20by%20encoding%20personnel%20as%203D%20point%20cloud%0Asequences%2C%20disentangling%20identity-relevant%20shape%20and%20motion%20patterns%20from%0Aappearance-based%20confounders.%20Our%20experiments%20demonstrate%20that%20while%20RGB%20and%0Ageometric%20methods%20achieve%20comparable%20performance%20on%20datasets%20with%20apparent%0Asimulation%20artifacts%2C%20RGB%20models%20suffer%20a%2012%25%20accuracy%20drop%20in%20realistic%0Aclinical%20settings%20with%20decreased%20visual%20diversity%20due%20to%20standardizations.%20This%0Aperformance%20gap%20confirms%20that%20geometric%20representations%20capture%20more%20meaningful%0Abiometric%20features%2C%20providing%20an%20avenue%20to%20developing%20robust%20methods%20of%0Amodeling%20humans%20in%20the%20OR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08028v1&entry.124074799=Read"},
{"title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "author": "Zhongqi Yang and Wenhang Ge and Yuqi Li and Jiaqi Chen and Haoyuan Li and Mengyin An and Fei Kang and Hua Xue and Baixin Xu and Yuyang Yin and Eric Li and Yang Liu and Yikai Wang and Hao-Xiang Guo and Yahui Zhou", "abstract": "  Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.\n", "link": "http://arxiv.org/abs/2508.08086v1", "date": "2025-08-11", "relevancy": 2.7185, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6831}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6831}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix-3D%3A%20Omnidirectional%20Explorable%203D%20World%20Generation&body=Title%3A%20Matrix-3D%3A%20Omnidirectional%20Explorable%203D%20World%20Generation%0AAuthor%3A%20Zhongqi%20Yang%20and%20Wenhang%20Ge%20and%20Yuqi%20Li%20and%20Jiaqi%20Chen%20and%20Haoyuan%20Li%20and%20Mengyin%20An%20and%20Fei%20Kang%20and%20Hua%20Xue%20and%20Baixin%20Xu%20and%20Yuyang%20Yin%20and%20Eric%20Li%20and%20Yang%20Liu%20and%20Yikai%20Wang%20and%20Hao-Xiang%20Guo%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20Explorable%203D%20world%20generation%20from%20a%20single%20image%20or%20text%20prompt%20forms%20a%0Acornerstone%20of%20spatial%20intelligence.%20Recent%20works%20utilize%20video%20model%20to%0Aachieve%20wide-scope%20and%20generalizable%203D%20world%20generation.%20However%2C%20existing%0Aapproaches%20often%20suffer%20from%20a%20limited%20scope%20in%20the%20generated%20scenes.%20In%20this%0Awork%2C%20we%20propose%20Matrix-3D%2C%20a%20framework%20that%20utilize%20panoramic%20representation%0Afor%20wide-coverage%20omnidirectional%20explorable%203D%20world%20generation%20that%20combines%0Aconditional%20video%20generation%20and%20panoramic%203D%20reconstruction.%20We%20first%20train%20a%0Atrajectory-guided%20panoramic%20video%20diffusion%20model%20that%20employs%20scene%20mesh%0Arenders%20as%20condition%2C%20to%20enable%20high-quality%20and%20geometrically%20consistent%20scene%0Avideo%20generation.%20To%20lift%20the%20panorama%20scene%20video%20to%203D%20world%2C%20we%20propose%20two%0Aseparate%20methods%3A%20%281%29%20a%20feed-forward%20large%20panorama%20reconstruction%20model%20for%0Arapid%203D%20scene%20reconstruction%20and%20%282%29%20an%20optimization-based%20pipeline%20for%0Aaccurate%20and%20detailed%203D%20scene%20reconstruction.%20To%20facilitate%20effective%0Atraining%2C%20we%20also%20introduce%20the%20Matrix-Pano%20dataset%2C%20the%20first%20large-scale%0Asynthetic%20collection%20comprising%20116K%20high-quality%20static%20panoramic%20video%0Asequences%20with%20depth%20and%20trajectory%20annotations.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20framework%20achieves%20state-of-the-art%20performance%0Ain%20panoramic%20video%20generation%20and%203D%20world%20generation.%20See%20more%20in%0Ahttps%3A//matrix-3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix-3D%253A%2520Omnidirectional%2520Explorable%25203D%2520World%2520Generation%26entry.906535625%3DZhongqi%2520Yang%2520and%2520Wenhang%2520Ge%2520and%2520Yuqi%2520Li%2520and%2520Jiaqi%2520Chen%2520and%2520Haoyuan%2520Li%2520and%2520Mengyin%2520An%2520and%2520Fei%2520Kang%2520and%2520Hua%2520Xue%2520and%2520Baixin%2520Xu%2520and%2520Yuyang%2520Yin%2520and%2520Eric%2520Li%2520and%2520Yang%2520Liu%2520and%2520Yikai%2520Wang%2520and%2520Hao-Xiang%2520Guo%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520Explorable%25203D%2520world%2520generation%2520from%2520a%2520single%2520image%2520or%2520text%2520prompt%2520forms%2520a%250Acornerstone%2520of%2520spatial%2520intelligence.%2520Recent%2520works%2520utilize%2520video%2520model%2520to%250Aachieve%2520wide-scope%2520and%2520generalizable%25203D%2520world%2520generation.%2520However%252C%2520existing%250Aapproaches%2520often%2520suffer%2520from%2520a%2520limited%2520scope%2520in%2520the%2520generated%2520scenes.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Matrix-3D%252C%2520a%2520framework%2520that%2520utilize%2520panoramic%2520representation%250Afor%2520wide-coverage%2520omnidirectional%2520explorable%25203D%2520world%2520generation%2520that%2520combines%250Aconditional%2520video%2520generation%2520and%2520panoramic%25203D%2520reconstruction.%2520We%2520first%2520train%2520a%250Atrajectory-guided%2520panoramic%2520video%2520diffusion%2520model%2520that%2520employs%2520scene%2520mesh%250Arenders%2520as%2520condition%252C%2520to%2520enable%2520high-quality%2520and%2520geometrically%2520consistent%2520scene%250Avideo%2520generation.%2520To%2520lift%2520the%2520panorama%2520scene%2520video%2520to%25203D%2520world%252C%2520we%2520propose%2520two%250Aseparate%2520methods%253A%2520%25281%2529%2520a%2520feed-forward%2520large%2520panorama%2520reconstruction%2520model%2520for%250Arapid%25203D%2520scene%2520reconstruction%2520and%2520%25282%2529%2520an%2520optimization-based%2520pipeline%2520for%250Aaccurate%2520and%2520detailed%25203D%2520scene%2520reconstruction.%2520To%2520facilitate%2520effective%250Atraining%252C%2520we%2520also%2520introduce%2520the%2520Matrix-Pano%2520dataset%252C%2520the%2520first%2520large-scale%250Asynthetic%2520collection%2520comprising%2520116K%2520high-quality%2520static%2520panoramic%2520video%250Asequences%2520with%2520depth%2520and%2520trajectory%2520annotations.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520proposed%2520framework%2520achieves%2520state-of-the-art%2520performance%250Ain%2520panoramic%2520video%2520generation%2520and%25203D%2520world%2520generation.%2520See%2520more%2520in%250Ahttps%253A//matrix-3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-3D%3A%20Omnidirectional%20Explorable%203D%20World%20Generation&entry.906535625=Zhongqi%20Yang%20and%20Wenhang%20Ge%20and%20Yuqi%20Li%20and%20Jiaqi%20Chen%20and%20Haoyuan%20Li%20and%20Mengyin%20An%20and%20Fei%20Kang%20and%20Hua%20Xue%20and%20Baixin%20Xu%20and%20Yuyang%20Yin%20and%20Eric%20Li%20and%20Yang%20Liu%20and%20Yikai%20Wang%20and%20Hao-Xiang%20Guo%20and%20Yahui%20Zhou&entry.1292438233=%20%20Explorable%203D%20world%20generation%20from%20a%20single%20image%20or%20text%20prompt%20forms%20a%0Acornerstone%20of%20spatial%20intelligence.%20Recent%20works%20utilize%20video%20model%20to%0Aachieve%20wide-scope%20and%20generalizable%203D%20world%20generation.%20However%2C%20existing%0Aapproaches%20often%20suffer%20from%20a%20limited%20scope%20in%20the%20generated%20scenes.%20In%20this%0Awork%2C%20we%20propose%20Matrix-3D%2C%20a%20framework%20that%20utilize%20panoramic%20representation%0Afor%20wide-coverage%20omnidirectional%20explorable%203D%20world%20generation%20that%20combines%0Aconditional%20video%20generation%20and%20panoramic%203D%20reconstruction.%20We%20first%20train%20a%0Atrajectory-guided%20panoramic%20video%20diffusion%20model%20that%20employs%20scene%20mesh%0Arenders%20as%20condition%2C%20to%20enable%20high-quality%20and%20geometrically%20consistent%20scene%0Avideo%20generation.%20To%20lift%20the%20panorama%20scene%20video%20to%203D%20world%2C%20we%20propose%20two%0Aseparate%20methods%3A%20%281%29%20a%20feed-forward%20large%20panorama%20reconstruction%20model%20for%0Arapid%203D%20scene%20reconstruction%20and%20%282%29%20an%20optimization-based%20pipeline%20for%0Aaccurate%20and%20detailed%203D%20scene%20reconstruction.%20To%20facilitate%20effective%0Atraining%2C%20we%20also%20introduce%20the%20Matrix-Pano%20dataset%2C%20the%20first%20large-scale%0Asynthetic%20collection%20comprising%20116K%20high-quality%20static%20panoramic%20video%0Asequences%20with%20depth%20and%20trajectory%20annotations.%20Extensive%20experiments%0Ademonstrate%20that%20our%20proposed%20framework%20achieves%20state-of-the-art%20performance%0Ain%20panoramic%20video%20generation%20and%203D%20world%20generation.%20See%20more%20in%0Ahttps%3A//matrix-3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08086v1&entry.124074799=Read"},
{"title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust\n  Detection and Fine Grained-Localization", "author": "Nicholas Klein and Hemlata Tak and James Fullwood and Krishna Regmi and Leonidas Spinoulas and Ganesh Sivaraman and Tianxiang Chen and Elie Khoury", "abstract": "  The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset.\n", "link": "http://arxiv.org/abs/2508.08141v1", "date": "2025-08-11", "relevancy": 2.7084, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5407}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pindrop%20it%21%20Audio%20and%20Visual%20Deepfake%20Countermeasures%20for%20Robust%0A%20%20Detection%20and%20Fine%20Grained-Localization&body=Title%3A%20Pindrop%20it%21%20Audio%20and%20Visual%20Deepfake%20Countermeasures%20for%20Robust%0A%20%20Detection%20and%20Fine%20Grained-Localization%0AAuthor%3A%20Nicholas%20Klein%20and%20Hemlata%20Tak%20and%20James%20Fullwood%20and%20Krishna%20Regmi%20and%20Leonidas%20Spinoulas%20and%20Ganesh%20Sivaraman%20and%20Tianxiang%20Chen%20and%20Elie%20Khoury%0AAbstract%3A%20%20%20The%20field%20of%20visual%20and%20audio%20generation%20is%20burgeoning%20with%20new%0Astate-of-the-art%20methods.%20This%20rapid%20proliferation%20of%20new%20techniques%0Aunderscores%20the%20need%20for%20robust%20solutions%20for%20detecting%20synthetic%20content%20in%0Avideos.%20In%20particular%2C%20when%20fine-grained%20alterations%20via%20localized%0Amanipulations%20are%20performed%20in%20visual%2C%20audio%2C%20or%20both%20domains%2C%20these%20subtle%0Amodifications%20add%20challenges%20to%20the%20detection%20algorithms.%20This%20paper%20presents%0Asolutions%20for%20the%20problems%20of%20deepfake%20video%20classification%20and%20localization.%0AThe%20methods%20were%20submitted%20to%20the%20ACM%201M%20Deepfakes%20Detection%20Challenge%2C%0Aachieving%20the%20best%20performance%20in%20the%20temporal%20localization%20task%20and%20a%20top%20four%0Aranking%20in%20the%20classification%20task%20for%20the%20TestA%20split%20of%20the%20evaluation%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPindrop%2520it%2521%2520Audio%2520and%2520Visual%2520Deepfake%2520Countermeasures%2520for%2520Robust%250A%2520%2520Detection%2520and%2520Fine%2520Grained-Localization%26entry.906535625%3DNicholas%2520Klein%2520and%2520Hemlata%2520Tak%2520and%2520James%2520Fullwood%2520and%2520Krishna%2520Regmi%2520and%2520Leonidas%2520Spinoulas%2520and%2520Ganesh%2520Sivaraman%2520and%2520Tianxiang%2520Chen%2520and%2520Elie%2520Khoury%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520visual%2520and%2520audio%2520generation%2520is%2520burgeoning%2520with%2520new%250Astate-of-the-art%2520methods.%2520This%2520rapid%2520proliferation%2520of%2520new%2520techniques%250Aunderscores%2520the%2520need%2520for%2520robust%2520solutions%2520for%2520detecting%2520synthetic%2520content%2520in%250Avideos.%2520In%2520particular%252C%2520when%2520fine-grained%2520alterations%2520via%2520localized%250Amanipulations%2520are%2520performed%2520in%2520visual%252C%2520audio%252C%2520or%2520both%2520domains%252C%2520these%2520subtle%250Amodifications%2520add%2520challenges%2520to%2520the%2520detection%2520algorithms.%2520This%2520paper%2520presents%250Asolutions%2520for%2520the%2520problems%2520of%2520deepfake%2520video%2520classification%2520and%2520localization.%250AThe%2520methods%2520were%2520submitted%2520to%2520the%2520ACM%25201M%2520Deepfakes%2520Detection%2520Challenge%252C%250Aachieving%2520the%2520best%2520performance%2520in%2520the%2520temporal%2520localization%2520task%2520and%2520a%2520top%2520four%250Aranking%2520in%2520the%2520classification%2520task%2520for%2520the%2520TestA%2520split%2520of%2520the%2520evaluation%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pindrop%20it%21%20Audio%20and%20Visual%20Deepfake%20Countermeasures%20for%20Robust%0A%20%20Detection%20and%20Fine%20Grained-Localization&entry.906535625=Nicholas%20Klein%20and%20Hemlata%20Tak%20and%20James%20Fullwood%20and%20Krishna%20Regmi%20and%20Leonidas%20Spinoulas%20and%20Ganesh%20Sivaraman%20and%20Tianxiang%20Chen%20and%20Elie%20Khoury&entry.1292438233=%20%20The%20field%20of%20visual%20and%20audio%20generation%20is%20burgeoning%20with%20new%0Astate-of-the-art%20methods.%20This%20rapid%20proliferation%20of%20new%20techniques%0Aunderscores%20the%20need%20for%20robust%20solutions%20for%20detecting%20synthetic%20content%20in%0Avideos.%20In%20particular%2C%20when%20fine-grained%20alterations%20via%20localized%0Amanipulations%20are%20performed%20in%20visual%2C%20audio%2C%20or%20both%20domains%2C%20these%20subtle%0Amodifications%20add%20challenges%20to%20the%20detection%20algorithms.%20This%20paper%20presents%0Asolutions%20for%20the%20problems%20of%20deepfake%20video%20classification%20and%20localization.%0AThe%20methods%20were%20submitted%20to%20the%20ACM%201M%20Deepfakes%20Detection%20Challenge%2C%0Aachieving%20the%20best%20performance%20in%20the%20temporal%20localization%20task%20and%20a%20top%20four%0Aranking%20in%20the%20classification%20task%20for%20the%20TestA%20split%20of%20the%20evaluation%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08141v1&entry.124074799=Read"},
{"title": "DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video\n  Diffusion Models", "author": "Hyeonwoo Kim and Sangwon Baik and Hanbyul Joo", "abstract": "  Modeling how humans interact with objects is crucial for AI to effectively\nassist or mimic human behaviors. Existing studies for learning such ability\nprimarily focus on static human-object interaction (HOI) patterns, such as\ncontact and spatial relationships, while dynamic HOI patterns, capturing the\nmovement of humans and objects over time, remain relatively underexplored. In\nthis paper, we present a novel framework for learning Dynamic Affordance across\nvarious target object categories. To address the scarcity of 4D HOI datasets,\nour method learns the 3D dynamic affordance from synthetically generated 4D HOI\nsamples. Specifically, we propose a pipeline that first generates 2D HOI videos\nfrom a given 3D target object using a pre-trained video diffusion model, then\nlifts them into 3D to generate 4D HOI samples. Leveraging these synthesized 4D\nHOI samples, we train DAViD, our generative 4D human-object interaction model,\nwhich is composed of two key components: (1) a human motion diffusion model\n(MDM) with Low-Rank Adaptation (LoRA) module to fine-tune a pre-trained MDM to\nlearn the HOI motion concepts from limited HOI motion samples, (2) a motion\ndiffusion model for 4D object poses conditioned by produced human interaction\nmotions. Interestingly, DAViD can integrate newly learned HOI motion concepts\nwith pre-trained human motions to create novel HOI motions, even for multiple\nHOI motion concepts, demonstrating the advantage of our pipeline with LoRA in\nintegrating dynamic HOI concepts. Through extensive experiments, we demonstrate\nthat DAViD outperforms baselines in synthesizing HOI motion.\n", "link": "http://arxiv.org/abs/2501.08333v3", "date": "2025-08-11", "relevancy": 2.662, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7218}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6274}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.62}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20Using%20Pre-trained%20Video%0A%20%20Diffusion%20Models&body=Title%3A%20DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20Using%20Pre-trained%20Video%0A%20%20Diffusion%20Models%0AAuthor%3A%20Hyeonwoo%20Kim%20and%20Sangwon%20Baik%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20Modeling%20how%20humans%20interact%20with%20objects%20is%20crucial%20for%20AI%20to%20effectively%0Aassist%20or%20mimic%20human%20behaviors.%20Existing%20studies%20for%20learning%20such%20ability%0Aprimarily%20focus%20on%20static%20human-object%20interaction%20%28HOI%29%20patterns%2C%20such%20as%0Acontact%20and%20spatial%20relationships%2C%20while%20dynamic%20HOI%20patterns%2C%20capturing%20the%0Amovement%20of%20humans%20and%20objects%20over%20time%2C%20remain%20relatively%20underexplored.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20framework%20for%20learning%20Dynamic%20Affordance%20across%0Avarious%20target%20object%20categories.%20To%20address%20the%20scarcity%20of%204D%20HOI%20datasets%2C%0Aour%20method%20learns%20the%203D%20dynamic%20affordance%20from%20synthetically%20generated%204D%20HOI%0Asamples.%20Specifically%2C%20we%20propose%20a%20pipeline%20that%20first%20generates%202D%20HOI%20videos%0Afrom%20a%20given%203D%20target%20object%20using%20a%20pre-trained%20video%20diffusion%20model%2C%20then%0Alifts%20them%20into%203D%20to%20generate%204D%20HOI%20samples.%20Leveraging%20these%20synthesized%204D%0AHOI%20samples%2C%20we%20train%20DAViD%2C%20our%20generative%204D%20human-object%20interaction%20model%2C%0Awhich%20is%20composed%20of%20two%20key%20components%3A%20%281%29%20a%20human%20motion%20diffusion%20model%0A%28MDM%29%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20module%20to%20fine-tune%20a%20pre-trained%20MDM%20to%0Alearn%20the%20HOI%20motion%20concepts%20from%20limited%20HOI%20motion%20samples%2C%20%282%29%20a%20motion%0Adiffusion%20model%20for%204D%20object%20poses%20conditioned%20by%20produced%20human%20interaction%0Amotions.%20Interestingly%2C%20DAViD%20can%20integrate%20newly%20learned%20HOI%20motion%20concepts%0Awith%20pre-trained%20human%20motions%20to%20create%20novel%20HOI%20motions%2C%20even%20for%20multiple%0AHOI%20motion%20concepts%2C%20demonstrating%20the%20advantage%20of%20our%20pipeline%20with%20LoRA%20in%0Aintegrating%20dynamic%20HOI%20concepts.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20DAViD%20outperforms%20baselines%20in%20synthesizing%20HOI%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08333v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAViD%253A%2520Modeling%2520Dynamic%2520Affordance%2520of%25203D%2520Objects%2520Using%2520Pre-trained%2520Video%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DHyeonwoo%2520Kim%2520and%2520Sangwon%2520Baik%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3D%2520%2520Modeling%2520how%2520humans%2520interact%2520with%2520objects%2520is%2520crucial%2520for%2520AI%2520to%2520effectively%250Aassist%2520or%2520mimic%2520human%2520behaviors.%2520Existing%2520studies%2520for%2520learning%2520such%2520ability%250Aprimarily%2520focus%2520on%2520static%2520human-object%2520interaction%2520%2528HOI%2529%2520patterns%252C%2520such%2520as%250Acontact%2520and%2520spatial%2520relationships%252C%2520while%2520dynamic%2520HOI%2520patterns%252C%2520capturing%2520the%250Amovement%2520of%2520humans%2520and%2520objects%2520over%2520time%252C%2520remain%2520relatively%2520underexplored.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520learning%2520Dynamic%2520Affordance%2520across%250Avarious%2520target%2520object%2520categories.%2520To%2520address%2520the%2520scarcity%2520of%25204D%2520HOI%2520datasets%252C%250Aour%2520method%2520learns%2520the%25203D%2520dynamic%2520affordance%2520from%2520synthetically%2520generated%25204D%2520HOI%250Asamples.%2520Specifically%252C%2520we%2520propose%2520a%2520pipeline%2520that%2520first%2520generates%25202D%2520HOI%2520videos%250Afrom%2520a%2520given%25203D%2520target%2520object%2520using%2520a%2520pre-trained%2520video%2520diffusion%2520model%252C%2520then%250Alifts%2520them%2520into%25203D%2520to%2520generate%25204D%2520HOI%2520samples.%2520Leveraging%2520these%2520synthesized%25204D%250AHOI%2520samples%252C%2520we%2520train%2520DAViD%252C%2520our%2520generative%25204D%2520human-object%2520interaction%2520model%252C%250Awhich%2520is%2520composed%2520of%2520two%2520key%2520components%253A%2520%25281%2529%2520a%2520human%2520motion%2520diffusion%2520model%250A%2528MDM%2529%2520with%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520module%2520to%2520fine-tune%2520a%2520pre-trained%2520MDM%2520to%250Alearn%2520the%2520HOI%2520motion%2520concepts%2520from%2520limited%2520HOI%2520motion%2520samples%252C%2520%25282%2529%2520a%2520motion%250Adiffusion%2520model%2520for%25204D%2520object%2520poses%2520conditioned%2520by%2520produced%2520human%2520interaction%250Amotions.%2520Interestingly%252C%2520DAViD%2520can%2520integrate%2520newly%2520learned%2520HOI%2520motion%2520concepts%250Awith%2520pre-trained%2520human%2520motions%2520to%2520create%2520novel%2520HOI%2520motions%252C%2520even%2520for%2520multiple%250AHOI%2520motion%2520concepts%252C%2520demonstrating%2520the%2520advantage%2520of%2520our%2520pipeline%2520with%2520LoRA%2520in%250Aintegrating%2520dynamic%2520HOI%2520concepts.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520DAViD%2520outperforms%2520baselines%2520in%2520synthesizing%2520HOI%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08333v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAViD%3A%20Modeling%20Dynamic%20Affordance%20of%203D%20Objects%20Using%20Pre-trained%20Video%0A%20%20Diffusion%20Models&entry.906535625=Hyeonwoo%20Kim%20and%20Sangwon%20Baik%20and%20Hanbyul%20Joo&entry.1292438233=%20%20Modeling%20how%20humans%20interact%20with%20objects%20is%20crucial%20for%20AI%20to%20effectively%0Aassist%20or%20mimic%20human%20behaviors.%20Existing%20studies%20for%20learning%20such%20ability%0Aprimarily%20focus%20on%20static%20human-object%20interaction%20%28HOI%29%20patterns%2C%20such%20as%0Acontact%20and%20spatial%20relationships%2C%20while%20dynamic%20HOI%20patterns%2C%20capturing%20the%0Amovement%20of%20humans%20and%20objects%20over%20time%2C%20remain%20relatively%20underexplored.%20In%0Athis%20paper%2C%20we%20present%20a%20novel%20framework%20for%20learning%20Dynamic%20Affordance%20across%0Avarious%20target%20object%20categories.%20To%20address%20the%20scarcity%20of%204D%20HOI%20datasets%2C%0Aour%20method%20learns%20the%203D%20dynamic%20affordance%20from%20synthetically%20generated%204D%20HOI%0Asamples.%20Specifically%2C%20we%20propose%20a%20pipeline%20that%20first%20generates%202D%20HOI%20videos%0Afrom%20a%20given%203D%20target%20object%20using%20a%20pre-trained%20video%20diffusion%20model%2C%20then%0Alifts%20them%20into%203D%20to%20generate%204D%20HOI%20samples.%20Leveraging%20these%20synthesized%204D%0AHOI%20samples%2C%20we%20train%20DAViD%2C%20our%20generative%204D%20human-object%20interaction%20model%2C%0Awhich%20is%20composed%20of%20two%20key%20components%3A%20%281%29%20a%20human%20motion%20diffusion%20model%0A%28MDM%29%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20module%20to%20fine-tune%20a%20pre-trained%20MDM%20to%0Alearn%20the%20HOI%20motion%20concepts%20from%20limited%20HOI%20motion%20samples%2C%20%282%29%20a%20motion%0Adiffusion%20model%20for%204D%20object%20poses%20conditioned%20by%20produced%20human%20interaction%0Amotions.%20Interestingly%2C%20DAViD%20can%20integrate%20newly%20learned%20HOI%20motion%20concepts%0Awith%20pre-trained%20human%20motions%20to%20create%20novel%20HOI%20motions%2C%20even%20for%20multiple%0AHOI%20motion%20concepts%2C%20demonstrating%20the%20advantage%20of%20our%20pipeline%20with%20LoRA%20in%0Aintegrating%20dynamic%20HOI%20concepts.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20DAViD%20outperforms%20baselines%20in%20synthesizing%20HOI%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08333v3&entry.124074799=Read"},
{"title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust\n  Tracking", "author": "Tony Danjun Wang and Christian Heiliger and Nassir Navab and Lennart Bastian", "abstract": "  Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support.\n", "link": "http://arxiv.org/abs/2508.07968v1", "date": "2025-08-11", "relevancy": 2.6549, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5333}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackOR%3A%20Towards%20Personalized%20Intelligent%20Operating%20Rooms%20Through%20Robust%0A%20%20Tracking&body=Title%3A%20TrackOR%3A%20Towards%20Personalized%20Intelligent%20Operating%20Rooms%20Through%20Robust%0A%20%20Tracking%0AAuthor%3A%20Tony%20Danjun%20Wang%20and%20Christian%20Heiliger%20and%20Nassir%20Navab%20and%20Lennart%20Bastian%0AAbstract%3A%20%20%20Providing%20intelligent%20support%20to%20surgical%20teams%20is%20a%20key%20frontier%20in%0Aautomated%20surgical%20scene%20understanding%2C%20with%20the%20long-term%20goal%20of%20improving%0Apatient%20outcomes.%20Developing%20personalized%20intelligence%20for%20all%20staff%20members%0Arequires%20maintaining%20a%20consistent%20state%20of%20who%20is%20located%20where%20for%20long%0Asurgical%20procedures%2C%20which%20still%20poses%20numerous%20computational%20challenges.%20We%0Apropose%20TrackOR%2C%20a%20framework%20for%20tackling%20long-term%20multi-person%20tracking%20and%0Are-identification%20in%20the%20operating%20room.%20TrackOR%20uses%203D%20geometric%20signatures%0Ato%20achieve%20state-of-the-art%20online%20tracking%20performance%20%28%2B11%25%20Association%0AAccuracy%20over%20the%20strongest%20baseline%29%2C%20while%20also%20enabling%20an%20effective%20offline%0Arecovery%20process%20to%20create%20analysis-ready%20trajectories.%20Our%20work%20shows%20that%20by%0Aleveraging%203D%20geometric%20information%2C%20persistent%20identity%20tracking%20becomes%0Aattainable%2C%20enabling%20a%20critical%20shift%20towards%20the%20more%20granular%2C%20staff-centric%0Aanalyses%20required%20for%20personalized%20intelligent%20systems%20in%20the%20operating%20room.%0AThis%20new%20capability%20opens%20up%20various%20applications%2C%20including%20our%20proposed%0Atemporal%20pathway%20imprints%20that%20translate%20raw%20tracking%20data%20into%20actionable%0Ainsights%20for%20improving%20team%20efficiency%20and%20safety%20and%20ultimately%20providing%0Apersonalized%20support.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackOR%253A%2520Towards%2520Personalized%2520Intelligent%2520Operating%2520Rooms%2520Through%2520Robust%250A%2520%2520Tracking%26entry.906535625%3DTony%2520Danjun%2520Wang%2520and%2520Christian%2520Heiliger%2520and%2520Nassir%2520Navab%2520and%2520Lennart%2520Bastian%26entry.1292438233%3D%2520%2520Providing%2520intelligent%2520support%2520to%2520surgical%2520teams%2520is%2520a%2520key%2520frontier%2520in%250Aautomated%2520surgical%2520scene%2520understanding%252C%2520with%2520the%2520long-term%2520goal%2520of%2520improving%250Apatient%2520outcomes.%2520Developing%2520personalized%2520intelligence%2520for%2520all%2520staff%2520members%250Arequires%2520maintaining%2520a%2520consistent%2520state%2520of%2520who%2520is%2520located%2520where%2520for%2520long%250Asurgical%2520procedures%252C%2520which%2520still%2520poses%2520numerous%2520computational%2520challenges.%2520We%250Apropose%2520TrackOR%252C%2520a%2520framework%2520for%2520tackling%2520long-term%2520multi-person%2520tracking%2520and%250Are-identification%2520in%2520the%2520operating%2520room.%2520TrackOR%2520uses%25203D%2520geometric%2520signatures%250Ato%2520achieve%2520state-of-the-art%2520online%2520tracking%2520performance%2520%2528%252B11%2525%2520Association%250AAccuracy%2520over%2520the%2520strongest%2520baseline%2529%252C%2520while%2520also%2520enabling%2520an%2520effective%2520offline%250Arecovery%2520process%2520to%2520create%2520analysis-ready%2520trajectories.%2520Our%2520work%2520shows%2520that%2520by%250Aleveraging%25203D%2520geometric%2520information%252C%2520persistent%2520identity%2520tracking%2520becomes%250Aattainable%252C%2520enabling%2520a%2520critical%2520shift%2520towards%2520the%2520more%2520granular%252C%2520staff-centric%250Aanalyses%2520required%2520for%2520personalized%2520intelligent%2520systems%2520in%2520the%2520operating%2520room.%250AThis%2520new%2520capability%2520opens%2520up%2520various%2520applications%252C%2520including%2520our%2520proposed%250Atemporal%2520pathway%2520imprints%2520that%2520translate%2520raw%2520tracking%2520data%2520into%2520actionable%250Ainsights%2520for%2520improving%2520team%2520efficiency%2520and%2520safety%2520and%2520ultimately%2520providing%250Apersonalized%2520support.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackOR%3A%20Towards%20Personalized%20Intelligent%20Operating%20Rooms%20Through%20Robust%0A%20%20Tracking&entry.906535625=Tony%20Danjun%20Wang%20and%20Christian%20Heiliger%20and%20Nassir%20Navab%20and%20Lennart%20Bastian&entry.1292438233=%20%20Providing%20intelligent%20support%20to%20surgical%20teams%20is%20a%20key%20frontier%20in%0Aautomated%20surgical%20scene%20understanding%2C%20with%20the%20long-term%20goal%20of%20improving%0Apatient%20outcomes.%20Developing%20personalized%20intelligence%20for%20all%20staff%20members%0Arequires%20maintaining%20a%20consistent%20state%20of%20who%20is%20located%20where%20for%20long%0Asurgical%20procedures%2C%20which%20still%20poses%20numerous%20computational%20challenges.%20We%0Apropose%20TrackOR%2C%20a%20framework%20for%20tackling%20long-term%20multi-person%20tracking%20and%0Are-identification%20in%20the%20operating%20room.%20TrackOR%20uses%203D%20geometric%20signatures%0Ato%20achieve%20state-of-the-art%20online%20tracking%20performance%20%28%2B11%25%20Association%0AAccuracy%20over%20the%20strongest%20baseline%29%2C%20while%20also%20enabling%20an%20effective%20offline%0Arecovery%20process%20to%20create%20analysis-ready%20trajectories.%20Our%20work%20shows%20that%20by%0Aleveraging%203D%20geometric%20information%2C%20persistent%20identity%20tracking%20becomes%0Aattainable%2C%20enabling%20a%20critical%20shift%20towards%20the%20more%20granular%2C%20staff-centric%0Aanalyses%20required%20for%20personalized%20intelligent%20systems%20in%20the%20operating%20room.%0AThis%20new%20capability%20opens%20up%20various%20applications%2C%20including%20our%20proposed%0Atemporal%20pathway%20imprints%20that%20translate%20raw%20tracking%20data%20into%20actionable%0Ainsights%20for%20improving%20team%20efficiency%20and%20safety%20and%20ultimately%20providing%0Apersonalized%20support.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07968v1&entry.124074799=Read"},
{"title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge,\n  Truthfulness, Refusal, and Confidence", "author": "Hongzhe Du and Weikai Li and Min Cai and Karim Saraipour and Zimin Zhang and Himabindu Lakkaraju and Yizhou Sun and Shichang Zhang", "abstract": "  Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis.\n", "link": "http://arxiv.org/abs/2504.02904v2", "date": "2025-08-11", "relevancy": 2.6454, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Post-Training%20Reshapes%20LLMs%3A%20A%20Mechanistic%20View%20on%20Knowledge%2C%0A%20%20Truthfulness%2C%20Refusal%2C%20and%20Confidence&body=Title%3A%20How%20Post-Training%20Reshapes%20LLMs%3A%20A%20Mechanistic%20View%20on%20Knowledge%2C%0A%20%20Truthfulness%2C%20Refusal%2C%20and%20Confidence%0AAuthor%3A%20Hongzhe%20Du%20and%20Weikai%20Li%20and%20Min%20Cai%20and%20Karim%20Saraipour%20and%20Zimin%20Zhang%20and%20Himabindu%20Lakkaraju%20and%20Yizhou%20Sun%20and%20Shichang%20Zhang%0AAbstract%3A%20%20%20Post-training%20is%20essential%20for%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%0Atransforming%20pre-trained%20base%20models%20into%20more%20useful%20and%20aligned%20post-trained%0Amodels.%20While%20plenty%20of%20works%20have%20studied%20post-training%20algorithms%20and%0Aevaluated%20post-training%20models%20by%20their%20outputs%2C%20it%20remains%20understudied%20how%0Apost-training%20reshapes%20LLMs%20internally.%20In%20this%20paper%2C%20we%20compare%20base%20and%0Apost-trained%20LLMs%20mechanistically%20from%20four%20perspectives%20to%20better%20understand%0Apost-training%20effects.%20Our%20findings%20across%20model%20families%20and%20datasets%20reveal%0Athat%3A%20%281%29%20Post-training%20does%20not%20change%20the%20factual%20knowledge%20storage%0Alocations%2C%20and%20it%20adapts%20knowledge%20representations%20from%20the%20base%20model%20while%0Adeveloping%20new%20knowledge%20representations%3B%20%282%29%20Both%20truthfulness%20and%20refusal%20can%0Abe%20represented%20by%20vectors%20in%20the%20hidden%20representation%20space.%20The%20truthfulness%0Adirection%20is%20highly%20similar%20between%20the%20base%20and%20post-trained%20model%2C%20and%20it%20is%0Aeffectively%20transferable%20for%20interventions%3B%20%283%29%20The%20refusal%20direction%20is%0Adifferent%20between%20the%20base%20and%20post-trained%20models%2C%20and%20it%20shows%20limited%0Aforward%20transferability%3B%20%284%29%20Differences%20in%20confidence%20between%20the%20base%20and%0Apost-trained%20models%20cannot%20be%20attributed%20to%20entropy%20neurons.%20Our%20study%20provides%0Ainsights%20into%20the%20fundamental%20mechanisms%20preserved%20and%20altered%20during%0Apost-training%2C%20facilitates%20downstream%20tasks%20like%20model%20steering%2C%20and%20could%0Apotentially%20benefit%20future%20research%20in%20interpretability%20and%20LLM%20post-training.%0AOur%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/HZD01/post-training-mechanistic-analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Post-Training%2520Reshapes%2520LLMs%253A%2520A%2520Mechanistic%2520View%2520on%2520Knowledge%252C%250A%2520%2520Truthfulness%252C%2520Refusal%252C%2520and%2520Confidence%26entry.906535625%3DHongzhe%2520Du%2520and%2520Weikai%2520Li%2520and%2520Min%2520Cai%2520and%2520Karim%2520Saraipour%2520and%2520Zimin%2520Zhang%2520and%2520Himabindu%2520Lakkaraju%2520and%2520Yizhou%2520Sun%2520and%2520Shichang%2520Zhang%26entry.1292438233%3D%2520%2520Post-training%2520is%2520essential%2520for%2520the%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Atransforming%2520pre-trained%2520base%2520models%2520into%2520more%2520useful%2520and%2520aligned%2520post-trained%250Amodels.%2520While%2520plenty%2520of%2520works%2520have%2520studied%2520post-training%2520algorithms%2520and%250Aevaluated%2520post-training%2520models%2520by%2520their%2520outputs%252C%2520it%2520remains%2520understudied%2520how%250Apost-training%2520reshapes%2520LLMs%2520internally.%2520In%2520this%2520paper%252C%2520we%2520compare%2520base%2520and%250Apost-trained%2520LLMs%2520mechanistically%2520from%2520four%2520perspectives%2520to%2520better%2520understand%250Apost-training%2520effects.%2520Our%2520findings%2520across%2520model%2520families%2520and%2520datasets%2520reveal%250Athat%253A%2520%25281%2529%2520Post-training%2520does%2520not%2520change%2520the%2520factual%2520knowledge%2520storage%250Alocations%252C%2520and%2520it%2520adapts%2520knowledge%2520representations%2520from%2520the%2520base%2520model%2520while%250Adeveloping%2520new%2520knowledge%2520representations%253B%2520%25282%2529%2520Both%2520truthfulness%2520and%2520refusal%2520can%250Abe%2520represented%2520by%2520vectors%2520in%2520the%2520hidden%2520representation%2520space.%2520The%2520truthfulness%250Adirection%2520is%2520highly%2520similar%2520between%2520the%2520base%2520and%2520post-trained%2520model%252C%2520and%2520it%2520is%250Aeffectively%2520transferable%2520for%2520interventions%253B%2520%25283%2529%2520The%2520refusal%2520direction%2520is%250Adifferent%2520between%2520the%2520base%2520and%2520post-trained%2520models%252C%2520and%2520it%2520shows%2520limited%250Aforward%2520transferability%253B%2520%25284%2529%2520Differences%2520in%2520confidence%2520between%2520the%2520base%2520and%250Apost-trained%2520models%2520cannot%2520be%2520attributed%2520to%2520entropy%2520neurons.%2520Our%2520study%2520provides%250Ainsights%2520into%2520the%2520fundamental%2520mechanisms%2520preserved%2520and%2520altered%2520during%250Apost-training%252C%2520facilitates%2520downstream%2520tasks%2520like%2520model%2520steering%252C%2520and%2520could%250Apotentially%2520benefit%2520future%2520research%2520in%2520interpretability%2520and%2520LLM%2520post-training.%250AOur%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HZD01/post-training-mechanistic-analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Post-Training%20Reshapes%20LLMs%3A%20A%20Mechanistic%20View%20on%20Knowledge%2C%0A%20%20Truthfulness%2C%20Refusal%2C%20and%20Confidence&entry.906535625=Hongzhe%20Du%20and%20Weikai%20Li%20and%20Min%20Cai%20and%20Karim%20Saraipour%20and%20Zimin%20Zhang%20and%20Himabindu%20Lakkaraju%20and%20Yizhou%20Sun%20and%20Shichang%20Zhang&entry.1292438233=%20%20Post-training%20is%20essential%20for%20the%20success%20of%20large%20language%20models%20%28LLMs%29%2C%0Atransforming%20pre-trained%20base%20models%20into%20more%20useful%20and%20aligned%20post-trained%0Amodels.%20While%20plenty%20of%20works%20have%20studied%20post-training%20algorithms%20and%0Aevaluated%20post-training%20models%20by%20their%20outputs%2C%20it%20remains%20understudied%20how%0Apost-training%20reshapes%20LLMs%20internally.%20In%20this%20paper%2C%20we%20compare%20base%20and%0Apost-trained%20LLMs%20mechanistically%20from%20four%20perspectives%20to%20better%20understand%0Apost-training%20effects.%20Our%20findings%20across%20model%20families%20and%20datasets%20reveal%0Athat%3A%20%281%29%20Post-training%20does%20not%20change%20the%20factual%20knowledge%20storage%0Alocations%2C%20and%20it%20adapts%20knowledge%20representations%20from%20the%20base%20model%20while%0Adeveloping%20new%20knowledge%20representations%3B%20%282%29%20Both%20truthfulness%20and%20refusal%20can%0Abe%20represented%20by%20vectors%20in%20the%20hidden%20representation%20space.%20The%20truthfulness%0Adirection%20is%20highly%20similar%20between%20the%20base%20and%20post-trained%20model%2C%20and%20it%20is%0Aeffectively%20transferable%20for%20interventions%3B%20%283%29%20The%20refusal%20direction%20is%0Adifferent%20between%20the%20base%20and%20post-trained%20models%2C%20and%20it%20shows%20limited%0Aforward%20transferability%3B%20%284%29%20Differences%20in%20confidence%20between%20the%20base%20and%0Apost-trained%20models%20cannot%20be%20attributed%20to%20entropy%20neurons.%20Our%20study%20provides%0Ainsights%20into%20the%20fundamental%20mechanisms%20preserved%20and%20altered%20during%0Apost-training%2C%20facilitates%20downstream%20tasks%20like%20model%20steering%2C%20and%20could%0Apotentially%20benefit%20future%20research%20in%20interpretability%20and%20LLM%20post-training.%0AOur%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/HZD01/post-training-mechanistic-analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02904v2&entry.124074799=Read"},
{"title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for\n  Effective Image Recognition", "author": "Anqi Xiao and Weichen Yu and Hongyuan Yu", "abstract": "  Automatic data augmentation (AutoDA) plays an important role in enhancing the\ngeneralization of neural networks. However, mainstream AutoDA methods often\nencounter two challenges: either the search process is excessively\ntime-consuming, hindering practical application, or the performance is\nsuboptimal due to insufficient policy adaptation during training. To address\nthese issues, we propose Sample-aware RandAugment (SRA), an asymmetric,\nsearch-free AutoDA method that dynamically adjusts augmentation policies while\nmaintaining straightforward implementation. SRA incorporates a heuristic\nscoring module that evaluates the complexity of the original training data,\nenabling the application of tailored augmentations for each sample.\nAdditionally, an asymmetric augmentation strategy is employed to maximize the\npotential of this scoring module. In multiple experimental settings, SRA\nnarrows the performance gap between search-based and search-free AutoDA\nmethods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet\nwith ResNet-50. Notably, SRA demonstrates good compatibility with existing\naugmentation pipelines and solid generalization across new tasks, without\nrequiring hyperparameter tuning. The pretrained models leveraging SRA also\nenhance recognition in downstream object detection tasks. SRA represents a\npromising step towards simpler, more effective, and practical AutoDA designs\napplicable to a variety of future tasks. Our code is available at\n\\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment\n", "link": "http://arxiv.org/abs/2508.08004v1", "date": "2025-08-11", "relevancy": 2.6256, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5435}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-aware%20RandAugment%3A%20Search-free%20Automatic%20Data%20Augmentation%20for%0A%20%20Effective%20Image%20Recognition&body=Title%3A%20Sample-aware%20RandAugment%3A%20Search-free%20Automatic%20Data%20Augmentation%20for%0A%20%20Effective%20Image%20Recognition%0AAuthor%3A%20Anqi%20Xiao%20and%20Weichen%20Yu%20and%20Hongyuan%20Yu%0AAbstract%3A%20%20%20Automatic%20data%20augmentation%20%28AutoDA%29%20plays%20an%20important%20role%20in%20enhancing%20the%0Ageneralization%20of%20neural%20networks.%20However%2C%20mainstream%20AutoDA%20methods%20often%0Aencounter%20two%20challenges%3A%20either%20the%20search%20process%20is%20excessively%0Atime-consuming%2C%20hindering%20practical%20application%2C%20or%20the%20performance%20is%0Asuboptimal%20due%20to%20insufficient%20policy%20adaptation%20during%20training.%20To%20address%0Athese%20issues%2C%20we%20propose%20Sample-aware%20RandAugment%20%28SRA%29%2C%20an%20asymmetric%2C%0Asearch-free%20AutoDA%20method%20that%20dynamically%20adjusts%20augmentation%20policies%20while%0Amaintaining%20straightforward%20implementation.%20SRA%20incorporates%20a%20heuristic%0Ascoring%20module%20that%20evaluates%20the%20complexity%20of%20the%20original%20training%20data%2C%0Aenabling%20the%20application%20of%20tailored%20augmentations%20for%20each%20sample.%0AAdditionally%2C%20an%20asymmetric%20augmentation%20strategy%20is%20employed%20to%20maximize%20the%0Apotential%20of%20this%20scoring%20module.%20In%20multiple%20experimental%20settings%2C%20SRA%0Anarrows%20the%20performance%20gap%20between%20search-based%20and%20search-free%20AutoDA%0Amethods%2C%20achieving%20a%20state-of-the-art%20Top-1%20accuracy%20of%2078.31%5C%25%20on%20ImageNet%0Awith%20ResNet-50.%20Notably%2C%20SRA%20demonstrates%20good%20compatibility%20with%20existing%0Aaugmentation%20pipelines%20and%20solid%20generalization%20across%20new%20tasks%2C%20without%0Arequiring%20hyperparameter%20tuning.%20The%20pretrained%20models%20leveraging%20SRA%20also%0Aenhance%20recognition%20in%20downstream%20object%20detection%20tasks.%20SRA%20represents%20a%0Apromising%20step%20towards%20simpler%2C%20more%20effective%2C%20and%20practical%20AutoDA%20designs%0Aapplicable%20to%20a%20variety%20of%20future%20tasks.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ainieli/Sample-awareRandAugment%7D%7Bhttps%3A//github.com/ainieli/Sample-awareRandAugment%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-aware%2520RandAugment%253A%2520Search-free%2520Automatic%2520Data%2520Augmentation%2520for%250A%2520%2520Effective%2520Image%2520Recognition%26entry.906535625%3DAnqi%2520Xiao%2520and%2520Weichen%2520Yu%2520and%2520Hongyuan%2520Yu%26entry.1292438233%3D%2520%2520Automatic%2520data%2520augmentation%2520%2528AutoDA%2529%2520plays%2520an%2520important%2520role%2520in%2520enhancing%2520the%250Ageneralization%2520of%2520neural%2520networks.%2520However%252C%2520mainstream%2520AutoDA%2520methods%2520often%250Aencounter%2520two%2520challenges%253A%2520either%2520the%2520search%2520process%2520is%2520excessively%250Atime-consuming%252C%2520hindering%2520practical%2520application%252C%2520or%2520the%2520performance%2520is%250Asuboptimal%2520due%2520to%2520insufficient%2520policy%2520adaptation%2520during%2520training.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520Sample-aware%2520RandAugment%2520%2528SRA%2529%252C%2520an%2520asymmetric%252C%250Asearch-free%2520AutoDA%2520method%2520that%2520dynamically%2520adjusts%2520augmentation%2520policies%2520while%250Amaintaining%2520straightforward%2520implementation.%2520SRA%2520incorporates%2520a%2520heuristic%250Ascoring%2520module%2520that%2520evaluates%2520the%2520complexity%2520of%2520the%2520original%2520training%2520data%252C%250Aenabling%2520the%2520application%2520of%2520tailored%2520augmentations%2520for%2520each%2520sample.%250AAdditionally%252C%2520an%2520asymmetric%2520augmentation%2520strategy%2520is%2520employed%2520to%2520maximize%2520the%250Apotential%2520of%2520this%2520scoring%2520module.%2520In%2520multiple%2520experimental%2520settings%252C%2520SRA%250Anarrows%2520the%2520performance%2520gap%2520between%2520search-based%2520and%2520search-free%2520AutoDA%250Amethods%252C%2520achieving%2520a%2520state-of-the-art%2520Top-1%2520accuracy%2520of%252078.31%255C%2525%2520on%2520ImageNet%250Awith%2520ResNet-50.%2520Notably%252C%2520SRA%2520demonstrates%2520good%2520compatibility%2520with%2520existing%250Aaugmentation%2520pipelines%2520and%2520solid%2520generalization%2520across%2520new%2520tasks%252C%2520without%250Arequiring%2520hyperparameter%2520tuning.%2520The%2520pretrained%2520models%2520leveraging%2520SRA%2520also%250Aenhance%2520recognition%2520in%2520downstream%2520object%2520detection%2520tasks.%2520SRA%2520represents%2520a%250Apromising%2520step%2520towards%2520simpler%252C%2520more%2520effective%252C%2520and%2520practical%2520AutoDA%2520designs%250Aapplicable%2520to%2520a%2520variety%2520of%2520future%2520tasks.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/ainieli/Sample-awareRandAugment%257D%257Bhttps%253A//github.com/ainieli/Sample-awareRandAugment%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-aware%20RandAugment%3A%20Search-free%20Automatic%20Data%20Augmentation%20for%0A%20%20Effective%20Image%20Recognition&entry.906535625=Anqi%20Xiao%20and%20Weichen%20Yu%20and%20Hongyuan%20Yu&entry.1292438233=%20%20Automatic%20data%20augmentation%20%28AutoDA%29%20plays%20an%20important%20role%20in%20enhancing%20the%0Ageneralization%20of%20neural%20networks.%20However%2C%20mainstream%20AutoDA%20methods%20often%0Aencounter%20two%20challenges%3A%20either%20the%20search%20process%20is%20excessively%0Atime-consuming%2C%20hindering%20practical%20application%2C%20or%20the%20performance%20is%0Asuboptimal%20due%20to%20insufficient%20policy%20adaptation%20during%20training.%20To%20address%0Athese%20issues%2C%20we%20propose%20Sample-aware%20RandAugment%20%28SRA%29%2C%20an%20asymmetric%2C%0Asearch-free%20AutoDA%20method%20that%20dynamically%20adjusts%20augmentation%20policies%20while%0Amaintaining%20straightforward%20implementation.%20SRA%20incorporates%20a%20heuristic%0Ascoring%20module%20that%20evaluates%20the%20complexity%20of%20the%20original%20training%20data%2C%0Aenabling%20the%20application%20of%20tailored%20augmentations%20for%20each%20sample.%0AAdditionally%2C%20an%20asymmetric%20augmentation%20strategy%20is%20employed%20to%20maximize%20the%0Apotential%20of%20this%20scoring%20module.%20In%20multiple%20experimental%20settings%2C%20SRA%0Anarrows%20the%20performance%20gap%20between%20search-based%20and%20search-free%20AutoDA%0Amethods%2C%20achieving%20a%20state-of-the-art%20Top-1%20accuracy%20of%2078.31%5C%25%20on%20ImageNet%0Awith%20ResNet-50.%20Notably%2C%20SRA%20demonstrates%20good%20compatibility%20with%20existing%0Aaugmentation%20pipelines%20and%20solid%20generalization%20across%20new%20tasks%2C%20without%0Arequiring%20hyperparameter%20tuning.%20The%20pretrained%20models%20leveraging%20SRA%20also%0Aenhance%20recognition%20in%20downstream%20object%20detection%20tasks.%20SRA%20represents%20a%0Apromising%20step%20towards%20simpler%2C%20more%20effective%2C%20and%20practical%20AutoDA%20designs%0Aapplicable%20to%20a%20variety%20of%20future%20tasks.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/ainieli/Sample-awareRandAugment%7D%7Bhttps%3A//github.com/ainieli/Sample-awareRandAugment%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08004v1&entry.124074799=Read"},
{"title": "3D Plant Root Skeleton Detection and Extraction", "author": "Jiakai Lin and Jinchang Zhang and Ge Jin and Wenzhan Song and Tianming Liu and Guoyu Lu", "abstract": "  Plant roots typically exhibit a highly complex and dense architecture,\nincorporating numerous slender lateral roots and branches, which significantly\nhinders the precise capture and modeling of the entire root system.\nAdditionally, roots often lack sufficient texture and color information, making\nit difficult to identify and track root traits using visual methods. Previous\nresearch on roots has been largely confined to 2D studies; however, exploring\nthe 3D architecture of roots is crucial in botany. Since roots grow in real 3D\nspace, 3D phenotypic information is more critical for studying genetic traits\nand their impact on root development. We have introduced a 3D root skeleton\nextraction method that efficiently derives the 3D architecture of plant roots\nfrom a few images. This method includes the detection and matching of lateral\nroots, triangulation to extract the skeletal structure of lateral roots, and\nthe integration of lateral and primary roots. We developed a highly complex\nroot dataset and tested our method on it. The extracted 3D root skeletons\nshowed considerable similarity to the ground truth, validating the\neffectiveness of the model. This method can play a significant role in\nautomated breeding robots. Through precise 3D root structure analysis, breeding\nrobots can better identify plant phenotypic traits, especially root structure\nand growth patterns, helping practitioners select seeds with superior root\nsystems. This automated approach not only improves breeding efficiency but also\nreduces manual intervention, making the breeding process more intelligent and\nefficient, thus advancing modern agriculture.\n", "link": "http://arxiv.org/abs/2508.08094v1", "date": "2025-08-11", "relevancy": 2.6201, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Plant%20Root%20Skeleton%20Detection%20and%20Extraction&body=Title%3A%203D%20Plant%20Root%20Skeleton%20Detection%20and%20Extraction%0AAuthor%3A%20Jiakai%20Lin%20and%20Jinchang%20Zhang%20and%20Ge%20Jin%20and%20Wenzhan%20Song%20and%20Tianming%20Liu%20and%20Guoyu%20Lu%0AAbstract%3A%20%20%20Plant%20roots%20typically%20exhibit%20a%20highly%20complex%20and%20dense%20architecture%2C%0Aincorporating%20numerous%20slender%20lateral%20roots%20and%20branches%2C%20which%20significantly%0Ahinders%20the%20precise%20capture%20and%20modeling%20of%20the%20entire%20root%20system.%0AAdditionally%2C%20roots%20often%20lack%20sufficient%20texture%20and%20color%20information%2C%20making%0Ait%20difficult%20to%20identify%20and%20track%20root%20traits%20using%20visual%20methods.%20Previous%0Aresearch%20on%20roots%20has%20been%20largely%20confined%20to%202D%20studies%3B%20however%2C%20exploring%0Athe%203D%20architecture%20of%20roots%20is%20crucial%20in%20botany.%20Since%20roots%20grow%20in%20real%203D%0Aspace%2C%203D%20phenotypic%20information%20is%20more%20critical%20for%20studying%20genetic%20traits%0Aand%20their%20impact%20on%20root%20development.%20We%20have%20introduced%20a%203D%20root%20skeleton%0Aextraction%20method%20that%20efficiently%20derives%20the%203D%20architecture%20of%20plant%20roots%0Afrom%20a%20few%20images.%20This%20method%20includes%20the%20detection%20and%20matching%20of%20lateral%0Aroots%2C%20triangulation%20to%20extract%20the%20skeletal%20structure%20of%20lateral%20roots%2C%20and%0Athe%20integration%20of%20lateral%20and%20primary%20roots.%20We%20developed%20a%20highly%20complex%0Aroot%20dataset%20and%20tested%20our%20method%20on%20it.%20The%20extracted%203D%20root%20skeletons%0Ashowed%20considerable%20similarity%20to%20the%20ground%20truth%2C%20validating%20the%0Aeffectiveness%20of%20the%20model.%20This%20method%20can%20play%20a%20significant%20role%20in%0Aautomated%20breeding%20robots.%20Through%20precise%203D%20root%20structure%20analysis%2C%20breeding%0Arobots%20can%20better%20identify%20plant%20phenotypic%20traits%2C%20especially%20root%20structure%0Aand%20growth%20patterns%2C%20helping%20practitioners%20select%20seeds%20with%20superior%20root%0Asystems.%20This%20automated%20approach%20not%20only%20improves%20breeding%20efficiency%20but%20also%0Areduces%20manual%20intervention%2C%20making%20the%20breeding%20process%20more%20intelligent%20and%0Aefficient%2C%20thus%20advancing%20modern%20agriculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Plant%2520Root%2520Skeleton%2520Detection%2520and%2520Extraction%26entry.906535625%3DJiakai%2520Lin%2520and%2520Jinchang%2520Zhang%2520and%2520Ge%2520Jin%2520and%2520Wenzhan%2520Song%2520and%2520Tianming%2520Liu%2520and%2520Guoyu%2520Lu%26entry.1292438233%3D%2520%2520Plant%2520roots%2520typically%2520exhibit%2520a%2520highly%2520complex%2520and%2520dense%2520architecture%252C%250Aincorporating%2520numerous%2520slender%2520lateral%2520roots%2520and%2520branches%252C%2520which%2520significantly%250Ahinders%2520the%2520precise%2520capture%2520and%2520modeling%2520of%2520the%2520entire%2520root%2520system.%250AAdditionally%252C%2520roots%2520often%2520lack%2520sufficient%2520texture%2520and%2520color%2520information%252C%2520making%250Ait%2520difficult%2520to%2520identify%2520and%2520track%2520root%2520traits%2520using%2520visual%2520methods.%2520Previous%250Aresearch%2520on%2520roots%2520has%2520been%2520largely%2520confined%2520to%25202D%2520studies%253B%2520however%252C%2520exploring%250Athe%25203D%2520architecture%2520of%2520roots%2520is%2520crucial%2520in%2520botany.%2520Since%2520roots%2520grow%2520in%2520real%25203D%250Aspace%252C%25203D%2520phenotypic%2520information%2520is%2520more%2520critical%2520for%2520studying%2520genetic%2520traits%250Aand%2520their%2520impact%2520on%2520root%2520development.%2520We%2520have%2520introduced%2520a%25203D%2520root%2520skeleton%250Aextraction%2520method%2520that%2520efficiently%2520derives%2520the%25203D%2520architecture%2520of%2520plant%2520roots%250Afrom%2520a%2520few%2520images.%2520This%2520method%2520includes%2520the%2520detection%2520and%2520matching%2520of%2520lateral%250Aroots%252C%2520triangulation%2520to%2520extract%2520the%2520skeletal%2520structure%2520of%2520lateral%2520roots%252C%2520and%250Athe%2520integration%2520of%2520lateral%2520and%2520primary%2520roots.%2520We%2520developed%2520a%2520highly%2520complex%250Aroot%2520dataset%2520and%2520tested%2520our%2520method%2520on%2520it.%2520The%2520extracted%25203D%2520root%2520skeletons%250Ashowed%2520considerable%2520similarity%2520to%2520the%2520ground%2520truth%252C%2520validating%2520the%250Aeffectiveness%2520of%2520the%2520model.%2520This%2520method%2520can%2520play%2520a%2520significant%2520role%2520in%250Aautomated%2520breeding%2520robots.%2520Through%2520precise%25203D%2520root%2520structure%2520analysis%252C%2520breeding%250Arobots%2520can%2520better%2520identify%2520plant%2520phenotypic%2520traits%252C%2520especially%2520root%2520structure%250Aand%2520growth%2520patterns%252C%2520helping%2520practitioners%2520select%2520seeds%2520with%2520superior%2520root%250Asystems.%2520This%2520automated%2520approach%2520not%2520only%2520improves%2520breeding%2520efficiency%2520but%2520also%250Areduces%2520manual%2520intervention%252C%2520making%2520the%2520breeding%2520process%2520more%2520intelligent%2520and%250Aefficient%252C%2520thus%2520advancing%2520modern%2520agriculture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Plant%20Root%20Skeleton%20Detection%20and%20Extraction&entry.906535625=Jiakai%20Lin%20and%20Jinchang%20Zhang%20and%20Ge%20Jin%20and%20Wenzhan%20Song%20and%20Tianming%20Liu%20and%20Guoyu%20Lu&entry.1292438233=%20%20Plant%20roots%20typically%20exhibit%20a%20highly%20complex%20and%20dense%20architecture%2C%0Aincorporating%20numerous%20slender%20lateral%20roots%20and%20branches%2C%20which%20significantly%0Ahinders%20the%20precise%20capture%20and%20modeling%20of%20the%20entire%20root%20system.%0AAdditionally%2C%20roots%20often%20lack%20sufficient%20texture%20and%20color%20information%2C%20making%0Ait%20difficult%20to%20identify%20and%20track%20root%20traits%20using%20visual%20methods.%20Previous%0Aresearch%20on%20roots%20has%20been%20largely%20confined%20to%202D%20studies%3B%20however%2C%20exploring%0Athe%203D%20architecture%20of%20roots%20is%20crucial%20in%20botany.%20Since%20roots%20grow%20in%20real%203D%0Aspace%2C%203D%20phenotypic%20information%20is%20more%20critical%20for%20studying%20genetic%20traits%0Aand%20their%20impact%20on%20root%20development.%20We%20have%20introduced%20a%203D%20root%20skeleton%0Aextraction%20method%20that%20efficiently%20derives%20the%203D%20architecture%20of%20plant%20roots%0Afrom%20a%20few%20images.%20This%20method%20includes%20the%20detection%20and%20matching%20of%20lateral%0Aroots%2C%20triangulation%20to%20extract%20the%20skeletal%20structure%20of%20lateral%20roots%2C%20and%0Athe%20integration%20of%20lateral%20and%20primary%20roots.%20We%20developed%20a%20highly%20complex%0Aroot%20dataset%20and%20tested%20our%20method%20on%20it.%20The%20extracted%203D%20root%20skeletons%0Ashowed%20considerable%20similarity%20to%20the%20ground%20truth%2C%20validating%20the%0Aeffectiveness%20of%20the%20model.%20This%20method%20can%20play%20a%20significant%20role%20in%0Aautomated%20breeding%20robots.%20Through%20precise%203D%20root%20structure%20analysis%2C%20breeding%0Arobots%20can%20better%20identify%20plant%20phenotypic%20traits%2C%20especially%20root%20structure%0Aand%20growth%20patterns%2C%20helping%20practitioners%20select%20seeds%20with%20superior%20root%0Asystems.%20This%20automated%20approach%20not%20only%20improves%20breeding%20efficiency%20but%20also%0Areduces%20manual%20intervention%2C%20making%20the%20breeding%20process%20more%20intelligent%20and%0Aefficient%2C%20thus%20advancing%20modern%20agriculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08094v1&entry.124074799=Read"},
{"title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame\n  Matrix", "author": "Peng Dai and Feitong Tan and Qiangeng Xu and Yihua Huang and David Futschik and Ruofei Du and Sean Fanello and Yinda Zhang and Xiaojuan Qi", "abstract": "  While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/\n", "link": "http://arxiv.org/abs/2508.08048v1", "date": "2025-08-11", "relevancy": 2.6093, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6767}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6357}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%5E2VG%3A%203D%20Stereoscopic%20and%20Spatial%20Video%20Generation%20via%20Denoising%20Frame%0A%20%20Matrix&body=Title%3A%20S%5E2VG%3A%203D%20Stereoscopic%20and%20Spatial%20Video%20Generation%20via%20Denoising%20Frame%0A%20%20Matrix%0AAuthor%3A%20Peng%20Dai%20and%20Feitong%20Tan%20and%20Qiangeng%20Xu%20and%20Yihua%20Huang%20and%20David%20Futschik%20and%20Ruofei%20Du%20and%20Sean%20Fanello%20and%20Yinda%20Zhang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20While%20video%20generation%20models%20excel%20at%20producing%20high-quality%20monocular%0Avideos%2C%20generating%203D%20stereoscopic%20and%20spatial%20videos%20for%20immersive%0Aapplications%20remains%20an%20underexplored%20challenge.%20We%20present%20a%20pose-free%20and%0Atraining-free%20method%20that%20leverages%20an%20off-the-shelf%20monocular%20video%20generation%0Amodel%20to%20produce%20immersive%203D%20videos.%20Our%20approach%20first%20warps%20the%20generated%0Amonocular%20video%20into%20pre-defined%20camera%20viewpoints%20using%20estimated%20depth%0Ainformation%2C%20then%20applies%20a%20novel%20%5Ctextit%7Bframe%20matrix%7D%20inpainting%20framework.%0AThis%20framework%20utilizes%20the%20original%20video%20generation%20model%20to%20synthesize%0Amissing%20content%20across%20different%20viewpoints%20and%20timestamps%2C%20ensuring%20spatial%0Aand%20temporal%20consistency%20without%20requiring%20additional%20model%20fine-tuning.%0AMoreover%2C%20we%20develop%20a%20%5Cdualupdate~scheme%20that%20further%20improves%20the%20quality%20of%0Avideo%20inpainting%20by%20alleviating%20the%20negative%20effects%20propagated%20from%0Adisoccluded%20areas%20in%20the%20latent%20space.%20The%20resulting%20multi-view%20videos%20are%20then%0Aadapted%20into%20stereoscopic%20pairs%20or%20optimized%20into%204D%20Gaussians%20for%20spatial%0Avideo%20synthesis.%20We%20validate%20the%20efficacy%20of%20our%20proposed%20method%20by%20conducting%0Aexperiments%20on%20videos%20from%20various%20generative%20models%2C%20such%20as%20Sora%2C%20Lumiere%2C%0AWALT%2C%20and%20Zeroscope.%20The%20experiments%20demonstrate%20that%20our%20method%20has%20a%0Asignificant%20improvement%20over%20previous%20methods.%20Project%20page%20at%3A%0Ahttps%3A//daipengwa.github.io/S-2VG_ProjectPage/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%255E2VG%253A%25203D%2520Stereoscopic%2520and%2520Spatial%2520Video%2520Generation%2520via%2520Denoising%2520Frame%250A%2520%2520Matrix%26entry.906535625%3DPeng%2520Dai%2520and%2520Feitong%2520Tan%2520and%2520Qiangeng%2520Xu%2520and%2520Yihua%2520Huang%2520and%2520David%2520Futschik%2520and%2520Ruofei%2520Du%2520and%2520Sean%2520Fanello%2520and%2520Yinda%2520Zhang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520While%2520video%2520generation%2520models%2520excel%2520at%2520producing%2520high-quality%2520monocular%250Avideos%252C%2520generating%25203D%2520stereoscopic%2520and%2520spatial%2520videos%2520for%2520immersive%250Aapplications%2520remains%2520an%2520underexplored%2520challenge.%2520We%2520present%2520a%2520pose-free%2520and%250Atraining-free%2520method%2520that%2520leverages%2520an%2520off-the-shelf%2520monocular%2520video%2520generation%250Amodel%2520to%2520produce%2520immersive%25203D%2520videos.%2520Our%2520approach%2520first%2520warps%2520the%2520generated%250Amonocular%2520video%2520into%2520pre-defined%2520camera%2520viewpoints%2520using%2520estimated%2520depth%250Ainformation%252C%2520then%2520applies%2520a%2520novel%2520%255Ctextit%257Bframe%2520matrix%257D%2520inpainting%2520framework.%250AThis%2520framework%2520utilizes%2520the%2520original%2520video%2520generation%2520model%2520to%2520synthesize%250Amissing%2520content%2520across%2520different%2520viewpoints%2520and%2520timestamps%252C%2520ensuring%2520spatial%250Aand%2520temporal%2520consistency%2520without%2520requiring%2520additional%2520model%2520fine-tuning.%250AMoreover%252C%2520we%2520develop%2520a%2520%255Cdualupdate~scheme%2520that%2520further%2520improves%2520the%2520quality%2520of%250Avideo%2520inpainting%2520by%2520alleviating%2520the%2520negative%2520effects%2520propagated%2520from%250Adisoccluded%2520areas%2520in%2520the%2520latent%2520space.%2520The%2520resulting%2520multi-view%2520videos%2520are%2520then%250Aadapted%2520into%2520stereoscopic%2520pairs%2520or%2520optimized%2520into%25204D%2520Gaussians%2520for%2520spatial%250Avideo%2520synthesis.%2520We%2520validate%2520the%2520efficacy%2520of%2520our%2520proposed%2520method%2520by%2520conducting%250Aexperiments%2520on%2520videos%2520from%2520various%2520generative%2520models%252C%2520such%2520as%2520Sora%252C%2520Lumiere%252C%250AWALT%252C%2520and%2520Zeroscope.%2520The%2520experiments%2520demonstrate%2520that%2520our%2520method%2520has%2520a%250Asignificant%2520improvement%2520over%2520previous%2520methods.%2520Project%2520page%2520at%253A%250Ahttps%253A//daipengwa.github.io/S-2VG_ProjectPage/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%5E2VG%3A%203D%20Stereoscopic%20and%20Spatial%20Video%20Generation%20via%20Denoising%20Frame%0A%20%20Matrix&entry.906535625=Peng%20Dai%20and%20Feitong%20Tan%20and%20Qiangeng%20Xu%20and%20Yihua%20Huang%20and%20David%20Futschik%20and%20Ruofei%20Du%20and%20Sean%20Fanello%20and%20Yinda%20Zhang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20While%20video%20generation%20models%20excel%20at%20producing%20high-quality%20monocular%0Avideos%2C%20generating%203D%20stereoscopic%20and%20spatial%20videos%20for%20immersive%0Aapplications%20remains%20an%20underexplored%20challenge.%20We%20present%20a%20pose-free%20and%0Atraining-free%20method%20that%20leverages%20an%20off-the-shelf%20monocular%20video%20generation%0Amodel%20to%20produce%20immersive%203D%20videos.%20Our%20approach%20first%20warps%20the%20generated%0Amonocular%20video%20into%20pre-defined%20camera%20viewpoints%20using%20estimated%20depth%0Ainformation%2C%20then%20applies%20a%20novel%20%5Ctextit%7Bframe%20matrix%7D%20inpainting%20framework.%0AThis%20framework%20utilizes%20the%20original%20video%20generation%20model%20to%20synthesize%0Amissing%20content%20across%20different%20viewpoints%20and%20timestamps%2C%20ensuring%20spatial%0Aand%20temporal%20consistency%20without%20requiring%20additional%20model%20fine-tuning.%0AMoreover%2C%20we%20develop%20a%20%5Cdualupdate~scheme%20that%20further%20improves%20the%20quality%20of%0Avideo%20inpainting%20by%20alleviating%20the%20negative%20effects%20propagated%20from%0Adisoccluded%20areas%20in%20the%20latent%20space.%20The%20resulting%20multi-view%20videos%20are%20then%0Aadapted%20into%20stereoscopic%20pairs%20or%20optimized%20into%204D%20Gaussians%20for%20spatial%0Avideo%20synthesis.%20We%20validate%20the%20efficacy%20of%20our%20proposed%20method%20by%20conducting%0Aexperiments%20on%20videos%20from%20various%20generative%20models%2C%20such%20as%20Sora%2C%20Lumiere%2C%0AWALT%2C%20and%20Zeroscope.%20The%20experiments%20demonstrate%20that%20our%20method%20has%20a%0Asignificant%20improvement%20over%20previous%20methods.%20Project%20page%20at%3A%0Ahttps%3A//daipengwa.github.io/S-2VG_ProjectPage/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08048v1&entry.124074799=Read"},
{"title": "Generative Video Matting", "author": "Yongtao Ge and Kangyang Xie and Guangkai Xu and Mingyu Liu and Li Ke and Longtao Huang and Hui Xue and Hao Chen and Chunhua Shen", "abstract": "  Video matting has traditionally been limited by the lack of high-quality\nground-truth data. Most existing video matting datasets provide only\nhuman-annotated imperfect alpha and foreground annotations, which must be\ncomposited to background images or videos during the training stage. Thus, the\ngeneralization capability of previous methods in real-world scenarios is\ntypically poor. In this work, we propose to solve the problem from two\nperspectives. First, we emphasize the importance of large-scale pre-training by\npursuing diverse synthetic and pseudo-labeled segmentation datasets. We also\ndevelop a scalable synthetic data generation pipeline that can render diverse\nhuman bodies and fine-grained hairs, yielding around 200 video clips with a\n3-second duration for fine-tuning. Second, we introduce a novel video matting\napproach that can effectively leverage the rich priors from pre-trained video\ndiffusion models. This architecture offers two key advantages. First, strong\npriors play a critical role in bridging the domain gap between synthetic and\nreal-world scenes. Second, unlike most existing methods that process video\nmatting frame-by-frame and use an independent decoder to aggregate temporal\ninformation, our model is inherently designed for video, ensuring strong\ntemporal consistency. We provide a comprehensive quantitative evaluation across\nthree benchmark datasets, demonstrating our approach's superior performance,\nand present comprehensive qualitative results in diverse real-world scenes,\nillustrating the strong generalization capability of our method. The code is\navailable at https://github.com/aim-uofa/GVM.\n", "link": "http://arxiv.org/abs/2508.07905v1", "date": "2025-08-11", "relevancy": 2.5895, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6534}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6441}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Video%20Matting&body=Title%3A%20Generative%20Video%20Matting%0AAuthor%3A%20Yongtao%20Ge%20and%20Kangyang%20Xie%20and%20Guangkai%20Xu%20and%20Mingyu%20Liu%20and%20Li%20Ke%20and%20Longtao%20Huang%20and%20Hui%20Xue%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Video%20matting%20has%20traditionally%20been%20limited%20by%20the%20lack%20of%20high-quality%0Aground-truth%20data.%20Most%20existing%20video%20matting%20datasets%20provide%20only%0Ahuman-annotated%20imperfect%20alpha%20and%20foreground%20annotations%2C%20which%20must%20be%0Acomposited%20to%20background%20images%20or%20videos%20during%20the%20training%20stage.%20Thus%2C%20the%0Ageneralization%20capability%20of%20previous%20methods%20in%20real-world%20scenarios%20is%0Atypically%20poor.%20In%20this%20work%2C%20we%20propose%20to%20solve%20the%20problem%20from%20two%0Aperspectives.%20First%2C%20we%20emphasize%20the%20importance%20of%20large-scale%20pre-training%20by%0Apursuing%20diverse%20synthetic%20and%20pseudo-labeled%20segmentation%20datasets.%20We%20also%0Adevelop%20a%20scalable%20synthetic%20data%20generation%20pipeline%20that%20can%20render%20diverse%0Ahuman%20bodies%20and%20fine-grained%20hairs%2C%20yielding%20around%20200%20video%20clips%20with%20a%0A3-second%20duration%20for%20fine-tuning.%20Second%2C%20we%20introduce%20a%20novel%20video%20matting%0Aapproach%20that%20can%20effectively%20leverage%20the%20rich%20priors%20from%20pre-trained%20video%0Adiffusion%20models.%20This%20architecture%20offers%20two%20key%20advantages.%20First%2C%20strong%0Apriors%20play%20a%20critical%20role%20in%20bridging%20the%20domain%20gap%20between%20synthetic%20and%0Areal-world%20scenes.%20Second%2C%20unlike%20most%20existing%20methods%20that%20process%20video%0Amatting%20frame-by-frame%20and%20use%20an%20independent%20decoder%20to%20aggregate%20temporal%0Ainformation%2C%20our%20model%20is%20inherently%20designed%20for%20video%2C%20ensuring%20strong%0Atemporal%20consistency.%20We%20provide%20a%20comprehensive%20quantitative%20evaluation%20across%0Athree%20benchmark%20datasets%2C%20demonstrating%20our%20approach%27s%20superior%20performance%2C%0Aand%20present%20comprehensive%20qualitative%20results%20in%20diverse%20real-world%20scenes%2C%0Aillustrating%20the%20strong%20generalization%20capability%20of%20our%20method.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/aim-uofa/GVM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Video%2520Matting%26entry.906535625%3DYongtao%2520Ge%2520and%2520Kangyang%2520Xie%2520and%2520Guangkai%2520Xu%2520and%2520Mingyu%2520Liu%2520and%2520Li%2520Ke%2520and%2520Longtao%2520Huang%2520and%2520Hui%2520Xue%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Video%2520matting%2520has%2520traditionally%2520been%2520limited%2520by%2520the%2520lack%2520of%2520high-quality%250Aground-truth%2520data.%2520Most%2520existing%2520video%2520matting%2520datasets%2520provide%2520only%250Ahuman-annotated%2520imperfect%2520alpha%2520and%2520foreground%2520annotations%252C%2520which%2520must%2520be%250Acomposited%2520to%2520background%2520images%2520or%2520videos%2520during%2520the%2520training%2520stage.%2520Thus%252C%2520the%250Ageneralization%2520capability%2520of%2520previous%2520methods%2520in%2520real-world%2520scenarios%2520is%250Atypically%2520poor.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520solve%2520the%2520problem%2520from%2520two%250Aperspectives.%2520First%252C%2520we%2520emphasize%2520the%2520importance%2520of%2520large-scale%2520pre-training%2520by%250Apursuing%2520diverse%2520synthetic%2520and%2520pseudo-labeled%2520segmentation%2520datasets.%2520We%2520also%250Adevelop%2520a%2520scalable%2520synthetic%2520data%2520generation%2520pipeline%2520that%2520can%2520render%2520diverse%250Ahuman%2520bodies%2520and%2520fine-grained%2520hairs%252C%2520yielding%2520around%2520200%2520video%2520clips%2520with%2520a%250A3-second%2520duration%2520for%2520fine-tuning.%2520Second%252C%2520we%2520introduce%2520a%2520novel%2520video%2520matting%250Aapproach%2520that%2520can%2520effectively%2520leverage%2520the%2520rich%2520priors%2520from%2520pre-trained%2520video%250Adiffusion%2520models.%2520This%2520architecture%2520offers%2520two%2520key%2520advantages.%2520First%252C%2520strong%250Apriors%2520play%2520a%2520critical%2520role%2520in%2520bridging%2520the%2520domain%2520gap%2520between%2520synthetic%2520and%250Areal-world%2520scenes.%2520Second%252C%2520unlike%2520most%2520existing%2520methods%2520that%2520process%2520video%250Amatting%2520frame-by-frame%2520and%2520use%2520an%2520independent%2520decoder%2520to%2520aggregate%2520temporal%250Ainformation%252C%2520our%2520model%2520is%2520inherently%2520designed%2520for%2520video%252C%2520ensuring%2520strong%250Atemporal%2520consistency.%2520We%2520provide%2520a%2520comprehensive%2520quantitative%2520evaluation%2520across%250Athree%2520benchmark%2520datasets%252C%2520demonstrating%2520our%2520approach%2527s%2520superior%2520performance%252C%250Aand%2520present%2520comprehensive%2520qualitative%2520results%2520in%2520diverse%2520real-world%2520scenes%252C%250Aillustrating%2520the%2520strong%2520generalization%2520capability%2520of%2520our%2520method.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/aim-uofa/GVM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Video%20Matting&entry.906535625=Yongtao%20Ge%20and%20Kangyang%20Xie%20and%20Guangkai%20Xu%20and%20Mingyu%20Liu%20and%20Li%20Ke%20and%20Longtao%20Huang%20and%20Hui%20Xue%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20Video%20matting%20has%20traditionally%20been%20limited%20by%20the%20lack%20of%20high-quality%0Aground-truth%20data.%20Most%20existing%20video%20matting%20datasets%20provide%20only%0Ahuman-annotated%20imperfect%20alpha%20and%20foreground%20annotations%2C%20which%20must%20be%0Acomposited%20to%20background%20images%20or%20videos%20during%20the%20training%20stage.%20Thus%2C%20the%0Ageneralization%20capability%20of%20previous%20methods%20in%20real-world%20scenarios%20is%0Atypically%20poor.%20In%20this%20work%2C%20we%20propose%20to%20solve%20the%20problem%20from%20two%0Aperspectives.%20First%2C%20we%20emphasize%20the%20importance%20of%20large-scale%20pre-training%20by%0Apursuing%20diverse%20synthetic%20and%20pseudo-labeled%20segmentation%20datasets.%20We%20also%0Adevelop%20a%20scalable%20synthetic%20data%20generation%20pipeline%20that%20can%20render%20diverse%0Ahuman%20bodies%20and%20fine-grained%20hairs%2C%20yielding%20around%20200%20video%20clips%20with%20a%0A3-second%20duration%20for%20fine-tuning.%20Second%2C%20we%20introduce%20a%20novel%20video%20matting%0Aapproach%20that%20can%20effectively%20leverage%20the%20rich%20priors%20from%20pre-trained%20video%0Adiffusion%20models.%20This%20architecture%20offers%20two%20key%20advantages.%20First%2C%20strong%0Apriors%20play%20a%20critical%20role%20in%20bridging%20the%20domain%20gap%20between%20synthetic%20and%0Areal-world%20scenes.%20Second%2C%20unlike%20most%20existing%20methods%20that%20process%20video%0Amatting%20frame-by-frame%20and%20use%20an%20independent%20decoder%20to%20aggregate%20temporal%0Ainformation%2C%20our%20model%20is%20inherently%20designed%20for%20video%2C%20ensuring%20strong%0Atemporal%20consistency.%20We%20provide%20a%20comprehensive%20quantitative%20evaluation%20across%0Athree%20benchmark%20datasets%2C%20demonstrating%20our%20approach%27s%20superior%20performance%2C%0Aand%20present%20comprehensive%20qualitative%20results%20in%20diverse%20real-world%20scenes%2C%0Aillustrating%20the%20strong%20generalization%20capability%20of%20our%20method.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/aim-uofa/GVM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07905v1&entry.124074799=Read"},
{"title": "Dual Information Speech Language Models for Emotional Conversations", "author": "Chun Wang and Chenyang Liu and Wenze Xu and Weihong Deng", "abstract": "  Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.\n", "link": "http://arxiv.org/abs/2508.08095v1", "date": "2025-08-11", "relevancy": 2.5856, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Information%20Speech%20Language%20Models%20for%20Emotional%20Conversations&body=Title%3A%20Dual%20Information%20Speech%20Language%20Models%20for%20Emotional%20Conversations%0AAuthor%3A%20Chun%20Wang%20and%20Chenyang%20Liu%20and%20Wenze%20Xu%20and%20Weihong%20Deng%0AAbstract%3A%20%20%20Conversational%20systems%20relying%20on%20text-based%20large%20language%20models%20%28LLMs%29%0Aoften%20overlook%20paralinguistic%20cues%2C%20essential%20for%20understanding%20emotions%20and%0Aintentions.%20Speech-language%20models%20%28SLMs%29%2C%20which%20use%20speech%20as%20input%2C%20are%0Aemerging%20as%20a%20promising%20solution.%20However%2C%20SLMs%20built%20by%20extending%20frozen%20LLMs%0Astruggle%20to%20capture%20paralinguistic%20information%20and%20exhibit%20reduced%20context%0Aunderstanding.%20We%20identify%20entangled%20information%20and%20improper%20training%0Astrategies%20as%20key%20issues.%20To%20address%20these%20issues%2C%20we%20propose%20two%20heterogeneous%0Aadapters%20and%20suggest%20a%20weakly%20supervised%20training%20strategy.%20Our%20approach%0Adisentangles%20paralinguistic%20and%20linguistic%20information%2C%20enabling%20SLMs%20to%0Ainterpret%20speech%20through%20structured%20representations.%20It%20also%20preserves%0Acontextual%20understanding%20by%20avoiding%20the%20generation%20of%20task-specific%20vectors%0Athrough%20controlled%20randomness.%20This%20approach%20trains%20only%20the%20adapters%20on%20common%0Adatasets%2C%20ensuring%20parameter%20and%20data%20efficiency.%20Experiments%20demonstrate%0Acompetitive%20performance%20in%20emotional%20conversation%20tasks%2C%20showcasing%20the%20model%27s%0Aability%20to%20effectively%20integrate%20both%20paralinguistic%20and%20linguistic%20information%0Awithin%20contextual%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Information%2520Speech%2520Language%2520Models%2520for%2520Emotional%2520Conversations%26entry.906535625%3DChun%2520Wang%2520and%2520Chenyang%2520Liu%2520and%2520Wenze%2520Xu%2520and%2520Weihong%2520Deng%26entry.1292438233%3D%2520%2520Conversational%2520systems%2520relying%2520on%2520text-based%2520large%2520language%2520models%2520%2528LLMs%2529%250Aoften%2520overlook%2520paralinguistic%2520cues%252C%2520essential%2520for%2520understanding%2520emotions%2520and%250Aintentions.%2520Speech-language%2520models%2520%2528SLMs%2529%252C%2520which%2520use%2520speech%2520as%2520input%252C%2520are%250Aemerging%2520as%2520a%2520promising%2520solution.%2520However%252C%2520SLMs%2520built%2520by%2520extending%2520frozen%2520LLMs%250Astruggle%2520to%2520capture%2520paralinguistic%2520information%2520and%2520exhibit%2520reduced%2520context%250Aunderstanding.%2520We%2520identify%2520entangled%2520information%2520and%2520improper%2520training%250Astrategies%2520as%2520key%2520issues.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520two%2520heterogeneous%250Aadapters%2520and%2520suggest%2520a%2520weakly%2520supervised%2520training%2520strategy.%2520Our%2520approach%250Adisentangles%2520paralinguistic%2520and%2520linguistic%2520information%252C%2520enabling%2520SLMs%2520to%250Ainterpret%2520speech%2520through%2520structured%2520representations.%2520It%2520also%2520preserves%250Acontextual%2520understanding%2520by%2520avoiding%2520the%2520generation%2520of%2520task-specific%2520vectors%250Athrough%2520controlled%2520randomness.%2520This%2520approach%2520trains%2520only%2520the%2520adapters%2520on%2520common%250Adatasets%252C%2520ensuring%2520parameter%2520and%2520data%2520efficiency.%2520Experiments%2520demonstrate%250Acompetitive%2520performance%2520in%2520emotional%2520conversation%2520tasks%252C%2520showcasing%2520the%2520model%2527s%250Aability%2520to%2520effectively%2520integrate%2520both%2520paralinguistic%2520and%2520linguistic%2520information%250Awithin%2520contextual%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Information%20Speech%20Language%20Models%20for%20Emotional%20Conversations&entry.906535625=Chun%20Wang%20and%20Chenyang%20Liu%20and%20Wenze%20Xu%20and%20Weihong%20Deng&entry.1292438233=%20%20Conversational%20systems%20relying%20on%20text-based%20large%20language%20models%20%28LLMs%29%0Aoften%20overlook%20paralinguistic%20cues%2C%20essential%20for%20understanding%20emotions%20and%0Aintentions.%20Speech-language%20models%20%28SLMs%29%2C%20which%20use%20speech%20as%20input%2C%20are%0Aemerging%20as%20a%20promising%20solution.%20However%2C%20SLMs%20built%20by%20extending%20frozen%20LLMs%0Astruggle%20to%20capture%20paralinguistic%20information%20and%20exhibit%20reduced%20context%0Aunderstanding.%20We%20identify%20entangled%20information%20and%20improper%20training%0Astrategies%20as%20key%20issues.%20To%20address%20these%20issues%2C%20we%20propose%20two%20heterogeneous%0Aadapters%20and%20suggest%20a%20weakly%20supervised%20training%20strategy.%20Our%20approach%0Adisentangles%20paralinguistic%20and%20linguistic%20information%2C%20enabling%20SLMs%20to%0Ainterpret%20speech%20through%20structured%20representations.%20It%20also%20preserves%0Acontextual%20understanding%20by%20avoiding%20the%20generation%20of%20task-specific%20vectors%0Athrough%20controlled%20randomness.%20This%20approach%20trains%20only%20the%20adapters%20on%20common%0Adatasets%2C%20ensuring%20parameter%20and%20data%20efficiency.%20Experiments%20demonstrate%0Acompetitive%20performance%20in%20emotional%20conversation%20tasks%2C%20showcasing%20the%20model%27s%0Aability%20to%20effectively%20integrate%20both%20paralinguistic%20and%20linguistic%20information%0Awithin%20contextual%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08095v1&entry.124074799=Read"},
{"title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking\n  Self-Supervised and Generative Approaches", "author": "Ahmed Aboeitta and Ahmed Sharshar and Youssef Nafea and Shady Shehata", "abstract": "  Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction.\n", "link": "http://arxiv.org/abs/2508.08027v1", "date": "2025-08-11", "relevancy": 2.5819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20ASR%20and%20LLMs%20for%20Dysarthric%20Speech%20Recognition%3A%20Benchmarking%0A%20%20Self-Supervised%20and%20Generative%20Approaches&body=Title%3A%20Bridging%20ASR%20and%20LLMs%20for%20Dysarthric%20Speech%20Recognition%3A%20Benchmarking%0A%20%20Self-Supervised%20and%20Generative%20Approaches%0AAuthor%3A%20Ahmed%20Aboeitta%20and%20Ahmed%20Sharshar%20and%20Youssef%20Nafea%20and%20Shady%20Shehata%0AAbstract%3A%20%20%20Speech%20Recognition%20%28ASR%29%20due%20to%20phoneme%20distortions%20and%20high%20variability.%0AWhile%20self-supervised%20ASR%20models%20like%20Wav2Vec%2C%20HuBERT%2C%20and%20Whisper%20have%20shown%0Apromise%2C%20their%20effectiveness%20in%20dysarthric%20speech%20remains%20unclear.%20This%20study%0Asystematically%20benchmarks%20these%20models%20with%20different%20decoding%20strategies%2C%0Aincluding%20CTC%2C%20seq2seq%2C%20and%20LLM-enhanced%20decoding%20%28BART%2CGPT-2%2C%20Vicuna%29.%20Our%0Acontributions%20include%20%281%29%20benchmarking%20ASR%20architectures%20for%20dysarthric%20speech%2C%0A%282%29%20introducing%20LLM-based%20decoding%20to%20improve%20intelligibility%2C%20%283%29%20analyzing%0Ageneralization%20across%20datasets%2C%20and%20%284%29%20providing%20insights%20into%20recognition%0Aerrors%20across%20severity%20levels.%20Findings%20highlight%20that%20LLM-enhanced%20decoding%0Aimproves%20dysarthric%20ASR%20by%20leveraging%20linguistic%20constraints%20for%20phoneme%0Arestoration%20and%20grammatical%20correction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520ASR%2520and%2520LLMs%2520for%2520Dysarthric%2520Speech%2520Recognition%253A%2520Benchmarking%250A%2520%2520Self-Supervised%2520and%2520Generative%2520Approaches%26entry.906535625%3DAhmed%2520Aboeitta%2520and%2520Ahmed%2520Sharshar%2520and%2520Youssef%2520Nafea%2520and%2520Shady%2520Shehata%26entry.1292438233%3D%2520%2520Speech%2520Recognition%2520%2528ASR%2529%2520due%2520to%2520phoneme%2520distortions%2520and%2520high%2520variability.%250AWhile%2520self-supervised%2520ASR%2520models%2520like%2520Wav2Vec%252C%2520HuBERT%252C%2520and%2520Whisper%2520have%2520shown%250Apromise%252C%2520their%2520effectiveness%2520in%2520dysarthric%2520speech%2520remains%2520unclear.%2520This%2520study%250Asystematically%2520benchmarks%2520these%2520models%2520with%2520different%2520decoding%2520strategies%252C%250Aincluding%2520CTC%252C%2520seq2seq%252C%2520and%2520LLM-enhanced%2520decoding%2520%2528BART%252CGPT-2%252C%2520Vicuna%2529.%2520Our%250Acontributions%2520include%2520%25281%2529%2520benchmarking%2520ASR%2520architectures%2520for%2520dysarthric%2520speech%252C%250A%25282%2529%2520introducing%2520LLM-based%2520decoding%2520to%2520improve%2520intelligibility%252C%2520%25283%2529%2520analyzing%250Ageneralization%2520across%2520datasets%252C%2520and%2520%25284%2529%2520providing%2520insights%2520into%2520recognition%250Aerrors%2520across%2520severity%2520levels.%2520Findings%2520highlight%2520that%2520LLM-enhanced%2520decoding%250Aimproves%2520dysarthric%2520ASR%2520by%2520leveraging%2520linguistic%2520constraints%2520for%2520phoneme%250Arestoration%2520and%2520grammatical%2520correction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20ASR%20and%20LLMs%20for%20Dysarthric%20Speech%20Recognition%3A%20Benchmarking%0A%20%20Self-Supervised%20and%20Generative%20Approaches&entry.906535625=Ahmed%20Aboeitta%20and%20Ahmed%20Sharshar%20and%20Youssef%20Nafea%20and%20Shady%20Shehata&entry.1292438233=%20%20Speech%20Recognition%20%28ASR%29%20due%20to%20phoneme%20distortions%20and%20high%20variability.%0AWhile%20self-supervised%20ASR%20models%20like%20Wav2Vec%2C%20HuBERT%2C%20and%20Whisper%20have%20shown%0Apromise%2C%20their%20effectiveness%20in%20dysarthric%20speech%20remains%20unclear.%20This%20study%0Asystematically%20benchmarks%20these%20models%20with%20different%20decoding%20strategies%2C%0Aincluding%20CTC%2C%20seq2seq%2C%20and%20LLM-enhanced%20decoding%20%28BART%2CGPT-2%2C%20Vicuna%29.%20Our%0Acontributions%20include%20%281%29%20benchmarking%20ASR%20architectures%20for%20dysarthric%20speech%2C%0A%282%29%20introducing%20LLM-based%20decoding%20to%20improve%20intelligibility%2C%20%283%29%20analyzing%0Ageneralization%20across%20datasets%2C%20and%20%284%29%20providing%20insights%20into%20recognition%0Aerrors%20across%20severity%20levels.%20Findings%20highlight%20that%20LLM-enhanced%20decoding%0Aimproves%20dysarthric%20ASR%20by%20leveraging%20linguistic%20constraints%20for%20phoneme%0Arestoration%20and%20grammatical%20correction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08027v1&entry.124074799=Read"},
{"title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With\n  Enhanced Security", "author": "Ajnas Muhammed and Iurri Medvedev and Nuno Gon\u00e7alves", "abstract": "  Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace\n", "link": "http://arxiv.org/abs/2508.07960v1", "date": "2025-08-11", "relevancy": 2.5724, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5241}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5111}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VOIDFace%3A%20A%20Privacy-Preserving%20Multi-Network%20Face%20Recognition%20With%0A%20%20Enhanced%20Security&body=Title%3A%20VOIDFace%3A%20A%20Privacy-Preserving%20Multi-Network%20Face%20Recognition%20With%0A%20%20Enhanced%20Security%0AAuthor%3A%20Ajnas%20Muhammed%20and%20Iurri%20Medvedev%20and%20Nuno%20Gon%C3%A7alves%0AAbstract%3A%20%20%20Advancement%20of%20machine%20learning%20techniques%2C%20combined%20with%20the%20availability%20of%0Alarge-scale%20datasets%2C%20has%20significantly%20improved%20the%20accuracy%20and%20efficiency%20of%0Afacial%20recognition.%20Modern%20facial%20recognition%20systems%20are%20trained%20using%20large%0Aface%20datasets%20collected%20from%20diverse%20individuals%20or%20public%20repositories.%0AHowever%2C%20for%20training%2C%20these%20datasets%20are%20often%20replicated%20and%20stored%20in%0Amultiple%20workstations%2C%20resulting%20in%20data%20replication%2C%20which%20complicates%0Adatabase%20management%20and%20oversight.%20Currently%2C%20once%20a%20user%20submits%20their%20face%0Afor%20dataset%20preparation%2C%20they%20lose%20control%20over%20how%20their%20data%20is%20used%2C%20raising%0Asignificant%20privacy%20and%20ethical%20concerns.%20This%20paper%20introduces%20VOIDFace%2C%20a%0Anovel%20framework%20for%20facial%20recognition%20systems%20that%20addresses%20two%20major%20issues.%0AFirst%2C%20it%20eliminates%20the%20need%20of%20data%20replication%20and%20improves%20data%20control%20to%0Asecurely%20store%20training%20face%20data%20by%20using%20visual%20secret%20sharing.%20Second%2C%20it%0Aproposes%20a%20patch-based%20multi-training%20network%20that%20uses%20this%20novel%20training%0Adata%20storage%20mechanism%20to%20develop%20a%20robust%2C%20privacy-preserving%20facial%0Arecognition%20system.%20By%20integrating%20these%20advancements%2C%20VOIDFace%20aims%20to%20improve%0Athe%20privacy%2C%20security%2C%20and%20efficiency%20of%20facial%20recognition%20training%2C%20while%0Aensuring%20greater%20control%20over%20sensitive%20personal%20face%20data.%20VOIDFace%20also%0Aenables%20users%20to%20exercise%20their%20Right-To-Be-Forgotten%20property%20to%20control%20their%0Apersonal%20data.%20Experimental%20evaluations%20on%20the%20VGGFace2%20dataset%20show%20that%0AVOIDFace%20provides%20Right-To-Be-Forgotten%2C%20improved%20data%20control%2C%20security%2C%20and%0Aprivacy%20while%20maintaining%20competitive%20facial%20recognition%20performance.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/ajnasmuhammed89/VOIDFace%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVOIDFace%253A%2520A%2520Privacy-Preserving%2520Multi-Network%2520Face%2520Recognition%2520With%250A%2520%2520Enhanced%2520Security%26entry.906535625%3DAjnas%2520Muhammed%2520and%2520Iurri%2520Medvedev%2520and%2520Nuno%2520Gon%25C3%25A7alves%26entry.1292438233%3D%2520%2520Advancement%2520of%2520machine%2520learning%2520techniques%252C%2520combined%2520with%2520the%2520availability%2520of%250Alarge-scale%2520datasets%252C%2520has%2520significantly%2520improved%2520the%2520accuracy%2520and%2520efficiency%2520of%250Afacial%2520recognition.%2520Modern%2520facial%2520recognition%2520systems%2520are%2520trained%2520using%2520large%250Aface%2520datasets%2520collected%2520from%2520diverse%2520individuals%2520or%2520public%2520repositories.%250AHowever%252C%2520for%2520training%252C%2520these%2520datasets%2520are%2520often%2520replicated%2520and%2520stored%2520in%250Amultiple%2520workstations%252C%2520resulting%2520in%2520data%2520replication%252C%2520which%2520complicates%250Adatabase%2520management%2520and%2520oversight.%2520Currently%252C%2520once%2520a%2520user%2520submits%2520their%2520face%250Afor%2520dataset%2520preparation%252C%2520they%2520lose%2520control%2520over%2520how%2520their%2520data%2520is%2520used%252C%2520raising%250Asignificant%2520privacy%2520and%2520ethical%2520concerns.%2520This%2520paper%2520introduces%2520VOIDFace%252C%2520a%250Anovel%2520framework%2520for%2520facial%2520recognition%2520systems%2520that%2520addresses%2520two%2520major%2520issues.%250AFirst%252C%2520it%2520eliminates%2520the%2520need%2520of%2520data%2520replication%2520and%2520improves%2520data%2520control%2520to%250Asecurely%2520store%2520training%2520face%2520data%2520by%2520using%2520visual%2520secret%2520sharing.%2520Second%252C%2520it%250Aproposes%2520a%2520patch-based%2520multi-training%2520network%2520that%2520uses%2520this%2520novel%2520training%250Adata%2520storage%2520mechanism%2520to%2520develop%2520a%2520robust%252C%2520privacy-preserving%2520facial%250Arecognition%2520system.%2520By%2520integrating%2520these%2520advancements%252C%2520VOIDFace%2520aims%2520to%2520improve%250Athe%2520privacy%252C%2520security%252C%2520and%2520efficiency%2520of%2520facial%2520recognition%2520training%252C%2520while%250Aensuring%2520greater%2520control%2520over%2520sensitive%2520personal%2520face%2520data.%2520VOIDFace%2520also%250Aenables%2520users%2520to%2520exercise%2520their%2520Right-To-Be-Forgotten%2520property%2520to%2520control%2520their%250Apersonal%2520data.%2520Experimental%2520evaluations%2520on%2520the%2520VGGFace2%2520dataset%2520show%2520that%250AVOIDFace%2520provides%2520Right-To-Be-Forgotten%252C%2520improved%2520data%2520control%252C%2520security%252C%2520and%250Aprivacy%2520while%2520maintaining%2520competitive%2520facial%2520recognition%2520performance.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/ajnasmuhammed89/VOIDFace%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VOIDFace%3A%20A%20Privacy-Preserving%20Multi-Network%20Face%20Recognition%20With%0A%20%20Enhanced%20Security&entry.906535625=Ajnas%20Muhammed%20and%20Iurri%20Medvedev%20and%20Nuno%20Gon%C3%A7alves&entry.1292438233=%20%20Advancement%20of%20machine%20learning%20techniques%2C%20combined%20with%20the%20availability%20of%0Alarge-scale%20datasets%2C%20has%20significantly%20improved%20the%20accuracy%20and%20efficiency%20of%0Afacial%20recognition.%20Modern%20facial%20recognition%20systems%20are%20trained%20using%20large%0Aface%20datasets%20collected%20from%20diverse%20individuals%20or%20public%20repositories.%0AHowever%2C%20for%20training%2C%20these%20datasets%20are%20often%20replicated%20and%20stored%20in%0Amultiple%20workstations%2C%20resulting%20in%20data%20replication%2C%20which%20complicates%0Adatabase%20management%20and%20oversight.%20Currently%2C%20once%20a%20user%20submits%20their%20face%0Afor%20dataset%20preparation%2C%20they%20lose%20control%20over%20how%20their%20data%20is%20used%2C%20raising%0Asignificant%20privacy%20and%20ethical%20concerns.%20This%20paper%20introduces%20VOIDFace%2C%20a%0Anovel%20framework%20for%20facial%20recognition%20systems%20that%20addresses%20two%20major%20issues.%0AFirst%2C%20it%20eliminates%20the%20need%20of%20data%20replication%20and%20improves%20data%20control%20to%0Asecurely%20store%20training%20face%20data%20by%20using%20visual%20secret%20sharing.%20Second%2C%20it%0Aproposes%20a%20patch-based%20multi-training%20network%20that%20uses%20this%20novel%20training%0Adata%20storage%20mechanism%20to%20develop%20a%20robust%2C%20privacy-preserving%20facial%0Arecognition%20system.%20By%20integrating%20these%20advancements%2C%20VOIDFace%20aims%20to%20improve%0Athe%20privacy%2C%20security%2C%20and%20efficiency%20of%20facial%20recognition%20training%2C%20while%0Aensuring%20greater%20control%20over%20sensitive%20personal%20face%20data.%20VOIDFace%20also%0Aenables%20users%20to%20exercise%20their%20Right-To-Be-Forgotten%20property%20to%20control%20their%0Apersonal%20data.%20Experimental%20evaluations%20on%20the%20VGGFace2%20dataset%20show%20that%0AVOIDFace%20provides%20Right-To-Be-Forgotten%2C%20improved%20data%20control%2C%20security%2C%20and%0Aprivacy%20while%20maintaining%20competitive%20facial%20recognition%20performance.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/ajnasmuhammed89/VOIDFace%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07960v1&entry.124074799=Read"},
{"title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone\n  Inscriptions", "author": "Xiao Zhang and Johan Bos", "abstract": "  Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.\n", "link": "http://arxiv.org/abs/2507.04377v3", "date": "2025-08-11", "relevancy": 2.5326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Semantic%20Parsing%20for%20the%20Interpretation%20of%20Tombstone%0A%20%20Inscriptions&body=Title%3A%20Multi-Modal%20Semantic%20Parsing%20for%20the%20Interpretation%20of%20Tombstone%0A%20%20Inscriptions%0AAuthor%3A%20Xiao%20Zhang%20and%20Johan%20Bos%0AAbstract%3A%20%20%20Tombstones%20are%20historically%20and%20culturally%20rich%20artifacts%2C%20encapsulating%0Aindividual%20lives%2C%20community%20memory%2C%20historical%20narratives%20and%20artistic%0Aexpression.%20Yet%2C%20many%20tombstones%20today%20face%20significant%20preservation%0Achallenges%2C%20including%20physical%20erosion%2C%20vandalism%2C%20environmental%20degradation%2C%0Aand%20political%20shifts.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20multi-modal%20framework%0Afor%20tombstones%20digitization%2C%20aiming%20to%20improve%20the%20interpretation%2C%20organization%0Aand%20retrieval%20of%20tombstone%20content.%20Our%20approach%20leverages%20vision-language%0Amodels%20%28VLMs%29%20to%20translate%20tombstone%20images%20into%20structured%20Tombstone%20Meaning%0ARepresentations%20%28TMRs%29%2C%20capturing%20both%20image%20and%20text%20information.%20To%20further%0Aenrich%20semantic%20parsing%2C%20we%20incorporate%20retrieval-augmented%20generation%20%28RAG%29%0Afor%20integrate%20externally%20dependent%20elements%20such%20as%20toponyms%2C%20occupation%20codes%2C%0Aand%20ontological%20concepts.%20Compared%20to%20traditional%20OCR-based%20pipelines%2C%20our%0Amethod%20improves%20parsing%20accuracy%20from%20an%20F1%20score%20of%2036.1%20to%2089.5.%20We%0Aadditionally%20evaluate%20the%20model%27s%20robustness%20across%20diverse%20linguistic%20and%0Acultural%20inscriptions%2C%20and%20simulate%20physical%20degradation%20through%20image%20fusion%0Ato%20assess%20performance%20under%20noisy%20or%20damaged%20conditions.%20Our%20work%20represents%0Athe%20first%20attempt%20to%20formalize%20tombstone%20understanding%20using%20large%0Avision-language%20models%2C%20presenting%20implications%20for%20heritage%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04377v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Semantic%2520Parsing%2520for%2520the%2520Interpretation%2520of%2520Tombstone%250A%2520%2520Inscriptions%26entry.906535625%3DXiao%2520Zhang%2520and%2520Johan%2520Bos%26entry.1292438233%3D%2520%2520Tombstones%2520are%2520historically%2520and%2520culturally%2520rich%2520artifacts%252C%2520encapsulating%250Aindividual%2520lives%252C%2520community%2520memory%252C%2520historical%2520narratives%2520and%2520artistic%250Aexpression.%2520Yet%252C%2520many%2520tombstones%2520today%2520face%2520significant%2520preservation%250Achallenges%252C%2520including%2520physical%2520erosion%252C%2520vandalism%252C%2520environmental%2520degradation%252C%250Aand%2520political%2520shifts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520multi-modal%2520framework%250Afor%2520tombstones%2520digitization%252C%2520aiming%2520to%2520improve%2520the%2520interpretation%252C%2520organization%250Aand%2520retrieval%2520of%2520tombstone%2520content.%2520Our%2520approach%2520leverages%2520vision-language%250Amodels%2520%2528VLMs%2529%2520to%2520translate%2520tombstone%2520images%2520into%2520structured%2520Tombstone%2520Meaning%250ARepresentations%2520%2528TMRs%2529%252C%2520capturing%2520both%2520image%2520and%2520text%2520information.%2520To%2520further%250Aenrich%2520semantic%2520parsing%252C%2520we%2520incorporate%2520retrieval-augmented%2520generation%2520%2528RAG%2529%250Afor%2520integrate%2520externally%2520dependent%2520elements%2520such%2520as%2520toponyms%252C%2520occupation%2520codes%252C%250Aand%2520ontological%2520concepts.%2520Compared%2520to%2520traditional%2520OCR-based%2520pipelines%252C%2520our%250Amethod%2520improves%2520parsing%2520accuracy%2520from%2520an%2520F1%2520score%2520of%252036.1%2520to%252089.5.%2520We%250Aadditionally%2520evaluate%2520the%2520model%2527s%2520robustness%2520across%2520diverse%2520linguistic%2520and%250Acultural%2520inscriptions%252C%2520and%2520simulate%2520physical%2520degradation%2520through%2520image%2520fusion%250Ato%2520assess%2520performance%2520under%2520noisy%2520or%2520damaged%2520conditions.%2520Our%2520work%2520represents%250Athe%2520first%2520attempt%2520to%2520formalize%2520tombstone%2520understanding%2520using%2520large%250Avision-language%2520models%252C%2520presenting%2520implications%2520for%2520heritage%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04377v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Semantic%20Parsing%20for%20the%20Interpretation%20of%20Tombstone%0A%20%20Inscriptions&entry.906535625=Xiao%20Zhang%20and%20Johan%20Bos&entry.1292438233=%20%20Tombstones%20are%20historically%20and%20culturally%20rich%20artifacts%2C%20encapsulating%0Aindividual%20lives%2C%20community%20memory%2C%20historical%20narratives%20and%20artistic%0Aexpression.%20Yet%2C%20many%20tombstones%20today%20face%20significant%20preservation%0Achallenges%2C%20including%20physical%20erosion%2C%20vandalism%2C%20environmental%20degradation%2C%0Aand%20political%20shifts.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20multi-modal%20framework%0Afor%20tombstones%20digitization%2C%20aiming%20to%20improve%20the%20interpretation%2C%20organization%0Aand%20retrieval%20of%20tombstone%20content.%20Our%20approach%20leverages%20vision-language%0Amodels%20%28VLMs%29%20to%20translate%20tombstone%20images%20into%20structured%20Tombstone%20Meaning%0ARepresentations%20%28TMRs%29%2C%20capturing%20both%20image%20and%20text%20information.%20To%20further%0Aenrich%20semantic%20parsing%2C%20we%20incorporate%20retrieval-augmented%20generation%20%28RAG%29%0Afor%20integrate%20externally%20dependent%20elements%20such%20as%20toponyms%2C%20occupation%20codes%2C%0Aand%20ontological%20concepts.%20Compared%20to%20traditional%20OCR-based%20pipelines%2C%20our%0Amethod%20improves%20parsing%20accuracy%20from%20an%20F1%20score%20of%2036.1%20to%2089.5.%20We%0Aadditionally%20evaluate%20the%20model%27s%20robustness%20across%20diverse%20linguistic%20and%0Acultural%20inscriptions%2C%20and%20simulate%20physical%20degradation%20through%20image%20fusion%0Ato%20assess%20performance%20under%20noisy%20or%20damaged%20conditions.%20Our%20work%20represents%0Athe%20first%20attempt%20to%20formalize%20tombstone%20understanding%20using%20large%0Avision-language%20models%2C%20presenting%20implications%20for%20heritage%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04377v3&entry.124074799=Read"},
{"title": "EEG-Language Pretraining for Highly Label-Efficient Clinical Phenotyping", "author": "Sam Gijsen and Kerstin Ritter", "abstract": "  Multimodal language modeling has enabled breakthroughs for representation\nlearning, yet remains unexplored in the realm of functional brain data for\nclinical phenotyping. This paper pioneers EEG-language models (ELMs) trained on\nclinical reports and 15000 EEGs. We propose to combine multimodal alignment in\nthis novel domain with timeseries cropping and text segmentation, enabling an\nextension based on multiple instance learning to alleviate misalignment between\nirrelevant EEG or text segments. Our multimodal models significantly improve\nover EEG-only models across four clinical evaluations and for the first time\nenable zero-shot classification as well as retrieval of both neural signals and\nreports. In sum, these results highlight the potential of ELMs, representing\nsignificant progress for clinical applications.\n", "link": "http://arxiv.org/abs/2409.07480v4", "date": "2025-08-11", "relevancy": 2.5265, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-Language%20Pretraining%20for%20Highly%20Label-Efficient%20Clinical%20Phenotyping&body=Title%3A%20EEG-Language%20Pretraining%20for%20Highly%20Label-Efficient%20Clinical%20Phenotyping%0AAuthor%3A%20Sam%20Gijsen%20and%20Kerstin%20Ritter%0AAbstract%3A%20%20%20Multimodal%20language%20modeling%20has%20enabled%20breakthroughs%20for%20representation%0Alearning%2C%20yet%20remains%20unexplored%20in%20the%20realm%20of%20functional%20brain%20data%20for%0Aclinical%20phenotyping.%20This%20paper%20pioneers%20EEG-language%20models%20%28ELMs%29%20trained%20on%0Aclinical%20reports%20and%2015000%20EEGs.%20We%20propose%20to%20combine%20multimodal%20alignment%20in%0Athis%20novel%20domain%20with%20timeseries%20cropping%20and%20text%20segmentation%2C%20enabling%20an%0Aextension%20based%20on%20multiple%20instance%20learning%20to%20alleviate%20misalignment%20between%0Airrelevant%20EEG%20or%20text%20segments.%20Our%20multimodal%20models%20significantly%20improve%0Aover%20EEG-only%20models%20across%20four%20clinical%20evaluations%20and%20for%20the%20first%20time%0Aenable%20zero-shot%20classification%20as%20well%20as%20retrieval%20of%20both%20neural%20signals%20and%0Areports.%20In%20sum%2C%20these%20results%20highlight%20the%20potential%20of%20ELMs%2C%20representing%0Asignificant%20progress%20for%20clinical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07480v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-Language%2520Pretraining%2520for%2520Highly%2520Label-Efficient%2520Clinical%2520Phenotyping%26entry.906535625%3DSam%2520Gijsen%2520and%2520Kerstin%2520Ritter%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520modeling%2520has%2520enabled%2520breakthroughs%2520for%2520representation%250Alearning%252C%2520yet%2520remains%2520unexplored%2520in%2520the%2520realm%2520of%2520functional%2520brain%2520data%2520for%250Aclinical%2520phenotyping.%2520This%2520paper%2520pioneers%2520EEG-language%2520models%2520%2528ELMs%2529%2520trained%2520on%250Aclinical%2520reports%2520and%252015000%2520EEGs.%2520We%2520propose%2520to%2520combine%2520multimodal%2520alignment%2520in%250Athis%2520novel%2520domain%2520with%2520timeseries%2520cropping%2520and%2520text%2520segmentation%252C%2520enabling%2520an%250Aextension%2520based%2520on%2520multiple%2520instance%2520learning%2520to%2520alleviate%2520misalignment%2520between%250Airrelevant%2520EEG%2520or%2520text%2520segments.%2520Our%2520multimodal%2520models%2520significantly%2520improve%250Aover%2520EEG-only%2520models%2520across%2520four%2520clinical%2520evaluations%2520and%2520for%2520the%2520first%2520time%250Aenable%2520zero-shot%2520classification%2520as%2520well%2520as%2520retrieval%2520of%2520both%2520neural%2520signals%2520and%250Areports.%2520In%2520sum%252C%2520these%2520results%2520highlight%2520the%2520potential%2520of%2520ELMs%252C%2520representing%250Asignificant%2520progress%2520for%2520clinical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07480v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-Language%20Pretraining%20for%20Highly%20Label-Efficient%20Clinical%20Phenotyping&entry.906535625=Sam%20Gijsen%20and%20Kerstin%20Ritter&entry.1292438233=%20%20Multimodal%20language%20modeling%20has%20enabled%20breakthroughs%20for%20representation%0Alearning%2C%20yet%20remains%20unexplored%20in%20the%20realm%20of%20functional%20brain%20data%20for%0Aclinical%20phenotyping.%20This%20paper%20pioneers%20EEG-language%20models%20%28ELMs%29%20trained%20on%0Aclinical%20reports%20and%2015000%20EEGs.%20We%20propose%20to%20combine%20multimodal%20alignment%20in%0Athis%20novel%20domain%20with%20timeseries%20cropping%20and%20text%20segmentation%2C%20enabling%20an%0Aextension%20based%20on%20multiple%20instance%20learning%20to%20alleviate%20misalignment%20between%0Airrelevant%20EEG%20or%20text%20segments.%20Our%20multimodal%20models%20significantly%20improve%0Aover%20EEG-only%20models%20across%20four%20clinical%20evaluations%20and%20for%20the%20first%20time%0Aenable%20zero-shot%20classification%20as%20well%20as%20retrieval%20of%20both%20neural%20signals%20and%0Areports.%20In%20sum%2C%20these%20results%20highlight%20the%20potential%20of%20ELMs%2C%20representing%0Asignificant%20progress%20for%20clinical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07480v4&entry.124074799=Read"},
{"title": "Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention\n  Asymmetry", "author": "Zs\u00f3fia Moln\u00e1r and Gergely Szab\u00f3 and Andr\u00e1s Horv\u00e1th", "abstract": "  Supervised pretrained models have become widely used in deep learning,\nespecially for image segmentation tasks. However, when applied to specialized\ndatasets such as biomedical imaging, pretrained weights often introduce\nunintended biases. These biases cause models to assign different levels of\nimportance to different slices, leading to inconsistencies in feature\nutilization, which can be observed as asymmetries in saliency map\ndistributions. This transfer of color distributions from natural images to\nnon-natural datasets can compromise model performance and reduce the\nreliability of results. In this study, we investigate the effects of these\nbiases and propose strategies to mitigate them. Through a series of\nexperiments, we test both pretrained and randomly initialized models, comparing\ntheir performance and saliency map distributions. Our proposed methods, which\naim to neutralize the bias introduced by pretrained color channel weights,\ndemonstrate promising results, offering a practical approach to improving model\nexplainability while maintaining the benefits of pretrained models. This\npublication presents our findings, providing insights into addressing\npretrained weight biases across various deep learning tasks.\n", "link": "http://arxiv.org/abs/2505.14105v2", "date": "2025-08-11", "relevancy": 2.5228, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.526}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unintended%20Bias%20in%202D%2B%20Image%20Segmentation%20and%20Its%20Effect%20on%20Attention%0A%20%20Asymmetry&body=Title%3A%20Unintended%20Bias%20in%202D%2B%20Image%20Segmentation%20and%20Its%20Effect%20on%20Attention%0A%20%20Asymmetry%0AAuthor%3A%20Zs%C3%B3fia%20Moln%C3%A1r%20and%20Gergely%20Szab%C3%B3%20and%20Andr%C3%A1s%20Horv%C3%A1th%0AAbstract%3A%20%20%20Supervised%20pretrained%20models%20have%20become%20widely%20used%20in%20deep%20learning%2C%0Aespecially%20for%20image%20segmentation%20tasks.%20However%2C%20when%20applied%20to%20specialized%0Adatasets%20such%20as%20biomedical%20imaging%2C%20pretrained%20weights%20often%20introduce%0Aunintended%20biases.%20These%20biases%20cause%20models%20to%20assign%20different%20levels%20of%0Aimportance%20to%20different%20slices%2C%20leading%20to%20inconsistencies%20in%20feature%0Autilization%2C%20which%20can%20be%20observed%20as%20asymmetries%20in%20saliency%20map%0Adistributions.%20This%20transfer%20of%20color%20distributions%20from%20natural%20images%20to%0Anon-natural%20datasets%20can%20compromise%20model%20performance%20and%20reduce%20the%0Areliability%20of%20results.%20In%20this%20study%2C%20we%20investigate%20the%20effects%20of%20these%0Abiases%20and%20propose%20strategies%20to%20mitigate%20them.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20test%20both%20pretrained%20and%20randomly%20initialized%20models%2C%20comparing%0Atheir%20performance%20and%20saliency%20map%20distributions.%20Our%20proposed%20methods%2C%20which%0Aaim%20to%20neutralize%20the%20bias%20introduced%20by%20pretrained%20color%20channel%20weights%2C%0Ademonstrate%20promising%20results%2C%20offering%20a%20practical%20approach%20to%20improving%20model%0Aexplainability%20while%20maintaining%20the%20benefits%20of%20pretrained%20models.%20This%0Apublication%20presents%20our%20findings%2C%20providing%20insights%20into%20addressing%0Apretrained%20weight%20biases%20across%20various%20deep%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnintended%2520Bias%2520in%25202D%252B%2520Image%2520Segmentation%2520and%2520Its%2520Effect%2520on%2520Attention%250A%2520%2520Asymmetry%26entry.906535625%3DZs%25C3%25B3fia%2520Moln%25C3%25A1r%2520and%2520Gergely%2520Szab%25C3%25B3%2520and%2520Andr%25C3%25A1s%2520Horv%25C3%25A1th%26entry.1292438233%3D%2520%2520Supervised%2520pretrained%2520models%2520have%2520become%2520widely%2520used%2520in%2520deep%2520learning%252C%250Aespecially%2520for%2520image%2520segmentation%2520tasks.%2520However%252C%2520when%2520applied%2520to%2520specialized%250Adatasets%2520such%2520as%2520biomedical%2520imaging%252C%2520pretrained%2520weights%2520often%2520introduce%250Aunintended%2520biases.%2520These%2520biases%2520cause%2520models%2520to%2520assign%2520different%2520levels%2520of%250Aimportance%2520to%2520different%2520slices%252C%2520leading%2520to%2520inconsistencies%2520in%2520feature%250Autilization%252C%2520which%2520can%2520be%2520observed%2520as%2520asymmetries%2520in%2520saliency%2520map%250Adistributions.%2520This%2520transfer%2520of%2520color%2520distributions%2520from%2520natural%2520images%2520to%250Anon-natural%2520datasets%2520can%2520compromise%2520model%2520performance%2520and%2520reduce%2520the%250Areliability%2520of%2520results.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520effects%2520of%2520these%250Abiases%2520and%2520propose%2520strategies%2520to%2520mitigate%2520them.%2520Through%2520a%2520series%2520of%250Aexperiments%252C%2520we%2520test%2520both%2520pretrained%2520and%2520randomly%2520initialized%2520models%252C%2520comparing%250Atheir%2520performance%2520and%2520saliency%2520map%2520distributions.%2520Our%2520proposed%2520methods%252C%2520which%250Aaim%2520to%2520neutralize%2520the%2520bias%2520introduced%2520by%2520pretrained%2520color%2520channel%2520weights%252C%250Ademonstrate%2520promising%2520results%252C%2520offering%2520a%2520practical%2520approach%2520to%2520improving%2520model%250Aexplainability%2520while%2520maintaining%2520the%2520benefits%2520of%2520pretrained%2520models.%2520This%250Apublication%2520presents%2520our%2520findings%252C%2520providing%2520insights%2520into%2520addressing%250Apretrained%2520weight%2520biases%2520across%2520various%2520deep%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unintended%20Bias%20in%202D%2B%20Image%20Segmentation%20and%20Its%20Effect%20on%20Attention%0A%20%20Asymmetry&entry.906535625=Zs%C3%B3fia%20Moln%C3%A1r%20and%20Gergely%20Szab%C3%B3%20and%20Andr%C3%A1s%20Horv%C3%A1th&entry.1292438233=%20%20Supervised%20pretrained%20models%20have%20become%20widely%20used%20in%20deep%20learning%2C%0Aespecially%20for%20image%20segmentation%20tasks.%20However%2C%20when%20applied%20to%20specialized%0Adatasets%20such%20as%20biomedical%20imaging%2C%20pretrained%20weights%20often%20introduce%0Aunintended%20biases.%20These%20biases%20cause%20models%20to%20assign%20different%20levels%20of%0Aimportance%20to%20different%20slices%2C%20leading%20to%20inconsistencies%20in%20feature%0Autilization%2C%20which%20can%20be%20observed%20as%20asymmetries%20in%20saliency%20map%0Adistributions.%20This%20transfer%20of%20color%20distributions%20from%20natural%20images%20to%0Anon-natural%20datasets%20can%20compromise%20model%20performance%20and%20reduce%20the%0Areliability%20of%20results.%20In%20this%20study%2C%20we%20investigate%20the%20effects%20of%20these%0Abiases%20and%20propose%20strategies%20to%20mitigate%20them.%20Through%20a%20series%20of%0Aexperiments%2C%20we%20test%20both%20pretrained%20and%20randomly%20initialized%20models%2C%20comparing%0Atheir%20performance%20and%20saliency%20map%20distributions.%20Our%20proposed%20methods%2C%20which%0Aaim%20to%20neutralize%20the%20bias%20introduced%20by%20pretrained%20color%20channel%20weights%2C%0Ademonstrate%20promising%20results%2C%20offering%20a%20practical%20approach%20to%20improving%20model%0Aexplainability%20while%20maintaining%20the%20benefits%20of%20pretrained%20models.%20This%0Apublication%20presents%20our%20findings%2C%20providing%20insights%20into%20addressing%0Apretrained%20weight%20biases%20across%20various%20deep%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14105v2&entry.124074799=Read"},
{"title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce\n  High-Resolution Time-Varying Data", "author": "Chongke Bi and Xin Gao and Jiangkang Deng and  Guan", "abstract": "  Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.\n", "link": "http://arxiv.org/abs/2508.08173v1", "date": "2025-08-11", "relevancy": 2.5225, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6327}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6327}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.62}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data&body=Title%3A%20CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data%0AAuthor%3A%20Chongke%20Bi%20and%20Xin%20Gao%20and%20Jiangkang%20Deng%20and%20%20Guan%0AAbstract%3A%20%20%20Large-scale%20scientific%20simulations%20require%20significant%20resources%20to%20generate%0Ahigh-resolution%20time-varying%20data%20%28TVD%29.%20While%20super-resolution%20is%20an%20efficient%0Apost-processing%20strategy%20to%20reduce%20costs%2C%20existing%20methods%20rely%20on%20a%20large%0Aamount%20of%20HR%20training%20data%2C%20limiting%20their%20applicability%20to%20diverse%20simulation%0Ascenarios.%20To%20address%20this%20constraint%2C%20we%20proposed%20CD-TVD%2C%20a%20novel%20framework%0Athat%20combines%20contrastive%20learning%20and%20an%20improved%20diffusion-based%0Asuper-resolution%20model%20to%20achieve%20accurate%203D%20super-resolution%20from%20limited%0Atime-step%20high-resolution%20data.%20During%20pre-training%20on%20historical%20simulation%0Adata%2C%20the%20contrastive%20encoder%20and%20diffusion%20superresolution%20modules%20learn%0Adegradation%20patterns%20and%20detailed%20features%20of%20high-resolution%20and%0Alow-resolution%20samples.%20In%20the%20training%20phase%2C%20the%20improved%20diffusion%20model%0Awith%20a%20local%20attention%20mechanism%20is%20fine-tuned%20using%20only%20one%20newly%20generated%0Ahigh-resolution%20timestep%2C%20leveraging%20the%20degradation%20knowledge%20learned%20by%20the%0Aencoder.%20This%20design%20minimizes%20the%20reliance%20on%20large-scale%20high-resolution%0Adatasets%20while%20maintaining%20the%20capability%20to%20recover%20fine-grained%20details.%0AExperimental%20results%20on%20fluid%20and%20atmospheric%20simulation%20datasets%20confirm%20that%0ACD-TVD%20delivers%20accurate%20and%20resource-efficient%203D%20super-resolution%2C%20marking%20a%0Asignificant%20advancement%20in%20data%20augmentation%20for%20large-scale%20scientific%0Asimulations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Xin-Gao-private/CD-TVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCD-TVD%253A%2520Contrastive%2520Diffusion%2520for%25203D%2520Super-Resolution%2520with%2520Scarce%250A%2520%2520High-Resolution%2520Time-Varying%2520Data%26entry.906535625%3DChongke%2520Bi%2520and%2520Xin%2520Gao%2520and%2520Jiangkang%2520Deng%2520and%2520%2520Guan%26entry.1292438233%3D%2520%2520Large-scale%2520scientific%2520simulations%2520require%2520significant%2520resources%2520to%2520generate%250Ahigh-resolution%2520time-varying%2520data%2520%2528TVD%2529.%2520While%2520super-resolution%2520is%2520an%2520efficient%250Apost-processing%2520strategy%2520to%2520reduce%2520costs%252C%2520existing%2520methods%2520rely%2520on%2520a%2520large%250Aamount%2520of%2520HR%2520training%2520data%252C%2520limiting%2520their%2520applicability%2520to%2520diverse%2520simulation%250Ascenarios.%2520To%2520address%2520this%2520constraint%252C%2520we%2520proposed%2520CD-TVD%252C%2520a%2520novel%2520framework%250Athat%2520combines%2520contrastive%2520learning%2520and%2520an%2520improved%2520diffusion-based%250Asuper-resolution%2520model%2520to%2520achieve%2520accurate%25203D%2520super-resolution%2520from%2520limited%250Atime-step%2520high-resolution%2520data.%2520During%2520pre-training%2520on%2520historical%2520simulation%250Adata%252C%2520the%2520contrastive%2520encoder%2520and%2520diffusion%2520superresolution%2520modules%2520learn%250Adegradation%2520patterns%2520and%2520detailed%2520features%2520of%2520high-resolution%2520and%250Alow-resolution%2520samples.%2520In%2520the%2520training%2520phase%252C%2520the%2520improved%2520diffusion%2520model%250Awith%2520a%2520local%2520attention%2520mechanism%2520is%2520fine-tuned%2520using%2520only%2520one%2520newly%2520generated%250Ahigh-resolution%2520timestep%252C%2520leveraging%2520the%2520degradation%2520knowledge%2520learned%2520by%2520the%250Aencoder.%2520This%2520design%2520minimizes%2520the%2520reliance%2520on%2520large-scale%2520high-resolution%250Adatasets%2520while%2520maintaining%2520the%2520capability%2520to%2520recover%2520fine-grained%2520details.%250AExperimental%2520results%2520on%2520fluid%2520and%2520atmospheric%2520simulation%2520datasets%2520confirm%2520that%250ACD-TVD%2520delivers%2520accurate%2520and%2520resource-efficient%25203D%2520super-resolution%252C%2520marking%2520a%250Asignificant%2520advancement%2520in%2520data%2520augmentation%2520for%2520large-scale%2520scientific%250Asimulations.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Xin-Gao-private/CD-TVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CD-TVD%3A%20Contrastive%20Diffusion%20for%203D%20Super-Resolution%20with%20Scarce%0A%20%20High-Resolution%20Time-Varying%20Data&entry.906535625=Chongke%20Bi%20and%20Xin%20Gao%20and%20Jiangkang%20Deng%20and%20%20Guan&entry.1292438233=%20%20Large-scale%20scientific%20simulations%20require%20significant%20resources%20to%20generate%0Ahigh-resolution%20time-varying%20data%20%28TVD%29.%20While%20super-resolution%20is%20an%20efficient%0Apost-processing%20strategy%20to%20reduce%20costs%2C%20existing%20methods%20rely%20on%20a%20large%0Aamount%20of%20HR%20training%20data%2C%20limiting%20their%20applicability%20to%20diverse%20simulation%0Ascenarios.%20To%20address%20this%20constraint%2C%20we%20proposed%20CD-TVD%2C%20a%20novel%20framework%0Athat%20combines%20contrastive%20learning%20and%20an%20improved%20diffusion-based%0Asuper-resolution%20model%20to%20achieve%20accurate%203D%20super-resolution%20from%20limited%0Atime-step%20high-resolution%20data.%20During%20pre-training%20on%20historical%20simulation%0Adata%2C%20the%20contrastive%20encoder%20and%20diffusion%20superresolution%20modules%20learn%0Adegradation%20patterns%20and%20detailed%20features%20of%20high-resolution%20and%0Alow-resolution%20samples.%20In%20the%20training%20phase%2C%20the%20improved%20diffusion%20model%0Awith%20a%20local%20attention%20mechanism%20is%20fine-tuned%20using%20only%20one%20newly%20generated%0Ahigh-resolution%20timestep%2C%20leveraging%20the%20degradation%20knowledge%20learned%20by%20the%0Aencoder.%20This%20design%20minimizes%20the%20reliance%20on%20large-scale%20high-resolution%0Adatasets%20while%20maintaining%20the%20capability%20to%20recover%20fine-grained%20details.%0AExperimental%20results%20on%20fluid%20and%20atmospheric%20simulation%20datasets%20confirm%20that%0ACD-TVD%20delivers%20accurate%20and%20resource-efficient%203D%20super-resolution%2C%20marking%20a%0Asignificant%20advancement%20in%20data%20augmentation%20for%20large-scale%20scientific%0Asimulations.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Xin-Gao-private/CD-TVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08173v1&entry.124074799=Read"},
{"title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "author": "Zihe Liu and Jiashun Liu and Yancheng He and Weixun Wang and Jiaheng Liu and Ling Pan and Xinyu Hu and Shaopan Xiong and Ju Huang and Jian Hu and Shengyi Huang and Siran Yang and Jiamang Wang and Wenbo Su and Bo Zheng", "abstract": "  Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.\n", "link": "http://arxiv.org/abs/2508.08221v1", "date": "2025-08-11", "relevancy": 2.4962, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part%20I%3A%20Tricks%20or%20Traps%3F%20A%20Deep%20Dive%20into%20RL%20for%20LLM%20Reasoning&body=Title%3A%20Part%20I%3A%20Tricks%20or%20Traps%3F%20A%20Deep%20Dive%20into%20RL%20for%20LLM%20Reasoning%0AAuthor%3A%20Zihe%20Liu%20and%20Jiashun%20Liu%20and%20Yancheng%20He%20and%20Weixun%20Wang%20and%20Jiaheng%20Liu%20and%20Ling%20Pan%20and%20Xinyu%20Hu%20and%20Shaopan%20Xiong%20and%20Ju%20Huang%20and%20Jian%20Hu%20and%20Shengyi%20Huang%20and%20Siran%20Yang%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Reinforcement%20learning%20for%20LLM%20reasoning%20has%20rapidly%20emerged%20as%20a%20prominent%0Aresearch%20area%2C%20marked%20by%20a%20significant%20surge%20in%20related%20studies%20on%20both%0Aalgorithmic%20innovations%20and%20practical%20applications.%20Despite%20this%20progress%2C%0Aseveral%20critical%20challenges%20remain%2C%20including%20the%20absence%20of%20standardized%0Aguidelines%20for%20employing%20RL%20techniques%20and%20a%20fragmented%20understanding%20of%20their%0Aunderlying%20mechanisms.%20Additionally%2C%20inconsistent%20experimental%20settings%2C%0Avariations%20in%20training%20data%2C%20and%20differences%20in%20model%20initialization%20have%20led%0Ato%20conflicting%20conclusions%2C%20obscuring%20the%20key%20characteristics%20of%20these%0Atechniques%20and%20creating%20confusion%20among%20practitioners%20when%20selecting%0Aappropriate%20techniques.%20This%20paper%20systematically%20reviews%20widely%20adopted%20RL%0Atechniques%20through%20rigorous%20reproductions%20and%20isolated%20evaluations%20within%20a%0Aunified%20open-source%20framework.%20We%20analyze%20the%20internal%20mechanisms%2C%20applicable%0Ascenarios%2C%20and%20core%20principles%20of%20each%20technique%20through%20fine-grained%0Aexperiments%2C%20including%20datasets%20of%20varying%20difficulty%2C%20model%20sizes%2C%20and%0Aarchitectures.%20Based%20on%20these%20insights%2C%20we%20present%20clear%20guidelines%20for%0Aselecting%20RL%20techniques%20tailored%20to%20specific%20setups%2C%20and%20provide%20a%20reliable%0Aroadmap%20for%20practitioners%20navigating%20the%20RL%20for%20the%20LLM%20domain.%20Finally%2C%20we%0Areveal%20that%20a%20minimalist%20combination%20of%20two%20techniques%20can%20unlock%20the%20learning%0Acapability%20of%20critic-free%20policies%20using%20vanilla%20PPO%20loss.%20The%20results%0Ademonstrate%20that%20our%20simple%20combination%20consistently%20improves%20performance%2C%0Asurpassing%20strategies%20like%20GRPO%20and%20DAPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart%2520I%253A%2520Tricks%2520or%2520Traps%253F%2520A%2520Deep%2520Dive%2520into%2520RL%2520for%2520LLM%2520Reasoning%26entry.906535625%3DZihe%2520Liu%2520and%2520Jiashun%2520Liu%2520and%2520Yancheng%2520He%2520and%2520Weixun%2520Wang%2520and%2520Jiaheng%2520Liu%2520and%2520Ling%2520Pan%2520and%2520Xinyu%2520Hu%2520and%2520Shaopan%2520Xiong%2520and%2520Ju%2520Huang%2520and%2520Jian%2520Hu%2520and%2520Shengyi%2520Huang%2520and%2520Siran%2520Yang%2520and%2520Jiamang%2520Wang%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520for%2520LLM%2520reasoning%2520has%2520rapidly%2520emerged%2520as%2520a%2520prominent%250Aresearch%2520area%252C%2520marked%2520by%2520a%2520significant%2520surge%2520in%2520related%2520studies%2520on%2520both%250Aalgorithmic%2520innovations%2520and%2520practical%2520applications.%2520Despite%2520this%2520progress%252C%250Aseveral%2520critical%2520challenges%2520remain%252C%2520including%2520the%2520absence%2520of%2520standardized%250Aguidelines%2520for%2520employing%2520RL%2520techniques%2520and%2520a%2520fragmented%2520understanding%2520of%2520their%250Aunderlying%2520mechanisms.%2520Additionally%252C%2520inconsistent%2520experimental%2520settings%252C%250Avariations%2520in%2520training%2520data%252C%2520and%2520differences%2520in%2520model%2520initialization%2520have%2520led%250Ato%2520conflicting%2520conclusions%252C%2520obscuring%2520the%2520key%2520characteristics%2520of%2520these%250Atechniques%2520and%2520creating%2520confusion%2520among%2520practitioners%2520when%2520selecting%250Aappropriate%2520techniques.%2520This%2520paper%2520systematically%2520reviews%2520widely%2520adopted%2520RL%250Atechniques%2520through%2520rigorous%2520reproductions%2520and%2520isolated%2520evaluations%2520within%2520a%250Aunified%2520open-source%2520framework.%2520We%2520analyze%2520the%2520internal%2520mechanisms%252C%2520applicable%250Ascenarios%252C%2520and%2520core%2520principles%2520of%2520each%2520technique%2520through%2520fine-grained%250Aexperiments%252C%2520including%2520datasets%2520of%2520varying%2520difficulty%252C%2520model%2520sizes%252C%2520and%250Aarchitectures.%2520Based%2520on%2520these%2520insights%252C%2520we%2520present%2520clear%2520guidelines%2520for%250Aselecting%2520RL%2520techniques%2520tailored%2520to%2520specific%2520setups%252C%2520and%2520provide%2520a%2520reliable%250Aroadmap%2520for%2520practitioners%2520navigating%2520the%2520RL%2520for%2520the%2520LLM%2520domain.%2520Finally%252C%2520we%250Areveal%2520that%2520a%2520minimalist%2520combination%2520of%2520two%2520techniques%2520can%2520unlock%2520the%2520learning%250Acapability%2520of%2520critic-free%2520policies%2520using%2520vanilla%2520PPO%2520loss.%2520The%2520results%250Ademonstrate%2520that%2520our%2520simple%2520combination%2520consistently%2520improves%2520performance%252C%250Asurpassing%2520strategies%2520like%2520GRPO%2520and%2520DAPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part%20I%3A%20Tricks%20or%20Traps%3F%20A%20Deep%20Dive%20into%20RL%20for%20LLM%20Reasoning&entry.906535625=Zihe%20Liu%20and%20Jiashun%20Liu%20and%20Yancheng%20He%20and%20Weixun%20Wang%20and%20Jiaheng%20Liu%20and%20Ling%20Pan%20and%20Xinyu%20Hu%20and%20Shaopan%20Xiong%20and%20Ju%20Huang%20and%20Jian%20Hu%20and%20Shengyi%20Huang%20and%20Siran%20Yang%20and%20Jiamang%20Wang%20and%20Wenbo%20Su%20and%20Bo%20Zheng&entry.1292438233=%20%20Reinforcement%20learning%20for%20LLM%20reasoning%20has%20rapidly%20emerged%20as%20a%20prominent%0Aresearch%20area%2C%20marked%20by%20a%20significant%20surge%20in%20related%20studies%20on%20both%0Aalgorithmic%20innovations%20and%20practical%20applications.%20Despite%20this%20progress%2C%0Aseveral%20critical%20challenges%20remain%2C%20including%20the%20absence%20of%20standardized%0Aguidelines%20for%20employing%20RL%20techniques%20and%20a%20fragmented%20understanding%20of%20their%0Aunderlying%20mechanisms.%20Additionally%2C%20inconsistent%20experimental%20settings%2C%0Avariations%20in%20training%20data%2C%20and%20differences%20in%20model%20initialization%20have%20led%0Ato%20conflicting%20conclusions%2C%20obscuring%20the%20key%20characteristics%20of%20these%0Atechniques%20and%20creating%20confusion%20among%20practitioners%20when%20selecting%0Aappropriate%20techniques.%20This%20paper%20systematically%20reviews%20widely%20adopted%20RL%0Atechniques%20through%20rigorous%20reproductions%20and%20isolated%20evaluations%20within%20a%0Aunified%20open-source%20framework.%20We%20analyze%20the%20internal%20mechanisms%2C%20applicable%0Ascenarios%2C%20and%20core%20principles%20of%20each%20technique%20through%20fine-grained%0Aexperiments%2C%20including%20datasets%20of%20varying%20difficulty%2C%20model%20sizes%2C%20and%0Aarchitectures.%20Based%20on%20these%20insights%2C%20we%20present%20clear%20guidelines%20for%0Aselecting%20RL%20techniques%20tailored%20to%20specific%20setups%2C%20and%20provide%20a%20reliable%0Aroadmap%20for%20practitioners%20navigating%20the%20RL%20for%20the%20LLM%20domain.%20Finally%2C%20we%0Areveal%20that%20a%20minimalist%20combination%20of%20two%20techniques%20can%20unlock%20the%20learning%0Acapability%20of%20critic-free%20policies%20using%20vanilla%20PPO%20loss.%20The%20results%0Ademonstrate%20that%20our%20simple%20combination%20consistently%20improves%20performance%2C%0Asurpassing%20strategies%20like%20GRPO%20and%20DAPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08221v1&entry.124074799=Read"},
{"title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces\n  Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "author": "Shoaib Ahmmad and Zubayer Ahmed Aditto and Md Mehrab Hossain and Noushin Yeasmin and Shorower Hossain", "abstract": "  This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.\n", "link": "http://arxiv.org/abs/2508.07885v1", "date": "2025-08-11", "relevancy": 2.4832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6115}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Navigation%20of%20Cloud-Controlled%20Quadcopters%20in%20Confined%20Spaces%0A%20%20Using%20Multi-Modal%20Perception%20and%20LLM-Driven%20High%20Semantic%20Reasoning&body=Title%3A%20Autonomous%20Navigation%20of%20Cloud-Controlled%20Quadcopters%20in%20Confined%20Spaces%0A%20%20Using%20Multi-Modal%20Perception%20and%20LLM-Driven%20High%20Semantic%20Reasoning%0AAuthor%3A%20Shoaib%20Ahmmad%20and%20Zubayer%20Ahmed%20Aditto%20and%20Md%20Mehrab%20Hossain%20and%20Noushin%20Yeasmin%20and%20Shorower%20Hossain%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20advanced%20AI-driven%20perception%20system%20for%20autonomous%0Aquadcopter%20navigation%20in%20GPS-denied%20indoor%20environments.%20The%20proposed%20framework%0Aleverages%20cloud%20computing%20to%20offload%20computationally%20intensive%20tasks%20and%0Aincorporates%20a%20custom-designed%20printed%20circuit%20board%20%28PCB%29%20for%20efficient%20sensor%0Adata%20acquisition%2C%20enabling%20robust%20navigation%20in%20confined%20spaces.%20The%20system%0Aintegrates%20YOLOv11%20for%20object%20detection%2C%20Depth%20Anything%20V2%20for%20monocular%20depth%0Aestimation%2C%20a%20PCB%20equipped%20with%20Time-of-Flight%20%28ToF%29%20sensors%20and%20an%20Inertial%0AMeasurement%20Unit%20%28IMU%29%2C%20and%20a%20cloud-based%20Large%20Language%20Model%20%28LLM%29%20for%0Acontext-aware%20decision-making.%20A%20virtual%20safety%20envelope%2C%20enforced%20by%0Acalibrated%20sensor%20offsets%2C%20ensures%20collision%20avoidance%2C%20while%20a%20multithreaded%0Aarchitecture%20achieves%20low-latency%20processing.%20Enhanced%20spatial%20awareness%20is%0Afacilitated%20by%203D%20bounding%20box%20estimation%20with%20Kalman%20filtering.%20Experimental%0Aresults%20in%20an%20indoor%20testbed%20demonstrate%20strong%20performance%2C%20with%20object%0Adetection%20achieving%20a%20mean%20Average%20Precision%20%28mAP50%29%20of%200.6%2C%20depth%20estimation%0AMean%20Absolute%20Error%20%28MAE%29%20of%207.2%20cm%2C%20only%2016%20safety%20envelope%20breaches%20across%2042%0Atrials%20over%20approximately%2011%20minutes%2C%20and%20end-to-end%20system%20latency%20below%201%0Asecond.%20This%20cloud-supported%2C%20high-intelligence%20framework%20serves%20as%20an%0Aauxiliary%20perception%20and%20navigation%20system%2C%20complementing%20state-of-the-art%0Adrone%20autonomy%20for%20GPS-denied%20confined%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Navigation%2520of%2520Cloud-Controlled%2520Quadcopters%2520in%2520Confined%2520Spaces%250A%2520%2520Using%2520Multi-Modal%2520Perception%2520and%2520LLM-Driven%2520High%2520Semantic%2520Reasoning%26entry.906535625%3DShoaib%2520Ahmmad%2520and%2520Zubayer%2520Ahmed%2520Aditto%2520and%2520Md%2520Mehrab%2520Hossain%2520and%2520Noushin%2520Yeasmin%2520and%2520Shorower%2520Hossain%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520advanced%2520AI-driven%2520perception%2520system%2520for%2520autonomous%250Aquadcopter%2520navigation%2520in%2520GPS-denied%2520indoor%2520environments.%2520The%2520proposed%2520framework%250Aleverages%2520cloud%2520computing%2520to%2520offload%2520computationally%2520intensive%2520tasks%2520and%250Aincorporates%2520a%2520custom-designed%2520printed%2520circuit%2520board%2520%2528PCB%2529%2520for%2520efficient%2520sensor%250Adata%2520acquisition%252C%2520enabling%2520robust%2520navigation%2520in%2520confined%2520spaces.%2520The%2520system%250Aintegrates%2520YOLOv11%2520for%2520object%2520detection%252C%2520Depth%2520Anything%2520V2%2520for%2520monocular%2520depth%250Aestimation%252C%2520a%2520PCB%2520equipped%2520with%2520Time-of-Flight%2520%2528ToF%2529%2520sensors%2520and%2520an%2520Inertial%250AMeasurement%2520Unit%2520%2528IMU%2529%252C%2520and%2520a%2520cloud-based%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520for%250Acontext-aware%2520decision-making.%2520A%2520virtual%2520safety%2520envelope%252C%2520enforced%2520by%250Acalibrated%2520sensor%2520offsets%252C%2520ensures%2520collision%2520avoidance%252C%2520while%2520a%2520multithreaded%250Aarchitecture%2520achieves%2520low-latency%2520processing.%2520Enhanced%2520spatial%2520awareness%2520is%250Afacilitated%2520by%25203D%2520bounding%2520box%2520estimation%2520with%2520Kalman%2520filtering.%2520Experimental%250Aresults%2520in%2520an%2520indoor%2520testbed%2520demonstrate%2520strong%2520performance%252C%2520with%2520object%250Adetection%2520achieving%2520a%2520mean%2520Average%2520Precision%2520%2528mAP50%2529%2520of%25200.6%252C%2520depth%2520estimation%250AMean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%25207.2%2520cm%252C%2520only%252016%2520safety%2520envelope%2520breaches%2520across%252042%250Atrials%2520over%2520approximately%252011%2520minutes%252C%2520and%2520end-to-end%2520system%2520latency%2520below%25201%250Asecond.%2520This%2520cloud-supported%252C%2520high-intelligence%2520framework%2520serves%2520as%2520an%250Aauxiliary%2520perception%2520and%2520navigation%2520system%252C%2520complementing%2520state-of-the-art%250Adrone%2520autonomy%2520for%2520GPS-denied%2520confined%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Navigation%20of%20Cloud-Controlled%20Quadcopters%20in%20Confined%20Spaces%0A%20%20Using%20Multi-Modal%20Perception%20and%20LLM-Driven%20High%20Semantic%20Reasoning&entry.906535625=Shoaib%20Ahmmad%20and%20Zubayer%20Ahmed%20Aditto%20and%20Md%20Mehrab%20Hossain%20and%20Noushin%20Yeasmin%20and%20Shorower%20Hossain&entry.1292438233=%20%20This%20paper%20introduces%20an%20advanced%20AI-driven%20perception%20system%20for%20autonomous%0Aquadcopter%20navigation%20in%20GPS-denied%20indoor%20environments.%20The%20proposed%20framework%0Aleverages%20cloud%20computing%20to%20offload%20computationally%20intensive%20tasks%20and%0Aincorporates%20a%20custom-designed%20printed%20circuit%20board%20%28PCB%29%20for%20efficient%20sensor%0Adata%20acquisition%2C%20enabling%20robust%20navigation%20in%20confined%20spaces.%20The%20system%0Aintegrates%20YOLOv11%20for%20object%20detection%2C%20Depth%20Anything%20V2%20for%20monocular%20depth%0Aestimation%2C%20a%20PCB%20equipped%20with%20Time-of-Flight%20%28ToF%29%20sensors%20and%20an%20Inertial%0AMeasurement%20Unit%20%28IMU%29%2C%20and%20a%20cloud-based%20Large%20Language%20Model%20%28LLM%29%20for%0Acontext-aware%20decision-making.%20A%20virtual%20safety%20envelope%2C%20enforced%20by%0Acalibrated%20sensor%20offsets%2C%20ensures%20collision%20avoidance%2C%20while%20a%20multithreaded%0Aarchitecture%20achieves%20low-latency%20processing.%20Enhanced%20spatial%20awareness%20is%0Afacilitated%20by%203D%20bounding%20box%20estimation%20with%20Kalman%20filtering.%20Experimental%0Aresults%20in%20an%20indoor%20testbed%20demonstrate%20strong%20performance%2C%20with%20object%0Adetection%20achieving%20a%20mean%20Average%20Precision%20%28mAP50%29%20of%200.6%2C%20depth%20estimation%0AMean%20Absolute%20Error%20%28MAE%29%20of%207.2%20cm%2C%20only%2016%20safety%20envelope%20breaches%20across%2042%0Atrials%20over%20approximately%2011%20minutes%2C%20and%20end-to-end%20system%20latency%20below%201%0Asecond.%20This%20cloud-supported%2C%20high-intelligence%20framework%20serves%20as%20an%0Aauxiliary%20perception%20and%20navigation%20system%2C%20complementing%20state-of-the-art%0Adrone%20autonomy%20for%20GPS-denied%20confined%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07885v1&entry.124074799=Read"},
{"title": "FairFLRep: Fairness aware fault localization and repair of Deep Neural\n  Networks", "author": "Moses Openja and Paolo Arcaini and Foutse Khomh and Fuyuki Ishikawa", "abstract": "  Deep neural networks (DNNs) are being utilized in various aspects of our\ndaily lives, including high-stakes decision-making applications that impact\nindividuals. However, these systems reflect and amplify bias from the data used\nduring training and testing, potentially resulting in biased behavior and\ninaccurate decisions. For instance, having different misclassification rates\nbetween white and black sub-populations. However, effectively and efficiently\nidentifying and correcting biased behavior in DNNs is a challenge. This paper\nintroduces FairFLRep, an automated fairness-aware fault localization and repair\ntechnique that identifies and corrects potentially bias-inducing neurons in DNN\nclassifiers. FairFLRep focuses on adjusting neuron weights associated with\nsensitive attributes, such as race or gender, that contribute to unfair\ndecisions. By analyzing the input-output relationships within the network,\nFairFLRep corrects neurons responsible for disparities in predictive quality\nparity. We evaluate FairFLRep on four image classification datasets using two\nDNN classifiers, and four tabular datasets with a DNN model. The results show\nthat FairFLRep consistently outperforms existing methods in improving fairness\nwhile preserving accuracy. An ablation study confirms the importance of\nconsidering fairness during both fault localization and repair stages. Our\nfindings also show that FairFLRep is more efficient than the baseline\napproaches in repairing the network.\n", "link": "http://arxiv.org/abs/2508.08151v1", "date": "2025-08-11", "relevancy": 2.4647, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4847}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairFLRep%3A%20Fairness%20aware%20fault%20localization%20and%20repair%20of%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20FairFLRep%3A%20Fairness%20aware%20fault%20localization%20and%20repair%20of%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Moses%20Openja%20and%20Paolo%20Arcaini%20and%20Foutse%20Khomh%20and%20Fuyuki%20Ishikawa%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20being%20utilized%20in%20various%20aspects%20of%20our%0Adaily%20lives%2C%20including%20high-stakes%20decision-making%20applications%20that%20impact%0Aindividuals.%20However%2C%20these%20systems%20reflect%20and%20amplify%20bias%20from%20the%20data%20used%0Aduring%20training%20and%20testing%2C%20potentially%20resulting%20in%20biased%20behavior%20and%0Ainaccurate%20decisions.%20For%20instance%2C%20having%20different%20misclassification%20rates%0Abetween%20white%20and%20black%20sub-populations.%20However%2C%20effectively%20and%20efficiently%0Aidentifying%20and%20correcting%20biased%20behavior%20in%20DNNs%20is%20a%20challenge.%20This%20paper%0Aintroduces%20FairFLRep%2C%20an%20automated%20fairness-aware%20fault%20localization%20and%20repair%0Atechnique%20that%20identifies%20and%20corrects%20potentially%20bias-inducing%20neurons%20in%20DNN%0Aclassifiers.%20FairFLRep%20focuses%20on%20adjusting%20neuron%20weights%20associated%20with%0Asensitive%20attributes%2C%20such%20as%20race%20or%20gender%2C%20that%20contribute%20to%20unfair%0Adecisions.%20By%20analyzing%20the%20input-output%20relationships%20within%20the%20network%2C%0AFairFLRep%20corrects%20neurons%20responsible%20for%20disparities%20in%20predictive%20quality%0Aparity.%20We%20evaluate%20FairFLRep%20on%20four%20image%20classification%20datasets%20using%20two%0ADNN%20classifiers%2C%20and%20four%20tabular%20datasets%20with%20a%20DNN%20model.%20The%20results%20show%0Athat%20FairFLRep%20consistently%20outperforms%20existing%20methods%20in%20improving%20fairness%0Awhile%20preserving%20accuracy.%20An%20ablation%20study%20confirms%20the%20importance%20of%0Aconsidering%20fairness%20during%20both%20fault%20localization%20and%20repair%20stages.%20Our%0Afindings%20also%20show%20that%20FairFLRep%20is%20more%20efficient%20than%20the%20baseline%0Aapproaches%20in%20repairing%20the%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairFLRep%253A%2520Fairness%2520aware%2520fault%2520localization%2520and%2520repair%2520of%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMoses%2520Openja%2520and%2520Paolo%2520Arcaini%2520and%2520Foutse%2520Khomh%2520and%2520Fuyuki%2520Ishikawa%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520being%2520utilized%2520in%2520various%2520aspects%2520of%2520our%250Adaily%2520lives%252C%2520including%2520high-stakes%2520decision-making%2520applications%2520that%2520impact%250Aindividuals.%2520However%252C%2520these%2520systems%2520reflect%2520and%2520amplify%2520bias%2520from%2520the%2520data%2520used%250Aduring%2520training%2520and%2520testing%252C%2520potentially%2520resulting%2520in%2520biased%2520behavior%2520and%250Ainaccurate%2520decisions.%2520For%2520instance%252C%2520having%2520different%2520misclassification%2520rates%250Abetween%2520white%2520and%2520black%2520sub-populations.%2520However%252C%2520effectively%2520and%2520efficiently%250Aidentifying%2520and%2520correcting%2520biased%2520behavior%2520in%2520DNNs%2520is%2520a%2520challenge.%2520This%2520paper%250Aintroduces%2520FairFLRep%252C%2520an%2520automated%2520fairness-aware%2520fault%2520localization%2520and%2520repair%250Atechnique%2520that%2520identifies%2520and%2520corrects%2520potentially%2520bias-inducing%2520neurons%2520in%2520DNN%250Aclassifiers.%2520FairFLRep%2520focuses%2520on%2520adjusting%2520neuron%2520weights%2520associated%2520with%250Asensitive%2520attributes%252C%2520such%2520as%2520race%2520or%2520gender%252C%2520that%2520contribute%2520to%2520unfair%250Adecisions.%2520By%2520analyzing%2520the%2520input-output%2520relationships%2520within%2520the%2520network%252C%250AFairFLRep%2520corrects%2520neurons%2520responsible%2520for%2520disparities%2520in%2520predictive%2520quality%250Aparity.%2520We%2520evaluate%2520FairFLRep%2520on%2520four%2520image%2520classification%2520datasets%2520using%2520two%250ADNN%2520classifiers%252C%2520and%2520four%2520tabular%2520datasets%2520with%2520a%2520DNN%2520model.%2520The%2520results%2520show%250Athat%2520FairFLRep%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520improving%2520fairness%250Awhile%2520preserving%2520accuracy.%2520An%2520ablation%2520study%2520confirms%2520the%2520importance%2520of%250Aconsidering%2520fairness%2520during%2520both%2520fault%2520localization%2520and%2520repair%2520stages.%2520Our%250Afindings%2520also%2520show%2520that%2520FairFLRep%2520is%2520more%2520efficient%2520than%2520the%2520baseline%250Aapproaches%2520in%2520repairing%2520the%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairFLRep%3A%20Fairness%20aware%20fault%20localization%20and%20repair%20of%20Deep%20Neural%0A%20%20Networks&entry.906535625=Moses%20Openja%20and%20Paolo%20Arcaini%20and%20Foutse%20Khomh%20and%20Fuyuki%20Ishikawa&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20being%20utilized%20in%20various%20aspects%20of%20our%0Adaily%20lives%2C%20including%20high-stakes%20decision-making%20applications%20that%20impact%0Aindividuals.%20However%2C%20these%20systems%20reflect%20and%20amplify%20bias%20from%20the%20data%20used%0Aduring%20training%20and%20testing%2C%20potentially%20resulting%20in%20biased%20behavior%20and%0Ainaccurate%20decisions.%20For%20instance%2C%20having%20different%20misclassification%20rates%0Abetween%20white%20and%20black%20sub-populations.%20However%2C%20effectively%20and%20efficiently%0Aidentifying%20and%20correcting%20biased%20behavior%20in%20DNNs%20is%20a%20challenge.%20This%20paper%0Aintroduces%20FairFLRep%2C%20an%20automated%20fairness-aware%20fault%20localization%20and%20repair%0Atechnique%20that%20identifies%20and%20corrects%20potentially%20bias-inducing%20neurons%20in%20DNN%0Aclassifiers.%20FairFLRep%20focuses%20on%20adjusting%20neuron%20weights%20associated%20with%0Asensitive%20attributes%2C%20such%20as%20race%20or%20gender%2C%20that%20contribute%20to%20unfair%0Adecisions.%20By%20analyzing%20the%20input-output%20relationships%20within%20the%20network%2C%0AFairFLRep%20corrects%20neurons%20responsible%20for%20disparities%20in%20predictive%20quality%0Aparity.%20We%20evaluate%20FairFLRep%20on%20four%20image%20classification%20datasets%20using%20two%0ADNN%20classifiers%2C%20and%20four%20tabular%20datasets%20with%20a%20DNN%20model.%20The%20results%20show%0Athat%20FairFLRep%20consistently%20outperforms%20existing%20methods%20in%20improving%20fairness%0Awhile%20preserving%20accuracy.%20An%20ablation%20study%20confirms%20the%20importance%20of%0Aconsidering%20fairness%20during%20both%20fault%20localization%20and%20repair%20stages.%20Our%0Afindings%20also%20show%20that%20FairFLRep%20is%20more%20efficient%20than%20the%20baseline%0Aapproaches%20in%20repairing%20the%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08151v1&entry.124074799=Read"},
{"title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model", "author": "Bin Cao and Sipeng Zheng and Ye Wang and Lujie Xia and Qianshan Wei and Qin Jin and Jing Liu and Zongqing Lu", "abstract": "  Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.\n", "link": "http://arxiv.org/abs/2508.07863v1", "date": "2025-08-11", "relevancy": 2.4635, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6542}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Being-M0.5%3A%20A%20Real-Time%20Controllable%20Vision-Language-Motion%20Model&body=Title%3A%20Being-M0.5%3A%20A%20Real-Time%20Controllable%20Vision-Language-Motion%20Model%0AAuthor%3A%20Bin%20Cao%20and%20Sipeng%20Zheng%20and%20Ye%20Wang%20and%20Lujie%20Xia%20and%20Qianshan%20Wei%20and%20Qin%20Jin%20and%20Jing%20Liu%20and%20Zongqing%20Lu%0AAbstract%3A%20%20%20Human%20motion%20generation%20has%20emerged%20as%20a%20critical%20technology%20with%0Atransformative%20potential%20for%20real-world%20applications.%20However%2C%20existing%0Avision-language-motion%20models%20%28VLMMs%29%20face%20significant%20limitations%20that%20hinder%0Atheir%20practical%20deployment.%20We%20identify%20controllability%20as%20a%20main%20bottleneck%2C%0Amanifesting%20in%20five%20key%20aspects%3A%20inadequate%20response%20to%20diverse%20human%20commands%2C%0Alimited%20pose%20initialization%20capabilities%2C%20poor%20performance%20on%20long-term%0Asequences%2C%20insufficient%20handling%20of%20unseen%20scenarios%2C%20and%20lack%20of%20fine-grained%0Acontrol%20over%20individual%20body%20parts.%20To%20overcome%20these%20limitations%2C%20we%20present%0ABeing-M0.5%2C%20the%20first%20real-time%2C%20controllable%20VLMM%20that%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20motion%20generation%20tasks.%20Our%0Aapproach%20is%20built%20upon%20HuMo100M%2C%20the%20largest%20and%20most%20comprehensive%20human%0Amotion%20dataset%20to%20date%2C%20comprising%20over%205%20million%20self-collected%20motion%0Asequences%2C%20100%20million%20multi-task%20instructional%20instances%2C%20and%20detailed%0Apart-level%20annotations%20that%20address%20a%20critical%20gap%20in%20existing%20datasets.%20We%0Aintroduce%20a%20novel%20part-aware%20residual%20quantization%20technique%20for%20motion%0Atokenization%20that%20enables%20precise%2C%20granular%20control%20over%20individual%20body%20parts%0Aduring%20generation.%20Extensive%20experimental%20validation%20demonstrates%20Being-M0.5%27s%0Asuperior%20performance%20across%20diverse%20motion%20benchmarks%2C%20while%20comprehensive%0Aefficiency%20analysis%20confirms%20its%20real-time%20capabilities.%20Our%20contributions%0Ainclude%20design%20insights%20and%20detailed%20computational%20analysis%20to%20guide%20future%0Adevelopment%20of%20practical%20motion%20generators.%20We%20believe%20that%20HuMo100M%20and%0ABeing-M0.5%20represent%20significant%20advances%20that%20will%20accelerate%20the%20adoption%20of%0Amotion%20generation%20technologies%20in%20real-world%20applications.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//beingbeyond.github.io/Being-M0.5.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeing-M0.5%253A%2520A%2520Real-Time%2520Controllable%2520Vision-Language-Motion%2520Model%26entry.906535625%3DBin%2520Cao%2520and%2520Sipeng%2520Zheng%2520and%2520Ye%2520Wang%2520and%2520Lujie%2520Xia%2520and%2520Qianshan%2520Wei%2520and%2520Qin%2520Jin%2520and%2520Jing%2520Liu%2520and%2520Zongqing%2520Lu%26entry.1292438233%3D%2520%2520Human%2520motion%2520generation%2520has%2520emerged%2520as%2520a%2520critical%2520technology%2520with%250Atransformative%2520potential%2520for%2520real-world%2520applications.%2520However%252C%2520existing%250Avision-language-motion%2520models%2520%2528VLMMs%2529%2520face%2520significant%2520limitations%2520that%2520hinder%250Atheir%2520practical%2520deployment.%2520We%2520identify%2520controllability%2520as%2520a%2520main%2520bottleneck%252C%250Amanifesting%2520in%2520five%2520key%2520aspects%253A%2520inadequate%2520response%2520to%2520diverse%2520human%2520commands%252C%250Alimited%2520pose%2520initialization%2520capabilities%252C%2520poor%2520performance%2520on%2520long-term%250Asequences%252C%2520insufficient%2520handling%2520of%2520unseen%2520scenarios%252C%2520and%2520lack%2520of%2520fine-grained%250Acontrol%2520over%2520individual%2520body%2520parts.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%250ABeing-M0.5%252C%2520the%2520first%2520real-time%252C%2520controllable%2520VLMM%2520that%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520motion%2520generation%2520tasks.%2520Our%250Aapproach%2520is%2520built%2520upon%2520HuMo100M%252C%2520the%2520largest%2520and%2520most%2520comprehensive%2520human%250Amotion%2520dataset%2520to%2520date%252C%2520comprising%2520over%25205%2520million%2520self-collected%2520motion%250Asequences%252C%2520100%2520million%2520multi-task%2520instructional%2520instances%252C%2520and%2520detailed%250Apart-level%2520annotations%2520that%2520address%2520a%2520critical%2520gap%2520in%2520existing%2520datasets.%2520We%250Aintroduce%2520a%2520novel%2520part-aware%2520residual%2520quantization%2520technique%2520for%2520motion%250Atokenization%2520that%2520enables%2520precise%252C%2520granular%2520control%2520over%2520individual%2520body%2520parts%250Aduring%2520generation.%2520Extensive%2520experimental%2520validation%2520demonstrates%2520Being-M0.5%2527s%250Asuperior%2520performance%2520across%2520diverse%2520motion%2520benchmarks%252C%2520while%2520comprehensive%250Aefficiency%2520analysis%2520confirms%2520its%2520real-time%2520capabilities.%2520Our%2520contributions%250Ainclude%2520design%2520insights%2520and%2520detailed%2520computational%2520analysis%2520to%2520guide%2520future%250Adevelopment%2520of%2520practical%2520motion%2520generators.%2520We%2520believe%2520that%2520HuMo100M%2520and%250ABeing-M0.5%2520represent%2520significant%2520advances%2520that%2520will%2520accelerate%2520the%2520adoption%2520of%250Amotion%2520generation%2520technologies%2520in%2520real-world%2520applications.%2520The%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//beingbeyond.github.io/Being-M0.5.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Being-M0.5%3A%20A%20Real-Time%20Controllable%20Vision-Language-Motion%20Model&entry.906535625=Bin%20Cao%20and%20Sipeng%20Zheng%20and%20Ye%20Wang%20and%20Lujie%20Xia%20and%20Qianshan%20Wei%20and%20Qin%20Jin%20and%20Jing%20Liu%20and%20Zongqing%20Lu&entry.1292438233=%20%20Human%20motion%20generation%20has%20emerged%20as%20a%20critical%20technology%20with%0Atransformative%20potential%20for%20real-world%20applications.%20However%2C%20existing%0Avision-language-motion%20models%20%28VLMMs%29%20face%20significant%20limitations%20that%20hinder%0Atheir%20practical%20deployment.%20We%20identify%20controllability%20as%20a%20main%20bottleneck%2C%0Amanifesting%20in%20five%20key%20aspects%3A%20inadequate%20response%20to%20diverse%20human%20commands%2C%0Alimited%20pose%20initialization%20capabilities%2C%20poor%20performance%20on%20long-term%0Asequences%2C%20insufficient%20handling%20of%20unseen%20scenarios%2C%20and%20lack%20of%20fine-grained%0Acontrol%20over%20individual%20body%20parts.%20To%20overcome%20these%20limitations%2C%20we%20present%0ABeing-M0.5%2C%20the%20first%20real-time%2C%20controllable%20VLMM%20that%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20motion%20generation%20tasks.%20Our%0Aapproach%20is%20built%20upon%20HuMo100M%2C%20the%20largest%20and%20most%20comprehensive%20human%0Amotion%20dataset%20to%20date%2C%20comprising%20over%205%20million%20self-collected%20motion%0Asequences%2C%20100%20million%20multi-task%20instructional%20instances%2C%20and%20detailed%0Apart-level%20annotations%20that%20address%20a%20critical%20gap%20in%20existing%20datasets.%20We%0Aintroduce%20a%20novel%20part-aware%20residual%20quantization%20technique%20for%20motion%0Atokenization%20that%20enables%20precise%2C%20granular%20control%20over%20individual%20body%20parts%0Aduring%20generation.%20Extensive%20experimental%20validation%20demonstrates%20Being-M0.5%27s%0Asuperior%20performance%20across%20diverse%20motion%20benchmarks%2C%20while%20comprehensive%0Aefficiency%20analysis%20confirms%20its%20real-time%20capabilities.%20Our%20contributions%0Ainclude%20design%20insights%20and%20detailed%20computational%20analysis%20to%20guide%20future%0Adevelopment%20of%20practical%20motion%20generators.%20We%20believe%20that%20HuMo100M%20and%0ABeing-M0.5%20represent%20significant%20advances%20that%20will%20accelerate%20the%20adoption%20of%0Amotion%20generation%20technologies%20in%20real-world%20applications.%20The%20project%20page%20is%0Aavailable%20at%20https%3A//beingbeyond.github.io/Being-M0.5.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07863v1&entry.124074799=Read"},
{"title": "Aligning Instruction Tuning with Pre-training", "author": "Yiming Liang and Tianyu Zheng and Xinrun Du and Ge Zhang and Jiaheng Liu and Xingwei Qu and Wenqiang Zu and Xingrun Xing and Chujie Zheng and Lei Ma and Guoyin Wang and Zhaoxiang Zhang and Wenhao Huang and Xiang Yue and Jiajun Zhang", "abstract": "  Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.\n", "link": "http://arxiv.org/abs/2501.09368v4", "date": "2025-08-11", "relevancy": 2.4612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4773}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Instruction%20Tuning%20with%20Pre-training&body=Title%3A%20Aligning%20Instruction%20Tuning%20with%20Pre-training%0AAuthor%3A%20Yiming%20Liang%20and%20Tianyu%20Zheng%20and%20Xinrun%20Du%20and%20Ge%20Zhang%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Wenqiang%20Zu%20and%20Xingrun%20Xing%20and%20Chujie%20Zheng%20and%20Lei%20Ma%20and%20Guoyin%20Wang%20and%20Zhaoxiang%20Zhang%20and%20Wenhao%20Huang%20and%20Xiang%20Yue%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Instruction%20tuning%20enhances%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions%20across%20diverse%20tasks%2C%20relying%20on%20high-quality%20datasets%20to%20guide%0Abehavior.%20However%2C%20these%20datasets%2C%20whether%20manually%20curated%20or%20synthetically%0Agenerated%2C%20are%20often%20narrowly%20focused%20and%20misaligned%20with%20the%20broad%0Adistributions%20captured%20during%20pre-training%2C%20limiting%20LLM%20generalization%20and%0Aeffective%20use%20of%20pre-trained%20knowledge.%20We%20propose%20Aligning%20Instruction%20Tuning%0Awith%20Pre-training%20%28AITP%29%2C%20a%20method%20that%20bridges%20this%20gap%20by%20identifying%0Acoverage%20shortfalls%20in%20instruction-tuning%20datasets%20and%20rewriting%0Aunderrepresented%20pre-training%20data%20into%20high-quality%20instruction-response%0Apairs.%20This%20approach%20enriches%20dataset%20diversity%20while%20preserving%20task-specific%0Aobjectives.%20Evaluations%20on%20three%20fully%20open%20LLMs%20across%20eight%20benchmarks%0Ademonstrate%20consistent%20performance%20improvements%20with%20AITP.%20Ablations%20highlight%0Athe%20benefits%20of%20adaptive%20data%20selection%2C%20controlled%20rewriting%2C%20and%20balanced%0Aintegration%2C%20emphasizing%20the%20importance%20of%20aligning%20instruction%20tuning%20with%0Apre-training%20distributions%20to%20unlock%20the%20full%20potential%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09368v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Instruction%2520Tuning%2520with%2520Pre-training%26entry.906535625%3DYiming%2520Liang%2520and%2520Tianyu%2520Zheng%2520and%2520Xinrun%2520Du%2520and%2520Ge%2520Zhang%2520and%2520Jiaheng%2520Liu%2520and%2520Xingwei%2520Qu%2520and%2520Wenqiang%2520Zu%2520and%2520Xingrun%2520Xing%2520and%2520Chujie%2520Zheng%2520and%2520Lei%2520Ma%2520and%2520Guoyin%2520Wang%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Wenhao%2520Huang%2520and%2520Xiang%2520Yue%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520enhances%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520follow%2520human%250Ainstructions%2520across%2520diverse%2520tasks%252C%2520relying%2520on%2520high-quality%2520datasets%2520to%2520guide%250Abehavior.%2520However%252C%2520these%2520datasets%252C%2520whether%2520manually%2520curated%2520or%2520synthetically%250Agenerated%252C%2520are%2520often%2520narrowly%2520focused%2520and%2520misaligned%2520with%2520the%2520broad%250Adistributions%2520captured%2520during%2520pre-training%252C%2520limiting%2520LLM%2520generalization%2520and%250Aeffective%2520use%2520of%2520pre-trained%2520knowledge.%2520We%2520propose%2520Aligning%2520Instruction%2520Tuning%250Awith%2520Pre-training%2520%2528AITP%2529%252C%2520a%2520method%2520that%2520bridges%2520this%2520gap%2520by%2520identifying%250Acoverage%2520shortfalls%2520in%2520instruction-tuning%2520datasets%2520and%2520rewriting%250Aunderrepresented%2520pre-training%2520data%2520into%2520high-quality%2520instruction-response%250Apairs.%2520This%2520approach%2520enriches%2520dataset%2520diversity%2520while%2520preserving%2520task-specific%250Aobjectives.%2520Evaluations%2520on%2520three%2520fully%2520open%2520LLMs%2520across%2520eight%2520benchmarks%250Ademonstrate%2520consistent%2520performance%2520improvements%2520with%2520AITP.%2520Ablations%2520highlight%250Athe%2520benefits%2520of%2520adaptive%2520data%2520selection%252C%2520controlled%2520rewriting%252C%2520and%2520balanced%250Aintegration%252C%2520emphasizing%2520the%2520importance%2520of%2520aligning%2520instruction%2520tuning%2520with%250Apre-training%2520distributions%2520to%2520unlock%2520the%2520full%2520potential%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09368v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Instruction%20Tuning%20with%20Pre-training&entry.906535625=Yiming%20Liang%20and%20Tianyu%20Zheng%20and%20Xinrun%20Du%20and%20Ge%20Zhang%20and%20Jiaheng%20Liu%20and%20Xingwei%20Qu%20and%20Wenqiang%20Zu%20and%20Xingrun%20Xing%20and%20Chujie%20Zheng%20and%20Lei%20Ma%20and%20Guoyin%20Wang%20and%20Zhaoxiang%20Zhang%20and%20Wenhao%20Huang%20and%20Xiang%20Yue%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Instruction%20tuning%20enhances%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions%20across%20diverse%20tasks%2C%20relying%20on%20high-quality%20datasets%20to%20guide%0Abehavior.%20However%2C%20these%20datasets%2C%20whether%20manually%20curated%20or%20synthetically%0Agenerated%2C%20are%20often%20narrowly%20focused%20and%20misaligned%20with%20the%20broad%0Adistributions%20captured%20during%20pre-training%2C%20limiting%20LLM%20generalization%20and%0Aeffective%20use%20of%20pre-trained%20knowledge.%20We%20propose%20Aligning%20Instruction%20Tuning%0Awith%20Pre-training%20%28AITP%29%2C%20a%20method%20that%20bridges%20this%20gap%20by%20identifying%0Acoverage%20shortfalls%20in%20instruction-tuning%20datasets%20and%20rewriting%0Aunderrepresented%20pre-training%20data%20into%20high-quality%20instruction-response%0Apairs.%20This%20approach%20enriches%20dataset%20diversity%20while%20preserving%20task-specific%0Aobjectives.%20Evaluations%20on%20three%20fully%20open%20LLMs%20across%20eight%20benchmarks%0Ademonstrate%20consistent%20performance%20improvements%20with%20AITP.%20Ablations%20highlight%0Athe%20benefits%20of%20adaptive%20data%20selection%2C%20controlled%20rewriting%2C%20and%20balanced%0Aintegration%2C%20emphasizing%20the%20importance%20of%20aligning%20instruction%20tuning%20with%0Apre-training%20distributions%20to%20unlock%20the%20full%20potential%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09368v4&entry.124074799=Read"},
{"title": "PrIINeR: Towards Prior-Informed Implicit Neural Representations for\n  Accelerated MRI", "author": "Ziad Al-Haj Hemidi and Eytan Kats and Mattias P. Heinrich", "abstract": "  Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often\ndegrades image quality. While Implicit Neural Representations (INRs) show\npromise for MRI reconstruction, they struggle at high acceleration factors due\nto weak prior constraints, leading to structural loss and aliasing artefacts.\nTo address this, we propose PrIINeR, an INR-based MRI reconstruction method\nthat integrates prior knowledge from pre-trained deep learning models into the\nINR framework. By combining population-level knowledge with instance-based\noptimization and enforcing dual data consistency, PrIINeR aligns both with the\nacquired k-space data and the prior-informed reconstruction. Evaluated on the\nNYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based\napproaches but also improves upon several learning-based state-of-the-art\nmethods, significantly improving structural preservation and fidelity while\neffectively removing aliasing artefacts.PrIINeR bridges deep learning and\nINR-based techniques, offering a more reliable solution for high-quality,\naccelerated MRI reconstruction. The code is publicly available on\nhttps://github.com/multimodallearning/PrIINeR.\n", "link": "http://arxiv.org/abs/2508.08058v1", "date": "2025-08-11", "relevancy": 2.4547, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5024}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4965}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrIINeR%3A%20Towards%20Prior-Informed%20Implicit%20Neural%20Representations%20for%0A%20%20Accelerated%20MRI&body=Title%3A%20PrIINeR%3A%20Towards%20Prior-Informed%20Implicit%20Neural%20Representations%20for%0A%20%20Accelerated%20MRI%0AAuthor%3A%20Ziad%20Al-Haj%20Hemidi%20and%20Eytan%20Kats%20and%20Mattias%20P.%20Heinrich%0AAbstract%3A%20%20%20Accelerating%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20reduces%20scan%20time%20but%20often%0Adegrades%20image%20quality.%20While%20Implicit%20Neural%20Representations%20%28INRs%29%20show%0Apromise%20for%20MRI%20reconstruction%2C%20they%20struggle%20at%20high%20acceleration%20factors%20due%0Ato%20weak%20prior%20constraints%2C%20leading%20to%20structural%20loss%20and%20aliasing%20artefacts.%0ATo%20address%20this%2C%20we%20propose%20PrIINeR%2C%20an%20INR-based%20MRI%20reconstruction%20method%0Athat%20integrates%20prior%20knowledge%20from%20pre-trained%20deep%20learning%20models%20into%20the%0AINR%20framework.%20By%20combining%20population-level%20knowledge%20with%20instance-based%0Aoptimization%20and%20enforcing%20dual%20data%20consistency%2C%20PrIINeR%20aligns%20both%20with%20the%0Aacquired%20k-space%20data%20and%20the%20prior-informed%20reconstruction.%20Evaluated%20on%20the%0ANYU%20fastMRI%20dataset%2C%20our%20method%20not%20only%20outperforms%20state-of-the-art%20INR-based%0Aapproaches%20but%20also%20improves%20upon%20several%20learning-based%20state-of-the-art%0Amethods%2C%20significantly%20improving%20structural%20preservation%20and%20fidelity%20while%0Aeffectively%20removing%20aliasing%20artefacts.PrIINeR%20bridges%20deep%20learning%20and%0AINR-based%20techniques%2C%20offering%20a%20more%20reliable%20solution%20for%20high-quality%2C%0Aaccelerated%20MRI%20reconstruction.%20The%20code%20is%20publicly%20available%20on%0Ahttps%3A//github.com/multimodallearning/PrIINeR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrIINeR%253A%2520Towards%2520Prior-Informed%2520Implicit%2520Neural%2520Representations%2520for%250A%2520%2520Accelerated%2520MRI%26entry.906535625%3DZiad%2520Al-Haj%2520Hemidi%2520and%2520Eytan%2520Kats%2520and%2520Mattias%2520P.%2520Heinrich%26entry.1292438233%3D%2520%2520Accelerating%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520reduces%2520scan%2520time%2520but%2520often%250Adegrades%2520image%2520quality.%2520While%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520show%250Apromise%2520for%2520MRI%2520reconstruction%252C%2520they%2520struggle%2520at%2520high%2520acceleration%2520factors%2520due%250Ato%2520weak%2520prior%2520constraints%252C%2520leading%2520to%2520structural%2520loss%2520and%2520aliasing%2520artefacts.%250ATo%2520address%2520this%252C%2520we%2520propose%2520PrIINeR%252C%2520an%2520INR-based%2520MRI%2520reconstruction%2520method%250Athat%2520integrates%2520prior%2520knowledge%2520from%2520pre-trained%2520deep%2520learning%2520models%2520into%2520the%250AINR%2520framework.%2520By%2520combining%2520population-level%2520knowledge%2520with%2520instance-based%250Aoptimization%2520and%2520enforcing%2520dual%2520data%2520consistency%252C%2520PrIINeR%2520aligns%2520both%2520with%2520the%250Aacquired%2520k-space%2520data%2520and%2520the%2520prior-informed%2520reconstruction.%2520Evaluated%2520on%2520the%250ANYU%2520fastMRI%2520dataset%252C%2520our%2520method%2520not%2520only%2520outperforms%2520state-of-the-art%2520INR-based%250Aapproaches%2520but%2520also%2520improves%2520upon%2520several%2520learning-based%2520state-of-the-art%250Amethods%252C%2520significantly%2520improving%2520structural%2520preservation%2520and%2520fidelity%2520while%250Aeffectively%2520removing%2520aliasing%2520artefacts.PrIINeR%2520bridges%2520deep%2520learning%2520and%250AINR-based%2520techniques%252C%2520offering%2520a%2520more%2520reliable%2520solution%2520for%2520high-quality%252C%250Aaccelerated%2520MRI%2520reconstruction.%2520The%2520code%2520is%2520publicly%2520available%2520on%250Ahttps%253A//github.com/multimodallearning/PrIINeR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrIINeR%3A%20Towards%20Prior-Informed%20Implicit%20Neural%20Representations%20for%0A%20%20Accelerated%20MRI&entry.906535625=Ziad%20Al-Haj%20Hemidi%20and%20Eytan%20Kats%20and%20Mattias%20P.%20Heinrich&entry.1292438233=%20%20Accelerating%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20reduces%20scan%20time%20but%20often%0Adegrades%20image%20quality.%20While%20Implicit%20Neural%20Representations%20%28INRs%29%20show%0Apromise%20for%20MRI%20reconstruction%2C%20they%20struggle%20at%20high%20acceleration%20factors%20due%0Ato%20weak%20prior%20constraints%2C%20leading%20to%20structural%20loss%20and%20aliasing%20artefacts.%0ATo%20address%20this%2C%20we%20propose%20PrIINeR%2C%20an%20INR-based%20MRI%20reconstruction%20method%0Athat%20integrates%20prior%20knowledge%20from%20pre-trained%20deep%20learning%20models%20into%20the%0AINR%20framework.%20By%20combining%20population-level%20knowledge%20with%20instance-based%0Aoptimization%20and%20enforcing%20dual%20data%20consistency%2C%20PrIINeR%20aligns%20both%20with%20the%0Aacquired%20k-space%20data%20and%20the%20prior-informed%20reconstruction.%20Evaluated%20on%20the%0ANYU%20fastMRI%20dataset%2C%20our%20method%20not%20only%20outperforms%20state-of-the-art%20INR-based%0Aapproaches%20but%20also%20improves%20upon%20several%20learning-based%20state-of-the-art%0Amethods%2C%20significantly%20improving%20structural%20preservation%20and%20fidelity%20while%0Aeffectively%20removing%20aliasing%20artefacts.PrIINeR%20bridges%20deep%20learning%20and%0AINR-based%20techniques%2C%20offering%20a%20more%20reliable%20solution%20for%20high-quality%2C%0Aaccelerated%20MRI%20reconstruction.%20The%20code%20is%20publicly%20available%20on%0Ahttps%3A//github.com/multimodallearning/PrIINeR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08058v1&entry.124074799=Read"},
{"title": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs", "author": "Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the\npower of Large Language Models (LLMs) and avoids heavy end-to-end training.\nSpotter+GPT breaks down the SLT task into two distinct stages. First, a sign\nspotter identifies individual signs within the input video. The spotted signs\nare then passed to an LLM, which transforms them into meaningful spoken\nlanguage sentences. Spotter+GPT eliminates the requirement for SLT-specific\ntraining. This significantly reduces computational costs and time requirements.\nThe source code and pretrained weights of the Spotter are available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.\n", "link": "http://arxiv.org/abs/2403.10434v3", "date": "2025-08-11", "relevancy": 2.452, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spotter%2BGPT%3A%20Turning%20Sign%20Spottings%20into%20Sentences%20with%20LLMs&body=Title%3A%20Spotter%2BGPT%3A%20Turning%20Sign%20Spottings%20into%20Sentences%20with%20LLMs%0AAuthor%3A%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20aims%20to%20generate%0Aspoken%20language%20sentences%20from%20sign%20language%20videos.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20lightweight%2C%20modular%20SLT%20framework%2C%20Spotter%2BGPT%2C%20that%20leverages%20the%0Apower%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20avoids%20heavy%20end-to-end%20training.%0ASpotter%2BGPT%20breaks%20down%20the%20SLT%20task%20into%20two%20distinct%20stages.%20First%2C%20a%20sign%0Aspotter%20identifies%20individual%20signs%20within%20the%20input%20video.%20The%20spotted%20signs%0Aare%20then%20passed%20to%20an%20LLM%2C%20which%20transforms%20them%20into%20meaningful%20spoken%0Alanguage%20sentences.%20Spotter%2BGPT%20eliminates%20the%20requirement%20for%20SLT-specific%0Atraining.%20This%20significantly%20reduces%20computational%20costs%20and%20time%20requirements.%0AThe%20source%20code%20and%20pretrained%20weights%20of%20the%20Spotter%20are%20available%20at%0Ahttps%3A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10434v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotter%252BGPT%253A%2520Turning%2520Sign%2520Spottings%2520into%2520Sentences%2520with%2520LLMs%26entry.906535625%3DOzge%2520Mercanoglu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%2520generate%250Aspoken%2520language%2520sentences%2520from%2520sign%2520language%2520videos.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520lightweight%252C%2520modular%2520SLT%2520framework%252C%2520Spotter%252BGPT%252C%2520that%2520leverages%2520the%250Apower%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520avoids%2520heavy%2520end-to-end%2520training.%250ASpotter%252BGPT%2520breaks%2520down%2520the%2520SLT%2520task%2520into%2520two%2520distinct%2520stages.%2520First%252C%2520a%2520sign%250Aspotter%2520identifies%2520individual%2520signs%2520within%2520the%2520input%2520video.%2520The%2520spotted%2520signs%250Aare%2520then%2520passed%2520to%2520an%2520LLM%252C%2520which%2520transforms%2520them%2520into%2520meaningful%2520spoken%250Alanguage%2520sentences.%2520Spotter%252BGPT%2520eliminates%2520the%2520requirement%2520for%2520SLT-specific%250Atraining.%2520This%2520significantly%2520reduces%2520computational%2520costs%2520and%2520time%2520requirements.%250AThe%2520source%2520code%2520and%2520pretrained%2520weights%2520of%2520the%2520Spotter%2520are%2520available%2520at%250Ahttps%253A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10434v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spotter%2BGPT%3A%20Turning%20Sign%20Spottings%20into%20Sentences%20with%20LLMs&entry.906535625=Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20aims%20to%20generate%0Aspoken%20language%20sentences%20from%20sign%20language%20videos.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20lightweight%2C%20modular%20SLT%20framework%2C%20Spotter%2BGPT%2C%20that%20leverages%20the%0Apower%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20avoids%20heavy%20end-to-end%20training.%0ASpotter%2BGPT%20breaks%20down%20the%20SLT%20task%20into%20two%20distinct%20stages.%20First%2C%20a%20sign%0Aspotter%20identifies%20individual%20signs%20within%20the%20input%20video.%20The%20spotted%20signs%0Aare%20then%20passed%20to%20an%20LLM%2C%20which%20transforms%20them%20into%20meaningful%20spoken%0Alanguage%20sentences.%20Spotter%2BGPT%20eliminates%20the%20requirement%20for%20SLT-specific%0Atraining.%20This%20significantly%20reduces%20computational%20costs%20and%20time%20requirements.%0AThe%20source%20code%20and%20pretrained%20weights%20of%20the%20Spotter%20are%20available%20at%0Ahttps%3A//gitlab.surrey.ac.uk/cogvispublic/sign-spotter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10434v3&entry.124074799=Read"},
{"title": "On Understanding of the Dynamics of Model Capacity in Continual Learning", "author": "Supriyo Chakraborty and Krishnan Raghavan", "abstract": "  The stability-plasticity dilemma, closely related to a neural network's (NN)\ncapacity-its ability to represent tasks-is a fundamental challenge in continual\nlearning (CL). Within this context, we introduce CL's effective model capacity\n(CLEMC) that characterizes the dynamic behavior of the stability-plasticity\nbalance point. We develop a difference equation to model the evolution of the\ninterplay between the NN, task data, and optimization procedure. We then\nleverage CLEMC to demonstrate that the effective capacity-and, by extension,\nthe stability-plasticity balance point is inherently non-stationary. We show\nthat regardless of the NN architecture or optimization method, a NN's ability\nto represent new tasks diminishes when incoming task distributions differ from\nprevious ones. We conduct extensive experiments to support our theoretical\nfindings, spanning a range of architectures-from small feedforward network and\nconvolutional networks to medium-sized graph neural networks and\ntransformer-based large language models with millions of parameters.\n", "link": "http://arxiv.org/abs/2508.08052v1", "date": "2025-08-11", "relevancy": 2.4433, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning&body=Title%3A%20On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning%0AAuthor%3A%20Supriyo%20Chakraborty%20and%20Krishnan%20Raghavan%0AAbstract%3A%20%20%20The%20stability-plasticity%20dilemma%2C%20closely%20related%20to%20a%20neural%20network%27s%20%28NN%29%0Acapacity-its%20ability%20to%20represent%20tasks-is%20a%20fundamental%20challenge%20in%20continual%0Alearning%20%28CL%29.%20Within%20this%20context%2C%20we%20introduce%20CL%27s%20effective%20model%20capacity%0A%28CLEMC%29%20that%20characterizes%20the%20dynamic%20behavior%20of%20the%20stability-plasticity%0Abalance%20point.%20We%20develop%20a%20difference%20equation%20to%20model%20the%20evolution%20of%20the%0Ainterplay%20between%20the%20NN%2C%20task%20data%2C%20and%20optimization%20procedure.%20We%20then%0Aleverage%20CLEMC%20to%20demonstrate%20that%20the%20effective%20capacity-and%2C%20by%20extension%2C%0Athe%20stability-plasticity%20balance%20point%20is%20inherently%20non-stationary.%20We%20show%0Athat%20regardless%20of%20the%20NN%20architecture%20or%20optimization%20method%2C%20a%20NN%27s%20ability%0Ato%20represent%20new%20tasks%20diminishes%20when%20incoming%20task%20distributions%20differ%20from%0Aprevious%20ones.%20We%20conduct%20extensive%20experiments%20to%20support%20our%20theoretical%0Afindings%2C%20spanning%20a%20range%20of%20architectures-from%20small%20feedforward%20network%20and%0Aconvolutional%20networks%20to%20medium-sized%20graph%20neural%20networks%20and%0Atransformer-based%20large%20language%20models%20with%20millions%20of%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Understanding%2520of%2520the%2520Dynamics%2520of%2520Model%2520Capacity%2520in%2520Continual%2520Learning%26entry.906535625%3DSupriyo%2520Chakraborty%2520and%2520Krishnan%2520Raghavan%26entry.1292438233%3D%2520%2520The%2520stability-plasticity%2520dilemma%252C%2520closely%2520related%2520to%2520a%2520neural%2520network%2527s%2520%2528NN%2529%250Acapacity-its%2520ability%2520to%2520represent%2520tasks-is%2520a%2520fundamental%2520challenge%2520in%2520continual%250Alearning%2520%2528CL%2529.%2520Within%2520this%2520context%252C%2520we%2520introduce%2520CL%2527s%2520effective%2520model%2520capacity%250A%2528CLEMC%2529%2520that%2520characterizes%2520the%2520dynamic%2520behavior%2520of%2520the%2520stability-plasticity%250Abalance%2520point.%2520We%2520develop%2520a%2520difference%2520equation%2520to%2520model%2520the%2520evolution%2520of%2520the%250Ainterplay%2520between%2520the%2520NN%252C%2520task%2520data%252C%2520and%2520optimization%2520procedure.%2520We%2520then%250Aleverage%2520CLEMC%2520to%2520demonstrate%2520that%2520the%2520effective%2520capacity-and%252C%2520by%2520extension%252C%250Athe%2520stability-plasticity%2520balance%2520point%2520is%2520inherently%2520non-stationary.%2520We%2520show%250Athat%2520regardless%2520of%2520the%2520NN%2520architecture%2520or%2520optimization%2520method%252C%2520a%2520NN%2527s%2520ability%250Ato%2520represent%2520new%2520tasks%2520diminishes%2520when%2520incoming%2520task%2520distributions%2520differ%2520from%250Aprevious%2520ones.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520support%2520our%2520theoretical%250Afindings%252C%2520spanning%2520a%2520range%2520of%2520architectures-from%2520small%2520feedforward%2520network%2520and%250Aconvolutional%2520networks%2520to%2520medium-sized%2520graph%2520neural%2520networks%2520and%250Atransformer-based%2520large%2520language%2520models%2520with%2520millions%2520of%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Understanding%20of%20the%20Dynamics%20of%20Model%20Capacity%20in%20Continual%20Learning&entry.906535625=Supriyo%20Chakraborty%20and%20Krishnan%20Raghavan&entry.1292438233=%20%20The%20stability-plasticity%20dilemma%2C%20closely%20related%20to%20a%20neural%20network%27s%20%28NN%29%0Acapacity-its%20ability%20to%20represent%20tasks-is%20a%20fundamental%20challenge%20in%20continual%0Alearning%20%28CL%29.%20Within%20this%20context%2C%20we%20introduce%20CL%27s%20effective%20model%20capacity%0A%28CLEMC%29%20that%20characterizes%20the%20dynamic%20behavior%20of%20the%20stability-plasticity%0Abalance%20point.%20We%20develop%20a%20difference%20equation%20to%20model%20the%20evolution%20of%20the%0Ainterplay%20between%20the%20NN%2C%20task%20data%2C%20and%20optimization%20procedure.%20We%20then%0Aleverage%20CLEMC%20to%20demonstrate%20that%20the%20effective%20capacity-and%2C%20by%20extension%2C%0Athe%20stability-plasticity%20balance%20point%20is%20inherently%20non-stationary.%20We%20show%0Athat%20regardless%20of%20the%20NN%20architecture%20or%20optimization%20method%2C%20a%20NN%27s%20ability%0Ato%20represent%20new%20tasks%20diminishes%20when%20incoming%20task%20distributions%20differ%20from%0Aprevious%20ones.%20We%20conduct%20extensive%20experiments%20to%20support%20our%20theoretical%0Afindings%2C%20spanning%20a%20range%20of%20architectures-from%20small%20feedforward%20network%20and%0Aconvolutional%20networks%20to%20medium-sized%20graph%20neural%20networks%20and%0Atransformer-based%20large%20language%20models%20with%20millions%20of%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08052v1&entry.124074799=Read"},
{"title": "SOPHY: Learning to Generate Simulation-Ready Objects with Physical\n  Materials", "author": "Junyi Cao and Evangelos Kalogerakis", "abstract": "  We present SOPHY, a generative model for 3D physics-aware shape synthesis.\nUnlike existing 3D generative models that focus solely on static geometry or 4D\nmodels that produce physics-agnostic animations, our method jointly synthesizes\nshape, texture, and material properties related to physics-grounded dynamics,\nmaking the generated objects ready for simulations and interactive, dynamic\nenvironments. To train our model, we introduce a dataset of 3D objects\nannotated with detailed physical material attributes, along with an efficient\npipeline for material annotation. Our method enables applications such as\ntext-driven generation of interactive, physics-aware 3D objects and\nsingle-image reconstruction of physically plausible shapes. Furthermore, our\nexperiments show that jointly modeling shape and material properties enhances\nthe realism and fidelity of the generated shapes, improving performance on both\ngenerative geometry and physical plausibility.\n", "link": "http://arxiv.org/abs/2504.12684v3", "date": "2025-08-11", "relevancy": 2.4302, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6255}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5989}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOPHY%3A%20Learning%20to%20Generate%20Simulation-Ready%20Objects%20with%20Physical%0A%20%20Materials&body=Title%3A%20SOPHY%3A%20Learning%20to%20Generate%20Simulation-Ready%20Objects%20with%20Physical%0A%20%20Materials%0AAuthor%3A%20Junyi%20Cao%20and%20Evangelos%20Kalogerakis%0AAbstract%3A%20%20%20We%20present%20SOPHY%2C%20a%20generative%20model%20for%203D%20physics-aware%20shape%20synthesis.%0AUnlike%20existing%203D%20generative%20models%20that%20focus%20solely%20on%20static%20geometry%20or%204D%0Amodels%20that%20produce%20physics-agnostic%20animations%2C%20our%20method%20jointly%20synthesizes%0Ashape%2C%20texture%2C%20and%20material%20properties%20related%20to%20physics-grounded%20dynamics%2C%0Amaking%20the%20generated%20objects%20ready%20for%20simulations%20and%20interactive%2C%20dynamic%0Aenvironments.%20To%20train%20our%20model%2C%20we%20introduce%20a%20dataset%20of%203D%20objects%0Aannotated%20with%20detailed%20physical%20material%20attributes%2C%20along%20with%20an%20efficient%0Apipeline%20for%20material%20annotation.%20Our%20method%20enables%20applications%20such%20as%0Atext-driven%20generation%20of%20interactive%2C%20physics-aware%203D%20objects%20and%0Asingle-image%20reconstruction%20of%20physically%20plausible%20shapes.%20Furthermore%2C%20our%0Aexperiments%20show%20that%20jointly%20modeling%20shape%20and%20material%20properties%20enhances%0Athe%20realism%20and%20fidelity%20of%20the%20generated%20shapes%2C%20improving%20performance%20on%20both%0Agenerative%20geometry%20and%20physical%20plausibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12684v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOPHY%253A%2520Learning%2520to%2520Generate%2520Simulation-Ready%2520Objects%2520with%2520Physical%250A%2520%2520Materials%26entry.906535625%3DJunyi%2520Cao%2520and%2520Evangelos%2520Kalogerakis%26entry.1292438233%3D%2520%2520We%2520present%2520SOPHY%252C%2520a%2520generative%2520model%2520for%25203D%2520physics-aware%2520shape%2520synthesis.%250AUnlike%2520existing%25203D%2520generative%2520models%2520that%2520focus%2520solely%2520on%2520static%2520geometry%2520or%25204D%250Amodels%2520that%2520produce%2520physics-agnostic%2520animations%252C%2520our%2520method%2520jointly%2520synthesizes%250Ashape%252C%2520texture%252C%2520and%2520material%2520properties%2520related%2520to%2520physics-grounded%2520dynamics%252C%250Amaking%2520the%2520generated%2520objects%2520ready%2520for%2520simulations%2520and%2520interactive%252C%2520dynamic%250Aenvironments.%2520To%2520train%2520our%2520model%252C%2520we%2520introduce%2520a%2520dataset%2520of%25203D%2520objects%250Aannotated%2520with%2520detailed%2520physical%2520material%2520attributes%252C%2520along%2520with%2520an%2520efficient%250Apipeline%2520for%2520material%2520annotation.%2520Our%2520method%2520enables%2520applications%2520such%2520as%250Atext-driven%2520generation%2520of%2520interactive%252C%2520physics-aware%25203D%2520objects%2520and%250Asingle-image%2520reconstruction%2520of%2520physically%2520plausible%2520shapes.%2520Furthermore%252C%2520our%250Aexperiments%2520show%2520that%2520jointly%2520modeling%2520shape%2520and%2520material%2520properties%2520enhances%250Athe%2520realism%2520and%2520fidelity%2520of%2520the%2520generated%2520shapes%252C%2520improving%2520performance%2520on%2520both%250Agenerative%2520geometry%2520and%2520physical%2520plausibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12684v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOPHY%3A%20Learning%20to%20Generate%20Simulation-Ready%20Objects%20with%20Physical%0A%20%20Materials&entry.906535625=Junyi%20Cao%20and%20Evangelos%20Kalogerakis&entry.1292438233=%20%20We%20present%20SOPHY%2C%20a%20generative%20model%20for%203D%20physics-aware%20shape%20synthesis.%0AUnlike%20existing%203D%20generative%20models%20that%20focus%20solely%20on%20static%20geometry%20or%204D%0Amodels%20that%20produce%20physics-agnostic%20animations%2C%20our%20method%20jointly%20synthesizes%0Ashape%2C%20texture%2C%20and%20material%20properties%20related%20to%20physics-grounded%20dynamics%2C%0Amaking%20the%20generated%20objects%20ready%20for%20simulations%20and%20interactive%2C%20dynamic%0Aenvironments.%20To%20train%20our%20model%2C%20we%20introduce%20a%20dataset%20of%203D%20objects%0Aannotated%20with%20detailed%20physical%20material%20attributes%2C%20along%20with%20an%20efficient%0Apipeline%20for%20material%20annotation.%20Our%20method%20enables%20applications%20such%20as%0Atext-driven%20generation%20of%20interactive%2C%20physics-aware%203D%20objects%20and%0Asingle-image%20reconstruction%20of%20physically%20plausible%20shapes.%20Furthermore%2C%20our%0Aexperiments%20show%20that%20jointly%20modeling%20shape%20and%20material%20properties%20enhances%0Athe%20realism%20and%20fidelity%20of%20the%20generated%20shapes%2C%20improving%20performance%20on%20both%0Agenerative%20geometry%20and%20physical%20plausibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12684v3&entry.124074799=Read"},
{"title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion\n  Generation", "author": "Sihan Zhao and Zixuan Wang and Tianyu Luan and Jia Jia and Wentao Zhu and Jiebo Luo and Junsong Yuan and Nan Xi", "abstract": "  Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work.\n", "link": "http://arxiv.org/abs/2508.08179v1", "date": "2025-08-11", "relevancy": 2.4105, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6123}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%0A%20%20Generation&body=Title%3A%20PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%0A%20%20Generation%0AAuthor%3A%20Sihan%20Zhao%20and%20Zixuan%20Wang%20and%20Tianyu%20Luan%20and%20Jia%20Jia%20and%20Wentao%20Zhu%20and%20Jiebo%20Luo%20and%20Junsong%20Yuan%20and%20Nan%20Xi%0AAbstract%3A%20%20%20Human%20motion%20generation%20has%20found%20widespread%20applications%20in%20AR/VR%2C%20film%2C%0Asports%2C%20and%20medical%20rehabilitation%2C%20offering%20a%20cost-effective%20alternative%20to%0Atraditional%20motion%20capture%20systems.%20However%2C%20evaluating%20the%20fidelity%20of%20such%0Agenerated%20motions%20is%20a%20crucial%2C%20multifaceted%20task.%20Although%20previous%20approaches%0Ahave%20attempted%20at%20motion%20fidelity%20evaluation%20using%20human%20perception%20or%20physical%0Aconstraints%2C%20there%20remains%20an%20inherent%20gap%20between%20human-perceived%20fidelity%20and%0Aphysical%20feasibility.%20Moreover%2C%20the%20subjective%20and%20coarse%20binary%20labeling%20of%0Ahuman%20perception%20further%20undermines%20the%20development%20of%20a%20robust%20data-driven%0Ametric.%20We%20address%20these%20issues%20by%20introducing%20a%20physical%20labeling%20method.%20This%0Amethod%20evaluates%20motion%20fidelity%20by%20calculating%20the%20minimum%20modifications%0Aneeded%20for%20a%20motion%20to%20align%20with%20physical%20laws.%20With%20this%20approach%2C%20we%20are%0Aable%20to%20produce%20fine-grained%2C%20continuous%20physical%20alignment%20annotations%20that%0Aserve%20as%20objective%20ground%20truth.%20With%20these%20annotations%2C%20we%20propose%20PP-Motion%2C%0Aa%20novel%20data-driven%20metric%20to%20evaluate%20both%20physical%20and%20perceptual%20fidelity%20of%0Ahuman%20motion.%20To%20effectively%20capture%20underlying%20physical%20priors%2C%20we%20employ%0APearson%27s%20correlation%20loss%20for%20the%20training%20of%20our%20metric.%20Additionally%2C%20by%0Aincorporating%20a%20human-based%20perceptual%20fidelity%20loss%2C%20our%20metric%20can%20capture%0Afidelity%20that%20simultaneously%20considers%20both%20human%20perception%20and%20physical%0Aalignment.%20Experimental%20results%20demonstrate%20that%20our%20metric%2C%20PP-Motion%2C%20not%0Aonly%20aligns%20with%20physical%20laws%20but%20also%20aligns%20better%20with%20human%20perception%20of%0Amotion%20fidelity%20than%20previous%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPP-Motion%253A%2520Physical-Perceptual%2520Fidelity%2520Evaluation%2520for%2520Human%2520Motion%250A%2520%2520Generation%26entry.906535625%3DSihan%2520Zhao%2520and%2520Zixuan%2520Wang%2520and%2520Tianyu%2520Luan%2520and%2520Jia%2520Jia%2520and%2520Wentao%2520Zhu%2520and%2520Jiebo%2520Luo%2520and%2520Junsong%2520Yuan%2520and%2520Nan%2520Xi%26entry.1292438233%3D%2520%2520Human%2520motion%2520generation%2520has%2520found%2520widespread%2520applications%2520in%2520AR/VR%252C%2520film%252C%250Asports%252C%2520and%2520medical%2520rehabilitation%252C%2520offering%2520a%2520cost-effective%2520alternative%2520to%250Atraditional%2520motion%2520capture%2520systems.%2520However%252C%2520evaluating%2520the%2520fidelity%2520of%2520such%250Agenerated%2520motions%2520is%2520a%2520crucial%252C%2520multifaceted%2520task.%2520Although%2520previous%2520approaches%250Ahave%2520attempted%2520at%2520motion%2520fidelity%2520evaluation%2520using%2520human%2520perception%2520or%2520physical%250Aconstraints%252C%2520there%2520remains%2520an%2520inherent%2520gap%2520between%2520human-perceived%2520fidelity%2520and%250Aphysical%2520feasibility.%2520Moreover%252C%2520the%2520subjective%2520and%2520coarse%2520binary%2520labeling%2520of%250Ahuman%2520perception%2520further%2520undermines%2520the%2520development%2520of%2520a%2520robust%2520data-driven%250Ametric.%2520We%2520address%2520these%2520issues%2520by%2520introducing%2520a%2520physical%2520labeling%2520method.%2520This%250Amethod%2520evaluates%2520motion%2520fidelity%2520by%2520calculating%2520the%2520minimum%2520modifications%250Aneeded%2520for%2520a%2520motion%2520to%2520align%2520with%2520physical%2520laws.%2520With%2520this%2520approach%252C%2520we%2520are%250Aable%2520to%2520produce%2520fine-grained%252C%2520continuous%2520physical%2520alignment%2520annotations%2520that%250Aserve%2520as%2520objective%2520ground%2520truth.%2520With%2520these%2520annotations%252C%2520we%2520propose%2520PP-Motion%252C%250Aa%2520novel%2520data-driven%2520metric%2520to%2520evaluate%2520both%2520physical%2520and%2520perceptual%2520fidelity%2520of%250Ahuman%2520motion.%2520To%2520effectively%2520capture%2520underlying%2520physical%2520priors%252C%2520we%2520employ%250APearson%2527s%2520correlation%2520loss%2520for%2520the%2520training%2520of%2520our%2520metric.%2520Additionally%252C%2520by%250Aincorporating%2520a%2520human-based%2520perceptual%2520fidelity%2520loss%252C%2520our%2520metric%2520can%2520capture%250Afidelity%2520that%2520simultaneously%2520considers%2520both%2520human%2520perception%2520and%2520physical%250Aalignment.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520metric%252C%2520PP-Motion%252C%2520not%250Aonly%2520aligns%2520with%2520physical%2520laws%2520but%2520also%2520aligns%2520better%2520with%2520human%2520perception%2520of%250Amotion%2520fidelity%2520than%2520previous%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PP-Motion%3A%20Physical-Perceptual%20Fidelity%20Evaluation%20for%20Human%20Motion%0A%20%20Generation&entry.906535625=Sihan%20Zhao%20and%20Zixuan%20Wang%20and%20Tianyu%20Luan%20and%20Jia%20Jia%20and%20Wentao%20Zhu%20and%20Jiebo%20Luo%20and%20Junsong%20Yuan%20and%20Nan%20Xi&entry.1292438233=%20%20Human%20motion%20generation%20has%20found%20widespread%20applications%20in%20AR/VR%2C%20film%2C%0Asports%2C%20and%20medical%20rehabilitation%2C%20offering%20a%20cost-effective%20alternative%20to%0Atraditional%20motion%20capture%20systems.%20However%2C%20evaluating%20the%20fidelity%20of%20such%0Agenerated%20motions%20is%20a%20crucial%2C%20multifaceted%20task.%20Although%20previous%20approaches%0Ahave%20attempted%20at%20motion%20fidelity%20evaluation%20using%20human%20perception%20or%20physical%0Aconstraints%2C%20there%20remains%20an%20inherent%20gap%20between%20human-perceived%20fidelity%20and%0Aphysical%20feasibility.%20Moreover%2C%20the%20subjective%20and%20coarse%20binary%20labeling%20of%0Ahuman%20perception%20further%20undermines%20the%20development%20of%20a%20robust%20data-driven%0Ametric.%20We%20address%20these%20issues%20by%20introducing%20a%20physical%20labeling%20method.%20This%0Amethod%20evaluates%20motion%20fidelity%20by%20calculating%20the%20minimum%20modifications%0Aneeded%20for%20a%20motion%20to%20align%20with%20physical%20laws.%20With%20this%20approach%2C%20we%20are%0Aable%20to%20produce%20fine-grained%2C%20continuous%20physical%20alignment%20annotations%20that%0Aserve%20as%20objective%20ground%20truth.%20With%20these%20annotations%2C%20we%20propose%20PP-Motion%2C%0Aa%20novel%20data-driven%20metric%20to%20evaluate%20both%20physical%20and%20perceptual%20fidelity%20of%0Ahuman%20motion.%20To%20effectively%20capture%20underlying%20physical%20priors%2C%20we%20employ%0APearson%27s%20correlation%20loss%20for%20the%20training%20of%20our%20metric.%20Additionally%2C%20by%0Aincorporating%20a%20human-based%20perceptual%20fidelity%20loss%2C%20our%20metric%20can%20capture%0Afidelity%20that%20simultaneously%20considers%20both%20human%20perception%20and%20physical%0Aalignment.%20Experimental%20results%20demonstrate%20that%20our%20metric%2C%20PP-Motion%2C%20not%0Aonly%20aligns%20with%20physical%20laws%20but%20also%20aligns%20better%20with%20human%20perception%20of%0Amotion%20fidelity%20than%20previous%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08179v1&entry.124074799=Read"},
{"title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "author": "Shuyuan Tu and Yueming Pan and Yinming Huang and Xintong Han and Zhen Xing and Qi Dai and Chong Luo and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2508.08248v1", "date": "2025-08-11", "relevancy": 2.4067, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6176}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6139}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableAvatar%3A%20Infinite-Length%20Audio-Driven%20Avatar%20Video%20Generation&body=Title%3A%20StableAvatar%3A%20Infinite-Length%20Audio-Driven%20Avatar%20Video%20Generation%0AAuthor%3A%20Shuyuan%20Tu%20and%20Yueming%20Pan%20and%20Yinming%20Huang%20and%20Xintong%20Han%20and%20Zhen%20Xing%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Current%20diffusion%20models%20for%20audio-driven%20avatar%20video%20generation%20struggle%20to%0Asynthesize%20long%20videos%20with%20natural%20audio%20synchronization%20and%20identity%0Aconsistency.%20This%20paper%20presents%20StableAvatar%2C%20the%20first%20end-to-end%20video%0Adiffusion%20transformer%20that%20synthesizes%20infinite-length%20high-quality%20videos%0Awithout%20post-processing.%20Conditioned%20on%20a%20reference%20image%20and%20audio%2C%0AStableAvatar%20integrates%20tailored%20training%20and%20inference%20modules%20to%20enable%0Ainfinite-length%20video%20generation.%20We%20observe%20that%20the%20main%20reason%20preventing%0Aexisting%20models%20from%20generating%20long%20videos%20lies%20in%20their%20audio%20modeling.%20They%0Atypically%20rely%20on%20third-party%20off-the-shelf%20extractors%20to%20obtain%20audio%0Aembeddings%2C%20which%20are%20then%20directly%20injected%20into%20the%20diffusion%20model%20via%0Across-attention.%20Since%20current%20diffusion%20backbones%20lack%20any%20audio-related%0Apriors%2C%20this%20approach%20causes%20severe%20latent%20distribution%20error%20accumulation%0Aacross%20video%20clips%2C%20leading%20the%20latent%20distribution%20of%20subsequent%20segments%20to%0Adrift%20away%20from%20the%20optimal%20distribution%20gradually.%20To%20address%20this%2C%0AStableAvatar%20introduces%20a%20novel%20Time-step-aware%20Audio%20Adapter%20that%20prevents%0Aerror%20accumulation%20via%20time-step-aware%20modulation.%20During%20inference%2C%20we%20propose%0Aa%20novel%20Audio%20Native%20Guidance%20Mechanism%20to%20further%20enhance%20the%20audio%0Asynchronization%20by%20leveraging%20the%20diffusion%27s%20own%20evolving%20joint%20audio-latent%0Aprediction%20as%20a%20dynamic%20guidance%20signal.%20To%20enhance%20the%20smoothness%20of%20the%0Ainfinite-length%20videos%2C%20we%20introduce%20a%20Dynamic%20Weighted%20Sliding-window%20Strategy%0Athat%20fuses%20latent%20over%20time.%20Experiments%20on%20benchmarks%20show%20the%20effectiveness%0Aof%20StableAvatar%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableAvatar%253A%2520Infinite-Length%2520Audio-Driven%2520Avatar%2520Video%2520Generation%26entry.906535625%3DShuyuan%2520Tu%2520and%2520Yueming%2520Pan%2520and%2520Yinming%2520Huang%2520and%2520Xintong%2520Han%2520and%2520Zhen%2520Xing%2520and%2520Qi%2520Dai%2520and%2520Chong%2520Luo%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Current%2520diffusion%2520models%2520for%2520audio-driven%2520avatar%2520video%2520generation%2520struggle%2520to%250Asynthesize%2520long%2520videos%2520with%2520natural%2520audio%2520synchronization%2520and%2520identity%250Aconsistency.%2520This%2520paper%2520presents%2520StableAvatar%252C%2520the%2520first%2520end-to-end%2520video%250Adiffusion%2520transformer%2520that%2520synthesizes%2520infinite-length%2520high-quality%2520videos%250Awithout%2520post-processing.%2520Conditioned%2520on%2520a%2520reference%2520image%2520and%2520audio%252C%250AStableAvatar%2520integrates%2520tailored%2520training%2520and%2520inference%2520modules%2520to%2520enable%250Ainfinite-length%2520video%2520generation.%2520We%2520observe%2520that%2520the%2520main%2520reason%2520preventing%250Aexisting%2520models%2520from%2520generating%2520long%2520videos%2520lies%2520in%2520their%2520audio%2520modeling.%2520They%250Atypically%2520rely%2520on%2520third-party%2520off-the-shelf%2520extractors%2520to%2520obtain%2520audio%250Aembeddings%252C%2520which%2520are%2520then%2520directly%2520injected%2520into%2520the%2520diffusion%2520model%2520via%250Across-attention.%2520Since%2520current%2520diffusion%2520backbones%2520lack%2520any%2520audio-related%250Apriors%252C%2520this%2520approach%2520causes%2520severe%2520latent%2520distribution%2520error%2520accumulation%250Aacross%2520video%2520clips%252C%2520leading%2520the%2520latent%2520distribution%2520of%2520subsequent%2520segments%2520to%250Adrift%2520away%2520from%2520the%2520optimal%2520distribution%2520gradually.%2520To%2520address%2520this%252C%250AStableAvatar%2520introduces%2520a%2520novel%2520Time-step-aware%2520Audio%2520Adapter%2520that%2520prevents%250Aerror%2520accumulation%2520via%2520time-step-aware%2520modulation.%2520During%2520inference%252C%2520we%2520propose%250Aa%2520novel%2520Audio%2520Native%2520Guidance%2520Mechanism%2520to%2520further%2520enhance%2520the%2520audio%250Asynchronization%2520by%2520leveraging%2520the%2520diffusion%2527s%2520own%2520evolving%2520joint%2520audio-latent%250Aprediction%2520as%2520a%2520dynamic%2520guidance%2520signal.%2520To%2520enhance%2520the%2520smoothness%2520of%2520the%250Ainfinite-length%2520videos%252C%2520we%2520introduce%2520a%2520Dynamic%2520Weighted%2520Sliding-window%2520Strategy%250Athat%2520fuses%2520latent%2520over%2520time.%2520Experiments%2520on%2520benchmarks%2520show%2520the%2520effectiveness%250Aof%2520StableAvatar%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableAvatar%3A%20Infinite-Length%20Audio-Driven%20Avatar%20Video%20Generation&entry.906535625=Shuyuan%20Tu%20and%20Yueming%20Pan%20and%20Yinming%20Huang%20and%20Xintong%20Han%20and%20Zhen%20Xing%20and%20Qi%20Dai%20and%20Chong%20Luo%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Current%20diffusion%20models%20for%20audio-driven%20avatar%20video%20generation%20struggle%20to%0Asynthesize%20long%20videos%20with%20natural%20audio%20synchronization%20and%20identity%0Aconsistency.%20This%20paper%20presents%20StableAvatar%2C%20the%20first%20end-to-end%20video%0Adiffusion%20transformer%20that%20synthesizes%20infinite-length%20high-quality%20videos%0Awithout%20post-processing.%20Conditioned%20on%20a%20reference%20image%20and%20audio%2C%0AStableAvatar%20integrates%20tailored%20training%20and%20inference%20modules%20to%20enable%0Ainfinite-length%20video%20generation.%20We%20observe%20that%20the%20main%20reason%20preventing%0Aexisting%20models%20from%20generating%20long%20videos%20lies%20in%20their%20audio%20modeling.%20They%0Atypically%20rely%20on%20third-party%20off-the-shelf%20extractors%20to%20obtain%20audio%0Aembeddings%2C%20which%20are%20then%20directly%20injected%20into%20the%20diffusion%20model%20via%0Across-attention.%20Since%20current%20diffusion%20backbones%20lack%20any%20audio-related%0Apriors%2C%20this%20approach%20causes%20severe%20latent%20distribution%20error%20accumulation%0Aacross%20video%20clips%2C%20leading%20the%20latent%20distribution%20of%20subsequent%20segments%20to%0Adrift%20away%20from%20the%20optimal%20distribution%20gradually.%20To%20address%20this%2C%0AStableAvatar%20introduces%20a%20novel%20Time-step-aware%20Audio%20Adapter%20that%20prevents%0Aerror%20accumulation%20via%20time-step-aware%20modulation.%20During%20inference%2C%20we%20propose%0Aa%20novel%20Audio%20Native%20Guidance%20Mechanism%20to%20further%20enhance%20the%20audio%0Asynchronization%20by%20leveraging%20the%20diffusion%27s%20own%20evolving%20joint%20audio-latent%0Aprediction%20as%20a%20dynamic%20guidance%20signal.%20To%20enhance%20the%20smoothness%20of%20the%0Ainfinite-length%20videos%2C%20we%20introduce%20a%20Dynamic%20Weighted%20Sliding-window%20Strategy%0Athat%20fuses%20latent%20over%20time.%20Experiments%20on%20benchmarks%20show%20the%20effectiveness%0Aof%20StableAvatar%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08248v1&entry.124074799=Read"},
{"title": "Vision-Based Localization and LLM-based Navigation for Indoor\n  Environments", "author": "Keyan Rahimi and Md. Wasiul Haque and Sagar Dasgupta and Mizanur Rahman", "abstract": "  Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.\n", "link": "http://arxiv.org/abs/2508.08120v1", "date": "2025-08-11", "relevancy": 2.4007, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6228}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Based%20Localization%20and%20LLM-based%20Navigation%20for%20Indoor%0A%20%20Environments&body=Title%3A%20Vision-Based%20Localization%20and%20LLM-based%20Navigation%20for%20Indoor%0A%20%20Environments%0AAuthor%3A%20Keyan%20Rahimi%20and%20Md.%20Wasiul%20Haque%20and%20Sagar%20Dasgupta%20and%20Mizanur%20Rahman%0AAbstract%3A%20%20%20Indoor%20navigation%20remains%20a%20complex%20challenge%20due%20to%20the%20absence%20of%20reliable%0AGPS%20signals%20and%20the%20architectural%20intricacies%20of%20large%20enclosed%20environments.%0AThis%20study%20presents%20an%20indoor%20localization%20and%20navigation%20approach%20that%0Aintegrates%20vision-based%20localization%20with%20large%20language%20model%20%28LLM%29-based%0Anavigation.%20The%20localization%20system%20utilizes%20a%20ResNet-50%20convolutional%20neural%0Anetwork%20fine-tuned%20through%20a%20two-stage%20process%20to%20identify%20the%20user%27s%20position%0Ausing%20smartphone%20camera%20input.%20To%20complement%20localization%2C%20the%20navigation%0Amodule%20employs%20an%20LLM%2C%20guided%20by%20a%20carefully%20crafted%20system%20prompt%2C%20to%0Ainterpret%20preprocessed%20floor%20plan%20images%20and%20generate%20step-by-step%20directions.%0AExperimental%20evaluation%20was%20conducted%20in%20a%20realistic%20office%20corridor%20with%0Arepetitive%20features%20and%20limited%20visibility%20to%20test%20localization%20robustness.%20The%0Amodel%20achieved%20high%20confidence%20and%20an%20accuracy%20of%2096%25%20across%20all%20tested%0Awaypoints%2C%20even%20under%20constrained%20viewing%20conditions%20and%20short-duration%0Aqueries.%20Navigation%20tests%20using%20ChatGPT%20on%20real%20building%20floor%20maps%20yielded%20an%0Aaverage%20instruction%20accuracy%20of%2075%25%2C%20with%20observed%20limitations%20in%20zero-shot%0Areasoning%20and%20inference%20time.%20This%20research%20demonstrates%20the%20potential%20for%0Ascalable%2C%20infrastructure-free%20indoor%20navigation%20using%20off-the-shelf%20cameras%20and%0Apublicly%20available%20floor%20plans%2C%20particularly%20in%20resource-constrained%20settings%0Alike%20hospitals%2C%20airports%2C%20and%20educational%20institutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08120v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Based%2520Localization%2520and%2520LLM-based%2520Navigation%2520for%2520Indoor%250A%2520%2520Environments%26entry.906535625%3DKeyan%2520Rahimi%2520and%2520Md.%2520Wasiul%2520Haque%2520and%2520Sagar%2520Dasgupta%2520and%2520Mizanur%2520Rahman%26entry.1292438233%3D%2520%2520Indoor%2520navigation%2520remains%2520a%2520complex%2520challenge%2520due%2520to%2520the%2520absence%2520of%2520reliable%250AGPS%2520signals%2520and%2520the%2520architectural%2520intricacies%2520of%2520large%2520enclosed%2520environments.%250AThis%2520study%2520presents%2520an%2520indoor%2520localization%2520and%2520navigation%2520approach%2520that%250Aintegrates%2520vision-based%2520localization%2520with%2520large%2520language%2520model%2520%2528LLM%2529-based%250Anavigation.%2520The%2520localization%2520system%2520utilizes%2520a%2520ResNet-50%2520convolutional%2520neural%250Anetwork%2520fine-tuned%2520through%2520a%2520two-stage%2520process%2520to%2520identify%2520the%2520user%2527s%2520position%250Ausing%2520smartphone%2520camera%2520input.%2520To%2520complement%2520localization%252C%2520the%2520navigation%250Amodule%2520employs%2520an%2520LLM%252C%2520guided%2520by%2520a%2520carefully%2520crafted%2520system%2520prompt%252C%2520to%250Ainterpret%2520preprocessed%2520floor%2520plan%2520images%2520and%2520generate%2520step-by-step%2520directions.%250AExperimental%2520evaluation%2520was%2520conducted%2520in%2520a%2520realistic%2520office%2520corridor%2520with%250Arepetitive%2520features%2520and%2520limited%2520visibility%2520to%2520test%2520localization%2520robustness.%2520The%250Amodel%2520achieved%2520high%2520confidence%2520and%2520an%2520accuracy%2520of%252096%2525%2520across%2520all%2520tested%250Awaypoints%252C%2520even%2520under%2520constrained%2520viewing%2520conditions%2520and%2520short-duration%250Aqueries.%2520Navigation%2520tests%2520using%2520ChatGPT%2520on%2520real%2520building%2520floor%2520maps%2520yielded%2520an%250Aaverage%2520instruction%2520accuracy%2520of%252075%2525%252C%2520with%2520observed%2520limitations%2520in%2520zero-shot%250Areasoning%2520and%2520inference%2520time.%2520This%2520research%2520demonstrates%2520the%2520potential%2520for%250Ascalable%252C%2520infrastructure-free%2520indoor%2520navigation%2520using%2520off-the-shelf%2520cameras%2520and%250Apublicly%2520available%2520floor%2520plans%252C%2520particularly%2520in%2520resource-constrained%2520settings%250Alike%2520hospitals%252C%2520airports%252C%2520and%2520educational%2520institutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08120v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Based%20Localization%20and%20LLM-based%20Navigation%20for%20Indoor%0A%20%20Environments&entry.906535625=Keyan%20Rahimi%20and%20Md.%20Wasiul%20Haque%20and%20Sagar%20Dasgupta%20and%20Mizanur%20Rahman&entry.1292438233=%20%20Indoor%20navigation%20remains%20a%20complex%20challenge%20due%20to%20the%20absence%20of%20reliable%0AGPS%20signals%20and%20the%20architectural%20intricacies%20of%20large%20enclosed%20environments.%0AThis%20study%20presents%20an%20indoor%20localization%20and%20navigation%20approach%20that%0Aintegrates%20vision-based%20localization%20with%20large%20language%20model%20%28LLM%29-based%0Anavigation.%20The%20localization%20system%20utilizes%20a%20ResNet-50%20convolutional%20neural%0Anetwork%20fine-tuned%20through%20a%20two-stage%20process%20to%20identify%20the%20user%27s%20position%0Ausing%20smartphone%20camera%20input.%20To%20complement%20localization%2C%20the%20navigation%0Amodule%20employs%20an%20LLM%2C%20guided%20by%20a%20carefully%20crafted%20system%20prompt%2C%20to%0Ainterpret%20preprocessed%20floor%20plan%20images%20and%20generate%20step-by-step%20directions.%0AExperimental%20evaluation%20was%20conducted%20in%20a%20realistic%20office%20corridor%20with%0Arepetitive%20features%20and%20limited%20visibility%20to%20test%20localization%20robustness.%20The%0Amodel%20achieved%20high%20confidence%20and%20an%20accuracy%20of%2096%25%20across%20all%20tested%0Awaypoints%2C%20even%20under%20constrained%20viewing%20conditions%20and%20short-duration%0Aqueries.%20Navigation%20tests%20using%20ChatGPT%20on%20real%20building%20floor%20maps%20yielded%20an%0Aaverage%20instruction%20accuracy%20of%2075%25%2C%20with%20observed%20limitations%20in%20zero-shot%0Areasoning%20and%20inference%20time.%20This%20research%20demonstrates%20the%20potential%20for%0Ascalable%2C%20infrastructure-free%20indoor%20navigation%20using%20off-the-shelf%20cameras%20and%0Apublicly%20available%20floor%20plans%2C%20particularly%20in%20resource-constrained%20settings%0Alike%20hospitals%2C%20airports%2C%20and%20educational%20institutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08120v1&entry.124074799=Read"},
{"title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models", "author": "Johanna P. M\u00fcller and Anika Knupfer and Pedro Bl\u00f6ss and Edoardo Berardi Vittur and Bernhard Kainz and Jana Hutter", "abstract": "  Despite significant progress in generative modelling, existing diffusion\nmodels often struggle to produce anatomically precise female pelvic images,\nlimiting their application in gynaecological imaging, where data scarcity and\npatient privacy concerns are critical. To overcome these barriers, we introduce\na novel diffusion-based framework for uterine MRI synthesis, integrating both\nunconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)\nand Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates\nanatomically coherent, high fidelity synthetic images that closely mimic real\nscans and provide valuable resources for training robust diagnostic models. We\nevaluate generative quality using advanced perceptual and distributional\nmetrics, benchmarking against standard reconstruction methods, and demonstrate\nsubstantial gains in diagnostic accuracy on a key classification task. A\nblinded expert evaluation further validates the clinical realism of our\nsynthetic images. We release our models with privacy safeguards and a\ncomprehensive synthetic uterine MRI dataset to support reproducible research\nand advance equitable AI in gynaecology.\n", "link": "http://arxiv.org/abs/2508.07903v1", "date": "2025-08-11", "relevancy": 2.3956, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6182}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusing%20the%20Blind%20Spot%3A%20Uterine%20MRI%20Synthesis%20with%20Diffusion%20Models&body=Title%3A%20Diffusing%20the%20Blind%20Spot%3A%20Uterine%20MRI%20Synthesis%20with%20Diffusion%20Models%0AAuthor%3A%20Johanna%20P.%20M%C3%BCller%20and%20Anika%20Knupfer%20and%20Pedro%20Bl%C3%B6ss%20and%20Edoardo%20Berardi%20Vittur%20and%20Bernhard%20Kainz%20and%20Jana%20Hutter%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20generative%20modelling%2C%20existing%20diffusion%0Amodels%20often%20struggle%20to%20produce%20anatomically%20precise%20female%20pelvic%20images%2C%0Alimiting%20their%20application%20in%20gynaecological%20imaging%2C%20where%20data%20scarcity%20and%0Apatient%20privacy%20concerns%20are%20critical.%20To%20overcome%20these%20barriers%2C%20we%20introduce%0Aa%20novel%20diffusion-based%20framework%20for%20uterine%20MRI%20synthesis%2C%20integrating%20both%0Aunconditional%20and%20conditioned%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%0Aand%20Latent%20Diffusion%20Models%20%28LDMs%29%20in%202D%20and%203D.%20Our%20approach%20generates%0Aanatomically%20coherent%2C%20high%20fidelity%20synthetic%20images%20that%20closely%20mimic%20real%0Ascans%20and%20provide%20valuable%20resources%20for%20training%20robust%20diagnostic%20models.%20We%0Aevaluate%20generative%20quality%20using%20advanced%20perceptual%20and%20distributional%0Ametrics%2C%20benchmarking%20against%20standard%20reconstruction%20methods%2C%20and%20demonstrate%0Asubstantial%20gains%20in%20diagnostic%20accuracy%20on%20a%20key%20classification%20task.%20A%0Ablinded%20expert%20evaluation%20further%20validates%20the%20clinical%20realism%20of%20our%0Asynthetic%20images.%20We%20release%20our%20models%20with%20privacy%20safeguards%20and%20a%0Acomprehensive%20synthetic%20uterine%20MRI%20dataset%20to%20support%20reproducible%20research%0Aand%20advance%20equitable%20AI%20in%20gynaecology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusing%2520the%2520Blind%2520Spot%253A%2520Uterine%2520MRI%2520Synthesis%2520with%2520Diffusion%2520Models%26entry.906535625%3DJohanna%2520P.%2520M%25C3%25BCller%2520and%2520Anika%2520Knupfer%2520and%2520Pedro%2520Bl%25C3%25B6ss%2520and%2520Edoardo%2520Berardi%2520Vittur%2520and%2520Bernhard%2520Kainz%2520and%2520Jana%2520Hutter%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520generative%2520modelling%252C%2520existing%2520diffusion%250Amodels%2520often%2520struggle%2520to%2520produce%2520anatomically%2520precise%2520female%2520pelvic%2520images%252C%250Alimiting%2520their%2520application%2520in%2520gynaecological%2520imaging%252C%2520where%2520data%2520scarcity%2520and%250Apatient%2520privacy%2520concerns%2520are%2520critical.%2520To%2520overcome%2520these%2520barriers%252C%2520we%2520introduce%250Aa%2520novel%2520diffusion-based%2520framework%2520for%2520uterine%2520MRI%2520synthesis%252C%2520integrating%2520both%250Aunconditional%2520and%2520conditioned%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%250Aand%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520in%25202D%2520and%25203D.%2520Our%2520approach%2520generates%250Aanatomically%2520coherent%252C%2520high%2520fidelity%2520synthetic%2520images%2520that%2520closely%2520mimic%2520real%250Ascans%2520and%2520provide%2520valuable%2520resources%2520for%2520training%2520robust%2520diagnostic%2520models.%2520We%250Aevaluate%2520generative%2520quality%2520using%2520advanced%2520perceptual%2520and%2520distributional%250Ametrics%252C%2520benchmarking%2520against%2520standard%2520reconstruction%2520methods%252C%2520and%2520demonstrate%250Asubstantial%2520gains%2520in%2520diagnostic%2520accuracy%2520on%2520a%2520key%2520classification%2520task.%2520A%250Ablinded%2520expert%2520evaluation%2520further%2520validates%2520the%2520clinical%2520realism%2520of%2520our%250Asynthetic%2520images.%2520We%2520release%2520our%2520models%2520with%2520privacy%2520safeguards%2520and%2520a%250Acomprehensive%2520synthetic%2520uterine%2520MRI%2520dataset%2520to%2520support%2520reproducible%2520research%250Aand%2520advance%2520equitable%2520AI%2520in%2520gynaecology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusing%20the%20Blind%20Spot%3A%20Uterine%20MRI%20Synthesis%20with%20Diffusion%20Models&entry.906535625=Johanna%20P.%20M%C3%BCller%20and%20Anika%20Knupfer%20and%20Pedro%20Bl%C3%B6ss%20and%20Edoardo%20Berardi%20Vittur%20and%20Bernhard%20Kainz%20and%20Jana%20Hutter&entry.1292438233=%20%20Despite%20significant%20progress%20in%20generative%20modelling%2C%20existing%20diffusion%0Amodels%20often%20struggle%20to%20produce%20anatomically%20precise%20female%20pelvic%20images%2C%0Alimiting%20their%20application%20in%20gynaecological%20imaging%2C%20where%20data%20scarcity%20and%0Apatient%20privacy%20concerns%20are%20critical.%20To%20overcome%20these%20barriers%2C%20we%20introduce%0Aa%20novel%20diffusion-based%20framework%20for%20uterine%20MRI%20synthesis%2C%20integrating%20both%0Aunconditional%20and%20conditioned%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%0Aand%20Latent%20Diffusion%20Models%20%28LDMs%29%20in%202D%20and%203D.%20Our%20approach%20generates%0Aanatomically%20coherent%2C%20high%20fidelity%20synthetic%20images%20that%20closely%20mimic%20real%0Ascans%20and%20provide%20valuable%20resources%20for%20training%20robust%20diagnostic%20models.%20We%0Aevaluate%20generative%20quality%20using%20advanced%20perceptual%20and%20distributional%0Ametrics%2C%20benchmarking%20against%20standard%20reconstruction%20methods%2C%20and%20demonstrate%0Asubstantial%20gains%20in%20diagnostic%20accuracy%20on%20a%20key%20classification%20task.%20A%0Ablinded%20expert%20evaluation%20further%20validates%20the%20clinical%20realism%20of%20our%0Asynthetic%20images.%20We%20release%20our%20models%20with%20privacy%20safeguards%20and%20a%0Acomprehensive%20synthetic%20uterine%20MRI%20dataset%20to%20support%20reproducible%20research%0Aand%20advance%20equitable%20AI%20in%20gynaecology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07903v1&entry.124074799=Read"},
{"title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video\n  Temporal Grounding", "author": "Jin-Seop Lee and SungJoon Lee and Jaehan Ahn and YunSeok Choi and Jee-Hyong Lee", "abstract": "  Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG\n", "link": "http://arxiv.org/abs/2508.07925v1", "date": "2025-08-11", "relevancy": 2.362, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.603}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5984}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAG%3A%20A%20Simple%20Yet%20Effective%20Temporal-Aware%20Approach%20for%20Zero-Shot%20Video%0A%20%20Temporal%20Grounding&body=Title%3A%20TAG%3A%20A%20Simple%20Yet%20Effective%20Temporal-Aware%20Approach%20for%20Zero-Shot%20Video%0A%20%20Temporal%20Grounding%0AAuthor%3A%20Jin-Seop%20Lee%20and%20SungJoon%20Lee%20and%20Jaehan%20Ahn%20and%20YunSeok%20Choi%20and%20Jee-Hyong%20Lee%0AAbstract%3A%20%20%20Video%20Temporal%20Grounding%20%28VTG%29%20aims%20to%20extract%20relevant%20video%20segments%20based%0Aon%20a%20given%20natural%20language%20query.%20Recently%2C%20zero-shot%20VTG%20methods%20have%20gained%0Aattention%20by%20leveraging%20pretrained%20vision-language%20models%20%28VLMs%29%20to%20localize%0Atarget%20moments%20without%20additional%20training.%20However%2C%20existing%20approaches%20suffer%0Afrom%20semantic%20fragmentation%2C%20where%20temporally%20continuous%20frames%20sharing%20the%0Asame%20semantics%20are%20split%20across%20multiple%20segments.%20When%20segments%20are%0Afragmented%2C%20it%20becomes%20difficult%20to%20predict%20an%20accurate%20target%20moment%20that%0Aaligns%20with%20the%20text%20query.%20Also%2C%20they%20rely%20on%20skewed%20similarity%20distributions%0Afor%20localization%2C%20making%20it%20difficult%20to%20select%20the%20optimal%20segment.%0AFurthermore%2C%20they%20heavily%20depend%20on%20the%20use%20of%20LLMs%20which%20require%20expensive%0Ainferences.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20%5Ctextit%7BTAG%7D%2C%20a%20simple%0Ayet%20effective%20Temporal-Aware%20approach%20for%20zero-shot%20video%20temporal%20Grounding%2C%0Awhich%20incorporates%20temporal%20pooling%2C%20temporal%20coherence%20clustering%2C%20and%0Asimilarity%20adjustment.%20Our%20proposed%20method%20effectively%20captures%20the%20temporal%0Acontext%20of%20videos%20and%20addresses%20distorted%20similarity%20distributions%20without%0Atraining.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20Charades-STA%20and%0AActivityNet%20Captions%20benchmark%20datasets%20without%20rely%20on%20LLMs.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Nuetee/TAG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAG%253A%2520A%2520Simple%2520Yet%2520Effective%2520Temporal-Aware%2520Approach%2520for%2520Zero-Shot%2520Video%250A%2520%2520Temporal%2520Grounding%26entry.906535625%3DJin-Seop%2520Lee%2520and%2520SungJoon%2520Lee%2520and%2520Jaehan%2520Ahn%2520and%2520YunSeok%2520Choi%2520and%2520Jee-Hyong%2520Lee%26entry.1292438233%3D%2520%2520Video%2520Temporal%2520Grounding%2520%2528VTG%2529%2520aims%2520to%2520extract%2520relevant%2520video%2520segments%2520based%250Aon%2520a%2520given%2520natural%2520language%2520query.%2520Recently%252C%2520zero-shot%2520VTG%2520methods%2520have%2520gained%250Aattention%2520by%2520leveraging%2520pretrained%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520localize%250Atarget%2520moments%2520without%2520additional%2520training.%2520However%252C%2520existing%2520approaches%2520suffer%250Afrom%2520semantic%2520fragmentation%252C%2520where%2520temporally%2520continuous%2520frames%2520sharing%2520the%250Asame%2520semantics%2520are%2520split%2520across%2520multiple%2520segments.%2520When%2520segments%2520are%250Afragmented%252C%2520it%2520becomes%2520difficult%2520to%2520predict%2520an%2520accurate%2520target%2520moment%2520that%250Aaligns%2520with%2520the%2520text%2520query.%2520Also%252C%2520they%2520rely%2520on%2520skewed%2520similarity%2520distributions%250Afor%2520localization%252C%2520making%2520it%2520difficult%2520to%2520select%2520the%2520optimal%2520segment.%250AFurthermore%252C%2520they%2520heavily%2520depend%2520on%2520the%2520use%2520of%2520LLMs%2520which%2520require%2520expensive%250Ainferences.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520%255Ctextit%257BTAG%257D%252C%2520a%2520simple%250Ayet%2520effective%2520Temporal-Aware%2520approach%2520for%2520zero-shot%2520video%2520temporal%2520Grounding%252C%250Awhich%2520incorporates%2520temporal%2520pooling%252C%2520temporal%2520coherence%2520clustering%252C%2520and%250Asimilarity%2520adjustment.%2520Our%2520proposed%2520method%2520effectively%2520captures%2520the%2520temporal%250Acontext%2520of%2520videos%2520and%2520addresses%2520distorted%2520similarity%2520distributions%2520without%250Atraining.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520Charades-STA%2520and%250AActivityNet%2520Captions%2520benchmark%2520datasets%2520without%2520rely%2520on%2520LLMs.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Nuetee/TAG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAG%3A%20A%20Simple%20Yet%20Effective%20Temporal-Aware%20Approach%20for%20Zero-Shot%20Video%0A%20%20Temporal%20Grounding&entry.906535625=Jin-Seop%20Lee%20and%20SungJoon%20Lee%20and%20Jaehan%20Ahn%20and%20YunSeok%20Choi%20and%20Jee-Hyong%20Lee&entry.1292438233=%20%20Video%20Temporal%20Grounding%20%28VTG%29%20aims%20to%20extract%20relevant%20video%20segments%20based%0Aon%20a%20given%20natural%20language%20query.%20Recently%2C%20zero-shot%20VTG%20methods%20have%20gained%0Aattention%20by%20leveraging%20pretrained%20vision-language%20models%20%28VLMs%29%20to%20localize%0Atarget%20moments%20without%20additional%20training.%20However%2C%20existing%20approaches%20suffer%0Afrom%20semantic%20fragmentation%2C%20where%20temporally%20continuous%20frames%20sharing%20the%0Asame%20semantics%20are%20split%20across%20multiple%20segments.%20When%20segments%20are%0Afragmented%2C%20it%20becomes%20difficult%20to%20predict%20an%20accurate%20target%20moment%20that%0Aaligns%20with%20the%20text%20query.%20Also%2C%20they%20rely%20on%20skewed%20similarity%20distributions%0Afor%20localization%2C%20making%20it%20difficult%20to%20select%20the%20optimal%20segment.%0AFurthermore%2C%20they%20heavily%20depend%20on%20the%20use%20of%20LLMs%20which%20require%20expensive%0Ainferences.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20%5Ctextit%7BTAG%7D%2C%20a%20simple%0Ayet%20effective%20Temporal-Aware%20approach%20for%20zero-shot%20video%20temporal%20Grounding%2C%0Awhich%20incorporates%20temporal%20pooling%2C%20temporal%20coherence%20clustering%2C%20and%0Asimilarity%20adjustment.%20Our%20proposed%20method%20effectively%20captures%20the%20temporal%0Acontext%20of%20videos%20and%20addresses%20distorted%20similarity%20distributions%20without%0Atraining.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20Charades-STA%20and%0AActivityNet%20Captions%20benchmark%20datasets%20without%20rely%20on%20LLMs.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Nuetee/TAG%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07925v1&entry.124074799=Read"},
{"title": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA\n  and Style-Consistent Keyframes", "author": "Zhende Song and Chenchen Wang and Jiamu Sheng and Chi Zhang and Shengji Tang and Jiayuan Fan and Tao Chen", "abstract": "  Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks.\n", "link": "http://arxiv.org/abs/2403.01422v5", "date": "2025-08-11", "relevancy": 2.3606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5925}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamFrame%3A%20Enhancing%20Video%20Understanding%20via%20Automatically%20Generated%20QA%0A%20%20and%20Style-Consistent%20Keyframes&body=Title%3A%20DreamFrame%3A%20Enhancing%20Video%20Understanding%20via%20Automatically%20Generated%20QA%0A%20%20and%20Style-Consistent%20Keyframes%0AAuthor%3A%20Zhende%20Song%20and%20Chenchen%20Wang%20and%20Jiamu%20Sheng%20and%20Chi%20Zhang%20and%20Shengji%20Tang%20and%20Jiayuan%20Fan%20and%20Tao%20Chen%0AAbstract%3A%20%20%20Recent%20large%20vision-language%20models%20%28LVLMs%29%20for%20video%20understanding%20are%0Aprimarily%20fine-tuned%20with%20various%20videos%20scraped%20from%20online%20platforms.%0AExisting%20datasets%2C%20such%20as%20ActivityNet%2C%20require%20considerable%20human%20labor%20for%0Astructuring%20and%20annotation%20before%20effectively%20utilized%20for%20tuning%20LVLMs.%20While%0Acurrent%20LVLMs%20are%20primarily%20trained%20on%20existing%20datasets%20in%20broad%2C%0Ageneral-purpose%20settings%2C%20adapting%20them%20to%20specific%20downstream%20scenarios%0Aremains%20challenging%2C%20as%20collecting%20and%20annotating%20task-specific%20videos%20is%0Ahighly%20labor-intensive%20and%20time-consuming.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Athree-stage%20framework%20named%20DreamFrame%20for%20automatically%20generating%0Astyle-consistent%20keyframes%20and%20corresponding%20question-answer%20%28QA%29%20pairs%20to%0Asupport%20LVLM%20instruction%20tuning.%20DreamFrame%20generates%20datasets%20in%20a%20movie-like%0Amanner.%20First%2C%20we%20utilize%20an%20LLM%20to%20generate%20structured%20movie%20plots%20including%0Amovie%20prior%20information%20%28like%20overview%20and%20style%29%2C%20frame%20descriptions%20and%0Aplot-related%20QA%20pairs%2C%20with%20a%20story%20expansion%20strategy%20to%20mitigate%20context%0Alength%20limitations.Then%2C%20to%20ensure%20visual%20consistency%20across%20generated%20frames%2C%0Awe%20design%20a%20Style%20Immobilization%20Process%20which%20maintains%20consistent%20style%0Athrough%20an%20embedding%20learning%20strategy.%20Finally%2C%20frame%20descriptions%20and%20style%0Aembeddings%20are%20integrated%20to%20produce%20coherent%20keyframes.%20Using%20DreamFrame%2C%20we%0Aconstruct%20a%20dataset%20comprising%20approximately%201k%20stylized%20keyframe-like%20videos%0Aand%20100k%20diverse%20QA%20pairs.%20Extensive%20fine-tuned%20experiments%20on%20various%20LVLM%0Aarchitectures%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20dataset.%0AFurthermore%2C%20based%20on%20the%20proposed%20dataset%2C%20we%20fine-tune%20a%20new%20LVLM%20named%0ADreamFrame-7B%2C%20which%20significantly%20surpasses%20the%20previous%20similar-sized%20LVLMs%0Aacross%20different%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01422v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamFrame%253A%2520Enhancing%2520Video%2520Understanding%2520via%2520Automatically%2520Generated%2520QA%250A%2520%2520and%2520Style-Consistent%2520Keyframes%26entry.906535625%3DZhende%2520Song%2520and%2520Chenchen%2520Wang%2520and%2520Jiamu%2520Sheng%2520and%2520Chi%2520Zhang%2520and%2520Shengji%2520Tang%2520and%2520Jiayuan%2520Fan%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520for%2520video%2520understanding%2520are%250Aprimarily%2520fine-tuned%2520with%2520various%2520videos%2520scraped%2520from%2520online%2520platforms.%250AExisting%2520datasets%252C%2520such%2520as%2520ActivityNet%252C%2520require%2520considerable%2520human%2520labor%2520for%250Astructuring%2520and%2520annotation%2520before%2520effectively%2520utilized%2520for%2520tuning%2520LVLMs.%2520While%250Acurrent%2520LVLMs%2520are%2520primarily%2520trained%2520on%2520existing%2520datasets%2520in%2520broad%252C%250Ageneral-purpose%2520settings%252C%2520adapting%2520them%2520to%2520specific%2520downstream%2520scenarios%250Aremains%2520challenging%252C%2520as%2520collecting%2520and%2520annotating%2520task-specific%2520videos%2520is%250Ahighly%2520labor-intensive%2520and%2520time-consuming.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Athree-stage%2520framework%2520named%2520DreamFrame%2520for%2520automatically%2520generating%250Astyle-consistent%2520keyframes%2520and%2520corresponding%2520question-answer%2520%2528QA%2529%2520pairs%2520to%250Asupport%2520LVLM%2520instruction%2520tuning.%2520DreamFrame%2520generates%2520datasets%2520in%2520a%2520movie-like%250Amanner.%2520First%252C%2520we%2520utilize%2520an%2520LLM%2520to%2520generate%2520structured%2520movie%2520plots%2520including%250Amovie%2520prior%2520information%2520%2528like%2520overview%2520and%2520style%2529%252C%2520frame%2520descriptions%2520and%250Aplot-related%2520QA%2520pairs%252C%2520with%2520a%2520story%2520expansion%2520strategy%2520to%2520mitigate%2520context%250Alength%2520limitations.Then%252C%2520to%2520ensure%2520visual%2520consistency%2520across%2520generated%2520frames%252C%250Awe%2520design%2520a%2520Style%2520Immobilization%2520Process%2520which%2520maintains%2520consistent%2520style%250Athrough%2520an%2520embedding%2520learning%2520strategy.%2520Finally%252C%2520frame%2520descriptions%2520and%2520style%250Aembeddings%2520are%2520integrated%2520to%2520produce%2520coherent%2520keyframes.%2520Using%2520DreamFrame%252C%2520we%250Aconstruct%2520a%2520dataset%2520comprising%2520approximately%25201k%2520stylized%2520keyframe-like%2520videos%250Aand%2520100k%2520diverse%2520QA%2520pairs.%2520Extensive%2520fine-tuned%2520experiments%2520on%2520various%2520LVLM%250Aarchitectures%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520dataset.%250AFurthermore%252C%2520based%2520on%2520the%2520proposed%2520dataset%252C%2520we%2520fine-tune%2520a%2520new%2520LVLM%2520named%250ADreamFrame-7B%252C%2520which%2520significantly%2520surpasses%2520the%2520previous%2520similar-sized%2520LVLMs%250Aacross%2520different%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01422v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamFrame%3A%20Enhancing%20Video%20Understanding%20via%20Automatically%20Generated%20QA%0A%20%20and%20Style-Consistent%20Keyframes&entry.906535625=Zhende%20Song%20and%20Chenchen%20Wang%20and%20Jiamu%20Sheng%20and%20Chi%20Zhang%20and%20Shengji%20Tang%20and%20Jiayuan%20Fan%20and%20Tao%20Chen&entry.1292438233=%20%20Recent%20large%20vision-language%20models%20%28LVLMs%29%20for%20video%20understanding%20are%0Aprimarily%20fine-tuned%20with%20various%20videos%20scraped%20from%20online%20platforms.%0AExisting%20datasets%2C%20such%20as%20ActivityNet%2C%20require%20considerable%20human%20labor%20for%0Astructuring%20and%20annotation%20before%20effectively%20utilized%20for%20tuning%20LVLMs.%20While%0Acurrent%20LVLMs%20are%20primarily%20trained%20on%20existing%20datasets%20in%20broad%2C%0Ageneral-purpose%20settings%2C%20adapting%20them%20to%20specific%20downstream%20scenarios%0Aremains%20challenging%2C%20as%20collecting%20and%20annotating%20task-specific%20videos%20is%0Ahighly%20labor-intensive%20and%20time-consuming.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Athree-stage%20framework%20named%20DreamFrame%20for%20automatically%20generating%0Astyle-consistent%20keyframes%20and%20corresponding%20question-answer%20%28QA%29%20pairs%20to%0Asupport%20LVLM%20instruction%20tuning.%20DreamFrame%20generates%20datasets%20in%20a%20movie-like%0Amanner.%20First%2C%20we%20utilize%20an%20LLM%20to%20generate%20structured%20movie%20plots%20including%0Amovie%20prior%20information%20%28like%20overview%20and%20style%29%2C%20frame%20descriptions%20and%0Aplot-related%20QA%20pairs%2C%20with%20a%20story%20expansion%20strategy%20to%20mitigate%20context%0Alength%20limitations.Then%2C%20to%20ensure%20visual%20consistency%20across%20generated%20frames%2C%0Awe%20design%20a%20Style%20Immobilization%20Process%20which%20maintains%20consistent%20style%0Athrough%20an%20embedding%20learning%20strategy.%20Finally%2C%20frame%20descriptions%20and%20style%0Aembeddings%20are%20integrated%20to%20produce%20coherent%20keyframes.%20Using%20DreamFrame%2C%20we%0Aconstruct%20a%20dataset%20comprising%20approximately%201k%20stylized%20keyframe-like%20videos%0Aand%20100k%20diverse%20QA%20pairs.%20Extensive%20fine-tuned%20experiments%20on%20various%20LVLM%0Aarchitectures%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20dataset.%0AFurthermore%2C%20based%20on%20the%20proposed%20dataset%2C%20we%20fine-tune%20a%20new%20LVLM%20named%0ADreamFrame-7B%2C%20which%20significantly%20surpasses%20the%20previous%20similar-sized%20LVLMs%0Aacross%20different%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01422v5&entry.124074799=Read"},
{"title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian\n  Splatting", "author": "Yitong Yang and Yinglin Wang and Changshuo Wang and Huajie Wang and Shuting He", "abstract": "  The success of 3DGS in generative and editing applications has sparked\ngrowing interest in 3DGS-based style transfer. However, current methods still\nface two major challenges: (1) multi-view inconsistency often leads to style\nconflicts, resulting in appearance smoothing and distortion; and (2) heavy\nreliance on VGG features, which struggle to disentangle style and content from\nstyle images, often causing content leakage and excessive stylization. To\ntackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style\ntransfer framework, and the first to rely entirely on diffusion model\ndistillation. It comprises two key components: (1) \\textbf{Multi-View Frequency\nConsistency}. We enhance cross-view consistency by applying a 3D filter to\nmulti-view noisy latent, selectively reducing low-frequency components to\nmitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized\nDistillation}. To suppress content leakage from style images, we introduce\nnegative guidance to exclude undesired content. In addition, we identify the\nlimitations of Score Distillation Sampling and Delta Denoising Score in 3D\nstyle transfer and remove the reconstruction term accordingly. Building on\nthese insights, we propose a controllable stylized distillation that leverages\nnegative guidance to more effectively optimize the 3D Gaussians. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art approaches, achieving higher stylization quality and visual\nrealism across various scenes and styles.\n", "link": "http://arxiv.org/abs/2508.08136v1", "date": "2025-08-11", "relevancy": 2.3441, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6119}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5781}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FantasyStyle%3A%20Controllable%20Stylized%20Distillation%20for%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20FantasyStyle%3A%20Controllable%20Stylized%20Distillation%20for%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Yitong%20Yang%20and%20Yinglin%20Wang%20and%20Changshuo%20Wang%20and%20Huajie%20Wang%20and%20Shuting%20He%0AAbstract%3A%20%20%20The%20success%20of%203DGS%20in%20generative%20and%20editing%20applications%20has%20sparked%0Agrowing%20interest%20in%203DGS-based%20style%20transfer.%20However%2C%20current%20methods%20still%0Aface%20two%20major%20challenges%3A%20%281%29%20multi-view%20inconsistency%20often%20leads%20to%20style%0Aconflicts%2C%20resulting%20in%20appearance%20smoothing%20and%20distortion%3B%20and%20%282%29%20heavy%0Areliance%20on%20VGG%20features%2C%20which%20struggle%20to%20disentangle%20style%20and%20content%20from%0Astyle%20images%2C%20often%20causing%20content%20leakage%20and%20excessive%20stylization.%20To%0Atackle%20these%20issues%2C%20we%20introduce%20%5Ctextbf%7BFantasyStyle%7D%2C%20a%203DGS-based%20style%0Atransfer%20framework%2C%20and%20the%20first%20to%20rely%20entirely%20on%20diffusion%20model%0Adistillation.%20It%20comprises%20two%20key%20components%3A%20%281%29%20%5Ctextbf%7BMulti-View%20Frequency%0AConsistency%7D.%20We%20enhance%20cross-view%20consistency%20by%20applying%20a%203D%20filter%20to%0Amulti-view%20noisy%20latent%2C%20selectively%20reducing%20low-frequency%20components%20to%0Amitigate%20stylized%20prior%20conflicts.%20%282%29%20%5Ctextbf%7BControllable%20Stylized%0ADistillation%7D.%20To%20suppress%20content%20leakage%20from%20style%20images%2C%20we%20introduce%0Anegative%20guidance%20to%20exclude%20undesired%20content.%20In%20addition%2C%20we%20identify%20the%0Alimitations%20of%20Score%20Distillation%20Sampling%20and%20Delta%20Denoising%20Score%20in%203D%0Astyle%20transfer%20and%20remove%20the%20reconstruction%20term%20accordingly.%20Building%20on%0Athese%20insights%2C%20we%20propose%20a%20controllable%20stylized%20distillation%20that%20leverages%0Anegative%20guidance%20to%20more%20effectively%20optimize%20the%203D%20Gaussians.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Astate-of-the-art%20approaches%2C%20achieving%20higher%20stylization%20quality%20and%20visual%0Arealism%20across%20various%20scenes%20and%20styles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFantasyStyle%253A%2520Controllable%2520Stylized%2520Distillation%2520for%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DYitong%2520Yang%2520and%2520Yinglin%2520Wang%2520and%2520Changshuo%2520Wang%2520and%2520Huajie%2520Wang%2520and%2520Shuting%2520He%26entry.1292438233%3D%2520%2520The%2520success%2520of%25203DGS%2520in%2520generative%2520and%2520editing%2520applications%2520has%2520sparked%250Agrowing%2520interest%2520in%25203DGS-based%2520style%2520transfer.%2520However%252C%2520current%2520methods%2520still%250Aface%2520two%2520major%2520challenges%253A%2520%25281%2529%2520multi-view%2520inconsistency%2520often%2520leads%2520to%2520style%250Aconflicts%252C%2520resulting%2520in%2520appearance%2520smoothing%2520and%2520distortion%253B%2520and%2520%25282%2529%2520heavy%250Areliance%2520on%2520VGG%2520features%252C%2520which%2520struggle%2520to%2520disentangle%2520style%2520and%2520content%2520from%250Astyle%2520images%252C%2520often%2520causing%2520content%2520leakage%2520and%2520excessive%2520stylization.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520introduce%2520%255Ctextbf%257BFantasyStyle%257D%252C%2520a%25203DGS-based%2520style%250Atransfer%2520framework%252C%2520and%2520the%2520first%2520to%2520rely%2520entirely%2520on%2520diffusion%2520model%250Adistillation.%2520It%2520comprises%2520two%2520key%2520components%253A%2520%25281%2529%2520%255Ctextbf%257BMulti-View%2520Frequency%250AConsistency%257D.%2520We%2520enhance%2520cross-view%2520consistency%2520by%2520applying%2520a%25203D%2520filter%2520to%250Amulti-view%2520noisy%2520latent%252C%2520selectively%2520reducing%2520low-frequency%2520components%2520to%250Amitigate%2520stylized%2520prior%2520conflicts.%2520%25282%2529%2520%255Ctextbf%257BControllable%2520Stylized%250ADistillation%257D.%2520To%2520suppress%2520content%2520leakage%2520from%2520style%2520images%252C%2520we%2520introduce%250Anegative%2520guidance%2520to%2520exclude%2520undesired%2520content.%2520In%2520addition%252C%2520we%2520identify%2520the%250Alimitations%2520of%2520Score%2520Distillation%2520Sampling%2520and%2520Delta%2520Denoising%2520Score%2520in%25203D%250Astyle%2520transfer%2520and%2520remove%2520the%2520reconstruction%2520term%2520accordingly.%2520Building%2520on%250Athese%2520insights%252C%2520we%2520propose%2520a%2520controllable%2520stylized%2520distillation%2520that%2520leverages%250Anegative%2520guidance%2520to%2520more%2520effectively%2520optimize%2520the%25203D%2520Gaussians.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%250Astate-of-the-art%2520approaches%252C%2520achieving%2520higher%2520stylization%2520quality%2520and%2520visual%250Arealism%2520across%2520various%2520scenes%2520and%2520styles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FantasyStyle%3A%20Controllable%20Stylized%20Distillation%20for%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Yitong%20Yang%20and%20Yinglin%20Wang%20and%20Changshuo%20Wang%20and%20Huajie%20Wang%20and%20Shuting%20He&entry.1292438233=%20%20The%20success%20of%203DGS%20in%20generative%20and%20editing%20applications%20has%20sparked%0Agrowing%20interest%20in%203DGS-based%20style%20transfer.%20However%2C%20current%20methods%20still%0Aface%20two%20major%20challenges%3A%20%281%29%20multi-view%20inconsistency%20often%20leads%20to%20style%0Aconflicts%2C%20resulting%20in%20appearance%20smoothing%20and%20distortion%3B%20and%20%282%29%20heavy%0Areliance%20on%20VGG%20features%2C%20which%20struggle%20to%20disentangle%20style%20and%20content%20from%0Astyle%20images%2C%20often%20causing%20content%20leakage%20and%20excessive%20stylization.%20To%0Atackle%20these%20issues%2C%20we%20introduce%20%5Ctextbf%7BFantasyStyle%7D%2C%20a%203DGS-based%20style%0Atransfer%20framework%2C%20and%20the%20first%20to%20rely%20entirely%20on%20diffusion%20model%0Adistillation.%20It%20comprises%20two%20key%20components%3A%20%281%29%20%5Ctextbf%7BMulti-View%20Frequency%0AConsistency%7D.%20We%20enhance%20cross-view%20consistency%20by%20applying%20a%203D%20filter%20to%0Amulti-view%20noisy%20latent%2C%20selectively%20reducing%20low-frequency%20components%20to%0Amitigate%20stylized%20prior%20conflicts.%20%282%29%20%5Ctextbf%7BControllable%20Stylized%0ADistillation%7D.%20To%20suppress%20content%20leakage%20from%20style%20images%2C%20we%20introduce%0Anegative%20guidance%20to%20exclude%20undesired%20content.%20In%20addition%2C%20we%20identify%20the%0Alimitations%20of%20Score%20Distillation%20Sampling%20and%20Delta%20Denoising%20Score%20in%203D%0Astyle%20transfer%20and%20remove%20the%20reconstruction%20term%20accordingly.%20Building%20on%0Athese%20insights%2C%20we%20propose%20a%20controllable%20stylized%20distillation%20that%20leverages%0Anegative%20guidance%20to%20more%20effectively%20optimize%20the%203D%20Gaussians.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Astate-of-the-art%20approaches%2C%20achieving%20higher%20stylization%20quality%20and%20visual%0Arealism%20across%20various%20scenes%20and%20styles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08136v1&entry.124074799=Read"},
{"title": "Gradient Descent Finds Over-Parameterized Neural Networks with Sharp\n  Generalization for Nonparametric Regression", "author": "Yingzhen Yang and Ping Li", "abstract": "  We study nonparametric regression by an over-parameterized two-layer neural\nnetwork trained by gradient descent (GD) in this paper. We show that, if the\nneural network is trained by GD with early stopping, then the trained network\nrenders a sharp rate of the nonparametric regression risk of\n$\\mathcal{O}(\\epsilon_n^2)$, which is the same rate as that for the classical\nkernel regression trained by GD with early stopping, where $\\epsilon_n$ is the\ncritical population rate of the Neural Tangent Kernel (NTK) associated with the\nnetwork and $n$ is the size of the training data. It is remarked that our\nresult does not require distributional assumptions about the covariate as long\nas the covariate is bounded, in a strong contrast with many existing results\nwhich rely on specific distributions of the covariates such as the spherical\nuniform data distribution or distributions satisfying certain restrictive\nconditions. The rate $\\mathcal{O}(\\epsilon_n^2)$ is known to be minimax optimal\nfor specific cases, such as the case that the NTK has a polynomial eigenvalue\ndecay rate which happens under certain distributional assumptions on the\ncovariates. Our result formally fills the gap between training a classical\nkernel regression model and training an over-parameterized but finite-width\nneural network by GD for nonparametric regression without distributional\nassumptions on the bounded covariate. We also provide confirmative answers to\ncertain open questions or address particular concerns in the literature of\ntraining over-parameterized neural networks by GD with early stopping for\nnonparametric regression, including the characterization of the stopping time,\nthe lower bound for the network width, and the constant learning rate used in\nGD.\n", "link": "http://arxiv.org/abs/2411.02904v5", "date": "2025-08-11", "relevancy": 2.3225, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.448}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression&body=Title%3A%20Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression%0AAuthor%3A%20Yingzhen%20Yang%20and%20Ping%20Li%0AAbstract%3A%20%20%20We%20study%20nonparametric%20regression%20by%20an%20over-parameterized%20two-layer%20neural%0Anetwork%20trained%20by%20gradient%20descent%20%28GD%29%20in%20this%20paper.%20We%20show%20that%2C%20if%20the%0Aneural%20network%20is%20trained%20by%20GD%20with%20early%20stopping%2C%20then%20the%20trained%20network%0Arenders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon_n%5E2%29%24%2C%20which%20is%20the%20same%20rate%20as%20that%20for%20the%20classical%0Akernel%20regression%20trained%20by%20GD%20with%20early%20stopping%2C%20where%20%24%5Cepsilon_n%24%20is%20the%0Acritical%20population%20rate%20of%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20associated%20with%20the%0Anetwork%20and%20%24n%24%20is%20the%20size%20of%20the%20training%20data.%20It%20is%20remarked%20that%20our%0Aresult%20does%20not%20require%20distributional%20assumptions%20about%20the%20covariate%20as%20long%0Aas%20the%20covariate%20is%20bounded%2C%20in%20a%20strong%20contrast%20with%20many%20existing%20results%0Awhich%20rely%20on%20specific%20distributions%20of%20the%20covariates%20such%20as%20the%20spherical%0Auniform%20data%20distribution%20or%20distributions%20satisfying%20certain%20restrictive%0Aconditions.%20The%20rate%20%24%5Cmathcal%7BO%7D%28%5Cepsilon_n%5E2%29%24%20is%20known%20to%20be%20minimax%20optimal%0Afor%20specific%20cases%2C%20such%20as%20the%20case%20that%20the%20NTK%20has%20a%20polynomial%20eigenvalue%0Adecay%20rate%20which%20happens%20under%20certain%20distributional%20assumptions%20on%20the%0Acovariates.%20Our%20result%20formally%20fills%20the%20gap%20between%20training%20a%20classical%0Akernel%20regression%20model%20and%20training%20an%20over-parameterized%20but%20finite-width%0Aneural%20network%20by%20GD%20for%20nonparametric%20regression%20without%20distributional%0Aassumptions%20on%20the%20bounded%20covariate.%20We%20also%20provide%20confirmative%20answers%20to%0Acertain%20open%20questions%20or%20address%20particular%20concerns%20in%20the%20literature%20of%0Atraining%20over-parameterized%20neural%20networks%20by%20GD%20with%20early%20stopping%20for%0Anonparametric%20regression%2C%20including%20the%20characterization%20of%20the%20stopping%20time%2C%0Athe%20lower%20bound%20for%20the%20network%20width%2C%20and%20the%20constant%20learning%20rate%20used%20in%0AGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02904v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Descent%2520Finds%2520Over-Parameterized%2520Neural%2520Networks%2520with%2520Sharp%250A%2520%2520Generalization%2520for%2520Nonparametric%2520Regression%26entry.906535625%3DYingzhen%2520Yang%2520and%2520Ping%2520Li%26entry.1292438233%3D%2520%2520We%2520study%2520nonparametric%2520regression%2520by%2520an%2520over-parameterized%2520two-layer%2520neural%250Anetwork%2520trained%2520by%2520gradient%2520descent%2520%2528GD%2529%2520in%2520this%2520paper.%2520We%2520show%2520that%252C%2520if%2520the%250Aneural%2520network%2520is%2520trained%2520by%2520GD%2520with%2520early%2520stopping%252C%2520then%2520the%2520trained%2520network%250Arenders%2520a%2520sharp%2520rate%2520of%2520the%2520nonparametric%2520regression%2520risk%2520of%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon_n%255E2%2529%2524%252C%2520which%2520is%2520the%2520same%2520rate%2520as%2520that%2520for%2520the%2520classical%250Akernel%2520regression%2520trained%2520by%2520GD%2520with%2520early%2520stopping%252C%2520where%2520%2524%255Cepsilon_n%2524%2520is%2520the%250Acritical%2520population%2520rate%2520of%2520the%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529%2520associated%2520with%2520the%250Anetwork%2520and%2520%2524n%2524%2520is%2520the%2520size%2520of%2520the%2520training%2520data.%2520It%2520is%2520remarked%2520that%2520our%250Aresult%2520does%2520not%2520require%2520distributional%2520assumptions%2520about%2520the%2520covariate%2520as%2520long%250Aas%2520the%2520covariate%2520is%2520bounded%252C%2520in%2520a%2520strong%2520contrast%2520with%2520many%2520existing%2520results%250Awhich%2520rely%2520on%2520specific%2520distributions%2520of%2520the%2520covariates%2520such%2520as%2520the%2520spherical%250Auniform%2520data%2520distribution%2520or%2520distributions%2520satisfying%2520certain%2520restrictive%250Aconditions.%2520The%2520rate%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon_n%255E2%2529%2524%2520is%2520known%2520to%2520be%2520minimax%2520optimal%250Afor%2520specific%2520cases%252C%2520such%2520as%2520the%2520case%2520that%2520the%2520NTK%2520has%2520a%2520polynomial%2520eigenvalue%250Adecay%2520rate%2520which%2520happens%2520under%2520certain%2520distributional%2520assumptions%2520on%2520the%250Acovariates.%2520Our%2520result%2520formally%2520fills%2520the%2520gap%2520between%2520training%2520a%2520classical%250Akernel%2520regression%2520model%2520and%2520training%2520an%2520over-parameterized%2520but%2520finite-width%250Aneural%2520network%2520by%2520GD%2520for%2520nonparametric%2520regression%2520without%2520distributional%250Aassumptions%2520on%2520the%2520bounded%2520covariate.%2520We%2520also%2520provide%2520confirmative%2520answers%2520to%250Acertain%2520open%2520questions%2520or%2520address%2520particular%2520concerns%2520in%2520the%2520literature%2520of%250Atraining%2520over-parameterized%2520neural%2520networks%2520by%2520GD%2520with%2520early%2520stopping%2520for%250Anonparametric%2520regression%252C%2520including%2520the%2520characterization%2520of%2520the%2520stopping%2520time%252C%250Athe%2520lower%2520bound%2520for%2520the%2520network%2520width%252C%2520and%2520the%2520constant%2520learning%2520rate%2520used%2520in%250AGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02904v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Descent%20Finds%20Over-Parameterized%20Neural%20Networks%20with%20Sharp%0A%20%20Generalization%20for%20Nonparametric%20Regression&entry.906535625=Yingzhen%20Yang%20and%20Ping%20Li&entry.1292438233=%20%20We%20study%20nonparametric%20regression%20by%20an%20over-parameterized%20two-layer%20neural%0Anetwork%20trained%20by%20gradient%20descent%20%28GD%29%20in%20this%20paper.%20We%20show%20that%2C%20if%20the%0Aneural%20network%20is%20trained%20by%20GD%20with%20early%20stopping%2C%20then%20the%20trained%20network%0Arenders%20a%20sharp%20rate%20of%20the%20nonparametric%20regression%20risk%20of%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon_n%5E2%29%24%2C%20which%20is%20the%20same%20rate%20as%20that%20for%20the%20classical%0Akernel%20regression%20trained%20by%20GD%20with%20early%20stopping%2C%20where%20%24%5Cepsilon_n%24%20is%20the%0Acritical%20population%20rate%20of%20the%20Neural%20Tangent%20Kernel%20%28NTK%29%20associated%20with%20the%0Anetwork%20and%20%24n%24%20is%20the%20size%20of%20the%20training%20data.%20It%20is%20remarked%20that%20our%0Aresult%20does%20not%20require%20distributional%20assumptions%20about%20the%20covariate%20as%20long%0Aas%20the%20covariate%20is%20bounded%2C%20in%20a%20strong%20contrast%20with%20many%20existing%20results%0Awhich%20rely%20on%20specific%20distributions%20of%20the%20covariates%20such%20as%20the%20spherical%0Auniform%20data%20distribution%20or%20distributions%20satisfying%20certain%20restrictive%0Aconditions.%20The%20rate%20%24%5Cmathcal%7BO%7D%28%5Cepsilon_n%5E2%29%24%20is%20known%20to%20be%20minimax%20optimal%0Afor%20specific%20cases%2C%20such%20as%20the%20case%20that%20the%20NTK%20has%20a%20polynomial%20eigenvalue%0Adecay%20rate%20which%20happens%20under%20certain%20distributional%20assumptions%20on%20the%0Acovariates.%20Our%20result%20formally%20fills%20the%20gap%20between%20training%20a%20classical%0Akernel%20regression%20model%20and%20training%20an%20over-parameterized%20but%20finite-width%0Aneural%20network%20by%20GD%20for%20nonparametric%20regression%20without%20distributional%0Aassumptions%20on%20the%20bounded%20covariate.%20We%20also%20provide%20confirmative%20answers%20to%0Acertain%20open%20questions%20or%20address%20particular%20concerns%20in%20the%20literature%20of%0Atraining%20over-parameterized%20neural%20networks%20by%20GD%20with%20early%20stopping%20for%0Anonparametric%20regression%2C%20including%20the%20characterization%20of%20the%20stopping%20time%2C%0Athe%20lower%20bound%20for%20the%20network%20width%2C%20and%20the%20constant%20learning%20rate%20used%20in%0AGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02904v5&entry.124074799=Read"},
{"title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced\n  Multimodal In-Context Learning", "author": "Yanshu Li and Jianjiang Yang and Zhennan Shen and Ligong Han and Haoyan Xu and Ruixiang Tang", "abstract": "  Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.\n", "link": "http://arxiv.org/abs/2508.07871v1", "date": "2025-08-11", "relevancy": 2.3215, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5969}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.576}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CATP%3A%20Contextually%20Adaptive%20Token%20Pruning%20for%20Efficient%20and%20Enhanced%0A%20%20Multimodal%20In-Context%20Learning&body=Title%3A%20CATP%3A%20Contextually%20Adaptive%20Token%20Pruning%20for%20Efficient%20and%20Enhanced%0A%20%20Multimodal%20In-Context%20Learning%0AAuthor%3A%20Yanshu%20Li%20and%20Jianjiang%20Yang%20and%20Zhennan%20Shen%20and%20Ligong%20Han%20and%20Haoyan%20Xu%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Modern%20large%20vision-language%20models%20%28LVLMs%29%20convert%20each%20input%20image%20into%20a%0Alarge%20set%20of%20tokens%2C%20far%20outnumbering%20the%20text%20tokens.%20Although%20this%20improves%0Avisual%20perception%2C%20it%20introduces%20severe%20image%20token%20redundancy.%20Because%20image%0Atokens%20carry%20sparse%20information%2C%20many%20add%20little%20to%20reasoning%2C%20yet%20greatly%0Aincrease%20inference%20cost.%20The%20emerging%20image%20token%20pruning%20methods%20tackle%20this%0Aissue%20by%20identifying%20the%20most%20important%20tokens%20and%20discarding%20the%20rest.%20These%0Amethods%20can%20raise%20efficiency%20with%20only%20modest%20performance%20loss.%20However%2C%20most%0Aof%20them%20only%20consider%20single-image%20tasks%20and%20overlook%20multimodal%20in-context%0Alearning%20%28ICL%29%2C%20where%20redundancy%20is%20greater%20and%20efficiency%20is%20more%20critical.%0ARedundant%20tokens%20weaken%20the%20advantage%20of%20multimodal%20ICL%20for%20rapid%20domain%0Aadaptation%20and%20cause%20unstable%20performance.%20Applying%20existing%20pruning%20methods%20in%0Athis%20setting%20leads%20to%20large%20accuracy%20drops%2C%20exposing%20a%20clear%20gap%20and%20the%20need%0Afor%20new%20techniques.%20Thus%2C%20we%20propose%20Contextually%20Adaptive%20Token%20Pruning%0A%28CATP%29%2C%20a%20training-free%20pruning%20method%20targeted%20at%20multimodal%20ICL.%20CATP%0Aconsists%20of%20two%20stages%20that%20perform%20progressive%20pruning%20to%20fully%20account%20for%0Athe%20complex%20cross-modal%20interactions%20in%20the%20input%20sequence.%20After%20removing%0A77.8%5C%25%20of%20the%20image%20tokens%2C%20CATP%20produces%20an%20average%20performance%20gain%20of%200.6%5C%25%0Aover%20the%20vanilla%20model%20on%20four%20LVLMs%20and%20eight%20benchmarks%2C%20exceeding%20all%0Abaselines%20remarkably.%20Meanwhile%2C%20it%20effectively%20improves%20efficiency%20by%0Aachieving%20an%20average%20reduction%20of%2010.78%5C%25%20in%20inference%20latency.%20CATP%20enhances%0Athe%20practical%20value%20of%20multimodal%20ICL%20and%20lays%20the%20groundwork%20for%20future%0Aprogress%20in%20interleaved%20image-text%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCATP%253A%2520Contextually%2520Adaptive%2520Token%2520Pruning%2520for%2520Efficient%2520and%2520Enhanced%250A%2520%2520Multimodal%2520In-Context%2520Learning%26entry.906535625%3DYanshu%2520Li%2520and%2520Jianjiang%2520Yang%2520and%2520Zhennan%2520Shen%2520and%2520Ligong%2520Han%2520and%2520Haoyan%2520Xu%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Modern%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520convert%2520each%2520input%2520image%2520into%2520a%250Alarge%2520set%2520of%2520tokens%252C%2520far%2520outnumbering%2520the%2520text%2520tokens.%2520Although%2520this%2520improves%250Avisual%2520perception%252C%2520it%2520introduces%2520severe%2520image%2520token%2520redundancy.%2520Because%2520image%250Atokens%2520carry%2520sparse%2520information%252C%2520many%2520add%2520little%2520to%2520reasoning%252C%2520yet%2520greatly%250Aincrease%2520inference%2520cost.%2520The%2520emerging%2520image%2520token%2520pruning%2520methods%2520tackle%2520this%250Aissue%2520by%2520identifying%2520the%2520most%2520important%2520tokens%2520and%2520discarding%2520the%2520rest.%2520These%250Amethods%2520can%2520raise%2520efficiency%2520with%2520only%2520modest%2520performance%2520loss.%2520However%252C%2520most%250Aof%2520them%2520only%2520consider%2520single-image%2520tasks%2520and%2520overlook%2520multimodal%2520in-context%250Alearning%2520%2528ICL%2529%252C%2520where%2520redundancy%2520is%2520greater%2520and%2520efficiency%2520is%2520more%2520critical.%250ARedundant%2520tokens%2520weaken%2520the%2520advantage%2520of%2520multimodal%2520ICL%2520for%2520rapid%2520domain%250Aadaptation%2520and%2520cause%2520unstable%2520performance.%2520Applying%2520existing%2520pruning%2520methods%2520in%250Athis%2520setting%2520leads%2520to%2520large%2520accuracy%2520drops%252C%2520exposing%2520a%2520clear%2520gap%2520and%2520the%2520need%250Afor%2520new%2520techniques.%2520Thus%252C%2520we%2520propose%2520Contextually%2520Adaptive%2520Token%2520Pruning%250A%2528CATP%2529%252C%2520a%2520training-free%2520pruning%2520method%2520targeted%2520at%2520multimodal%2520ICL.%2520CATP%250Aconsists%2520of%2520two%2520stages%2520that%2520perform%2520progressive%2520pruning%2520to%2520fully%2520account%2520for%250Athe%2520complex%2520cross-modal%2520interactions%2520in%2520the%2520input%2520sequence.%2520After%2520removing%250A77.8%255C%2525%2520of%2520the%2520image%2520tokens%252C%2520CATP%2520produces%2520an%2520average%2520performance%2520gain%2520of%25200.6%255C%2525%250Aover%2520the%2520vanilla%2520model%2520on%2520four%2520LVLMs%2520and%2520eight%2520benchmarks%252C%2520exceeding%2520all%250Abaselines%2520remarkably.%2520Meanwhile%252C%2520it%2520effectively%2520improves%2520efficiency%2520by%250Aachieving%2520an%2520average%2520reduction%2520of%252010.78%255C%2525%2520in%2520inference%2520latency.%2520CATP%2520enhances%250Athe%2520practical%2520value%2520of%2520multimodal%2520ICL%2520and%2520lays%2520the%2520groundwork%2520for%2520future%250Aprogress%2520in%2520interleaved%2520image-text%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CATP%3A%20Contextually%20Adaptive%20Token%20Pruning%20for%20Efficient%20and%20Enhanced%0A%20%20Multimodal%20In-Context%20Learning&entry.906535625=Yanshu%20Li%20and%20Jianjiang%20Yang%20and%20Zhennan%20Shen%20and%20Ligong%20Han%20and%20Haoyan%20Xu%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Modern%20large%20vision-language%20models%20%28LVLMs%29%20convert%20each%20input%20image%20into%20a%0Alarge%20set%20of%20tokens%2C%20far%20outnumbering%20the%20text%20tokens.%20Although%20this%20improves%0Avisual%20perception%2C%20it%20introduces%20severe%20image%20token%20redundancy.%20Because%20image%0Atokens%20carry%20sparse%20information%2C%20many%20add%20little%20to%20reasoning%2C%20yet%20greatly%0Aincrease%20inference%20cost.%20The%20emerging%20image%20token%20pruning%20methods%20tackle%20this%0Aissue%20by%20identifying%20the%20most%20important%20tokens%20and%20discarding%20the%20rest.%20These%0Amethods%20can%20raise%20efficiency%20with%20only%20modest%20performance%20loss.%20However%2C%20most%0Aof%20them%20only%20consider%20single-image%20tasks%20and%20overlook%20multimodal%20in-context%0Alearning%20%28ICL%29%2C%20where%20redundancy%20is%20greater%20and%20efficiency%20is%20more%20critical.%0ARedundant%20tokens%20weaken%20the%20advantage%20of%20multimodal%20ICL%20for%20rapid%20domain%0Aadaptation%20and%20cause%20unstable%20performance.%20Applying%20existing%20pruning%20methods%20in%0Athis%20setting%20leads%20to%20large%20accuracy%20drops%2C%20exposing%20a%20clear%20gap%20and%20the%20need%0Afor%20new%20techniques.%20Thus%2C%20we%20propose%20Contextually%20Adaptive%20Token%20Pruning%0A%28CATP%29%2C%20a%20training-free%20pruning%20method%20targeted%20at%20multimodal%20ICL.%20CATP%0Aconsists%20of%20two%20stages%20that%20perform%20progressive%20pruning%20to%20fully%20account%20for%0Athe%20complex%20cross-modal%20interactions%20in%20the%20input%20sequence.%20After%20removing%0A77.8%5C%25%20of%20the%20image%20tokens%2C%20CATP%20produces%20an%20average%20performance%20gain%20of%200.6%5C%25%0Aover%20the%20vanilla%20model%20on%20four%20LVLMs%20and%20eight%20benchmarks%2C%20exceeding%20all%0Abaselines%20remarkably.%20Meanwhile%2C%20it%20effectively%20improves%20efficiency%20by%0Aachieving%20an%20average%20reduction%20of%2010.78%5C%25%20in%20inference%20latency.%20CATP%20enhances%0Athe%20practical%20value%20of%20multimodal%20ICL%20and%20lays%20the%20groundwork%20for%20future%0Aprogress%20in%20interleaved%20image-text%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07871v1&entry.124074799=Read"},
{"title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for\n  Accessibility", "author": "Xiantao Zhang", "abstract": "  Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments.\n", "link": "http://arxiv.org/abs/2508.07989v1", "date": "2025-08-11", "relevancy": 2.309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6193}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5941}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Escalator%20Problem%3A%20Identifying%20Implicit%20Motion%20Blindness%20in%20AI%20for%0A%20%20Accessibility&body=Title%3A%20The%20Escalator%20Problem%3A%20Identifying%20Implicit%20Motion%20Blindness%20in%20AI%20for%0A%20%20Accessibility%0AAuthor%3A%20Xiantao%20Zhang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20immense%20promise%20as%20assistive%0Atechnologies%20for%20the%20blind%20and%20visually%20impaired%20%28BVI%29%20community.%20However%2C%20we%0Aidentify%20a%20critical%20failure%20mode%20that%20undermines%20their%20trustworthiness%20in%0Areal-world%20applications.%20We%20introduce%20the%20Escalator%20Problem%20--%20the%20inability%20of%0Astate-of-the-art%20models%20to%20perceive%20an%20escalator%27s%20direction%20of%20travel%20--%20as%20a%0Acanonical%20example%20of%20a%20deeper%20limitation%20we%20term%20Implicit%20Motion%20Blindness.%0AThis%20blindness%20stems%20from%20the%20dominant%20frame-sampling%20paradigm%20in%20video%0Aunderstanding%2C%20which%2C%20by%20treating%20videos%20as%20discrete%20sequences%20of%20static%0Aimages%2C%20fundamentally%20struggles%20to%20perceive%20continuous%2C%20low-signal%20motion.%20As%20a%0Aposition%20paper%2C%20our%20contribution%20is%20not%20a%20new%20model%20but%20rather%20to%3A%20%28I%29%20formally%0Aarticulate%20this%20blind%20spot%2C%20%28II%29%20analyze%20its%20implications%20for%20user%20trust%2C%20and%0A%28III%29%20issue%20a%20call%20to%20action.%20We%20advocate%20for%20a%20paradigm%20shift%20from%20purely%0Asemantic%20recognition%20towards%20robust%20physical%20perception%20and%20urge%20the%0Adevelopment%20of%20new%2C%20human-centered%20benchmarks%20that%20prioritize%20safety%2C%0Areliability%2C%20and%20the%20genuine%20needs%20of%20users%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Escalator%2520Problem%253A%2520Identifying%2520Implicit%2520Motion%2520Blindness%2520in%2520AI%2520for%250A%2520%2520Accessibility%26entry.906535625%3DXiantao%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520hold%2520immense%2520promise%2520as%2520assistive%250Atechnologies%2520for%2520the%2520blind%2520and%2520visually%2520impaired%2520%2528BVI%2529%2520community.%2520However%252C%2520we%250Aidentify%2520a%2520critical%2520failure%2520mode%2520that%2520undermines%2520their%2520trustworthiness%2520in%250Areal-world%2520applications.%2520We%2520introduce%2520the%2520Escalator%2520Problem%2520--%2520the%2520inability%2520of%250Astate-of-the-art%2520models%2520to%2520perceive%2520an%2520escalator%2527s%2520direction%2520of%2520travel%2520--%2520as%2520a%250Acanonical%2520example%2520of%2520a%2520deeper%2520limitation%2520we%2520term%2520Implicit%2520Motion%2520Blindness.%250AThis%2520blindness%2520stems%2520from%2520the%2520dominant%2520frame-sampling%2520paradigm%2520in%2520video%250Aunderstanding%252C%2520which%252C%2520by%2520treating%2520videos%2520as%2520discrete%2520sequences%2520of%2520static%250Aimages%252C%2520fundamentally%2520struggles%2520to%2520perceive%2520continuous%252C%2520low-signal%2520motion.%2520As%2520a%250Aposition%2520paper%252C%2520our%2520contribution%2520is%2520not%2520a%2520new%2520model%2520but%2520rather%2520to%253A%2520%2528I%2529%2520formally%250Aarticulate%2520this%2520blind%2520spot%252C%2520%2528II%2529%2520analyze%2520its%2520implications%2520for%2520user%2520trust%252C%2520and%250A%2528III%2529%2520issue%2520a%2520call%2520to%2520action.%2520We%2520advocate%2520for%2520a%2520paradigm%2520shift%2520from%2520purely%250Asemantic%2520recognition%2520towards%2520robust%2520physical%2520perception%2520and%2520urge%2520the%250Adevelopment%2520of%2520new%252C%2520human-centered%2520benchmarks%2520that%2520prioritize%2520safety%252C%250Areliability%252C%2520and%2520the%2520genuine%2520needs%2520of%2520users%2520in%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Escalator%20Problem%3A%20Identifying%20Implicit%20Motion%20Blindness%20in%20AI%20for%0A%20%20Accessibility&entry.906535625=Xiantao%20Zhang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20immense%20promise%20as%20assistive%0Atechnologies%20for%20the%20blind%20and%20visually%20impaired%20%28BVI%29%20community.%20However%2C%20we%0Aidentify%20a%20critical%20failure%20mode%20that%20undermines%20their%20trustworthiness%20in%0Areal-world%20applications.%20We%20introduce%20the%20Escalator%20Problem%20--%20the%20inability%20of%0Astate-of-the-art%20models%20to%20perceive%20an%20escalator%27s%20direction%20of%20travel%20--%20as%20a%0Acanonical%20example%20of%20a%20deeper%20limitation%20we%20term%20Implicit%20Motion%20Blindness.%0AThis%20blindness%20stems%20from%20the%20dominant%20frame-sampling%20paradigm%20in%20video%0Aunderstanding%2C%20which%2C%20by%20treating%20videos%20as%20discrete%20sequences%20of%20static%0Aimages%2C%20fundamentally%20struggles%20to%20perceive%20continuous%2C%20low-signal%20motion.%20As%20a%0Aposition%20paper%2C%20our%20contribution%20is%20not%20a%20new%20model%20but%20rather%20to%3A%20%28I%29%20formally%0Aarticulate%20this%20blind%20spot%2C%20%28II%29%20analyze%20its%20implications%20for%20user%20trust%2C%20and%0A%28III%29%20issue%20a%20call%20to%20action.%20We%20advocate%20for%20a%20paradigm%20shift%20from%20purely%0Asemantic%20recognition%20towards%20robust%20physical%20perception%20and%20urge%20the%0Adevelopment%20of%20new%2C%20human-centered%20benchmarks%20that%20prioritize%20safety%2C%0Areliability%2C%20and%20the%20genuine%20needs%20of%20users%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07989v1&entry.124074799=Read"},
{"title": "THAT: Token-wise High-frequency Augmentation Transformer for\n  Hyperspectral Pansharpening", "author": "Hongkun Jin and Hongcheng Jiang and Zejun Zhang and Yuan Zhang and Jia Fu and Tingfeng Li and Kai Luo", "abstract": "  Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT.\n", "link": "http://arxiv.org/abs/2508.08183v1", "date": "2025-08-11", "relevancy": 2.3044, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6252}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5708}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20THAT%3A%20Token-wise%20High-frequency%20Augmentation%20Transformer%20for%0A%20%20Hyperspectral%20Pansharpening&body=Title%3A%20THAT%3A%20Token-wise%20High-frequency%20Augmentation%20Transformer%20for%0A%20%20Hyperspectral%20Pansharpening%0AAuthor%3A%20Hongkun%20Jin%20and%20Hongcheng%20Jiang%20and%20Zejun%20Zhang%20and%20Yuan%20Zhang%20and%20Jia%20Fu%20and%20Tingfeng%20Li%20and%20Kai%20Luo%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20demonstrated%20strong%20potential%20in%20hyperspectral%0Apansharpening%20by%20modeling%20long-range%20dependencies.%20However%2C%20their%20effectiveness%0Ais%20often%20limited%20by%20redundant%20token%20representations%20and%20a%20lack%20of%20multi-scale%0Afeature%20modeling.%20Hyperspectral%20images%20exhibit%20intrinsic%20spectral%20priors%20%28e.g.%2C%0Aabundance%20sparsity%29%20and%20spatial%20priors%20%28e.g.%2C%20non-local%20similarity%29%2C%20which%20are%0Acritical%20for%20accurate%20reconstruction.%20From%20a%20spectral-spatial%20perspective%2C%0AVision%20Transformers%20%28ViTs%29%20face%20two%20major%20limitations%3A%20they%20struggle%20to%0Apreserve%20high-frequency%20components--such%20as%20material%20edges%20and%20texture%0Atransitions--and%20suffer%20from%20attention%20dispersion%20across%20redundant%20tokens.%0AThese%20issues%20stem%20from%20the%20global%20self-attention%20mechanism%2C%20which%20tends%20to%0Adilute%20high-frequency%20signals%20and%20overlook%20localized%20details.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Token-wise%20High-frequency%20Augmentation%20Transformer%0A%28THAT%29%2C%20a%20novel%20framework%20designed%20to%20enhance%20hyperspectral%20pansharpening%0Athrough%20improved%20high-frequency%20feature%20representation%20and%20token%20selection.%0ASpecifically%2C%20THAT%20introduces%3A%20%281%29%20Pivotal%20Token%20Selective%20Attention%20%28PTSA%29%20to%0Aprioritize%20informative%20tokens%20and%20suppress%20redundancy%3B%20%282%29%20a%20Multi-level%0AVariance-aware%20Feed-forward%20Network%20%28MVFN%29%20to%20enhance%20high-frequency%20detail%0Alearning.%20Experiments%20on%20standard%20benchmarks%20show%20that%20THAT%20achieves%0Astate-of-the-art%20performance%20with%20improved%20reconstruction%20quality%20and%0Aefficiency.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/kailuo93/THAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTHAT%253A%2520Token-wise%2520High-frequency%2520Augmentation%2520Transformer%2520for%250A%2520%2520Hyperspectral%2520Pansharpening%26entry.906535625%3DHongkun%2520Jin%2520and%2520Hongcheng%2520Jiang%2520and%2520Zejun%2520Zhang%2520and%2520Yuan%2520Zhang%2520and%2520Jia%2520Fu%2520and%2520Tingfeng%2520Li%2520and%2520Kai%2520Luo%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520demonstrated%2520strong%2520potential%2520in%2520hyperspectral%250Apansharpening%2520by%2520modeling%2520long-range%2520dependencies.%2520However%252C%2520their%2520effectiveness%250Ais%2520often%2520limited%2520by%2520redundant%2520token%2520representations%2520and%2520a%2520lack%2520of%2520multi-scale%250Afeature%2520modeling.%2520Hyperspectral%2520images%2520exhibit%2520intrinsic%2520spectral%2520priors%2520%2528e.g.%252C%250Aabundance%2520sparsity%2529%2520and%2520spatial%2520priors%2520%2528e.g.%252C%2520non-local%2520similarity%2529%252C%2520which%2520are%250Acritical%2520for%2520accurate%2520reconstruction.%2520From%2520a%2520spectral-spatial%2520perspective%252C%250AVision%2520Transformers%2520%2528ViTs%2529%2520face%2520two%2520major%2520limitations%253A%2520they%2520struggle%2520to%250Apreserve%2520high-frequency%2520components--such%2520as%2520material%2520edges%2520and%2520texture%250Atransitions--and%2520suffer%2520from%2520attention%2520dispersion%2520across%2520redundant%2520tokens.%250AThese%2520issues%2520stem%2520from%2520the%2520global%2520self-attention%2520mechanism%252C%2520which%2520tends%2520to%250Adilute%2520high-frequency%2520signals%2520and%2520overlook%2520localized%2520details.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520Token-wise%2520High-frequency%2520Augmentation%2520Transformer%250A%2528THAT%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520hyperspectral%2520pansharpening%250Athrough%2520improved%2520high-frequency%2520feature%2520representation%2520and%2520token%2520selection.%250ASpecifically%252C%2520THAT%2520introduces%253A%2520%25281%2529%2520Pivotal%2520Token%2520Selective%2520Attention%2520%2528PTSA%2529%2520to%250Aprioritize%2520informative%2520tokens%2520and%2520suppress%2520redundancy%253B%2520%25282%2529%2520a%2520Multi-level%250AVariance-aware%2520Feed-forward%2520Network%2520%2528MVFN%2529%2520to%2520enhance%2520high-frequency%2520detail%250Alearning.%2520Experiments%2520on%2520standard%2520benchmarks%2520show%2520that%2520THAT%2520achieves%250Astate-of-the-art%2520performance%2520with%2520improved%2520reconstruction%2520quality%2520and%250Aefficiency.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/kailuo93/THAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=THAT%3A%20Token-wise%20High-frequency%20Augmentation%20Transformer%20for%0A%20%20Hyperspectral%20Pansharpening&entry.906535625=Hongkun%20Jin%20and%20Hongcheng%20Jiang%20and%20Zejun%20Zhang%20and%20Yuan%20Zhang%20and%20Jia%20Fu%20and%20Tingfeng%20Li%20and%20Kai%20Luo&entry.1292438233=%20%20Transformer-based%20methods%20have%20demonstrated%20strong%20potential%20in%20hyperspectral%0Apansharpening%20by%20modeling%20long-range%20dependencies.%20However%2C%20their%20effectiveness%0Ais%20often%20limited%20by%20redundant%20token%20representations%20and%20a%20lack%20of%20multi-scale%0Afeature%20modeling.%20Hyperspectral%20images%20exhibit%20intrinsic%20spectral%20priors%20%28e.g.%2C%0Aabundance%20sparsity%29%20and%20spatial%20priors%20%28e.g.%2C%20non-local%20similarity%29%2C%20which%20are%0Acritical%20for%20accurate%20reconstruction.%20From%20a%20spectral-spatial%20perspective%2C%0AVision%20Transformers%20%28ViTs%29%20face%20two%20major%20limitations%3A%20they%20struggle%20to%0Apreserve%20high-frequency%20components--such%20as%20material%20edges%20and%20texture%0Atransitions--and%20suffer%20from%20attention%20dispersion%20across%20redundant%20tokens.%0AThese%20issues%20stem%20from%20the%20global%20self-attention%20mechanism%2C%20which%20tends%20to%0Adilute%20high-frequency%20signals%20and%20overlook%20localized%20details.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Token-wise%20High-frequency%20Augmentation%20Transformer%0A%28THAT%29%2C%20a%20novel%20framework%20designed%20to%20enhance%20hyperspectral%20pansharpening%0Athrough%20improved%20high-frequency%20feature%20representation%20and%20token%20selection.%0ASpecifically%2C%20THAT%20introduces%3A%20%281%29%20Pivotal%20Token%20Selective%20Attention%20%28PTSA%29%20to%0Aprioritize%20informative%20tokens%20and%20suppress%20redundancy%3B%20%282%29%20a%20Multi-level%0AVariance-aware%20Feed-forward%20Network%20%28MVFN%29%20to%20enhance%20high-frequency%20detail%0Alearning.%20Experiments%20on%20standard%20benchmarks%20show%20that%20THAT%20achieves%0Astate-of-the-art%20performance%20with%20improved%20reconstruction%20quality%20and%0Aefficiency.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/kailuo93/THAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08183v1&entry.124074799=Read"},
{"title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and\n  Projection for Multi-Object Tracking", "author": "Xudong Han and Pengcheng Fang and Yueying Tian and Jianhui Yu and Xiaohao Cai and Daniel Roggen and Philip Birch", "abstract": "  Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns.\n", "link": "http://arxiv.org/abs/2508.08117v1", "date": "2025-08-11", "relevancy": 2.2959, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5805}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRASPTrack%3A%20Geometry-Reasoned%20Association%20via%20Segmentation%20and%0A%20%20Projection%20for%20Multi-Object%20Tracking&body=Title%3A%20GRASPTrack%3A%20Geometry-Reasoned%20Association%20via%20Segmentation%20and%0A%20%20Projection%20for%20Multi-Object%20Tracking%0AAuthor%3A%20Xudong%20Han%20and%20Pengcheng%20Fang%20and%20Yueying%20Tian%20and%20Jianhui%20Yu%20and%20Xiaohao%20Cai%20and%20Daniel%20Roggen%20and%20Philip%20Birch%0AAbstract%3A%20%20%20Multi-object%20tracking%20%28MOT%29%20in%20monocular%20videos%20is%20fundamentally%20challenged%0Aby%20occlusions%20and%20depth%20ambiguity%2C%20issues%20that%20conventional%0Atracking-by-detection%20%28TBD%29%20methods%20struggle%20to%20resolve%20owing%20to%20a%20lack%20of%0Ageometric%20awareness.%20To%20address%20these%20limitations%2C%20we%20introduce%20GRASPTrack%2C%20a%0Anovel%20depth-aware%20MOT%20framework%20that%20integrates%20monocular%20depth%20estimation%20and%0Ainstance%20segmentation%20into%20a%20standard%20TBD%20pipeline%20to%20generate%20high-fidelity%203D%0Apoint%20clouds%20from%202D%20detections%2C%20thereby%20enabling%20explicit%203D%20geometric%0Areasoning.%20These%203D%20point%20clouds%20are%20then%20voxelized%20to%20enable%20a%20precise%20and%0Arobust%20Voxel-Based%203D%20Intersection-over-Union%20%28IoU%29%20for%20spatial%20association.%20To%0Afurther%20enhance%20tracking%20robustness%2C%20our%20approach%20incorporates%20Depth-aware%0AAdaptive%20Noise%20Compensation%2C%20which%20dynamically%20adjusts%20the%20Kalman%20filter%0Aprocess%20noise%20based%20on%20occlusion%20severity%20for%20more%20reliable%20state%20estimation.%0AAdditionally%2C%20we%20propose%20a%20Depth-enhanced%20Observation-Centric%20Momentum%2C%20which%0Aextends%20the%20motion%20direction%20consistency%20from%20the%20image%20plane%20into%203D%20space%20to%0Aimprove%20motion-based%20association%20cues%2C%20particularly%20for%20objects%20with%20complex%0Atrajectories.%20Extensive%20experiments%20on%20the%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%2C%0Asignificantly%20improving%20tracking%20robustness%20in%20complex%20scenes%20with%20frequent%0Aocclusions%20and%20intricate%20motion%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRASPTrack%253A%2520Geometry-Reasoned%2520Association%2520via%2520Segmentation%2520and%250A%2520%2520Projection%2520for%2520Multi-Object%2520Tracking%26entry.906535625%3DXudong%2520Han%2520and%2520Pengcheng%2520Fang%2520and%2520Yueying%2520Tian%2520and%2520Jianhui%2520Yu%2520and%2520Xiaohao%2520Cai%2520and%2520Daniel%2520Roggen%2520and%2520Philip%2520Birch%26entry.1292438233%3D%2520%2520Multi-object%2520tracking%2520%2528MOT%2529%2520in%2520monocular%2520videos%2520is%2520fundamentally%2520challenged%250Aby%2520occlusions%2520and%2520depth%2520ambiguity%252C%2520issues%2520that%2520conventional%250Atracking-by-detection%2520%2528TBD%2529%2520methods%2520struggle%2520to%2520resolve%2520owing%2520to%2520a%2520lack%2520of%250Ageometric%2520awareness.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520GRASPTrack%252C%2520a%250Anovel%2520depth-aware%2520MOT%2520framework%2520that%2520integrates%2520monocular%2520depth%2520estimation%2520and%250Ainstance%2520segmentation%2520into%2520a%2520standard%2520TBD%2520pipeline%2520to%2520generate%2520high-fidelity%25203D%250Apoint%2520clouds%2520from%25202D%2520detections%252C%2520thereby%2520enabling%2520explicit%25203D%2520geometric%250Areasoning.%2520These%25203D%2520point%2520clouds%2520are%2520then%2520voxelized%2520to%2520enable%2520a%2520precise%2520and%250Arobust%2520Voxel-Based%25203D%2520Intersection-over-Union%2520%2528IoU%2529%2520for%2520spatial%2520association.%2520To%250Afurther%2520enhance%2520tracking%2520robustness%252C%2520our%2520approach%2520incorporates%2520Depth-aware%250AAdaptive%2520Noise%2520Compensation%252C%2520which%2520dynamically%2520adjusts%2520the%2520Kalman%2520filter%250Aprocess%2520noise%2520based%2520on%2520occlusion%2520severity%2520for%2520more%2520reliable%2520state%2520estimation.%250AAdditionally%252C%2520we%2520propose%2520a%2520Depth-enhanced%2520Observation-Centric%2520Momentum%252C%2520which%250Aextends%2520the%2520motion%2520direction%2520consistency%2520from%2520the%2520image%2520plane%2520into%25203D%2520space%2520to%250Aimprove%2520motion-based%2520association%2520cues%252C%2520particularly%2520for%2520objects%2520with%2520complex%250Atrajectories.%2520Extensive%2520experiments%2520on%2520the%2520MOT17%252C%2520MOT20%252C%2520and%2520DanceTrack%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%252C%250Asignificantly%2520improving%2520tracking%2520robustness%2520in%2520complex%2520scenes%2520with%2520frequent%250Aocclusions%2520and%2520intricate%2520motion%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRASPTrack%3A%20Geometry-Reasoned%20Association%20via%20Segmentation%20and%0A%20%20Projection%20for%20Multi-Object%20Tracking&entry.906535625=Xudong%20Han%20and%20Pengcheng%20Fang%20and%20Yueying%20Tian%20and%20Jianhui%20Yu%20and%20Xiaohao%20Cai%20and%20Daniel%20Roggen%20and%20Philip%20Birch&entry.1292438233=%20%20Multi-object%20tracking%20%28MOT%29%20in%20monocular%20videos%20is%20fundamentally%20challenged%0Aby%20occlusions%20and%20depth%20ambiguity%2C%20issues%20that%20conventional%0Atracking-by-detection%20%28TBD%29%20methods%20struggle%20to%20resolve%20owing%20to%20a%20lack%20of%0Ageometric%20awareness.%20To%20address%20these%20limitations%2C%20we%20introduce%20GRASPTrack%2C%20a%0Anovel%20depth-aware%20MOT%20framework%20that%20integrates%20monocular%20depth%20estimation%20and%0Ainstance%20segmentation%20into%20a%20standard%20TBD%20pipeline%20to%20generate%20high-fidelity%203D%0Apoint%20clouds%20from%202D%20detections%2C%20thereby%20enabling%20explicit%203D%20geometric%0Areasoning.%20These%203D%20point%20clouds%20are%20then%20voxelized%20to%20enable%20a%20precise%20and%0Arobust%20Voxel-Based%203D%20Intersection-over-Union%20%28IoU%29%20for%20spatial%20association.%20To%0Afurther%20enhance%20tracking%20robustness%2C%20our%20approach%20incorporates%20Depth-aware%0AAdaptive%20Noise%20Compensation%2C%20which%20dynamically%20adjusts%20the%20Kalman%20filter%0Aprocess%20noise%20based%20on%20occlusion%20severity%20for%20more%20reliable%20state%20estimation.%0AAdditionally%2C%20we%20propose%20a%20Depth-enhanced%20Observation-Centric%20Momentum%2C%20which%0Aextends%20the%20motion%20direction%20consistency%20from%20the%20image%20plane%20into%203D%20space%20to%0Aimprove%20motion-based%20association%20cues%2C%20particularly%20for%20objects%20with%20complex%0Atrajectories.%20Extensive%20experiments%20on%20the%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%2C%0Asignificantly%20improving%20tracking%20robustness%20in%20complex%20scenes%20with%20frequent%0Aocclusions%20and%20intricate%20motion%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08117v1&entry.124074799=Read"},
{"title": "Reconstruction of boosted and resolved multi-Higgs-boson events with\n  symmetry-preserving attention networks", "author": "Haoyang Li and Marko Stamenkovic and Alexander Shmakov and Michael Fenton and Darius Shih-Chieh Chao and Kaitlyn Maiya White and Caden Mikkelsen and Jovan Mitic and Cristina Mantilla Suarez and Melissa Quinnan and Greg Landsberg and Harvey Newman and Pierre Baldi and Daniel Whiteson and Javier Duarte", "abstract": "  The production of multiple Higgs bosons at the CERN LHC provides a direct way\nto measure the trilinear and quartic Higgs self-interaction strengths as well\nas potential access to beyond the standard model effects that can enhance\nproduction at large transverse momentum $p_{\\mathrm{T}}$. The largest event\nfraction arises from the fully hadronic final state in which every Higgs boson\ndecays to a bottom quark-antiquark pair ($b\\bar{b}$). This introduces a\ncombinatorial challenge known as the \\emph{jet assignment problem}: assigning\njets to sets representing Higgs boson candidates. Symmetry-preserving attention\nnetworks (SPA-Nets) have been been developed to address this challenge.\nHowever, the complexity of jet assignment increases when simultaneously\nconsidering both $H\\rightarrow b\\bar{b}$ reconstruction possibilities, i.e.,\ntwo \"resolved\" small-radius jets each containing a shower initiated by a\n$b$-quark or one \"boosted\" large-radius jet containing a merged shower\ninitiated by a $b\\bar{b}$ pair. The latter improves the reconstruction\nefficiency at high $p_{\\mathrm{T}}$. In this work, we introduce a\ngeneralization to the SPA-Net approach to simultaneously consider both boosted\nand resolved reconstruction possibilities and unambiguously interpret an event\nas \"fully resolved'', \"fully boosted\", or in between. We report the performance\nof baseline methods, the original SPA-Net approach, and our generalized version\non nonresonant $HH$ and $HHH$ production at the LHC. Considering both boosted\nand resolved topologies, our SPA-Net approach increases the Higgs boson\nreconstruction purity by 57--62\\% and the efficiency by 23--38\\% compared to\nthe baseline method depending on the final state.\n", "link": "http://arxiv.org/abs/2412.03819v3", "date": "2025-08-11", "relevancy": 2.2886, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4713}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4634}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstruction%20of%20boosted%20and%20resolved%20multi-Higgs-boson%20events%20with%0A%20%20symmetry-preserving%20attention%20networks&body=Title%3A%20Reconstruction%20of%20boosted%20and%20resolved%20multi-Higgs-boson%20events%20with%0A%20%20symmetry-preserving%20attention%20networks%0AAuthor%3A%20Haoyang%20Li%20and%20Marko%20Stamenkovic%20and%20Alexander%20Shmakov%20and%20Michael%20Fenton%20and%20Darius%20Shih-Chieh%20Chao%20and%20Kaitlyn%20Maiya%20White%20and%20Caden%20Mikkelsen%20and%20Jovan%20Mitic%20and%20Cristina%20Mantilla%20Suarez%20and%20Melissa%20Quinnan%20and%20Greg%20Landsberg%20and%20Harvey%20Newman%20and%20Pierre%20Baldi%20and%20Daniel%20Whiteson%20and%20Javier%20Duarte%0AAbstract%3A%20%20%20The%20production%20of%20multiple%20Higgs%20bosons%20at%20the%20CERN%20LHC%20provides%20a%20direct%20way%0Ato%20measure%20the%20trilinear%20and%20quartic%20Higgs%20self-interaction%20strengths%20as%20well%0Aas%20potential%20access%20to%20beyond%20the%20standard%20model%20effects%20that%20can%20enhance%0Aproduction%20at%20large%20transverse%20momentum%20%24p_%7B%5Cmathrm%7BT%7D%7D%24.%20The%20largest%20event%0Afraction%20arises%20from%20the%20fully%20hadronic%20final%20state%20in%20which%20every%20Higgs%20boson%0Adecays%20to%20a%20bottom%20quark-antiquark%20pair%20%28%24b%5Cbar%7Bb%7D%24%29.%20This%20introduces%20a%0Acombinatorial%20challenge%20known%20as%20the%20%5Cemph%7Bjet%20assignment%20problem%7D%3A%20assigning%0Ajets%20to%20sets%20representing%20Higgs%20boson%20candidates.%20Symmetry-preserving%20attention%0Anetworks%20%28SPA-Nets%29%20have%20been%20been%20developed%20to%20address%20this%20challenge.%0AHowever%2C%20the%20complexity%20of%20jet%20assignment%20increases%20when%20simultaneously%0Aconsidering%20both%20%24H%5Crightarrow%20b%5Cbar%7Bb%7D%24%20reconstruction%20possibilities%2C%20i.e.%2C%0Atwo%20%22resolved%22%20small-radius%20jets%20each%20containing%20a%20shower%20initiated%20by%20a%0A%24b%24-quark%20or%20one%20%22boosted%22%20large-radius%20jet%20containing%20a%20merged%20shower%0Ainitiated%20by%20a%20%24b%5Cbar%7Bb%7D%24%20pair.%20The%20latter%20improves%20the%20reconstruction%0Aefficiency%20at%20high%20%24p_%7B%5Cmathrm%7BT%7D%7D%24.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneralization%20to%20the%20SPA-Net%20approach%20to%20simultaneously%20consider%20both%20boosted%0Aand%20resolved%20reconstruction%20possibilities%20and%20unambiguously%20interpret%20an%20event%0Aas%20%22fully%20resolved%27%27%2C%20%22fully%20boosted%22%2C%20or%20in%20between.%20We%20report%20the%20performance%0Aof%20baseline%20methods%2C%20the%20original%20SPA-Net%20approach%2C%20and%20our%20generalized%20version%0Aon%20nonresonant%20%24HH%24%20and%20%24HHH%24%20production%20at%20the%20LHC.%20Considering%20both%20boosted%0Aand%20resolved%20topologies%2C%20our%20SPA-Net%20approach%20increases%20the%20Higgs%20boson%0Areconstruction%20purity%20by%2057--62%5C%25%20and%20the%20efficiency%20by%2023--38%5C%25%20compared%20to%0Athe%20baseline%20method%20depending%20on%20the%20final%20state.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03819v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstruction%2520of%2520boosted%2520and%2520resolved%2520multi-Higgs-boson%2520events%2520with%250A%2520%2520symmetry-preserving%2520attention%2520networks%26entry.906535625%3DHaoyang%2520Li%2520and%2520Marko%2520Stamenkovic%2520and%2520Alexander%2520Shmakov%2520and%2520Michael%2520Fenton%2520and%2520Darius%2520Shih-Chieh%2520Chao%2520and%2520Kaitlyn%2520Maiya%2520White%2520and%2520Caden%2520Mikkelsen%2520and%2520Jovan%2520Mitic%2520and%2520Cristina%2520Mantilla%2520Suarez%2520and%2520Melissa%2520Quinnan%2520and%2520Greg%2520Landsberg%2520and%2520Harvey%2520Newman%2520and%2520Pierre%2520Baldi%2520and%2520Daniel%2520Whiteson%2520and%2520Javier%2520Duarte%26entry.1292438233%3D%2520%2520The%2520production%2520of%2520multiple%2520Higgs%2520bosons%2520at%2520the%2520CERN%2520LHC%2520provides%2520a%2520direct%2520way%250Ato%2520measure%2520the%2520trilinear%2520and%2520quartic%2520Higgs%2520self-interaction%2520strengths%2520as%2520well%250Aas%2520potential%2520access%2520to%2520beyond%2520the%2520standard%2520model%2520effects%2520that%2520can%2520enhance%250Aproduction%2520at%2520large%2520transverse%2520momentum%2520%2524p_%257B%255Cmathrm%257BT%257D%257D%2524.%2520The%2520largest%2520event%250Afraction%2520arises%2520from%2520the%2520fully%2520hadronic%2520final%2520state%2520in%2520which%2520every%2520Higgs%2520boson%250Adecays%2520to%2520a%2520bottom%2520quark-antiquark%2520pair%2520%2528%2524b%255Cbar%257Bb%257D%2524%2529.%2520This%2520introduces%2520a%250Acombinatorial%2520challenge%2520known%2520as%2520the%2520%255Cemph%257Bjet%2520assignment%2520problem%257D%253A%2520assigning%250Ajets%2520to%2520sets%2520representing%2520Higgs%2520boson%2520candidates.%2520Symmetry-preserving%2520attention%250Anetworks%2520%2528SPA-Nets%2529%2520have%2520been%2520been%2520developed%2520to%2520address%2520this%2520challenge.%250AHowever%252C%2520the%2520complexity%2520of%2520jet%2520assignment%2520increases%2520when%2520simultaneously%250Aconsidering%2520both%2520%2524H%255Crightarrow%2520b%255Cbar%257Bb%257D%2524%2520reconstruction%2520possibilities%252C%2520i.e.%252C%250Atwo%2520%2522resolved%2522%2520small-radius%2520jets%2520each%2520containing%2520a%2520shower%2520initiated%2520by%2520a%250A%2524b%2524-quark%2520or%2520one%2520%2522boosted%2522%2520large-radius%2520jet%2520containing%2520a%2520merged%2520shower%250Ainitiated%2520by%2520a%2520%2524b%255Cbar%257Bb%257D%2524%2520pair.%2520The%2520latter%2520improves%2520the%2520reconstruction%250Aefficiency%2520at%2520high%2520%2524p_%257B%255Cmathrm%257BT%257D%257D%2524.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Ageneralization%2520to%2520the%2520SPA-Net%2520approach%2520to%2520simultaneously%2520consider%2520both%2520boosted%250Aand%2520resolved%2520reconstruction%2520possibilities%2520and%2520unambiguously%2520interpret%2520an%2520event%250Aas%2520%2522fully%2520resolved%2527%2527%252C%2520%2522fully%2520boosted%2522%252C%2520or%2520in%2520between.%2520We%2520report%2520the%2520performance%250Aof%2520baseline%2520methods%252C%2520the%2520original%2520SPA-Net%2520approach%252C%2520and%2520our%2520generalized%2520version%250Aon%2520nonresonant%2520%2524HH%2524%2520and%2520%2524HHH%2524%2520production%2520at%2520the%2520LHC.%2520Considering%2520both%2520boosted%250Aand%2520resolved%2520topologies%252C%2520our%2520SPA-Net%2520approach%2520increases%2520the%2520Higgs%2520boson%250Areconstruction%2520purity%2520by%252057--62%255C%2525%2520and%2520the%2520efficiency%2520by%252023--38%255C%2525%2520compared%2520to%250Athe%2520baseline%2520method%2520depending%2520on%2520the%2520final%2520state.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03819v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstruction%20of%20boosted%20and%20resolved%20multi-Higgs-boson%20events%20with%0A%20%20symmetry-preserving%20attention%20networks&entry.906535625=Haoyang%20Li%20and%20Marko%20Stamenkovic%20and%20Alexander%20Shmakov%20and%20Michael%20Fenton%20and%20Darius%20Shih-Chieh%20Chao%20and%20Kaitlyn%20Maiya%20White%20and%20Caden%20Mikkelsen%20and%20Jovan%20Mitic%20and%20Cristina%20Mantilla%20Suarez%20and%20Melissa%20Quinnan%20and%20Greg%20Landsberg%20and%20Harvey%20Newman%20and%20Pierre%20Baldi%20and%20Daniel%20Whiteson%20and%20Javier%20Duarte&entry.1292438233=%20%20The%20production%20of%20multiple%20Higgs%20bosons%20at%20the%20CERN%20LHC%20provides%20a%20direct%20way%0Ato%20measure%20the%20trilinear%20and%20quartic%20Higgs%20self-interaction%20strengths%20as%20well%0Aas%20potential%20access%20to%20beyond%20the%20standard%20model%20effects%20that%20can%20enhance%0Aproduction%20at%20large%20transverse%20momentum%20%24p_%7B%5Cmathrm%7BT%7D%7D%24.%20The%20largest%20event%0Afraction%20arises%20from%20the%20fully%20hadronic%20final%20state%20in%20which%20every%20Higgs%20boson%0Adecays%20to%20a%20bottom%20quark-antiquark%20pair%20%28%24b%5Cbar%7Bb%7D%24%29.%20This%20introduces%20a%0Acombinatorial%20challenge%20known%20as%20the%20%5Cemph%7Bjet%20assignment%20problem%7D%3A%20assigning%0Ajets%20to%20sets%20representing%20Higgs%20boson%20candidates.%20Symmetry-preserving%20attention%0Anetworks%20%28SPA-Nets%29%20have%20been%20been%20developed%20to%20address%20this%20challenge.%0AHowever%2C%20the%20complexity%20of%20jet%20assignment%20increases%20when%20simultaneously%0Aconsidering%20both%20%24H%5Crightarrow%20b%5Cbar%7Bb%7D%24%20reconstruction%20possibilities%2C%20i.e.%2C%0Atwo%20%22resolved%22%20small-radius%20jets%20each%20containing%20a%20shower%20initiated%20by%20a%0A%24b%24-quark%20or%20one%20%22boosted%22%20large-radius%20jet%20containing%20a%20merged%20shower%0Ainitiated%20by%20a%20%24b%5Cbar%7Bb%7D%24%20pair.%20The%20latter%20improves%20the%20reconstruction%0Aefficiency%20at%20high%20%24p_%7B%5Cmathrm%7BT%7D%7D%24.%20In%20this%20work%2C%20we%20introduce%20a%0Ageneralization%20to%20the%20SPA-Net%20approach%20to%20simultaneously%20consider%20both%20boosted%0Aand%20resolved%20reconstruction%20possibilities%20and%20unambiguously%20interpret%20an%20event%0Aas%20%22fully%20resolved%27%27%2C%20%22fully%20boosted%22%2C%20or%20in%20between.%20We%20report%20the%20performance%0Aof%20baseline%20methods%2C%20the%20original%20SPA-Net%20approach%2C%20and%20our%20generalized%20version%0Aon%20nonresonant%20%24HH%24%20and%20%24HHH%24%20production%20at%20the%20LHC.%20Considering%20both%20boosted%0Aand%20resolved%20topologies%2C%20our%20SPA-Net%20approach%20increases%20the%20Higgs%20boson%0Areconstruction%20purity%20by%2057--62%5C%25%20and%20the%20efficiency%20by%2023--38%5C%25%20compared%20to%0Athe%20baseline%20method%20depending%20on%20the%20final%20state.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03819v3&entry.124074799=Read"},
{"title": "Investigating the Design Space of Visual Grounding in Multimodal Large\n  Language Model", "author": "Weitai Kang and Weiming Zhuang and Zhizhong Li and Yan Yan and Lingjuan Lyu", "abstract": "  Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.\n", "link": "http://arxiv.org/abs/2508.08066v1", "date": "2025-08-11", "relevancy": 2.2876, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.572}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Design%20Space%20of%20Visual%20Grounding%20in%20Multimodal%20Large%0A%20%20Language%20Model&body=Title%3A%20Investigating%20the%20Design%20Space%20of%20Visual%20Grounding%20in%20Multimodal%20Large%0A%20%20Language%20Model%0AAuthor%3A%20Weitai%20Kang%20and%20Weiming%20Zhuang%20and%20Zhizhong%20Li%20and%20Yan%20Yan%20and%20Lingjuan%20Lyu%0AAbstract%3A%20%20%20Fine-grained%20multimodal%20capability%20in%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20has%20emerged%20as%20a%20critical%20research%20direction%2C%20particularly%20for%20tackling%0Athe%20visual%20grounding%20%28VG%29%20problem.%20Despite%20the%20strong%20performance%20achieved%20by%0Aexisting%20approaches%2C%20they%20often%20employ%20disparate%20design%20choices%20when%0Afine-tuning%20MLLMs%20for%20VG%2C%20lacking%20systematic%20verification%20to%20support%20these%0Adesigns.%20To%20bridge%20this%20gap%2C%20this%20paper%20presents%20a%20comprehensive%20study%20of%0Avarious%20design%20choices%20that%20impact%20the%20VG%20performance%20of%20MLLMs.%20We%20conduct%20our%0Aanalysis%20using%20LLaVA-1.5%2C%20which%20has%20been%20widely%20adopted%20in%20prior%20empirical%0Astudies%20of%20MLLMs.%20While%20more%20recent%20models%20exist%2C%20we%20follow%20this%20convention%20to%0Aensure%20our%20findings%20remain%20broadly%20applicable%20and%20extendable%20to%20other%0Aarchitectures.%20We%20cover%20two%20key%20aspects%3A%20%281%29%20exploring%20different%20visual%0Agrounding%20paradigms%20in%20MLLMs%2C%20identifying%20the%20most%20effective%20design%2C%20and%0Aproviding%20our%20insights%3B%20and%20%282%29%20conducting%20ablation%20studies%20on%20the%20design%20of%0Agrounding%20data%20to%20optimize%20MLLMs%27%20fine-tuning%20for%20the%20VG%20task.%20Finally%2C%20our%0Afindings%20contribute%20to%20a%20stronger%20MLLM%20for%20VG%2C%20achieving%20improvements%20of%20%2B5.6%25%0A/%20%2B6.9%25%20/%20%2B7.0%25%20on%20RefCOCO/%2B/g%20over%20the%20LLaVA-1.5.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Design%2520Space%2520of%2520Visual%2520Grounding%2520in%2520Multimodal%2520Large%250A%2520%2520Language%2520Model%26entry.906535625%3DWeitai%2520Kang%2520and%2520Weiming%2520Zhuang%2520and%2520Zhizhong%2520Li%2520and%2520Yan%2520Yan%2520and%2520Lingjuan%2520Lyu%26entry.1292438233%3D%2520%2520Fine-grained%2520multimodal%2520capability%2520in%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520has%2520emerged%2520as%2520a%2520critical%2520research%2520direction%252C%2520particularly%2520for%2520tackling%250Athe%2520visual%2520grounding%2520%2528VG%2529%2520problem.%2520Despite%2520the%2520strong%2520performance%2520achieved%2520by%250Aexisting%2520approaches%252C%2520they%2520often%2520employ%2520disparate%2520design%2520choices%2520when%250Afine-tuning%2520MLLMs%2520for%2520VG%252C%2520lacking%2520systematic%2520verification%2520to%2520support%2520these%250Adesigns.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%2520study%2520of%250Avarious%2520design%2520choices%2520that%2520impact%2520the%2520VG%2520performance%2520of%2520MLLMs.%2520We%2520conduct%2520our%250Aanalysis%2520using%2520LLaVA-1.5%252C%2520which%2520has%2520been%2520widely%2520adopted%2520in%2520prior%2520empirical%250Astudies%2520of%2520MLLMs.%2520While%2520more%2520recent%2520models%2520exist%252C%2520we%2520follow%2520this%2520convention%2520to%250Aensure%2520our%2520findings%2520remain%2520broadly%2520applicable%2520and%2520extendable%2520to%2520other%250Aarchitectures.%2520We%2520cover%2520two%2520key%2520aspects%253A%2520%25281%2529%2520exploring%2520different%2520visual%250Agrounding%2520paradigms%2520in%2520MLLMs%252C%2520identifying%2520the%2520most%2520effective%2520design%252C%2520and%250Aproviding%2520our%2520insights%253B%2520and%2520%25282%2529%2520conducting%2520ablation%2520studies%2520on%2520the%2520design%2520of%250Agrounding%2520data%2520to%2520optimize%2520MLLMs%2527%2520fine-tuning%2520for%2520the%2520VG%2520task.%2520Finally%252C%2520our%250Afindings%2520contribute%2520to%2520a%2520stronger%2520MLLM%2520for%2520VG%252C%2520achieving%2520improvements%2520of%2520%252B5.6%2525%250A/%2520%252B6.9%2525%2520/%2520%252B7.0%2525%2520on%2520RefCOCO/%252B/g%2520over%2520the%2520LLaVA-1.5.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Design%20Space%20of%20Visual%20Grounding%20in%20Multimodal%20Large%0A%20%20Language%20Model&entry.906535625=Weitai%20Kang%20and%20Weiming%20Zhuang%20and%20Zhizhong%20Li%20and%20Yan%20Yan%20and%20Lingjuan%20Lyu&entry.1292438233=%20%20Fine-grained%20multimodal%20capability%20in%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20has%20emerged%20as%20a%20critical%20research%20direction%2C%20particularly%20for%20tackling%0Athe%20visual%20grounding%20%28VG%29%20problem.%20Despite%20the%20strong%20performance%20achieved%20by%0Aexisting%20approaches%2C%20they%20often%20employ%20disparate%20design%20choices%20when%0Afine-tuning%20MLLMs%20for%20VG%2C%20lacking%20systematic%20verification%20to%20support%20these%0Adesigns.%20To%20bridge%20this%20gap%2C%20this%20paper%20presents%20a%20comprehensive%20study%20of%0Avarious%20design%20choices%20that%20impact%20the%20VG%20performance%20of%20MLLMs.%20We%20conduct%20our%0Aanalysis%20using%20LLaVA-1.5%2C%20which%20has%20been%20widely%20adopted%20in%20prior%20empirical%0Astudies%20of%20MLLMs.%20While%20more%20recent%20models%20exist%2C%20we%20follow%20this%20convention%20to%0Aensure%20our%20findings%20remain%20broadly%20applicable%20and%20extendable%20to%20other%0Aarchitectures.%20We%20cover%20two%20key%20aspects%3A%20%281%29%20exploring%20different%20visual%0Agrounding%20paradigms%20in%20MLLMs%2C%20identifying%20the%20most%20effective%20design%2C%20and%0Aproviding%20our%20insights%3B%20and%20%282%29%20conducting%20ablation%20studies%20on%20the%20design%20of%0Agrounding%20data%20to%20optimize%20MLLMs%27%20fine-tuning%20for%20the%20VG%20task.%20Finally%2C%20our%0Afindings%20contribute%20to%20a%20stronger%20MLLM%20for%20VG%2C%20achieving%20improvements%20of%20%2B5.6%25%0A/%20%2B6.9%25%20/%20%2B7.0%25%20on%20RefCOCO/%2B/g%20over%20the%20LLaVA-1.5.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08066v1&entry.124074799=Read"},
{"title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language\n  Model-based Question Answering", "author": "Xing Zi and Jinghao Xiao and Yunxiao Shi and Xian Tao and Jun Li and Ali Braytee and Mukesh Prasad", "abstract": "  Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.\n", "link": "http://arxiv.org/abs/2508.07918v1", "date": "2025-08-11", "relevancy": 2.2805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5841}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSVLM-QA%3A%20A%20Benchmark%20Dataset%20for%20Remote%20Sensing%20Vision%20Language%0A%20%20Model-based%20Question%20Answering&body=Title%3A%20RSVLM-QA%3A%20A%20Benchmark%20Dataset%20for%20Remote%20Sensing%20Vision%20Language%0A%20%20Model-based%20Question%20Answering%0AAuthor%3A%20Xing%20Zi%20and%20Jinghao%20Xiao%20and%20Yunxiao%20Shi%20and%20Xian%20Tao%20and%20Jun%20Li%20and%20Ali%20Braytee%20and%20Mukesh%20Prasad%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20in%20remote%20sensing%20%28RS%29%20is%20pivotal%20for%0Ainterpreting%20Earth%20observation%20data.%20However%2C%20existing%20RS%20VQA%20datasets%20are%0Aconstrained%20by%20limitations%20in%20annotation%20richness%2C%20question%20diversity%2C%20and%20the%0Aassessment%20of%20specific%20reasoning%20capabilities.%20This%20paper%20introduces%20RSVLM-QA%0Adataset%2C%20a%20new%20large-scale%2C%20content-rich%20VQA%20dataset%20for%20the%20RS%20domain.%0ARSVLM-QA%20is%20constructed%20by%20integrating%20data%20from%20several%20prominent%20RS%0Asegmentation%20and%20detection%20datasets%3A%20WHU%2C%20LoveDA%2C%20INRIA%2C%20and%20iSAID.%20We%20employ%0Aan%20innovative%20dual-track%20annotation%20generation%20pipeline.%20Firstly%2C%20we%20leverage%0ALarge%20Language%20Models%20%28LLMs%29%2C%20specifically%20GPT-4.1%2C%20with%20meticulously%20designed%0Aprompts%20to%20automatically%20generate%20a%20suite%20of%20detailed%20annotations%20including%0Aimage%20captions%2C%20spatial%20relations%2C%20and%20semantic%20tags%2C%20alongside%20complex%0Acaption-based%20VQA%20pairs.%20Secondly%2C%20to%20address%20the%20challenging%20task%20of%20object%0Acounting%20in%20RS%20imagery%2C%20we%20have%20developed%20a%20specialized%20automated%20process%20that%0Aextracts%20object%20counts%20directly%20from%20the%20original%20segmentation%20data%3B%20GPT-4.1%0Athen%20formulates%20natural%20language%20answers%20from%20these%20counts%2C%20which%20are%20paired%0Awith%20preset%20question%20templates%20to%20create%20counting%20QA%20pairs.%20RSVLM-QA%20comprises%0A13%2C820%20images%20and%20162%2C373%20VQA%20pairs%2C%20featuring%20extensive%20annotations%20and%0Adiverse%20question%20types.%20We%20provide%20a%20detailed%20statistical%20analysis%20of%20the%0Adataset%20and%20a%20comparison%20with%20existing%20RS%20VQA%20benchmarks%2C%20highlighting%20the%0Asuperior%20depth%20and%20breadth%20of%20RSVLM-QA%27s%20annotations.%20Furthermore%2C%20we%20conduct%0Abenchmark%20experiments%20on%20Six%20mainstream%20Vision%20Language%20Models%20%28VLMs%29%2C%0Ademonstrating%20that%20RSVLM-QA%20effectively%20evaluates%20and%20challenges%20the%0Aunderstanding%20and%20reasoning%20abilities%20of%20current%20VLMs%20in%20the%20RS%20domain.%20We%0Abelieve%20RSVLM-QA%20will%20serve%20as%20a%20pivotal%20resource%20for%20the%20RS%20VQA%20and%20VLM%0Aresearch%20communities%2C%20poised%20to%20catalyze%20advancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSVLM-QA%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Remote%2520Sensing%2520Vision%2520Language%250A%2520%2520Model-based%2520Question%2520Answering%26entry.906535625%3DXing%2520Zi%2520and%2520Jinghao%2520Xiao%2520and%2520Yunxiao%2520Shi%2520and%2520Xian%2520Tao%2520and%2520Jun%2520Li%2520and%2520Ali%2520Braytee%2520and%2520Mukesh%2520Prasad%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520in%2520remote%2520sensing%2520%2528RS%2529%2520is%2520pivotal%2520for%250Ainterpreting%2520Earth%2520observation%2520data.%2520However%252C%2520existing%2520RS%2520VQA%2520datasets%2520are%250Aconstrained%2520by%2520limitations%2520in%2520annotation%2520richness%252C%2520question%2520diversity%252C%2520and%2520the%250Aassessment%2520of%2520specific%2520reasoning%2520capabilities.%2520This%2520paper%2520introduces%2520RSVLM-QA%250Adataset%252C%2520a%2520new%2520large-scale%252C%2520content-rich%2520VQA%2520dataset%2520for%2520the%2520RS%2520domain.%250ARSVLM-QA%2520is%2520constructed%2520by%2520integrating%2520data%2520from%2520several%2520prominent%2520RS%250Asegmentation%2520and%2520detection%2520datasets%253A%2520WHU%252C%2520LoveDA%252C%2520INRIA%252C%2520and%2520iSAID.%2520We%2520employ%250Aan%2520innovative%2520dual-track%2520annotation%2520generation%2520pipeline.%2520Firstly%252C%2520we%2520leverage%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520specifically%2520GPT-4.1%252C%2520with%2520meticulously%2520designed%250Aprompts%2520to%2520automatically%2520generate%2520a%2520suite%2520of%2520detailed%2520annotations%2520including%250Aimage%2520captions%252C%2520spatial%2520relations%252C%2520and%2520semantic%2520tags%252C%2520alongside%2520complex%250Acaption-based%2520VQA%2520pairs.%2520Secondly%252C%2520to%2520address%2520the%2520challenging%2520task%2520of%2520object%250Acounting%2520in%2520RS%2520imagery%252C%2520we%2520have%2520developed%2520a%2520specialized%2520automated%2520process%2520that%250Aextracts%2520object%2520counts%2520directly%2520from%2520the%2520original%2520segmentation%2520data%253B%2520GPT-4.1%250Athen%2520formulates%2520natural%2520language%2520answers%2520from%2520these%2520counts%252C%2520which%2520are%2520paired%250Awith%2520preset%2520question%2520templates%2520to%2520create%2520counting%2520QA%2520pairs.%2520RSVLM-QA%2520comprises%250A13%252C820%2520images%2520and%2520162%252C373%2520VQA%2520pairs%252C%2520featuring%2520extensive%2520annotations%2520and%250Adiverse%2520question%2520types.%2520We%2520provide%2520a%2520detailed%2520statistical%2520analysis%2520of%2520the%250Adataset%2520and%2520a%2520comparison%2520with%2520existing%2520RS%2520VQA%2520benchmarks%252C%2520highlighting%2520the%250Asuperior%2520depth%2520and%2520breadth%2520of%2520RSVLM-QA%2527s%2520annotations.%2520Furthermore%252C%2520we%2520conduct%250Abenchmark%2520experiments%2520on%2520Six%2520mainstream%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%250Ademonstrating%2520that%2520RSVLM-QA%2520effectively%2520evaluates%2520and%2520challenges%2520the%250Aunderstanding%2520and%2520reasoning%2520abilities%2520of%2520current%2520VLMs%2520in%2520the%2520RS%2520domain.%2520We%250Abelieve%2520RSVLM-QA%2520will%2520serve%2520as%2520a%2520pivotal%2520resource%2520for%2520the%2520RS%2520VQA%2520and%2520VLM%250Aresearch%2520communities%252C%2520poised%2520to%2520catalyze%2520advancements%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSVLM-QA%3A%20A%20Benchmark%20Dataset%20for%20Remote%20Sensing%20Vision%20Language%0A%20%20Model-based%20Question%20Answering&entry.906535625=Xing%20Zi%20and%20Jinghao%20Xiao%20and%20Yunxiao%20Shi%20and%20Xian%20Tao%20and%20Jun%20Li%20and%20Ali%20Braytee%20and%20Mukesh%20Prasad&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20in%20remote%20sensing%20%28RS%29%20is%20pivotal%20for%0Ainterpreting%20Earth%20observation%20data.%20However%2C%20existing%20RS%20VQA%20datasets%20are%0Aconstrained%20by%20limitations%20in%20annotation%20richness%2C%20question%20diversity%2C%20and%20the%0Aassessment%20of%20specific%20reasoning%20capabilities.%20This%20paper%20introduces%20RSVLM-QA%0Adataset%2C%20a%20new%20large-scale%2C%20content-rich%20VQA%20dataset%20for%20the%20RS%20domain.%0ARSVLM-QA%20is%20constructed%20by%20integrating%20data%20from%20several%20prominent%20RS%0Asegmentation%20and%20detection%20datasets%3A%20WHU%2C%20LoveDA%2C%20INRIA%2C%20and%20iSAID.%20We%20employ%0Aan%20innovative%20dual-track%20annotation%20generation%20pipeline.%20Firstly%2C%20we%20leverage%0ALarge%20Language%20Models%20%28LLMs%29%2C%20specifically%20GPT-4.1%2C%20with%20meticulously%20designed%0Aprompts%20to%20automatically%20generate%20a%20suite%20of%20detailed%20annotations%20including%0Aimage%20captions%2C%20spatial%20relations%2C%20and%20semantic%20tags%2C%20alongside%20complex%0Acaption-based%20VQA%20pairs.%20Secondly%2C%20to%20address%20the%20challenging%20task%20of%20object%0Acounting%20in%20RS%20imagery%2C%20we%20have%20developed%20a%20specialized%20automated%20process%20that%0Aextracts%20object%20counts%20directly%20from%20the%20original%20segmentation%20data%3B%20GPT-4.1%0Athen%20formulates%20natural%20language%20answers%20from%20these%20counts%2C%20which%20are%20paired%0Awith%20preset%20question%20templates%20to%20create%20counting%20QA%20pairs.%20RSVLM-QA%20comprises%0A13%2C820%20images%20and%20162%2C373%20VQA%20pairs%2C%20featuring%20extensive%20annotations%20and%0Adiverse%20question%20types.%20We%20provide%20a%20detailed%20statistical%20analysis%20of%20the%0Adataset%20and%20a%20comparison%20with%20existing%20RS%20VQA%20benchmarks%2C%20highlighting%20the%0Asuperior%20depth%20and%20breadth%20of%20RSVLM-QA%27s%20annotations.%20Furthermore%2C%20we%20conduct%0Abenchmark%20experiments%20on%20Six%20mainstream%20Vision%20Language%20Models%20%28VLMs%29%2C%0Ademonstrating%20that%20RSVLM-QA%20effectively%20evaluates%20and%20challenges%20the%0Aunderstanding%20and%20reasoning%20abilities%20of%20current%20VLMs%20in%20the%20RS%20domain.%20We%0Abelieve%20RSVLM-QA%20will%20serve%20as%20a%20pivotal%20resource%20for%20the%20RS%20VQA%20and%20VLM%0Aresearch%20communities%2C%20poised%20to%20catalyze%20advancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07918v1&entry.124074799=Read"},
{"title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from\n  Clinical Thought to Pixel-Level Precision", "author": "Zhonghao Yan and Muxi Diao and Yuxuan Yang and Jiayuan Xu and Kaizhou Zhang and Ruoyan Jing and Lele Yang and Yanxi Liu and Kongming Liang and Zhanyu Ma", "abstract": "  Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.\n", "link": "http://arxiv.org/abs/2508.08177v1", "date": "2025-08-11", "relevancy": 2.261, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedReasoner%3A%20Reinforcement%20Learning%20Drives%20Reasoning%20Grounding%20from%0A%20%20Clinical%20Thought%20to%20Pixel-Level%20Precision&body=Title%3A%20MedReasoner%3A%20Reinforcement%20Learning%20Drives%20Reasoning%20Grounding%20from%0A%20%20Clinical%20Thought%20to%20Pixel-Level%20Precision%0AAuthor%3A%20Zhonghao%20Yan%20and%20Muxi%20Diao%20and%20Yuxuan%20Yang%20and%20Jiayuan%20Xu%20and%20Kaizhou%20Zhang%20and%20Ruoyan%20Jing%20and%20Lele%20Yang%20and%20Yanxi%20Liu%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma%0AAbstract%3A%20%20%20Accurately%20grounding%20regions%20of%20interest%20%28ROIs%29%20is%20critical%20for%20diagnosis%20and%0Atreatment%20planning%20in%20medical%20imaging.%20While%20multimodal%20large%20language%20models%0A%28MLLMs%29%20combine%20visual%20perception%20with%20natural%20language%2C%20current%0Amedical-grounding%20pipelines%20still%20rely%20on%20supervised%20fine-tuning%20with%20explicit%0Aspatial%20hints%2C%20making%20them%20ill-equipped%20to%20handle%20the%20implicit%20queries%20common%0Ain%20clinical%20practice.%20This%20work%20makes%20three%20core%20contributions.%20We%20first%20define%0AUnified%20Medical%20Reasoning%20Grounding%20%28UMRG%29%2C%20a%20novel%20vision-language%20task%20that%0Ademands%20clinical%20reasoning%20and%20pixel-level%20grounding.%20Second%2C%20we%20release%0AU-MRG-14K%2C%20a%20dataset%20of%2014K%20samples%20featuring%20pixel-level%20masks%20alongside%0Aimplicit%20clinical%20queries%20and%20reasoning%20traces%2C%20spanning%2010%20modalities%2C%2015%0Asuper-categories%2C%20and%20108%20specific%20categories.%20Finally%2C%20we%20introduce%0AMedReasoner%2C%20a%20modular%20framework%20that%20distinctly%20separates%20reasoning%20from%0Asegmentation%3A%20an%20MLLM%20reasoner%20is%20optimized%20with%20reinforcement%20learning%2C%20while%0Aa%20frozen%20segmentation%20expert%20converts%20spatial%20prompts%20into%20masks%2C%20with%0Aalignment%20achieved%20through%20format%20and%20accuracy%20rewards.%20MedReasoner%20achieves%0Astate-of-the-art%20performance%20on%20U-MRG-14K%20and%20demonstrates%20strong%0Ageneralization%20to%20unseen%20clinical%20queries%2C%20underscoring%20the%20significant%20promise%0Aof%20reinforcement%20learning%20for%20interpretable%20medical%20grounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedReasoner%253A%2520Reinforcement%2520Learning%2520Drives%2520Reasoning%2520Grounding%2520from%250A%2520%2520Clinical%2520Thought%2520to%2520Pixel-Level%2520Precision%26entry.906535625%3DZhonghao%2520Yan%2520and%2520Muxi%2520Diao%2520and%2520Yuxuan%2520Yang%2520and%2520Jiayuan%2520Xu%2520and%2520Kaizhou%2520Zhang%2520and%2520Ruoyan%2520Jing%2520and%2520Lele%2520Yang%2520and%2520Yanxi%2520Liu%2520and%2520Kongming%2520Liang%2520and%2520Zhanyu%2520Ma%26entry.1292438233%3D%2520%2520Accurately%2520grounding%2520regions%2520of%2520interest%2520%2528ROIs%2529%2520is%2520critical%2520for%2520diagnosis%2520and%250Atreatment%2520planning%2520in%2520medical%2520imaging.%2520While%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520combine%2520visual%2520perception%2520with%2520natural%2520language%252C%2520current%250Amedical-grounding%2520pipelines%2520still%2520rely%2520on%2520supervised%2520fine-tuning%2520with%2520explicit%250Aspatial%2520hints%252C%2520making%2520them%2520ill-equipped%2520to%2520handle%2520the%2520implicit%2520queries%2520common%250Ain%2520clinical%2520practice.%2520This%2520work%2520makes%2520three%2520core%2520contributions.%2520We%2520first%2520define%250AUnified%2520Medical%2520Reasoning%2520Grounding%2520%2528UMRG%2529%252C%2520a%2520novel%2520vision-language%2520task%2520that%250Ademands%2520clinical%2520reasoning%2520and%2520pixel-level%2520grounding.%2520Second%252C%2520we%2520release%250AU-MRG-14K%252C%2520a%2520dataset%2520of%252014K%2520samples%2520featuring%2520pixel-level%2520masks%2520alongside%250Aimplicit%2520clinical%2520queries%2520and%2520reasoning%2520traces%252C%2520spanning%252010%2520modalities%252C%252015%250Asuper-categories%252C%2520and%2520108%2520specific%2520categories.%2520Finally%252C%2520we%2520introduce%250AMedReasoner%252C%2520a%2520modular%2520framework%2520that%2520distinctly%2520separates%2520reasoning%2520from%250Asegmentation%253A%2520an%2520MLLM%2520reasoner%2520is%2520optimized%2520with%2520reinforcement%2520learning%252C%2520while%250Aa%2520frozen%2520segmentation%2520expert%2520converts%2520spatial%2520prompts%2520into%2520masks%252C%2520with%250Aalignment%2520achieved%2520through%2520format%2520and%2520accuracy%2520rewards.%2520MedReasoner%2520achieves%250Astate-of-the-art%2520performance%2520on%2520U-MRG-14K%2520and%2520demonstrates%2520strong%250Ageneralization%2520to%2520unseen%2520clinical%2520queries%252C%2520underscoring%2520the%2520significant%2520promise%250Aof%2520reinforcement%2520learning%2520for%2520interpretable%2520medical%2520grounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedReasoner%3A%20Reinforcement%20Learning%20Drives%20Reasoning%20Grounding%20from%0A%20%20Clinical%20Thought%20to%20Pixel-Level%20Precision&entry.906535625=Zhonghao%20Yan%20and%20Muxi%20Diao%20and%20Yuxuan%20Yang%20and%20Jiayuan%20Xu%20and%20Kaizhou%20Zhang%20and%20Ruoyan%20Jing%20and%20Lele%20Yang%20and%20Yanxi%20Liu%20and%20Kongming%20Liang%20and%20Zhanyu%20Ma&entry.1292438233=%20%20Accurately%20grounding%20regions%20of%20interest%20%28ROIs%29%20is%20critical%20for%20diagnosis%20and%0Atreatment%20planning%20in%20medical%20imaging.%20While%20multimodal%20large%20language%20models%0A%28MLLMs%29%20combine%20visual%20perception%20with%20natural%20language%2C%20current%0Amedical-grounding%20pipelines%20still%20rely%20on%20supervised%20fine-tuning%20with%20explicit%0Aspatial%20hints%2C%20making%20them%20ill-equipped%20to%20handle%20the%20implicit%20queries%20common%0Ain%20clinical%20practice.%20This%20work%20makes%20three%20core%20contributions.%20We%20first%20define%0AUnified%20Medical%20Reasoning%20Grounding%20%28UMRG%29%2C%20a%20novel%20vision-language%20task%20that%0Ademands%20clinical%20reasoning%20and%20pixel-level%20grounding.%20Second%2C%20we%20release%0AU-MRG-14K%2C%20a%20dataset%20of%2014K%20samples%20featuring%20pixel-level%20masks%20alongside%0Aimplicit%20clinical%20queries%20and%20reasoning%20traces%2C%20spanning%2010%20modalities%2C%2015%0Asuper-categories%2C%20and%20108%20specific%20categories.%20Finally%2C%20we%20introduce%0AMedReasoner%2C%20a%20modular%20framework%20that%20distinctly%20separates%20reasoning%20from%0Asegmentation%3A%20an%20MLLM%20reasoner%20is%20optimized%20with%20reinforcement%20learning%2C%20while%0Aa%20frozen%20segmentation%20expert%20converts%20spatial%20prompts%20into%20masks%2C%20with%0Aalignment%20achieved%20through%20format%20and%20accuracy%20rewards.%20MedReasoner%20achieves%0Astate-of-the-art%20performance%20on%20U-MRG-14K%20and%20demonstrates%20strong%0Ageneralization%20to%20unseen%20clinical%20queries%2C%20underscoring%20the%20significant%20promise%0Aof%20reinforcement%20learning%20for%20interpretable%20medical%20grounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08177v1&entry.124074799=Read"},
{"title": "MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge\n  Tracing", "author": "Mingrong Lin and Ke Deng and Zhengyang Wu and Zetao Zheng and Jie Li", "abstract": "  Knowledge Tracing (KT) is committed to capturing students' knowledge mastery\nfrom their historical interactions. Simulating students' memory states is a\npromising approach to enhance both the performance and interpretability of\nknowledge tracing models. Memory consists of three fundamental processes:\nencoding, storage, and retrieval. Although forgetting primarily manifests\nduring the storage stage, most existing studies rely on a single,\nundifferentiated forgetting mechanism, overlooking other memory processes as\nwell as personalized forgetting patterns. To address this, this paper proposes\nmemoryKT, a knowledge tracing model based on a novel temporal variational\nautoencoder. The model simulates memory dynamics through a three-stage process:\n(i) Learning the distribution of students' knowledge memory features, (ii)\nReconstructing their exercise feedback, while (iii) Embedding a personalized\nforgetting module within the temporal workflow to dynamically modulate memory\nstorage strength. This jointly models the complete encoding-storage-retrieval\ncycle, significantly enhancing the model's perception capability for individual\ndifferences. Extensive experiments on four public datasets demonstrate that our\nproposed approach significantly outperforms state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2508.08122v1", "date": "2025-08-11", "relevancy": 2.2602, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MemoryKT%3A%20An%20Integrative%20Memory-and-Forgetting%20Method%20for%20Knowledge%0A%20%20Tracing&body=Title%3A%20MemoryKT%3A%20An%20Integrative%20Memory-and-Forgetting%20Method%20for%20Knowledge%0A%20%20Tracing%0AAuthor%3A%20Mingrong%20Lin%20and%20Ke%20Deng%20and%20Zhengyang%20Wu%20and%20Zetao%20Zheng%20and%20Jie%20Li%0AAbstract%3A%20%20%20Knowledge%20Tracing%20%28KT%29%20is%20committed%20to%20capturing%20students%27%20knowledge%20mastery%0Afrom%20their%20historical%20interactions.%20Simulating%20students%27%20memory%20states%20is%20a%0Apromising%20approach%20to%20enhance%20both%20the%20performance%20and%20interpretability%20of%0Aknowledge%20tracing%20models.%20Memory%20consists%20of%20three%20fundamental%20processes%3A%0Aencoding%2C%20storage%2C%20and%20retrieval.%20Although%20forgetting%20primarily%20manifests%0Aduring%20the%20storage%20stage%2C%20most%20existing%20studies%20rely%20on%20a%20single%2C%0Aundifferentiated%20forgetting%20mechanism%2C%20overlooking%20other%20memory%20processes%20as%0Awell%20as%20personalized%20forgetting%20patterns.%20To%20address%20this%2C%20this%20paper%20proposes%0AmemoryKT%2C%20a%20knowledge%20tracing%20model%20based%20on%20a%20novel%20temporal%20variational%0Aautoencoder.%20The%20model%20simulates%20memory%20dynamics%20through%20a%20three-stage%20process%3A%0A%28i%29%20Learning%20the%20distribution%20of%20students%27%20knowledge%20memory%20features%2C%20%28ii%29%0AReconstructing%20their%20exercise%20feedback%2C%20while%20%28iii%29%20Embedding%20a%20personalized%0Aforgetting%20module%20within%20the%20temporal%20workflow%20to%20dynamically%20modulate%20memory%0Astorage%20strength.%20This%20jointly%20models%20the%20complete%20encoding-storage-retrieval%0Acycle%2C%20significantly%20enhancing%20the%20model%27s%20perception%20capability%20for%20individual%0Adifferences.%20Extensive%20experiments%20on%20four%20public%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20significantly%20outperforms%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemoryKT%253A%2520An%2520Integrative%2520Memory-and-Forgetting%2520Method%2520for%2520Knowledge%250A%2520%2520Tracing%26entry.906535625%3DMingrong%2520Lin%2520and%2520Ke%2520Deng%2520and%2520Zhengyang%2520Wu%2520and%2520Zetao%2520Zheng%2520and%2520Jie%2520Li%26entry.1292438233%3D%2520%2520Knowledge%2520Tracing%2520%2528KT%2529%2520is%2520committed%2520to%2520capturing%2520students%2527%2520knowledge%2520mastery%250Afrom%2520their%2520historical%2520interactions.%2520Simulating%2520students%2527%2520memory%2520states%2520is%2520a%250Apromising%2520approach%2520to%2520enhance%2520both%2520the%2520performance%2520and%2520interpretability%2520of%250Aknowledge%2520tracing%2520models.%2520Memory%2520consists%2520of%2520three%2520fundamental%2520processes%253A%250Aencoding%252C%2520storage%252C%2520and%2520retrieval.%2520Although%2520forgetting%2520primarily%2520manifests%250Aduring%2520the%2520storage%2520stage%252C%2520most%2520existing%2520studies%2520rely%2520on%2520a%2520single%252C%250Aundifferentiated%2520forgetting%2520mechanism%252C%2520overlooking%2520other%2520memory%2520processes%2520as%250Awell%2520as%2520personalized%2520forgetting%2520patterns.%2520To%2520address%2520this%252C%2520this%2520paper%2520proposes%250AmemoryKT%252C%2520a%2520knowledge%2520tracing%2520model%2520based%2520on%2520a%2520novel%2520temporal%2520variational%250Aautoencoder.%2520The%2520model%2520simulates%2520memory%2520dynamics%2520through%2520a%2520three-stage%2520process%253A%250A%2528i%2529%2520Learning%2520the%2520distribution%2520of%2520students%2527%2520knowledge%2520memory%2520features%252C%2520%2528ii%2529%250AReconstructing%2520their%2520exercise%2520feedback%252C%2520while%2520%2528iii%2529%2520Embedding%2520a%2520personalized%250Aforgetting%2520module%2520within%2520the%2520temporal%2520workflow%2520to%2520dynamically%2520modulate%2520memory%250Astorage%2520strength.%2520This%2520jointly%2520models%2520the%2520complete%2520encoding-storage-retrieval%250Acycle%252C%2520significantly%2520enhancing%2520the%2520model%2527s%2520perception%2520capability%2520for%2520individual%250Adifferences.%2520Extensive%2520experiments%2520on%2520four%2520public%2520datasets%2520demonstrate%2520that%2520our%250Aproposed%2520approach%2520significantly%2520outperforms%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MemoryKT%3A%20An%20Integrative%20Memory-and-Forgetting%20Method%20for%20Knowledge%0A%20%20Tracing&entry.906535625=Mingrong%20Lin%20and%20Ke%20Deng%20and%20Zhengyang%20Wu%20and%20Zetao%20Zheng%20and%20Jie%20Li&entry.1292438233=%20%20Knowledge%20Tracing%20%28KT%29%20is%20committed%20to%20capturing%20students%27%20knowledge%20mastery%0Afrom%20their%20historical%20interactions.%20Simulating%20students%27%20memory%20states%20is%20a%0Apromising%20approach%20to%20enhance%20both%20the%20performance%20and%20interpretability%20of%0Aknowledge%20tracing%20models.%20Memory%20consists%20of%20three%20fundamental%20processes%3A%0Aencoding%2C%20storage%2C%20and%20retrieval.%20Although%20forgetting%20primarily%20manifests%0Aduring%20the%20storage%20stage%2C%20most%20existing%20studies%20rely%20on%20a%20single%2C%0Aundifferentiated%20forgetting%20mechanism%2C%20overlooking%20other%20memory%20processes%20as%0Awell%20as%20personalized%20forgetting%20patterns.%20To%20address%20this%2C%20this%20paper%20proposes%0AmemoryKT%2C%20a%20knowledge%20tracing%20model%20based%20on%20a%20novel%20temporal%20variational%0Aautoencoder.%20The%20model%20simulates%20memory%20dynamics%20through%20a%20three-stage%20process%3A%0A%28i%29%20Learning%20the%20distribution%20of%20students%27%20knowledge%20memory%20features%2C%20%28ii%29%0AReconstructing%20their%20exercise%20feedback%2C%20while%20%28iii%29%20Embedding%20a%20personalized%0Aforgetting%20module%20within%20the%20temporal%20workflow%20to%20dynamically%20modulate%20memory%0Astorage%20strength.%20This%20jointly%20models%20the%20complete%20encoding-storage-retrieval%0Acycle%2C%20significantly%20enhancing%20the%20model%27s%20perception%20capability%20for%20individual%0Adifferences.%20Extensive%20experiments%20on%20four%20public%20datasets%20demonstrate%20that%20our%0Aproposed%20approach%20significantly%20outperforms%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08122v1&entry.124074799=Read"},
{"title": "Dynamic Layer Detection of Thin Materials using DenseTact Optical\n  Tactile Sensors", "author": "Ankush Kundan Dhawan and Camille Chungyoun and Karina Ting and Monroe Kennedy III", "abstract": "  Manipulation of thin materials is critical for many everyday tasks and\nremains a significant challenge for robots. While existing research has made\nstrides in tasks like material smoothing and folding, many studies struggle\nwith common failure modes (crumpled corners/edges, incorrect grasp\nconfigurations) that a preliminary step of layer detection could solve. We\npresent a novel method for classifying the number of grasped material layers\nusing a custom gripper equipped with DenseTact 2.0 optical tactile sensors.\nAfter grasping, the gripper performs an anthropomorphic rubbing motion while\ncollecting optical flow, 6-axis wrench, and joint state data. Using this data\nin a transformer-based network achieves a test accuracy of 98.21\\% in\nclassifying the number of grasped cloth layers, and 81.25\\% accuracy in\nclassifying layers of grasped paper, showing the effectiveness of our dynamic\nrubbing method. Evaluating different inputs and model architectures highlights\nthe usefulness of tactile sensor information and a transformer model for this\ntask. A comprehensive dataset of 568 labeled trials (368 for cloth and 200 for\npaper) was collected and made open-source along with this paper. Our project\npage is available at https://armlabstanford.github.io/dynamic-cloth-detection.\n", "link": "http://arxiv.org/abs/2409.09849v3", "date": "2025-08-11", "relevancy": 2.2588, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.578}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.567}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Layer%20Detection%20of%20Thin%20Materials%20using%20DenseTact%20Optical%0A%20%20Tactile%20Sensors&body=Title%3A%20Dynamic%20Layer%20Detection%20of%20Thin%20Materials%20using%20DenseTact%20Optical%0A%20%20Tactile%20Sensors%0AAuthor%3A%20Ankush%20Kundan%20Dhawan%20and%20Camille%20Chungyoun%20and%20Karina%20Ting%20and%20Monroe%20Kennedy%20III%0AAbstract%3A%20%20%20Manipulation%20of%20thin%20materials%20is%20critical%20for%20many%20everyday%20tasks%20and%0Aremains%20a%20significant%20challenge%20for%20robots.%20While%20existing%20research%20has%20made%0Astrides%20in%20tasks%20like%20material%20smoothing%20and%20folding%2C%20many%20studies%20struggle%0Awith%20common%20failure%20modes%20%28crumpled%20corners/edges%2C%20incorrect%20grasp%0Aconfigurations%29%20that%20a%20preliminary%20step%20of%20layer%20detection%20could%20solve.%20We%0Apresent%20a%20novel%20method%20for%20classifying%20the%20number%20of%20grasped%20material%20layers%0Ausing%20a%20custom%20gripper%20equipped%20with%20DenseTact%202.0%20optical%20tactile%20sensors.%0AAfter%20grasping%2C%20the%20gripper%20performs%20an%20anthropomorphic%20rubbing%20motion%20while%0Acollecting%20optical%20flow%2C%206-axis%20wrench%2C%20and%20joint%20state%20data.%20Using%20this%20data%0Ain%20a%20transformer-based%20network%20achieves%20a%20test%20accuracy%20of%2098.21%5C%25%20in%0Aclassifying%20the%20number%20of%20grasped%20cloth%20layers%2C%20and%2081.25%5C%25%20accuracy%20in%0Aclassifying%20layers%20of%20grasped%20paper%2C%20showing%20the%20effectiveness%20of%20our%20dynamic%0Arubbing%20method.%20Evaluating%20different%20inputs%20and%20model%20architectures%20highlights%0Athe%20usefulness%20of%20tactile%20sensor%20information%20and%20a%20transformer%20model%20for%20this%0Atask.%20A%20comprehensive%20dataset%20of%20568%20labeled%20trials%20%28368%20for%20cloth%20and%20200%20for%0Apaper%29%20was%20collected%20and%20made%20open-source%20along%20with%20this%20paper.%20Our%20project%0Apage%20is%20available%20at%20https%3A//armlabstanford.github.io/dynamic-cloth-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09849v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Layer%2520Detection%2520of%2520Thin%2520Materials%2520using%2520DenseTact%2520Optical%250A%2520%2520Tactile%2520Sensors%26entry.906535625%3DAnkush%2520Kundan%2520Dhawan%2520and%2520Camille%2520Chungyoun%2520and%2520Karina%2520Ting%2520and%2520Monroe%2520Kennedy%2520III%26entry.1292438233%3D%2520%2520Manipulation%2520of%2520thin%2520materials%2520is%2520critical%2520for%2520many%2520everyday%2520tasks%2520and%250Aremains%2520a%2520significant%2520challenge%2520for%2520robots.%2520While%2520existing%2520research%2520has%2520made%250Astrides%2520in%2520tasks%2520like%2520material%2520smoothing%2520and%2520folding%252C%2520many%2520studies%2520struggle%250Awith%2520common%2520failure%2520modes%2520%2528crumpled%2520corners/edges%252C%2520incorrect%2520grasp%250Aconfigurations%2529%2520that%2520a%2520preliminary%2520step%2520of%2520layer%2520detection%2520could%2520solve.%2520We%250Apresent%2520a%2520novel%2520method%2520for%2520classifying%2520the%2520number%2520of%2520grasped%2520material%2520layers%250Ausing%2520a%2520custom%2520gripper%2520equipped%2520with%2520DenseTact%25202.0%2520optical%2520tactile%2520sensors.%250AAfter%2520grasping%252C%2520the%2520gripper%2520performs%2520an%2520anthropomorphic%2520rubbing%2520motion%2520while%250Acollecting%2520optical%2520flow%252C%25206-axis%2520wrench%252C%2520and%2520joint%2520state%2520data.%2520Using%2520this%2520data%250Ain%2520a%2520transformer-based%2520network%2520achieves%2520a%2520test%2520accuracy%2520of%252098.21%255C%2525%2520in%250Aclassifying%2520the%2520number%2520of%2520grasped%2520cloth%2520layers%252C%2520and%252081.25%255C%2525%2520accuracy%2520in%250Aclassifying%2520layers%2520of%2520grasped%2520paper%252C%2520showing%2520the%2520effectiveness%2520of%2520our%2520dynamic%250Arubbing%2520method.%2520Evaluating%2520different%2520inputs%2520and%2520model%2520architectures%2520highlights%250Athe%2520usefulness%2520of%2520tactile%2520sensor%2520information%2520and%2520a%2520transformer%2520model%2520for%2520this%250Atask.%2520A%2520comprehensive%2520dataset%2520of%2520568%2520labeled%2520trials%2520%2528368%2520for%2520cloth%2520and%2520200%2520for%250Apaper%2529%2520was%2520collected%2520and%2520made%2520open-source%2520along%2520with%2520this%2520paper.%2520Our%2520project%250Apage%2520is%2520available%2520at%2520https%253A//armlabstanford.github.io/dynamic-cloth-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09849v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Layer%20Detection%20of%20Thin%20Materials%20using%20DenseTact%20Optical%0A%20%20Tactile%20Sensors&entry.906535625=Ankush%20Kundan%20Dhawan%20and%20Camille%20Chungyoun%20and%20Karina%20Ting%20and%20Monroe%20Kennedy%20III&entry.1292438233=%20%20Manipulation%20of%20thin%20materials%20is%20critical%20for%20many%20everyday%20tasks%20and%0Aremains%20a%20significant%20challenge%20for%20robots.%20While%20existing%20research%20has%20made%0Astrides%20in%20tasks%20like%20material%20smoothing%20and%20folding%2C%20many%20studies%20struggle%0Awith%20common%20failure%20modes%20%28crumpled%20corners/edges%2C%20incorrect%20grasp%0Aconfigurations%29%20that%20a%20preliminary%20step%20of%20layer%20detection%20could%20solve.%20We%0Apresent%20a%20novel%20method%20for%20classifying%20the%20number%20of%20grasped%20material%20layers%0Ausing%20a%20custom%20gripper%20equipped%20with%20DenseTact%202.0%20optical%20tactile%20sensors.%0AAfter%20grasping%2C%20the%20gripper%20performs%20an%20anthropomorphic%20rubbing%20motion%20while%0Acollecting%20optical%20flow%2C%206-axis%20wrench%2C%20and%20joint%20state%20data.%20Using%20this%20data%0Ain%20a%20transformer-based%20network%20achieves%20a%20test%20accuracy%20of%2098.21%5C%25%20in%0Aclassifying%20the%20number%20of%20grasped%20cloth%20layers%2C%20and%2081.25%5C%25%20accuracy%20in%0Aclassifying%20layers%20of%20grasped%20paper%2C%20showing%20the%20effectiveness%20of%20our%20dynamic%0Arubbing%20method.%20Evaluating%20different%20inputs%20and%20model%20architectures%20highlights%0Athe%20usefulness%20of%20tactile%20sensor%20information%20and%20a%20transformer%20model%20for%20this%0Atask.%20A%20comprehensive%20dataset%20of%20568%20labeled%20trials%20%28368%20for%20cloth%20and%20200%20for%0Apaper%29%20was%20collected%20and%20made%20open-source%20along%20with%20this%20paper.%20Our%20project%0Apage%20is%20available%20at%20https%3A//armlabstanford.github.io/dynamic-cloth-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09849v3&entry.124074799=Read"},
{"title": "Reinforcement Learning in Vision: A Survey", "author": "Weijia Wu and Chen Gao and Joya Chen and Kevin Qinghong Lin and Qingwei Meng and Yiming Zhang and Yuke Qiu and Hong Zhou and Mike Zheng Shou", "abstract": "  Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.\n", "link": "http://arxiv.org/abs/2508.08189v1", "date": "2025-08-11", "relevancy": 2.2523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey&body=Title%3A%20Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey%0AAuthor%3A%20Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%0Aintelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%0Abut%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%0Aand%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%0Aand%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%0Averifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%0ARelative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%0Aworks%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%0Ageneration%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%0Aeach%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%0Aprogress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%0Apreference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%0Aevaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%0Astate-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%0Aefficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%0Alandscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%0Ainquiry.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520in%2520Vision%253A%2520A%2520Survey%26entry.906535625%3DWeijia%2520Wu%2520and%2520Chen%2520Gao%2520and%2520Joya%2520Chen%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Qingwei%2520Meng%2520and%2520Yiming%2520Zhang%2520and%2520Yuke%2520Qiu%2520and%2520Hong%2520Zhou%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520at%2520the%2520intersection%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520visual%250Aintelligence%2520have%2520enabled%2520agents%2520that%2520not%2520only%2520perceive%2520complex%2520visual%2520scenes%250Abut%2520also%2520reason%252C%2520generate%252C%2520and%2520act%2520within%2520them.%2520This%2520survey%2520offers%2520a%2520critical%250Aand%2520up-to-date%2520synthesis%2520of%2520the%2520field.%2520We%2520first%2520formalize%2520visual%2520RL%2520problems%250Aand%2520trace%2520the%2520evolution%2520of%2520policy-optimization%2520strategies%2520from%2520RLHF%2520to%250Averifiable%2520reward%2520paradigms%252C%2520and%2520from%2520Proximal%2520Policy%2520Optimization%2520to%2520Group%250ARelative%2520Policy%2520Optimization.%2520We%2520then%2520organize%2520more%2520than%2520200%2520representative%250Aworks%2520into%2520four%2520thematic%2520pillars%253A%2520multi-modal%2520large%2520language%2520models%252C%2520visual%250Ageneration%252C%2520unified%2520model%2520frameworks%252C%2520and%2520vision-language-action%2520models.%2520For%250Aeach%2520pillar%2520we%2520examine%2520algorithmic%2520design%252C%2520reward%2520engineering%252C%2520benchmark%250Aprogress%252C%2520and%2520we%2520distill%2520trends%2520such%2520as%2520curriculum-driven%2520training%252C%250Apreference-aligned%2520diffusion%252C%2520and%2520unified%2520reward%2520modeling.%2520Finally%252C%2520we%2520review%250Aevaluation%2520protocols%2520spanning%2520set-level%2520fidelity%252C%2520sample-level%2520preference%252C%2520and%250Astate-level%2520stability%252C%2520and%2520we%2520identify%2520open%2520challenges%2520that%2520include%2520sample%250Aefficiency%252C%2520generalization%252C%2520and%2520safe%2520deployment.%2520Our%2520goal%2520is%2520to%2520provide%250Aresearchers%2520and%2520practitioners%2520with%2520a%2520coherent%2520map%2520of%2520the%2520rapidly%2520expanding%250Alandscape%2520of%2520visual%2520RL%2520and%2520to%2520highlight%2520promising%2520directions%2520for%2520future%250Ainquiry.%2520Resources%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20in%20Vision%3A%20A%20Survey&entry.906535625=Weijia%20Wu%20and%20Chen%20Gao%20and%20Joya%20Chen%20and%20Kevin%20Qinghong%20Lin%20and%20Qingwei%20Meng%20and%20Yiming%20Zhang%20and%20Yuke%20Qiu%20and%20Hong%20Zhou%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Recent%20advances%20at%20the%20intersection%20of%20reinforcement%20learning%20%28RL%29%20and%20visual%0Aintelligence%20have%20enabled%20agents%20that%20not%20only%20perceive%20complex%20visual%20scenes%0Abut%20also%20reason%2C%20generate%2C%20and%20act%20within%20them.%20This%20survey%20offers%20a%20critical%0Aand%20up-to-date%20synthesis%20of%20the%20field.%20We%20first%20formalize%20visual%20RL%20problems%0Aand%20trace%20the%20evolution%20of%20policy-optimization%20strategies%20from%20RLHF%20to%0Averifiable%20reward%20paradigms%2C%20and%20from%20Proximal%20Policy%20Optimization%20to%20Group%0ARelative%20Policy%20Optimization.%20We%20then%20organize%20more%20than%20200%20representative%0Aworks%20into%20four%20thematic%20pillars%3A%20multi-modal%20large%20language%20models%2C%20visual%0Ageneration%2C%20unified%20model%20frameworks%2C%20and%20vision-language-action%20models.%20For%0Aeach%20pillar%20we%20examine%20algorithmic%20design%2C%20reward%20engineering%2C%20benchmark%0Aprogress%2C%20and%20we%20distill%20trends%20such%20as%20curriculum-driven%20training%2C%0Apreference-aligned%20diffusion%2C%20and%20unified%20reward%20modeling.%20Finally%2C%20we%20review%0Aevaluation%20protocols%20spanning%20set-level%20fidelity%2C%20sample-level%20preference%2C%20and%0Astate-level%20stability%2C%20and%20we%20identify%20open%20challenges%20that%20include%20sample%0Aefficiency%2C%20generalization%2C%20and%20safe%20deployment.%20Our%20goal%20is%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20coherent%20map%20of%20the%20rapidly%20expanding%0Alandscape%20of%20visual%20RL%20and%20to%20highlight%20promising%20directions%20for%20future%0Ainquiry.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08189v1&entry.124074799=Read"},
{"title": "ME-TST+: Micro-expression Analysis via Temporal State Transition with\n  ROI Relationship Awareness", "author": "Zizheng Guo and Bochao Zou and Junbao Zhuo and Huimin Ma", "abstract": "  Micro-expressions (MEs) are regarded as important indicators of an\nindividual's intrinsic emotions, preferences, and tendencies. ME analysis\nrequires spotting of ME intervals within long video sequences and recognition\nof their corresponding emotional categories. Previous deep learning approaches\ncommonly employ sliding-window classification networks. However, the use of\nfixed window lengths and hard classification presents notable limitations in\npractice. Furthermore, these methods typically treat ME spotting and\nrecognition as two separate tasks, overlooking the essential relationship\nbetween them. To address these challenges, this paper proposes two state space\nmodel-based architectures, namely ME-TST and ME-TST+, which utilize temporal\nstate transition mechanisms to replace conventional window-level classification\nwith video-level regression. This enables a more precise characterization of\nthe temporal dynamics of MEs and supports the modeling of MEs with varying\ndurations. In ME-TST+, we further introduce multi-granularity ROI modeling and\nthe slowfast Mamba framework to alleviate information loss associated with\ntreating ME analysis as a time-series task. Additionally, we propose a synergy\nstrategy for spotting and recognition at both the feature and result levels,\nleveraging their intrinsic connection to enhance overall analysis performance.\nExtensive experiments demonstrate that the proposed methods achieve\nstate-of-the-art performance. The codes are available at\nhttps://github.com/zizheng-guo/ME-TST.\n", "link": "http://arxiv.org/abs/2508.08082v1", "date": "2025-08-11", "relevancy": 2.2426, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5788}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5529}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ME-TST%2B%3A%20Micro-expression%20Analysis%20via%20Temporal%20State%20Transition%20with%0A%20%20ROI%20Relationship%20Awareness&body=Title%3A%20ME-TST%2B%3A%20Micro-expression%20Analysis%20via%20Temporal%20State%20Transition%20with%0A%20%20ROI%20Relationship%20Awareness%0AAuthor%3A%20Zizheng%20Guo%20and%20Bochao%20Zou%20and%20Junbao%20Zhuo%20and%20Huimin%20Ma%0AAbstract%3A%20%20%20Micro-expressions%20%28MEs%29%20are%20regarded%20as%20important%20indicators%20of%20an%0Aindividual%27s%20intrinsic%20emotions%2C%20preferences%2C%20and%20tendencies.%20ME%20analysis%0Arequires%20spotting%20of%20ME%20intervals%20within%20long%20video%20sequences%20and%20recognition%0Aof%20their%20corresponding%20emotional%20categories.%20Previous%20deep%20learning%20approaches%0Acommonly%20employ%20sliding-window%20classification%20networks.%20However%2C%20the%20use%20of%0Afixed%20window%20lengths%20and%20hard%20classification%20presents%20notable%20limitations%20in%0Apractice.%20Furthermore%2C%20these%20methods%20typically%20treat%20ME%20spotting%20and%0Arecognition%20as%20two%20separate%20tasks%2C%20overlooking%20the%20essential%20relationship%0Abetween%20them.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20two%20state%20space%0Amodel-based%20architectures%2C%20namely%20ME-TST%20and%20ME-TST%2B%2C%20which%20utilize%20temporal%0Astate%20transition%20mechanisms%20to%20replace%20conventional%20window-level%20classification%0Awith%20video-level%20regression.%20This%20enables%20a%20more%20precise%20characterization%20of%0Athe%20temporal%20dynamics%20of%20MEs%20and%20supports%20the%20modeling%20of%20MEs%20with%20varying%0Adurations.%20In%20ME-TST%2B%2C%20we%20further%20introduce%20multi-granularity%20ROI%20modeling%20and%0Athe%20slowfast%20Mamba%20framework%20to%20alleviate%20information%20loss%20associated%20with%0Atreating%20ME%20analysis%20as%20a%20time-series%20task.%20Additionally%2C%20we%20propose%20a%20synergy%0Astrategy%20for%20spotting%20and%20recognition%20at%20both%20the%20feature%20and%20result%20levels%2C%0Aleveraging%20their%20intrinsic%20connection%20to%20enhance%20overall%20analysis%20performance.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20methods%20achieve%0Astate-of-the-art%20performance.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/zizheng-guo/ME-TST.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DME-TST%252B%253A%2520Micro-expression%2520Analysis%2520via%2520Temporal%2520State%2520Transition%2520with%250A%2520%2520ROI%2520Relationship%2520Awareness%26entry.906535625%3DZizheng%2520Guo%2520and%2520Bochao%2520Zou%2520and%2520Junbao%2520Zhuo%2520and%2520Huimin%2520Ma%26entry.1292438233%3D%2520%2520Micro-expressions%2520%2528MEs%2529%2520are%2520regarded%2520as%2520important%2520indicators%2520of%2520an%250Aindividual%2527s%2520intrinsic%2520emotions%252C%2520preferences%252C%2520and%2520tendencies.%2520ME%2520analysis%250Arequires%2520spotting%2520of%2520ME%2520intervals%2520within%2520long%2520video%2520sequences%2520and%2520recognition%250Aof%2520their%2520corresponding%2520emotional%2520categories.%2520Previous%2520deep%2520learning%2520approaches%250Acommonly%2520employ%2520sliding-window%2520classification%2520networks.%2520However%252C%2520the%2520use%2520of%250Afixed%2520window%2520lengths%2520and%2520hard%2520classification%2520presents%2520notable%2520limitations%2520in%250Apractice.%2520Furthermore%252C%2520these%2520methods%2520typically%2520treat%2520ME%2520spotting%2520and%250Arecognition%2520as%2520two%2520separate%2520tasks%252C%2520overlooking%2520the%2520essential%2520relationship%250Abetween%2520them.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520two%2520state%2520space%250Amodel-based%2520architectures%252C%2520namely%2520ME-TST%2520and%2520ME-TST%252B%252C%2520which%2520utilize%2520temporal%250Astate%2520transition%2520mechanisms%2520to%2520replace%2520conventional%2520window-level%2520classification%250Awith%2520video-level%2520regression.%2520This%2520enables%2520a%2520more%2520precise%2520characterization%2520of%250Athe%2520temporal%2520dynamics%2520of%2520MEs%2520and%2520supports%2520the%2520modeling%2520of%2520MEs%2520with%2520varying%250Adurations.%2520In%2520ME-TST%252B%252C%2520we%2520further%2520introduce%2520multi-granularity%2520ROI%2520modeling%2520and%250Athe%2520slowfast%2520Mamba%2520framework%2520to%2520alleviate%2520information%2520loss%2520associated%2520with%250Atreating%2520ME%2520analysis%2520as%2520a%2520time-series%2520task.%2520Additionally%252C%2520we%2520propose%2520a%2520synergy%250Astrategy%2520for%2520spotting%2520and%2520recognition%2520at%2520both%2520the%2520feature%2520and%2520result%2520levels%252C%250Aleveraging%2520their%2520intrinsic%2520connection%2520to%2520enhance%2520overall%2520analysis%2520performance.%250AExtensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520methods%2520achieve%250Astate-of-the-art%2520performance.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/zizheng-guo/ME-TST.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ME-TST%2B%3A%20Micro-expression%20Analysis%20via%20Temporal%20State%20Transition%20with%0A%20%20ROI%20Relationship%20Awareness&entry.906535625=Zizheng%20Guo%20and%20Bochao%20Zou%20and%20Junbao%20Zhuo%20and%20Huimin%20Ma&entry.1292438233=%20%20Micro-expressions%20%28MEs%29%20are%20regarded%20as%20important%20indicators%20of%20an%0Aindividual%27s%20intrinsic%20emotions%2C%20preferences%2C%20and%20tendencies.%20ME%20analysis%0Arequires%20spotting%20of%20ME%20intervals%20within%20long%20video%20sequences%20and%20recognition%0Aof%20their%20corresponding%20emotional%20categories.%20Previous%20deep%20learning%20approaches%0Acommonly%20employ%20sliding-window%20classification%20networks.%20However%2C%20the%20use%20of%0Afixed%20window%20lengths%20and%20hard%20classification%20presents%20notable%20limitations%20in%0Apractice.%20Furthermore%2C%20these%20methods%20typically%20treat%20ME%20spotting%20and%0Arecognition%20as%20two%20separate%20tasks%2C%20overlooking%20the%20essential%20relationship%0Abetween%20them.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20two%20state%20space%0Amodel-based%20architectures%2C%20namely%20ME-TST%20and%20ME-TST%2B%2C%20which%20utilize%20temporal%0Astate%20transition%20mechanisms%20to%20replace%20conventional%20window-level%20classification%0Awith%20video-level%20regression.%20This%20enables%20a%20more%20precise%20characterization%20of%0Athe%20temporal%20dynamics%20of%20MEs%20and%20supports%20the%20modeling%20of%20MEs%20with%20varying%0Adurations.%20In%20ME-TST%2B%2C%20we%20further%20introduce%20multi-granularity%20ROI%20modeling%20and%0Athe%20slowfast%20Mamba%20framework%20to%20alleviate%20information%20loss%20associated%20with%0Atreating%20ME%20analysis%20as%20a%20time-series%20task.%20Additionally%2C%20we%20propose%20a%20synergy%0Astrategy%20for%20spotting%20and%20recognition%20at%20both%20the%20feature%20and%20result%20levels%2C%0Aleveraging%20their%20intrinsic%20connection%20to%20enhance%20overall%20analysis%20performance.%0AExtensive%20experiments%20demonstrate%20that%20the%20proposed%20methods%20achieve%0Astate-of-the-art%20performance.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/zizheng-guo/ME-TST.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08082v1&entry.124074799=Read"},
{"title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation", "author": "Fangyuan Mao and Aiming Hao and Jintao Chen and Dongxia Liu and Xiaokun Feng and Jiashu Zhu and Meiqi Wu and Chubin Chen and Jiahong Wu and Xiangxiang Chu", "abstract": "  Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.\n", "link": "http://arxiv.org/abs/2508.07981v1", "date": "2025-08-11", "relevancy": 2.2402, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-Effects%3A%20Unified%20and%20Spatially-Controllable%20Visual%20Effects%0A%20%20Generation&body=Title%3A%20Omni-Effects%3A%20Unified%20and%20Spatially-Controllable%20Visual%20Effects%0A%20%20Generation%0AAuthor%3A%20Fangyuan%20Mao%20and%20Aiming%20Hao%20and%20Jintao%20Chen%20and%20Dongxia%20Liu%20and%20Xiaokun%20Feng%20and%20Jiashu%20Zhu%20and%20Meiqi%20Wu%20and%20Chubin%20Chen%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%0AAbstract%3A%20%20%20Visual%20effects%20%28VFX%29%20are%20essential%20visual%20enhancements%20fundamental%20to%20modern%0Acinematic%20production.%20Although%20video%20generation%20models%20offer%20cost-efficient%0Asolutions%20for%20VFX%20production%2C%20current%20methods%20are%20constrained%20by%20per-effect%0ALoRA%20training%2C%20which%20limits%20generation%20to%20single%20effects.%20This%20fundamental%0Alimitation%20impedes%20applications%20that%20require%20spatially%20controllable%20composite%0Aeffects%2C%20i.e.%2C%20the%20concurrent%20generation%20of%20multiple%20effects%20at%20designated%0Alocations.%20However%2C%20integrating%20diverse%20effects%20into%20a%20unified%20framework%20faces%0Amajor%20challenges%3A%20interference%20from%20effect%20variations%20and%20spatial%0Auncontrollability%20during%20multi-VFX%20joint%20training.%20To%20tackle%20these%20challenges%2C%0Awe%20propose%20Omni-Effects%2C%20a%20first%20unified%20framework%20capable%20of%20generating%0Aprompt-guided%20effects%20and%20spatially%20controllable%20composite%20effects.%20The%20core%20of%0Aour%20framework%20comprises%20two%20key%20innovations%3A%20%281%29%20LoRA-based%20Mixture%20of%20Experts%0A%28LoRA-MoE%29%2C%20which%20employs%20a%20group%20of%20expert%20LoRAs%2C%20integrating%20diverse%20effects%0Awithin%20a%20unified%20model%20while%20effectively%20mitigating%20cross-task%20interference.%0A%282%29%20Spatial-Aware%20Prompt%20%28SAP%29%20incorporates%20spatial%20mask%20information%20into%20the%0Atext%20token%2C%20enabling%20precise%20spatial%20control.%20Furthermore%2C%20we%20introduce%20an%0AIndependent-Information%20Flow%20%28IIF%29%20module%20integrated%20within%20the%20SAP%2C%20isolating%0Athe%20control%20signals%20corresponding%20to%20individual%20effects%20to%20prevent%20any%20unwanted%0Ablending.%20To%20facilitate%20this%20research%2C%20we%20construct%20a%20comprehensive%20VFX%20dataset%0AOmni-VFX%20via%20a%20novel%20data%20collection%20pipeline%20combining%20image%20editing%20and%0AFirst-Last%20Frame-to-Video%20%28FLF2V%29%20synthesis%2C%20and%20introduce%20a%20dedicated%20VFX%0Aevaluation%20framework%20for%20validating%20model%20performance.%20Extensive%20experiments%0Ademonstrate%20that%20Omni-Effects%20achieves%20precise%20spatial%20control%20and%20diverse%0Aeffect%20generation%2C%20enabling%20users%20to%20specify%20both%20the%20category%20and%20location%20of%0Adesired%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-Effects%253A%2520Unified%2520and%2520Spatially-Controllable%2520Visual%2520Effects%250A%2520%2520Generation%26entry.906535625%3DFangyuan%2520Mao%2520and%2520Aiming%2520Hao%2520and%2520Jintao%2520Chen%2520and%2520Dongxia%2520Liu%2520and%2520Xiaokun%2520Feng%2520and%2520Jiashu%2520Zhu%2520and%2520Meiqi%2520Wu%2520and%2520Chubin%2520Chen%2520and%2520Jiahong%2520Wu%2520and%2520Xiangxiang%2520Chu%26entry.1292438233%3D%2520%2520Visual%2520effects%2520%2528VFX%2529%2520are%2520essential%2520visual%2520enhancements%2520fundamental%2520to%2520modern%250Acinematic%2520production.%2520Although%2520video%2520generation%2520models%2520offer%2520cost-efficient%250Asolutions%2520for%2520VFX%2520production%252C%2520current%2520methods%2520are%2520constrained%2520by%2520per-effect%250ALoRA%2520training%252C%2520which%2520limits%2520generation%2520to%2520single%2520effects.%2520This%2520fundamental%250Alimitation%2520impedes%2520applications%2520that%2520require%2520spatially%2520controllable%2520composite%250Aeffects%252C%2520i.e.%252C%2520the%2520concurrent%2520generation%2520of%2520multiple%2520effects%2520at%2520designated%250Alocations.%2520However%252C%2520integrating%2520diverse%2520effects%2520into%2520a%2520unified%2520framework%2520faces%250Amajor%2520challenges%253A%2520interference%2520from%2520effect%2520variations%2520and%2520spatial%250Auncontrollability%2520during%2520multi-VFX%2520joint%2520training.%2520To%2520tackle%2520these%2520challenges%252C%250Awe%2520propose%2520Omni-Effects%252C%2520a%2520first%2520unified%2520framework%2520capable%2520of%2520generating%250Aprompt-guided%2520effects%2520and%2520spatially%2520controllable%2520composite%2520effects.%2520The%2520core%2520of%250Aour%2520framework%2520comprises%2520two%2520key%2520innovations%253A%2520%25281%2529%2520LoRA-based%2520Mixture%2520of%2520Experts%250A%2528LoRA-MoE%2529%252C%2520which%2520employs%2520a%2520group%2520of%2520expert%2520LoRAs%252C%2520integrating%2520diverse%2520effects%250Awithin%2520a%2520unified%2520model%2520while%2520effectively%2520mitigating%2520cross-task%2520interference.%250A%25282%2529%2520Spatial-Aware%2520Prompt%2520%2528SAP%2529%2520incorporates%2520spatial%2520mask%2520information%2520into%2520the%250Atext%2520token%252C%2520enabling%2520precise%2520spatial%2520control.%2520Furthermore%252C%2520we%2520introduce%2520an%250AIndependent-Information%2520Flow%2520%2528IIF%2529%2520module%2520integrated%2520within%2520the%2520SAP%252C%2520isolating%250Athe%2520control%2520signals%2520corresponding%2520to%2520individual%2520effects%2520to%2520prevent%2520any%2520unwanted%250Ablending.%2520To%2520facilitate%2520this%2520research%252C%2520we%2520construct%2520a%2520comprehensive%2520VFX%2520dataset%250AOmni-VFX%2520via%2520a%2520novel%2520data%2520collection%2520pipeline%2520combining%2520image%2520editing%2520and%250AFirst-Last%2520Frame-to-Video%2520%2528FLF2V%2529%2520synthesis%252C%2520and%2520introduce%2520a%2520dedicated%2520VFX%250Aevaluation%2520framework%2520for%2520validating%2520model%2520performance.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520Omni-Effects%2520achieves%2520precise%2520spatial%2520control%2520and%2520diverse%250Aeffect%2520generation%252C%2520enabling%2520users%2520to%2520specify%2520both%2520the%2520category%2520and%2520location%2520of%250Adesired%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-Effects%3A%20Unified%20and%20Spatially-Controllable%20Visual%20Effects%0A%20%20Generation&entry.906535625=Fangyuan%20Mao%20and%20Aiming%20Hao%20and%20Jintao%20Chen%20and%20Dongxia%20Liu%20and%20Xiaokun%20Feng%20and%20Jiashu%20Zhu%20and%20Meiqi%20Wu%20and%20Chubin%20Chen%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu&entry.1292438233=%20%20Visual%20effects%20%28VFX%29%20are%20essential%20visual%20enhancements%20fundamental%20to%20modern%0Acinematic%20production.%20Although%20video%20generation%20models%20offer%20cost-efficient%0Asolutions%20for%20VFX%20production%2C%20current%20methods%20are%20constrained%20by%20per-effect%0ALoRA%20training%2C%20which%20limits%20generation%20to%20single%20effects.%20This%20fundamental%0Alimitation%20impedes%20applications%20that%20require%20spatially%20controllable%20composite%0Aeffects%2C%20i.e.%2C%20the%20concurrent%20generation%20of%20multiple%20effects%20at%20designated%0Alocations.%20However%2C%20integrating%20diverse%20effects%20into%20a%20unified%20framework%20faces%0Amajor%20challenges%3A%20interference%20from%20effect%20variations%20and%20spatial%0Auncontrollability%20during%20multi-VFX%20joint%20training.%20To%20tackle%20these%20challenges%2C%0Awe%20propose%20Omni-Effects%2C%20a%20first%20unified%20framework%20capable%20of%20generating%0Aprompt-guided%20effects%20and%20spatially%20controllable%20composite%20effects.%20The%20core%20of%0Aour%20framework%20comprises%20two%20key%20innovations%3A%20%281%29%20LoRA-based%20Mixture%20of%20Experts%0A%28LoRA-MoE%29%2C%20which%20employs%20a%20group%20of%20expert%20LoRAs%2C%20integrating%20diverse%20effects%0Awithin%20a%20unified%20model%20while%20effectively%20mitigating%20cross-task%20interference.%0A%282%29%20Spatial-Aware%20Prompt%20%28SAP%29%20incorporates%20spatial%20mask%20information%20into%20the%0Atext%20token%2C%20enabling%20precise%20spatial%20control.%20Furthermore%2C%20we%20introduce%20an%0AIndependent-Information%20Flow%20%28IIF%29%20module%20integrated%20within%20the%20SAP%2C%20isolating%0Athe%20control%20signals%20corresponding%20to%20individual%20effects%20to%20prevent%20any%20unwanted%0Ablending.%20To%20facilitate%20this%20research%2C%20we%20construct%20a%20comprehensive%20VFX%20dataset%0AOmni-VFX%20via%20a%20novel%20data%20collection%20pipeline%20combining%20image%20editing%20and%0AFirst-Last%20Frame-to-Video%20%28FLF2V%29%20synthesis%2C%20and%20introduce%20a%20dedicated%20VFX%0Aevaluation%20framework%20for%20validating%20model%20performance.%20Extensive%20experiments%0Ademonstrate%20that%20Omni-Effects%20achieves%20precise%20spatial%20control%20and%20diverse%0Aeffect%20generation%2C%20enabling%20users%20to%20specify%20both%20the%20category%20and%20location%20of%0Adesired%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07981v1&entry.124074799=Read"},
{"title": "SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption\n  Datasets for Vision-Language Models", "author": "Zheng Liu and Hao Liang and Bozhou Li and Wentao Xiong and Chong Chen and Conghui He and Wentao Zhang and Bin Cui", "abstract": "  Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable\nvision-understanding capabilities. However, training these models requires\nlarge-scale datasets, which brings challenges related to efficiency,\neffectiveness, and quality of web data. In this paper, we introduce SynthVLM, a\nnew data synthesis and curation method for generating image-caption pairs.\nUnlike traditional methods, where captions are generated from images, SynthVLM\nutilizes advanced diffusion models and high-quality captions to synthesize and\nselect images from text captions, thereby creating precisely aligned image-text\npairs. We further introduce SynthVLM-100K, a high-quality dataset consisting of\n100K curated and synthesized image-caption pairs. In both model and human\nevaluations, SynthVLM-100K outperforms traditional real-world datasets.\nLeveraging this dataset, we develop a new family of multimodal large language\nmodels (MLLMs), SynthVLM-7B and SynthVLM-13B, which achieve state-of-the-art\n(SOTA) performance on various vision question-answering (VQA) tasks. Notably,\nour models outperform LLaVA across most metrics with only 18\\% pretrain data.\nFurthermore, SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU\nbenchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves\nlanguage abilities.\n", "link": "http://arxiv.org/abs/2407.20756v5", "date": "2025-08-11", "relevancy": 2.2358, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthVLM%3A%20Towards%20High-Quality%20and%20Efficient%20Synthesis%20of%20Image-Caption%0A%20%20Datasets%20for%20Vision-Language%20Models&body=Title%3A%20SynthVLM%3A%20Towards%20High-Quality%20and%20Efficient%20Synthesis%20of%20Image-Caption%0A%20%20Datasets%20for%20Vision-Language%20Models%0AAuthor%3A%20Zheng%20Liu%20and%20Hao%20Liang%20and%20Bozhou%20Li%20and%20Wentao%20Xiong%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20recently%20emerged%2C%20demonstrating%20remarkable%0Avision-understanding%20capabilities.%20However%2C%20training%20these%20models%20requires%0Alarge-scale%20datasets%2C%20which%20brings%20challenges%20related%20to%20efficiency%2C%0Aeffectiveness%2C%20and%20quality%20of%20web%20data.%20In%20this%20paper%2C%20we%20introduce%20SynthVLM%2C%20a%0Anew%20data%20synthesis%20and%20curation%20method%20for%20generating%20image-caption%20pairs.%0AUnlike%20traditional%20methods%2C%20where%20captions%20are%20generated%20from%20images%2C%20SynthVLM%0Autilizes%20advanced%20diffusion%20models%20and%20high-quality%20captions%20to%20synthesize%20and%0Aselect%20images%20from%20text%20captions%2C%20thereby%20creating%20precisely%20aligned%20image-text%0Apairs.%20We%20further%20introduce%20SynthVLM-100K%2C%20a%20high-quality%20dataset%20consisting%20of%0A100K%20curated%20and%20synthesized%20image-caption%20pairs.%20In%20both%20model%20and%20human%0Aevaluations%2C%20SynthVLM-100K%20outperforms%20traditional%20real-world%20datasets.%0ALeveraging%20this%20dataset%2C%20we%20develop%20a%20new%20family%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20SynthVLM-7B%20and%20SynthVLM-13B%2C%20which%20achieve%20state-of-the-art%0A%28SOTA%29%20performance%20on%20various%20vision%20question-answering%20%28VQA%29%20tasks.%20Notably%2C%0Aour%20models%20outperform%20LLaVA%20across%20most%20metrics%20with%20only%2018%5C%25%20pretrain%20data.%0AFurthermore%2C%20SynthVLM-7B%20and%20SynthVLM-13B%20attain%20SOTA%20performance%20on%20the%20MMLU%0Abenchmark%2C%20demonstrating%20that%20the%20high-quality%20SynthVLM-100K%20dataset%20preserves%0Alanguage%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20756v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthVLM%253A%2520Towards%2520High-Quality%2520and%2520Efficient%2520Synthesis%2520of%2520Image-Caption%250A%2520%2520Datasets%2520for%2520Vision-Language%2520Models%26entry.906535625%3DZheng%2520Liu%2520and%2520Hao%2520Liang%2520and%2520Bozhou%2520Li%2520and%2520Wentao%2520Xiong%2520and%2520Chong%2520Chen%2520and%2520Conghui%2520He%2520and%2520Wentao%2520Zhang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520recently%2520emerged%252C%2520demonstrating%2520remarkable%250Avision-understanding%2520capabilities.%2520However%252C%2520training%2520these%2520models%2520requires%250Alarge-scale%2520datasets%252C%2520which%2520brings%2520challenges%2520related%2520to%2520efficiency%252C%250Aeffectiveness%252C%2520and%2520quality%2520of%2520web%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SynthVLM%252C%2520a%250Anew%2520data%2520synthesis%2520and%2520curation%2520method%2520for%2520generating%2520image-caption%2520pairs.%250AUnlike%2520traditional%2520methods%252C%2520where%2520captions%2520are%2520generated%2520from%2520images%252C%2520SynthVLM%250Autilizes%2520advanced%2520diffusion%2520models%2520and%2520high-quality%2520captions%2520to%2520synthesize%2520and%250Aselect%2520images%2520from%2520text%2520captions%252C%2520thereby%2520creating%2520precisely%2520aligned%2520image-text%250Apairs.%2520We%2520further%2520introduce%2520SynthVLM-100K%252C%2520a%2520high-quality%2520dataset%2520consisting%2520of%250A100K%2520curated%2520and%2520synthesized%2520image-caption%2520pairs.%2520In%2520both%2520model%2520and%2520human%250Aevaluations%252C%2520SynthVLM-100K%2520outperforms%2520traditional%2520real-world%2520datasets.%250ALeveraging%2520this%2520dataset%252C%2520we%2520develop%2520a%2520new%2520family%2520of%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%252C%2520SynthVLM-7B%2520and%2520SynthVLM-13B%252C%2520which%2520achieve%2520state-of-the-art%250A%2528SOTA%2529%2520performance%2520on%2520various%2520vision%2520question-answering%2520%2528VQA%2529%2520tasks.%2520Notably%252C%250Aour%2520models%2520outperform%2520LLaVA%2520across%2520most%2520metrics%2520with%2520only%252018%255C%2525%2520pretrain%2520data.%250AFurthermore%252C%2520SynthVLM-7B%2520and%2520SynthVLM-13B%2520attain%2520SOTA%2520performance%2520on%2520the%2520MMLU%250Abenchmark%252C%2520demonstrating%2520that%2520the%2520high-quality%2520SynthVLM-100K%2520dataset%2520preserves%250Alanguage%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20756v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthVLM%3A%20Towards%20High-Quality%20and%20Efficient%20Synthesis%20of%20Image-Caption%0A%20%20Datasets%20for%20Vision-Language%20Models&entry.906535625=Zheng%20Liu%20and%20Hao%20Liang%20and%20Bozhou%20Li%20and%20Wentao%20Xiong%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Bin%20Cui&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20recently%20emerged%2C%20demonstrating%20remarkable%0Avision-understanding%20capabilities.%20However%2C%20training%20these%20models%20requires%0Alarge-scale%20datasets%2C%20which%20brings%20challenges%20related%20to%20efficiency%2C%0Aeffectiveness%2C%20and%20quality%20of%20web%20data.%20In%20this%20paper%2C%20we%20introduce%20SynthVLM%2C%20a%0Anew%20data%20synthesis%20and%20curation%20method%20for%20generating%20image-caption%20pairs.%0AUnlike%20traditional%20methods%2C%20where%20captions%20are%20generated%20from%20images%2C%20SynthVLM%0Autilizes%20advanced%20diffusion%20models%20and%20high-quality%20captions%20to%20synthesize%20and%0Aselect%20images%20from%20text%20captions%2C%20thereby%20creating%20precisely%20aligned%20image-text%0Apairs.%20We%20further%20introduce%20SynthVLM-100K%2C%20a%20high-quality%20dataset%20consisting%20of%0A100K%20curated%20and%20synthesized%20image-caption%20pairs.%20In%20both%20model%20and%20human%0Aevaluations%2C%20SynthVLM-100K%20outperforms%20traditional%20real-world%20datasets.%0ALeveraging%20this%20dataset%2C%20we%20develop%20a%20new%20family%20of%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%2C%20SynthVLM-7B%20and%20SynthVLM-13B%2C%20which%20achieve%20state-of-the-art%0A%28SOTA%29%20performance%20on%20various%20vision%20question-answering%20%28VQA%29%20tasks.%20Notably%2C%0Aour%20models%20outperform%20LLaVA%20across%20most%20metrics%20with%20only%2018%5C%25%20pretrain%20data.%0AFurthermore%2C%20SynthVLM-7B%20and%20SynthVLM-13B%20attain%20SOTA%20performance%20on%20the%20MMLU%0Abenchmark%2C%20demonstrating%20that%20the%20high-quality%20SynthVLM-100K%20dataset%20preserves%0Alanguage%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20756v5&entry.124074799=Read"},
{"title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with\n  N-DoF", "author": "En Yen Puang and Federico Ceola and Giulia Pasquale and Lorenzo Natale", "abstract": "  We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.\n", "link": "http://arxiv.org/abs/2508.07945v1", "date": "2025-08-11", "relevancy": 2.2239, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5657}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCHands%3A%20PCA-based%20Hand%20Pose%20Synergy%20Representation%20on%20Manipulators%20with%0A%20%20N-DoF&body=Title%3A%20PCHands%3A%20PCA-based%20Hand%20Pose%20Synergy%20Representation%20on%20Manipulators%20with%0A%20%20N-DoF%0AAuthor%3A%20En%20Yen%20Puang%20and%20Federico%20Ceola%20and%20Giulia%20Pasquale%20and%20Lorenzo%20Natale%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20a%20common%20representation%20for%20dexterous%0Amanipulation%20across%20manipulators%20of%20different%20morphologies.%20To%20this%20end%2C%20we%0Apropose%20PCHands%2C%20a%20novel%20approach%20for%20extracting%20hand%20postural%20synergies%20from%20a%0Alarge%20set%20of%20manipulators.%20We%20define%20a%20simplified%20and%20unified%20description%0Aformat%20based%20on%20anchor%20positions%20for%20manipulators%20ranging%20from%202-finger%0Agrippers%20to%205-finger%20anthropomorphic%20hands.%20This%20enables%20learning%20a%0Avariable-length%20latent%20representation%20of%20the%20manipulator%20configuration%20and%20the%0Aalignment%20of%20the%20end-effector%20frame%20of%20all%20manipulators.%20We%20show%20that%20it%20is%0Apossible%20to%20extract%20principal%20components%20from%20this%20latent%20representation%20that%0Ais%20universal%20across%20manipulators%20of%20different%20structures%20and%20degrees%20of%0Afreedom.%20To%20evaluate%20PCHands%2C%20we%20use%20this%20compact%20representation%20to%20encode%0Aobservation%20and%20action%20spaces%20of%20control%20policies%20for%20dexterous%20manipulation%0Atasks%20learned%20with%20RL.%20In%20terms%20of%20learning%20efficiency%20and%20consistency%2C%20the%0Aproposed%20representation%20outperforms%20a%20baseline%20that%20learns%20the%20same%20tasks%20in%0Ajoint%20space.%20We%20additionally%20show%20that%20PCHands%20performs%20robustly%20in%20RL%20from%0Ademonstration%2C%20when%20demonstrations%20are%20provided%20from%20a%20different%20manipulator.%0AWe%20further%20support%20our%20results%20with%20real-world%20experiments%20that%20involve%20a%0A2-finger%20gripper%20and%20a%204-finger%20anthropomorphic%20hand.%20Code%20and%20additional%0Amaterial%20are%20available%20at%20https%3A//hsp-iit.github.io/PCHands/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCHands%253A%2520PCA-based%2520Hand%2520Pose%2520Synergy%2520Representation%2520on%2520Manipulators%2520with%250A%2520%2520N-DoF%26entry.906535625%3DEn%2520Yen%2520Puang%2520and%2520Federico%2520Ceola%2520and%2520Giulia%2520Pasquale%2520and%2520Lorenzo%2520Natale%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520a%2520common%2520representation%2520for%2520dexterous%250Amanipulation%2520across%2520manipulators%2520of%2520different%2520morphologies.%2520To%2520this%2520end%252C%2520we%250Apropose%2520PCHands%252C%2520a%2520novel%2520approach%2520for%2520extracting%2520hand%2520postural%2520synergies%2520from%2520a%250Alarge%2520set%2520of%2520manipulators.%2520We%2520define%2520a%2520simplified%2520and%2520unified%2520description%250Aformat%2520based%2520on%2520anchor%2520positions%2520for%2520manipulators%2520ranging%2520from%25202-finger%250Agrippers%2520to%25205-finger%2520anthropomorphic%2520hands.%2520This%2520enables%2520learning%2520a%250Avariable-length%2520latent%2520representation%2520of%2520the%2520manipulator%2520configuration%2520and%2520the%250Aalignment%2520of%2520the%2520end-effector%2520frame%2520of%2520all%2520manipulators.%2520We%2520show%2520that%2520it%2520is%250Apossible%2520to%2520extract%2520principal%2520components%2520from%2520this%2520latent%2520representation%2520that%250Ais%2520universal%2520across%2520manipulators%2520of%2520different%2520structures%2520and%2520degrees%2520of%250Afreedom.%2520To%2520evaluate%2520PCHands%252C%2520we%2520use%2520this%2520compact%2520representation%2520to%2520encode%250Aobservation%2520and%2520action%2520spaces%2520of%2520control%2520policies%2520for%2520dexterous%2520manipulation%250Atasks%2520learned%2520with%2520RL.%2520In%2520terms%2520of%2520learning%2520efficiency%2520and%2520consistency%252C%2520the%250Aproposed%2520representation%2520outperforms%2520a%2520baseline%2520that%2520learns%2520the%2520same%2520tasks%2520in%250Ajoint%2520space.%2520We%2520additionally%2520show%2520that%2520PCHands%2520performs%2520robustly%2520in%2520RL%2520from%250Ademonstration%252C%2520when%2520demonstrations%2520are%2520provided%2520from%2520a%2520different%2520manipulator.%250AWe%2520further%2520support%2520our%2520results%2520with%2520real-world%2520experiments%2520that%2520involve%2520a%250A2-finger%2520gripper%2520and%2520a%25204-finger%2520anthropomorphic%2520hand.%2520Code%2520and%2520additional%250Amaterial%2520are%2520available%2520at%2520https%253A//hsp-iit.github.io/PCHands/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCHands%3A%20PCA-based%20Hand%20Pose%20Synergy%20Representation%20on%20Manipulators%20with%0A%20%20N-DoF&entry.906535625=En%20Yen%20Puang%20and%20Federico%20Ceola%20and%20Giulia%20Pasquale%20and%20Lorenzo%20Natale&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20a%20common%20representation%20for%20dexterous%0Amanipulation%20across%20manipulators%20of%20different%20morphologies.%20To%20this%20end%2C%20we%0Apropose%20PCHands%2C%20a%20novel%20approach%20for%20extracting%20hand%20postural%20synergies%20from%20a%0Alarge%20set%20of%20manipulators.%20We%20define%20a%20simplified%20and%20unified%20description%0Aformat%20based%20on%20anchor%20positions%20for%20manipulators%20ranging%20from%202-finger%0Agrippers%20to%205-finger%20anthropomorphic%20hands.%20This%20enables%20learning%20a%0Avariable-length%20latent%20representation%20of%20the%20manipulator%20configuration%20and%20the%0Aalignment%20of%20the%20end-effector%20frame%20of%20all%20manipulators.%20We%20show%20that%20it%20is%0Apossible%20to%20extract%20principal%20components%20from%20this%20latent%20representation%20that%0Ais%20universal%20across%20manipulators%20of%20different%20structures%20and%20degrees%20of%0Afreedom.%20To%20evaluate%20PCHands%2C%20we%20use%20this%20compact%20representation%20to%20encode%0Aobservation%20and%20action%20spaces%20of%20control%20policies%20for%20dexterous%20manipulation%0Atasks%20learned%20with%20RL.%20In%20terms%20of%20learning%20efficiency%20and%20consistency%2C%20the%0Aproposed%20representation%20outperforms%20a%20baseline%20that%20learns%20the%20same%20tasks%20in%0Ajoint%20space.%20We%20additionally%20show%20that%20PCHands%20performs%20robustly%20in%20RL%20from%0Ademonstration%2C%20when%20demonstrations%20are%20provided%20from%20a%20different%20manipulator.%0AWe%20further%20support%20our%20results%20with%20real-world%20experiments%20that%20involve%20a%0A2-finger%20gripper%20and%20a%204-finger%20anthropomorphic%20hand.%20Code%20and%20additional%0Amaterial%20are%20available%20at%20https%3A//hsp-iit.github.io/PCHands/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07945v1&entry.124074799=Read"},
{"title": "Gaussian Approximation for Two-Timescale Linear Stochastic Approximation", "author": "Bogdan Butyrin and Artemy Rubtsov and Alexey Naumov and Vladimir Ulyanov and Sergey Samsonov", "abstract": "  In this paper, we establish non-asymptotic bounds for accuracy of normal\napproximation for linear two-timescale stochastic approximation (TTSA)\nalgorithms driven by martingale difference or Markov noise. Focusing on both\nthe last iterate and Polyak-Ruppert averaging regimes, we derive bounds for\nnormal approximation in terms of the convex distance between probability\ndistributions. Our analysis reveals a non-trivial interaction between the fast\nand slow timescales: the normal approximation rate for the last iterate\nimproves as the timescale separation increases, while it decreases in the\nPolyak-Ruppert averaged setting. We also provide the high-order moment bounds\nfor the error of linear TTSA algorithm, which may be of independent interest.\n", "link": "http://arxiv.org/abs/2508.07928v1", "date": "2025-08-11", "relevancy": 2.2171, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4701}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation&body=Title%3A%20Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation%0AAuthor%3A%20Bogdan%20Butyrin%20and%20Artemy%20Rubtsov%20and%20Alexey%20Naumov%20and%20Vladimir%20Ulyanov%20and%20Sergey%20Samsonov%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20establish%20non-asymptotic%20bounds%20for%20accuracy%20of%20normal%0Aapproximation%20for%20linear%20two-timescale%20stochastic%20approximation%20%28TTSA%29%0Aalgorithms%20driven%20by%20martingale%20difference%20or%20Markov%20noise.%20Focusing%20on%20both%0Athe%20last%20iterate%20and%20Polyak-Ruppert%20averaging%20regimes%2C%20we%20derive%20bounds%20for%0Anormal%20approximation%20in%20terms%20of%20the%20convex%20distance%20between%20probability%0Adistributions.%20Our%20analysis%20reveals%20a%20non-trivial%20interaction%20between%20the%20fast%0Aand%20slow%20timescales%3A%20the%20normal%20approximation%20rate%20for%20the%20last%20iterate%0Aimproves%20as%20the%20timescale%20separation%20increases%2C%20while%20it%20decreases%20in%20the%0APolyak-Ruppert%20averaged%20setting.%20We%20also%20provide%20the%20high-order%20moment%20bounds%0Afor%20the%20error%20of%20linear%20TTSA%20algorithm%2C%20which%20may%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Approximation%2520for%2520Two-Timescale%2520Linear%2520Stochastic%2520Approximation%26entry.906535625%3DBogdan%2520Butyrin%2520and%2520Artemy%2520Rubtsov%2520and%2520Alexey%2520Naumov%2520and%2520Vladimir%2520Ulyanov%2520and%2520Sergey%2520Samsonov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520establish%2520non-asymptotic%2520bounds%2520for%2520accuracy%2520of%2520normal%250Aapproximation%2520for%2520linear%2520two-timescale%2520stochastic%2520approximation%2520%2528TTSA%2529%250Aalgorithms%2520driven%2520by%2520martingale%2520difference%2520or%2520Markov%2520noise.%2520Focusing%2520on%2520both%250Athe%2520last%2520iterate%2520and%2520Polyak-Ruppert%2520averaging%2520regimes%252C%2520we%2520derive%2520bounds%2520for%250Anormal%2520approximation%2520in%2520terms%2520of%2520the%2520convex%2520distance%2520between%2520probability%250Adistributions.%2520Our%2520analysis%2520reveals%2520a%2520non-trivial%2520interaction%2520between%2520the%2520fast%250Aand%2520slow%2520timescales%253A%2520the%2520normal%2520approximation%2520rate%2520for%2520the%2520last%2520iterate%250Aimproves%2520as%2520the%2520timescale%2520separation%2520increases%252C%2520while%2520it%2520decreases%2520in%2520the%250APolyak-Ruppert%2520averaged%2520setting.%2520We%2520also%2520provide%2520the%2520high-order%2520moment%2520bounds%250Afor%2520the%2520error%2520of%2520linear%2520TTSA%2520algorithm%252C%2520which%2520may%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation&entry.906535625=Bogdan%20Butyrin%20and%20Artemy%20Rubtsov%20and%20Alexey%20Naumov%20and%20Vladimir%20Ulyanov%20and%20Sergey%20Samsonov&entry.1292438233=%20%20In%20this%20paper%2C%20we%20establish%20non-asymptotic%20bounds%20for%20accuracy%20of%20normal%0Aapproximation%20for%20linear%20two-timescale%20stochastic%20approximation%20%28TTSA%29%0Aalgorithms%20driven%20by%20martingale%20difference%20or%20Markov%20noise.%20Focusing%20on%20both%0Athe%20last%20iterate%20and%20Polyak-Ruppert%20averaging%20regimes%2C%20we%20derive%20bounds%20for%0Anormal%20approximation%20in%20terms%20of%20the%20convex%20distance%20between%20probability%0Adistributions.%20Our%20analysis%20reveals%20a%20non-trivial%20interaction%20between%20the%20fast%0Aand%20slow%20timescales%3A%20the%20normal%20approximation%20rate%20for%20the%20last%20iterate%0Aimproves%20as%20the%20timescale%20separation%20increases%2C%20while%20it%20decreases%20in%20the%0APolyak-Ruppert%20averaged%20setting.%20We%20also%20provide%20the%20high-order%20moment%20bounds%0Afor%20the%20error%20of%20linear%20TTSA%20algorithm%2C%20which%20may%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07928v1&entry.124074799=Read"},
{"title": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models", "author": "Tianyi Zhou and Johanne Medina and Sanjay Chawla", "abstract": "  Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.\n", "link": "http://arxiv.org/abs/2508.08139v1", "date": "2025-08-11", "relevancy": 2.2158, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5857}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5722}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%0A%20%20Uncertainty-Aware%20Language%20Models&body=Title%3A%20Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%0A%20%20Uncertainty-Aware%20Language%20Models%0AAuthor%3A%20Tianyi%20Zhou%20and%20Johanne%20Medina%20and%20Sanjay%20Chawla%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20prone%20to%20generating%20fluent%20but%20incorrect%0Acontent%2C%20known%20as%20confabulation%2C%20which%20poses%20increasing%20risks%20in%20multi-turn%20or%0Aagentic%20applications%20where%20outputs%20may%20be%20reused%20as%20context.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20in-context%20information%20influences%20model%20behavior%20and%20whether%0ALLMs%20can%20identify%20their%20unreliable%20responses.%20We%20propose%20a%20reliability%0Aestimation%20that%20leverages%20token-level%20uncertainty%20to%20guide%20the%20aggregation%20of%0Ainternal%20model%20representations.%20Specifically%2C%20we%20compute%20aleatoric%20and%0Aepistemic%20uncertainty%20from%20output%20logits%20to%20identify%20salient%20tokens%20and%0Aaggregate%20their%20hidden%20states%20into%20compact%20representations%20for%20response-level%0Areliability%20prediction.%20Through%20controlled%20experiments%20on%20open%20QA%20benchmarks%2C%0Awe%20find%20that%20correct%20in-context%20information%20improves%20both%20answer%20accuracy%20and%0Amodel%20confidence%2C%20while%20misleading%20context%20often%20induces%20confidently%20incorrect%0Aresponses%2C%20revealing%20a%20misalignment%20between%20uncertainty%20and%20correctness.%20Our%0Aprobing-based%20method%20captures%20these%20shifts%20in%20model%20behavior%20and%20improves%20the%0Adetection%20of%20unreliable%20outputs%20across%20multiple%20open-source%20LLMs.%20These%20results%0Aunderscore%20the%20limitations%20of%20direct%20uncertainty%20signals%20and%20highlight%20the%0Apotential%20of%20uncertainty-guided%20probing%20for%20reliability-aware%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Detect%2520Their%2520Confabulations%253F%2520Estimating%2520Reliability%2520in%250A%2520%2520Uncertainty-Aware%2520Language%2520Models%26entry.906535625%3DTianyi%2520Zhou%2520and%2520Johanne%2520Medina%2520and%2520Sanjay%2520Chawla%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520prone%2520to%2520generating%2520fluent%2520but%2520incorrect%250Acontent%252C%2520known%2520as%2520confabulation%252C%2520which%2520poses%2520increasing%2520risks%2520in%2520multi-turn%2520or%250Aagentic%2520applications%2520where%2520outputs%2520may%2520be%2520reused%2520as%2520context.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520how%2520in-context%2520information%2520influences%2520model%2520behavior%2520and%2520whether%250ALLMs%2520can%2520identify%2520their%2520unreliable%2520responses.%2520We%2520propose%2520a%2520reliability%250Aestimation%2520that%2520leverages%2520token-level%2520uncertainty%2520to%2520guide%2520the%2520aggregation%2520of%250Ainternal%2520model%2520representations.%2520Specifically%252C%2520we%2520compute%2520aleatoric%2520and%250Aepistemic%2520uncertainty%2520from%2520output%2520logits%2520to%2520identify%2520salient%2520tokens%2520and%250Aaggregate%2520their%2520hidden%2520states%2520into%2520compact%2520representations%2520for%2520response-level%250Areliability%2520prediction.%2520Through%2520controlled%2520experiments%2520on%2520open%2520QA%2520benchmarks%252C%250Awe%2520find%2520that%2520correct%2520in-context%2520information%2520improves%2520both%2520answer%2520accuracy%2520and%250Amodel%2520confidence%252C%2520while%2520misleading%2520context%2520often%2520induces%2520confidently%2520incorrect%250Aresponses%252C%2520revealing%2520a%2520misalignment%2520between%2520uncertainty%2520and%2520correctness.%2520Our%250Aprobing-based%2520method%2520captures%2520these%2520shifts%2520in%2520model%2520behavior%2520and%2520improves%2520the%250Adetection%2520of%2520unreliable%2520outputs%2520across%2520multiple%2520open-source%2520LLMs.%2520These%2520results%250Aunderscore%2520the%2520limitations%2520of%2520direct%2520uncertainty%2520signals%2520and%2520highlight%2520the%250Apotential%2520of%2520uncertainty-guided%2520probing%2520for%2520reliability-aware%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Detect%20Their%20Confabulations%3F%20Estimating%20Reliability%20in%0A%20%20Uncertainty-Aware%20Language%20Models&entry.906535625=Tianyi%20Zhou%20and%20Johanne%20Medina%20and%20Sanjay%20Chawla&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20prone%20to%20generating%20fluent%20but%20incorrect%0Acontent%2C%20known%20as%20confabulation%2C%20which%20poses%20increasing%20risks%20in%20multi-turn%20or%0Aagentic%20applications%20where%20outputs%20may%20be%20reused%20as%20context.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20in-context%20information%20influences%20model%20behavior%20and%20whether%0ALLMs%20can%20identify%20their%20unreliable%20responses.%20We%20propose%20a%20reliability%0Aestimation%20that%20leverages%20token-level%20uncertainty%20to%20guide%20the%20aggregation%20of%0Ainternal%20model%20representations.%20Specifically%2C%20we%20compute%20aleatoric%20and%0Aepistemic%20uncertainty%20from%20output%20logits%20to%20identify%20salient%20tokens%20and%0Aaggregate%20their%20hidden%20states%20into%20compact%20representations%20for%20response-level%0Areliability%20prediction.%20Through%20controlled%20experiments%20on%20open%20QA%20benchmarks%2C%0Awe%20find%20that%20correct%20in-context%20information%20improves%20both%20answer%20accuracy%20and%0Amodel%20confidence%2C%20while%20misleading%20context%20often%20induces%20confidently%20incorrect%0Aresponses%2C%20revealing%20a%20misalignment%20between%20uncertainty%20and%20correctness.%20Our%0Aprobing-based%20method%20captures%20these%20shifts%20in%20model%20behavior%20and%20improves%20the%0Adetection%20of%20unreliable%20outputs%20across%20multiple%20open-source%20LLMs.%20These%20results%0Aunderscore%20the%20limitations%20of%20direct%20uncertainty%20signals%20and%20highlight%20the%0Apotential%20of%20uncertainty-guided%20probing%20for%20reliability-aware%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08139v1&entry.124074799=Read"},
{"title": "L-FUSION: Laplacian Fetal Ultrasound Segmentation & Uncertainty\n  Estimation", "author": "Johanna P. M\u00fcller and Robert Wright and Thomas G. Day and Lorenzo Venturini and Samuel F. Budd and Hadrien Reynaud and Joseph V. Hajnal and Reza Razavi and Bernhard Kainz", "abstract": "  Accurate analysis of prenatal ultrasound (US) is essential for early\ndetection of developmental anomalies. However, operator dependency and\ntechnical limitations (e.g. intrinsic artefacts and effects, setting errors)\ncan complicate image interpretation and the assessment of diagnostic\nuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation with\nIntegrated FoundatiON models), a framework that integrates uncertainty\nquantification through unsupervised, normative learning and large-scale\nfoundation models for robust segmentation of fetal structures in normal and\npathological scans. We propose to utilise the aleatoric logit distributions of\nStochastic Segmentation Networks and Laplace approximations with fast Hessian\nestimations to estimate epistemic uncertainty only from the segmentation head.\nThis enables us to achieve reliable abnormality quantification for instant\ndiagnostic feedback. Combined with an integrated Dropout component, L-FUSION\nenables reliable differentiation of lesions from normal fetal anatomy with\nenhanced uncertainty maps and segmentation counterfactuals in US imaging. It\nimproves epistemic and aleatoric uncertainty interpretation and removes the\nneed for manual disease-labelling. Evaluations across multiple datasets show\nthat L-FUSION achieves superior segmentation accuracy and consistent\nuncertainty quantification, supporting on-site decision-making and offering a\nscalable solution for advancing fetal ultrasound analysis in clinical settings.\n", "link": "http://arxiv.org/abs/2503.05245v4", "date": "2025-08-11", "relevancy": 2.2072, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5817}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-FUSION%3A%20Laplacian%20Fetal%20Ultrasound%20Segmentation%20%26%20Uncertainty%0A%20%20Estimation&body=Title%3A%20L-FUSION%3A%20Laplacian%20Fetal%20Ultrasound%20Segmentation%20%26%20Uncertainty%0A%20%20Estimation%0AAuthor%3A%20Johanna%20P.%20M%C3%BCller%20and%20Robert%20Wright%20and%20Thomas%20G.%20Day%20and%20Lorenzo%20Venturini%20and%20Samuel%20F.%20Budd%20and%20Hadrien%20Reynaud%20and%20Joseph%20V.%20Hajnal%20and%20Reza%20Razavi%20and%20Bernhard%20Kainz%0AAbstract%3A%20%20%20Accurate%20analysis%20of%20prenatal%20ultrasound%20%28US%29%20is%20essential%20for%20early%0Adetection%20of%20developmental%20anomalies.%20However%2C%20operator%20dependency%20and%0Atechnical%20limitations%20%28e.g.%20intrinsic%20artefacts%20and%20effects%2C%20setting%20errors%29%0Acan%20complicate%20image%20interpretation%20and%20the%20assessment%20of%20diagnostic%0Auncertainty.%20We%20present%20L-FUSION%20%28Laplacian%20Fetal%20US%20Segmentation%20with%0AIntegrated%20FoundatiON%20models%29%2C%20a%20framework%20that%20integrates%20uncertainty%0Aquantification%20through%20unsupervised%2C%20normative%20learning%20and%20large-scale%0Afoundation%20models%20for%20robust%20segmentation%20of%20fetal%20structures%20in%20normal%20and%0Apathological%20scans.%20We%20propose%20to%20utilise%20the%20aleatoric%20logit%20distributions%20of%0AStochastic%20Segmentation%20Networks%20and%20Laplace%20approximations%20with%20fast%20Hessian%0Aestimations%20to%20estimate%20epistemic%20uncertainty%20only%20from%20the%20segmentation%20head.%0AThis%20enables%20us%20to%20achieve%20reliable%20abnormality%20quantification%20for%20instant%0Adiagnostic%20feedback.%20Combined%20with%20an%20integrated%20Dropout%20component%2C%20L-FUSION%0Aenables%20reliable%20differentiation%20of%20lesions%20from%20normal%20fetal%20anatomy%20with%0Aenhanced%20uncertainty%20maps%20and%20segmentation%20counterfactuals%20in%20US%20imaging.%20It%0Aimproves%20epistemic%20and%20aleatoric%20uncertainty%20interpretation%20and%20removes%20the%0Aneed%20for%20manual%20disease-labelling.%20Evaluations%20across%20multiple%20datasets%20show%0Athat%20L-FUSION%20achieves%20superior%20segmentation%20accuracy%20and%20consistent%0Auncertainty%20quantification%2C%20supporting%20on-site%20decision-making%20and%20offering%20a%0Ascalable%20solution%20for%20advancing%20fetal%20ultrasound%20analysis%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05245v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-FUSION%253A%2520Laplacian%2520Fetal%2520Ultrasound%2520Segmentation%2520%2526%2520Uncertainty%250A%2520%2520Estimation%26entry.906535625%3DJohanna%2520P.%2520M%25C3%25BCller%2520and%2520Robert%2520Wright%2520and%2520Thomas%2520G.%2520Day%2520and%2520Lorenzo%2520Venturini%2520and%2520Samuel%2520F.%2520Budd%2520and%2520Hadrien%2520Reynaud%2520and%2520Joseph%2520V.%2520Hajnal%2520and%2520Reza%2520Razavi%2520and%2520Bernhard%2520Kainz%26entry.1292438233%3D%2520%2520Accurate%2520analysis%2520of%2520prenatal%2520ultrasound%2520%2528US%2529%2520is%2520essential%2520for%2520early%250Adetection%2520of%2520developmental%2520anomalies.%2520However%252C%2520operator%2520dependency%2520and%250Atechnical%2520limitations%2520%2528e.g.%2520intrinsic%2520artefacts%2520and%2520effects%252C%2520setting%2520errors%2529%250Acan%2520complicate%2520image%2520interpretation%2520and%2520the%2520assessment%2520of%2520diagnostic%250Auncertainty.%2520We%2520present%2520L-FUSION%2520%2528Laplacian%2520Fetal%2520US%2520Segmentation%2520with%250AIntegrated%2520FoundatiON%2520models%2529%252C%2520a%2520framework%2520that%2520integrates%2520uncertainty%250Aquantification%2520through%2520unsupervised%252C%2520normative%2520learning%2520and%2520large-scale%250Afoundation%2520models%2520for%2520robust%2520segmentation%2520of%2520fetal%2520structures%2520in%2520normal%2520and%250Apathological%2520scans.%2520We%2520propose%2520to%2520utilise%2520the%2520aleatoric%2520logit%2520distributions%2520of%250AStochastic%2520Segmentation%2520Networks%2520and%2520Laplace%2520approximations%2520with%2520fast%2520Hessian%250Aestimations%2520to%2520estimate%2520epistemic%2520uncertainty%2520only%2520from%2520the%2520segmentation%2520head.%250AThis%2520enables%2520us%2520to%2520achieve%2520reliable%2520abnormality%2520quantification%2520for%2520instant%250Adiagnostic%2520feedback.%2520Combined%2520with%2520an%2520integrated%2520Dropout%2520component%252C%2520L-FUSION%250Aenables%2520reliable%2520differentiation%2520of%2520lesions%2520from%2520normal%2520fetal%2520anatomy%2520with%250Aenhanced%2520uncertainty%2520maps%2520and%2520segmentation%2520counterfactuals%2520in%2520US%2520imaging.%2520It%250Aimproves%2520epistemic%2520and%2520aleatoric%2520uncertainty%2520interpretation%2520and%2520removes%2520the%250Aneed%2520for%2520manual%2520disease-labelling.%2520Evaluations%2520across%2520multiple%2520datasets%2520show%250Athat%2520L-FUSION%2520achieves%2520superior%2520segmentation%2520accuracy%2520and%2520consistent%250Auncertainty%2520quantification%252C%2520supporting%2520on-site%2520decision-making%2520and%2520offering%2520a%250Ascalable%2520solution%2520for%2520advancing%2520fetal%2520ultrasound%2520analysis%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05245v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-FUSION%3A%20Laplacian%20Fetal%20Ultrasound%20Segmentation%20%26%20Uncertainty%0A%20%20Estimation&entry.906535625=Johanna%20P.%20M%C3%BCller%20and%20Robert%20Wright%20and%20Thomas%20G.%20Day%20and%20Lorenzo%20Venturini%20and%20Samuel%20F.%20Budd%20and%20Hadrien%20Reynaud%20and%20Joseph%20V.%20Hajnal%20and%20Reza%20Razavi%20and%20Bernhard%20Kainz&entry.1292438233=%20%20Accurate%20analysis%20of%20prenatal%20ultrasound%20%28US%29%20is%20essential%20for%20early%0Adetection%20of%20developmental%20anomalies.%20However%2C%20operator%20dependency%20and%0Atechnical%20limitations%20%28e.g.%20intrinsic%20artefacts%20and%20effects%2C%20setting%20errors%29%0Acan%20complicate%20image%20interpretation%20and%20the%20assessment%20of%20diagnostic%0Auncertainty.%20We%20present%20L-FUSION%20%28Laplacian%20Fetal%20US%20Segmentation%20with%0AIntegrated%20FoundatiON%20models%29%2C%20a%20framework%20that%20integrates%20uncertainty%0Aquantification%20through%20unsupervised%2C%20normative%20learning%20and%20large-scale%0Afoundation%20models%20for%20robust%20segmentation%20of%20fetal%20structures%20in%20normal%20and%0Apathological%20scans.%20We%20propose%20to%20utilise%20the%20aleatoric%20logit%20distributions%20of%0AStochastic%20Segmentation%20Networks%20and%20Laplace%20approximations%20with%20fast%20Hessian%0Aestimations%20to%20estimate%20epistemic%20uncertainty%20only%20from%20the%20segmentation%20head.%0AThis%20enables%20us%20to%20achieve%20reliable%20abnormality%20quantification%20for%20instant%0Adiagnostic%20feedback.%20Combined%20with%20an%20integrated%20Dropout%20component%2C%20L-FUSION%0Aenables%20reliable%20differentiation%20of%20lesions%20from%20normal%20fetal%20anatomy%20with%0Aenhanced%20uncertainty%20maps%20and%20segmentation%20counterfactuals%20in%20US%20imaging.%20It%0Aimproves%20epistemic%20and%20aleatoric%20uncertainty%20interpretation%20and%20removes%20the%0Aneed%20for%20manual%20disease-labelling.%20Evaluations%20across%20multiple%20datasets%20show%0Athat%20L-FUSION%20achieves%20superior%20segmentation%20accuracy%20and%20consistent%0Auncertainty%20quantification%2C%20supporting%20on-site%20decision-making%20and%20offering%20a%0Ascalable%20solution%20for%20advancing%20fetal%20ultrasound%20analysis%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05245v4&entry.124074799=Read"},
{"title": "Hyperspectral Imaging", "author": "Danfeng Hong and Chenyu Li and Naoto Yokoya and Bing Zhang and Xiuping Jia and Antonio Plaza and Paolo Gamba and Jon Atli Benediktsson and Jocelyn Chanussot", "abstract": "  Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society.\n", "link": "http://arxiv.org/abs/2508.08107v1", "date": "2025-08-11", "relevancy": 2.2058, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Imaging&body=Title%3A%20Hyperspectral%20Imaging%0AAuthor%3A%20Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Naoto%20Yokoya%20and%20Bing%20Zhang%20and%20Xiuping%20Jia%20and%20Antonio%20Plaza%20and%20Paolo%20Gamba%20and%20Jon%20Atli%20Benediktsson%20and%20Jocelyn%20Chanussot%0AAbstract%3A%20%20%20Hyperspectral%20imaging%20%28HSI%29%20is%20an%20advanced%20sensing%20modality%20that%0Asimultaneously%20captures%20spatial%20and%20spectral%20information%2C%20enabling%0Anon-invasive%2C%20label-free%20analysis%20of%20material%2C%20chemical%2C%20and%20biological%0Aproperties.%20This%20Primer%20presents%20a%20comprehensive%20overview%20of%20HSI%2C%20from%20the%0Aunderlying%20physical%20principles%20and%20sensor%20architectures%20to%20key%20steps%20in%20data%0Aacquisition%2C%20calibration%2C%20and%20correction.%20We%20summarize%20common%20data%20structures%0Aand%20highlight%20classical%20and%20modern%20analysis%20methods%2C%20including%20dimensionality%0Areduction%2C%20classification%2C%20spectral%20unmixing%2C%20and%20AI-driven%20techniques%20such%20as%0Adeep%20learning.%20Representative%20applications%20across%20Earth%20observation%2C%20precision%0Aagriculture%2C%20biomedicine%2C%20industrial%20inspection%2C%20cultural%20heritage%2C%20and%0Asecurity%20are%20also%20discussed%2C%20emphasizing%20HSI%27s%20ability%20to%20uncover%20sub-visual%0Afeatures%20for%20advanced%20monitoring%2C%20diagnostics%2C%20and%20decision-making.%20Persistent%0Achallenges%2C%20such%20as%20hardware%20trade-offs%2C%20acquisition%20variability%2C%20and%20the%0Acomplexity%20of%20high-dimensional%20data%2C%20are%20examined%20alongside%20emerging%20solutions%2C%0Aincluding%20computational%20imaging%2C%20physics-informed%20modeling%2C%20cross-modal%20fusion%2C%0Aand%20self-supervised%20learning.%20Best%20practices%20for%20dataset%20sharing%2C%0Areproducibility%2C%20and%20metadata%20documentation%20are%20further%20highlighted%20to%20support%0Atransparency%20and%20reuse.%20Looking%20ahead%2C%20we%20explore%20future%20directions%20toward%0Ascalable%2C%20real-time%2C%20and%20embedded%20HSI%20systems%2C%20driven%20by%20sensor%0Aminiaturization%2C%20self-supervised%20learning%2C%20and%20foundation%20models.%20As%20HSI%0Aevolves%20into%20a%20general-purpose%2C%20cross-disciplinary%20platform%2C%20it%20holds%20promise%0Afor%20transformative%20applications%20in%20science%2C%20technology%2C%20and%20society.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Imaging%26entry.906535625%3DDanfeng%2520Hong%2520and%2520Chenyu%2520Li%2520and%2520Naoto%2520Yokoya%2520and%2520Bing%2520Zhang%2520and%2520Xiuping%2520Jia%2520and%2520Antonio%2520Plaza%2520and%2520Paolo%2520Gamba%2520and%2520Jon%2520Atli%2520Benediktsson%2520and%2520Jocelyn%2520Chanussot%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%2520%2528HSI%2529%2520is%2520an%2520advanced%2520sensing%2520modality%2520that%250Asimultaneously%2520captures%2520spatial%2520and%2520spectral%2520information%252C%2520enabling%250Anon-invasive%252C%2520label-free%2520analysis%2520of%2520material%252C%2520chemical%252C%2520and%2520biological%250Aproperties.%2520This%2520Primer%2520presents%2520a%2520comprehensive%2520overview%2520of%2520HSI%252C%2520from%2520the%250Aunderlying%2520physical%2520principles%2520and%2520sensor%2520architectures%2520to%2520key%2520steps%2520in%2520data%250Aacquisition%252C%2520calibration%252C%2520and%2520correction.%2520We%2520summarize%2520common%2520data%2520structures%250Aand%2520highlight%2520classical%2520and%2520modern%2520analysis%2520methods%252C%2520including%2520dimensionality%250Areduction%252C%2520classification%252C%2520spectral%2520unmixing%252C%2520and%2520AI-driven%2520techniques%2520such%2520as%250Adeep%2520learning.%2520Representative%2520applications%2520across%2520Earth%2520observation%252C%2520precision%250Aagriculture%252C%2520biomedicine%252C%2520industrial%2520inspection%252C%2520cultural%2520heritage%252C%2520and%250Asecurity%2520are%2520also%2520discussed%252C%2520emphasizing%2520HSI%2527s%2520ability%2520to%2520uncover%2520sub-visual%250Afeatures%2520for%2520advanced%2520monitoring%252C%2520diagnostics%252C%2520and%2520decision-making.%2520Persistent%250Achallenges%252C%2520such%2520as%2520hardware%2520trade-offs%252C%2520acquisition%2520variability%252C%2520and%2520the%250Acomplexity%2520of%2520high-dimensional%2520data%252C%2520are%2520examined%2520alongside%2520emerging%2520solutions%252C%250Aincluding%2520computational%2520imaging%252C%2520physics-informed%2520modeling%252C%2520cross-modal%2520fusion%252C%250Aand%2520self-supervised%2520learning.%2520Best%2520practices%2520for%2520dataset%2520sharing%252C%250Areproducibility%252C%2520and%2520metadata%2520documentation%2520are%2520further%2520highlighted%2520to%2520support%250Atransparency%2520and%2520reuse.%2520Looking%2520ahead%252C%2520we%2520explore%2520future%2520directions%2520toward%250Ascalable%252C%2520real-time%252C%2520and%2520embedded%2520HSI%2520systems%252C%2520driven%2520by%2520sensor%250Aminiaturization%252C%2520self-supervised%2520learning%252C%2520and%2520foundation%2520models.%2520As%2520HSI%250Aevolves%2520into%2520a%2520general-purpose%252C%2520cross-disciplinary%2520platform%252C%2520it%2520holds%2520promise%250Afor%2520transformative%2520applications%2520in%2520science%252C%2520technology%252C%2520and%2520society.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Imaging&entry.906535625=Danfeng%20Hong%20and%20Chenyu%20Li%20and%20Naoto%20Yokoya%20and%20Bing%20Zhang%20and%20Xiuping%20Jia%20and%20Antonio%20Plaza%20and%20Paolo%20Gamba%20and%20Jon%20Atli%20Benediktsson%20and%20Jocelyn%20Chanussot&entry.1292438233=%20%20Hyperspectral%20imaging%20%28HSI%29%20is%20an%20advanced%20sensing%20modality%20that%0Asimultaneously%20captures%20spatial%20and%20spectral%20information%2C%20enabling%0Anon-invasive%2C%20label-free%20analysis%20of%20material%2C%20chemical%2C%20and%20biological%0Aproperties.%20This%20Primer%20presents%20a%20comprehensive%20overview%20of%20HSI%2C%20from%20the%0Aunderlying%20physical%20principles%20and%20sensor%20architectures%20to%20key%20steps%20in%20data%0Aacquisition%2C%20calibration%2C%20and%20correction.%20We%20summarize%20common%20data%20structures%0Aand%20highlight%20classical%20and%20modern%20analysis%20methods%2C%20including%20dimensionality%0Areduction%2C%20classification%2C%20spectral%20unmixing%2C%20and%20AI-driven%20techniques%20such%20as%0Adeep%20learning.%20Representative%20applications%20across%20Earth%20observation%2C%20precision%0Aagriculture%2C%20biomedicine%2C%20industrial%20inspection%2C%20cultural%20heritage%2C%20and%0Asecurity%20are%20also%20discussed%2C%20emphasizing%20HSI%27s%20ability%20to%20uncover%20sub-visual%0Afeatures%20for%20advanced%20monitoring%2C%20diagnostics%2C%20and%20decision-making.%20Persistent%0Achallenges%2C%20such%20as%20hardware%20trade-offs%2C%20acquisition%20variability%2C%20and%20the%0Acomplexity%20of%20high-dimensional%20data%2C%20are%20examined%20alongside%20emerging%20solutions%2C%0Aincluding%20computational%20imaging%2C%20physics-informed%20modeling%2C%20cross-modal%20fusion%2C%0Aand%20self-supervised%20learning.%20Best%20practices%20for%20dataset%20sharing%2C%0Areproducibility%2C%20and%20metadata%20documentation%20are%20further%20highlighted%20to%20support%0Atransparency%20and%20reuse.%20Looking%20ahead%2C%20we%20explore%20future%20directions%20toward%0Ascalable%2C%20real-time%2C%20and%20embedded%20HSI%20systems%2C%20driven%20by%20sensor%0Aminiaturization%2C%20self-supervised%20learning%2C%20and%20foundation%20models.%20As%20HSI%0Aevolves%20into%20a%20general-purpose%2C%20cross-disciplinary%20platform%2C%20it%20holds%20promise%0Afor%20transformative%20applications%20in%20science%2C%20technology%2C%20and%20society.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08107v1&entry.124074799=Read"},
{"title": "Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series\n  Forecasting", "author": "Amal Saadallah and Abdulaziz Al-Ademi", "abstract": "  Time series forecasting poses significant challenges in non-stationary\nenvironments where underlying patterns evolve over time. In this work, we\npropose a novel framework that enhances deep neural network (DNN) performance\nby leveraging specialized model adaptation and selection. Initially, a base DNN\nis trained offline on historical time series data. A reserved validation subset\nis then segmented to extract and cluster the most dominant patterns within the\nseries, thereby identifying distinct regimes. For each identified cluster, the\nbase DNN is fine-tuned to produce a specialized version that captures unique\npattern characteristics. At inference, the most recent input is matched against\nthe cluster centroids, and the corresponding fine-tuned version is deployed\nbased on the closest similarity measure. Additionally, our approach integrates\na concept drift detection mechanism to identify and adapt to emerging patterns\ncaused by non-stationary behavior. The proposed framework is generalizable\nacross various DNN architectures and has demonstrated significant performance\ngains on both traditional DNNs and recent advanced architectures implemented in\nthe GluonTS library.\n", "link": "http://arxiv.org/abs/2508.07927v1", "date": "2025-08-11", "relevancy": 2.2026, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5964}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Fine-Tuning%20via%20Pattern%20Specialization%20for%20Deep%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Adaptive%20Fine-Tuning%20via%20Pattern%20Specialization%20for%20Deep%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Amal%20Saadallah%20and%20Abdulaziz%20Al-Ademi%0AAbstract%3A%20%20%20Time%20series%20forecasting%20poses%20significant%20challenges%20in%20non-stationary%0Aenvironments%20where%20underlying%20patterns%20evolve%20over%20time.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20framework%20that%20enhances%20deep%20neural%20network%20%28DNN%29%20performance%0Aby%20leveraging%20specialized%20model%20adaptation%20and%20selection.%20Initially%2C%20a%20base%20DNN%0Ais%20trained%20offline%20on%20historical%20time%20series%20data.%20A%20reserved%20validation%20subset%0Ais%20then%20segmented%20to%20extract%20and%20cluster%20the%20most%20dominant%20patterns%20within%20the%0Aseries%2C%20thereby%20identifying%20distinct%20regimes.%20For%20each%20identified%20cluster%2C%20the%0Abase%20DNN%20is%20fine-tuned%20to%20produce%20a%20specialized%20version%20that%20captures%20unique%0Apattern%20characteristics.%20At%20inference%2C%20the%20most%20recent%20input%20is%20matched%20against%0Athe%20cluster%20centroids%2C%20and%20the%20corresponding%20fine-tuned%20version%20is%20deployed%0Abased%20on%20the%20closest%20similarity%20measure.%20Additionally%2C%20our%20approach%20integrates%0Aa%20concept%20drift%20detection%20mechanism%20to%20identify%20and%20adapt%20to%20emerging%20patterns%0Acaused%20by%20non-stationary%20behavior.%20The%20proposed%20framework%20is%20generalizable%0Aacross%20various%20DNN%20architectures%20and%20has%20demonstrated%20significant%20performance%0Agains%20on%20both%20traditional%20DNNs%20and%20recent%20advanced%20architectures%20implemented%20in%0Athe%20GluonTS%20library.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Fine-Tuning%2520via%2520Pattern%2520Specialization%2520for%2520Deep%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DAmal%2520Saadallah%2520and%2520Abdulaziz%2520Al-Ademi%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520poses%2520significant%2520challenges%2520in%2520non-stationary%250Aenvironments%2520where%2520underlying%2520patterns%2520evolve%2520over%2520time.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520that%2520enhances%2520deep%2520neural%2520network%2520%2528DNN%2529%2520performance%250Aby%2520leveraging%2520specialized%2520model%2520adaptation%2520and%2520selection.%2520Initially%252C%2520a%2520base%2520DNN%250Ais%2520trained%2520offline%2520on%2520historical%2520time%2520series%2520data.%2520A%2520reserved%2520validation%2520subset%250Ais%2520then%2520segmented%2520to%2520extract%2520and%2520cluster%2520the%2520most%2520dominant%2520patterns%2520within%2520the%250Aseries%252C%2520thereby%2520identifying%2520distinct%2520regimes.%2520For%2520each%2520identified%2520cluster%252C%2520the%250Abase%2520DNN%2520is%2520fine-tuned%2520to%2520produce%2520a%2520specialized%2520version%2520that%2520captures%2520unique%250Apattern%2520characteristics.%2520At%2520inference%252C%2520the%2520most%2520recent%2520input%2520is%2520matched%2520against%250Athe%2520cluster%2520centroids%252C%2520and%2520the%2520corresponding%2520fine-tuned%2520version%2520is%2520deployed%250Abased%2520on%2520the%2520closest%2520similarity%2520measure.%2520Additionally%252C%2520our%2520approach%2520integrates%250Aa%2520concept%2520drift%2520detection%2520mechanism%2520to%2520identify%2520and%2520adapt%2520to%2520emerging%2520patterns%250Acaused%2520by%2520non-stationary%2520behavior.%2520The%2520proposed%2520framework%2520is%2520generalizable%250Aacross%2520various%2520DNN%2520architectures%2520and%2520has%2520demonstrated%2520significant%2520performance%250Agains%2520on%2520both%2520traditional%2520DNNs%2520and%2520recent%2520advanced%2520architectures%2520implemented%2520in%250Athe%2520GluonTS%2520library.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Fine-Tuning%20via%20Pattern%20Specialization%20for%20Deep%20Time%20Series%0A%20%20Forecasting&entry.906535625=Amal%20Saadallah%20and%20Abdulaziz%20Al-Ademi&entry.1292438233=%20%20Time%20series%20forecasting%20poses%20significant%20challenges%20in%20non-stationary%0Aenvironments%20where%20underlying%20patterns%20evolve%20over%20time.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20framework%20that%20enhances%20deep%20neural%20network%20%28DNN%29%20performance%0Aby%20leveraging%20specialized%20model%20adaptation%20and%20selection.%20Initially%2C%20a%20base%20DNN%0Ais%20trained%20offline%20on%20historical%20time%20series%20data.%20A%20reserved%20validation%20subset%0Ais%20then%20segmented%20to%20extract%20and%20cluster%20the%20most%20dominant%20patterns%20within%20the%0Aseries%2C%20thereby%20identifying%20distinct%20regimes.%20For%20each%20identified%20cluster%2C%20the%0Abase%20DNN%20is%20fine-tuned%20to%20produce%20a%20specialized%20version%20that%20captures%20unique%0Apattern%20characteristics.%20At%20inference%2C%20the%20most%20recent%20input%20is%20matched%20against%0Athe%20cluster%20centroids%2C%20and%20the%20corresponding%20fine-tuned%20version%20is%20deployed%0Abased%20on%20the%20closest%20similarity%20measure.%20Additionally%2C%20our%20approach%20integrates%0Aa%20concept%20drift%20detection%20mechanism%20to%20identify%20and%20adapt%20to%20emerging%20patterns%0Acaused%20by%20non-stationary%20behavior.%20The%20proposed%20framework%20is%20generalizable%0Aacross%20various%20DNN%20architectures%20and%20has%20demonstrated%20significant%20performance%0Agains%20on%20both%20traditional%20DNNs%20and%20recent%20advanced%20architectures%20implemented%20in%0Athe%20GluonTS%20library.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07927v1&entry.124074799=Read"},
{"title": "SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias\n  Analysis", "author": "Vojt\u011bch Stan\u011bk and Karel Srna and Anton Firc and Kamil Malinka", "abstract": "  Despite growing attention to deepfake speech detection, the aspects of bias\nand fairness remain underexplored in the speech domain. To address this gap, we\nintroduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly\nannotated resource enabling systematic evaluation of demographic biases in\ndeepfake speech detection. SCDF contains over 237,000 utterances in a balanced\nrepresentation of both male and female speakers spanning five languages and a\nwide age range. We evaluate several state-of-the-art detectors and show that\nspeaker characteristics significantly influence detection performance,\nrevealing disparities across sex, language, age, and synthesizer type. These\nfindings highlight the need for bias-aware development and provide a foundation\nfor building non-discriminatory deepfake detection systems aligned with ethical\nand regulatory standards.\n", "link": "http://arxiv.org/abs/2508.07944v1", "date": "2025-08-11", "relevancy": 2.1863, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCDF%3A%20A%20Speaker%20Characteristics%20DeepFake%20Speech%20Dataset%20for%20Bias%0A%20%20Analysis&body=Title%3A%20SCDF%3A%20A%20Speaker%20Characteristics%20DeepFake%20Speech%20Dataset%20for%20Bias%0A%20%20Analysis%0AAuthor%3A%20Vojt%C4%9Bch%20Stan%C4%9Bk%20and%20Karel%20Srna%20and%20Anton%20Firc%20and%20Kamil%20Malinka%0AAbstract%3A%20%20%20Despite%20growing%20attention%20to%20deepfake%20speech%20detection%2C%20the%20aspects%20of%20bias%0Aand%20fairness%20remain%20underexplored%20in%20the%20speech%20domain.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20Speaker%20Characteristics%20Deepfake%20%28SCDF%29%20dataset%3A%20a%20novel%2C%20richly%0Aannotated%20resource%20enabling%20systematic%20evaluation%20of%20demographic%20biases%20in%0Adeepfake%20speech%20detection.%20SCDF%20contains%20over%20237%2C000%20utterances%20in%20a%20balanced%0Arepresentation%20of%20both%20male%20and%20female%20speakers%20spanning%20five%20languages%20and%20a%0Awide%20age%20range.%20We%20evaluate%20several%20state-of-the-art%20detectors%20and%20show%20that%0Aspeaker%20characteristics%20significantly%20influence%20detection%20performance%2C%0Arevealing%20disparities%20across%20sex%2C%20language%2C%20age%2C%20and%20synthesizer%20type.%20These%0Afindings%20highlight%20the%20need%20for%20bias-aware%20development%20and%20provide%20a%20foundation%0Afor%20building%20non-discriminatory%20deepfake%20detection%20systems%20aligned%20with%20ethical%0Aand%20regulatory%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCDF%253A%2520A%2520Speaker%2520Characteristics%2520DeepFake%2520Speech%2520Dataset%2520for%2520Bias%250A%2520%2520Analysis%26entry.906535625%3DVojt%25C4%259Bch%2520Stan%25C4%259Bk%2520and%2520Karel%2520Srna%2520and%2520Anton%2520Firc%2520and%2520Kamil%2520Malinka%26entry.1292438233%3D%2520%2520Despite%2520growing%2520attention%2520to%2520deepfake%2520speech%2520detection%252C%2520the%2520aspects%2520of%2520bias%250Aand%2520fairness%2520remain%2520underexplored%2520in%2520the%2520speech%2520domain.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520the%2520Speaker%2520Characteristics%2520Deepfake%2520%2528SCDF%2529%2520dataset%253A%2520a%2520novel%252C%2520richly%250Aannotated%2520resource%2520enabling%2520systematic%2520evaluation%2520of%2520demographic%2520biases%2520in%250Adeepfake%2520speech%2520detection.%2520SCDF%2520contains%2520over%2520237%252C000%2520utterances%2520in%2520a%2520balanced%250Arepresentation%2520of%2520both%2520male%2520and%2520female%2520speakers%2520spanning%2520five%2520languages%2520and%2520a%250Awide%2520age%2520range.%2520We%2520evaluate%2520several%2520state-of-the-art%2520detectors%2520and%2520show%2520that%250Aspeaker%2520characteristics%2520significantly%2520influence%2520detection%2520performance%252C%250Arevealing%2520disparities%2520across%2520sex%252C%2520language%252C%2520age%252C%2520and%2520synthesizer%2520type.%2520These%250Afindings%2520highlight%2520the%2520need%2520for%2520bias-aware%2520development%2520and%2520provide%2520a%2520foundation%250Afor%2520building%2520non-discriminatory%2520deepfake%2520detection%2520systems%2520aligned%2520with%2520ethical%250Aand%2520regulatory%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCDF%3A%20A%20Speaker%20Characteristics%20DeepFake%20Speech%20Dataset%20for%20Bias%0A%20%20Analysis&entry.906535625=Vojt%C4%9Bch%20Stan%C4%9Bk%20and%20Karel%20Srna%20and%20Anton%20Firc%20and%20Kamil%20Malinka&entry.1292438233=%20%20Despite%20growing%20attention%20to%20deepfake%20speech%20detection%2C%20the%20aspects%20of%20bias%0Aand%20fairness%20remain%20underexplored%20in%20the%20speech%20domain.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20the%20Speaker%20Characteristics%20Deepfake%20%28SCDF%29%20dataset%3A%20a%20novel%2C%20richly%0Aannotated%20resource%20enabling%20systematic%20evaluation%20of%20demographic%20biases%20in%0Adeepfake%20speech%20detection.%20SCDF%20contains%20over%20237%2C000%20utterances%20in%20a%20balanced%0Arepresentation%20of%20both%20male%20and%20female%20speakers%20spanning%20five%20languages%20and%20a%0Awide%20age%20range.%20We%20evaluate%20several%20state-of-the-art%20detectors%20and%20show%20that%0Aspeaker%20characteristics%20significantly%20influence%20detection%20performance%2C%0Arevealing%20disparities%20across%20sex%2C%20language%2C%20age%2C%20and%20synthesizer%20type.%20These%0Afindings%20highlight%20the%20need%20for%20bias-aware%20development%20and%20provide%20a%20foundation%0Afor%20building%20non-discriminatory%20deepfake%20detection%20systems%20aligned%20with%20ethical%0Aand%20regulatory%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07944v1&entry.124074799=Read"},
{"title": "Integrating Task-Specific and Universal Adapters for Pre-Trained\n  Model-based Class-Incremental Learning", "author": "Yan Wang and Da-Wei Zhou and Han-Jia Ye", "abstract": "  Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA\n", "link": "http://arxiv.org/abs/2508.08165v1", "date": "2025-08-11", "relevancy": 2.1802, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6247}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4898}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Task-Specific%20and%20Universal%20Adapters%20for%20Pre-Trained%0A%20%20Model-based%20Class-Incremental%20Learning&body=Title%3A%20Integrating%20Task-Specific%20and%20Universal%20Adapters%20for%20Pre-Trained%0A%20%20Model-based%20Class-Incremental%20Learning%0AAuthor%3A%20Yan%20Wang%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Class-Incremental%20Learning%20%28CIL%29%20requires%20a%20learning%20system%20to%20continually%0Alearn%20new%20classes%20without%20forgetting.%20Existing%20pre-trained%20model-based%20CIL%0Amethods%20often%20freeze%20the%20pre-trained%20network%20and%20adapt%20to%20incremental%20tasks%0Ausing%20additional%20lightweight%20modules%20such%20as%20adapters.%20However%2C%20incorrect%0Amodule%20selection%20during%20inference%20hurts%20performance%2C%20and%20task-specific%20modules%0Aoften%20overlook%20shared%20general%20knowledge%2C%20leading%20to%20errors%20on%20distinguishing%0Abetween%20similar%20classes%20across%20tasks.%20To%20address%20the%20aforementioned%20challenges%2C%0Awe%20propose%20integrating%20Task-Specific%20and%20Universal%20Adapters%20%28TUNA%29%20in%20this%0Apaper.%20Specifically%2C%20we%20train%20task-specific%20adapters%20to%20capture%20the%20most%0Acrucial%20features%20relevant%20to%20their%20respective%20tasks%20and%20introduce%20an%0Aentropy-based%20selection%20mechanism%20to%20choose%20the%20most%20suitable%20adapter.%0AFurthermore%2C%20we%20leverage%20an%20adapter%20fusion%20strategy%20to%20construct%20a%20universal%0Aadapter%2C%20which%20encodes%20the%20most%20discriminative%20features%20shared%20across%20tasks.%20We%0Acombine%20task-specific%20and%20universal%20adapter%20predictions%20to%20harness%20both%0Aspecialized%20and%20general%20knowledge%20during%20inference.%20Extensive%20experiments%20on%0Avarious%20benchmark%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%0Aapproach.%20Code%20is%20available%20at%3A%20https%3A//github.com/LAMDA-CL/ICCV2025-TUNA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Task-Specific%2520and%2520Universal%2520Adapters%2520for%2520Pre-Trained%250A%2520%2520Model-based%2520Class-Incremental%2520Learning%26entry.906535625%3DYan%2520Wang%2520and%2520Da-Wei%2520Zhou%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520requires%2520a%2520learning%2520system%2520to%2520continually%250Alearn%2520new%2520classes%2520without%2520forgetting.%2520Existing%2520pre-trained%2520model-based%2520CIL%250Amethods%2520often%2520freeze%2520the%2520pre-trained%2520network%2520and%2520adapt%2520to%2520incremental%2520tasks%250Ausing%2520additional%2520lightweight%2520modules%2520such%2520as%2520adapters.%2520However%252C%2520incorrect%250Amodule%2520selection%2520during%2520inference%2520hurts%2520performance%252C%2520and%2520task-specific%2520modules%250Aoften%2520overlook%2520shared%2520general%2520knowledge%252C%2520leading%2520to%2520errors%2520on%2520distinguishing%250Abetween%2520similar%2520classes%2520across%2520tasks.%2520To%2520address%2520the%2520aforementioned%2520challenges%252C%250Awe%2520propose%2520integrating%2520Task-Specific%2520and%2520Universal%2520Adapters%2520%2528TUNA%2529%2520in%2520this%250Apaper.%2520Specifically%252C%2520we%2520train%2520task-specific%2520adapters%2520to%2520capture%2520the%2520most%250Acrucial%2520features%2520relevant%2520to%2520their%2520respective%2520tasks%2520and%2520introduce%2520an%250Aentropy-based%2520selection%2520mechanism%2520to%2520choose%2520the%2520most%2520suitable%2520adapter.%250AFurthermore%252C%2520we%2520leverage%2520an%2520adapter%2520fusion%2520strategy%2520to%2520construct%2520a%2520universal%250Aadapter%252C%2520which%2520encodes%2520the%2520most%2520discriminative%2520features%2520shared%2520across%2520tasks.%2520We%250Acombine%2520task-specific%2520and%2520universal%2520adapter%2520predictions%2520to%2520harness%2520both%250Aspecialized%2520and%2520general%2520knowledge%2520during%2520inference.%2520Extensive%2520experiments%2520on%250Avarious%2520benchmark%2520datasets%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520our%250Aapproach.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/LAMDA-CL/ICCV2025-TUNA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Task-Specific%20and%20Universal%20Adapters%20for%20Pre-Trained%0A%20%20Model-based%20Class-Incremental%20Learning&entry.906535625=Yan%20Wang%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20requires%20a%20learning%20system%20to%20continually%0Alearn%20new%20classes%20without%20forgetting.%20Existing%20pre-trained%20model-based%20CIL%0Amethods%20often%20freeze%20the%20pre-trained%20network%20and%20adapt%20to%20incremental%20tasks%0Ausing%20additional%20lightweight%20modules%20such%20as%20adapters.%20However%2C%20incorrect%0Amodule%20selection%20during%20inference%20hurts%20performance%2C%20and%20task-specific%20modules%0Aoften%20overlook%20shared%20general%20knowledge%2C%20leading%20to%20errors%20on%20distinguishing%0Abetween%20similar%20classes%20across%20tasks.%20To%20address%20the%20aforementioned%20challenges%2C%0Awe%20propose%20integrating%20Task-Specific%20and%20Universal%20Adapters%20%28TUNA%29%20in%20this%0Apaper.%20Specifically%2C%20we%20train%20task-specific%20adapters%20to%20capture%20the%20most%0Acrucial%20features%20relevant%20to%20their%20respective%20tasks%20and%20introduce%20an%0Aentropy-based%20selection%20mechanism%20to%20choose%20the%20most%20suitable%20adapter.%0AFurthermore%2C%20we%20leverage%20an%20adapter%20fusion%20strategy%20to%20construct%20a%20universal%0Aadapter%2C%20which%20encodes%20the%20most%20discriminative%20features%20shared%20across%20tasks.%20We%0Acombine%20task-specific%20and%20universal%20adapter%20predictions%20to%20harness%20both%0Aspecialized%20and%20general%20knowledge%20during%20inference.%20Extensive%20experiments%20on%0Avarious%20benchmark%20datasets%20demonstrate%20the%20state-of-the-art%20performance%20of%20our%0Aapproach.%20Code%20is%20available%20at%3A%20https%3A//github.com/LAMDA-CL/ICCV2025-TUNA%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08165v1&entry.124074799=Read"},
{"title": "TextInPlace: Indoor Visual Place Recognition in Repetitive Structures\n  with Scene Text Spotting and Verification", "author": "Huaqi Tao and Bingxi Liu and Calvin Chen and Tingjun Huang and He Li and Jinqiang Cui and Hong Zhang", "abstract": "  Visual Place Recognition (VPR) is a crucial capability for long-term\nautonomous robots, enabling them to identify previously visited locations using\nvisual information. However, existing methods remain limited in indoor settings\ndue to the highly repetitive structures inherent in such environments. We\nobserve that scene texts frequently appear in indoor spaces and can help\ndistinguish visually similar but different places. This inspires us to propose\nTextInPlace, a simple yet effective VPR framework that integrates Scene Text\nSpotting (STS) to mitigate visual perceptual ambiguity in repetitive indoor\nenvironments. Specifically, TextInPlace adopts a dual-branch architecture\nwithin a local parameter sharing network. The VPR branch employs\nattention-based aggregation to extract global descriptors for coarse-grained\nretrieval, while the STS branch utilizes a bridging text spotter to detect and\nrecognize scene texts. Finally, the discriminative texts are filtered to\ncompute text similarity and re-rank the top-K retrieved images. To bridge the\ngap between current text-based repetitive indoor scene datasets and the typical\nscenarios encountered in robot navigation, we establish an indoor VPR benchmark\ndataset, called Maze-with-Text. Extensive experiments on both custom and public\ndatasets demonstrate that TextInPlace achieves superior performance over\nexisting methods that rely solely on appearance information. The dataset, code,\nand trained models are publicly available at\nhttps://github.com/HqiTao/TextInPlace.\n", "link": "http://arxiv.org/abs/2503.06501v2", "date": "2025-08-11", "relevancy": 2.1695, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextInPlace%3A%20Indoor%20Visual%20Place%20Recognition%20in%20Repetitive%20Structures%0A%20%20with%20Scene%20Text%20Spotting%20and%20Verification&body=Title%3A%20TextInPlace%3A%20Indoor%20Visual%20Place%20Recognition%20in%20Repetitive%20Structures%0A%20%20with%20Scene%20Text%20Spotting%20and%20Verification%0AAuthor%3A%20Huaqi%20Tao%20and%20Bingxi%20Liu%20and%20Calvin%20Chen%20and%20Tingjun%20Huang%20and%20He%20Li%20and%20Jinqiang%20Cui%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Visual%20Place%20Recognition%20%28VPR%29%20is%20a%20crucial%20capability%20for%20long-term%0Aautonomous%20robots%2C%20enabling%20them%20to%20identify%20previously%20visited%20locations%20using%0Avisual%20information.%20However%2C%20existing%20methods%20remain%20limited%20in%20indoor%20settings%0Adue%20to%20the%20highly%20repetitive%20structures%20inherent%20in%20such%20environments.%20We%0Aobserve%20that%20scene%20texts%20frequently%20appear%20in%20indoor%20spaces%20and%20can%20help%0Adistinguish%20visually%20similar%20but%20different%20places.%20This%20inspires%20us%20to%20propose%0ATextInPlace%2C%20a%20simple%20yet%20effective%20VPR%20framework%20that%20integrates%20Scene%20Text%0ASpotting%20%28STS%29%20to%20mitigate%20visual%20perceptual%20ambiguity%20in%20repetitive%20indoor%0Aenvironments.%20Specifically%2C%20TextInPlace%20adopts%20a%20dual-branch%20architecture%0Awithin%20a%20local%20parameter%20sharing%20network.%20The%20VPR%20branch%20employs%0Aattention-based%20aggregation%20to%20extract%20global%20descriptors%20for%20coarse-grained%0Aretrieval%2C%20while%20the%20STS%20branch%20utilizes%20a%20bridging%20text%20spotter%20to%20detect%20and%0Arecognize%20scene%20texts.%20Finally%2C%20the%20discriminative%20texts%20are%20filtered%20to%0Acompute%20text%20similarity%20and%20re-rank%20the%20top-K%20retrieved%20images.%20To%20bridge%20the%0Agap%20between%20current%20text-based%20repetitive%20indoor%20scene%20datasets%20and%20the%20typical%0Ascenarios%20encountered%20in%20robot%20navigation%2C%20we%20establish%20an%20indoor%20VPR%20benchmark%0Adataset%2C%20called%20Maze-with-Text.%20Extensive%20experiments%20on%20both%20custom%20and%20public%0Adatasets%20demonstrate%20that%20TextInPlace%20achieves%20superior%20performance%20over%0Aexisting%20methods%20that%20rely%20solely%20on%20appearance%20information.%20The%20dataset%2C%20code%2C%0Aand%20trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/HqiTao/TextInPlace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06501v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextInPlace%253A%2520Indoor%2520Visual%2520Place%2520Recognition%2520in%2520Repetitive%2520Structures%250A%2520%2520with%2520Scene%2520Text%2520Spotting%2520and%2520Verification%26entry.906535625%3DHuaqi%2520Tao%2520and%2520Bingxi%2520Liu%2520and%2520Calvin%2520Chen%2520and%2520Tingjun%2520Huang%2520and%2520He%2520Li%2520and%2520Jinqiang%2520Cui%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520is%2520a%2520crucial%2520capability%2520for%2520long-term%250Aautonomous%2520robots%252C%2520enabling%2520them%2520to%2520identify%2520previously%2520visited%2520locations%2520using%250Avisual%2520information.%2520However%252C%2520existing%2520methods%2520remain%2520limited%2520in%2520indoor%2520settings%250Adue%2520to%2520the%2520highly%2520repetitive%2520structures%2520inherent%2520in%2520such%2520environments.%2520We%250Aobserve%2520that%2520scene%2520texts%2520frequently%2520appear%2520in%2520indoor%2520spaces%2520and%2520can%2520help%250Adistinguish%2520visually%2520similar%2520but%2520different%2520places.%2520This%2520inspires%2520us%2520to%2520propose%250ATextInPlace%252C%2520a%2520simple%2520yet%2520effective%2520VPR%2520framework%2520that%2520integrates%2520Scene%2520Text%250ASpotting%2520%2528STS%2529%2520to%2520mitigate%2520visual%2520perceptual%2520ambiguity%2520in%2520repetitive%2520indoor%250Aenvironments.%2520Specifically%252C%2520TextInPlace%2520adopts%2520a%2520dual-branch%2520architecture%250Awithin%2520a%2520local%2520parameter%2520sharing%2520network.%2520The%2520VPR%2520branch%2520employs%250Aattention-based%2520aggregation%2520to%2520extract%2520global%2520descriptors%2520for%2520coarse-grained%250Aretrieval%252C%2520while%2520the%2520STS%2520branch%2520utilizes%2520a%2520bridging%2520text%2520spotter%2520to%2520detect%2520and%250Arecognize%2520scene%2520texts.%2520Finally%252C%2520the%2520discriminative%2520texts%2520are%2520filtered%2520to%250Acompute%2520text%2520similarity%2520and%2520re-rank%2520the%2520top-K%2520retrieved%2520images.%2520To%2520bridge%2520the%250Agap%2520between%2520current%2520text-based%2520repetitive%2520indoor%2520scene%2520datasets%2520and%2520the%2520typical%250Ascenarios%2520encountered%2520in%2520robot%2520navigation%252C%2520we%2520establish%2520an%2520indoor%2520VPR%2520benchmark%250Adataset%252C%2520called%2520Maze-with-Text.%2520Extensive%2520experiments%2520on%2520both%2520custom%2520and%2520public%250Adatasets%2520demonstrate%2520that%2520TextInPlace%2520achieves%2520superior%2520performance%2520over%250Aexisting%2520methods%2520that%2520rely%2520solely%2520on%2520appearance%2520information.%2520The%2520dataset%252C%2520code%252C%250Aand%2520trained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HqiTao/TextInPlace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06501v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextInPlace%3A%20Indoor%20Visual%20Place%20Recognition%20in%20Repetitive%20Structures%0A%20%20with%20Scene%20Text%20Spotting%20and%20Verification&entry.906535625=Huaqi%20Tao%20and%20Bingxi%20Liu%20and%20Calvin%20Chen%20and%20Tingjun%20Huang%20and%20He%20Li%20and%20Jinqiang%20Cui%20and%20Hong%20Zhang&entry.1292438233=%20%20Visual%20Place%20Recognition%20%28VPR%29%20is%20a%20crucial%20capability%20for%20long-term%0Aautonomous%20robots%2C%20enabling%20them%20to%20identify%20previously%20visited%20locations%20using%0Avisual%20information.%20However%2C%20existing%20methods%20remain%20limited%20in%20indoor%20settings%0Adue%20to%20the%20highly%20repetitive%20structures%20inherent%20in%20such%20environments.%20We%0Aobserve%20that%20scene%20texts%20frequently%20appear%20in%20indoor%20spaces%20and%20can%20help%0Adistinguish%20visually%20similar%20but%20different%20places.%20This%20inspires%20us%20to%20propose%0ATextInPlace%2C%20a%20simple%20yet%20effective%20VPR%20framework%20that%20integrates%20Scene%20Text%0ASpotting%20%28STS%29%20to%20mitigate%20visual%20perceptual%20ambiguity%20in%20repetitive%20indoor%0Aenvironments.%20Specifically%2C%20TextInPlace%20adopts%20a%20dual-branch%20architecture%0Awithin%20a%20local%20parameter%20sharing%20network.%20The%20VPR%20branch%20employs%0Aattention-based%20aggregation%20to%20extract%20global%20descriptors%20for%20coarse-grained%0Aretrieval%2C%20while%20the%20STS%20branch%20utilizes%20a%20bridging%20text%20spotter%20to%20detect%20and%0Arecognize%20scene%20texts.%20Finally%2C%20the%20discriminative%20texts%20are%20filtered%20to%0Acompute%20text%20similarity%20and%20re-rank%20the%20top-K%20retrieved%20images.%20To%20bridge%20the%0Agap%20between%20current%20text-based%20repetitive%20indoor%20scene%20datasets%20and%20the%20typical%0Ascenarios%20encountered%20in%20robot%20navigation%2C%20we%20establish%20an%20indoor%20VPR%20benchmark%0Adataset%2C%20called%20Maze-with-Text.%20Extensive%20experiments%20on%20both%20custom%20and%20public%0Adatasets%20demonstrate%20that%20TextInPlace%20achieves%20superior%20performance%20over%0Aexisting%20methods%20that%20rely%20solely%20on%20appearance%20information.%20The%20dataset%2C%20code%2C%0Aand%20trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/HqiTao/TextInPlace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06501v2&entry.124074799=Read"},
{"title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with\n  Rough Terrain", "author": "Wei Zhang and Yinchuan Wang and Wangtao Lu and Pengyu Zhang and Xiang Zhang and Yue Wang and Chaoqun Wang", "abstract": "  It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.\n", "link": "http://arxiv.org/abs/2508.08108v1", "date": "2025-08-11", "relevancy": 2.1501, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capsizing-Guided%20Trajectory%20Optimization%20for%20Autonomous%20Navigation%20with%0A%20%20Rough%20Terrain&body=Title%3A%20Capsizing-Guided%20Trajectory%20Optimization%20for%20Autonomous%20Navigation%20with%0A%20%20Rough%20Terrain%0AAuthor%3A%20Wei%20Zhang%20and%20Yinchuan%20Wang%20and%20Wangtao%20Lu%20and%20Pengyu%20Zhang%20and%20Xiang%20Zhang%20and%20Yue%20Wang%20and%20Chaoqun%20Wang%0AAbstract%3A%20%20%20It%20is%20a%20challenging%20task%20for%20ground%20robots%20to%20autonomously%20navigate%20in%20harsh%0Aenvironments%20due%20to%20the%20presence%20of%20non-trivial%20obstacles%20and%20uneven%20terrain.%0AThis%20requires%20trajectory%20planning%20that%20balances%20safety%20and%20efficiency.%20The%0Aprimary%20challenge%20is%20to%20generate%20a%20feasible%20trajectory%20that%20prevents%20robot%20from%0Atip-over%20while%20ensuring%20effective%20navigation.%20In%20this%20paper%2C%20we%20propose%20a%0Acapsizing-aware%20trajectory%20planner%20%28CAP%29%20to%20achieve%20trajectory%20planning%20on%20the%0Auneven%20terrain.%20The%20tip-over%20stability%20of%20the%20robot%20on%20rough%20terrain%20is%0Aanalyzed.%20Based%20on%20the%20tip-over%20stability%2C%20we%20define%20the%20traversable%0Aorientation%2C%20which%20indicates%20the%20safe%20range%20of%20robot%20orientations.%20This%0Aorientation%20is%20then%20incorporated%20into%20a%20capsizing-safety%20constraint%20for%0Atrajectory%20optimization.%20We%20employ%20a%20graph-based%20solver%20to%20compute%20a%20robust%20and%0Afeasible%20trajectory%20while%20adhering%20to%20the%20capsizing-safety%20constraint.%0AExtensive%20simulation%20and%20real-world%20experiments%20validate%20the%20effectiveness%20and%0Arobustness%20of%20the%20proposed%20method.%20The%20results%20demonstrate%20that%20CAP%20outperforms%0Aexisting%20state-of-the-art%20approaches%2C%20providing%20enhanced%20navigation%20performance%0Aon%20uneven%20terrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapsizing-Guided%2520Trajectory%2520Optimization%2520for%2520Autonomous%2520Navigation%2520with%250A%2520%2520Rough%2520Terrain%26entry.906535625%3DWei%2520Zhang%2520and%2520Yinchuan%2520Wang%2520and%2520Wangtao%2520Lu%2520and%2520Pengyu%2520Zhang%2520and%2520Xiang%2520Zhang%2520and%2520Yue%2520Wang%2520and%2520Chaoqun%2520Wang%26entry.1292438233%3D%2520%2520It%2520is%2520a%2520challenging%2520task%2520for%2520ground%2520robots%2520to%2520autonomously%2520navigate%2520in%2520harsh%250Aenvironments%2520due%2520to%2520the%2520presence%2520of%2520non-trivial%2520obstacles%2520and%2520uneven%2520terrain.%250AThis%2520requires%2520trajectory%2520planning%2520that%2520balances%2520safety%2520and%2520efficiency.%2520The%250Aprimary%2520challenge%2520is%2520to%2520generate%2520a%2520feasible%2520trajectory%2520that%2520prevents%2520robot%2520from%250Atip-over%2520while%2520ensuring%2520effective%2520navigation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Acapsizing-aware%2520trajectory%2520planner%2520%2528CAP%2529%2520to%2520achieve%2520trajectory%2520planning%2520on%2520the%250Auneven%2520terrain.%2520The%2520tip-over%2520stability%2520of%2520the%2520robot%2520on%2520rough%2520terrain%2520is%250Aanalyzed.%2520Based%2520on%2520the%2520tip-over%2520stability%252C%2520we%2520define%2520the%2520traversable%250Aorientation%252C%2520which%2520indicates%2520the%2520safe%2520range%2520of%2520robot%2520orientations.%2520This%250Aorientation%2520is%2520then%2520incorporated%2520into%2520a%2520capsizing-safety%2520constraint%2520for%250Atrajectory%2520optimization.%2520We%2520employ%2520a%2520graph-based%2520solver%2520to%2520compute%2520a%2520robust%2520and%250Afeasible%2520trajectory%2520while%2520adhering%2520to%2520the%2520capsizing-safety%2520constraint.%250AExtensive%2520simulation%2520and%2520real-world%2520experiments%2520validate%2520the%2520effectiveness%2520and%250Arobustness%2520of%2520the%2520proposed%2520method.%2520The%2520results%2520demonstrate%2520that%2520CAP%2520outperforms%250Aexisting%2520state-of-the-art%2520approaches%252C%2520providing%2520enhanced%2520navigation%2520performance%250Aon%2520uneven%2520terrains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capsizing-Guided%20Trajectory%20Optimization%20for%20Autonomous%20Navigation%20with%0A%20%20Rough%20Terrain&entry.906535625=Wei%20Zhang%20and%20Yinchuan%20Wang%20and%20Wangtao%20Lu%20and%20Pengyu%20Zhang%20and%20Xiang%20Zhang%20and%20Yue%20Wang%20and%20Chaoqun%20Wang&entry.1292438233=%20%20It%20is%20a%20challenging%20task%20for%20ground%20robots%20to%20autonomously%20navigate%20in%20harsh%0Aenvironments%20due%20to%20the%20presence%20of%20non-trivial%20obstacles%20and%20uneven%20terrain.%0AThis%20requires%20trajectory%20planning%20that%20balances%20safety%20and%20efficiency.%20The%0Aprimary%20challenge%20is%20to%20generate%20a%20feasible%20trajectory%20that%20prevents%20robot%20from%0Atip-over%20while%20ensuring%20effective%20navigation.%20In%20this%20paper%2C%20we%20propose%20a%0Acapsizing-aware%20trajectory%20planner%20%28CAP%29%20to%20achieve%20trajectory%20planning%20on%20the%0Auneven%20terrain.%20The%20tip-over%20stability%20of%20the%20robot%20on%20rough%20terrain%20is%0Aanalyzed.%20Based%20on%20the%20tip-over%20stability%2C%20we%20define%20the%20traversable%0Aorientation%2C%20which%20indicates%20the%20safe%20range%20of%20robot%20orientations.%20This%0Aorientation%20is%20then%20incorporated%20into%20a%20capsizing-safety%20constraint%20for%0Atrajectory%20optimization.%20We%20employ%20a%20graph-based%20solver%20to%20compute%20a%20robust%20and%0Afeasible%20trajectory%20while%20adhering%20to%20the%20capsizing-safety%20constraint.%0AExtensive%20simulation%20and%20real-world%20experiments%20validate%20the%20effectiveness%20and%0Arobustness%20of%20the%20proposed%20method.%20The%20results%20demonstrate%20that%20CAP%20outperforms%0Aexisting%20state-of-the-art%20approaches%2C%20providing%20enhanced%20navigation%20performance%0Aon%20uneven%20terrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08108v1&entry.124074799=Read"},
{"title": "A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and\n  Street Images", "author": "Wooyong Jung and Sola Kim and Dongwook Kim and Maryam Tabar and Dongwon Lee", "abstract": "  Homelessness in the United States has surged to levels unseen since the Great\nDepression. However, existing methods for monitoring it, such as point-in-time\n(PIT) counts, have limitations in terms of frequency, consistency, and spatial\ndetail. This study proposes a new approach using publicly available,\ncrowdsourced data, specifically 311 Service Calls and street-level imagery, to\ntrack and forecast homeless tent trends in San Francisco. Our predictive model\ncaptures fine-grained daily and neighborhood-level variations, uncovering\npatterns that traditional counts often overlook, such as rapid fluctuations\nduring the COVID-19 pandemic and spatial shifts in tent locations over time. By\nproviding more timely, localized, and cost-effective information, this approach\nserves as a valuable tool for guiding policy responses and evaluating\ninterventions aimed at reducing unsheltered homelessness.\n", "link": "http://arxiv.org/abs/2508.06409v2", "date": "2025-08-11", "relevancy": 2.1473, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.419}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images&body=Title%3A%20A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images%0AAuthor%3A%20Wooyong%20Jung%20and%20Sola%20Kim%20and%20Dongwook%20Kim%20and%20Maryam%20Tabar%20and%20Dongwon%20Lee%0AAbstract%3A%20%20%20Homelessness%20in%20the%20United%20States%20has%20surged%20to%20levels%20unseen%20since%20the%20Great%0ADepression.%20However%2C%20existing%20methods%20for%20monitoring%20it%2C%20such%20as%20point-in-time%0A%28PIT%29%20counts%2C%20have%20limitations%20in%20terms%20of%20frequency%2C%20consistency%2C%20and%20spatial%0Adetail.%20This%20study%20proposes%20a%20new%20approach%20using%20publicly%20available%2C%0Acrowdsourced%20data%2C%20specifically%20311%20Service%20Calls%20and%20street-level%20imagery%2C%20to%0Atrack%20and%20forecast%20homeless%20tent%20trends%20in%20San%20Francisco.%20Our%20predictive%20model%0Acaptures%20fine-grained%20daily%20and%20neighborhood-level%20variations%2C%20uncovering%0Apatterns%20that%20traditional%20counts%20often%20overlook%2C%20such%20as%20rapid%20fluctuations%0Aduring%20the%20COVID-19%20pandemic%20and%20spatial%20shifts%20in%20tent%20locations%20over%20time.%20By%0Aproviding%20more%20timely%2C%20localized%2C%20and%20cost-effective%20information%2C%20this%20approach%0Aserves%20as%20a%20valuable%20tool%20for%20guiding%20policy%20responses%20and%20evaluating%0Ainterventions%20aimed%20at%20reducing%20unsheltered%20homelessness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06409v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Lens%2520on%2520Homelessness%253A%2520Daily%2520Tent%2520Monitoring%2520with%2520311%2520Calls%2520and%250A%2520%2520Street%2520Images%26entry.906535625%3DWooyong%2520Jung%2520and%2520Sola%2520Kim%2520and%2520Dongwook%2520Kim%2520and%2520Maryam%2520Tabar%2520and%2520Dongwon%2520Lee%26entry.1292438233%3D%2520%2520Homelessness%2520in%2520the%2520United%2520States%2520has%2520surged%2520to%2520levels%2520unseen%2520since%2520the%2520Great%250ADepression.%2520However%252C%2520existing%2520methods%2520for%2520monitoring%2520it%252C%2520such%2520as%2520point-in-time%250A%2528PIT%2529%2520counts%252C%2520have%2520limitations%2520in%2520terms%2520of%2520frequency%252C%2520consistency%252C%2520and%2520spatial%250Adetail.%2520This%2520study%2520proposes%2520a%2520new%2520approach%2520using%2520publicly%2520available%252C%250Acrowdsourced%2520data%252C%2520specifically%2520311%2520Service%2520Calls%2520and%2520street-level%2520imagery%252C%2520to%250Atrack%2520and%2520forecast%2520homeless%2520tent%2520trends%2520in%2520San%2520Francisco.%2520Our%2520predictive%2520model%250Acaptures%2520fine-grained%2520daily%2520and%2520neighborhood-level%2520variations%252C%2520uncovering%250Apatterns%2520that%2520traditional%2520counts%2520often%2520overlook%252C%2520such%2520as%2520rapid%2520fluctuations%250Aduring%2520the%2520COVID-19%2520pandemic%2520and%2520spatial%2520shifts%2520in%2520tent%2520locations%2520over%2520time.%2520By%250Aproviding%2520more%2520timely%252C%2520localized%252C%2520and%2520cost-effective%2520information%252C%2520this%2520approach%250Aserves%2520as%2520a%2520valuable%2520tool%2520for%2520guiding%2520policy%2520responses%2520and%2520evaluating%250Ainterventions%2520aimed%2520at%2520reducing%2520unsheltered%2520homelessness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06409v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Lens%20on%20Homelessness%3A%20Daily%20Tent%20Monitoring%20with%20311%20Calls%20and%0A%20%20Street%20Images&entry.906535625=Wooyong%20Jung%20and%20Sola%20Kim%20and%20Dongwook%20Kim%20and%20Maryam%20Tabar%20and%20Dongwon%20Lee&entry.1292438233=%20%20Homelessness%20in%20the%20United%20States%20has%20surged%20to%20levels%20unseen%20since%20the%20Great%0ADepression.%20However%2C%20existing%20methods%20for%20monitoring%20it%2C%20such%20as%20point-in-time%0A%28PIT%29%20counts%2C%20have%20limitations%20in%20terms%20of%20frequency%2C%20consistency%2C%20and%20spatial%0Adetail.%20This%20study%20proposes%20a%20new%20approach%20using%20publicly%20available%2C%0Acrowdsourced%20data%2C%20specifically%20311%20Service%20Calls%20and%20street-level%20imagery%2C%20to%0Atrack%20and%20forecast%20homeless%20tent%20trends%20in%20San%20Francisco.%20Our%20predictive%20model%0Acaptures%20fine-grained%20daily%20and%20neighborhood-level%20variations%2C%20uncovering%0Apatterns%20that%20traditional%20counts%20often%20overlook%2C%20such%20as%20rapid%20fluctuations%0Aduring%20the%20COVID-19%20pandemic%20and%20spatial%20shifts%20in%20tent%20locations%20over%20time.%20By%0Aproviding%20more%20timely%2C%20localized%2C%20and%20cost-effective%20information%2C%20this%20approach%0Aserves%20as%20a%20valuable%20tool%20for%20guiding%20policy%20responses%20and%20evaluating%0Ainterventions%20aimed%20at%20reducing%20unsheltered%20homelessness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06409v2&entry.124074799=Read"},
{"title": "OFAL: An Oracle-Free Active Learning Framework", "author": "Hadi Khorsand and Vahid Pourahmadi", "abstract": "  In the active learning paradigm, using an oracle to label data has always\nbeen a complex and expensive task, and with the emersion of large unlabeled\ndata pools, it would be highly beneficial If we could achieve better results\nwithout relying on an oracle. This research introduces OFAL, an oracle-free\nactive learning scheme that utilizes neural network uncertainty. OFAL uses the\nmodel's own uncertainty to transform highly confident unlabeled samples into\ninformative uncertain samples. First, we start with separating and quantifying\ndifferent parts of uncertainty and introduce Monte Carlo Dropouts as an\napproximation of the Bayesian Neural Network model. Secondly, by adding a\nvariational autoencoder, we go on to generate new uncertain samples by stepping\ntoward the uncertain part of latent space starting from a confidence seed\nsample. By generating these new informative samples, we can perform active\nlearning and enhance the model's accuracy. Lastly, we try to compare and\nintegrate our method with other widely used active learning sampling methods.\n", "link": "http://arxiv.org/abs/2508.08126v1", "date": "2025-08-11", "relevancy": 2.1434, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5513}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OFAL%3A%20An%20Oracle-Free%20Active%20Learning%20Framework&body=Title%3A%20OFAL%3A%20An%20Oracle-Free%20Active%20Learning%20Framework%0AAuthor%3A%20Hadi%20Khorsand%20and%20Vahid%20Pourahmadi%0AAbstract%3A%20%20%20In%20the%20active%20learning%20paradigm%2C%20using%20an%20oracle%20to%20label%20data%20has%20always%0Abeen%20a%20complex%20and%20expensive%20task%2C%20and%20with%20the%20emersion%20of%20large%20unlabeled%0Adata%20pools%2C%20it%20would%20be%20highly%20beneficial%20If%20we%20could%20achieve%20better%20results%0Awithout%20relying%20on%20an%20oracle.%20This%20research%20introduces%20OFAL%2C%20an%20oracle-free%0Aactive%20learning%20scheme%20that%20utilizes%20neural%20network%20uncertainty.%20OFAL%20uses%20the%0Amodel%27s%20own%20uncertainty%20to%20transform%20highly%20confident%20unlabeled%20samples%20into%0Ainformative%20uncertain%20samples.%20First%2C%20we%20start%20with%20separating%20and%20quantifying%0Adifferent%20parts%20of%20uncertainty%20and%20introduce%20Monte%20Carlo%20Dropouts%20as%20an%0Aapproximation%20of%20the%20Bayesian%20Neural%20Network%20model.%20Secondly%2C%20by%20adding%20a%0Avariational%20autoencoder%2C%20we%20go%20on%20to%20generate%20new%20uncertain%20samples%20by%20stepping%0Atoward%20the%20uncertain%20part%20of%20latent%20space%20starting%20from%20a%20confidence%20seed%0Asample.%20By%20generating%20these%20new%20informative%20samples%2C%20we%20can%20perform%20active%0Alearning%20and%20enhance%20the%20model%27s%20accuracy.%20Lastly%2C%20we%20try%20to%20compare%20and%0Aintegrate%20our%20method%20with%20other%20widely%20used%20active%20learning%20sampling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOFAL%253A%2520An%2520Oracle-Free%2520Active%2520Learning%2520Framework%26entry.906535625%3DHadi%2520Khorsand%2520and%2520Vahid%2520Pourahmadi%26entry.1292438233%3D%2520%2520In%2520the%2520active%2520learning%2520paradigm%252C%2520using%2520an%2520oracle%2520to%2520label%2520data%2520has%2520always%250Abeen%2520a%2520complex%2520and%2520expensive%2520task%252C%2520and%2520with%2520the%2520emersion%2520of%2520large%2520unlabeled%250Adata%2520pools%252C%2520it%2520would%2520be%2520highly%2520beneficial%2520If%2520we%2520could%2520achieve%2520better%2520results%250Awithout%2520relying%2520on%2520an%2520oracle.%2520This%2520research%2520introduces%2520OFAL%252C%2520an%2520oracle-free%250Aactive%2520learning%2520scheme%2520that%2520utilizes%2520neural%2520network%2520uncertainty.%2520OFAL%2520uses%2520the%250Amodel%2527s%2520own%2520uncertainty%2520to%2520transform%2520highly%2520confident%2520unlabeled%2520samples%2520into%250Ainformative%2520uncertain%2520samples.%2520First%252C%2520we%2520start%2520with%2520separating%2520and%2520quantifying%250Adifferent%2520parts%2520of%2520uncertainty%2520and%2520introduce%2520Monte%2520Carlo%2520Dropouts%2520as%2520an%250Aapproximation%2520of%2520the%2520Bayesian%2520Neural%2520Network%2520model.%2520Secondly%252C%2520by%2520adding%2520a%250Avariational%2520autoencoder%252C%2520we%2520go%2520on%2520to%2520generate%2520new%2520uncertain%2520samples%2520by%2520stepping%250Atoward%2520the%2520uncertain%2520part%2520of%2520latent%2520space%2520starting%2520from%2520a%2520confidence%2520seed%250Asample.%2520By%2520generating%2520these%2520new%2520informative%2520samples%252C%2520we%2520can%2520perform%2520active%250Alearning%2520and%2520enhance%2520the%2520model%2527s%2520accuracy.%2520Lastly%252C%2520we%2520try%2520to%2520compare%2520and%250Aintegrate%2520our%2520method%2520with%2520other%2520widely%2520used%2520active%2520learning%2520sampling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OFAL%3A%20An%20Oracle-Free%20Active%20Learning%20Framework&entry.906535625=Hadi%20Khorsand%20and%20Vahid%20Pourahmadi&entry.1292438233=%20%20In%20the%20active%20learning%20paradigm%2C%20using%20an%20oracle%20to%20label%20data%20has%20always%0Abeen%20a%20complex%20and%20expensive%20task%2C%20and%20with%20the%20emersion%20of%20large%20unlabeled%0Adata%20pools%2C%20it%20would%20be%20highly%20beneficial%20If%20we%20could%20achieve%20better%20results%0Awithout%20relying%20on%20an%20oracle.%20This%20research%20introduces%20OFAL%2C%20an%20oracle-free%0Aactive%20learning%20scheme%20that%20utilizes%20neural%20network%20uncertainty.%20OFAL%20uses%20the%0Amodel%27s%20own%20uncertainty%20to%20transform%20highly%20confident%20unlabeled%20samples%20into%0Ainformative%20uncertain%20samples.%20First%2C%20we%20start%20with%20separating%20and%20quantifying%0Adifferent%20parts%20of%20uncertainty%20and%20introduce%20Monte%20Carlo%20Dropouts%20as%20an%0Aapproximation%20of%20the%20Bayesian%20Neural%20Network%20model.%20Secondly%2C%20by%20adding%20a%0Avariational%20autoencoder%2C%20we%20go%20on%20to%20generate%20new%20uncertain%20samples%20by%20stepping%0Atoward%20the%20uncertain%20part%20of%20latent%20space%20starting%20from%20a%20confidence%20seed%0Asample.%20By%20generating%20these%20new%20informative%20samples%2C%20we%20can%20perform%20active%0Alearning%20and%20enhance%20the%20model%27s%20accuracy.%20Lastly%2C%20we%20try%20to%20compare%20and%0Aintegrate%20our%20method%20with%20other%20widely%20used%20active%20learning%20sampling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08126v1&entry.124074799=Read"},
{"title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning", "author": "Md Meftahul Ferdaus and Mahdi Abdelguerfi and Elias Ioup and Steven Sloan and Kendall N. Niles and Ken Pathak", "abstract": "  Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.\n", "link": "http://arxiv.org/abs/2508.08186v1", "date": "2025-08-11", "relevancy": 2.134, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KARMA%3A%20Efficient%20Structural%20Defect%20Segmentation%20via%20Kolmogorov-Arnold%0A%20%20Representation%20Learning&body=Title%3A%20KARMA%3A%20Efficient%20Structural%20Defect%20Segmentation%20via%20Kolmogorov-Arnold%0A%20%20Representation%20Learning%0AAuthor%3A%20Md%20Meftahul%20Ferdaus%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup%20and%20Steven%20Sloan%20and%20Kendall%20N.%20Niles%20and%20Ken%20Pathak%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20structural%20defects%20in%20civil%20infrastructure%20remains%0Achallenging%20due%20to%20variable%20defect%20appearances%2C%20harsh%20imaging%20conditions%2C%20and%0Asignificant%20class%20imbalance.%20Current%20deep%20learning%20methods%2C%20despite%20their%0Aeffectiveness%2C%20typically%20require%20millions%20of%20parameters%2C%20rendering%20them%0Aimpractical%20for%20real-time%20inspection%20systems.%20We%20introduce%20KARMA%0A%28Kolmogorov-Arnold%20Representation%20Mapping%20Architecture%29%2C%20a%20highly%20efficient%0Asemantic%20segmentation%20framework%20that%20models%20complex%20defect%20patterns%20through%0Acompositions%20of%20one-dimensional%20functions%20rather%20than%20conventional%0Aconvolutions.%20KARMA%20features%20three%20technical%20innovations%3A%20%281%29%20a%0Aparameter-efficient%20Tiny%20Kolmogorov-Arnold%20Network%20%28TiKAN%29%20module%20leveraging%0Alow-rank%20factorization%20for%20KAN-based%20feature%20transformation%3B%20%282%29%20an%20optimized%0Afeature%20pyramid%20structure%20with%20separable%20convolutions%20for%20multi-scale%20defect%0Aanalysis%3B%20and%20%283%29%20a%20static-dynamic%20prototype%20mechanism%20that%20enhances%20feature%0Arepresentation%20for%20imbalanced%20classes.%20Extensive%20experiments%20on%20benchmark%0Ainfrastructure%20inspection%20datasets%20demonstrate%20that%20KARMA%20achieves%20competitive%0Aor%20superior%20mean%20IoU%20performance%20compared%20to%20state-of-the-art%20approaches%2C%20while%0Ausing%20significantly%20fewer%20parameters%20%280.959M%20vs.%2031.04M%2C%20a%2097%25%20reduction%29.%0AOperating%20at%200.264%20GFLOPS%2C%20KARMA%20maintains%20inference%20speeds%20suitable%20for%0Areal-time%20deployment%2C%20enabling%20practical%20automated%20infrastructure%20inspection%0Asystems%20without%20compromising%20accuracy.%20The%20source%20code%20can%20be%20accessed%20at%20the%0Afollowing%20URL%3A%20https%3A//github.com/faeyelab/karma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKARMA%253A%2520Efficient%2520Structural%2520Defect%2520Segmentation%2520via%2520Kolmogorov-Arnold%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMd%2520Meftahul%2520Ferdaus%2520and%2520Mahdi%2520Abdelguerfi%2520and%2520Elias%2520Ioup%2520and%2520Steven%2520Sloan%2520and%2520Kendall%2520N.%2520Niles%2520and%2520Ken%2520Pathak%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520structural%2520defects%2520in%2520civil%2520infrastructure%2520remains%250Achallenging%2520due%2520to%2520variable%2520defect%2520appearances%252C%2520harsh%2520imaging%2520conditions%252C%2520and%250Asignificant%2520class%2520imbalance.%2520Current%2520deep%2520learning%2520methods%252C%2520despite%2520their%250Aeffectiveness%252C%2520typically%2520require%2520millions%2520of%2520parameters%252C%2520rendering%2520them%250Aimpractical%2520for%2520real-time%2520inspection%2520systems.%2520We%2520introduce%2520KARMA%250A%2528Kolmogorov-Arnold%2520Representation%2520Mapping%2520Architecture%2529%252C%2520a%2520highly%2520efficient%250Asemantic%2520segmentation%2520framework%2520that%2520models%2520complex%2520defect%2520patterns%2520through%250Acompositions%2520of%2520one-dimensional%2520functions%2520rather%2520than%2520conventional%250Aconvolutions.%2520KARMA%2520features%2520three%2520technical%2520innovations%253A%2520%25281%2529%2520a%250Aparameter-efficient%2520Tiny%2520Kolmogorov-Arnold%2520Network%2520%2528TiKAN%2529%2520module%2520leveraging%250Alow-rank%2520factorization%2520for%2520KAN-based%2520feature%2520transformation%253B%2520%25282%2529%2520an%2520optimized%250Afeature%2520pyramid%2520structure%2520with%2520separable%2520convolutions%2520for%2520multi-scale%2520defect%250Aanalysis%253B%2520and%2520%25283%2529%2520a%2520static-dynamic%2520prototype%2520mechanism%2520that%2520enhances%2520feature%250Arepresentation%2520for%2520imbalanced%2520classes.%2520Extensive%2520experiments%2520on%2520benchmark%250Ainfrastructure%2520inspection%2520datasets%2520demonstrate%2520that%2520KARMA%2520achieves%2520competitive%250Aor%2520superior%2520mean%2520IoU%2520performance%2520compared%2520to%2520state-of-the-art%2520approaches%252C%2520while%250Ausing%2520significantly%2520fewer%2520parameters%2520%25280.959M%2520vs.%252031.04M%252C%2520a%252097%2525%2520reduction%2529.%250AOperating%2520at%25200.264%2520GFLOPS%252C%2520KARMA%2520maintains%2520inference%2520speeds%2520suitable%2520for%250Areal-time%2520deployment%252C%2520enabling%2520practical%2520automated%2520infrastructure%2520inspection%250Asystems%2520without%2520compromising%2520accuracy.%2520The%2520source%2520code%2520can%2520be%2520accessed%2520at%2520the%250Afollowing%2520URL%253A%2520https%253A//github.com/faeyelab/karma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KARMA%3A%20Efficient%20Structural%20Defect%20Segmentation%20via%20Kolmogorov-Arnold%0A%20%20Representation%20Learning&entry.906535625=Md%20Meftahul%20Ferdaus%20and%20Mahdi%20Abdelguerfi%20and%20Elias%20Ioup%20and%20Steven%20Sloan%20and%20Kendall%20N.%20Niles%20and%20Ken%20Pathak&entry.1292438233=%20%20Semantic%20segmentation%20of%20structural%20defects%20in%20civil%20infrastructure%20remains%0Achallenging%20due%20to%20variable%20defect%20appearances%2C%20harsh%20imaging%20conditions%2C%20and%0Asignificant%20class%20imbalance.%20Current%20deep%20learning%20methods%2C%20despite%20their%0Aeffectiveness%2C%20typically%20require%20millions%20of%20parameters%2C%20rendering%20them%0Aimpractical%20for%20real-time%20inspection%20systems.%20We%20introduce%20KARMA%0A%28Kolmogorov-Arnold%20Representation%20Mapping%20Architecture%29%2C%20a%20highly%20efficient%0Asemantic%20segmentation%20framework%20that%20models%20complex%20defect%20patterns%20through%0Acompositions%20of%20one-dimensional%20functions%20rather%20than%20conventional%0Aconvolutions.%20KARMA%20features%20three%20technical%20innovations%3A%20%281%29%20a%0Aparameter-efficient%20Tiny%20Kolmogorov-Arnold%20Network%20%28TiKAN%29%20module%20leveraging%0Alow-rank%20factorization%20for%20KAN-based%20feature%20transformation%3B%20%282%29%20an%20optimized%0Afeature%20pyramid%20structure%20with%20separable%20convolutions%20for%20multi-scale%20defect%0Aanalysis%3B%20and%20%283%29%20a%20static-dynamic%20prototype%20mechanism%20that%20enhances%20feature%0Arepresentation%20for%20imbalanced%20classes.%20Extensive%20experiments%20on%20benchmark%0Ainfrastructure%20inspection%20datasets%20demonstrate%20that%20KARMA%20achieves%20competitive%0Aor%20superior%20mean%20IoU%20performance%20compared%20to%20state-of-the-art%20approaches%2C%20while%0Ausing%20significantly%20fewer%20parameters%20%280.959M%20vs.%2031.04M%2C%20a%2097%25%20reduction%29.%0AOperating%20at%200.264%20GFLOPS%2C%20KARMA%20maintains%20inference%20speeds%20suitable%20for%0Areal-time%20deployment%2C%20enabling%20practical%20automated%20infrastructure%20inspection%0Asystems%20without%20compromising%20accuracy.%20The%20source%20code%20can%20be%20accessed%20at%20the%0Afollowing%20URL%3A%20https%3A//github.com/faeyelab/karma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08186v1&entry.124074799=Read"},
{"title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual\n  Tangent Space Alignment and Spatial-Riemannian Feature Fusion", "author": "Nicole Lai-Tan and Xiao Gu and Marios G. Philiastides and Fani Deligianni", "abstract": "  Personalised music-based interventions offer a powerful means of supporting\nmotor rehabilitation by dynamically tailoring auditory stimuli to provide\nexternal timekeeping cues, modulate affective states, and stabilise gait\npatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for\nadapting these interventions across individuals. However, inter-subject\nvariability in EEG signals, further compounded by movement-induced artefacts\nand motor planning differences, hinders the generalisability of BCIs and\nresults in lengthy calibration processes. We propose Individual Tangent Space\nAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specific\nrecentering, distribution matching, and supervised rotational alignment to\nenhance cross-subject generalisation. Our hybrid architecture fuses Regularised\nCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel and\nsequential configurations, improving class separability while maintaining the\ngeometric structure of covariance matrices for robust statistical computation.\nUsing leave-one-subject-out cross-validation, `ITSA' demonstrates significant\nperformance improvements across subjects and conditions. The parallel fusion\napproach shows the greatest enhancement over its sequential counterpart, with\nrobust performance maintained across varying data conditions and electrode\nconfigurations. The code will be made publicly available at the time of\npublication.\n", "link": "http://arxiv.org/abs/2508.08216v1", "date": "2025-08-11", "relevancy": 2.1299, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5313}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Subject%20and%20Cross-Montage%20EEG%20Transfer%20Learning%20via%20Individual%0A%20%20Tangent%20Space%20Alignment%20and%20Spatial-Riemannian%20Feature%20Fusion&body=Title%3A%20Cross-Subject%20and%20Cross-Montage%20EEG%20Transfer%20Learning%20via%20Individual%0A%20%20Tangent%20Space%20Alignment%20and%20Spatial-Riemannian%20Feature%20Fusion%0AAuthor%3A%20Nicole%20Lai-Tan%20and%20Xiao%20Gu%20and%20Marios%20G.%20Philiastides%20and%20Fani%20Deligianni%0AAbstract%3A%20%20%20Personalised%20music-based%20interventions%20offer%20a%20powerful%20means%20of%20supporting%0Amotor%20rehabilitation%20by%20dynamically%20tailoring%20auditory%20stimuli%20to%20provide%0Aexternal%20timekeeping%20cues%2C%20modulate%20affective%20states%2C%20and%20stabilise%20gait%0Apatterns.%20Generalisable%20Brain-Computer%20Interfaces%20%28BCIs%29%20thus%20hold%20promise%20for%0Aadapting%20these%20interventions%20across%20individuals.%20However%2C%20inter-subject%0Avariability%20in%20EEG%20signals%2C%20further%20compounded%20by%20movement-induced%20artefacts%0Aand%20motor%20planning%20differences%2C%20hinders%20the%20generalisability%20of%20BCIs%20and%0Aresults%20in%20lengthy%20calibration%20processes.%20We%20propose%20Individual%20Tangent%20Space%0AAlignment%20%28ITSA%29%2C%20a%20novel%20pre-alignment%20strategy%20incorporating%20subject-specific%0Arecentering%2C%20distribution%20matching%2C%20and%20supervised%20rotational%20alignment%20to%0Aenhance%20cross-subject%20generalisation.%20Our%20hybrid%20architecture%20fuses%20Regularised%0ACommon%20Spatial%20Patterns%20%28RCSP%29%20with%20Riemannian%20geometry%20in%20parallel%20and%0Asequential%20configurations%2C%20improving%20class%20separability%20while%20maintaining%20the%0Ageometric%20structure%20of%20covariance%20matrices%20for%20robust%20statistical%20computation.%0AUsing%20leave-one-subject-out%20cross-validation%2C%20%60ITSA%27%20demonstrates%20significant%0Aperformance%20improvements%20across%20subjects%20and%20conditions.%20The%20parallel%20fusion%0Aapproach%20shows%20the%20greatest%20enhancement%20over%20its%20sequential%20counterpart%2C%20with%0Arobust%20performance%20maintained%20across%20varying%20data%20conditions%20and%20electrode%0Aconfigurations.%20The%20code%20will%20be%20made%20publicly%20available%20at%20the%20time%20of%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Subject%2520and%2520Cross-Montage%2520EEG%2520Transfer%2520Learning%2520via%2520Individual%250A%2520%2520Tangent%2520Space%2520Alignment%2520and%2520Spatial-Riemannian%2520Feature%2520Fusion%26entry.906535625%3DNicole%2520Lai-Tan%2520and%2520Xiao%2520Gu%2520and%2520Marios%2520G.%2520Philiastides%2520and%2520Fani%2520Deligianni%26entry.1292438233%3D%2520%2520Personalised%2520music-based%2520interventions%2520offer%2520a%2520powerful%2520means%2520of%2520supporting%250Amotor%2520rehabilitation%2520by%2520dynamically%2520tailoring%2520auditory%2520stimuli%2520to%2520provide%250Aexternal%2520timekeeping%2520cues%252C%2520modulate%2520affective%2520states%252C%2520and%2520stabilise%2520gait%250Apatterns.%2520Generalisable%2520Brain-Computer%2520Interfaces%2520%2528BCIs%2529%2520thus%2520hold%2520promise%2520for%250Aadapting%2520these%2520interventions%2520across%2520individuals.%2520However%252C%2520inter-subject%250Avariability%2520in%2520EEG%2520signals%252C%2520further%2520compounded%2520by%2520movement-induced%2520artefacts%250Aand%2520motor%2520planning%2520differences%252C%2520hinders%2520the%2520generalisability%2520of%2520BCIs%2520and%250Aresults%2520in%2520lengthy%2520calibration%2520processes.%2520We%2520propose%2520Individual%2520Tangent%2520Space%250AAlignment%2520%2528ITSA%2529%252C%2520a%2520novel%2520pre-alignment%2520strategy%2520incorporating%2520subject-specific%250Arecentering%252C%2520distribution%2520matching%252C%2520and%2520supervised%2520rotational%2520alignment%2520to%250Aenhance%2520cross-subject%2520generalisation.%2520Our%2520hybrid%2520architecture%2520fuses%2520Regularised%250ACommon%2520Spatial%2520Patterns%2520%2528RCSP%2529%2520with%2520Riemannian%2520geometry%2520in%2520parallel%2520and%250Asequential%2520configurations%252C%2520improving%2520class%2520separability%2520while%2520maintaining%2520the%250Ageometric%2520structure%2520of%2520covariance%2520matrices%2520for%2520robust%2520statistical%2520computation.%250AUsing%2520leave-one-subject-out%2520cross-validation%252C%2520%2560ITSA%2527%2520demonstrates%2520significant%250Aperformance%2520improvements%2520across%2520subjects%2520and%2520conditions.%2520The%2520parallel%2520fusion%250Aapproach%2520shows%2520the%2520greatest%2520enhancement%2520over%2520its%2520sequential%2520counterpart%252C%2520with%250Arobust%2520performance%2520maintained%2520across%2520varying%2520data%2520conditions%2520and%2520electrode%250Aconfigurations.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%2520the%2520time%2520of%250Apublication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Subject%20and%20Cross-Montage%20EEG%20Transfer%20Learning%20via%20Individual%0A%20%20Tangent%20Space%20Alignment%20and%20Spatial-Riemannian%20Feature%20Fusion&entry.906535625=Nicole%20Lai-Tan%20and%20Xiao%20Gu%20and%20Marios%20G.%20Philiastides%20and%20Fani%20Deligianni&entry.1292438233=%20%20Personalised%20music-based%20interventions%20offer%20a%20powerful%20means%20of%20supporting%0Amotor%20rehabilitation%20by%20dynamically%20tailoring%20auditory%20stimuli%20to%20provide%0Aexternal%20timekeeping%20cues%2C%20modulate%20affective%20states%2C%20and%20stabilise%20gait%0Apatterns.%20Generalisable%20Brain-Computer%20Interfaces%20%28BCIs%29%20thus%20hold%20promise%20for%0Aadapting%20these%20interventions%20across%20individuals.%20However%2C%20inter-subject%0Avariability%20in%20EEG%20signals%2C%20further%20compounded%20by%20movement-induced%20artefacts%0Aand%20motor%20planning%20differences%2C%20hinders%20the%20generalisability%20of%20BCIs%20and%0Aresults%20in%20lengthy%20calibration%20processes.%20We%20propose%20Individual%20Tangent%20Space%0AAlignment%20%28ITSA%29%2C%20a%20novel%20pre-alignment%20strategy%20incorporating%20subject-specific%0Arecentering%2C%20distribution%20matching%2C%20and%20supervised%20rotational%20alignment%20to%0Aenhance%20cross-subject%20generalisation.%20Our%20hybrid%20architecture%20fuses%20Regularised%0ACommon%20Spatial%20Patterns%20%28RCSP%29%20with%20Riemannian%20geometry%20in%20parallel%20and%0Asequential%20configurations%2C%20improving%20class%20separability%20while%20maintaining%20the%0Ageometric%20structure%20of%20covariance%20matrices%20for%20robust%20statistical%20computation.%0AUsing%20leave-one-subject-out%20cross-validation%2C%20%60ITSA%27%20demonstrates%20significant%0Aperformance%20improvements%20across%20subjects%20and%20conditions.%20The%20parallel%20fusion%0Aapproach%20shows%20the%20greatest%20enhancement%20over%20its%20sequential%20counterpart%2C%20with%0Arobust%20performance%20maintained%20across%20varying%20data%20conditions%20and%20electrode%0Aconfigurations.%20The%20code%20will%20be%20made%20publicly%20available%20at%20the%20time%20of%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08216v1&entry.124074799=Read"},
{"title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution", "author": "Zailong Tian and Zhuoheng Han and Yanzhe Chen and Haozhe Xu and Xi Yang and Richeng Xuan and Houfeng Wang and Lizi Liao", "abstract": "  Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines.\n", "link": "http://arxiv.org/abs/2508.06225v2", "date": "2025-08-11", "relevancy": 2.1148, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&body=Title%3A%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution%0AAuthor%3A%20Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20Richeng%20Xuan%20and%20Houfeng%20Wang%20and%20Lizi%20Liao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20Overconfidence%20Phenomenon%20in%20current%0ALLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%20actual%0Acorrectness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%20quantify%20this%0Aphenomenon%2C%20we%20introduce%20TH-Score%2C%20a%20novel%20metric%20measuring%20confidence-accuracy%0Aalignment.%20Furthermore%2C%20we%20propose%20LLM-as-a-Fuser%2C%20an%20ensemble%20framework%20that%0Atransforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20substantially%20improves%20calibration%20and%20enables%0Aadaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%20achieving%20superior%0Areliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06225v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverconfidence%2520in%2520LLM-as-a-Judge%253A%2520Diagnosis%2520and%2520Confidence-Driven%250A%2520%2520Solution%26entry.906535625%3DZailong%2520Tian%2520and%2520Zhuoheng%2520Han%2520and%2520Yanzhe%2520Chen%2520and%2520Haozhe%2520Xu%2520and%2520Xi%2520Yang%2520and%2520Richeng%2520Xuan%2520and%2520Houfeng%2520Wang%2520and%2520Lizi%2520Liao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520as%2520automated%2520judges%252C%2520where%250Apractical%2520value%2520depends%2520on%2520both%2520accuracy%2520and%2520trustworthy%252C%2520risk-aware%2520judgments.%250AExisting%2520approaches%2520predominantly%2520focus%2520on%2520accuracy%252C%2520overlooking%2520the%2520necessity%250Aof%2520well-calibrated%2520confidence%252C%2520which%2520is%2520vital%2520for%2520adaptive%2520and%2520reliable%250Aevaluation%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520advocate%2520a%2520shift%2520from%2520accuracy-centric%250Aevaluation%2520to%2520confidence-driven%252C%2520risk-aware%2520LLM-as-a-Judge%2520systems%252C%2520emphasizing%250Athe%2520necessity%2520of%2520well-calibrated%2520confidence%2520for%2520trustworthy%2520and%2520adaptive%250Aevaluation.%2520We%2520systematically%2520identify%2520the%2520Overconfidence%2520Phenomenon%2520in%2520current%250ALLM-as-a-Judges%252C%2520where%2520predicted%2520confidence%2520significantly%2520overstates%2520actual%250Acorrectness%252C%2520undermining%2520reliability%2520in%2520practical%2520deployment.%2520To%2520quantify%2520this%250Aphenomenon%252C%2520we%2520introduce%2520TH-Score%252C%2520a%2520novel%2520metric%2520measuring%2520confidence-accuracy%250Aalignment.%2520Furthermore%252C%2520we%2520propose%2520LLM-as-a-Fuser%252C%2520an%2520ensemble%2520framework%2520that%250Atransforms%2520LLMs%2520into%2520reliable%252C%2520risk-aware%2520evaluators.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520substantially%2520improves%2520calibration%2520and%2520enables%250Aadaptive%252C%2520confidence-driven%2520evaluation%2520pipelines%252C%2520achieving%2520superior%250Areliability%2520and%2520accuracy%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06225v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&entry.906535625=Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20Richeng%20Xuan%20and%20Houfeng%20Wang%20and%20Lizi%20Liao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20Overconfidence%20Phenomenon%20in%20current%0ALLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%20actual%0Acorrectness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%20quantify%20this%0Aphenomenon%2C%20we%20introduce%20TH-Score%2C%20a%20novel%20metric%20measuring%20confidence-accuracy%0Aalignment.%20Furthermore%2C%20we%20propose%20LLM-as-a-Fuser%2C%20an%20ensemble%20framework%20that%0Atransforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20substantially%20improves%20calibration%20and%20enables%0Aadaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%20achieving%20superior%0Areliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06225v2&entry.124074799=Read"},
{"title": "Runtime Monitoring and Enforcement of Conditional Fairness in Generative\n  AIs", "author": "Chih-Hong Cheng and Changshun Wu and Xingyu Zhao and Saddek Bensalem and Harald Ruess", "abstract": "  The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires ``conditional fairness''\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for assessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems.\n", "link": "http://arxiv.org/abs/2404.16663v5", "date": "2025-08-11", "relevancy": 2.1115, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5778}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5327}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Runtime%20Monitoring%20and%20Enforcement%20of%20Conditional%20Fairness%20in%20Generative%0A%20%20AIs&body=Title%3A%20Runtime%20Monitoring%20and%20Enforcement%20of%20Conditional%20Fairness%20in%20Generative%0A%20%20AIs%0AAuthor%3A%20Chih-Hong%20Cheng%20and%20Changshun%20Wu%20and%20Xingyu%20Zhao%20and%20Saddek%20Bensalem%20and%20Harald%20Ruess%0AAbstract%3A%20%20%20The%20deployment%20of%20generative%20AI%20%28GenAI%29%20models%20raises%20significant%20fairness%0Aconcerns%2C%20addressed%20in%20this%20paper%20through%20novel%20characterization%20and%0Aenforcement%20techniques%20specific%20to%20GenAI.%20Unlike%20standard%20AI%20performing%0Aspecific%20tasks%2C%20GenAI%27s%20broad%20functionality%20requires%20%60%60conditional%20fairness%27%27%0Atailored%20to%20the%20context%20being%20generated%2C%20such%20as%20demographic%20fairness%20in%0Agenerating%20images%20of%20poor%20people%20versus%20successful%20business%20leaders.%20We%20define%0Atwo%20fairness%20levels%3A%20the%20first%20evaluates%20fairness%20in%20generated%20outputs%2C%0Aindependent%20of%20prompts%20and%20models%3B%20the%20second%20assesses%20inherent%20fairness%20with%0Aneutral%20prompts.%20Given%20the%20complexity%20of%20GenAI%20and%20challenges%20in%20fairness%0Aspecifications%2C%20we%20focus%20on%20bounding%20the%20worst%20case%2C%20considering%20a%20GenAI%20system%0Aunfair%20if%20the%20distance%20between%20appearances%20of%20a%20specific%20group%20exceeds%20preset%0Athresholds.%20We%20also%20explore%20combinatorial%20testing%20for%20assessing%20relative%0Acompleteness%20in%20intersectional%20fairness.%20By%20bounding%20the%20worst%20case%2C%20we%20develop%0Aa%20prompt%20injection%20scheme%20within%20an%20agent-based%20framework%20to%20enforce%0Aconditional%20fairness%20with%20minimal%20intervention%2C%20validated%20on%20state-of-the-art%0AGenAI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16663v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRuntime%2520Monitoring%2520and%2520Enforcement%2520of%2520Conditional%2520Fairness%2520in%2520Generative%250A%2520%2520AIs%26entry.906535625%3DChih-Hong%2520Cheng%2520and%2520Changshun%2520Wu%2520and%2520Xingyu%2520Zhao%2520and%2520Saddek%2520Bensalem%2520and%2520Harald%2520Ruess%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520models%2520raises%2520significant%2520fairness%250Aconcerns%252C%2520addressed%2520in%2520this%2520paper%2520through%2520novel%2520characterization%2520and%250Aenforcement%2520techniques%2520specific%2520to%2520GenAI.%2520Unlike%2520standard%2520AI%2520performing%250Aspecific%2520tasks%252C%2520GenAI%2527s%2520broad%2520functionality%2520requires%2520%2560%2560conditional%2520fairness%2527%2527%250Atailored%2520to%2520the%2520context%2520being%2520generated%252C%2520such%2520as%2520demographic%2520fairness%2520in%250Agenerating%2520images%2520of%2520poor%2520people%2520versus%2520successful%2520business%2520leaders.%2520We%2520define%250Atwo%2520fairness%2520levels%253A%2520the%2520first%2520evaluates%2520fairness%2520in%2520generated%2520outputs%252C%250Aindependent%2520of%2520prompts%2520and%2520models%253B%2520the%2520second%2520assesses%2520inherent%2520fairness%2520with%250Aneutral%2520prompts.%2520Given%2520the%2520complexity%2520of%2520GenAI%2520and%2520challenges%2520in%2520fairness%250Aspecifications%252C%2520we%2520focus%2520on%2520bounding%2520the%2520worst%2520case%252C%2520considering%2520a%2520GenAI%2520system%250Aunfair%2520if%2520the%2520distance%2520between%2520appearances%2520of%2520a%2520specific%2520group%2520exceeds%2520preset%250Athresholds.%2520We%2520also%2520explore%2520combinatorial%2520testing%2520for%2520assessing%2520relative%250Acompleteness%2520in%2520intersectional%2520fairness.%2520By%2520bounding%2520the%2520worst%2520case%252C%2520we%2520develop%250Aa%2520prompt%2520injection%2520scheme%2520within%2520an%2520agent-based%2520framework%2520to%2520enforce%250Aconditional%2520fairness%2520with%2520minimal%2520intervention%252C%2520validated%2520on%2520state-of-the-art%250AGenAI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16663v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Runtime%20Monitoring%20and%20Enforcement%20of%20Conditional%20Fairness%20in%20Generative%0A%20%20AIs&entry.906535625=Chih-Hong%20Cheng%20and%20Changshun%20Wu%20and%20Xingyu%20Zhao%20and%20Saddek%20Bensalem%20and%20Harald%20Ruess&entry.1292438233=%20%20The%20deployment%20of%20generative%20AI%20%28GenAI%29%20models%20raises%20significant%20fairness%0Aconcerns%2C%20addressed%20in%20this%20paper%20through%20novel%20characterization%20and%0Aenforcement%20techniques%20specific%20to%20GenAI.%20Unlike%20standard%20AI%20performing%0Aspecific%20tasks%2C%20GenAI%27s%20broad%20functionality%20requires%20%60%60conditional%20fairness%27%27%0Atailored%20to%20the%20context%20being%20generated%2C%20such%20as%20demographic%20fairness%20in%0Agenerating%20images%20of%20poor%20people%20versus%20successful%20business%20leaders.%20We%20define%0Atwo%20fairness%20levels%3A%20the%20first%20evaluates%20fairness%20in%20generated%20outputs%2C%0Aindependent%20of%20prompts%20and%20models%3B%20the%20second%20assesses%20inherent%20fairness%20with%0Aneutral%20prompts.%20Given%20the%20complexity%20of%20GenAI%20and%20challenges%20in%20fairness%0Aspecifications%2C%20we%20focus%20on%20bounding%20the%20worst%20case%2C%20considering%20a%20GenAI%20system%0Aunfair%20if%20the%20distance%20between%20appearances%20of%20a%20specific%20group%20exceeds%20preset%0Athresholds.%20We%20also%20explore%20combinatorial%20testing%20for%20assessing%20relative%0Acompleteness%20in%20intersectional%20fairness.%20By%20bounding%20the%20worst%20case%2C%20we%20develop%0Aa%20prompt%20injection%20scheme%20within%20an%20agent-based%20framework%20to%20enforce%0Aconditional%20fairness%20with%20minimal%20intervention%2C%20validated%20on%20state-of-the-art%0AGenAI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16663v5&entry.124074799=Read"},
{"title": "WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer", "author": "Junyu Wu and Weiming Chang and Xiaotao Liu and Guanyou He and Tingfeng Xian and Haoqiang Hong and Boqi Chen and Haotao Tian and Tao Yang and Yunsheng Shi and Feng Lin and Ting Yao", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent\nparadigm for training large language models and multimodal systems. Despite\nnotable advances enabled by existing RLHF training frameworks, significant\nchallenges remain in scaling to complex multimodal workflows and adapting to\ndynamic workloads. In particular, current systems often encounter limitations\nrelated to controller scalability when managing large models, as well as\ninefficiencies in orchestrating intricate RLHF pipelines, especially in\nscenarios that require dynamic sampling and resource allocation. In this paper,\nwe introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,\nscalable, and balanced RLHF training framework specifically designed to address\nthese challenges. WeChat-YATT features a parallel controller programming model\nthat enables flexible and efficient orchestration of complex RLHF workflows,\neffectively mitigating the bottlenecks associated with centralized controller\narchitectures and facilitating scalability in large-scale data scenarios. In\naddition, we propose a dynamic placement schema that adaptively partitions\ncomputational resources and schedules workloads, thereby significantly reducing\nhardware idle time and improving GPU utilization under variable training\nconditions. We evaluate WeChat-YATT across a range of experimental scenarios,\ndemonstrating that it achieves substantial improvements in throughput compared\nto state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been\nsuccessfully deployed to train models supporting WeChat product features for a\nlarge-scale user base, underscoring its effectiveness and robustness in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2508.07970v1", "date": "2025-08-11", "relevancy": 2.0796, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5432}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&body=Title%3A%20WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer%0AAuthor%3A%20Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Tingfeng%20Xian%20and%20Haoqiang%20Hong%20and%20Boqi%20Chen%20and%20Haotao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20emerged%20as%20a%20prominent%0Aparadigm%20for%20training%20large%20language%20models%20and%20multimodal%20systems.%20Despite%0Anotable%20advances%20enabled%20by%20existing%20RLHF%20training%20frameworks%2C%20significant%0Achallenges%20remain%20in%20scaling%20to%20complex%20multimodal%20workflows%20and%20adapting%20to%0Adynamic%20workloads.%20In%20particular%2C%20current%20systems%20often%20encounter%20limitations%0Arelated%20to%20controller%20scalability%20when%20managing%20large%20models%2C%20as%20well%20as%0Ainefficiencies%20in%20orchestrating%20intricate%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20that%20require%20dynamic%20sampling%20and%20resource%20allocation.%20In%20this%20paper%2C%0Awe%20introduce%20WeChat-YATT%20%28Yet%20Another%20Transformer%20Trainer%20in%20WeChat%29%2C%20a%20simple%2C%0Ascalable%2C%20and%20balanced%20RLHF%20training%20framework%20specifically%20designed%20to%20address%0Athese%20challenges.%20WeChat-YATT%20features%20a%20parallel%20controller%20programming%20model%0Athat%20enables%20flexible%20and%20efficient%20orchestration%20of%20complex%20RLHF%20workflows%2C%0Aeffectively%20mitigating%20the%20bottlenecks%20associated%20with%20centralized%20controller%0Aarchitectures%20and%20facilitating%20scalability%20in%20large-scale%20data%20scenarios.%20In%0Aaddition%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%20adaptively%20partitions%0Acomputational%20resources%20and%20schedules%20workloads%2C%20thereby%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20GPU%20utilization%20under%20variable%20training%0Aconditions.%20We%20evaluate%20WeChat-YATT%20across%20a%20range%20of%20experimental%20scenarios%2C%0Ademonstrating%20that%20it%20achieves%20substantial%20improvements%20in%20throughput%20compared%0Ato%20state-of-the-art%20RLHF%20training%20frameworks.%20Furthermore%2C%20WeChat-YATT%20has%20been%0Asuccessfully%20deployed%20to%20train%20models%20supporting%20WeChat%20product%20features%20for%20a%0Alarge-scale%20user%20base%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeChat-YATT%253A%2520A%2520Simple%252C%2520Scalable%2520and%2520Balanced%2520RLHF%2520Trainer%26entry.906535625%3DJunyu%2520Wu%2520and%2520Weiming%2520Chang%2520and%2520Xiaotao%2520Liu%2520and%2520Guanyou%2520He%2520and%2520Tingfeng%2520Xian%2520and%2520Haoqiang%2520Hong%2520and%2520Boqi%2520Chen%2520and%2520Haotao%2520Tian%2520and%2520Tao%2520Yang%2520and%2520Yunsheng%2520Shi%2520and%2520Feng%2520Lin%2520and%2520Ting%2520Yao%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520emerged%2520as%2520a%2520prominent%250Aparadigm%2520for%2520training%2520large%2520language%2520models%2520and%2520multimodal%2520systems.%2520Despite%250Anotable%2520advances%2520enabled%2520by%2520existing%2520RLHF%2520training%2520frameworks%252C%2520significant%250Achallenges%2520remain%2520in%2520scaling%2520to%2520complex%2520multimodal%2520workflows%2520and%2520adapting%2520to%250Adynamic%2520workloads.%2520In%2520particular%252C%2520current%2520systems%2520often%2520encounter%2520limitations%250Arelated%2520to%2520controller%2520scalability%2520when%2520managing%2520large%2520models%252C%2520as%2520well%2520as%250Ainefficiencies%2520in%2520orchestrating%2520intricate%2520RLHF%2520pipelines%252C%2520especially%2520in%250Ascenarios%2520that%2520require%2520dynamic%2520sampling%2520and%2520resource%2520allocation.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520WeChat-YATT%2520%2528Yet%2520Another%2520Transformer%2520Trainer%2520in%2520WeChat%2529%252C%2520a%2520simple%252C%250Ascalable%252C%2520and%2520balanced%2520RLHF%2520training%2520framework%2520specifically%2520designed%2520to%2520address%250Athese%2520challenges.%2520WeChat-YATT%2520features%2520a%2520parallel%2520controller%2520programming%2520model%250Athat%2520enables%2520flexible%2520and%2520efficient%2520orchestration%2520of%2520complex%2520RLHF%2520workflows%252C%250Aeffectively%2520mitigating%2520the%2520bottlenecks%2520associated%2520with%2520centralized%2520controller%250Aarchitectures%2520and%2520facilitating%2520scalability%2520in%2520large-scale%2520data%2520scenarios.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520dynamic%2520placement%2520schema%2520that%2520adaptively%2520partitions%250Acomputational%2520resources%2520and%2520schedules%2520workloads%252C%2520thereby%2520significantly%2520reducing%250Ahardware%2520idle%2520time%2520and%2520improving%2520GPU%2520utilization%2520under%2520variable%2520training%250Aconditions.%2520We%2520evaluate%2520WeChat-YATT%2520across%2520a%2520range%2520of%2520experimental%2520scenarios%252C%250Ademonstrating%2520that%2520it%2520achieves%2520substantial%2520improvements%2520in%2520throughput%2520compared%250Ato%2520state-of-the-art%2520RLHF%2520training%2520frameworks.%2520Furthermore%252C%2520WeChat-YATT%2520has%2520been%250Asuccessfully%2520deployed%2520to%2520train%2520models%2520supporting%2520WeChat%2520product%2520features%2520for%2520a%250Alarge-scale%2520user%2520base%252C%2520underscoring%2520its%2520effectiveness%2520and%2520robustness%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeChat-YATT%3A%20A%20Simple%2C%20Scalable%20and%20Balanced%20RLHF%20Trainer&entry.906535625=Junyu%20Wu%20and%20Weiming%20Chang%20and%20Xiaotao%20Liu%20and%20Guanyou%20He%20and%20Tingfeng%20Xian%20and%20Haoqiang%20Hong%20and%20Boqi%20Chen%20and%20Haotao%20Tian%20and%20Tao%20Yang%20and%20Yunsheng%20Shi%20and%20Feng%20Lin%20and%20Ting%20Yao&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20emerged%20as%20a%20prominent%0Aparadigm%20for%20training%20large%20language%20models%20and%20multimodal%20systems.%20Despite%0Anotable%20advances%20enabled%20by%20existing%20RLHF%20training%20frameworks%2C%20significant%0Achallenges%20remain%20in%20scaling%20to%20complex%20multimodal%20workflows%20and%20adapting%20to%0Adynamic%20workloads.%20In%20particular%2C%20current%20systems%20often%20encounter%20limitations%0Arelated%20to%20controller%20scalability%20when%20managing%20large%20models%2C%20as%20well%20as%0Ainefficiencies%20in%20orchestrating%20intricate%20RLHF%20pipelines%2C%20especially%20in%0Ascenarios%20that%20require%20dynamic%20sampling%20and%20resource%20allocation.%20In%20this%20paper%2C%0Awe%20introduce%20WeChat-YATT%20%28Yet%20Another%20Transformer%20Trainer%20in%20WeChat%29%2C%20a%20simple%2C%0Ascalable%2C%20and%20balanced%20RLHF%20training%20framework%20specifically%20designed%20to%20address%0Athese%20challenges.%20WeChat-YATT%20features%20a%20parallel%20controller%20programming%20model%0Athat%20enables%20flexible%20and%20efficient%20orchestration%20of%20complex%20RLHF%20workflows%2C%0Aeffectively%20mitigating%20the%20bottlenecks%20associated%20with%20centralized%20controller%0Aarchitectures%20and%20facilitating%20scalability%20in%20large-scale%20data%20scenarios.%20In%0Aaddition%2C%20we%20propose%20a%20dynamic%20placement%20schema%20that%20adaptively%20partitions%0Acomputational%20resources%20and%20schedules%20workloads%2C%20thereby%20significantly%20reducing%0Ahardware%20idle%20time%20and%20improving%20GPU%20utilization%20under%20variable%20training%0Aconditions.%20We%20evaluate%20WeChat-YATT%20across%20a%20range%20of%20experimental%20scenarios%2C%0Ademonstrating%20that%20it%20achieves%20substantial%20improvements%20in%20throughput%20compared%0Ato%20state-of-the-art%20RLHF%20training%20frameworks.%20Furthermore%2C%20WeChat-YATT%20has%20been%0Asuccessfully%20deployed%20to%20train%20models%20supporting%20WeChat%20product%20features%20for%20a%0Alarge-scale%20user%20base%2C%20underscoring%20its%20effectiveness%20and%20robustness%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07970v1&entry.124074799=Read"},
{"title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning", "author": "Runchuan Zhu and Bowen Jiang and Lingrui Mei and Fangkai Yang and Lu Wang and Haoxiang Gao and Fengshuo Bai and Pu Zhao and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang", "abstract": "  Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.\n", "link": "http://arxiv.org/abs/2508.08053v1", "date": "2025-08-11", "relevancy": 2.0658, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5124}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptFlow%3A%20Adaptive%20Workflow%20Optimization%20via%20Meta-Learning&body=Title%3A%20AdaptFlow%3A%20Adaptive%20Workflow%20Optimization%20via%20Meta-Learning%0AAuthor%3A%20Runchuan%20Zhu%20and%20Bowen%20Jiang%20and%20Lingrui%20Mei%20and%20Fangkai%20Yang%20and%20Lu%20Wang%20and%20Haoxiang%20Gao%20and%20Fengshuo%20Bai%20and%20Pu%20Zhao%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20sparked%20growing%20interest%0Ain%20agentic%20workflows%2C%20which%20are%20structured%20sequences%20of%20LLM%20invocations%0Aintended%20to%20solve%20complex%20tasks.%20However%2C%20existing%20approaches%20often%20rely%20on%0Astatic%20templates%20or%20manually%20designed%20workflows%2C%20which%20limit%20adaptability%20to%0Adiverse%20tasks%20and%20hinder%20scalability.%20We%20propose%20AdaptFlow%2C%20a%20natural%0Alanguage-based%20meta-learning%20framework%20inspired%20by%20model-agnostic%20meta-learning%0A%28MAML%29.%20AdaptFlow%20learns%20a%20generalizable%20workflow%20initialization%20that%20enables%0Arapid%20subtask-level%20adaptation.%20It%20employs%20a%20bi-level%20optimization%20scheme%3A%20the%0Ainner%20loop%20refines%20the%20workflow%20for%20a%20specific%20subtask%20using%20LLM-generated%0Afeedback%2C%20while%20the%20outer%20loop%20updates%20the%20shared%20initialization%20to%20perform%0Awell%20across%20tasks.%20This%20setup%20allows%20AdaptFlow%20to%20generalize%20effectively%20to%0Aunseen%20tasks%20by%20adapting%20the%20initialized%20workflow%20through%20language-guided%0Amodifications.%20Evaluated%20across%20question%20answering%2C%20code%20generation%2C%20and%0Amathematical%20reasoning%20benchmarks%2C%20AdaptFlow%20consistently%20outperforms%20both%0Amanually%20crafted%20and%20automatically%20searched%20baselines%2C%20achieving%0Astate-of-the-art%20results%20with%20strong%20generalization%20across%20tasks%20and%20models.%0AThe%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptFlow%253A%2520Adaptive%2520Workflow%2520Optimization%2520via%2520Meta-Learning%26entry.906535625%3DRunchuan%2520Zhu%2520and%2520Bowen%2520Jiang%2520and%2520Lingrui%2520Mei%2520and%2520Fangkai%2520Yang%2520and%2520Lu%2520Wang%2520and%2520Haoxiang%2520Gao%2520and%2520Fengshuo%2520Bai%2520and%2520Pu%2520Zhao%2520and%2520Qingwei%2520Lin%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520sparked%2520growing%2520interest%250Ain%2520agentic%2520workflows%252C%2520which%2520are%2520structured%2520sequences%2520of%2520LLM%2520invocations%250Aintended%2520to%2520solve%2520complex%2520tasks.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%250Astatic%2520templates%2520or%2520manually%2520designed%2520workflows%252C%2520which%2520limit%2520adaptability%2520to%250Adiverse%2520tasks%2520and%2520hinder%2520scalability.%2520We%2520propose%2520AdaptFlow%252C%2520a%2520natural%250Alanguage-based%2520meta-learning%2520framework%2520inspired%2520by%2520model-agnostic%2520meta-learning%250A%2528MAML%2529.%2520AdaptFlow%2520learns%2520a%2520generalizable%2520workflow%2520initialization%2520that%2520enables%250Arapid%2520subtask-level%2520adaptation.%2520It%2520employs%2520a%2520bi-level%2520optimization%2520scheme%253A%2520the%250Ainner%2520loop%2520refines%2520the%2520workflow%2520for%2520a%2520specific%2520subtask%2520using%2520LLM-generated%250Afeedback%252C%2520while%2520the%2520outer%2520loop%2520updates%2520the%2520shared%2520initialization%2520to%2520perform%250Awell%2520across%2520tasks.%2520This%2520setup%2520allows%2520AdaptFlow%2520to%2520generalize%2520effectively%2520to%250Aunseen%2520tasks%2520by%2520adapting%2520the%2520initialized%2520workflow%2520through%2520language-guided%250Amodifications.%2520Evaluated%2520across%2520question%2520answering%252C%2520code%2520generation%252C%2520and%250Amathematical%2520reasoning%2520benchmarks%252C%2520AdaptFlow%2520consistently%2520outperforms%2520both%250Amanually%2520crafted%2520and%2520automatically%2520searched%2520baselines%252C%2520achieving%250Astate-of-the-art%2520results%2520with%2520strong%2520generalization%2520across%2520tasks%2520and%2520models.%250AThe%2520source%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptFlow%3A%20Adaptive%20Workflow%20Optimization%20via%20Meta-Learning&entry.906535625=Runchuan%20Zhu%20and%20Bowen%20Jiang%20and%20Lingrui%20Mei%20and%20Fangkai%20Yang%20and%20Lu%20Wang%20and%20Haoxiang%20Gao%20and%20Fengshuo%20Bai%20and%20Pu%20Zhao%20and%20Qingwei%20Lin%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20sparked%20growing%20interest%0Ain%20agentic%20workflows%2C%20which%20are%20structured%20sequences%20of%20LLM%20invocations%0Aintended%20to%20solve%20complex%20tasks.%20However%2C%20existing%20approaches%20often%20rely%20on%0Astatic%20templates%20or%20manually%20designed%20workflows%2C%20which%20limit%20adaptability%20to%0Adiverse%20tasks%20and%20hinder%20scalability.%20We%20propose%20AdaptFlow%2C%20a%20natural%0Alanguage-based%20meta-learning%20framework%20inspired%20by%20model-agnostic%20meta-learning%0A%28MAML%29.%20AdaptFlow%20learns%20a%20generalizable%20workflow%20initialization%20that%20enables%0Arapid%20subtask-level%20adaptation.%20It%20employs%20a%20bi-level%20optimization%20scheme%3A%20the%0Ainner%20loop%20refines%20the%20workflow%20for%20a%20specific%20subtask%20using%20LLM-generated%0Afeedback%2C%20while%20the%20outer%20loop%20updates%20the%20shared%20initialization%20to%20perform%0Awell%20across%20tasks.%20This%20setup%20allows%20AdaptFlow%20to%20generalize%20effectively%20to%0Aunseen%20tasks%20by%20adapting%20the%20initialized%20workflow%20through%20language-guided%0Amodifications.%20Evaluated%20across%20question%20answering%2C%20code%20generation%2C%20and%0Amathematical%20reasoning%20benchmarks%2C%20AdaptFlow%20consistently%20outperforms%20both%0Amanually%20crafted%20and%20automatically%20searched%20baselines%2C%20achieving%0Astate-of-the-art%20results%20with%20strong%20generalization%20across%20tasks%20and%20models.%0AThe%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08053v1&entry.124074799=Read"},
{"title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval", "author": "Meixiu Long and Duolin Sun and Dan Yang and Junjie Wang and Yue Shen and Jian Wang and Peng Wei and Jinjie Gu and Jiahai Wang", "abstract": "  Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.\n", "link": "http://arxiv.org/abs/2508.07995v1", "date": "2025-08-11", "relevancy": 2.0576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5182}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval&body=Title%3A%20DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval%0AAuthor%3A%20Meixiu%20Long%20and%20Duolin%20Sun%20and%20Dan%20Yang%20and%20Junjie%20Wang%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Jinjie%20Gu%20and%20Jiahai%20Wang%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20has%20achieved%20strong%20performance%20on%0Aknowledge-intensive%20tasks%20where%20query-document%20relevance%20can%20be%20identified%0Athrough%20direct%20lexical%20or%20semantic%20matches.%20However%2C%20many%20real-world%20queries%0Ainvolve%20abstract%20reasoning%2C%20analogical%20thinking%2C%20or%20multi-step%20inference%2C%20which%0Aexisting%20retrievers%20often%20struggle%20to%20capture.%20To%20address%20this%20challenge%2C%20we%0Apresent%20%5Ctextbf%7BDIVER%7D%2C%20a%20retrieval%20pipeline%20tailored%20for%20reasoning-intensive%0Ainformation%20retrieval.%20DIVER%20consists%20of%20four%20components%3A%20document%20processing%0Ato%20improve%20input%20quality%2C%20LLM-driven%20query%20expansion%20via%20iterative%20document%0Ainteraction%2C%20a%20reasoning-enhanced%20retriever%20fine-tuned%20on%20synthetic%0Amulti-domain%20data%20with%20hard%20negatives%2C%20and%20a%20pointwise%20reranker%20that%20combines%0ALLM-assigned%20helpfulness%20scores%20with%20retrieval%20scores.%20On%20the%20BRIGHT%20benchmark%2C%0ADIVER%20achieves%20state-of-the-art%20nDCG%4010%20scores%20of%2041.6%20and%2028.9%20on%20original%0Aqueries%2C%20consistently%20outperforming%20competitive%20reasoning-aware%20models.%20These%0Aresults%20demonstrate%20the%20effectiveness%20of%20reasoning-aware%20retrieval%20strategies%0Ain%20complex%20real-world%20tasks.%20Our%20code%20and%20retrieval%20model%20will%20be%20released%0Asoon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVER%253A%2520A%2520Multi-Stage%2520Approach%2520for%2520Reasoning-intensive%2520Information%250A%2520%2520Retrieval%26entry.906535625%3DMeixiu%2520Long%2520and%2520Duolin%2520Sun%2520and%2520Dan%2520Yang%2520and%2520Junjie%2520Wang%2520and%2520Yue%2520Shen%2520and%2520Jian%2520Wang%2520and%2520Peng%2520Wei%2520and%2520Jinjie%2520Gu%2520and%2520Jiahai%2520Wang%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520has%2520achieved%2520strong%2520performance%2520on%250Aknowledge-intensive%2520tasks%2520where%2520query-document%2520relevance%2520can%2520be%2520identified%250Athrough%2520direct%2520lexical%2520or%2520semantic%2520matches.%2520However%252C%2520many%2520real-world%2520queries%250Ainvolve%2520abstract%2520reasoning%252C%2520analogical%2520thinking%252C%2520or%2520multi-step%2520inference%252C%2520which%250Aexisting%2520retrievers%2520often%2520struggle%2520to%2520capture.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apresent%2520%255Ctextbf%257BDIVER%257D%252C%2520a%2520retrieval%2520pipeline%2520tailored%2520for%2520reasoning-intensive%250Ainformation%2520retrieval.%2520DIVER%2520consists%2520of%2520four%2520components%253A%2520document%2520processing%250Ato%2520improve%2520input%2520quality%252C%2520LLM-driven%2520query%2520expansion%2520via%2520iterative%2520document%250Ainteraction%252C%2520a%2520reasoning-enhanced%2520retriever%2520fine-tuned%2520on%2520synthetic%250Amulti-domain%2520data%2520with%2520hard%2520negatives%252C%2520and%2520a%2520pointwise%2520reranker%2520that%2520combines%250ALLM-assigned%2520helpfulness%2520scores%2520with%2520retrieval%2520scores.%2520On%2520the%2520BRIGHT%2520benchmark%252C%250ADIVER%2520achieves%2520state-of-the-art%2520nDCG%254010%2520scores%2520of%252041.6%2520and%252028.9%2520on%2520original%250Aqueries%252C%2520consistently%2520outperforming%2520competitive%2520reasoning-aware%2520models.%2520These%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520reasoning-aware%2520retrieval%2520strategies%250Ain%2520complex%2520real-world%2520tasks.%2520Our%2520code%2520and%2520retrieval%2520model%2520will%2520be%2520released%250Asoon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVER%3A%20A%20Multi-Stage%20Approach%20for%20Reasoning-intensive%20Information%0A%20%20Retrieval&entry.906535625=Meixiu%20Long%20and%20Duolin%20Sun%20and%20Dan%20Yang%20and%20Junjie%20Wang%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Jinjie%20Gu%20and%20Jiahai%20Wang&entry.1292438233=%20%20Retrieval-augmented%20generation%20has%20achieved%20strong%20performance%20on%0Aknowledge-intensive%20tasks%20where%20query-document%20relevance%20can%20be%20identified%0Athrough%20direct%20lexical%20or%20semantic%20matches.%20However%2C%20many%20real-world%20queries%0Ainvolve%20abstract%20reasoning%2C%20analogical%20thinking%2C%20or%20multi-step%20inference%2C%20which%0Aexisting%20retrievers%20often%20struggle%20to%20capture.%20To%20address%20this%20challenge%2C%20we%0Apresent%20%5Ctextbf%7BDIVER%7D%2C%20a%20retrieval%20pipeline%20tailored%20for%20reasoning-intensive%0Ainformation%20retrieval.%20DIVER%20consists%20of%20four%20components%3A%20document%20processing%0Ato%20improve%20input%20quality%2C%20LLM-driven%20query%20expansion%20via%20iterative%20document%0Ainteraction%2C%20a%20reasoning-enhanced%20retriever%20fine-tuned%20on%20synthetic%0Amulti-domain%20data%20with%20hard%20negatives%2C%20and%20a%20pointwise%20reranker%20that%20combines%0ALLM-assigned%20helpfulness%20scores%20with%20retrieval%20scores.%20On%20the%20BRIGHT%20benchmark%2C%0ADIVER%20achieves%20state-of-the-art%20nDCG%4010%20scores%20of%2041.6%20and%2028.9%20on%20original%0Aqueries%2C%20consistently%20outperforming%20competitive%20reasoning-aware%20models.%20These%0Aresults%20demonstrate%20the%20effectiveness%20of%20reasoning-aware%20retrieval%20strategies%0Ain%20complex%20real-world%20tasks.%20Our%20code%20and%20retrieval%20model%20will%20be%20released%0Asoon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07995v1&entry.124074799=Read"},
{"title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "author": "Long Phan and Mantas Mazeika and Andy Zou and Dan Hendrycks", "abstract": "  Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.\n", "link": "http://arxiv.org/abs/2507.23701v2", "date": "2025-08-11", "relevancy": 2.0478, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F&body=Title%3A%20TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F%0AAuthor%3A%20Long%20Phan%20and%20Mantas%20Mazeika%20and%20Andy%20Zou%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20Evaluating%20AI%20agents%20within%20complex%2C%20interactive%20environments%20that%20mirror%0Areal-world%20challenges%20is%20critical%20for%20understanding%20their%20practical%0Acapabilities.%20While%20existing%20agent%20benchmarks%20effectively%20assess%20skills%20like%0Atool%20use%20or%20performance%20on%20structured%20tasks%2C%20they%20often%20do%20not%20fully%20capture%20an%0Aagent%27s%20ability%20to%20operate%20autonomously%20in%20exploratory%20environments%20that%20demand%0Asustained%2C%20self-directed%20reasoning%20over%20a%20long%20and%20growing%20context.%20To%20spur%20the%0Adevelopment%20of%20agents%20capable%20of%20more%20robust%20intrinsic%20reasoning%20over%20long%0Ahorizons%2C%20we%20introduce%20TextQuests%2C%20a%20benchmark%20based%20on%20the%20Infocom%20suite%20of%0Ainteractive%20fiction%20games.%20These%20text-based%20adventures%2C%20which%20can%20take%20human%0Aplayers%20over%2030%20hours%20and%20require%20hundreds%20of%20precise%20actions%20to%20solve%2C%20serve%0Aas%20an%20effective%20proxy%20for%20evaluating%20AI%20agents%20on%20focused%2C%20stateful%20tasks.%20The%0Abenchmark%20is%20specifically%20designed%20to%20assess%20an%20LLM%20agent%27s%20capacity%20for%0Aself-contained%20problem-solving%20by%20precluding%20the%20use%20of%20external%20tools%2C%20thereby%0Afocusing%20on%20intrinsic%20long-context%20reasoning%20capabilities%20in%20an%20exploratory%0Aenvironment%20characterized%20by%20the%20need%20for%20trial-and-error%20learning%20and%0Asustained%20problem-solving%20within%20a%20single%20interactive%20session.%20We%20release%0ATextQuests%20at%20https%3A//textquests.ai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextQuests%253A%2520How%2520Good%2520are%2520LLMs%2520at%2520Text-Based%2520Video%2520Games%253F%26entry.906535625%3DLong%2520Phan%2520and%2520Mantas%2520Mazeika%2520and%2520Andy%2520Zou%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520Evaluating%2520AI%2520agents%2520within%2520complex%252C%2520interactive%2520environments%2520that%2520mirror%250Areal-world%2520challenges%2520is%2520critical%2520for%2520understanding%2520their%2520practical%250Acapabilities.%2520While%2520existing%2520agent%2520benchmarks%2520effectively%2520assess%2520skills%2520like%250Atool%2520use%2520or%2520performance%2520on%2520structured%2520tasks%252C%2520they%2520often%2520do%2520not%2520fully%2520capture%2520an%250Aagent%2527s%2520ability%2520to%2520operate%2520autonomously%2520in%2520exploratory%2520environments%2520that%2520demand%250Asustained%252C%2520self-directed%2520reasoning%2520over%2520a%2520long%2520and%2520growing%2520context.%2520To%2520spur%2520the%250Adevelopment%2520of%2520agents%2520capable%2520of%2520more%2520robust%2520intrinsic%2520reasoning%2520over%2520long%250Ahorizons%252C%2520we%2520introduce%2520TextQuests%252C%2520a%2520benchmark%2520based%2520on%2520the%2520Infocom%2520suite%2520of%250Ainteractive%2520fiction%2520games.%2520These%2520text-based%2520adventures%252C%2520which%2520can%2520take%2520human%250Aplayers%2520over%252030%2520hours%2520and%2520require%2520hundreds%2520of%2520precise%2520actions%2520to%2520solve%252C%2520serve%250Aas%2520an%2520effective%2520proxy%2520for%2520evaluating%2520AI%2520agents%2520on%2520focused%252C%2520stateful%2520tasks.%2520The%250Abenchmark%2520is%2520specifically%2520designed%2520to%2520assess%2520an%2520LLM%2520agent%2527s%2520capacity%2520for%250Aself-contained%2520problem-solving%2520by%2520precluding%2520the%2520use%2520of%2520external%2520tools%252C%2520thereby%250Afocusing%2520on%2520intrinsic%2520long-context%2520reasoning%2520capabilities%2520in%2520an%2520exploratory%250Aenvironment%2520characterized%2520by%2520the%2520need%2520for%2520trial-and-error%2520learning%2520and%250Asustained%2520problem-solving%2520within%2520a%2520single%2520interactive%2520session.%2520We%2520release%250ATextQuests%2520at%2520https%253A//textquests.ai.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F&entry.906535625=Long%20Phan%20and%20Mantas%20Mazeika%20and%20Andy%20Zou%20and%20Dan%20Hendrycks&entry.1292438233=%20%20Evaluating%20AI%20agents%20within%20complex%2C%20interactive%20environments%20that%20mirror%0Areal-world%20challenges%20is%20critical%20for%20understanding%20their%20practical%0Acapabilities.%20While%20existing%20agent%20benchmarks%20effectively%20assess%20skills%20like%0Atool%20use%20or%20performance%20on%20structured%20tasks%2C%20they%20often%20do%20not%20fully%20capture%20an%0Aagent%27s%20ability%20to%20operate%20autonomously%20in%20exploratory%20environments%20that%20demand%0Asustained%2C%20self-directed%20reasoning%20over%20a%20long%20and%20growing%20context.%20To%20spur%20the%0Adevelopment%20of%20agents%20capable%20of%20more%20robust%20intrinsic%20reasoning%20over%20long%0Ahorizons%2C%20we%20introduce%20TextQuests%2C%20a%20benchmark%20based%20on%20the%20Infocom%20suite%20of%0Ainteractive%20fiction%20games.%20These%20text-based%20adventures%2C%20which%20can%20take%20human%0Aplayers%20over%2030%20hours%20and%20require%20hundreds%20of%20precise%20actions%20to%20solve%2C%20serve%0Aas%20an%20effective%20proxy%20for%20evaluating%20AI%20agents%20on%20focused%2C%20stateful%20tasks.%20The%0Abenchmark%20is%20specifically%20designed%20to%20assess%20an%20LLM%20agent%27s%20capacity%20for%0Aself-contained%20problem-solving%20by%20precluding%20the%20use%20of%20external%20tools%2C%20thereby%0Afocusing%20on%20intrinsic%20long-context%20reasoning%20capabilities%20in%20an%20exploratory%0Aenvironment%20characterized%20by%20the%20need%20for%20trial-and-error%20learning%20and%0Asustained%20problem-solving%20within%20a%20single%20interactive%20session.%20We%20release%0ATextQuests%20at%20https%3A//textquests.ai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23701v2&entry.124074799=Read"},
{"title": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological\n  Disorder Detection", "author": "Guanghao Jin and Yuan Liang and Yihan Ma and Jingpei Wu and Guoyang Liu", "abstract": "  Large-scale models pre-trained on Electroencephalography (EEG) have shown\npromise in clinical applications such as neurological disorder detection.\nHowever, the practical deployment of EEG-based large-scale models faces\ncritical challenges such as limited labeled EEG data and suboptimal performance\nin clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel\nlarge-scale model specifically designed for detecting EEG-based neurological\ndisorders. Our key contributions include (i) a Selective Temporal-Frequency\nEmbedding mechanism that adaptively captures complex temporal and spectral\npatterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy\nthat refines feature representation in a two-stage process. In the first stage,\nour model learns the fundamental discriminative features of EEG activities; in\nthe second stage, the model further extracts more specialized fine-grained\nfeatures for accurate diagnostic performance. We evaluated NeuroDx-LM on the\nCHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in\nEEG-based seizure and schizophrenia detection, respectively. These results\ndemonstrate the great potential of EEG-based large-scale models to advance\nclinical applicability. Our code is available at\nhttps://github.com/LetItBe12345/NeuroDx-LM.\n", "link": "http://arxiv.org/abs/2508.08124v1", "date": "2025-08-11", "relevancy": 2.0474, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroDx-LM%3A%20A%20Clinical%20Large-Scale%20Model%20for%20EEG-based%20Neurological%0A%20%20Disorder%20Detection&body=Title%3A%20NeuroDx-LM%3A%20A%20Clinical%20Large-Scale%20Model%20for%20EEG-based%20Neurological%0A%20%20Disorder%20Detection%0AAuthor%3A%20Guanghao%20Jin%20and%20Yuan%20Liang%20and%20Yihan%20Ma%20and%20Jingpei%20Wu%20and%20Guoyang%20Liu%0AAbstract%3A%20%20%20Large-scale%20models%20pre-trained%20on%20Electroencephalography%20%28EEG%29%20have%20shown%0Apromise%20in%20clinical%20applications%20such%20as%20neurological%20disorder%20detection.%0AHowever%2C%20the%20practical%20deployment%20of%20EEG-based%20large-scale%20models%20faces%0Acritical%20challenges%20such%20as%20limited%20labeled%20EEG%20data%20and%20suboptimal%20performance%0Ain%20clinical%20scenarios.%20To%20address%20these%20issues%2C%20we%20propose%20NeuroDx-LM%2C%20a%20novel%0Alarge-scale%20model%20specifically%20designed%20for%20detecting%20EEG-based%20neurological%0Adisorders.%20Our%20key%20contributions%20include%20%28i%29%20a%20Selective%20Temporal-Frequency%0AEmbedding%20mechanism%20that%20adaptively%20captures%20complex%20temporal%20and%20spectral%0Apatterns%20in%20EEG%20signals%3B%20and%20%28ii%29%20a%20Progressive%20Feature-Aware%20Training%20strategy%0Athat%20refines%20feature%20representation%20in%20a%20two-stage%20process.%20In%20the%20first%20stage%2C%0Aour%20model%20learns%20the%20fundamental%20discriminative%20features%20of%20EEG%20activities%3B%20in%0Athe%20second%20stage%2C%20the%20model%20further%20extracts%20more%20specialized%20fine-grained%0Afeatures%20for%20accurate%20diagnostic%20performance.%20We%20evaluated%20NeuroDx-LM%20on%20the%0ACHB-MIT%20and%20Schizophrenia%20datasets%2C%20achieving%20state-of-the-art%20performance%20in%0AEEG-based%20seizure%20and%20schizophrenia%20detection%2C%20respectively.%20These%20results%0Ademonstrate%20the%20great%20potential%20of%20EEG-based%20large-scale%20models%20to%20advance%0Aclinical%20applicability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LetItBe12345/NeuroDx-LM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroDx-LM%253A%2520A%2520Clinical%2520Large-Scale%2520Model%2520for%2520EEG-based%2520Neurological%250A%2520%2520Disorder%2520Detection%26entry.906535625%3DGuanghao%2520Jin%2520and%2520Yuan%2520Liang%2520and%2520Yihan%2520Ma%2520and%2520Jingpei%2520Wu%2520and%2520Guoyang%2520Liu%26entry.1292438233%3D%2520%2520Large-scale%2520models%2520pre-trained%2520on%2520Electroencephalography%2520%2528EEG%2529%2520have%2520shown%250Apromise%2520in%2520clinical%2520applications%2520such%2520as%2520neurological%2520disorder%2520detection.%250AHowever%252C%2520the%2520practical%2520deployment%2520of%2520EEG-based%2520large-scale%2520models%2520faces%250Acritical%2520challenges%2520such%2520as%2520limited%2520labeled%2520EEG%2520data%2520and%2520suboptimal%2520performance%250Ain%2520clinical%2520scenarios.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520NeuroDx-LM%252C%2520a%2520novel%250Alarge-scale%2520model%2520specifically%2520designed%2520for%2520detecting%2520EEG-based%2520neurological%250Adisorders.%2520Our%2520key%2520contributions%2520include%2520%2528i%2529%2520a%2520Selective%2520Temporal-Frequency%250AEmbedding%2520mechanism%2520that%2520adaptively%2520captures%2520complex%2520temporal%2520and%2520spectral%250Apatterns%2520in%2520EEG%2520signals%253B%2520and%2520%2528ii%2529%2520a%2520Progressive%2520Feature-Aware%2520Training%2520strategy%250Athat%2520refines%2520feature%2520representation%2520in%2520a%2520two-stage%2520process.%2520In%2520the%2520first%2520stage%252C%250Aour%2520model%2520learns%2520the%2520fundamental%2520discriminative%2520features%2520of%2520EEG%2520activities%253B%2520in%250Athe%2520second%2520stage%252C%2520the%2520model%2520further%2520extracts%2520more%2520specialized%2520fine-grained%250Afeatures%2520for%2520accurate%2520diagnostic%2520performance.%2520We%2520evaluated%2520NeuroDx-LM%2520on%2520the%250ACHB-MIT%2520and%2520Schizophrenia%2520datasets%252C%2520achieving%2520state-of-the-art%2520performance%2520in%250AEEG-based%2520seizure%2520and%2520schizophrenia%2520detection%252C%2520respectively.%2520These%2520results%250Ademonstrate%2520the%2520great%2520potential%2520of%2520EEG-based%2520large-scale%2520models%2520to%2520advance%250Aclinical%2520applicability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LetItBe12345/NeuroDx-LM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroDx-LM%3A%20A%20Clinical%20Large-Scale%20Model%20for%20EEG-based%20Neurological%0A%20%20Disorder%20Detection&entry.906535625=Guanghao%20Jin%20and%20Yuan%20Liang%20and%20Yihan%20Ma%20and%20Jingpei%20Wu%20and%20Guoyang%20Liu&entry.1292438233=%20%20Large-scale%20models%20pre-trained%20on%20Electroencephalography%20%28EEG%29%20have%20shown%0Apromise%20in%20clinical%20applications%20such%20as%20neurological%20disorder%20detection.%0AHowever%2C%20the%20practical%20deployment%20of%20EEG-based%20large-scale%20models%20faces%0Acritical%20challenges%20such%20as%20limited%20labeled%20EEG%20data%20and%20suboptimal%20performance%0Ain%20clinical%20scenarios.%20To%20address%20these%20issues%2C%20we%20propose%20NeuroDx-LM%2C%20a%20novel%0Alarge-scale%20model%20specifically%20designed%20for%20detecting%20EEG-based%20neurological%0Adisorders.%20Our%20key%20contributions%20include%20%28i%29%20a%20Selective%20Temporal-Frequency%0AEmbedding%20mechanism%20that%20adaptively%20captures%20complex%20temporal%20and%20spectral%0Apatterns%20in%20EEG%20signals%3B%20and%20%28ii%29%20a%20Progressive%20Feature-Aware%20Training%20strategy%0Athat%20refines%20feature%20representation%20in%20a%20two-stage%20process.%20In%20the%20first%20stage%2C%0Aour%20model%20learns%20the%20fundamental%20discriminative%20features%20of%20EEG%20activities%3B%20in%0Athe%20second%20stage%2C%20the%20model%20further%20extracts%20more%20specialized%20fine-grained%0Afeatures%20for%20accurate%20diagnostic%20performance.%20We%20evaluated%20NeuroDx-LM%20on%20the%0ACHB-MIT%20and%20Schizophrenia%20datasets%2C%20achieving%20state-of-the-art%20performance%20in%0AEEG-based%20seizure%20and%20schizophrenia%20detection%2C%20respectively.%20These%20results%0Ademonstrate%20the%20great%20potential%20of%20EEG-based%20large-scale%20models%20to%20advance%0Aclinical%20applicability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/LetItBe12345/NeuroDx-LM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08124v1&entry.124074799=Read"},
{"title": "RedDino: A foundation model for red blood cell analysis", "author": "Luca Zedda and Andrea Loddo and Cecilia Di Ruberto and Carsten Marr", "abstract": "  Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc\n", "link": "http://arxiv.org/abs/2508.08180v1", "date": "2025-08-11", "relevancy": 2.0466, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RedDino%3A%20A%20foundation%20model%20for%20red%20blood%20cell%20analysis&body=Title%3A%20RedDino%3A%20A%20foundation%20model%20for%20red%20blood%20cell%20analysis%0AAuthor%3A%20Luca%20Zedda%20and%20Andrea%20Loddo%20and%20Cecilia%20Di%20Ruberto%20and%20Carsten%20Marr%0AAbstract%3A%20%20%20Red%20blood%20cells%20%28RBCs%29%20are%20essential%20to%20human%20health%2C%20and%20their%20precise%0Amorphological%20analysis%20is%20important%20for%20diagnosing%20hematological%20disorders.%0ADespite%20the%20promise%20of%20foundation%20models%20in%20medical%20diagnostics%2C%20comprehensive%0AAI%20solutions%20for%20RBC%20analysis%20remain%20scarce.%20We%20present%20RedDino%2C%20a%0Aself-supervised%20foundation%20model%20designed%20for%20RBC%20image%20analysis.%20RedDino%20uses%0Aan%20RBC-specific%20adaptation%20of%20the%20DINOv2%20self-supervised%20learning%20framework%20and%0Ais%20trained%20on%20a%20curated%20dataset%20of%201.25%20million%20RBC%20images%20from%20diverse%0Aacquisition%20modalities%20and%20sources.%20Extensive%20evaluations%20show%20that%20RedDino%0Aoutperforms%20existing%20state-of-the-art%20models%20on%20RBC%20shape%20classification.%0AThrough%20assessments%20including%20linear%20probing%20and%20nearest%20neighbor%0Aclassification%2C%20we%20confirm%20its%20strong%20feature%20representations%20and%0Ageneralization%20ability.%20Our%20main%20contributions%20are%3A%20%281%29%20a%20foundation%20model%0Atailored%20for%20RBC%20analysis%2C%20%282%29%20ablation%20studies%20exploring%20DINOv2%20configurations%0Afor%20RBC%20modeling%2C%20and%20%283%29%20a%20detailed%20evaluation%20of%20generalization%20performance.%0ARedDino%20addresses%20key%20challenges%20in%20computational%20hematology%20by%20capturing%0Anuanced%20morphological%20features%2C%20advancing%20the%20development%20of%20reliable%0Adiagnostic%20tools.%20The%20source%20code%20and%20pretrained%20models%20for%20RedDino%20are%0Aavailable%20at%20https%3A//github.com/Snarci/RedDino%2C%20and%20the%20pretrained%20models%20can%0Abe%20downloaded%20from%20our%20Hugging%20Face%20collection%20at%0Ahttps%3A//huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedDino%253A%2520A%2520foundation%2520model%2520for%2520red%2520blood%2520cell%2520analysis%26entry.906535625%3DLuca%2520Zedda%2520and%2520Andrea%2520Loddo%2520and%2520Cecilia%2520Di%2520Ruberto%2520and%2520Carsten%2520Marr%26entry.1292438233%3D%2520%2520Red%2520blood%2520cells%2520%2528RBCs%2529%2520are%2520essential%2520to%2520human%2520health%252C%2520and%2520their%2520precise%250Amorphological%2520analysis%2520is%2520important%2520for%2520diagnosing%2520hematological%2520disorders.%250ADespite%2520the%2520promise%2520of%2520foundation%2520models%2520in%2520medical%2520diagnostics%252C%2520comprehensive%250AAI%2520solutions%2520for%2520RBC%2520analysis%2520remain%2520scarce.%2520We%2520present%2520RedDino%252C%2520a%250Aself-supervised%2520foundation%2520model%2520designed%2520for%2520RBC%2520image%2520analysis.%2520RedDino%2520uses%250Aan%2520RBC-specific%2520adaptation%2520of%2520the%2520DINOv2%2520self-supervised%2520learning%2520framework%2520and%250Ais%2520trained%2520on%2520a%2520curated%2520dataset%2520of%25201.25%2520million%2520RBC%2520images%2520from%2520diverse%250Aacquisition%2520modalities%2520and%2520sources.%2520Extensive%2520evaluations%2520show%2520that%2520RedDino%250Aoutperforms%2520existing%2520state-of-the-art%2520models%2520on%2520RBC%2520shape%2520classification.%250AThrough%2520assessments%2520including%2520linear%2520probing%2520and%2520nearest%2520neighbor%250Aclassification%252C%2520we%2520confirm%2520its%2520strong%2520feature%2520representations%2520and%250Ageneralization%2520ability.%2520Our%2520main%2520contributions%2520are%253A%2520%25281%2529%2520a%2520foundation%2520model%250Atailored%2520for%2520RBC%2520analysis%252C%2520%25282%2529%2520ablation%2520studies%2520exploring%2520DINOv2%2520configurations%250Afor%2520RBC%2520modeling%252C%2520and%2520%25283%2529%2520a%2520detailed%2520evaluation%2520of%2520generalization%2520performance.%250ARedDino%2520addresses%2520key%2520challenges%2520in%2520computational%2520hematology%2520by%2520capturing%250Anuanced%2520morphological%2520features%252C%2520advancing%2520the%2520development%2520of%2520reliable%250Adiagnostic%2520tools.%2520The%2520source%2520code%2520and%2520pretrained%2520models%2520for%2520RedDino%2520are%250Aavailable%2520at%2520https%253A//github.com/Snarci/RedDino%252C%2520and%2520the%2520pretrained%2520models%2520can%250Abe%2520downloaded%2520from%2520our%2520Hugging%2520Face%2520collection%2520at%250Ahttps%253A//huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RedDino%3A%20A%20foundation%20model%20for%20red%20blood%20cell%20analysis&entry.906535625=Luca%20Zedda%20and%20Andrea%20Loddo%20and%20Cecilia%20Di%20Ruberto%20and%20Carsten%20Marr&entry.1292438233=%20%20Red%20blood%20cells%20%28RBCs%29%20are%20essential%20to%20human%20health%2C%20and%20their%20precise%0Amorphological%20analysis%20is%20important%20for%20diagnosing%20hematological%20disorders.%0ADespite%20the%20promise%20of%20foundation%20models%20in%20medical%20diagnostics%2C%20comprehensive%0AAI%20solutions%20for%20RBC%20analysis%20remain%20scarce.%20We%20present%20RedDino%2C%20a%0Aself-supervised%20foundation%20model%20designed%20for%20RBC%20image%20analysis.%20RedDino%20uses%0Aan%20RBC-specific%20adaptation%20of%20the%20DINOv2%20self-supervised%20learning%20framework%20and%0Ais%20trained%20on%20a%20curated%20dataset%20of%201.25%20million%20RBC%20images%20from%20diverse%0Aacquisition%20modalities%20and%20sources.%20Extensive%20evaluations%20show%20that%20RedDino%0Aoutperforms%20existing%20state-of-the-art%20models%20on%20RBC%20shape%20classification.%0AThrough%20assessments%20including%20linear%20probing%20and%20nearest%20neighbor%0Aclassification%2C%20we%20confirm%20its%20strong%20feature%20representations%20and%0Ageneralization%20ability.%20Our%20main%20contributions%20are%3A%20%281%29%20a%20foundation%20model%0Atailored%20for%20RBC%20analysis%2C%20%282%29%20ablation%20studies%20exploring%20DINOv2%20configurations%0Afor%20RBC%20modeling%2C%20and%20%283%29%20a%20detailed%20evaluation%20of%20generalization%20performance.%0ARedDino%20addresses%20key%20challenges%20in%20computational%20hematology%20by%20capturing%0Anuanced%20morphological%20features%2C%20advancing%20the%20development%20of%20reliable%0Adiagnostic%20tools.%20The%20source%20code%20and%20pretrained%20models%20for%20RedDino%20are%0Aavailable%20at%20https%3A//github.com/Snarci/RedDino%2C%20and%20the%20pretrained%20models%20can%0Abe%20downloaded%20from%20our%20Hugging%20Face%20collection%20at%0Ahttps%3A//huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08180v1&entry.124074799=Read"},
{"title": "Fitting Description Logic Ontologies to ABox and Query Examples", "author": "Maurice Funk and Marvin Grosser and Carsten Lutz", "abstract": "  We study a fitting problem inspired by ontology-mediated querying: given a\ncollection\n  of positive and negative examples of\n  the form $(\\mathcal{A},q)$ with\n  $\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek\n  an ontology $\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash\nq$ for all positive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for\nall negative examples.\n  We consider the description logics $\\mathcal{ALC}$ and $\\mathcal{ALCI}$ as\nontology languages and\n  a range of query languages that\n  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof\n(UCQs).\n  For all of the resulting fitting problems,\n  we provide\n  effective characterizations and determine the computational complexity\n  of deciding whether a fitting ontology exists. This problem turns out to be\n${\\small CO}NP$ for AQs and full CQs\n  and $2E{\\small XP}T{\\small IME}$-complete for CQs and UCQs.\n  These results hold for both $\\mathcal{ALC}$ and $\\mathcal{ALCI}$.\n", "link": "http://arxiv.org/abs/2508.08007v1", "date": "2025-08-11", "relevancy": 2.0401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4118}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fitting%20Description%20Logic%20Ontologies%20to%20ABox%20and%20Query%20Examples&body=Title%3A%20Fitting%20Description%20Logic%20Ontologies%20to%20ABox%20and%20Query%20Examples%0AAuthor%3A%20Maurice%20Funk%20and%20Marvin%20Grosser%20and%20Carsten%20Lutz%0AAbstract%3A%20%20%20We%20study%20a%20fitting%20problem%20inspired%20by%20ontology-mediated%20querying%3A%20given%20a%0Acollection%0A%20%20of%20positive%20and%20negative%20examples%20of%0A%20%20the%20form%20%24%28%5Cmathcal%7BA%7D%2Cq%29%24%20with%0A%20%20%24%5Cmathcal%7BA%7D%24%20an%20ABox%20and%20%24q%24%20a%20Boolean%20query%2C%20we%20seek%0A%20%20an%20ontology%20%24%5Cmathcal%7BO%7D%24%20that%20satisfies%20%24%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BO%7D%20%5CvDash%0Aq%24%20for%20all%20positive%20examples%20and%20%24%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BO%7D%5Cnot%5CvDash%20q%24%20for%0Aall%20negative%20examples.%0A%20%20We%20consider%20the%20description%20logics%20%24%5Cmathcal%7BALC%7D%24%20and%20%24%5Cmathcal%7BALCI%7D%24%20as%0Aontology%20languages%20and%0A%20%20a%20range%20of%20query%20languages%20that%0A%20%20includes%20atomic%20queries%20%28AQs%29%2C%20conjunctive%20queries%20%28CQs%29%2C%20and%20unions%20thereof%0A%28UCQs%29.%0A%20%20For%20all%20of%20the%20resulting%20fitting%20problems%2C%0A%20%20we%20provide%0A%20%20effective%20characterizations%20and%20determine%20the%20computational%20complexity%0A%20%20of%20deciding%20whether%20a%20fitting%20ontology%20exists.%20This%20problem%20turns%20out%20to%20be%0A%24%7B%5Csmall%20CO%7DNP%24%20for%20AQs%20and%20full%20CQs%0A%20%20and%20%242E%7B%5Csmall%20XP%7DT%7B%5Csmall%20IME%7D%24-complete%20for%20CQs%20and%20UCQs.%0A%20%20These%20results%20hold%20for%20both%20%24%5Cmathcal%7BALC%7D%24%20and%20%24%5Cmathcal%7BALCI%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFitting%2520Description%2520Logic%2520Ontologies%2520to%2520ABox%2520and%2520Query%2520Examples%26entry.906535625%3DMaurice%2520Funk%2520and%2520Marvin%2520Grosser%2520and%2520Carsten%2520Lutz%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520fitting%2520problem%2520inspired%2520by%2520ontology-mediated%2520querying%253A%2520given%2520a%250Acollection%250A%2520%2520of%2520positive%2520and%2520negative%2520examples%2520of%250A%2520%2520the%2520form%2520%2524%2528%255Cmathcal%257BA%257D%252Cq%2529%2524%2520with%250A%2520%2520%2524%255Cmathcal%257BA%257D%2524%2520an%2520ABox%2520and%2520%2524q%2524%2520a%2520Boolean%2520query%252C%2520we%2520seek%250A%2520%2520an%2520ontology%2520%2524%255Cmathcal%257BO%257D%2524%2520that%2520satisfies%2520%2524%255Cmathcal%257BA%257D%2520%255Ccup%2520%255Cmathcal%257BO%257D%2520%255CvDash%250Aq%2524%2520for%2520all%2520positive%2520examples%2520and%2520%2524%255Cmathcal%257BA%257D%2520%255Ccup%2520%255Cmathcal%257BO%257D%255Cnot%255CvDash%2520q%2524%2520for%250Aall%2520negative%2520examples.%250A%2520%2520We%2520consider%2520the%2520description%2520logics%2520%2524%255Cmathcal%257BALC%257D%2524%2520and%2520%2524%255Cmathcal%257BALCI%257D%2524%2520as%250Aontology%2520languages%2520and%250A%2520%2520a%2520range%2520of%2520query%2520languages%2520that%250A%2520%2520includes%2520atomic%2520queries%2520%2528AQs%2529%252C%2520conjunctive%2520queries%2520%2528CQs%2529%252C%2520and%2520unions%2520thereof%250A%2528UCQs%2529.%250A%2520%2520For%2520all%2520of%2520the%2520resulting%2520fitting%2520problems%252C%250A%2520%2520we%2520provide%250A%2520%2520effective%2520characterizations%2520and%2520determine%2520the%2520computational%2520complexity%250A%2520%2520of%2520deciding%2520whether%2520a%2520fitting%2520ontology%2520exists.%2520This%2520problem%2520turns%2520out%2520to%2520be%250A%2524%257B%255Csmall%2520CO%257DNP%2524%2520for%2520AQs%2520and%2520full%2520CQs%250A%2520%2520and%2520%25242E%257B%255Csmall%2520XP%257DT%257B%255Csmall%2520IME%257D%2524-complete%2520for%2520CQs%2520and%2520UCQs.%250A%2520%2520These%2520results%2520hold%2520for%2520both%2520%2524%255Cmathcal%257BALC%257D%2524%2520and%2520%2524%255Cmathcal%257BALCI%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fitting%20Description%20Logic%20Ontologies%20to%20ABox%20and%20Query%20Examples&entry.906535625=Maurice%20Funk%20and%20Marvin%20Grosser%20and%20Carsten%20Lutz&entry.1292438233=%20%20We%20study%20a%20fitting%20problem%20inspired%20by%20ontology-mediated%20querying%3A%20given%20a%0Acollection%0A%20%20of%20positive%20and%20negative%20examples%20of%0A%20%20the%20form%20%24%28%5Cmathcal%7BA%7D%2Cq%29%24%20with%0A%20%20%24%5Cmathcal%7BA%7D%24%20an%20ABox%20and%20%24q%24%20a%20Boolean%20query%2C%20we%20seek%0A%20%20an%20ontology%20%24%5Cmathcal%7BO%7D%24%20that%20satisfies%20%24%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BO%7D%20%5CvDash%0Aq%24%20for%20all%20positive%20examples%20and%20%24%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BO%7D%5Cnot%5CvDash%20q%24%20for%0Aall%20negative%20examples.%0A%20%20We%20consider%20the%20description%20logics%20%24%5Cmathcal%7BALC%7D%24%20and%20%24%5Cmathcal%7BALCI%7D%24%20as%0Aontology%20languages%20and%0A%20%20a%20range%20of%20query%20languages%20that%0A%20%20includes%20atomic%20queries%20%28AQs%29%2C%20conjunctive%20queries%20%28CQs%29%2C%20and%20unions%20thereof%0A%28UCQs%29.%0A%20%20For%20all%20of%20the%20resulting%20fitting%20problems%2C%0A%20%20we%20provide%0A%20%20effective%20characterizations%20and%20determine%20the%20computational%20complexity%0A%20%20of%20deciding%20whether%20a%20fitting%20ontology%20exists.%20This%20problem%20turns%20out%20to%20be%0A%24%7B%5Csmall%20CO%7DNP%24%20for%20AQs%20and%20full%20CQs%0A%20%20and%20%242E%7B%5Csmall%20XP%7DT%7B%5Csmall%20IME%7D%24-complete%20for%20CQs%20and%20UCQs.%0A%20%20These%20results%20hold%20for%20both%20%24%5Cmathcal%7BALC%7D%24%20and%20%24%5Cmathcal%7BALCI%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08007v1&entry.124074799=Read"},
{"title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL", "author": "Jiaxuan Gao and Wei Fu and Minyang Xie and Shusheng Xu and Chuyi He and Zhiyu Mei and Banghua Zhu and Yi Wu", "abstract": "  Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.\n", "link": "http://arxiv.org/abs/2508.07976v1", "date": "2025-08-11", "relevancy": 2.0229, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5576}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5044}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Ten%20Turns%3A%20Unlocking%20Long-Horizon%20Agentic%20Search%20with%20Large-Scale%0A%20%20Asynchronous%20RL&body=Title%3A%20Beyond%20Ten%20Turns%3A%20Unlocking%20Long-Horizon%20Agentic%20Search%20with%20Large-Scale%0A%20%20Asynchronous%20RL%0AAuthor%3A%20Jiaxuan%20Gao%20and%20Wei%20Fu%20and%20Minyang%20Xie%20and%20Shusheng%20Xu%20and%20Chuyi%20He%20and%20Zhiyu%20Mei%20and%20Banghua%20Zhu%20and%20Yi%20Wu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20LLM-based%20agents%20have%20demonstrated%20remarkable%0Acapabilities%20in%20handling%20complex%2C%20knowledge-intensive%20tasks%20by%20integrating%0Aexternal%20tools.%20Among%20diverse%20choices%20of%20tools%2C%20search%20tools%20play%20a%20pivotal%0Arole%20in%20accessing%20vast%20external%20knowledge.%20However%2C%20open-source%20agents%20still%0Afall%20short%20of%20achieving%20expert-level%20Search%20Intelligence%2C%20the%20ability%20to%0Aresolve%20ambiguous%20queries%2C%20generate%20precise%20searches%2C%20analyze%20results%2C%20and%0Aconduct%20thorough%20exploration.%20Existing%20approaches%20fall%20short%20in%20scalability%2C%0Aefficiency%2C%20and%20data%20quality.%20For%20example%2C%20small%20turn%20limits%20in%20existing%20online%0ARL%20methods%2C%20e.g.%20%3C%3D10%2C%20restrict%20complex%20strategy%20learning.%20This%20paper%0Aintroduces%20ASearcher%2C%20an%20open-source%20project%20for%20large-scale%20RL%20training%20of%0Asearch%20agents.%20Our%20key%20contributions%20include%3A%20%281%29%20Scalable%20fully%20asynchronous%0ARL%20training%20that%20enables%20long-horizon%20search%20while%20maintaining%20high%20training%0Aefficiency.%20%282%29%20A%20prompt-based%20LLM%20agent%20that%20autonomously%20synthesizes%0Ahigh-quality%20and%20challenging%20QAs%2C%20creating%20a%20large-scale%20QA%20dataset.%20Through%20RL%0Atraining%2C%20our%20prompt-based%20QwQ-32B%20agent%20achieves%20substantial%20improvements%2C%0Awith%2046.7%25%20and%2020.8%25%20Avg%404%20gains%20on%20xBench%20and%20GAIA%2C%20respectively.%20Notably%2C%20our%0Aagent%20exhibits%20extreme%20long-horizon%20search%2C%20with%20tool%20calls%20exceeding%2040%20turns%0Aand%20output%20tokens%20exceeding%20150k%20during%20training%20time.%20With%20a%20simple%20agent%0Adesign%20and%20no%20external%20LLMs%2C%20ASearcher-Web-QwQ%20achieves%20Avg%404%20scores%20of%2042.1%20on%0AxBench%20and%2052.8%20on%20GAIA%2C%20surpassing%20existing%20open-source%2032B%20agents.%20We%0Aopen-source%20our%20models%2C%20training%20data%2C%20and%20codes%20in%0Ahttps%3A//github.com/inclusionAI/ASearcher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Ten%2520Turns%253A%2520Unlocking%2520Long-Horizon%2520Agentic%2520Search%2520with%2520Large-Scale%250A%2520%2520Asynchronous%2520RL%26entry.906535625%3DJiaxuan%2520Gao%2520and%2520Wei%2520Fu%2520and%2520Minyang%2520Xie%2520and%2520Shusheng%2520Xu%2520and%2520Chuyi%2520He%2520and%2520Zhiyu%2520Mei%2520and%2520Banghua%2520Zhu%2520and%2520Yi%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520LLM-based%2520agents%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520handling%2520complex%252C%2520knowledge-intensive%2520tasks%2520by%2520integrating%250Aexternal%2520tools.%2520Among%2520diverse%2520choices%2520of%2520tools%252C%2520search%2520tools%2520play%2520a%2520pivotal%250Arole%2520in%2520accessing%2520vast%2520external%2520knowledge.%2520However%252C%2520open-source%2520agents%2520still%250Afall%2520short%2520of%2520achieving%2520expert-level%2520Search%2520Intelligence%252C%2520the%2520ability%2520to%250Aresolve%2520ambiguous%2520queries%252C%2520generate%2520precise%2520searches%252C%2520analyze%2520results%252C%2520and%250Aconduct%2520thorough%2520exploration.%2520Existing%2520approaches%2520fall%2520short%2520in%2520scalability%252C%250Aefficiency%252C%2520and%2520data%2520quality.%2520For%2520example%252C%2520small%2520turn%2520limits%2520in%2520existing%2520online%250ARL%2520methods%252C%2520e.g.%2520%253C%253D10%252C%2520restrict%2520complex%2520strategy%2520learning.%2520This%2520paper%250Aintroduces%2520ASearcher%252C%2520an%2520open-source%2520project%2520for%2520large-scale%2520RL%2520training%2520of%250Asearch%2520agents.%2520Our%2520key%2520contributions%2520include%253A%2520%25281%2529%2520Scalable%2520fully%2520asynchronous%250ARL%2520training%2520that%2520enables%2520long-horizon%2520search%2520while%2520maintaining%2520high%2520training%250Aefficiency.%2520%25282%2529%2520A%2520prompt-based%2520LLM%2520agent%2520that%2520autonomously%2520synthesizes%250Ahigh-quality%2520and%2520challenging%2520QAs%252C%2520creating%2520a%2520large-scale%2520QA%2520dataset.%2520Through%2520RL%250Atraining%252C%2520our%2520prompt-based%2520QwQ-32B%2520agent%2520achieves%2520substantial%2520improvements%252C%250Awith%252046.7%2525%2520and%252020.8%2525%2520Avg%25404%2520gains%2520on%2520xBench%2520and%2520GAIA%252C%2520respectively.%2520Notably%252C%2520our%250Aagent%2520exhibits%2520extreme%2520long-horizon%2520search%252C%2520with%2520tool%2520calls%2520exceeding%252040%2520turns%250Aand%2520output%2520tokens%2520exceeding%2520150k%2520during%2520training%2520time.%2520With%2520a%2520simple%2520agent%250Adesign%2520and%2520no%2520external%2520LLMs%252C%2520ASearcher-Web-QwQ%2520achieves%2520Avg%25404%2520scores%2520of%252042.1%2520on%250AxBench%2520and%252052.8%2520on%2520GAIA%252C%2520surpassing%2520existing%2520open-source%252032B%2520agents.%2520We%250Aopen-source%2520our%2520models%252C%2520training%2520data%252C%2520and%2520codes%2520in%250Ahttps%253A//github.com/inclusionAI/ASearcher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Ten%20Turns%3A%20Unlocking%20Long-Horizon%20Agentic%20Search%20with%20Large-Scale%0A%20%20Asynchronous%20RL&entry.906535625=Jiaxuan%20Gao%20and%20Wei%20Fu%20and%20Minyang%20Xie%20and%20Shusheng%20Xu%20and%20Chuyi%20He%20and%20Zhiyu%20Mei%20and%20Banghua%20Zhu%20and%20Yi%20Wu&entry.1292438233=%20%20Recent%20advancements%20in%20LLM-based%20agents%20have%20demonstrated%20remarkable%0Acapabilities%20in%20handling%20complex%2C%20knowledge-intensive%20tasks%20by%20integrating%0Aexternal%20tools.%20Among%20diverse%20choices%20of%20tools%2C%20search%20tools%20play%20a%20pivotal%0Arole%20in%20accessing%20vast%20external%20knowledge.%20However%2C%20open-source%20agents%20still%0Afall%20short%20of%20achieving%20expert-level%20Search%20Intelligence%2C%20the%20ability%20to%0Aresolve%20ambiguous%20queries%2C%20generate%20precise%20searches%2C%20analyze%20results%2C%20and%0Aconduct%20thorough%20exploration.%20Existing%20approaches%20fall%20short%20in%20scalability%2C%0Aefficiency%2C%20and%20data%20quality.%20For%20example%2C%20small%20turn%20limits%20in%20existing%20online%0ARL%20methods%2C%20e.g.%20%3C%3D10%2C%20restrict%20complex%20strategy%20learning.%20This%20paper%0Aintroduces%20ASearcher%2C%20an%20open-source%20project%20for%20large-scale%20RL%20training%20of%0Asearch%20agents.%20Our%20key%20contributions%20include%3A%20%281%29%20Scalable%20fully%20asynchronous%0ARL%20training%20that%20enables%20long-horizon%20search%20while%20maintaining%20high%20training%0Aefficiency.%20%282%29%20A%20prompt-based%20LLM%20agent%20that%20autonomously%20synthesizes%0Ahigh-quality%20and%20challenging%20QAs%2C%20creating%20a%20large-scale%20QA%20dataset.%20Through%20RL%0Atraining%2C%20our%20prompt-based%20QwQ-32B%20agent%20achieves%20substantial%20improvements%2C%0Awith%2046.7%25%20and%2020.8%25%20Avg%404%20gains%20on%20xBench%20and%20GAIA%2C%20respectively.%20Notably%2C%20our%0Aagent%20exhibits%20extreme%20long-horizon%20search%2C%20with%20tool%20calls%20exceeding%2040%20turns%0Aand%20output%20tokens%20exceeding%20150k%20during%20training%20time.%20With%20a%20simple%20agent%0Adesign%20and%20no%20external%20LLMs%2C%20ASearcher-Web-QwQ%20achieves%20Avg%404%20scores%20of%2042.1%20on%0AxBench%20and%2052.8%20on%20GAIA%2C%20surpassing%20existing%20open-source%2032B%20agents.%20We%0Aopen-source%20our%20models%2C%20training%20data%2C%20and%20codes%20in%0Ahttps%3A//github.com/inclusionAI/ASearcher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07976v1&entry.124074799=Read"},
{"title": "Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating\n  Discrimination from AI", "author": "Holli Sargeant and Mackenzie Jorgensen and Arina Shah and Adrian Weller and Umang Bhatt", "abstract": "  Uncertainty in artificial intelligence (AI) predictions poses urgent legal\nand ethical challenges for AI-assisted decision-making. We examine two\nalgorithmic interventions that act as guardrails for human-AI collaboration:\nselective abstention, which withholds high-uncertainty predictions from human\ndecision-makers, and selective friction, which delivers those predictions\ntogether with salient warnings or disclosures that slow the decision process.\nResearch has shown that selective abstention based on uncertainty can\ninadvertently exacerbate disparities and disadvantage under-represented groups\nthat disproportionately receive uncertain predictions. In this paper, we\nprovide the first integrated socio-technical and legal analysis of\nuncertainty-based algorithmic interventions. Through two case studies,\nAI-assisted consumer credit decisions and AI-assisted content moderation, we\ndemonstrate how the seemingly neutral use of uncertainty thresholds can trigger\ndiscriminatory impacts. We argue that, although both interventions pose risks\nof unlawful discrimination under UK law, selective frictions offer a promising\npathway toward fairer and more accountable AI-assisted decision-making by\npreserving transparency and encouraging more cautious human judgment.\n", "link": "http://arxiv.org/abs/2508.07872v1", "date": "2025-08-11", "relevancy": 1.4516, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5202}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4738}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unequal%20Uncertainty%3A%20Rethinking%20Algorithmic%20Interventions%20for%20Mitigating%0A%20%20Discrimination%20from%20AI&body=Title%3A%20Unequal%20Uncertainty%3A%20Rethinking%20Algorithmic%20Interventions%20for%20Mitigating%0A%20%20Discrimination%20from%20AI%0AAuthor%3A%20Holli%20Sargeant%20and%20Mackenzie%20Jorgensen%20and%20Arina%20Shah%20and%20Adrian%20Weller%20and%20Umang%20Bhatt%0AAbstract%3A%20%20%20Uncertainty%20in%20artificial%20intelligence%20%28AI%29%20predictions%20poses%20urgent%20legal%0Aand%20ethical%20challenges%20for%20AI-assisted%20decision-making.%20We%20examine%20two%0Aalgorithmic%20interventions%20that%20act%20as%20guardrails%20for%20human-AI%20collaboration%3A%0Aselective%20abstention%2C%20which%20withholds%20high-uncertainty%20predictions%20from%20human%0Adecision-makers%2C%20and%20selective%20friction%2C%20which%20delivers%20those%20predictions%0Atogether%20with%20salient%20warnings%20or%20disclosures%20that%20slow%20the%20decision%20process.%0AResearch%20has%20shown%20that%20selective%20abstention%20based%20on%20uncertainty%20can%0Ainadvertently%20exacerbate%20disparities%20and%20disadvantage%20under-represented%20groups%0Athat%20disproportionately%20receive%20uncertain%20predictions.%20In%20this%20paper%2C%20we%0Aprovide%20the%20first%20integrated%20socio-technical%20and%20legal%20analysis%20of%0Auncertainty-based%20algorithmic%20interventions.%20Through%20two%20case%20studies%2C%0AAI-assisted%20consumer%20credit%20decisions%20and%20AI-assisted%20content%20moderation%2C%20we%0Ademonstrate%20how%20the%20seemingly%20neutral%20use%20of%20uncertainty%20thresholds%20can%20trigger%0Adiscriminatory%20impacts.%20We%20argue%20that%2C%20although%20both%20interventions%20pose%20risks%0Aof%20unlawful%20discrimination%20under%20UK%20law%2C%20selective%20frictions%20offer%20a%20promising%0Apathway%20toward%20fairer%20and%20more%20accountable%20AI-assisted%20decision-making%20by%0Apreserving%20transparency%20and%20encouraging%20more%20cautious%20human%20judgment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnequal%2520Uncertainty%253A%2520Rethinking%2520Algorithmic%2520Interventions%2520for%2520Mitigating%250A%2520%2520Discrimination%2520from%2520AI%26entry.906535625%3DHolli%2520Sargeant%2520and%2520Mackenzie%2520Jorgensen%2520and%2520Arina%2520Shah%2520and%2520Adrian%2520Weller%2520and%2520Umang%2520Bhatt%26entry.1292438233%3D%2520%2520Uncertainty%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520predictions%2520poses%2520urgent%2520legal%250Aand%2520ethical%2520challenges%2520for%2520AI-assisted%2520decision-making.%2520We%2520examine%2520two%250Aalgorithmic%2520interventions%2520that%2520act%2520as%2520guardrails%2520for%2520human-AI%2520collaboration%253A%250Aselective%2520abstention%252C%2520which%2520withholds%2520high-uncertainty%2520predictions%2520from%2520human%250Adecision-makers%252C%2520and%2520selective%2520friction%252C%2520which%2520delivers%2520those%2520predictions%250Atogether%2520with%2520salient%2520warnings%2520or%2520disclosures%2520that%2520slow%2520the%2520decision%2520process.%250AResearch%2520has%2520shown%2520that%2520selective%2520abstention%2520based%2520on%2520uncertainty%2520can%250Ainadvertently%2520exacerbate%2520disparities%2520and%2520disadvantage%2520under-represented%2520groups%250Athat%2520disproportionately%2520receive%2520uncertain%2520predictions.%2520In%2520this%2520paper%252C%2520we%250Aprovide%2520the%2520first%2520integrated%2520socio-technical%2520and%2520legal%2520analysis%2520of%250Auncertainty-based%2520algorithmic%2520interventions.%2520Through%2520two%2520case%2520studies%252C%250AAI-assisted%2520consumer%2520credit%2520decisions%2520and%2520AI-assisted%2520content%2520moderation%252C%2520we%250Ademonstrate%2520how%2520the%2520seemingly%2520neutral%2520use%2520of%2520uncertainty%2520thresholds%2520can%2520trigger%250Adiscriminatory%2520impacts.%2520We%2520argue%2520that%252C%2520although%2520both%2520interventions%2520pose%2520risks%250Aof%2520unlawful%2520discrimination%2520under%2520UK%2520law%252C%2520selective%2520frictions%2520offer%2520a%2520promising%250Apathway%2520toward%2520fairer%2520and%2520more%2520accountable%2520AI-assisted%2520decision-making%2520by%250Apreserving%2520transparency%2520and%2520encouraging%2520more%2520cautious%2520human%2520judgment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unequal%20Uncertainty%3A%20Rethinking%20Algorithmic%20Interventions%20for%20Mitigating%0A%20%20Discrimination%20from%20AI&entry.906535625=Holli%20Sargeant%20and%20Mackenzie%20Jorgensen%20and%20Arina%20Shah%20and%20Adrian%20Weller%20and%20Umang%20Bhatt&entry.1292438233=%20%20Uncertainty%20in%20artificial%20intelligence%20%28AI%29%20predictions%20poses%20urgent%20legal%0Aand%20ethical%20challenges%20for%20AI-assisted%20decision-making.%20We%20examine%20two%0Aalgorithmic%20interventions%20that%20act%20as%20guardrails%20for%20human-AI%20collaboration%3A%0Aselective%20abstention%2C%20which%20withholds%20high-uncertainty%20predictions%20from%20human%0Adecision-makers%2C%20and%20selective%20friction%2C%20which%20delivers%20those%20predictions%0Atogether%20with%20salient%20warnings%20or%20disclosures%20that%20slow%20the%20decision%20process.%0AResearch%20has%20shown%20that%20selective%20abstention%20based%20on%20uncertainty%20can%0Ainadvertently%20exacerbate%20disparities%20and%20disadvantage%20under-represented%20groups%0Athat%20disproportionately%20receive%20uncertain%20predictions.%20In%20this%20paper%2C%20we%0Aprovide%20the%20first%20integrated%20socio-technical%20and%20legal%20analysis%20of%0Auncertainty-based%20algorithmic%20interventions.%20Through%20two%20case%20studies%2C%0AAI-assisted%20consumer%20credit%20decisions%20and%20AI-assisted%20content%20moderation%2C%20we%0Ademonstrate%20how%20the%20seemingly%20neutral%20use%20of%20uncertainty%20thresholds%20can%20trigger%0Adiscriminatory%20impacts.%20We%20argue%20that%2C%20although%20both%20interventions%20pose%20risks%0Aof%20unlawful%20discrimination%20under%20UK%20law%2C%20selective%20frictions%20offer%20a%20promising%0Apathway%20toward%20fairer%20and%20more%20accountable%20AI-assisted%20decision-making%20by%0Apreserving%20transparency%20and%20encouraging%20more%20cautious%20human%20judgment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07872v1&entry.124074799=Read"},
{"title": "Selective Contrastive Learning for Weakly Supervised Affordance\n  Grounding", "author": "WonJun Moon and Hyun Seok Seong and Jae-Pil Heo", "abstract": "  Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.\n", "link": "http://arxiv.org/abs/2508.07877v1", "date": "2025-08-11", "relevancy": 1.608, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Contrastive%20Learning%20for%20Weakly%20Supervised%20Affordance%0A%20%20Grounding&body=Title%3A%20Selective%20Contrastive%20Learning%20for%20Weakly%20Supervised%20Affordance%0A%20%20Grounding%0AAuthor%3A%20WonJun%20Moon%20and%20Hyun%20Seok%20Seong%20and%20Jae-Pil%20Heo%0AAbstract%3A%20%20%20Facilitating%20an%20entity%27s%20interaction%20with%20objects%20requires%20accurately%0Aidentifying%20parts%20that%20afford%20specific%20actions.%20Weakly%20supervised%20affordance%0Agrounding%20%28WSAG%29%20seeks%20to%20imitate%20human%20learning%20from%20third-person%0Ademonstrations%2C%20where%20humans%20intuitively%20grasp%20functional%20parts%20without%20needing%0Apixel-level%20annotations.%20To%20achieve%20this%2C%20grounding%20is%20typically%20learned%20using%0Aa%20shared%20classifier%20across%20images%20from%20different%20perspectives%2C%20along%20with%0Adistillation%20strategies%20incorporating%20part%20discovery%20process.%20However%2C%20since%0Aaffordance-relevant%20parts%20are%20not%20always%20easily%20distinguishable%2C%20models%0Aprimarily%20rely%20on%20classification%2C%20often%20focusing%20on%20common%20class-specific%0Apatterns%20that%20are%20unrelated%20to%20affordance.%20To%20address%20this%20limitation%2C%20we%20move%0Abeyond%20isolated%20part-level%20learning%20by%20introducing%20selective%20prototypical%20and%0Apixel%20contrastive%20objectives%20that%20adaptively%20learn%20affordance-relevant%20cues%20at%0Aboth%20the%20part%20and%20object%20levels%2C%20depending%20on%20the%20granularity%20of%20the%20available%0Ainformation.%20Initially%2C%20we%20find%20the%20action-associated%20objects%20in%20both%0Aegocentric%20%28object-focused%29%20and%20exocentric%20%28third-person%20example%29%20images%20by%0Aleveraging%20CLIP.%20Then%2C%20by%20cross-referencing%20the%20discovered%20objects%20of%0Acomplementary%20views%2C%20we%20excavate%20the%20precise%20part-level%20affordance%20clues%20in%0Aeach%20perspective.%20By%20consistently%20learning%20to%20distinguish%20affordance-relevant%0Aregions%20from%20affordance-irrelevant%20background%20context%2C%20our%20approach%20effectively%0Ashifts%20activation%20from%20irrelevant%20areas%20toward%20meaningful%20affordance%20cues.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Codes%20are%0Aavailable%20at%20github.com/hynnsk/SelectiveCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Contrastive%2520Learning%2520for%2520Weakly%2520Supervised%2520Affordance%250A%2520%2520Grounding%26entry.906535625%3DWonJun%2520Moon%2520and%2520Hyun%2520Seok%2520Seong%2520and%2520Jae-Pil%2520Heo%26entry.1292438233%3D%2520%2520Facilitating%2520an%2520entity%2527s%2520interaction%2520with%2520objects%2520requires%2520accurately%250Aidentifying%2520parts%2520that%2520afford%2520specific%2520actions.%2520Weakly%2520supervised%2520affordance%250Agrounding%2520%2528WSAG%2529%2520seeks%2520to%2520imitate%2520human%2520learning%2520from%2520third-person%250Ademonstrations%252C%2520where%2520humans%2520intuitively%2520grasp%2520functional%2520parts%2520without%2520needing%250Apixel-level%2520annotations.%2520To%2520achieve%2520this%252C%2520grounding%2520is%2520typically%2520learned%2520using%250Aa%2520shared%2520classifier%2520across%2520images%2520from%2520different%2520perspectives%252C%2520along%2520with%250Adistillation%2520strategies%2520incorporating%2520part%2520discovery%2520process.%2520However%252C%2520since%250Aaffordance-relevant%2520parts%2520are%2520not%2520always%2520easily%2520distinguishable%252C%2520models%250Aprimarily%2520rely%2520on%2520classification%252C%2520often%2520focusing%2520on%2520common%2520class-specific%250Apatterns%2520that%2520are%2520unrelated%2520to%2520affordance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520move%250Abeyond%2520isolated%2520part-level%2520learning%2520by%2520introducing%2520selective%2520prototypical%2520and%250Apixel%2520contrastive%2520objectives%2520that%2520adaptively%2520learn%2520affordance-relevant%2520cues%2520at%250Aboth%2520the%2520part%2520and%2520object%2520levels%252C%2520depending%2520on%2520the%2520granularity%2520of%2520the%2520available%250Ainformation.%2520Initially%252C%2520we%2520find%2520the%2520action-associated%2520objects%2520in%2520both%250Aegocentric%2520%2528object-focused%2529%2520and%2520exocentric%2520%2528third-person%2520example%2529%2520images%2520by%250Aleveraging%2520CLIP.%2520Then%252C%2520by%2520cross-referencing%2520the%2520discovered%2520objects%2520of%250Acomplementary%2520views%252C%2520we%2520excavate%2520the%2520precise%2520part-level%2520affordance%2520clues%2520in%250Aeach%2520perspective.%2520By%2520consistently%2520learning%2520to%2520distinguish%2520affordance-relevant%250Aregions%2520from%2520affordance-irrelevant%2520background%2520context%252C%2520our%2520approach%2520effectively%250Ashifts%2520activation%2520from%2520irrelevant%2520areas%2520toward%2520meaningful%2520affordance%2520cues.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Codes%2520are%250Aavailable%2520at%2520github.com/hynnsk/SelectiveCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Contrastive%20Learning%20for%20Weakly%20Supervised%20Affordance%0A%20%20Grounding&entry.906535625=WonJun%20Moon%20and%20Hyun%20Seok%20Seong%20and%20Jae-Pil%20Heo&entry.1292438233=%20%20Facilitating%20an%20entity%27s%20interaction%20with%20objects%20requires%20accurately%0Aidentifying%20parts%20that%20afford%20specific%20actions.%20Weakly%20supervised%20affordance%0Agrounding%20%28WSAG%29%20seeks%20to%20imitate%20human%20learning%20from%20third-person%0Ademonstrations%2C%20where%20humans%20intuitively%20grasp%20functional%20parts%20without%20needing%0Apixel-level%20annotations.%20To%20achieve%20this%2C%20grounding%20is%20typically%20learned%20using%0Aa%20shared%20classifier%20across%20images%20from%20different%20perspectives%2C%20along%20with%0Adistillation%20strategies%20incorporating%20part%20discovery%20process.%20However%2C%20since%0Aaffordance-relevant%20parts%20are%20not%20always%20easily%20distinguishable%2C%20models%0Aprimarily%20rely%20on%20classification%2C%20often%20focusing%20on%20common%20class-specific%0Apatterns%20that%20are%20unrelated%20to%20affordance.%20To%20address%20this%20limitation%2C%20we%20move%0Abeyond%20isolated%20part-level%20learning%20by%20introducing%20selective%20prototypical%20and%0Apixel%20contrastive%20objectives%20that%20adaptively%20learn%20affordance-relevant%20cues%20at%0Aboth%20the%20part%20and%20object%20levels%2C%20depending%20on%20the%20granularity%20of%20the%20available%0Ainformation.%20Initially%2C%20we%20find%20the%20action-associated%20objects%20in%20both%0Aegocentric%20%28object-focused%29%20and%20exocentric%20%28third-person%20example%29%20images%20by%0Aleveraging%20CLIP.%20Then%2C%20by%20cross-referencing%20the%20discovered%20objects%20of%0Acomplementary%20views%2C%20we%20excavate%20the%20precise%20part-level%20affordance%20clues%20in%0Aeach%20perspective.%20By%20consistently%20learning%20to%20distinguish%20affordance-relevant%0Aregions%20from%20affordance-irrelevant%20background%20context%2C%20our%20approach%20effectively%0Ashifts%20activation%20from%20irrelevant%20areas%20toward%20meaningful%20affordance%20cues.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20method.%20Codes%20are%0Aavailable%20at%20github.com/hynnsk/SelectiveCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07877v1&entry.124074799=Read"},
{"title": "Uniform Loss vs. Specialized Optimization: A Comparative Analysis in\n  Multi-Task Learning", "author": "Gabriel S. Gama and Valdir Grassi Jr", "abstract": "  Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task\nLearning by addressing issues like conflicting gradients and differing gradient\nnorms, which hinder equal-weighted task training. However, recent critiques\nsuggest that equally weighted tasks can achieve competitive results compared to\nSMTOs, arguing that previous SMTO results were influenced by poor\nhyperparameter optimization and lack of regularization. In this work, we\nevaluate these claims through an extensive empirical evaluation of SMTOs,\nincluding some of the latest methods, on more complex multi-task problems to\nclarify this behavior. Our findings indicate that SMTOs perform well compared\nto uniform loss and that fixed weights can achieve competitive performance\ncompared to SMTOs. Furthermore, we demonstrate why uniform loss perform\nsimilarly to SMTOs in some instances. The source code is available at\nhttps://github.com/Gabriel-SGama/UnitScal_vs_SMTOs.\n", "link": "http://arxiv.org/abs/2505.10347v2", "date": "2025-08-11", "relevancy": 1.8447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4619}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uniform%20Loss%20vs.%20Specialized%20Optimization%3A%20A%20Comparative%20Analysis%20in%0A%20%20Multi-Task%20Learning&body=Title%3A%20Uniform%20Loss%20vs.%20Specialized%20Optimization%3A%20A%20Comparative%20Analysis%20in%0A%20%20Multi-Task%20Learning%0AAuthor%3A%20Gabriel%20S.%20Gama%20and%20Valdir%20Grassi%20Jr%0AAbstract%3A%20%20%20Specialized%20Multi-Task%20Optimizers%20%28SMTOs%29%20balance%20task%20learning%20in%20Multi-Task%0ALearning%20by%20addressing%20issues%20like%20conflicting%20gradients%20and%20differing%20gradient%0Anorms%2C%20which%20hinder%20equal-weighted%20task%20training.%20However%2C%20recent%20critiques%0Asuggest%20that%20equally%20weighted%20tasks%20can%20achieve%20competitive%20results%20compared%20to%0ASMTOs%2C%20arguing%20that%20previous%20SMTO%20results%20were%20influenced%20by%20poor%0Ahyperparameter%20optimization%20and%20lack%20of%20regularization.%20In%20this%20work%2C%20we%0Aevaluate%20these%20claims%20through%20an%20extensive%20empirical%20evaluation%20of%20SMTOs%2C%0Aincluding%20some%20of%20the%20latest%20methods%2C%20on%20more%20complex%20multi-task%20problems%20to%0Aclarify%20this%20behavior.%20Our%20findings%20indicate%20that%20SMTOs%20perform%20well%20compared%0Ato%20uniform%20loss%20and%20that%20fixed%20weights%20can%20achieve%20competitive%20performance%0Acompared%20to%20SMTOs.%20Furthermore%2C%20we%20demonstrate%20why%20uniform%20loss%20perform%0Asimilarly%20to%20SMTOs%20in%20some%20instances.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Gabriel-SGama/UnitScal_vs_SMTOs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniform%2520Loss%2520vs.%2520Specialized%2520Optimization%253A%2520A%2520Comparative%2520Analysis%2520in%250A%2520%2520Multi-Task%2520Learning%26entry.906535625%3DGabriel%2520S.%2520Gama%2520and%2520Valdir%2520Grassi%2520Jr%26entry.1292438233%3D%2520%2520Specialized%2520Multi-Task%2520Optimizers%2520%2528SMTOs%2529%2520balance%2520task%2520learning%2520in%2520Multi-Task%250ALearning%2520by%2520addressing%2520issues%2520like%2520conflicting%2520gradients%2520and%2520differing%2520gradient%250Anorms%252C%2520which%2520hinder%2520equal-weighted%2520task%2520training.%2520However%252C%2520recent%2520critiques%250Asuggest%2520that%2520equally%2520weighted%2520tasks%2520can%2520achieve%2520competitive%2520results%2520compared%2520to%250ASMTOs%252C%2520arguing%2520that%2520previous%2520SMTO%2520results%2520were%2520influenced%2520by%2520poor%250Ahyperparameter%2520optimization%2520and%2520lack%2520of%2520regularization.%2520In%2520this%2520work%252C%2520we%250Aevaluate%2520these%2520claims%2520through%2520an%2520extensive%2520empirical%2520evaluation%2520of%2520SMTOs%252C%250Aincluding%2520some%2520of%2520the%2520latest%2520methods%252C%2520on%2520more%2520complex%2520multi-task%2520problems%2520to%250Aclarify%2520this%2520behavior.%2520Our%2520findings%2520indicate%2520that%2520SMTOs%2520perform%2520well%2520compared%250Ato%2520uniform%2520loss%2520and%2520that%2520fixed%2520weights%2520can%2520achieve%2520competitive%2520performance%250Acompared%2520to%2520SMTOs.%2520Furthermore%252C%2520we%2520demonstrate%2520why%2520uniform%2520loss%2520perform%250Asimilarly%2520to%2520SMTOs%2520in%2520some%2520instances.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Gabriel-SGama/UnitScal_vs_SMTOs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uniform%20Loss%20vs.%20Specialized%20Optimization%3A%20A%20Comparative%20Analysis%20in%0A%20%20Multi-Task%20Learning&entry.906535625=Gabriel%20S.%20Gama%20and%20Valdir%20Grassi%20Jr&entry.1292438233=%20%20Specialized%20Multi-Task%20Optimizers%20%28SMTOs%29%20balance%20task%20learning%20in%20Multi-Task%0ALearning%20by%20addressing%20issues%20like%20conflicting%20gradients%20and%20differing%20gradient%0Anorms%2C%20which%20hinder%20equal-weighted%20task%20training.%20However%2C%20recent%20critiques%0Asuggest%20that%20equally%20weighted%20tasks%20can%20achieve%20competitive%20results%20compared%20to%0ASMTOs%2C%20arguing%20that%20previous%20SMTO%20results%20were%20influenced%20by%20poor%0Ahyperparameter%20optimization%20and%20lack%20of%20regularization.%20In%20this%20work%2C%20we%0Aevaluate%20these%20claims%20through%20an%20extensive%20empirical%20evaluation%20of%20SMTOs%2C%0Aincluding%20some%20of%20the%20latest%20methods%2C%20on%20more%20complex%20multi-task%20problems%20to%0Aclarify%20this%20behavior.%20Our%20findings%20indicate%20that%20SMTOs%20perform%20well%20compared%0Ato%20uniform%20loss%20and%20that%20fixed%20weights%20can%20achieve%20competitive%20performance%0Acompared%20to%20SMTOs.%20Furthermore%2C%20we%20demonstrate%20why%20uniform%20loss%20perform%0Asimilarly%20to%20SMTOs%20in%20some%20instances.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Gabriel-SGama/UnitScal_vs_SMTOs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10347v2&entry.124074799=Read"},
{"title": "Neural Logic Networks for Interpretable Classification", "author": "Vincent Perreault and Katsumi Inoue and Richard Labib and Alain Hertz", "abstract": "  Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on an example from the medical field where\ninterpretability has tangible value.\n", "link": "http://arxiv.org/abs/2508.08172v1", "date": "2025-08-11", "relevancy": 1.921, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.505}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4799}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Logic%20Networks%20for%20Interpretable%20Classification&body=Title%3A%20Neural%20Logic%20Networks%20for%20Interpretable%20Classification%0AAuthor%3A%20Vincent%20Perreault%20and%20Katsumi%20Inoue%20and%20Richard%20Labib%20and%20Alain%20Hertz%0AAbstract%3A%20%20%20Traditional%20neural%20networks%20have%20an%20impressive%20classification%20performance%2C%0Abut%20what%20they%20learn%20cannot%20be%20inspected%2C%20verified%20or%20extracted.%20Neural%20Logic%0ANetworks%20on%20the%20other%20hand%20have%20an%20interpretable%20structure%20that%20enables%20them%20to%0Alearn%20a%20logical%20mechanism%20relating%20the%20inputs%20and%20outputs%20with%20AND%20and%20OR%0Aoperations.%20We%20generalize%20these%20networks%20with%20NOT%20operations%20and%20biases%20that%0Atake%20into%20account%20unobserved%20data%20and%20develop%20a%20rigorous%20logical%20and%0Aprobabilistic%20modeling%20in%20terms%20of%20concept%20combinations%20to%20motivate%20their%20use.%0AWe%20also%20propose%20a%20novel%20factorized%20IF-THEN%20rule%20structure%20for%20the%20model%20as%20well%0Aas%20a%20modified%20learning%20algorithm.%20Our%20method%20improves%20the%20state-of-the-art%20in%0ABoolean%20networks%20discovery%20and%20is%20able%20to%20learn%20relevant%2C%20interpretable%20rules%0Ain%20tabular%20classification%2C%20notably%20on%20an%20example%20from%20the%20medical%20field%20where%0Ainterpretability%20has%20tangible%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Logic%2520Networks%2520for%2520Interpretable%2520Classification%26entry.906535625%3DVincent%2520Perreault%2520and%2520Katsumi%2520Inoue%2520and%2520Richard%2520Labib%2520and%2520Alain%2520Hertz%26entry.1292438233%3D%2520%2520Traditional%2520neural%2520networks%2520have%2520an%2520impressive%2520classification%2520performance%252C%250Abut%2520what%2520they%2520learn%2520cannot%2520be%2520inspected%252C%2520verified%2520or%2520extracted.%2520Neural%2520Logic%250ANetworks%2520on%2520the%2520other%2520hand%2520have%2520an%2520interpretable%2520structure%2520that%2520enables%2520them%2520to%250Alearn%2520a%2520logical%2520mechanism%2520relating%2520the%2520inputs%2520and%2520outputs%2520with%2520AND%2520and%2520OR%250Aoperations.%2520We%2520generalize%2520these%2520networks%2520with%2520NOT%2520operations%2520and%2520biases%2520that%250Atake%2520into%2520account%2520unobserved%2520data%2520and%2520develop%2520a%2520rigorous%2520logical%2520and%250Aprobabilistic%2520modeling%2520in%2520terms%2520of%2520concept%2520combinations%2520to%2520motivate%2520their%2520use.%250AWe%2520also%2520propose%2520a%2520novel%2520factorized%2520IF-THEN%2520rule%2520structure%2520for%2520the%2520model%2520as%2520well%250Aas%2520a%2520modified%2520learning%2520algorithm.%2520Our%2520method%2520improves%2520the%2520state-of-the-art%2520in%250ABoolean%2520networks%2520discovery%2520and%2520is%2520able%2520to%2520learn%2520relevant%252C%2520interpretable%2520rules%250Ain%2520tabular%2520classification%252C%2520notably%2520on%2520an%2520example%2520from%2520the%2520medical%2520field%2520where%250Ainterpretability%2520has%2520tangible%2520value.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Logic%20Networks%20for%20Interpretable%20Classification&entry.906535625=Vincent%20Perreault%20and%20Katsumi%20Inoue%20and%20Richard%20Labib%20and%20Alain%20Hertz&entry.1292438233=%20%20Traditional%20neural%20networks%20have%20an%20impressive%20classification%20performance%2C%0Abut%20what%20they%20learn%20cannot%20be%20inspected%2C%20verified%20or%20extracted.%20Neural%20Logic%0ANetworks%20on%20the%20other%20hand%20have%20an%20interpretable%20structure%20that%20enables%20them%20to%0Alearn%20a%20logical%20mechanism%20relating%20the%20inputs%20and%20outputs%20with%20AND%20and%20OR%0Aoperations.%20We%20generalize%20these%20networks%20with%20NOT%20operations%20and%20biases%20that%0Atake%20into%20account%20unobserved%20data%20and%20develop%20a%20rigorous%20logical%20and%0Aprobabilistic%20modeling%20in%20terms%20of%20concept%20combinations%20to%20motivate%20their%20use.%0AWe%20also%20propose%20a%20novel%20factorized%20IF-THEN%20rule%20structure%20for%20the%20model%20as%20well%0Aas%20a%20modified%20learning%20algorithm.%20Our%20method%20improves%20the%20state-of-the-art%20in%0ABoolean%20networks%20discovery%20and%20is%20able%20to%20learn%20relevant%2C%20interpretable%20rules%0Ain%20tabular%20classification%2C%20notably%20on%20an%20example%20from%20the%20medical%20field%20where%0Ainterpretability%20has%20tangible%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08172v1&entry.124074799=Read"},
{"title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control", "author": "Zeqian Long and Mingzhe Zheng and Kunyu Feng and Xinhua Zhang and Hongyu Liu and Harry Yang and Linfeng Zhang and Qifeng Chen and Yue Ma", "abstract": "  While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.\n", "link": "http://arxiv.org/abs/2508.08134v1", "date": "2025-08-11", "relevancy": 1.6894, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5712}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5626}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Follow-Your-Shape%3A%20Shape-Aware%20Image%20Editing%20via%20Trajectory-Guided%0A%20%20Region%20Control&body=Title%3A%20Follow-Your-Shape%3A%20Shape-Aware%20Image%20Editing%20via%20Trajectory-Guided%0A%20%20Region%20Control%0AAuthor%3A%20Zeqian%20Long%20and%20Mingzhe%20Zheng%20and%20Kunyu%20Feng%20and%20Xinhua%20Zhang%20and%20Hongyu%20Liu%20and%20Harry%20Yang%20and%20Linfeng%20Zhang%20and%20Qifeng%20Chen%20and%20Yue%20Ma%0AAbstract%3A%20%20%20While%20recent%20flow-based%20image%20editing%20models%20demonstrate%20general-purpose%0Acapabilities%20across%20diverse%20tasks%2C%20they%20often%20struggle%20to%20specialize%20in%0Achallenging%20scenarios%20--%20particularly%20those%20involving%20large-scale%20shape%0Atransformations.%20When%20performing%20such%20structural%20edits%2C%20these%20methods%20either%0Afail%20to%20achieve%20the%20intended%20shape%20change%20or%20inadvertently%20alter%20non-target%0Aregions%2C%20resulting%20in%20degraded%20background%20quality.%20We%20propose%0AFollow-Your-Shape%2C%20a%20training-free%20and%20mask-free%20framework%20that%20supports%0Aprecise%20and%20controllable%20editing%20of%20object%20shapes%20while%20strictly%20preserving%0Anon-target%20content.%20Motivated%20by%20the%20divergence%20between%20inversion%20and%20editing%0Atrajectories%2C%20we%20compute%20a%20Trajectory%20Divergence%20Map%20%28TDM%29%20by%20comparing%0Atoken-wise%20velocity%20differences%20between%20the%20inversion%20and%20denoising%20paths.%20The%0ATDM%20enables%20precise%20localization%20of%20editable%20regions%20and%20guides%20a%20Scheduled%20KV%0AInjection%20mechanism%20that%20ensures%20stable%20and%20faithful%20editing.%20To%20facilitate%20a%0Arigorous%20evaluation%2C%20we%20introduce%20ReShapeBench%2C%20a%20new%20benchmark%20comprising%20120%0Anew%20images%20and%20enriched%20prompt%20pairs%20specifically%20curated%20for%20shape-aware%0Aediting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20superior%20editability%0Aand%20visual%20fidelity%2C%20particularly%20in%20tasks%20requiring%20large-scale%20shape%0Areplacement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFollow-Your-Shape%253A%2520Shape-Aware%2520Image%2520Editing%2520via%2520Trajectory-Guided%250A%2520%2520Region%2520Control%26entry.906535625%3DZeqian%2520Long%2520and%2520Mingzhe%2520Zheng%2520and%2520Kunyu%2520Feng%2520and%2520Xinhua%2520Zhang%2520and%2520Hongyu%2520Liu%2520and%2520Harry%2520Yang%2520and%2520Linfeng%2520Zhang%2520and%2520Qifeng%2520Chen%2520and%2520Yue%2520Ma%26entry.1292438233%3D%2520%2520While%2520recent%2520flow-based%2520image%2520editing%2520models%2520demonstrate%2520general-purpose%250Acapabilities%2520across%2520diverse%2520tasks%252C%2520they%2520often%2520struggle%2520to%2520specialize%2520in%250Achallenging%2520scenarios%2520--%2520particularly%2520those%2520involving%2520large-scale%2520shape%250Atransformations.%2520When%2520performing%2520such%2520structural%2520edits%252C%2520these%2520methods%2520either%250Afail%2520to%2520achieve%2520the%2520intended%2520shape%2520change%2520or%2520inadvertently%2520alter%2520non-target%250Aregions%252C%2520resulting%2520in%2520degraded%2520background%2520quality.%2520We%2520propose%250AFollow-Your-Shape%252C%2520a%2520training-free%2520and%2520mask-free%2520framework%2520that%2520supports%250Aprecise%2520and%2520controllable%2520editing%2520of%2520object%2520shapes%2520while%2520strictly%2520preserving%250Anon-target%2520content.%2520Motivated%2520by%2520the%2520divergence%2520between%2520inversion%2520and%2520editing%250Atrajectories%252C%2520we%2520compute%2520a%2520Trajectory%2520Divergence%2520Map%2520%2528TDM%2529%2520by%2520comparing%250Atoken-wise%2520velocity%2520differences%2520between%2520the%2520inversion%2520and%2520denoising%2520paths.%2520The%250ATDM%2520enables%2520precise%2520localization%2520of%2520editable%2520regions%2520and%2520guides%2520a%2520Scheduled%2520KV%250AInjection%2520mechanism%2520that%2520ensures%2520stable%2520and%2520faithful%2520editing.%2520To%2520facilitate%2520a%250Arigorous%2520evaluation%252C%2520we%2520introduce%2520ReShapeBench%252C%2520a%2520new%2520benchmark%2520comprising%2520120%250Anew%2520images%2520and%2520enriched%2520prompt%2520pairs%2520specifically%2520curated%2520for%2520shape-aware%250Aediting.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520editability%250Aand%2520visual%2520fidelity%252C%2520particularly%2520in%2520tasks%2520requiring%2520large-scale%2520shape%250Areplacement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Follow-Your-Shape%3A%20Shape-Aware%20Image%20Editing%20via%20Trajectory-Guided%0A%20%20Region%20Control&entry.906535625=Zeqian%20Long%20and%20Mingzhe%20Zheng%20and%20Kunyu%20Feng%20and%20Xinhua%20Zhang%20and%20Hongyu%20Liu%20and%20Harry%20Yang%20and%20Linfeng%20Zhang%20and%20Qifeng%20Chen%20and%20Yue%20Ma&entry.1292438233=%20%20While%20recent%20flow-based%20image%20editing%20models%20demonstrate%20general-purpose%0Acapabilities%20across%20diverse%20tasks%2C%20they%20often%20struggle%20to%20specialize%20in%0Achallenging%20scenarios%20--%20particularly%20those%20involving%20large-scale%20shape%0Atransformations.%20When%20performing%20such%20structural%20edits%2C%20these%20methods%20either%0Afail%20to%20achieve%20the%20intended%20shape%20change%20or%20inadvertently%20alter%20non-target%0Aregions%2C%20resulting%20in%20degraded%20background%20quality.%20We%20propose%0AFollow-Your-Shape%2C%20a%20training-free%20and%20mask-free%20framework%20that%20supports%0Aprecise%20and%20controllable%20editing%20of%20object%20shapes%20while%20strictly%20preserving%0Anon-target%20content.%20Motivated%20by%20the%20divergence%20between%20inversion%20and%20editing%0Atrajectories%2C%20we%20compute%20a%20Trajectory%20Divergence%20Map%20%28TDM%29%20by%20comparing%0Atoken-wise%20velocity%20differences%20between%20the%20inversion%20and%20denoising%20paths.%20The%0ATDM%20enables%20precise%20localization%20of%20editable%20regions%20and%20guides%20a%20Scheduled%20KV%0AInjection%20mechanism%20that%20ensures%20stable%20and%20faithful%20editing.%20To%20facilitate%20a%0Arigorous%20evaluation%2C%20we%20introduce%20ReShapeBench%2C%20a%20new%20benchmark%20comprising%20120%0Anew%20images%20and%20enriched%20prompt%20pairs%20specifically%20curated%20for%20shape-aware%0Aediting.%20Experiments%20demonstrate%20that%20our%20method%20achieves%20superior%20editability%0Aand%20visual%20fidelity%2C%20particularly%20in%20tasks%20requiring%20large-scale%20shape%0Areplacement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08134v1&entry.124074799=Read"},
{"title": "A Deep Learning Based Resource Allocator for Communication Networks with\n  Dynamic User Utility Demands", "author": "Pourya Behmandpoor and Mark Eisen and Panagiotis Patrinos and Marc Moonen", "abstract": "  Deep learning (DL) based resource allocation (RA) has recently gained\nsignificant attention due to its performance efficiency. However, most related\nstudies assume an ideal case where the number of users and their utility\ndemands, e.g., data rate constraints, are fixed, and the designed DL-based RA\nscheme exploits a policy trained only for these fixed parameters. Consequently,\ncomputationally complex policy retraining is required whenever these parameters\nchange. In this paper, we introduce a DL-based resource allocator (ALCOR) that\nallows users to adjust their utility demands freely, such as based on their\napplication layer requirements. ALCOR employs deep neural networks (DNNs) as\nthe policy in a time-sharing problem. The underlying optimization algorithm\niteratively optimizes the on-off status of users to satisfy their utility\ndemands in expectation. The policy performs unconstrained RA (URA) -- RA\nwithout considering user utility demands -- among active users to maximize the\nsum utility (SU) at each time instant. Depending on the chosen URA scheme,\nALCOR can perform RA in either a centralized or distributed scenario. The\nderived convergence analyses provide theoretical guarantees for ALCOR's\nconvergence, and numerical experiments corroborate its effectiveness compared\nto meta-learning and reinforcement learning approaches.\n", "link": "http://arxiv.org/abs/2311.04600v3", "date": "2025-08-11", "relevancy": 1.7967, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4578}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4453}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Based%20Resource%20Allocator%20for%20Communication%20Networks%20with%0A%20%20Dynamic%20User%20Utility%20Demands&body=Title%3A%20A%20Deep%20Learning%20Based%20Resource%20Allocator%20for%20Communication%20Networks%20with%0A%20%20Dynamic%20User%20Utility%20Demands%0AAuthor%3A%20Pourya%20Behmandpoor%20and%20Mark%20Eisen%20and%20Panagiotis%20Patrinos%20and%20Marc%20Moonen%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20based%20resource%20allocation%20%28RA%29%20has%20recently%20gained%0Asignificant%20attention%20due%20to%20its%20performance%20efficiency.%20However%2C%20most%20related%0Astudies%20assume%20an%20ideal%20case%20where%20the%20number%20of%20users%20and%20their%20utility%0Ademands%2C%20e.g.%2C%20data%20rate%20constraints%2C%20are%20fixed%2C%20and%20the%20designed%20DL-based%20RA%0Ascheme%20exploits%20a%20policy%20trained%20only%20for%20these%20fixed%20parameters.%20Consequently%2C%0Acomputationally%20complex%20policy%20retraining%20is%20required%20whenever%20these%20parameters%0Achange.%20In%20this%20paper%2C%20we%20introduce%20a%20DL-based%20resource%20allocator%20%28ALCOR%29%20that%0Aallows%20users%20to%20adjust%20their%20utility%20demands%20freely%2C%20such%20as%20based%20on%20their%0Aapplication%20layer%20requirements.%20ALCOR%20employs%20deep%20neural%20networks%20%28DNNs%29%20as%0Athe%20policy%20in%20a%20time-sharing%20problem.%20The%20underlying%20optimization%20algorithm%0Aiteratively%20optimizes%20the%20on-off%20status%20of%20users%20to%20satisfy%20their%20utility%0Ademands%20in%20expectation.%20The%20policy%20performs%20unconstrained%20RA%20%28URA%29%20--%20RA%0Awithout%20considering%20user%20utility%20demands%20--%20among%20active%20users%20to%20maximize%20the%0Asum%20utility%20%28SU%29%20at%20each%20time%20instant.%20Depending%20on%20the%20chosen%20URA%20scheme%2C%0AALCOR%20can%20perform%20RA%20in%20either%20a%20centralized%20or%20distributed%20scenario.%20The%0Aderived%20convergence%20analyses%20provide%20theoretical%20guarantees%20for%20ALCOR%27s%0Aconvergence%2C%20and%20numerical%20experiments%20corroborate%20its%20effectiveness%20compared%0Ato%20meta-learning%20and%20reinforcement%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Based%2520Resource%2520Allocator%2520for%2520Communication%2520Networks%2520with%250A%2520%2520Dynamic%2520User%2520Utility%2520Demands%26entry.906535625%3DPourya%2520Behmandpoor%2520and%2520Mark%2520Eisen%2520and%2520Panagiotis%2520Patrinos%2520and%2520Marc%2520Moonen%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520based%2520resource%2520allocation%2520%2528RA%2529%2520has%2520recently%2520gained%250Asignificant%2520attention%2520due%2520to%2520its%2520performance%2520efficiency.%2520However%252C%2520most%2520related%250Astudies%2520assume%2520an%2520ideal%2520case%2520where%2520the%2520number%2520of%2520users%2520and%2520their%2520utility%250Ademands%252C%2520e.g.%252C%2520data%2520rate%2520constraints%252C%2520are%2520fixed%252C%2520and%2520the%2520designed%2520DL-based%2520RA%250Ascheme%2520exploits%2520a%2520policy%2520trained%2520only%2520for%2520these%2520fixed%2520parameters.%2520Consequently%252C%250Acomputationally%2520complex%2520policy%2520retraining%2520is%2520required%2520whenever%2520these%2520parameters%250Achange.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520DL-based%2520resource%2520allocator%2520%2528ALCOR%2529%2520that%250Aallows%2520users%2520to%2520adjust%2520their%2520utility%2520demands%2520freely%252C%2520such%2520as%2520based%2520on%2520their%250Aapplication%2520layer%2520requirements.%2520ALCOR%2520employs%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520as%250Athe%2520policy%2520in%2520a%2520time-sharing%2520problem.%2520The%2520underlying%2520optimization%2520algorithm%250Aiteratively%2520optimizes%2520the%2520on-off%2520status%2520of%2520users%2520to%2520satisfy%2520their%2520utility%250Ademands%2520in%2520expectation.%2520The%2520policy%2520performs%2520unconstrained%2520RA%2520%2528URA%2529%2520--%2520RA%250Awithout%2520considering%2520user%2520utility%2520demands%2520--%2520among%2520active%2520users%2520to%2520maximize%2520the%250Asum%2520utility%2520%2528SU%2529%2520at%2520each%2520time%2520instant.%2520Depending%2520on%2520the%2520chosen%2520URA%2520scheme%252C%250AALCOR%2520can%2520perform%2520RA%2520in%2520either%2520a%2520centralized%2520or%2520distributed%2520scenario.%2520The%250Aderived%2520convergence%2520analyses%2520provide%2520theoretical%2520guarantees%2520for%2520ALCOR%2527s%250Aconvergence%252C%2520and%2520numerical%2520experiments%2520corroborate%2520its%2520effectiveness%2520compared%250Ato%2520meta-learning%2520and%2520reinforcement%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Based%20Resource%20Allocator%20for%20Communication%20Networks%20with%0A%20%20Dynamic%20User%20Utility%20Demands&entry.906535625=Pourya%20Behmandpoor%20and%20Mark%20Eisen%20and%20Panagiotis%20Patrinos%20and%20Marc%20Moonen&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20based%20resource%20allocation%20%28RA%29%20has%20recently%20gained%0Asignificant%20attention%20due%20to%20its%20performance%20efficiency.%20However%2C%20most%20related%0Astudies%20assume%20an%20ideal%20case%20where%20the%20number%20of%20users%20and%20their%20utility%0Ademands%2C%20e.g.%2C%20data%20rate%20constraints%2C%20are%20fixed%2C%20and%20the%20designed%20DL-based%20RA%0Ascheme%20exploits%20a%20policy%20trained%20only%20for%20these%20fixed%20parameters.%20Consequently%2C%0Acomputationally%20complex%20policy%20retraining%20is%20required%20whenever%20these%20parameters%0Achange.%20In%20this%20paper%2C%20we%20introduce%20a%20DL-based%20resource%20allocator%20%28ALCOR%29%20that%0Aallows%20users%20to%20adjust%20their%20utility%20demands%20freely%2C%20such%20as%20based%20on%20their%0Aapplication%20layer%20requirements.%20ALCOR%20employs%20deep%20neural%20networks%20%28DNNs%29%20as%0Athe%20policy%20in%20a%20time-sharing%20problem.%20The%20underlying%20optimization%20algorithm%0Aiteratively%20optimizes%20the%20on-off%20status%20of%20users%20to%20satisfy%20their%20utility%0Ademands%20in%20expectation.%20The%20policy%20performs%20unconstrained%20RA%20%28URA%29%20--%20RA%0Awithout%20considering%20user%20utility%20demands%20--%20among%20active%20users%20to%20maximize%20the%0Asum%20utility%20%28SU%29%20at%20each%20time%20instant.%20Depending%20on%20the%20chosen%20URA%20scheme%2C%0AALCOR%20can%20perform%20RA%20in%20either%20a%20centralized%20or%20distributed%20scenario.%20The%0Aderived%20convergence%20analyses%20provide%20theoretical%20guarantees%20for%20ALCOR%27s%0Aconvergence%2C%20and%20numerical%20experiments%20corroborate%20its%20effectiveness%20compared%0Ato%20meta-learning%20and%20reinforcement%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04600v3&entry.124074799=Read"},
{"title": "An effective potential for generative modelling with active matter", "author": "Adrian Baule", "abstract": "  Score-based diffusion models generate samples from a complex underlying data\ndistribution by time-reversal of a diffusion process and represent the\nstate-of-the-art in many generative AI applications such as artificial image\nsynthesis. Here, I show how a generative diffusion model can be implemented\nbased on an underlying active particle process with finite correlation time. In\ncontrast to previous approaches that use a score function acting on the\nvelocity coordinate of the active particle, time reversal is here achieved by\nimposing an effective time-dependent potential on the position coordinate only.\nThe effective potential is valid to first order in the persistence time and\nleads to a force field that is fully determined by the standard score function\nand its derivatives up to 2nd order. Numerical experiments for artificial data\ndistributions confirm the validity of the effective potential.\n", "link": "http://arxiv.org/abs/2508.08146v1", "date": "2025-08-11", "relevancy": 1.7287, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.598}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.571}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20effective%20potential%20for%20generative%20modelling%20with%20active%20matter&body=Title%3A%20An%20effective%20potential%20for%20generative%20modelling%20with%20active%20matter%0AAuthor%3A%20Adrian%20Baule%0AAbstract%3A%20%20%20Score-based%20diffusion%20models%20generate%20samples%20from%20a%20complex%20underlying%20data%0Adistribution%20by%20time-reversal%20of%20a%20diffusion%20process%20and%20represent%20the%0Astate-of-the-art%20in%20many%20generative%20AI%20applications%20such%20as%20artificial%20image%0Asynthesis.%20Here%2C%20I%20show%20how%20a%20generative%20diffusion%20model%20can%20be%20implemented%0Abased%20on%20an%20underlying%20active%20particle%20process%20with%20finite%20correlation%20time.%20In%0Acontrast%20to%20previous%20approaches%20that%20use%20a%20score%20function%20acting%20on%20the%0Avelocity%20coordinate%20of%20the%20active%20particle%2C%20time%20reversal%20is%20here%20achieved%20by%0Aimposing%20an%20effective%20time-dependent%20potential%20on%20the%20position%20coordinate%20only.%0AThe%20effective%20potential%20is%20valid%20to%20first%20order%20in%20the%20persistence%20time%20and%0Aleads%20to%20a%20force%20field%20that%20is%20fully%20determined%20by%20the%20standard%20score%20function%0Aand%20its%20derivatives%20up%20to%202nd%20order.%20Numerical%20experiments%20for%20artificial%20data%0Adistributions%20confirm%20the%20validity%20of%20the%20effective%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520effective%2520potential%2520for%2520generative%2520modelling%2520with%2520active%2520matter%26entry.906535625%3DAdrian%2520Baule%26entry.1292438233%3D%2520%2520Score-based%2520diffusion%2520models%2520generate%2520samples%2520from%2520a%2520complex%2520underlying%2520data%250Adistribution%2520by%2520time-reversal%2520of%2520a%2520diffusion%2520process%2520and%2520represent%2520the%250Astate-of-the-art%2520in%2520many%2520generative%2520AI%2520applications%2520such%2520as%2520artificial%2520image%250Asynthesis.%2520Here%252C%2520I%2520show%2520how%2520a%2520generative%2520diffusion%2520model%2520can%2520be%2520implemented%250Abased%2520on%2520an%2520underlying%2520active%2520particle%2520process%2520with%2520finite%2520correlation%2520time.%2520In%250Acontrast%2520to%2520previous%2520approaches%2520that%2520use%2520a%2520score%2520function%2520acting%2520on%2520the%250Avelocity%2520coordinate%2520of%2520the%2520active%2520particle%252C%2520time%2520reversal%2520is%2520here%2520achieved%2520by%250Aimposing%2520an%2520effective%2520time-dependent%2520potential%2520on%2520the%2520position%2520coordinate%2520only.%250AThe%2520effective%2520potential%2520is%2520valid%2520to%2520first%2520order%2520in%2520the%2520persistence%2520time%2520and%250Aleads%2520to%2520a%2520force%2520field%2520that%2520is%2520fully%2520determined%2520by%2520the%2520standard%2520score%2520function%250Aand%2520its%2520derivatives%2520up%2520to%25202nd%2520order.%2520Numerical%2520experiments%2520for%2520artificial%2520data%250Adistributions%2520confirm%2520the%2520validity%2520of%2520the%2520effective%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20effective%20potential%20for%20generative%20modelling%20with%20active%20matter&entry.906535625=Adrian%20Baule&entry.1292438233=%20%20Score-based%20diffusion%20models%20generate%20samples%20from%20a%20complex%20underlying%20data%0Adistribution%20by%20time-reversal%20of%20a%20diffusion%20process%20and%20represent%20the%0Astate-of-the-art%20in%20many%20generative%20AI%20applications%20such%20as%20artificial%20image%0Asynthesis.%20Here%2C%20I%20show%20how%20a%20generative%20diffusion%20model%20can%20be%20implemented%0Abased%20on%20an%20underlying%20active%20particle%20process%20with%20finite%20correlation%20time.%20In%0Acontrast%20to%20previous%20approaches%20that%20use%20a%20score%20function%20acting%20on%20the%0Avelocity%20coordinate%20of%20the%20active%20particle%2C%20time%20reversal%20is%20here%20achieved%20by%0Aimposing%20an%20effective%20time-dependent%20potential%20on%20the%20position%20coordinate%20only.%0AThe%20effective%20potential%20is%20valid%20to%20first%20order%20in%20the%20persistence%20time%20and%0Aleads%20to%20a%20force%20field%20that%20is%20fully%20determined%20by%20the%20standard%20score%20function%0Aand%20its%20derivatives%20up%20to%202nd%20order.%20Numerical%20experiments%20for%20artificial%20data%0Adistributions%20confirm%20the%20validity%20of%20the%20effective%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08146v1&entry.124074799=Read"},
{"title": "Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation", "author": "Van-Khang Nguyen and Duc-Hoang Pham and Huy-Son Nguyen and Cam-Van Thi Nguyen and Hoang-Quynh Le and Duc-Trong Le", "abstract": "  Recommendation systems have faced significant challenges in cold-start\nscenarios, where new items with a limited history of interaction need to be\neffectively recommended to users. Though multimodal data (e.g., images, text,\naudio, etc.) offer rich information to address this issue, existing approaches\noften employ simplistic integration methods such as concatenation, average\npooling, or fixed weighting schemes, which fail to capture the complex\nrelationships between modalities. Our study proposes a novel Mixture of Experts\n(MoE) framework for multimodal cold-start recommendation, named MAMEX, which\ndynamically leverages latent representation from different modalities. MAMEX\nutilizes modality-specific expert networks and introduces a learnable gating\nmechanism that adaptively weights the contribution of each modality based on\nits content characteristics. This approach enables MAMEX to emphasize the most\ninformative modalities for each item while maintaining robustness when certain\nmodalities are less relevant or missing. Extensive experiments on benchmark\ndatasets show that MAMEX outperforms state-of-the-art methods in cold-start\nscenarios, with superior accuracy and adaptability. For reproducibility, the\ncode has been made available on Github https://github.com/L2R-UET/MAMEX.\n", "link": "http://arxiv.org/abs/2508.08042v1", "date": "2025-08-11", "relevancy": 1.4634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.495}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.487}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Adaptive%20Mixture%20of%20Experts%20for%20Cold-start%20Recommendation&body=Title%3A%20Multi-modal%20Adaptive%20Mixture%20of%20Experts%20for%20Cold-start%20Recommendation%0AAuthor%3A%20Van-Khang%20Nguyen%20and%20Duc-Hoang%20Pham%20and%20Huy-Son%20Nguyen%20and%20Cam-Van%20Thi%20Nguyen%20and%20Hoang-Quynh%20Le%20and%20Duc-Trong%20Le%0AAbstract%3A%20%20%20Recommendation%20systems%20have%20faced%20significant%20challenges%20in%20cold-start%0Ascenarios%2C%20where%20new%20items%20with%20a%20limited%20history%20of%20interaction%20need%20to%20be%0Aeffectively%20recommended%20to%20users.%20Though%20multimodal%20data%20%28e.g.%2C%20images%2C%20text%2C%0Aaudio%2C%20etc.%29%20offer%20rich%20information%20to%20address%20this%20issue%2C%20existing%20approaches%0Aoften%20employ%20simplistic%20integration%20methods%20such%20as%20concatenation%2C%20average%0Apooling%2C%20or%20fixed%20weighting%20schemes%2C%20which%20fail%20to%20capture%20the%20complex%0Arelationships%20between%20modalities.%20Our%20study%20proposes%20a%20novel%20Mixture%20of%20Experts%0A%28MoE%29%20framework%20for%20multimodal%20cold-start%20recommendation%2C%20named%20MAMEX%2C%20which%0Adynamically%20leverages%20latent%20representation%20from%20different%20modalities.%20MAMEX%0Autilizes%20modality-specific%20expert%20networks%20and%20introduces%20a%20learnable%20gating%0Amechanism%20that%20adaptively%20weights%20the%20contribution%20of%20each%20modality%20based%20on%0Aits%20content%20characteristics.%20This%20approach%20enables%20MAMEX%20to%20emphasize%20the%20most%0Ainformative%20modalities%20for%20each%20item%20while%20maintaining%20robustness%20when%20certain%0Amodalities%20are%20less%20relevant%20or%20missing.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20show%20that%20MAMEX%20outperforms%20state-of-the-art%20methods%20in%20cold-start%0Ascenarios%2C%20with%20superior%20accuracy%20and%20adaptability.%20For%20reproducibility%2C%20the%0Acode%20has%20been%20made%20available%20on%20Github%20https%3A//github.com/L2R-UET/MAMEX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Adaptive%2520Mixture%2520of%2520Experts%2520for%2520Cold-start%2520Recommendation%26entry.906535625%3DVan-Khang%2520Nguyen%2520and%2520Duc-Hoang%2520Pham%2520and%2520Huy-Son%2520Nguyen%2520and%2520Cam-Van%2520Thi%2520Nguyen%2520and%2520Hoang-Quynh%2520Le%2520and%2520Duc-Trong%2520Le%26entry.1292438233%3D%2520%2520Recommendation%2520systems%2520have%2520faced%2520significant%2520challenges%2520in%2520cold-start%250Ascenarios%252C%2520where%2520new%2520items%2520with%2520a%2520limited%2520history%2520of%2520interaction%2520need%2520to%2520be%250Aeffectively%2520recommended%2520to%2520users.%2520Though%2520multimodal%2520data%2520%2528e.g.%252C%2520images%252C%2520text%252C%250Aaudio%252C%2520etc.%2529%2520offer%2520rich%2520information%2520to%2520address%2520this%2520issue%252C%2520existing%2520approaches%250Aoften%2520employ%2520simplistic%2520integration%2520methods%2520such%2520as%2520concatenation%252C%2520average%250Apooling%252C%2520or%2520fixed%2520weighting%2520schemes%252C%2520which%2520fail%2520to%2520capture%2520the%2520complex%250Arelationships%2520between%2520modalities.%2520Our%2520study%2520proposes%2520a%2520novel%2520Mixture%2520of%2520Experts%250A%2528MoE%2529%2520framework%2520for%2520multimodal%2520cold-start%2520recommendation%252C%2520named%2520MAMEX%252C%2520which%250Adynamically%2520leverages%2520latent%2520representation%2520from%2520different%2520modalities.%2520MAMEX%250Autilizes%2520modality-specific%2520expert%2520networks%2520and%2520introduces%2520a%2520learnable%2520gating%250Amechanism%2520that%2520adaptively%2520weights%2520the%2520contribution%2520of%2520each%2520modality%2520based%2520on%250Aits%2520content%2520characteristics.%2520This%2520approach%2520enables%2520MAMEX%2520to%2520emphasize%2520the%2520most%250Ainformative%2520modalities%2520for%2520each%2520item%2520while%2520maintaining%2520robustness%2520when%2520certain%250Amodalities%2520are%2520less%2520relevant%2520or%2520missing.%2520Extensive%2520experiments%2520on%2520benchmark%250Adatasets%2520show%2520that%2520MAMEX%2520outperforms%2520state-of-the-art%2520methods%2520in%2520cold-start%250Ascenarios%252C%2520with%2520superior%2520accuracy%2520and%2520adaptability.%2520For%2520reproducibility%252C%2520the%250Acode%2520has%2520been%2520made%2520available%2520on%2520Github%2520https%253A//github.com/L2R-UET/MAMEX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Adaptive%20Mixture%20of%20Experts%20for%20Cold-start%20Recommendation&entry.906535625=Van-Khang%20Nguyen%20and%20Duc-Hoang%20Pham%20and%20Huy-Son%20Nguyen%20and%20Cam-Van%20Thi%20Nguyen%20and%20Hoang-Quynh%20Le%20and%20Duc-Trong%20Le&entry.1292438233=%20%20Recommendation%20systems%20have%20faced%20significant%20challenges%20in%20cold-start%0Ascenarios%2C%20where%20new%20items%20with%20a%20limited%20history%20of%20interaction%20need%20to%20be%0Aeffectively%20recommended%20to%20users.%20Though%20multimodal%20data%20%28e.g.%2C%20images%2C%20text%2C%0Aaudio%2C%20etc.%29%20offer%20rich%20information%20to%20address%20this%20issue%2C%20existing%20approaches%0Aoften%20employ%20simplistic%20integration%20methods%20such%20as%20concatenation%2C%20average%0Apooling%2C%20or%20fixed%20weighting%20schemes%2C%20which%20fail%20to%20capture%20the%20complex%0Arelationships%20between%20modalities.%20Our%20study%20proposes%20a%20novel%20Mixture%20of%20Experts%0A%28MoE%29%20framework%20for%20multimodal%20cold-start%20recommendation%2C%20named%20MAMEX%2C%20which%0Adynamically%20leverages%20latent%20representation%20from%20different%20modalities.%20MAMEX%0Autilizes%20modality-specific%20expert%20networks%20and%20introduces%20a%20learnable%20gating%0Amechanism%20that%20adaptively%20weights%20the%20contribution%20of%20each%20modality%20based%20on%0Aits%20content%20characteristics.%20This%20approach%20enables%20MAMEX%20to%20emphasize%20the%20most%0Ainformative%20modalities%20for%20each%20item%20while%20maintaining%20robustness%20when%20certain%0Amodalities%20are%20less%20relevant%20or%20missing.%20Extensive%20experiments%20on%20benchmark%0Adatasets%20show%20that%20MAMEX%20outperforms%20state-of-the-art%20methods%20in%20cold-start%0Ascenarios%2C%20with%20superior%20accuracy%20and%20adaptability.%20For%20reproducibility%2C%20the%0Acode%20has%20been%20made%20available%20on%20Github%20https%3A//github.com/L2R-UET/MAMEX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08042v1&entry.124074799=Read"},
{"title": "A variational Bayes approach to debiased inference for low-dimensional\n  parameters in high-dimensional linear regression", "author": "Isma\u00ebl Castillo and Alice L'Huillier and Kolyan Ray and Luke Travis", "abstract": "  We propose a scalable variational Bayes method for statistical inference for\na single or low-dimensional subset of the coordinates of a high-dimensional\nparameter in sparse linear regression. Our approach relies on assigning a\nmean-field approximation to the nuisance coordinates and carefully modelling\nthe conditional distribution of the target given the nuisance. This requires\nonly a preprocessing step and preserves the computational advantages of\nmean-field variational Bayes, while ensuring accurate and reliable inference\nfor the target parameter, including for uncertainty quantification. We\ninvestigate the numerical performance of our algorithm, showing that it\nperforms competitively with existing methods. We further establish accompanying\ntheoretical guarantees for estimation and uncertainty quantification in the\nform of a Bernstein--von Mises theorem.\n", "link": "http://arxiv.org/abs/2406.12659v2", "date": "2025-08-11", "relevancy": 1.4301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4631}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20variational%20Bayes%20approach%20to%20debiased%20inference%20for%20low-dimensional%0A%20%20parameters%20in%20high-dimensional%20linear%20regression&body=Title%3A%20A%20variational%20Bayes%20approach%20to%20debiased%20inference%20for%20low-dimensional%0A%20%20parameters%20in%20high-dimensional%20linear%20regression%0AAuthor%3A%20Isma%C3%ABl%20Castillo%20and%20Alice%20L%27Huillier%20and%20Kolyan%20Ray%20and%20Luke%20Travis%0AAbstract%3A%20%20%20We%20propose%20a%20scalable%20variational%20Bayes%20method%20for%20statistical%20inference%20for%0Aa%20single%20or%20low-dimensional%20subset%20of%20the%20coordinates%20of%20a%20high-dimensional%0Aparameter%20in%20sparse%20linear%20regression.%20Our%20approach%20relies%20on%20assigning%20a%0Amean-field%20approximation%20to%20the%20nuisance%20coordinates%20and%20carefully%20modelling%0Athe%20conditional%20distribution%20of%20the%20target%20given%20the%20nuisance.%20This%20requires%0Aonly%20a%20preprocessing%20step%20and%20preserves%20the%20computational%20advantages%20of%0Amean-field%20variational%20Bayes%2C%20while%20ensuring%20accurate%20and%20reliable%20inference%0Afor%20the%20target%20parameter%2C%20including%20for%20uncertainty%20quantification.%20We%0Ainvestigate%20the%20numerical%20performance%20of%20our%20algorithm%2C%20showing%20that%20it%0Aperforms%20competitively%20with%20existing%20methods.%20We%20further%20establish%20accompanying%0Atheoretical%20guarantees%20for%20estimation%20and%20uncertainty%20quantification%20in%20the%0Aform%20of%20a%20Bernstein--von%20Mises%20theorem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520variational%2520Bayes%2520approach%2520to%2520debiased%2520inference%2520for%2520low-dimensional%250A%2520%2520parameters%2520in%2520high-dimensional%2520linear%2520regression%26entry.906535625%3DIsma%25C3%25ABl%2520Castillo%2520and%2520Alice%2520L%2527Huillier%2520and%2520Kolyan%2520Ray%2520and%2520Luke%2520Travis%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520scalable%2520variational%2520Bayes%2520method%2520for%2520statistical%2520inference%2520for%250Aa%2520single%2520or%2520low-dimensional%2520subset%2520of%2520the%2520coordinates%2520of%2520a%2520high-dimensional%250Aparameter%2520in%2520sparse%2520linear%2520regression.%2520Our%2520approach%2520relies%2520on%2520assigning%2520a%250Amean-field%2520approximation%2520to%2520the%2520nuisance%2520coordinates%2520and%2520carefully%2520modelling%250Athe%2520conditional%2520distribution%2520of%2520the%2520target%2520given%2520the%2520nuisance.%2520This%2520requires%250Aonly%2520a%2520preprocessing%2520step%2520and%2520preserves%2520the%2520computational%2520advantages%2520of%250Amean-field%2520variational%2520Bayes%252C%2520while%2520ensuring%2520accurate%2520and%2520reliable%2520inference%250Afor%2520the%2520target%2520parameter%252C%2520including%2520for%2520uncertainty%2520quantification.%2520We%250Ainvestigate%2520the%2520numerical%2520performance%2520of%2520our%2520algorithm%252C%2520showing%2520that%2520it%250Aperforms%2520competitively%2520with%2520existing%2520methods.%2520We%2520further%2520establish%2520accompanying%250Atheoretical%2520guarantees%2520for%2520estimation%2520and%2520uncertainty%2520quantification%2520in%2520the%250Aform%2520of%2520a%2520Bernstein--von%2520Mises%2520theorem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20variational%20Bayes%20approach%20to%20debiased%20inference%20for%20low-dimensional%0A%20%20parameters%20in%20high-dimensional%20linear%20regression&entry.906535625=Isma%C3%ABl%20Castillo%20and%20Alice%20L%27Huillier%20and%20Kolyan%20Ray%20and%20Luke%20Travis&entry.1292438233=%20%20We%20propose%20a%20scalable%20variational%20Bayes%20method%20for%20statistical%20inference%20for%0Aa%20single%20or%20low-dimensional%20subset%20of%20the%20coordinates%20of%20a%20high-dimensional%0Aparameter%20in%20sparse%20linear%20regression.%20Our%20approach%20relies%20on%20assigning%20a%0Amean-field%20approximation%20to%20the%20nuisance%20coordinates%20and%20carefully%20modelling%0Athe%20conditional%20distribution%20of%20the%20target%20given%20the%20nuisance.%20This%20requires%0Aonly%20a%20preprocessing%20step%20and%20preserves%20the%20computational%20advantages%20of%0Amean-field%20variational%20Bayes%2C%20while%20ensuring%20accurate%20and%20reliable%20inference%0Afor%20the%20target%20parameter%2C%20including%20for%20uncertainty%20quantification.%20We%0Ainvestigate%20the%20numerical%20performance%20of%20our%20algorithm%2C%20showing%20that%20it%0Aperforms%20competitively%20with%20existing%20methods.%20We%20further%20establish%20accompanying%0Atheoretical%20guarantees%20for%20estimation%20and%20uncertainty%20quantification%20in%20the%0Aform%20of%20a%20Bernstein--von%20Mises%20theorem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12659v2&entry.124074799=Read"},
{"title": "MDD-Net: Multimodal Depression Detection through Mutual Transformer", "author": "Md Rezwanul Haque and Md. Milon Islam and S M Taslim Uddin Raju and Hamdi Altaheri and Lobna Nassar and Fakhri Karray", "abstract": "  Depression is a major mental health condition that severely impacts the\nemotional and physical well-being of individuals. The simple nature of data\ncollection from social media platforms has attracted significant interest in\nproperly utilizing this information for mental health research. A Multimodal\nDepression Detection Network (MDD-Net), utilizing acoustic and visual data\nobtained from social media networks, is proposed in this work where mutual\ntransformers are exploited to efficiently extract and fuse multimodal features\nfor efficient depression detection. The MDD-Net consists of four core modules:\nan acoustic feature extraction module for retrieving relevant acoustic\nattributes, a visual feature extraction module for extracting significant\nhigh-level patterns, a mutual transformer for computing the correlations among\nthe generated features and fusing these features from multiple modalities, and\na detection layer for detecting depression using the fused feature\nrepresentations. The extensive experiments are performed using the multimodal\nD-Vlog dataset, and the findings reveal that the developed multimodal\ndepression detection network surpasses the state-of-the-art by up to 17.37% for\nF1-Score, demonstrating the greater performance of the proposed system. The\nsource code is accessible at\nhttps://github.com/rezwanh001/Multimodal-Depression-Detection.\n", "link": "http://arxiv.org/abs/2508.08093v1", "date": "2025-08-11", "relevancy": 1.514, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDD-Net%3A%20Multimodal%20Depression%20Detection%20through%20Mutual%20Transformer&body=Title%3A%20MDD-Net%3A%20Multimodal%20Depression%20Detection%20through%20Mutual%20Transformer%0AAuthor%3A%20Md%20Rezwanul%20Haque%20and%20Md.%20Milon%20Islam%20and%20S%20M%20Taslim%20Uddin%20Raju%20and%20Hamdi%20Altaheri%20and%20Lobna%20Nassar%20and%20Fakhri%20Karray%0AAbstract%3A%20%20%20Depression%20is%20a%20major%20mental%20health%20condition%20that%20severely%20impacts%20the%0Aemotional%20and%20physical%20well-being%20of%20individuals.%20The%20simple%20nature%20of%20data%0Acollection%20from%20social%20media%20platforms%20has%20attracted%20significant%20interest%20in%0Aproperly%20utilizing%20this%20information%20for%20mental%20health%20research.%20A%20Multimodal%0ADepression%20Detection%20Network%20%28MDD-Net%29%2C%20utilizing%20acoustic%20and%20visual%20data%0Aobtained%20from%20social%20media%20networks%2C%20is%20proposed%20in%20this%20work%20where%20mutual%0Atransformers%20are%20exploited%20to%20efficiently%20extract%20and%20fuse%20multimodal%20features%0Afor%20efficient%20depression%20detection.%20The%20MDD-Net%20consists%20of%20four%20core%20modules%3A%0Aan%20acoustic%20feature%20extraction%20module%20for%20retrieving%20relevant%20acoustic%0Aattributes%2C%20a%20visual%20feature%20extraction%20module%20for%20extracting%20significant%0Ahigh-level%20patterns%2C%20a%20mutual%20transformer%20for%20computing%20the%20correlations%20among%0Athe%20generated%20features%20and%20fusing%20these%20features%20from%20multiple%20modalities%2C%20and%0Aa%20detection%20layer%20for%20detecting%20depression%20using%20the%20fused%20feature%0Arepresentations.%20The%20extensive%20experiments%20are%20performed%20using%20the%20multimodal%0AD-Vlog%20dataset%2C%20and%20the%20findings%20reveal%20that%20the%20developed%20multimodal%0Adepression%20detection%20network%20surpasses%20the%20state-of-the-art%20by%20up%20to%2017.37%25%20for%0AF1-Score%2C%20demonstrating%20the%20greater%20performance%20of%20the%20proposed%20system.%20The%0Asource%20code%20is%20accessible%20at%0Ahttps%3A//github.com/rezwanh001/Multimodal-Depression-Detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.08093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDD-Net%253A%2520Multimodal%2520Depression%2520Detection%2520through%2520Mutual%2520Transformer%26entry.906535625%3DMd%2520Rezwanul%2520Haque%2520and%2520Md.%2520Milon%2520Islam%2520and%2520S%2520M%2520Taslim%2520Uddin%2520Raju%2520and%2520Hamdi%2520Altaheri%2520and%2520Lobna%2520Nassar%2520and%2520Fakhri%2520Karray%26entry.1292438233%3D%2520%2520Depression%2520is%2520a%2520major%2520mental%2520health%2520condition%2520that%2520severely%2520impacts%2520the%250Aemotional%2520and%2520physical%2520well-being%2520of%2520individuals.%2520The%2520simple%2520nature%2520of%2520data%250Acollection%2520from%2520social%2520media%2520platforms%2520has%2520attracted%2520significant%2520interest%2520in%250Aproperly%2520utilizing%2520this%2520information%2520for%2520mental%2520health%2520research.%2520A%2520Multimodal%250ADepression%2520Detection%2520Network%2520%2528MDD-Net%2529%252C%2520utilizing%2520acoustic%2520and%2520visual%2520data%250Aobtained%2520from%2520social%2520media%2520networks%252C%2520is%2520proposed%2520in%2520this%2520work%2520where%2520mutual%250Atransformers%2520are%2520exploited%2520to%2520efficiently%2520extract%2520and%2520fuse%2520multimodal%2520features%250Afor%2520efficient%2520depression%2520detection.%2520The%2520MDD-Net%2520consists%2520of%2520four%2520core%2520modules%253A%250Aan%2520acoustic%2520feature%2520extraction%2520module%2520for%2520retrieving%2520relevant%2520acoustic%250Aattributes%252C%2520a%2520visual%2520feature%2520extraction%2520module%2520for%2520extracting%2520significant%250Ahigh-level%2520patterns%252C%2520a%2520mutual%2520transformer%2520for%2520computing%2520the%2520correlations%2520among%250Athe%2520generated%2520features%2520and%2520fusing%2520these%2520features%2520from%2520multiple%2520modalities%252C%2520and%250Aa%2520detection%2520layer%2520for%2520detecting%2520depression%2520using%2520the%2520fused%2520feature%250Arepresentations.%2520The%2520extensive%2520experiments%2520are%2520performed%2520using%2520the%2520multimodal%250AD-Vlog%2520dataset%252C%2520and%2520the%2520findings%2520reveal%2520that%2520the%2520developed%2520multimodal%250Adepression%2520detection%2520network%2520surpasses%2520the%2520state-of-the-art%2520by%2520up%2520to%252017.37%2525%2520for%250AF1-Score%252C%2520demonstrating%2520the%2520greater%2520performance%2520of%2520the%2520proposed%2520system.%2520The%250Asource%2520code%2520is%2520accessible%2520at%250Ahttps%253A//github.com/rezwanh001/Multimodal-Depression-Detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.08093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDD-Net%3A%20Multimodal%20Depression%20Detection%20through%20Mutual%20Transformer&entry.906535625=Md%20Rezwanul%20Haque%20and%20Md.%20Milon%20Islam%20and%20S%20M%20Taslim%20Uddin%20Raju%20and%20Hamdi%20Altaheri%20and%20Lobna%20Nassar%20and%20Fakhri%20Karray&entry.1292438233=%20%20Depression%20is%20a%20major%20mental%20health%20condition%20that%20severely%20impacts%20the%0Aemotional%20and%20physical%20well-being%20of%20individuals.%20The%20simple%20nature%20of%20data%0Acollection%20from%20social%20media%20platforms%20has%20attracted%20significant%20interest%20in%0Aproperly%20utilizing%20this%20information%20for%20mental%20health%20research.%20A%20Multimodal%0ADepression%20Detection%20Network%20%28MDD-Net%29%2C%20utilizing%20acoustic%20and%20visual%20data%0Aobtained%20from%20social%20media%20networks%2C%20is%20proposed%20in%20this%20work%20where%20mutual%0Atransformers%20are%20exploited%20to%20efficiently%20extract%20and%20fuse%20multimodal%20features%0Afor%20efficient%20depression%20detection.%20The%20MDD-Net%20consists%20of%20four%20core%20modules%3A%0Aan%20acoustic%20feature%20extraction%20module%20for%20retrieving%20relevant%20acoustic%0Aattributes%2C%20a%20visual%20feature%20extraction%20module%20for%20extracting%20significant%0Ahigh-level%20patterns%2C%20a%20mutual%20transformer%20for%20computing%20the%20correlations%20among%0Athe%20generated%20features%20and%20fusing%20these%20features%20from%20multiple%20modalities%2C%20and%0Aa%20detection%20layer%20for%20detecting%20depression%20using%20the%20fused%20feature%0Arepresentations.%20The%20extensive%20experiments%20are%20performed%20using%20the%20multimodal%0AD-Vlog%20dataset%2C%20and%20the%20findings%20reveal%20that%20the%20developed%20multimodal%0Adepression%20detection%20network%20surpasses%20the%20state-of-the-art%20by%20up%20to%2017.37%25%20for%0AF1-Score%2C%20demonstrating%20the%20greater%20performance%20of%20the%20proposed%20system.%20The%0Asource%20code%20is%20accessible%20at%0Ahttps%3A//github.com/rezwanh001/Multimodal-Depression-Detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.08093v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


