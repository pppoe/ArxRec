<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250918.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic\n  Segmentation", "author": "Bo-Wen Yin and Jiao-Long Cao and Xuying Zhang and Yuming Chen and Ming-Ming Cheng and Qibin Hou", "abstract": "  Recent research on representation learning has proved the merits of\nmulti-modal clues for robust semantic segmentation. Nevertheless, a flexible\npretrain-and-finetune pipeline for multiple visual modalities remains\nunexplored. In this paper, we propose a novel multi-modal learning framework,\ntermed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we\nassemble a large-scale dataset for multi-modal pretraining, called ImageNeXt,\nwhich contains five popular visual modalities. 2) We provide an efficient\npretraining manner to endow the model with the capacity to encode different\nmodality information in the ImageNeXt. For the first time, we introduce a\nuniversal multi-modal pretraining framework that consistently amplifies the\nmodel's perceptual capabilities across various scenarios, regardless of the\narbitrary combination of the involved modalities. Remarkably, our OmniSegmentor\nachieves new state-of-the-art records on a wide range of multi-modal semantic\nsegmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER,\nSUNRGBD, and KITTI-360.\n", "link": "http://arxiv.org/abs/2509.15096v1", "date": "2025-09-18", "relevancy": 3.1296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSegmentor%3A%20A%20Flexible%20Multi-Modal%20Learning%20Framework%20for%20Semantic%0A%20%20Segmentation&body=Title%3A%20OmniSegmentor%3A%20A%20Flexible%20Multi-Modal%20Learning%20Framework%20for%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Bo-Wen%20Yin%20and%20Jiao-Long%20Cao%20and%20Xuying%20Zhang%20and%20Yuming%20Chen%20and%20Ming-Ming%20Cheng%20and%20Qibin%20Hou%0AAbstract%3A%20%20%20Recent%20research%20on%20representation%20learning%20has%20proved%20the%20merits%20of%0Amulti-modal%20clues%20for%20robust%20semantic%20segmentation.%20Nevertheless%2C%20a%20flexible%0Apretrain-and-finetune%20pipeline%20for%20multiple%20visual%20modalities%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20multi-modal%20learning%20framework%2C%0Atermed%20OmniSegmentor.%20It%20has%20two%20key%20innovations%3A%201%29%20Based%20on%20ImageNet%2C%20we%0Aassemble%20a%20large-scale%20dataset%20for%20multi-modal%20pretraining%2C%20called%20ImageNeXt%2C%0Awhich%20contains%20five%20popular%20visual%20modalities.%202%29%20We%20provide%20an%20efficient%0Apretraining%20manner%20to%20endow%20the%20model%20with%20the%20capacity%20to%20encode%20different%0Amodality%20information%20in%20the%20ImageNeXt.%20For%20the%20first%20time%2C%20we%20introduce%20a%0Auniversal%20multi-modal%20pretraining%20framework%20that%20consistently%20amplifies%20the%0Amodel%27s%20perceptual%20capabilities%20across%20various%20scenarios%2C%20regardless%20of%20the%0Aarbitrary%20combination%20of%20the%20involved%20modalities.%20Remarkably%2C%20our%20OmniSegmentor%0Aachieves%20new%20state-of-the-art%20records%20on%20a%20wide%20range%20of%20multi-modal%20semantic%0Asegmentation%20datasets%2C%20including%20NYU%20Depthv2%2C%20EventScape%2C%20MFNet%2C%20DeLiVER%2C%0ASUNRGBD%2C%20and%20KITTI-360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSegmentor%253A%2520A%2520Flexible%2520Multi-Modal%2520Learning%2520Framework%2520for%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DBo-Wen%2520Yin%2520and%2520Jiao-Long%2520Cao%2520and%2520Xuying%2520Zhang%2520and%2520Yuming%2520Chen%2520and%2520Ming-Ming%2520Cheng%2520and%2520Qibin%2520Hou%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520representation%2520learning%2520has%2520proved%2520the%2520merits%2520of%250Amulti-modal%2520clues%2520for%2520robust%2520semantic%2520segmentation.%2520Nevertheless%252C%2520a%2520flexible%250Apretrain-and-finetune%2520pipeline%2520for%2520multiple%2520visual%2520modalities%2520remains%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520learning%2520framework%252C%250Atermed%2520OmniSegmentor.%2520It%2520has%2520two%2520key%2520innovations%253A%25201%2529%2520Based%2520on%2520ImageNet%252C%2520we%250Aassemble%2520a%2520large-scale%2520dataset%2520for%2520multi-modal%2520pretraining%252C%2520called%2520ImageNeXt%252C%250Awhich%2520contains%2520five%2520popular%2520visual%2520modalities.%25202%2529%2520We%2520provide%2520an%2520efficient%250Apretraining%2520manner%2520to%2520endow%2520the%2520model%2520with%2520the%2520capacity%2520to%2520encode%2520different%250Amodality%2520information%2520in%2520the%2520ImageNeXt.%2520For%2520the%2520first%2520time%252C%2520we%2520introduce%2520a%250Auniversal%2520multi-modal%2520pretraining%2520framework%2520that%2520consistently%2520amplifies%2520the%250Amodel%2527s%2520perceptual%2520capabilities%2520across%2520various%2520scenarios%252C%2520regardless%2520of%2520the%250Aarbitrary%2520combination%2520of%2520the%2520involved%2520modalities.%2520Remarkably%252C%2520our%2520OmniSegmentor%250Aachieves%2520new%2520state-of-the-art%2520records%2520on%2520a%2520wide%2520range%2520of%2520multi-modal%2520semantic%250Asegmentation%2520datasets%252C%2520including%2520NYU%2520Depthv2%252C%2520EventScape%252C%2520MFNet%252C%2520DeLiVER%252C%250ASUNRGBD%252C%2520and%2520KITTI-360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSegmentor%3A%20A%20Flexible%20Multi-Modal%20Learning%20Framework%20for%20Semantic%0A%20%20Segmentation&entry.906535625=Bo-Wen%20Yin%20and%20Jiao-Long%20Cao%20and%20Xuying%20Zhang%20and%20Yuming%20Chen%20and%20Ming-Ming%20Cheng%20and%20Qibin%20Hou&entry.1292438233=%20%20Recent%20research%20on%20representation%20learning%20has%20proved%20the%20merits%20of%0Amulti-modal%20clues%20for%20robust%20semantic%20segmentation.%20Nevertheless%2C%20a%20flexible%0Apretrain-and-finetune%20pipeline%20for%20multiple%20visual%20modalities%20remains%0Aunexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20multi-modal%20learning%20framework%2C%0Atermed%20OmniSegmentor.%20It%20has%20two%20key%20innovations%3A%201%29%20Based%20on%20ImageNet%2C%20we%0Aassemble%20a%20large-scale%20dataset%20for%20multi-modal%20pretraining%2C%20called%20ImageNeXt%2C%0Awhich%20contains%20five%20popular%20visual%20modalities.%202%29%20We%20provide%20an%20efficient%0Apretraining%20manner%20to%20endow%20the%20model%20with%20the%20capacity%20to%20encode%20different%0Amodality%20information%20in%20the%20ImageNeXt.%20For%20the%20first%20time%2C%20we%20introduce%20a%0Auniversal%20multi-modal%20pretraining%20framework%20that%20consistently%20amplifies%20the%0Amodel%27s%20perceptual%20capabilities%20across%20various%20scenarios%2C%20regardless%20of%20the%0Aarbitrary%20combination%20of%20the%20involved%20modalities.%20Remarkably%2C%20our%20OmniSegmentor%0Aachieves%20new%20state-of-the-art%20records%20on%20a%20wide%20range%20of%20multi-modal%20semantic%0Asegmentation%20datasets%2C%20including%20NYU%20Depthv2%2C%20EventScape%2C%20MFNet%2C%20DeLiVER%2C%0ASUNRGBD%2C%20and%20KITTI-360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15096v1&entry.124074799=Read"},
{"title": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models", "author": "Matthew Lyle Olson and Musashi Hinck and Neale Ratzlaff and Changbai Li and Phillip Howard and Vasudev Lal and Shao-Yen Tseng", "abstract": "  Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.\n", "link": "http://arxiv.org/abs/2508.11277v2", "date": "2025-09-18", "relevancy": 3.109, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6449}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6449}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Representational%20Power%20of%20Sparse%20Autoencoders%20in%20Vision%0A%20%20Models&body=Title%3A%20Probing%20the%20Representational%20Power%20of%20Sparse%20Autoencoders%20in%20Vision%0A%20%20Models%0AAuthor%3A%20Matthew%20Lyle%20Olson%20and%20Musashi%20Hinck%20and%20Neale%20Ratzlaff%20and%20Changbai%20Li%20and%20Phillip%20Howard%20and%20Vasudev%20Lal%20and%20Shao-Yen%20Tseng%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20popular%20tool%20for%20interpreting%0Athe%20hidden%20states%20of%20large%20language%20models%20%28LLMs%29.%20By%20learning%20to%20reconstruct%0Aactivations%20from%20a%20sparse%20bottleneck%20layer%2C%20SAEs%20discover%20interpretable%0Afeatures%20from%20the%20high-dimensional%20internal%20representations%20of%20LLMs.%20Despite%0Atheir%20popularity%20with%20language%20models%2C%20SAEs%20remain%20understudied%20in%20the%20visual%0Adomain.%20In%20this%20work%2C%20we%20provide%20an%20extensive%20evaluation%20the%20representational%0Apower%20of%20SAEs%20for%20vision%20models%20using%20a%20broad%20range%20of%20image-based%20tasks.%20Our%0Aexperimental%20results%20demonstrate%20that%20SAE%20features%20are%20semantically%20meaningful%2C%0Aimprove%20out-of-distribution%20generalization%2C%20and%20enable%20controllable%20generation%0Aacross%20three%20vision%20model%20architectures%3A%20vision%20embedding%20models%2C%20multi-modal%0ALMMs%20and%20diffusion%20models.%20In%20vision%20embedding%20models%2C%20we%20find%20that%20learned%20SAE%0Afeatures%20can%20be%20used%20for%20OOD%20detection%20and%20provide%20evidence%20that%20they%20recover%0Athe%20ontological%20structure%20of%20the%20underlying%20model.%20For%20diffusion%20models%2C%20we%0Ademonstrate%20that%20SAEs%20enable%20semantic%20steering%20through%20text%20encoder%0Amanipulation%20and%20develop%20an%20automated%20pipeline%20for%20discovering%0Ahuman-interpretable%20attributes.%20Finally%2C%20we%20conduct%20exploratory%20experiments%20on%0Amulti-modal%20LLMs%2C%20finding%20evidence%20that%20SAE%20features%20reveal%20shared%0Arepresentations%20across%20vision%20and%20language%20modalities.%20Our%20study%20provides%20a%0Afoundation%20for%20SAE%20evaluation%20in%20vision%20models%2C%20highlighting%20their%20strong%0Apotential%20improving%20interpretability%2C%20generalization%2C%20and%20steerability%20in%20the%0Avisual%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Representational%2520Power%2520of%2520Sparse%2520Autoencoders%2520in%2520Vision%250A%2520%2520Models%26entry.906535625%3DMatthew%2520Lyle%2520Olson%2520and%2520Musashi%2520Hinck%2520and%2520Neale%2520Ratzlaff%2520and%2520Changbai%2520Li%2520and%2520Phillip%2520Howard%2520and%2520Vasudev%2520Lal%2520and%2520Shao-Yen%2520Tseng%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520popular%2520tool%2520for%2520interpreting%250Athe%2520hidden%2520states%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520By%2520learning%2520to%2520reconstruct%250Aactivations%2520from%2520a%2520sparse%2520bottleneck%2520layer%252C%2520SAEs%2520discover%2520interpretable%250Afeatures%2520from%2520the%2520high-dimensional%2520internal%2520representations%2520of%2520LLMs.%2520Despite%250Atheir%2520popularity%2520with%2520language%2520models%252C%2520SAEs%2520remain%2520understudied%2520in%2520the%2520visual%250Adomain.%2520In%2520this%2520work%252C%2520we%2520provide%2520an%2520extensive%2520evaluation%2520the%2520representational%250Apower%2520of%2520SAEs%2520for%2520vision%2520models%2520using%2520a%2520broad%2520range%2520of%2520image-based%2520tasks.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520SAE%2520features%2520are%2520semantically%2520meaningful%252C%250Aimprove%2520out-of-distribution%2520generalization%252C%2520and%2520enable%2520controllable%2520generation%250Aacross%2520three%2520vision%2520model%2520architectures%253A%2520vision%2520embedding%2520models%252C%2520multi-modal%250ALMMs%2520and%2520diffusion%2520models.%2520In%2520vision%2520embedding%2520models%252C%2520we%2520find%2520that%2520learned%2520SAE%250Afeatures%2520can%2520be%2520used%2520for%2520OOD%2520detection%2520and%2520provide%2520evidence%2520that%2520they%2520recover%250Athe%2520ontological%2520structure%2520of%2520the%2520underlying%2520model.%2520For%2520diffusion%2520models%252C%2520we%250Ademonstrate%2520that%2520SAEs%2520enable%2520semantic%2520steering%2520through%2520text%2520encoder%250Amanipulation%2520and%2520develop%2520an%2520automated%2520pipeline%2520for%2520discovering%250Ahuman-interpretable%2520attributes.%2520Finally%252C%2520we%2520conduct%2520exploratory%2520experiments%2520on%250Amulti-modal%2520LLMs%252C%2520finding%2520evidence%2520that%2520SAE%2520features%2520reveal%2520shared%250Arepresentations%2520across%2520vision%2520and%2520language%2520modalities.%2520Our%2520study%2520provides%2520a%250Afoundation%2520for%2520SAE%2520evaluation%2520in%2520vision%2520models%252C%2520highlighting%2520their%2520strong%250Apotential%2520improving%2520interpretability%252C%2520generalization%252C%2520and%2520steerability%2520in%2520the%250Avisual%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Representational%20Power%20of%20Sparse%20Autoencoders%20in%20Vision%0A%20%20Models&entry.906535625=Matthew%20Lyle%20Olson%20and%20Musashi%20Hinck%20and%20Neale%20Ratzlaff%20and%20Changbai%20Li%20and%20Phillip%20Howard%20and%20Vasudev%20Lal%20and%20Shao-Yen%20Tseng&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20popular%20tool%20for%20interpreting%0Athe%20hidden%20states%20of%20large%20language%20models%20%28LLMs%29.%20By%20learning%20to%20reconstruct%0Aactivations%20from%20a%20sparse%20bottleneck%20layer%2C%20SAEs%20discover%20interpretable%0Afeatures%20from%20the%20high-dimensional%20internal%20representations%20of%20LLMs.%20Despite%0Atheir%20popularity%20with%20language%20models%2C%20SAEs%20remain%20understudied%20in%20the%20visual%0Adomain.%20In%20this%20work%2C%20we%20provide%20an%20extensive%20evaluation%20the%20representational%0Apower%20of%20SAEs%20for%20vision%20models%20using%20a%20broad%20range%20of%20image-based%20tasks.%20Our%0Aexperimental%20results%20demonstrate%20that%20SAE%20features%20are%20semantically%20meaningful%2C%0Aimprove%20out-of-distribution%20generalization%2C%20and%20enable%20controllable%20generation%0Aacross%20three%20vision%20model%20architectures%3A%20vision%20embedding%20models%2C%20multi-modal%0ALMMs%20and%20diffusion%20models.%20In%20vision%20embedding%20models%2C%20we%20find%20that%20learned%20SAE%0Afeatures%20can%20be%20used%20for%20OOD%20detection%20and%20provide%20evidence%20that%20they%20recover%0Athe%20ontological%20structure%20of%20the%20underlying%20model.%20For%20diffusion%20models%2C%20we%0Ademonstrate%20that%20SAEs%20enable%20semantic%20steering%20through%20text%20encoder%0Amanipulation%20and%20develop%20an%20automated%20pipeline%20for%20discovering%0Ahuman-interpretable%20attributes.%20Finally%2C%20we%20conduct%20exploratory%20experiments%20on%0Amulti-modal%20LLMs%2C%20finding%20evidence%20that%20SAE%20features%20reveal%20shared%0Arepresentations%20across%20vision%20and%20language%20modalities.%20Our%20study%20provides%20a%0Afoundation%20for%20SAE%20evaluation%20in%20vision%20models%2C%20highlighting%20their%20strong%0Apotential%20improving%20interpretability%2C%20generalization%2C%20and%20steerability%20in%20the%0Avisual%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11277v2&entry.124074799=Read"},
{"title": "Understand Before You Generate: Self-Guided Training for Autoregressive\n  Image Generation", "author": "Xiaoyu Yue and Zidong Wang and Yuqing Wang and Wenlong Zhang and Xihui Liu and Wanli Ouyang and Lei Bai and Luping Zhou", "abstract": "  Recent studies have demonstrated the importance of high-quality visual\nrepresentations in image generation and have highlighted the limitations of\ngenerative models in image understanding. As a generative paradigm originally\ndesigned for natural language, autoregressive models face similar challenges.\nIn this work, we present the first systematic investigation into the mechanisms\nof applying the next-token prediction paradigm to the visual domain. We\nidentify three key properties that hinder the learning of high-level visual\nsemantics: local and conditional dependence, inter-step semantic inconsistency,\nand spatial invariance deficiency. We show that these issues can be effectively\naddressed by introducing self-supervised objectives during training, leading to\na novel training framework, Self-guided Training for AutoRegressive models\n(ST-AR). Without relying on pre-trained representation models, ST-AR\nsignificantly enhances the image understanding ability of autoregressive models\nand leads to improved generation quality. Specifically, ST-AR brings\napproximately 42% FID improvement for LlamaGen-L and 49% FID improvement for\nLlamaGen-XL, while maintaining the same sampling strategy.\n", "link": "http://arxiv.org/abs/2509.15185v1", "date": "2025-09-18", "relevancy": 2.9961, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6046}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5971}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understand%20Before%20You%20Generate%3A%20Self-Guided%20Training%20for%20Autoregressive%0A%20%20Image%20Generation&body=Title%3A%20Understand%20Before%20You%20Generate%3A%20Self-Guided%20Training%20for%20Autoregressive%0A%20%20Image%20Generation%0AAuthor%3A%20Xiaoyu%20Yue%20and%20Zidong%20Wang%20and%20Yuqing%20Wang%20and%20Wenlong%20Zhang%20and%20Xihui%20Liu%20and%20Wanli%20Ouyang%20and%20Lei%20Bai%20and%20Luping%20Zhou%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20the%20importance%20of%20high-quality%20visual%0Arepresentations%20in%20image%20generation%20and%20have%20highlighted%20the%20limitations%20of%0Agenerative%20models%20in%20image%20understanding.%20As%20a%20generative%20paradigm%20originally%0Adesigned%20for%20natural%20language%2C%20autoregressive%20models%20face%20similar%20challenges.%0AIn%20this%20work%2C%20we%20present%20the%20first%20systematic%20investigation%20into%20the%20mechanisms%0Aof%20applying%20the%20next-token%20prediction%20paradigm%20to%20the%20visual%20domain.%20We%0Aidentify%20three%20key%20properties%20that%20hinder%20the%20learning%20of%20high-level%20visual%0Asemantics%3A%20local%20and%20conditional%20dependence%2C%20inter-step%20semantic%20inconsistency%2C%0Aand%20spatial%20invariance%20deficiency.%20We%20show%20that%20these%20issues%20can%20be%20effectively%0Aaddressed%20by%20introducing%20self-supervised%20objectives%20during%20training%2C%20leading%20to%0Aa%20novel%20training%20framework%2C%20Self-guided%20Training%20for%20AutoRegressive%20models%0A%28ST-AR%29.%20Without%20relying%20on%20pre-trained%20representation%20models%2C%20ST-AR%0Asignificantly%20enhances%20the%20image%20understanding%20ability%20of%20autoregressive%20models%0Aand%20leads%20to%20improved%20generation%20quality.%20Specifically%2C%20ST-AR%20brings%0Aapproximately%2042%25%20FID%20improvement%20for%20LlamaGen-L%20and%2049%25%20FID%20improvement%20for%0ALlamaGen-XL%2C%20while%20maintaining%20the%20same%20sampling%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstand%2520Before%2520You%2520Generate%253A%2520Self-Guided%2520Training%2520for%2520Autoregressive%250A%2520%2520Image%2520Generation%26entry.906535625%3DXiaoyu%2520Yue%2520and%2520Zidong%2520Wang%2520and%2520Yuqing%2520Wang%2520and%2520Wenlong%2520Zhang%2520and%2520Xihui%2520Liu%2520and%2520Wanli%2520Ouyang%2520and%2520Lei%2520Bai%2520and%2520Luping%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520the%2520importance%2520of%2520high-quality%2520visual%250Arepresentations%2520in%2520image%2520generation%2520and%2520have%2520highlighted%2520the%2520limitations%2520of%250Agenerative%2520models%2520in%2520image%2520understanding.%2520As%2520a%2520generative%2520paradigm%2520originally%250Adesigned%2520for%2520natural%2520language%252C%2520autoregressive%2520models%2520face%2520similar%2520challenges.%250AIn%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520systematic%2520investigation%2520into%2520the%2520mechanisms%250Aof%2520applying%2520the%2520next-token%2520prediction%2520paradigm%2520to%2520the%2520visual%2520domain.%2520We%250Aidentify%2520three%2520key%2520properties%2520that%2520hinder%2520the%2520learning%2520of%2520high-level%2520visual%250Asemantics%253A%2520local%2520and%2520conditional%2520dependence%252C%2520inter-step%2520semantic%2520inconsistency%252C%250Aand%2520spatial%2520invariance%2520deficiency.%2520We%2520show%2520that%2520these%2520issues%2520can%2520be%2520effectively%250Aaddressed%2520by%2520introducing%2520self-supervised%2520objectives%2520during%2520training%252C%2520leading%2520to%250Aa%2520novel%2520training%2520framework%252C%2520Self-guided%2520Training%2520for%2520AutoRegressive%2520models%250A%2528ST-AR%2529.%2520Without%2520relying%2520on%2520pre-trained%2520representation%2520models%252C%2520ST-AR%250Asignificantly%2520enhances%2520the%2520image%2520understanding%2520ability%2520of%2520autoregressive%2520models%250Aand%2520leads%2520to%2520improved%2520generation%2520quality.%2520Specifically%252C%2520ST-AR%2520brings%250Aapproximately%252042%2525%2520FID%2520improvement%2520for%2520LlamaGen-L%2520and%252049%2525%2520FID%2520improvement%2520for%250ALlamaGen-XL%252C%2520while%2520maintaining%2520the%2520same%2520sampling%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understand%20Before%20You%20Generate%3A%20Self-Guided%20Training%20for%20Autoregressive%0A%20%20Image%20Generation&entry.906535625=Xiaoyu%20Yue%20and%20Zidong%20Wang%20and%20Yuqing%20Wang%20and%20Wenlong%20Zhang%20and%20Xihui%20Liu%20and%20Wanli%20Ouyang%20and%20Lei%20Bai%20and%20Luping%20Zhou&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20importance%20of%20high-quality%20visual%0Arepresentations%20in%20image%20generation%20and%20have%20highlighted%20the%20limitations%20of%0Agenerative%20models%20in%20image%20understanding.%20As%20a%20generative%20paradigm%20originally%0Adesigned%20for%20natural%20language%2C%20autoregressive%20models%20face%20similar%20challenges.%0AIn%20this%20work%2C%20we%20present%20the%20first%20systematic%20investigation%20into%20the%20mechanisms%0Aof%20applying%20the%20next-token%20prediction%20paradigm%20to%20the%20visual%20domain.%20We%0Aidentify%20three%20key%20properties%20that%20hinder%20the%20learning%20of%20high-level%20visual%0Asemantics%3A%20local%20and%20conditional%20dependence%2C%20inter-step%20semantic%20inconsistency%2C%0Aand%20spatial%20invariance%20deficiency.%20We%20show%20that%20these%20issues%20can%20be%20effectively%0Aaddressed%20by%20introducing%20self-supervised%20objectives%20during%20training%2C%20leading%20to%0Aa%20novel%20training%20framework%2C%20Self-guided%20Training%20for%20AutoRegressive%20models%0A%28ST-AR%29.%20Without%20relying%20on%20pre-trained%20representation%20models%2C%20ST-AR%0Asignificantly%20enhances%20the%20image%20understanding%20ability%20of%20autoregressive%20models%0Aand%20leads%20to%20improved%20generation%20quality.%20Specifically%2C%20ST-AR%20brings%0Aapproximately%2042%25%20FID%20improvement%20for%20LlamaGen-L%20and%2049%25%20FID%20improvement%20for%0ALlamaGen-XL%2C%20while%20maintaining%20the%20same%20sampling%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15185v1&entry.124074799=Read"},
{"title": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music", "author": "Xinran Liu and Xu Dong and Diptesh Kanojia and Wenwu Wang and Zhenhua Feng", "abstract": "  Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation.\n", "link": "http://arxiv.org/abs/2502.18309v2", "date": "2025-09-18", "relevancy": 2.9155, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6148}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5824}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music&body=Title%3A%20GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music%0AAuthor%3A%20Xinran%20Liu%20and%20Xu%20Dong%20and%20Diptesh%20Kanojia%20and%20Wenwu%20Wang%20and%20Zhenhua%20Feng%0AAbstract%3A%20%20%20Generating%20high-quality%20full-body%20dance%20sequences%20from%20music%20is%20a%20challenging%0Atask%20as%20it%20requires%20strict%20adherence%20to%20genre-specific%20choreography.%20Moreover%2C%0Athe%20generated%20sequences%20must%20be%20both%20physically%20realistic%20and%20precisely%0Asynchronized%20with%20the%20beats%20and%20rhythm%20of%20the%20music.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20GCDance%2C%20a%20classifier-free%20diffusion%20framework%20for%0Agenerating%20genre-specific%20dance%20motions%20conditioned%20on%20both%20music%20and%20textual%0Aprompts.%20Specifically%2C%20our%20approach%20extracts%20music%20features%20by%20combining%0Ahigh-level%20pre-trained%20music%20foundation%20model%20features%20with%20hand-crafted%0Afeatures%20for%20multi-granularity%20feature%20fusion.%20To%20achieve%20genre%0Acontrollability%2C%20we%20leverage%20CLIP%20to%20efficiently%20embed%20genre-based%20textual%0Aprompt%20representations%20at%20each%20time%20step%20within%20our%20dance%20generation%20pipeline.%0AOur%20GCDance%20framework%20can%20generate%20diverse%20dance%20styles%20from%20the%20same%20piece%20of%0Amusic%20while%20ensuring%20coherence%20with%20the%20rhythm%20and%20melody%20of%20the%20music.%0AExtensive%20experimental%20results%20obtained%20on%20the%20FineDance%20dataset%20demonstrate%0Athat%20GCDance%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Aapproaches%2C%20which%20also%20achieve%20competitive%20results%20on%20the%20AIST%2B%2B%20dataset.%20Our%0Aablation%20and%20inference%20time%20analysis%20demonstrate%20that%20GCDance%20provides%20an%0Aeffective%20solution%20for%20high-quality%20music-driven%20dance%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCDance%253A%2520Genre-Controlled%25203D%2520Full%2520Body%2520Dance%2520Generation%2520Driven%2520By%2520Music%26entry.906535625%3DXinran%2520Liu%2520and%2520Xu%2520Dong%2520and%2520Diptesh%2520Kanojia%2520and%2520Wenwu%2520Wang%2520and%2520Zhenhua%2520Feng%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520full-body%2520dance%2520sequences%2520from%2520music%2520is%2520a%2520challenging%250Atask%2520as%2520it%2520requires%2520strict%2520adherence%2520to%2520genre-specific%2520choreography.%2520Moreover%252C%250Athe%2520generated%2520sequences%2520must%2520be%2520both%2520physically%2520realistic%2520and%2520precisely%250Asynchronized%2520with%2520the%2520beats%2520and%2520rhythm%2520of%2520the%2520music.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520GCDance%252C%2520a%2520classifier-free%2520diffusion%2520framework%2520for%250Agenerating%2520genre-specific%2520dance%2520motions%2520conditioned%2520on%2520both%2520music%2520and%2520textual%250Aprompts.%2520Specifically%252C%2520our%2520approach%2520extracts%2520music%2520features%2520by%2520combining%250Ahigh-level%2520pre-trained%2520music%2520foundation%2520model%2520features%2520with%2520hand-crafted%250Afeatures%2520for%2520multi-granularity%2520feature%2520fusion.%2520To%2520achieve%2520genre%250Acontrollability%252C%2520we%2520leverage%2520CLIP%2520to%2520efficiently%2520embed%2520genre-based%2520textual%250Aprompt%2520representations%2520at%2520each%2520time%2520step%2520within%2520our%2520dance%2520generation%2520pipeline.%250AOur%2520GCDance%2520framework%2520can%2520generate%2520diverse%2520dance%2520styles%2520from%2520the%2520same%2520piece%2520of%250Amusic%2520while%2520ensuring%2520coherence%2520with%2520the%2520rhythm%2520and%2520melody%2520of%2520the%2520music.%250AExtensive%2520experimental%2520results%2520obtained%2520on%2520the%2520FineDance%2520dataset%2520demonstrate%250Athat%2520GCDance%2520significantly%2520outperforms%2520the%2520existing%2520state-of-the-art%250Aapproaches%252C%2520which%2520also%2520achieve%2520competitive%2520results%2520on%2520the%2520AIST%252B%252B%2520dataset.%2520Our%250Aablation%2520and%2520inference%2520time%2520analysis%2520demonstrate%2520that%2520GCDance%2520provides%2520an%250Aeffective%2520solution%2520for%2520high-quality%2520music-driven%2520dance%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music&entry.906535625=Xinran%20Liu%20and%20Xu%20Dong%20and%20Diptesh%20Kanojia%20and%20Wenwu%20Wang%20and%20Zhenhua%20Feng&entry.1292438233=%20%20Generating%20high-quality%20full-body%20dance%20sequences%20from%20music%20is%20a%20challenging%0Atask%20as%20it%20requires%20strict%20adherence%20to%20genre-specific%20choreography.%20Moreover%2C%0Athe%20generated%20sequences%20must%20be%20both%20physically%20realistic%20and%20precisely%0Asynchronized%20with%20the%20beats%20and%20rhythm%20of%20the%20music.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20GCDance%2C%20a%20classifier-free%20diffusion%20framework%20for%0Agenerating%20genre-specific%20dance%20motions%20conditioned%20on%20both%20music%20and%20textual%0Aprompts.%20Specifically%2C%20our%20approach%20extracts%20music%20features%20by%20combining%0Ahigh-level%20pre-trained%20music%20foundation%20model%20features%20with%20hand-crafted%0Afeatures%20for%20multi-granularity%20feature%20fusion.%20To%20achieve%20genre%0Acontrollability%2C%20we%20leverage%20CLIP%20to%20efficiently%20embed%20genre-based%20textual%0Aprompt%20representations%20at%20each%20time%20step%20within%20our%20dance%20generation%20pipeline.%0AOur%20GCDance%20framework%20can%20generate%20diverse%20dance%20styles%20from%20the%20same%20piece%20of%0Amusic%20while%20ensuring%20coherence%20with%20the%20rhythm%20and%20melody%20of%20the%20music.%0AExtensive%20experimental%20results%20obtained%20on%20the%20FineDance%20dataset%20demonstrate%0Athat%20GCDance%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Aapproaches%2C%20which%20also%20achieve%20competitive%20results%20on%20the%20AIST%2B%2B%20dataset.%20Our%0Aablation%20and%20inference%20time%20analysis%20demonstrate%20that%20GCDance%20provides%20an%0Aeffective%20solution%20for%20high-quality%20music-driven%20dance%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18309v2&entry.124074799=Read"},
{"title": "Dense Video Understanding with Gated Residual Tokenization", "author": "Haichao Zhang and Wenhao Chai and Shwai He and Ang Li and Yun Fu", "abstract": "  High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.\n", "link": "http://arxiv.org/abs/2509.14199v2", "date": "2025-09-18", "relevancy": 2.9063, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization&body=Title%3A%20Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization%0AAuthor%3A%20Haichao%20Zhang%20and%20Wenhao%20Chai%20and%20Shwai%20He%20and%20Ang%20Li%20and%20Yun%20Fu%0AAbstract%3A%20%20%20High%20temporal%20resolution%20is%20essential%20for%20capturing%20fine-grained%20details%20in%0Avideo%20understanding.%20However%2C%20current%20video%20large%20language%20models%20%28VLLMs%29%20and%0Abenchmarks%20mostly%20rely%20on%20low-frame-rate%20sampling%2C%20such%20as%20uniform%20sampling%20or%0Akeyframe%20selection%2C%20discarding%20dense%20temporal%20information.%20This%20compromise%0Aavoids%20the%20high%20cost%20of%20tokenizing%20every%20frame%2C%20which%20otherwise%20leads%20to%0Aredundant%20computation%20and%20linear%20token%20growth%20as%20video%20length%20increases.%20While%0Athis%20trade-off%20works%20for%20slowly%20changing%20content%2C%20it%20fails%20for%20tasks%20like%0Alecture%20comprehension%2C%20where%20information%20appears%20in%20nearly%20every%20frame%20and%0Arequires%20precise%20temporal%20alignment.%20To%20address%20this%20gap%2C%20we%20introduce%20Dense%0AVideo%20Understanding%20%28DVU%29%2C%20which%20enables%20high-FPS%20video%20comprehension%20by%0Areducing%20both%20tokenization%20time%20and%20token%20overhead.%20Existing%20benchmarks%20are%0Aalso%20limited%2C%20as%20their%20QA%20pairs%20focus%20on%20coarse%20content%20changes.%20We%20therefore%0Apropose%20DIVE%20%28Dense%20Information%20Video%20Evaluation%29%2C%20the%20first%20benchmark%20designed%0Afor%20dense%20temporal%20reasoning.%20To%20make%20DVU%20practical%2C%20we%20present%20Gated%20Residual%0ATokenization%20%28GRT%29%2C%20a%20two-stage%20framework%3A%20%281%29%20Motion-Compensated%20Inter-Gated%0ATokenization%20uses%20pixel-level%20motion%20estimation%20to%20skip%20static%20regions%20during%0Atokenization%2C%20achieving%20sub-linear%20growth%20in%20token%20count%20and%20compute.%20%282%29%0ASemantic-Scene%20Intra-Tokenization%20Merging%20fuses%20tokens%20across%20static%20regions%0Awithin%20a%20scene%2C%20further%20reducing%20redundancy%20while%20preserving%20dynamic%20semantics.%0AExperiments%20on%20DIVE%20show%20that%20GRT%20outperforms%20larger%20VLLM%20baselines%20and%20scales%0Apositively%20with%20FPS.%20These%20results%20highlight%20the%20importance%20of%20dense%20temporal%0Ainformation%20and%20demonstrate%20that%20GRT%20enables%20efficient%2C%20scalable%20high-FPS%20video%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14199v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Video%2520Understanding%2520with%2520Gated%2520Residual%2520Tokenization%26entry.906535625%3DHaichao%2520Zhang%2520and%2520Wenhao%2520Chai%2520and%2520Shwai%2520He%2520and%2520Ang%2520Li%2520and%2520Yun%2520Fu%26entry.1292438233%3D%2520%2520High%2520temporal%2520resolution%2520is%2520essential%2520for%2520capturing%2520fine-grained%2520details%2520in%250Avideo%2520understanding.%2520However%252C%2520current%2520video%2520large%2520language%2520models%2520%2528VLLMs%2529%2520and%250Abenchmarks%2520mostly%2520rely%2520on%2520low-frame-rate%2520sampling%252C%2520such%2520as%2520uniform%2520sampling%2520or%250Akeyframe%2520selection%252C%2520discarding%2520dense%2520temporal%2520information.%2520This%2520compromise%250Aavoids%2520the%2520high%2520cost%2520of%2520tokenizing%2520every%2520frame%252C%2520which%2520otherwise%2520leads%2520to%250Aredundant%2520computation%2520and%2520linear%2520token%2520growth%2520as%2520video%2520length%2520increases.%2520While%250Athis%2520trade-off%2520works%2520for%2520slowly%2520changing%2520content%252C%2520it%2520fails%2520for%2520tasks%2520like%250Alecture%2520comprehension%252C%2520where%2520information%2520appears%2520in%2520nearly%2520every%2520frame%2520and%250Arequires%2520precise%2520temporal%2520alignment.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Dense%250AVideo%2520Understanding%2520%2528DVU%2529%252C%2520which%2520enables%2520high-FPS%2520video%2520comprehension%2520by%250Areducing%2520both%2520tokenization%2520time%2520and%2520token%2520overhead.%2520Existing%2520benchmarks%2520are%250Aalso%2520limited%252C%2520as%2520their%2520QA%2520pairs%2520focus%2520on%2520coarse%2520content%2520changes.%2520We%2520therefore%250Apropose%2520DIVE%2520%2528Dense%2520Information%2520Video%2520Evaluation%2529%252C%2520the%2520first%2520benchmark%2520designed%250Afor%2520dense%2520temporal%2520reasoning.%2520To%2520make%2520DVU%2520practical%252C%2520we%2520present%2520Gated%2520Residual%250ATokenization%2520%2528GRT%2529%252C%2520a%2520two-stage%2520framework%253A%2520%25281%2529%2520Motion-Compensated%2520Inter-Gated%250ATokenization%2520uses%2520pixel-level%2520motion%2520estimation%2520to%2520skip%2520static%2520regions%2520during%250Atokenization%252C%2520achieving%2520sub-linear%2520growth%2520in%2520token%2520count%2520and%2520compute.%2520%25282%2529%250ASemantic-Scene%2520Intra-Tokenization%2520Merging%2520fuses%2520tokens%2520across%2520static%2520regions%250Awithin%2520a%2520scene%252C%2520further%2520reducing%2520redundancy%2520while%2520preserving%2520dynamic%2520semantics.%250AExperiments%2520on%2520DIVE%2520show%2520that%2520GRT%2520outperforms%2520larger%2520VLLM%2520baselines%2520and%2520scales%250Apositively%2520with%2520FPS.%2520These%2520results%2520highlight%2520the%2520importance%2520of%2520dense%2520temporal%250Ainformation%2520and%2520demonstrate%2520that%2520GRT%2520enables%2520efficient%252C%2520scalable%2520high-FPS%2520video%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14199v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Video%20Understanding%20with%20Gated%20Residual%20Tokenization&entry.906535625=Haichao%20Zhang%20and%20Wenhao%20Chai%20and%20Shwai%20He%20and%20Ang%20Li%20and%20Yun%20Fu&entry.1292438233=%20%20High%20temporal%20resolution%20is%20essential%20for%20capturing%20fine-grained%20details%20in%0Avideo%20understanding.%20However%2C%20current%20video%20large%20language%20models%20%28VLLMs%29%20and%0Abenchmarks%20mostly%20rely%20on%20low-frame-rate%20sampling%2C%20such%20as%20uniform%20sampling%20or%0Akeyframe%20selection%2C%20discarding%20dense%20temporal%20information.%20This%20compromise%0Aavoids%20the%20high%20cost%20of%20tokenizing%20every%20frame%2C%20which%20otherwise%20leads%20to%0Aredundant%20computation%20and%20linear%20token%20growth%20as%20video%20length%20increases.%20While%0Athis%20trade-off%20works%20for%20slowly%20changing%20content%2C%20it%20fails%20for%20tasks%20like%0Alecture%20comprehension%2C%20where%20information%20appears%20in%20nearly%20every%20frame%20and%0Arequires%20precise%20temporal%20alignment.%20To%20address%20this%20gap%2C%20we%20introduce%20Dense%0AVideo%20Understanding%20%28DVU%29%2C%20which%20enables%20high-FPS%20video%20comprehension%20by%0Areducing%20both%20tokenization%20time%20and%20token%20overhead.%20Existing%20benchmarks%20are%0Aalso%20limited%2C%20as%20their%20QA%20pairs%20focus%20on%20coarse%20content%20changes.%20We%20therefore%0Apropose%20DIVE%20%28Dense%20Information%20Video%20Evaluation%29%2C%20the%20first%20benchmark%20designed%0Afor%20dense%20temporal%20reasoning.%20To%20make%20DVU%20practical%2C%20we%20present%20Gated%20Residual%0ATokenization%20%28GRT%29%2C%20a%20two-stage%20framework%3A%20%281%29%20Motion-Compensated%20Inter-Gated%0ATokenization%20uses%20pixel-level%20motion%20estimation%20to%20skip%20static%20regions%20during%0Atokenization%2C%20achieving%20sub-linear%20growth%20in%20token%20count%20and%20compute.%20%282%29%0ASemantic-Scene%20Intra-Tokenization%20Merging%20fuses%20tokens%20across%20static%20regions%0Awithin%20a%20scene%2C%20further%20reducing%20redundancy%20while%20preserving%20dynamic%20semantics.%0AExperiments%20on%20DIVE%20show%20that%20GRT%20outperforms%20larger%20VLLM%20baselines%20and%20scales%0Apositively%20with%20FPS.%20These%20results%20highlight%20the%20importance%20of%20dense%20temporal%0Ainformation%20and%20demonstrate%20that%20GRT%20enables%20efficient%2C%20scalable%20high-FPS%20video%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14199v2&entry.124074799=Read"},
{"title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking\n  in VLMs", "author": "Asif Azad and Mohammad Sadat Hossain and MD Sadik Hossain Shanto and M Saifur Rahman and Md Rizwan Parvez", "abstract": "  Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.\n", "link": "http://arxiv.org/abs/2509.13379v2", "date": "2025-09-18", "relevancy": 2.8927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5845}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Art%20of%20Saying%20%22Maybe%22%3A%20A%20Conformal%20Lens%20for%20Uncertainty%20Benchmarking%0A%20%20in%20VLMs&body=Title%3A%20The%20Art%20of%20Saying%20%22Maybe%22%3A%20A%20Conformal%20Lens%20for%20Uncertainty%20Benchmarking%0A%20%20in%20VLMs%0AAuthor%3A%20Asif%20Azad%20and%20Mohammad%20Sadat%20Hossain%20and%20MD%20Sadik%20Hossain%20Shanto%20and%20M%20Saifur%20Rahman%20and%20Md%20Rizwan%20Parvez%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%20in%20complex%0Avisual%20understanding%20across%20scientific%20and%20reasoning%20tasks.%20While%20performance%0Abenchmarking%20has%20advanced%20our%20understanding%20of%20these%20capabilities%2C%20the%20critical%0Adimension%20of%20uncertainty%20quantification%20has%20received%20insufficient%20attention.%0ATherefore%2C%20unlike%20prior%20conformal%20prediction%20studies%20that%20focused%20on%20limited%0Asettings%2C%20we%20conduct%20a%20comprehensive%20uncertainty%20benchmarking%20study%2C%20evaluating%0A16%20state-of-the-art%20VLMs%20%28open%20and%20closed-source%29%20across%206%20multimodal%20datasets%0Awith%203%20distinct%20scoring%20functions.%20Our%20findings%20demonstrate%20that%20larger%20models%0Aconsistently%20exhibit%20better%20uncertainty%20quantification%3B%20models%20that%20know%20more%0Aalso%20know%20better%20what%20they%20don%27t%20know.%20More%20certain%20models%20achieve%20higher%0Aaccuracy%2C%20while%20mathematical%20and%20reasoning%20tasks%20elicit%20poorer%20uncertainty%0Aperformance%20across%20all%20models%20compared%20to%20other%20domains.%20This%20work%20establishes%0Aa%20foundation%20for%20reliable%20uncertainty%20evaluation%20in%20multimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Art%2520of%2520Saying%2520%2522Maybe%2522%253A%2520A%2520Conformal%2520Lens%2520for%2520Uncertainty%2520Benchmarking%250A%2520%2520in%2520VLMs%26entry.906535625%3DAsif%2520Azad%2520and%2520Mohammad%2520Sadat%2520Hossain%2520and%2520MD%2520Sadik%2520Hossain%2520Shanto%2520and%2520M%2520Saifur%2520Rahman%2520and%2520Md%2520Rizwan%2520Parvez%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520complex%250Avisual%2520understanding%2520across%2520scientific%2520and%2520reasoning%2520tasks.%2520While%2520performance%250Abenchmarking%2520has%2520advanced%2520our%2520understanding%2520of%2520these%2520capabilities%252C%2520the%2520critical%250Adimension%2520of%2520uncertainty%2520quantification%2520has%2520received%2520insufficient%2520attention.%250ATherefore%252C%2520unlike%2520prior%2520conformal%2520prediction%2520studies%2520that%2520focused%2520on%2520limited%250Asettings%252C%2520we%2520conduct%2520a%2520comprehensive%2520uncertainty%2520benchmarking%2520study%252C%2520evaluating%250A16%2520state-of-the-art%2520VLMs%2520%2528open%2520and%2520closed-source%2529%2520across%25206%2520multimodal%2520datasets%250Awith%25203%2520distinct%2520scoring%2520functions.%2520Our%2520findings%2520demonstrate%2520that%2520larger%2520models%250Aconsistently%2520exhibit%2520better%2520uncertainty%2520quantification%253B%2520models%2520that%2520know%2520more%250Aalso%2520know%2520better%2520what%2520they%2520don%2527t%2520know.%2520More%2520certain%2520models%2520achieve%2520higher%250Aaccuracy%252C%2520while%2520mathematical%2520and%2520reasoning%2520tasks%2520elicit%2520poorer%2520uncertainty%250Aperformance%2520across%2520all%2520models%2520compared%2520to%2520other%2520domains.%2520This%2520work%2520establishes%250Aa%2520foundation%2520for%2520reliable%2520uncertainty%2520evaluation%2520in%2520multimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Art%20of%20Saying%20%22Maybe%22%3A%20A%20Conformal%20Lens%20for%20Uncertainty%20Benchmarking%0A%20%20in%20VLMs&entry.906535625=Asif%20Azad%20and%20Mohammad%20Sadat%20Hossain%20and%20MD%20Sadik%20Hossain%20Shanto%20and%20M%20Saifur%20Rahman%20and%20Md%20Rizwan%20Parvez&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%20in%20complex%0Avisual%20understanding%20across%20scientific%20and%20reasoning%20tasks.%20While%20performance%0Abenchmarking%20has%20advanced%20our%20understanding%20of%20these%20capabilities%2C%20the%20critical%0Adimension%20of%20uncertainty%20quantification%20has%20received%20insufficient%20attention.%0ATherefore%2C%20unlike%20prior%20conformal%20prediction%20studies%20that%20focused%20on%20limited%0Asettings%2C%20we%20conduct%20a%20comprehensive%20uncertainty%20benchmarking%20study%2C%20evaluating%0A16%20state-of-the-art%20VLMs%20%28open%20and%20closed-source%29%20across%206%20multimodal%20datasets%0Awith%203%20distinct%20scoring%20functions.%20Our%20findings%20demonstrate%20that%20larger%20models%0Aconsistently%20exhibit%20better%20uncertainty%20quantification%3B%20models%20that%20know%20more%0Aalso%20know%20better%20what%20they%20don%27t%20know.%20More%20certain%20models%20achieve%20higher%0Aaccuracy%2C%20while%20mathematical%20and%20reasoning%20tasks%20elicit%20poorer%20uncertainty%0Aperformance%20across%20all%20models%20compared%20to%20other%20domains.%20This%20work%20establishes%0Aa%20foundation%20for%20reliable%20uncertainty%20evaluation%20in%20multimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13379v2&entry.124074799=Read"},
{"title": "Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned\n  for Biometric Applications", "author": "Tahar Chettaoui and Naser Damer and Fadi Boutros", "abstract": "  Foundation models such as CLIP have demonstrated exceptional zero- and\nfew-shot transfer capabilities across diverse vision tasks. However, when\nfine-tuned for highly specialized biometric tasks, face recognition (FR),\nmorphing attack detection (MAD), and presentation attack detection (PAD), these\nmodels may suffer from over-specialization. Thus, they may lose one of their\nfoundational strengths, cross-domain generalization. In this work, we\nsystematically quantify these trade-offs by evaluating three instances of CLIP\nfine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the\noriginal CLIP baseline on 14 general vision datasets under zero-shot and\nlinear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our\nresults indicate that fine-tuned models suffer from over-specialization,\nespecially when fine-tuned for complex tasks of FR. Also, our results pointed\nout that task complexity and classification head design, multi-class (FR) vs.\nbinary (MAD and PAD), correlate with the degree of catastrophic forgetting. The\nFRoundation model with the ViT-L backbone outperforms other approaches on the\nlarge-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%.\nHowever, it experiences a substantial performance drop on ImageNetV2, reaching\nonly 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover,\nthe larger CLIP architecture consistently preserves more of the model's\noriginal generalization ability than the smaller variant, indicating that\nincreased model capacity may help mitigate over-specialization.\n", "link": "http://arxiv.org/abs/2509.14921v1", "date": "2025-09-18", "relevancy": 2.8817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trade-offs%20in%20Cross-Domain%20Generalization%20of%20Foundation%20Model%20Fine-Tuned%0A%20%20for%20Biometric%20Applications&body=Title%3A%20Trade-offs%20in%20Cross-Domain%20Generalization%20of%20Foundation%20Model%20Fine-Tuned%0A%20%20for%20Biometric%20Applications%0AAuthor%3A%20Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros%0AAbstract%3A%20%20%20Foundation%20models%20such%20as%20CLIP%20have%20demonstrated%20exceptional%20zero-%20and%0Afew-shot%20transfer%20capabilities%20across%20diverse%20vision%20tasks.%20However%2C%20when%0Afine-tuned%20for%20highly%20specialized%20biometric%20tasks%2C%20face%20recognition%20%28FR%29%2C%0Amorphing%20attack%20detection%20%28MAD%29%2C%20and%20presentation%20attack%20detection%20%28PAD%29%2C%20these%0Amodels%20may%20suffer%20from%20over-specialization.%20Thus%2C%20they%20may%20lose%20one%20of%20their%0Afoundational%20strengths%2C%20cross-domain%20generalization.%20In%20this%20work%2C%20we%0Asystematically%20quantify%20these%20trade-offs%20by%20evaluating%20three%20instances%20of%20CLIP%0Afine-tuned%20for%20FR%2C%20MAD%2C%20and%20PAD.%20We%20evaluate%20each%20adapted%20model%20as%20well%20as%20the%0Aoriginal%20CLIP%20baseline%20on%2014%20general%20vision%20datasets%20under%20zero-shot%20and%0Alinear-probe%20protocols%2C%20alongside%20common%20FR%2C%20MAD%2C%20and%20PAD%20benchmarks.%20Our%0Aresults%20indicate%20that%20fine-tuned%20models%20suffer%20from%20over-specialization%2C%0Aespecially%20when%20fine-tuned%20for%20complex%20tasks%20of%20FR.%20Also%2C%20our%20results%20pointed%0Aout%20that%20task%20complexity%20and%20classification%20head%20design%2C%20multi-class%20%28FR%29%20vs.%0Abinary%20%28MAD%20and%20PAD%29%2C%20correlate%20with%20the%20degree%20of%20catastrophic%20forgetting.%20The%0AFRoundation%20model%20with%20the%20ViT-L%20backbone%20outperforms%20other%20approaches%20on%20the%0Alarge-scale%20FR%20benchmark%20IJB-C%2C%20achieving%20an%20improvement%20of%20up%20to%2058.52%25.%0AHowever%2C%20it%20experiences%20a%20substantial%20performance%20drop%20on%20ImageNetV2%2C%20reaching%0Aonly%2051.63%25%20compared%20to%2069.84%25%20achieved%20by%20the%20baseline%20CLIP%20model.%20Moreover%2C%0Athe%20larger%20CLIP%20architecture%20consistently%20preserves%20more%20of%20the%20model%27s%0Aoriginal%20generalization%20ability%20than%20the%20smaller%20variant%2C%20indicating%20that%0Aincreased%20model%20capacity%20may%20help%20mitigate%20over-specialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrade-offs%2520in%2520Cross-Domain%2520Generalization%2520of%2520Foundation%2520Model%2520Fine-Tuned%250A%2520%2520for%2520Biometric%2520Applications%26entry.906535625%3DTahar%2520Chettaoui%2520and%2520Naser%2520Damer%2520and%2520Fadi%2520Boutros%26entry.1292438233%3D%2520%2520Foundation%2520models%2520such%2520as%2520CLIP%2520have%2520demonstrated%2520exceptional%2520zero-%2520and%250Afew-shot%2520transfer%2520capabilities%2520across%2520diverse%2520vision%2520tasks.%2520However%252C%2520when%250Afine-tuned%2520for%2520highly%2520specialized%2520biometric%2520tasks%252C%2520face%2520recognition%2520%2528FR%2529%252C%250Amorphing%2520attack%2520detection%2520%2528MAD%2529%252C%2520and%2520presentation%2520attack%2520detection%2520%2528PAD%2529%252C%2520these%250Amodels%2520may%2520suffer%2520from%2520over-specialization.%2520Thus%252C%2520they%2520may%2520lose%2520one%2520of%2520their%250Afoundational%2520strengths%252C%2520cross-domain%2520generalization.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520quantify%2520these%2520trade-offs%2520by%2520evaluating%2520three%2520instances%2520of%2520CLIP%250Afine-tuned%2520for%2520FR%252C%2520MAD%252C%2520and%2520PAD.%2520We%2520evaluate%2520each%2520adapted%2520model%2520as%2520well%2520as%2520the%250Aoriginal%2520CLIP%2520baseline%2520on%252014%2520general%2520vision%2520datasets%2520under%2520zero-shot%2520and%250Alinear-probe%2520protocols%252C%2520alongside%2520common%2520FR%252C%2520MAD%252C%2520and%2520PAD%2520benchmarks.%2520Our%250Aresults%2520indicate%2520that%2520fine-tuned%2520models%2520suffer%2520from%2520over-specialization%252C%250Aespecially%2520when%2520fine-tuned%2520for%2520complex%2520tasks%2520of%2520FR.%2520Also%252C%2520our%2520results%2520pointed%250Aout%2520that%2520task%2520complexity%2520and%2520classification%2520head%2520design%252C%2520multi-class%2520%2528FR%2529%2520vs.%250Abinary%2520%2528MAD%2520and%2520PAD%2529%252C%2520correlate%2520with%2520the%2520degree%2520of%2520catastrophic%2520forgetting.%2520The%250AFRoundation%2520model%2520with%2520the%2520ViT-L%2520backbone%2520outperforms%2520other%2520approaches%2520on%2520the%250Alarge-scale%2520FR%2520benchmark%2520IJB-C%252C%2520achieving%2520an%2520improvement%2520of%2520up%2520to%252058.52%2525.%250AHowever%252C%2520it%2520experiences%2520a%2520substantial%2520performance%2520drop%2520on%2520ImageNetV2%252C%2520reaching%250Aonly%252051.63%2525%2520compared%2520to%252069.84%2525%2520achieved%2520by%2520the%2520baseline%2520CLIP%2520model.%2520Moreover%252C%250Athe%2520larger%2520CLIP%2520architecture%2520consistently%2520preserves%2520more%2520of%2520the%2520model%2527s%250Aoriginal%2520generalization%2520ability%2520than%2520the%2520smaller%2520variant%252C%2520indicating%2520that%250Aincreased%2520model%2520capacity%2520may%2520help%2520mitigate%2520over-specialization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trade-offs%20in%20Cross-Domain%20Generalization%20of%20Foundation%20Model%20Fine-Tuned%0A%20%20for%20Biometric%20Applications&entry.906535625=Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros&entry.1292438233=%20%20Foundation%20models%20such%20as%20CLIP%20have%20demonstrated%20exceptional%20zero-%20and%0Afew-shot%20transfer%20capabilities%20across%20diverse%20vision%20tasks.%20However%2C%20when%0Afine-tuned%20for%20highly%20specialized%20biometric%20tasks%2C%20face%20recognition%20%28FR%29%2C%0Amorphing%20attack%20detection%20%28MAD%29%2C%20and%20presentation%20attack%20detection%20%28PAD%29%2C%20these%0Amodels%20may%20suffer%20from%20over-specialization.%20Thus%2C%20they%20may%20lose%20one%20of%20their%0Afoundational%20strengths%2C%20cross-domain%20generalization.%20In%20this%20work%2C%20we%0Asystematically%20quantify%20these%20trade-offs%20by%20evaluating%20three%20instances%20of%20CLIP%0Afine-tuned%20for%20FR%2C%20MAD%2C%20and%20PAD.%20We%20evaluate%20each%20adapted%20model%20as%20well%20as%20the%0Aoriginal%20CLIP%20baseline%20on%2014%20general%20vision%20datasets%20under%20zero-shot%20and%0Alinear-probe%20protocols%2C%20alongside%20common%20FR%2C%20MAD%2C%20and%20PAD%20benchmarks.%20Our%0Aresults%20indicate%20that%20fine-tuned%20models%20suffer%20from%20over-specialization%2C%0Aespecially%20when%20fine-tuned%20for%20complex%20tasks%20of%20FR.%20Also%2C%20our%20results%20pointed%0Aout%20that%20task%20complexity%20and%20classification%20head%20design%2C%20multi-class%20%28FR%29%20vs.%0Abinary%20%28MAD%20and%20PAD%29%2C%20correlate%20with%20the%20degree%20of%20catastrophic%20forgetting.%20The%0AFRoundation%20model%20with%20the%20ViT-L%20backbone%20outperforms%20other%20approaches%20on%20the%0Alarge-scale%20FR%20benchmark%20IJB-C%2C%20achieving%20an%20improvement%20of%20up%20to%2058.52%25.%0AHowever%2C%20it%20experiences%20a%20substantial%20performance%20drop%20on%20ImageNetV2%2C%20reaching%0Aonly%2051.63%25%20compared%20to%2069.84%25%20achieved%20by%20the%20baseline%20CLIP%20model.%20Moreover%2C%0Athe%20larger%20CLIP%20architecture%20consistently%20preserves%20more%20of%20the%20model%27s%0Aoriginal%20generalization%20ability%20than%20the%20smaller%20variant%2C%20indicating%20that%0Aincreased%20model%20capacity%20may%20help%20mitigate%20over-specialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14921v1&entry.124074799=Read"},
{"title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for\n  Realistic Underwater Image Generation", "author": "Vasiliki Ismiroglou and Malte Pedersen and Stefan H. Bengtson and Andreas Aakerberg and Thomas B. Moeslund", "abstract": "  In recent years, the underwater image formation model has found extensive use\nin the generation of synthetic underwater data. Although many approaches focus\non scenes primarily affected by discoloration, they often overlook the model's\nability to capture the complex, distance-dependent visibility loss present in\nhighly turbid environments. In this work, we propose an improved synthetic data\ngeneration pipeline that includes the commonly omitted forward scattering term,\nwhile also considering a nonuniform medium. Additionally, we collected the\nBUCKET dataset under controlled turbidity conditions to acquire real turbid\nfootage with the corresponding reference images. Our results demonstrate\nqualitative improvements over the reference model, particularly under\nincreasing turbidity, with a selection rate of 82. 5\\% by survey participants.\nData and code can be accessed on the project page:\nvap.aau.dk/sea-ing-through-scattered-rays.\n", "link": "http://arxiv.org/abs/2509.15011v1", "date": "2025-09-18", "relevancy": 2.8603, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5797}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5797}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sea-ing%20Through%20Scattered%20Rays%3A%20Revisiting%20the%20Image%20Formation%20Model%20for%0A%20%20Realistic%20Underwater%20Image%20Generation&body=Title%3A%20Sea-ing%20Through%20Scattered%20Rays%3A%20Revisiting%20the%20Image%20Formation%20Model%20for%0A%20%20Realistic%20Underwater%20Image%20Generation%0AAuthor%3A%20Vasiliki%20Ismiroglou%20and%20Malte%20Pedersen%20and%20Stefan%20H.%20Bengtson%20and%20Andreas%20Aakerberg%20and%20Thomas%20B.%20Moeslund%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20underwater%20image%20formation%20model%20has%20found%20extensive%20use%0Ain%20the%20generation%20of%20synthetic%20underwater%20data.%20Although%20many%20approaches%20focus%0Aon%20scenes%20primarily%20affected%20by%20discoloration%2C%20they%20often%20overlook%20the%20model%27s%0Aability%20to%20capture%20the%20complex%2C%20distance-dependent%20visibility%20loss%20present%20in%0Ahighly%20turbid%20environments.%20In%20this%20work%2C%20we%20propose%20an%20improved%20synthetic%20data%0Ageneration%20pipeline%20that%20includes%20the%20commonly%20omitted%20forward%20scattering%20term%2C%0Awhile%20also%20considering%20a%20nonuniform%20medium.%20Additionally%2C%20we%20collected%20the%0ABUCKET%20dataset%20under%20controlled%20turbidity%20conditions%20to%20acquire%20real%20turbid%0Afootage%20with%20the%20corresponding%20reference%20images.%20Our%20results%20demonstrate%0Aqualitative%20improvements%20over%20the%20reference%20model%2C%20particularly%20under%0Aincreasing%20turbidity%2C%20with%20a%20selection%20rate%20of%2082.%205%5C%25%20by%20survey%20participants.%0AData%20and%20code%20can%20be%20accessed%20on%20the%20project%20page%3A%0Avap.aau.dk/sea-ing-through-scattered-rays.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSea-ing%2520Through%2520Scattered%2520Rays%253A%2520Revisiting%2520the%2520Image%2520Formation%2520Model%2520for%250A%2520%2520Realistic%2520Underwater%2520Image%2520Generation%26entry.906535625%3DVasiliki%2520Ismiroglou%2520and%2520Malte%2520Pedersen%2520and%2520Stefan%2520H.%2520Bengtson%2520and%2520Andreas%2520Aakerberg%2520and%2520Thomas%2520B.%2520Moeslund%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520underwater%2520image%2520formation%2520model%2520has%2520found%2520extensive%2520use%250Ain%2520the%2520generation%2520of%2520synthetic%2520underwater%2520data.%2520Although%2520many%2520approaches%2520focus%250Aon%2520scenes%2520primarily%2520affected%2520by%2520discoloration%252C%2520they%2520often%2520overlook%2520the%2520model%2527s%250Aability%2520to%2520capture%2520the%2520complex%252C%2520distance-dependent%2520visibility%2520loss%2520present%2520in%250Ahighly%2520turbid%2520environments.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520improved%2520synthetic%2520data%250Ageneration%2520pipeline%2520that%2520includes%2520the%2520commonly%2520omitted%2520forward%2520scattering%2520term%252C%250Awhile%2520also%2520considering%2520a%2520nonuniform%2520medium.%2520Additionally%252C%2520we%2520collected%2520the%250ABUCKET%2520dataset%2520under%2520controlled%2520turbidity%2520conditions%2520to%2520acquire%2520real%2520turbid%250Afootage%2520with%2520the%2520corresponding%2520reference%2520images.%2520Our%2520results%2520demonstrate%250Aqualitative%2520improvements%2520over%2520the%2520reference%2520model%252C%2520particularly%2520under%250Aincreasing%2520turbidity%252C%2520with%2520a%2520selection%2520rate%2520of%252082.%25205%255C%2525%2520by%2520survey%2520participants.%250AData%2520and%2520code%2520can%2520be%2520accessed%2520on%2520the%2520project%2520page%253A%250Avap.aau.dk/sea-ing-through-scattered-rays.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sea-ing%20Through%20Scattered%20Rays%3A%20Revisiting%20the%20Image%20Formation%20Model%20for%0A%20%20Realistic%20Underwater%20Image%20Generation&entry.906535625=Vasiliki%20Ismiroglou%20and%20Malte%20Pedersen%20and%20Stefan%20H.%20Bengtson%20and%20Andreas%20Aakerberg%20and%20Thomas%20B.%20Moeslund&entry.1292438233=%20%20In%20recent%20years%2C%20the%20underwater%20image%20formation%20model%20has%20found%20extensive%20use%0Ain%20the%20generation%20of%20synthetic%20underwater%20data.%20Although%20many%20approaches%20focus%0Aon%20scenes%20primarily%20affected%20by%20discoloration%2C%20they%20often%20overlook%20the%20model%27s%0Aability%20to%20capture%20the%20complex%2C%20distance-dependent%20visibility%20loss%20present%20in%0Ahighly%20turbid%20environments.%20In%20this%20work%2C%20we%20propose%20an%20improved%20synthetic%20data%0Ageneration%20pipeline%20that%20includes%20the%20commonly%20omitted%20forward%20scattering%20term%2C%0Awhile%20also%20considering%20a%20nonuniform%20medium.%20Additionally%2C%20we%20collected%20the%0ABUCKET%20dataset%20under%20controlled%20turbidity%20conditions%20to%20acquire%20real%20turbid%0Afootage%20with%20the%20corresponding%20reference%20images.%20Our%20results%20demonstrate%0Aqualitative%20improvements%20over%20the%20reference%20model%2C%20particularly%20under%0Aincreasing%20turbidity%2C%20with%20a%20selection%20rate%20of%2082.%205%5C%25%20by%20survey%20participants.%0AData%20and%20code%20can%20be%20accessed%20on%20the%20project%20page%3A%0Avap.aau.dk/sea-ing-through-scattered-rays.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15011v1&entry.124074799=Read"},
{"title": "MovieCORE: COgnitive REasoning in Movies", "author": "Gueter Josmy Faure and Min-Hung Chen and Jia-Fong Yeh and Ying Cheng and Hung-Ting Su and Yung-Hao Tang and Shang-Hong Lai and Winston H. Hsu", "abstract": "  This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.\n", "link": "http://arxiv.org/abs/2508.19026v3", "date": "2025-09-18", "relevancy": 2.8111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MovieCORE%3A%20COgnitive%20REasoning%20in%20Movies&body=Title%3A%20MovieCORE%3A%20COgnitive%20REasoning%20in%20Movies%0AAuthor%3A%20Gueter%20Josmy%20Faure%20and%20Min-Hung%20Chen%20and%20Jia-Fong%20Yeh%20and%20Ying%20Cheng%20and%20Hung-Ting%20Su%20and%20Yung-Hao%20Tang%20and%20Shang-Hong%20Lai%20and%20Winston%20H.%20Hsu%0AAbstract%3A%20%20%20This%20paper%20introduces%20MovieCORE%2C%20a%20novel%20video%20question%20answering%20%28VQA%29%0Adataset%20designed%20to%20probe%20deeper%20cognitive%20understanding%20of%20movie%20content.%0AUnlike%20existing%20datasets%20that%20focus%20on%20surface-level%20comprehension%2C%20MovieCORE%0Aemphasizes%20questions%20that%20engage%20System-2%20thinking%20while%20remaining%20specific%20to%0Athe%20video%20material.%20We%20present%20an%20innovative%20agentic%20brainstorming%20approach%2C%0Autilizing%20multiple%20large%20language%20models%20%28LLMs%29%20as%20thought%20agents%20to%20generate%0Aand%20refine%20high-quality%20question-answer%20pairs.%20To%20evaluate%20dataset%20quality%2C%20we%0Adevelop%20a%20set%20of%20cognitive%20tests%20assessing%20depth%2C%20thought-provocation%0Apotential%2C%20and%20syntactic%20complexity.%20We%20also%20propose%20a%20comprehensive%20evaluation%0Ascheme%20for%20assessing%20VQA%20model%20performance%20on%20deeper%20cognitive%20tasks.%20To%0Aaddress%20the%20limitations%20of%20existing%20video-language%20models%20%28VLMs%29%2C%20we%20introduce%0Aan%20agentic%20enhancement%20module%2C%20Agentic%20Choice%20Enhancement%20%28ACE%29%2C%20which%20improves%0Amodel%20reasoning%20capabilities%20post-training%20by%20up%20to%2025%25.%20Our%20work%20contributes%0Ato%20advancing%20movie%20understanding%20in%20AI%20systems%20and%20provides%20valuable%20insights%0Ainto%20the%20capabilities%20and%20limitations%20of%20current%20VQA%20models%20when%20faced%20with%0Amore%20challenging%2C%20nuanced%20questions%20about%20cinematic%20content.%20Our%20project%20page%2C%0Adataset%20and%20code%20can%20be%20found%20at%0Ahttps%3A//joslefaure.github.io/assets/html/moviecore.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19026v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovieCORE%253A%2520COgnitive%2520REasoning%2520in%2520Movies%26entry.906535625%3DGueter%2520Josmy%2520Faure%2520and%2520Min-Hung%2520Chen%2520and%2520Jia-Fong%2520Yeh%2520and%2520Ying%2520Cheng%2520and%2520Hung-Ting%2520Su%2520and%2520Yung-Hao%2520Tang%2520and%2520Shang-Hong%2520Lai%2520and%2520Winston%2520H.%2520Hsu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MovieCORE%252C%2520a%2520novel%2520video%2520question%2520answering%2520%2528VQA%2529%250Adataset%2520designed%2520to%2520probe%2520deeper%2520cognitive%2520understanding%2520of%2520movie%2520content.%250AUnlike%2520existing%2520datasets%2520that%2520focus%2520on%2520surface-level%2520comprehension%252C%2520MovieCORE%250Aemphasizes%2520questions%2520that%2520engage%2520System-2%2520thinking%2520while%2520remaining%2520specific%2520to%250Athe%2520video%2520material.%2520We%2520present%2520an%2520innovative%2520agentic%2520brainstorming%2520approach%252C%250Autilizing%2520multiple%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520thought%2520agents%2520to%2520generate%250Aand%2520refine%2520high-quality%2520question-answer%2520pairs.%2520To%2520evaluate%2520dataset%2520quality%252C%2520we%250Adevelop%2520a%2520set%2520of%2520cognitive%2520tests%2520assessing%2520depth%252C%2520thought-provocation%250Apotential%252C%2520and%2520syntactic%2520complexity.%2520We%2520also%2520propose%2520a%2520comprehensive%2520evaluation%250Ascheme%2520for%2520assessing%2520VQA%2520model%2520performance%2520on%2520deeper%2520cognitive%2520tasks.%2520To%250Aaddress%2520the%2520limitations%2520of%2520existing%2520video-language%2520models%2520%2528VLMs%2529%252C%2520we%2520introduce%250Aan%2520agentic%2520enhancement%2520module%252C%2520Agentic%2520Choice%2520Enhancement%2520%2528ACE%2529%252C%2520which%2520improves%250Amodel%2520reasoning%2520capabilities%2520post-training%2520by%2520up%2520to%252025%2525.%2520Our%2520work%2520contributes%250Ato%2520advancing%2520movie%2520understanding%2520in%2520AI%2520systems%2520and%2520provides%2520valuable%2520insights%250Ainto%2520the%2520capabilities%2520and%2520limitations%2520of%2520current%2520VQA%2520models%2520when%2520faced%2520with%250Amore%2520challenging%252C%2520nuanced%2520questions%2520about%2520cinematic%2520content.%2520Our%2520project%2520page%252C%250Adataset%2520and%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//joslefaure.github.io/assets/html/moviecore.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19026v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MovieCORE%3A%20COgnitive%20REasoning%20in%20Movies&entry.906535625=Gueter%20Josmy%20Faure%20and%20Min-Hung%20Chen%20and%20Jia-Fong%20Yeh%20and%20Ying%20Cheng%20and%20Hung-Ting%20Su%20and%20Yung-Hao%20Tang%20and%20Shang-Hong%20Lai%20and%20Winston%20H.%20Hsu&entry.1292438233=%20%20This%20paper%20introduces%20MovieCORE%2C%20a%20novel%20video%20question%20answering%20%28VQA%29%0Adataset%20designed%20to%20probe%20deeper%20cognitive%20understanding%20of%20movie%20content.%0AUnlike%20existing%20datasets%20that%20focus%20on%20surface-level%20comprehension%2C%20MovieCORE%0Aemphasizes%20questions%20that%20engage%20System-2%20thinking%20while%20remaining%20specific%20to%0Athe%20video%20material.%20We%20present%20an%20innovative%20agentic%20brainstorming%20approach%2C%0Autilizing%20multiple%20large%20language%20models%20%28LLMs%29%20as%20thought%20agents%20to%20generate%0Aand%20refine%20high-quality%20question-answer%20pairs.%20To%20evaluate%20dataset%20quality%2C%20we%0Adevelop%20a%20set%20of%20cognitive%20tests%20assessing%20depth%2C%20thought-provocation%0Apotential%2C%20and%20syntactic%20complexity.%20We%20also%20propose%20a%20comprehensive%20evaluation%0Ascheme%20for%20assessing%20VQA%20model%20performance%20on%20deeper%20cognitive%20tasks.%20To%0Aaddress%20the%20limitations%20of%20existing%20video-language%20models%20%28VLMs%29%2C%20we%20introduce%0Aan%20agentic%20enhancement%20module%2C%20Agentic%20Choice%20Enhancement%20%28ACE%29%2C%20which%20improves%0Amodel%20reasoning%20capabilities%20post-training%20by%20up%20to%2025%25.%20Our%20work%20contributes%0Ato%20advancing%20movie%20understanding%20in%20AI%20systems%20and%20provides%20valuable%20insights%0Ainto%20the%20capabilities%20and%20limitations%20of%20current%20VQA%20models%20when%20faced%20with%0Amore%20challenging%2C%20nuanced%20questions%20about%20cinematic%20content.%20Our%20project%20page%2C%0Adataset%20and%20code%20can%20be%20found%20at%0Ahttps%3A//joslefaure.github.io/assets/html/moviecore.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19026v3&entry.124074799=Read"},
{"title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain\n  Adaptation in Open-Vocabulary Semantic Segmentation", "author": "Silvio Mazzucco and Carl Persson and Mattia Segu and Pier Luigi Dovesi and Federico Tombari and Luc Van Gool and Matteo Poggi", "abstract": "  We introduce VocAlign, a novel source-free domain adaptation framework\nspecifically designed for VLMs in open-vocabulary semantic segmentation. Our\nmethod adopts a student-teacher paradigm enhanced with a vocabulary alignment\nstrategy, which improves pseudo-label generation by incorporating additional\nclass concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to\nfine-tune the model, preserving its original capabilities while minimizing\ncomputational overhead. In addition, we propose a Top-K class selection\nmechanism for the student model, which significantly reduces memory\nrequirements while further improving adaptation performance. Our approach\nachieves a notable 6.11 mIoU improvement on the CityScapes dataset and\ndemonstrates superior performance on zero-shot segmentation benchmarks, setting\na new standard for source-free adaptation in the open-vocabulary setting.\n", "link": "http://arxiv.org/abs/2509.15225v1", "date": "2025-09-18", "relevancy": 2.8072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Translation%3F%20Vocabulary%20Alignment%20for%20Source-Free%20Domain%0A%20%20Adaptation%20in%20Open-Vocabulary%20Semantic%20Segmentation&body=Title%3A%20Lost%20in%20Translation%3F%20Vocabulary%20Alignment%20for%20Source-Free%20Domain%0A%20%20Adaptation%20in%20Open-Vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Silvio%20Mazzucco%20and%20Carl%20Persson%20and%20Mattia%20Segu%20and%20Pier%20Luigi%20Dovesi%20and%20Federico%20Tombari%20and%20Luc%20Van%20Gool%20and%20Matteo%20Poggi%0AAbstract%3A%20%20%20We%20introduce%20VocAlign%2C%20a%20novel%20source-free%20domain%20adaptation%20framework%0Aspecifically%20designed%20for%20VLMs%20in%20open-vocabulary%20semantic%20segmentation.%20Our%0Amethod%20adopts%20a%20student-teacher%20paradigm%20enhanced%20with%20a%20vocabulary%20alignment%0Astrategy%2C%20which%20improves%20pseudo-label%20generation%20by%20incorporating%20additional%0Aclass%20concepts.%20To%20ensure%20efficiency%2C%20we%20use%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Afine-tune%20the%20model%2C%20preserving%20its%20original%20capabilities%20while%20minimizing%0Acomputational%20overhead.%20In%20addition%2C%20we%20propose%20a%20Top-K%20class%20selection%0Amechanism%20for%20the%20student%20model%2C%20which%20significantly%20reduces%20memory%0Arequirements%20while%20further%20improving%20adaptation%20performance.%20Our%20approach%0Aachieves%20a%20notable%206.11%20mIoU%20improvement%20on%20the%20CityScapes%20dataset%20and%0Ademonstrates%20superior%20performance%20on%20zero-shot%20segmentation%20benchmarks%2C%20setting%0Aa%20new%20standard%20for%20source-free%20adaptation%20in%20the%20open-vocabulary%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Translation%253F%2520Vocabulary%2520Alignment%2520for%2520Source-Free%2520Domain%250A%2520%2520Adaptation%2520in%2520Open-Vocabulary%2520Semantic%2520Segmentation%26entry.906535625%3DSilvio%2520Mazzucco%2520and%2520Carl%2520Persson%2520and%2520Mattia%2520Segu%2520and%2520Pier%2520Luigi%2520Dovesi%2520and%2520Federico%2520Tombari%2520and%2520Luc%2520Van%2520Gool%2520and%2520Matteo%2520Poggi%26entry.1292438233%3D%2520%2520We%2520introduce%2520VocAlign%252C%2520a%2520novel%2520source-free%2520domain%2520adaptation%2520framework%250Aspecifically%2520designed%2520for%2520VLMs%2520in%2520open-vocabulary%2520semantic%2520segmentation.%2520Our%250Amethod%2520adopts%2520a%2520student-teacher%2520paradigm%2520enhanced%2520with%2520a%2520vocabulary%2520alignment%250Astrategy%252C%2520which%2520improves%2520pseudo-label%2520generation%2520by%2520incorporating%2520additional%250Aclass%2520concepts.%2520To%2520ensure%2520efficiency%252C%2520we%2520use%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%250Afine-tune%2520the%2520model%252C%2520preserving%2520its%2520original%2520capabilities%2520while%2520minimizing%250Acomputational%2520overhead.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Top-K%2520class%2520selection%250Amechanism%2520for%2520the%2520student%2520model%252C%2520which%2520significantly%2520reduces%2520memory%250Arequirements%2520while%2520further%2520improving%2520adaptation%2520performance.%2520Our%2520approach%250Aachieves%2520a%2520notable%25206.11%2520mIoU%2520improvement%2520on%2520the%2520CityScapes%2520dataset%2520and%250Ademonstrates%2520superior%2520performance%2520on%2520zero-shot%2520segmentation%2520benchmarks%252C%2520setting%250Aa%2520new%2520standard%2520for%2520source-free%2520adaptation%2520in%2520the%2520open-vocabulary%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Translation%3F%20Vocabulary%20Alignment%20for%20Source-Free%20Domain%0A%20%20Adaptation%20in%20Open-Vocabulary%20Semantic%20Segmentation&entry.906535625=Silvio%20Mazzucco%20and%20Carl%20Persson%20and%20Mattia%20Segu%20and%20Pier%20Luigi%20Dovesi%20and%20Federico%20Tombari%20and%20Luc%20Van%20Gool%20and%20Matteo%20Poggi&entry.1292438233=%20%20We%20introduce%20VocAlign%2C%20a%20novel%20source-free%20domain%20adaptation%20framework%0Aspecifically%20designed%20for%20VLMs%20in%20open-vocabulary%20semantic%20segmentation.%20Our%0Amethod%20adopts%20a%20student-teacher%20paradigm%20enhanced%20with%20a%20vocabulary%20alignment%0Astrategy%2C%20which%20improves%20pseudo-label%20generation%20by%20incorporating%20additional%0Aclass%20concepts.%20To%20ensure%20efficiency%2C%20we%20use%20Low-Rank%20Adaptation%20%28LoRA%29%20to%0Afine-tune%20the%20model%2C%20preserving%20its%20original%20capabilities%20while%20minimizing%0Acomputational%20overhead.%20In%20addition%2C%20we%20propose%20a%20Top-K%20class%20selection%0Amechanism%20for%20the%20student%20model%2C%20which%20significantly%20reduces%20memory%0Arequirements%20while%20further%20improving%20adaptation%20performance.%20Our%20approach%0Aachieves%20a%20notable%206.11%20mIoU%20improvement%20on%20the%20CityScapes%20dataset%20and%0Ademonstrates%20superior%20performance%20on%20zero-shot%20segmentation%20benchmarks%2C%20setting%0Aa%20new%20standard%20for%20source-free%20adaptation%20in%20the%20open-vocabulary%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15225v1&entry.124074799=Read"},
{"title": "HAM: Hierarchical Adapter Merging for Scalable Continual Learning", "author": "Eric Nuertey Coleman and Luigi Quarantiello and Samrat Mukherjee and Julio Hurtado and Vincenzo Lomonaco", "abstract": "  Continual learning is an essential capability of human cognition, yet it\nposes significant challenges for current deep learning models. The primary\nissue is that new knowledge can interfere with previously learned information,\ncausing the model to forget earlier knowledge in favor of the new, a phenomenon\nknown as catastrophic forgetting. Although large pre-trained models can\npartially mitigate forgetting by leveraging their existing knowledge and\nover-parameterization, they often struggle when confronted with novel data\ndistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\nenable efficient adaptation to new knowledge. However, they still face\nchallenges in scaling to dynamic learning scenarios and long sequences of\ntasks, as maintaining one adapter per task introduces complexity and increases\nthe potential for interference. In this paper, we introduce Hierarchical\nAdapters Merging (HAM), a novel framework that dynamically combines adapters\nfrom different tasks during training. This approach enables HAM to scale\neffectively, allowing it to manage more tasks than competing baselines with\nimproved efficiency. To achieve this, HAM maintains a fixed set of groups that\nhierarchically consolidate new adapters. For each task, HAM trains a low-rank\nadapter along with an importance scalar, then dynamically groups tasks based on\nadapter similarity. Within each group, adapters are pruned, scaled and merge,\nfacilitating transfer learning between related tasks. Extensive experiments on\nthree vision benchmarks show that HAM significantly outperforms\nstate-of-the-art methods, particularly as the number of tasks increases.\n", "link": "http://arxiv.org/abs/2509.13211v3", "date": "2025-09-18", "relevancy": 2.7803, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.52}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&body=Title%3A%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning%0AAuthor%3A%20Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13211v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAM%253A%2520Hierarchical%2520Adapter%2520Merging%2520for%2520Scalable%2520Continual%2520Learning%26entry.906535625%3DEric%2520Nuertey%2520Coleman%2520and%2520Luigi%2520Quarantiello%2520and%2520Samrat%2520Mukherjee%2520and%2520Julio%2520Hurtado%2520and%2520Vincenzo%2520Lomonaco%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520an%2520essential%2520capability%2520of%2520human%2520cognition%252C%2520yet%2520it%250Aposes%2520significant%2520challenges%2520for%2520current%2520deep%2520learning%2520models.%2520The%2520primary%250Aissue%2520is%2520that%2520new%2520knowledge%2520can%2520interfere%2520with%2520previously%2520learned%2520information%252C%250Acausing%2520the%2520model%2520to%2520forget%2520earlier%2520knowledge%2520in%2520favor%2520of%2520the%2520new%252C%2520a%2520phenomenon%250Aknown%2520as%2520catastrophic%2520forgetting.%2520Although%2520large%2520pre-trained%2520models%2520can%250Apartially%2520mitigate%2520forgetting%2520by%2520leveraging%2520their%2520existing%2520knowledge%2520and%250Aover-parameterization%252C%2520they%2520often%2520struggle%2520when%2520confronted%2520with%2520novel%2520data%250Adistributions.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520LoRA%252C%250Aenable%2520efficient%2520adaptation%2520to%2520new%2520knowledge.%2520However%252C%2520they%2520still%2520face%250Achallenges%2520in%2520scaling%2520to%2520dynamic%2520learning%2520scenarios%2520and%2520long%2520sequences%2520of%250Atasks%252C%2520as%2520maintaining%2520one%2520adapter%2520per%2520task%2520introduces%2520complexity%2520and%2520increases%250Athe%2520potential%2520for%2520interference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Hierarchical%250AAdapters%2520Merging%2520%2528HAM%2529%252C%2520a%2520novel%2520framework%2520that%2520dynamically%2520combines%2520adapters%250Afrom%2520different%2520tasks%2520during%2520training.%2520This%2520approach%2520enables%2520HAM%2520to%2520scale%250Aeffectively%252C%2520allowing%2520it%2520to%2520manage%2520more%2520tasks%2520than%2520competing%2520baselines%2520with%250Aimproved%2520efficiency.%2520To%2520achieve%2520this%252C%2520HAM%2520maintains%2520a%2520fixed%2520set%2520of%2520groups%2520that%250Ahierarchically%2520consolidate%2520new%2520adapters.%2520For%2520each%2520task%252C%2520HAM%2520trains%2520a%2520low-rank%250Aadapter%2520along%2520with%2520an%2520importance%2520scalar%252C%2520then%2520dynamically%2520groups%2520tasks%2520based%2520on%250Aadapter%2520similarity.%2520Within%2520each%2520group%252C%2520adapters%2520are%2520pruned%252C%2520scaled%2520and%2520merge%252C%250Afacilitating%2520transfer%2520learning%2520between%2520related%2520tasks.%2520Extensive%2520experiments%2520on%250Athree%2520vision%2520benchmarks%2520show%2520that%2520HAM%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%252C%2520particularly%2520as%2520the%2520number%2520of%2520tasks%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13211v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&entry.906535625=Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13211v3&entry.124074799=Read"},
{"title": "T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images", "author": "Christopher Wiedeman and Anastasiia Sarmakeeva and Elena Sizikova and Daniil Filienko and Miguel Lago and Jana G. Delfino and Aldo Badano", "abstract": "  One of the key impediments for developing and assessing robust medical\nimaging algorithms is limited access to large-scale datasets with suitable\nannotations. Synthetic data generated with plausible physical and biological\nconstraints may address some of these data limitations. We propose the use of\nphysics simulations to generate synthetic images with pixel-level segmentation\nannotations, which are notoriously difficult to obtain. Specifically, we apply\nthis approach to breast imaging analysis and release T-SYNTH, a large-scale\nopen-source dataset of paired 2D digital mammography (DM) and 3D digital breast\ntomosynthesis (DBT) images. Our initial experimental results indicate that\nT-SYNTH images show promise for augmenting limited real patient datasets for\ndetection tasks in DM and DBT. Our data and code are publicly available at\nhttps://github.com/DIDSR/tsynth-release.\n", "link": "http://arxiv.org/abs/2507.04038v2", "date": "2025-09-18", "relevancy": 2.7403, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5548}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5487}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-SYNTH%3A%20A%20Knowledge-Based%20Dataset%20of%20Synthetic%20Breast%20Images&body=Title%3A%20T-SYNTH%3A%20A%20Knowledge-Based%20Dataset%20of%20Synthetic%20Breast%20Images%0AAuthor%3A%20Christopher%20Wiedeman%20and%20Anastasiia%20Sarmakeeva%20and%20Elena%20Sizikova%20and%20Daniil%20Filienko%20and%20Miguel%20Lago%20and%20Jana%20G.%20Delfino%20and%20Aldo%20Badano%0AAbstract%3A%20%20%20One%20of%20the%20key%20impediments%20for%20developing%20and%20assessing%20robust%20medical%0Aimaging%20algorithms%20is%20limited%20access%20to%20large-scale%20datasets%20with%20suitable%0Aannotations.%20Synthetic%20data%20generated%20with%20plausible%20physical%20and%20biological%0Aconstraints%20may%20address%20some%20of%20these%20data%20limitations.%20We%20propose%20the%20use%20of%0Aphysics%20simulations%20to%20generate%20synthetic%20images%20with%20pixel-level%20segmentation%0Aannotations%2C%20which%20are%20notoriously%20difficult%20to%20obtain.%20Specifically%2C%20we%20apply%0Athis%20approach%20to%20breast%20imaging%20analysis%20and%20release%20T-SYNTH%2C%20a%20large-scale%0Aopen-source%20dataset%20of%20paired%202D%20digital%20mammography%20%28DM%29%20and%203D%20digital%20breast%0Atomosynthesis%20%28DBT%29%20images.%20Our%20initial%20experimental%20results%20indicate%20that%0AT-SYNTH%20images%20show%20promise%20for%20augmenting%20limited%20real%20patient%20datasets%20for%0Adetection%20tasks%20in%20DM%20and%20DBT.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/DIDSR/tsynth-release.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-SYNTH%253A%2520A%2520Knowledge-Based%2520Dataset%2520of%2520Synthetic%2520Breast%2520Images%26entry.906535625%3DChristopher%2520Wiedeman%2520and%2520Anastasiia%2520Sarmakeeva%2520and%2520Elena%2520Sizikova%2520and%2520Daniil%2520Filienko%2520and%2520Miguel%2520Lago%2520and%2520Jana%2520G.%2520Delfino%2520and%2520Aldo%2520Badano%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520key%2520impediments%2520for%2520developing%2520and%2520assessing%2520robust%2520medical%250Aimaging%2520algorithms%2520is%2520limited%2520access%2520to%2520large-scale%2520datasets%2520with%2520suitable%250Aannotations.%2520Synthetic%2520data%2520generated%2520with%2520plausible%2520physical%2520and%2520biological%250Aconstraints%2520may%2520address%2520some%2520of%2520these%2520data%2520limitations.%2520We%2520propose%2520the%2520use%2520of%250Aphysics%2520simulations%2520to%2520generate%2520synthetic%2520images%2520with%2520pixel-level%2520segmentation%250Aannotations%252C%2520which%2520are%2520notoriously%2520difficult%2520to%2520obtain.%2520Specifically%252C%2520we%2520apply%250Athis%2520approach%2520to%2520breast%2520imaging%2520analysis%2520and%2520release%2520T-SYNTH%252C%2520a%2520large-scale%250Aopen-source%2520dataset%2520of%2520paired%25202D%2520digital%2520mammography%2520%2528DM%2529%2520and%25203D%2520digital%2520breast%250Atomosynthesis%2520%2528DBT%2529%2520images.%2520Our%2520initial%2520experimental%2520results%2520indicate%2520that%250AT-SYNTH%2520images%2520show%2520promise%2520for%2520augmenting%2520limited%2520real%2520patient%2520datasets%2520for%250Adetection%2520tasks%2520in%2520DM%2520and%2520DBT.%2520Our%2520data%2520and%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/DIDSR/tsynth-release.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-SYNTH%3A%20A%20Knowledge-Based%20Dataset%20of%20Synthetic%20Breast%20Images&entry.906535625=Christopher%20Wiedeman%20and%20Anastasiia%20Sarmakeeva%20and%20Elena%20Sizikova%20and%20Daniil%20Filienko%20and%20Miguel%20Lago%20and%20Jana%20G.%20Delfino%20and%20Aldo%20Badano&entry.1292438233=%20%20One%20of%20the%20key%20impediments%20for%20developing%20and%20assessing%20robust%20medical%0Aimaging%20algorithms%20is%20limited%20access%20to%20large-scale%20datasets%20with%20suitable%0Aannotations.%20Synthetic%20data%20generated%20with%20plausible%20physical%20and%20biological%0Aconstraints%20may%20address%20some%20of%20these%20data%20limitations.%20We%20propose%20the%20use%20of%0Aphysics%20simulations%20to%20generate%20synthetic%20images%20with%20pixel-level%20segmentation%0Aannotations%2C%20which%20are%20notoriously%20difficult%20to%20obtain.%20Specifically%2C%20we%20apply%0Athis%20approach%20to%20breast%20imaging%20analysis%20and%20release%20T-SYNTH%2C%20a%20large-scale%0Aopen-source%20dataset%20of%20paired%202D%20digital%20mammography%20%28DM%29%20and%203D%20digital%20breast%0Atomosynthesis%20%28DBT%29%20images.%20Our%20initial%20experimental%20results%20indicate%20that%0AT-SYNTH%20images%20show%20promise%20for%20augmenting%20limited%20real%20patient%20datasets%20for%0Adetection%20tasks%20in%20DM%20and%20DBT.%20Our%20data%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/DIDSR/tsynth-release.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04038v2&entry.124074799=Read"},
{"title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models", "author": "Xin Wang and Haoyang Li and Haibo Chen and Zeyang Zhang and Wenwu Zhu", "abstract": "  Large language models (LLMs) have substantially advanced machine learning\nresearch, including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in explainability,\nreliability, adaptability, and extensibility. In this paper, we overview a\npromising learning paradigm, i.e., Modular Machine Learning (MML), as an\nessential approach toward new-generation LLMs capable of addressing these\nissues. We begin by systematically and comprehensively surveying the existing\nliterature on modular machine learning, with a particular focus on modular data\nrepresentation and modular models. Then, we propose a unified MML framework for\nLLMs, which decomposes the complex structure of LLMs into three interdependent\ncomponents: modular representation, modular model, and modular reasoning.\nSpecifically, the MML paradigm discussed in this article is able to: i) clarify\nthe internal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\nan interpretable and logic-driven decision-making process. We further elaborate\na feasible implementation of MML-based LLMs via leveraging advanced techniques\nsuch as disentangled representation learning, neural architecture search and\nneuro-symbolic learning. Last but not least, we critically identify the\nremaining key challenges, such as the integration of continuous neural and\ndiscrete symbolic processes, joint optimization, and computational scalability,\npresent promising future research directions that deserve further exploration.\nUltimately, we believe the integration of the MML with LLMs has the potential\nto bridge the gap between statistical (deep) learning and formal (logical)\nreasoning, thereby paving the way for robust, adaptable, and trustworthy AI\nsystems across a wide range of real-world applications.\n", "link": "http://arxiv.org/abs/2504.20020v2", "date": "2025-09-18", "relevancy": 2.7333, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models&body=Title%3A%20Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Xin%20Wang%20and%20Haoyang%20Li%20and%20Haibo%20Chen%20and%20Zeyang%20Zhang%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20substantially%20advanced%20machine%20learning%0Aresearch%2C%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20data%20mining%2C%0Aetc.%2C%20yet%20they%20still%20exhibit%20critical%20limitations%20in%20explainability%2C%0Areliability%2C%20adaptability%2C%20and%20extensibility.%20In%20this%20paper%2C%20we%20overview%20a%0Apromising%20learning%20paradigm%2C%20i.e.%2C%20Modular%20Machine%20Learning%20%28MML%29%2C%20as%20an%0Aessential%20approach%20toward%20new-generation%20LLMs%20capable%20of%20addressing%20these%0Aissues.%20We%20begin%20by%20systematically%20and%20comprehensively%20surveying%20the%20existing%0Aliterature%20on%20modular%20machine%20learning%2C%20with%20a%20particular%20focus%20on%20modular%20data%0Arepresentation%20and%20modular%20models.%20Then%2C%20we%20propose%20a%20unified%20MML%20framework%20for%0ALLMs%2C%20which%20decomposes%20the%20complex%20structure%20of%20LLMs%20into%20three%20interdependent%0Acomponents%3A%20modular%20representation%2C%20modular%20model%2C%20and%20modular%20reasoning.%0ASpecifically%2C%20the%20MML%20paradigm%20discussed%20in%20this%20article%20is%20able%20to%3A%20i%29%20clarify%0Athe%20internal%20working%20mechanism%20of%20LLMs%20through%20the%20disentanglement%20of%20semantic%0Acomponents%3B%20ii%29%20allow%20for%20flexible%20and%20task-adaptive%20model%20design%3B%20iii%29%20enable%0Aan%20interpretable%20and%20logic-driven%20decision-making%20process.%20We%20further%20elaborate%0Aa%20feasible%20implementation%20of%20MML-based%20LLMs%20via%20leveraging%20advanced%20techniques%0Asuch%20as%20disentangled%20representation%20learning%2C%20neural%20architecture%20search%20and%0Aneuro-symbolic%20learning.%20Last%20but%20not%20least%2C%20we%20critically%20identify%20the%0Aremaining%20key%20challenges%2C%20such%20as%20the%20integration%20of%20continuous%20neural%20and%0Adiscrete%20symbolic%20processes%2C%20joint%20optimization%2C%20and%20computational%20scalability%2C%0Apresent%20promising%20future%20research%20directions%20that%20deserve%20further%20exploration.%0AUltimately%2C%20we%20believe%20the%20integration%20of%20the%20MML%20with%20LLMs%20has%20the%20potential%0Ato%20bridge%20the%20gap%20between%20statistical%20%28deep%29%20learning%20and%20formal%20%28logical%29%0Areasoning%2C%20thereby%20paving%20the%20way%20for%20robust%2C%20adaptable%2C%20and%20trustworthy%20AI%0Asystems%20across%20a%20wide%20range%20of%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Machine%2520Learning%253A%2520An%2520Indispensable%2520Path%2520towards%2520New-Generation%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DXin%2520Wang%2520and%2520Haoyang%2520Li%2520and%2520Haibo%2520Chen%2520and%2520Zeyang%2520Zhang%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520substantially%2520advanced%2520machine%2520learning%250Aresearch%252C%2520including%2520natural%2520language%2520processing%252C%2520computer%2520vision%252C%2520data%2520mining%252C%250Aetc.%252C%2520yet%2520they%2520still%2520exhibit%2520critical%2520limitations%2520in%2520explainability%252C%250Areliability%252C%2520adaptability%252C%2520and%2520extensibility.%2520In%2520this%2520paper%252C%2520we%2520overview%2520a%250Apromising%2520learning%2520paradigm%252C%2520i.e.%252C%2520Modular%2520Machine%2520Learning%2520%2528MML%2529%252C%2520as%2520an%250Aessential%2520approach%2520toward%2520new-generation%2520LLMs%2520capable%2520of%2520addressing%2520these%250Aissues.%2520We%2520begin%2520by%2520systematically%2520and%2520comprehensively%2520surveying%2520the%2520existing%250Aliterature%2520on%2520modular%2520machine%2520learning%252C%2520with%2520a%2520particular%2520focus%2520on%2520modular%2520data%250Arepresentation%2520and%2520modular%2520models.%2520Then%252C%2520we%2520propose%2520a%2520unified%2520MML%2520framework%2520for%250ALLMs%252C%2520which%2520decomposes%2520the%2520complex%2520structure%2520of%2520LLMs%2520into%2520three%2520interdependent%250Acomponents%253A%2520modular%2520representation%252C%2520modular%2520model%252C%2520and%2520modular%2520reasoning.%250ASpecifically%252C%2520the%2520MML%2520paradigm%2520discussed%2520in%2520this%2520article%2520is%2520able%2520to%253A%2520i%2529%2520clarify%250Athe%2520internal%2520working%2520mechanism%2520of%2520LLMs%2520through%2520the%2520disentanglement%2520of%2520semantic%250Acomponents%253B%2520ii%2529%2520allow%2520for%2520flexible%2520and%2520task-adaptive%2520model%2520design%253B%2520iii%2529%2520enable%250Aan%2520interpretable%2520and%2520logic-driven%2520decision-making%2520process.%2520We%2520further%2520elaborate%250Aa%2520feasible%2520implementation%2520of%2520MML-based%2520LLMs%2520via%2520leveraging%2520advanced%2520techniques%250Asuch%2520as%2520disentangled%2520representation%2520learning%252C%2520neural%2520architecture%2520search%2520and%250Aneuro-symbolic%2520learning.%2520Last%2520but%2520not%2520least%252C%2520we%2520critically%2520identify%2520the%250Aremaining%2520key%2520challenges%252C%2520such%2520as%2520the%2520integration%2520of%2520continuous%2520neural%2520and%250Adiscrete%2520symbolic%2520processes%252C%2520joint%2520optimization%252C%2520and%2520computational%2520scalability%252C%250Apresent%2520promising%2520future%2520research%2520directions%2520that%2520deserve%2520further%2520exploration.%250AUltimately%252C%2520we%2520believe%2520the%2520integration%2520of%2520the%2520MML%2520with%2520LLMs%2520has%2520the%2520potential%250Ato%2520bridge%2520the%2520gap%2520between%2520statistical%2520%2528deep%2529%2520learning%2520and%2520formal%2520%2528logical%2529%250Areasoning%252C%2520thereby%2520paving%2520the%2520way%2520for%2520robust%252C%2520adaptable%252C%2520and%2520trustworthy%2520AI%250Asystems%2520across%2520a%2520wide%2520range%2520of%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Machine%20Learning%3A%20An%20Indispensable%20Path%20towards%20New-Generation%0A%20%20Large%20Language%20Models&entry.906535625=Xin%20Wang%20and%20Haoyang%20Li%20and%20Haibo%20Chen%20and%20Zeyang%20Zhang%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20substantially%20advanced%20machine%20learning%0Aresearch%2C%20including%20natural%20language%20processing%2C%20computer%20vision%2C%20data%20mining%2C%0Aetc.%2C%20yet%20they%20still%20exhibit%20critical%20limitations%20in%20explainability%2C%0Areliability%2C%20adaptability%2C%20and%20extensibility.%20In%20this%20paper%2C%20we%20overview%20a%0Apromising%20learning%20paradigm%2C%20i.e.%2C%20Modular%20Machine%20Learning%20%28MML%29%2C%20as%20an%0Aessential%20approach%20toward%20new-generation%20LLMs%20capable%20of%20addressing%20these%0Aissues.%20We%20begin%20by%20systematically%20and%20comprehensively%20surveying%20the%20existing%0Aliterature%20on%20modular%20machine%20learning%2C%20with%20a%20particular%20focus%20on%20modular%20data%0Arepresentation%20and%20modular%20models.%20Then%2C%20we%20propose%20a%20unified%20MML%20framework%20for%0ALLMs%2C%20which%20decomposes%20the%20complex%20structure%20of%20LLMs%20into%20three%20interdependent%0Acomponents%3A%20modular%20representation%2C%20modular%20model%2C%20and%20modular%20reasoning.%0ASpecifically%2C%20the%20MML%20paradigm%20discussed%20in%20this%20article%20is%20able%20to%3A%20i%29%20clarify%0Athe%20internal%20working%20mechanism%20of%20LLMs%20through%20the%20disentanglement%20of%20semantic%0Acomponents%3B%20ii%29%20allow%20for%20flexible%20and%20task-adaptive%20model%20design%3B%20iii%29%20enable%0Aan%20interpretable%20and%20logic-driven%20decision-making%20process.%20We%20further%20elaborate%0Aa%20feasible%20implementation%20of%20MML-based%20LLMs%20via%20leveraging%20advanced%20techniques%0Asuch%20as%20disentangled%20representation%20learning%2C%20neural%20architecture%20search%20and%0Aneuro-symbolic%20learning.%20Last%20but%20not%20least%2C%20we%20critically%20identify%20the%0Aremaining%20key%20challenges%2C%20such%20as%20the%20integration%20of%20continuous%20neural%20and%0Adiscrete%20symbolic%20processes%2C%20joint%20optimization%2C%20and%20computational%20scalability%2C%0Apresent%20promising%20future%20research%20directions%20that%20deserve%20further%20exploration.%0AUltimately%2C%20we%20believe%20the%20integration%20of%20the%20MML%20with%20LLMs%20has%20the%20potential%0Ato%20bridge%20the%20gap%20between%20statistical%20%28deep%29%20learning%20and%20formal%20%28logical%29%0Areasoning%2C%20thereby%20paving%20the%20way%20for%20robust%2C%20adaptable%2C%20and%20trustworthy%20AI%0Asystems%20across%20a%20wide%20range%20of%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20020v2&entry.124074799=Read"},
{"title": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of\n  Cardiac Displacement Fields from Sparse MRI-like Observations", "author": "Francesco C. Mantegazza and Federica Caforio and Christoph Augustin and Matthias A. F. Gsell and Gundolf Haase and Elias Karabelas", "abstract": "  Personalized cardiac diagnostics require accurate reconstruction of\nmyocardial displacement fields from sparse clinical imaging data, yet current\nmethods often demand intrusive access to computational models. In this work, we\napply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to\nthree-dimensional (3D) cardiac displacement field reconstruction from limited\nMagnetic Resonance Image (MRI)-like observations. Our implementation requires\nonly solution snapshots -- no governing equations, assembly routines, or solver\naccess -- enabling immediate deployment across commercial and research codes\nusing different constitutive models. Additionally, we introduce two\nenhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP)\nalgorithm that improves Sensor Selection (SS) computational efficiency while\nmaintaining reconstruction accuracy, and memory optimization techniques\nexploiting block matrix structures in vectorial problems. We demonstrate the\neffectiveness of the method through validation on a 3D left ventricular model\nwith simulated scar tissue. Starting with noise-free reconstruction, we\nsystematically incorporate Gaussian noise and spatial sparsity mimicking\nrealistic MRI acquisition protocols. Results show exceptional accuracy in\nnoise-free conditions (relative L2 error of order O(1e-5)), robust performance\nwith 10% noise (relative L2 error of order O(1e-2)), and effective\nreconstruction from sparse measurements (relative L2 error of order O(1e-2)).\nThe online reconstruction achieves four-order-of-magnitude computational\nspeed-up compared to full Finite Element (FE) simulations, with reconstruction\ntimes under one tenth of second for sparse scenarios, demonstrating significant\npotential for integration into clinical cardiac modeling workflows.\n", "link": "http://arxiv.org/abs/2509.14844v1", "date": "2025-09-18", "relevancy": 2.722, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5461}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%0A%20%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations&body=Title%3A%20Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%0A%20%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations%0AAuthor%3A%20Francesco%20C.%20Mantegazza%20and%20Federica%20Caforio%20and%20Christoph%20Augustin%20and%20Matthias%20A.%20F.%20Gsell%20and%20Gundolf%20Haase%20and%20Elias%20Karabelas%0AAbstract%3A%20%20%20Personalized%20cardiac%20diagnostics%20require%20accurate%20reconstruction%20of%0Amyocardial%20displacement%20fields%20from%20sparse%20clinical%20imaging%20data%2C%20yet%20current%0Amethods%20often%20demand%20intrusive%20access%20to%20computational%20models.%20In%20this%20work%2C%20we%0Aapply%20the%20non-intrusive%20Parametrized-Background%20Data-Weak%20%28PBDW%29%20approach%20to%0Athree-dimensional%20%283D%29%20cardiac%20displacement%20field%20reconstruction%20from%20limited%0AMagnetic%20Resonance%20Image%20%28MRI%29-like%20observations.%20Our%20implementation%20requires%0Aonly%20solution%20snapshots%20--%20no%20governing%20equations%2C%20assembly%20routines%2C%20or%20solver%0Aaccess%20--%20enabling%20immediate%20deployment%20across%20commercial%20and%20research%20codes%0Ausing%20different%20constitutive%20models.%20Additionally%2C%20we%20introduce%20two%0Aenhancements%3A%20an%20H-size%20minibatch%20worst-case%20Orthogonal%20Matching%20Pursuit%20%28wOMP%29%0Aalgorithm%20that%20improves%20Sensor%20Selection%20%28SS%29%20computational%20efficiency%20while%0Amaintaining%20reconstruction%20accuracy%2C%20and%20memory%20optimization%20techniques%0Aexploiting%20block%20matrix%20structures%20in%20vectorial%20problems.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20method%20through%20validation%20on%20a%203D%20left%20ventricular%20model%0Awith%20simulated%20scar%20tissue.%20Starting%20with%20noise-free%20reconstruction%2C%20we%0Asystematically%20incorporate%20Gaussian%20noise%20and%20spatial%20sparsity%20mimicking%0Arealistic%20MRI%20acquisition%20protocols.%20Results%20show%20exceptional%20accuracy%20in%0Anoise-free%20conditions%20%28relative%20L2%20error%20of%20order%20O%281e-5%29%29%2C%20robust%20performance%0Awith%2010%25%20noise%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29%2C%20and%20effective%0Areconstruction%20from%20sparse%20measurements%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29.%0AThe%20online%20reconstruction%20achieves%20four-order-of-magnitude%20computational%0Aspeed-up%20compared%20to%20full%20Finite%20Element%20%28FE%29%20simulations%2C%20with%20reconstruction%0Atimes%20under%20one%20tenth%20of%20second%20for%20sparse%20scenarios%2C%20demonstrating%20significant%0Apotential%20for%20integration%20into%20clinical%20cardiac%20modeling%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Intrusive%2520Parametrized-Background%2520Data-Weak%2520Reconstruction%2520of%250A%2520%2520Cardiac%2520Displacement%2520Fields%2520from%2520Sparse%2520MRI-like%2520Observations%26entry.906535625%3DFrancesco%2520C.%2520Mantegazza%2520and%2520Federica%2520Caforio%2520and%2520Christoph%2520Augustin%2520and%2520Matthias%2520A.%2520F.%2520Gsell%2520and%2520Gundolf%2520Haase%2520and%2520Elias%2520Karabelas%26entry.1292438233%3D%2520%2520Personalized%2520cardiac%2520diagnostics%2520require%2520accurate%2520reconstruction%2520of%250Amyocardial%2520displacement%2520fields%2520from%2520sparse%2520clinical%2520imaging%2520data%252C%2520yet%2520current%250Amethods%2520often%2520demand%2520intrusive%2520access%2520to%2520computational%2520models.%2520In%2520this%2520work%252C%2520we%250Aapply%2520the%2520non-intrusive%2520Parametrized-Background%2520Data-Weak%2520%2528PBDW%2529%2520approach%2520to%250Athree-dimensional%2520%25283D%2529%2520cardiac%2520displacement%2520field%2520reconstruction%2520from%2520limited%250AMagnetic%2520Resonance%2520Image%2520%2528MRI%2529-like%2520observations.%2520Our%2520implementation%2520requires%250Aonly%2520solution%2520snapshots%2520--%2520no%2520governing%2520equations%252C%2520assembly%2520routines%252C%2520or%2520solver%250Aaccess%2520--%2520enabling%2520immediate%2520deployment%2520across%2520commercial%2520and%2520research%2520codes%250Ausing%2520different%2520constitutive%2520models.%2520Additionally%252C%2520we%2520introduce%2520two%250Aenhancements%253A%2520an%2520H-size%2520minibatch%2520worst-case%2520Orthogonal%2520Matching%2520Pursuit%2520%2528wOMP%2529%250Aalgorithm%2520that%2520improves%2520Sensor%2520Selection%2520%2528SS%2529%2520computational%2520efficiency%2520while%250Amaintaining%2520reconstruction%2520accuracy%252C%2520and%2520memory%2520optimization%2520techniques%250Aexploiting%2520block%2520matrix%2520structures%2520in%2520vectorial%2520problems.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520method%2520through%2520validation%2520on%2520a%25203D%2520left%2520ventricular%2520model%250Awith%2520simulated%2520scar%2520tissue.%2520Starting%2520with%2520noise-free%2520reconstruction%252C%2520we%250Asystematically%2520incorporate%2520Gaussian%2520noise%2520and%2520spatial%2520sparsity%2520mimicking%250Arealistic%2520MRI%2520acquisition%2520protocols.%2520Results%2520show%2520exceptional%2520accuracy%2520in%250Anoise-free%2520conditions%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-5%2529%2529%252C%2520robust%2520performance%250Awith%252010%2525%2520noise%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-2%2529%2529%252C%2520and%2520effective%250Areconstruction%2520from%2520sparse%2520measurements%2520%2528relative%2520L2%2520error%2520of%2520order%2520O%25281e-2%2529%2529.%250AThe%2520online%2520reconstruction%2520achieves%2520four-order-of-magnitude%2520computational%250Aspeed-up%2520compared%2520to%2520full%2520Finite%2520Element%2520%2528FE%2529%2520simulations%252C%2520with%2520reconstruction%250Atimes%2520under%2520one%2520tenth%2520of%2520second%2520for%2520sparse%2520scenarios%252C%2520demonstrating%2520significant%250Apotential%2520for%2520integration%2520into%2520clinical%2520cardiac%2520modeling%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Intrusive%20Parametrized-Background%20Data-Weak%20Reconstruction%20of%0A%20%20Cardiac%20Displacement%20Fields%20from%20Sparse%20MRI-like%20Observations&entry.906535625=Francesco%20C.%20Mantegazza%20and%20Federica%20Caforio%20and%20Christoph%20Augustin%20and%20Matthias%20A.%20F.%20Gsell%20and%20Gundolf%20Haase%20and%20Elias%20Karabelas&entry.1292438233=%20%20Personalized%20cardiac%20diagnostics%20require%20accurate%20reconstruction%20of%0Amyocardial%20displacement%20fields%20from%20sparse%20clinical%20imaging%20data%2C%20yet%20current%0Amethods%20often%20demand%20intrusive%20access%20to%20computational%20models.%20In%20this%20work%2C%20we%0Aapply%20the%20non-intrusive%20Parametrized-Background%20Data-Weak%20%28PBDW%29%20approach%20to%0Athree-dimensional%20%283D%29%20cardiac%20displacement%20field%20reconstruction%20from%20limited%0AMagnetic%20Resonance%20Image%20%28MRI%29-like%20observations.%20Our%20implementation%20requires%0Aonly%20solution%20snapshots%20--%20no%20governing%20equations%2C%20assembly%20routines%2C%20or%20solver%0Aaccess%20--%20enabling%20immediate%20deployment%20across%20commercial%20and%20research%20codes%0Ausing%20different%20constitutive%20models.%20Additionally%2C%20we%20introduce%20two%0Aenhancements%3A%20an%20H-size%20minibatch%20worst-case%20Orthogonal%20Matching%20Pursuit%20%28wOMP%29%0Aalgorithm%20that%20improves%20Sensor%20Selection%20%28SS%29%20computational%20efficiency%20while%0Amaintaining%20reconstruction%20accuracy%2C%20and%20memory%20optimization%20techniques%0Aexploiting%20block%20matrix%20structures%20in%20vectorial%20problems.%20We%20demonstrate%20the%0Aeffectiveness%20of%20the%20method%20through%20validation%20on%20a%203D%20left%20ventricular%20model%0Awith%20simulated%20scar%20tissue.%20Starting%20with%20noise-free%20reconstruction%2C%20we%0Asystematically%20incorporate%20Gaussian%20noise%20and%20spatial%20sparsity%20mimicking%0Arealistic%20MRI%20acquisition%20protocols.%20Results%20show%20exceptional%20accuracy%20in%0Anoise-free%20conditions%20%28relative%20L2%20error%20of%20order%20O%281e-5%29%29%2C%20robust%20performance%0Awith%2010%25%20noise%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29%2C%20and%20effective%0Areconstruction%20from%20sparse%20measurements%20%28relative%20L2%20error%20of%20order%20O%281e-2%29%29.%0AThe%20online%20reconstruction%20achieves%20four-order-of-magnitude%20computational%0Aspeed-up%20compared%20to%20full%20Finite%20Element%20%28FE%29%20simulations%2C%20with%20reconstruction%0Atimes%20under%20one%20tenth%20of%20second%20for%20sparse%20scenarios%2C%20demonstrating%20significant%0Apotential%20for%20integration%20into%20clinical%20cardiac%20modeling%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14844v1&entry.124074799=Read"},
{"title": "MapAnything: Mapping Urban Assets using Single Street-View Images", "author": "Miriam Louise Carnot and Jonas Kunze and Erik Fastermann and Eric Peukert and Andr\u00e9 Ludwig and Bogdan Franczyk", "abstract": "  To maintain an overview of urban conditions, city administrations manage\ndatabases of objects like traffic signs and trees, complete with their\ngeocoordinates. Incidents such as graffiti or road damage are also relevant. As\ndigitization increases, so does the need for more data and up-to-date\ndatabases, requiring significant manual effort. This paper introduces\nMapAnything, a module that automatically determines the geocoordinates of\nobjects using individual images. Utilizing advanced Metric Depth Estimation\nmodels, MapAnything calculates geocoordinates based on the object's distance\nfrom the camera, geometric principles, and camera specifications. We detail and\nvalidate the module, providing recommendations for automating urban object and\nincident mapping. Our evaluation measures the accuracy of estimated distances\nagainst LiDAR point clouds in urban environments, analyzing performance across\ndistance intervals and semantic areas like roads and vegetation. The module's\neffectiveness is demonstrated through practical use cases involving traffic\nsigns and road damage.\n", "link": "http://arxiv.org/abs/2509.14839v1", "date": "2025-09-18", "relevancy": 2.6972, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5906}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5138}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapAnything%3A%20Mapping%20Urban%20Assets%20using%20Single%20Street-View%20Images&body=Title%3A%20MapAnything%3A%20Mapping%20Urban%20Assets%20using%20Single%20Street-View%20Images%0AAuthor%3A%20Miriam%20Louise%20Carnot%20and%20Jonas%20Kunze%20and%20Erik%20Fastermann%20and%20Eric%20Peukert%20and%20Andr%C3%A9%20Ludwig%20and%20Bogdan%20Franczyk%0AAbstract%3A%20%20%20To%20maintain%20an%20overview%20of%20urban%20conditions%2C%20city%20administrations%20manage%0Adatabases%20of%20objects%20like%20traffic%20signs%20and%20trees%2C%20complete%20with%20their%0Ageocoordinates.%20Incidents%20such%20as%20graffiti%20or%20road%20damage%20are%20also%20relevant.%20As%0Adigitization%20increases%2C%20so%20does%20the%20need%20for%20more%20data%20and%20up-to-date%0Adatabases%2C%20requiring%20significant%20manual%20effort.%20This%20paper%20introduces%0AMapAnything%2C%20a%20module%20that%20automatically%20determines%20the%20geocoordinates%20of%0Aobjects%20using%20individual%20images.%20Utilizing%20advanced%20Metric%20Depth%20Estimation%0Amodels%2C%20MapAnything%20calculates%20geocoordinates%20based%20on%20the%20object%27s%20distance%0Afrom%20the%20camera%2C%20geometric%20principles%2C%20and%20camera%20specifications.%20We%20detail%20and%0Avalidate%20the%20module%2C%20providing%20recommendations%20for%20automating%20urban%20object%20and%0Aincident%20mapping.%20Our%20evaluation%20measures%20the%20accuracy%20of%20estimated%20distances%0Aagainst%20LiDAR%20point%20clouds%20in%20urban%20environments%2C%20analyzing%20performance%20across%0Adistance%20intervals%20and%20semantic%20areas%20like%20roads%20and%20vegetation.%20The%20module%27s%0Aeffectiveness%20is%20demonstrated%20through%20practical%20use%20cases%20involving%20traffic%0Asigns%20and%20road%20damage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapAnything%253A%2520Mapping%2520Urban%2520Assets%2520using%2520Single%2520Street-View%2520Images%26entry.906535625%3DMiriam%2520Louise%2520Carnot%2520and%2520Jonas%2520Kunze%2520and%2520Erik%2520Fastermann%2520and%2520Eric%2520Peukert%2520and%2520Andr%25C3%25A9%2520Ludwig%2520and%2520Bogdan%2520Franczyk%26entry.1292438233%3D%2520%2520To%2520maintain%2520an%2520overview%2520of%2520urban%2520conditions%252C%2520city%2520administrations%2520manage%250Adatabases%2520of%2520objects%2520like%2520traffic%2520signs%2520and%2520trees%252C%2520complete%2520with%2520their%250Ageocoordinates.%2520Incidents%2520such%2520as%2520graffiti%2520or%2520road%2520damage%2520are%2520also%2520relevant.%2520As%250Adigitization%2520increases%252C%2520so%2520does%2520the%2520need%2520for%2520more%2520data%2520and%2520up-to-date%250Adatabases%252C%2520requiring%2520significant%2520manual%2520effort.%2520This%2520paper%2520introduces%250AMapAnything%252C%2520a%2520module%2520that%2520automatically%2520determines%2520the%2520geocoordinates%2520of%250Aobjects%2520using%2520individual%2520images.%2520Utilizing%2520advanced%2520Metric%2520Depth%2520Estimation%250Amodels%252C%2520MapAnything%2520calculates%2520geocoordinates%2520based%2520on%2520the%2520object%2527s%2520distance%250Afrom%2520the%2520camera%252C%2520geometric%2520principles%252C%2520and%2520camera%2520specifications.%2520We%2520detail%2520and%250Avalidate%2520the%2520module%252C%2520providing%2520recommendations%2520for%2520automating%2520urban%2520object%2520and%250Aincident%2520mapping.%2520Our%2520evaluation%2520measures%2520the%2520accuracy%2520of%2520estimated%2520distances%250Aagainst%2520LiDAR%2520point%2520clouds%2520in%2520urban%2520environments%252C%2520analyzing%2520performance%2520across%250Adistance%2520intervals%2520and%2520semantic%2520areas%2520like%2520roads%2520and%2520vegetation.%2520The%2520module%2527s%250Aeffectiveness%2520is%2520demonstrated%2520through%2520practical%2520use%2520cases%2520involving%2520traffic%250Asigns%2520and%2520road%2520damage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapAnything%3A%20Mapping%20Urban%20Assets%20using%20Single%20Street-View%20Images&entry.906535625=Miriam%20Louise%20Carnot%20and%20Jonas%20Kunze%20and%20Erik%20Fastermann%20and%20Eric%20Peukert%20and%20Andr%C3%A9%20Ludwig%20and%20Bogdan%20Franczyk&entry.1292438233=%20%20To%20maintain%20an%20overview%20of%20urban%20conditions%2C%20city%20administrations%20manage%0Adatabases%20of%20objects%20like%20traffic%20signs%20and%20trees%2C%20complete%20with%20their%0Ageocoordinates.%20Incidents%20such%20as%20graffiti%20or%20road%20damage%20are%20also%20relevant.%20As%0Adigitization%20increases%2C%20so%20does%20the%20need%20for%20more%20data%20and%20up-to-date%0Adatabases%2C%20requiring%20significant%20manual%20effort.%20This%20paper%20introduces%0AMapAnything%2C%20a%20module%20that%20automatically%20determines%20the%20geocoordinates%20of%0Aobjects%20using%20individual%20images.%20Utilizing%20advanced%20Metric%20Depth%20Estimation%0Amodels%2C%20MapAnything%20calculates%20geocoordinates%20based%20on%20the%20object%27s%20distance%0Afrom%20the%20camera%2C%20geometric%20principles%2C%20and%20camera%20specifications.%20We%20detail%20and%0Avalidate%20the%20module%2C%20providing%20recommendations%20for%20automating%20urban%20object%20and%0Aincident%20mapping.%20Our%20evaluation%20measures%20the%20accuracy%20of%20estimated%20distances%0Aagainst%20LiDAR%20point%20clouds%20in%20urban%20environments%2C%20analyzing%20performance%20across%0Adistance%20intervals%20and%20semantic%20areas%20like%20roads%20and%20vegetation.%20The%20module%27s%0Aeffectiveness%20is%20demonstrated%20through%20practical%20use%20cases%20involving%20traffic%0Asigns%20and%20road%20damage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14839v1&entry.124074799=Read"},
{"title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware\n  Diffusion Model", "author": "Fangjinhua Wang and Qingshan Xu and Yew-Soon Ong and Marc Pollefeys", "abstract": "  To reconstruct the 3D geometry from calibrated images, learning-based\nmulti-view stereo (MVS) methods typically perform multi-view depth estimation\nand then fuse depth maps into a mesh or point cloud. To improve the\ncomputational efficiency, many methods initialize a coarse depth map and then\ngradually refine it in higher resolutions. Recently, diffusion models achieve\ngreat success in generation tasks. Starting from a random noise, diffusion\nmodels gradually recover the sample with an iterative denoising process. In\nthis paper, we propose a novel MVS framework, which introduces diffusion models\nin MVS. Specifically, we formulate depth refinement as a conditional diffusion\nprocess. Considering the discriminative characteristic of depth estimation, we\ndesign a condition encoder to guide the diffusion process. To improve\nefficiency, we propose a novel diffusion network combining lightweight 2D U-Net\nand convolutional GRU. Moreover, we propose a novel confidence-based sampling\nstrategy to adaptively sample depth hypotheses based on the confidence\nestimated by diffusion model. Based on our novel MVS framework, we propose two\nnovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive\nperformance with state-of-the-art efficiency in run-time and GPU memory.\nCasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and\nETH3D. Code is available at: https://github.com/cvg/diffmvs.\n", "link": "http://arxiv.org/abs/2509.15220v1", "date": "2025-09-18", "relevancy": 2.696, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6789}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6789}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20and%20Accurate%20Multi-View%20Stereo%20with%20Confidence-Aware%0A%20%20Diffusion%20Model&body=Title%3A%20Lightweight%20and%20Accurate%20Multi-View%20Stereo%20with%20Confidence-Aware%0A%20%20Diffusion%20Model%0AAuthor%3A%20Fangjinhua%20Wang%20and%20Qingshan%20Xu%20and%20Yew-Soon%20Ong%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20To%20reconstruct%20the%203D%20geometry%20from%20calibrated%20images%2C%20learning-based%0Amulti-view%20stereo%20%28MVS%29%20methods%20typically%20perform%20multi-view%20depth%20estimation%0Aand%20then%20fuse%20depth%20maps%20into%20a%20mesh%20or%20point%20cloud.%20To%20improve%20the%0Acomputational%20efficiency%2C%20many%20methods%20initialize%20a%20coarse%20depth%20map%20and%20then%0Agradually%20refine%20it%20in%20higher%20resolutions.%20Recently%2C%20diffusion%20models%20achieve%0Agreat%20success%20in%20generation%20tasks.%20Starting%20from%20a%20random%20noise%2C%20diffusion%0Amodels%20gradually%20recover%20the%20sample%20with%20an%20iterative%20denoising%20process.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20MVS%20framework%2C%20which%20introduces%20diffusion%20models%0Ain%20MVS.%20Specifically%2C%20we%20formulate%20depth%20refinement%20as%20a%20conditional%20diffusion%0Aprocess.%20Considering%20the%20discriminative%20characteristic%20of%20depth%20estimation%2C%20we%0Adesign%20a%20condition%20encoder%20to%20guide%20the%20diffusion%20process.%20To%20improve%0Aefficiency%2C%20we%20propose%20a%20novel%20diffusion%20network%20combining%20lightweight%202D%20U-Net%0Aand%20convolutional%20GRU.%20Moreover%2C%20we%20propose%20a%20novel%20confidence-based%20sampling%0Astrategy%20to%20adaptively%20sample%20depth%20hypotheses%20based%20on%20the%20confidence%0Aestimated%20by%20diffusion%20model.%20Based%20on%20our%20novel%20MVS%20framework%2C%20we%20propose%20two%0Anovel%20MVS%20methods%2C%20DiffMVS%20and%20CasDiffMVS.%20DiffMVS%20achieves%20competitive%0Aperformance%20with%20state-of-the-art%20efficiency%20in%20run-time%20and%20GPU%20memory.%0ACasDiffMVS%20achieves%20state-of-the-art%20performance%20on%20DTU%2C%20Tanks%20%26%20Temples%20and%0AETH3D.%20Code%20is%20available%20at%3A%20https%3A//github.com/cvg/diffmvs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520and%2520Accurate%2520Multi-View%2520Stereo%2520with%2520Confidence-Aware%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DFangjinhua%2520Wang%2520and%2520Qingshan%2520Xu%2520and%2520Yew-Soon%2520Ong%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520To%2520reconstruct%2520the%25203D%2520geometry%2520from%2520calibrated%2520images%252C%2520learning-based%250Amulti-view%2520stereo%2520%2528MVS%2529%2520methods%2520typically%2520perform%2520multi-view%2520depth%2520estimation%250Aand%2520then%2520fuse%2520depth%2520maps%2520into%2520a%2520mesh%2520or%2520point%2520cloud.%2520To%2520improve%2520the%250Acomputational%2520efficiency%252C%2520many%2520methods%2520initialize%2520a%2520coarse%2520depth%2520map%2520and%2520then%250Agradually%2520refine%2520it%2520in%2520higher%2520resolutions.%2520Recently%252C%2520diffusion%2520models%2520achieve%250Agreat%2520success%2520in%2520generation%2520tasks.%2520Starting%2520from%2520a%2520random%2520noise%252C%2520diffusion%250Amodels%2520gradually%2520recover%2520the%2520sample%2520with%2520an%2520iterative%2520denoising%2520process.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520MVS%2520framework%252C%2520which%2520introduces%2520diffusion%2520models%250Ain%2520MVS.%2520Specifically%252C%2520we%2520formulate%2520depth%2520refinement%2520as%2520a%2520conditional%2520diffusion%250Aprocess.%2520Considering%2520the%2520discriminative%2520characteristic%2520of%2520depth%2520estimation%252C%2520we%250Adesign%2520a%2520condition%2520encoder%2520to%2520guide%2520the%2520diffusion%2520process.%2520To%2520improve%250Aefficiency%252C%2520we%2520propose%2520a%2520novel%2520diffusion%2520network%2520combining%2520lightweight%25202D%2520U-Net%250Aand%2520convolutional%2520GRU.%2520Moreover%252C%2520we%2520propose%2520a%2520novel%2520confidence-based%2520sampling%250Astrategy%2520to%2520adaptively%2520sample%2520depth%2520hypotheses%2520based%2520on%2520the%2520confidence%250Aestimated%2520by%2520diffusion%2520model.%2520Based%2520on%2520our%2520novel%2520MVS%2520framework%252C%2520we%2520propose%2520two%250Anovel%2520MVS%2520methods%252C%2520DiffMVS%2520and%2520CasDiffMVS.%2520DiffMVS%2520achieves%2520competitive%250Aperformance%2520with%2520state-of-the-art%2520efficiency%2520in%2520run-time%2520and%2520GPU%2520memory.%250ACasDiffMVS%2520achieves%2520state-of-the-art%2520performance%2520on%2520DTU%252C%2520Tanks%2520%2526%2520Temples%2520and%250AETH3D.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/cvg/diffmvs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20and%20Accurate%20Multi-View%20Stereo%20with%20Confidence-Aware%0A%20%20Diffusion%20Model&entry.906535625=Fangjinhua%20Wang%20and%20Qingshan%20Xu%20and%20Yew-Soon%20Ong%20and%20Marc%20Pollefeys&entry.1292438233=%20%20To%20reconstruct%20the%203D%20geometry%20from%20calibrated%20images%2C%20learning-based%0Amulti-view%20stereo%20%28MVS%29%20methods%20typically%20perform%20multi-view%20depth%20estimation%0Aand%20then%20fuse%20depth%20maps%20into%20a%20mesh%20or%20point%20cloud.%20To%20improve%20the%0Acomputational%20efficiency%2C%20many%20methods%20initialize%20a%20coarse%20depth%20map%20and%20then%0Agradually%20refine%20it%20in%20higher%20resolutions.%20Recently%2C%20diffusion%20models%20achieve%0Agreat%20success%20in%20generation%20tasks.%20Starting%20from%20a%20random%20noise%2C%20diffusion%0Amodels%20gradually%20recover%20the%20sample%20with%20an%20iterative%20denoising%20process.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20MVS%20framework%2C%20which%20introduces%20diffusion%20models%0Ain%20MVS.%20Specifically%2C%20we%20formulate%20depth%20refinement%20as%20a%20conditional%20diffusion%0Aprocess.%20Considering%20the%20discriminative%20characteristic%20of%20depth%20estimation%2C%20we%0Adesign%20a%20condition%20encoder%20to%20guide%20the%20diffusion%20process.%20To%20improve%0Aefficiency%2C%20we%20propose%20a%20novel%20diffusion%20network%20combining%20lightweight%202D%20U-Net%0Aand%20convolutional%20GRU.%20Moreover%2C%20we%20propose%20a%20novel%20confidence-based%20sampling%0Astrategy%20to%20adaptively%20sample%20depth%20hypotheses%20based%20on%20the%20confidence%0Aestimated%20by%20diffusion%20model.%20Based%20on%20our%20novel%20MVS%20framework%2C%20we%20propose%20two%0Anovel%20MVS%20methods%2C%20DiffMVS%20and%20CasDiffMVS.%20DiffMVS%20achieves%20competitive%0Aperformance%20with%20state-of-the-art%20efficiency%20in%20run-time%20and%20GPU%20memory.%0ACasDiffMVS%20achieves%20state-of-the-art%20performance%20on%20DTU%2C%20Tanks%20%26%20Temples%20and%0AETH3D.%20Code%20is%20available%20at%3A%20https%3A//github.com/cvg/diffmvs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15220v1&entry.124074799=Read"},
{"title": "FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes", "author": "Muraam Abdel-Ghani and Mahmoud Ali and Mohamed Ali and Fatmaelzahraa Ahmed and Muhammad Arsalan and Abdulaziz Al-Ali and Shidin Balakrishnan", "abstract": "  The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.\n", "link": "http://arxiv.org/abs/2509.06159v2", "date": "2025-09-18", "relevancy": 2.6741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FASL-Seg%3A%20Anatomy%20and%20Tool%20Segmentation%20of%20Surgical%20Scenes&body=Title%3A%20FASL-Seg%3A%20Anatomy%20and%20Tool%20Segmentation%20of%20Surgical%20Scenes%0AAuthor%3A%20Muraam%20Abdel-Ghani%20and%20Mahmoud%20Ali%20and%20Mohamed%20Ali%20and%20Fatmaelzahraa%20Ahmed%20and%20Muhammad%20Arsalan%20and%20Abdulaziz%20Al-Ali%20and%20Shidin%20Balakrishnan%0AAbstract%3A%20%20%20The%20growing%20popularity%20of%20robotic%20minimally%20invasive%20surgeries%20has%20made%20deep%0Alearning-based%20surgical%20training%20a%20key%20area%20of%20research.%20A%20thorough%0Aunderstanding%20of%20the%20surgical%20scene%20components%20is%20crucial%2C%20which%20semantic%0Asegmentation%20models%20can%20help%20achieve.%20However%2C%20most%20existing%20work%20focuses%20on%0Asurgical%20tools%20and%20overlooks%20anatomical%20objects.%20Additionally%2C%20current%0Astate-of-the-art%20%28SOTA%29%20models%20struggle%20to%20balance%20capturing%20high-level%0Acontextual%20features%20and%20low-level%20edge%20features.%20We%20propose%20a%20Feature-Adaptive%0ASpatial%20Localization%20model%20%28FASL-Seg%29%2C%20designed%20to%20capture%20features%20at%20multiple%0Alevels%20of%20detail%20through%20two%20distinct%20processing%20streams%2C%20namely%20a%20Low-Level%0AFeature%20Projection%20%28LLFP%29%20and%20a%20High-Level%20Feature%20Projection%20%28HLFP%29%20stream%2C%0Afor%20varying%20feature%20resolutions%20-%20enabling%20precise%20segmentation%20of%20anatomy%20and%0Asurgical%20instruments.%20We%20evaluated%20FASL-Seg%20on%20surgical%20segmentation%20benchmark%0Adatasets%20EndoVis18%20and%20EndoVis17%20on%20three%20use%20cases.%20The%20FASL-Seg%20model%0Aachieves%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2072.71%25%20on%20parts%20and%20anatomy%0Asegmentation%20in%20EndoVis18%2C%20improving%20on%20SOTA%20by%205%25.%20It%20further%20achieves%20a%20mIoU%0Aof%2085.61%25%20and%2072.78%25%20in%20EndoVis18%20and%20EndoVis17%20tool%20type%20segmentation%2C%0Arespectively%2C%20outperforming%20SOTA%20overall%20performance%2C%20with%20comparable%20per-class%0ASOTA%20results%20in%20both%20datasets%20and%20consistent%20performance%20in%20various%20classes%20for%0Aanatomy%20and%20instruments%2C%20demonstrating%20the%20effectiveness%20of%20distinct%20processing%0Astreams%20for%20varying%20feature%20resolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFASL-Seg%253A%2520Anatomy%2520and%2520Tool%2520Segmentation%2520of%2520Surgical%2520Scenes%26entry.906535625%3DMuraam%2520Abdel-Ghani%2520and%2520Mahmoud%2520Ali%2520and%2520Mohamed%2520Ali%2520and%2520Fatmaelzahraa%2520Ahmed%2520and%2520Muhammad%2520Arsalan%2520and%2520Abdulaziz%2520Al-Ali%2520and%2520Shidin%2520Balakrishnan%26entry.1292438233%3D%2520%2520The%2520growing%2520popularity%2520of%2520robotic%2520minimally%2520invasive%2520surgeries%2520has%2520made%2520deep%250Alearning-based%2520surgical%2520training%2520a%2520key%2520area%2520of%2520research.%2520A%2520thorough%250Aunderstanding%2520of%2520the%2520surgical%2520scene%2520components%2520is%2520crucial%252C%2520which%2520semantic%250Asegmentation%2520models%2520can%2520help%2520achieve.%2520However%252C%2520most%2520existing%2520work%2520focuses%2520on%250Asurgical%2520tools%2520and%2520overlooks%2520anatomical%2520objects.%2520Additionally%252C%2520current%250Astate-of-the-art%2520%2528SOTA%2529%2520models%2520struggle%2520to%2520balance%2520capturing%2520high-level%250Acontextual%2520features%2520and%2520low-level%2520edge%2520features.%2520We%2520propose%2520a%2520Feature-Adaptive%250ASpatial%2520Localization%2520model%2520%2528FASL-Seg%2529%252C%2520designed%2520to%2520capture%2520features%2520at%2520multiple%250Alevels%2520of%2520detail%2520through%2520two%2520distinct%2520processing%2520streams%252C%2520namely%2520a%2520Low-Level%250AFeature%2520Projection%2520%2528LLFP%2529%2520and%2520a%2520High-Level%2520Feature%2520Projection%2520%2528HLFP%2529%2520stream%252C%250Afor%2520varying%2520feature%2520resolutions%2520-%2520enabling%2520precise%2520segmentation%2520of%2520anatomy%2520and%250Asurgical%2520instruments.%2520We%2520evaluated%2520FASL-Seg%2520on%2520surgical%2520segmentation%2520benchmark%250Adatasets%2520EndoVis18%2520and%2520EndoVis17%2520on%2520three%2520use%2520cases.%2520The%2520FASL-Seg%2520model%250Aachieves%2520a%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520of%252072.71%2525%2520on%2520parts%2520and%2520anatomy%250Asegmentation%2520in%2520EndoVis18%252C%2520improving%2520on%2520SOTA%2520by%25205%2525.%2520It%2520further%2520achieves%2520a%2520mIoU%250Aof%252085.61%2525%2520and%252072.78%2525%2520in%2520EndoVis18%2520and%2520EndoVis17%2520tool%2520type%2520segmentation%252C%250Arespectively%252C%2520outperforming%2520SOTA%2520overall%2520performance%252C%2520with%2520comparable%2520per-class%250ASOTA%2520results%2520in%2520both%2520datasets%2520and%2520consistent%2520performance%2520in%2520various%2520classes%2520for%250Aanatomy%2520and%2520instruments%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520distinct%2520processing%250Astreams%2520for%2520varying%2520feature%2520resolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FASL-Seg%3A%20Anatomy%20and%20Tool%20Segmentation%20of%20Surgical%20Scenes&entry.906535625=Muraam%20Abdel-Ghani%20and%20Mahmoud%20Ali%20and%20Mohamed%20Ali%20and%20Fatmaelzahraa%20Ahmed%20and%20Muhammad%20Arsalan%20and%20Abdulaziz%20Al-Ali%20and%20Shidin%20Balakrishnan&entry.1292438233=%20%20The%20growing%20popularity%20of%20robotic%20minimally%20invasive%20surgeries%20has%20made%20deep%0Alearning-based%20surgical%20training%20a%20key%20area%20of%20research.%20A%20thorough%0Aunderstanding%20of%20the%20surgical%20scene%20components%20is%20crucial%2C%20which%20semantic%0Asegmentation%20models%20can%20help%20achieve.%20However%2C%20most%20existing%20work%20focuses%20on%0Asurgical%20tools%20and%20overlooks%20anatomical%20objects.%20Additionally%2C%20current%0Astate-of-the-art%20%28SOTA%29%20models%20struggle%20to%20balance%20capturing%20high-level%0Acontextual%20features%20and%20low-level%20edge%20features.%20We%20propose%20a%20Feature-Adaptive%0ASpatial%20Localization%20model%20%28FASL-Seg%29%2C%20designed%20to%20capture%20features%20at%20multiple%0Alevels%20of%20detail%20through%20two%20distinct%20processing%20streams%2C%20namely%20a%20Low-Level%0AFeature%20Projection%20%28LLFP%29%20and%20a%20High-Level%20Feature%20Projection%20%28HLFP%29%20stream%2C%0Afor%20varying%20feature%20resolutions%20-%20enabling%20precise%20segmentation%20of%20anatomy%20and%0Asurgical%20instruments.%20We%20evaluated%20FASL-Seg%20on%20surgical%20segmentation%20benchmark%0Adatasets%20EndoVis18%20and%20EndoVis17%20on%20three%20use%20cases.%20The%20FASL-Seg%20model%0Aachieves%20a%20mean%20Intersection%20over%20Union%20%28mIoU%29%20of%2072.71%25%20on%20parts%20and%20anatomy%0Asegmentation%20in%20EndoVis18%2C%20improving%20on%20SOTA%20by%205%25.%20It%20further%20achieves%20a%20mIoU%0Aof%2085.61%25%20and%2072.78%25%20in%20EndoVis18%20and%20EndoVis17%20tool%20type%20segmentation%2C%0Arespectively%2C%20outperforming%20SOTA%20overall%20performance%2C%20with%20comparable%20per-class%0ASOTA%20results%20in%20both%20datasets%20and%20consistent%20performance%20in%20various%20classes%20for%0Aanatomy%20and%20instruments%2C%20demonstrating%20the%20effectiveness%20of%20distinct%20processing%0Astreams%20for%20varying%20feature%20resolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06159v2&entry.124074799=Read"},
{"title": "PRISM: Product Retrieval In Shopping Carts using Hybrid Matching", "author": "Arda Kabadayi and Senem Velipasalar and Jiajing Chen", "abstract": "  Compared to traditional image retrieval tasks, product retrieval in retail\nsettings is even more challenging. Products of the same type from different\nbrands may have highly similar visual appearances, and the query image may be\ntaken from an angle that differs significantly from view angles of the stored\ncatalog images. Foundational models, such as CLIP and SigLIP, often struggle to\ndistinguish these subtle but important local differences. Pixel-wise matching\nmethods, on the other hand, are computationally expensive and incur\nprohibitively high matching times. In this paper, we propose a new, hybrid\nmethod, called PRISM, for product retrieval in retail settings by leveraging\nthe advantages of both vision-language model-based and pixel-wise matching\napproaches. To provide both efficiency/speed and finegrained retrieval\naccuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP)\nis employed first to retrieve the top 35 most semantically similar products\nfrom a fixed gallery, thereby narrowing the search space significantly; 2) a\nsegmentation model (YOLO-E) is applied to eliminate background clutter; 3)\nfine-grained pixel-level matching is performed using LightGlue across the\nfiltered candidates. This framework enables more accurate discrimination\nbetween products with high inter-class similarity by focusing on subtle visual\ncues often missed by global models. Experiments performed on the ABV dataset\nshow that our proposed PRISM outperforms the state-of-the-art image retrieval\nmethods by 4.21% in top-1 accuracy while still remaining within the bounds of\nreal-time processing for practical retail deployments.\n", "link": "http://arxiv.org/abs/2509.14985v1", "date": "2025-09-18", "relevancy": 2.6339, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5824}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM%3A%20Product%20Retrieval%20In%20Shopping%20Carts%20using%20Hybrid%20Matching&body=Title%3A%20PRISM%3A%20Product%20Retrieval%20In%20Shopping%20Carts%20using%20Hybrid%20Matching%0AAuthor%3A%20Arda%20Kabadayi%20and%20Senem%20Velipasalar%20and%20Jiajing%20Chen%0AAbstract%3A%20%20%20Compared%20to%20traditional%20image%20retrieval%20tasks%2C%20product%20retrieval%20in%20retail%0Asettings%20is%20even%20more%20challenging.%20Products%20of%20the%20same%20type%20from%20different%0Abrands%20may%20have%20highly%20similar%20visual%20appearances%2C%20and%20the%20query%20image%20may%20be%0Ataken%20from%20an%20angle%20that%20differs%20significantly%20from%20view%20angles%20of%20the%20stored%0Acatalog%20images.%20Foundational%20models%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20often%20struggle%20to%0Adistinguish%20these%20subtle%20but%20important%20local%20differences.%20Pixel-wise%20matching%0Amethods%2C%20on%20the%20other%20hand%2C%20are%20computationally%20expensive%20and%20incur%0Aprohibitively%20high%20matching%20times.%20In%20this%20paper%2C%20we%20propose%20a%20new%2C%20hybrid%0Amethod%2C%20called%20PRISM%2C%20for%20product%20retrieval%20in%20retail%20settings%20by%20leveraging%0Athe%20advantages%20of%20both%20vision-language%20model-based%20and%20pixel-wise%20matching%0Aapproaches.%20To%20provide%20both%20efficiency/speed%20and%20finegrained%20retrieval%0Aaccuracy%2C%20PRISM%20consists%20of%20three%20stages%3A%201%29%20A%20vision-language%20model%20%28SigLIP%29%0Ais%20employed%20first%20to%20retrieve%20the%20top%2035%20most%20semantically%20similar%20products%0Afrom%20a%20fixed%20gallery%2C%20thereby%20narrowing%20the%20search%20space%20significantly%3B%202%29%20a%0Asegmentation%20model%20%28YOLO-E%29%20is%20applied%20to%20eliminate%20background%20clutter%3B%203%29%0Afine-grained%20pixel-level%20matching%20is%20performed%20using%20LightGlue%20across%20the%0Afiltered%20candidates.%20This%20framework%20enables%20more%20accurate%20discrimination%0Abetween%20products%20with%20high%20inter-class%20similarity%20by%20focusing%20on%20subtle%20visual%0Acues%20often%20missed%20by%20global%20models.%20Experiments%20performed%20on%20the%20ABV%20dataset%0Ashow%20that%20our%20proposed%20PRISM%20outperforms%20the%20state-of-the-art%20image%20retrieval%0Amethods%20by%204.21%25%20in%20top-1%20accuracy%20while%20still%20remaining%20within%20the%20bounds%20of%0Areal-time%20processing%20for%20practical%20retail%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM%253A%2520Product%2520Retrieval%2520In%2520Shopping%2520Carts%2520using%2520Hybrid%2520Matching%26entry.906535625%3DArda%2520Kabadayi%2520and%2520Senem%2520Velipasalar%2520and%2520Jiajing%2520Chen%26entry.1292438233%3D%2520%2520Compared%2520to%2520traditional%2520image%2520retrieval%2520tasks%252C%2520product%2520retrieval%2520in%2520retail%250Asettings%2520is%2520even%2520more%2520challenging.%2520Products%2520of%2520the%2520same%2520type%2520from%2520different%250Abrands%2520may%2520have%2520highly%2520similar%2520visual%2520appearances%252C%2520and%2520the%2520query%2520image%2520may%2520be%250Ataken%2520from%2520an%2520angle%2520that%2520differs%2520significantly%2520from%2520view%2520angles%2520of%2520the%2520stored%250Acatalog%2520images.%2520Foundational%2520models%252C%2520such%2520as%2520CLIP%2520and%2520SigLIP%252C%2520often%2520struggle%2520to%250Adistinguish%2520these%2520subtle%2520but%2520important%2520local%2520differences.%2520Pixel-wise%2520matching%250Amethods%252C%2520on%2520the%2520other%2520hand%252C%2520are%2520computationally%2520expensive%2520and%2520incur%250Aprohibitively%2520high%2520matching%2520times.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%252C%2520hybrid%250Amethod%252C%2520called%2520PRISM%252C%2520for%2520product%2520retrieval%2520in%2520retail%2520settings%2520by%2520leveraging%250Athe%2520advantages%2520of%2520both%2520vision-language%2520model-based%2520and%2520pixel-wise%2520matching%250Aapproaches.%2520To%2520provide%2520both%2520efficiency/speed%2520and%2520finegrained%2520retrieval%250Aaccuracy%252C%2520PRISM%2520consists%2520of%2520three%2520stages%253A%25201%2529%2520A%2520vision-language%2520model%2520%2528SigLIP%2529%250Ais%2520employed%2520first%2520to%2520retrieve%2520the%2520top%252035%2520most%2520semantically%2520similar%2520products%250Afrom%2520a%2520fixed%2520gallery%252C%2520thereby%2520narrowing%2520the%2520search%2520space%2520significantly%253B%25202%2529%2520a%250Asegmentation%2520model%2520%2528YOLO-E%2529%2520is%2520applied%2520to%2520eliminate%2520background%2520clutter%253B%25203%2529%250Afine-grained%2520pixel-level%2520matching%2520is%2520performed%2520using%2520LightGlue%2520across%2520the%250Afiltered%2520candidates.%2520This%2520framework%2520enables%2520more%2520accurate%2520discrimination%250Abetween%2520products%2520with%2520high%2520inter-class%2520similarity%2520by%2520focusing%2520on%2520subtle%2520visual%250Acues%2520often%2520missed%2520by%2520global%2520models.%2520Experiments%2520performed%2520on%2520the%2520ABV%2520dataset%250Ashow%2520that%2520our%2520proposed%2520PRISM%2520outperforms%2520the%2520state-of-the-art%2520image%2520retrieval%250Amethods%2520by%25204.21%2525%2520in%2520top-1%2520accuracy%2520while%2520still%2520remaining%2520within%2520the%2520bounds%2520of%250Areal-time%2520processing%2520for%2520practical%2520retail%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM%3A%20Product%20Retrieval%20In%20Shopping%20Carts%20using%20Hybrid%20Matching&entry.906535625=Arda%20Kabadayi%20and%20Senem%20Velipasalar%20and%20Jiajing%20Chen&entry.1292438233=%20%20Compared%20to%20traditional%20image%20retrieval%20tasks%2C%20product%20retrieval%20in%20retail%0Asettings%20is%20even%20more%20challenging.%20Products%20of%20the%20same%20type%20from%20different%0Abrands%20may%20have%20highly%20similar%20visual%20appearances%2C%20and%20the%20query%20image%20may%20be%0Ataken%20from%20an%20angle%20that%20differs%20significantly%20from%20view%20angles%20of%20the%20stored%0Acatalog%20images.%20Foundational%20models%2C%20such%20as%20CLIP%20and%20SigLIP%2C%20often%20struggle%20to%0Adistinguish%20these%20subtle%20but%20important%20local%20differences.%20Pixel-wise%20matching%0Amethods%2C%20on%20the%20other%20hand%2C%20are%20computationally%20expensive%20and%20incur%0Aprohibitively%20high%20matching%20times.%20In%20this%20paper%2C%20we%20propose%20a%20new%2C%20hybrid%0Amethod%2C%20called%20PRISM%2C%20for%20product%20retrieval%20in%20retail%20settings%20by%20leveraging%0Athe%20advantages%20of%20both%20vision-language%20model-based%20and%20pixel-wise%20matching%0Aapproaches.%20To%20provide%20both%20efficiency/speed%20and%20finegrained%20retrieval%0Aaccuracy%2C%20PRISM%20consists%20of%20three%20stages%3A%201%29%20A%20vision-language%20model%20%28SigLIP%29%0Ais%20employed%20first%20to%20retrieve%20the%20top%2035%20most%20semantically%20similar%20products%0Afrom%20a%20fixed%20gallery%2C%20thereby%20narrowing%20the%20search%20space%20significantly%3B%202%29%20a%0Asegmentation%20model%20%28YOLO-E%29%20is%20applied%20to%20eliminate%20background%20clutter%3B%203%29%0Afine-grained%20pixel-level%20matching%20is%20performed%20using%20LightGlue%20across%20the%0Afiltered%20candidates.%20This%20framework%20enables%20more%20accurate%20discrimination%0Abetween%20products%20with%20high%20inter-class%20similarity%20by%20focusing%20on%20subtle%20visual%0Acues%20often%20missed%20by%20global%20models.%20Experiments%20performed%20on%20the%20ABV%20dataset%0Ashow%20that%20our%20proposed%20PRISM%20outperforms%20the%20state-of-the-art%20image%20retrieval%0Amethods%20by%204.21%25%20in%20top-1%20accuracy%20while%20still%20remaining%20within%20the%20bounds%20of%0Areal-time%20processing%20for%20practical%20retail%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14985v1&entry.124074799=Read"},
{"title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation", "author": "Chuan Fang and Heng Li and Yixun Liang and Jia Zheng and Yongsen Mao and Yuan Liu and Rui Tang and Zihan Zhou and Ping Tan", "abstract": "  Creating high-fidelity 3D models of indoor environments is essential for\napplications in design, virtual reality, and robotics. However, manual 3D\nmodeling remains time-consuming and labor-intensive. While recent advances in\ngenerative AI have enabled automated scene synthesis, existing methods often\nface challenges in balancing visual quality, diversity, semantic consistency,\nand user control. A major bottleneck is the lack of a large-scale, high-quality\ndataset tailored to this task. To address this gap, we introduce a\ncomprehensive synthetic dataset, featuring 12,328 structured annotated scenes\nwith 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this\ndataset, we present SpatialGen, a novel multi-view multi-modal diffusion model\nthat generates realistic and semantically consistent 3D indoor scenes. Given a\n3D layout and a reference image (derived from a text prompt), our model\nsynthesizes appearance (color image), geometry (scene coordinate map), and\nsemantic (semantic segmentation map) from arbitrary viewpoints, while\npreserving spatial consistency across modalities. SpatialGen consistently\ngenerates superior results to previous methods in our experiments. We are\nopen-sourcing our data and models to empower the community and advance the\nfield of indoor scene understanding and generation.\n", "link": "http://arxiv.org/abs/2509.14981v1", "date": "2025-09-18", "relevancy": 2.6119, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6536}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.65}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation&body=Title%3A%20SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation%0AAuthor%3A%20Chuan%20Fang%20and%20Heng%20Li%20and%20Yixun%20Liang%20and%20Jia%20Zheng%20and%20Yongsen%20Mao%20and%20Yuan%20Liu%20and%20Rui%20Tang%20and%20Zihan%20Zhou%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Creating%20high-fidelity%203D%20models%20of%20indoor%20environments%20is%20essential%20for%0Aapplications%20in%20design%2C%20virtual%20reality%2C%20and%20robotics.%20However%2C%20manual%203D%0Amodeling%20remains%20time-consuming%20and%20labor-intensive.%20While%20recent%20advances%20in%0Agenerative%20AI%20have%20enabled%20automated%20scene%20synthesis%2C%20existing%20methods%20often%0Aface%20challenges%20in%20balancing%20visual%20quality%2C%20diversity%2C%20semantic%20consistency%2C%0Aand%20user%20control.%20A%20major%20bottleneck%20is%20the%20lack%20of%20a%20large-scale%2C%20high-quality%0Adataset%20tailored%20to%20this%20task.%20To%20address%20this%20gap%2C%20we%20introduce%20a%0Acomprehensive%20synthetic%20dataset%2C%20featuring%2012%2C328%20structured%20annotated%20scenes%0Awith%2057%2C440%20rooms%2C%20and%204.7M%20photorealistic%202D%20renderings.%20Leveraging%20this%0Adataset%2C%20we%20present%20SpatialGen%2C%20a%20novel%20multi-view%20multi-modal%20diffusion%20model%0Athat%20generates%20realistic%20and%20semantically%20consistent%203D%20indoor%20scenes.%20Given%20a%0A3D%20layout%20and%20a%20reference%20image%20%28derived%20from%20a%20text%20prompt%29%2C%20our%20model%0Asynthesizes%20appearance%20%28color%20image%29%2C%20geometry%20%28scene%20coordinate%20map%29%2C%20and%0Asemantic%20%28semantic%20segmentation%20map%29%20from%20arbitrary%20viewpoints%2C%20while%0Apreserving%20spatial%20consistency%20across%20modalities.%20SpatialGen%20consistently%0Agenerates%20superior%20results%20to%20previous%20methods%20in%20our%20experiments.%20We%20are%0Aopen-sourcing%20our%20data%20and%20models%20to%20empower%20the%20community%20and%20advance%20the%0Afield%20of%20indoor%20scene%20understanding%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPATIALGEN%253A%2520Layout-guided%25203D%2520Indoor%2520Scene%2520Generation%26entry.906535625%3DChuan%2520Fang%2520and%2520Heng%2520Li%2520and%2520Yixun%2520Liang%2520and%2520Jia%2520Zheng%2520and%2520Yongsen%2520Mao%2520and%2520Yuan%2520Liu%2520and%2520Rui%2520Tang%2520and%2520Zihan%2520Zhou%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Creating%2520high-fidelity%25203D%2520models%2520of%2520indoor%2520environments%2520is%2520essential%2520for%250Aapplications%2520in%2520design%252C%2520virtual%2520reality%252C%2520and%2520robotics.%2520However%252C%2520manual%25203D%250Amodeling%2520remains%2520time-consuming%2520and%2520labor-intensive.%2520While%2520recent%2520advances%2520in%250Agenerative%2520AI%2520have%2520enabled%2520automated%2520scene%2520synthesis%252C%2520existing%2520methods%2520often%250Aface%2520challenges%2520in%2520balancing%2520visual%2520quality%252C%2520diversity%252C%2520semantic%2520consistency%252C%250Aand%2520user%2520control.%2520A%2520major%2520bottleneck%2520is%2520the%2520lack%2520of%2520a%2520large-scale%252C%2520high-quality%250Adataset%2520tailored%2520to%2520this%2520task.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%250Acomprehensive%2520synthetic%2520dataset%252C%2520featuring%252012%252C328%2520structured%2520annotated%2520scenes%250Awith%252057%252C440%2520rooms%252C%2520and%25204.7M%2520photorealistic%25202D%2520renderings.%2520Leveraging%2520this%250Adataset%252C%2520we%2520present%2520SpatialGen%252C%2520a%2520novel%2520multi-view%2520multi-modal%2520diffusion%2520model%250Athat%2520generates%2520realistic%2520and%2520semantically%2520consistent%25203D%2520indoor%2520scenes.%2520Given%2520a%250A3D%2520layout%2520and%2520a%2520reference%2520image%2520%2528derived%2520from%2520a%2520text%2520prompt%2529%252C%2520our%2520model%250Asynthesizes%2520appearance%2520%2528color%2520image%2529%252C%2520geometry%2520%2528scene%2520coordinate%2520map%2529%252C%2520and%250Asemantic%2520%2528semantic%2520segmentation%2520map%2529%2520from%2520arbitrary%2520viewpoints%252C%2520while%250Apreserving%2520spatial%2520consistency%2520across%2520modalities.%2520SpatialGen%2520consistently%250Agenerates%2520superior%2520results%2520to%2520previous%2520methods%2520in%2520our%2520experiments.%2520We%2520are%250Aopen-sourcing%2520our%2520data%2520and%2520models%2520to%2520empower%2520the%2520community%2520and%2520advance%2520the%250Afield%2520of%2520indoor%2520scene%2520understanding%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation&entry.906535625=Chuan%20Fang%20and%20Heng%20Li%20and%20Yixun%20Liang%20and%20Jia%20Zheng%20and%20Yongsen%20Mao%20and%20Yuan%20Liu%20and%20Rui%20Tang%20and%20Zihan%20Zhou%20and%20Ping%20Tan&entry.1292438233=%20%20Creating%20high-fidelity%203D%20models%20of%20indoor%20environments%20is%20essential%20for%0Aapplications%20in%20design%2C%20virtual%20reality%2C%20and%20robotics.%20However%2C%20manual%203D%0Amodeling%20remains%20time-consuming%20and%20labor-intensive.%20While%20recent%20advances%20in%0Agenerative%20AI%20have%20enabled%20automated%20scene%20synthesis%2C%20existing%20methods%20often%0Aface%20challenges%20in%20balancing%20visual%20quality%2C%20diversity%2C%20semantic%20consistency%2C%0Aand%20user%20control.%20A%20major%20bottleneck%20is%20the%20lack%20of%20a%20large-scale%2C%20high-quality%0Adataset%20tailored%20to%20this%20task.%20To%20address%20this%20gap%2C%20we%20introduce%20a%0Acomprehensive%20synthetic%20dataset%2C%20featuring%2012%2C328%20structured%20annotated%20scenes%0Awith%2057%2C440%20rooms%2C%20and%204.7M%20photorealistic%202D%20renderings.%20Leveraging%20this%0Adataset%2C%20we%20present%20SpatialGen%2C%20a%20novel%20multi-view%20multi-modal%20diffusion%20model%0Athat%20generates%20realistic%20and%20semantically%20consistent%203D%20indoor%20scenes.%20Given%20a%0A3D%20layout%20and%20a%20reference%20image%20%28derived%20from%20a%20text%20prompt%29%2C%20our%20model%0Asynthesizes%20appearance%20%28color%20image%29%2C%20geometry%20%28scene%20coordinate%20map%29%2C%20and%0Asemantic%20%28semantic%20segmentation%20map%29%20from%20arbitrary%20viewpoints%2C%20while%0Apreserving%20spatial%20consistency%20across%20modalities.%20SpatialGen%20consistently%0Agenerates%20superior%20results%20to%20previous%20methods%20in%20our%20experiments.%20We%20are%0Aopen-sourcing%20our%20data%20and%20models%20to%20empower%20the%20community%20and%20advance%20the%0Afield%20of%20indoor%20scene%20understanding%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14981v1&entry.124074799=Read"},
{"title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "author": "Liu Liu and Alexandra Kudaeva and Marco Cipriano and Fatimeh Al Ghannam and Freya Tan and Gerard de Melo and Andres Sevtsuk", "abstract": "  Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.\n", "link": "http://arxiv.org/abs/2509.13484v2", "date": "2025-09-18", "relevancy": 2.6073, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MINGLE%3A%20VLMs%20for%20Semantically%20Complex%20Region%20Detection%20in%20Urban%20Scenes&body=Title%3A%20MINGLE%3A%20VLMs%20for%20Semantically%20Complex%20Region%20Detection%20in%20Urban%20Scenes%0AAuthor%3A%20Liu%20Liu%20and%20Alexandra%20Kudaeva%20and%20Marco%20Cipriano%20and%20Fatimeh%20Al%20Ghannam%20and%20Freya%20Tan%20and%20Gerard%20de%20Melo%20and%20Andres%20Sevtsuk%0AAbstract%3A%20%20%20Understanding%20group-level%20social%20interactions%20in%20public%20spaces%20is%20crucial%20for%0Aurban%20planning%2C%20informing%20the%20design%20of%20socially%20vibrant%20and%20inclusive%0Aenvironments.%20Detecting%20such%20interactions%20from%20images%20involves%20interpreting%0Asubtle%20visual%20cues%20such%20as%20relations%2C%20proximity%2C%20and%20co-movement%20-%20semantically%0Acomplex%20signals%20that%20go%20beyond%20traditional%20object%20detection.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20social%20group%20region%20detection%20task%2C%20which%20requires%0Ainferring%20and%20spatially%20grounding%20visual%20regions%20defined%20by%20abstract%0Ainterpersonal%20relations.%20We%20propose%20MINGLE%20%28Modeling%20INterpersonal%20Group-Level%0AEngagement%29%2C%20a%20modular%20three-stage%20pipeline%20that%20integrates%3A%20%281%29%20off-the-shelf%0Ahuman%20detection%20and%20depth%20estimation%2C%20%282%29%20VLM-based%20reasoning%20to%20classify%0Apairwise%20social%20affiliation%2C%20and%20%283%29%20a%20lightweight%20spatial%20aggregation%0Aalgorithm%20to%20localize%20socially%20connected%20groups.%20To%20support%20this%20task%20and%0Aencourage%20future%20research%2C%20we%20present%20a%20new%20dataset%20of%20100K%20urban%20street-view%0Aimages%20annotated%20with%20bounding%20boxes%20and%20labels%20for%20both%20individuals%20and%0Asocially%20interacting%20groups.%20The%20annotations%20combine%20human-created%20labels%20and%0Aoutputs%20from%20the%20MINGLE%20pipeline%2C%20ensuring%20semantic%20richness%20and%20broad%20coverage%0Aof%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13484v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMINGLE%253A%2520VLMs%2520for%2520Semantically%2520Complex%2520Region%2520Detection%2520in%2520Urban%2520Scenes%26entry.906535625%3DLiu%2520Liu%2520and%2520Alexandra%2520Kudaeva%2520and%2520Marco%2520Cipriano%2520and%2520Fatimeh%2520Al%2520Ghannam%2520and%2520Freya%2520Tan%2520and%2520Gerard%2520de%2520Melo%2520and%2520Andres%2520Sevtsuk%26entry.1292438233%3D%2520%2520Understanding%2520group-level%2520social%2520interactions%2520in%2520public%2520spaces%2520is%2520crucial%2520for%250Aurban%2520planning%252C%2520informing%2520the%2520design%2520of%2520socially%2520vibrant%2520and%2520inclusive%250Aenvironments.%2520Detecting%2520such%2520interactions%2520from%2520images%2520involves%2520interpreting%250Asubtle%2520visual%2520cues%2520such%2520as%2520relations%252C%2520proximity%252C%2520and%2520co-movement%2520-%2520semantically%250Acomplex%2520signals%2520that%2520go%2520beyond%2520traditional%2520object%2520detection.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520a%2520social%2520group%2520region%2520detection%2520task%252C%2520which%2520requires%250Ainferring%2520and%2520spatially%2520grounding%2520visual%2520regions%2520defined%2520by%2520abstract%250Ainterpersonal%2520relations.%2520We%2520propose%2520MINGLE%2520%2528Modeling%2520INterpersonal%2520Group-Level%250AEngagement%2529%252C%2520a%2520modular%2520three-stage%2520pipeline%2520that%2520integrates%253A%2520%25281%2529%2520off-the-shelf%250Ahuman%2520detection%2520and%2520depth%2520estimation%252C%2520%25282%2529%2520VLM-based%2520reasoning%2520to%2520classify%250Apairwise%2520social%2520affiliation%252C%2520and%2520%25283%2529%2520a%2520lightweight%2520spatial%2520aggregation%250Aalgorithm%2520to%2520localize%2520socially%2520connected%2520groups.%2520To%2520support%2520this%2520task%2520and%250Aencourage%2520future%2520research%252C%2520we%2520present%2520a%2520new%2520dataset%2520of%2520100K%2520urban%2520street-view%250Aimages%2520annotated%2520with%2520bounding%2520boxes%2520and%2520labels%2520for%2520both%2520individuals%2520and%250Asocially%2520interacting%2520groups.%2520The%2520annotations%2520combine%2520human-created%2520labels%2520and%250Aoutputs%2520from%2520the%2520MINGLE%2520pipeline%252C%2520ensuring%2520semantic%2520richness%2520and%2520broad%2520coverage%250Aof%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13484v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MINGLE%3A%20VLMs%20for%20Semantically%20Complex%20Region%20Detection%20in%20Urban%20Scenes&entry.906535625=Liu%20Liu%20and%20Alexandra%20Kudaeva%20and%20Marco%20Cipriano%20and%20Fatimeh%20Al%20Ghannam%20and%20Freya%20Tan%20and%20Gerard%20de%20Melo%20and%20Andres%20Sevtsuk&entry.1292438233=%20%20Understanding%20group-level%20social%20interactions%20in%20public%20spaces%20is%20crucial%20for%0Aurban%20planning%2C%20informing%20the%20design%20of%20socially%20vibrant%20and%20inclusive%0Aenvironments.%20Detecting%20such%20interactions%20from%20images%20involves%20interpreting%0Asubtle%20visual%20cues%20such%20as%20relations%2C%20proximity%2C%20and%20co-movement%20-%20semantically%0Acomplex%20signals%20that%20go%20beyond%20traditional%20object%20detection.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20a%20social%20group%20region%20detection%20task%2C%20which%20requires%0Ainferring%20and%20spatially%20grounding%20visual%20regions%20defined%20by%20abstract%0Ainterpersonal%20relations.%20We%20propose%20MINGLE%20%28Modeling%20INterpersonal%20Group-Level%0AEngagement%29%2C%20a%20modular%20three-stage%20pipeline%20that%20integrates%3A%20%281%29%20off-the-shelf%0Ahuman%20detection%20and%20depth%20estimation%2C%20%282%29%20VLM-based%20reasoning%20to%20classify%0Apairwise%20social%20affiliation%2C%20and%20%283%29%20a%20lightweight%20spatial%20aggregation%0Aalgorithm%20to%20localize%20socially%20connected%20groups.%20To%20support%20this%20task%20and%0Aencourage%20future%20research%2C%20we%20present%20a%20new%20dataset%20of%20100K%20urban%20street-view%0Aimages%20annotated%20with%20bounding%20boxes%20and%20labels%20for%20both%20individuals%20and%0Asocially%20interacting%20groups.%20The%20annotations%20combine%20human-created%20labels%20and%0Aoutputs%20from%20the%20MINGLE%20pipeline%2C%20ensuring%20semantic%20richness%20and%20broad%20coverage%0Aof%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13484v2&entry.124074799=Read"},
{"title": "Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via\n  Cross-Modal Geometric Rectification", "author": "Xiang Tuo and Xu Xuemiao and Liu Bangzhen and Li Jinyi and Li Yong and He Shengfeng", "abstract": "  The rapid growth of 3D digital content necessitates expandable recognition\nsystems for open-world scenarios. However, existing 3D class-incremental\nlearning methods struggle under extreme data scarcity due to geometric\nmisalignment and texture bias. While recent approaches integrate 3D data with\n2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by\ntexture-biased projections and indiscriminate fusion of geometric-textural\ncues, leading to unstable decision prototypes and catastrophic forgetting. To\naddress these issues, we propose Cross-Modal Geometric Rectification (CMGR), a\nframework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical\nspatial semantics. Specifically, we introduce a Structure-Aware Geometric\nRectification module that hierarchically aligns 3D part structures with CLIP's\nintermediate spatial priors through attention-driven geometric fusion.\nAdditionally, a Texture Amplification Module synthesizes minimal yet\ndiscriminative textures to suppress noise and reinforce cross-modal\nconsistency. To further stabilize incremental prototypes, we employ a\nBase-Novel Discriminator that isolates geometric variations. Extensive\nexperiments demonstrate that our method significantly improves 3D few-shot\nclass-incremental learning, achieving superior geometric coherence and\nrobustness to texture bias across cross-domain and within-domain settings.\n", "link": "http://arxiv.org/abs/2509.14958v1", "date": "2025-09-18", "relevancy": 2.5941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6195}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%203D%20Through%202D%20Lenses%3A%203D%20Few-Shot%20Class-Incremental%20Learning%20via%0A%20%20Cross-Modal%20Geometric%20Rectification&body=Title%3A%20Seeing%203D%20Through%202D%20Lenses%3A%203D%20Few-Shot%20Class-Incremental%20Learning%20via%0A%20%20Cross-Modal%20Geometric%20Rectification%0AAuthor%3A%20Xiang%20Tuo%20and%20Xu%20Xuemiao%20and%20Liu%20Bangzhen%20and%20Li%20Jinyi%20and%20Li%20Yong%20and%20He%20Shengfeng%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%203D%20digital%20content%20necessitates%20expandable%20recognition%0Asystems%20for%20open-world%20scenarios.%20However%2C%20existing%203D%20class-incremental%0Alearning%20methods%20struggle%20under%20extreme%20data%20scarcity%20due%20to%20geometric%0Amisalignment%20and%20texture%20bias.%20While%20recent%20approaches%20integrate%203D%20data%20with%0A2D%20foundation%20models%20%28e.g.%2C%20CLIP%29%2C%20they%20suffer%20from%20semantic%20blurring%20caused%20by%0Atexture-biased%20projections%20and%20indiscriminate%20fusion%20of%20geometric-textural%0Acues%2C%20leading%20to%20unstable%20decision%20prototypes%20and%20catastrophic%20forgetting.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Cross-Modal%20Geometric%20Rectification%20%28CMGR%29%2C%20a%0Aframework%20that%20enhances%203D%20geometric%20fidelity%20by%20leveraging%20CLIP%27s%20hierarchical%0Aspatial%20semantics.%20Specifically%2C%20we%20introduce%20a%20Structure-Aware%20Geometric%0ARectification%20module%20that%20hierarchically%20aligns%203D%20part%20structures%20with%20CLIP%27s%0Aintermediate%20spatial%20priors%20through%20attention-driven%20geometric%20fusion.%0AAdditionally%2C%20a%20Texture%20Amplification%20Module%20synthesizes%20minimal%20yet%0Adiscriminative%20textures%20to%20suppress%20noise%20and%20reinforce%20cross-modal%0Aconsistency.%20To%20further%20stabilize%20incremental%20prototypes%2C%20we%20employ%20a%0ABase-Novel%20Discriminator%20that%20isolates%20geometric%20variations.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20significantly%20improves%203D%20few-shot%0Aclass-incremental%20learning%2C%20achieving%20superior%20geometric%20coherence%20and%0Arobustness%20to%20texture%20bias%20across%20cross-domain%20and%20within-domain%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%25203D%2520Through%25202D%2520Lenses%253A%25203D%2520Few-Shot%2520Class-Incremental%2520Learning%2520via%250A%2520%2520Cross-Modal%2520Geometric%2520Rectification%26entry.906535625%3DXiang%2520Tuo%2520and%2520Xu%2520Xuemiao%2520and%2520Liu%2520Bangzhen%2520and%2520Li%2520Jinyi%2520and%2520Li%2520Yong%2520and%2520He%2520Shengfeng%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%25203D%2520digital%2520content%2520necessitates%2520expandable%2520recognition%250Asystems%2520for%2520open-world%2520scenarios.%2520However%252C%2520existing%25203D%2520class-incremental%250Alearning%2520methods%2520struggle%2520under%2520extreme%2520data%2520scarcity%2520due%2520to%2520geometric%250Amisalignment%2520and%2520texture%2520bias.%2520While%2520recent%2520approaches%2520integrate%25203D%2520data%2520with%250A2D%2520foundation%2520models%2520%2528e.g.%252C%2520CLIP%2529%252C%2520they%2520suffer%2520from%2520semantic%2520blurring%2520caused%2520by%250Atexture-biased%2520projections%2520and%2520indiscriminate%2520fusion%2520of%2520geometric-textural%250Acues%252C%2520leading%2520to%2520unstable%2520decision%2520prototypes%2520and%2520catastrophic%2520forgetting.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520Cross-Modal%2520Geometric%2520Rectification%2520%2528CMGR%2529%252C%2520a%250Aframework%2520that%2520enhances%25203D%2520geometric%2520fidelity%2520by%2520leveraging%2520CLIP%2527s%2520hierarchical%250Aspatial%2520semantics.%2520Specifically%252C%2520we%2520introduce%2520a%2520Structure-Aware%2520Geometric%250ARectification%2520module%2520that%2520hierarchically%2520aligns%25203D%2520part%2520structures%2520with%2520CLIP%2527s%250Aintermediate%2520spatial%2520priors%2520through%2520attention-driven%2520geometric%2520fusion.%250AAdditionally%252C%2520a%2520Texture%2520Amplification%2520Module%2520synthesizes%2520minimal%2520yet%250Adiscriminative%2520textures%2520to%2520suppress%2520noise%2520and%2520reinforce%2520cross-modal%250Aconsistency.%2520To%2520further%2520stabilize%2520incremental%2520prototypes%252C%2520we%2520employ%2520a%250ABase-Novel%2520Discriminator%2520that%2520isolates%2520geometric%2520variations.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%25203D%2520few-shot%250Aclass-incremental%2520learning%252C%2520achieving%2520superior%2520geometric%2520coherence%2520and%250Arobustness%2520to%2520texture%2520bias%2520across%2520cross-domain%2520and%2520within-domain%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%203D%20Through%202D%20Lenses%3A%203D%20Few-Shot%20Class-Incremental%20Learning%20via%0A%20%20Cross-Modal%20Geometric%20Rectification&entry.906535625=Xiang%20Tuo%20and%20Xu%20Xuemiao%20and%20Liu%20Bangzhen%20and%20Li%20Jinyi%20and%20Li%20Yong%20and%20He%20Shengfeng&entry.1292438233=%20%20The%20rapid%20growth%20of%203D%20digital%20content%20necessitates%20expandable%20recognition%0Asystems%20for%20open-world%20scenarios.%20However%2C%20existing%203D%20class-incremental%0Alearning%20methods%20struggle%20under%20extreme%20data%20scarcity%20due%20to%20geometric%0Amisalignment%20and%20texture%20bias.%20While%20recent%20approaches%20integrate%203D%20data%20with%0A2D%20foundation%20models%20%28e.g.%2C%20CLIP%29%2C%20they%20suffer%20from%20semantic%20blurring%20caused%20by%0Atexture-biased%20projections%20and%20indiscriminate%20fusion%20of%20geometric-textural%0Acues%2C%20leading%20to%20unstable%20decision%20prototypes%20and%20catastrophic%20forgetting.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20Cross-Modal%20Geometric%20Rectification%20%28CMGR%29%2C%20a%0Aframework%20that%20enhances%203D%20geometric%20fidelity%20by%20leveraging%20CLIP%27s%20hierarchical%0Aspatial%20semantics.%20Specifically%2C%20we%20introduce%20a%20Structure-Aware%20Geometric%0ARectification%20module%20that%20hierarchically%20aligns%203D%20part%20structures%20with%20CLIP%27s%0Aintermediate%20spatial%20priors%20through%20attention-driven%20geometric%20fusion.%0AAdditionally%2C%20a%20Texture%20Amplification%20Module%20synthesizes%20minimal%20yet%0Adiscriminative%20textures%20to%20suppress%20noise%20and%20reinforce%20cross-modal%0Aconsistency.%20To%20further%20stabilize%20incremental%20prototypes%2C%20we%20employ%20a%0ABase-Novel%20Discriminator%20that%20isolates%20geometric%20variations.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20significantly%20improves%203D%20few-shot%0Aclass-incremental%20learning%2C%20achieving%20superior%20geometric%20coherence%20and%0Arobustness%20to%20texture%20bias%20across%20cross-domain%20and%20within-domain%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14958v1&entry.124074799=Read"},
{"title": "Boost 3D Reconstruction using Diffusion-based Monocular Camera\n  Calibration", "author": "Junyuan Deng and Wei Yin and Xiaoyang Guo and Qian Zhang and Xiaotao Hu and Weiqiang Ren and Xiao-Xiao Long and Ping Tan", "abstract": "  In this paper, we present DM-Calib, a diffusion-based approach for estimating\npinhole camera intrinsic parameters from a single input image. Monocular camera\ncalibration is essential for many 3D vision tasks. However, most existing\nmethods depend on handcrafted assumptions or are constrained by limited\ntraining data, resulting in poor generalization across diverse real-world\nimages. Recent advancements in stable diffusion models, trained on massive\ndata, have shown the ability to generate high-quality images with varied\ncharacteristics. Emerging evidence indicates that these models implicitly\ncapture the relationship between camera focal length and image content.\nBuilding on this insight, we explore how to leverage the powerful priors of\ndiffusion models for monocular pinhole camera calibration. Specifically, we\nintroduce a new image-based representation, termed Camera Image, which\nlosslessly encodes the numerical camera intrinsics and integrates seamlessly\nwith the diffusion framework. Using this representation, we reformulate the\nproblem of estimating camera intrinsics as the generation of a dense Camera\nImage conditioned on an input image. By fine-tuning a stable diffusion model to\ngenerate a Camera Image from a single RGB input, we can extract camera\nintrinsics via a RANSAC operation. We further demonstrate that our monocular\ncalibration method enhances performance across various 3D tasks, including\nzero-shot metric depth estimation, 3D metrology, pose estimation and\nsparse-view reconstruction. Extensive experiments on multiple public datasets\nshow that our approach significantly outperforms baselines and provides broad\nbenefits to 3D vision tasks.\n", "link": "http://arxiv.org/abs/2411.17240v3", "date": "2025-09-18", "relevancy": 2.5755, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6723}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6382}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boost%203D%20Reconstruction%20using%20Diffusion-based%20Monocular%20Camera%0A%20%20Calibration&body=Title%3A%20Boost%203D%20Reconstruction%20using%20Diffusion-based%20Monocular%20Camera%0A%20%20Calibration%0AAuthor%3A%20Junyuan%20Deng%20and%20Wei%20Yin%20and%20Xiaoyang%20Guo%20and%20Qian%20Zhang%20and%20Xiaotao%20Hu%20and%20Weiqiang%20Ren%20and%20Xiao-Xiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20DM-Calib%2C%20a%20diffusion-based%20approach%20for%20estimating%0Apinhole%20camera%20intrinsic%20parameters%20from%20a%20single%20input%20image.%20Monocular%20camera%0Acalibration%20is%20essential%20for%20many%203D%20vision%20tasks.%20However%2C%20most%20existing%0Amethods%20depend%20on%20handcrafted%20assumptions%20or%20are%20constrained%20by%20limited%0Atraining%20data%2C%20resulting%20in%20poor%20generalization%20across%20diverse%20real-world%0Aimages.%20Recent%20advancements%20in%20stable%20diffusion%20models%2C%20trained%20on%20massive%0Adata%2C%20have%20shown%20the%20ability%20to%20generate%20high-quality%20images%20with%20varied%0Acharacteristics.%20Emerging%20evidence%20indicates%20that%20these%20models%20implicitly%0Acapture%20the%20relationship%20between%20camera%20focal%20length%20and%20image%20content.%0ABuilding%20on%20this%20insight%2C%20we%20explore%20how%20to%20leverage%20the%20powerful%20priors%20of%0Adiffusion%20models%20for%20monocular%20pinhole%20camera%20calibration.%20Specifically%2C%20we%0Aintroduce%20a%20new%20image-based%20representation%2C%20termed%20Camera%20Image%2C%20which%0Alosslessly%20encodes%20the%20numerical%20camera%20intrinsics%20and%20integrates%20seamlessly%0Awith%20the%20diffusion%20framework.%20Using%20this%20representation%2C%20we%20reformulate%20the%0Aproblem%20of%20estimating%20camera%20intrinsics%20as%20the%20generation%20of%20a%20dense%20Camera%0AImage%20conditioned%20on%20an%20input%20image.%20By%20fine-tuning%20a%20stable%20diffusion%20model%20to%0Agenerate%20a%20Camera%20Image%20from%20a%20single%20RGB%20input%2C%20we%20can%20extract%20camera%0Aintrinsics%20via%20a%20RANSAC%20operation.%20We%20further%20demonstrate%20that%20our%20monocular%0Acalibration%20method%20enhances%20performance%20across%20various%203D%20tasks%2C%20including%0Azero-shot%20metric%20depth%20estimation%2C%203D%20metrology%2C%20pose%20estimation%20and%0Asparse-view%20reconstruction.%20Extensive%20experiments%20on%20multiple%20public%20datasets%0Ashow%20that%20our%20approach%20significantly%20outperforms%20baselines%20and%20provides%20broad%0Abenefits%20to%203D%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17240v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoost%25203D%2520Reconstruction%2520using%2520Diffusion-based%2520Monocular%2520Camera%250A%2520%2520Calibration%26entry.906535625%3DJunyuan%2520Deng%2520and%2520Wei%2520Yin%2520and%2520Xiaoyang%2520Guo%2520and%2520Qian%2520Zhang%2520and%2520Xiaotao%2520Hu%2520and%2520Weiqiang%2520Ren%2520and%2520Xiao-Xiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520DM-Calib%252C%2520a%2520diffusion-based%2520approach%2520for%2520estimating%250Apinhole%2520camera%2520intrinsic%2520parameters%2520from%2520a%2520single%2520input%2520image.%2520Monocular%2520camera%250Acalibration%2520is%2520essential%2520for%2520many%25203D%2520vision%2520tasks.%2520However%252C%2520most%2520existing%250Amethods%2520depend%2520on%2520handcrafted%2520assumptions%2520or%2520are%2520constrained%2520by%2520limited%250Atraining%2520data%252C%2520resulting%2520in%2520poor%2520generalization%2520across%2520diverse%2520real-world%250Aimages.%2520Recent%2520advancements%2520in%2520stable%2520diffusion%2520models%252C%2520trained%2520on%2520massive%250Adata%252C%2520have%2520shown%2520the%2520ability%2520to%2520generate%2520high-quality%2520images%2520with%2520varied%250Acharacteristics.%2520Emerging%2520evidence%2520indicates%2520that%2520these%2520models%2520implicitly%250Acapture%2520the%2520relationship%2520between%2520camera%2520focal%2520length%2520and%2520image%2520content.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520explore%2520how%2520to%2520leverage%2520the%2520powerful%2520priors%2520of%250Adiffusion%2520models%2520for%2520monocular%2520pinhole%2520camera%2520calibration.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520new%2520image-based%2520representation%252C%2520termed%2520Camera%2520Image%252C%2520which%250Alosslessly%2520encodes%2520the%2520numerical%2520camera%2520intrinsics%2520and%2520integrates%2520seamlessly%250Awith%2520the%2520diffusion%2520framework.%2520Using%2520this%2520representation%252C%2520we%2520reformulate%2520the%250Aproblem%2520of%2520estimating%2520camera%2520intrinsics%2520as%2520the%2520generation%2520of%2520a%2520dense%2520Camera%250AImage%2520conditioned%2520on%2520an%2520input%2520image.%2520By%2520fine-tuning%2520a%2520stable%2520diffusion%2520model%2520to%250Agenerate%2520a%2520Camera%2520Image%2520from%2520a%2520single%2520RGB%2520input%252C%2520we%2520can%2520extract%2520camera%250Aintrinsics%2520via%2520a%2520RANSAC%2520operation.%2520We%2520further%2520demonstrate%2520that%2520our%2520monocular%250Acalibration%2520method%2520enhances%2520performance%2520across%2520various%25203D%2520tasks%252C%2520including%250Azero-shot%2520metric%2520depth%2520estimation%252C%25203D%2520metrology%252C%2520pose%2520estimation%2520and%250Asparse-view%2520reconstruction.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520datasets%250Ashow%2520that%2520our%2520approach%2520significantly%2520outperforms%2520baselines%2520and%2520provides%2520broad%250Abenefits%2520to%25203D%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17240v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boost%203D%20Reconstruction%20using%20Diffusion-based%20Monocular%20Camera%0A%20%20Calibration&entry.906535625=Junyuan%20Deng%20and%20Wei%20Yin%20and%20Xiaoyang%20Guo%20and%20Qian%20Zhang%20and%20Xiaotao%20Hu%20and%20Weiqiang%20Ren%20and%20Xiao-Xiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20DM-Calib%2C%20a%20diffusion-based%20approach%20for%20estimating%0Apinhole%20camera%20intrinsic%20parameters%20from%20a%20single%20input%20image.%20Monocular%20camera%0Acalibration%20is%20essential%20for%20many%203D%20vision%20tasks.%20However%2C%20most%20existing%0Amethods%20depend%20on%20handcrafted%20assumptions%20or%20are%20constrained%20by%20limited%0Atraining%20data%2C%20resulting%20in%20poor%20generalization%20across%20diverse%20real-world%0Aimages.%20Recent%20advancements%20in%20stable%20diffusion%20models%2C%20trained%20on%20massive%0Adata%2C%20have%20shown%20the%20ability%20to%20generate%20high-quality%20images%20with%20varied%0Acharacteristics.%20Emerging%20evidence%20indicates%20that%20these%20models%20implicitly%0Acapture%20the%20relationship%20between%20camera%20focal%20length%20and%20image%20content.%0ABuilding%20on%20this%20insight%2C%20we%20explore%20how%20to%20leverage%20the%20powerful%20priors%20of%0Adiffusion%20models%20for%20monocular%20pinhole%20camera%20calibration.%20Specifically%2C%20we%0Aintroduce%20a%20new%20image-based%20representation%2C%20termed%20Camera%20Image%2C%20which%0Alosslessly%20encodes%20the%20numerical%20camera%20intrinsics%20and%20integrates%20seamlessly%0Awith%20the%20diffusion%20framework.%20Using%20this%20representation%2C%20we%20reformulate%20the%0Aproblem%20of%20estimating%20camera%20intrinsics%20as%20the%20generation%20of%20a%20dense%20Camera%0AImage%20conditioned%20on%20an%20input%20image.%20By%20fine-tuning%20a%20stable%20diffusion%20model%20to%0Agenerate%20a%20Camera%20Image%20from%20a%20single%20RGB%20input%2C%20we%20can%20extract%20camera%0Aintrinsics%20via%20a%20RANSAC%20operation.%20We%20further%20demonstrate%20that%20our%20monocular%0Acalibration%20method%20enhances%20performance%20across%20various%203D%20tasks%2C%20including%0Azero-shot%20metric%20depth%20estimation%2C%203D%20metrology%2C%20pose%20estimation%20and%0Asparse-view%20reconstruction.%20Extensive%20experiments%20on%20multiple%20public%20datasets%0Ashow%20that%20our%20approach%20significantly%20outperforms%20baselines%20and%20provides%20broad%0Abenefits%20to%203D%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17240v3&entry.124074799=Read"},
{"title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR\n  Generation", "author": "Chen Si and Qianyi Wu and Chaitanya Amballa and Romit Roy Choudhury", "abstract": "  Realistic sound simulation plays a critical role in many applications. A key\nelement in sound simulation is the room impulse response (RIR), which\ncharacterizes how sound propagates from a source to a listener within a given\nspace. Recent studies have applied neural implicit methods to learn RIR using\ncontext information collected from the environment, such as scene images.\nHowever, these approaches do not effectively leverage explicit geometric\ninformation from the environment. To further exploit the potential of neural\nimplicit models with direct geometric features, we present Mesh-infused Neural\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\nextracts distance distributions as an explicit representation of local context.\nOur approach demonstrates that incorporating explicit local geometric features\ncan better guide the neural network in generating more accurate RIR\npredictions. Through comparisons with conventional and state-of-the-art\nbaseline methods, we show that MiNAF performs competitively across various\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\nwith limited training samples, demonstrating an advance in high-fidelity sound\nsimulation.\n", "link": "http://arxiv.org/abs/2509.15210v1", "date": "2025-09-18", "relevancy": 2.5628, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5178}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5143}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Context-Driven%20Neural%20Acoustic%20Modeling%20for%20High-Fidelity%20RIR%0A%20%20Generation&body=Title%3A%20Explicit%20Context-Driven%20Neural%20Acoustic%20Modeling%20for%20High-Fidelity%20RIR%0A%20%20Generation%0AAuthor%3A%20Chen%20Si%20and%20Qianyi%20Wu%20and%20Chaitanya%20Amballa%20and%20Romit%20Roy%20Choudhury%0AAbstract%3A%20%20%20Realistic%20sound%20simulation%20plays%20a%20critical%20role%20in%20many%20applications.%20A%20key%0Aelement%20in%20sound%20simulation%20is%20the%20room%20impulse%20response%20%28RIR%29%2C%20which%0Acharacterizes%20how%20sound%20propagates%20from%20a%20source%20to%20a%20listener%20within%20a%20given%0Aspace.%20Recent%20studies%20have%20applied%20neural%20implicit%20methods%20to%20learn%20RIR%20using%0Acontext%20information%20collected%20from%20the%20environment%2C%20such%20as%20scene%20images.%0AHowever%2C%20these%20approaches%20do%20not%20effectively%20leverage%20explicit%20geometric%0Ainformation%20from%20the%20environment.%20To%20further%20exploit%20the%20potential%20of%20neural%0Aimplicit%20models%20with%20direct%20geometric%20features%2C%20we%20present%20Mesh-infused%20Neural%0AAcoustic%20Field%20%28MiNAF%29%2C%20which%20queries%20a%20rough%20room%20mesh%20at%20given%20locations%20and%0Aextracts%20distance%20distributions%20as%20an%20explicit%20representation%20of%20local%20context.%0AOur%20approach%20demonstrates%20that%20incorporating%20explicit%20local%20geometric%20features%0Acan%20better%20guide%20the%20neural%20network%20in%20generating%20more%20accurate%20RIR%0Apredictions.%20Through%20comparisons%20with%20conventional%20and%20state-of-the-art%0Abaseline%20methods%2C%20we%20show%20that%20MiNAF%20performs%20competitively%20across%20various%0Aevaluation%20metrics.%20Furthermore%2C%20we%20verify%20the%20robustness%20of%20MiNAF%20in%20datasets%0Awith%20limited%20training%20samples%2C%20demonstrating%20an%20advance%20in%20high-fidelity%20sound%0Asimulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Context-Driven%2520Neural%2520Acoustic%2520Modeling%2520for%2520High-Fidelity%2520RIR%250A%2520%2520Generation%26entry.906535625%3DChen%2520Si%2520and%2520Qianyi%2520Wu%2520and%2520Chaitanya%2520Amballa%2520and%2520Romit%2520Roy%2520Choudhury%26entry.1292438233%3D%2520%2520Realistic%2520sound%2520simulation%2520plays%2520a%2520critical%2520role%2520in%2520many%2520applications.%2520A%2520key%250Aelement%2520in%2520sound%2520simulation%2520is%2520the%2520room%2520impulse%2520response%2520%2528RIR%2529%252C%2520which%250Acharacterizes%2520how%2520sound%2520propagates%2520from%2520a%2520source%2520to%2520a%2520listener%2520within%2520a%2520given%250Aspace.%2520Recent%2520studies%2520have%2520applied%2520neural%2520implicit%2520methods%2520to%2520learn%2520RIR%2520using%250Acontext%2520information%2520collected%2520from%2520the%2520environment%252C%2520such%2520as%2520scene%2520images.%250AHowever%252C%2520these%2520approaches%2520do%2520not%2520effectively%2520leverage%2520explicit%2520geometric%250Ainformation%2520from%2520the%2520environment.%2520To%2520further%2520exploit%2520the%2520potential%2520of%2520neural%250Aimplicit%2520models%2520with%2520direct%2520geometric%2520features%252C%2520we%2520present%2520Mesh-infused%2520Neural%250AAcoustic%2520Field%2520%2528MiNAF%2529%252C%2520which%2520queries%2520a%2520rough%2520room%2520mesh%2520at%2520given%2520locations%2520and%250Aextracts%2520distance%2520distributions%2520as%2520an%2520explicit%2520representation%2520of%2520local%2520context.%250AOur%2520approach%2520demonstrates%2520that%2520incorporating%2520explicit%2520local%2520geometric%2520features%250Acan%2520better%2520guide%2520the%2520neural%2520network%2520in%2520generating%2520more%2520accurate%2520RIR%250Apredictions.%2520Through%2520comparisons%2520with%2520conventional%2520and%2520state-of-the-art%250Abaseline%2520methods%252C%2520we%2520show%2520that%2520MiNAF%2520performs%2520competitively%2520across%2520various%250Aevaluation%2520metrics.%2520Furthermore%252C%2520we%2520verify%2520the%2520robustness%2520of%2520MiNAF%2520in%2520datasets%250Awith%2520limited%2520training%2520samples%252C%2520demonstrating%2520an%2520advance%2520in%2520high-fidelity%2520sound%250Asimulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Context-Driven%20Neural%20Acoustic%20Modeling%20for%20High-Fidelity%20RIR%0A%20%20Generation&entry.906535625=Chen%20Si%20and%20Qianyi%20Wu%20and%20Chaitanya%20Amballa%20and%20Romit%20Roy%20Choudhury&entry.1292438233=%20%20Realistic%20sound%20simulation%20plays%20a%20critical%20role%20in%20many%20applications.%20A%20key%0Aelement%20in%20sound%20simulation%20is%20the%20room%20impulse%20response%20%28RIR%29%2C%20which%0Acharacterizes%20how%20sound%20propagates%20from%20a%20source%20to%20a%20listener%20within%20a%20given%0Aspace.%20Recent%20studies%20have%20applied%20neural%20implicit%20methods%20to%20learn%20RIR%20using%0Acontext%20information%20collected%20from%20the%20environment%2C%20such%20as%20scene%20images.%0AHowever%2C%20these%20approaches%20do%20not%20effectively%20leverage%20explicit%20geometric%0Ainformation%20from%20the%20environment.%20To%20further%20exploit%20the%20potential%20of%20neural%0Aimplicit%20models%20with%20direct%20geometric%20features%2C%20we%20present%20Mesh-infused%20Neural%0AAcoustic%20Field%20%28MiNAF%29%2C%20which%20queries%20a%20rough%20room%20mesh%20at%20given%20locations%20and%0Aextracts%20distance%20distributions%20as%20an%20explicit%20representation%20of%20local%20context.%0AOur%20approach%20demonstrates%20that%20incorporating%20explicit%20local%20geometric%20features%0Acan%20better%20guide%20the%20neural%20network%20in%20generating%20more%20accurate%20RIR%0Apredictions.%20Through%20comparisons%20with%20conventional%20and%20state-of-the-art%0Abaseline%20methods%2C%20we%20show%20that%20MiNAF%20performs%20competitively%20across%20various%0Aevaluation%20metrics.%20Furthermore%2C%20we%20verify%20the%20robustness%20of%20MiNAF%20in%20datasets%0Awith%20limited%20training%20samples%2C%20demonstrating%20an%20advance%20in%20high-fidelity%20sound%0Asimulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15210v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality\n  Monitoring of the Hadron Calorimeter", "author": "Mulugeta Weldezgina Asres and Christian Walter Omlin and Long Wang and David Yu and Pavel Parygin and Jay Dittmann and Georgia Karapostoli and Markus Seidel and Rosamaria Venditti and Luka Lambrecht and Emanuele Usai and Muhammad Ahmad and Javier Fernandez Menendez and Kaori Maeshima and the CMS-HCAL Collaboration", "abstract": "  The Compact Muon Solenoid (CMS) experiment is a general-purpose detector for\nhigh-energy collision at the Large Hadron Collider (LHC) at CERN. It employs an\nonline data quality monitoring (DQM) system to promptly spot and diagnose\nparticle data acquisition problems to avoid data quality loss. In this study,\nwe present a semi-supervised spatio-temporal anomaly detection (AD) monitoring\nsystem for the physics particle reading channels of the Hadron Calorimeter\n(HCAL) of the CMS using three-dimensional digi-occupancy map data of the DQM.\nWe propose the GraphSTAD system, which employs convolutional and graph neural\nnetworks to learn local spatial characteristics induced by particles traversing\nthe detector and the global behavior owing to shared backend circuit\nconnections and housing boxes of the channels, respectively. Recurrent neural\nnetworks capture the temporal evolution of the extracted spatial features. We\nvalidate the accuracy of the proposed AD system in capturing diverse channel\nfault types using the LHC collision data sets. The GraphSTAD system achieves\nproduction-level accuracy and is being integrated into the CMS core production\nsystem for real-time monitoring of the HCAL. We provide a quantitative\nperformance comparison with alternative benchmark models to demonstrate the\npromising leverage of the presented system. Code:\n\\href{https://github.com/muleina/CMS_HCAL_ML_OnlineDQM}{https://github.com/muleina/CMS\\_HCAL\\_ML\\_OnlineDQM}\n", "link": "http://arxiv.org/abs/2311.04190v2", "date": "2025-09-18", "relevancy": 2.5244, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5245}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5061}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Anomaly%20Detection%20with%20Graph%20Networks%20for%20Data%20Quality%0A%20%20Monitoring%20of%20the%20Hadron%20Calorimeter&body=Title%3A%20Spatio-Temporal%20Anomaly%20Detection%20with%20Graph%20Networks%20for%20Data%20Quality%0A%20%20Monitoring%20of%20the%20Hadron%20Calorimeter%0AAuthor%3A%20Mulugeta%20Weldezgina%20Asres%20and%20Christian%20Walter%20Omlin%20and%20Long%20Wang%20and%20David%20Yu%20and%20Pavel%20Parygin%20and%20Jay%20Dittmann%20and%20Georgia%20Karapostoli%20and%20Markus%20Seidel%20and%20Rosamaria%20Venditti%20and%20Luka%20Lambrecht%20and%20Emanuele%20Usai%20and%20Muhammad%20Ahmad%20and%20Javier%20Fernandez%20Menendez%20and%20Kaori%20Maeshima%20and%20the%20CMS-HCAL%20Collaboration%0AAbstract%3A%20%20%20The%20Compact%20Muon%20Solenoid%20%28CMS%29%20experiment%20is%20a%20general-purpose%20detector%20for%0Ahigh-energy%20collision%20at%20the%20Large%20Hadron%20Collider%20%28LHC%29%20at%20CERN.%20It%20employs%20an%0Aonline%20data%20quality%20monitoring%20%28DQM%29%20system%20to%20promptly%20spot%20and%20diagnose%0Aparticle%20data%20acquisition%20problems%20to%20avoid%20data%20quality%20loss.%20In%20this%20study%2C%0Awe%20present%20a%20semi-supervised%20spatio-temporal%20anomaly%20detection%20%28AD%29%20monitoring%0Asystem%20for%20the%20physics%20particle%20reading%20channels%20of%20the%20Hadron%20Calorimeter%0A%28HCAL%29%20of%20the%20CMS%20using%20three-dimensional%20digi-occupancy%20map%20data%20of%20the%20DQM.%0AWe%20propose%20the%20GraphSTAD%20system%2C%20which%20employs%20convolutional%20and%20graph%20neural%0Anetworks%20to%20learn%20local%20spatial%20characteristics%20induced%20by%20particles%20traversing%0Athe%20detector%20and%20the%20global%20behavior%20owing%20to%20shared%20backend%20circuit%0Aconnections%20and%20housing%20boxes%20of%20the%20channels%2C%20respectively.%20Recurrent%20neural%0Anetworks%20capture%20the%20temporal%20evolution%20of%20the%20extracted%20spatial%20features.%20We%0Avalidate%20the%20accuracy%20of%20the%20proposed%20AD%20system%20in%20capturing%20diverse%20channel%0Afault%20types%20using%20the%20LHC%20collision%20data%20sets.%20The%20GraphSTAD%20system%20achieves%0Aproduction-level%20accuracy%20and%20is%20being%20integrated%20into%20the%20CMS%20core%20production%0Asystem%20for%20real-time%20monitoring%20of%20the%20HCAL.%20We%20provide%20a%20quantitative%0Aperformance%20comparison%20with%20alternative%20benchmark%20models%20to%20demonstrate%20the%0Apromising%20leverage%20of%20the%20presented%20system.%20Code%3A%0A%5Chref%7Bhttps%3A//github.com/muleina/CMS_HCAL_ML_OnlineDQM%7D%7Bhttps%3A//github.com/muleina/CMS%5C_HCAL%5C_ML%5C_OnlineDQM%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Anomaly%2520Detection%2520with%2520Graph%2520Networks%2520for%2520Data%2520Quality%250A%2520%2520Monitoring%2520of%2520the%2520Hadron%2520Calorimeter%26entry.906535625%3DMulugeta%2520Weldezgina%2520Asres%2520and%2520Christian%2520Walter%2520Omlin%2520and%2520Long%2520Wang%2520and%2520David%2520Yu%2520and%2520Pavel%2520Parygin%2520and%2520Jay%2520Dittmann%2520and%2520Georgia%2520Karapostoli%2520and%2520Markus%2520Seidel%2520and%2520Rosamaria%2520Venditti%2520and%2520Luka%2520Lambrecht%2520and%2520Emanuele%2520Usai%2520and%2520Muhammad%2520Ahmad%2520and%2520Javier%2520Fernandez%2520Menendez%2520and%2520Kaori%2520Maeshima%2520and%2520the%2520CMS-HCAL%2520Collaboration%26entry.1292438233%3D%2520%2520The%2520Compact%2520Muon%2520Solenoid%2520%2528CMS%2529%2520experiment%2520is%2520a%2520general-purpose%2520detector%2520for%250Ahigh-energy%2520collision%2520at%2520the%2520Large%2520Hadron%2520Collider%2520%2528LHC%2529%2520at%2520CERN.%2520It%2520employs%2520an%250Aonline%2520data%2520quality%2520monitoring%2520%2528DQM%2529%2520system%2520to%2520promptly%2520spot%2520and%2520diagnose%250Aparticle%2520data%2520acquisition%2520problems%2520to%2520avoid%2520data%2520quality%2520loss.%2520In%2520this%2520study%252C%250Awe%2520present%2520a%2520semi-supervised%2520spatio-temporal%2520anomaly%2520detection%2520%2528AD%2529%2520monitoring%250Asystem%2520for%2520the%2520physics%2520particle%2520reading%2520channels%2520of%2520the%2520Hadron%2520Calorimeter%250A%2528HCAL%2529%2520of%2520the%2520CMS%2520using%2520three-dimensional%2520digi-occupancy%2520map%2520data%2520of%2520the%2520DQM.%250AWe%2520propose%2520the%2520GraphSTAD%2520system%252C%2520which%2520employs%2520convolutional%2520and%2520graph%2520neural%250Anetworks%2520to%2520learn%2520local%2520spatial%2520characteristics%2520induced%2520by%2520particles%2520traversing%250Athe%2520detector%2520and%2520the%2520global%2520behavior%2520owing%2520to%2520shared%2520backend%2520circuit%250Aconnections%2520and%2520housing%2520boxes%2520of%2520the%2520channels%252C%2520respectively.%2520Recurrent%2520neural%250Anetworks%2520capture%2520the%2520temporal%2520evolution%2520of%2520the%2520extracted%2520spatial%2520features.%2520We%250Avalidate%2520the%2520accuracy%2520of%2520the%2520proposed%2520AD%2520system%2520in%2520capturing%2520diverse%2520channel%250Afault%2520types%2520using%2520the%2520LHC%2520collision%2520data%2520sets.%2520The%2520GraphSTAD%2520system%2520achieves%250Aproduction-level%2520accuracy%2520and%2520is%2520being%2520integrated%2520into%2520the%2520CMS%2520core%2520production%250Asystem%2520for%2520real-time%2520monitoring%2520of%2520the%2520HCAL.%2520We%2520provide%2520a%2520quantitative%250Aperformance%2520comparison%2520with%2520alternative%2520benchmark%2520models%2520to%2520demonstrate%2520the%250Apromising%2520leverage%2520of%2520the%2520presented%2520system.%2520Code%253A%250A%255Chref%257Bhttps%253A//github.com/muleina/CMS_HCAL_ML_OnlineDQM%257D%257Bhttps%253A//github.com/muleina/CMS%255C_HCAL%255C_ML%255C_OnlineDQM%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Anomaly%20Detection%20with%20Graph%20Networks%20for%20Data%20Quality%0A%20%20Monitoring%20of%20the%20Hadron%20Calorimeter&entry.906535625=Mulugeta%20Weldezgina%20Asres%20and%20Christian%20Walter%20Omlin%20and%20Long%20Wang%20and%20David%20Yu%20and%20Pavel%20Parygin%20and%20Jay%20Dittmann%20and%20Georgia%20Karapostoli%20and%20Markus%20Seidel%20and%20Rosamaria%20Venditti%20and%20Luka%20Lambrecht%20and%20Emanuele%20Usai%20and%20Muhammad%20Ahmad%20and%20Javier%20Fernandez%20Menendez%20and%20Kaori%20Maeshima%20and%20the%20CMS-HCAL%20Collaboration&entry.1292438233=%20%20The%20Compact%20Muon%20Solenoid%20%28CMS%29%20experiment%20is%20a%20general-purpose%20detector%20for%0Ahigh-energy%20collision%20at%20the%20Large%20Hadron%20Collider%20%28LHC%29%20at%20CERN.%20It%20employs%20an%0Aonline%20data%20quality%20monitoring%20%28DQM%29%20system%20to%20promptly%20spot%20and%20diagnose%0Aparticle%20data%20acquisition%20problems%20to%20avoid%20data%20quality%20loss.%20In%20this%20study%2C%0Awe%20present%20a%20semi-supervised%20spatio-temporal%20anomaly%20detection%20%28AD%29%20monitoring%0Asystem%20for%20the%20physics%20particle%20reading%20channels%20of%20the%20Hadron%20Calorimeter%0A%28HCAL%29%20of%20the%20CMS%20using%20three-dimensional%20digi-occupancy%20map%20data%20of%20the%20DQM.%0AWe%20propose%20the%20GraphSTAD%20system%2C%20which%20employs%20convolutional%20and%20graph%20neural%0Anetworks%20to%20learn%20local%20spatial%20characteristics%20induced%20by%20particles%20traversing%0Athe%20detector%20and%20the%20global%20behavior%20owing%20to%20shared%20backend%20circuit%0Aconnections%20and%20housing%20boxes%20of%20the%20channels%2C%20respectively.%20Recurrent%20neural%0Anetworks%20capture%20the%20temporal%20evolution%20of%20the%20extracted%20spatial%20features.%20We%0Avalidate%20the%20accuracy%20of%20the%20proposed%20AD%20system%20in%20capturing%20diverse%20channel%0Afault%20types%20using%20the%20LHC%20collision%20data%20sets.%20The%20GraphSTAD%20system%20achieves%0Aproduction-level%20accuracy%20and%20is%20being%20integrated%20into%20the%20CMS%20core%20production%0Asystem%20for%20real-time%20monitoring%20of%20the%20HCAL.%20We%20provide%20a%20quantitative%0Aperformance%20comparison%20with%20alternative%20benchmark%20models%20to%20demonstrate%20the%0Apromising%20leverage%20of%20the%20presented%20system.%20Code%3A%0A%5Chref%7Bhttps%3A//github.com/muleina/CMS_HCAL_ML_OnlineDQM%7D%7Bhttps%3A//github.com/muleina/CMS%5C_HCAL%5C_ML%5C_OnlineDQM%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04190v2&entry.124074799=Read"},
{"title": "GAF: Gaussian Action Field as a Dynamic World Model for Robotic\n  Manipulation", "author": "Ying Chai and Litao Deng and Ruizhi Shao and Jiajun Zhang and Liangjun Xing and Hongwen Zhang and Yebin Liu", "abstract": "  Accurate scene perception is critical for vision-based robotic manipulation.\nExisting approaches typically follow either a Vision-to-Action (V-A) paradigm,\npredicting actions directly from visual inputs, or a Vision-to-3D-to-Action\n(V-3D-A) paradigm, leveraging intermediate 3D representations. However, these\nmethods often struggle with action inaccuracies due to the complexity and\ndynamic nature of manipulation scenes. In this paper, we adopt a V-4D-A\nframework that enables direct action reasoning from motion-aware 4D\nrepresentations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian\nSplatting (3DGS) by incorporating learnable motion attributes, allowing 4D\nmodeling of dynamic scenes and manipulation actions. To learn time-varying\nscene geometry and action-aware robot motion, GAF provides three interrelated\noutputs: reconstruction of the current scene, prediction of future frames, and\nestimation of init action via Gaussian motion. Furthermore, we employ an\naction-vision-aligned denoising framework, conditioned on a unified\nrepresentation that combines the init action and the Gaussian perception, both\ngenerated by the GAF, to further obtain more precise actions. Extensive\nexperiments demonstrate significant improvements, with GAF achieving +11.5385\ndB PSNR, +0.3864 SSIM and -0.5574 LPIPS improvements in reconstruction quality,\nwhile boosting the average +7.3% success rate in robotic manipulation tasks\nover state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2506.14135v3", "date": "2025-09-18", "relevancy": 2.493, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6465}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6237}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAF%3A%20Gaussian%20Action%20Field%20as%20a%20Dynamic%20World%20Model%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20GAF%3A%20Gaussian%20Action%20Field%20as%20a%20Dynamic%20World%20Model%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Ying%20Chai%20and%20Litao%20Deng%20and%20Ruizhi%20Shao%20and%20Jiajun%20Zhang%20and%20Liangjun%20Xing%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20Accurate%20scene%20perception%20is%20critical%20for%20vision-based%20robotic%20manipulation.%0AExisting%20approaches%20typically%20follow%20either%20a%20Vision-to-Action%20%28V-A%29%20paradigm%2C%0Apredicting%20actions%20directly%20from%20visual%20inputs%2C%20or%20a%20Vision-to-3D-to-Action%0A%28V-3D-A%29%20paradigm%2C%20leveraging%20intermediate%203D%20representations.%20However%2C%20these%0Amethods%20often%20struggle%20with%20action%20inaccuracies%20due%20to%20the%20complexity%20and%0Adynamic%20nature%20of%20manipulation%20scenes.%20In%20this%20paper%2C%20we%20adopt%20a%20V-4D-A%0Aframework%20that%20enables%20direct%20action%20reasoning%20from%20motion-aware%204D%0Arepresentations%20via%20a%20Gaussian%20Action%20Field%20%28GAF%29.%20GAF%20extends%203D%20Gaussian%0ASplatting%20%283DGS%29%20by%20incorporating%20learnable%20motion%20attributes%2C%20allowing%204D%0Amodeling%20of%20dynamic%20scenes%20and%20manipulation%20actions.%20To%20learn%20time-varying%0Ascene%20geometry%20and%20action-aware%20robot%20motion%2C%20GAF%20provides%20three%20interrelated%0Aoutputs%3A%20reconstruction%20of%20the%20current%20scene%2C%20prediction%20of%20future%20frames%2C%20and%0Aestimation%20of%20init%20action%20via%20Gaussian%20motion.%20Furthermore%2C%20we%20employ%20an%0Aaction-vision-aligned%20denoising%20framework%2C%20conditioned%20on%20a%20unified%0Arepresentation%20that%20combines%20the%20init%20action%20and%20the%20Gaussian%20perception%2C%20both%0Agenerated%20by%20the%20GAF%2C%20to%20further%20obtain%20more%20precise%20actions.%20Extensive%0Aexperiments%20demonstrate%20significant%20improvements%2C%20with%20GAF%20achieving%20%2B11.5385%0AdB%20PSNR%2C%20%2B0.3864%20SSIM%20and%20-0.5574%20LPIPS%20improvements%20in%20reconstruction%20quality%2C%0Awhile%20boosting%20the%20average%20%2B7.3%25%20success%20rate%20in%20robotic%20manipulation%20tasks%0Aover%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14135v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAF%253A%2520Gaussian%2520Action%2520Field%2520as%2520a%2520Dynamic%2520World%2520Model%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DYing%2520Chai%2520and%2520Litao%2520Deng%2520and%2520Ruizhi%2520Shao%2520and%2520Jiajun%2520Zhang%2520and%2520Liangjun%2520Xing%2520and%2520Hongwen%2520Zhang%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520Accurate%2520scene%2520perception%2520is%2520critical%2520for%2520vision-based%2520robotic%2520manipulation.%250AExisting%2520approaches%2520typically%2520follow%2520either%2520a%2520Vision-to-Action%2520%2528V-A%2529%2520paradigm%252C%250Apredicting%2520actions%2520directly%2520from%2520visual%2520inputs%252C%2520or%2520a%2520Vision-to-3D-to-Action%250A%2528V-3D-A%2529%2520paradigm%252C%2520leveraging%2520intermediate%25203D%2520representations.%2520However%252C%2520these%250Amethods%2520often%2520struggle%2520with%2520action%2520inaccuracies%2520due%2520to%2520the%2520complexity%2520and%250Adynamic%2520nature%2520of%2520manipulation%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520adopt%2520a%2520V-4D-A%250Aframework%2520that%2520enables%2520direct%2520action%2520reasoning%2520from%2520motion-aware%25204D%250Arepresentations%2520via%2520a%2520Gaussian%2520Action%2520Field%2520%2528GAF%2529.%2520GAF%2520extends%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520by%2520incorporating%2520learnable%2520motion%2520attributes%252C%2520allowing%25204D%250Amodeling%2520of%2520dynamic%2520scenes%2520and%2520manipulation%2520actions.%2520To%2520learn%2520time-varying%250Ascene%2520geometry%2520and%2520action-aware%2520robot%2520motion%252C%2520GAF%2520provides%2520three%2520interrelated%250Aoutputs%253A%2520reconstruction%2520of%2520the%2520current%2520scene%252C%2520prediction%2520of%2520future%2520frames%252C%2520and%250Aestimation%2520of%2520init%2520action%2520via%2520Gaussian%2520motion.%2520Furthermore%252C%2520we%2520employ%2520an%250Aaction-vision-aligned%2520denoising%2520framework%252C%2520conditioned%2520on%2520a%2520unified%250Arepresentation%2520that%2520combines%2520the%2520init%2520action%2520and%2520the%2520Gaussian%2520perception%252C%2520both%250Agenerated%2520by%2520the%2520GAF%252C%2520to%2520further%2520obtain%2520more%2520precise%2520actions.%2520Extensive%250Aexperiments%2520demonstrate%2520significant%2520improvements%252C%2520with%2520GAF%2520achieving%2520%252B11.5385%250AdB%2520PSNR%252C%2520%252B0.3864%2520SSIM%2520and%2520-0.5574%2520LPIPS%2520improvements%2520in%2520reconstruction%2520quality%252C%250Awhile%2520boosting%2520the%2520average%2520%252B7.3%2525%2520success%2520rate%2520in%2520robotic%2520manipulation%2520tasks%250Aover%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14135v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAF%3A%20Gaussian%20Action%20Field%20as%20a%20Dynamic%20World%20Model%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Ying%20Chai%20and%20Litao%20Deng%20and%20Ruizhi%20Shao%20and%20Jiajun%20Zhang%20and%20Liangjun%20Xing%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu&entry.1292438233=%20%20Accurate%20scene%20perception%20is%20critical%20for%20vision-based%20robotic%20manipulation.%0AExisting%20approaches%20typically%20follow%20either%20a%20Vision-to-Action%20%28V-A%29%20paradigm%2C%0Apredicting%20actions%20directly%20from%20visual%20inputs%2C%20or%20a%20Vision-to-3D-to-Action%0A%28V-3D-A%29%20paradigm%2C%20leveraging%20intermediate%203D%20representations.%20However%2C%20these%0Amethods%20often%20struggle%20with%20action%20inaccuracies%20due%20to%20the%20complexity%20and%0Adynamic%20nature%20of%20manipulation%20scenes.%20In%20this%20paper%2C%20we%20adopt%20a%20V-4D-A%0Aframework%20that%20enables%20direct%20action%20reasoning%20from%20motion-aware%204D%0Arepresentations%20via%20a%20Gaussian%20Action%20Field%20%28GAF%29.%20GAF%20extends%203D%20Gaussian%0ASplatting%20%283DGS%29%20by%20incorporating%20learnable%20motion%20attributes%2C%20allowing%204D%0Amodeling%20of%20dynamic%20scenes%20and%20manipulation%20actions.%20To%20learn%20time-varying%0Ascene%20geometry%20and%20action-aware%20robot%20motion%2C%20GAF%20provides%20three%20interrelated%0Aoutputs%3A%20reconstruction%20of%20the%20current%20scene%2C%20prediction%20of%20future%20frames%2C%20and%0Aestimation%20of%20init%20action%20via%20Gaussian%20motion.%20Furthermore%2C%20we%20employ%20an%0Aaction-vision-aligned%20denoising%20framework%2C%20conditioned%20on%20a%20unified%0Arepresentation%20that%20combines%20the%20init%20action%20and%20the%20Gaussian%20perception%2C%20both%0Agenerated%20by%20the%20GAF%2C%20to%20further%20obtain%20more%20precise%20actions.%20Extensive%0Aexperiments%20demonstrate%20significant%20improvements%2C%20with%20GAF%20achieving%20%2B11.5385%0AdB%20PSNR%2C%20%2B0.3864%20SSIM%20and%20-0.5574%20LPIPS%20improvements%20in%20reconstruction%20quality%2C%0Awhile%20boosting%20the%20average%20%2B7.3%25%20success%20rate%20in%20robotic%20manipulation%20tasks%0Aover%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14135v3&entry.124074799=Read"},
{"title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data", "author": "Zhaoyang Liu and JingJing Xie and Zichen Ding and Zehao Li and Bowen Yang and Zhenyu Wu and Xuehui Wang and Qiushi Sun and Shi Liu and Weiyun Wang and Shenglong Ye and Qingyun Li and Zeyue Tian and Gen Luo and Xiangyu Yue and Biqing Qi and Kai Chen and Bowen Zhou and Yu Qiao and Qifeng Chen and Wenhai Wang", "abstract": "  Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.\n", "link": "http://arxiv.org/abs/2509.15221v1", "date": "2025-09-18", "relevancy": 2.4923, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaleCUA%3A%20Scaling%20Open-Source%20Computer%20Use%20Agents%20with%20Cross-Platform%0A%20%20Data&body=Title%3A%20ScaleCUA%3A%20Scaling%20Open-Source%20Computer%20Use%20Agents%20with%20Cross-Platform%0A%20%20Data%0AAuthor%3A%20Zhaoyang%20Liu%20and%20JingJing%20Xie%20and%20Zichen%20Ding%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Zhenyu%20Wu%20and%20Xuehui%20Wang%20and%20Qiushi%20Sun%20and%20Shi%20Liu%20and%20Weiyun%20Wang%20and%20Shenglong%20Ye%20and%20Qingyun%20Li%20and%20Zeyue%20Tian%20and%20Gen%20Luo%20and%20Xiangyu%20Yue%20and%20Biqing%20Qi%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Yu%20Qiao%20and%20Qifeng%20Chen%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20enabled%20computer%20use%20agents%20%28CUAs%29%20that%0Aoperate%20GUIs%20autonomously%2C%20showing%20great%20potential%2C%20yet%20progress%20is%20limited%20by%0Athe%20lack%20of%20large-scale%2C%20open-source%20computer%20use%20data%20and%20foundation%20models.%0AIn%20this%20work%2C%20we%20introduce%20ScaleCUA%2C%20a%20step%20toward%20scaling%20open-source%20CUAs.%20It%0Aoffers%20a%20large-scale%20dataset%20spanning%206%20operating%20systems%20and%203%20task%20domains%2C%0Abuilt%20via%20a%20closed-loop%20pipeline%20uniting%20automated%20agents%20with%20human%20experts.%0ATrained%20on%20this%20scaled-up%20data%2C%20ScaleCUA%20can%20operate%20seamlessly%20across%0Aplatforms.%20Specifically%2C%20it%20delivers%20strong%20gains%20over%20baselines%20%28%2B26.6%20on%0AWebArena-Lite-v2%2C%20%2B10.7%20on%20ScreenSpot-Pro%29%20and%20sets%20new%20state-of-the-art%0Aresults%20%2894.4%25%20on%20MMBench-GUI%20L1-Hard%2C%2060.6%25%20on%20OSWorld-G%2C%2047.4%25%20on%0AWebArena-Lite-v2%29.%20These%20findings%20underscore%20the%20power%20of%20data-driven%20scaling%0Afor%20general-purpose%20computer%20use%20agents.%20We%20will%20release%20data%2C%20models%2C%20and%20code%0Ato%20advance%20future%20research%3A%20https%3A//github.com/OpenGVLab/ScaleCUA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaleCUA%253A%2520Scaling%2520Open-Source%2520Computer%2520Use%2520Agents%2520with%2520Cross-Platform%250A%2520%2520Data%26entry.906535625%3DZhaoyang%2520Liu%2520and%2520JingJing%2520Xie%2520and%2520Zichen%2520Ding%2520and%2520Zehao%2520Li%2520and%2520Bowen%2520Yang%2520and%2520Zhenyu%2520Wu%2520and%2520Xuehui%2520Wang%2520and%2520Qiushi%2520Sun%2520and%2520Shi%2520Liu%2520and%2520Weiyun%2520Wang%2520and%2520Shenglong%2520Ye%2520and%2520Qingyun%2520Li%2520and%2520Zeyue%2520Tian%2520and%2520Gen%2520Luo%2520and%2520Xiangyu%2520Yue%2520and%2520Biqing%2520Qi%2520and%2520Kai%2520Chen%2520and%2520Bowen%2520Zhou%2520and%2520Yu%2520Qiao%2520and%2520Qifeng%2520Chen%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520enabled%2520computer%2520use%2520agents%2520%2528CUAs%2529%2520that%250Aoperate%2520GUIs%2520autonomously%252C%2520showing%2520great%2520potential%252C%2520yet%2520progress%2520is%2520limited%2520by%250Athe%2520lack%2520of%2520large-scale%252C%2520open-source%2520computer%2520use%2520data%2520and%2520foundation%2520models.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520ScaleCUA%252C%2520a%2520step%2520toward%2520scaling%2520open-source%2520CUAs.%2520It%250Aoffers%2520a%2520large-scale%2520dataset%2520spanning%25206%2520operating%2520systems%2520and%25203%2520task%2520domains%252C%250Abuilt%2520via%2520a%2520closed-loop%2520pipeline%2520uniting%2520automated%2520agents%2520with%2520human%2520experts.%250ATrained%2520on%2520this%2520scaled-up%2520data%252C%2520ScaleCUA%2520can%2520operate%2520seamlessly%2520across%250Aplatforms.%2520Specifically%252C%2520it%2520delivers%2520strong%2520gains%2520over%2520baselines%2520%2528%252B26.6%2520on%250AWebArena-Lite-v2%252C%2520%252B10.7%2520on%2520ScreenSpot-Pro%2529%2520and%2520sets%2520new%2520state-of-the-art%250Aresults%2520%252894.4%2525%2520on%2520MMBench-GUI%2520L1-Hard%252C%252060.6%2525%2520on%2520OSWorld-G%252C%252047.4%2525%2520on%250AWebArena-Lite-v2%2529.%2520These%2520findings%2520underscore%2520the%2520power%2520of%2520data-driven%2520scaling%250Afor%2520general-purpose%2520computer%2520use%2520agents.%2520We%2520will%2520release%2520data%252C%2520models%252C%2520and%2520code%250Ato%2520advance%2520future%2520research%253A%2520https%253A//github.com/OpenGVLab/ScaleCUA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaleCUA%3A%20Scaling%20Open-Source%20Computer%20Use%20Agents%20with%20Cross-Platform%0A%20%20Data&entry.906535625=Zhaoyang%20Liu%20and%20JingJing%20Xie%20and%20Zichen%20Ding%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Zhenyu%20Wu%20and%20Xuehui%20Wang%20and%20Qiushi%20Sun%20and%20Shi%20Liu%20and%20Weiyun%20Wang%20and%20Shenglong%20Ye%20and%20Qingyun%20Li%20and%20Zeyue%20Tian%20and%20Gen%20Luo%20and%20Xiangyu%20Yue%20and%20Biqing%20Qi%20and%20Kai%20Chen%20and%20Bowen%20Zhou%20and%20Yu%20Qiao%20and%20Qifeng%20Chen%20and%20Wenhai%20Wang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20enabled%20computer%20use%20agents%20%28CUAs%29%20that%0Aoperate%20GUIs%20autonomously%2C%20showing%20great%20potential%2C%20yet%20progress%20is%20limited%20by%0Athe%20lack%20of%20large-scale%2C%20open-source%20computer%20use%20data%20and%20foundation%20models.%0AIn%20this%20work%2C%20we%20introduce%20ScaleCUA%2C%20a%20step%20toward%20scaling%20open-source%20CUAs.%20It%0Aoffers%20a%20large-scale%20dataset%20spanning%206%20operating%20systems%20and%203%20task%20domains%2C%0Abuilt%20via%20a%20closed-loop%20pipeline%20uniting%20automated%20agents%20with%20human%20experts.%0ATrained%20on%20this%20scaled-up%20data%2C%20ScaleCUA%20can%20operate%20seamlessly%20across%0Aplatforms.%20Specifically%2C%20it%20delivers%20strong%20gains%20over%20baselines%20%28%2B26.6%20on%0AWebArena-Lite-v2%2C%20%2B10.7%20on%20ScreenSpot-Pro%29%20and%20sets%20new%20state-of-the-art%0Aresults%20%2894.4%25%20on%20MMBench-GUI%20L1-Hard%2C%2060.6%25%20on%20OSWorld-G%2C%2047.4%25%20on%0AWebArena-Lite-v2%29.%20These%20findings%20underscore%20the%20power%20of%20data-driven%20scaling%0Afor%20general-purpose%20computer%20use%20agents.%20We%20will%20release%20data%2C%20models%2C%20and%20code%0Ato%20advance%20future%20research%3A%20https%3A//github.com/OpenGVLab/ScaleCUA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15221v1&entry.124074799=Read"},
{"title": "Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain\n  Functional Network Analysis", "author": "Junhao Jia and Yunyou Liu and Cheng Yang and Yifei Sun and Feiwei Qin and Changmiao Wang and Yong Peng", "abstract": "  Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive\nwindow into the brain's functional organization by generating complex\nfunctional networks, typically modeled as graphs. These brain networks exhibit\na hierarchical topology that is crucial for cognitive processing. However, due\nto inherent spatial constraints, standard Euclidean GNNs struggle to represent\nthese hierarchical structures without high distortion, limiting their clinical\nperformance. To address this limitation, we propose Brain-HGCN, a geometric\ndeep learning framework based on hyperbolic geometry, which leverages the\nintrinsic property of negatively curved space to model the brain's network\nhierarchy with high fidelity. Grounded in the Lorentz model, our model employs\na novel hyperbolic graph attention layer with a signed aggregation mechanism to\ndistinctly process excitatory and inhibitory connections, ultimately learning\nrobust graph-level representations via a geometrically sound Fr\\'echet mean for\ngraph readout. Experiments on two large-scale fMRI datasets for psychiatric\ndisorder classification demonstrate that our approach significantly outperforms\na wide range of state-of-the-art Euclidean baselines. This work pioneers a new\ngeometric deep learning paradigm for fMRI analysis, highlighting the immense\npotential of hyperbolic GNNs in the field of computational psychiatry.\n", "link": "http://arxiv.org/abs/2509.14965v1", "date": "2025-09-18", "relevancy": 2.4815, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5033}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4948}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-HGCN%3A%20A%20Hyperbolic%20Graph%20Convolutional%20Network%20for%20Brain%0A%20%20Functional%20Network%20Analysis&body=Title%3A%20Brain-HGCN%3A%20A%20Hyperbolic%20Graph%20Convolutional%20Network%20for%20Brain%0A%20%20Functional%20Network%20Analysis%0AAuthor%3A%20Junhao%20Jia%20and%20Yunyou%20Liu%20and%20Cheng%20Yang%20and%20Yifei%20Sun%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Yong%20Peng%0AAbstract%3A%20%20%20Functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20provides%20a%20powerful%20non-invasive%0Awindow%20into%20the%20brain%27s%20functional%20organization%20by%20generating%20complex%0Afunctional%20networks%2C%20typically%20modeled%20as%20graphs.%20These%20brain%20networks%20exhibit%0Aa%20hierarchical%20topology%20that%20is%20crucial%20for%20cognitive%20processing.%20However%2C%20due%0Ato%20inherent%20spatial%20constraints%2C%20standard%20Euclidean%20GNNs%20struggle%20to%20represent%0Athese%20hierarchical%20structures%20without%20high%20distortion%2C%20limiting%20their%20clinical%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20propose%20Brain-HGCN%2C%20a%20geometric%0Adeep%20learning%20framework%20based%20on%20hyperbolic%20geometry%2C%20which%20leverages%20the%0Aintrinsic%20property%20of%20negatively%20curved%20space%20to%20model%20the%20brain%27s%20network%0Ahierarchy%20with%20high%20fidelity.%20Grounded%20in%20the%20Lorentz%20model%2C%20our%20model%20employs%0Aa%20novel%20hyperbolic%20graph%20attention%20layer%20with%20a%20signed%20aggregation%20mechanism%20to%0Adistinctly%20process%20excitatory%20and%20inhibitory%20connections%2C%20ultimately%20learning%0Arobust%20graph-level%20representations%20via%20a%20geometrically%20sound%20Fr%5C%27echet%20mean%20for%0Agraph%20readout.%20Experiments%20on%20two%20large-scale%20fMRI%20datasets%20for%20psychiatric%0Adisorder%20classification%20demonstrate%20that%20our%20approach%20significantly%20outperforms%0Aa%20wide%20range%20of%20state-of-the-art%20Euclidean%20baselines.%20This%20work%20pioneers%20a%20new%0Ageometric%20deep%20learning%20paradigm%20for%20fMRI%20analysis%2C%20highlighting%20the%20immense%0Apotential%20of%20hyperbolic%20GNNs%20in%20the%20field%20of%20computational%20psychiatry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-HGCN%253A%2520A%2520Hyperbolic%2520Graph%2520Convolutional%2520Network%2520for%2520Brain%250A%2520%2520Functional%2520Network%2520Analysis%26entry.906535625%3DJunhao%2520Jia%2520and%2520Yunyou%2520Liu%2520and%2520Cheng%2520Yang%2520and%2520Yifei%2520Sun%2520and%2520Feiwei%2520Qin%2520and%2520Changmiao%2520Wang%2520and%2520Yong%2520Peng%26entry.1292438233%3D%2520%2520Functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520provides%2520a%2520powerful%2520non-invasive%250Awindow%2520into%2520the%2520brain%2527s%2520functional%2520organization%2520by%2520generating%2520complex%250Afunctional%2520networks%252C%2520typically%2520modeled%2520as%2520graphs.%2520These%2520brain%2520networks%2520exhibit%250Aa%2520hierarchical%2520topology%2520that%2520is%2520crucial%2520for%2520cognitive%2520processing.%2520However%252C%2520due%250Ato%2520inherent%2520spatial%2520constraints%252C%2520standard%2520Euclidean%2520GNNs%2520struggle%2520to%2520represent%250Athese%2520hierarchical%2520structures%2520without%2520high%2520distortion%252C%2520limiting%2520their%2520clinical%250Aperformance.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Brain-HGCN%252C%2520a%2520geometric%250Adeep%2520learning%2520framework%2520based%2520on%2520hyperbolic%2520geometry%252C%2520which%2520leverages%2520the%250Aintrinsic%2520property%2520of%2520negatively%2520curved%2520space%2520to%2520model%2520the%2520brain%2527s%2520network%250Ahierarchy%2520with%2520high%2520fidelity.%2520Grounded%2520in%2520the%2520Lorentz%2520model%252C%2520our%2520model%2520employs%250Aa%2520novel%2520hyperbolic%2520graph%2520attention%2520layer%2520with%2520a%2520signed%2520aggregation%2520mechanism%2520to%250Adistinctly%2520process%2520excitatory%2520and%2520inhibitory%2520connections%252C%2520ultimately%2520learning%250Arobust%2520graph-level%2520representations%2520via%2520a%2520geometrically%2520sound%2520Fr%255C%2527echet%2520mean%2520for%250Agraph%2520readout.%2520Experiments%2520on%2520two%2520large-scale%2520fMRI%2520datasets%2520for%2520psychiatric%250Adisorder%2520classification%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%250Aa%2520wide%2520range%2520of%2520state-of-the-art%2520Euclidean%2520baselines.%2520This%2520work%2520pioneers%2520a%2520new%250Ageometric%2520deep%2520learning%2520paradigm%2520for%2520fMRI%2520analysis%252C%2520highlighting%2520the%2520immense%250Apotential%2520of%2520hyperbolic%2520GNNs%2520in%2520the%2520field%2520of%2520computational%2520psychiatry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-HGCN%3A%20A%20Hyperbolic%20Graph%20Convolutional%20Network%20for%20Brain%0A%20%20Functional%20Network%20Analysis&entry.906535625=Junhao%20Jia%20and%20Yunyou%20Liu%20and%20Cheng%20Yang%20and%20Yifei%20Sun%20and%20Feiwei%20Qin%20and%20Changmiao%20Wang%20and%20Yong%20Peng&entry.1292438233=%20%20Functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20provides%20a%20powerful%20non-invasive%0Awindow%20into%20the%20brain%27s%20functional%20organization%20by%20generating%20complex%0Afunctional%20networks%2C%20typically%20modeled%20as%20graphs.%20These%20brain%20networks%20exhibit%0Aa%20hierarchical%20topology%20that%20is%20crucial%20for%20cognitive%20processing.%20However%2C%20due%0Ato%20inherent%20spatial%20constraints%2C%20standard%20Euclidean%20GNNs%20struggle%20to%20represent%0Athese%20hierarchical%20structures%20without%20high%20distortion%2C%20limiting%20their%20clinical%0Aperformance.%20To%20address%20this%20limitation%2C%20we%20propose%20Brain-HGCN%2C%20a%20geometric%0Adeep%20learning%20framework%20based%20on%20hyperbolic%20geometry%2C%20which%20leverages%20the%0Aintrinsic%20property%20of%20negatively%20curved%20space%20to%20model%20the%20brain%27s%20network%0Ahierarchy%20with%20high%20fidelity.%20Grounded%20in%20the%20Lorentz%20model%2C%20our%20model%20employs%0Aa%20novel%20hyperbolic%20graph%20attention%20layer%20with%20a%20signed%20aggregation%20mechanism%20to%0Adistinctly%20process%20excitatory%20and%20inhibitory%20connections%2C%20ultimately%20learning%0Arobust%20graph-level%20representations%20via%20a%20geometrically%20sound%20Fr%5C%27echet%20mean%20for%0Agraph%20readout.%20Experiments%20on%20two%20large-scale%20fMRI%20datasets%20for%20psychiatric%0Adisorder%20classification%20demonstrate%20that%20our%20approach%20significantly%20outperforms%0Aa%20wide%20range%20of%20state-of-the-art%20Euclidean%20baselines.%20This%20work%20pioneers%20a%20new%0Ageometric%20deep%20learning%20paradigm%20for%20fMRI%20analysis%2C%20highlighting%20the%20immense%0Apotential%20of%20hyperbolic%20GNNs%20in%20the%20field%20of%20computational%20psychiatry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14965v1&entry.124074799=Read"},
{"title": "Template-Based Cortical Surface Reconstruction with Minimal Energy\n  Deformation", "author": "Patrick Madlindl and Fabian Bongratz and Christian Wachinger", "abstract": "  Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI)\nis fundamental to neuroimage analysis, enabling morphological studies of the\ncerebral cortex and functional brain mapping. Recent advances in learning-based\nCSR have dramatically accelerated processing, allowing for reconstructions\nthrough the deformation of anatomical templates within seconds. However,\nensuring the learned deformations are optimal in terms of deformation energy\nand consistent across training runs remains a particular challenge. In this\nwork, we design a Minimal Energy Deformation (MED) loss, acting as a\nregularizer on the deformation trajectories and complementing the widely used\nChamfer distance in CSR. We incorporate it into the recent V2C-Flow model and\ndemonstrate considerable improvements in previously neglected training\nconsistency and reproducibility without harming reconstruction accuracy and\ntopological correctness.\n", "link": "http://arxiv.org/abs/2509.14827v1", "date": "2025-09-18", "relevancy": 2.4644, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4929}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4929}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Template-Based%20Cortical%20Surface%20Reconstruction%20with%20Minimal%20Energy%0A%20%20Deformation&body=Title%3A%20Template-Based%20Cortical%20Surface%20Reconstruction%20with%20Minimal%20Energy%0A%20%20Deformation%0AAuthor%3A%20Patrick%20Madlindl%20and%20Fabian%20Bongratz%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Cortical%20surface%20reconstruction%20%28CSR%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%0Ais%20fundamental%20to%20neuroimage%20analysis%2C%20enabling%20morphological%20studies%20of%20the%0Acerebral%20cortex%20and%20functional%20brain%20mapping.%20Recent%20advances%20in%20learning-based%0ACSR%20have%20dramatically%20accelerated%20processing%2C%20allowing%20for%20reconstructions%0Athrough%20the%20deformation%20of%20anatomical%20templates%20within%20seconds.%20However%2C%0Aensuring%20the%20learned%20deformations%20are%20optimal%20in%20terms%20of%20deformation%20energy%0Aand%20consistent%20across%20training%20runs%20remains%20a%20particular%20challenge.%20In%20this%0Awork%2C%20we%20design%20a%20Minimal%20Energy%20Deformation%20%28MED%29%20loss%2C%20acting%20as%20a%0Aregularizer%20on%20the%20deformation%20trajectories%20and%20complementing%20the%20widely%20used%0AChamfer%20distance%20in%20CSR.%20We%20incorporate%20it%20into%20the%20recent%20V2C-Flow%20model%20and%0Ademonstrate%20considerable%20improvements%20in%20previously%20neglected%20training%0Aconsistency%20and%20reproducibility%20without%20harming%20reconstruction%20accuracy%20and%0Atopological%20correctness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemplate-Based%2520Cortical%2520Surface%2520Reconstruction%2520with%2520Minimal%2520Energy%250A%2520%2520Deformation%26entry.906535625%3DPatrick%2520Madlindl%2520and%2520Fabian%2520Bongratz%2520and%2520Christian%2520Wachinger%26entry.1292438233%3D%2520%2520Cortical%2520surface%2520reconstruction%2520%2528CSR%2529%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Ais%2520fundamental%2520to%2520neuroimage%2520analysis%252C%2520enabling%2520morphological%2520studies%2520of%2520the%250Acerebral%2520cortex%2520and%2520functional%2520brain%2520mapping.%2520Recent%2520advances%2520in%2520learning-based%250ACSR%2520have%2520dramatically%2520accelerated%2520processing%252C%2520allowing%2520for%2520reconstructions%250Athrough%2520the%2520deformation%2520of%2520anatomical%2520templates%2520within%2520seconds.%2520However%252C%250Aensuring%2520the%2520learned%2520deformations%2520are%2520optimal%2520in%2520terms%2520of%2520deformation%2520energy%250Aand%2520consistent%2520across%2520training%2520runs%2520remains%2520a%2520particular%2520challenge.%2520In%2520this%250Awork%252C%2520we%2520design%2520a%2520Minimal%2520Energy%2520Deformation%2520%2528MED%2529%2520loss%252C%2520acting%2520as%2520a%250Aregularizer%2520on%2520the%2520deformation%2520trajectories%2520and%2520complementing%2520the%2520widely%2520used%250AChamfer%2520distance%2520in%2520CSR.%2520We%2520incorporate%2520it%2520into%2520the%2520recent%2520V2C-Flow%2520model%2520and%250Ademonstrate%2520considerable%2520improvements%2520in%2520previously%2520neglected%2520training%250Aconsistency%2520and%2520reproducibility%2520without%2520harming%2520reconstruction%2520accuracy%2520and%250Atopological%2520correctness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Template-Based%20Cortical%20Surface%20Reconstruction%20with%20Minimal%20Energy%0A%20%20Deformation&entry.906535625=Patrick%20Madlindl%20and%20Fabian%20Bongratz%20and%20Christian%20Wachinger&entry.1292438233=%20%20Cortical%20surface%20reconstruction%20%28CSR%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%0Ais%20fundamental%20to%20neuroimage%20analysis%2C%20enabling%20morphological%20studies%20of%20the%0Acerebral%20cortex%20and%20functional%20brain%20mapping.%20Recent%20advances%20in%20learning-based%0ACSR%20have%20dramatically%20accelerated%20processing%2C%20allowing%20for%20reconstructions%0Athrough%20the%20deformation%20of%20anatomical%20templates%20within%20seconds.%20However%2C%0Aensuring%20the%20learned%20deformations%20are%20optimal%20in%20terms%20of%20deformation%20energy%0Aand%20consistent%20across%20training%20runs%20remains%20a%20particular%20challenge.%20In%20this%0Awork%2C%20we%20design%20a%20Minimal%20Energy%20Deformation%20%28MED%29%20loss%2C%20acting%20as%20a%0Aregularizer%20on%20the%20deformation%20trajectories%20and%20complementing%20the%20widely%20used%0AChamfer%20distance%20in%20CSR.%20We%20incorporate%20it%20into%20the%20recent%20V2C-Flow%20model%20and%0Ademonstrate%20considerable%20improvements%20in%20previously%20neglected%20training%0Aconsistency%20and%20reproducibility%20without%20harming%20reconstruction%20accuracy%20and%0Atopological%20correctness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14827v1&entry.124074799=Read"},
{"title": "Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in\n  Large-Scale Dynamic Environments", "author": "Haoxuan Jiang and Peicong Qian and Yusen Xie and Linwei Zheng and Xiaocong Li and Ming Liu and Jun Ma", "abstract": "  Reliable, drift-free global localization presents significant challenges yet\nremains crucial for autonomous navigation in large-scale dynamic environments.\nIn this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel\nOdometry fusion framework, which is specifically designed to provide\nhigh-precision state estimation and robust localization in large-scale dynamic\nenvironments. Our framework leverages an efficient semantic-voxel map\nrepresentation and employs an improved scan matching algorithm, which utilizes\nglobal semantic information to significantly reduce long-term trajectory drift.\nFurthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using\na tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter\n(iESKF). This ensures reliable localization without experiencing abnormal\ndrift. Moreover, to tackle the challenges posed by terrain variations and\ndynamic movements, we introduce a 3D adaptive scaling strategy that allows for\nflexible adjustments to wheel odometry measurement weights, thereby enhancing\nlocalization precision. This study presents extensive real-world experiments\nconducted in a one-million-square-meter automated port, encompassing 3,575\nhours of operational data from 35 Intelligent Guided Vehicles (IGVs). The\nresults consistently demonstrate that our system outperforms state-of-the-art\nLiDAR-based localization methods in large-scale dynamic environments,\nhighlighting the framework's reliability and practical value.\n", "link": "http://arxiv.org/abs/2509.14999v1", "date": "2025-09-18", "relevancy": 2.4593, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6008}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-LiDAR-Inertial-Wheel%20Odometry%20Fusion%20for%20Robust%20Localization%20in%0A%20%20Large-Scale%20Dynamic%20Environments&body=Title%3A%20Semantic-LiDAR-Inertial-Wheel%20Odometry%20Fusion%20for%20Robust%20Localization%20in%0A%20%20Large-Scale%20Dynamic%20Environments%0AAuthor%3A%20Haoxuan%20Jiang%20and%20Peicong%20Qian%20and%20Yusen%20Xie%20and%20Linwei%20Zheng%20and%20Xiaocong%20Li%20and%20Ming%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Reliable%2C%20drift-free%20global%20localization%20presents%20significant%20challenges%20yet%0Aremains%20crucial%20for%20autonomous%20navigation%20in%20large-scale%20dynamic%20environments.%0AIn%20this%20paper%2C%20we%20introduce%20a%20tightly-coupled%20Semantic-LiDAR-Inertial-Wheel%0AOdometry%20fusion%20framework%2C%20which%20is%20specifically%20designed%20to%20provide%0Ahigh-precision%20state%20estimation%20and%20robust%20localization%20in%20large-scale%20dynamic%0Aenvironments.%20Our%20framework%20leverages%20an%20efficient%20semantic-voxel%20map%0Arepresentation%20and%20employs%20an%20improved%20scan%20matching%20algorithm%2C%20which%20utilizes%0Aglobal%20semantic%20information%20to%20significantly%20reduce%20long-term%20trajectory%20drift.%0AFurthermore%2C%20it%20seamlessly%20fuses%20data%20from%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%20using%0Aa%20tightly-coupled%20multi-sensor%20fusion%20Iterative%20Error-State%20Kalman%20Filter%0A%28iESKF%29.%20This%20ensures%20reliable%20localization%20without%20experiencing%20abnormal%0Adrift.%20Moreover%2C%20to%20tackle%20the%20challenges%20posed%20by%20terrain%20variations%20and%0Adynamic%20movements%2C%20we%20introduce%20a%203D%20adaptive%20scaling%20strategy%20that%20allows%20for%0Aflexible%20adjustments%20to%20wheel%20odometry%20measurement%20weights%2C%20thereby%20enhancing%0Alocalization%20precision.%20This%20study%20presents%20extensive%20real-world%20experiments%0Aconducted%20in%20a%20one-million-square-meter%20automated%20port%2C%20encompassing%203%2C575%0Ahours%20of%20operational%20data%20from%2035%20Intelligent%20Guided%20Vehicles%20%28IGVs%29.%20The%0Aresults%20consistently%20demonstrate%20that%20our%20system%20outperforms%20state-of-the-art%0ALiDAR-based%20localization%20methods%20in%20large-scale%20dynamic%20environments%2C%0Ahighlighting%20the%20framework%27s%20reliability%20and%20practical%20value.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-LiDAR-Inertial-Wheel%2520Odometry%2520Fusion%2520for%2520Robust%2520Localization%2520in%250A%2520%2520Large-Scale%2520Dynamic%2520Environments%26entry.906535625%3DHaoxuan%2520Jiang%2520and%2520Peicong%2520Qian%2520and%2520Yusen%2520Xie%2520and%2520Linwei%2520Zheng%2520and%2520Xiaocong%2520Li%2520and%2520Ming%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Reliable%252C%2520drift-free%2520global%2520localization%2520presents%2520significant%2520challenges%2520yet%250Aremains%2520crucial%2520for%2520autonomous%2520navigation%2520in%2520large-scale%2520dynamic%2520environments.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520tightly-coupled%2520Semantic-LiDAR-Inertial-Wheel%250AOdometry%2520fusion%2520framework%252C%2520which%2520is%2520specifically%2520designed%2520to%2520provide%250Ahigh-precision%2520state%2520estimation%2520and%2520robust%2520localization%2520in%2520large-scale%2520dynamic%250Aenvironments.%2520Our%2520framework%2520leverages%2520an%2520efficient%2520semantic-voxel%2520map%250Arepresentation%2520and%2520employs%2520an%2520improved%2520scan%2520matching%2520algorithm%252C%2520which%2520utilizes%250Aglobal%2520semantic%2520information%2520to%2520significantly%2520reduce%2520long-term%2520trajectory%2520drift.%250AFurthermore%252C%2520it%2520seamlessly%2520fuses%2520data%2520from%2520LiDAR%252C%2520IMU%252C%2520and%2520wheel%2520odometry%2520using%250Aa%2520tightly-coupled%2520multi-sensor%2520fusion%2520Iterative%2520Error-State%2520Kalman%2520Filter%250A%2528iESKF%2529.%2520This%2520ensures%2520reliable%2520localization%2520without%2520experiencing%2520abnormal%250Adrift.%2520Moreover%252C%2520to%2520tackle%2520the%2520challenges%2520posed%2520by%2520terrain%2520variations%2520and%250Adynamic%2520movements%252C%2520we%2520introduce%2520a%25203D%2520adaptive%2520scaling%2520strategy%2520that%2520allows%2520for%250Aflexible%2520adjustments%2520to%2520wheel%2520odometry%2520measurement%2520weights%252C%2520thereby%2520enhancing%250Alocalization%2520precision.%2520This%2520study%2520presents%2520extensive%2520real-world%2520experiments%250Aconducted%2520in%2520a%2520one-million-square-meter%2520automated%2520port%252C%2520encompassing%25203%252C575%250Ahours%2520of%2520operational%2520data%2520from%252035%2520Intelligent%2520Guided%2520Vehicles%2520%2528IGVs%2529.%2520The%250Aresults%2520consistently%2520demonstrate%2520that%2520our%2520system%2520outperforms%2520state-of-the-art%250ALiDAR-based%2520localization%2520methods%2520in%2520large-scale%2520dynamic%2520environments%252C%250Ahighlighting%2520the%2520framework%2527s%2520reliability%2520and%2520practical%2520value.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-LiDAR-Inertial-Wheel%20Odometry%20Fusion%20for%20Robust%20Localization%20in%0A%20%20Large-Scale%20Dynamic%20Environments&entry.906535625=Haoxuan%20Jiang%20and%20Peicong%20Qian%20and%20Yusen%20Xie%20and%20Linwei%20Zheng%20and%20Xiaocong%20Li%20and%20Ming%20Liu%20and%20Jun%20Ma&entry.1292438233=%20%20Reliable%2C%20drift-free%20global%20localization%20presents%20significant%20challenges%20yet%0Aremains%20crucial%20for%20autonomous%20navigation%20in%20large-scale%20dynamic%20environments.%0AIn%20this%20paper%2C%20we%20introduce%20a%20tightly-coupled%20Semantic-LiDAR-Inertial-Wheel%0AOdometry%20fusion%20framework%2C%20which%20is%20specifically%20designed%20to%20provide%0Ahigh-precision%20state%20estimation%20and%20robust%20localization%20in%20large-scale%20dynamic%0Aenvironments.%20Our%20framework%20leverages%20an%20efficient%20semantic-voxel%20map%0Arepresentation%20and%20employs%20an%20improved%20scan%20matching%20algorithm%2C%20which%20utilizes%0Aglobal%20semantic%20information%20to%20significantly%20reduce%20long-term%20trajectory%20drift.%0AFurthermore%2C%20it%20seamlessly%20fuses%20data%20from%20LiDAR%2C%20IMU%2C%20and%20wheel%20odometry%20using%0Aa%20tightly-coupled%20multi-sensor%20fusion%20Iterative%20Error-State%20Kalman%20Filter%0A%28iESKF%29.%20This%20ensures%20reliable%20localization%20without%20experiencing%20abnormal%0Adrift.%20Moreover%2C%20to%20tackle%20the%20challenges%20posed%20by%20terrain%20variations%20and%0Adynamic%20movements%2C%20we%20introduce%20a%203D%20adaptive%20scaling%20strategy%20that%20allows%20for%0Aflexible%20adjustments%20to%20wheel%20odometry%20measurement%20weights%2C%20thereby%20enhancing%0Alocalization%20precision.%20This%20study%20presents%20extensive%20real-world%20experiments%0Aconducted%20in%20a%20one-million-square-meter%20automated%20port%2C%20encompassing%203%2C575%0Ahours%20of%20operational%20data%20from%2035%20Intelligent%20Guided%20Vehicles%20%28IGVs%29.%20The%0Aresults%20consistently%20demonstrate%20that%20our%20system%20outperforms%20state-of-the-art%0ALiDAR-based%20localization%20methods%20in%20large-scale%20dynamic%20environments%2C%0Ahighlighting%20the%20framework%27s%20reliability%20and%20practical%20value.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14999v1&entry.124074799=Read"},
{"title": "Tight PAC-Bayesian Risk Certificates for Contrastive Learning", "author": "Anna Van Elst and Debarghya Ghoshdastidar", "abstract": "  Contrastive representation learning is a modern paradigm for learning\nrepresentations of unlabeled data via augmentations -- precisely, contrastive\nmodels learn to embed semantically similar pairs of samples (positive pairs)\ncloser than independently drawn samples (negative samples). In spite of its\nempirical success and widespread use in foundation models, statistical theory\nfor contrastive learning remains less explored. Recent works have developed\ngeneralization error bounds for contrastive losses, but the resulting risk\ncertificates are either vacuous (certificates based on Rademacher complexity or\n$f$-divergence) or require strong assumptions about samples that are\nunreasonable in practice. The present paper develops non-vacuous PAC-Bayesian\nrisk certificates for contrastive representation learning, considering the\npractical considerations of the popular SimCLR framework. Notably, we take into\naccount that SimCLR reuses positive pairs of augmented data as negative samples\nfor other data, thereby inducing strong dependence and making classical PAC or\nPAC-Bayesian bounds inapplicable. We further refine existing bounds on the\ndownstream classification loss by incorporating SimCLR-specific factors,\nincluding data augmentation and temperature scaling, and derive risk\ncertificates for the contrastive zero-one risk. The resulting bounds for\ncontrastive loss and downstream prediction are much tighter than those of\nprevious risk certificates, as demonstrated by experiments on CIFAR-10.\n", "link": "http://arxiv.org/abs/2412.03486v3", "date": "2025-09-18", "relevancy": 2.4574, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4817}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning&body=Title%3A%20Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning%0AAuthor%3A%20Anna%20Van%20Elst%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20Contrastive%20representation%20learning%20is%20a%20modern%20paradigm%20for%20learning%0Arepresentations%20of%20unlabeled%20data%20via%20augmentations%20--%20precisely%2C%20contrastive%0Amodels%20learn%20to%20embed%20semantically%20similar%20pairs%20of%20samples%20%28positive%20pairs%29%0Acloser%20than%20independently%20drawn%20samples%20%28negative%20samples%29.%20In%20spite%20of%20its%0Aempirical%20success%20and%20widespread%20use%20in%20foundation%20models%2C%20statistical%20theory%0Afor%20contrastive%20learning%20remains%20less%20explored.%20Recent%20works%20have%20developed%0Ageneralization%20error%20bounds%20for%20contrastive%20losses%2C%20but%20the%20resulting%20risk%0Acertificates%20are%20either%20vacuous%20%28certificates%20based%20on%20Rademacher%20complexity%20or%0A%24f%24-divergence%29%20or%20require%20strong%20assumptions%20about%20samples%20that%20are%0Aunreasonable%20in%20practice.%20The%20present%20paper%20develops%20non-vacuous%20PAC-Bayesian%0Arisk%20certificates%20for%20contrastive%20representation%20learning%2C%20considering%20the%0Apractical%20considerations%20of%20the%20popular%20SimCLR%20framework.%20Notably%2C%20we%20take%20into%0Aaccount%20that%20SimCLR%20reuses%20positive%20pairs%20of%20augmented%20data%20as%20negative%20samples%0Afor%20other%20data%2C%20thereby%20inducing%20strong%20dependence%20and%20making%20classical%20PAC%20or%0APAC-Bayesian%20bounds%20inapplicable.%20We%20further%20refine%20existing%20bounds%20on%20the%0Adownstream%20classification%20loss%20by%20incorporating%20SimCLR-specific%20factors%2C%0Aincluding%20data%20augmentation%20and%20temperature%20scaling%2C%20and%20derive%20risk%0Acertificates%20for%20the%20contrastive%20zero-one%20risk.%20The%20resulting%20bounds%20for%0Acontrastive%20loss%20and%20downstream%20prediction%20are%20much%20tighter%20than%20those%20of%0Aprevious%20risk%20certificates%2C%20as%20demonstrated%20by%20experiments%20on%20CIFAR-10.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03486v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520PAC-Bayesian%2520Risk%2520Certificates%2520for%2520Contrastive%2520Learning%26entry.906535625%3DAnna%2520Van%2520Elst%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520Contrastive%2520representation%2520learning%2520is%2520a%2520modern%2520paradigm%2520for%2520learning%250Arepresentations%2520of%2520unlabeled%2520data%2520via%2520augmentations%2520--%2520precisely%252C%2520contrastive%250Amodels%2520learn%2520to%2520embed%2520semantically%2520similar%2520pairs%2520of%2520samples%2520%2528positive%2520pairs%2529%250Acloser%2520than%2520independently%2520drawn%2520samples%2520%2528negative%2520samples%2529.%2520In%2520spite%2520of%2520its%250Aempirical%2520success%2520and%2520widespread%2520use%2520in%2520foundation%2520models%252C%2520statistical%2520theory%250Afor%2520contrastive%2520learning%2520remains%2520less%2520explored.%2520Recent%2520works%2520have%2520developed%250Ageneralization%2520error%2520bounds%2520for%2520contrastive%2520losses%252C%2520but%2520the%2520resulting%2520risk%250Acertificates%2520are%2520either%2520vacuous%2520%2528certificates%2520based%2520on%2520Rademacher%2520complexity%2520or%250A%2524f%2524-divergence%2529%2520or%2520require%2520strong%2520assumptions%2520about%2520samples%2520that%2520are%250Aunreasonable%2520in%2520practice.%2520The%2520present%2520paper%2520develops%2520non-vacuous%2520PAC-Bayesian%250Arisk%2520certificates%2520for%2520contrastive%2520representation%2520learning%252C%2520considering%2520the%250Apractical%2520considerations%2520of%2520the%2520popular%2520SimCLR%2520framework.%2520Notably%252C%2520we%2520take%2520into%250Aaccount%2520that%2520SimCLR%2520reuses%2520positive%2520pairs%2520of%2520augmented%2520data%2520as%2520negative%2520samples%250Afor%2520other%2520data%252C%2520thereby%2520inducing%2520strong%2520dependence%2520and%2520making%2520classical%2520PAC%2520or%250APAC-Bayesian%2520bounds%2520inapplicable.%2520We%2520further%2520refine%2520existing%2520bounds%2520on%2520the%250Adownstream%2520classification%2520loss%2520by%2520incorporating%2520SimCLR-specific%2520factors%252C%250Aincluding%2520data%2520augmentation%2520and%2520temperature%2520scaling%252C%2520and%2520derive%2520risk%250Acertificates%2520for%2520the%2520contrastive%2520zero-one%2520risk.%2520The%2520resulting%2520bounds%2520for%250Acontrastive%2520loss%2520and%2520downstream%2520prediction%2520are%2520much%2520tighter%2520than%2520those%2520of%250Aprevious%2520risk%2520certificates%252C%2520as%2520demonstrated%2520by%2520experiments%2520on%2520CIFAR-10.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03486v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20PAC-Bayesian%20Risk%20Certificates%20for%20Contrastive%20Learning&entry.906535625=Anna%20Van%20Elst%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20Contrastive%20representation%20learning%20is%20a%20modern%20paradigm%20for%20learning%0Arepresentations%20of%20unlabeled%20data%20via%20augmentations%20--%20precisely%2C%20contrastive%0Amodels%20learn%20to%20embed%20semantically%20similar%20pairs%20of%20samples%20%28positive%20pairs%29%0Acloser%20than%20independently%20drawn%20samples%20%28negative%20samples%29.%20In%20spite%20of%20its%0Aempirical%20success%20and%20widespread%20use%20in%20foundation%20models%2C%20statistical%20theory%0Afor%20contrastive%20learning%20remains%20less%20explored.%20Recent%20works%20have%20developed%0Ageneralization%20error%20bounds%20for%20contrastive%20losses%2C%20but%20the%20resulting%20risk%0Acertificates%20are%20either%20vacuous%20%28certificates%20based%20on%20Rademacher%20complexity%20or%0A%24f%24-divergence%29%20or%20require%20strong%20assumptions%20about%20samples%20that%20are%0Aunreasonable%20in%20practice.%20The%20present%20paper%20develops%20non-vacuous%20PAC-Bayesian%0Arisk%20certificates%20for%20contrastive%20representation%20learning%2C%20considering%20the%0Apractical%20considerations%20of%20the%20popular%20SimCLR%20framework.%20Notably%2C%20we%20take%20into%0Aaccount%20that%20SimCLR%20reuses%20positive%20pairs%20of%20augmented%20data%20as%20negative%20samples%0Afor%20other%20data%2C%20thereby%20inducing%20strong%20dependence%20and%20making%20classical%20PAC%20or%0APAC-Bayesian%20bounds%20inapplicable.%20We%20further%20refine%20existing%20bounds%20on%20the%0Adownstream%20classification%20loss%20by%20incorporating%20SimCLR-specific%20factors%2C%0Aincluding%20data%20augmentation%20and%20temperature%20scaling%2C%20and%20derive%20risk%0Acertificates%20for%20the%20contrastive%20zero-one%20risk.%20The%20resulting%20bounds%20for%0Acontrastive%20loss%20and%20downstream%20prediction%20are%20much%20tighter%20than%20those%20of%0Aprevious%20risk%20certificates%2C%20as%20demonstrated%20by%20experiments%20on%20CIFAR-10.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03486v3&entry.124074799=Read"},
{"title": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated\n  Sensing and Communication Indoor Scene Inference", "author": "Carlos Barroso-Fern\u00e1ndez and Alejandro Calvillo-Fernandez and Antonio de la Oliva and Carlos J. Bernardos", "abstract": "  The upcoming generations of wireless technologies promise an era where\neverything is interconnected and intelligent. As the need for intelligence\ngrows, networks must learn to better understand the physical world. However,\ndeploying dedicated hardware to perceive the environment is not always\nfeasible, mainly due to costs and/or complexity. Integrated Sensing and\nCommunication (ISAC) has made a step forward in addressing this challenge.\nWithin ISAC, passive sensing emerges as a cost-effective solution that reuses\nwireless communications to sense the environment, without interfering with\nexisting communications. Nevertheless, the majority of current solutions are\nlimited to one technology (mostly Wi-Fi or 5G), constraining the maximum\naccuracy reachable. As different technologies work with different spectrums, we\nsee a necessity in integrating more than one technology to augment the coverage\narea. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a\nMultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.\nFAWN is based on the original transformers architecture, to fuse information\nfrom Wi-Fi and 5G, making the network capable of understanding the physical\nworld without interfering with the current communication. To test our solution,\nwe have built a prototype and integrated it in a real scenario. Results show\nerrors below 0.6 m around 84% of times.\n", "link": "http://arxiv.org/abs/2509.14968v1", "date": "2025-09-18", "relevancy": 2.4535, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAWN%3A%20A%20MultiEncoder%20Fusion-Attention%20Wave%20Network%20for%20Integrated%0A%20%20Sensing%20and%20Communication%20Indoor%20Scene%20Inference&body=Title%3A%20FAWN%3A%20A%20MultiEncoder%20Fusion-Attention%20Wave%20Network%20for%20Integrated%0A%20%20Sensing%20and%20Communication%20Indoor%20Scene%20Inference%0AAuthor%3A%20Carlos%20Barroso-Fern%C3%A1ndez%20and%20Alejandro%20Calvillo-Fernandez%20and%20Antonio%20de%20la%20Oliva%20and%20Carlos%20J.%20Bernardos%0AAbstract%3A%20%20%20The%20upcoming%20generations%20of%20wireless%20technologies%20promise%20an%20era%20where%0Aeverything%20is%20interconnected%20and%20intelligent.%20As%20the%20need%20for%20intelligence%0Agrows%2C%20networks%20must%20learn%20to%20better%20understand%20the%20physical%20world.%20However%2C%0Adeploying%20dedicated%20hardware%20to%20perceive%20the%20environment%20is%20not%20always%0Afeasible%2C%20mainly%20due%20to%20costs%20and/or%20complexity.%20Integrated%20Sensing%20and%0ACommunication%20%28ISAC%29%20has%20made%20a%20step%20forward%20in%20addressing%20this%20challenge.%0AWithin%20ISAC%2C%20passive%20sensing%20emerges%20as%20a%20cost-effective%20solution%20that%20reuses%0Awireless%20communications%20to%20sense%20the%20environment%2C%20without%20interfering%20with%0Aexisting%20communications.%20Nevertheless%2C%20the%20majority%20of%20current%20solutions%20are%0Alimited%20to%20one%20technology%20%28mostly%20Wi-Fi%20or%205G%29%2C%20constraining%20the%20maximum%0Aaccuracy%20reachable.%20As%20different%20technologies%20work%20with%20different%20spectrums%2C%20we%0Asee%20a%20necessity%20in%20integrating%20more%20than%20one%20technology%20to%20augment%20the%20coverage%0Aarea.%20Hence%2C%20we%20take%20the%20advantage%20of%20ISAC%20passive%20sensing%2C%20to%20present%20FAWN%2C%20a%0AMultiEncoder%20Fusion-Attention%20Wave%20Network%20for%20ISAC%20indoor%20scene%20inference.%0AFAWN%20is%20based%20on%20the%20original%20transformers%20architecture%2C%20to%20fuse%20information%0Afrom%20Wi-Fi%20and%205G%2C%20making%20the%20network%20capable%20of%20understanding%20the%20physical%0Aworld%20without%20interfering%20with%20the%20current%20communication.%20To%20test%20our%20solution%2C%0Awe%20have%20built%20a%20prototype%20and%20integrated%20it%20in%20a%20real%20scenario.%20Results%20show%0Aerrors%20below%200.6%20m%20around%2084%25%20of%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAWN%253A%2520A%2520MultiEncoder%2520Fusion-Attention%2520Wave%2520Network%2520for%2520Integrated%250A%2520%2520Sensing%2520and%2520Communication%2520Indoor%2520Scene%2520Inference%26entry.906535625%3DCarlos%2520Barroso-Fern%25C3%25A1ndez%2520and%2520Alejandro%2520Calvillo-Fernandez%2520and%2520Antonio%2520de%2520la%2520Oliva%2520and%2520Carlos%2520J.%2520Bernardos%26entry.1292438233%3D%2520%2520The%2520upcoming%2520generations%2520of%2520wireless%2520technologies%2520promise%2520an%2520era%2520where%250Aeverything%2520is%2520interconnected%2520and%2520intelligent.%2520As%2520the%2520need%2520for%2520intelligence%250Agrows%252C%2520networks%2520must%2520learn%2520to%2520better%2520understand%2520the%2520physical%2520world.%2520However%252C%250Adeploying%2520dedicated%2520hardware%2520to%2520perceive%2520the%2520environment%2520is%2520not%2520always%250Afeasible%252C%2520mainly%2520due%2520to%2520costs%2520and/or%2520complexity.%2520Integrated%2520Sensing%2520and%250ACommunication%2520%2528ISAC%2529%2520has%2520made%2520a%2520step%2520forward%2520in%2520addressing%2520this%2520challenge.%250AWithin%2520ISAC%252C%2520passive%2520sensing%2520emerges%2520as%2520a%2520cost-effective%2520solution%2520that%2520reuses%250Awireless%2520communications%2520to%2520sense%2520the%2520environment%252C%2520without%2520interfering%2520with%250Aexisting%2520communications.%2520Nevertheless%252C%2520the%2520majority%2520of%2520current%2520solutions%2520are%250Alimited%2520to%2520one%2520technology%2520%2528mostly%2520Wi-Fi%2520or%25205G%2529%252C%2520constraining%2520the%2520maximum%250Aaccuracy%2520reachable.%2520As%2520different%2520technologies%2520work%2520with%2520different%2520spectrums%252C%2520we%250Asee%2520a%2520necessity%2520in%2520integrating%2520more%2520than%2520one%2520technology%2520to%2520augment%2520the%2520coverage%250Aarea.%2520Hence%252C%2520we%2520take%2520the%2520advantage%2520of%2520ISAC%2520passive%2520sensing%252C%2520to%2520present%2520FAWN%252C%2520a%250AMultiEncoder%2520Fusion-Attention%2520Wave%2520Network%2520for%2520ISAC%2520indoor%2520scene%2520inference.%250AFAWN%2520is%2520based%2520on%2520the%2520original%2520transformers%2520architecture%252C%2520to%2520fuse%2520information%250Afrom%2520Wi-Fi%2520and%25205G%252C%2520making%2520the%2520network%2520capable%2520of%2520understanding%2520the%2520physical%250Aworld%2520without%2520interfering%2520with%2520the%2520current%2520communication.%2520To%2520test%2520our%2520solution%252C%250Awe%2520have%2520built%2520a%2520prototype%2520and%2520integrated%2520it%2520in%2520a%2520real%2520scenario.%2520Results%2520show%250Aerrors%2520below%25200.6%2520m%2520around%252084%2525%2520of%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAWN%3A%20A%20MultiEncoder%20Fusion-Attention%20Wave%20Network%20for%20Integrated%0A%20%20Sensing%20and%20Communication%20Indoor%20Scene%20Inference&entry.906535625=Carlos%20Barroso-Fern%C3%A1ndez%20and%20Alejandro%20Calvillo-Fernandez%20and%20Antonio%20de%20la%20Oliva%20and%20Carlos%20J.%20Bernardos&entry.1292438233=%20%20The%20upcoming%20generations%20of%20wireless%20technologies%20promise%20an%20era%20where%0Aeverything%20is%20interconnected%20and%20intelligent.%20As%20the%20need%20for%20intelligence%0Agrows%2C%20networks%20must%20learn%20to%20better%20understand%20the%20physical%20world.%20However%2C%0Adeploying%20dedicated%20hardware%20to%20perceive%20the%20environment%20is%20not%20always%0Afeasible%2C%20mainly%20due%20to%20costs%20and/or%20complexity.%20Integrated%20Sensing%20and%0ACommunication%20%28ISAC%29%20has%20made%20a%20step%20forward%20in%20addressing%20this%20challenge.%0AWithin%20ISAC%2C%20passive%20sensing%20emerges%20as%20a%20cost-effective%20solution%20that%20reuses%0Awireless%20communications%20to%20sense%20the%20environment%2C%20without%20interfering%20with%0Aexisting%20communications.%20Nevertheless%2C%20the%20majority%20of%20current%20solutions%20are%0Alimited%20to%20one%20technology%20%28mostly%20Wi-Fi%20or%205G%29%2C%20constraining%20the%20maximum%0Aaccuracy%20reachable.%20As%20different%20technologies%20work%20with%20different%20spectrums%2C%20we%0Asee%20a%20necessity%20in%20integrating%20more%20than%20one%20technology%20to%20augment%20the%20coverage%0Aarea.%20Hence%2C%20we%20take%20the%20advantage%20of%20ISAC%20passive%20sensing%2C%20to%20present%20FAWN%2C%20a%0AMultiEncoder%20Fusion-Attention%20Wave%20Network%20for%20ISAC%20indoor%20scene%20inference.%0AFAWN%20is%20based%20on%20the%20original%20transformers%20architecture%2C%20to%20fuse%20information%0Afrom%20Wi-Fi%20and%205G%2C%20making%20the%20network%20capable%20of%20understanding%20the%20physical%0Aworld%20without%20interfering%20with%20the%20current%20communication.%20To%20test%20our%20solution%2C%0Awe%20have%20built%20a%20prototype%20and%20integrated%20it%20in%20a%20real%20scenario.%20Results%20show%0Aerrors%20below%200.6%20m%20around%2084%25%20of%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14968v1&entry.124074799=Read"},
{"title": "QuizRank: Picking Images by Quizzing VLMs", "author": "Tenghao Ji and Eytan Adar", "abstract": "  Images play a vital role in improving the readability and comprehension of\nWikipedia articles by serving as `illustrative aids.' However, not all images\nare equally effective and not all Wikipedia editors are trained in their\nselection. We propose QuizRank, a novel method of image selection that\nleverages large language models (LLMs) and vision language models (VLMs) to\nrank images as learning interventions. Our approach transforms textual\ndescriptions of the article's subject into multiple-choice questions about\nimportant visual characteristics of the concept. We utilize these questions to\nquiz the VLM: the better an image can help answer questions, the higher it is\nranked. To further improve discrimination between visually similar items, we\nintroduce a Contrastive QuizRank that leverages differences in the features of\ntarget (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain\nBluebird) to generate questions. We demonstrate the potential of VLMs as\neffective visual evaluators by showing a high congruence with human quiz-takers\nand an effective discriminative ranking of images.\n", "link": "http://arxiv.org/abs/2509.15059v1", "date": "2025-09-18", "relevancy": 2.4415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuizRank%3A%20Picking%20Images%20by%20Quizzing%20VLMs&body=Title%3A%20QuizRank%3A%20Picking%20Images%20by%20Quizzing%20VLMs%0AAuthor%3A%20Tenghao%20Ji%20and%20Eytan%20Adar%0AAbstract%3A%20%20%20Images%20play%20a%20vital%20role%20in%20improving%20the%20readability%20and%20comprehension%20of%0AWikipedia%20articles%20by%20serving%20as%20%60illustrative%20aids.%27%20However%2C%20not%20all%20images%0Aare%20equally%20effective%20and%20not%20all%20Wikipedia%20editors%20are%20trained%20in%20their%0Aselection.%20We%20propose%20QuizRank%2C%20a%20novel%20method%20of%20image%20selection%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20and%20vision%20language%20models%20%28VLMs%29%20to%0Arank%20images%20as%20learning%20interventions.%20Our%20approach%20transforms%20textual%0Adescriptions%20of%20the%20article%27s%20subject%20into%20multiple-choice%20questions%20about%0Aimportant%20visual%20characteristics%20of%20the%20concept.%20We%20utilize%20these%20questions%20to%0Aquiz%20the%20VLM%3A%20the%20better%20an%20image%20can%20help%20answer%20questions%2C%20the%20higher%20it%20is%0Aranked.%20To%20further%20improve%20discrimination%20between%20visually%20similar%20items%2C%20we%0Aintroduce%20a%20Contrastive%20QuizRank%20that%20leverages%20differences%20in%20the%20features%20of%0Atarget%20%28e.g.%2C%20a%20Western%20Bluebird%29%20and%20distractor%20concepts%20%28e.g.%2C%20Mountain%0ABluebird%29%20to%20generate%20questions.%20We%20demonstrate%20the%20potential%20of%20VLMs%20as%0Aeffective%20visual%20evaluators%20by%20showing%20a%20high%20congruence%20with%20human%20quiz-takers%0Aand%20an%20effective%20discriminative%20ranking%20of%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuizRank%253A%2520Picking%2520Images%2520by%2520Quizzing%2520VLMs%26entry.906535625%3DTenghao%2520Ji%2520and%2520Eytan%2520Adar%26entry.1292438233%3D%2520%2520Images%2520play%2520a%2520vital%2520role%2520in%2520improving%2520the%2520readability%2520and%2520comprehension%2520of%250AWikipedia%2520articles%2520by%2520serving%2520as%2520%2560illustrative%2520aids.%2527%2520However%252C%2520not%2520all%2520images%250Aare%2520equally%2520effective%2520and%2520not%2520all%2520Wikipedia%2520editors%2520are%2520trained%2520in%2520their%250Aselection.%2520We%2520propose%2520QuizRank%252C%2520a%2520novel%2520method%2520of%2520image%2520selection%2520that%250Aleverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision%2520language%2520models%2520%2528VLMs%2529%2520to%250Arank%2520images%2520as%2520learning%2520interventions.%2520Our%2520approach%2520transforms%2520textual%250Adescriptions%2520of%2520the%2520article%2527s%2520subject%2520into%2520multiple-choice%2520questions%2520about%250Aimportant%2520visual%2520characteristics%2520of%2520the%2520concept.%2520We%2520utilize%2520these%2520questions%2520to%250Aquiz%2520the%2520VLM%253A%2520the%2520better%2520an%2520image%2520can%2520help%2520answer%2520questions%252C%2520the%2520higher%2520it%2520is%250Aranked.%2520To%2520further%2520improve%2520discrimination%2520between%2520visually%2520similar%2520items%252C%2520we%250Aintroduce%2520a%2520Contrastive%2520QuizRank%2520that%2520leverages%2520differences%2520in%2520the%2520features%2520of%250Atarget%2520%2528e.g.%252C%2520a%2520Western%2520Bluebird%2529%2520and%2520distractor%2520concepts%2520%2528e.g.%252C%2520Mountain%250ABluebird%2529%2520to%2520generate%2520questions.%2520We%2520demonstrate%2520the%2520potential%2520of%2520VLMs%2520as%250Aeffective%2520visual%2520evaluators%2520by%2520showing%2520a%2520high%2520congruence%2520with%2520human%2520quiz-takers%250Aand%2520an%2520effective%2520discriminative%2520ranking%2520of%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuizRank%3A%20Picking%20Images%20by%20Quizzing%20VLMs&entry.906535625=Tenghao%20Ji%20and%20Eytan%20Adar&entry.1292438233=%20%20Images%20play%20a%20vital%20role%20in%20improving%20the%20readability%20and%20comprehension%20of%0AWikipedia%20articles%20by%20serving%20as%20%60illustrative%20aids.%27%20However%2C%20not%20all%20images%0Aare%20equally%20effective%20and%20not%20all%20Wikipedia%20editors%20are%20trained%20in%20their%0Aselection.%20We%20propose%20QuizRank%2C%20a%20novel%20method%20of%20image%20selection%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20and%20vision%20language%20models%20%28VLMs%29%20to%0Arank%20images%20as%20learning%20interventions.%20Our%20approach%20transforms%20textual%0Adescriptions%20of%20the%20article%27s%20subject%20into%20multiple-choice%20questions%20about%0Aimportant%20visual%20characteristics%20of%20the%20concept.%20We%20utilize%20these%20questions%20to%0Aquiz%20the%20VLM%3A%20the%20better%20an%20image%20can%20help%20answer%20questions%2C%20the%20higher%20it%20is%0Aranked.%20To%20further%20improve%20discrimination%20between%20visually%20similar%20items%2C%20we%0Aintroduce%20a%20Contrastive%20QuizRank%20that%20leverages%20differences%20in%20the%20features%20of%0Atarget%20%28e.g.%2C%20a%20Western%20Bluebird%29%20and%20distractor%20concepts%20%28e.g.%2C%20Mountain%0ABluebird%29%20to%20generate%20questions.%20We%20demonstrate%20the%20potential%20of%20VLMs%20as%0Aeffective%20visual%20evaluators%20by%20showing%20a%20high%20congruence%20with%20human%20quiz-takers%0Aand%20an%20effective%20discriminative%20ranking%20of%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15059v1&entry.124074799=Read"},
{"title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D\n  Geometric Keypoint Matching", "author": "Xingwu Zhang and Guanxuan Li and Zhuocheng Zhang and Zijun Long", "abstract": "  The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.\n", "link": "http://arxiv.org/abs/2509.14966v1", "date": "2025-09-18", "relevancy": 2.4396, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6204}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6096}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboEye%3A%20Enhancing%202D%20Robotic%20Object%20Identification%20with%20Selective%203D%0A%20%20Geometric%20Keypoint%20Matching&body=Title%3A%20RoboEye%3A%20Enhancing%202D%20Robotic%20Object%20Identification%20with%20Selective%203D%0A%20%20Geometric%20Keypoint%20Matching%0AAuthor%3A%20Xingwu%20Zhang%20and%20Guanxuan%20Li%20and%20Zhuocheng%20Zhang%20and%20Zijun%20Long%0AAbstract%3A%20%20%20The%20rapidly%20growing%20number%20of%20product%20categories%20in%20large-scale%20e-commerce%0Amakes%20accurate%20object%20identification%20for%20automated%20packing%20in%20warehouses%0Asubstantially%20more%20difficult.%20As%20the%20catalog%20grows%2C%20intra-class%20variability%20and%0Aa%20long%20tail%20of%20rare%20or%20visually%20similar%20items%20increase%2C%20and%20when%20combined%20with%0Adiverse%20packaging%2C%20cluttered%20containers%2C%20frequent%20occlusion%2C%20and%20large%0Aviewpoint%20changes-these%20factors%20amplify%20discrepancies%20between%20query%20and%0Areference%20images%2C%20causing%20sharp%20performance%20drops%20for%20methods%20that%20rely%20solely%0Aon%202D%20appearance%20features.%20Thus%2C%20we%20propose%20RoboEye%2C%20a%20two-stage%20identification%0Aframework%20that%20dynamically%20augments%202D%20semantic%20features%20with%20domain-adapted%203D%0Areasoning%20and%20lightweight%20adapters%20to%20bridge%20training%20deployment%20gaps.%20In%20the%0Afirst%20stage%2C%20we%20train%20a%20large%20vision%20model%20to%20extract%202D%20features%20for%0Agenerating%20candidate%20rankings.%20A%20lightweight%203D-feature-awareness%20module%20then%0Aestimates%203D%20feature%20quality%20and%20predicts%20whether%203D%20re-ranking%20is%20necessary%2C%0Apreventing%20performance%20degradation%20and%20avoiding%20unnecessary%20computation.%20When%0Ainvoked%2C%20the%20second%20stage%20uses%20our%20robot%203D%20retrieval%20transformer%2C%20comprising%20a%0A3D%20feature%20extractor%20that%20produces%20geometry-aware%20dense%20features%20and%20a%0Akeypoint-based%20matcher%20that%20computes%20keypoint-correspondence%20confidences%0Abetween%20query%20and%20reference%20images%20instead%20of%20conventional%20cosine-similarity%0Ascoring.%20Experiments%20show%20that%20RoboEye%20improves%20Recall%401%20by%207.1%25%20over%20the%20prior%0Astate%20of%20the%20art%20%28RoboLLM%29.%20Moreover%2C%20RoboEye%20operates%20using%20only%20RGB%20images%2C%0Aavoiding%20reliance%20on%20explicit%203D%20inputs%20and%20reducing%20deployment%20costs.%20The%20code%0Aused%20in%20this%20paper%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/longkukuhi/RoboEye.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboEye%253A%2520Enhancing%25202D%2520Robotic%2520Object%2520Identification%2520with%2520Selective%25203D%250A%2520%2520Geometric%2520Keypoint%2520Matching%26entry.906535625%3DXingwu%2520Zhang%2520and%2520Guanxuan%2520Li%2520and%2520Zhuocheng%2520Zhang%2520and%2520Zijun%2520Long%26entry.1292438233%3D%2520%2520The%2520rapidly%2520growing%2520number%2520of%2520product%2520categories%2520in%2520large-scale%2520e-commerce%250Amakes%2520accurate%2520object%2520identification%2520for%2520automated%2520packing%2520in%2520warehouses%250Asubstantially%2520more%2520difficult.%2520As%2520the%2520catalog%2520grows%252C%2520intra-class%2520variability%2520and%250Aa%2520long%2520tail%2520of%2520rare%2520or%2520visually%2520similar%2520items%2520increase%252C%2520and%2520when%2520combined%2520with%250Adiverse%2520packaging%252C%2520cluttered%2520containers%252C%2520frequent%2520occlusion%252C%2520and%2520large%250Aviewpoint%2520changes-these%2520factors%2520amplify%2520discrepancies%2520between%2520query%2520and%250Areference%2520images%252C%2520causing%2520sharp%2520performance%2520drops%2520for%2520methods%2520that%2520rely%2520solely%250Aon%25202D%2520appearance%2520features.%2520Thus%252C%2520we%2520propose%2520RoboEye%252C%2520a%2520two-stage%2520identification%250Aframework%2520that%2520dynamically%2520augments%25202D%2520semantic%2520features%2520with%2520domain-adapted%25203D%250Areasoning%2520and%2520lightweight%2520adapters%2520to%2520bridge%2520training%2520deployment%2520gaps.%2520In%2520the%250Afirst%2520stage%252C%2520we%2520train%2520a%2520large%2520vision%2520model%2520to%2520extract%25202D%2520features%2520for%250Agenerating%2520candidate%2520rankings.%2520A%2520lightweight%25203D-feature-awareness%2520module%2520then%250Aestimates%25203D%2520feature%2520quality%2520and%2520predicts%2520whether%25203D%2520re-ranking%2520is%2520necessary%252C%250Apreventing%2520performance%2520degradation%2520and%2520avoiding%2520unnecessary%2520computation.%2520When%250Ainvoked%252C%2520the%2520second%2520stage%2520uses%2520our%2520robot%25203D%2520retrieval%2520transformer%252C%2520comprising%2520a%250A3D%2520feature%2520extractor%2520that%2520produces%2520geometry-aware%2520dense%2520features%2520and%2520a%250Akeypoint-based%2520matcher%2520that%2520computes%2520keypoint-correspondence%2520confidences%250Abetween%2520query%2520and%2520reference%2520images%2520instead%2520of%2520conventional%2520cosine-similarity%250Ascoring.%2520Experiments%2520show%2520that%2520RoboEye%2520improves%2520Recall%25401%2520by%25207.1%2525%2520over%2520the%2520prior%250Astate%2520of%2520the%2520art%2520%2528RoboLLM%2529.%2520Moreover%252C%2520RoboEye%2520operates%2520using%2520only%2520RGB%2520images%252C%250Aavoiding%2520reliance%2520on%2520explicit%25203D%2520inputs%2520and%2520reducing%2520deployment%2520costs.%2520The%2520code%250Aused%2520in%2520this%2520paper%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/longkukuhi/RoboEye.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboEye%3A%20Enhancing%202D%20Robotic%20Object%20Identification%20with%20Selective%203D%0A%20%20Geometric%20Keypoint%20Matching&entry.906535625=Xingwu%20Zhang%20and%20Guanxuan%20Li%20and%20Zhuocheng%20Zhang%20and%20Zijun%20Long&entry.1292438233=%20%20The%20rapidly%20growing%20number%20of%20product%20categories%20in%20large-scale%20e-commerce%0Amakes%20accurate%20object%20identification%20for%20automated%20packing%20in%20warehouses%0Asubstantially%20more%20difficult.%20As%20the%20catalog%20grows%2C%20intra-class%20variability%20and%0Aa%20long%20tail%20of%20rare%20or%20visually%20similar%20items%20increase%2C%20and%20when%20combined%20with%0Adiverse%20packaging%2C%20cluttered%20containers%2C%20frequent%20occlusion%2C%20and%20large%0Aviewpoint%20changes-these%20factors%20amplify%20discrepancies%20between%20query%20and%0Areference%20images%2C%20causing%20sharp%20performance%20drops%20for%20methods%20that%20rely%20solely%0Aon%202D%20appearance%20features.%20Thus%2C%20we%20propose%20RoboEye%2C%20a%20two-stage%20identification%0Aframework%20that%20dynamically%20augments%202D%20semantic%20features%20with%20domain-adapted%203D%0Areasoning%20and%20lightweight%20adapters%20to%20bridge%20training%20deployment%20gaps.%20In%20the%0Afirst%20stage%2C%20we%20train%20a%20large%20vision%20model%20to%20extract%202D%20features%20for%0Agenerating%20candidate%20rankings.%20A%20lightweight%203D-feature-awareness%20module%20then%0Aestimates%203D%20feature%20quality%20and%20predicts%20whether%203D%20re-ranking%20is%20necessary%2C%0Apreventing%20performance%20degradation%20and%20avoiding%20unnecessary%20computation.%20When%0Ainvoked%2C%20the%20second%20stage%20uses%20our%20robot%203D%20retrieval%20transformer%2C%20comprising%20a%0A3D%20feature%20extractor%20that%20produces%20geometry-aware%20dense%20features%20and%20a%0Akeypoint-based%20matcher%20that%20computes%20keypoint-correspondence%20confidences%0Abetween%20query%20and%20reference%20images%20instead%20of%20conventional%20cosine-similarity%0Ascoring.%20Experiments%20show%20that%20RoboEye%20improves%20Recall%401%20by%207.1%25%20over%20the%20prior%0Astate%20of%20the%20art%20%28RoboLLM%29.%20Moreover%2C%20RoboEye%20operates%20using%20only%20RGB%20images%2C%0Aavoiding%20reliance%20on%20explicit%203D%20inputs%20and%20reducing%20deployment%20costs.%20The%20code%0Aused%20in%20this%20paper%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/longkukuhi/RoboEye.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14966v1&entry.124074799=Read"},
{"title": "Geometric Image Synchronization with Deep Watermarking", "author": "Pierre Fernandez and Tom\u00e1\u0161 Sou\u010dek and Nikola Jovanovi\u0107 and Hady Elsahar and Sylvestre-Alvise Rebuffi and Valeriu Lacatusu and Tuan Tran and Alexandre Mourachko", "abstract": "  Synchronization is the task of estimating and inverting geometric\ntransformations (e.g., crop, rotation) applied to an image. This work\nintroduces SyncSeal, a bespoke watermarking method for robust image\nsynchronization, which can be applied on top of existing watermarking methods\nto enhance their robustness against geometric transformations. It relies on an\nembedder network that imperceptibly alters images and an extractor network that\npredicts the geometric transformation to which the image was subjected. Both\nnetworks are end-to-end trained to minimize the error between the predicted and\nground-truth parameters of the transformation, combined with a discriminator to\nmaintain high perceptual quality. We experimentally validate our method on a\nwide variety of geometric and valuemetric transformations, demonstrating its\neffectiveness in accurately synchronizing images. We further show that our\nsynchronization can effectively upgrade existing watermarking methods to\nwithstand geometric transformations to which they were previously vulnerable.\n", "link": "http://arxiv.org/abs/2509.15208v1", "date": "2025-09-18", "relevancy": 2.4181, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4997}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4831}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Image%20Synchronization%20with%20Deep%20Watermarking&body=Title%3A%20Geometric%20Image%20Synchronization%20with%20Deep%20Watermarking%0AAuthor%3A%20Pierre%20Fernandez%20and%20Tom%C3%A1%C5%A1%20Sou%C4%8Dek%20and%20Nikola%20Jovanovi%C4%87%20and%20Hady%20Elsahar%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Valeriu%20Lacatusu%20and%20Tuan%20Tran%20and%20Alexandre%20Mourachko%0AAbstract%3A%20%20%20Synchronization%20is%20the%20task%20of%20estimating%20and%20inverting%20geometric%0Atransformations%20%28e.g.%2C%20crop%2C%20rotation%29%20applied%20to%20an%20image.%20This%20work%0Aintroduces%20SyncSeal%2C%20a%20bespoke%20watermarking%20method%20for%20robust%20image%0Asynchronization%2C%20which%20can%20be%20applied%20on%20top%20of%20existing%20watermarking%20methods%0Ato%20enhance%20their%20robustness%20against%20geometric%20transformations.%20It%20relies%20on%20an%0Aembedder%20network%20that%20imperceptibly%20alters%20images%20and%20an%20extractor%20network%20that%0Apredicts%20the%20geometric%20transformation%20to%20which%20the%20image%20was%20subjected.%20Both%0Anetworks%20are%20end-to-end%20trained%20to%20minimize%20the%20error%20between%20the%20predicted%20and%0Aground-truth%20parameters%20of%20the%20transformation%2C%20combined%20with%20a%20discriminator%20to%0Amaintain%20high%20perceptual%20quality.%20We%20experimentally%20validate%20our%20method%20on%20a%0Awide%20variety%20of%20geometric%20and%20valuemetric%20transformations%2C%20demonstrating%20its%0Aeffectiveness%20in%20accurately%20synchronizing%20images.%20We%20further%20show%20that%20our%0Asynchronization%20can%20effectively%20upgrade%20existing%20watermarking%20methods%20to%0Awithstand%20geometric%20transformations%20to%20which%20they%20were%20previously%20vulnerable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Image%2520Synchronization%2520with%2520Deep%2520Watermarking%26entry.906535625%3DPierre%2520Fernandez%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Sou%25C4%258Dek%2520and%2520Nikola%2520Jovanovi%25C4%2587%2520and%2520Hady%2520Elsahar%2520and%2520Sylvestre-Alvise%2520Rebuffi%2520and%2520Valeriu%2520Lacatusu%2520and%2520Tuan%2520Tran%2520and%2520Alexandre%2520Mourachko%26entry.1292438233%3D%2520%2520Synchronization%2520is%2520the%2520task%2520of%2520estimating%2520and%2520inverting%2520geometric%250Atransformations%2520%2528e.g.%252C%2520crop%252C%2520rotation%2529%2520applied%2520to%2520an%2520image.%2520This%2520work%250Aintroduces%2520SyncSeal%252C%2520a%2520bespoke%2520watermarking%2520method%2520for%2520robust%2520image%250Asynchronization%252C%2520which%2520can%2520be%2520applied%2520on%2520top%2520of%2520existing%2520watermarking%2520methods%250Ato%2520enhance%2520their%2520robustness%2520against%2520geometric%2520transformations.%2520It%2520relies%2520on%2520an%250Aembedder%2520network%2520that%2520imperceptibly%2520alters%2520images%2520and%2520an%2520extractor%2520network%2520that%250Apredicts%2520the%2520geometric%2520transformation%2520to%2520which%2520the%2520image%2520was%2520subjected.%2520Both%250Anetworks%2520are%2520end-to-end%2520trained%2520to%2520minimize%2520the%2520error%2520between%2520the%2520predicted%2520and%250Aground-truth%2520parameters%2520of%2520the%2520transformation%252C%2520combined%2520with%2520a%2520discriminator%2520to%250Amaintain%2520high%2520perceptual%2520quality.%2520We%2520experimentally%2520validate%2520our%2520method%2520on%2520a%250Awide%2520variety%2520of%2520geometric%2520and%2520valuemetric%2520transformations%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520accurately%2520synchronizing%2520images.%2520We%2520further%2520show%2520that%2520our%250Asynchronization%2520can%2520effectively%2520upgrade%2520existing%2520watermarking%2520methods%2520to%250Awithstand%2520geometric%2520transformations%2520to%2520which%2520they%2520were%2520previously%2520vulnerable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Image%20Synchronization%20with%20Deep%20Watermarking&entry.906535625=Pierre%20Fernandez%20and%20Tom%C3%A1%C5%A1%20Sou%C4%8Dek%20and%20Nikola%20Jovanovi%C4%87%20and%20Hady%20Elsahar%20and%20Sylvestre-Alvise%20Rebuffi%20and%20Valeriu%20Lacatusu%20and%20Tuan%20Tran%20and%20Alexandre%20Mourachko&entry.1292438233=%20%20Synchronization%20is%20the%20task%20of%20estimating%20and%20inverting%20geometric%0Atransformations%20%28e.g.%2C%20crop%2C%20rotation%29%20applied%20to%20an%20image.%20This%20work%0Aintroduces%20SyncSeal%2C%20a%20bespoke%20watermarking%20method%20for%20robust%20image%0Asynchronization%2C%20which%20can%20be%20applied%20on%20top%20of%20existing%20watermarking%20methods%0Ato%20enhance%20their%20robustness%20against%20geometric%20transformations.%20It%20relies%20on%20an%0Aembedder%20network%20that%20imperceptibly%20alters%20images%20and%20an%20extractor%20network%20that%0Apredicts%20the%20geometric%20transformation%20to%20which%20the%20image%20was%20subjected.%20Both%0Anetworks%20are%20end-to-end%20trained%20to%20minimize%20the%20error%20between%20the%20predicted%20and%0Aground-truth%20parameters%20of%20the%20transformation%2C%20combined%20with%20a%20discriminator%20to%0Amaintain%20high%20perceptual%20quality.%20We%20experimentally%20validate%20our%20method%20on%20a%0Awide%20variety%20of%20geometric%20and%20valuemetric%20transformations%2C%20demonstrating%20its%0Aeffectiveness%20in%20accurately%20synchronizing%20images.%20We%20further%20show%20that%20our%0Asynchronization%20can%20effectively%20upgrade%20existing%20watermarking%20methods%20to%0Awithstand%20geometric%20transformations%20to%20which%20they%20were%20previously%20vulnerable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15208v1&entry.124074799=Read"},
{"title": "BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting\n  Speakers in Child-Centered Long-Form Recordings", "author": "Th\u00e9o Charlot and Tarek Kunze and Maxime Poli and Alejandrina Cristia and Emmanuel Dupoux and Marvin Lavechin", "abstract": "  Child-centered long-form recordings are essential for studying early language\ndevelopment, but existing speech models trained on clean adult data perform\npoorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the\nfirst self-supervised speech representation model trained on 13,000 hours of\nmultilingual child-centered long-form recordings spanning over 40 languages. We\nevaluate BabyHuBERT on speaker segmentation, identifying when target children\nspeak versus female adults, male adults, or other children -- a fundamental\npreprocessing step for analyzing naturalistic language experiences. BabyHuBERT\nachieves F1-scores from 52.1% to 74.4% across six diverse datasets,\nconsistently outperforming W2V2-LL4300 (trained on English long-forms) and\nstandard HuBERT (trained on clean adult speech). Notable improvements include\n13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon\nIslands corpora, demonstrating effectiveness on underrepresented languages. By\nsharing code and models, BabyHuBERT serves as a foundation model for child\nspeech research, enabling fine-tuning on diverse downstream tasks.\n", "link": "http://arxiv.org/abs/2509.15001v1", "date": "2025-09-18", "relevancy": 2.4142, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BabyHuBERT%3A%20Multilingual%20Self-Supervised%20Learning%20for%20Segmenting%0A%20%20Speakers%20in%20Child-Centered%20Long-Form%20Recordings&body=Title%3A%20BabyHuBERT%3A%20Multilingual%20Self-Supervised%20Learning%20for%20Segmenting%0A%20%20Speakers%20in%20Child-Centered%20Long-Form%20Recordings%0AAuthor%3A%20Th%C3%A9o%20Charlot%20and%20Tarek%20Kunze%20and%20Maxime%20Poli%20and%20Alejandrina%20Cristia%20and%20Emmanuel%20Dupoux%20and%20Marvin%20Lavechin%0AAbstract%3A%20%20%20Child-centered%20long-form%20recordings%20are%20essential%20for%20studying%20early%20language%0Adevelopment%2C%20but%20existing%20speech%20models%20trained%20on%20clean%20adult%20data%20perform%0Apoorly%20due%20to%20acoustic%20and%20linguistic%20differences.%20We%20introduce%20BabyHuBERT%2C%20the%0Afirst%20self-supervised%20speech%20representation%20model%20trained%20on%2013%2C000%20hours%20of%0Amultilingual%20child-centered%20long-form%20recordings%20spanning%20over%2040%20languages.%20We%0Aevaluate%20BabyHuBERT%20on%20speaker%20segmentation%2C%20identifying%20when%20target%20children%0Aspeak%20versus%20female%20adults%2C%20male%20adults%2C%20or%20other%20children%20--%20a%20fundamental%0Apreprocessing%20step%20for%20analyzing%20naturalistic%20language%20experiences.%20BabyHuBERT%0Aachieves%20F1-scores%20from%2052.1%25%20to%2074.4%25%20across%20six%20diverse%20datasets%2C%0Aconsistently%20outperforming%20W2V2-LL4300%20%28trained%20on%20English%20long-forms%29%20and%0Astandard%20HuBERT%20%28trained%20on%20clean%20adult%20speech%29.%20Notable%20improvements%20include%0A13.2%20absolute%20F1%20points%20over%20HuBERT%20on%20Vanuatu%20and%2015.9%20points%20on%20Solomon%0AIslands%20corpora%2C%20demonstrating%20effectiveness%20on%20underrepresented%20languages.%20By%0Asharing%20code%20and%20models%2C%20BabyHuBERT%20serves%20as%20a%20foundation%20model%20for%20child%0Aspeech%20research%2C%20enabling%20fine-tuning%20on%20diverse%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBabyHuBERT%253A%2520Multilingual%2520Self-Supervised%2520Learning%2520for%2520Segmenting%250A%2520%2520Speakers%2520in%2520Child-Centered%2520Long-Form%2520Recordings%26entry.906535625%3DTh%25C3%25A9o%2520Charlot%2520and%2520Tarek%2520Kunze%2520and%2520Maxime%2520Poli%2520and%2520Alejandrina%2520Cristia%2520and%2520Emmanuel%2520Dupoux%2520and%2520Marvin%2520Lavechin%26entry.1292438233%3D%2520%2520Child-centered%2520long-form%2520recordings%2520are%2520essential%2520for%2520studying%2520early%2520language%250Adevelopment%252C%2520but%2520existing%2520speech%2520models%2520trained%2520on%2520clean%2520adult%2520data%2520perform%250Apoorly%2520due%2520to%2520acoustic%2520and%2520linguistic%2520differences.%2520We%2520introduce%2520BabyHuBERT%252C%2520the%250Afirst%2520self-supervised%2520speech%2520representation%2520model%2520trained%2520on%252013%252C000%2520hours%2520of%250Amultilingual%2520child-centered%2520long-form%2520recordings%2520spanning%2520over%252040%2520languages.%2520We%250Aevaluate%2520BabyHuBERT%2520on%2520speaker%2520segmentation%252C%2520identifying%2520when%2520target%2520children%250Aspeak%2520versus%2520female%2520adults%252C%2520male%2520adults%252C%2520or%2520other%2520children%2520--%2520a%2520fundamental%250Apreprocessing%2520step%2520for%2520analyzing%2520naturalistic%2520language%2520experiences.%2520BabyHuBERT%250Aachieves%2520F1-scores%2520from%252052.1%2525%2520to%252074.4%2525%2520across%2520six%2520diverse%2520datasets%252C%250Aconsistently%2520outperforming%2520W2V2-LL4300%2520%2528trained%2520on%2520English%2520long-forms%2529%2520and%250Astandard%2520HuBERT%2520%2528trained%2520on%2520clean%2520adult%2520speech%2529.%2520Notable%2520improvements%2520include%250A13.2%2520absolute%2520F1%2520points%2520over%2520HuBERT%2520on%2520Vanuatu%2520and%252015.9%2520points%2520on%2520Solomon%250AIslands%2520corpora%252C%2520demonstrating%2520effectiveness%2520on%2520underrepresented%2520languages.%2520By%250Asharing%2520code%2520and%2520models%252C%2520BabyHuBERT%2520serves%2520as%2520a%2520foundation%2520model%2520for%2520child%250Aspeech%2520research%252C%2520enabling%2520fine-tuning%2520on%2520diverse%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BabyHuBERT%3A%20Multilingual%20Self-Supervised%20Learning%20for%20Segmenting%0A%20%20Speakers%20in%20Child-Centered%20Long-Form%20Recordings&entry.906535625=Th%C3%A9o%20Charlot%20and%20Tarek%20Kunze%20and%20Maxime%20Poli%20and%20Alejandrina%20Cristia%20and%20Emmanuel%20Dupoux%20and%20Marvin%20Lavechin&entry.1292438233=%20%20Child-centered%20long-form%20recordings%20are%20essential%20for%20studying%20early%20language%0Adevelopment%2C%20but%20existing%20speech%20models%20trained%20on%20clean%20adult%20data%20perform%0Apoorly%20due%20to%20acoustic%20and%20linguistic%20differences.%20We%20introduce%20BabyHuBERT%2C%20the%0Afirst%20self-supervised%20speech%20representation%20model%20trained%20on%2013%2C000%20hours%20of%0Amultilingual%20child-centered%20long-form%20recordings%20spanning%20over%2040%20languages.%20We%0Aevaluate%20BabyHuBERT%20on%20speaker%20segmentation%2C%20identifying%20when%20target%20children%0Aspeak%20versus%20female%20adults%2C%20male%20adults%2C%20or%20other%20children%20--%20a%20fundamental%0Apreprocessing%20step%20for%20analyzing%20naturalistic%20language%20experiences.%20BabyHuBERT%0Aachieves%20F1-scores%20from%2052.1%25%20to%2074.4%25%20across%20six%20diverse%20datasets%2C%0Aconsistently%20outperforming%20W2V2-LL4300%20%28trained%20on%20English%20long-forms%29%20and%0Astandard%20HuBERT%20%28trained%20on%20clean%20adult%20speech%29.%20Notable%20improvements%20include%0A13.2%20absolute%20F1%20points%20over%20HuBERT%20on%20Vanuatu%20and%2015.9%20points%20on%20Solomon%0AIslands%20corpora%2C%20demonstrating%20effectiveness%20on%20underrepresented%20languages.%20By%0Asharing%20code%20and%20models%2C%20BabyHuBERT%20serves%20as%20a%20foundation%20model%20for%20child%0Aspeech%20research%2C%20enabling%20fine-tuning%20on%20diverse%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15001v1&entry.124074799=Read"},
{"title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes", "author": "Fang Li and Hao Zhang and Narendra Ahuja", "abstract": "  Although COLMAP has long remained the predominant method for camera parameter\noptimization in static scenes, it is constrained by its lengthy runtime and\nreliance on ground truth (GT) motion masks for application to dynamic scenes.\nMany efforts attempted to improve it by incorporating more priors as\nsupervision such as GT focal length, motion masks, 3D point clouds, camera\nposes, and metric depth, which, however, are typically unavailable in casually\ncaptured RGB videos. In this paper, we propose a novel method for more accurate\nand efficient camera parameter optimization in dynamic scenes solely supervised\nby a single RGB video. Our method consists of three key components: (1)\nPatch-wise Tracking Filters, to establish robust and maximally sparse\nhinge-like relations across the RGB video. (2) Outlier-aware Joint\nOptimization, for efficient camera parameter optimization by adaptive\ndown-weighting of moving outliers, without reliance on motion priors. (3) A\nTwo-stage Optimization Strategy, to enhance stability and optimization speed by\na trade-off between the Softplus limits and convex minima in losses. We\nvisually and numerically evaluate our camera estimates. To further validate\naccuracy, we feed the camera estimates into a 4D reconstruction method and\nassess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform\nexperiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)\nand 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates\ncamera parameters more efficiently and accurately with a single RGB video as\nthe only supervision.\n", "link": "http://arxiv.org/abs/2509.15123v1", "date": "2025-09-18", "relevancy": 2.4113, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6723}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6182}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Only%20Supervised%20Camera%20Parameter%20Optimization%20in%20Dynamic%20Scenes&body=Title%3A%20RGB-Only%20Supervised%20Camera%20Parameter%20Optimization%20in%20Dynamic%20Scenes%0AAuthor%3A%20Fang%20Li%20and%20Hao%20Zhang%20and%20Narendra%20Ahuja%0AAbstract%3A%20%20%20Although%20COLMAP%20has%20long%20remained%20the%20predominant%20method%20for%20camera%20parameter%0Aoptimization%20in%20static%20scenes%2C%20it%20is%20constrained%20by%20its%20lengthy%20runtime%20and%0Areliance%20on%20ground%20truth%20%28GT%29%20motion%20masks%20for%20application%20to%20dynamic%20scenes.%0AMany%20efforts%20attempted%20to%20improve%20it%20by%20incorporating%20more%20priors%20as%0Asupervision%20such%20as%20GT%20focal%20length%2C%20motion%20masks%2C%203D%20point%20clouds%2C%20camera%0Aposes%2C%20and%20metric%20depth%2C%20which%2C%20however%2C%20are%20typically%20unavailable%20in%20casually%0Acaptured%20RGB%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20more%20accurate%0Aand%20efficient%20camera%20parameter%20optimization%20in%20dynamic%20scenes%20solely%20supervised%0Aby%20a%20single%20RGB%20video.%20Our%20method%20consists%20of%20three%20key%20components%3A%20%281%29%0APatch-wise%20Tracking%20Filters%2C%20to%20establish%20robust%20and%20maximally%20sparse%0Ahinge-like%20relations%20across%20the%20RGB%20video.%20%282%29%20Outlier-aware%20Joint%0AOptimization%2C%20for%20efficient%20camera%20parameter%20optimization%20by%20adaptive%0Adown-weighting%20of%20moving%20outliers%2C%20without%20reliance%20on%20motion%20priors.%20%283%29%20A%0ATwo-stage%20Optimization%20Strategy%2C%20to%20enhance%20stability%20and%20optimization%20speed%20by%0Aa%20trade-off%20between%20the%20Softplus%20limits%20and%20convex%20minima%20in%20losses.%20We%0Avisually%20and%20numerically%20evaluate%20our%20camera%20estimates.%20To%20further%20validate%0Aaccuracy%2C%20we%20feed%20the%20camera%20estimates%20into%20a%204D%20reconstruction%20method%20and%0Aassess%20the%20resulting%203D%20scenes%2C%20and%20rendered%202D%20RGB%20and%20depth%20maps.%20We%20perform%0Aexperiments%20on%204%20real-world%20datasets%20%28NeRF-DS%2C%20DAVIS%2C%20iPhone%2C%20and%20TUM-dynamics%29%0Aand%201%20synthetic%20dataset%20%28MPI-Sintel%29%2C%20demonstrating%20that%20our%20method%20estimates%0Acamera%20parameters%20more%20efficiently%20and%20accurately%20with%20a%20single%20RGB%20video%20as%0Athe%20only%20supervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Only%2520Supervised%2520Camera%2520Parameter%2520Optimization%2520in%2520Dynamic%2520Scenes%26entry.906535625%3DFang%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Narendra%2520Ahuja%26entry.1292438233%3D%2520%2520Although%2520COLMAP%2520has%2520long%2520remained%2520the%2520predominant%2520method%2520for%2520camera%2520parameter%250Aoptimization%2520in%2520static%2520scenes%252C%2520it%2520is%2520constrained%2520by%2520its%2520lengthy%2520runtime%2520and%250Areliance%2520on%2520ground%2520truth%2520%2528GT%2529%2520motion%2520masks%2520for%2520application%2520to%2520dynamic%2520scenes.%250AMany%2520efforts%2520attempted%2520to%2520improve%2520it%2520by%2520incorporating%2520more%2520priors%2520as%250Asupervision%2520such%2520as%2520GT%2520focal%2520length%252C%2520motion%2520masks%252C%25203D%2520point%2520clouds%252C%2520camera%250Aposes%252C%2520and%2520metric%2520depth%252C%2520which%252C%2520however%252C%2520are%2520typically%2520unavailable%2520in%2520casually%250Acaptured%2520RGB%2520videos.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520more%2520accurate%250Aand%2520efficient%2520camera%2520parameter%2520optimization%2520in%2520dynamic%2520scenes%2520solely%2520supervised%250Aby%2520a%2520single%2520RGB%2520video.%2520Our%2520method%2520consists%2520of%2520three%2520key%2520components%253A%2520%25281%2529%250APatch-wise%2520Tracking%2520Filters%252C%2520to%2520establish%2520robust%2520and%2520maximally%2520sparse%250Ahinge-like%2520relations%2520across%2520the%2520RGB%2520video.%2520%25282%2529%2520Outlier-aware%2520Joint%250AOptimization%252C%2520for%2520efficient%2520camera%2520parameter%2520optimization%2520by%2520adaptive%250Adown-weighting%2520of%2520moving%2520outliers%252C%2520without%2520reliance%2520on%2520motion%2520priors.%2520%25283%2529%2520A%250ATwo-stage%2520Optimization%2520Strategy%252C%2520to%2520enhance%2520stability%2520and%2520optimization%2520speed%2520by%250Aa%2520trade-off%2520between%2520the%2520Softplus%2520limits%2520and%2520convex%2520minima%2520in%2520losses.%2520We%250Avisually%2520and%2520numerically%2520evaluate%2520our%2520camera%2520estimates.%2520To%2520further%2520validate%250Aaccuracy%252C%2520we%2520feed%2520the%2520camera%2520estimates%2520into%2520a%25204D%2520reconstruction%2520method%2520and%250Aassess%2520the%2520resulting%25203D%2520scenes%252C%2520and%2520rendered%25202D%2520RGB%2520and%2520depth%2520maps.%2520We%2520perform%250Aexperiments%2520on%25204%2520real-world%2520datasets%2520%2528NeRF-DS%252C%2520DAVIS%252C%2520iPhone%252C%2520and%2520TUM-dynamics%2529%250Aand%25201%2520synthetic%2520dataset%2520%2528MPI-Sintel%2529%252C%2520demonstrating%2520that%2520our%2520method%2520estimates%250Acamera%2520parameters%2520more%2520efficiently%2520and%2520accurately%2520with%2520a%2520single%2520RGB%2520video%2520as%250Athe%2520only%2520supervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Only%20Supervised%20Camera%20Parameter%20Optimization%20in%20Dynamic%20Scenes&entry.906535625=Fang%20Li%20and%20Hao%20Zhang%20and%20Narendra%20Ahuja&entry.1292438233=%20%20Although%20COLMAP%20has%20long%20remained%20the%20predominant%20method%20for%20camera%20parameter%0Aoptimization%20in%20static%20scenes%2C%20it%20is%20constrained%20by%20its%20lengthy%20runtime%20and%0Areliance%20on%20ground%20truth%20%28GT%29%20motion%20masks%20for%20application%20to%20dynamic%20scenes.%0AMany%20efforts%20attempted%20to%20improve%20it%20by%20incorporating%20more%20priors%20as%0Asupervision%20such%20as%20GT%20focal%20length%2C%20motion%20masks%2C%203D%20point%20clouds%2C%20camera%0Aposes%2C%20and%20metric%20depth%2C%20which%2C%20however%2C%20are%20typically%20unavailable%20in%20casually%0Acaptured%20RGB%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20more%20accurate%0Aand%20efficient%20camera%20parameter%20optimization%20in%20dynamic%20scenes%20solely%20supervised%0Aby%20a%20single%20RGB%20video.%20Our%20method%20consists%20of%20three%20key%20components%3A%20%281%29%0APatch-wise%20Tracking%20Filters%2C%20to%20establish%20robust%20and%20maximally%20sparse%0Ahinge-like%20relations%20across%20the%20RGB%20video.%20%282%29%20Outlier-aware%20Joint%0AOptimization%2C%20for%20efficient%20camera%20parameter%20optimization%20by%20adaptive%0Adown-weighting%20of%20moving%20outliers%2C%20without%20reliance%20on%20motion%20priors.%20%283%29%20A%0ATwo-stage%20Optimization%20Strategy%2C%20to%20enhance%20stability%20and%20optimization%20speed%20by%0Aa%20trade-off%20between%20the%20Softplus%20limits%20and%20convex%20minima%20in%20losses.%20We%0Avisually%20and%20numerically%20evaluate%20our%20camera%20estimates.%20To%20further%20validate%0Aaccuracy%2C%20we%20feed%20the%20camera%20estimates%20into%20a%204D%20reconstruction%20method%20and%0Aassess%20the%20resulting%203D%20scenes%2C%20and%20rendered%202D%20RGB%20and%20depth%20maps.%20We%20perform%0Aexperiments%20on%204%20real-world%20datasets%20%28NeRF-DS%2C%20DAVIS%2C%20iPhone%2C%20and%20TUM-dynamics%29%0Aand%201%20synthetic%20dataset%20%28MPI-Sintel%29%2C%20demonstrating%20that%20our%20method%20estimates%0Acamera%20parameters%20more%20efficiently%20and%20accurately%20with%20a%20single%20RGB%20video%20as%0Athe%20only%20supervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15123v1&entry.124074799=Read"},
{"title": "T-araVLN: Translator for Agricultural Robotic Agents on\n  Vision-and-Language Navigation", "author": "Xiaobei Zhao and Xingqi Lyu and Xiang Li", "abstract": "  Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, however, still heavily rely on manual operation or\nfixed railways for movement. To address this limitation, the AgriVLN method and\nthe A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to\nthe agricultural domain, enabling agents to navigate to the target positions\nfollowing the natural language instructions. AgriVLN effectively understands\nthe simple instructions, but often misunderstands the complex ones. To bridge\nthis gap, we propose the method of Translator for Agricultural Robotic Agents\non Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be more refined and\nprecise. When evaluated on the A2A benchmark, our T-araVLN effectively improves\nSuccess Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to\n2.28m, demonstrating the state-of-the-art performance in the agricultural\ndomain. Code: https://github.com/AlexTraveling/T-araVLN.\n", "link": "http://arxiv.org/abs/2509.06644v4", "date": "2025-09-18", "relevancy": 2.3976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation&body=Title%3A%20T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation%0AAuthor%3A%20Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Agricultural%20robotic%20agents%20have%20been%20becoming%20powerful%20helpers%20in%20a%20wide%0Arange%20of%20agricultural%20tasks%2C%20however%2C%20still%20heavily%20rely%20on%20manual%20operation%20or%0Afixed%20railways%20for%20movement.%20To%20address%20this%20limitation%2C%20the%20AgriVLN%20method%20and%0Athe%20A2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%0Athe%20agricultural%20domain%2C%20enabling%20agents%20to%20navigate%20to%20the%20target%20positions%0Afollowing%20the%20natural%20language%20instructions.%20AgriVLN%20effectively%20understands%0Athe%20simple%20instructions%2C%20but%20often%20misunderstands%20the%20complex%20ones.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20the%20method%20of%20Translator%20for%20Agricultural%20Robotic%20Agents%0Aon%20Vision-and-Language%20Navigation%20%28T-araVLN%29%2C%20in%20which%20the%20Instruction%0ATranslator%20module%20translates%20the%20original%20instruction%20to%20be%20more%20refined%20and%0Aprecise.%20When%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20T-araVLN%20effectively%20improves%0ASuccess%20Rate%20from%200.47%20to%200.63%20and%20reduces%20Navigation%20Error%20from%202.91m%20to%0A2.28m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%0Adomain.%20Code%3A%20https%3A//github.com/AlexTraveling/T-araVLN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06644v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-araVLN%253A%2520Translator%2520for%2520Agricultural%2520Robotic%2520Agents%2520on%250A%2520%2520Vision-and-Language%2520Navigation%26entry.906535625%3DXiaobei%2520Zhao%2520and%2520Xingqi%2520Lyu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Agricultural%2520robotic%2520agents%2520have%2520been%2520becoming%2520powerful%2520helpers%2520in%2520a%2520wide%250Arange%2520of%2520agricultural%2520tasks%252C%2520however%252C%2520still%2520heavily%2520rely%2520on%2520manual%2520operation%2520or%250Afixed%2520railways%2520for%2520movement.%2520To%2520address%2520this%2520limitation%252C%2520the%2520AgriVLN%2520method%2520and%250Athe%2520A2A%2520benchmark%2520pioneeringly%2520extend%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520to%250Athe%2520agricultural%2520domain%252C%2520enabling%2520agents%2520to%2520navigate%2520to%2520the%2520target%2520positions%250Afollowing%2520the%2520natural%2520language%2520instructions.%2520AgriVLN%2520effectively%2520understands%250Athe%2520simple%2520instructions%252C%2520but%2520often%2520misunderstands%2520the%2520complex%2520ones.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520propose%2520the%2520method%2520of%2520Translator%2520for%2520Agricultural%2520Robotic%2520Agents%250Aon%2520Vision-and-Language%2520Navigation%2520%2528T-araVLN%2529%252C%2520in%2520which%2520the%2520Instruction%250ATranslator%2520module%2520translates%2520the%2520original%2520instruction%2520to%2520be%2520more%2520refined%2520and%250Aprecise.%2520When%2520evaluated%2520on%2520the%2520A2A%2520benchmark%252C%2520our%2520T-araVLN%2520effectively%2520improves%250ASuccess%2520Rate%2520from%25200.47%2520to%25200.63%2520and%2520reduces%2520Navigation%2520Error%2520from%25202.91m%2520to%250A2.28m%252C%2520demonstrating%2520the%2520state-of-the-art%2520performance%2520in%2520the%2520agricultural%250Adomain.%2520Code%253A%2520https%253A//github.com/AlexTraveling/T-araVLN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06644v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-araVLN%3A%20Translator%20for%20Agricultural%20Robotic%20Agents%20on%0A%20%20Vision-and-Language%20Navigation&entry.906535625=Xiaobei%20Zhao%20and%20Xingqi%20Lyu%20and%20Xiang%20Li&entry.1292438233=%20%20Agricultural%20robotic%20agents%20have%20been%20becoming%20powerful%20helpers%20in%20a%20wide%0Arange%20of%20agricultural%20tasks%2C%20however%2C%20still%20heavily%20rely%20on%20manual%20operation%20or%0Afixed%20railways%20for%20movement.%20To%20address%20this%20limitation%2C%20the%20AgriVLN%20method%20and%0Athe%20A2A%20benchmark%20pioneeringly%20extend%20Vision-and-Language%20Navigation%20%28VLN%29%20to%0Athe%20agricultural%20domain%2C%20enabling%20agents%20to%20navigate%20to%20the%20target%20positions%0Afollowing%20the%20natural%20language%20instructions.%20AgriVLN%20effectively%20understands%0Athe%20simple%20instructions%2C%20but%20often%20misunderstands%20the%20complex%20ones.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20the%20method%20of%20Translator%20for%20Agricultural%20Robotic%20Agents%0Aon%20Vision-and-Language%20Navigation%20%28T-araVLN%29%2C%20in%20which%20the%20Instruction%0ATranslator%20module%20translates%20the%20original%20instruction%20to%20be%20more%20refined%20and%0Aprecise.%20When%20evaluated%20on%20the%20A2A%20benchmark%2C%20our%20T-araVLN%20effectively%20improves%0ASuccess%20Rate%20from%200.47%20to%200.63%20and%20reduces%20Navigation%20Error%20from%202.91m%20to%0A2.28m%2C%20demonstrating%20the%20state-of-the-art%20performance%20in%20the%20agricultural%0Adomain.%20Code%3A%20https%3A//github.com/AlexTraveling/T-araVLN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06644v4&entry.124074799=Read"},
{"title": "A Race Bias Free Face Aging Model for Reliable Kinship Verification", "author": "Ali Nazari and Bardiya Kariminia and Mohsen Ebrahimi Moghaddam", "abstract": "  The age gap in kinship verification addresses the time difference between the\nphotos of the parent and the child. Moreover, their same-age photos are often\nunavailable, and face aging models are racially biased, which impacts the\nlikeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,\nconsisting of two new modules, RACEpSp and a feature mixer, to produce racially\nunbiased images. The unbiased synthesized photos are used in kinship\nverification to investigate the results of verifying same-age parent-child\nimages. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an\naverage of 13.14\\% across all age groups, and CUSP-GAN in the 60+ age group by\n9.1\\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'\nidentities better than SAM-GAN and CUSP-GAN across all age groups.\nAdditionally, we demonstrate that transforming parent and child images from the\nKinFaceW-I and KinFaceW-II datasets to the same age can enhance the\nverification accuracy across all age groups. The accuracy increases with our\nRA-GAN for the kinship relationships of father-son and father-daughter,\nmother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,\nrespectively, on KinFaceW-I. Additionally, the accuracy for the relationships\nof father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on\nKinFaceW-II, respectively. The code is available\nat~\\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}\n", "link": "http://arxiv.org/abs/2509.15177v1", "date": "2025-09-18", "relevancy": 2.3942, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5015}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4699}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Race%20Bias%20Free%20Face%20Aging%20Model%20for%20Reliable%20Kinship%20Verification&body=Title%3A%20A%20Race%20Bias%20Free%20Face%20Aging%20Model%20for%20Reliable%20Kinship%20Verification%0AAuthor%3A%20Ali%20Nazari%20and%20Bardiya%20Kariminia%20and%20Mohsen%20Ebrahimi%20Moghaddam%0AAbstract%3A%20%20%20The%20age%20gap%20in%20kinship%20verification%20addresses%20the%20time%20difference%20between%20the%0Aphotos%20of%20the%20parent%20and%20the%20child.%20Moreover%2C%20their%20same-age%20photos%20are%20often%0Aunavailable%2C%20and%20face%20aging%20models%20are%20racially%20biased%2C%20which%20impacts%20the%0Alikeness%20of%20photos.%20Therefore%2C%20we%20propose%20a%20face%20aging%20GAN%20model%2C%20RA-GAN%2C%0Aconsisting%20of%20two%20new%20modules%2C%20RACEpSp%20and%20a%20feature%20mixer%2C%20to%20produce%20racially%0Aunbiased%20images.%20The%20unbiased%20synthesized%20photos%20are%20used%20in%20kinship%0Averification%20to%20investigate%20the%20results%20of%20verifying%20same-age%20parent-child%0Aimages.%20The%20experiments%20demonstrate%20that%20our%20RA-GAN%20outperforms%20SAM-GAN%20on%20an%0Aaverage%20of%2013.14%5C%25%20across%20all%20age%20groups%2C%20and%20CUSP-GAN%20in%20the%2060%2B%20age%20group%20by%0A9.1%5C%25%20in%20terms%20of%20racial%20accuracy.%20Moreover%2C%20RA-GAN%20can%20preserve%20subjects%27%0Aidentities%20better%20than%20SAM-GAN%20and%20CUSP-GAN%20across%20all%20age%20groups.%0AAdditionally%2C%20we%20demonstrate%20that%20transforming%20parent%20and%20child%20images%20from%20the%0AKinFaceW-I%20and%20KinFaceW-II%20datasets%20to%20the%20same%20age%20can%20enhance%20the%0Averification%20accuracy%20across%20all%20age%20groups.%20The%20accuracy%20increases%20with%20our%0ARA-GAN%20for%20the%20kinship%20relationships%20of%20father-son%20and%20father-daughter%2C%0Amother-son%2C%20and%20mother-daughter%2C%20which%20are%205.22%2C%205.12%2C%201.63%2C%20and%200.41%2C%0Arespectively%2C%20on%20KinFaceW-I.%20Additionally%2C%20the%20accuracy%20for%20the%20relationships%0Aof%20father-daughter%2C%20father-son%2C%20and%20mother-son%20is%202.9%2C%200.39%2C%20and%201.6%20on%0AKinFaceW-II%2C%20respectively.%20The%20code%20is%20available%0Aat~%5Chref%7Bhttps%3A//github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification%7D%7BGithub%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Race%2520Bias%2520Free%2520Face%2520Aging%2520Model%2520for%2520Reliable%2520Kinship%2520Verification%26entry.906535625%3DAli%2520Nazari%2520and%2520Bardiya%2520Kariminia%2520and%2520Mohsen%2520Ebrahimi%2520Moghaddam%26entry.1292438233%3D%2520%2520The%2520age%2520gap%2520in%2520kinship%2520verification%2520addresses%2520the%2520time%2520difference%2520between%2520the%250Aphotos%2520of%2520the%2520parent%2520and%2520the%2520child.%2520Moreover%252C%2520their%2520same-age%2520photos%2520are%2520often%250Aunavailable%252C%2520and%2520face%2520aging%2520models%2520are%2520racially%2520biased%252C%2520which%2520impacts%2520the%250Alikeness%2520of%2520photos.%2520Therefore%252C%2520we%2520propose%2520a%2520face%2520aging%2520GAN%2520model%252C%2520RA-GAN%252C%250Aconsisting%2520of%2520two%2520new%2520modules%252C%2520RACEpSp%2520and%2520a%2520feature%2520mixer%252C%2520to%2520produce%2520racially%250Aunbiased%2520images.%2520The%2520unbiased%2520synthesized%2520photos%2520are%2520used%2520in%2520kinship%250Averification%2520to%2520investigate%2520the%2520results%2520of%2520verifying%2520same-age%2520parent-child%250Aimages.%2520The%2520experiments%2520demonstrate%2520that%2520our%2520RA-GAN%2520outperforms%2520SAM-GAN%2520on%2520an%250Aaverage%2520of%252013.14%255C%2525%2520across%2520all%2520age%2520groups%252C%2520and%2520CUSP-GAN%2520in%2520the%252060%252B%2520age%2520group%2520by%250A9.1%255C%2525%2520in%2520terms%2520of%2520racial%2520accuracy.%2520Moreover%252C%2520RA-GAN%2520can%2520preserve%2520subjects%2527%250Aidentities%2520better%2520than%2520SAM-GAN%2520and%2520CUSP-GAN%2520across%2520all%2520age%2520groups.%250AAdditionally%252C%2520we%2520demonstrate%2520that%2520transforming%2520parent%2520and%2520child%2520images%2520from%2520the%250AKinFaceW-I%2520and%2520KinFaceW-II%2520datasets%2520to%2520the%2520same%2520age%2520can%2520enhance%2520the%250Averification%2520accuracy%2520across%2520all%2520age%2520groups.%2520The%2520accuracy%2520increases%2520with%2520our%250ARA-GAN%2520for%2520the%2520kinship%2520relationships%2520of%2520father-son%2520and%2520father-daughter%252C%250Amother-son%252C%2520and%2520mother-daughter%252C%2520which%2520are%25205.22%252C%25205.12%252C%25201.63%252C%2520and%25200.41%252C%250Arespectively%252C%2520on%2520KinFaceW-I.%2520Additionally%252C%2520the%2520accuracy%2520for%2520the%2520relationships%250Aof%2520father-daughter%252C%2520father-son%252C%2520and%2520mother-son%2520is%25202.9%252C%25200.39%252C%2520and%25201.6%2520on%250AKinFaceW-II%252C%2520respectively.%2520The%2520code%2520is%2520available%250Aat~%255Chref%257Bhttps%253A//github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification%257D%257BGithub%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Race%20Bias%20Free%20Face%20Aging%20Model%20for%20Reliable%20Kinship%20Verification&entry.906535625=Ali%20Nazari%20and%20Bardiya%20Kariminia%20and%20Mohsen%20Ebrahimi%20Moghaddam&entry.1292438233=%20%20The%20age%20gap%20in%20kinship%20verification%20addresses%20the%20time%20difference%20between%20the%0Aphotos%20of%20the%20parent%20and%20the%20child.%20Moreover%2C%20their%20same-age%20photos%20are%20often%0Aunavailable%2C%20and%20face%20aging%20models%20are%20racially%20biased%2C%20which%20impacts%20the%0Alikeness%20of%20photos.%20Therefore%2C%20we%20propose%20a%20face%20aging%20GAN%20model%2C%20RA-GAN%2C%0Aconsisting%20of%20two%20new%20modules%2C%20RACEpSp%20and%20a%20feature%20mixer%2C%20to%20produce%20racially%0Aunbiased%20images.%20The%20unbiased%20synthesized%20photos%20are%20used%20in%20kinship%0Averification%20to%20investigate%20the%20results%20of%20verifying%20same-age%20parent-child%0Aimages.%20The%20experiments%20demonstrate%20that%20our%20RA-GAN%20outperforms%20SAM-GAN%20on%20an%0Aaverage%20of%2013.14%5C%25%20across%20all%20age%20groups%2C%20and%20CUSP-GAN%20in%20the%2060%2B%20age%20group%20by%0A9.1%5C%25%20in%20terms%20of%20racial%20accuracy.%20Moreover%2C%20RA-GAN%20can%20preserve%20subjects%27%0Aidentities%20better%20than%20SAM-GAN%20and%20CUSP-GAN%20across%20all%20age%20groups.%0AAdditionally%2C%20we%20demonstrate%20that%20transforming%20parent%20and%20child%20images%20from%20the%0AKinFaceW-I%20and%20KinFaceW-II%20datasets%20to%20the%20same%20age%20can%20enhance%20the%0Averification%20accuracy%20across%20all%20age%20groups.%20The%20accuracy%20increases%20with%20our%0ARA-GAN%20for%20the%20kinship%20relationships%20of%20father-son%20and%20father-daughter%2C%0Amother-son%2C%20and%20mother-daughter%2C%20which%20are%205.22%2C%205.12%2C%201.63%2C%20and%200.41%2C%0Arespectively%2C%20on%20KinFaceW-I.%20Additionally%2C%20the%20accuracy%20for%20the%20relationships%0Aof%20father-daughter%2C%20father-son%2C%20and%20mother-son%20is%202.9%2C%200.39%2C%20and%201.6%20on%0AKinFaceW-II%2C%20respectively.%20The%20code%20is%20available%0Aat~%5Chref%7Bhttps%3A//github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification%7D%7BGithub%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15177v1&entry.124074799=Read"},
{"title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for\n  Hyperspectral Image Clustering", "author": "Jianhan Qi and Yuheng Jia and Hui Liu and Junhui Hou", "abstract": "  Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.\n", "link": "http://arxiv.org/abs/2506.09920v2", "date": "2025-09-18", "relevancy": 2.3935, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.48}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering&body=Title%3A%20Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering%0AAuthor%3A%20Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20clustering%20assigns%20similar%20pixels%20to%20the%20same%20class%0Awithout%20any%20annotations%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%0Alarge-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%0Asuperpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Aexisting%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%0Aand%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%0Adifferent%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%0A%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%0Arepresentation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%0Afeatures.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%0Amodule%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%0Atopological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%0Aframework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%0Aare%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%0Aimproves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%0Acompared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jhqi/SSGCO-EGAEL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural-Spectral%2520Graph%2520Convolution%2520with%2520Evidential%2520Edge%2520Learning%2520for%250A%2520%2520Hyperspectral%2520Image%2520Clustering%26entry.906535625%3DJianhan%2520Qi%2520and%2520Yuheng%2520Jia%2520and%2520Hui%2520Liu%2520and%2520Junhui%2520Hou%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520clustering%2520assigns%2520similar%2520pixels%2520to%2520the%2520same%2520class%250Awithout%2520any%2520annotations%252C%2520which%2520is%2520an%2520important%2520yet%2520challenging%2520task.%2520For%250Alarge-scale%2520HSIs%252C%2520most%2520methods%2520rely%2520on%2520superpixel%2520segmentation%2520and%2520perform%250Asuperpixel-level%2520clustering%2520based%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520However%252C%250Aexisting%2520GNNs%2520cannot%2520fully%2520exploit%2520the%2520spectral%2520information%2520of%2520the%2520input%2520HSI%252C%250Aand%2520the%2520inaccurate%2520superpixel%2520topological%2520graph%2520may%2520lead%2520to%2520the%2520confusion%2520of%250Adifferent%2520class%2520semantics%2520during%2520information%2520aggregation.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520first%2520propose%2520a%2520structural-spectral%2520graph%2520convolutional%2520operator%250A%2528SSGCO%2529%2520tailored%2520for%2520graph-structured%2520HSI%2520superpixels%2520to%2520improve%2520their%250Arepresentation%2520quality%2520through%2520the%2520co-extraction%2520of%2520spatial%2520and%2520spectral%250Afeatures.%2520Second%252C%2520we%2520propose%2520an%2520evidence-guided%2520adaptive%2520edge%2520learning%2520%2528EGAEL%2529%250Amodule%2520that%2520adaptively%2520predicts%2520and%2520refines%2520edge%2520weights%2520in%2520the%2520superpixel%250Atopological%2520graph.%2520We%2520integrate%2520the%2520proposed%2520method%2520into%2520a%2520contrastive%2520learning%250Aframework%2520to%2520achieve%2520clustering%252C%2520where%2520representation%2520learning%2520and%2520clustering%250Aare%2520simultaneously%2520conducted.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aimproves%2520clustering%2520accuracy%2520by%25202.61%2525%252C%25206.06%2525%252C%25204.96%2525%2520and%25203.15%2525%2520over%2520the%2520best%250Acompared%2520methods%2520on%2520four%2520HSI%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jhqi/SSGCO-EGAEL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural-Spectral%20Graph%20Convolution%20with%20Evidential%20Edge%20Learning%20for%0A%20%20Hyperspectral%20Image%20Clustering&entry.906535625=Jianhan%20Qi%20and%20Yuheng%20Jia%20and%20Hui%20Liu%20and%20Junhui%20Hou&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20clustering%20assigns%20similar%20pixels%20to%20the%20same%20class%0Awithout%20any%20annotations%2C%20which%20is%20an%20important%20yet%20challenging%20task.%20For%0Alarge-scale%20HSIs%2C%20most%20methods%20rely%20on%20superpixel%20segmentation%20and%20perform%0Asuperpixel-level%20clustering%20based%20on%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Aexisting%20GNNs%20cannot%20fully%20exploit%20the%20spectral%20information%20of%20the%20input%20HSI%2C%0Aand%20the%20inaccurate%20superpixel%20topological%20graph%20may%20lead%20to%20the%20confusion%20of%0Adifferent%20class%20semantics%20during%20information%20aggregation.%20To%20address%20these%0Achallenges%2C%20we%20first%20propose%20a%20structural-spectral%20graph%20convolutional%20operator%0A%28SSGCO%29%20tailored%20for%20graph-structured%20HSI%20superpixels%20to%20improve%20their%0Arepresentation%20quality%20through%20the%20co-extraction%20of%20spatial%20and%20spectral%0Afeatures.%20Second%2C%20we%20propose%20an%20evidence-guided%20adaptive%20edge%20learning%20%28EGAEL%29%0Amodule%20that%20adaptively%20predicts%20and%20refines%20edge%20weights%20in%20the%20superpixel%0Atopological%20graph.%20We%20integrate%20the%20proposed%20method%20into%20a%20contrastive%20learning%0Aframework%20to%20achieve%20clustering%2C%20where%20representation%20learning%20and%20clustering%0Aare%20simultaneously%20conducted.%20Experiments%20demonstrate%20that%20the%20proposed%20method%0Aimproves%20clustering%20accuracy%20by%202.61%25%2C%206.06%25%2C%204.96%25%20and%203.15%25%20over%20the%20best%0Acompared%20methods%20on%20four%20HSI%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jhqi/SSGCO-EGAEL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09920v2&entry.124074799=Read"},
{"title": "Hierarchical Federated Learning for Social Network with Mobility", "author": "Zeyu Chen and Wen Chen and Jun Li and Qingqing Wu and Ming Ding and Xuefeng Han and Xiumei Deng and Liwei Wang", "abstract": "  Federated Learning (FL) offers a decentralized solution that allows\ncollaborative local model training and global aggregation, thereby protecting\ndata privacy. In conventional FL frameworks, data privacy is typically\npreserved under the assumption that local data remains absolutely private,\nwhereas the mobility of clients is frequently neglected in explicit modeling.\nIn this paper, we propose a hierarchical federated learning framework based on\nthe social network with mobility namely HFL-SNM that considers both data\nsharing among clients and their mobility patterns. Under the constraints of\nlimited resources, we formulate a joint optimization problem of resource\nallocation and client scheduling, which objective is to minimize the energy\nconsumption of clients during the FL process. In social network, we introduce\nthe concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.\nWe analyze the impact of effective data and redundant data on the model\nperformance through preliminary experiments. We decouple the optimization\nproblem into multiple sub-problems, analyze them based on preliminary\nexperimental results, and propose Dynamic Optimization in Social Network with\nMobility (DO-SNM) algorithm. Experimental results demonstrate that our\nalgorithm achieves superior model performance while significantly reducing\nenergy consumption, compared to traditional baseline algorithms.\n", "link": "http://arxiv.org/abs/2509.14938v1", "date": "2025-09-18", "relevancy": 2.3929, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4815}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4775}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility&body=Title%3A%20Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility%0AAuthor%3A%20Zeyu%20Chen%20and%20Wen%20Chen%20and%20Jun%20Li%20and%20Qingqing%20Wu%20and%20Ming%20Ding%20and%20Xuefeng%20Han%20and%20Xiumei%20Deng%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20solution%20that%20allows%0Acollaborative%20local%20model%20training%20and%20global%20aggregation%2C%20thereby%20protecting%0Adata%20privacy.%20In%20conventional%20FL%20frameworks%2C%20data%20privacy%20is%20typically%0Apreserved%20under%20the%20assumption%20that%20local%20data%20remains%20absolutely%20private%2C%0Awhereas%20the%20mobility%20of%20clients%20is%20frequently%20neglected%20in%20explicit%20modeling.%0AIn%20this%20paper%2C%20we%20propose%20a%20hierarchical%20federated%20learning%20framework%20based%20on%0Athe%20social%20network%20with%20mobility%20namely%20HFL-SNM%20that%20considers%20both%20data%0Asharing%20among%20clients%20and%20their%20mobility%20patterns.%20Under%20the%20constraints%20of%0Alimited%20resources%2C%20we%20formulate%20a%20joint%20optimization%20problem%20of%20resource%0Aallocation%20and%20client%20scheduling%2C%20which%20objective%20is%20to%20minimize%20the%20energy%0Aconsumption%20of%20clients%20during%20the%20FL%20process.%20In%20social%20network%2C%20we%20introduce%0Athe%20concepts%20of%20Effective%20Data%20Coverage%20Rate%20and%20Redundant%20Data%20Coverage%20Rate.%0AWe%20analyze%20the%20impact%20of%20effective%20data%20and%20redundant%20data%20on%20the%20model%0Aperformance%20through%20preliminary%20experiments.%20We%20decouple%20the%20optimization%0Aproblem%20into%20multiple%20sub-problems%2C%20analyze%20them%20based%20on%20preliminary%0Aexperimental%20results%2C%20and%20propose%20Dynamic%20Optimization%20in%20Social%20Network%20with%0AMobility%20%28DO-SNM%29%20algorithm.%20Experimental%20results%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20model%20performance%20while%20significantly%20reducing%0Aenergy%20consumption%2C%20compared%20to%20traditional%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Federated%2520Learning%2520for%2520Social%2520Network%2520with%2520Mobility%26entry.906535625%3DZeyu%2520Chen%2520and%2520Wen%2520Chen%2520and%2520Jun%2520Li%2520and%2520Qingqing%2520Wu%2520and%2520Ming%2520Ding%2520and%2520Xuefeng%2520Han%2520and%2520Xiumei%2520Deng%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520offers%2520a%2520decentralized%2520solution%2520that%2520allows%250Acollaborative%2520local%2520model%2520training%2520and%2520global%2520aggregation%252C%2520thereby%2520protecting%250Adata%2520privacy.%2520In%2520conventional%2520FL%2520frameworks%252C%2520data%2520privacy%2520is%2520typically%250Apreserved%2520under%2520the%2520assumption%2520that%2520local%2520data%2520remains%2520absolutely%2520private%252C%250Awhereas%2520the%2520mobility%2520of%2520clients%2520is%2520frequently%2520neglected%2520in%2520explicit%2520modeling.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520hierarchical%2520federated%2520learning%2520framework%2520based%2520on%250Athe%2520social%2520network%2520with%2520mobility%2520namely%2520HFL-SNM%2520that%2520considers%2520both%2520data%250Asharing%2520among%2520clients%2520and%2520their%2520mobility%2520patterns.%2520Under%2520the%2520constraints%2520of%250Alimited%2520resources%252C%2520we%2520formulate%2520a%2520joint%2520optimization%2520problem%2520of%2520resource%250Aallocation%2520and%2520client%2520scheduling%252C%2520which%2520objective%2520is%2520to%2520minimize%2520the%2520energy%250Aconsumption%2520of%2520clients%2520during%2520the%2520FL%2520process.%2520In%2520social%2520network%252C%2520we%2520introduce%250Athe%2520concepts%2520of%2520Effective%2520Data%2520Coverage%2520Rate%2520and%2520Redundant%2520Data%2520Coverage%2520Rate.%250AWe%2520analyze%2520the%2520impact%2520of%2520effective%2520data%2520and%2520redundant%2520data%2520on%2520the%2520model%250Aperformance%2520through%2520preliminary%2520experiments.%2520We%2520decouple%2520the%2520optimization%250Aproblem%2520into%2520multiple%2520sub-problems%252C%2520analyze%2520them%2520based%2520on%2520preliminary%250Aexperimental%2520results%252C%2520and%2520propose%2520Dynamic%2520Optimization%2520in%2520Social%2520Network%2520with%250AMobility%2520%2528DO-SNM%2529%2520algorithm.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aalgorithm%2520achieves%2520superior%2520model%2520performance%2520while%2520significantly%2520reducing%250Aenergy%2520consumption%252C%2520compared%2520to%2520traditional%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Federated%20Learning%20for%20Social%20Network%20with%20Mobility&entry.906535625=Zeyu%20Chen%20and%20Wen%20Chen%20and%20Jun%20Li%20and%20Qingqing%20Wu%20and%20Ming%20Ding%20and%20Xuefeng%20Han%20and%20Xiumei%20Deng%20and%20Liwei%20Wang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20offers%20a%20decentralized%20solution%20that%20allows%0Acollaborative%20local%20model%20training%20and%20global%20aggregation%2C%20thereby%20protecting%0Adata%20privacy.%20In%20conventional%20FL%20frameworks%2C%20data%20privacy%20is%20typically%0Apreserved%20under%20the%20assumption%20that%20local%20data%20remains%20absolutely%20private%2C%0Awhereas%20the%20mobility%20of%20clients%20is%20frequently%20neglected%20in%20explicit%20modeling.%0AIn%20this%20paper%2C%20we%20propose%20a%20hierarchical%20federated%20learning%20framework%20based%20on%0Athe%20social%20network%20with%20mobility%20namely%20HFL-SNM%20that%20considers%20both%20data%0Asharing%20among%20clients%20and%20their%20mobility%20patterns.%20Under%20the%20constraints%20of%0Alimited%20resources%2C%20we%20formulate%20a%20joint%20optimization%20problem%20of%20resource%0Aallocation%20and%20client%20scheduling%2C%20which%20objective%20is%20to%20minimize%20the%20energy%0Aconsumption%20of%20clients%20during%20the%20FL%20process.%20In%20social%20network%2C%20we%20introduce%0Athe%20concepts%20of%20Effective%20Data%20Coverage%20Rate%20and%20Redundant%20Data%20Coverage%20Rate.%0AWe%20analyze%20the%20impact%20of%20effective%20data%20and%20redundant%20data%20on%20the%20model%0Aperformance%20through%20preliminary%20experiments.%20We%20decouple%20the%20optimization%0Aproblem%20into%20multiple%20sub-problems%2C%20analyze%20them%20based%20on%20preliminary%0Aexperimental%20results%2C%20and%20propose%20Dynamic%20Optimization%20in%20Social%20Network%20with%0AMobility%20%28DO-SNM%29%20algorithm.%20Experimental%20results%20demonstrate%20that%20our%0Aalgorithm%20achieves%20superior%20model%20performance%20while%20significantly%20reducing%0Aenergy%20consumption%2C%20compared%20to%20traditional%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14938v1&entry.124074799=Read"},
{"title": "Credit Card Fraud Detection", "author": "Iva Popova and Hamza A. A. Gardi", "abstract": "  Credit card fraud remains a significant challenge due to class imbalance and\nfraudsters mimicking legitimate behavior. This study evaluates five machine\nlearning models - Logistic Regression, Random Forest, XGBoost, K-Nearest\nNeighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using\nundersampling, SMOTE, and a hybrid approach. Our models are evaluated on the\noriginal imbalanced test set to better reflect real-world performance. Results\nshow that the hybrid method achieves the best balance between recall and\nprecision, especially improving MLP and KNN performance.\n", "link": "http://arxiv.org/abs/2509.15044v1", "date": "2025-09-18", "relevancy": 2.3874, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5149}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4725}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Credit%20Card%20Fraud%20Detection&body=Title%3A%20Credit%20Card%20Fraud%20Detection%0AAuthor%3A%20Iva%20Popova%20and%20Hamza%20A.%20A.%20Gardi%0AAbstract%3A%20%20%20Credit%20card%20fraud%20remains%20a%20significant%20challenge%20due%20to%20class%20imbalance%20and%0Afraudsters%20mimicking%20legitimate%20behavior.%20This%20study%20evaluates%20five%20machine%0Alearning%20models%20-%20Logistic%20Regression%2C%20Random%20Forest%2C%20XGBoost%2C%20K-Nearest%0ANeighbors%20%28KNN%29%2C%20and%20Multi-Layer%20Perceptron%20%28MLP%29%20on%20a%20real-world%20dataset%20using%0Aundersampling%2C%20SMOTE%2C%20and%20a%20hybrid%20approach.%20Our%20models%20are%20evaluated%20on%20the%0Aoriginal%20imbalanced%20test%20set%20to%20better%20reflect%20real-world%20performance.%20Results%0Ashow%20that%20the%20hybrid%20method%20achieves%20the%20best%20balance%20between%20recall%20and%0Aprecision%2C%20especially%20improving%20MLP%20and%20KNN%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCredit%2520Card%2520Fraud%2520Detection%26entry.906535625%3DIva%2520Popova%2520and%2520Hamza%2520A.%2520A.%2520Gardi%26entry.1292438233%3D%2520%2520Credit%2520card%2520fraud%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520class%2520imbalance%2520and%250Afraudsters%2520mimicking%2520legitimate%2520behavior.%2520This%2520study%2520evaluates%2520five%2520machine%250Alearning%2520models%2520-%2520Logistic%2520Regression%252C%2520Random%2520Forest%252C%2520XGBoost%252C%2520K-Nearest%250ANeighbors%2520%2528KNN%2529%252C%2520and%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520on%2520a%2520real-world%2520dataset%2520using%250Aundersampling%252C%2520SMOTE%252C%2520and%2520a%2520hybrid%2520approach.%2520Our%2520models%2520are%2520evaluated%2520on%2520the%250Aoriginal%2520imbalanced%2520test%2520set%2520to%2520better%2520reflect%2520real-world%2520performance.%2520Results%250Ashow%2520that%2520the%2520hybrid%2520method%2520achieves%2520the%2520best%2520balance%2520between%2520recall%2520and%250Aprecision%252C%2520especially%2520improving%2520MLP%2520and%2520KNN%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Credit%20Card%20Fraud%20Detection&entry.906535625=Iva%20Popova%20and%20Hamza%20A.%20A.%20Gardi&entry.1292438233=%20%20Credit%20card%20fraud%20remains%20a%20significant%20challenge%20due%20to%20class%20imbalance%20and%0Afraudsters%20mimicking%20legitimate%20behavior.%20This%20study%20evaluates%20five%20machine%0Alearning%20models%20-%20Logistic%20Regression%2C%20Random%20Forest%2C%20XGBoost%2C%20K-Nearest%0ANeighbors%20%28KNN%29%2C%20and%20Multi-Layer%20Perceptron%20%28MLP%29%20on%20a%20real-world%20dataset%20using%0Aundersampling%2C%20SMOTE%2C%20and%20a%20hybrid%20approach.%20Our%20models%20are%20evaluated%20on%20the%0Aoriginal%20imbalanced%20test%20set%20to%20better%20reflect%20real-world%20performance.%20Results%0Ashow%20that%20the%20hybrid%20method%20achieves%20the%20best%20balance%20between%20recall%20and%0Aprecision%2C%20especially%20improving%20MLP%20and%20KNN%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15044v1&entry.124074799=Read"},
{"title": "Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland", "author": "Luca Rolshoven and Vishvaksenan Rasiah and Srinanda Br\u00fcgger Bose and Sarah Hostettler and Lara Burkhalter and Matthias St\u00fcrmer and Joel Niklaus", "abstract": "  Legal research depends on headnotes: concise summaries that help lawyers\nquickly identify relevant cases. Yet, many court decisions lack them due to the\nhigh cost of manual annotation. To address this gap, we introduce the Swiss\nLandmark Decisions Summarization (SLDS) dataset containing 20K rulings from the\nSwiss Federal Supreme Court, each with headnotes in German, French, and\nItalian. SLDS has the potential to significantly improve access to legal\ninformation and transform legal research in Switzerland. We fine-tune open\nmodels (Qwen2.5, Llama 3.2, Phi-3.5) and compare them to larger general-purpose\nand reasoning-tuned LLMs, including GPT-4o, Claude 3.5 Sonnet, and the\nopen-source DeepSeek R1. Using an LLM-as-a-Judge framework, we find that\nfine-tuned models perform well in terms of lexical similarity, while larger\nmodels generate more legally accurate and coherent summaries. Interestingly,\nreasoning-focused models show no consistent benefit, suggesting that factual\nprecision is more important than deep reasoning in this task. We release SLDS\nunder a CC BY 4.0 license to support future research in cross-lingual legal\nsummarization.\n", "link": "http://arxiv.org/abs/2410.13456v3", "date": "2025-09-18", "relevancy": 2.3853, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4927}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4927}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Legal%20Knowledge%3A%20A%20Multilingual%20Dataset%20for%20Judicial%0A%20%20Summarization%20in%20Switzerland&body=Title%3A%20Unlocking%20Legal%20Knowledge%3A%20A%20Multilingual%20Dataset%20for%20Judicial%0A%20%20Summarization%20in%20Switzerland%0AAuthor%3A%20Luca%20Rolshoven%20and%20Vishvaksenan%20Rasiah%20and%20Srinanda%20Br%C3%BCgger%20Bose%20and%20Sarah%20Hostettler%20and%20Lara%20Burkhalter%20and%20Matthias%20St%C3%BCrmer%20and%20Joel%20Niklaus%0AAbstract%3A%20%20%20Legal%20research%20depends%20on%20headnotes%3A%20concise%20summaries%20that%20help%20lawyers%0Aquickly%20identify%20relevant%20cases.%20Yet%2C%20many%20court%20decisions%20lack%20them%20due%20to%20the%0Ahigh%20cost%20of%20manual%20annotation.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Swiss%0ALandmark%20Decisions%20Summarization%20%28SLDS%29%20dataset%20containing%2020K%20rulings%20from%20the%0ASwiss%20Federal%20Supreme%20Court%2C%20each%20with%20headnotes%20in%20German%2C%20French%2C%20and%0AItalian.%20SLDS%20has%20the%20potential%20to%20significantly%20improve%20access%20to%20legal%0Ainformation%20and%20transform%20legal%20research%20in%20Switzerland.%20We%20fine-tune%20open%0Amodels%20%28Qwen2.5%2C%20Llama%203.2%2C%20Phi-3.5%29%20and%20compare%20them%20to%20larger%20general-purpose%0Aand%20reasoning-tuned%20LLMs%2C%20including%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20and%20the%0Aopen-source%20DeepSeek%20R1.%20Using%20an%20LLM-as-a-Judge%20framework%2C%20we%20find%20that%0Afine-tuned%20models%20perform%20well%20in%20terms%20of%20lexical%20similarity%2C%20while%20larger%0Amodels%20generate%20more%20legally%20accurate%20and%20coherent%20summaries.%20Interestingly%2C%0Areasoning-focused%20models%20show%20no%20consistent%20benefit%2C%20suggesting%20that%20factual%0Aprecision%20is%20more%20important%20than%20deep%20reasoning%20in%20this%20task.%20We%20release%20SLDS%0Aunder%20a%20CC%20BY%204.0%20license%20to%20support%20future%20research%20in%20cross-lingual%20legal%0Asummarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13456v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Legal%2520Knowledge%253A%2520A%2520Multilingual%2520Dataset%2520for%2520Judicial%250A%2520%2520Summarization%2520in%2520Switzerland%26entry.906535625%3DLuca%2520Rolshoven%2520and%2520Vishvaksenan%2520Rasiah%2520and%2520Srinanda%2520Br%25C3%25BCgger%2520Bose%2520and%2520Sarah%2520Hostettler%2520and%2520Lara%2520Burkhalter%2520and%2520Matthias%2520St%25C3%25BCrmer%2520and%2520Joel%2520Niklaus%26entry.1292438233%3D%2520%2520Legal%2520research%2520depends%2520on%2520headnotes%253A%2520concise%2520summaries%2520that%2520help%2520lawyers%250Aquickly%2520identify%2520relevant%2520cases.%2520Yet%252C%2520many%2520court%2520decisions%2520lack%2520them%2520due%2520to%2520the%250Ahigh%2520cost%2520of%2520manual%2520annotation.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Swiss%250ALandmark%2520Decisions%2520Summarization%2520%2528SLDS%2529%2520dataset%2520containing%252020K%2520rulings%2520from%2520the%250ASwiss%2520Federal%2520Supreme%2520Court%252C%2520each%2520with%2520headnotes%2520in%2520German%252C%2520French%252C%2520and%250AItalian.%2520SLDS%2520has%2520the%2520potential%2520to%2520significantly%2520improve%2520access%2520to%2520legal%250Ainformation%2520and%2520transform%2520legal%2520research%2520in%2520Switzerland.%2520We%2520fine-tune%2520open%250Amodels%2520%2528Qwen2.5%252C%2520Llama%25203.2%252C%2520Phi-3.5%2529%2520and%2520compare%2520them%2520to%2520larger%2520general-purpose%250Aand%2520reasoning-tuned%2520LLMs%252C%2520including%2520GPT-4o%252C%2520Claude%25203.5%2520Sonnet%252C%2520and%2520the%250Aopen-source%2520DeepSeek%2520R1.%2520Using%2520an%2520LLM-as-a-Judge%2520framework%252C%2520we%2520find%2520that%250Afine-tuned%2520models%2520perform%2520well%2520in%2520terms%2520of%2520lexical%2520similarity%252C%2520while%2520larger%250Amodels%2520generate%2520more%2520legally%2520accurate%2520and%2520coherent%2520summaries.%2520Interestingly%252C%250Areasoning-focused%2520models%2520show%2520no%2520consistent%2520benefit%252C%2520suggesting%2520that%2520factual%250Aprecision%2520is%2520more%2520important%2520than%2520deep%2520reasoning%2520in%2520this%2520task.%2520We%2520release%2520SLDS%250Aunder%2520a%2520CC%2520BY%25204.0%2520license%2520to%2520support%2520future%2520research%2520in%2520cross-lingual%2520legal%250Asummarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13456v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Legal%20Knowledge%3A%20A%20Multilingual%20Dataset%20for%20Judicial%0A%20%20Summarization%20in%20Switzerland&entry.906535625=Luca%20Rolshoven%20and%20Vishvaksenan%20Rasiah%20and%20Srinanda%20Br%C3%BCgger%20Bose%20and%20Sarah%20Hostettler%20and%20Lara%20Burkhalter%20and%20Matthias%20St%C3%BCrmer%20and%20Joel%20Niklaus&entry.1292438233=%20%20Legal%20research%20depends%20on%20headnotes%3A%20concise%20summaries%20that%20help%20lawyers%0Aquickly%20identify%20relevant%20cases.%20Yet%2C%20many%20court%20decisions%20lack%20them%20due%20to%20the%0Ahigh%20cost%20of%20manual%20annotation.%20To%20address%20this%20gap%2C%20we%20introduce%20the%20Swiss%0ALandmark%20Decisions%20Summarization%20%28SLDS%29%20dataset%20containing%2020K%20rulings%20from%20the%0ASwiss%20Federal%20Supreme%20Court%2C%20each%20with%20headnotes%20in%20German%2C%20French%2C%20and%0AItalian.%20SLDS%20has%20the%20potential%20to%20significantly%20improve%20access%20to%20legal%0Ainformation%20and%20transform%20legal%20research%20in%20Switzerland.%20We%20fine-tune%20open%0Amodels%20%28Qwen2.5%2C%20Llama%203.2%2C%20Phi-3.5%29%20and%20compare%20them%20to%20larger%20general-purpose%0Aand%20reasoning-tuned%20LLMs%2C%20including%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20and%20the%0Aopen-source%20DeepSeek%20R1.%20Using%20an%20LLM-as-a-Judge%20framework%2C%20we%20find%20that%0Afine-tuned%20models%20perform%20well%20in%20terms%20of%20lexical%20similarity%2C%20while%20larger%0Amodels%20generate%20more%20legally%20accurate%20and%20coherent%20summaries.%20Interestingly%2C%0Areasoning-focused%20models%20show%20no%20consistent%20benefit%2C%20suggesting%20that%20factual%0Aprecision%20is%20more%20important%20than%20deep%20reasoning%20in%20this%20task.%20We%20release%20SLDS%0Aunder%20a%20CC%20BY%204.0%20license%20to%20support%20future%20research%20in%20cross-lingual%20legal%0Asummarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13456v3&entry.124074799=Read"},
{"title": "PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in\n  Spherical Robots", "author": "Shenghai Yuan and Jason Wai Hao Yee and Weixiang Guo and Zhongyuan Liu and Thien-Minh Nguyen and Lihua Xie", "abstract": "  Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for\nnavigation and mapping, yet horizontally mounted LiDARs such as the MID360\ncapture few near-ground returns, limiting terrain awareness and degrading\nperformance in feature-scarce environments. Prior solutions - static tilt,\nactive rotation, or high-density sensors - either sacrifice horizontal\nperception or incur added actuators, cost, and power. We introduce PERAL, a\nperception-aware motion control framework for spherical robots that achieves\npassive LiDAR excitation without dedicated hardware. By modeling the coupling\nbetween internal differential-drive actuation and sensor attitude, PERAL\nsuperimposes bounded, non-periodic oscillations onto nominal goal- or\ntrajectory-tracking commands, enriching vertical scan diversity while\npreserving navigation accuracy. Implemented on a compact spherical robot, PERAL\nis validated across laboratory, corridor, and tactical environments.\nExperiments demonstrate up to 96 percent map completeness, a 27 percent\nreduction in trajectory tracking error, and robust near-ground human detection,\nall at lower weight, power, and cost compared with static tilt, active\nrotation, and fixed horizontal baselines. The design and code will be\nopen-sourced upon acceptance.\n", "link": "http://arxiv.org/abs/2509.14915v1", "date": "2025-09-18", "relevancy": 2.3727, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PERAL%3A%20Perception-Aware%20Motion%20Control%20for%20Passive%20LiDAR%20Excitation%20in%0A%20%20Spherical%20Robots&body=Title%3A%20PERAL%3A%20Perception-Aware%20Motion%20Control%20for%20Passive%20LiDAR%20Excitation%20in%0A%20%20Spherical%20Robots%0AAuthor%3A%20Shenghai%20Yuan%20and%20Jason%20Wai%20Hao%20Yee%20and%20Weixiang%20Guo%20and%20Zhongyuan%20Liu%20and%20Thien-Minh%20Nguyen%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Autonomous%20mobile%20robots%20increasingly%20rely%20on%20LiDAR-IMU%20odometry%20for%0Anavigation%20and%20mapping%2C%20yet%20horizontally%20mounted%20LiDARs%20such%20as%20the%20MID360%0Acapture%20few%20near-ground%20returns%2C%20limiting%20terrain%20awareness%20and%20degrading%0Aperformance%20in%20feature-scarce%20environments.%20Prior%20solutions%20-%20static%20tilt%2C%0Aactive%20rotation%2C%20or%20high-density%20sensors%20-%20either%20sacrifice%20horizontal%0Aperception%20or%20incur%20added%20actuators%2C%20cost%2C%20and%20power.%20We%20introduce%20PERAL%2C%20a%0Aperception-aware%20motion%20control%20framework%20for%20spherical%20robots%20that%20achieves%0Apassive%20LiDAR%20excitation%20without%20dedicated%20hardware.%20By%20modeling%20the%20coupling%0Abetween%20internal%20differential-drive%20actuation%20and%20sensor%20attitude%2C%20PERAL%0Asuperimposes%20bounded%2C%20non-periodic%20oscillations%20onto%20nominal%20goal-%20or%0Atrajectory-tracking%20commands%2C%20enriching%20vertical%20scan%20diversity%20while%0Apreserving%20navigation%20accuracy.%20Implemented%20on%20a%20compact%20spherical%20robot%2C%20PERAL%0Ais%20validated%20across%20laboratory%2C%20corridor%2C%20and%20tactical%20environments.%0AExperiments%20demonstrate%20up%20to%2096%20percent%20map%20completeness%2C%20a%2027%20percent%0Areduction%20in%20trajectory%20tracking%20error%2C%20and%20robust%20near-ground%20human%20detection%2C%0Aall%20at%20lower%20weight%2C%20power%2C%20and%20cost%20compared%20with%20static%20tilt%2C%20active%0Arotation%2C%20and%20fixed%20horizontal%20baselines.%20The%20design%20and%20code%20will%20be%0Aopen-sourced%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPERAL%253A%2520Perception-Aware%2520Motion%2520Control%2520for%2520Passive%2520LiDAR%2520Excitation%2520in%250A%2520%2520Spherical%2520Robots%26entry.906535625%3DShenghai%2520Yuan%2520and%2520Jason%2520Wai%2520Hao%2520Yee%2520and%2520Weixiang%2520Guo%2520and%2520Zhongyuan%2520Liu%2520and%2520Thien-Minh%2520Nguyen%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Autonomous%2520mobile%2520robots%2520increasingly%2520rely%2520on%2520LiDAR-IMU%2520odometry%2520for%250Anavigation%2520and%2520mapping%252C%2520yet%2520horizontally%2520mounted%2520LiDARs%2520such%2520as%2520the%2520MID360%250Acapture%2520few%2520near-ground%2520returns%252C%2520limiting%2520terrain%2520awareness%2520and%2520degrading%250Aperformance%2520in%2520feature-scarce%2520environments.%2520Prior%2520solutions%2520-%2520static%2520tilt%252C%250Aactive%2520rotation%252C%2520or%2520high-density%2520sensors%2520-%2520either%2520sacrifice%2520horizontal%250Aperception%2520or%2520incur%2520added%2520actuators%252C%2520cost%252C%2520and%2520power.%2520We%2520introduce%2520PERAL%252C%2520a%250Aperception-aware%2520motion%2520control%2520framework%2520for%2520spherical%2520robots%2520that%2520achieves%250Apassive%2520LiDAR%2520excitation%2520without%2520dedicated%2520hardware.%2520By%2520modeling%2520the%2520coupling%250Abetween%2520internal%2520differential-drive%2520actuation%2520and%2520sensor%2520attitude%252C%2520PERAL%250Asuperimposes%2520bounded%252C%2520non-periodic%2520oscillations%2520onto%2520nominal%2520goal-%2520or%250Atrajectory-tracking%2520commands%252C%2520enriching%2520vertical%2520scan%2520diversity%2520while%250Apreserving%2520navigation%2520accuracy.%2520Implemented%2520on%2520a%2520compact%2520spherical%2520robot%252C%2520PERAL%250Ais%2520validated%2520across%2520laboratory%252C%2520corridor%252C%2520and%2520tactical%2520environments.%250AExperiments%2520demonstrate%2520up%2520to%252096%2520percent%2520map%2520completeness%252C%2520a%252027%2520percent%250Areduction%2520in%2520trajectory%2520tracking%2520error%252C%2520and%2520robust%2520near-ground%2520human%2520detection%252C%250Aall%2520at%2520lower%2520weight%252C%2520power%252C%2520and%2520cost%2520compared%2520with%2520static%2520tilt%252C%2520active%250Arotation%252C%2520and%2520fixed%2520horizontal%2520baselines.%2520The%2520design%2520and%2520code%2520will%2520be%250Aopen-sourced%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PERAL%3A%20Perception-Aware%20Motion%20Control%20for%20Passive%20LiDAR%20Excitation%20in%0A%20%20Spherical%20Robots&entry.906535625=Shenghai%20Yuan%20and%20Jason%20Wai%20Hao%20Yee%20and%20Weixiang%20Guo%20and%20Zhongyuan%20Liu%20and%20Thien-Minh%20Nguyen%20and%20Lihua%20Xie&entry.1292438233=%20%20Autonomous%20mobile%20robots%20increasingly%20rely%20on%20LiDAR-IMU%20odometry%20for%0Anavigation%20and%20mapping%2C%20yet%20horizontally%20mounted%20LiDARs%20such%20as%20the%20MID360%0Acapture%20few%20near-ground%20returns%2C%20limiting%20terrain%20awareness%20and%20degrading%0Aperformance%20in%20feature-scarce%20environments.%20Prior%20solutions%20-%20static%20tilt%2C%0Aactive%20rotation%2C%20or%20high-density%20sensors%20-%20either%20sacrifice%20horizontal%0Aperception%20or%20incur%20added%20actuators%2C%20cost%2C%20and%20power.%20We%20introduce%20PERAL%2C%20a%0Aperception-aware%20motion%20control%20framework%20for%20spherical%20robots%20that%20achieves%0Apassive%20LiDAR%20excitation%20without%20dedicated%20hardware.%20By%20modeling%20the%20coupling%0Abetween%20internal%20differential-drive%20actuation%20and%20sensor%20attitude%2C%20PERAL%0Asuperimposes%20bounded%2C%20non-periodic%20oscillations%20onto%20nominal%20goal-%20or%0Atrajectory-tracking%20commands%2C%20enriching%20vertical%20scan%20diversity%20while%0Apreserving%20navigation%20accuracy.%20Implemented%20on%20a%20compact%20spherical%20robot%2C%20PERAL%0Ais%20validated%20across%20laboratory%2C%20corridor%2C%20and%20tactical%20environments.%0AExperiments%20demonstrate%20up%20to%2096%20percent%20map%20completeness%2C%20a%2027%20percent%0Areduction%20in%20trajectory%20tracking%20error%2C%20and%20robust%20near-ground%20human%20detection%2C%0Aall%20at%20lower%20weight%2C%20power%2C%20and%20cost%20compared%20with%20static%20tilt%2C%20active%0Arotation%2C%20and%20fixed%20horizontal%20baselines.%20The%20design%20and%20code%20will%20be%0Aopen-sourced%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14915v1&entry.124074799=Read"},
{"title": "Controllable Localized Face Anonymization Via Diffusion Inpainting", "author": "Ali Salar and Qing Liu and Guoying Zhao", "abstract": "  The growing use of portrait images in computer vision highlights the need to\nprotect personal identities. At the same time, anonymized images must remain\nuseful for downstream computer vision tasks. In this work, we propose a unified\nframework that leverages the inpainting ability of latent diffusion models to\ngenerate realistic anonymized images. Unlike prior approaches, we have complete\ncontrol over the anonymization process by designing an adaptive\nattribute-guidance module that applies gradient correction during the reverse\ndenoising process, aligning the facial attributes of the generated image with\nthose of the synthesized target image. Our framework also supports localized\nanonymization, allowing users to specify which facial regions are left\nunchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ\ndatasets show that our method outperforms state-of-the-art approaches while\nrequiring no additional model training. The source code is available on our\npage.\n", "link": "http://arxiv.org/abs/2509.14866v1", "date": "2025-09-18", "relevancy": 2.3644, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5926}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Localized%20Face%20Anonymization%20Via%20Diffusion%20Inpainting&body=Title%3A%20Controllable%20Localized%20Face%20Anonymization%20Via%20Diffusion%20Inpainting%0AAuthor%3A%20Ali%20Salar%20and%20Qing%20Liu%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20The%20growing%20use%20of%20portrait%20images%20in%20computer%20vision%20highlights%20the%20need%20to%0Aprotect%20personal%20identities.%20At%20the%20same%20time%2C%20anonymized%20images%20must%20remain%0Auseful%20for%20downstream%20computer%20vision%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20unified%0Aframework%20that%20leverages%20the%20inpainting%20ability%20of%20latent%20diffusion%20models%20to%0Agenerate%20realistic%20anonymized%20images.%20Unlike%20prior%20approaches%2C%20we%20have%20complete%0Acontrol%20over%20the%20anonymization%20process%20by%20designing%20an%20adaptive%0Aattribute-guidance%20module%20that%20applies%20gradient%20correction%20during%20the%20reverse%0Adenoising%20process%2C%20aligning%20the%20facial%20attributes%20of%20the%20generated%20image%20with%0Athose%20of%20the%20synthesized%20target%20image.%20Our%20framework%20also%20supports%20localized%0Aanonymization%2C%20allowing%20users%20to%20specify%20which%20facial%20regions%20are%20left%0Aunchanged.%20Extensive%20experiments%20conducted%20on%20the%20public%20CelebA-HQ%20and%20FFHQ%0Adatasets%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20while%0Arequiring%20no%20additional%20model%20training.%20The%20source%20code%20is%20available%20on%20our%0Apage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Localized%2520Face%2520Anonymization%2520Via%2520Diffusion%2520Inpainting%26entry.906535625%3DAli%2520Salar%2520and%2520Qing%2520Liu%2520and%2520Guoying%2520Zhao%26entry.1292438233%3D%2520%2520The%2520growing%2520use%2520of%2520portrait%2520images%2520in%2520computer%2520vision%2520highlights%2520the%2520need%2520to%250Aprotect%2520personal%2520identities.%2520At%2520the%2520same%2520time%252C%2520anonymized%2520images%2520must%2520remain%250Auseful%2520for%2520downstream%2520computer%2520vision%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520unified%250Aframework%2520that%2520leverages%2520the%2520inpainting%2520ability%2520of%2520latent%2520diffusion%2520models%2520to%250Agenerate%2520realistic%2520anonymized%2520images.%2520Unlike%2520prior%2520approaches%252C%2520we%2520have%2520complete%250Acontrol%2520over%2520the%2520anonymization%2520process%2520by%2520designing%2520an%2520adaptive%250Aattribute-guidance%2520module%2520that%2520applies%2520gradient%2520correction%2520during%2520the%2520reverse%250Adenoising%2520process%252C%2520aligning%2520the%2520facial%2520attributes%2520of%2520the%2520generated%2520image%2520with%250Athose%2520of%2520the%2520synthesized%2520target%2520image.%2520Our%2520framework%2520also%2520supports%2520localized%250Aanonymization%252C%2520allowing%2520users%2520to%2520specify%2520which%2520facial%2520regions%2520are%2520left%250Aunchanged.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520public%2520CelebA-HQ%2520and%2520FFHQ%250Adatasets%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520approaches%2520while%250Arequiring%2520no%2520additional%2520model%2520training.%2520The%2520source%2520code%2520is%2520available%2520on%2520our%250Apage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Localized%20Face%20Anonymization%20Via%20Diffusion%20Inpainting&entry.906535625=Ali%20Salar%20and%20Qing%20Liu%20and%20Guoying%20Zhao&entry.1292438233=%20%20The%20growing%20use%20of%20portrait%20images%20in%20computer%20vision%20highlights%20the%20need%20to%0Aprotect%20personal%20identities.%20At%20the%20same%20time%2C%20anonymized%20images%20must%20remain%0Auseful%20for%20downstream%20computer%20vision%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20unified%0Aframework%20that%20leverages%20the%20inpainting%20ability%20of%20latent%20diffusion%20models%20to%0Agenerate%20realistic%20anonymized%20images.%20Unlike%20prior%20approaches%2C%20we%20have%20complete%0Acontrol%20over%20the%20anonymization%20process%20by%20designing%20an%20adaptive%0Aattribute-guidance%20module%20that%20applies%20gradient%20correction%20during%20the%20reverse%0Adenoising%20process%2C%20aligning%20the%20facial%20attributes%20of%20the%20generated%20image%20with%0Athose%20of%20the%20synthesized%20target%20image.%20Our%20framework%20also%20supports%20localized%0Aanonymization%2C%20allowing%20users%20to%20specify%20which%20facial%20regions%20are%20left%0Aunchanged.%20Extensive%20experiments%20conducted%20on%20the%20public%20CelebA-HQ%20and%20FFHQ%0Adatasets%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20while%0Arequiring%20no%20additional%20model%20training.%20The%20source%20code%20is%20available%20on%20our%0Apage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14866v1&entry.124074799=Read"},
{"title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for\n  Quadrotor Navigation in Unknown Environments", "author": "Yifan Zhai and Rudolf Reiter and Davide Scaramuzza", "abstract": "  Quadrotor navigation in unknown environments is critical for practical\nmissions such as search-and-rescue. Solving it requires addressing three key\nchallenges: the non-convexity of free space due to obstacles,\nquadrotor-specific dynamics and objectives, and the need for exploration of\nunknown regions to find a path to the goal. Recently, the Model Predictive Path\nIntegral (MPPI) method has emerged as a promising solution that solves the\nfirst two challenges. By leveraging sampling-based optimization, it can\neffectively handle non-convex free space while directly optimizing over the\nfull quadrotor dynamics, enabling the inclusion of quadrotor-specific costs\nsuch as energy consumption. However, its performance in unknown environments is\nlimited, as it lacks the ability to explore unknown regions when blocked by\nlarge obstacles. To solve this issue, we introduce Perception-Aware MPPI\n(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory\nonline based on perception objectives. Specifically, when the goal is occluded,\nPA-MPPI's perception cost biases trajectories that can perceive unknown\nregions. This expands the mapped traversable space and increases the likelihood\nof finding alternative paths to the goal. Through hardware experiments, we\ndemonstrate that PA-MPPI, running at 50 Hz with our efficient perception and\nmapping module, performs up to 100% better than the baseline in our challenging\nsettings where the state-of-the-art MPPI fails. In addition, we demonstrate\nthat PA-MPPI can be used as a safe and robust action policy for navigation\nfoundation models, which often provide goal poses that are not directly\nreachable.\n", "link": "http://arxiv.org/abs/2509.14978v1", "date": "2025-09-18", "relevancy": 2.3619, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6029}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%0A%20%20Quadrotor%20Navigation%20in%20Unknown%20Environments&body=Title%3A%20PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%0A%20%20Quadrotor%20Navigation%20in%20Unknown%20Environments%0AAuthor%3A%20Yifan%20Zhai%20and%20Rudolf%20Reiter%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Quadrotor%20navigation%20in%20unknown%20environments%20is%20critical%20for%20practical%0Amissions%20such%20as%20search-and-rescue.%20Solving%20it%20requires%20addressing%20three%20key%0Achallenges%3A%20the%20non-convexity%20of%20free%20space%20due%20to%20obstacles%2C%0Aquadrotor-specific%20dynamics%20and%20objectives%2C%20and%20the%20need%20for%20exploration%20of%0Aunknown%20regions%20to%20find%20a%20path%20to%20the%20goal.%20Recently%2C%20the%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20method%20has%20emerged%20as%20a%20promising%20solution%20that%20solves%20the%0Afirst%20two%20challenges.%20By%20leveraging%20sampling-based%20optimization%2C%20it%20can%0Aeffectively%20handle%20non-convex%20free%20space%20while%20directly%20optimizing%20over%20the%0Afull%20quadrotor%20dynamics%2C%20enabling%20the%20inclusion%20of%20quadrotor-specific%20costs%0Asuch%20as%20energy%20consumption.%20However%2C%20its%20performance%20in%20unknown%20environments%20is%0Alimited%2C%20as%20it%20lacks%20the%20ability%20to%20explore%20unknown%20regions%20when%20blocked%20by%0Alarge%20obstacles.%20To%20solve%20this%20issue%2C%20we%20introduce%20Perception-Aware%20MPPI%0A%28PA-MPPI%29.%20Here%2C%20perception-awareness%20is%20defined%20as%20adapting%20the%20trajectory%0Aonline%20based%20on%20perception%20objectives.%20Specifically%2C%20when%20the%20goal%20is%20occluded%2C%0APA-MPPI%27s%20perception%20cost%20biases%20trajectories%20that%20can%20perceive%20unknown%0Aregions.%20This%20expands%20the%20mapped%20traversable%20space%20and%20increases%20the%20likelihood%0Aof%20finding%20alternative%20paths%20to%20the%20goal.%20Through%20hardware%20experiments%2C%20we%0Ademonstrate%20that%20PA-MPPI%2C%20running%20at%2050%20Hz%20with%20our%20efficient%20perception%20and%0Amapping%20module%2C%20performs%20up%20to%20100%25%20better%20than%20the%20baseline%20in%20our%20challenging%0Asettings%20where%20the%20state-of-the-art%20MPPI%20fails.%20In%20addition%2C%20we%20demonstrate%0Athat%20PA-MPPI%20can%20be%20used%20as%20a%20safe%20and%20robust%20action%20policy%20for%20navigation%0Afoundation%20models%2C%20which%20often%20provide%20goal%20poses%20that%20are%20not%20directly%0Areachable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPA-MPPI%253A%2520Perception-Aware%2520Model%2520Predictive%2520Path%2520Integral%2520Control%2520for%250A%2520%2520Quadrotor%2520Navigation%2520in%2520Unknown%2520Environments%26entry.906535625%3DYifan%2520Zhai%2520and%2520Rudolf%2520Reiter%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Quadrotor%2520navigation%2520in%2520unknown%2520environments%2520is%2520critical%2520for%2520practical%250Amissions%2520such%2520as%2520search-and-rescue.%2520Solving%2520it%2520requires%2520addressing%2520three%2520key%250Achallenges%253A%2520the%2520non-convexity%2520of%2520free%2520space%2520due%2520to%2520obstacles%252C%250Aquadrotor-specific%2520dynamics%2520and%2520objectives%252C%2520and%2520the%2520need%2520for%2520exploration%2520of%250Aunknown%2520regions%2520to%2520find%2520a%2520path%2520to%2520the%2520goal.%2520Recently%252C%2520the%2520Model%2520Predictive%2520Path%250AIntegral%2520%2528MPPI%2529%2520method%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520that%2520solves%2520the%250Afirst%2520two%2520challenges.%2520By%2520leveraging%2520sampling-based%2520optimization%252C%2520it%2520can%250Aeffectively%2520handle%2520non-convex%2520free%2520space%2520while%2520directly%2520optimizing%2520over%2520the%250Afull%2520quadrotor%2520dynamics%252C%2520enabling%2520the%2520inclusion%2520of%2520quadrotor-specific%2520costs%250Asuch%2520as%2520energy%2520consumption.%2520However%252C%2520its%2520performance%2520in%2520unknown%2520environments%2520is%250Alimited%252C%2520as%2520it%2520lacks%2520the%2520ability%2520to%2520explore%2520unknown%2520regions%2520when%2520blocked%2520by%250Alarge%2520obstacles.%2520To%2520solve%2520this%2520issue%252C%2520we%2520introduce%2520Perception-Aware%2520MPPI%250A%2528PA-MPPI%2529.%2520Here%252C%2520perception-awareness%2520is%2520defined%2520as%2520adapting%2520the%2520trajectory%250Aonline%2520based%2520on%2520perception%2520objectives.%2520Specifically%252C%2520when%2520the%2520goal%2520is%2520occluded%252C%250APA-MPPI%2527s%2520perception%2520cost%2520biases%2520trajectories%2520that%2520can%2520perceive%2520unknown%250Aregions.%2520This%2520expands%2520the%2520mapped%2520traversable%2520space%2520and%2520increases%2520the%2520likelihood%250Aof%2520finding%2520alternative%2520paths%2520to%2520the%2520goal.%2520Through%2520hardware%2520experiments%252C%2520we%250Ademonstrate%2520that%2520PA-MPPI%252C%2520running%2520at%252050%2520Hz%2520with%2520our%2520efficient%2520perception%2520and%250Amapping%2520module%252C%2520performs%2520up%2520to%2520100%2525%2520better%2520than%2520the%2520baseline%2520in%2520our%2520challenging%250Asettings%2520where%2520the%2520state-of-the-art%2520MPPI%2520fails.%2520In%2520addition%252C%2520we%2520demonstrate%250Athat%2520PA-MPPI%2520can%2520be%2520used%2520as%2520a%2520safe%2520and%2520robust%2520action%2520policy%2520for%2520navigation%250Afoundation%2520models%252C%2520which%2520often%2520provide%2520goal%2520poses%2520that%2520are%2520not%2520directly%250Areachable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PA-MPPI%3A%20Perception-Aware%20Model%20Predictive%20Path%20Integral%20Control%20for%0A%20%20Quadrotor%20Navigation%20in%20Unknown%20Environments&entry.906535625=Yifan%20Zhai%20and%20Rudolf%20Reiter%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Quadrotor%20navigation%20in%20unknown%20environments%20is%20critical%20for%20practical%0Amissions%20such%20as%20search-and-rescue.%20Solving%20it%20requires%20addressing%20three%20key%0Achallenges%3A%20the%20non-convexity%20of%20free%20space%20due%20to%20obstacles%2C%0Aquadrotor-specific%20dynamics%20and%20objectives%2C%20and%20the%20need%20for%20exploration%20of%0Aunknown%20regions%20to%20find%20a%20path%20to%20the%20goal.%20Recently%2C%20the%20Model%20Predictive%20Path%0AIntegral%20%28MPPI%29%20method%20has%20emerged%20as%20a%20promising%20solution%20that%20solves%20the%0Afirst%20two%20challenges.%20By%20leveraging%20sampling-based%20optimization%2C%20it%20can%0Aeffectively%20handle%20non-convex%20free%20space%20while%20directly%20optimizing%20over%20the%0Afull%20quadrotor%20dynamics%2C%20enabling%20the%20inclusion%20of%20quadrotor-specific%20costs%0Asuch%20as%20energy%20consumption.%20However%2C%20its%20performance%20in%20unknown%20environments%20is%0Alimited%2C%20as%20it%20lacks%20the%20ability%20to%20explore%20unknown%20regions%20when%20blocked%20by%0Alarge%20obstacles.%20To%20solve%20this%20issue%2C%20we%20introduce%20Perception-Aware%20MPPI%0A%28PA-MPPI%29.%20Here%2C%20perception-awareness%20is%20defined%20as%20adapting%20the%20trajectory%0Aonline%20based%20on%20perception%20objectives.%20Specifically%2C%20when%20the%20goal%20is%20occluded%2C%0APA-MPPI%27s%20perception%20cost%20biases%20trajectories%20that%20can%20perceive%20unknown%0Aregions.%20This%20expands%20the%20mapped%20traversable%20space%20and%20increases%20the%20likelihood%0Aof%20finding%20alternative%20paths%20to%20the%20goal.%20Through%20hardware%20experiments%2C%20we%0Ademonstrate%20that%20PA-MPPI%2C%20running%20at%2050%20Hz%20with%20our%20efficient%20perception%20and%0Amapping%20module%2C%20performs%20up%20to%20100%25%20better%20than%20the%20baseline%20in%20our%20challenging%0Asettings%20where%20the%20state-of-the-art%20MPPI%20fails.%20In%20addition%2C%20we%20demonstrate%0Athat%20PA-MPPI%20can%20be%20used%20as%20a%20safe%20and%20robust%20action%20policy%20for%20navigation%0Afoundation%20models%2C%20which%20often%20provide%20goal%20poses%20that%20are%20not%20directly%0Areachable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14978v1&entry.124074799=Read"},
{"title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional\n  Linguistics Discourse Analysis of Reviews of Peer Reviews", "author": "Gabriela C. Zapata and Bill Cope and Mary Kalantzis and Duane Searsmith", "abstract": "  This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review.\n", "link": "http://arxiv.org/abs/2509.15035v1", "date": "2025-09-18", "relevancy": 2.3466, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4874}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4676}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Generative%20AI%20as%20Meta-Reviewer%3A%20A%20Systemic%20Functional%0A%20%20Linguistics%20Discourse%20Analysis%20of%20Reviews%20of%20Peer%20Reviews&body=Title%3A%20Calibrated%20Generative%20AI%20as%20Meta-Reviewer%3A%20A%20Systemic%20Functional%0A%20%20Linguistics%20Discourse%20Analysis%20of%20Reviews%20of%20Peer%20Reviews%0AAuthor%3A%20Gabriela%20C.%20Zapata%20and%20Bill%20Cope%20and%20Mary%20Kalantzis%20and%20Duane%20Searsmith%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20use%20of%20generative%20AI%20to%20support%20formative%0Aassessment%20through%20machine%20generated%20reviews%20of%20peer%20reviews%20in%20graduate%20online%0Acourses%20in%20a%20public%20university%20in%20the%20United%20States.%20Drawing%20on%20Systemic%0AFunctional%20Linguistics%20and%20Appraisal%20Theory%2C%20we%20analyzed%20120%20metareviews%20to%0Aexplore%20how%20generative%20AI%20feedback%20constructs%20meaning%20across%20ideational%2C%0Ainterpersonal%2C%20and%20textual%20dimensions.%20The%20findings%20suggest%20that%20generative%20AI%0Acan%20approximate%20key%20rhetorical%20and%20relational%20features%20of%20effective%20human%0Afeedback%2C%20offering%20directive%20clarity%20while%20also%20maintaining%20a%20supportive%0Astance.%20The%20reviews%20analyzed%20demonstrated%20a%20balance%20of%20praise%20and%20constructive%0Acritique%2C%20alignment%20with%20rubric%20expectations%2C%20and%20structured%20staging%20that%0Aforegrounded%20student%20agency.%20By%20modeling%20these%20qualities%2C%20AI%20metafeedback%20has%0Athe%20potential%20to%20scaffold%20feedback%20literacy%20and%20enhance%20leaner%20engagement%20with%0Apeer%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Generative%2520AI%2520as%2520Meta-Reviewer%253A%2520A%2520Systemic%2520Functional%250A%2520%2520Linguistics%2520Discourse%2520Analysis%2520of%2520Reviews%2520of%2520Peer%2520Reviews%26entry.906535625%3DGabriela%2520C.%2520Zapata%2520and%2520Bill%2520Cope%2520and%2520Mary%2520Kalantzis%2520and%2520Duane%2520Searsmith%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520use%2520of%2520generative%2520AI%2520to%2520support%2520formative%250Aassessment%2520through%2520machine%2520generated%2520reviews%2520of%2520peer%2520reviews%2520in%2520graduate%2520online%250Acourses%2520in%2520a%2520public%2520university%2520in%2520the%2520United%2520States.%2520Drawing%2520on%2520Systemic%250AFunctional%2520Linguistics%2520and%2520Appraisal%2520Theory%252C%2520we%2520analyzed%2520120%2520metareviews%2520to%250Aexplore%2520how%2520generative%2520AI%2520feedback%2520constructs%2520meaning%2520across%2520ideational%252C%250Ainterpersonal%252C%2520and%2520textual%2520dimensions.%2520The%2520findings%2520suggest%2520that%2520generative%2520AI%250Acan%2520approximate%2520key%2520rhetorical%2520and%2520relational%2520features%2520of%2520effective%2520human%250Afeedback%252C%2520offering%2520directive%2520clarity%2520while%2520also%2520maintaining%2520a%2520supportive%250Astance.%2520The%2520reviews%2520analyzed%2520demonstrated%2520a%2520balance%2520of%2520praise%2520and%2520constructive%250Acritique%252C%2520alignment%2520with%2520rubric%2520expectations%252C%2520and%2520structured%2520staging%2520that%250Aforegrounded%2520student%2520agency.%2520By%2520modeling%2520these%2520qualities%252C%2520AI%2520metafeedback%2520has%250Athe%2520potential%2520to%2520scaffold%2520feedback%2520literacy%2520and%2520enhance%2520leaner%2520engagement%2520with%250Apeer%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Generative%20AI%20as%20Meta-Reviewer%3A%20A%20Systemic%20Functional%0A%20%20Linguistics%20Discourse%20Analysis%20of%20Reviews%20of%20Peer%20Reviews&entry.906535625=Gabriela%20C.%20Zapata%20and%20Bill%20Cope%20and%20Mary%20Kalantzis%20and%20Duane%20Searsmith&entry.1292438233=%20%20This%20study%20investigates%20the%20use%20of%20generative%20AI%20to%20support%20formative%0Aassessment%20through%20machine%20generated%20reviews%20of%20peer%20reviews%20in%20graduate%20online%0Acourses%20in%20a%20public%20university%20in%20the%20United%20States.%20Drawing%20on%20Systemic%0AFunctional%20Linguistics%20and%20Appraisal%20Theory%2C%20we%20analyzed%20120%20metareviews%20to%0Aexplore%20how%20generative%20AI%20feedback%20constructs%20meaning%20across%20ideational%2C%0Ainterpersonal%2C%20and%20textual%20dimensions.%20The%20findings%20suggest%20that%20generative%20AI%0Acan%20approximate%20key%20rhetorical%20and%20relational%20features%20of%20effective%20human%0Afeedback%2C%20offering%20directive%20clarity%20while%20also%20maintaining%20a%20supportive%0Astance.%20The%20reviews%20analyzed%20demonstrated%20a%20balance%20of%20praise%20and%20constructive%0Acritique%2C%20alignment%20with%20rubric%20expectations%2C%20and%20structured%20staging%20that%0Aforegrounded%20student%20agency.%20By%20modeling%20these%20qualities%2C%20AI%20metafeedback%20has%0Athe%20potential%20to%20scaffold%20feedback%20literacy%20and%20enhance%20leaner%20engagement%20with%0Apeer%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15035v1&entry.124074799=Read"},
{"title": "Self-Adapting Language Models", "author": "Adam Zweiger and Jyothish Pari and Han Guo and Ekin Aky\u00fcrek and Yoon Kim and Pulkit Agrawal", "abstract": "  Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.\n", "link": "http://arxiv.org/abs/2506.10943v2", "date": "2025-09-18", "relevancy": 2.3316, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4875}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.459}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Adapting%20Language%20Models&body=Title%3A%20Self-Adapting%20Language%20Models%0AAuthor%3A%20Adam%20Zweiger%20and%20Jyothish%20Pari%20and%20Han%20Guo%20and%20Ekin%20Aky%C3%BCrek%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20but%20static%3B%20they%20lack%20mechanisms%20to%0Aadapt%20their%20weights%20in%20response%20to%20new%20tasks%2C%20knowledge%2C%20or%20examples.%20We%0Aintroduce%20Self-Adapting%20LLMs%20%28SEAL%29%2C%20a%20framework%20that%20enables%20LLMs%20to%0Aself-adapt%20by%20generating%20their%20own%20finetuning%20data%20and%20update%20directives.%20Given%0Aa%20new%20input%2C%20the%20model%20produces%20a%20self-edit-a%20generation%20that%20may%20restructure%0Athe%20information%20in%20different%20ways%2C%20specify%20optimization%20hyperparameters%2C%20or%0Ainvoke%20tools%20for%20data%20augmentation%20and%20gradient-based%20updates.%20Through%0Asupervised%20finetuning%20%28SFT%29%2C%20these%20self-edits%20result%20in%20persistent%20weight%0Aupdates%2C%20enabling%20lasting%20adaptation.%20To%20train%20the%20model%20to%20produce%20effective%0Aself-edits%2C%20we%20use%20a%20reinforcement%20learning%20loop%20with%20the%20downstream%0Aperformance%20of%20the%20updated%20model%20as%20the%20reward%20signal.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20separate%20adaptation%20modules%20or%20auxiliary%20networks%2C%20SEAL%20directly%0Auses%20the%20model%27s%20own%20generation%20to%20control%20its%20adaptation%20process.%20Experiments%0Aon%20knowledge%20incorporation%20and%20few-shot%20generalization%20show%20that%20SEAL%20is%20a%0Apromising%20step%20toward%20language%20models%20capable%20of%20self-directed%20adaptation.%20Our%0Awebsite%20and%20code%20is%20available%20at%20https%3A//jyopari.github.io/posts/seal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Adapting%2520Language%2520Models%26entry.906535625%3DAdam%2520Zweiger%2520and%2520Jyothish%2520Pari%2520and%2520Han%2520Guo%2520and%2520Ekin%2520Aky%25C3%25BCrek%2520and%2520Yoon%2520Kim%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520powerful%2520but%2520static%253B%2520they%2520lack%2520mechanisms%2520to%250Aadapt%2520their%2520weights%2520in%2520response%2520to%2520new%2520tasks%252C%2520knowledge%252C%2520or%2520examples.%2520We%250Aintroduce%2520Self-Adapting%2520LLMs%2520%2528SEAL%2529%252C%2520a%2520framework%2520that%2520enables%2520LLMs%2520to%250Aself-adapt%2520by%2520generating%2520their%2520own%2520finetuning%2520data%2520and%2520update%2520directives.%2520Given%250Aa%2520new%2520input%252C%2520the%2520model%2520produces%2520a%2520self-edit-a%2520generation%2520that%2520may%2520restructure%250Athe%2520information%2520in%2520different%2520ways%252C%2520specify%2520optimization%2520hyperparameters%252C%2520or%250Ainvoke%2520tools%2520for%2520data%2520augmentation%2520and%2520gradient-based%2520updates.%2520Through%250Asupervised%2520finetuning%2520%2528SFT%2529%252C%2520these%2520self-edits%2520result%2520in%2520persistent%2520weight%250Aupdates%252C%2520enabling%2520lasting%2520adaptation.%2520To%2520train%2520the%2520model%2520to%2520produce%2520effective%250Aself-edits%252C%2520we%2520use%2520a%2520reinforcement%2520learning%2520loop%2520with%2520the%2520downstream%250Aperformance%2520of%2520the%2520updated%2520model%2520as%2520the%2520reward%2520signal.%2520Unlike%2520prior%2520approaches%250Athat%2520rely%2520on%2520separate%2520adaptation%2520modules%2520or%2520auxiliary%2520networks%252C%2520SEAL%2520directly%250Auses%2520the%2520model%2527s%2520own%2520generation%2520to%2520control%2520its%2520adaptation%2520process.%2520Experiments%250Aon%2520knowledge%2520incorporation%2520and%2520few-shot%2520generalization%2520show%2520that%2520SEAL%2520is%2520a%250Apromising%2520step%2520toward%2520language%2520models%2520capable%2520of%2520self-directed%2520adaptation.%2520Our%250Awebsite%2520and%2520code%2520is%2520available%2520at%2520https%253A//jyopari.github.io/posts/seal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Adapting%20Language%20Models&entry.906535625=Adam%20Zweiger%20and%20Jyothish%20Pari%20and%20Han%20Guo%20and%20Ekin%20Aky%C3%BCrek%20and%20Yoon%20Kim%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20powerful%20but%20static%3B%20they%20lack%20mechanisms%20to%0Aadapt%20their%20weights%20in%20response%20to%20new%20tasks%2C%20knowledge%2C%20or%20examples.%20We%0Aintroduce%20Self-Adapting%20LLMs%20%28SEAL%29%2C%20a%20framework%20that%20enables%20LLMs%20to%0Aself-adapt%20by%20generating%20their%20own%20finetuning%20data%20and%20update%20directives.%20Given%0Aa%20new%20input%2C%20the%20model%20produces%20a%20self-edit-a%20generation%20that%20may%20restructure%0Athe%20information%20in%20different%20ways%2C%20specify%20optimization%20hyperparameters%2C%20or%0Ainvoke%20tools%20for%20data%20augmentation%20and%20gradient-based%20updates.%20Through%0Asupervised%20finetuning%20%28SFT%29%2C%20these%20self-edits%20result%20in%20persistent%20weight%0Aupdates%2C%20enabling%20lasting%20adaptation.%20To%20train%20the%20model%20to%20produce%20effective%0Aself-edits%2C%20we%20use%20a%20reinforcement%20learning%20loop%20with%20the%20downstream%0Aperformance%20of%20the%20updated%20model%20as%20the%20reward%20signal.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20separate%20adaptation%20modules%20or%20auxiliary%20networks%2C%20SEAL%20directly%0Auses%20the%20model%27s%20own%20generation%20to%20control%20its%20adaptation%20process.%20Experiments%0Aon%20knowledge%20incorporation%20and%20few-shot%20generalization%20show%20that%20SEAL%20is%20a%0Apromising%20step%20toward%20language%20models%20capable%20of%20self-directed%20adaptation.%20Our%0Awebsite%20and%20code%20is%20available%20at%20https%3A//jyopari.github.io/posts/seal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10943v2&entry.124074799=Read"},
{"title": "GenKOL: Modular Generative AI Framework For Scalable Virtual KOL\n  Generation", "author": "Tan-Hiep To and Duy-Khang Nguyen and Tam V. Nguyen and Minh-Triet Tran and Trung-Nghia Le", "abstract": "  Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping\nconsumer perceptions and enhancing brand credibility. However, collaborating\nwith human KOLs often involves high costs and logistical challenges. To address\nthis, we present GenKOL, an interactive system that empowers marketing\nprofessionals to efficiently generate high-quality virtual KOL images using\ngenerative AI. GenKOL enables users to dynamically compose promotional visuals\nthrough an intuitive interface that integrates multiple AI capabilities,\nincluding garment generation, makeup transfer, background synthesis, and hair\nediting. These capabilities are implemented as modular, interchangeable\nservices that can be deployed flexibly on local machines or in the cloud. This\nmodular architecture ensures adaptability across diverse use cases and\ncomputational environments. Our system can significantly streamline the\nproduction of branded content, lowering costs and accelerating marketing\nworkflows through scalable virtual KOL creation.\n", "link": "http://arxiv.org/abs/2509.14927v1", "date": "2025-09-18", "relevancy": 2.3307, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6087}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5832}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenKOL%3A%20Modular%20Generative%20AI%20Framework%20For%20Scalable%20Virtual%20KOL%0A%20%20Generation&body=Title%3A%20GenKOL%3A%20Modular%20Generative%20AI%20Framework%20For%20Scalable%20Virtual%20KOL%0A%20%20Generation%0AAuthor%3A%20Tan-Hiep%20To%20and%20Duy-Khang%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le%0AAbstract%3A%20%20%20Key%20Opinion%20Leader%20%28KOL%29%20play%20a%20crucial%20role%20in%20modern%20marketing%20by%20shaping%0Aconsumer%20perceptions%20and%20enhancing%20brand%20credibility.%20However%2C%20collaborating%0Awith%20human%20KOLs%20often%20involves%20high%20costs%20and%20logistical%20challenges.%20To%20address%0Athis%2C%20we%20present%20GenKOL%2C%20an%20interactive%20system%20that%20empowers%20marketing%0Aprofessionals%20to%20efficiently%20generate%20high-quality%20virtual%20KOL%20images%20using%0Agenerative%20AI.%20GenKOL%20enables%20users%20to%20dynamically%20compose%20promotional%20visuals%0Athrough%20an%20intuitive%20interface%20that%20integrates%20multiple%20AI%20capabilities%2C%0Aincluding%20garment%20generation%2C%20makeup%20transfer%2C%20background%20synthesis%2C%20and%20hair%0Aediting.%20These%20capabilities%20are%20implemented%20as%20modular%2C%20interchangeable%0Aservices%20that%20can%20be%20deployed%20flexibly%20on%20local%20machines%20or%20in%20the%20cloud.%20This%0Amodular%20architecture%20ensures%20adaptability%20across%20diverse%20use%20cases%20and%0Acomputational%20environments.%20Our%20system%20can%20significantly%20streamline%20the%0Aproduction%20of%20branded%20content%2C%20lowering%20costs%20and%20accelerating%20marketing%0Aworkflows%20through%20scalable%20virtual%20KOL%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenKOL%253A%2520Modular%2520Generative%2520AI%2520Framework%2520For%2520Scalable%2520Virtual%2520KOL%250A%2520%2520Generation%26entry.906535625%3DTan-Hiep%2520To%2520and%2520Duy-Khang%2520Nguyen%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Minh-Triet%2520Tran%2520and%2520Trung-Nghia%2520Le%26entry.1292438233%3D%2520%2520Key%2520Opinion%2520Leader%2520%2528KOL%2529%2520play%2520a%2520crucial%2520role%2520in%2520modern%2520marketing%2520by%2520shaping%250Aconsumer%2520perceptions%2520and%2520enhancing%2520brand%2520credibility.%2520However%252C%2520collaborating%250Awith%2520human%2520KOLs%2520often%2520involves%2520high%2520costs%2520and%2520logistical%2520challenges.%2520To%2520address%250Athis%252C%2520we%2520present%2520GenKOL%252C%2520an%2520interactive%2520system%2520that%2520empowers%2520marketing%250Aprofessionals%2520to%2520efficiently%2520generate%2520high-quality%2520virtual%2520KOL%2520images%2520using%250Agenerative%2520AI.%2520GenKOL%2520enables%2520users%2520to%2520dynamically%2520compose%2520promotional%2520visuals%250Athrough%2520an%2520intuitive%2520interface%2520that%2520integrates%2520multiple%2520AI%2520capabilities%252C%250Aincluding%2520garment%2520generation%252C%2520makeup%2520transfer%252C%2520background%2520synthesis%252C%2520and%2520hair%250Aediting.%2520These%2520capabilities%2520are%2520implemented%2520as%2520modular%252C%2520interchangeable%250Aservices%2520that%2520can%2520be%2520deployed%2520flexibly%2520on%2520local%2520machines%2520or%2520in%2520the%2520cloud.%2520This%250Amodular%2520architecture%2520ensures%2520adaptability%2520across%2520diverse%2520use%2520cases%2520and%250Acomputational%2520environments.%2520Our%2520system%2520can%2520significantly%2520streamline%2520the%250Aproduction%2520of%2520branded%2520content%252C%2520lowering%2520costs%2520and%2520accelerating%2520marketing%250Aworkflows%2520through%2520scalable%2520virtual%2520KOL%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenKOL%3A%20Modular%20Generative%20AI%20Framework%20For%20Scalable%20Virtual%20KOL%0A%20%20Generation&entry.906535625=Tan-Hiep%20To%20and%20Duy-Khang%20Nguyen%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Trung-Nghia%20Le&entry.1292438233=%20%20Key%20Opinion%20Leader%20%28KOL%29%20play%20a%20crucial%20role%20in%20modern%20marketing%20by%20shaping%0Aconsumer%20perceptions%20and%20enhancing%20brand%20credibility.%20However%2C%20collaborating%0Awith%20human%20KOLs%20often%20involves%20high%20costs%20and%20logistical%20challenges.%20To%20address%0Athis%2C%20we%20present%20GenKOL%2C%20an%20interactive%20system%20that%20empowers%20marketing%0Aprofessionals%20to%20efficiently%20generate%20high-quality%20virtual%20KOL%20images%20using%0Agenerative%20AI.%20GenKOL%20enables%20users%20to%20dynamically%20compose%20promotional%20visuals%0Athrough%20an%20intuitive%20interface%20that%20integrates%20multiple%20AI%20capabilities%2C%0Aincluding%20garment%20generation%2C%20makeup%20transfer%2C%20background%20synthesis%2C%20and%20hair%0Aediting.%20These%20capabilities%20are%20implemented%20as%20modular%2C%20interchangeable%0Aservices%20that%20can%20be%20deployed%20flexibly%20on%20local%20machines%20or%20in%20the%20cloud.%20This%0Amodular%20architecture%20ensures%20adaptability%20across%20diverse%20use%20cases%20and%0Acomputational%20environments.%20Our%20system%20can%20significantly%20streamline%20the%0Aproduction%20of%20branded%20content%2C%20lowering%20costs%20and%20accelerating%20marketing%0Aworkflows%20through%20scalable%20virtual%20KOL%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14927v1&entry.124074799=Read"},
{"title": "Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant\n  Point Cloud Masked Autoencoders", "author": "Xuanhua Yin and Dingxin Zhang and Yu Feng and Shunqi Mao and Jianhui Yu and Weidong Cai", "abstract": "  Existing rotation-invariant point cloud masked autoencoders (MAE) rely on\nrandom masking strategies that overlook geometric structure and semantic\ncoherence. Random masking treats patches independently, failing to capture\nspatial relationships consistent across orientations and overlooking semantic\nobject parts that maintain identity regardless of rotation. We propose a\ndual-stream masking approach combining 3D Spatial Grid Masking and Progressive\nSemantic Masking to address these fundamental limitations. Grid masking creates\nstructured patterns through coordinate sorting to capture geometric\nrelationships that persist across different orientations, while semantic\nmasking uses attention-driven clustering to discover semantically meaningful\nparts and maintain their coherence during masking. These complementary streams\nare orchestrated via curriculum learning with dynamic weighting, progressing\nfrom geometric understanding to semantic discovery. Designed as plug-and-play\ncomponents, our strategies integrate into existing rotation-invariant\nframeworks without architectural changes, ensuring broad compatibility across\ndifferent approaches. Comprehensive experiments on ModelNet40, ScanObjectNN,\nand OmniObject3D demonstrate consistent improvements across various rotation\nscenarios, showing substantial performance gains over the baseline\nrotation-invariant methods.\n", "link": "http://arxiv.org/abs/2509.14975v1", "date": "2025-09-18", "relevancy": 2.3304, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6079}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5809}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Random%20Masking%3A%20A%20Dual-Stream%20Approach%20for%20Rotation-Invariant%0A%20%20Point%20Cloud%20Masked%20Autoencoders&body=Title%3A%20Beyond%20Random%20Masking%3A%20A%20Dual-Stream%20Approach%20for%20Rotation-Invariant%0A%20%20Point%20Cloud%20Masked%20Autoencoders%0AAuthor%3A%20Xuanhua%20Yin%20and%20Dingxin%20Zhang%20and%20Yu%20Feng%20and%20Shunqi%20Mao%20and%20Jianhui%20Yu%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Existing%20rotation-invariant%20point%20cloud%20masked%20autoencoders%20%28MAE%29%20rely%20on%0Arandom%20masking%20strategies%20that%20overlook%20geometric%20structure%20and%20semantic%0Acoherence.%20Random%20masking%20treats%20patches%20independently%2C%20failing%20to%20capture%0Aspatial%20relationships%20consistent%20across%20orientations%20and%20overlooking%20semantic%0Aobject%20parts%20that%20maintain%20identity%20regardless%20of%20rotation.%20We%20propose%20a%0Adual-stream%20masking%20approach%20combining%203D%20Spatial%20Grid%20Masking%20and%20Progressive%0ASemantic%20Masking%20to%20address%20these%20fundamental%20limitations.%20Grid%20masking%20creates%0Astructured%20patterns%20through%20coordinate%20sorting%20to%20capture%20geometric%0Arelationships%20that%20persist%20across%20different%20orientations%2C%20while%20semantic%0Amasking%20uses%20attention-driven%20clustering%20to%20discover%20semantically%20meaningful%0Aparts%20and%20maintain%20their%20coherence%20during%20masking.%20These%20complementary%20streams%0Aare%20orchestrated%20via%20curriculum%20learning%20with%20dynamic%20weighting%2C%20progressing%0Afrom%20geometric%20understanding%20to%20semantic%20discovery.%20Designed%20as%20plug-and-play%0Acomponents%2C%20our%20strategies%20integrate%20into%20existing%20rotation-invariant%0Aframeworks%20without%20architectural%20changes%2C%20ensuring%20broad%20compatibility%20across%0Adifferent%20approaches.%20Comprehensive%20experiments%20on%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20OmniObject3D%20demonstrate%20consistent%20improvements%20across%20various%20rotation%0Ascenarios%2C%20showing%20substantial%20performance%20gains%20over%20the%20baseline%0Arotation-invariant%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Random%2520Masking%253A%2520A%2520Dual-Stream%2520Approach%2520for%2520Rotation-Invariant%250A%2520%2520Point%2520Cloud%2520Masked%2520Autoencoders%26entry.906535625%3DXuanhua%2520Yin%2520and%2520Dingxin%2520Zhang%2520and%2520Yu%2520Feng%2520and%2520Shunqi%2520Mao%2520and%2520Jianhui%2520Yu%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Existing%2520rotation-invariant%2520point%2520cloud%2520masked%2520autoencoders%2520%2528MAE%2529%2520rely%2520on%250Arandom%2520masking%2520strategies%2520that%2520overlook%2520geometric%2520structure%2520and%2520semantic%250Acoherence.%2520Random%2520masking%2520treats%2520patches%2520independently%252C%2520failing%2520to%2520capture%250Aspatial%2520relationships%2520consistent%2520across%2520orientations%2520and%2520overlooking%2520semantic%250Aobject%2520parts%2520that%2520maintain%2520identity%2520regardless%2520of%2520rotation.%2520We%2520propose%2520a%250Adual-stream%2520masking%2520approach%2520combining%25203D%2520Spatial%2520Grid%2520Masking%2520and%2520Progressive%250ASemantic%2520Masking%2520to%2520address%2520these%2520fundamental%2520limitations.%2520Grid%2520masking%2520creates%250Astructured%2520patterns%2520through%2520coordinate%2520sorting%2520to%2520capture%2520geometric%250Arelationships%2520that%2520persist%2520across%2520different%2520orientations%252C%2520while%2520semantic%250Amasking%2520uses%2520attention-driven%2520clustering%2520to%2520discover%2520semantically%2520meaningful%250Aparts%2520and%2520maintain%2520their%2520coherence%2520during%2520masking.%2520These%2520complementary%2520streams%250Aare%2520orchestrated%2520via%2520curriculum%2520learning%2520with%2520dynamic%2520weighting%252C%2520progressing%250Afrom%2520geometric%2520understanding%2520to%2520semantic%2520discovery.%2520Designed%2520as%2520plug-and-play%250Acomponents%252C%2520our%2520strategies%2520integrate%2520into%2520existing%2520rotation-invariant%250Aframeworks%2520without%2520architectural%2520changes%252C%2520ensuring%2520broad%2520compatibility%2520across%250Adifferent%2520approaches.%2520Comprehensive%2520experiments%2520on%2520ModelNet40%252C%2520ScanObjectNN%252C%250Aand%2520OmniObject3D%2520demonstrate%2520consistent%2520improvements%2520across%2520various%2520rotation%250Ascenarios%252C%2520showing%2520substantial%2520performance%2520gains%2520over%2520the%2520baseline%250Arotation-invariant%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Random%20Masking%3A%20A%20Dual-Stream%20Approach%20for%20Rotation-Invariant%0A%20%20Point%20Cloud%20Masked%20Autoencoders&entry.906535625=Xuanhua%20Yin%20and%20Dingxin%20Zhang%20and%20Yu%20Feng%20and%20Shunqi%20Mao%20and%20Jianhui%20Yu%20and%20Weidong%20Cai&entry.1292438233=%20%20Existing%20rotation-invariant%20point%20cloud%20masked%20autoencoders%20%28MAE%29%20rely%20on%0Arandom%20masking%20strategies%20that%20overlook%20geometric%20structure%20and%20semantic%0Acoherence.%20Random%20masking%20treats%20patches%20independently%2C%20failing%20to%20capture%0Aspatial%20relationships%20consistent%20across%20orientations%20and%20overlooking%20semantic%0Aobject%20parts%20that%20maintain%20identity%20regardless%20of%20rotation.%20We%20propose%20a%0Adual-stream%20masking%20approach%20combining%203D%20Spatial%20Grid%20Masking%20and%20Progressive%0ASemantic%20Masking%20to%20address%20these%20fundamental%20limitations.%20Grid%20masking%20creates%0Astructured%20patterns%20through%20coordinate%20sorting%20to%20capture%20geometric%0Arelationships%20that%20persist%20across%20different%20orientations%2C%20while%20semantic%0Amasking%20uses%20attention-driven%20clustering%20to%20discover%20semantically%20meaningful%0Aparts%20and%20maintain%20their%20coherence%20during%20masking.%20These%20complementary%20streams%0Aare%20orchestrated%20via%20curriculum%20learning%20with%20dynamic%20weighting%2C%20progressing%0Afrom%20geometric%20understanding%20to%20semantic%20discovery.%20Designed%20as%20plug-and-play%0Acomponents%2C%20our%20strategies%20integrate%20into%20existing%20rotation-invariant%0Aframeworks%20without%20architectural%20changes%2C%20ensuring%20broad%20compatibility%20across%0Adifferent%20approaches.%20Comprehensive%20experiments%20on%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20OmniObject3D%20demonstrate%20consistent%20improvements%20across%20various%20rotation%0Ascenarios%2C%20showing%20substantial%20performance%20gains%20over%20the%20baseline%0Arotation-invariant%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14975v1&entry.124074799=Read"},
{"title": "Binarized Neural Networks Converge Toward Algorithmic Simplicity:\n  Empirical Support for the Learning-as-Compression Hypothesis", "author": "Eduardo Y. Sakabe and Felipe S. Abrah\u00e3o and Alexandre Sim\u00f5es and Esther Colombini and Paula Costa and Ricardo Gudwin and Hector Zenil", "abstract": "  Understanding and controlling the informational complexity of neural networks\nis a central challenge in machine learning, with implications for\ngeneralization, optimization, and model capacity. While most approaches rely on\nentropy-based loss functions and statistical metrics, these measures often fail\nto capture deeper, causally relevant algorithmic regularities embedded in\nnetwork structure. We propose a shift toward algorithmic information theory,\nusing Binarized Neural Networks (BNNs) as a first proxy. Grounded in\nalgorithmic probability (AP) and the universal distribution it defines, our\napproach characterizes learning dynamics through a formal, causally grounded\nlens. We apply the Block Decomposition Method (BDM) -- a scalable approximation\nof algorithmic complexity based on AP -- and demonstrate that it more closely\ntracks structural changes during training than entropy, consistently exhibiting\nstronger correlations with training loss across varying model sizes and\nrandomized training runs. These results support the view of training as a\nprocess of algorithmic compression, where learning corresponds to the\nprogressive internalization of structured regularities. In doing so, our work\noffers a principled estimate of learning progression and suggests a framework\nfor complexity-aware learning and regularization, grounded in first principles\nfrom information theory, complexity, and computability.\n", "link": "http://arxiv.org/abs/2505.20646v3", "date": "2025-09-18", "relevancy": 2.3285, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4616}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binarized%20Neural%20Networks%20Converge%20Toward%20Algorithmic%20Simplicity%3A%0A%20%20Empirical%20Support%20for%20the%20Learning-as-Compression%20Hypothesis&body=Title%3A%20Binarized%20Neural%20Networks%20Converge%20Toward%20Algorithmic%20Simplicity%3A%0A%20%20Empirical%20Support%20for%20the%20Learning-as-Compression%20Hypothesis%0AAuthor%3A%20Eduardo%20Y.%20Sakabe%20and%20Felipe%20S.%20Abrah%C3%A3o%20and%20Alexandre%20Sim%C3%B5es%20and%20Esther%20Colombini%20and%20Paula%20Costa%20and%20Ricardo%20Gudwin%20and%20Hector%20Zenil%0AAbstract%3A%20%20%20Understanding%20and%20controlling%20the%20informational%20complexity%20of%20neural%20networks%0Ais%20a%20central%20challenge%20in%20machine%20learning%2C%20with%20implications%20for%0Ageneralization%2C%20optimization%2C%20and%20model%20capacity.%20While%20most%20approaches%20rely%20on%0Aentropy-based%20loss%20functions%20and%20statistical%20metrics%2C%20these%20measures%20often%20fail%0Ato%20capture%20deeper%2C%20causally%20relevant%20algorithmic%20regularities%20embedded%20in%0Anetwork%20structure.%20We%20propose%20a%20shift%20toward%20algorithmic%20information%20theory%2C%0Ausing%20Binarized%20Neural%20Networks%20%28BNNs%29%20as%20a%20first%20proxy.%20Grounded%20in%0Aalgorithmic%20probability%20%28AP%29%20and%20the%20universal%20distribution%20it%20defines%2C%20our%0Aapproach%20characterizes%20learning%20dynamics%20through%20a%20formal%2C%20causally%20grounded%0Alens.%20We%20apply%20the%20Block%20Decomposition%20Method%20%28BDM%29%20--%20a%20scalable%20approximation%0Aof%20algorithmic%20complexity%20based%20on%20AP%20--%20and%20demonstrate%20that%20it%20more%20closely%0Atracks%20structural%20changes%20during%20training%20than%20entropy%2C%20consistently%20exhibiting%0Astronger%20correlations%20with%20training%20loss%20across%20varying%20model%20sizes%20and%0Arandomized%20training%20runs.%20These%20results%20support%20the%20view%20of%20training%20as%20a%0Aprocess%20of%20algorithmic%20compression%2C%20where%20learning%20corresponds%20to%20the%0Aprogressive%20internalization%20of%20structured%20regularities.%20In%20doing%20so%2C%20our%20work%0Aoffers%20a%20principled%20estimate%20of%20learning%20progression%20and%20suggests%20a%20framework%0Afor%20complexity-aware%20learning%20and%20regularization%2C%20grounded%20in%20first%20principles%0Afrom%20information%20theory%2C%20complexity%2C%20and%20computability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinarized%2520Neural%2520Networks%2520Converge%2520Toward%2520Algorithmic%2520Simplicity%253A%250A%2520%2520Empirical%2520Support%2520for%2520the%2520Learning-as-Compression%2520Hypothesis%26entry.906535625%3DEduardo%2520Y.%2520Sakabe%2520and%2520Felipe%2520S.%2520Abrah%25C3%25A3o%2520and%2520Alexandre%2520Sim%25C3%25B5es%2520and%2520Esther%2520Colombini%2520and%2520Paula%2520Costa%2520and%2520Ricardo%2520Gudwin%2520and%2520Hector%2520Zenil%26entry.1292438233%3D%2520%2520Understanding%2520and%2520controlling%2520the%2520informational%2520complexity%2520of%2520neural%2520networks%250Ais%2520a%2520central%2520challenge%2520in%2520machine%2520learning%252C%2520with%2520implications%2520for%250Ageneralization%252C%2520optimization%252C%2520and%2520model%2520capacity.%2520While%2520most%2520approaches%2520rely%2520on%250Aentropy-based%2520loss%2520functions%2520and%2520statistical%2520metrics%252C%2520these%2520measures%2520often%2520fail%250Ato%2520capture%2520deeper%252C%2520causally%2520relevant%2520algorithmic%2520regularities%2520embedded%2520in%250Anetwork%2520structure.%2520We%2520propose%2520a%2520shift%2520toward%2520algorithmic%2520information%2520theory%252C%250Ausing%2520Binarized%2520Neural%2520Networks%2520%2528BNNs%2529%2520as%2520a%2520first%2520proxy.%2520Grounded%2520in%250Aalgorithmic%2520probability%2520%2528AP%2529%2520and%2520the%2520universal%2520distribution%2520it%2520defines%252C%2520our%250Aapproach%2520characterizes%2520learning%2520dynamics%2520through%2520a%2520formal%252C%2520causally%2520grounded%250Alens.%2520We%2520apply%2520the%2520Block%2520Decomposition%2520Method%2520%2528BDM%2529%2520--%2520a%2520scalable%2520approximation%250Aof%2520algorithmic%2520complexity%2520based%2520on%2520AP%2520--%2520and%2520demonstrate%2520that%2520it%2520more%2520closely%250Atracks%2520structural%2520changes%2520during%2520training%2520than%2520entropy%252C%2520consistently%2520exhibiting%250Astronger%2520correlations%2520with%2520training%2520loss%2520across%2520varying%2520model%2520sizes%2520and%250Arandomized%2520training%2520runs.%2520These%2520results%2520support%2520the%2520view%2520of%2520training%2520as%2520a%250Aprocess%2520of%2520algorithmic%2520compression%252C%2520where%2520learning%2520corresponds%2520to%2520the%250Aprogressive%2520internalization%2520of%2520structured%2520regularities.%2520In%2520doing%2520so%252C%2520our%2520work%250Aoffers%2520a%2520principled%2520estimate%2520of%2520learning%2520progression%2520and%2520suggests%2520a%2520framework%250Afor%2520complexity-aware%2520learning%2520and%2520regularization%252C%2520grounded%2520in%2520first%2520principles%250Afrom%2520information%2520theory%252C%2520complexity%252C%2520and%2520computability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binarized%20Neural%20Networks%20Converge%20Toward%20Algorithmic%20Simplicity%3A%0A%20%20Empirical%20Support%20for%20the%20Learning-as-Compression%20Hypothesis&entry.906535625=Eduardo%20Y.%20Sakabe%20and%20Felipe%20S.%20Abrah%C3%A3o%20and%20Alexandre%20Sim%C3%B5es%20and%20Esther%20Colombini%20and%20Paula%20Costa%20and%20Ricardo%20Gudwin%20and%20Hector%20Zenil&entry.1292438233=%20%20Understanding%20and%20controlling%20the%20informational%20complexity%20of%20neural%20networks%0Ais%20a%20central%20challenge%20in%20machine%20learning%2C%20with%20implications%20for%0Ageneralization%2C%20optimization%2C%20and%20model%20capacity.%20While%20most%20approaches%20rely%20on%0Aentropy-based%20loss%20functions%20and%20statistical%20metrics%2C%20these%20measures%20often%20fail%0Ato%20capture%20deeper%2C%20causally%20relevant%20algorithmic%20regularities%20embedded%20in%0Anetwork%20structure.%20We%20propose%20a%20shift%20toward%20algorithmic%20information%20theory%2C%0Ausing%20Binarized%20Neural%20Networks%20%28BNNs%29%20as%20a%20first%20proxy.%20Grounded%20in%0Aalgorithmic%20probability%20%28AP%29%20and%20the%20universal%20distribution%20it%20defines%2C%20our%0Aapproach%20characterizes%20learning%20dynamics%20through%20a%20formal%2C%20causally%20grounded%0Alens.%20We%20apply%20the%20Block%20Decomposition%20Method%20%28BDM%29%20--%20a%20scalable%20approximation%0Aof%20algorithmic%20complexity%20based%20on%20AP%20--%20and%20demonstrate%20that%20it%20more%20closely%0Atracks%20structural%20changes%20during%20training%20than%20entropy%2C%20consistently%20exhibiting%0Astronger%20correlations%20with%20training%20loss%20across%20varying%20model%20sizes%20and%0Arandomized%20training%20runs.%20These%20results%20support%20the%20view%20of%20training%20as%20a%0Aprocess%20of%20algorithmic%20compression%2C%20where%20learning%20corresponds%20to%20the%0Aprogressive%20internalization%20of%20structured%20regularities.%20In%20doing%20so%2C%20our%20work%0Aoffers%20a%20principled%20estimate%20of%20learning%20progression%20and%20suggests%20a%20framework%0Afor%20complexity-aware%20learning%20and%20regularization%2C%20grounded%20in%20first%20principles%0Afrom%20information%20theory%2C%20complexity%2C%20and%20computability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20646v3&entry.124074799=Read"},
{"title": "Robust Shape Regularity Criteria for Superpixel Evaluation", "author": "R\u00e9mi Giraud and Vinh-Thong Ta and Nicolas Papadakis", "abstract": "  Regular decompositions are necessary for most superpixel-based object\nrecognition or tracking applications. So far in the literature, the regularity\nor compactness of a superpixel shape is mainly measured by its circularity. In\nthis work, we first demonstrate that such measure is not adapted for superpixel\nevaluation, since it does not directly express regularity but circular\nappearance. Then, we propose a new metric that considers several shape\nregularity aspects: convexity, balanced repartition, and contour smoothness.\nFinally, we demonstrate that our measure is robust to scale and noise and\nenables to more relevantly compare superpixel methods.\n", "link": "http://arxiv.org/abs/1903.07146v3", "date": "2025-09-18", "relevancy": 2.3151, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation&body=Title%3A%20Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation%0AAuthor%3A%20R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis%0AAbstract%3A%20%20%20Regular%20decompositions%20are%20necessary%20for%20most%20superpixel-based%20object%0Arecognition%20or%20tracking%20applications.%20So%20far%20in%20the%20literature%2C%20the%20regularity%0Aor%20compactness%20of%20a%20superpixel%20shape%20is%20mainly%20measured%20by%20its%20circularity.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20that%20such%20measure%20is%20not%20adapted%20for%20superpixel%0Aevaluation%2C%20since%20it%20does%20not%20directly%20express%20regularity%20but%20circular%0Aappearance.%20Then%2C%20we%20propose%20a%20new%20metric%20that%20considers%20several%20shape%0Aregularity%20aspects%3A%20convexity%2C%20balanced%20repartition%2C%20and%20contour%20smoothness.%0AFinally%2C%20we%20demonstrate%20that%20our%20measure%20is%20robust%20to%20scale%20and%20noise%20and%0Aenables%20to%20more%20relevantly%20compare%20superpixel%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1903.07146v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Shape%2520Regularity%2520Criteria%2520for%2520Superpixel%2520Evaluation%26entry.906535625%3DR%25C3%25A9mi%2520Giraud%2520and%2520Vinh-Thong%2520Ta%2520and%2520Nicolas%2520Papadakis%26entry.1292438233%3D%2520%2520Regular%2520decompositions%2520are%2520necessary%2520for%2520most%2520superpixel-based%2520object%250Arecognition%2520or%2520tracking%2520applications.%2520So%2520far%2520in%2520the%2520literature%252C%2520the%2520regularity%250Aor%2520compactness%2520of%2520a%2520superpixel%2520shape%2520is%2520mainly%2520measured%2520by%2520its%2520circularity.%2520In%250Athis%2520work%252C%2520we%2520first%2520demonstrate%2520that%2520such%2520measure%2520is%2520not%2520adapted%2520for%2520superpixel%250Aevaluation%252C%2520since%2520it%2520does%2520not%2520directly%2520express%2520regularity%2520but%2520circular%250Aappearance.%2520Then%252C%2520we%2520propose%2520a%2520new%2520metric%2520that%2520considers%2520several%2520shape%250Aregularity%2520aspects%253A%2520convexity%252C%2520balanced%2520repartition%252C%2520and%2520contour%2520smoothness.%250AFinally%252C%2520we%2520demonstrate%2520that%2520our%2520measure%2520is%2520robust%2520to%2520scale%2520and%2520noise%2520and%250Aenables%2520to%2520more%2520relevantly%2520compare%2520superpixel%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1903.07146v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Shape%20Regularity%20Criteria%20for%20Superpixel%20Evaluation&entry.906535625=R%C3%A9mi%20Giraud%20and%20Vinh-Thong%20Ta%20and%20Nicolas%20Papadakis&entry.1292438233=%20%20Regular%20decompositions%20are%20necessary%20for%20most%20superpixel-based%20object%0Arecognition%20or%20tracking%20applications.%20So%20far%20in%20the%20literature%2C%20the%20regularity%0Aor%20compactness%20of%20a%20superpixel%20shape%20is%20mainly%20measured%20by%20its%20circularity.%20In%0Athis%20work%2C%20we%20first%20demonstrate%20that%20such%20measure%20is%20not%20adapted%20for%20superpixel%0Aevaluation%2C%20since%20it%20does%20not%20directly%20express%20regularity%20but%20circular%0Aappearance.%20Then%2C%20we%20propose%20a%20new%20metric%20that%20considers%20several%20shape%0Aregularity%20aspects%3A%20convexity%2C%20balanced%20repartition%2C%20and%20contour%20smoothness.%0AFinally%2C%20we%20demonstrate%20that%20our%20measure%20is%20robust%20to%20scale%20and%20noise%20and%0Aenables%20to%20more%20relevantly%20compare%20superpixel%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/1903.07146v3&entry.124074799=Read"},
{"title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion", "author": "Pei Liu and Haipeng Liu and Haichao Liu and Xin Liu and Jinxin Ni and Jun Ma", "abstract": "  Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modalities is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and achieve\nsignificant improvements in perception, prediction, and planning over the\nbaseline end-to-end model, showcasing the effectiveness of our\nattention-enhanced BEV representation in enabling more accurate and reliable\nautonomous driving tasks.\n", "link": "http://arxiv.org/abs/2502.18042v2", "date": "2025-09-18", "relevancy": 2.3061, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion&body=Title%3A%20VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion%0AAuthor%3A%20Pei%20Liu%20and%20Haipeng%20Liu%20and%20Haichao%20Liu%20and%20Xin%20Liu%20and%20Jinxin%20Ni%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Human%20drivers%20adeptly%20navigate%20complex%20scenarios%20by%20utilizing%20rich%0Aattentional%20semantics%2C%20but%20the%20current%20autonomous%20systems%20struggle%20to%20replicate%0Athis%20ability%2C%20as%20they%20often%20lose%20critical%20semantic%20information%20when%20converting%0A2D%20observations%20into%203D%20space.%20In%20this%20sense%2C%20it%20hinders%20their%20effective%0Adeployment%20in%20dynamic%20and%20complex%20environments.%20Leveraging%20the%20superior%20scene%0Aunderstanding%20and%20reasoning%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20we%0Apropose%20VLM-E2E%2C%20a%20novel%20framework%20that%20uses%20the%20VLMs%20to%20enhance%20training%20by%0Aproviding%20attentional%20cues.%20Our%20method%20integrates%20textual%20representations%20into%0ABird%27s-Eye-View%20%28BEV%29%20features%20for%20semantic%20supervision%2C%20which%20enables%20the%0Amodel%20to%20learn%20richer%20feature%20representations%20that%20explicitly%20capture%20the%0Adriver%27s%20attentional%20semantics.%20By%20focusing%20on%20attentional%20semantics%2C%20VLM-E2E%0Abetter%20aligns%20with%20human-like%20driving%20behavior%2C%20which%20is%20critical%20for%0Anavigating%20dynamic%20and%20complex%20environments.%20Furthermore%2C%20we%20introduce%20a%0ABEV-Text%20learnable%20weighted%20fusion%20strategy%20to%20address%20the%20issue%20of%20modality%0Aimportance%20imbalance%20in%20fusing%20multimodal%20information.%20This%20approach%0Adynamically%20balances%20the%20contributions%20of%20BEV%20and%20text%20features%2C%20ensuring%20that%0Athe%20complementary%20information%20from%20visual%20and%20textual%20modalities%20is%20effectively%0Autilized.%20By%20explicitly%20addressing%20the%20imbalance%20in%20multimodal%20fusion%2C%20our%0Amethod%20facilitates%20a%20more%20holistic%20and%20robust%20representation%20of%20driving%0Aenvironments.%20We%20evaluate%20VLM-E2E%20on%20the%20nuScenes%20dataset%20and%20achieve%0Asignificant%20improvements%20in%20perception%2C%20prediction%2C%20and%20planning%20over%20the%0Abaseline%20end-to-end%20model%2C%20showcasing%20the%20effectiveness%20of%20our%0Aattention-enhanced%20BEV%20representation%20in%20enabling%20more%20accurate%20and%20reliable%0Aautonomous%20driving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-E2E%253A%2520Enhancing%2520End-to-End%2520Autonomous%2520Driving%2520with%2520Multimodal%2520Driver%250A%2520%2520Attention%2520Fusion%26entry.906535625%3DPei%2520Liu%2520and%2520Haipeng%2520Liu%2520and%2520Haichao%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Jinxin%2520Ni%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Human%2520drivers%2520adeptly%2520navigate%2520complex%2520scenarios%2520by%2520utilizing%2520rich%250Aattentional%2520semantics%252C%2520but%2520the%2520current%2520autonomous%2520systems%2520struggle%2520to%2520replicate%250Athis%2520ability%252C%2520as%2520they%2520often%2520lose%2520critical%2520semantic%2520information%2520when%2520converting%250A2D%2520observations%2520into%25203D%2520space.%2520In%2520this%2520sense%252C%2520it%2520hinders%2520their%2520effective%250Adeployment%2520in%2520dynamic%2520and%2520complex%2520environments.%2520Leveraging%2520the%2520superior%2520scene%250Aunderstanding%2520and%2520reasoning%2520abilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520we%250Apropose%2520VLM-E2E%252C%2520a%2520novel%2520framework%2520that%2520uses%2520the%2520VLMs%2520to%2520enhance%2520training%2520by%250Aproviding%2520attentional%2520cues.%2520Our%2520method%2520integrates%2520textual%2520representations%2520into%250ABird%2527s-Eye-View%2520%2528BEV%2529%2520features%2520for%2520semantic%2520supervision%252C%2520which%2520enables%2520the%250Amodel%2520to%2520learn%2520richer%2520feature%2520representations%2520that%2520explicitly%2520capture%2520the%250Adriver%2527s%2520attentional%2520semantics.%2520By%2520focusing%2520on%2520attentional%2520semantics%252C%2520VLM-E2E%250Abetter%2520aligns%2520with%2520human-like%2520driving%2520behavior%252C%2520which%2520is%2520critical%2520for%250Anavigating%2520dynamic%2520and%2520complex%2520environments.%2520Furthermore%252C%2520we%2520introduce%2520a%250ABEV-Text%2520learnable%2520weighted%2520fusion%2520strategy%2520to%2520address%2520the%2520issue%2520of%2520modality%250Aimportance%2520imbalance%2520in%2520fusing%2520multimodal%2520information.%2520This%2520approach%250Adynamically%2520balances%2520the%2520contributions%2520of%2520BEV%2520and%2520text%2520features%252C%2520ensuring%2520that%250Athe%2520complementary%2520information%2520from%2520visual%2520and%2520textual%2520modalities%2520is%2520effectively%250Autilized.%2520By%2520explicitly%2520addressing%2520the%2520imbalance%2520in%2520multimodal%2520fusion%252C%2520our%250Amethod%2520facilitates%2520a%2520more%2520holistic%2520and%2520robust%2520representation%2520of%2520driving%250Aenvironments.%2520We%2520evaluate%2520VLM-E2E%2520on%2520the%2520nuScenes%2520dataset%2520and%2520achieve%250Asignificant%2520improvements%2520in%2520perception%252C%2520prediction%252C%2520and%2520planning%2520over%2520the%250Abaseline%2520end-to-end%2520model%252C%2520showcasing%2520the%2520effectiveness%2520of%2520our%250Aattention-enhanced%2520BEV%2520representation%2520in%2520enabling%2520more%2520accurate%2520and%2520reliable%250Aautonomous%2520driving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion&entry.906535625=Pei%20Liu%20and%20Haipeng%20Liu%20and%20Haichao%20Liu%20and%20Xin%20Liu%20and%20Jinxin%20Ni%20and%20Jun%20Ma&entry.1292438233=%20%20Human%20drivers%20adeptly%20navigate%20complex%20scenarios%20by%20utilizing%20rich%0Aattentional%20semantics%2C%20but%20the%20current%20autonomous%20systems%20struggle%20to%20replicate%0Athis%20ability%2C%20as%20they%20often%20lose%20critical%20semantic%20information%20when%20converting%0A2D%20observations%20into%203D%20space.%20In%20this%20sense%2C%20it%20hinders%20their%20effective%0Adeployment%20in%20dynamic%20and%20complex%20environments.%20Leveraging%20the%20superior%20scene%0Aunderstanding%20and%20reasoning%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20we%0Apropose%20VLM-E2E%2C%20a%20novel%20framework%20that%20uses%20the%20VLMs%20to%20enhance%20training%20by%0Aproviding%20attentional%20cues.%20Our%20method%20integrates%20textual%20representations%20into%0ABird%27s-Eye-View%20%28BEV%29%20features%20for%20semantic%20supervision%2C%20which%20enables%20the%0Amodel%20to%20learn%20richer%20feature%20representations%20that%20explicitly%20capture%20the%0Adriver%27s%20attentional%20semantics.%20By%20focusing%20on%20attentional%20semantics%2C%20VLM-E2E%0Abetter%20aligns%20with%20human-like%20driving%20behavior%2C%20which%20is%20critical%20for%0Anavigating%20dynamic%20and%20complex%20environments.%20Furthermore%2C%20we%20introduce%20a%0ABEV-Text%20learnable%20weighted%20fusion%20strategy%20to%20address%20the%20issue%20of%20modality%0Aimportance%20imbalance%20in%20fusing%20multimodal%20information.%20This%20approach%0Adynamically%20balances%20the%20contributions%20of%20BEV%20and%20text%20features%2C%20ensuring%20that%0Athe%20complementary%20information%20from%20visual%20and%20textual%20modalities%20is%20effectively%0Autilized.%20By%20explicitly%20addressing%20the%20imbalance%20in%20multimodal%20fusion%2C%20our%0Amethod%20facilitates%20a%20more%20holistic%20and%20robust%20representation%20of%20driving%0Aenvironments.%20We%20evaluate%20VLM-E2E%20on%20the%20nuScenes%20dataset%20and%20achieve%0Asignificant%20improvements%20in%20perception%2C%20prediction%2C%20and%20planning%20over%20the%0Abaseline%20end-to-end%20model%2C%20showcasing%20the%20effectiveness%20of%20our%0Aattention-enhanced%20BEV%20representation%20in%20enabling%20more%20accurate%20and%20reliable%0Aautonomous%20driving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18042v2&entry.124074799=Read"},
{"title": "Domain Generalization for In-Orbit 6D Pose Estimation", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  We address the problem of estimating the relative 6D pose, i.e., position and\norientation, of a target spacecraft, from a monocular image, a key capability\nfor future autonomous Rendezvous and Proximity Operations. Due to the\ndifficulty of acquiring large sets of real images, spacecraft pose estimation\nnetworks are exclusively trained on synthetic ones. However, because those\nimages do not capture the illumination conditions encountered in orbit, pose\nestimation networks face a domain gap problem, i.e., they do not generalize to\nreal images. Our work introduces a method that bridges this domain gap. It\nrelies on a novel, end-to-end, neural-based architecture as well as a novel\nlearning strategy. This strategy improves the domain generalization abilities\nof the network through multi-task learning and aggressive data augmentation\npolicies, thereby enforcing the network to learn domain-invariant features. We\ndemonstrate that our method effectively closes the domain gap, achieving\nstate-of-the-art accuracy on the widespread SPEED+ dataset. Finally, ablation\nstudies assess the impact of key components of our method on its generalization\nabilities.\n", "link": "http://arxiv.org/abs/2406.11743v2", "date": "2025-09-18", "relevancy": 2.3, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5903}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5692}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation&body=Title%3A%20Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20estimating%20the%20relative%206D%20pose%2C%20i.e.%2C%20position%20and%0Aorientation%2C%20of%20a%20target%20spacecraft%2C%20from%20a%20monocular%20image%2C%20a%20key%20capability%0Afor%20future%20autonomous%20Rendezvous%20and%20Proximity%20Operations.%20Due%20to%20the%0Adifficulty%20of%20acquiring%20large%20sets%20of%20real%20images%2C%20spacecraft%20pose%20estimation%0Anetworks%20are%20exclusively%20trained%20on%20synthetic%20ones.%20However%2C%20because%20those%0Aimages%20do%20not%20capture%20the%20illumination%20conditions%20encountered%20in%20orbit%2C%20pose%0Aestimation%20networks%20face%20a%20domain%20gap%20problem%2C%20i.e.%2C%20they%20do%20not%20generalize%20to%0Areal%20images.%20Our%20work%20introduces%20a%20method%20that%20bridges%20this%20domain%20gap.%20It%0Arelies%20on%20a%20novel%2C%20end-to-end%2C%20neural-based%20architecture%20as%20well%20as%20a%20novel%0Alearning%20strategy.%20This%20strategy%20improves%20the%20domain%20generalization%20abilities%0Aof%20the%20network%20through%20multi-task%20learning%20and%20aggressive%20data%20augmentation%0Apolicies%2C%20thereby%20enforcing%20the%20network%20to%20learn%20domain-invariant%20features.%20We%0Ademonstrate%20that%20our%20method%20effectively%20closes%20the%20domain%20gap%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20widespread%20SPEED%2B%20dataset.%20Finally%2C%20ablation%0Astudies%20assess%20the%20impact%20of%20key%20components%20of%20our%20method%20on%20its%20generalization%0Aabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11743v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520for%2520In-Orbit%25206D%2520Pose%2520Estimation%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520estimating%2520the%2520relative%25206D%2520pose%252C%2520i.e.%252C%2520position%2520and%250Aorientation%252C%2520of%2520a%2520target%2520spacecraft%252C%2520from%2520a%2520monocular%2520image%252C%2520a%2520key%2520capability%250Afor%2520future%2520autonomous%2520Rendezvous%2520and%2520Proximity%2520Operations.%2520Due%2520to%2520the%250Adifficulty%2520of%2520acquiring%2520large%2520sets%2520of%2520real%2520images%252C%2520spacecraft%2520pose%2520estimation%250Anetworks%2520are%2520exclusively%2520trained%2520on%2520synthetic%2520ones.%2520However%252C%2520because%2520those%250Aimages%2520do%2520not%2520capture%2520the%2520illumination%2520conditions%2520encountered%2520in%2520orbit%252C%2520pose%250Aestimation%2520networks%2520face%2520a%2520domain%2520gap%2520problem%252C%2520i.e.%252C%2520they%2520do%2520not%2520generalize%2520to%250Areal%2520images.%2520Our%2520work%2520introduces%2520a%2520method%2520that%2520bridges%2520this%2520domain%2520gap.%2520It%250Arelies%2520on%2520a%2520novel%252C%2520end-to-end%252C%2520neural-based%2520architecture%2520as%2520well%2520as%2520a%2520novel%250Alearning%2520strategy.%2520This%2520strategy%2520improves%2520the%2520domain%2520generalization%2520abilities%250Aof%2520the%2520network%2520through%2520multi-task%2520learning%2520and%2520aggressive%2520data%2520augmentation%250Apolicies%252C%2520thereby%2520enforcing%2520the%2520network%2520to%2520learn%2520domain-invariant%2520features.%2520We%250Ademonstrate%2520that%2520our%2520method%2520effectively%2520closes%2520the%2520domain%2520gap%252C%2520achieving%250Astate-of-the-art%2520accuracy%2520on%2520the%2520widespread%2520SPEED%252B%2520dataset.%2520Finally%252C%2520ablation%250Astudies%2520assess%2520the%2520impact%2520of%2520key%2520components%2520of%2520our%2520method%2520on%2520its%2520generalization%250Aabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11743v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20We%20address%20the%20problem%20of%20estimating%20the%20relative%206D%20pose%2C%20i.e.%2C%20position%20and%0Aorientation%2C%20of%20a%20target%20spacecraft%2C%20from%20a%20monocular%20image%2C%20a%20key%20capability%0Afor%20future%20autonomous%20Rendezvous%20and%20Proximity%20Operations.%20Due%20to%20the%0Adifficulty%20of%20acquiring%20large%20sets%20of%20real%20images%2C%20spacecraft%20pose%20estimation%0Anetworks%20are%20exclusively%20trained%20on%20synthetic%20ones.%20However%2C%20because%20those%0Aimages%20do%20not%20capture%20the%20illumination%20conditions%20encountered%20in%20orbit%2C%20pose%0Aestimation%20networks%20face%20a%20domain%20gap%20problem%2C%20i.e.%2C%20they%20do%20not%20generalize%20to%0Areal%20images.%20Our%20work%20introduces%20a%20method%20that%20bridges%20this%20domain%20gap.%20It%0Arelies%20on%20a%20novel%2C%20end-to-end%2C%20neural-based%20architecture%20as%20well%20as%20a%20novel%0Alearning%20strategy.%20This%20strategy%20improves%20the%20domain%20generalization%20abilities%0Aof%20the%20network%20through%20multi-task%20learning%20and%20aggressive%20data%20augmentation%0Apolicies%2C%20thereby%20enforcing%20the%20network%20to%20learn%20domain-invariant%20features.%20We%0Ademonstrate%20that%20our%20method%20effectively%20closes%20the%20domain%20gap%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20widespread%20SPEED%2B%20dataset.%20Finally%2C%20ablation%0Astudies%20assess%20the%20impact%20of%20key%20components%20of%20our%20method%20on%20its%20generalization%0Aabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11743v2&entry.124074799=Read"},
{"title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework\n  for Generalizable Image Super-Resolution", "author": "Hongjun Wang and Jiyuan Chen and Zhengwei Yin and Xuan Song and Yinqiang Zheng", "abstract": "  Generalizable Image Super-Resolution aims to enhance model generalization\ncapabilities under unknown degradations. To achieve this goal, the models are\nexpected to focus only on image content-related features instead of overfitting\ndegradations. Recently, numerous approaches such as Dropout and Feature\nAlignment have been proposed to suppress models' natural tendency to overfit\ndegradations and yield promising results. Nevertheless, these works have\nassumed that models overfit to all degradation types (e.g., blur, noise, JPEG),\nwhile through careful investigations in this paper, we discover that models\npredominantly overfit to noise, largely attributable to its distinct\ndegradation pattern compared to other degradation types. In this paper, we\npropose a targeted feature denoising framework, comprising noise detection and\ndenoising modules. Our approach presents a general solution that can be\nseamlessly integrated with existing super-resolution models without requiring\narchitectural modifications. Our framework demonstrates superior performance\ncompared to previous regularization-based methods across five traditional\nbenchmarks and datasets, encompassing both synthetic and real-world scenarios.\n", "link": "http://arxiv.org/abs/2509.14841v1", "date": "2025-09-18", "relevancy": 2.2889, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5897}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5624}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Degradations%20Are%20Equal%3A%20A%20Targeted%20Feature%20Denoising%20Framework%0A%20%20for%20Generalizable%20Image%20Super-Resolution&body=Title%3A%20Not%20All%20Degradations%20Are%20Equal%3A%20A%20Targeted%20Feature%20Denoising%20Framework%0A%20%20for%20Generalizable%20Image%20Super-Resolution%0AAuthor%3A%20Hongjun%20Wang%20and%20Jiyuan%20Chen%20and%20Zhengwei%20Yin%20and%20Xuan%20Song%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Generalizable%20Image%20Super-Resolution%20aims%20to%20enhance%20model%20generalization%0Acapabilities%20under%20unknown%20degradations.%20To%20achieve%20this%20goal%2C%20the%20models%20are%0Aexpected%20to%20focus%20only%20on%20image%20content-related%20features%20instead%20of%20overfitting%0Adegradations.%20Recently%2C%20numerous%20approaches%20such%20as%20Dropout%20and%20Feature%0AAlignment%20have%20been%20proposed%20to%20suppress%20models%27%20natural%20tendency%20to%20overfit%0Adegradations%20and%20yield%20promising%20results.%20Nevertheless%2C%20these%20works%20have%0Aassumed%20that%20models%20overfit%20to%20all%20degradation%20types%20%28e.g.%2C%20blur%2C%20noise%2C%20JPEG%29%2C%0Awhile%20through%20careful%20investigations%20in%20this%20paper%2C%20we%20discover%20that%20models%0Apredominantly%20overfit%20to%20noise%2C%20largely%20attributable%20to%20its%20distinct%0Adegradation%20pattern%20compared%20to%20other%20degradation%20types.%20In%20this%20paper%2C%20we%0Apropose%20a%20targeted%20feature%20denoising%20framework%2C%20comprising%20noise%20detection%20and%0Adenoising%20modules.%20Our%20approach%20presents%20a%20general%20solution%20that%20can%20be%0Aseamlessly%20integrated%20with%20existing%20super-resolution%20models%20without%20requiring%0Aarchitectural%20modifications.%20Our%20framework%20demonstrates%20superior%20performance%0Acompared%20to%20previous%20regularization-based%20methods%20across%20five%20traditional%0Abenchmarks%20and%20datasets%2C%20encompassing%20both%20synthetic%20and%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Degradations%2520Are%2520Equal%253A%2520A%2520Targeted%2520Feature%2520Denoising%2520Framework%250A%2520%2520for%2520Generalizable%2520Image%2520Super-Resolution%26entry.906535625%3DHongjun%2520Wang%2520and%2520Jiyuan%2520Chen%2520and%2520Zhengwei%2520Yin%2520and%2520Xuan%2520Song%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Generalizable%2520Image%2520Super-Resolution%2520aims%2520to%2520enhance%2520model%2520generalization%250Acapabilities%2520under%2520unknown%2520degradations.%2520To%2520achieve%2520this%2520goal%252C%2520the%2520models%2520are%250Aexpected%2520to%2520focus%2520only%2520on%2520image%2520content-related%2520features%2520instead%2520of%2520overfitting%250Adegradations.%2520Recently%252C%2520numerous%2520approaches%2520such%2520as%2520Dropout%2520and%2520Feature%250AAlignment%2520have%2520been%2520proposed%2520to%2520suppress%2520models%2527%2520natural%2520tendency%2520to%2520overfit%250Adegradations%2520and%2520yield%2520promising%2520results.%2520Nevertheless%252C%2520these%2520works%2520have%250Aassumed%2520that%2520models%2520overfit%2520to%2520all%2520degradation%2520types%2520%2528e.g.%252C%2520blur%252C%2520noise%252C%2520JPEG%2529%252C%250Awhile%2520through%2520careful%2520investigations%2520in%2520this%2520paper%252C%2520we%2520discover%2520that%2520models%250Apredominantly%2520overfit%2520to%2520noise%252C%2520largely%2520attributable%2520to%2520its%2520distinct%250Adegradation%2520pattern%2520compared%2520to%2520other%2520degradation%2520types.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520targeted%2520feature%2520denoising%2520framework%252C%2520comprising%2520noise%2520detection%2520and%250Adenoising%2520modules.%2520Our%2520approach%2520presents%2520a%2520general%2520solution%2520that%2520can%2520be%250Aseamlessly%2520integrated%2520with%2520existing%2520super-resolution%2520models%2520without%2520requiring%250Aarchitectural%2520modifications.%2520Our%2520framework%2520demonstrates%2520superior%2520performance%250Acompared%2520to%2520previous%2520regularization-based%2520methods%2520across%2520five%2520traditional%250Abenchmarks%2520and%2520datasets%252C%2520encompassing%2520both%2520synthetic%2520and%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Degradations%20Are%20Equal%3A%20A%20Targeted%20Feature%20Denoising%20Framework%0A%20%20for%20Generalizable%20Image%20Super-Resolution&entry.906535625=Hongjun%20Wang%20and%20Jiyuan%20Chen%20and%20Zhengwei%20Yin%20and%20Xuan%20Song%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Generalizable%20Image%20Super-Resolution%20aims%20to%20enhance%20model%20generalization%0Acapabilities%20under%20unknown%20degradations.%20To%20achieve%20this%20goal%2C%20the%20models%20are%0Aexpected%20to%20focus%20only%20on%20image%20content-related%20features%20instead%20of%20overfitting%0Adegradations.%20Recently%2C%20numerous%20approaches%20such%20as%20Dropout%20and%20Feature%0AAlignment%20have%20been%20proposed%20to%20suppress%20models%27%20natural%20tendency%20to%20overfit%0Adegradations%20and%20yield%20promising%20results.%20Nevertheless%2C%20these%20works%20have%0Aassumed%20that%20models%20overfit%20to%20all%20degradation%20types%20%28e.g.%2C%20blur%2C%20noise%2C%20JPEG%29%2C%0Awhile%20through%20careful%20investigations%20in%20this%20paper%2C%20we%20discover%20that%20models%0Apredominantly%20overfit%20to%20noise%2C%20largely%20attributable%20to%20its%20distinct%0Adegradation%20pattern%20compared%20to%20other%20degradation%20types.%20In%20this%20paper%2C%20we%0Apropose%20a%20targeted%20feature%20denoising%20framework%2C%20comprising%20noise%20detection%20and%0Adenoising%20modules.%20Our%20approach%20presents%20a%20general%20solution%20that%20can%20be%0Aseamlessly%20integrated%20with%20existing%20super-resolution%20models%20without%20requiring%0Aarchitectural%20modifications.%20Our%20framework%20demonstrates%20superior%20performance%0Acompared%20to%20previous%20regularization-based%20methods%20across%20five%20traditional%0Abenchmarks%20and%20datasets%2C%20encompassing%20both%20synthetic%20and%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14841v1&entry.124074799=Read"},
{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning", "author": "Chi-Pin Huang and Yueh-Hua Wu and Min-Hung Chen and Yu-Chiang Frank Wang and Fu-En Yang", "abstract": "  Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.\n", "link": "http://arxiv.org/abs/2507.16815v2", "date": "2025-09-18", "relevancy": 2.2875, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5732}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning&body=Title%3A%20ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning%0AAuthor%3A%20Chi-Pin%20Huang%20and%20Yueh-Hua%20Wu%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fu-En%20Yang%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20reasoning%20tasks%20require%20agents%20to%20interpret%0Amultimodal%20instructions%2C%20perform%20long-horizon%20planning%2C%20and%20act%20adaptively%20in%0Adynamic%20environments.%20Existing%20approaches%20typically%20train%20VLA%20models%20in%20an%0Aend-to-end%20fashion%2C%20directly%20mapping%20inputs%20to%20actions%20without%20explicit%0Areasoning%2C%20which%20hinders%20their%20ability%20to%20plan%20over%20multiple%20steps%20or%20adapt%20to%0Acomplex%20task%20variations.%20In%20this%20paper%2C%20we%20propose%20ThinkAct%2C%20a%20dual-system%0Aframework%20that%20bridges%20high-level%20reasoning%20with%20low-level%20action%20execution%20via%0Areinforced%20visual%20latent%20planning.%20ThinkAct%20trains%20a%20multimodal%20LLM%20to%20generate%0Aembodied%20reasoning%20plans%20guided%20by%20reinforcing%20action-aligned%20visual%20rewards%0Abased%20on%20goal%20completion%20and%20trajectory%20consistency.%20These%20reasoning%20plans%20are%0Acompressed%20into%20a%20visual%20plan%20latent%20that%20conditions%20a%20downstream%20action%20model%0Afor%20robust%20action%20execution%20on%20target%20environments.%20Extensive%20experiments%20on%0Aembodied%20reasoning%20and%20robot%20manipulation%20benchmarks%20demonstrate%20that%20ThinkAct%0Aenables%20few-shot%20adaptation%2C%20long-horizon%20planning%2C%20and%20self-correction%0Abehaviors%20in%20complex%20embodied%20AI%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinkAct%253A%2520Vision-Language-Action%2520Reasoning%2520via%2520Reinforced%2520Visual%2520Latent%250A%2520%2520Planning%26entry.906535625%3DChi-Pin%2520Huang%2520and%2520Yueh-Hua%2520Wu%2520and%2520Min-Hung%2520Chen%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Fu-En%2520Yang%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520reasoning%2520tasks%2520require%2520agents%2520to%2520interpret%250Amultimodal%2520instructions%252C%2520perform%2520long-horizon%2520planning%252C%2520and%2520act%2520adaptively%2520in%250Adynamic%2520environments.%2520Existing%2520approaches%2520typically%2520train%2520VLA%2520models%2520in%2520an%250Aend-to-end%2520fashion%252C%2520directly%2520mapping%2520inputs%2520to%2520actions%2520without%2520explicit%250Areasoning%252C%2520which%2520hinders%2520their%2520ability%2520to%2520plan%2520over%2520multiple%2520steps%2520or%2520adapt%2520to%250Acomplex%2520task%2520variations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ThinkAct%252C%2520a%2520dual-system%250Aframework%2520that%2520bridges%2520high-level%2520reasoning%2520with%2520low-level%2520action%2520execution%2520via%250Areinforced%2520visual%2520latent%2520planning.%2520ThinkAct%2520trains%2520a%2520multimodal%2520LLM%2520to%2520generate%250Aembodied%2520reasoning%2520plans%2520guided%2520by%2520reinforcing%2520action-aligned%2520visual%2520rewards%250Abased%2520on%2520goal%2520completion%2520and%2520trajectory%2520consistency.%2520These%2520reasoning%2520plans%2520are%250Acompressed%2520into%2520a%2520visual%2520plan%2520latent%2520that%2520conditions%2520a%2520downstream%2520action%2520model%250Afor%2520robust%2520action%2520execution%2520on%2520target%2520environments.%2520Extensive%2520experiments%2520on%250Aembodied%2520reasoning%2520and%2520robot%2520manipulation%2520benchmarks%2520demonstrate%2520that%2520ThinkAct%250Aenables%2520few-shot%2520adaptation%252C%2520long-horizon%2520planning%252C%2520and%2520self-correction%250Abehaviors%2520in%2520complex%2520embodied%2520AI%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning&entry.906535625=Chi-Pin%20Huang%20and%20Yueh-Hua%20Wu%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fu-En%20Yang&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20reasoning%20tasks%20require%20agents%20to%20interpret%0Amultimodal%20instructions%2C%20perform%20long-horizon%20planning%2C%20and%20act%20adaptively%20in%0Adynamic%20environments.%20Existing%20approaches%20typically%20train%20VLA%20models%20in%20an%0Aend-to-end%20fashion%2C%20directly%20mapping%20inputs%20to%20actions%20without%20explicit%0Areasoning%2C%20which%20hinders%20their%20ability%20to%20plan%20over%20multiple%20steps%20or%20adapt%20to%0Acomplex%20task%20variations.%20In%20this%20paper%2C%20we%20propose%20ThinkAct%2C%20a%20dual-system%0Aframework%20that%20bridges%20high-level%20reasoning%20with%20low-level%20action%20execution%20via%0Areinforced%20visual%20latent%20planning.%20ThinkAct%20trains%20a%20multimodal%20LLM%20to%20generate%0Aembodied%20reasoning%20plans%20guided%20by%20reinforcing%20action-aligned%20visual%20rewards%0Abased%20on%20goal%20completion%20and%20trajectory%20consistency.%20These%20reasoning%20plans%20are%0Acompressed%20into%20a%20visual%20plan%20latent%20that%20conditions%20a%20downstream%20action%20model%0Afor%20robust%20action%20execution%20on%20target%20environments.%20Extensive%20experiments%20on%0Aembodied%20reasoning%20and%20robot%20manipulation%20benchmarks%20demonstrate%20that%20ThinkAct%0Aenables%20few-shot%20adaptation%2C%20long-horizon%20planning%2C%20and%20self-correction%0Abehaviors%20in%20complex%20embodied%20AI%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16815v2&entry.124074799=Read"},
{"title": "Next-Depth Lookahead Tree", "author": "Jaeho Lee and Kangjin Kim and Gyeong Taek Lee", "abstract": "  This paper proposes the Next-Depth Lookahead Tree (NDLT), a single-tree model\ndesigned to improve performance by evaluating node splits not only at the node\nbeing optimized but also by evaluating the quality of the next depth level.\n", "link": "http://arxiv.org/abs/2509.15143v1", "date": "2025-09-18", "relevancy": 2.287, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4932}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next-Depth%20Lookahead%20Tree&body=Title%3A%20Next-Depth%20Lookahead%20Tree%0AAuthor%3A%20Jaeho%20Lee%20and%20Kangjin%20Kim%20and%20Gyeong%20Taek%20Lee%0AAbstract%3A%20%20%20This%20paper%20proposes%20the%20Next-Depth%20Lookahead%20Tree%20%28NDLT%29%2C%20a%20single-tree%20model%0Adesigned%20to%20improve%20performance%20by%20evaluating%20node%20splits%20not%20only%20at%20the%20node%0Abeing%20optimized%20but%20also%20by%20evaluating%20the%20quality%20of%20the%20next%20depth%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext-Depth%2520Lookahead%2520Tree%26entry.906535625%3DJaeho%2520Lee%2520and%2520Kangjin%2520Kim%2520and%2520Gyeong%2520Taek%2520Lee%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520the%2520Next-Depth%2520Lookahead%2520Tree%2520%2528NDLT%2529%252C%2520a%2520single-tree%2520model%250Adesigned%2520to%2520improve%2520performance%2520by%2520evaluating%2520node%2520splits%2520not%2520only%2520at%2520the%2520node%250Abeing%2520optimized%2520but%2520also%2520by%2520evaluating%2520the%2520quality%2520of%2520the%2520next%2520depth%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next-Depth%20Lookahead%20Tree&entry.906535625=Jaeho%20Lee%20and%20Kangjin%20Kim%20and%20Gyeong%20Taek%20Lee&entry.1292438233=%20%20This%20paper%20proposes%20the%20Next-Depth%20Lookahead%20Tree%20%28NDLT%29%2C%20a%20single-tree%20model%0Adesigned%20to%20improve%20performance%20by%20evaluating%20node%20splits%20not%20only%20at%20the%20node%0Abeing%20optimized%20but%20also%20by%20evaluating%20the%20quality%20of%20the%20next%20depth%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15143v1&entry.124074799=Read"},
{"title": "Sampling Method for Generalized Graph Signals with Pre-selected Vertices\n  via DC Optimization", "author": "Keitaro Yamashita and Kazuki Naganuma and Shunsuke Ono", "abstract": "  This paper proposes a method for vertex-wise flexible sampling of a broad\nclass of graph signals, designed to attain the best possible recovery based on\nthe generalized sampling theory. This is achieved by designing a sampling\noperator by an optimization problem, which is inherently non-convex, as the\nbest possible recovery imposes a rank constraint. An existing method for\nvertex-wise flexible sampling is able to control the number of active vertices\nbut cannot incorporate prior knowledge of mandatory or forbidden vertices. To\naddress these challenges, we formulate the operator design as a problem that\nhandles a constraint of the number of active vertices and prior knowledge on\nspecific vertices for sampling, mandatory inclusion or exclusion. We\ntransformed this constrained problem into a difference-of-convex (DC)\noptimization problem by using the nuclear norm and a DC penalty for vertex\nselection. To solve this, we develop a convergent solver based on the general\ndouble-proximal gradient DC algorithm. The effectiveness of our method is\ndemonstrated through experiments on various graph signal models, including\nreal-world data, showing superior performance in the recovery accuracy by\ncomparing to existing methods.\n", "link": "http://arxiv.org/abs/2509.14836v1", "date": "2025-09-18", "relevancy": 2.2834, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4776}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20Method%20for%20Generalized%20Graph%20Signals%20with%20Pre-selected%20Vertices%0A%20%20via%20DC%20Optimization&body=Title%3A%20Sampling%20Method%20for%20Generalized%20Graph%20Signals%20with%20Pre-selected%20Vertices%0A%20%20via%20DC%20Optimization%0AAuthor%3A%20Keitaro%20Yamashita%20and%20Kazuki%20Naganuma%20and%20Shunsuke%20Ono%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20method%20for%20vertex-wise%20flexible%20sampling%20of%20a%20broad%0Aclass%20of%20graph%20signals%2C%20designed%20to%20attain%20the%20best%20possible%20recovery%20based%20on%0Athe%20generalized%20sampling%20theory.%20This%20is%20achieved%20by%20designing%20a%20sampling%0Aoperator%20by%20an%20optimization%20problem%2C%20which%20is%20inherently%20non-convex%2C%20as%20the%0Abest%20possible%20recovery%20imposes%20a%20rank%20constraint.%20An%20existing%20method%20for%0Avertex-wise%20flexible%20sampling%20is%20able%20to%20control%20the%20number%20of%20active%20vertices%0Abut%20cannot%20incorporate%20prior%20knowledge%20of%20mandatory%20or%20forbidden%20vertices.%20To%0Aaddress%20these%20challenges%2C%20we%20formulate%20the%20operator%20design%20as%20a%20problem%20that%0Ahandles%20a%20constraint%20of%20the%20number%20of%20active%20vertices%20and%20prior%20knowledge%20on%0Aspecific%20vertices%20for%20sampling%2C%20mandatory%20inclusion%20or%20exclusion.%20We%0Atransformed%20this%20constrained%20problem%20into%20a%20difference-of-convex%20%28DC%29%0Aoptimization%20problem%20by%20using%20the%20nuclear%20norm%20and%20a%20DC%20penalty%20for%20vertex%0Aselection.%20To%20solve%20this%2C%20we%20develop%20a%20convergent%20solver%20based%20on%20the%20general%0Adouble-proximal%20gradient%20DC%20algorithm.%20The%20effectiveness%20of%20our%20method%20is%0Ademonstrated%20through%20experiments%20on%20various%20graph%20signal%20models%2C%20including%0Areal-world%20data%2C%20showing%20superior%20performance%20in%20the%20recovery%20accuracy%20by%0Acomparing%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520Method%2520for%2520Generalized%2520Graph%2520Signals%2520with%2520Pre-selected%2520Vertices%250A%2520%2520via%2520DC%2520Optimization%26entry.906535625%3DKeitaro%2520Yamashita%2520and%2520Kazuki%2520Naganuma%2520and%2520Shunsuke%2520Ono%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520method%2520for%2520vertex-wise%2520flexible%2520sampling%2520of%2520a%2520broad%250Aclass%2520of%2520graph%2520signals%252C%2520designed%2520to%2520attain%2520the%2520best%2520possible%2520recovery%2520based%2520on%250Athe%2520generalized%2520sampling%2520theory.%2520This%2520is%2520achieved%2520by%2520designing%2520a%2520sampling%250Aoperator%2520by%2520an%2520optimization%2520problem%252C%2520which%2520is%2520inherently%2520non-convex%252C%2520as%2520the%250Abest%2520possible%2520recovery%2520imposes%2520a%2520rank%2520constraint.%2520An%2520existing%2520method%2520for%250Avertex-wise%2520flexible%2520sampling%2520is%2520able%2520to%2520control%2520the%2520number%2520of%2520active%2520vertices%250Abut%2520cannot%2520incorporate%2520prior%2520knowledge%2520of%2520mandatory%2520or%2520forbidden%2520vertices.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520formulate%2520the%2520operator%2520design%2520as%2520a%2520problem%2520that%250Ahandles%2520a%2520constraint%2520of%2520the%2520number%2520of%2520active%2520vertices%2520and%2520prior%2520knowledge%2520on%250Aspecific%2520vertices%2520for%2520sampling%252C%2520mandatory%2520inclusion%2520or%2520exclusion.%2520We%250Atransformed%2520this%2520constrained%2520problem%2520into%2520a%2520difference-of-convex%2520%2528DC%2529%250Aoptimization%2520problem%2520by%2520using%2520the%2520nuclear%2520norm%2520and%2520a%2520DC%2520penalty%2520for%2520vertex%250Aselection.%2520To%2520solve%2520this%252C%2520we%2520develop%2520a%2520convergent%2520solver%2520based%2520on%2520the%2520general%250Adouble-proximal%2520gradient%2520DC%2520algorithm.%2520The%2520effectiveness%2520of%2520our%2520method%2520is%250Ademonstrated%2520through%2520experiments%2520on%2520various%2520graph%2520signal%2520models%252C%2520including%250Areal-world%2520data%252C%2520showing%2520superior%2520performance%2520in%2520the%2520recovery%2520accuracy%2520by%250Acomparing%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20Method%20for%20Generalized%20Graph%20Signals%20with%20Pre-selected%20Vertices%0A%20%20via%20DC%20Optimization&entry.906535625=Keitaro%20Yamashita%20and%20Kazuki%20Naganuma%20and%20Shunsuke%20Ono&entry.1292438233=%20%20This%20paper%20proposes%20a%20method%20for%20vertex-wise%20flexible%20sampling%20of%20a%20broad%0Aclass%20of%20graph%20signals%2C%20designed%20to%20attain%20the%20best%20possible%20recovery%20based%20on%0Athe%20generalized%20sampling%20theory.%20This%20is%20achieved%20by%20designing%20a%20sampling%0Aoperator%20by%20an%20optimization%20problem%2C%20which%20is%20inherently%20non-convex%2C%20as%20the%0Abest%20possible%20recovery%20imposes%20a%20rank%20constraint.%20An%20existing%20method%20for%0Avertex-wise%20flexible%20sampling%20is%20able%20to%20control%20the%20number%20of%20active%20vertices%0Abut%20cannot%20incorporate%20prior%20knowledge%20of%20mandatory%20or%20forbidden%20vertices.%20To%0Aaddress%20these%20challenges%2C%20we%20formulate%20the%20operator%20design%20as%20a%20problem%20that%0Ahandles%20a%20constraint%20of%20the%20number%20of%20active%20vertices%20and%20prior%20knowledge%20on%0Aspecific%20vertices%20for%20sampling%2C%20mandatory%20inclusion%20or%20exclusion.%20We%0Atransformed%20this%20constrained%20problem%20into%20a%20difference-of-convex%20%28DC%29%0Aoptimization%20problem%20by%20using%20the%20nuclear%20norm%20and%20a%20DC%20penalty%20for%20vertex%0Aselection.%20To%20solve%20this%2C%20we%20develop%20a%20convergent%20solver%20based%20on%20the%20general%0Adouble-proximal%20gradient%20DC%20algorithm.%20The%20effectiveness%20of%20our%20method%20is%0Ademonstrated%20through%20experiments%20on%20various%20graph%20signal%20models%2C%20including%0Areal-world%20data%2C%20showing%20superior%20performance%20in%20the%20recovery%20accuracy%20by%0Acomparing%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14836v1&entry.124074799=Read"},
{"title": "A new dataset and comparison for multi-camera frame synthesis", "author": "Conall Daly and Anil Kokaram", "abstract": "  Many methods exist for frame synthesis in image sequences but can be broadly\ncategorised into frame interpolation and view synthesis techniques.\nFundamentally, both frame interpolation and view synthesis tackle the same\ntask, interpolating a frame given surrounding frames in time or space. However,\nmost frame interpolation datasets focus on temporal aspects with single cameras\nmoving through time and space, while view synthesis datasets are typically\nbiased toward stereoscopic depth estimation use cases. This makes direct\ncomparison between view synthesis and frame interpolation methods challenging.\nIn this paper, we develop a novel multi-camera dataset using a custom-built\ndense linear camera array to enable fair comparison between these approaches.\nWe evaluate classical and deep learning frame interpolators against a view\nsynthesis method (3D Gaussian Splatting) for the task of view in-betweening.\nOur results reveal that deep learning methods do not significantly outperform\nclassical methods on real image data, with 3D Gaussian Splatting actually\nunderperforming frame interpolators by as much as 3.5 dB PSNR. However, in\nsynthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms\nframe interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.\n", "link": "http://arxiv.org/abs/2508.09068v2", "date": "2025-09-18", "relevancy": 2.2809, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5884}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5725}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis&body=Title%3A%20A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis%0AAuthor%3A%20Conall%20Daly%20and%20Anil%20Kokaram%0AAbstract%3A%20%20%20Many%20methods%20exist%20for%20frame%20synthesis%20in%20image%20sequences%20but%20can%20be%20broadly%0Acategorised%20into%20frame%20interpolation%20and%20view%20synthesis%20techniques.%0AFundamentally%2C%20both%20frame%20interpolation%20and%20view%20synthesis%20tackle%20the%20same%0Atask%2C%20interpolating%20a%20frame%20given%20surrounding%20frames%20in%20time%20or%20space.%20However%2C%0Amost%20frame%20interpolation%20datasets%20focus%20on%20temporal%20aspects%20with%20single%20cameras%0Amoving%20through%20time%20and%20space%2C%20while%20view%20synthesis%20datasets%20are%20typically%0Abiased%20toward%20stereoscopic%20depth%20estimation%20use%20cases.%20This%20makes%20direct%0Acomparison%20between%20view%20synthesis%20and%20frame%20interpolation%20methods%20challenging.%0AIn%20this%20paper%2C%20we%20develop%20a%20novel%20multi-camera%20dataset%20using%20a%20custom-built%0Adense%20linear%20camera%20array%20to%20enable%20fair%20comparison%20between%20these%20approaches.%0AWe%20evaluate%20classical%20and%20deep%20learning%20frame%20interpolators%20against%20a%20view%0Asynthesis%20method%20%283D%20Gaussian%20Splatting%29%20for%20the%20task%20of%20view%20in-betweening.%0AOur%20results%20reveal%20that%20deep%20learning%20methods%20do%20not%20significantly%20outperform%0Aclassical%20methods%20on%20real%20image%20data%2C%20with%203D%20Gaussian%20Splatting%20actually%0Aunderperforming%20frame%20interpolators%20by%20as%20much%20as%203.5%20dB%20PSNR.%20However%2C%20in%0Asynthetic%20scenes%2C%20the%20situation%20reverses%20--%203D%20Gaussian%20Splatting%20outperforms%0Aframe%20interpolation%20algorithms%20by%20almost%205%20dB%20PSNR%20at%20a%2095%25%20confidence%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520dataset%2520and%2520comparison%2520for%2520multi-camera%2520frame%2520synthesis%26entry.906535625%3DConall%2520Daly%2520and%2520Anil%2520Kokaram%26entry.1292438233%3D%2520%2520Many%2520methods%2520exist%2520for%2520frame%2520synthesis%2520in%2520image%2520sequences%2520but%2520can%2520be%2520broadly%250Acategorised%2520into%2520frame%2520interpolation%2520and%2520view%2520synthesis%2520techniques.%250AFundamentally%252C%2520both%2520frame%2520interpolation%2520and%2520view%2520synthesis%2520tackle%2520the%2520same%250Atask%252C%2520interpolating%2520a%2520frame%2520given%2520surrounding%2520frames%2520in%2520time%2520or%2520space.%2520However%252C%250Amost%2520frame%2520interpolation%2520datasets%2520focus%2520on%2520temporal%2520aspects%2520with%2520single%2520cameras%250Amoving%2520through%2520time%2520and%2520space%252C%2520while%2520view%2520synthesis%2520datasets%2520are%2520typically%250Abiased%2520toward%2520stereoscopic%2520depth%2520estimation%2520use%2520cases.%2520This%2520makes%2520direct%250Acomparison%2520between%2520view%2520synthesis%2520and%2520frame%2520interpolation%2520methods%2520challenging.%250AIn%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520multi-camera%2520dataset%2520using%2520a%2520custom-built%250Adense%2520linear%2520camera%2520array%2520to%2520enable%2520fair%2520comparison%2520between%2520these%2520approaches.%250AWe%2520evaluate%2520classical%2520and%2520deep%2520learning%2520frame%2520interpolators%2520against%2520a%2520view%250Asynthesis%2520method%2520%25283D%2520Gaussian%2520Splatting%2529%2520for%2520the%2520task%2520of%2520view%2520in-betweening.%250AOur%2520results%2520reveal%2520that%2520deep%2520learning%2520methods%2520do%2520not%2520significantly%2520outperform%250Aclassical%2520methods%2520on%2520real%2520image%2520data%252C%2520with%25203D%2520Gaussian%2520Splatting%2520actually%250Aunderperforming%2520frame%2520interpolators%2520by%2520as%2520much%2520as%25203.5%2520dB%2520PSNR.%2520However%252C%2520in%250Asynthetic%2520scenes%252C%2520the%2520situation%2520reverses%2520--%25203D%2520Gaussian%2520Splatting%2520outperforms%250Aframe%2520interpolation%2520algorithms%2520by%2520almost%25205%2520dB%2520PSNR%2520at%2520a%252095%2525%2520confidence%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20dataset%20and%20comparison%20for%20multi-camera%20frame%20synthesis&entry.906535625=Conall%20Daly%20and%20Anil%20Kokaram&entry.1292438233=%20%20Many%20methods%20exist%20for%20frame%20synthesis%20in%20image%20sequences%20but%20can%20be%20broadly%0Acategorised%20into%20frame%20interpolation%20and%20view%20synthesis%20techniques.%0AFundamentally%2C%20both%20frame%20interpolation%20and%20view%20synthesis%20tackle%20the%20same%0Atask%2C%20interpolating%20a%20frame%20given%20surrounding%20frames%20in%20time%20or%20space.%20However%2C%0Amost%20frame%20interpolation%20datasets%20focus%20on%20temporal%20aspects%20with%20single%20cameras%0Amoving%20through%20time%20and%20space%2C%20while%20view%20synthesis%20datasets%20are%20typically%0Abiased%20toward%20stereoscopic%20depth%20estimation%20use%20cases.%20This%20makes%20direct%0Acomparison%20between%20view%20synthesis%20and%20frame%20interpolation%20methods%20challenging.%0AIn%20this%20paper%2C%20we%20develop%20a%20novel%20multi-camera%20dataset%20using%20a%20custom-built%0Adense%20linear%20camera%20array%20to%20enable%20fair%20comparison%20between%20these%20approaches.%0AWe%20evaluate%20classical%20and%20deep%20learning%20frame%20interpolators%20against%20a%20view%0Asynthesis%20method%20%283D%20Gaussian%20Splatting%29%20for%20the%20task%20of%20view%20in-betweening.%0AOur%20results%20reveal%20that%20deep%20learning%20methods%20do%20not%20significantly%20outperform%0Aclassical%20methods%20on%20real%20image%20data%2C%20with%203D%20Gaussian%20Splatting%20actually%0Aunderperforming%20frame%20interpolators%20by%20as%20much%20as%203.5%20dB%20PSNR.%20However%2C%20in%0Asynthetic%20scenes%2C%20the%20situation%20reverses%20--%203D%20Gaussian%20Splatting%20outperforms%0Aframe%20interpolation%20algorithms%20by%20almost%205%20dB%20PSNR%20at%20a%2095%25%20confidence%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09068v2&entry.124074799=Read"},
{"title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images\n  Pretrained Model", "author": "Pak-Hei Yeung and Jayroop Ramesh and Pengfei Lyu and Ana Namburete and Jagath Rajapakse", "abstract": "  This paper explores the transfer of knowledge from general vision models\npretrained on 2D natural images to improve 3D medical image segmentation. We\nfocus on the semi-supervised setting, where only a few labeled 3D medical\nimages are available, along with a large set of unlabeled images. To tackle\nthis, we propose a model-agnostic framework that progressively distills\nknowledge from a 2D pretrained model to a 3D segmentation model trained from\nscratch. Our approach, M&N, involves iterative co-training of the two models\nusing pseudo-masks generated by each other, along with our proposed learning\nrate guided sampling that adaptively adjusts the proportion of labeled and\nunlabeled data in each training batch to align with the models' prediction\naccuracy and stability, minimizing the adverse effect caused by inaccurate\npseudo-masks. Extensive experiments on multiple publicly available datasets\ndemonstrate that M&N achieves state-of-the-art performance, outperforming\nthirteen existing semi-supervised segmentation approaches under all different\nsettings. Importantly, ablation studies show that M&N remains model-agnostic,\nallowing seamless integration with different architectures. This ensures its\nadaptability as more advanced models emerge. The code is available at\nhttps://github.com/pakheiyeung/M-N.\n", "link": "http://arxiv.org/abs/2509.15167v1", "date": "2025-09-18", "relevancy": 2.276, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.567}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%203D%20Medical%20Segmentation%20from%202D%20Natural%20Images%0A%20%20Pretrained%20Model&body=Title%3A%20Semi-Supervised%203D%20Medical%20Segmentation%20from%202D%20Natural%20Images%0A%20%20Pretrained%20Model%0AAuthor%3A%20Pak-Hei%20Yeung%20and%20Jayroop%20Ramesh%20and%20Pengfei%20Lyu%20and%20Ana%20Namburete%20and%20Jagath%20Rajapakse%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20transfer%20of%20knowledge%20from%20general%20vision%20models%0Apretrained%20on%202D%20natural%20images%20to%20improve%203D%20medical%20image%20segmentation.%20We%0Afocus%20on%20the%20semi-supervised%20setting%2C%20where%20only%20a%20few%20labeled%203D%20medical%0Aimages%20are%20available%2C%20along%20with%20a%20large%20set%20of%20unlabeled%20images.%20To%20tackle%0Athis%2C%20we%20propose%20a%20model-agnostic%20framework%20that%20progressively%20distills%0Aknowledge%20from%20a%202D%20pretrained%20model%20to%20a%203D%20segmentation%20model%20trained%20from%0Ascratch.%20Our%20approach%2C%20M%26N%2C%20involves%20iterative%20co-training%20of%20the%20two%20models%0Ausing%20pseudo-masks%20generated%20by%20each%20other%2C%20along%20with%20our%20proposed%20learning%0Arate%20guided%20sampling%20that%20adaptively%20adjusts%20the%20proportion%20of%20labeled%20and%0Aunlabeled%20data%20in%20each%20training%20batch%20to%20align%20with%20the%20models%27%20prediction%0Aaccuracy%20and%20stability%2C%20minimizing%20the%20adverse%20effect%20caused%20by%20inaccurate%0Apseudo-masks.%20Extensive%20experiments%20on%20multiple%20publicly%20available%20datasets%0Ademonstrate%20that%20M%26N%20achieves%20state-of-the-art%20performance%2C%20outperforming%0Athirteen%20existing%20semi-supervised%20segmentation%20approaches%20under%20all%20different%0Asettings.%20Importantly%2C%20ablation%20studies%20show%20that%20M%26N%20remains%20model-agnostic%2C%0Aallowing%20seamless%20integration%20with%20different%20architectures.%20This%20ensures%20its%0Aadaptability%20as%20more%20advanced%20models%20emerge.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/pakheiyeung/M-N.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%25203D%2520Medical%2520Segmentation%2520from%25202D%2520Natural%2520Images%250A%2520%2520Pretrained%2520Model%26entry.906535625%3DPak-Hei%2520Yeung%2520and%2520Jayroop%2520Ramesh%2520and%2520Pengfei%2520Lyu%2520and%2520Ana%2520Namburete%2520and%2520Jagath%2520Rajapakse%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520transfer%2520of%2520knowledge%2520from%2520general%2520vision%2520models%250Apretrained%2520on%25202D%2520natural%2520images%2520to%2520improve%25203D%2520medical%2520image%2520segmentation.%2520We%250Afocus%2520on%2520the%2520semi-supervised%2520setting%252C%2520where%2520only%2520a%2520few%2520labeled%25203D%2520medical%250Aimages%2520are%2520available%252C%2520along%2520with%2520a%2520large%2520set%2520of%2520unlabeled%2520images.%2520To%2520tackle%250Athis%252C%2520we%2520propose%2520a%2520model-agnostic%2520framework%2520that%2520progressively%2520distills%250Aknowledge%2520from%2520a%25202D%2520pretrained%2520model%2520to%2520a%25203D%2520segmentation%2520model%2520trained%2520from%250Ascratch.%2520Our%2520approach%252C%2520M%2526N%252C%2520involves%2520iterative%2520co-training%2520of%2520the%2520two%2520models%250Ausing%2520pseudo-masks%2520generated%2520by%2520each%2520other%252C%2520along%2520with%2520our%2520proposed%2520learning%250Arate%2520guided%2520sampling%2520that%2520adaptively%2520adjusts%2520the%2520proportion%2520of%2520labeled%2520and%250Aunlabeled%2520data%2520in%2520each%2520training%2520batch%2520to%2520align%2520with%2520the%2520models%2527%2520prediction%250Aaccuracy%2520and%2520stability%252C%2520minimizing%2520the%2520adverse%2520effect%2520caused%2520by%2520inaccurate%250Apseudo-masks.%2520Extensive%2520experiments%2520on%2520multiple%2520publicly%2520available%2520datasets%250Ademonstrate%2520that%2520M%2526N%2520achieves%2520state-of-the-art%2520performance%252C%2520outperforming%250Athirteen%2520existing%2520semi-supervised%2520segmentation%2520approaches%2520under%2520all%2520different%250Asettings.%2520Importantly%252C%2520ablation%2520studies%2520show%2520that%2520M%2526N%2520remains%2520model-agnostic%252C%250Aallowing%2520seamless%2520integration%2520with%2520different%2520architectures.%2520This%2520ensures%2520its%250Aadaptability%2520as%2520more%2520advanced%2520models%2520emerge.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/pakheiyeung/M-N.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%203D%20Medical%20Segmentation%20from%202D%20Natural%20Images%0A%20%20Pretrained%20Model&entry.906535625=Pak-Hei%20Yeung%20and%20Jayroop%20Ramesh%20and%20Pengfei%20Lyu%20and%20Ana%20Namburete%20and%20Jagath%20Rajapakse&entry.1292438233=%20%20This%20paper%20explores%20the%20transfer%20of%20knowledge%20from%20general%20vision%20models%0Apretrained%20on%202D%20natural%20images%20to%20improve%203D%20medical%20image%20segmentation.%20We%0Afocus%20on%20the%20semi-supervised%20setting%2C%20where%20only%20a%20few%20labeled%203D%20medical%0Aimages%20are%20available%2C%20along%20with%20a%20large%20set%20of%20unlabeled%20images.%20To%20tackle%0Athis%2C%20we%20propose%20a%20model-agnostic%20framework%20that%20progressively%20distills%0Aknowledge%20from%20a%202D%20pretrained%20model%20to%20a%203D%20segmentation%20model%20trained%20from%0Ascratch.%20Our%20approach%2C%20M%26N%2C%20involves%20iterative%20co-training%20of%20the%20two%20models%0Ausing%20pseudo-masks%20generated%20by%20each%20other%2C%20along%20with%20our%20proposed%20learning%0Arate%20guided%20sampling%20that%20adaptively%20adjusts%20the%20proportion%20of%20labeled%20and%0Aunlabeled%20data%20in%20each%20training%20batch%20to%20align%20with%20the%20models%27%20prediction%0Aaccuracy%20and%20stability%2C%20minimizing%20the%20adverse%20effect%20caused%20by%20inaccurate%0Apseudo-masks.%20Extensive%20experiments%20on%20multiple%20publicly%20available%20datasets%0Ademonstrate%20that%20M%26N%20achieves%20state-of-the-art%20performance%2C%20outperforming%0Athirteen%20existing%20semi-supervised%20segmentation%20approaches%20under%20all%20different%0Asettings.%20Importantly%2C%20ablation%20studies%20show%20that%20M%26N%20remains%20model-agnostic%2C%0Aallowing%20seamless%20integration%20with%20different%20architectures.%20This%20ensures%20its%0Aadaptability%20as%20more%20advanced%20models%20emerge.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/pakheiyeung/M-N.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15167v1&entry.124074799=Read"},
{"title": "OmniSync: Towards Universal Lip Synchronization via Diffusion\n  Transformers", "author": "Ziqiao Peng and Jiwen Liu and Haoxian Zhang and Xiaoqiang Liu and Songlin Tang and Pengfei Wan and Di Zhang and Hongyan Liu and Jun He", "abstract": "  Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.\n", "link": "http://arxiv.org/abs/2505.21448v2", "date": "2025-09-18", "relevancy": 2.2722, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5865}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5674}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers&body=Title%3A%20OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Ziqiao%20Peng%20and%20Jiwen%20Liu%20and%20Haoxian%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Hongyan%20Liu%20and%20Jun%20He%0AAbstract%3A%20%20%20Lip%20synchronization%20is%20the%20task%20of%20aligning%20a%20speaker%27s%20lip%20movements%20in%0Avideo%20with%20corresponding%20speech%20audio%2C%20and%20it%20is%20essential%20for%20creating%0Arealistic%2C%20expressive%20video%20content.%20However%2C%20existing%20methods%20often%20rely%20on%0Areference%20frames%20and%20masked-frame%20inpainting%2C%20which%20limit%20their%20robustness%20to%0Aidentity%20consistency%2C%20pose%20variations%2C%20facial%20occlusions%2C%20and%20stylized%20content.%0AIn%20addition%2C%20since%20audio%20signals%20provide%20weaker%20conditioning%20than%20visual%20cues%2C%0Alip%20shape%20leakage%20from%20the%20original%20video%20will%20affect%20lip%20sync%20quality.%20In%20this%0Apaper%2C%20we%20present%20OmniSync%2C%20a%20universal%20lip%20synchronization%20framework%20for%0Adiverse%20visual%20scenarios.%20Our%20approach%20introduces%20a%20mask-free%20training%20paradigm%0Ausing%20Diffusion%20Transformer%20models%20for%20direct%20frame%20editing%20without%20explicit%0Amasks%2C%20enabling%20unlimited-duration%20inference%20while%20maintaining%20natural%20facial%0Adynamics%20and%20preserving%20character%20identity.%20During%20inference%2C%20we%20propose%20a%0Aflow-matching-based%20progressive%20noise%20initialization%20to%20ensure%20pose%20and%0Aidentity%20consistency%2C%20while%20allowing%20precise%20mouth-region%20editing.%20To%20address%0Athe%20weak%20conditioning%20signal%20of%20audio%2C%20we%20develop%20a%20Dynamic%20Spatiotemporal%0AClassifier-Free%20Guidance%20%28DS-CFG%29%20mechanism%20that%20adaptively%20adjusts%20guidance%0Astrength%20over%20time%20and%20space.%20We%20also%20establish%20the%20AIGC-LipSync%20Benchmark%2C%20the%0Afirst%20evaluation%20suite%20for%20lip%20synchronization%20in%20diverse%20AI-generated%20videos.%0AExtensive%20experiments%20demonstrate%20that%20OmniSync%20significantly%20outperforms%20prior%0Amethods%20in%20both%20visual%20quality%20and%20lip%20sync%20accuracy%2C%20achieving%20superior%0Aresults%20in%20both%20real-world%20and%20AI-generated%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSync%253A%2520Towards%2520Universal%2520Lip%2520Synchronization%2520via%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DZiqiao%2520Peng%2520and%2520Jiwen%2520Liu%2520and%2520Haoxian%2520Zhang%2520and%2520Xiaoqiang%2520Liu%2520and%2520Songlin%2520Tang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Hongyan%2520Liu%2520and%2520Jun%2520He%26entry.1292438233%3D%2520%2520Lip%2520synchronization%2520is%2520the%2520task%2520of%2520aligning%2520a%2520speaker%2527s%2520lip%2520movements%2520in%250Avideo%2520with%2520corresponding%2520speech%2520audio%252C%2520and%2520it%2520is%2520essential%2520for%2520creating%250Arealistic%252C%2520expressive%2520video%2520content.%2520However%252C%2520existing%2520methods%2520often%2520rely%2520on%250Areference%2520frames%2520and%2520masked-frame%2520inpainting%252C%2520which%2520limit%2520their%2520robustness%2520to%250Aidentity%2520consistency%252C%2520pose%2520variations%252C%2520facial%2520occlusions%252C%2520and%2520stylized%2520content.%250AIn%2520addition%252C%2520since%2520audio%2520signals%2520provide%2520weaker%2520conditioning%2520than%2520visual%2520cues%252C%250Alip%2520shape%2520leakage%2520from%2520the%2520original%2520video%2520will%2520affect%2520lip%2520sync%2520quality.%2520In%2520this%250Apaper%252C%2520we%2520present%2520OmniSync%252C%2520a%2520universal%2520lip%2520synchronization%2520framework%2520for%250Adiverse%2520visual%2520scenarios.%2520Our%2520approach%2520introduces%2520a%2520mask-free%2520training%2520paradigm%250Ausing%2520Diffusion%2520Transformer%2520models%2520for%2520direct%2520frame%2520editing%2520without%2520explicit%250Amasks%252C%2520enabling%2520unlimited-duration%2520inference%2520while%2520maintaining%2520natural%2520facial%250Adynamics%2520and%2520preserving%2520character%2520identity.%2520During%2520inference%252C%2520we%2520propose%2520a%250Aflow-matching-based%2520progressive%2520noise%2520initialization%2520to%2520ensure%2520pose%2520and%250Aidentity%2520consistency%252C%2520while%2520allowing%2520precise%2520mouth-region%2520editing.%2520To%2520address%250Athe%2520weak%2520conditioning%2520signal%2520of%2520audio%252C%2520we%2520develop%2520a%2520Dynamic%2520Spatiotemporal%250AClassifier-Free%2520Guidance%2520%2528DS-CFG%2529%2520mechanism%2520that%2520adaptively%2520adjusts%2520guidance%250Astrength%2520over%2520time%2520and%2520space.%2520We%2520also%2520establish%2520the%2520AIGC-LipSync%2520Benchmark%252C%2520the%250Afirst%2520evaluation%2520suite%2520for%2520lip%2520synchronization%2520in%2520diverse%2520AI-generated%2520videos.%250AExtensive%2520experiments%2520demonstrate%2520that%2520OmniSync%2520significantly%2520outperforms%2520prior%250Amethods%2520in%2520both%2520visual%2520quality%2520and%2520lip%2520sync%2520accuracy%252C%2520achieving%2520superior%250Aresults%2520in%2520both%2520real-world%2520and%2520AI-generated%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSync%3A%20Towards%20Universal%20Lip%20Synchronization%20via%20Diffusion%0A%20%20Transformers&entry.906535625=Ziqiao%20Peng%20and%20Jiwen%20Liu%20and%20Haoxian%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Songlin%20Tang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Hongyan%20Liu%20and%20Jun%20He&entry.1292438233=%20%20Lip%20synchronization%20is%20the%20task%20of%20aligning%20a%20speaker%27s%20lip%20movements%20in%0Avideo%20with%20corresponding%20speech%20audio%2C%20and%20it%20is%20essential%20for%20creating%0Arealistic%2C%20expressive%20video%20content.%20However%2C%20existing%20methods%20often%20rely%20on%0Areference%20frames%20and%20masked-frame%20inpainting%2C%20which%20limit%20their%20robustness%20to%0Aidentity%20consistency%2C%20pose%20variations%2C%20facial%20occlusions%2C%20and%20stylized%20content.%0AIn%20addition%2C%20since%20audio%20signals%20provide%20weaker%20conditioning%20than%20visual%20cues%2C%0Alip%20shape%20leakage%20from%20the%20original%20video%20will%20affect%20lip%20sync%20quality.%20In%20this%0Apaper%2C%20we%20present%20OmniSync%2C%20a%20universal%20lip%20synchronization%20framework%20for%0Adiverse%20visual%20scenarios.%20Our%20approach%20introduces%20a%20mask-free%20training%20paradigm%0Ausing%20Diffusion%20Transformer%20models%20for%20direct%20frame%20editing%20without%20explicit%0Amasks%2C%20enabling%20unlimited-duration%20inference%20while%20maintaining%20natural%20facial%0Adynamics%20and%20preserving%20character%20identity.%20During%20inference%2C%20we%20propose%20a%0Aflow-matching-based%20progressive%20noise%20initialization%20to%20ensure%20pose%20and%0Aidentity%20consistency%2C%20while%20allowing%20precise%20mouth-region%20editing.%20To%20address%0Athe%20weak%20conditioning%20signal%20of%20audio%2C%20we%20develop%20a%20Dynamic%20Spatiotemporal%0AClassifier-Free%20Guidance%20%28DS-CFG%29%20mechanism%20that%20adaptively%20adjusts%20guidance%0Astrength%20over%20time%20and%20space.%20We%20also%20establish%20the%20AIGC-LipSync%20Benchmark%2C%20the%0Afirst%20evaluation%20suite%20for%20lip%20synchronization%20in%20diverse%20AI-generated%20videos.%0AExtensive%20experiments%20demonstrate%20that%20OmniSync%20significantly%20outperforms%20prior%0Amethods%20in%20both%20visual%20quality%20and%20lip%20sync%20accuracy%2C%20achieving%20superior%0Aresults%20in%20both%20real-world%20and%20AI-generated%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21448v2&entry.124074799=Read"},
{"title": "NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft\n  Pose Estimation", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  On-orbit operations require the estimation of the relative 6D pose, i.e.,\nposition and orientation, between a chaser spacecraft and its target. While\ndata-driven spacecraft pose estimation methods have been developed, their\nadoption in real missions is hampered by the lack of understanding of their\ndecision process. This paper presents a method to visualize the 3D visual cues\non which a given pose estimator relies. For this purpose, we train a NeRF-based\nimage generator using the gradients back-propagated through the pose estimation\nnetwork. This enforces the generator to render the main 3D features exploited\nby the spacecraft pose estimation network. Experiments demonstrate that our\nmethod recovers the relevant 3D cues. Furthermore, they offer additional\ninsights on the relationship between the pose estimation network supervision\nand its implicit representation of the target spacecraft.\n", "link": "http://arxiv.org/abs/2509.14890v1", "date": "2025-09-18", "relevancy": 2.2706, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5975}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation&body=Title%3A%20NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20On-orbit%20operations%20require%20the%20estimation%20of%20the%20relative%206D%20pose%2C%20i.e.%2C%0Aposition%20and%20orientation%2C%20between%20a%20chaser%20spacecraft%20and%20its%20target.%20While%0Adata-driven%20spacecraft%20pose%20estimation%20methods%20have%20been%20developed%2C%20their%0Aadoption%20in%20real%20missions%20is%20hampered%20by%20the%20lack%20of%20understanding%20of%20their%0Adecision%20process.%20This%20paper%20presents%20a%20method%20to%20visualize%20the%203D%20visual%20cues%0Aon%20which%20a%20given%20pose%20estimator%20relies.%20For%20this%20purpose%2C%20we%20train%20a%20NeRF-based%0Aimage%20generator%20using%20the%20gradients%20back-propagated%20through%20the%20pose%20estimation%0Anetwork.%20This%20enforces%20the%20generator%20to%20render%20the%20main%203D%20features%20exploited%0Aby%20the%20spacecraft%20pose%20estimation%20network.%20Experiments%20demonstrate%20that%20our%0Amethod%20recovers%20the%20relevant%203D%20cues.%20Furthermore%2C%20they%20offer%20additional%0Ainsights%20on%20the%20relationship%20between%20the%20pose%20estimation%20network%20supervision%0Aand%20its%20implicit%20representation%20of%20the%20target%20spacecraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-based%2520Visualization%2520of%25203D%2520Cues%2520Supporting%2520Data-Driven%2520Spacecraft%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520On-orbit%2520operations%2520require%2520the%2520estimation%2520of%2520the%2520relative%25206D%2520pose%252C%2520i.e.%252C%250Aposition%2520and%2520orientation%252C%2520between%2520a%2520chaser%2520spacecraft%2520and%2520its%2520target.%2520While%250Adata-driven%2520spacecraft%2520pose%2520estimation%2520methods%2520have%2520been%2520developed%252C%2520their%250Aadoption%2520in%2520real%2520missions%2520is%2520hampered%2520by%2520the%2520lack%2520of%2520understanding%2520of%2520their%250Adecision%2520process.%2520This%2520paper%2520presents%2520a%2520method%2520to%2520visualize%2520the%25203D%2520visual%2520cues%250Aon%2520which%2520a%2520given%2520pose%2520estimator%2520relies.%2520For%2520this%2520purpose%252C%2520we%2520train%2520a%2520NeRF-based%250Aimage%2520generator%2520using%2520the%2520gradients%2520back-propagated%2520through%2520the%2520pose%2520estimation%250Anetwork.%2520This%2520enforces%2520the%2520generator%2520to%2520render%2520the%2520main%25203D%2520features%2520exploited%250Aby%2520the%2520spacecraft%2520pose%2520estimation%2520network.%2520Experiments%2520demonstrate%2520that%2520our%250Amethod%2520recovers%2520the%2520relevant%25203D%2520cues.%2520Furthermore%252C%2520they%2520offer%2520additional%250Ainsights%2520on%2520the%2520relationship%2520between%2520the%2520pose%2520estimation%2520network%2520supervision%250Aand%2520its%2520implicit%2520representation%2520of%2520the%2520target%2520spacecraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-based%20Visualization%20of%203D%20Cues%20Supporting%20Data-Driven%20Spacecraft%0A%20%20Pose%20Estimation&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20On-orbit%20operations%20require%20the%20estimation%20of%20the%20relative%206D%20pose%2C%20i.e.%2C%0Aposition%20and%20orientation%2C%20between%20a%20chaser%20spacecraft%20and%20its%20target.%20While%0Adata-driven%20spacecraft%20pose%20estimation%20methods%20have%20been%20developed%2C%20their%0Aadoption%20in%20real%20missions%20is%20hampered%20by%20the%20lack%20of%20understanding%20of%20their%0Adecision%20process.%20This%20paper%20presents%20a%20method%20to%20visualize%20the%203D%20visual%20cues%0Aon%20which%20a%20given%20pose%20estimator%20relies.%20For%20this%20purpose%2C%20we%20train%20a%20NeRF-based%0Aimage%20generator%20using%20the%20gradients%20back-propagated%20through%20the%20pose%20estimation%0Anetwork.%20This%20enforces%20the%20generator%20to%20render%20the%20main%203D%20features%20exploited%0Aby%20the%20spacecraft%20pose%20estimation%20network.%20Experiments%20demonstrate%20that%20our%0Amethod%20recovers%20the%20relevant%203D%20cues.%20Furthermore%2C%20they%20offer%20additional%0Ainsights%20on%20the%20relationship%20between%20the%20pose%20estimation%20network%20supervision%0Aand%20its%20implicit%20representation%20of%20the%20target%20spacecraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14890v1&entry.124074799=Read"},
{"title": "EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal\n  Ultrasound Intelligence", "author": "Chaoyin She and Ruifang Lu and Lida Chen and Wei Wang and Qinghua Huang", "abstract": "  Ultrasound imaging has become the preferred imaging modality for early cancer\nscreening due to its advantages of non-ionizing radiation, low cost, and\nreal-time imaging capabilities. However, conventional ultrasound diagnosis\nheavily relies on physician expertise, presenting challenges of high\nsubjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer\npromising solutions for this issue, but existing general-purpose models\ndemonstrate limited knowledge in ultrasound medical tasks, with poor\ngeneralization in multi-organ lesion recognition and low efficiency across\nmulti-task diagnostics. To address these limitations, we propose EchoVLM, a\nvision-language model specifically designed for ultrasound medical imaging. The\nmodel employs a Mixture of Experts (MoE) architecture trained on data spanning\nseven anatomical regions. This design enables the model to perform multiple\ntasks, including ultrasound report generation, diagnosis and visual\nquestion-answering (VQA). The experimental results demonstrated that EchoVLM\nachieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and\nROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report\ngeneration task. These findings suggest that EchoVLM has substantial potential\nto enhance diagnostic accuracy in ultrasound imaging, thereby providing a\nviable technical solution for future clinical applications. Source code and\nmodel weights are available at https://github.com/Asunatan/EchoVLM.\n", "link": "http://arxiv.org/abs/2509.14977v1", "date": "2025-09-18", "relevancy": 2.2692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoVLM%3A%20Dynamic%20Mixture-of-Experts%20Vision-Language%20Model%20for%20Universal%0A%20%20Ultrasound%20Intelligence&body=Title%3A%20EchoVLM%3A%20Dynamic%20Mixture-of-Experts%20Vision-Language%20Model%20for%20Universal%0A%20%20Ultrasound%20Intelligence%0AAuthor%3A%20Chaoyin%20She%20and%20Ruifang%20Lu%20and%20Lida%20Chen%20and%20Wei%20Wang%20and%20Qinghua%20Huang%0AAbstract%3A%20%20%20Ultrasound%20imaging%20has%20become%20the%20preferred%20imaging%20modality%20for%20early%20cancer%0Ascreening%20due%20to%20its%20advantages%20of%20non-ionizing%20radiation%2C%20low%20cost%2C%20and%0Areal-time%20imaging%20capabilities.%20However%2C%20conventional%20ultrasound%20diagnosis%0Aheavily%20relies%20on%20physician%20expertise%2C%20presenting%20challenges%20of%20high%0Asubjectivity%20and%20low%20diagnostic%20efficiency.%20Vision-language%20models%20%28VLMs%29%20offer%0Apromising%20solutions%20for%20this%20issue%2C%20but%20existing%20general-purpose%20models%0Ademonstrate%20limited%20knowledge%20in%20ultrasound%20medical%20tasks%2C%20with%20poor%0Ageneralization%20in%20multi-organ%20lesion%20recognition%20and%20low%20efficiency%20across%0Amulti-task%20diagnostics.%20To%20address%20these%20limitations%2C%20we%20propose%20EchoVLM%2C%20a%0Avision-language%20model%20specifically%20designed%20for%20ultrasound%20medical%20imaging.%20The%0Amodel%20employs%20a%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20trained%20on%20data%20spanning%0Aseven%20anatomical%20regions.%20This%20design%20enables%20the%20model%20to%20perform%20multiple%0Atasks%2C%20including%20ultrasound%20report%20generation%2C%20diagnosis%20and%20visual%0Aquestion-answering%20%28VQA%29.%20The%20experimental%20results%20demonstrated%20that%20EchoVLM%0Aachieved%20significant%20improvements%20of%2010.15%20and%204.77%20points%20in%20BLEU-1%20scores%20and%0AROUGE-1%20scores%20respectively%20compared%20to%20Qwen2-VL%20on%20the%20ultrasound%20report%0Ageneration%20task.%20These%20findings%20suggest%20that%20EchoVLM%20has%20substantial%20potential%0Ato%20enhance%20diagnostic%20accuracy%20in%20ultrasound%20imaging%2C%20thereby%20providing%20a%0Aviable%20technical%20solution%20for%20future%20clinical%20applications.%20Source%20code%20and%0Amodel%20weights%20are%20available%20at%20https%3A//github.com/Asunatan/EchoVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoVLM%253A%2520Dynamic%2520Mixture-of-Experts%2520Vision-Language%2520Model%2520for%2520Universal%250A%2520%2520Ultrasound%2520Intelligence%26entry.906535625%3DChaoyin%2520She%2520and%2520Ruifang%2520Lu%2520and%2520Lida%2520Chen%2520and%2520Wei%2520Wang%2520and%2520Qinghua%2520Huang%26entry.1292438233%3D%2520%2520Ultrasound%2520imaging%2520has%2520become%2520the%2520preferred%2520imaging%2520modality%2520for%2520early%2520cancer%250Ascreening%2520due%2520to%2520its%2520advantages%2520of%2520non-ionizing%2520radiation%252C%2520low%2520cost%252C%2520and%250Areal-time%2520imaging%2520capabilities.%2520However%252C%2520conventional%2520ultrasound%2520diagnosis%250Aheavily%2520relies%2520on%2520physician%2520expertise%252C%2520presenting%2520challenges%2520of%2520high%250Asubjectivity%2520and%2520low%2520diagnostic%2520efficiency.%2520Vision-language%2520models%2520%2528VLMs%2529%2520offer%250Apromising%2520solutions%2520for%2520this%2520issue%252C%2520but%2520existing%2520general-purpose%2520models%250Ademonstrate%2520limited%2520knowledge%2520in%2520ultrasound%2520medical%2520tasks%252C%2520with%2520poor%250Ageneralization%2520in%2520multi-organ%2520lesion%2520recognition%2520and%2520low%2520efficiency%2520across%250Amulti-task%2520diagnostics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520EchoVLM%252C%2520a%250Avision-language%2520model%2520specifically%2520designed%2520for%2520ultrasound%2520medical%2520imaging.%2520The%250Amodel%2520employs%2520a%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520architecture%2520trained%2520on%2520data%2520spanning%250Aseven%2520anatomical%2520regions.%2520This%2520design%2520enables%2520the%2520model%2520to%2520perform%2520multiple%250Atasks%252C%2520including%2520ultrasound%2520report%2520generation%252C%2520diagnosis%2520and%2520visual%250Aquestion-answering%2520%2528VQA%2529.%2520The%2520experimental%2520results%2520demonstrated%2520that%2520EchoVLM%250Aachieved%2520significant%2520improvements%2520of%252010.15%2520and%25204.77%2520points%2520in%2520BLEU-1%2520scores%2520and%250AROUGE-1%2520scores%2520respectively%2520compared%2520to%2520Qwen2-VL%2520on%2520the%2520ultrasound%2520report%250Ageneration%2520task.%2520These%2520findings%2520suggest%2520that%2520EchoVLM%2520has%2520substantial%2520potential%250Ato%2520enhance%2520diagnostic%2520accuracy%2520in%2520ultrasound%2520imaging%252C%2520thereby%2520providing%2520a%250Aviable%2520technical%2520solution%2520for%2520future%2520clinical%2520applications.%2520Source%2520code%2520and%250Amodel%2520weights%2520are%2520available%2520at%2520https%253A//github.com/Asunatan/EchoVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoVLM%3A%20Dynamic%20Mixture-of-Experts%20Vision-Language%20Model%20for%20Universal%0A%20%20Ultrasound%20Intelligence&entry.906535625=Chaoyin%20She%20and%20Ruifang%20Lu%20and%20Lida%20Chen%20and%20Wei%20Wang%20and%20Qinghua%20Huang&entry.1292438233=%20%20Ultrasound%20imaging%20has%20become%20the%20preferred%20imaging%20modality%20for%20early%20cancer%0Ascreening%20due%20to%20its%20advantages%20of%20non-ionizing%20radiation%2C%20low%20cost%2C%20and%0Areal-time%20imaging%20capabilities.%20However%2C%20conventional%20ultrasound%20diagnosis%0Aheavily%20relies%20on%20physician%20expertise%2C%20presenting%20challenges%20of%20high%0Asubjectivity%20and%20low%20diagnostic%20efficiency.%20Vision-language%20models%20%28VLMs%29%20offer%0Apromising%20solutions%20for%20this%20issue%2C%20but%20existing%20general-purpose%20models%0Ademonstrate%20limited%20knowledge%20in%20ultrasound%20medical%20tasks%2C%20with%20poor%0Ageneralization%20in%20multi-organ%20lesion%20recognition%20and%20low%20efficiency%20across%0Amulti-task%20diagnostics.%20To%20address%20these%20limitations%2C%20we%20propose%20EchoVLM%2C%20a%0Avision-language%20model%20specifically%20designed%20for%20ultrasound%20medical%20imaging.%20The%0Amodel%20employs%20a%20Mixture%20of%20Experts%20%28MoE%29%20architecture%20trained%20on%20data%20spanning%0Aseven%20anatomical%20regions.%20This%20design%20enables%20the%20model%20to%20perform%20multiple%0Atasks%2C%20including%20ultrasound%20report%20generation%2C%20diagnosis%20and%20visual%0Aquestion-answering%20%28VQA%29.%20The%20experimental%20results%20demonstrated%20that%20EchoVLM%0Aachieved%20significant%20improvements%20of%2010.15%20and%204.77%20points%20in%20BLEU-1%20scores%20and%0AROUGE-1%20scores%20respectively%20compared%20to%20Qwen2-VL%20on%20the%20ultrasound%20report%0Ageneration%20task.%20These%20findings%20suggest%20that%20EchoVLM%20has%20substantial%20potential%0Ato%20enhance%20diagnostic%20accuracy%20in%20ultrasound%20imaging%2C%20thereby%20providing%20a%0Aviable%20technical%20solution%20for%20future%20clinical%20applications.%20Source%20code%20and%0Amodel%20weights%20are%20available%20at%20https%3A//github.com/Asunatan/EchoVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14977v1&entry.124074799=Read"},
{"title": "MATTER: Multiscale Attention for Registration Error Regression", "author": "Shipeng Liu and Ziliang Xiong and Khac-Hoang Ngo and Per-Erik Forss\u00e9n", "abstract": "  Point cloud registration (PCR) is crucial for many downstream tasks, such as\nsimultaneous localization and mapping (SLAM) and object tracking. This makes\ndetecting and quantifying registration misalignment, i.e., PCR quality\nvalidation, an important task. All existing methods treat validation as a\nclassification task, aiming to assign the PCR quality to a few classes. In this\nwork, we instead use regression for PCR validation, allowing for a more\nfine-grained quantification of the registration quality. We also extend\npreviously used misalignment-related features by using multiscale extraction\nand attention-based aggregation. This leads to accurate and robust registration\nerror estimation on diverse datasets, especially for point clouds with\nheterogeneous spatial densities. Furthermore, when used to guide a mapping\ndownstream task, our method significantly improves the mapping quality for a\ngiven amount of re-registered frames, compared to the state-of-the-art\nclassification-based method.\n", "link": "http://arxiv.org/abs/2509.12924v2", "date": "2025-09-18", "relevancy": 2.2555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5775}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5674}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression&body=Title%3A%20MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression%0AAuthor%3A%20Shipeng%20Liu%20and%20Ziliang%20Xiong%20and%20Khac-Hoang%20Ngo%20and%20Per-Erik%20Forss%C3%A9n%0AAbstract%3A%20%20%20Point%20cloud%20registration%20%28PCR%29%20is%20crucial%20for%20many%20downstream%20tasks%2C%20such%20as%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20and%20object%20tracking.%20This%20makes%0Adetecting%20and%20quantifying%20registration%20misalignment%2C%20i.e.%2C%20PCR%20quality%0Avalidation%2C%20an%20important%20task.%20All%20existing%20methods%20treat%20validation%20as%20a%0Aclassification%20task%2C%20aiming%20to%20assign%20the%20PCR%20quality%20to%20a%20few%20classes.%20In%20this%0Awork%2C%20we%20instead%20use%20regression%20for%20PCR%20validation%2C%20allowing%20for%20a%20more%0Afine-grained%20quantification%20of%20the%20registration%20quality.%20We%20also%20extend%0Apreviously%20used%20misalignment-related%20features%20by%20using%20multiscale%20extraction%0Aand%20attention-based%20aggregation.%20This%20leads%20to%20accurate%20and%20robust%20registration%0Aerror%20estimation%20on%20diverse%20datasets%2C%20especially%20for%20point%20clouds%20with%0Aheterogeneous%20spatial%20densities.%20Furthermore%2C%20when%20used%20to%20guide%20a%20mapping%0Adownstream%20task%2C%20our%20method%20significantly%20improves%20the%20mapping%20quality%20for%20a%0Agiven%20amount%20of%20re-registered%20frames%2C%20compared%20to%20the%20state-of-the-art%0Aclassification-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMATTER%253A%2520Multiscale%2520Attention%2520for%2520Registration%2520Error%2520Regression%26entry.906535625%3DShipeng%2520Liu%2520and%2520Ziliang%2520Xiong%2520and%2520Khac-Hoang%2520Ngo%2520and%2520Per-Erik%2520Forss%25C3%25A9n%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520%2528PCR%2529%2520is%2520crucial%2520for%2520many%2520downstream%2520tasks%252C%2520such%2520as%250Asimultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520and%2520object%2520tracking.%2520This%2520makes%250Adetecting%2520and%2520quantifying%2520registration%2520misalignment%252C%2520i.e.%252C%2520PCR%2520quality%250Avalidation%252C%2520an%2520important%2520task.%2520All%2520existing%2520methods%2520treat%2520validation%2520as%2520a%250Aclassification%2520task%252C%2520aiming%2520to%2520assign%2520the%2520PCR%2520quality%2520to%2520a%2520few%2520classes.%2520In%2520this%250Awork%252C%2520we%2520instead%2520use%2520regression%2520for%2520PCR%2520validation%252C%2520allowing%2520for%2520a%2520more%250Afine-grained%2520quantification%2520of%2520the%2520registration%2520quality.%2520We%2520also%2520extend%250Apreviously%2520used%2520misalignment-related%2520features%2520by%2520using%2520multiscale%2520extraction%250Aand%2520attention-based%2520aggregation.%2520This%2520leads%2520to%2520accurate%2520and%2520robust%2520registration%250Aerror%2520estimation%2520on%2520diverse%2520datasets%252C%2520especially%2520for%2520point%2520clouds%2520with%250Aheterogeneous%2520spatial%2520densities.%2520Furthermore%252C%2520when%2520used%2520to%2520guide%2520a%2520mapping%250Adownstream%2520task%252C%2520our%2520method%2520significantly%2520improves%2520the%2520mapping%2520quality%2520for%2520a%250Agiven%2520amount%2520of%2520re-registered%2520frames%252C%2520compared%2520to%2520the%2520state-of-the-art%250Aclassification-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression&entry.906535625=Shipeng%20Liu%20and%20Ziliang%20Xiong%20and%20Khac-Hoang%20Ngo%20and%20Per-Erik%20Forss%C3%A9n&entry.1292438233=%20%20Point%20cloud%20registration%20%28PCR%29%20is%20crucial%20for%20many%20downstream%20tasks%2C%20such%20as%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20and%20object%20tracking.%20This%20makes%0Adetecting%20and%20quantifying%20registration%20misalignment%2C%20i.e.%2C%20PCR%20quality%0Avalidation%2C%20an%20important%20task.%20All%20existing%20methods%20treat%20validation%20as%20a%0Aclassification%20task%2C%20aiming%20to%20assign%20the%20PCR%20quality%20to%20a%20few%20classes.%20In%20this%0Awork%2C%20we%20instead%20use%20regression%20for%20PCR%20validation%2C%20allowing%20for%20a%20more%0Afine-grained%20quantification%20of%20the%20registration%20quality.%20We%20also%20extend%0Apreviously%20used%20misalignment-related%20features%20by%20using%20multiscale%20extraction%0Aand%20attention-based%20aggregation.%20This%20leads%20to%20accurate%20and%20robust%20registration%0Aerror%20estimation%20on%20diverse%20datasets%2C%20especially%20for%20point%20clouds%20with%0Aheterogeneous%20spatial%20densities.%20Furthermore%2C%20when%20used%20to%20guide%20a%20mapping%0Adownstream%20task%2C%20our%20method%20significantly%20improves%20the%20mapping%20quality%20for%20a%0Agiven%20amount%20of%20re-registered%20frames%2C%20compared%20to%20the%20state-of-the-art%0Aclassification-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12924v2&entry.124074799=Read"},
{"title": "Generalizable Geometric Image Caption Synthesis", "author": "Yue Xin and Wenyuan Wang and Rui Pan and Ruida Wang and Howard Meng and Renjie Pi and Shizhe Diao and Tong Zhang", "abstract": "  Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.\n", "link": "http://arxiv.org/abs/2509.15217v1", "date": "2025-09-18", "relevancy": 2.2534, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5716}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5704}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Geometric%20Image%20Caption%20Synthesis&body=Title%3A%20Generalizable%20Geometric%20Image%20Caption%20Synthesis%0AAuthor%3A%20Yue%20Xin%20and%20Wenyuan%20Wang%20and%20Rui%20Pan%20and%20Ruida%20Wang%20and%20Howard%20Meng%20and%20Renjie%20Pi%20and%20Shizhe%20Diao%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20have%20various%20practical%20applications%20that%0Ademand%20strong%20reasoning%20abilities.%20Despite%20recent%20advancements%2C%20these%20models%0Astill%20struggle%20to%20solve%20complex%20geometric%20problems.%20A%20key%20challenge%20stems%20from%0Athe%20lack%20of%20high-quality%20image-text%20pair%20datasets%20for%20understanding%20geometric%0Aimages.%20Furthermore%2C%20most%20template-based%20data%20synthesis%20pipelines%20typically%0Afail%20to%20generalize%20to%20questions%20beyond%20their%20predefined%20templates.%20In%20this%0Apaper%2C%20we%20bridge%20this%20gap%20by%20introducing%20a%20complementary%20process%20of%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20into%20the%20data%20generation%0Apipeline.%20By%20adopting%20RLVR%20to%20refine%20captions%20for%20geometric%20images%20synthesized%0Afrom%2050%20basic%20geometric%20relations%20and%20using%20reward%20signals%20derived%20from%0Amathematical%20problem-solving%20tasks%2C%20our%20pipeline%20successfully%20captures%20the%20key%0Afeatures%20of%20geometry%20problem-solving.%20This%20enables%20better%20task%20generalization%0Aand%20yields%20non-trivial%20improvements.%20Furthermore%2C%20even%20in%20out-of-distribution%0Ascenarios%2C%20the%20generated%20dataset%20enhances%20the%20general%20reasoning%20capabilities%20of%0Amultimodal%20large%20language%20models%2C%20yielding%20accuracy%20improvements%20of%0A%242.8%5C%25%5Ctext%7B-%7D4.8%5C%25%24%20in%20statistics%2C%20arithmetic%2C%20algebraic%2C%20and%20numerical%20tasks%0Awith%20non-geometric%20input%20images%20of%20MathVista%20and%20MathVerse%2C%20along%20with%0A%242.4%5C%25%5Ctext%7B-%7D3.9%5C%25%24%20improvements%20in%20Art%2C%20Design%2C%20Tech%2C%20and%20Engineering%20tasks%0Ain%20MMMU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Geometric%2520Image%2520Caption%2520Synthesis%26entry.906535625%3DYue%2520Xin%2520and%2520Wenyuan%2520Wang%2520and%2520Rui%2520Pan%2520and%2520Ruida%2520Wang%2520and%2520Howard%2520Meng%2520and%2520Renjie%2520Pi%2520and%2520Shizhe%2520Diao%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520have%2520various%2520practical%2520applications%2520that%250Ademand%2520strong%2520reasoning%2520abilities.%2520Despite%2520recent%2520advancements%252C%2520these%2520models%250Astill%2520struggle%2520to%2520solve%2520complex%2520geometric%2520problems.%2520A%2520key%2520challenge%2520stems%2520from%250Athe%2520lack%2520of%2520high-quality%2520image-text%2520pair%2520datasets%2520for%2520understanding%2520geometric%250Aimages.%2520Furthermore%252C%2520most%2520template-based%2520data%2520synthesis%2520pipelines%2520typically%250Afail%2520to%2520generalize%2520to%2520questions%2520beyond%2520their%2520predefined%2520templates.%2520In%2520this%250Apaper%252C%2520we%2520bridge%2520this%2520gap%2520by%2520introducing%2520a%2520complementary%2520process%2520of%250AReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520into%2520the%2520data%2520generation%250Apipeline.%2520By%2520adopting%2520RLVR%2520to%2520refine%2520captions%2520for%2520geometric%2520images%2520synthesized%250Afrom%252050%2520basic%2520geometric%2520relations%2520and%2520using%2520reward%2520signals%2520derived%2520from%250Amathematical%2520problem-solving%2520tasks%252C%2520our%2520pipeline%2520successfully%2520captures%2520the%2520key%250Afeatures%2520of%2520geometry%2520problem-solving.%2520This%2520enables%2520better%2520task%2520generalization%250Aand%2520yields%2520non-trivial%2520improvements.%2520Furthermore%252C%2520even%2520in%2520out-of-distribution%250Ascenarios%252C%2520the%2520generated%2520dataset%2520enhances%2520the%2520general%2520reasoning%2520capabilities%2520of%250Amultimodal%2520large%2520language%2520models%252C%2520yielding%2520accuracy%2520improvements%2520of%250A%25242.8%255C%2525%255Ctext%257B-%257D4.8%255C%2525%2524%2520in%2520statistics%252C%2520arithmetic%252C%2520algebraic%252C%2520and%2520numerical%2520tasks%250Awith%2520non-geometric%2520input%2520images%2520of%2520MathVista%2520and%2520MathVerse%252C%2520along%2520with%250A%25242.4%255C%2525%255Ctext%257B-%257D3.9%255C%2525%2524%2520improvements%2520in%2520Art%252C%2520Design%252C%2520Tech%252C%2520and%2520Engineering%2520tasks%250Ain%2520MMMU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Geometric%20Image%20Caption%20Synthesis&entry.906535625=Yue%20Xin%20and%20Wenyuan%20Wang%20and%20Rui%20Pan%20and%20Ruida%20Wang%20and%20Howard%20Meng%20and%20Renjie%20Pi%20and%20Shizhe%20Diao%20and%20Tong%20Zhang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20have%20various%20practical%20applications%20that%0Ademand%20strong%20reasoning%20abilities.%20Despite%20recent%20advancements%2C%20these%20models%0Astill%20struggle%20to%20solve%20complex%20geometric%20problems.%20A%20key%20challenge%20stems%20from%0Athe%20lack%20of%20high-quality%20image-text%20pair%20datasets%20for%20understanding%20geometric%0Aimages.%20Furthermore%2C%20most%20template-based%20data%20synthesis%20pipelines%20typically%0Afail%20to%20generalize%20to%20questions%20beyond%20their%20predefined%20templates.%20In%20this%0Apaper%2C%20we%20bridge%20this%20gap%20by%20introducing%20a%20complementary%20process%20of%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20into%20the%20data%20generation%0Apipeline.%20By%20adopting%20RLVR%20to%20refine%20captions%20for%20geometric%20images%20synthesized%0Afrom%2050%20basic%20geometric%20relations%20and%20using%20reward%20signals%20derived%20from%0Amathematical%20problem-solving%20tasks%2C%20our%20pipeline%20successfully%20captures%20the%20key%0Afeatures%20of%20geometry%20problem-solving.%20This%20enables%20better%20task%20generalization%0Aand%20yields%20non-trivial%20improvements.%20Furthermore%2C%20even%20in%20out-of-distribution%0Ascenarios%2C%20the%20generated%20dataset%20enhances%20the%20general%20reasoning%20capabilities%20of%0Amultimodal%20large%20language%20models%2C%20yielding%20accuracy%20improvements%20of%0A%242.8%5C%25%5Ctext%7B-%7D4.8%5C%25%24%20in%20statistics%2C%20arithmetic%2C%20algebraic%2C%20and%20numerical%20tasks%0Awith%20non-geometric%20input%20images%20of%20MathVista%20and%20MathVerse%2C%20along%20with%0A%242.4%5C%25%5Ctext%7B-%7D3.9%5C%25%24%20improvements%20in%20Art%2C%20Design%2C%20Tech%2C%20and%20Engineering%20tasks%0Ain%20MMMU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15217v1&entry.124074799=Read"},
{"title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for\n  Vision Models", "author": "Haobo Yang and Minghao Guo and Dequan Yang and Wenyu Wang", "abstract": "  Contemporary deep learning models have achieved impressive performance in\nimage classification by primarily leveraging statistical regularities within\nlarge datasets, but they rarely incorporate structured insights drawn directly\nfrom perceptual psychology. To explore the potential of perceptually motivated\ninductive biases, we propose integrating classic geometric visual illusions\nwell-studied phenomena from human perception into standard image-classification\ntraining pipelines. Specifically, we introduce a synthetic, parametric\ngeometric-illusion dataset and evaluate three multi-source learning strategies\nthat combine illusion recognition tasks with ImageNet classification\nobjectives. Our experiments reveal two key conceptual insights: (i)\nincorporating geometric illusions as auxiliary supervision systematically\nimproves generalization, especially in visually challenging cases involving\nintricate contours and fine textures; and (ii) perceptually driven inductive\nbiases, even when derived from synthetic stimuli traditionally considered\nunrelated to natural image recognition, can enhance the structural sensitivity\nof both CNN and transformer-based architectures. These results demonstrate a\nnovel integration of perceptual science and machine learning and suggest new\ndirections for embedding perceptual priors into vision model design.\n", "link": "http://arxiv.org/abs/2509.15156v1", "date": "2025-09-18", "relevancy": 2.2506, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5718}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Geometric%20Visual%20Illusions%20as%20Perceptual%20Inductive%20Biases%20for%0A%20%20Vision%20Models&body=Title%3A%20Leveraging%20Geometric%20Visual%20Illusions%20as%20Perceptual%20Inductive%20Biases%20for%0A%20%20Vision%20Models%0AAuthor%3A%20Haobo%20Yang%20and%20Minghao%20Guo%20and%20Dequan%20Yang%20and%20Wenyu%20Wang%0AAbstract%3A%20%20%20Contemporary%20deep%20learning%20models%20have%20achieved%20impressive%20performance%20in%0Aimage%20classification%20by%20primarily%20leveraging%20statistical%20regularities%20within%0Alarge%20datasets%2C%20but%20they%20rarely%20incorporate%20structured%20insights%20drawn%20directly%0Afrom%20perceptual%20psychology.%20To%20explore%20the%20potential%20of%20perceptually%20motivated%0Ainductive%20biases%2C%20we%20propose%20integrating%20classic%20geometric%20visual%20illusions%0Awell-studied%20phenomena%20from%20human%20perception%20into%20standard%20image-classification%0Atraining%20pipelines.%20Specifically%2C%20we%20introduce%20a%20synthetic%2C%20parametric%0Ageometric-illusion%20dataset%20and%20evaluate%20three%20multi-source%20learning%20strategies%0Athat%20combine%20illusion%20recognition%20tasks%20with%20ImageNet%20classification%0Aobjectives.%20Our%20experiments%20reveal%20two%20key%20conceptual%20insights%3A%20%28i%29%0Aincorporating%20geometric%20illusions%20as%20auxiliary%20supervision%20systematically%0Aimproves%20generalization%2C%20especially%20in%20visually%20challenging%20cases%20involving%0Aintricate%20contours%20and%20fine%20textures%3B%20and%20%28ii%29%20perceptually%20driven%20inductive%0Abiases%2C%20even%20when%20derived%20from%20synthetic%20stimuli%20traditionally%20considered%0Aunrelated%20to%20natural%20image%20recognition%2C%20can%20enhance%20the%20structural%20sensitivity%0Aof%20both%20CNN%20and%20transformer-based%20architectures.%20These%20results%20demonstrate%20a%0Anovel%20integration%20of%20perceptual%20science%20and%20machine%20learning%20and%20suggest%20new%0Adirections%20for%20embedding%20perceptual%20priors%20into%20vision%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Geometric%2520Visual%2520Illusions%2520as%2520Perceptual%2520Inductive%2520Biases%2520for%250A%2520%2520Vision%2520Models%26entry.906535625%3DHaobo%2520Yang%2520and%2520Minghao%2520Guo%2520and%2520Dequan%2520Yang%2520and%2520Wenyu%2520Wang%26entry.1292438233%3D%2520%2520Contemporary%2520deep%2520learning%2520models%2520have%2520achieved%2520impressive%2520performance%2520in%250Aimage%2520classification%2520by%2520primarily%2520leveraging%2520statistical%2520regularities%2520within%250Alarge%2520datasets%252C%2520but%2520they%2520rarely%2520incorporate%2520structured%2520insights%2520drawn%2520directly%250Afrom%2520perceptual%2520psychology.%2520To%2520explore%2520the%2520potential%2520of%2520perceptually%2520motivated%250Ainductive%2520biases%252C%2520we%2520propose%2520integrating%2520classic%2520geometric%2520visual%2520illusions%250Awell-studied%2520phenomena%2520from%2520human%2520perception%2520into%2520standard%2520image-classification%250Atraining%2520pipelines.%2520Specifically%252C%2520we%2520introduce%2520a%2520synthetic%252C%2520parametric%250Ageometric-illusion%2520dataset%2520and%2520evaluate%2520three%2520multi-source%2520learning%2520strategies%250Athat%2520combine%2520illusion%2520recognition%2520tasks%2520with%2520ImageNet%2520classification%250Aobjectives.%2520Our%2520experiments%2520reveal%2520two%2520key%2520conceptual%2520insights%253A%2520%2528i%2529%250Aincorporating%2520geometric%2520illusions%2520as%2520auxiliary%2520supervision%2520systematically%250Aimproves%2520generalization%252C%2520especially%2520in%2520visually%2520challenging%2520cases%2520involving%250Aintricate%2520contours%2520and%2520fine%2520textures%253B%2520and%2520%2528ii%2529%2520perceptually%2520driven%2520inductive%250Abiases%252C%2520even%2520when%2520derived%2520from%2520synthetic%2520stimuli%2520traditionally%2520considered%250Aunrelated%2520to%2520natural%2520image%2520recognition%252C%2520can%2520enhance%2520the%2520structural%2520sensitivity%250Aof%2520both%2520CNN%2520and%2520transformer-based%2520architectures.%2520These%2520results%2520demonstrate%2520a%250Anovel%2520integration%2520of%2520perceptual%2520science%2520and%2520machine%2520learning%2520and%2520suggest%2520new%250Adirections%2520for%2520embedding%2520perceptual%2520priors%2520into%2520vision%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Geometric%20Visual%20Illusions%20as%20Perceptual%20Inductive%20Biases%20for%0A%20%20Vision%20Models&entry.906535625=Haobo%20Yang%20and%20Minghao%20Guo%20and%20Dequan%20Yang%20and%20Wenyu%20Wang&entry.1292438233=%20%20Contemporary%20deep%20learning%20models%20have%20achieved%20impressive%20performance%20in%0Aimage%20classification%20by%20primarily%20leveraging%20statistical%20regularities%20within%0Alarge%20datasets%2C%20but%20they%20rarely%20incorporate%20structured%20insights%20drawn%20directly%0Afrom%20perceptual%20psychology.%20To%20explore%20the%20potential%20of%20perceptually%20motivated%0Ainductive%20biases%2C%20we%20propose%20integrating%20classic%20geometric%20visual%20illusions%0Awell-studied%20phenomena%20from%20human%20perception%20into%20standard%20image-classification%0Atraining%20pipelines.%20Specifically%2C%20we%20introduce%20a%20synthetic%2C%20parametric%0Ageometric-illusion%20dataset%20and%20evaluate%20three%20multi-source%20learning%20strategies%0Athat%20combine%20illusion%20recognition%20tasks%20with%20ImageNet%20classification%0Aobjectives.%20Our%20experiments%20reveal%20two%20key%20conceptual%20insights%3A%20%28i%29%0Aincorporating%20geometric%20illusions%20as%20auxiliary%20supervision%20systematically%0Aimproves%20generalization%2C%20especially%20in%20visually%20challenging%20cases%20involving%0Aintricate%20contours%20and%20fine%20textures%3B%20and%20%28ii%29%20perceptually%20driven%20inductive%0Abiases%2C%20even%20when%20derived%20from%20synthetic%20stimuli%20traditionally%20considered%0Aunrelated%20to%20natural%20image%20recognition%2C%20can%20enhance%20the%20structural%20sensitivity%0Aof%20both%20CNN%20and%20transformer-based%20architectures.%20These%20results%20demonstrate%20a%0Anovel%20integration%20of%20perceptual%20science%20and%20machine%20learning%20and%20suggest%20new%0Adirections%20for%20embedding%20perceptual%20priors%20into%20vision%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15156v1&entry.124074799=Read"},
{"title": "Image-Text-Image Knowledge Transfer for Lifelong Person\n  Re-Identification with Hybrid Clothing States", "author": "Qizao Wang and Xuelin Qian and Bin Li and Yanwei Fu and Xiangyang Xue", "abstract": "  With the continuous expansion of intelligent surveillance networks, lifelong\nperson re-identification (LReID) has received widespread attention, pursuing\nthe need of self-evolution across different domains. However, existing LReID\nstudies accumulate knowledge with the assumption that people would not change\ntheir clothes. In this paper, we propose a more practical task, namely lifelong\nperson re-identification with hybrid clothing states (LReID-Hybrid), which\ntakes a series of cloth-changing and same-cloth domains into account during\nlifelong learning. To tackle the challenges of knowledge granularity mismatch\nand knowledge presentation mismatch in LReID-Hybrid, we take advantage of the\nconsistency and generalization capabilities of the text space, and propose a\nnovel framework, dubbed $Teata$, to effectively align, transfer, and accumulate\nknowledge in an \"image-text-image\" closed loop. Concretely, to achieve\neffective knowledge transfer, we design a Structured Semantic Prompt (SSP)\nlearning to decompose the text prompt into several structured pairs to distill\nknowledge from the image space with a unified granularity of text description.\nThen, we introduce a Knowledge Adaptation and Projection (KAP) strategy, which\ntunes text knowledge via a slow-paced learner to adapt to different tasks\nwithout catastrophic forgetting. Extensive experiments demonstrate the\nsuperiority of our proposed $Teata$ for LReID-Hybrid as well as on conventional\nLReID benchmarks over advanced methods.\n", "link": "http://arxiv.org/abs/2405.16600v2", "date": "2025-09-18", "relevancy": 2.2496, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5587}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image-Text-Image%20Knowledge%20Transfer%20for%20Lifelong%20Person%0A%20%20Re-Identification%20with%20Hybrid%20Clothing%20States&body=Title%3A%20Image-Text-Image%20Knowledge%20Transfer%20for%20Lifelong%20Person%0A%20%20Re-Identification%20with%20Hybrid%20Clothing%20States%0AAuthor%3A%20Qizao%20Wang%20and%20Xuelin%20Qian%20and%20Bin%20Li%20and%20Yanwei%20Fu%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20With%20the%20continuous%20expansion%20of%20intelligent%20surveillance%20networks%2C%20lifelong%0Aperson%20re-identification%20%28LReID%29%20has%20received%20widespread%20attention%2C%20pursuing%0Athe%20need%20of%20self-evolution%20across%20different%20domains.%20However%2C%20existing%20LReID%0Astudies%20accumulate%20knowledge%20with%20the%20assumption%20that%20people%20would%20not%20change%0Atheir%20clothes.%20In%20this%20paper%2C%20we%20propose%20a%20more%20practical%20task%2C%20namely%20lifelong%0Aperson%20re-identification%20with%20hybrid%20clothing%20states%20%28LReID-Hybrid%29%2C%20which%0Atakes%20a%20series%20of%20cloth-changing%20and%20same-cloth%20domains%20into%20account%20during%0Alifelong%20learning.%20To%20tackle%20the%20challenges%20of%20knowledge%20granularity%20mismatch%0Aand%20knowledge%20presentation%20mismatch%20in%20LReID-Hybrid%2C%20we%20take%20advantage%20of%20the%0Aconsistency%20and%20generalization%20capabilities%20of%20the%20text%20space%2C%20and%20propose%20a%0Anovel%20framework%2C%20dubbed%20%24Teata%24%2C%20to%20effectively%20align%2C%20transfer%2C%20and%20accumulate%0Aknowledge%20in%20an%20%22image-text-image%22%20closed%20loop.%20Concretely%2C%20to%20achieve%0Aeffective%20knowledge%20transfer%2C%20we%20design%20a%20Structured%20Semantic%20Prompt%20%28SSP%29%0Alearning%20to%20decompose%20the%20text%20prompt%20into%20several%20structured%20pairs%20to%20distill%0Aknowledge%20from%20the%20image%20space%20with%20a%20unified%20granularity%20of%20text%20description.%0AThen%2C%20we%20introduce%20a%20Knowledge%20Adaptation%20and%20Projection%20%28KAP%29%20strategy%2C%20which%0Atunes%20text%20knowledge%20via%20a%20slow-paced%20learner%20to%20adapt%20to%20different%20tasks%0Awithout%20catastrophic%20forgetting.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20%24Teata%24%20for%20LReID-Hybrid%20as%20well%20as%20on%20conventional%0ALReID%20benchmarks%20over%20advanced%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage-Text-Image%2520Knowledge%2520Transfer%2520for%2520Lifelong%2520Person%250A%2520%2520Re-Identification%2520with%2520Hybrid%2520Clothing%2520States%26entry.906535625%3DQizao%2520Wang%2520and%2520Xuelin%2520Qian%2520and%2520Bin%2520Li%2520and%2520Yanwei%2520Fu%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520With%2520the%2520continuous%2520expansion%2520of%2520intelligent%2520surveillance%2520networks%252C%2520lifelong%250Aperson%2520re-identification%2520%2528LReID%2529%2520has%2520received%2520widespread%2520attention%252C%2520pursuing%250Athe%2520need%2520of%2520self-evolution%2520across%2520different%2520domains.%2520However%252C%2520existing%2520LReID%250Astudies%2520accumulate%2520knowledge%2520with%2520the%2520assumption%2520that%2520people%2520would%2520not%2520change%250Atheir%2520clothes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520more%2520practical%2520task%252C%2520namely%2520lifelong%250Aperson%2520re-identification%2520with%2520hybrid%2520clothing%2520states%2520%2528LReID-Hybrid%2529%252C%2520which%250Atakes%2520a%2520series%2520of%2520cloth-changing%2520and%2520same-cloth%2520domains%2520into%2520account%2520during%250Alifelong%2520learning.%2520To%2520tackle%2520the%2520challenges%2520of%2520knowledge%2520granularity%2520mismatch%250Aand%2520knowledge%2520presentation%2520mismatch%2520in%2520LReID-Hybrid%252C%2520we%2520take%2520advantage%2520of%2520the%250Aconsistency%2520and%2520generalization%2520capabilities%2520of%2520the%2520text%2520space%252C%2520and%2520propose%2520a%250Anovel%2520framework%252C%2520dubbed%2520%2524Teata%2524%252C%2520to%2520effectively%2520align%252C%2520transfer%252C%2520and%2520accumulate%250Aknowledge%2520in%2520an%2520%2522image-text-image%2522%2520closed%2520loop.%2520Concretely%252C%2520to%2520achieve%250Aeffective%2520knowledge%2520transfer%252C%2520we%2520design%2520a%2520Structured%2520Semantic%2520Prompt%2520%2528SSP%2529%250Alearning%2520to%2520decompose%2520the%2520text%2520prompt%2520into%2520several%2520structured%2520pairs%2520to%2520distill%250Aknowledge%2520from%2520the%2520image%2520space%2520with%2520a%2520unified%2520granularity%2520of%2520text%2520description.%250AThen%252C%2520we%2520introduce%2520a%2520Knowledge%2520Adaptation%2520and%2520Projection%2520%2528KAP%2529%2520strategy%252C%2520which%250Atunes%2520text%2520knowledge%2520via%2520a%2520slow-paced%2520learner%2520to%2520adapt%2520to%2520different%2520tasks%250Awithout%2520catastrophic%2520forgetting.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520%2524Teata%2524%2520for%2520LReID-Hybrid%2520as%2520well%2520as%2520on%2520conventional%250ALReID%2520benchmarks%2520over%2520advanced%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image-Text-Image%20Knowledge%20Transfer%20for%20Lifelong%20Person%0A%20%20Re-Identification%20with%20Hybrid%20Clothing%20States&entry.906535625=Qizao%20Wang%20and%20Xuelin%20Qian%20and%20Bin%20Li%20and%20Yanwei%20Fu%20and%20Xiangyang%20Xue&entry.1292438233=%20%20With%20the%20continuous%20expansion%20of%20intelligent%20surveillance%20networks%2C%20lifelong%0Aperson%20re-identification%20%28LReID%29%20has%20received%20widespread%20attention%2C%20pursuing%0Athe%20need%20of%20self-evolution%20across%20different%20domains.%20However%2C%20existing%20LReID%0Astudies%20accumulate%20knowledge%20with%20the%20assumption%20that%20people%20would%20not%20change%0Atheir%20clothes.%20In%20this%20paper%2C%20we%20propose%20a%20more%20practical%20task%2C%20namely%20lifelong%0Aperson%20re-identification%20with%20hybrid%20clothing%20states%20%28LReID-Hybrid%29%2C%20which%0Atakes%20a%20series%20of%20cloth-changing%20and%20same-cloth%20domains%20into%20account%20during%0Alifelong%20learning.%20To%20tackle%20the%20challenges%20of%20knowledge%20granularity%20mismatch%0Aand%20knowledge%20presentation%20mismatch%20in%20LReID-Hybrid%2C%20we%20take%20advantage%20of%20the%0Aconsistency%20and%20generalization%20capabilities%20of%20the%20text%20space%2C%20and%20propose%20a%0Anovel%20framework%2C%20dubbed%20%24Teata%24%2C%20to%20effectively%20align%2C%20transfer%2C%20and%20accumulate%0Aknowledge%20in%20an%20%22image-text-image%22%20closed%20loop.%20Concretely%2C%20to%20achieve%0Aeffective%20knowledge%20transfer%2C%20we%20design%20a%20Structured%20Semantic%20Prompt%20%28SSP%29%0Alearning%20to%20decompose%20the%20text%20prompt%20into%20several%20structured%20pairs%20to%20distill%0Aknowledge%20from%20the%20image%20space%20with%20a%20unified%20granularity%20of%20text%20description.%0AThen%2C%20we%20introduce%20a%20Knowledge%20Adaptation%20and%20Projection%20%28KAP%29%20strategy%2C%20which%0Atunes%20text%20knowledge%20via%20a%20slow-paced%20learner%20to%20adapt%20to%20different%20tasks%0Awithout%20catastrophic%20forgetting.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20%24Teata%24%20for%20LReID-Hybrid%20as%20well%20as%20on%20conventional%0ALReID%20benchmarks%20over%20advanced%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16600v2&entry.124074799=Read"},
{"title": "Statistical Methods in Generative AI", "author": "Edgar Dobriban", "abstract": "  Generative Artificial Intelligence is emerging as an important technology,\npromising to be transformative in many areas. At the same time, generative AI\ntechniques are based on sampling from probabilistic models, and by default,\nthey come with no guarantees about correctness, safety, fairness, or other\nproperties. Statistical methods offer a promising potential approach to improve\nthe reliability of generative AI techniques. In addition, statistical methods\nare also promising for improving the quality and efficiency of AI evaluation,\nas well as for designing interventions and experiments in AI. In this paper, we\nreview some of the existing work on these topics, explaining both the general\nstatistical techniques used, as well as their applications to generative AI. We\nalso discuss limitations and potential future directions.\n", "link": "http://arxiv.org/abs/2509.07054v2", "date": "2025-09-18", "relevancy": 2.2401, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5899}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5535}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Methods%20in%20Generative%20AI&body=Title%3A%20Statistical%20Methods%20in%20Generative%20AI%0AAuthor%3A%20Edgar%20Dobriban%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20is%20emerging%20as%20an%20important%20technology%2C%0Apromising%20to%20be%20transformative%20in%20many%20areas.%20At%20the%20same%20time%2C%20generative%20AI%0Atechniques%20are%20based%20on%20sampling%20from%20probabilistic%20models%2C%20and%20by%20default%2C%0Athey%20come%20with%20no%20guarantees%20about%20correctness%2C%20safety%2C%20fairness%2C%20or%20other%0Aproperties.%20Statistical%20methods%20offer%20a%20promising%20potential%20approach%20to%20improve%0Athe%20reliability%20of%20generative%20AI%20techniques.%20In%20addition%2C%20statistical%20methods%0Aare%20also%20promising%20for%20improving%20the%20quality%20and%20efficiency%20of%20AI%20evaluation%2C%0Aas%20well%20as%20for%20designing%20interventions%20and%20experiments%20in%20AI.%20In%20this%20paper%2C%20we%0Areview%20some%20of%20the%20existing%20work%20on%20these%20topics%2C%20explaining%20both%20the%20general%0Astatistical%20techniques%20used%2C%20as%20well%20as%20their%20applications%20to%20generative%20AI.%20We%0Aalso%20discuss%20limitations%20and%20potential%20future%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Methods%2520in%2520Generative%2520AI%26entry.906535625%3DEdgar%2520Dobriban%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520is%2520emerging%2520as%2520an%2520important%2520technology%252C%250Apromising%2520to%2520be%2520transformative%2520in%2520many%2520areas.%2520At%2520the%2520same%2520time%252C%2520generative%2520AI%250Atechniques%2520are%2520based%2520on%2520sampling%2520from%2520probabilistic%2520models%252C%2520and%2520by%2520default%252C%250Athey%2520come%2520with%2520no%2520guarantees%2520about%2520correctness%252C%2520safety%252C%2520fairness%252C%2520or%2520other%250Aproperties.%2520Statistical%2520methods%2520offer%2520a%2520promising%2520potential%2520approach%2520to%2520improve%250Athe%2520reliability%2520of%2520generative%2520AI%2520techniques.%2520In%2520addition%252C%2520statistical%2520methods%250Aare%2520also%2520promising%2520for%2520improving%2520the%2520quality%2520and%2520efficiency%2520of%2520AI%2520evaluation%252C%250Aas%2520well%2520as%2520for%2520designing%2520interventions%2520and%2520experiments%2520in%2520AI.%2520In%2520this%2520paper%252C%2520we%250Areview%2520some%2520of%2520the%2520existing%2520work%2520on%2520these%2520topics%252C%2520explaining%2520both%2520the%2520general%250Astatistical%2520techniques%2520used%252C%2520as%2520well%2520as%2520their%2520applications%2520to%2520generative%2520AI.%2520We%250Aalso%2520discuss%2520limitations%2520and%2520potential%2520future%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Methods%20in%20Generative%20AI&entry.906535625=Edgar%20Dobriban&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20is%20emerging%20as%20an%20important%20technology%2C%0Apromising%20to%20be%20transformative%20in%20many%20areas.%20At%20the%20same%20time%2C%20generative%20AI%0Atechniques%20are%20based%20on%20sampling%20from%20probabilistic%20models%2C%20and%20by%20default%2C%0Athey%20come%20with%20no%20guarantees%20about%20correctness%2C%20safety%2C%20fairness%2C%20or%20other%0Aproperties.%20Statistical%20methods%20offer%20a%20promising%20potential%20approach%20to%20improve%0Athe%20reliability%20of%20generative%20AI%20techniques.%20In%20addition%2C%20statistical%20methods%0Aare%20also%20promising%20for%20improving%20the%20quality%20and%20efficiency%20of%20AI%20evaluation%2C%0Aas%20well%20as%20for%20designing%20interventions%20and%20experiments%20in%20AI.%20In%20this%20paper%2C%20we%0Areview%20some%20of%20the%20existing%20work%20on%20these%20topics%2C%20explaining%20both%20the%20general%0Astatistical%20techniques%20used%2C%20as%20well%20as%20their%20applications%20to%20generative%20AI.%20We%0Aalso%20discuss%20limitations%20and%20potential%20future%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07054v2&entry.124074799=Read"},
{"title": "Discrete optimal transport is a strong audio adversarial attack", "author": "Anton Selitskiy and Akib Shahriyar and Jishnuraj Prakasan", "abstract": "  In this paper, we show that discrete optimal transport (DOT) is an effective\nblack-box adversarial attack against modern audio anti-spoofing countermeasures\n(CMs). Our attack operates as a post-processing, distribution-alignment step:\nframe-level WavLM embeddings of generated speech are aligned to an unpaired\nbona fide pool via entropic OT and a top-$k$ barycentric projection, then\ndecoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with\nAASIST baselines, DOT yields consistently high equal error rate (EER) across\ndatasets and remains competitive after CM fine-tuning, outperforming several\nconventional attacks in cross-dataset transfer. Ablation analysis highlights\nthe practical impact of vocoder overlap. Results indicate that\ndistribution-level alignment is a powerful and stable attack surface for\ndeployed CMs.\n", "link": "http://arxiv.org/abs/2509.14959v1", "date": "2025-09-18", "relevancy": 2.2333, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4627}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4415}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20optimal%20transport%20is%20a%20strong%20audio%20adversarial%20attack&body=Title%3A%20Discrete%20optimal%20transport%20is%20a%20strong%20audio%20adversarial%20attack%0AAuthor%3A%20Anton%20Selitskiy%20and%20Akib%20Shahriyar%20and%20Jishnuraj%20Prakasan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20that%20discrete%20optimal%20transport%20%28DOT%29%20is%20an%20effective%0Ablack-box%20adversarial%20attack%20against%20modern%20audio%20anti-spoofing%20countermeasures%0A%28CMs%29.%20Our%20attack%20operates%20as%20a%20post-processing%2C%20distribution-alignment%20step%3A%0Aframe-level%20WavLM%20embeddings%20of%20generated%20speech%20are%20aligned%20to%20an%20unpaired%0Abona%20fide%20pool%20via%20entropic%20OT%20and%20a%20top-%24k%24%20barycentric%20projection%2C%20then%0Adecoded%20with%20a%20neural%20vocoder.%20Evaluated%20on%20ASVspoof2019%20and%20ASVspoof5%20with%0AAASIST%20baselines%2C%20DOT%20yields%20consistently%20high%20equal%20error%20rate%20%28EER%29%20across%0Adatasets%20and%20remains%20competitive%20after%20CM%20fine-tuning%2C%20outperforming%20several%0Aconventional%20attacks%20in%20cross-dataset%20transfer.%20Ablation%20analysis%20highlights%0Athe%20practical%20impact%20of%20vocoder%20overlap.%20Results%20indicate%20that%0Adistribution-level%20alignment%20is%20a%20powerful%20and%20stable%20attack%20surface%20for%0Adeployed%20CMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520optimal%2520transport%2520is%2520a%2520strong%2520audio%2520adversarial%2520attack%26entry.906535625%3DAnton%2520Selitskiy%2520and%2520Akib%2520Shahriyar%2520and%2520Jishnuraj%2520Prakasan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520discrete%2520optimal%2520transport%2520%2528DOT%2529%2520is%2520an%2520effective%250Ablack-box%2520adversarial%2520attack%2520against%2520modern%2520audio%2520anti-spoofing%2520countermeasures%250A%2528CMs%2529.%2520Our%2520attack%2520operates%2520as%2520a%2520post-processing%252C%2520distribution-alignment%2520step%253A%250Aframe-level%2520WavLM%2520embeddings%2520of%2520generated%2520speech%2520are%2520aligned%2520to%2520an%2520unpaired%250Abona%2520fide%2520pool%2520via%2520entropic%2520OT%2520and%2520a%2520top-%2524k%2524%2520barycentric%2520projection%252C%2520then%250Adecoded%2520with%2520a%2520neural%2520vocoder.%2520Evaluated%2520on%2520ASVspoof2019%2520and%2520ASVspoof5%2520with%250AAASIST%2520baselines%252C%2520DOT%2520yields%2520consistently%2520high%2520equal%2520error%2520rate%2520%2528EER%2529%2520across%250Adatasets%2520and%2520remains%2520competitive%2520after%2520CM%2520fine-tuning%252C%2520outperforming%2520several%250Aconventional%2520attacks%2520in%2520cross-dataset%2520transfer.%2520Ablation%2520analysis%2520highlights%250Athe%2520practical%2520impact%2520of%2520vocoder%2520overlap.%2520Results%2520indicate%2520that%250Adistribution-level%2520alignment%2520is%2520a%2520powerful%2520and%2520stable%2520attack%2520surface%2520for%250Adeployed%2520CMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20optimal%20transport%20is%20a%20strong%20audio%20adversarial%20attack&entry.906535625=Anton%20Selitskiy%20and%20Akib%20Shahriyar%20and%20Jishnuraj%20Prakasan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20that%20discrete%20optimal%20transport%20%28DOT%29%20is%20an%20effective%0Ablack-box%20adversarial%20attack%20against%20modern%20audio%20anti-spoofing%20countermeasures%0A%28CMs%29.%20Our%20attack%20operates%20as%20a%20post-processing%2C%20distribution-alignment%20step%3A%0Aframe-level%20WavLM%20embeddings%20of%20generated%20speech%20are%20aligned%20to%20an%20unpaired%0Abona%20fide%20pool%20via%20entropic%20OT%20and%20a%20top-%24k%24%20barycentric%20projection%2C%20then%0Adecoded%20with%20a%20neural%20vocoder.%20Evaluated%20on%20ASVspoof2019%20and%20ASVspoof5%20with%0AAASIST%20baselines%2C%20DOT%20yields%20consistently%20high%20equal%20error%20rate%20%28EER%29%20across%0Adatasets%20and%20remains%20competitive%20after%20CM%20fine-tuning%2C%20outperforming%20several%0Aconventional%20attacks%20in%20cross-dataset%20transfer.%20Ablation%20analysis%20highlights%0Athe%20practical%20impact%20of%20vocoder%20overlap.%20Results%20indicate%20that%0Adistribution-level%20alignment%20is%20a%20powerful%20and%20stable%20attack%20surface%20for%0Adeployed%20CMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14959v1&entry.124074799=Read"},
{"title": "[Re] Improving Interpretation Faithfulness for Vision Transformers", "author": "Izabela Kurek and Wojciech Trejter and Stipe Frkovic and Andro Erdelez", "abstract": "  This work aims to reproduce the results of Faithful Vision Transformers\n(FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for\nVision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate\nclaims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised\nSmoothing (DDS) improves interpretability robustness to (1) attacks in a\nsegmentation task and (2) perturbation and attacks in a classification task. We\nalso extend the original study by investigating the authors' claims that adding\nDDS to any interpretability method can improve its robustness under attack.\nThis is tested on baseline methods and the recently proposed Attribution\nRollout method. In addition, we measure the computational costs and\nenvironmental impact of obtaining an FViT through DDS. Our results broadly\nagree with the original study's findings, although minor discrepancies were\nfound and discussed.\n", "link": "http://arxiv.org/abs/2509.14846v1", "date": "2025-09-18", "relevancy": 2.2265, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5889}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%5BRe%5D%20Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers&body=Title%3A%20%5BRe%5D%20Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers%0AAuthor%3A%20Izabela%20Kurek%20and%20Wojciech%20Trejter%20and%20Stipe%20Frkovic%20and%20Andro%20Erdelez%0AAbstract%3A%20%20%20This%20work%20aims%20to%20reproduce%20the%20results%20of%20Faithful%20Vision%20Transformers%0A%28FViTs%29%20proposed%20by%20arXiv%3A2311.17983%20alongside%20interpretability%20methods%20for%0AVision%20Transformers%20from%20arXiv%3A2012.09838%20and%20Xu%20%282022%29%20et%20al.%20We%20investigate%0Aclaims%20made%20by%20arXiv%3A2311.17983%2C%20namely%20that%20the%20usage%20of%20Diffusion%20Denoised%0ASmoothing%20%28DDS%29%20improves%20interpretability%20robustness%20to%20%281%29%20attacks%20in%20a%0Asegmentation%20task%20and%20%282%29%20perturbation%20and%20attacks%20in%20a%20classification%20task.%20We%0Aalso%20extend%20the%20original%20study%20by%20investigating%20the%20authors%27%20claims%20that%20adding%0ADDS%20to%20any%20interpretability%20method%20can%20improve%20its%20robustness%20under%20attack.%0AThis%20is%20tested%20on%20baseline%20methods%20and%20the%20recently%20proposed%20Attribution%0ARollout%20method.%20In%20addition%2C%20we%20measure%20the%20computational%20costs%20and%0Aenvironmental%20impact%20of%20obtaining%20an%20FViT%20through%20DDS.%20Our%20results%20broadly%0Aagree%20with%20the%20original%20study%27s%20findings%2C%20although%20minor%20discrepancies%20were%0Afound%20and%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%255BRe%255D%2520Improving%2520Interpretation%2520Faithfulness%2520for%2520Vision%2520Transformers%26entry.906535625%3DIzabela%2520Kurek%2520and%2520Wojciech%2520Trejter%2520and%2520Stipe%2520Frkovic%2520and%2520Andro%2520Erdelez%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520reproduce%2520the%2520results%2520of%2520Faithful%2520Vision%2520Transformers%250A%2528FViTs%2529%2520proposed%2520by%2520arXiv%253A2311.17983%2520alongside%2520interpretability%2520methods%2520for%250AVision%2520Transformers%2520from%2520arXiv%253A2012.09838%2520and%2520Xu%2520%25282022%2529%2520et%2520al.%2520We%2520investigate%250Aclaims%2520made%2520by%2520arXiv%253A2311.17983%252C%2520namely%2520that%2520the%2520usage%2520of%2520Diffusion%2520Denoised%250ASmoothing%2520%2528DDS%2529%2520improves%2520interpretability%2520robustness%2520to%2520%25281%2529%2520attacks%2520in%2520a%250Asegmentation%2520task%2520and%2520%25282%2529%2520perturbation%2520and%2520attacks%2520in%2520a%2520classification%2520task.%2520We%250Aalso%2520extend%2520the%2520original%2520study%2520by%2520investigating%2520the%2520authors%2527%2520claims%2520that%2520adding%250ADDS%2520to%2520any%2520interpretability%2520method%2520can%2520improve%2520its%2520robustness%2520under%2520attack.%250AThis%2520is%2520tested%2520on%2520baseline%2520methods%2520and%2520the%2520recently%2520proposed%2520Attribution%250ARollout%2520method.%2520In%2520addition%252C%2520we%2520measure%2520the%2520computational%2520costs%2520and%250Aenvironmental%2520impact%2520of%2520obtaining%2520an%2520FViT%2520through%2520DDS.%2520Our%2520results%2520broadly%250Aagree%2520with%2520the%2520original%2520study%2527s%2520findings%252C%2520although%2520minor%2520discrepancies%2520were%250Afound%2520and%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%5BRe%5D%20Improving%20Interpretation%20Faithfulness%20for%20Vision%20Transformers&entry.906535625=Izabela%20Kurek%20and%20Wojciech%20Trejter%20and%20Stipe%20Frkovic%20and%20Andro%20Erdelez&entry.1292438233=%20%20This%20work%20aims%20to%20reproduce%20the%20results%20of%20Faithful%20Vision%20Transformers%0A%28FViTs%29%20proposed%20by%20arXiv%3A2311.17983%20alongside%20interpretability%20methods%20for%0AVision%20Transformers%20from%20arXiv%3A2012.09838%20and%20Xu%20%282022%29%20et%20al.%20We%20investigate%0Aclaims%20made%20by%20arXiv%3A2311.17983%2C%20namely%20that%20the%20usage%20of%20Diffusion%20Denoised%0ASmoothing%20%28DDS%29%20improves%20interpretability%20robustness%20to%20%281%29%20attacks%20in%20a%0Asegmentation%20task%20and%20%282%29%20perturbation%20and%20attacks%20in%20a%20classification%20task.%20We%0Aalso%20extend%20the%20original%20study%20by%20investigating%20the%20authors%27%20claims%20that%20adding%0ADDS%20to%20any%20interpretability%20method%20can%20improve%20its%20robustness%20under%20attack.%0AThis%20is%20tested%20on%20baseline%20methods%20and%20the%20recently%20proposed%20Attribution%0ARollout%20method.%20In%20addition%2C%20we%20measure%20the%20computational%20costs%20and%0Aenvironmental%20impact%20of%20obtaining%20an%20FViT%20through%20DDS.%20Our%20results%20broadly%0Aagree%20with%20the%20original%20study%27s%20findings%2C%20although%20minor%20discrepancies%20were%0Afound%20and%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14846v1&entry.124074799=Read"},
{"title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation", "author": "Yuming Jiang and Siteng Huang and Shengke Xue and Yaxi Zhao and Jun Cen and Sicong Leng and Kehan Li and Jiayan Guo and Kexiang Wang and Mingxiu Chen and Fan Wang and Deli Zhao and Xin Li", "abstract": "  This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.\n", "link": "http://arxiv.org/abs/2509.15212v1", "date": "2025-09-18", "relevancy": 2.2146, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5733}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5413}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RynnVLA-001%3A%20Using%20Human%20Demonstrations%20to%20Improve%20Robot%20Manipulation&body=Title%3A%20RynnVLA-001%3A%20Using%20Human%20Demonstrations%20to%20Improve%20Robot%20Manipulation%0AAuthor%3A%20Yuming%20Jiang%20and%20Siteng%20Huang%20and%20Shengke%20Xue%20and%20Yaxi%20Zhao%20and%20Jun%20Cen%20and%20Sicong%20Leng%20and%20Kehan%20Li%20and%20Jiayan%20Guo%20and%20Kexiang%20Wang%20and%20Mingxiu%20Chen%20and%20Fan%20Wang%20and%20Deli%20Zhao%20and%20Xin%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20RynnVLA-001%2C%20a%20vision-language-action%28VLA%29%20model%20built%0Aupon%20large-scale%20video%20generative%20pretraining%20from%20human%20demonstrations.%20We%0Apropose%20a%20novel%20two-stage%20pretraining%20methodology.%20The%20first%20stage%2C%20Ego-Centric%0AVideo%20Generative%20Pretraining%2C%20trains%20an%20Image-to-Video%20model%20on%2012M%20ego-centric%0Amanipulation%20videos%20to%20predict%20future%20frames%20conditioned%20on%20an%20initial%20frame%0Aand%20a%20language%20instruction.%20The%20second%20stage%2C%20Human-Centric%20Trajectory-Aware%0AModeling%2C%20extends%20this%20by%20jointly%20predicting%20future%20keypoint%20trajectories%2C%0Athereby%20effectively%20bridging%20visual%20frame%20prediction%20with%20action%20prediction.%0AFurthermore%2C%20to%20enhance%20action%20representation%2C%20we%20propose%20ActionVAE%2C%20a%0Avariational%20autoencoder%20that%20compresses%20sequences%20of%20actions%20into%20compact%0Alatent%20embeddings%2C%20reducing%20the%20complexity%20of%20the%20VLA%20output%20space.%20When%0Afinetuned%20on%20the%20same%20downstream%20robotics%20datasets%2C%20RynnVLA-001%20achieves%0Asuperior%20performance%20over%20state-of-the-art%20baselines%2C%20demonstrating%20that%20the%0Aproposed%20pretraining%20strategy%20provides%20a%20more%20effective%20initialization%20for%20VLA%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRynnVLA-001%253A%2520Using%2520Human%2520Demonstrations%2520to%2520Improve%2520Robot%2520Manipulation%26entry.906535625%3DYuming%2520Jiang%2520and%2520Siteng%2520Huang%2520and%2520Shengke%2520Xue%2520and%2520Yaxi%2520Zhao%2520and%2520Jun%2520Cen%2520and%2520Sicong%2520Leng%2520and%2520Kehan%2520Li%2520and%2520Jiayan%2520Guo%2520and%2520Kexiang%2520Wang%2520and%2520Mingxiu%2520Chen%2520and%2520Fan%2520Wang%2520and%2520Deli%2520Zhao%2520and%2520Xin%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520RynnVLA-001%252C%2520a%2520vision-language-action%2528VLA%2529%2520model%2520built%250Aupon%2520large-scale%2520video%2520generative%2520pretraining%2520from%2520human%2520demonstrations.%2520We%250Apropose%2520a%2520novel%2520two-stage%2520pretraining%2520methodology.%2520The%2520first%2520stage%252C%2520Ego-Centric%250AVideo%2520Generative%2520Pretraining%252C%2520trains%2520an%2520Image-to-Video%2520model%2520on%252012M%2520ego-centric%250Amanipulation%2520videos%2520to%2520predict%2520future%2520frames%2520conditioned%2520on%2520an%2520initial%2520frame%250Aand%2520a%2520language%2520instruction.%2520The%2520second%2520stage%252C%2520Human-Centric%2520Trajectory-Aware%250AModeling%252C%2520extends%2520this%2520by%2520jointly%2520predicting%2520future%2520keypoint%2520trajectories%252C%250Athereby%2520effectively%2520bridging%2520visual%2520frame%2520prediction%2520with%2520action%2520prediction.%250AFurthermore%252C%2520to%2520enhance%2520action%2520representation%252C%2520we%2520propose%2520ActionVAE%252C%2520a%250Avariational%2520autoencoder%2520that%2520compresses%2520sequences%2520of%2520actions%2520into%2520compact%250Alatent%2520embeddings%252C%2520reducing%2520the%2520complexity%2520of%2520the%2520VLA%2520output%2520space.%2520When%250Afinetuned%2520on%2520the%2520same%2520downstream%2520robotics%2520datasets%252C%2520RynnVLA-001%2520achieves%250Asuperior%2520performance%2520over%2520state-of-the-art%2520baselines%252C%2520demonstrating%2520that%2520the%250Aproposed%2520pretraining%2520strategy%2520provides%2520a%2520more%2520effective%2520initialization%2520for%2520VLA%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RynnVLA-001%3A%20Using%20Human%20Demonstrations%20to%20Improve%20Robot%20Manipulation&entry.906535625=Yuming%20Jiang%20and%20Siteng%20Huang%20and%20Shengke%20Xue%20and%20Yaxi%20Zhao%20and%20Jun%20Cen%20and%20Sicong%20Leng%20and%20Kehan%20Li%20and%20Jiayan%20Guo%20and%20Kexiang%20Wang%20and%20Mingxiu%20Chen%20and%20Fan%20Wang%20and%20Deli%20Zhao%20and%20Xin%20Li&entry.1292438233=%20%20This%20paper%20presents%20RynnVLA-001%2C%20a%20vision-language-action%28VLA%29%20model%20built%0Aupon%20large-scale%20video%20generative%20pretraining%20from%20human%20demonstrations.%20We%0Apropose%20a%20novel%20two-stage%20pretraining%20methodology.%20The%20first%20stage%2C%20Ego-Centric%0AVideo%20Generative%20Pretraining%2C%20trains%20an%20Image-to-Video%20model%20on%2012M%20ego-centric%0Amanipulation%20videos%20to%20predict%20future%20frames%20conditioned%20on%20an%20initial%20frame%0Aand%20a%20language%20instruction.%20The%20second%20stage%2C%20Human-Centric%20Trajectory-Aware%0AModeling%2C%20extends%20this%20by%20jointly%20predicting%20future%20keypoint%20trajectories%2C%0Athereby%20effectively%20bridging%20visual%20frame%20prediction%20with%20action%20prediction.%0AFurthermore%2C%20to%20enhance%20action%20representation%2C%20we%20propose%20ActionVAE%2C%20a%0Avariational%20autoencoder%20that%20compresses%20sequences%20of%20actions%20into%20compact%0Alatent%20embeddings%2C%20reducing%20the%20complexity%20of%20the%20VLA%20output%20space.%20When%0Afinetuned%20on%20the%20same%20downstream%20robotics%20datasets%2C%20RynnVLA-001%20achieves%0Asuperior%20performance%20over%20state-of-the-art%20baselines%2C%20demonstrating%20that%20the%0Aproposed%20pretraining%20strategy%20provides%20a%20more%20effective%20initialization%20for%20VLA%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15212v1&entry.124074799=Read"},
{"title": "Advanced Physics-Informed Neural Network with Residuals for Solving\n  Complex Integral Equations", "author": "Mahdi Movahedian Moghaddam and Kourosh Parand and Saeed Reza Kheradpisheh", "abstract": "  In this paper, we present the Residual Integral Solver Network (RISN), a\nnovel neural network architecture designed to solve a wide range of integral\nand integro-differential equations, including one-dimensional,\nmulti-dimensional, ordinary and partial integro-differential, systems,\nfractional types, and Helmholtz-type integral equations involving oscillatory\nkernels. RISN integrates residual connections with high-accuracy numerical\nmethods such as Gaussian quadrature and fractional derivative operational\nmatrices, enabling it to achieve higher accuracy and stability than traditional\nPhysics-Informed Neural Networks (PINN). The residual connections help mitigate\nvanishing gradient issues, allowing RISN to handle deeper networks and more\ncomplex kernels, particularly in multi-dimensional problems. Through extensive\nexperiments, we demonstrate that RISN consistently outperforms not only\nclassical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and\nSelf-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute\nErrors (MAE) across various types of equations. These results highlight RISN's\nrobustness and efficiency in solving challenging integral and\nintegro-differential problems, making it a valuable tool for real-world\napplications where traditional methods often struggle.\n", "link": "http://arxiv.org/abs/2501.16370v3", "date": "2025-09-18", "relevancy": 2.2114, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations&body=Title%3A%20Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations%0AAuthor%3A%20Mahdi%20Movahedian%20Moghaddam%20and%20Kourosh%20Parand%20and%20Saeed%20Reza%20Kheradpisheh%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20Residual%20Integral%20Solver%20Network%20%28RISN%29%2C%20a%0Anovel%20neural%20network%20architecture%20designed%20to%20solve%20a%20wide%20range%20of%20integral%0Aand%20integro-differential%20equations%2C%20including%20one-dimensional%2C%0Amulti-dimensional%2C%20ordinary%20and%20partial%20integro-differential%2C%20systems%2C%0Afractional%20types%2C%20and%20Helmholtz-type%20integral%20equations%20involving%20oscillatory%0Akernels.%20RISN%20integrates%20residual%20connections%20with%20high-accuracy%20numerical%0Amethods%20such%20as%20Gaussian%20quadrature%20and%20fractional%20derivative%20operational%0Amatrices%2C%20enabling%20it%20to%20achieve%20higher%20accuracy%20and%20stability%20than%20traditional%0APhysics-Informed%20Neural%20Networks%20%28PINN%29.%20The%20residual%20connections%20help%20mitigate%0Avanishing%20gradient%20issues%2C%20allowing%20RISN%20to%20handle%20deeper%20networks%20and%20more%0Acomplex%20kernels%2C%20particularly%20in%20multi-dimensional%20problems.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20RISN%20consistently%20outperforms%20not%20only%0Aclassical%20PINNs%20but%20also%20advanced%20variants%20such%20as%20Auxiliary%20PINN%20%28A-PINN%29%20and%0ASelf-Adaptive%20PINN%20%28SA-PINN%29%2C%20achieving%20significantly%20lower%20Mean%20Absolute%0AErrors%20%28MAE%29%20across%20various%20types%20of%20equations.%20These%20results%20highlight%20RISN%27s%0Arobustness%20and%20efficiency%20in%20solving%20challenging%20integral%20and%0Aintegro-differential%20problems%2C%20making%20it%20a%20valuable%20tool%20for%20real-world%0Aapplications%20where%20traditional%20methods%20often%20struggle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.16370v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Physics-Informed%2520Neural%2520Network%2520with%2520Residuals%2520for%2520Solving%250A%2520%2520Complex%2520Integral%2520Equations%26entry.906535625%3DMahdi%2520Movahedian%2520Moghaddam%2520and%2520Kourosh%2520Parand%2520and%2520Saeed%2520Reza%2520Kheradpisheh%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Residual%2520Integral%2520Solver%2520Network%2520%2528RISN%2529%252C%2520a%250Anovel%2520neural%2520network%2520architecture%2520designed%2520to%2520solve%2520a%2520wide%2520range%2520of%2520integral%250Aand%2520integro-differential%2520equations%252C%2520including%2520one-dimensional%252C%250Amulti-dimensional%252C%2520ordinary%2520and%2520partial%2520integro-differential%252C%2520systems%252C%250Afractional%2520types%252C%2520and%2520Helmholtz-type%2520integral%2520equations%2520involving%2520oscillatory%250Akernels.%2520RISN%2520integrates%2520residual%2520connections%2520with%2520high-accuracy%2520numerical%250Amethods%2520such%2520as%2520Gaussian%2520quadrature%2520and%2520fractional%2520derivative%2520operational%250Amatrices%252C%2520enabling%2520it%2520to%2520achieve%2520higher%2520accuracy%2520and%2520stability%2520than%2520traditional%250APhysics-Informed%2520Neural%2520Networks%2520%2528PINN%2529.%2520The%2520residual%2520connections%2520help%2520mitigate%250Avanishing%2520gradient%2520issues%252C%2520allowing%2520RISN%2520to%2520handle%2520deeper%2520networks%2520and%2520more%250Acomplex%2520kernels%252C%2520particularly%2520in%2520multi-dimensional%2520problems.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520RISN%2520consistently%2520outperforms%2520not%2520only%250Aclassical%2520PINNs%2520but%2520also%2520advanced%2520variants%2520such%2520as%2520Auxiliary%2520PINN%2520%2528A-PINN%2529%2520and%250ASelf-Adaptive%2520PINN%2520%2528SA-PINN%2529%252C%2520achieving%2520significantly%2520lower%2520Mean%2520Absolute%250AErrors%2520%2528MAE%2529%2520across%2520various%2520types%2520of%2520equations.%2520These%2520results%2520highlight%2520RISN%2527s%250Arobustness%2520and%2520efficiency%2520in%2520solving%2520challenging%2520integral%2520and%250Aintegro-differential%2520problems%252C%2520making%2520it%2520a%2520valuable%2520tool%2520for%2520real-world%250Aapplications%2520where%2520traditional%2520methods%2520often%2520struggle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.16370v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Physics-Informed%20Neural%20Network%20with%20Residuals%20for%20Solving%0A%20%20Complex%20Integral%20Equations&entry.906535625=Mahdi%20Movahedian%20Moghaddam%20and%20Kourosh%20Parand%20and%20Saeed%20Reza%20Kheradpisheh&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20Residual%20Integral%20Solver%20Network%20%28RISN%29%2C%20a%0Anovel%20neural%20network%20architecture%20designed%20to%20solve%20a%20wide%20range%20of%20integral%0Aand%20integro-differential%20equations%2C%20including%20one-dimensional%2C%0Amulti-dimensional%2C%20ordinary%20and%20partial%20integro-differential%2C%20systems%2C%0Afractional%20types%2C%20and%20Helmholtz-type%20integral%20equations%20involving%20oscillatory%0Akernels.%20RISN%20integrates%20residual%20connections%20with%20high-accuracy%20numerical%0Amethods%20such%20as%20Gaussian%20quadrature%20and%20fractional%20derivative%20operational%0Amatrices%2C%20enabling%20it%20to%20achieve%20higher%20accuracy%20and%20stability%20than%20traditional%0APhysics-Informed%20Neural%20Networks%20%28PINN%29.%20The%20residual%20connections%20help%20mitigate%0Avanishing%20gradient%20issues%2C%20allowing%20RISN%20to%20handle%20deeper%20networks%20and%20more%0Acomplex%20kernels%2C%20particularly%20in%20multi-dimensional%20problems.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20RISN%20consistently%20outperforms%20not%20only%0Aclassical%20PINNs%20but%20also%20advanced%20variants%20such%20as%20Auxiliary%20PINN%20%28A-PINN%29%20and%0ASelf-Adaptive%20PINN%20%28SA-PINN%29%2C%20achieving%20significantly%20lower%20Mean%20Absolute%0AErrors%20%28MAE%29%20across%20various%20types%20of%20equations.%20These%20results%20highlight%20RISN%27s%0Arobustness%20and%20efficiency%20in%20solving%20challenging%20integral%20and%0Aintegro-differential%20problems%2C%20making%20it%20a%20valuable%20tool%20for%20real-world%0Aapplications%20where%20traditional%20methods%20often%20struggle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.16370v3&entry.124074799=Read"},
{"title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains", "author": "Rogerio Guimaraes and Frank Xiao and Pietro Perona and Markus Marks", "abstract": "  Humans can recognize the same actions despite large context and viewpoint\nvariations, such as differences between species (walking in spiders vs.\nhorses), viewpoints (egocentric vs. third-person), and contexts (real life vs\nmovies). Current deep learning models struggle with such generalization. We\npropose using features generated by a Vision Diffusion Model (VDM), aggregated\nvia a transformer, to achieve human-like action recognition across these\nchallenging conditions. We find that generalization is enhanced by the use of a\nmodel conditioned on earlier timesteps of the diffusion process to highlight\nsemantic information over pixel level details in the extracted features. We\nexperimentally explore the generalization properties of our approach in\nclassifying actions across animal species, across different viewing angles, and\ndifferent recording contexts. Our model sets a new state-of-the-art across all\nthree generalization benchmarks, bringing machine action recognition closer to\nhuman-like robustness. Project page:\n$\\href{https://www.vision.caltech.edu/actiondiff/}{\\text{vision.caltech.edu/actiondiff}}$\nCode:\n$\\href{https://github.com/frankyaoxiao/ActionDiff}{\\text{github.com/frankyaoxiao/ActionDiff}}$\n", "link": "http://arxiv.org/abs/2509.08908v2", "date": "2025-09-18", "relevancy": 2.21, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5595}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5512}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Action%20Recognition%20Generalizes%20to%20Untrained%20Domains&body=Title%3A%20Diffusion-Based%20Action%20Recognition%20Generalizes%20to%20Untrained%20Domains%0AAuthor%3A%20Rogerio%20Guimaraes%20and%20Frank%20Xiao%20and%20Pietro%20Perona%20and%20Markus%20Marks%0AAbstract%3A%20%20%20Humans%20can%20recognize%20the%20same%20actions%20despite%20large%20context%20and%20viewpoint%0Avariations%2C%20such%20as%20differences%20between%20species%20%28walking%20in%20spiders%20vs.%0Ahorses%29%2C%20viewpoints%20%28egocentric%20vs.%20third-person%29%2C%20and%20contexts%20%28real%20life%20vs%0Amovies%29.%20Current%20deep%20learning%20models%20struggle%20with%20such%20generalization.%20We%0Apropose%20using%20features%20generated%20by%20a%20Vision%20Diffusion%20Model%20%28VDM%29%2C%20aggregated%0Avia%20a%20transformer%2C%20to%20achieve%20human-like%20action%20recognition%20across%20these%0Achallenging%20conditions.%20We%20find%20that%20generalization%20is%20enhanced%20by%20the%20use%20of%20a%0Amodel%20conditioned%20on%20earlier%20timesteps%20of%20the%20diffusion%20process%20to%20highlight%0Asemantic%20information%20over%20pixel%20level%20details%20in%20the%20extracted%20features.%20We%0Aexperimentally%20explore%20the%20generalization%20properties%20of%20our%20approach%20in%0Aclassifying%20actions%20across%20animal%20species%2C%20across%20different%20viewing%20angles%2C%20and%0Adifferent%20recording%20contexts.%20Our%20model%20sets%20a%20new%20state-of-the-art%20across%20all%0Athree%20generalization%20benchmarks%2C%20bringing%20machine%20action%20recognition%20closer%20to%0Ahuman-like%20robustness.%20Project%20page%3A%0A%24%5Chref%7Bhttps%3A//www.vision.caltech.edu/actiondiff/%7D%7B%5Ctext%7Bvision.caltech.edu/actiondiff%7D%7D%24%0ACode%3A%0A%24%5Chref%7Bhttps%3A//github.com/frankyaoxiao/ActionDiff%7D%7B%5Ctext%7Bgithub.com/frankyaoxiao/ActionDiff%7D%7D%24%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.08908v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Based%2520Action%2520Recognition%2520Generalizes%2520to%2520Untrained%2520Domains%26entry.906535625%3DRogerio%2520Guimaraes%2520and%2520Frank%2520Xiao%2520and%2520Pietro%2520Perona%2520and%2520Markus%2520Marks%26entry.1292438233%3D%2520%2520Humans%2520can%2520recognize%2520the%2520same%2520actions%2520despite%2520large%2520context%2520and%2520viewpoint%250Avariations%252C%2520such%2520as%2520differences%2520between%2520species%2520%2528walking%2520in%2520spiders%2520vs.%250Ahorses%2529%252C%2520viewpoints%2520%2528egocentric%2520vs.%2520third-person%2529%252C%2520and%2520contexts%2520%2528real%2520life%2520vs%250Amovies%2529.%2520Current%2520deep%2520learning%2520models%2520struggle%2520with%2520such%2520generalization.%2520We%250Apropose%2520using%2520features%2520generated%2520by%2520a%2520Vision%2520Diffusion%2520Model%2520%2528VDM%2529%252C%2520aggregated%250Avia%2520a%2520transformer%252C%2520to%2520achieve%2520human-like%2520action%2520recognition%2520across%2520these%250Achallenging%2520conditions.%2520We%2520find%2520that%2520generalization%2520is%2520enhanced%2520by%2520the%2520use%2520of%2520a%250Amodel%2520conditioned%2520on%2520earlier%2520timesteps%2520of%2520the%2520diffusion%2520process%2520to%2520highlight%250Asemantic%2520information%2520over%2520pixel%2520level%2520details%2520in%2520the%2520extracted%2520features.%2520We%250Aexperimentally%2520explore%2520the%2520generalization%2520properties%2520of%2520our%2520approach%2520in%250Aclassifying%2520actions%2520across%2520animal%2520species%252C%2520across%2520different%2520viewing%2520angles%252C%2520and%250Adifferent%2520recording%2520contexts.%2520Our%2520model%2520sets%2520a%2520new%2520state-of-the-art%2520across%2520all%250Athree%2520generalization%2520benchmarks%252C%2520bringing%2520machine%2520action%2520recognition%2520closer%2520to%250Ahuman-like%2520robustness.%2520Project%2520page%253A%250A%2524%255Chref%257Bhttps%253A//www.vision.caltech.edu/actiondiff/%257D%257B%255Ctext%257Bvision.caltech.edu/actiondiff%257D%257D%2524%250ACode%253A%250A%2524%255Chref%257Bhttps%253A//github.com/frankyaoxiao/ActionDiff%257D%257B%255Ctext%257Bgithub.com/frankyaoxiao/ActionDiff%257D%257D%2524%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08908v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Action%20Recognition%20Generalizes%20to%20Untrained%20Domains&entry.906535625=Rogerio%20Guimaraes%20and%20Frank%20Xiao%20and%20Pietro%20Perona%20and%20Markus%20Marks&entry.1292438233=%20%20Humans%20can%20recognize%20the%20same%20actions%20despite%20large%20context%20and%20viewpoint%0Avariations%2C%20such%20as%20differences%20between%20species%20%28walking%20in%20spiders%20vs.%0Ahorses%29%2C%20viewpoints%20%28egocentric%20vs.%20third-person%29%2C%20and%20contexts%20%28real%20life%20vs%0Amovies%29.%20Current%20deep%20learning%20models%20struggle%20with%20such%20generalization.%20We%0Apropose%20using%20features%20generated%20by%20a%20Vision%20Diffusion%20Model%20%28VDM%29%2C%20aggregated%0Avia%20a%20transformer%2C%20to%20achieve%20human-like%20action%20recognition%20across%20these%0Achallenging%20conditions.%20We%20find%20that%20generalization%20is%20enhanced%20by%20the%20use%20of%20a%0Amodel%20conditioned%20on%20earlier%20timesteps%20of%20the%20diffusion%20process%20to%20highlight%0Asemantic%20information%20over%20pixel%20level%20details%20in%20the%20extracted%20features.%20We%0Aexperimentally%20explore%20the%20generalization%20properties%20of%20our%20approach%20in%0Aclassifying%20actions%20across%20animal%20species%2C%20across%20different%20viewing%20angles%2C%20and%0Adifferent%20recording%20contexts.%20Our%20model%20sets%20a%20new%20state-of-the-art%20across%20all%0Athree%20generalization%20benchmarks%2C%20bringing%20machine%20action%20recognition%20closer%20to%0Ahuman-like%20robustness.%20Project%20page%3A%0A%24%5Chref%7Bhttps%3A//www.vision.caltech.edu/actiondiff/%7D%7B%5Ctext%7Bvision.caltech.edu/actiondiff%7D%7D%24%0ACode%3A%0A%24%5Chref%7Bhttps%3A//github.com/frankyaoxiao/ActionDiff%7D%7B%5Ctext%7Bgithub.com/frankyaoxiao/ActionDiff%7D%7D%24%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.08908v2&entry.124074799=Read"},
{"title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding", "author": "Zaiquan Yang and Yuhao Liu and Gerhard Hancke and Rynson W. H. Lau", "abstract": "  Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.\n", "link": "http://arxiv.org/abs/2509.15178v1", "date": "2025-09-18", "relevancy": 2.2038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5452}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Multimodal%20LLMs%20for%20Zero-Shot%0A%20%20Spatio-Temporal%20Video%20Grounding&body=Title%3A%20Unleashing%20the%20Potential%20of%20Multimodal%20LLMs%20for%20Zero-Shot%0A%20%20Spatio-Temporal%20Video%20Grounding%0AAuthor%3A%20Zaiquan%20Yang%20and%20Yuhao%20Liu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Spatio-temporal%20video%20grounding%20%28STVG%29%20aims%20at%20localizing%20the%20spatio-temporal%0Atube%20of%20a%20video%2C%20as%20specified%20by%20the%20input%20text%20query.%20In%20this%20paper%2C%20we%0Autilize%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20explore%20a%20zero-shot%0Asolution%20in%20STVG.%20We%20reveal%20two%20key%20insights%20about%20MLLMs%3A%20%281%29%20MLLMs%20tend%20to%0Adynamically%20assign%20special%20tokens%2C%20referred%20to%20as%20%5Ctextit%7Bgrounding%20tokens%7D%2C%0Afor%20grounding%20the%20text%20query%3B%20and%20%282%29%20MLLMs%20often%20suffer%20from%20suboptimal%0Agrounding%20due%20to%20the%20inability%20to%20fully%20integrate%20the%20cues%20in%20the%20text%20query%0A%28%5Ctextit%7Be.g.%7D%2C%20attributes%2C%20actions%29%20for%20inference.%20Based%20on%20these%20insights%2C%20we%0Apropose%20a%20MLLM-based%20zero-shot%20framework%20for%20STVG%2C%20which%20includes%20novel%0Adecomposed%20spatio-temporal%20highlighting%20%28DSTH%29%20and%20temporal-augmented%0Aassembling%20%28TAS%29%20strategies%20to%20unleash%20the%20reasoning%20ability%20of%20MLLMs.%20The%20DSTH%0Astrategy%20first%20decouples%20the%20original%20query%20into%20attribute%20and%20action%0Asub-queries%20for%20inquiring%20the%20existence%20of%20the%20target%20both%20spatially%20and%0Atemporally.%20It%20then%20uses%20a%20novel%20logit-guided%20re-attention%20%28LRA%29%20module%20to%0Alearn%20latent%20variables%20as%20spatial%20and%20temporal%20prompts%2C%20by%20regularizing%20token%0Apredictions%20for%20each%20sub-query.%20These%20prompts%20highlight%20attribute%20and%20action%0Acues%2C%20respectively%2C%20directing%20the%20model%27s%20attention%20to%20reliable%20spatial%20and%0Atemporal%20related%20visual%20regions.%20In%20addition%2C%20as%20the%20spatial%20grounding%20by%20the%0Aattribute%20sub-query%20should%20be%20temporally%20consistent%2C%20we%20introduce%20the%20TAS%0Astrategy%20to%20assemble%20the%20predictions%20using%20the%20original%20video%20frames%20and%20the%0Atemporal-augmented%20frames%20as%20inputs%20to%20help%20improve%20temporal%20consistency.%20We%0Aevaluate%20our%20method%20on%20various%20MLLMs%2C%20and%20show%20that%20it%20outperforms%20SOTA%20methods%0Aon%20three%20common%20STVG%20benchmarks.%0A%20%20The%20code%20will%20be%20available%20at%20https%3A//github.com/zaiquanyang/LLaVA_Next_STVG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Potential%2520of%2520Multimodal%2520LLMs%2520for%2520Zero-Shot%250A%2520%2520Spatio-Temporal%2520Video%2520Grounding%26entry.906535625%3DZaiquan%2520Yang%2520and%2520Yuhao%2520Liu%2520and%2520Gerhard%2520Hancke%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Spatio-temporal%2520video%2520grounding%2520%2528STVG%2529%2520aims%2520at%2520localizing%2520the%2520spatio-temporal%250Atube%2520of%2520a%2520video%252C%2520as%2520specified%2520by%2520the%2520input%2520text%2520query.%2520In%2520this%2520paper%252C%2520we%250Autilize%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520explore%2520a%2520zero-shot%250Asolution%2520in%2520STVG.%2520We%2520reveal%2520two%2520key%2520insights%2520about%2520MLLMs%253A%2520%25281%2529%2520MLLMs%2520tend%2520to%250Adynamically%2520assign%2520special%2520tokens%252C%2520referred%2520to%2520as%2520%255Ctextit%257Bgrounding%2520tokens%257D%252C%250Afor%2520grounding%2520the%2520text%2520query%253B%2520and%2520%25282%2529%2520MLLMs%2520often%2520suffer%2520from%2520suboptimal%250Agrounding%2520due%2520to%2520the%2520inability%2520to%2520fully%2520integrate%2520the%2520cues%2520in%2520the%2520text%2520query%250A%2528%255Ctextit%257Be.g.%257D%252C%2520attributes%252C%2520actions%2529%2520for%2520inference.%2520Based%2520on%2520these%2520insights%252C%2520we%250Apropose%2520a%2520MLLM-based%2520zero-shot%2520framework%2520for%2520STVG%252C%2520which%2520includes%2520novel%250Adecomposed%2520spatio-temporal%2520highlighting%2520%2528DSTH%2529%2520and%2520temporal-augmented%250Aassembling%2520%2528TAS%2529%2520strategies%2520to%2520unleash%2520the%2520reasoning%2520ability%2520of%2520MLLMs.%2520The%2520DSTH%250Astrategy%2520first%2520decouples%2520the%2520original%2520query%2520into%2520attribute%2520and%2520action%250Asub-queries%2520for%2520inquiring%2520the%2520existence%2520of%2520the%2520target%2520both%2520spatially%2520and%250Atemporally.%2520It%2520then%2520uses%2520a%2520novel%2520logit-guided%2520re-attention%2520%2528LRA%2529%2520module%2520to%250Alearn%2520latent%2520variables%2520as%2520spatial%2520and%2520temporal%2520prompts%252C%2520by%2520regularizing%2520token%250Apredictions%2520for%2520each%2520sub-query.%2520These%2520prompts%2520highlight%2520attribute%2520and%2520action%250Acues%252C%2520respectively%252C%2520directing%2520the%2520model%2527s%2520attention%2520to%2520reliable%2520spatial%2520and%250Atemporal%2520related%2520visual%2520regions.%2520In%2520addition%252C%2520as%2520the%2520spatial%2520grounding%2520by%2520the%250Aattribute%2520sub-query%2520should%2520be%2520temporally%2520consistent%252C%2520we%2520introduce%2520the%2520TAS%250Astrategy%2520to%2520assemble%2520the%2520predictions%2520using%2520the%2520original%2520video%2520frames%2520and%2520the%250Atemporal-augmented%2520frames%2520as%2520inputs%2520to%2520help%2520improve%2520temporal%2520consistency.%2520We%250Aevaluate%2520our%2520method%2520on%2520various%2520MLLMs%252C%2520and%2520show%2520that%2520it%2520outperforms%2520SOTA%2520methods%250Aon%2520three%2520common%2520STVG%2520benchmarks.%250A%2520%2520The%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/zaiquanyang/LLaVA_Next_STVG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Multimodal%20LLMs%20for%20Zero-Shot%0A%20%20Spatio-Temporal%20Video%20Grounding&entry.906535625=Zaiquan%20Yang%20and%20Yuhao%20Liu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Spatio-temporal%20video%20grounding%20%28STVG%29%20aims%20at%20localizing%20the%20spatio-temporal%0Atube%20of%20a%20video%2C%20as%20specified%20by%20the%20input%20text%20query.%20In%20this%20paper%2C%20we%0Autilize%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20explore%20a%20zero-shot%0Asolution%20in%20STVG.%20We%20reveal%20two%20key%20insights%20about%20MLLMs%3A%20%281%29%20MLLMs%20tend%20to%0Adynamically%20assign%20special%20tokens%2C%20referred%20to%20as%20%5Ctextit%7Bgrounding%20tokens%7D%2C%0Afor%20grounding%20the%20text%20query%3B%20and%20%282%29%20MLLMs%20often%20suffer%20from%20suboptimal%0Agrounding%20due%20to%20the%20inability%20to%20fully%20integrate%20the%20cues%20in%20the%20text%20query%0A%28%5Ctextit%7Be.g.%7D%2C%20attributes%2C%20actions%29%20for%20inference.%20Based%20on%20these%20insights%2C%20we%0Apropose%20a%20MLLM-based%20zero-shot%20framework%20for%20STVG%2C%20which%20includes%20novel%0Adecomposed%20spatio-temporal%20highlighting%20%28DSTH%29%20and%20temporal-augmented%0Aassembling%20%28TAS%29%20strategies%20to%20unleash%20the%20reasoning%20ability%20of%20MLLMs.%20The%20DSTH%0Astrategy%20first%20decouples%20the%20original%20query%20into%20attribute%20and%20action%0Asub-queries%20for%20inquiring%20the%20existence%20of%20the%20target%20both%20spatially%20and%0Atemporally.%20It%20then%20uses%20a%20novel%20logit-guided%20re-attention%20%28LRA%29%20module%20to%0Alearn%20latent%20variables%20as%20spatial%20and%20temporal%20prompts%2C%20by%20regularizing%20token%0Apredictions%20for%20each%20sub-query.%20These%20prompts%20highlight%20attribute%20and%20action%0Acues%2C%20respectively%2C%20directing%20the%20model%27s%20attention%20to%20reliable%20spatial%20and%0Atemporal%20related%20visual%20regions.%20In%20addition%2C%20as%20the%20spatial%20grounding%20by%20the%0Aattribute%20sub-query%20should%20be%20temporally%20consistent%2C%20we%20introduce%20the%20TAS%0Astrategy%20to%20assemble%20the%20predictions%20using%20the%20original%20video%20frames%20and%20the%0Atemporal-augmented%20frames%20as%20inputs%20to%20help%20improve%20temporal%20consistency.%20We%0Aevaluate%20our%20method%20on%20various%20MLLMs%2C%20and%20show%20that%20it%20outperforms%20SOTA%20methods%0Aon%20three%20common%20STVG%20benchmarks.%0A%20%20The%20code%20will%20be%20available%20at%20https%3A//github.com/zaiquanyang/LLaVA_Next_STVG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15178v1&entry.124074799=Read"},
{"title": "MARIC: Multi-Agent Reasoning for Image Classification", "author": "Wonduk Seo and Minhyeong Yu and Hyunjin An and Seunghyun Lee", "abstract": "  Image classification has traditionally relied on parameter-intensive model\ntraining, requiring large-scale annotated datasets and extensive fine tuning to\nachieve competitive performance. While recent vision language models (VLMs)\nalleviate some of these constraints, they remain limited by their reliance on\nsingle pass representations, often failing to capture complementary aspects of\nvisual content. In this paper, we introduce Multi Agent based Reasoning for\nImage Classification (MARIC), a multi agent framework that reformulates image\nclassification as a collaborative reasoning process. MARIC first utilizes an\nOutliner Agent to analyze the global theme of the image and generate targeted\nprompts. Based on these prompts, three Aspect Agents extract fine grained\ndescriptions along distinct visual dimensions. Finally, a Reasoning Agent\nsynthesizes these complementary outputs through integrated reflection step,\nproducing a unified representation for classification. By explicitly\ndecomposing the task into multiple perspectives and encouraging reflective\nsynthesis, MARIC mitigates the shortcomings of both parameter-heavy training\nand monolithic VLM reasoning. Experiments on 4 diverse image classification\nbenchmark datasets demonstrate that MARIC significantly outperforms baselines,\nhighlighting the effectiveness of multi-agent visual reasoning for robust and\ninterpretable image classification.\n", "link": "http://arxiv.org/abs/2509.14860v1", "date": "2025-09-18", "relevancy": 2.1914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARIC%3A%20Multi-Agent%20Reasoning%20for%20Image%20Classification&body=Title%3A%20MARIC%3A%20Multi-Agent%20Reasoning%20for%20Image%20Classification%0AAuthor%3A%20Wonduk%20Seo%20and%20Minhyeong%20Yu%20and%20Hyunjin%20An%20and%20Seunghyun%20Lee%0AAbstract%3A%20%20%20Image%20classification%20has%20traditionally%20relied%20on%20parameter-intensive%20model%0Atraining%2C%20requiring%20large-scale%20annotated%20datasets%20and%20extensive%20fine%20tuning%20to%0Aachieve%20competitive%20performance.%20While%20recent%20vision%20language%20models%20%28VLMs%29%0Aalleviate%20some%20of%20these%20constraints%2C%20they%20remain%20limited%20by%20their%20reliance%20on%0Asingle%20pass%20representations%2C%20often%20failing%20to%20capture%20complementary%20aspects%20of%0Avisual%20content.%20In%20this%20paper%2C%20we%20introduce%20Multi%20Agent%20based%20Reasoning%20for%0AImage%20Classification%20%28MARIC%29%2C%20a%20multi%20agent%20framework%20that%20reformulates%20image%0Aclassification%20as%20a%20collaborative%20reasoning%20process.%20MARIC%20first%20utilizes%20an%0AOutliner%20Agent%20to%20analyze%20the%20global%20theme%20of%20the%20image%20and%20generate%20targeted%0Aprompts.%20Based%20on%20these%20prompts%2C%20three%20Aspect%20Agents%20extract%20fine%20grained%0Adescriptions%20along%20distinct%20visual%20dimensions.%20Finally%2C%20a%20Reasoning%20Agent%0Asynthesizes%20these%20complementary%20outputs%20through%20integrated%20reflection%20step%2C%0Aproducing%20a%20unified%20representation%20for%20classification.%20By%20explicitly%0Adecomposing%20the%20task%20into%20multiple%20perspectives%20and%20encouraging%20reflective%0Asynthesis%2C%20MARIC%20mitigates%20the%20shortcomings%20of%20both%20parameter-heavy%20training%0Aand%20monolithic%20VLM%20reasoning.%20Experiments%20on%204%20diverse%20image%20classification%0Abenchmark%20datasets%20demonstrate%20that%20MARIC%20significantly%20outperforms%20baselines%2C%0Ahighlighting%20the%20effectiveness%20of%20multi-agent%20visual%20reasoning%20for%20robust%20and%0Ainterpretable%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARIC%253A%2520Multi-Agent%2520Reasoning%2520for%2520Image%2520Classification%26entry.906535625%3DWonduk%2520Seo%2520and%2520Minhyeong%2520Yu%2520and%2520Hyunjin%2520An%2520and%2520Seunghyun%2520Lee%26entry.1292438233%3D%2520%2520Image%2520classification%2520has%2520traditionally%2520relied%2520on%2520parameter-intensive%2520model%250Atraining%252C%2520requiring%2520large-scale%2520annotated%2520datasets%2520and%2520extensive%2520fine%2520tuning%2520to%250Aachieve%2520competitive%2520performance.%2520While%2520recent%2520vision%2520language%2520models%2520%2528VLMs%2529%250Aalleviate%2520some%2520of%2520these%2520constraints%252C%2520they%2520remain%2520limited%2520by%2520their%2520reliance%2520on%250Asingle%2520pass%2520representations%252C%2520often%2520failing%2520to%2520capture%2520complementary%2520aspects%2520of%250Avisual%2520content.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Multi%2520Agent%2520based%2520Reasoning%2520for%250AImage%2520Classification%2520%2528MARIC%2529%252C%2520a%2520multi%2520agent%2520framework%2520that%2520reformulates%2520image%250Aclassification%2520as%2520a%2520collaborative%2520reasoning%2520process.%2520MARIC%2520first%2520utilizes%2520an%250AOutliner%2520Agent%2520to%2520analyze%2520the%2520global%2520theme%2520of%2520the%2520image%2520and%2520generate%2520targeted%250Aprompts.%2520Based%2520on%2520these%2520prompts%252C%2520three%2520Aspect%2520Agents%2520extract%2520fine%2520grained%250Adescriptions%2520along%2520distinct%2520visual%2520dimensions.%2520Finally%252C%2520a%2520Reasoning%2520Agent%250Asynthesizes%2520these%2520complementary%2520outputs%2520through%2520integrated%2520reflection%2520step%252C%250Aproducing%2520a%2520unified%2520representation%2520for%2520classification.%2520By%2520explicitly%250Adecomposing%2520the%2520task%2520into%2520multiple%2520perspectives%2520and%2520encouraging%2520reflective%250Asynthesis%252C%2520MARIC%2520mitigates%2520the%2520shortcomings%2520of%2520both%2520parameter-heavy%2520training%250Aand%2520monolithic%2520VLM%2520reasoning.%2520Experiments%2520on%25204%2520diverse%2520image%2520classification%250Abenchmark%2520datasets%2520demonstrate%2520that%2520MARIC%2520significantly%2520outperforms%2520baselines%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520multi-agent%2520visual%2520reasoning%2520for%2520robust%2520and%250Ainterpretable%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARIC%3A%20Multi-Agent%20Reasoning%20for%20Image%20Classification&entry.906535625=Wonduk%20Seo%20and%20Minhyeong%20Yu%20and%20Hyunjin%20An%20and%20Seunghyun%20Lee&entry.1292438233=%20%20Image%20classification%20has%20traditionally%20relied%20on%20parameter-intensive%20model%0Atraining%2C%20requiring%20large-scale%20annotated%20datasets%20and%20extensive%20fine%20tuning%20to%0Aachieve%20competitive%20performance.%20While%20recent%20vision%20language%20models%20%28VLMs%29%0Aalleviate%20some%20of%20these%20constraints%2C%20they%20remain%20limited%20by%20their%20reliance%20on%0Asingle%20pass%20representations%2C%20often%20failing%20to%20capture%20complementary%20aspects%20of%0Avisual%20content.%20In%20this%20paper%2C%20we%20introduce%20Multi%20Agent%20based%20Reasoning%20for%0AImage%20Classification%20%28MARIC%29%2C%20a%20multi%20agent%20framework%20that%20reformulates%20image%0Aclassification%20as%20a%20collaborative%20reasoning%20process.%20MARIC%20first%20utilizes%20an%0AOutliner%20Agent%20to%20analyze%20the%20global%20theme%20of%20the%20image%20and%20generate%20targeted%0Aprompts.%20Based%20on%20these%20prompts%2C%20three%20Aspect%20Agents%20extract%20fine%20grained%0Adescriptions%20along%20distinct%20visual%20dimensions.%20Finally%2C%20a%20Reasoning%20Agent%0Asynthesizes%20these%20complementary%20outputs%20through%20integrated%20reflection%20step%2C%0Aproducing%20a%20unified%20representation%20for%20classification.%20By%20explicitly%0Adecomposing%20the%20task%20into%20multiple%20perspectives%20and%20encouraging%20reflective%0Asynthesis%2C%20MARIC%20mitigates%20the%20shortcomings%20of%20both%20parameter-heavy%20training%0Aand%20monolithic%20VLM%20reasoning.%20Experiments%20on%204%20diverse%20image%20classification%0Abenchmark%20datasets%20demonstrate%20that%20MARIC%20significantly%20outperforms%20baselines%2C%0Ahighlighting%20the%20effectiveness%20of%20multi-agent%20visual%20reasoning%20for%20robust%20and%0Ainterpretable%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14860v1&entry.124074799=Read"},
{"title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based\n  Information for Code Large Language Models", "author": "Jian Wang and Xiaofei Xie and Qiang Hu and Shangqing Liu and Yi Li", "abstract": "  Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.\n", "link": "http://arxiv.org/abs/2509.11686v2", "date": "2025-09-18", "relevancy": 2.1761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Code%20Semantics%20Help%3F%20A%20Comprehensive%20Study%20on%20Execution%20Trace-Based%0A%20%20Information%20for%20Code%20Large%20Language%20Models&body=Title%3A%20Do%20Code%20Semantics%20Help%3F%20A%20Comprehensive%20Study%20on%20Execution%20Trace-Based%0A%20%20Information%20for%20Code%20Large%20Language%20Models%0AAuthor%3A%20Jian%20Wang%20and%20Xiaofei%20Xie%20and%20Qiang%20Hu%20and%20Shangqing%20Liu%20and%20Yi%20Li%0AAbstract%3A%20%20%20Code%20Large%20Language%20Models%20%28Code%20LLMs%29%20have%20opened%20a%20new%20era%20in%20programming%0Awith%20their%20impressive%20capabilities.%20However%2C%20recent%20research%20has%20revealed%0Acritical%20limitations%20in%20their%20ability%20to%20reason%20about%20runtime%20behavior%20and%0Aunderstand%20the%20actual%20functionality%20of%20programs%2C%20which%20poses%20significant%0Achallenges%20for%20their%20post-training%20and%20practical%20deployment.%20Specifically%2C%20Code%0ALLMs%20encounter%20two%20principal%20issues%3A%20%281%29%20a%20lack%20of%20proficiency%20in%20reasoning%0Aabout%20program%20execution%20behavior%2C%20as%20they%20struggle%20to%20interpret%20what%20programs%0Aactually%20do%20during%20runtime%2C%20and%20%282%29%20the%20inconsistent%20and%20fragmented%0Arepresentation%20of%20semantic%20information%2C%20such%20as%20execution%20traces%2C%20across%0Aexisting%20methods%2C%20which%20hinders%20their%20ability%20to%20generalize%20and%20reason%0Aeffectively.%20These%20challenges%20underscore%20the%20necessity%20for%20more%20systematic%0Aapproaches%20to%20enhance%20the%20reasoning%20capabilities%20of%20Code%20LLMs.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20generic%20framework%20to%20support%20integrating%20semantic%0Ainformation~%28e.g.%2C%20execution%20trace%29%20to%20code%20task-relevant%20prompts%2C%20and%20conduct%0Aa%20comprehensive%20study%20to%20explore%20the%20role%20of%20semantic%20information%20in%20enhancing%0Athe%20reasoning%20ability%20of%20Code%20LLMs%20accordingly.%20Specifically%2C%20we%20focus%20on%0Ainvestigating%20the%20usefulness%20of%20trace-based%20semantic%20information%20in%20boosting%0Asupervised%20fine-tuning~%28SFT%29%20and%20post-phase%20inference%20of%20Code%20LLMs.%20The%0Aexperimental%20results%20surprisingly%20disagree%20with%20previous%20works%20and%20demonstrate%0Athat%20semantic%20information%20has%20limited%20usefulness%20for%20SFT%20and%20test%20time%20scaling%0Aof%20Code%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Code%2520Semantics%2520Help%253F%2520A%2520Comprehensive%2520Study%2520on%2520Execution%2520Trace-Based%250A%2520%2520Information%2520for%2520Code%2520Large%2520Language%2520Models%26entry.906535625%3DJian%2520Wang%2520and%2520Xiaofei%2520Xie%2520and%2520Qiang%2520Hu%2520and%2520Shangqing%2520Liu%2520and%2520Yi%2520Li%26entry.1292438233%3D%2520%2520Code%2520Large%2520Language%2520Models%2520%2528Code%2520LLMs%2529%2520have%2520opened%2520a%2520new%2520era%2520in%2520programming%250Awith%2520their%2520impressive%2520capabilities.%2520However%252C%2520recent%2520research%2520has%2520revealed%250Acritical%2520limitations%2520in%2520their%2520ability%2520to%2520reason%2520about%2520runtime%2520behavior%2520and%250Aunderstand%2520the%2520actual%2520functionality%2520of%2520programs%252C%2520which%2520poses%2520significant%250Achallenges%2520for%2520their%2520post-training%2520and%2520practical%2520deployment.%2520Specifically%252C%2520Code%250ALLMs%2520encounter%2520two%2520principal%2520issues%253A%2520%25281%2529%2520a%2520lack%2520of%2520proficiency%2520in%2520reasoning%250Aabout%2520program%2520execution%2520behavior%252C%2520as%2520they%2520struggle%2520to%2520interpret%2520what%2520programs%250Aactually%2520do%2520during%2520runtime%252C%2520and%2520%25282%2529%2520the%2520inconsistent%2520and%2520fragmented%250Arepresentation%2520of%2520semantic%2520information%252C%2520such%2520as%2520execution%2520traces%252C%2520across%250Aexisting%2520methods%252C%2520which%2520hinders%2520their%2520ability%2520to%2520generalize%2520and%2520reason%250Aeffectively.%2520These%2520challenges%2520underscore%2520the%2520necessity%2520for%2520more%2520systematic%250Aapproaches%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520Code%2520LLMs.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520a%2520generic%2520framework%2520to%2520support%2520integrating%2520semantic%250Ainformation~%2528e.g.%252C%2520execution%2520trace%2529%2520to%2520code%2520task-relevant%2520prompts%252C%2520and%2520conduct%250Aa%2520comprehensive%2520study%2520to%2520explore%2520the%2520role%2520of%2520semantic%2520information%2520in%2520enhancing%250Athe%2520reasoning%2520ability%2520of%2520Code%2520LLMs%2520accordingly.%2520Specifically%252C%2520we%2520focus%2520on%250Ainvestigating%2520the%2520usefulness%2520of%2520trace-based%2520semantic%2520information%2520in%2520boosting%250Asupervised%2520fine-tuning~%2528SFT%2529%2520and%2520post-phase%2520inference%2520of%2520Code%2520LLMs.%2520The%250Aexperimental%2520results%2520surprisingly%2520disagree%2520with%2520previous%2520works%2520and%2520demonstrate%250Athat%2520semantic%2520information%2520has%2520limited%2520usefulness%2520for%2520SFT%2520and%2520test%2520time%2520scaling%250Aof%2520Code%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Code%20Semantics%20Help%3F%20A%20Comprehensive%20Study%20on%20Execution%20Trace-Based%0A%20%20Information%20for%20Code%20Large%20Language%20Models&entry.906535625=Jian%20Wang%20and%20Xiaofei%20Xie%20and%20Qiang%20Hu%20and%20Shangqing%20Liu%20and%20Yi%20Li&entry.1292438233=%20%20Code%20Large%20Language%20Models%20%28Code%20LLMs%29%20have%20opened%20a%20new%20era%20in%20programming%0Awith%20their%20impressive%20capabilities.%20However%2C%20recent%20research%20has%20revealed%0Acritical%20limitations%20in%20their%20ability%20to%20reason%20about%20runtime%20behavior%20and%0Aunderstand%20the%20actual%20functionality%20of%20programs%2C%20which%20poses%20significant%0Achallenges%20for%20their%20post-training%20and%20practical%20deployment.%20Specifically%2C%20Code%0ALLMs%20encounter%20two%20principal%20issues%3A%20%281%29%20a%20lack%20of%20proficiency%20in%20reasoning%0Aabout%20program%20execution%20behavior%2C%20as%20they%20struggle%20to%20interpret%20what%20programs%0Aactually%20do%20during%20runtime%2C%20and%20%282%29%20the%20inconsistent%20and%20fragmented%0Arepresentation%20of%20semantic%20information%2C%20such%20as%20execution%20traces%2C%20across%0Aexisting%20methods%2C%20which%20hinders%20their%20ability%20to%20generalize%20and%20reason%0Aeffectively.%20These%20challenges%20underscore%20the%20necessity%20for%20more%20systematic%0Aapproaches%20to%20enhance%20the%20reasoning%20capabilities%20of%20Code%20LLMs.%20To%20address%20these%0Aissues%2C%20we%20introduce%20a%20generic%20framework%20to%20support%20integrating%20semantic%0Ainformation~%28e.g.%2C%20execution%20trace%29%20to%20code%20task-relevant%20prompts%2C%20and%20conduct%0Aa%20comprehensive%20study%20to%20explore%20the%20role%20of%20semantic%20information%20in%20enhancing%0Athe%20reasoning%20ability%20of%20Code%20LLMs%20accordingly.%20Specifically%2C%20we%20focus%20on%0Ainvestigating%20the%20usefulness%20of%20trace-based%20semantic%20information%20in%20boosting%0Asupervised%20fine-tuning~%28SFT%29%20and%20post-phase%20inference%20of%20Code%20LLMs.%20The%0Aexperimental%20results%20surprisingly%20disagree%20with%20previous%20works%20and%20demonstrate%0Athat%20semantic%20information%20has%20limited%20usefulness%20for%20SFT%20and%20test%20time%20scaling%0Aof%20Code%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11686v2&entry.124074799=Read"},
{"title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich\n  Manipulation", "author": "Jiawen Yu and Hairuo Liu and Qiaojun Yu and Jieji Ren and Ce Hao and Haitong Ding and Guangyu Huang and Guofan Huang and Yan Song and Panpan Cai and Cewu Lu and Wenqiang Zhang", "abstract": "  Vision-Language-Action (VLA) models have advanced general-purpose robotic\nmanipulation by leveraging pretrained visual and linguistic representations.\nHowever, they struggle with contact-rich tasks that require fine-grained\ncontrol involving force, especially under visual occlusion or dynamic\nuncertainty. To address these limitations, we propose ForceVLA, a novel\nend-to-end manipulation framework that treats external force sensing as a\nfirst-class modality within VLA systems. ForceVLA introduces FVLMoE, a\nforce-aware Mixture-of-Experts fusion module that dynamically integrates\npretrained visual-language embeddings with real-time 6-axis force feedback\nduring action decoding. This enables context-aware routing across\nmodality-specific experts, enhancing the robot's ability to adapt to subtle\ncontact dynamics. We also introduce \\textbf{ForceVLA-Data}, a new dataset\ncomprising synchronized vision, proprioception, and force-torque signals across\nfive contact-rich manipulation tasks. ForceVLA improves average task success by\n23.2% over strong pi_0-based baselines, achieving up to 80% success in tasks\nsuch as plug insertion. Our approach highlights the importance of multimodal\nintegration for dexterous manipulation and sets a new benchmark for physically\nintelligent robotic control. Code and data will be released at\nhttps://sites.google.com/view/forcevla2025.\n", "link": "http://arxiv.org/abs/2505.22159v3", "date": "2025-09-18", "relevancy": 2.1606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation&body=Title%3A%20ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation%0AAuthor%3A%20Jiawen%20Yu%20and%20Hairuo%20Liu%20and%20Qiaojun%20Yu%20and%20Jieji%20Ren%20and%20Ce%20Hao%20and%20Haitong%20Ding%20and%20Guangyu%20Huang%20and%20Guofan%20Huang%20and%20Yan%20Song%20and%20Panpan%20Cai%20and%20Cewu%20Lu%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20general-purpose%20robotic%0Amanipulation%20by%20leveraging%20pretrained%20visual%20and%20linguistic%20representations.%0AHowever%2C%20they%20struggle%20with%20contact-rich%20tasks%20that%20require%20fine-grained%0Acontrol%20involving%20force%2C%20especially%20under%20visual%20occlusion%20or%20dynamic%0Auncertainty.%20To%20address%20these%20limitations%2C%20we%20propose%20ForceVLA%2C%20a%20novel%0Aend-to-end%20manipulation%20framework%20that%20treats%20external%20force%20sensing%20as%20a%0Afirst-class%20modality%20within%20VLA%20systems.%20ForceVLA%20introduces%20FVLMoE%2C%20a%0Aforce-aware%20Mixture-of-Experts%20fusion%20module%20that%20dynamically%20integrates%0Apretrained%20visual-language%20embeddings%20with%20real-time%206-axis%20force%20feedback%0Aduring%20action%20decoding.%20This%20enables%20context-aware%20routing%20across%0Amodality-specific%20experts%2C%20enhancing%20the%20robot%27s%20ability%20to%20adapt%20to%20subtle%0Acontact%20dynamics.%20We%20also%20introduce%20%5Ctextbf%7BForceVLA-Data%7D%2C%20a%20new%20dataset%0Acomprising%20synchronized%20vision%2C%20proprioception%2C%20and%20force-torque%20signals%20across%0Afive%20contact-rich%20manipulation%20tasks.%20ForceVLA%20improves%20average%20task%20success%20by%0A23.2%25%20over%20strong%20pi_0-based%20baselines%2C%20achieving%20up%20to%2080%25%20success%20in%20tasks%0Asuch%20as%20plug%20insertion.%20Our%20approach%20highlights%20the%20importance%20of%20multimodal%0Aintegration%20for%20dexterous%20manipulation%20and%20sets%20a%20new%20benchmark%20for%20physically%0Aintelligent%20robotic%20control.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/forcevla2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22159v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForceVLA%253A%2520Enhancing%2520VLA%2520Models%2520with%2520a%2520Force-aware%2520MoE%2520for%2520Contact-rich%250A%2520%2520Manipulation%26entry.906535625%3DJiawen%2520Yu%2520and%2520Hairuo%2520Liu%2520and%2520Qiaojun%2520Yu%2520and%2520Jieji%2520Ren%2520and%2520Ce%2520Hao%2520and%2520Haitong%2520Ding%2520and%2520Guangyu%2520Huang%2520and%2520Guofan%2520Huang%2520and%2520Yan%2520Song%2520and%2520Panpan%2520Cai%2520and%2520Cewu%2520Lu%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520advanced%2520general-purpose%2520robotic%250Amanipulation%2520by%2520leveraging%2520pretrained%2520visual%2520and%2520linguistic%2520representations.%250AHowever%252C%2520they%2520struggle%2520with%2520contact-rich%2520tasks%2520that%2520require%2520fine-grained%250Acontrol%2520involving%2520force%252C%2520especially%2520under%2520visual%2520occlusion%2520or%2520dynamic%250Auncertainty.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ForceVLA%252C%2520a%2520novel%250Aend-to-end%2520manipulation%2520framework%2520that%2520treats%2520external%2520force%2520sensing%2520as%2520a%250Afirst-class%2520modality%2520within%2520VLA%2520systems.%2520ForceVLA%2520introduces%2520FVLMoE%252C%2520a%250Aforce-aware%2520Mixture-of-Experts%2520fusion%2520module%2520that%2520dynamically%2520integrates%250Apretrained%2520visual-language%2520embeddings%2520with%2520real-time%25206-axis%2520force%2520feedback%250Aduring%2520action%2520decoding.%2520This%2520enables%2520context-aware%2520routing%2520across%250Amodality-specific%2520experts%252C%2520enhancing%2520the%2520robot%2527s%2520ability%2520to%2520adapt%2520to%2520subtle%250Acontact%2520dynamics.%2520We%2520also%2520introduce%2520%255Ctextbf%257BForceVLA-Data%257D%252C%2520a%2520new%2520dataset%250Acomprising%2520synchronized%2520vision%252C%2520proprioception%252C%2520and%2520force-torque%2520signals%2520across%250Afive%2520contact-rich%2520manipulation%2520tasks.%2520ForceVLA%2520improves%2520average%2520task%2520success%2520by%250A23.2%2525%2520over%2520strong%2520pi_0-based%2520baselines%252C%2520achieving%2520up%2520to%252080%2525%2520success%2520in%2520tasks%250Asuch%2520as%2520plug%2520insertion.%2520Our%2520approach%2520highlights%2520the%2520importance%2520of%2520multimodal%250Aintegration%2520for%2520dexterous%2520manipulation%2520and%2520sets%2520a%2520new%2520benchmark%2520for%2520physically%250Aintelligent%2520robotic%2520control.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%250Ahttps%253A//sites.google.com/view/forcevla2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22159v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation&entry.906535625=Jiawen%20Yu%20and%20Hairuo%20Liu%20and%20Qiaojun%20Yu%20and%20Jieji%20Ren%20and%20Ce%20Hao%20and%20Haitong%20Ding%20and%20Guangyu%20Huang%20and%20Guofan%20Huang%20and%20Yan%20Song%20and%20Panpan%20Cai%20and%20Cewu%20Lu%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20general-purpose%20robotic%0Amanipulation%20by%20leveraging%20pretrained%20visual%20and%20linguistic%20representations.%0AHowever%2C%20they%20struggle%20with%20contact-rich%20tasks%20that%20require%20fine-grained%0Acontrol%20involving%20force%2C%20especially%20under%20visual%20occlusion%20or%20dynamic%0Auncertainty.%20To%20address%20these%20limitations%2C%20we%20propose%20ForceVLA%2C%20a%20novel%0Aend-to-end%20manipulation%20framework%20that%20treats%20external%20force%20sensing%20as%20a%0Afirst-class%20modality%20within%20VLA%20systems.%20ForceVLA%20introduces%20FVLMoE%2C%20a%0Aforce-aware%20Mixture-of-Experts%20fusion%20module%20that%20dynamically%20integrates%0Apretrained%20visual-language%20embeddings%20with%20real-time%206-axis%20force%20feedback%0Aduring%20action%20decoding.%20This%20enables%20context-aware%20routing%20across%0Amodality-specific%20experts%2C%20enhancing%20the%20robot%27s%20ability%20to%20adapt%20to%20subtle%0Acontact%20dynamics.%20We%20also%20introduce%20%5Ctextbf%7BForceVLA-Data%7D%2C%20a%20new%20dataset%0Acomprising%20synchronized%20vision%2C%20proprioception%2C%20and%20force-torque%20signals%20across%0Afive%20contact-rich%20manipulation%20tasks.%20ForceVLA%20improves%20average%20task%20success%20by%0A23.2%25%20over%20strong%20pi_0-based%20baselines%2C%20achieving%20up%20to%2080%25%20success%20in%20tasks%0Asuch%20as%20plug%20insertion.%20Our%20approach%20highlights%20the%20importance%20of%20multimodal%0Aintegration%20for%20dexterous%20manipulation%20and%20sets%20a%20new%20benchmark%20for%20physically%0Aintelligent%20robotic%20control.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/forcevla2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22159v3&entry.124074799=Read"},
{"title": "Communication Efficient Split Learning of ViTs with Attention-based\n  Double Compression", "author": "Federico Alvetreti and Jary Pomponi and Paolo Di Lorenzo and Simone Scardapane", "abstract": "  This paper proposes a novel communication-efficient Split Learning (SL)\nframework, named Attention-based Double Compression (ADC), which reduces the\ncommunication overhead required for transmitting intermediate Vision\nTransformers activations during the SL training process. ADC incorporates two\nparallel compression strategies. The first one merges samples' activations that\nare similar, based on the average attention score calculated in the last client\nlayer; this strategy is class-agnostic, meaning that it can also merge samples\nhaving different classes, without losing generalization ability nor decreasing\nfinal results. The second strategy follows the first and discards the least\nmeaningful tokens, further reducing the communication cost. Combining these\nstrategies not only allows for sending less during the forward pass, but also\nthe gradients are naturally compressed, allowing the whole model to be trained\nwithout additional tuning or approximations of the gradients. Simulation\nresults demonstrate that Attention-based Double Compression outperforms\nstate-of-the-art SL frameworks by significantly reducing communication\noverheads while maintaining high accuracy.\n", "link": "http://arxiv.org/abs/2509.15058v1", "date": "2025-09-18", "relevancy": 2.1502, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5527}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5275}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication%20Efficient%20Split%20Learning%20of%20ViTs%20with%20Attention-based%0A%20%20Double%20Compression&body=Title%3A%20Communication%20Efficient%20Split%20Learning%20of%20ViTs%20with%20Attention-based%0A%20%20Double%20Compression%0AAuthor%3A%20Federico%20Alvetreti%20and%20Jary%20Pomponi%20and%20Paolo%20Di%20Lorenzo%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20communication-efficient%20Split%20Learning%20%28SL%29%0Aframework%2C%20named%20Attention-based%20Double%20Compression%20%28ADC%29%2C%20which%20reduces%20the%0Acommunication%20overhead%20required%20for%20transmitting%20intermediate%20Vision%0ATransformers%20activations%20during%20the%20SL%20training%20process.%20ADC%20incorporates%20two%0Aparallel%20compression%20strategies.%20The%20first%20one%20merges%20samples%27%20activations%20that%0Aare%20similar%2C%20based%20on%20the%20average%20attention%20score%20calculated%20in%20the%20last%20client%0Alayer%3B%20this%20strategy%20is%20class-agnostic%2C%20meaning%20that%20it%20can%20also%20merge%20samples%0Ahaving%20different%20classes%2C%20without%20losing%20generalization%20ability%20nor%20decreasing%0Afinal%20results.%20The%20second%20strategy%20follows%20the%20first%20and%20discards%20the%20least%0Ameaningful%20tokens%2C%20further%20reducing%20the%20communication%20cost.%20Combining%20these%0Astrategies%20not%20only%20allows%20for%20sending%20less%20during%20the%20forward%20pass%2C%20but%20also%0Athe%20gradients%20are%20naturally%20compressed%2C%20allowing%20the%20whole%20model%20to%20be%20trained%0Awithout%20additional%20tuning%20or%20approximations%20of%20the%20gradients.%20Simulation%0Aresults%20demonstrate%20that%20Attention-based%20Double%20Compression%20outperforms%0Astate-of-the-art%20SL%20frameworks%20by%20significantly%20reducing%20communication%0Aoverheads%20while%20maintaining%20high%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication%2520Efficient%2520Split%2520Learning%2520of%2520ViTs%2520with%2520Attention-based%250A%2520%2520Double%2520Compression%26entry.906535625%3DFederico%2520Alvetreti%2520and%2520Jary%2520Pomponi%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520communication-efficient%2520Split%2520Learning%2520%2528SL%2529%250Aframework%252C%2520named%2520Attention-based%2520Double%2520Compression%2520%2528ADC%2529%252C%2520which%2520reduces%2520the%250Acommunication%2520overhead%2520required%2520for%2520transmitting%2520intermediate%2520Vision%250ATransformers%2520activations%2520during%2520the%2520SL%2520training%2520process.%2520ADC%2520incorporates%2520two%250Aparallel%2520compression%2520strategies.%2520The%2520first%2520one%2520merges%2520samples%2527%2520activations%2520that%250Aare%2520similar%252C%2520based%2520on%2520the%2520average%2520attention%2520score%2520calculated%2520in%2520the%2520last%2520client%250Alayer%253B%2520this%2520strategy%2520is%2520class-agnostic%252C%2520meaning%2520that%2520it%2520can%2520also%2520merge%2520samples%250Ahaving%2520different%2520classes%252C%2520without%2520losing%2520generalization%2520ability%2520nor%2520decreasing%250Afinal%2520results.%2520The%2520second%2520strategy%2520follows%2520the%2520first%2520and%2520discards%2520the%2520least%250Ameaningful%2520tokens%252C%2520further%2520reducing%2520the%2520communication%2520cost.%2520Combining%2520these%250Astrategies%2520not%2520only%2520allows%2520for%2520sending%2520less%2520during%2520the%2520forward%2520pass%252C%2520but%2520also%250Athe%2520gradients%2520are%2520naturally%2520compressed%252C%2520allowing%2520the%2520whole%2520model%2520to%2520be%2520trained%250Awithout%2520additional%2520tuning%2520or%2520approximations%2520of%2520the%2520gradients.%2520Simulation%250Aresults%2520demonstrate%2520that%2520Attention-based%2520Double%2520Compression%2520outperforms%250Astate-of-the-art%2520SL%2520frameworks%2520by%2520significantly%2520reducing%2520communication%250Aoverheads%2520while%2520maintaining%2520high%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication%20Efficient%20Split%20Learning%20of%20ViTs%20with%20Attention-based%0A%20%20Double%20Compression&entry.906535625=Federico%20Alvetreti%20and%20Jary%20Pomponi%20and%20Paolo%20Di%20Lorenzo%20and%20Simone%20Scardapane&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20communication-efficient%20Split%20Learning%20%28SL%29%0Aframework%2C%20named%20Attention-based%20Double%20Compression%20%28ADC%29%2C%20which%20reduces%20the%0Acommunication%20overhead%20required%20for%20transmitting%20intermediate%20Vision%0ATransformers%20activations%20during%20the%20SL%20training%20process.%20ADC%20incorporates%20two%0Aparallel%20compression%20strategies.%20The%20first%20one%20merges%20samples%27%20activations%20that%0Aare%20similar%2C%20based%20on%20the%20average%20attention%20score%20calculated%20in%20the%20last%20client%0Alayer%3B%20this%20strategy%20is%20class-agnostic%2C%20meaning%20that%20it%20can%20also%20merge%20samples%0Ahaving%20different%20classes%2C%20without%20losing%20generalization%20ability%20nor%20decreasing%0Afinal%20results.%20The%20second%20strategy%20follows%20the%20first%20and%20discards%20the%20least%0Ameaningful%20tokens%2C%20further%20reducing%20the%20communication%20cost.%20Combining%20these%0Astrategies%20not%20only%20allows%20for%20sending%20less%20during%20the%20forward%20pass%2C%20but%20also%0Athe%20gradients%20are%20naturally%20compressed%2C%20allowing%20the%20whole%20model%20to%20be%20trained%0Awithout%20additional%20tuning%20or%20approximations%20of%20the%20gradients.%20Simulation%0Aresults%20demonstrate%20that%20Attention-based%20Double%20Compression%20outperforms%0Astate-of-the-art%20SL%20frameworks%20by%20significantly%20reducing%20communication%0Aoverheads%20while%20maintaining%20high%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15058v1&entry.124074799=Read"},
{"title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based\n  Incremental Learning", "author": "Mohammad Saleh Vahdatpour and Huaiyuan Chu and Yanqing Zhang", "abstract": "  The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments.\n", "link": "http://arxiv.org/abs/2509.15097v1", "date": "2025-09-18", "relevancy": 2.1476, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5578}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5409}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Energy-Efficient%20Hierarchical%20Neural%20Network%20with%20Fast%20FPGA-Based%0A%20%20Incremental%20Learning&body=Title%3A%20The%20Energy-Efficient%20Hierarchical%20Neural%20Network%20with%20Fast%20FPGA-Based%0A%20%20Incremental%20Learning%0AAuthor%3A%20Mohammad%20Saleh%20Vahdatpour%20and%20Huaiyuan%20Chu%20and%20Yanqing%20Zhang%0AAbstract%3A%20%20%20The%20rising%20computational%20and%20energy%20demands%20of%20deep%20learning%2C%20particularly%20in%0Alarge-scale%20architectures%20such%20as%20foundation%20models%20and%20large%20language%20models%0A%28LLMs%29%2C%20pose%20significant%20challenges%20to%20sustainability.%20Traditional%0Agradient-based%20training%20methods%20are%20inefficient%2C%20requiring%20numerous%20iterative%0Aupdates%20and%20high%20power%20consumption.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ahybrid%20framework%20that%20combines%20hierarchical%20decomposition%20with%20FPGA-based%0Adirect%20equation%20solving%20and%20incremental%20learning.%20Our%20method%20divides%20the%20neural%0Anetwork%20into%20two%20functional%20tiers%3A%20lower%20layers%20are%20optimized%20via%20single-step%0Aequation%20solving%20on%20FPGAs%20for%20efficient%20and%20parallelizable%20feature%20extraction%2C%0Awhile%20higher%20layers%20employ%20adaptive%20incremental%20learning%20to%20support%20continual%0Aupdates%20without%20full%20retraining.%20Building%20upon%20this%20foundation%2C%20we%20introduce%0Athe%20Compound%20LLM%20framework%2C%20which%20explicitly%20deploys%20LLM%20modules%20across%20both%0Ahierarchy%20levels.%20The%20lower-level%20LLM%20handles%20reusable%20representation%20learning%0Awith%20minimal%20energy%20overhead%2C%20while%20the%20upper-level%20LLM%20performs%20adaptive%0Adecision-making%20through%20energy-aware%20updates.%20This%20integrated%20design%20enhances%0Ascalability%2C%20reduces%20redundant%20computation%2C%20and%20aligns%20with%20the%20principles%20of%0Asustainable%20AI.%20Theoretical%20analysis%20and%20architectural%20insights%20demonstrate%0Athat%20our%20method%20reduces%20computational%20costs%20significantly%20while%20preserving%20high%0Amodel%20performance%2C%20making%20it%20well-suited%20for%20edge%20deployment%20and%20real-time%0Aadaptation%20in%20energy-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Energy-Efficient%2520Hierarchical%2520Neural%2520Network%2520with%2520Fast%2520FPGA-Based%250A%2520%2520Incremental%2520Learning%26entry.906535625%3DMohammad%2520Saleh%2520Vahdatpour%2520and%2520Huaiyuan%2520Chu%2520and%2520Yanqing%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rising%2520computational%2520and%2520energy%2520demands%2520of%2520deep%2520learning%252C%2520particularly%2520in%250Alarge-scale%2520architectures%2520such%2520as%2520foundation%2520models%2520and%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520pose%2520significant%2520challenges%2520to%2520sustainability.%2520Traditional%250Agradient-based%2520training%2520methods%2520are%2520inefficient%252C%2520requiring%2520numerous%2520iterative%250Aupdates%2520and%2520high%2520power%2520consumption.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Ahybrid%2520framework%2520that%2520combines%2520hierarchical%2520decomposition%2520with%2520FPGA-based%250Adirect%2520equation%2520solving%2520and%2520incremental%2520learning.%2520Our%2520method%2520divides%2520the%2520neural%250Anetwork%2520into%2520two%2520functional%2520tiers%253A%2520lower%2520layers%2520are%2520optimized%2520via%2520single-step%250Aequation%2520solving%2520on%2520FPGAs%2520for%2520efficient%2520and%2520parallelizable%2520feature%2520extraction%252C%250Awhile%2520higher%2520layers%2520employ%2520adaptive%2520incremental%2520learning%2520to%2520support%2520continual%250Aupdates%2520without%2520full%2520retraining.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520introduce%250Athe%2520Compound%2520LLM%2520framework%252C%2520which%2520explicitly%2520deploys%2520LLM%2520modules%2520across%2520both%250Ahierarchy%2520levels.%2520The%2520lower-level%2520LLM%2520handles%2520reusable%2520representation%2520learning%250Awith%2520minimal%2520energy%2520overhead%252C%2520while%2520the%2520upper-level%2520LLM%2520performs%2520adaptive%250Adecision-making%2520through%2520energy-aware%2520updates.%2520This%2520integrated%2520design%2520enhances%250Ascalability%252C%2520reduces%2520redundant%2520computation%252C%2520and%2520aligns%2520with%2520the%2520principles%2520of%250Asustainable%2520AI.%2520Theoretical%2520analysis%2520and%2520architectural%2520insights%2520demonstrate%250Athat%2520our%2520method%2520reduces%2520computational%2520costs%2520significantly%2520while%2520preserving%2520high%250Amodel%2520performance%252C%2520making%2520it%2520well-suited%2520for%2520edge%2520deployment%2520and%2520real-time%250Aadaptation%2520in%2520energy-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Energy-Efficient%20Hierarchical%20Neural%20Network%20with%20Fast%20FPGA-Based%0A%20%20Incremental%20Learning&entry.906535625=Mohammad%20Saleh%20Vahdatpour%20and%20Huaiyuan%20Chu%20and%20Yanqing%20Zhang&entry.1292438233=%20%20The%20rising%20computational%20and%20energy%20demands%20of%20deep%20learning%2C%20particularly%20in%0Alarge-scale%20architectures%20such%20as%20foundation%20models%20and%20large%20language%20models%0A%28LLMs%29%2C%20pose%20significant%20challenges%20to%20sustainability.%20Traditional%0Agradient-based%20training%20methods%20are%20inefficient%2C%20requiring%20numerous%20iterative%0Aupdates%20and%20high%20power%20consumption.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Ahybrid%20framework%20that%20combines%20hierarchical%20decomposition%20with%20FPGA-based%0Adirect%20equation%20solving%20and%20incremental%20learning.%20Our%20method%20divides%20the%20neural%0Anetwork%20into%20two%20functional%20tiers%3A%20lower%20layers%20are%20optimized%20via%20single-step%0Aequation%20solving%20on%20FPGAs%20for%20efficient%20and%20parallelizable%20feature%20extraction%2C%0Awhile%20higher%20layers%20employ%20adaptive%20incremental%20learning%20to%20support%20continual%0Aupdates%20without%20full%20retraining.%20Building%20upon%20this%20foundation%2C%20we%20introduce%0Athe%20Compound%20LLM%20framework%2C%20which%20explicitly%20deploys%20LLM%20modules%20across%20both%0Ahierarchy%20levels.%20The%20lower-level%20LLM%20handles%20reusable%20representation%20learning%0Awith%20minimal%20energy%20overhead%2C%20while%20the%20upper-level%20LLM%20performs%20adaptive%0Adecision-making%20through%20energy-aware%20updates.%20This%20integrated%20design%20enhances%0Ascalability%2C%20reduces%20redundant%20computation%2C%20and%20aligns%20with%20the%20principles%20of%0Asustainable%20AI.%20Theoretical%20analysis%20and%20architectural%20insights%20demonstrate%0Athat%20our%20method%20reduces%20computational%20costs%20significantly%20while%20preserving%20high%0Amodel%20performance%2C%20making%20it%20well-suited%20for%20edge%20deployment%20and%20real-time%0Aadaptation%20in%20energy-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15097v1&entry.124074799=Read"},
{"title": "Optimal Learning from Label Proportions with General Loss Functions", "author": "Lorne Applebaum and Travis Dick and Claudio Gentile and Haim Kaplan and Tomer Koren", "abstract": "  Motivated by problems in online advertising, we address the task of Learning\nfrom Label Proportions (LLP). In this partially-supervised setting, training\ndata consists of groups of examples, termed bags, for which we only observe the\naverage label value. The main goal, however, remains the design of a predictor\nfor the labels of individual examples. We introduce a novel and versatile\nlow-variance de-biasing methodology to learn from aggregate label information,\nsignificantly advancing the state of the art in LLP. Our approach exhibits\nremarkable flexibility, seamlessly accommodating a broad spectrum of\npractically relevant loss functions across both binary and multi-class\nclassification settings. By carefully combining our estimators with standard\ntechniques, we substantially improve sample complexity guarantees for a large\nclass of losses of practical relevance. We also empirically validate the\nefficacy of our proposed approach across a diverse array of benchmark datasets,\ndemonstrating compelling empirical advantages over standard baselines.\n", "link": "http://arxiv.org/abs/2509.15145v1", "date": "2025-09-18", "relevancy": 2.1255, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5649}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Learning%20from%20Label%20Proportions%20with%20General%20Loss%20Functions&body=Title%3A%20Optimal%20Learning%20from%20Label%20Proportions%20with%20General%20Loss%20Functions%0AAuthor%3A%20Lorne%20Applebaum%20and%20Travis%20Dick%20and%20Claudio%20Gentile%20and%20Haim%20Kaplan%20and%20Tomer%20Koren%0AAbstract%3A%20%20%20Motivated%20by%20problems%20in%20online%20advertising%2C%20we%20address%20the%20task%20of%20Learning%0Afrom%20Label%20Proportions%20%28LLP%29.%20In%20this%20partially-supervised%20setting%2C%20training%0Adata%20consists%20of%20groups%20of%20examples%2C%20termed%20bags%2C%20for%20which%20we%20only%20observe%20the%0Aaverage%20label%20value.%20The%20main%20goal%2C%20however%2C%20remains%20the%20design%20of%20a%20predictor%0Afor%20the%20labels%20of%20individual%20examples.%20We%20introduce%20a%20novel%20and%20versatile%0Alow-variance%20de-biasing%20methodology%20to%20learn%20from%20aggregate%20label%20information%2C%0Asignificantly%20advancing%20the%20state%20of%20the%20art%20in%20LLP.%20Our%20approach%20exhibits%0Aremarkable%20flexibility%2C%20seamlessly%20accommodating%20a%20broad%20spectrum%20of%0Apractically%20relevant%20loss%20functions%20across%20both%20binary%20and%20multi-class%0Aclassification%20settings.%20By%20carefully%20combining%20our%20estimators%20with%20standard%0Atechniques%2C%20we%20substantially%20improve%20sample%20complexity%20guarantees%20for%20a%20large%0Aclass%20of%20losses%20of%20practical%20relevance.%20We%20also%20empirically%20validate%20the%0Aefficacy%20of%20our%20proposed%20approach%20across%20a%20diverse%20array%20of%20benchmark%20datasets%2C%0Ademonstrating%20compelling%20empirical%20advantages%20over%20standard%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Learning%2520from%2520Label%2520Proportions%2520with%2520General%2520Loss%2520Functions%26entry.906535625%3DLorne%2520Applebaum%2520and%2520Travis%2520Dick%2520and%2520Claudio%2520Gentile%2520and%2520Haim%2520Kaplan%2520and%2520Tomer%2520Koren%26entry.1292438233%3D%2520%2520Motivated%2520by%2520problems%2520in%2520online%2520advertising%252C%2520we%2520address%2520the%2520task%2520of%2520Learning%250Afrom%2520Label%2520Proportions%2520%2528LLP%2529.%2520In%2520this%2520partially-supervised%2520setting%252C%2520training%250Adata%2520consists%2520of%2520groups%2520of%2520examples%252C%2520termed%2520bags%252C%2520for%2520which%2520we%2520only%2520observe%2520the%250Aaverage%2520label%2520value.%2520The%2520main%2520goal%252C%2520however%252C%2520remains%2520the%2520design%2520of%2520a%2520predictor%250Afor%2520the%2520labels%2520of%2520individual%2520examples.%2520We%2520introduce%2520a%2520novel%2520and%2520versatile%250Alow-variance%2520de-biasing%2520methodology%2520to%2520learn%2520from%2520aggregate%2520label%2520information%252C%250Asignificantly%2520advancing%2520the%2520state%2520of%2520the%2520art%2520in%2520LLP.%2520Our%2520approach%2520exhibits%250Aremarkable%2520flexibility%252C%2520seamlessly%2520accommodating%2520a%2520broad%2520spectrum%2520of%250Apractically%2520relevant%2520loss%2520functions%2520across%2520both%2520binary%2520and%2520multi-class%250Aclassification%2520settings.%2520By%2520carefully%2520combining%2520our%2520estimators%2520with%2520standard%250Atechniques%252C%2520we%2520substantially%2520improve%2520sample%2520complexity%2520guarantees%2520for%2520a%2520large%250Aclass%2520of%2520losses%2520of%2520practical%2520relevance.%2520We%2520also%2520empirically%2520validate%2520the%250Aefficacy%2520of%2520our%2520proposed%2520approach%2520across%2520a%2520diverse%2520array%2520of%2520benchmark%2520datasets%252C%250Ademonstrating%2520compelling%2520empirical%2520advantages%2520over%2520standard%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Learning%20from%20Label%20Proportions%20with%20General%20Loss%20Functions&entry.906535625=Lorne%20Applebaum%20and%20Travis%20Dick%20and%20Claudio%20Gentile%20and%20Haim%20Kaplan%20and%20Tomer%20Koren&entry.1292438233=%20%20Motivated%20by%20problems%20in%20online%20advertising%2C%20we%20address%20the%20task%20of%20Learning%0Afrom%20Label%20Proportions%20%28LLP%29.%20In%20this%20partially-supervised%20setting%2C%20training%0Adata%20consists%20of%20groups%20of%20examples%2C%20termed%20bags%2C%20for%20which%20we%20only%20observe%20the%0Aaverage%20label%20value.%20The%20main%20goal%2C%20however%2C%20remains%20the%20design%20of%20a%20predictor%0Afor%20the%20labels%20of%20individual%20examples.%20We%20introduce%20a%20novel%20and%20versatile%0Alow-variance%20de-biasing%20methodology%20to%20learn%20from%20aggregate%20label%20information%2C%0Asignificantly%20advancing%20the%20state%20of%20the%20art%20in%20LLP.%20Our%20approach%20exhibits%0Aremarkable%20flexibility%2C%20seamlessly%20accommodating%20a%20broad%20spectrum%20of%0Apractically%20relevant%20loss%20functions%20across%20both%20binary%20and%20multi-class%0Aclassification%20settings.%20By%20carefully%20combining%20our%20estimators%20with%20standard%0Atechniques%2C%20we%20substantially%20improve%20sample%20complexity%20guarantees%20for%20a%20large%0Aclass%20of%20losses%20of%20practical%20relevance.%20We%20also%20empirically%20validate%20the%0Aefficacy%20of%20our%20proposed%20approach%20across%20a%20diverse%20array%20of%20benchmark%20datasets%2C%0Ademonstrating%20compelling%20empirical%20advantages%20over%20standard%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15145v1&entry.124074799=Read"},
{"title": "Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS\n  2025 VOS Track", "author": "An Yan and Leilei Cao and Feng Lu and Ran Hong and Youhai Jiang and Fengjie Zhu", "abstract": "  Complex Video Object Segmentation (VOS) presents significant challenges in\naccurately segmenting objects across frames, especially in the presence of\nsmall and similar targets, frequent occlusions, rapid motion, and complex\ninteractions. In this report, we present our solution for the LSVOS 2025 VOS\nTrack based on the SAM2 framework. We adopt a pseudo-labeling strategy during\ntraining: a trained SAM2 checkpoint is deployed within the SAM2Long framework\nto generate pseudo labels for the MOSE test set, which are then combined with\nexisting data for further training. For inference, the SAM2Long framework is\nemployed to obtain our primary segmentation results, while an open-source SeC\nmodel runs in parallel to produce complementary predictions. A cascaded\ndecision mechanism dynamically integrates outputs from both models, exploiting\nthe temporal stability of SAM2Long and the concept-level robustness of SeC.\nBenefiting from pseudo-label training and cascaded multi-model inference, our\napproach achieves a J\\&F score of 0.8616 on the MOSE test set -- +1.4 points\nover our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS\nTrack, and demonstrating strong robustness and accuracy in long, complex video\nsegmentation scenarios.\n", "link": "http://arxiv.org/abs/2509.14901v1", "date": "2025-09-18", "relevancy": 2.1187, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Label%20Enhanced%20Cascaded%20Framework%3A%202nd%20Technical%20Report%20for%20LSVOS%0A%20%202025%20VOS%20Track&body=Title%3A%20Pseudo-Label%20Enhanced%20Cascaded%20Framework%3A%202nd%20Technical%20Report%20for%20LSVOS%0A%20%202025%20VOS%20Track%0AAuthor%3A%20An%20Yan%20and%20Leilei%20Cao%20and%20Feng%20Lu%20and%20Ran%20Hong%20and%20Youhai%20Jiang%20and%20Fengjie%20Zhu%0AAbstract%3A%20%20%20Complex%20Video%20Object%20Segmentation%20%28VOS%29%20presents%20significant%20challenges%20in%0Aaccurately%20segmenting%20objects%20across%20frames%2C%20especially%20in%20the%20presence%20of%0Asmall%20and%20similar%20targets%2C%20frequent%20occlusions%2C%20rapid%20motion%2C%20and%20complex%0Ainteractions.%20In%20this%20report%2C%20we%20present%20our%20solution%20for%20the%20LSVOS%202025%20VOS%0ATrack%20based%20on%20the%20SAM2%20framework.%20We%20adopt%20a%20pseudo-labeling%20strategy%20during%0Atraining%3A%20a%20trained%20SAM2%20checkpoint%20is%20deployed%20within%20the%20SAM2Long%20framework%0Ato%20generate%20pseudo%20labels%20for%20the%20MOSE%20test%20set%2C%20which%20are%20then%20combined%20with%0Aexisting%20data%20for%20further%20training.%20For%20inference%2C%20the%20SAM2Long%20framework%20is%0Aemployed%20to%20obtain%20our%20primary%20segmentation%20results%2C%20while%20an%20open-source%20SeC%0Amodel%20runs%20in%20parallel%20to%20produce%20complementary%20predictions.%20A%20cascaded%0Adecision%20mechanism%20dynamically%20integrates%20outputs%20from%20both%20models%2C%20exploiting%0Athe%20temporal%20stability%20of%20SAM2Long%20and%20the%20concept-level%20robustness%20of%20SeC.%0ABenefiting%20from%20pseudo-label%20training%20and%20cascaded%20multi-model%20inference%2C%20our%0Aapproach%20achieves%20a%20J%5C%26F%20score%20of%200.8616%20on%20the%20MOSE%20test%20set%20--%20%2B1.4%20points%0Aover%20our%20SAM2Long%20baseline%20--%20securing%20the%202nd%20place%20in%20the%20LSVOS%202025%20VOS%0ATrack%2C%20and%20demonstrating%20strong%20robustness%20and%20accuracy%20in%20long%2C%20complex%20video%0Asegmentation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Label%2520Enhanced%2520Cascaded%2520Framework%253A%25202nd%2520Technical%2520Report%2520for%2520LSVOS%250A%2520%25202025%2520VOS%2520Track%26entry.906535625%3DAn%2520Yan%2520and%2520Leilei%2520Cao%2520and%2520Feng%2520Lu%2520and%2520Ran%2520Hong%2520and%2520Youhai%2520Jiang%2520and%2520Fengjie%2520Zhu%26entry.1292438233%3D%2520%2520Complex%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520presents%2520significant%2520challenges%2520in%250Aaccurately%2520segmenting%2520objects%2520across%2520frames%252C%2520especially%2520in%2520the%2520presence%2520of%250Asmall%2520and%2520similar%2520targets%252C%2520frequent%2520occlusions%252C%2520rapid%2520motion%252C%2520and%2520complex%250Ainteractions.%2520In%2520this%2520report%252C%2520we%2520present%2520our%2520solution%2520for%2520the%2520LSVOS%25202025%2520VOS%250ATrack%2520based%2520on%2520the%2520SAM2%2520framework.%2520We%2520adopt%2520a%2520pseudo-labeling%2520strategy%2520during%250Atraining%253A%2520a%2520trained%2520SAM2%2520checkpoint%2520is%2520deployed%2520within%2520the%2520SAM2Long%2520framework%250Ato%2520generate%2520pseudo%2520labels%2520for%2520the%2520MOSE%2520test%2520set%252C%2520which%2520are%2520then%2520combined%2520with%250Aexisting%2520data%2520for%2520further%2520training.%2520For%2520inference%252C%2520the%2520SAM2Long%2520framework%2520is%250Aemployed%2520to%2520obtain%2520our%2520primary%2520segmentation%2520results%252C%2520while%2520an%2520open-source%2520SeC%250Amodel%2520runs%2520in%2520parallel%2520to%2520produce%2520complementary%2520predictions.%2520A%2520cascaded%250Adecision%2520mechanism%2520dynamically%2520integrates%2520outputs%2520from%2520both%2520models%252C%2520exploiting%250Athe%2520temporal%2520stability%2520of%2520SAM2Long%2520and%2520the%2520concept-level%2520robustness%2520of%2520SeC.%250ABenefiting%2520from%2520pseudo-label%2520training%2520and%2520cascaded%2520multi-model%2520inference%252C%2520our%250Aapproach%2520achieves%2520a%2520J%255C%2526F%2520score%2520of%25200.8616%2520on%2520the%2520MOSE%2520test%2520set%2520--%2520%252B1.4%2520points%250Aover%2520our%2520SAM2Long%2520baseline%2520--%2520securing%2520the%25202nd%2520place%2520in%2520the%2520LSVOS%25202025%2520VOS%250ATrack%252C%2520and%2520demonstrating%2520strong%2520robustness%2520and%2520accuracy%2520in%2520long%252C%2520complex%2520video%250Asegmentation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Label%20Enhanced%20Cascaded%20Framework%3A%202nd%20Technical%20Report%20for%20LSVOS%0A%20%202025%20VOS%20Track&entry.906535625=An%20Yan%20and%20Leilei%20Cao%20and%20Feng%20Lu%20and%20Ran%20Hong%20and%20Youhai%20Jiang%20and%20Fengjie%20Zhu&entry.1292438233=%20%20Complex%20Video%20Object%20Segmentation%20%28VOS%29%20presents%20significant%20challenges%20in%0Aaccurately%20segmenting%20objects%20across%20frames%2C%20especially%20in%20the%20presence%20of%0Asmall%20and%20similar%20targets%2C%20frequent%20occlusions%2C%20rapid%20motion%2C%20and%20complex%0Ainteractions.%20In%20this%20report%2C%20we%20present%20our%20solution%20for%20the%20LSVOS%202025%20VOS%0ATrack%20based%20on%20the%20SAM2%20framework.%20We%20adopt%20a%20pseudo-labeling%20strategy%20during%0Atraining%3A%20a%20trained%20SAM2%20checkpoint%20is%20deployed%20within%20the%20SAM2Long%20framework%0Ato%20generate%20pseudo%20labels%20for%20the%20MOSE%20test%20set%2C%20which%20are%20then%20combined%20with%0Aexisting%20data%20for%20further%20training.%20For%20inference%2C%20the%20SAM2Long%20framework%20is%0Aemployed%20to%20obtain%20our%20primary%20segmentation%20results%2C%20while%20an%20open-source%20SeC%0Amodel%20runs%20in%20parallel%20to%20produce%20complementary%20predictions.%20A%20cascaded%0Adecision%20mechanism%20dynamically%20integrates%20outputs%20from%20both%20models%2C%20exploiting%0Athe%20temporal%20stability%20of%20SAM2Long%20and%20the%20concept-level%20robustness%20of%20SeC.%0ABenefiting%20from%20pseudo-label%20training%20and%20cascaded%20multi-model%20inference%2C%20our%0Aapproach%20achieves%20a%20J%5C%26F%20score%20of%200.8616%20on%20the%20MOSE%20test%20set%20--%20%2B1.4%20points%0Aover%20our%20SAM2Long%20baseline%20--%20securing%20the%202nd%20place%20in%20the%20LSVOS%202025%20VOS%0ATrack%2C%20and%20demonstrating%20strong%20robustness%20and%20accuracy%20in%20long%2C%20complex%20video%0Asegmentation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14901v1&entry.124074799=Read"},
{"title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for\n  Atypical Mitotic Figure Classification in MIDOG 2025", "author": "Guillaume Balezo and Hana Feki and Rapha\u00ebl Bourgade and Lily Monnier and Alice Blondel and Albert Pla Planas and Thomas Walter", "abstract": "  Atypical mitotic figures (AMFs) represent abnormal cell division associated\nwith poor prognosis. Yet their detection remains difficult due to low\nprevalence, subtle morphology, and inter-observer variability. The MIDOG 2025\nchallenge introduces a benchmark for AMF classification across multiple\ndomains. In this work, we fine-tuned the recently published DINOv3-H+ vision\ntransformer, pretrained on natural images, using low-rank adaptation (LoRA),\ntraining only ~1.3M parameters in combination with extensive augmentation and a\ndomain-weighted Focal Loss to handle domain heterogeneity. Despite the domain\ngap, our fine-tuned DINOv3 transfers effectively to histopathology, reaching\nsecond place on the preliminary test set. These results highlight the\nadvantages of DINOv3 pretraining and underline the efficiency and robustness of\nour fine-tuning strategy, yielding state-of-the-art results for the atypical\nmitosis classification challenge in MIDOG 2025.\n", "link": "http://arxiv.org/abs/2508.21041v2", "date": "2025-09-18", "relevancy": 2.1171, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.53}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025&body=Title%3A%20Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025%0AAuthor%3A%20Guillaume%20Balezo%20and%20Hana%20Feki%20and%20Rapha%C3%ABl%20Bourgade%20and%20Lily%20Monnier%20and%20Alice%20Blondel%20and%20Albert%20Pla%20Planas%20and%20Thomas%20Walter%0AAbstract%3A%20%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20represent%20abnormal%20cell%20division%20associated%0Awith%20poor%20prognosis.%20Yet%20their%20detection%20remains%20difficult%20due%20to%20low%0Aprevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%202025%0Achallenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20fine-tuned%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%0Atraining%20only%20~1.3M%20parameters%20in%20combination%20with%20extensive%20augmentation%20and%20a%0Adomain-weighted%20Focal%20Loss%20to%20handle%20domain%20heterogeneity.%20Despite%20the%20domain%0Agap%2C%20our%20fine-tuned%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%20reaching%0Asecond%20place%20on%20the%20preliminary%20test%20set.%20These%20results%20highlight%20the%0Aadvantages%20of%20DINOv3%20pretraining%20and%20underline%20the%20efficiency%20and%20robustness%20of%0Aour%20fine-tuning%20strategy%2C%20yielding%20state-of-the-art%20results%20for%20the%20atypical%0Amitosis%20classification%20challenge%20in%20MIDOG%202025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.21041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Fine-Tuning%2520of%2520DINOv3%2520Pretrained%2520on%2520Natural%2520Images%2520for%250A%2520%2520Atypical%2520Mitotic%2520Figure%2520Classification%2520in%2520MIDOG%25202025%26entry.906535625%3DGuillaume%2520Balezo%2520and%2520Hana%2520Feki%2520and%2520Rapha%25C3%25ABl%2520Bourgade%2520and%2520Lily%2520Monnier%2520and%2520Alice%2520Blondel%2520and%2520Albert%2520Pla%2520Planas%2520and%2520Thomas%2520Walter%26entry.1292438233%3D%2520%2520Atypical%2520mitotic%2520figures%2520%2528AMFs%2529%2520represent%2520abnormal%2520cell%2520division%2520associated%250Awith%2520poor%2520prognosis.%2520Yet%2520their%2520detection%2520remains%2520difficult%2520due%2520to%2520low%250Aprevalence%252C%2520subtle%2520morphology%252C%2520and%2520inter-observer%2520variability.%2520The%2520MIDOG%25202025%250Achallenge%2520introduces%2520a%2520benchmark%2520for%2520AMF%2520classification%2520across%2520multiple%250Adomains.%2520In%2520this%2520work%252C%2520we%2520fine-tuned%2520the%2520recently%2520published%2520DINOv3-H%252B%2520vision%250Atransformer%252C%2520pretrained%2520on%2520natural%2520images%252C%2520using%2520low-rank%2520adaptation%2520%2528LoRA%2529%252C%250Atraining%2520only%2520~1.3M%2520parameters%2520in%2520combination%2520with%2520extensive%2520augmentation%2520and%2520a%250Adomain-weighted%2520Focal%2520Loss%2520to%2520handle%2520domain%2520heterogeneity.%2520Despite%2520the%2520domain%250Agap%252C%2520our%2520fine-tuned%2520DINOv3%2520transfers%2520effectively%2520to%2520histopathology%252C%2520reaching%250Asecond%2520place%2520on%2520the%2520preliminary%2520test%2520set.%2520These%2520results%2520highlight%2520the%250Aadvantages%2520of%2520DINOv3%2520pretraining%2520and%2520underline%2520the%2520efficiency%2520and%2520robustness%2520of%250Aour%2520fine-tuning%2520strategy%252C%2520yielding%2520state-of-the-art%2520results%2520for%2520the%2520atypical%250Amitosis%2520classification%2520challenge%2520in%2520MIDOG%25202025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Fine-Tuning%20of%20DINOv3%20Pretrained%20on%20Natural%20Images%20for%0A%20%20Atypical%20Mitotic%20Figure%20Classification%20in%20MIDOG%202025&entry.906535625=Guillaume%20Balezo%20and%20Hana%20Feki%20and%20Rapha%C3%ABl%20Bourgade%20and%20Lily%20Monnier%20and%20Alice%20Blondel%20and%20Albert%20Pla%20Planas%20and%20Thomas%20Walter&entry.1292438233=%20%20Atypical%20mitotic%20figures%20%28AMFs%29%20represent%20abnormal%20cell%20division%20associated%0Awith%20poor%20prognosis.%20Yet%20their%20detection%20remains%20difficult%20due%20to%20low%0Aprevalence%2C%20subtle%20morphology%2C%20and%20inter-observer%20variability.%20The%20MIDOG%202025%0Achallenge%20introduces%20a%20benchmark%20for%20AMF%20classification%20across%20multiple%0Adomains.%20In%20this%20work%2C%20we%20fine-tuned%20the%20recently%20published%20DINOv3-H%2B%20vision%0Atransformer%2C%20pretrained%20on%20natural%20images%2C%20using%20low-rank%20adaptation%20%28LoRA%29%2C%0Atraining%20only%20~1.3M%20parameters%20in%20combination%20with%20extensive%20augmentation%20and%20a%0Adomain-weighted%20Focal%20Loss%20to%20handle%20domain%20heterogeneity.%20Despite%20the%20domain%0Agap%2C%20our%20fine-tuned%20DINOv3%20transfers%20effectively%20to%20histopathology%2C%20reaching%0Asecond%20place%20on%20the%20preliminary%20test%20set.%20These%20results%20highlight%20the%0Aadvantages%20of%20DINOv3%20pretraining%20and%20underline%20the%20efficiency%20and%20robustness%20of%0Aour%20fine-tuning%20strategy%2C%20yielding%20state-of-the-art%20results%20for%20the%20atypical%0Amitosis%20classification%20challenge%20in%20MIDOG%202025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.21041v2&entry.124074799=Read"},
{"title": "CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids", "author": "Punith Reddy Vanteddu and Davide Gorbani and Giuseppe L'Erario and Hosameldin Awadalla Omer Mohamed and Fabio Bergonti and Daniele Pucci", "abstract": "  This paper presents a CAD-driven co-design framework for optimizing\njet-powered aerial humanoid robots to execute dynamically constrained\ntrajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments\n(DoE) approach is used to generate 5,000 geometrically varied and mechanically\nfeasible designs by modifying limb dimensions, jet interface geometry (e.g.,\nangle and offset), and overall mass distribution. Each model is constructed\nthrough CAD assemblies to ensure structural validity and compatibility with\nsimulation tools. To reduce computational cost and enable parameter sensitivity\nanalysis, the models are clustered using K-means, with representative centroids\nselected for evaluation. A minimum-jerk trajectory is used to assess flight\nperformance, providing position and velocity references for a momentum-based\nlinearized Model Predictive Control (MPC) strategy. A multi-objective\noptimization is then conducted using the NSGA-II algorithm, jointly exploring\nthe space of design centroids and MPC gain parameters. The objectives are to\nminimize trajectory tracking error and mechanical energy expenditure. The\nframework outputs a set of flight-ready humanoid configurations with validated\ncontrol parameters, offering a structured method for selecting and implementing\nfeasible aerial humanoid designs.\n", "link": "http://arxiv.org/abs/2509.14935v1", "date": "2025-09-18", "relevancy": 2.1133, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5314}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Driven%20Co-Design%20for%20Flight-Ready%20Jet-Powered%20Humanoids&body=Title%3A%20CAD-Driven%20Co-Design%20for%20Flight-Ready%20Jet-Powered%20Humanoids%0AAuthor%3A%20Punith%20Reddy%20Vanteddu%20and%20Davide%20Gorbani%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Fabio%20Bergonti%20and%20Daniele%20Pucci%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20CAD-driven%20co-design%20framework%20for%20optimizing%0Ajet-powered%20aerial%20humanoid%20robots%20to%20execute%20dynamically%20constrained%0Atrajectories.%20Starting%20from%20the%20iRonCub-Mk3%20model%2C%20a%20Design%20of%20Experiments%0A%28DoE%29%20approach%20is%20used%20to%20generate%205%2C000%20geometrically%20varied%20and%20mechanically%0Afeasible%20designs%20by%20modifying%20limb%20dimensions%2C%20jet%20interface%20geometry%20%28e.g.%2C%0Aangle%20and%20offset%29%2C%20and%20overall%20mass%20distribution.%20Each%20model%20is%20constructed%0Athrough%20CAD%20assemblies%20to%20ensure%20structural%20validity%20and%20compatibility%20with%0Asimulation%20tools.%20To%20reduce%20computational%20cost%20and%20enable%20parameter%20sensitivity%0Aanalysis%2C%20the%20models%20are%20clustered%20using%20K-means%2C%20with%20representative%20centroids%0Aselected%20for%20evaluation.%20A%20minimum-jerk%20trajectory%20is%20used%20to%20assess%20flight%0Aperformance%2C%20providing%20position%20and%20velocity%20references%20for%20a%20momentum-based%0Alinearized%20Model%20Predictive%20Control%20%28MPC%29%20strategy.%20A%20multi-objective%0Aoptimization%20is%20then%20conducted%20using%20the%20NSGA-II%20algorithm%2C%20jointly%20exploring%0Athe%20space%20of%20design%20centroids%20and%20MPC%20gain%20parameters.%20The%20objectives%20are%20to%0Aminimize%20trajectory%20tracking%20error%20and%20mechanical%20energy%20expenditure.%20The%0Aframework%20outputs%20a%20set%20of%20flight-ready%20humanoid%20configurations%20with%20validated%0Acontrol%20parameters%2C%20offering%20a%20structured%20method%20for%20selecting%20and%20implementing%0Afeasible%20aerial%20humanoid%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Driven%2520Co-Design%2520for%2520Flight-Ready%2520Jet-Powered%2520Humanoids%26entry.906535625%3DPunith%2520Reddy%2520Vanteddu%2520and%2520Davide%2520Gorbani%2520and%2520Giuseppe%2520L%2527Erario%2520and%2520Hosameldin%2520Awadalla%2520Omer%2520Mohamed%2520and%2520Fabio%2520Bergonti%2520and%2520Daniele%2520Pucci%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520CAD-driven%2520co-design%2520framework%2520for%2520optimizing%250Ajet-powered%2520aerial%2520humanoid%2520robots%2520to%2520execute%2520dynamically%2520constrained%250Atrajectories.%2520Starting%2520from%2520the%2520iRonCub-Mk3%2520model%252C%2520a%2520Design%2520of%2520Experiments%250A%2528DoE%2529%2520approach%2520is%2520used%2520to%2520generate%25205%252C000%2520geometrically%2520varied%2520and%2520mechanically%250Afeasible%2520designs%2520by%2520modifying%2520limb%2520dimensions%252C%2520jet%2520interface%2520geometry%2520%2528e.g.%252C%250Aangle%2520and%2520offset%2529%252C%2520and%2520overall%2520mass%2520distribution.%2520Each%2520model%2520is%2520constructed%250Athrough%2520CAD%2520assemblies%2520to%2520ensure%2520structural%2520validity%2520and%2520compatibility%2520with%250Asimulation%2520tools.%2520To%2520reduce%2520computational%2520cost%2520and%2520enable%2520parameter%2520sensitivity%250Aanalysis%252C%2520the%2520models%2520are%2520clustered%2520using%2520K-means%252C%2520with%2520representative%2520centroids%250Aselected%2520for%2520evaluation.%2520A%2520minimum-jerk%2520trajectory%2520is%2520used%2520to%2520assess%2520flight%250Aperformance%252C%2520providing%2520position%2520and%2520velocity%2520references%2520for%2520a%2520momentum-based%250Alinearized%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520strategy.%2520A%2520multi-objective%250Aoptimization%2520is%2520then%2520conducted%2520using%2520the%2520NSGA-II%2520algorithm%252C%2520jointly%2520exploring%250Athe%2520space%2520of%2520design%2520centroids%2520and%2520MPC%2520gain%2520parameters.%2520The%2520objectives%2520are%2520to%250Aminimize%2520trajectory%2520tracking%2520error%2520and%2520mechanical%2520energy%2520expenditure.%2520The%250Aframework%2520outputs%2520a%2520set%2520of%2520flight-ready%2520humanoid%2520configurations%2520with%2520validated%250Acontrol%2520parameters%252C%2520offering%2520a%2520structured%2520method%2520for%2520selecting%2520and%2520implementing%250Afeasible%2520aerial%2520humanoid%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Driven%20Co-Design%20for%20Flight-Ready%20Jet-Powered%20Humanoids&entry.906535625=Punith%20Reddy%20Vanteddu%20and%20Davide%20Gorbani%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Fabio%20Bergonti%20and%20Daniele%20Pucci&entry.1292438233=%20%20This%20paper%20presents%20a%20CAD-driven%20co-design%20framework%20for%20optimizing%0Ajet-powered%20aerial%20humanoid%20robots%20to%20execute%20dynamically%20constrained%0Atrajectories.%20Starting%20from%20the%20iRonCub-Mk3%20model%2C%20a%20Design%20of%20Experiments%0A%28DoE%29%20approach%20is%20used%20to%20generate%205%2C000%20geometrically%20varied%20and%20mechanically%0Afeasible%20designs%20by%20modifying%20limb%20dimensions%2C%20jet%20interface%20geometry%20%28e.g.%2C%0Aangle%20and%20offset%29%2C%20and%20overall%20mass%20distribution.%20Each%20model%20is%20constructed%0Athrough%20CAD%20assemblies%20to%20ensure%20structural%20validity%20and%20compatibility%20with%0Asimulation%20tools.%20To%20reduce%20computational%20cost%20and%20enable%20parameter%20sensitivity%0Aanalysis%2C%20the%20models%20are%20clustered%20using%20K-means%2C%20with%20representative%20centroids%0Aselected%20for%20evaluation.%20A%20minimum-jerk%20trajectory%20is%20used%20to%20assess%20flight%0Aperformance%2C%20providing%20position%20and%20velocity%20references%20for%20a%20momentum-based%0Alinearized%20Model%20Predictive%20Control%20%28MPC%29%20strategy.%20A%20multi-objective%0Aoptimization%20is%20then%20conducted%20using%20the%20NSGA-II%20algorithm%2C%20jointly%20exploring%0Athe%20space%20of%20design%20centroids%20and%20MPC%20gain%20parameters.%20The%20objectives%20are%20to%0Aminimize%20trajectory%20tracking%20error%20and%20mechanical%20energy%20expenditure.%20The%0Aframework%20outputs%20a%20set%20of%20flight-ready%20humanoid%20configurations%20with%20validated%0Acontrol%20parameters%2C%20offering%20a%20structured%20method%20for%20selecting%20and%20implementing%0Afeasible%20aerial%20humanoid%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14935v1&entry.124074799=Read"},
{"title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming\n  Together with Human", "author": "Nan Sun and Yongchang Li and Chenxu Wang and Huiying Li and Huaping Liu", "abstract": "  In this work, we present CollabVLA, a self-reflective vision-language-action\nframework that transforms a standard visuomotor policy into a collaborative\nassistant. CollabVLA tackles key limitations of prior VLAs, including domain\noverfitting, non-interpretable reasoning, and the high latency of auxiliary\ngenerative models, by integrating VLM-based reflective reasoning with\ndiffusion-based action generation under a mixture-of-experts design. Through a\ntwo-stage training recipe of action grounding and reflection tuning, it\nsupports explicit self-reflection and proactively solicits human guidance when\nconfronted with uncertainty or repeated failure. It cuts normalized Time by ~2x\nand Dream counts by ~4x vs. generative agents, achieving higher success rates,\nimproved interpretability, and balanced low latency compared with existing\nmethods. This work takes a pioneering step toward shifting VLAs from opaque\ncontrollers to genuinely assistive agents capable of reasoning, acting, and\ncollaborating with humans.\n", "link": "http://arxiv.org/abs/2509.14889v1", "date": "2025-09-18", "relevancy": 2.1097, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CollabVLA%3A%20Self-Reflective%20Vision-Language-Action%20Model%20Dreaming%0A%20%20Together%20with%20Human&body=Title%3A%20CollabVLA%3A%20Self-Reflective%20Vision-Language-Action%20Model%20Dreaming%0A%20%20Together%20with%20Human%0AAuthor%3A%20Nan%20Sun%20and%20Yongchang%20Li%20and%20Chenxu%20Wang%20and%20Huiying%20Li%20and%20Huaping%20Liu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20CollabVLA%2C%20a%20self-reflective%20vision-language-action%0Aframework%20that%20transforms%20a%20standard%20visuomotor%20policy%20into%20a%20collaborative%0Aassistant.%20CollabVLA%20tackles%20key%20limitations%20of%20prior%20VLAs%2C%20including%20domain%0Aoverfitting%2C%20non-interpretable%20reasoning%2C%20and%20the%20high%20latency%20of%20auxiliary%0Agenerative%20models%2C%20by%20integrating%20VLM-based%20reflective%20reasoning%20with%0Adiffusion-based%20action%20generation%20under%20a%20mixture-of-experts%20design.%20Through%20a%0Atwo-stage%20training%20recipe%20of%20action%20grounding%20and%20reflection%20tuning%2C%20it%0Asupports%20explicit%20self-reflection%20and%20proactively%20solicits%20human%20guidance%20when%0Aconfronted%20with%20uncertainty%20or%20repeated%20failure.%20It%20cuts%20normalized%20Time%20by%20~2x%0Aand%20Dream%20counts%20by%20~4x%20vs.%20generative%20agents%2C%20achieving%20higher%20success%20rates%2C%0Aimproved%20interpretability%2C%20and%20balanced%20low%20latency%20compared%20with%20existing%0Amethods.%20This%20work%20takes%20a%20pioneering%20step%20toward%20shifting%20VLAs%20from%20opaque%0Acontrollers%20to%20genuinely%20assistive%20agents%20capable%20of%20reasoning%2C%20acting%2C%20and%0Acollaborating%20with%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollabVLA%253A%2520Self-Reflective%2520Vision-Language-Action%2520Model%2520Dreaming%250A%2520%2520Together%2520with%2520Human%26entry.906535625%3DNan%2520Sun%2520and%2520Yongchang%2520Li%2520and%2520Chenxu%2520Wang%2520and%2520Huiying%2520Li%2520and%2520Huaping%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520CollabVLA%252C%2520a%2520self-reflective%2520vision-language-action%250Aframework%2520that%2520transforms%2520a%2520standard%2520visuomotor%2520policy%2520into%2520a%2520collaborative%250Aassistant.%2520CollabVLA%2520tackles%2520key%2520limitations%2520of%2520prior%2520VLAs%252C%2520including%2520domain%250Aoverfitting%252C%2520non-interpretable%2520reasoning%252C%2520and%2520the%2520high%2520latency%2520of%2520auxiliary%250Agenerative%2520models%252C%2520by%2520integrating%2520VLM-based%2520reflective%2520reasoning%2520with%250Adiffusion-based%2520action%2520generation%2520under%2520a%2520mixture-of-experts%2520design.%2520Through%2520a%250Atwo-stage%2520training%2520recipe%2520of%2520action%2520grounding%2520and%2520reflection%2520tuning%252C%2520it%250Asupports%2520explicit%2520self-reflection%2520and%2520proactively%2520solicits%2520human%2520guidance%2520when%250Aconfronted%2520with%2520uncertainty%2520or%2520repeated%2520failure.%2520It%2520cuts%2520normalized%2520Time%2520by%2520~2x%250Aand%2520Dream%2520counts%2520by%2520~4x%2520vs.%2520generative%2520agents%252C%2520achieving%2520higher%2520success%2520rates%252C%250Aimproved%2520interpretability%252C%2520and%2520balanced%2520low%2520latency%2520compared%2520with%2520existing%250Amethods.%2520This%2520work%2520takes%2520a%2520pioneering%2520step%2520toward%2520shifting%2520VLAs%2520from%2520opaque%250Acontrollers%2520to%2520genuinely%2520assistive%2520agents%2520capable%2520of%2520reasoning%252C%2520acting%252C%2520and%250Acollaborating%2520with%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CollabVLA%3A%20Self-Reflective%20Vision-Language-Action%20Model%20Dreaming%0A%20%20Together%20with%20Human&entry.906535625=Nan%20Sun%20and%20Yongchang%20Li%20and%20Chenxu%20Wang%20and%20Huiying%20Li%20and%20Huaping%20Liu&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20CollabVLA%2C%20a%20self-reflective%20vision-language-action%0Aframework%20that%20transforms%20a%20standard%20visuomotor%20policy%20into%20a%20collaborative%0Aassistant.%20CollabVLA%20tackles%20key%20limitations%20of%20prior%20VLAs%2C%20including%20domain%0Aoverfitting%2C%20non-interpretable%20reasoning%2C%20and%20the%20high%20latency%20of%20auxiliary%0Agenerative%20models%2C%20by%20integrating%20VLM-based%20reflective%20reasoning%20with%0Adiffusion-based%20action%20generation%20under%20a%20mixture-of-experts%20design.%20Through%20a%0Atwo-stage%20training%20recipe%20of%20action%20grounding%20and%20reflection%20tuning%2C%20it%0Asupports%20explicit%20self-reflection%20and%20proactively%20solicits%20human%20guidance%20when%0Aconfronted%20with%20uncertainty%20or%20repeated%20failure.%20It%20cuts%20normalized%20Time%20by%20~2x%0Aand%20Dream%20counts%20by%20~4x%20vs.%20generative%20agents%2C%20achieving%20higher%20success%20rates%2C%0Aimproved%20interpretability%2C%20and%20balanced%20low%20latency%20compared%20with%20existing%0Amethods.%20This%20work%20takes%20a%20pioneering%20step%20toward%20shifting%20VLAs%20from%20opaque%0Acontrollers%20to%20genuinely%20assistive%20agents%20capable%20of%20reasoning%2C%20acting%2C%20and%0Acollaborating%20with%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14889v1&entry.124074799=Read"},
{"title": "Probabilistic and nonlinear compressive sensing", "author": "Lukas Silvester Barth and Paulo von Petersenn", "abstract": "  We present a smooth probabilistic reformulation of $\\ell_0$ regularized\nregression that does not require Monte Carlo sampling and allows for the\ncomputation of exact gradients, facilitating rapid convergence to local optima\nof the best subset selection problem. The method drastically improves\nconvergence speed compared to similar Monte Carlo based approaches.\nFurthermore, we empirically demonstrate that it outperforms compressive sensing\nalgorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and\nsignal-to-noise ratios. The implementation runs efficiently on both CPUs and\nGPUs and is freely available at\nhttps://github.com/L0-and-behold/probabilistic-nonlinear-cs.\n  We also contribute to research on nonlinear generalizations of compressive\nsensing by investigating when parameter recovery of a nonlinear teacher network\nis possible through compression of a student network. Building upon theorems of\nFefferman and Markel, we show theoretically that the global optimum in the\ninfinite-data limit enforces recovery up to certain symmetries. For empirical\nvalidation, we implement a normal-form algorithm that selects a canonical\nrepresentative within each symmetry class. However, while compression can help\nto improve test loss, we find that exact parameter recovery is not even\npossible up to symmetries. In particular, we observe a surprising rebound\neffect where teacher and student configurations initially converge but\nsubsequently diverge despite continuous decrease in test loss. These findings\nindicate fundamental differences between linear and nonlinear compressive\nsensing.\n", "link": "http://arxiv.org/abs/2509.15060v1", "date": "2025-09-18", "relevancy": 2.1088, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20and%20nonlinear%20compressive%20sensing&body=Title%3A%20Probabilistic%20and%20nonlinear%20compressive%20sensing%0AAuthor%3A%20Lukas%20Silvester%20Barth%20and%20Paulo%20von%20Petersenn%0AAbstract%3A%20%20%20We%20present%20a%20smooth%20probabilistic%20reformulation%20of%20%24%5Cell_0%24%20regularized%0Aregression%20that%20does%20not%20require%20Monte%20Carlo%20sampling%20and%20allows%20for%20the%0Acomputation%20of%20exact%20gradients%2C%20facilitating%20rapid%20convergence%20to%20local%20optima%0Aof%20the%20best%20subset%20selection%20problem.%20The%20method%20drastically%20improves%0Aconvergence%20speed%20compared%20to%20similar%20Monte%20Carlo%20based%20approaches.%0AFurthermore%2C%20we%20empirically%20demonstrate%20that%20it%20outperforms%20compressive%20sensing%0Aalgorithms%20such%20as%20IHT%20and%20%28Relaxed-%29%20Lasso%20across%20a%20wide%20range%20of%20settings%20and%0Asignal-to-noise%20ratios.%20The%20implementation%20runs%20efficiently%20on%20both%20CPUs%20and%0AGPUs%20and%20is%20freely%20available%20at%0Ahttps%3A//github.com/L0-and-behold/probabilistic-nonlinear-cs.%0A%20%20We%20also%20contribute%20to%20research%20on%20nonlinear%20generalizations%20of%20compressive%0Asensing%20by%20investigating%20when%20parameter%20recovery%20of%20a%20nonlinear%20teacher%20network%0Ais%20possible%20through%20compression%20of%20a%20student%20network.%20Building%20upon%20theorems%20of%0AFefferman%20and%20Markel%2C%20we%20show%20theoretically%20that%20the%20global%20optimum%20in%20the%0Ainfinite-data%20limit%20enforces%20recovery%20up%20to%20certain%20symmetries.%20For%20empirical%0Avalidation%2C%20we%20implement%20a%20normal-form%20algorithm%20that%20selects%20a%20canonical%0Arepresentative%20within%20each%20symmetry%20class.%20However%2C%20while%20compression%20can%20help%0Ato%20improve%20test%20loss%2C%20we%20find%20that%20exact%20parameter%20recovery%20is%20not%20even%0Apossible%20up%20to%20symmetries.%20In%20particular%2C%20we%20observe%20a%20surprising%20rebound%0Aeffect%20where%20teacher%20and%20student%20configurations%20initially%20converge%20but%0Asubsequently%20diverge%20despite%20continuous%20decrease%20in%20test%20loss.%20These%20findings%0Aindicate%20fundamental%20differences%20between%20linear%20and%20nonlinear%20compressive%0Asensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520and%2520nonlinear%2520compressive%2520sensing%26entry.906535625%3DLukas%2520Silvester%2520Barth%2520and%2520Paulo%2520von%2520Petersenn%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520smooth%2520probabilistic%2520reformulation%2520of%2520%2524%255Cell_0%2524%2520regularized%250Aregression%2520that%2520does%2520not%2520require%2520Monte%2520Carlo%2520sampling%2520and%2520allows%2520for%2520the%250Acomputation%2520of%2520exact%2520gradients%252C%2520facilitating%2520rapid%2520convergence%2520to%2520local%2520optima%250Aof%2520the%2520best%2520subset%2520selection%2520problem.%2520The%2520method%2520drastically%2520improves%250Aconvergence%2520speed%2520compared%2520to%2520similar%2520Monte%2520Carlo%2520based%2520approaches.%250AFurthermore%252C%2520we%2520empirically%2520demonstrate%2520that%2520it%2520outperforms%2520compressive%2520sensing%250Aalgorithms%2520such%2520as%2520IHT%2520and%2520%2528Relaxed-%2529%2520Lasso%2520across%2520a%2520wide%2520range%2520of%2520settings%2520and%250Asignal-to-noise%2520ratios.%2520The%2520implementation%2520runs%2520efficiently%2520on%2520both%2520CPUs%2520and%250AGPUs%2520and%2520is%2520freely%2520available%2520at%250Ahttps%253A//github.com/L0-and-behold/probabilistic-nonlinear-cs.%250A%2520%2520We%2520also%2520contribute%2520to%2520research%2520on%2520nonlinear%2520generalizations%2520of%2520compressive%250Asensing%2520by%2520investigating%2520when%2520parameter%2520recovery%2520of%2520a%2520nonlinear%2520teacher%2520network%250Ais%2520possible%2520through%2520compression%2520of%2520a%2520student%2520network.%2520Building%2520upon%2520theorems%2520of%250AFefferman%2520and%2520Markel%252C%2520we%2520show%2520theoretically%2520that%2520the%2520global%2520optimum%2520in%2520the%250Ainfinite-data%2520limit%2520enforces%2520recovery%2520up%2520to%2520certain%2520symmetries.%2520For%2520empirical%250Avalidation%252C%2520we%2520implement%2520a%2520normal-form%2520algorithm%2520that%2520selects%2520a%2520canonical%250Arepresentative%2520within%2520each%2520symmetry%2520class.%2520However%252C%2520while%2520compression%2520can%2520help%250Ato%2520improve%2520test%2520loss%252C%2520we%2520find%2520that%2520exact%2520parameter%2520recovery%2520is%2520not%2520even%250Apossible%2520up%2520to%2520symmetries.%2520In%2520particular%252C%2520we%2520observe%2520a%2520surprising%2520rebound%250Aeffect%2520where%2520teacher%2520and%2520student%2520configurations%2520initially%2520converge%2520but%250Asubsequently%2520diverge%2520despite%2520continuous%2520decrease%2520in%2520test%2520loss.%2520These%2520findings%250Aindicate%2520fundamental%2520differences%2520between%2520linear%2520and%2520nonlinear%2520compressive%250Asensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20and%20nonlinear%20compressive%20sensing&entry.906535625=Lukas%20Silvester%20Barth%20and%20Paulo%20von%20Petersenn&entry.1292438233=%20%20We%20present%20a%20smooth%20probabilistic%20reformulation%20of%20%24%5Cell_0%24%20regularized%0Aregression%20that%20does%20not%20require%20Monte%20Carlo%20sampling%20and%20allows%20for%20the%0Acomputation%20of%20exact%20gradients%2C%20facilitating%20rapid%20convergence%20to%20local%20optima%0Aof%20the%20best%20subset%20selection%20problem.%20The%20method%20drastically%20improves%0Aconvergence%20speed%20compared%20to%20similar%20Monte%20Carlo%20based%20approaches.%0AFurthermore%2C%20we%20empirically%20demonstrate%20that%20it%20outperforms%20compressive%20sensing%0Aalgorithms%20such%20as%20IHT%20and%20%28Relaxed-%29%20Lasso%20across%20a%20wide%20range%20of%20settings%20and%0Asignal-to-noise%20ratios.%20The%20implementation%20runs%20efficiently%20on%20both%20CPUs%20and%0AGPUs%20and%20is%20freely%20available%20at%0Ahttps%3A//github.com/L0-and-behold/probabilistic-nonlinear-cs.%0A%20%20We%20also%20contribute%20to%20research%20on%20nonlinear%20generalizations%20of%20compressive%0Asensing%20by%20investigating%20when%20parameter%20recovery%20of%20a%20nonlinear%20teacher%20network%0Ais%20possible%20through%20compression%20of%20a%20student%20network.%20Building%20upon%20theorems%20of%0AFefferman%20and%20Markel%2C%20we%20show%20theoretically%20that%20the%20global%20optimum%20in%20the%0Ainfinite-data%20limit%20enforces%20recovery%20up%20to%20certain%20symmetries.%20For%20empirical%0Avalidation%2C%20we%20implement%20a%20normal-form%20algorithm%20that%20selects%20a%20canonical%0Arepresentative%20within%20each%20symmetry%20class.%20However%2C%20while%20compression%20can%20help%0Ato%20improve%20test%20loss%2C%20we%20find%20that%20exact%20parameter%20recovery%20is%20not%20even%0Apossible%20up%20to%20symmetries.%20In%20particular%2C%20we%20observe%20a%20surprising%20rebound%0Aeffect%20where%20teacher%20and%20student%20configurations%20initially%20converge%20but%0Asubsequently%20diverge%20despite%20continuous%20decrease%20in%20test%20loss.%20These%20findings%0Aindicate%20fundamental%20differences%20between%20linear%20and%20nonlinear%20compressive%0Asensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15060v1&entry.124074799=Read"},
{"title": "Mitigating data replication in text-to-audio generative diffusion models\n  through anti-memorization guidance", "author": "Francisco Messina and Francesca Ronchini and Luca Comanducci and Paolo Bestagini and Fabio Antonacci", "abstract": "  A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment.\n", "link": "http://arxiv.org/abs/2509.14934v1", "date": "2025-09-18", "relevancy": 2.1038, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5315}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5291}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20data%20replication%20in%20text-to-audio%20generative%20diffusion%20models%0A%20%20through%20anti-memorization%20guidance&body=Title%3A%20Mitigating%20data%20replication%20in%20text-to-audio%20generative%20diffusion%20models%0A%20%20through%20anti-memorization%20guidance%0AAuthor%3A%20Francisco%20Messina%20and%20Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Paolo%20Bestagini%20and%20Fabio%20Antonacci%0AAbstract%3A%20%20%20A%20persistent%20challenge%20in%20generative%20audio%20models%20is%20data%20replication%2C%20where%0Athe%20model%20unintentionally%20generates%20parts%20of%20its%20training%20data%20during%0Ainference.%20In%20this%20work%2C%20we%20address%20this%20issue%20in%20text-to-audio%20diffusion%0Amodels%20by%20exploring%20the%20use%20of%20anti-memorization%20strategies.%20We%20adopt%0AAnti-Memorization%20Guidance%20%28AMG%29%2C%20a%20technique%20that%20modifies%20the%20sampling%0Aprocess%20of%20pre-trained%20diffusion%20models%20to%20discourage%20memorization.%20Our%20study%0Aexplores%20three%20types%20of%20guidance%20within%20AMG%2C%20each%20designed%20to%20reduce%0Areplication%20while%20preserving%20generation%20quality.%20We%20use%20Stable%20Audio%20Open%20as%0Aour%20backbone%2C%20leveraging%20its%20fully%20open-source%20architecture%20and%20training%0Adataset.%20Our%20comprehensive%20experimental%20analysis%20suggests%20that%20AMG%0Asignificantly%20mitigates%20memorization%20in%20diffusion-based%20text-to-audio%0Ageneration%20without%20compromising%20audio%20fidelity%20or%20semantic%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520data%2520replication%2520in%2520text-to-audio%2520generative%2520diffusion%2520models%250A%2520%2520through%2520anti-memorization%2520guidance%26entry.906535625%3DFrancisco%2520Messina%2520and%2520Francesca%2520Ronchini%2520and%2520Luca%2520Comanducci%2520and%2520Paolo%2520Bestagini%2520and%2520Fabio%2520Antonacci%26entry.1292438233%3D%2520%2520A%2520persistent%2520challenge%2520in%2520generative%2520audio%2520models%2520is%2520data%2520replication%252C%2520where%250Athe%2520model%2520unintentionally%2520generates%2520parts%2520of%2520its%2520training%2520data%2520during%250Ainference.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520issue%2520in%2520text-to-audio%2520diffusion%250Amodels%2520by%2520exploring%2520the%2520use%2520of%2520anti-memorization%2520strategies.%2520We%2520adopt%250AAnti-Memorization%2520Guidance%2520%2528AMG%2529%252C%2520a%2520technique%2520that%2520modifies%2520the%2520sampling%250Aprocess%2520of%2520pre-trained%2520diffusion%2520models%2520to%2520discourage%2520memorization.%2520Our%2520study%250Aexplores%2520three%2520types%2520of%2520guidance%2520within%2520AMG%252C%2520each%2520designed%2520to%2520reduce%250Areplication%2520while%2520preserving%2520generation%2520quality.%2520We%2520use%2520Stable%2520Audio%2520Open%2520as%250Aour%2520backbone%252C%2520leveraging%2520its%2520fully%2520open-source%2520architecture%2520and%2520training%250Adataset.%2520Our%2520comprehensive%2520experimental%2520analysis%2520suggests%2520that%2520AMG%250Asignificantly%2520mitigates%2520memorization%2520in%2520diffusion-based%2520text-to-audio%250Ageneration%2520without%2520compromising%2520audio%2520fidelity%2520or%2520semantic%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20data%20replication%20in%20text-to-audio%20generative%20diffusion%20models%0A%20%20through%20anti-memorization%20guidance&entry.906535625=Francisco%20Messina%20and%20Francesca%20Ronchini%20and%20Luca%20Comanducci%20and%20Paolo%20Bestagini%20and%20Fabio%20Antonacci&entry.1292438233=%20%20A%20persistent%20challenge%20in%20generative%20audio%20models%20is%20data%20replication%2C%20where%0Athe%20model%20unintentionally%20generates%20parts%20of%20its%20training%20data%20during%0Ainference.%20In%20this%20work%2C%20we%20address%20this%20issue%20in%20text-to-audio%20diffusion%0Amodels%20by%20exploring%20the%20use%20of%20anti-memorization%20strategies.%20We%20adopt%0AAnti-Memorization%20Guidance%20%28AMG%29%2C%20a%20technique%20that%20modifies%20the%20sampling%0Aprocess%20of%20pre-trained%20diffusion%20models%20to%20discourage%20memorization.%20Our%20study%0Aexplores%20three%20types%20of%20guidance%20within%20AMG%2C%20each%20designed%20to%20reduce%0Areplication%20while%20preserving%20generation%20quality.%20We%20use%20Stable%20Audio%20Open%20as%0Aour%20backbone%2C%20leveraging%20its%20fully%20open-source%20architecture%20and%20training%0Adataset.%20Our%20comprehensive%20experimental%20analysis%20suggests%20that%20AMG%0Asignificantly%20mitigates%20memorization%20in%20diffusion-based%20text-to-audio%0Ageneration%20without%20compromising%20audio%20fidelity%20or%20semantic%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14934v1&entry.124074799=Read"},
{"title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching", "author": "Simon Welker and Tal Peer and Timo Gerkmann", "abstract": "  The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram\nto an audio waveform, is still a key component in many text-to-speech (TTS)\nsystems today. Based on generative flow matching, our prior work on generative\nSTFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel\nfilterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for\nspeech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total\nlatency of 48 ms. We show real-time streaming capability at this latency not\nonly in theory, but in practice on a consumer laptop GPU. Furthermore, we show\nthat our model achieves substantially better PESQ and SI-SDR values compared to\nwell-established not streaming-capable baselines for Mel vocoding including\nHiFi-GAN.\n", "link": "http://arxiv.org/abs/2509.15085v1", "date": "2025-09-18", "relevancy": 2.0819, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5864}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Streaming%20Mel%20Vocoding%20with%20Generative%20Flow%20Matching&body=Title%3A%20Real-Time%20Streaming%20Mel%20Vocoding%20with%20Generative%20Flow%20Matching%0AAuthor%3A%20Simon%20Welker%20and%20Tal%20Peer%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20The%20task%20of%20Mel%20vocoding%2C%20i.e.%2C%20the%20inversion%20of%20a%20Mel%20magnitude%20spectrogram%0Ato%20an%20audio%20waveform%2C%20is%20still%20a%20key%20component%20in%20many%20text-to-speech%20%28TTS%29%0Asystems%20today.%20Based%20on%20generative%20flow%20matching%2C%20our%20prior%20work%20on%20generative%0ASTFT%20phase%20retrieval%20%28DiffPhase%29%2C%20and%20the%20pseudoinverse%20operator%20of%20the%20Mel%0Afilterbank%2C%20we%20develop%20MelFlow%2C%20a%20streaming-capable%20generative%20Mel%20vocoder%20for%0Aspeech%20sampled%20at%2016%20kHz%20with%20an%20algorithmic%20latency%20of%20only%2032%20ms%20and%20a%20total%0Alatency%20of%2048%20ms.%20We%20show%20real-time%20streaming%20capability%20at%20this%20latency%20not%0Aonly%20in%20theory%2C%20but%20in%20practice%20on%20a%20consumer%20laptop%20GPU.%20Furthermore%2C%20we%20show%0Athat%20our%20model%20achieves%20substantially%20better%20PESQ%20and%20SI-SDR%20values%20compared%20to%0Awell-established%20not%20streaming-capable%20baselines%20for%20Mel%20vocoding%20including%0AHiFi-GAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Streaming%2520Mel%2520Vocoding%2520with%2520Generative%2520Flow%2520Matching%26entry.906535625%3DSimon%2520Welker%2520and%2520Tal%2520Peer%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520Mel%2520vocoding%252C%2520i.e.%252C%2520the%2520inversion%2520of%2520a%2520Mel%2520magnitude%2520spectrogram%250Ato%2520an%2520audio%2520waveform%252C%2520is%2520still%2520a%2520key%2520component%2520in%2520many%2520text-to-speech%2520%2528TTS%2529%250Asystems%2520today.%2520Based%2520on%2520generative%2520flow%2520matching%252C%2520our%2520prior%2520work%2520on%2520generative%250ASTFT%2520phase%2520retrieval%2520%2528DiffPhase%2529%252C%2520and%2520the%2520pseudoinverse%2520operator%2520of%2520the%2520Mel%250Afilterbank%252C%2520we%2520develop%2520MelFlow%252C%2520a%2520streaming-capable%2520generative%2520Mel%2520vocoder%2520for%250Aspeech%2520sampled%2520at%252016%2520kHz%2520with%2520an%2520algorithmic%2520latency%2520of%2520only%252032%2520ms%2520and%2520a%2520total%250Alatency%2520of%252048%2520ms.%2520We%2520show%2520real-time%2520streaming%2520capability%2520at%2520this%2520latency%2520not%250Aonly%2520in%2520theory%252C%2520but%2520in%2520practice%2520on%2520a%2520consumer%2520laptop%2520GPU.%2520Furthermore%252C%2520we%2520show%250Athat%2520our%2520model%2520achieves%2520substantially%2520better%2520PESQ%2520and%2520SI-SDR%2520values%2520compared%2520to%250Awell-established%2520not%2520streaming-capable%2520baselines%2520for%2520Mel%2520vocoding%2520including%250AHiFi-GAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Streaming%20Mel%20Vocoding%20with%20Generative%20Flow%20Matching&entry.906535625=Simon%20Welker%20and%20Tal%20Peer%20and%20Timo%20Gerkmann&entry.1292438233=%20%20The%20task%20of%20Mel%20vocoding%2C%20i.e.%2C%20the%20inversion%20of%20a%20Mel%20magnitude%20spectrogram%0Ato%20an%20audio%20waveform%2C%20is%20still%20a%20key%20component%20in%20many%20text-to-speech%20%28TTS%29%0Asystems%20today.%20Based%20on%20generative%20flow%20matching%2C%20our%20prior%20work%20on%20generative%0ASTFT%20phase%20retrieval%20%28DiffPhase%29%2C%20and%20the%20pseudoinverse%20operator%20of%20the%20Mel%0Afilterbank%2C%20we%20develop%20MelFlow%2C%20a%20streaming-capable%20generative%20Mel%20vocoder%20for%0Aspeech%20sampled%20at%2016%20kHz%20with%20an%20algorithmic%20latency%20of%20only%2032%20ms%20and%20a%20total%0Alatency%20of%2048%20ms.%20We%20show%20real-time%20streaming%20capability%20at%20this%20latency%20not%0Aonly%20in%20theory%2C%20but%20in%20practice%20on%20a%20consumer%20laptop%20GPU.%20Furthermore%2C%20we%20show%0Athat%20our%20model%20achieves%20substantially%20better%20PESQ%20and%20SI-SDR%20values%20compared%20to%0Awell-established%20not%20streaming-capable%20baselines%20for%20Mel%20vocoding%20including%0AHiFi-GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15085v1&entry.124074799=Read"},
{"title": "Fovea Stacking: Imaging with Dynamic Localized Aberration Correction", "author": "Shi Mao and Yogeshwar Nath Mishra and Wolfgang Heidrich", "abstract": "  The desire for cameras with smaller form factors has recently lead to a push\nfor exploring computational imaging systems with reduced optical complexity\nsuch as a smaller number of lens elements. Unfortunately such simplified\noptical systems usually suffer from severe aberrations, especially in off-axis\nregions, which can be difficult to correct purely in software. In this paper we\nintroduce Fovea Stacking , a new type of imaging system that utilizes emerging\ndynamic optical components called deformable phase plates (DPPs) for localized\naberration correction anywhere on the image sensor. By optimizing DPP\ndeformations through a differentiable optical model, off-axis aberrations are\ncorrected locally, producing a foveated image with enhanced sharpness at the\nfixation point - analogous to the eye's fovea. Stacking multiple such foveated\nimages, each with a different fixation point, yields a composite image free\nfrom aberrations. To efficiently cover the entire field of view, we propose\njoint optimization of DPP deformations under imaging budget constraints. Due to\nthe DPP device's non-linear behavior, we introduce a neural network-based\ncontrol model for improved alignment between simulation-hardware performance.\nWe further demonstrated that for extended depth-of-field imaging, fovea\nstacking outperforms traditional focus stacking in image quality. By\nintegrating object detection or eye-tracking, the system can dynamically adjust\nthe lens to track the object of interest-enabling real-time foveated video\nsuitable for downstream applications such as surveillance or foveated virtual\nreality displays\n", "link": "http://arxiv.org/abs/2506.00716v2", "date": "2025-09-18", "relevancy": 2.0719, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5282}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5178}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fovea%20Stacking%3A%20Imaging%20with%20Dynamic%20Localized%20Aberration%20Correction&body=Title%3A%20Fovea%20Stacking%3A%20Imaging%20with%20Dynamic%20Localized%20Aberration%20Correction%0AAuthor%3A%20Shi%20Mao%20and%20Yogeshwar%20Nath%20Mishra%20and%20Wolfgang%20Heidrich%0AAbstract%3A%20%20%20The%20desire%20for%20cameras%20with%20smaller%20form%20factors%20has%20recently%20lead%20to%20a%20push%0Afor%20exploring%20computational%20imaging%20systems%20with%20reduced%20optical%20complexity%0Asuch%20as%20a%20smaller%20number%20of%20lens%20elements.%20Unfortunately%20such%20simplified%0Aoptical%20systems%20usually%20suffer%20from%20severe%20aberrations%2C%20especially%20in%20off-axis%0Aregions%2C%20which%20can%20be%20difficult%20to%20correct%20purely%20in%20software.%20In%20this%20paper%20we%0Aintroduce%20Fovea%20Stacking%20%2C%20a%20new%20type%20of%20imaging%20system%20that%20utilizes%20emerging%0Adynamic%20optical%20components%20called%20deformable%20phase%20plates%20%28DPPs%29%20for%20localized%0Aaberration%20correction%20anywhere%20on%20the%20image%20sensor.%20By%20optimizing%20DPP%0Adeformations%20through%20a%20differentiable%20optical%20model%2C%20off-axis%20aberrations%20are%0Acorrected%20locally%2C%20producing%20a%20foveated%20image%20with%20enhanced%20sharpness%20at%20the%0Afixation%20point%20-%20analogous%20to%20the%20eye%27s%20fovea.%20Stacking%20multiple%20such%20foveated%0Aimages%2C%20each%20with%20a%20different%20fixation%20point%2C%20yields%20a%20composite%20image%20free%0Afrom%20aberrations.%20To%20efficiently%20cover%20the%20entire%20field%20of%20view%2C%20we%20propose%0Ajoint%20optimization%20of%20DPP%20deformations%20under%20imaging%20budget%20constraints.%20Due%20to%0Athe%20DPP%20device%27s%20non-linear%20behavior%2C%20we%20introduce%20a%20neural%20network-based%0Acontrol%20model%20for%20improved%20alignment%20between%20simulation-hardware%20performance.%0AWe%20further%20demonstrated%20that%20for%20extended%20depth-of-field%20imaging%2C%20fovea%0Astacking%20outperforms%20traditional%20focus%20stacking%20in%20image%20quality.%20By%0Aintegrating%20object%20detection%20or%20eye-tracking%2C%20the%20system%20can%20dynamically%20adjust%0Athe%20lens%20to%20track%20the%20object%20of%20interest-enabling%20real-time%20foveated%20video%0Asuitable%20for%20downstream%20applications%20such%20as%20surveillance%20or%20foveated%20virtual%0Areality%20displays%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFovea%2520Stacking%253A%2520Imaging%2520with%2520Dynamic%2520Localized%2520Aberration%2520Correction%26entry.906535625%3DShi%2520Mao%2520and%2520Yogeshwar%2520Nath%2520Mishra%2520and%2520Wolfgang%2520Heidrich%26entry.1292438233%3D%2520%2520The%2520desire%2520for%2520cameras%2520with%2520smaller%2520form%2520factors%2520has%2520recently%2520lead%2520to%2520a%2520push%250Afor%2520exploring%2520computational%2520imaging%2520systems%2520with%2520reduced%2520optical%2520complexity%250Asuch%2520as%2520a%2520smaller%2520number%2520of%2520lens%2520elements.%2520Unfortunately%2520such%2520simplified%250Aoptical%2520systems%2520usually%2520suffer%2520from%2520severe%2520aberrations%252C%2520especially%2520in%2520off-axis%250Aregions%252C%2520which%2520can%2520be%2520difficult%2520to%2520correct%2520purely%2520in%2520software.%2520In%2520this%2520paper%2520we%250Aintroduce%2520Fovea%2520Stacking%2520%252C%2520a%2520new%2520type%2520of%2520imaging%2520system%2520that%2520utilizes%2520emerging%250Adynamic%2520optical%2520components%2520called%2520deformable%2520phase%2520plates%2520%2528DPPs%2529%2520for%2520localized%250Aaberration%2520correction%2520anywhere%2520on%2520the%2520image%2520sensor.%2520By%2520optimizing%2520DPP%250Adeformations%2520through%2520a%2520differentiable%2520optical%2520model%252C%2520off-axis%2520aberrations%2520are%250Acorrected%2520locally%252C%2520producing%2520a%2520foveated%2520image%2520with%2520enhanced%2520sharpness%2520at%2520the%250Afixation%2520point%2520-%2520analogous%2520to%2520the%2520eye%2527s%2520fovea.%2520Stacking%2520multiple%2520such%2520foveated%250Aimages%252C%2520each%2520with%2520a%2520different%2520fixation%2520point%252C%2520yields%2520a%2520composite%2520image%2520free%250Afrom%2520aberrations.%2520To%2520efficiently%2520cover%2520the%2520entire%2520field%2520of%2520view%252C%2520we%2520propose%250Ajoint%2520optimization%2520of%2520DPP%2520deformations%2520under%2520imaging%2520budget%2520constraints.%2520Due%2520to%250Athe%2520DPP%2520device%2527s%2520non-linear%2520behavior%252C%2520we%2520introduce%2520a%2520neural%2520network-based%250Acontrol%2520model%2520for%2520improved%2520alignment%2520between%2520simulation-hardware%2520performance.%250AWe%2520further%2520demonstrated%2520that%2520for%2520extended%2520depth-of-field%2520imaging%252C%2520fovea%250Astacking%2520outperforms%2520traditional%2520focus%2520stacking%2520in%2520image%2520quality.%2520By%250Aintegrating%2520object%2520detection%2520or%2520eye-tracking%252C%2520the%2520system%2520can%2520dynamically%2520adjust%250Athe%2520lens%2520to%2520track%2520the%2520object%2520of%2520interest-enabling%2520real-time%2520foveated%2520video%250Asuitable%2520for%2520downstream%2520applications%2520such%2520as%2520surveillance%2520or%2520foveated%2520virtual%250Areality%2520displays%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fovea%20Stacking%3A%20Imaging%20with%20Dynamic%20Localized%20Aberration%20Correction&entry.906535625=Shi%20Mao%20and%20Yogeshwar%20Nath%20Mishra%20and%20Wolfgang%20Heidrich&entry.1292438233=%20%20The%20desire%20for%20cameras%20with%20smaller%20form%20factors%20has%20recently%20lead%20to%20a%20push%0Afor%20exploring%20computational%20imaging%20systems%20with%20reduced%20optical%20complexity%0Asuch%20as%20a%20smaller%20number%20of%20lens%20elements.%20Unfortunately%20such%20simplified%0Aoptical%20systems%20usually%20suffer%20from%20severe%20aberrations%2C%20especially%20in%20off-axis%0Aregions%2C%20which%20can%20be%20difficult%20to%20correct%20purely%20in%20software.%20In%20this%20paper%20we%0Aintroduce%20Fovea%20Stacking%20%2C%20a%20new%20type%20of%20imaging%20system%20that%20utilizes%20emerging%0Adynamic%20optical%20components%20called%20deformable%20phase%20plates%20%28DPPs%29%20for%20localized%0Aaberration%20correction%20anywhere%20on%20the%20image%20sensor.%20By%20optimizing%20DPP%0Adeformations%20through%20a%20differentiable%20optical%20model%2C%20off-axis%20aberrations%20are%0Acorrected%20locally%2C%20producing%20a%20foveated%20image%20with%20enhanced%20sharpness%20at%20the%0Afixation%20point%20-%20analogous%20to%20the%20eye%27s%20fovea.%20Stacking%20multiple%20such%20foveated%0Aimages%2C%20each%20with%20a%20different%20fixation%20point%2C%20yields%20a%20composite%20image%20free%0Afrom%20aberrations.%20To%20efficiently%20cover%20the%20entire%20field%20of%20view%2C%20we%20propose%0Ajoint%20optimization%20of%20DPP%20deformations%20under%20imaging%20budget%20constraints.%20Due%20to%0Athe%20DPP%20device%27s%20non-linear%20behavior%2C%20we%20introduce%20a%20neural%20network-based%0Acontrol%20model%20for%20improved%20alignment%20between%20simulation-hardware%20performance.%0AWe%20further%20demonstrated%20that%20for%20extended%20depth-of-field%20imaging%2C%20fovea%0Astacking%20outperforms%20traditional%20focus%20stacking%20in%20image%20quality.%20By%0Aintegrating%20object%20detection%20or%20eye-tracking%2C%20the%20system%20can%20dynamically%20adjust%0Athe%20lens%20to%20track%20the%20object%20of%20interest-enabling%20real-time%20foveated%20video%0Asuitable%20for%20downstream%20applications%20such%20as%20surveillance%20or%20foveated%20virtual%0Areality%20displays%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00716v2&entry.124074799=Read"},
{"title": "Exploring How Audio Effects Alter Emotion with Foundation Models", "author": "Stelios Katsis and Vassilis Lyberatos and Spyridon Kantarelis and Edmund Dervakos and Giorgos Stamou", "abstract": "  Audio effects (FX) such as reverberation, distortion, modulation, and dynamic\nrange processing play a pivotal role in shaping emotional responses during\nmusic listening. While prior studies have examined links between low-level\naudio features and affective perception, the systematic impact of audio FX on\nemotion remains underexplored. This work investigates how foundation models -\nlarge-scale neural architectures pretrained on multimodal data - can be\nleveraged to analyze these effects. Such models encode rich associations\nbetween musical structure, timbre, and affective meaning, offering a powerful\nframework for probing the emotional consequences of sound design techniques. By\napplying various probing methods to embeddings from deep learning models, we\nexamine the complex, nonlinear relationships between audio FX and estimated\nemotion, uncovering patterns tied to specific effects and evaluating the\nrobustness of foundation audio models. Our findings aim to advance\nunderstanding of the perceptual impact of audio production practices, with\nimplications for music cognition, performance, and affective computing.\n", "link": "http://arxiv.org/abs/2509.15151v1", "date": "2025-09-18", "relevancy": 2.0683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20How%20Audio%20Effects%20Alter%20Emotion%20with%20Foundation%20Models&body=Title%3A%20Exploring%20How%20Audio%20Effects%20Alter%20Emotion%20with%20Foundation%20Models%0AAuthor%3A%20Stelios%20Katsis%20and%20Vassilis%20Lyberatos%20and%20Spyridon%20Kantarelis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Audio%20effects%20%28FX%29%20such%20as%20reverberation%2C%20distortion%2C%20modulation%2C%20and%20dynamic%0Arange%20processing%20play%20a%20pivotal%20role%20in%20shaping%20emotional%20responses%20during%0Amusic%20listening.%20While%20prior%20studies%20have%20examined%20links%20between%20low-level%0Aaudio%20features%20and%20affective%20perception%2C%20the%20systematic%20impact%20of%20audio%20FX%20on%0Aemotion%20remains%20underexplored.%20This%20work%20investigates%20how%20foundation%20models%20-%0Alarge-scale%20neural%20architectures%20pretrained%20on%20multimodal%20data%20-%20can%20be%0Aleveraged%20to%20analyze%20these%20effects.%20Such%20models%20encode%20rich%20associations%0Abetween%20musical%20structure%2C%20timbre%2C%20and%20affective%20meaning%2C%20offering%20a%20powerful%0Aframework%20for%20probing%20the%20emotional%20consequences%20of%20sound%20design%20techniques.%20By%0Aapplying%20various%20probing%20methods%20to%20embeddings%20from%20deep%20learning%20models%2C%20we%0Aexamine%20the%20complex%2C%20nonlinear%20relationships%20between%20audio%20FX%20and%20estimated%0Aemotion%2C%20uncovering%20patterns%20tied%20to%20specific%20effects%20and%20evaluating%20the%0Arobustness%20of%20foundation%20audio%20models.%20Our%20findings%20aim%20to%20advance%0Aunderstanding%20of%20the%20perceptual%20impact%20of%20audio%20production%20practices%2C%20with%0Aimplications%20for%20music%20cognition%2C%20performance%2C%20and%20affective%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520How%2520Audio%2520Effects%2520Alter%2520Emotion%2520with%2520Foundation%2520Models%26entry.906535625%3DStelios%2520Katsis%2520and%2520Vassilis%2520Lyberatos%2520and%2520Spyridon%2520Kantarelis%2520and%2520Edmund%2520Dervakos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Audio%2520effects%2520%2528FX%2529%2520such%2520as%2520reverberation%252C%2520distortion%252C%2520modulation%252C%2520and%2520dynamic%250Arange%2520processing%2520play%2520a%2520pivotal%2520role%2520in%2520shaping%2520emotional%2520responses%2520during%250Amusic%2520listening.%2520While%2520prior%2520studies%2520have%2520examined%2520links%2520between%2520low-level%250Aaudio%2520features%2520and%2520affective%2520perception%252C%2520the%2520systematic%2520impact%2520of%2520audio%2520FX%2520on%250Aemotion%2520remains%2520underexplored.%2520This%2520work%2520investigates%2520how%2520foundation%2520models%2520-%250Alarge-scale%2520neural%2520architectures%2520pretrained%2520on%2520multimodal%2520data%2520-%2520can%2520be%250Aleveraged%2520to%2520analyze%2520these%2520effects.%2520Such%2520models%2520encode%2520rich%2520associations%250Abetween%2520musical%2520structure%252C%2520timbre%252C%2520and%2520affective%2520meaning%252C%2520offering%2520a%2520powerful%250Aframework%2520for%2520probing%2520the%2520emotional%2520consequences%2520of%2520sound%2520design%2520techniques.%2520By%250Aapplying%2520various%2520probing%2520methods%2520to%2520embeddings%2520from%2520deep%2520learning%2520models%252C%2520we%250Aexamine%2520the%2520complex%252C%2520nonlinear%2520relationships%2520between%2520audio%2520FX%2520and%2520estimated%250Aemotion%252C%2520uncovering%2520patterns%2520tied%2520to%2520specific%2520effects%2520and%2520evaluating%2520the%250Arobustness%2520of%2520foundation%2520audio%2520models.%2520Our%2520findings%2520aim%2520to%2520advance%250Aunderstanding%2520of%2520the%2520perceptual%2520impact%2520of%2520audio%2520production%2520practices%252C%2520with%250Aimplications%2520for%2520music%2520cognition%252C%2520performance%252C%2520and%2520affective%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20How%20Audio%20Effects%20Alter%20Emotion%20with%20Foundation%20Models&entry.906535625=Stelios%20Katsis%20and%20Vassilis%20Lyberatos%20and%20Spyridon%20Kantarelis%20and%20Edmund%20Dervakos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Audio%20effects%20%28FX%29%20such%20as%20reverberation%2C%20distortion%2C%20modulation%2C%20and%20dynamic%0Arange%20processing%20play%20a%20pivotal%20role%20in%20shaping%20emotional%20responses%20during%0Amusic%20listening.%20While%20prior%20studies%20have%20examined%20links%20between%20low-level%0Aaudio%20features%20and%20affective%20perception%2C%20the%20systematic%20impact%20of%20audio%20FX%20on%0Aemotion%20remains%20underexplored.%20This%20work%20investigates%20how%20foundation%20models%20-%0Alarge-scale%20neural%20architectures%20pretrained%20on%20multimodal%20data%20-%20can%20be%0Aleveraged%20to%20analyze%20these%20effects.%20Such%20models%20encode%20rich%20associations%0Abetween%20musical%20structure%2C%20timbre%2C%20and%20affective%20meaning%2C%20offering%20a%20powerful%0Aframework%20for%20probing%20the%20emotional%20consequences%20of%20sound%20design%20techniques.%20By%0Aapplying%20various%20probing%20methods%20to%20embeddings%20from%20deep%20learning%20models%2C%20we%0Aexamine%20the%20complex%2C%20nonlinear%20relationships%20between%20audio%20FX%20and%20estimated%0Aemotion%2C%20uncovering%20patterns%20tied%20to%20specific%20effects%20and%20evaluating%20the%0Arobustness%20of%20foundation%20audio%20models.%20Our%20findings%20aim%20to%20advance%0Aunderstanding%20of%20the%20perceptual%20impact%20of%20audio%20production%20practices%2C%20with%0Aimplications%20for%20music%20cognition%2C%20performance%2C%20and%20affective%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15151v1&entry.124074799=Read"},
{"title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for\n  Text-to-Image Models", "author": "Jisung Hwang and Jaihoon Kim and Minhyuk Sung", "abstract": "  We propose a novel regularization loss that enforces standard Gaussianity,\nencouraging samples to align with a standard Gaussian distribution. This\nfacilitates a range of downstream tasks involving optimization in the latent\nspace of text-to-image models. We treat elements of a high-dimensional sample\nas one-dimensional standard Gaussian variables and define a composite loss that\ncombines moment-based regularization in the spatial domain with power\nspectrum-based regularization in the spectral domain. Since the expected values\nof moments and power spectrum distributions are analytically known, the loss\npromotes conformity to these properties. To ensure permutation invariance, the\nlosses are applied to randomly permuted inputs. Notably, existing\nGaussianity-based regularizations fall within our unified framework: some\ncorrespond to moment losses of specific orders, while the previous\ncovariance-matching loss is equivalent to our spectral loss but incurs higher\ntime complexity due to its spatial-domain computation. We showcase the\napplication of our regularization in generative modeling for test-time reward\nalignment with a text-to-image model, specifically to enhance aesthetics and\ntext alignment. Our regularization outperforms previous Gaussianity\nregularization, effectively prevents reward hacking and accelerates\nconvergence.\n", "link": "http://arxiv.org/abs/2509.07027v3", "date": "2025-09-18", "relevancy": 2.0672, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5409}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5138}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moment-%20and%20Power-Spectrum-Based%20Gaussianity%20Regularization%20for%0A%20%20Text-to-Image%20Models&body=Title%3A%20Moment-%20and%20Power-Spectrum-Based%20Gaussianity%20Regularization%20for%0A%20%20Text-to-Image%20Models%0AAuthor%3A%20Jisung%20Hwang%20and%20Jaihoon%20Kim%20and%20Minhyuk%20Sung%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20regularization%20loss%20that%20enforces%20standard%20Gaussianity%2C%0Aencouraging%20samples%20to%20align%20with%20a%20standard%20Gaussian%20distribution.%20This%0Afacilitates%20a%20range%20of%20downstream%20tasks%20involving%20optimization%20in%20the%20latent%0Aspace%20of%20text-to-image%20models.%20We%20treat%20elements%20of%20a%20high-dimensional%20sample%0Aas%20one-dimensional%20standard%20Gaussian%20variables%20and%20define%20a%20composite%20loss%20that%0Acombines%20moment-based%20regularization%20in%20the%20spatial%20domain%20with%20power%0Aspectrum-based%20regularization%20in%20the%20spectral%20domain.%20Since%20the%20expected%20values%0Aof%20moments%20and%20power%20spectrum%20distributions%20are%20analytically%20known%2C%20the%20loss%0Apromotes%20conformity%20to%20these%20properties.%20To%20ensure%20permutation%20invariance%2C%20the%0Alosses%20are%20applied%20to%20randomly%20permuted%20inputs.%20Notably%2C%20existing%0AGaussianity-based%20regularizations%20fall%20within%20our%20unified%20framework%3A%20some%0Acorrespond%20to%20moment%20losses%20of%20specific%20orders%2C%20while%20the%20previous%0Acovariance-matching%20loss%20is%20equivalent%20to%20our%20spectral%20loss%20but%20incurs%20higher%0Atime%20complexity%20due%20to%20its%20spatial-domain%20computation.%20We%20showcase%20the%0Aapplication%20of%20our%20regularization%20in%20generative%20modeling%20for%20test-time%20reward%0Aalignment%20with%20a%20text-to-image%20model%2C%20specifically%20to%20enhance%20aesthetics%20and%0Atext%20alignment.%20Our%20regularization%20outperforms%20previous%20Gaussianity%0Aregularization%2C%20effectively%20prevents%20reward%20hacking%20and%20accelerates%0Aconvergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.07027v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoment-%2520and%2520Power-Spectrum-Based%2520Gaussianity%2520Regularization%2520for%250A%2520%2520Text-to-Image%2520Models%26entry.906535625%3DJisung%2520Hwang%2520and%2520Jaihoon%2520Kim%2520and%2520Minhyuk%2520Sung%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520regularization%2520loss%2520that%2520enforces%2520standard%2520Gaussianity%252C%250Aencouraging%2520samples%2520to%2520align%2520with%2520a%2520standard%2520Gaussian%2520distribution.%2520This%250Afacilitates%2520a%2520range%2520of%2520downstream%2520tasks%2520involving%2520optimization%2520in%2520the%2520latent%250Aspace%2520of%2520text-to-image%2520models.%2520We%2520treat%2520elements%2520of%2520a%2520high-dimensional%2520sample%250Aas%2520one-dimensional%2520standard%2520Gaussian%2520variables%2520and%2520define%2520a%2520composite%2520loss%2520that%250Acombines%2520moment-based%2520regularization%2520in%2520the%2520spatial%2520domain%2520with%2520power%250Aspectrum-based%2520regularization%2520in%2520the%2520spectral%2520domain.%2520Since%2520the%2520expected%2520values%250Aof%2520moments%2520and%2520power%2520spectrum%2520distributions%2520are%2520analytically%2520known%252C%2520the%2520loss%250Apromotes%2520conformity%2520to%2520these%2520properties.%2520To%2520ensure%2520permutation%2520invariance%252C%2520the%250Alosses%2520are%2520applied%2520to%2520randomly%2520permuted%2520inputs.%2520Notably%252C%2520existing%250AGaussianity-based%2520regularizations%2520fall%2520within%2520our%2520unified%2520framework%253A%2520some%250Acorrespond%2520to%2520moment%2520losses%2520of%2520specific%2520orders%252C%2520while%2520the%2520previous%250Acovariance-matching%2520loss%2520is%2520equivalent%2520to%2520our%2520spectral%2520loss%2520but%2520incurs%2520higher%250Atime%2520complexity%2520due%2520to%2520its%2520spatial-domain%2520computation.%2520We%2520showcase%2520the%250Aapplication%2520of%2520our%2520regularization%2520in%2520generative%2520modeling%2520for%2520test-time%2520reward%250Aalignment%2520with%2520a%2520text-to-image%2520model%252C%2520specifically%2520to%2520enhance%2520aesthetics%2520and%250Atext%2520alignment.%2520Our%2520regularization%2520outperforms%2520previous%2520Gaussianity%250Aregularization%252C%2520effectively%2520prevents%2520reward%2520hacking%2520and%2520accelerates%250Aconvergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07027v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moment-%20and%20Power-Spectrum-Based%20Gaussianity%20Regularization%20for%0A%20%20Text-to-Image%20Models&entry.906535625=Jisung%20Hwang%20and%20Jaihoon%20Kim%20and%20Minhyuk%20Sung&entry.1292438233=%20%20We%20propose%20a%20novel%20regularization%20loss%20that%20enforces%20standard%20Gaussianity%2C%0Aencouraging%20samples%20to%20align%20with%20a%20standard%20Gaussian%20distribution.%20This%0Afacilitates%20a%20range%20of%20downstream%20tasks%20involving%20optimization%20in%20the%20latent%0Aspace%20of%20text-to-image%20models.%20We%20treat%20elements%20of%20a%20high-dimensional%20sample%0Aas%20one-dimensional%20standard%20Gaussian%20variables%20and%20define%20a%20composite%20loss%20that%0Acombines%20moment-based%20regularization%20in%20the%20spatial%20domain%20with%20power%0Aspectrum-based%20regularization%20in%20the%20spectral%20domain.%20Since%20the%20expected%20values%0Aof%20moments%20and%20power%20spectrum%20distributions%20are%20analytically%20known%2C%20the%20loss%0Apromotes%20conformity%20to%20these%20properties.%20To%20ensure%20permutation%20invariance%2C%20the%0Alosses%20are%20applied%20to%20randomly%20permuted%20inputs.%20Notably%2C%20existing%0AGaussianity-based%20regularizations%20fall%20within%20our%20unified%20framework%3A%20some%0Acorrespond%20to%20moment%20losses%20of%20specific%20orders%2C%20while%20the%20previous%0Acovariance-matching%20loss%20is%20equivalent%20to%20our%20spectral%20loss%20but%20incurs%20higher%0Atime%20complexity%20due%20to%20its%20spatial-domain%20computation.%20We%20showcase%20the%0Aapplication%20of%20our%20regularization%20in%20generative%20modeling%20for%20test-time%20reward%0Aalignment%20with%20a%20text-to-image%20model%2C%20specifically%20to%20enhance%20aesthetics%20and%0Atext%20alignment.%20Our%20regularization%20outperforms%20previous%20Gaussianity%0Aregularization%2C%20effectively%20prevents%20reward%20hacking%20and%20accelerates%0Aconvergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.07027v3&entry.124074799=Read"},
{"title": "Efficient motion-based metrics for video frame interpolation", "author": "Conall Daly and Darren Ramsook and Anil Kokaram", "abstract": "  Video frame interpolation (VFI) offers a way to generate intermediate frames\nbetween consecutive frames of a video sequence. Although the development of\nadvanced frame interpolation algorithms has received increased attention in\nrecent years, assessing the perceptual quality of interpolated content remains\nan ongoing area of research. In this paper, we investigate simple ways to\nprocess motion fields, with the purposes of using them as video quality metric\nfor evaluating frame interpolation algorithms. We evaluate these quality\nmetrics using the BVI-VFI dataset which contains perceptual scores measured for\ninterpolated sequences. From our investigation we propose a motion metric based\non measuring the divergence of motion fields. This metric correlates reasonably\nwith these perceptual scores (PLCC=0.51) and is more computationally efficient\n(x2.7 speedup) compared to FloLPIPS (a well known motion-based metric). We then\nuse our new proposed metrics to evaluate a range of state of the art frame\ninterpolation metrics and find our metrics tend to favour more perceptual\npleasing interpolated frames that may not score highly in terms of PSNR or\nSSIM.\n", "link": "http://arxiv.org/abs/2508.09078v2", "date": "2025-09-18", "relevancy": 2.0659, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5641}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5037}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation&body=Title%3A%20Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation%0AAuthor%3A%20Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram%0AAbstract%3A%20%20%20Video%20frame%20interpolation%20%28VFI%29%20offers%20a%20way%20to%20generate%20intermediate%20frames%0Abetween%20consecutive%20frames%20of%20a%20video%20sequence.%20Although%20the%20development%20of%0Aadvanced%20frame%20interpolation%20algorithms%20has%20received%20increased%20attention%20in%0Arecent%20years%2C%20assessing%20the%20perceptual%20quality%20of%20interpolated%20content%20remains%0Aan%20ongoing%20area%20of%20research.%20In%20this%20paper%2C%20we%20investigate%20simple%20ways%20to%0Aprocess%20motion%20fields%2C%20with%20the%20purposes%20of%20using%20them%20as%20video%20quality%20metric%0Afor%20evaluating%20frame%20interpolation%20algorithms.%20We%20evaluate%20these%20quality%0Ametrics%20using%20the%20BVI-VFI%20dataset%20which%20contains%20perceptual%20scores%20measured%20for%0Ainterpolated%20sequences.%20From%20our%20investigation%20we%20propose%20a%20motion%20metric%20based%0Aon%20measuring%20the%20divergence%20of%20motion%20fields.%20This%20metric%20correlates%20reasonably%0Awith%20these%20perceptual%20scores%20%28PLCC%3D0.51%29%20and%20is%20more%20computationally%20efficient%0A%28x2.7%20speedup%29%20compared%20to%20FloLPIPS%20%28a%20well%20known%20motion-based%20metric%29.%20We%20then%0Ause%20our%20new%20proposed%20metrics%20to%20evaluate%20a%20range%20of%20state%20of%20the%20art%20frame%0Ainterpolation%20metrics%20and%20find%20our%20metrics%20tend%20to%20favour%20more%20perceptual%0Apleasing%20interpolated%20frames%20that%20may%20not%20score%20highly%20in%20terms%20of%20PSNR%20or%0ASSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.09078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520motion-based%2520metrics%2520for%2520video%2520frame%2520interpolation%26entry.906535625%3DConall%2520Daly%2520and%2520Darren%2520Ramsook%2520and%2520Anil%2520Kokaram%26entry.1292438233%3D%2520%2520Video%2520frame%2520interpolation%2520%2528VFI%2529%2520offers%2520a%2520way%2520to%2520generate%2520intermediate%2520frames%250Abetween%2520consecutive%2520frames%2520of%2520a%2520video%2520sequence.%2520Although%2520the%2520development%2520of%250Aadvanced%2520frame%2520interpolation%2520algorithms%2520has%2520received%2520increased%2520attention%2520in%250Arecent%2520years%252C%2520assessing%2520the%2520perceptual%2520quality%2520of%2520interpolated%2520content%2520remains%250Aan%2520ongoing%2520area%2520of%2520research.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520simple%2520ways%2520to%250Aprocess%2520motion%2520fields%252C%2520with%2520the%2520purposes%2520of%2520using%2520them%2520as%2520video%2520quality%2520metric%250Afor%2520evaluating%2520frame%2520interpolation%2520algorithms.%2520We%2520evaluate%2520these%2520quality%250Ametrics%2520using%2520the%2520BVI-VFI%2520dataset%2520which%2520contains%2520perceptual%2520scores%2520measured%2520for%250Ainterpolated%2520sequences.%2520From%2520our%2520investigation%2520we%2520propose%2520a%2520motion%2520metric%2520based%250Aon%2520measuring%2520the%2520divergence%2520of%2520motion%2520fields.%2520This%2520metric%2520correlates%2520reasonably%250Awith%2520these%2520perceptual%2520scores%2520%2528PLCC%253D0.51%2529%2520and%2520is%2520more%2520computationally%2520efficient%250A%2528x2.7%2520speedup%2529%2520compared%2520to%2520FloLPIPS%2520%2528a%2520well%2520known%2520motion-based%2520metric%2529.%2520We%2520then%250Ause%2520our%2520new%2520proposed%2520metrics%2520to%2520evaluate%2520a%2520range%2520of%2520state%2520of%2520the%2520art%2520frame%250Ainterpolation%2520metrics%2520and%2520find%2520our%2520metrics%2520tend%2520to%2520favour%2520more%2520perceptual%250Apleasing%2520interpolated%2520frames%2520that%2520may%2520not%2520score%2520highly%2520in%2520terms%2520of%2520PSNR%2520or%250ASSIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20motion-based%20metrics%20for%20video%20frame%20interpolation&entry.906535625=Conall%20Daly%20and%20Darren%20Ramsook%20and%20Anil%20Kokaram&entry.1292438233=%20%20Video%20frame%20interpolation%20%28VFI%29%20offers%20a%20way%20to%20generate%20intermediate%20frames%0Abetween%20consecutive%20frames%20of%20a%20video%20sequence.%20Although%20the%20development%20of%0Aadvanced%20frame%20interpolation%20algorithms%20has%20received%20increased%20attention%20in%0Arecent%20years%2C%20assessing%20the%20perceptual%20quality%20of%20interpolated%20content%20remains%0Aan%20ongoing%20area%20of%20research.%20In%20this%20paper%2C%20we%20investigate%20simple%20ways%20to%0Aprocess%20motion%20fields%2C%20with%20the%20purposes%20of%20using%20them%20as%20video%20quality%20metric%0Afor%20evaluating%20frame%20interpolation%20algorithms.%20We%20evaluate%20these%20quality%0Ametrics%20using%20the%20BVI-VFI%20dataset%20which%20contains%20perceptual%20scores%20measured%20for%0Ainterpolated%20sequences.%20From%20our%20investigation%20we%20propose%20a%20motion%20metric%20based%0Aon%20measuring%20the%20divergence%20of%20motion%20fields.%20This%20metric%20correlates%20reasonably%0Awith%20these%20perceptual%20scores%20%28PLCC%3D0.51%29%20and%20is%20more%20computationally%20efficient%0A%28x2.7%20speedup%29%20compared%20to%20FloLPIPS%20%28a%20well%20known%20motion-based%20metric%29.%20We%20then%0Ause%20our%20new%20proposed%20metrics%20to%20evaluate%20a%20range%20of%20state%20of%20the%20art%20frame%0Ainterpolation%20metrics%20and%20find%20our%20metrics%20tend%20to%20favour%20more%20perceptual%0Apleasing%20interpolated%20frames%20that%20may%20not%20score%20highly%20in%20terms%20of%20PSNR%20or%0ASSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.09078v2&entry.124074799=Read"},
{"title": "Manipulation Facing Threats: Evaluating Physical Vulnerabilities in\n  End-to-End Vision Language Action Models", "author": "Hao Cheng and Erjia Xiao and Yichi Wang and Chengyuan Yu and Mengshu Sun and Qiang Zhang and Yijie Guo and Kaidi Xu and Jize Zhang and Chao Shen and Philip Torr and Jindong Gu and Renjing Xu", "abstract": "  Recently, driven by advancements in Multimodal Large Language Models (MLLMs),\nVision Language Action Models (VLAMs) are being proposed to achieve better\nperformance in open-vocabulary scenarios for robotic manipulation tasks. Since\nmanipulation tasks involve direct interaction with the physical world, ensuring\nrobustness and safety during the execution of this task is always a very\ncritical issue. In this paper, by synthesizing current safety research on MLLMs\nand the specific application scenarios of the manipulation task in the physical\nworld, we comprehensively evaluate VLAMs in the face of potential physical\nthreats. Specifically, we propose the Physical Vulnerability Evaluating\nPipeline (PVEP) that can incorporate as many visual modal physical threats as\npossible for evaluating the physical robustness of VLAMs. The physical threats\nin PVEP specifically include Out-of-Distribution, Typography-based Visual\nPrompt, and Adversarial Patch Attacks. By comparing the performance\nfluctuations of VLAMs before and after being attacked, we provide generalizable\n\\textbf{\\textit{Analyses}} of how VLAMs respond to different physical threats.\n", "link": "http://arxiv.org/abs/2409.13174v3", "date": "2025-09-18", "relevancy": 1.6045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5825}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manipulation%20Facing%20Threats%3A%20Evaluating%20Physical%20Vulnerabilities%20in%0A%20%20End-to-End%20Vision%20Language%20Action%20Models&body=Title%3A%20Manipulation%20Facing%20Threats%3A%20Evaluating%20Physical%20Vulnerabilities%20in%0A%20%20End-to-End%20Vision%20Language%20Action%20Models%0AAuthor%3A%20Hao%20Cheng%20and%20Erjia%20Xiao%20and%20Yichi%20Wang%20and%20Chengyuan%20Yu%20and%20Mengshu%20Sun%20and%20Qiang%20Zhang%20and%20Yijie%20Guo%20and%20Kaidi%20Xu%20and%20Jize%20Zhang%20and%20Chao%20Shen%20and%20Philip%20Torr%20and%20Jindong%20Gu%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Recently%2C%20driven%20by%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0AVision%20Language%20Action%20Models%20%28VLAMs%29%20are%20being%20proposed%20to%20achieve%20better%0Aperformance%20in%20open-vocabulary%20scenarios%20for%20robotic%20manipulation%20tasks.%20Since%0Amanipulation%20tasks%20involve%20direct%20interaction%20with%20the%20physical%20world%2C%20ensuring%0Arobustness%20and%20safety%20during%20the%20execution%20of%20this%20task%20is%20always%20a%20very%0Acritical%20issue.%20In%20this%20paper%2C%20by%20synthesizing%20current%20safety%20research%20on%20MLLMs%0Aand%20the%20specific%20application%20scenarios%20of%20the%20manipulation%20task%20in%20the%20physical%0Aworld%2C%20we%20comprehensively%20evaluate%20VLAMs%20in%20the%20face%20of%20potential%20physical%0Athreats.%20Specifically%2C%20we%20propose%20the%20Physical%20Vulnerability%20Evaluating%0APipeline%20%28PVEP%29%20that%20can%20incorporate%20as%20many%20visual%20modal%20physical%20threats%20as%0Apossible%20for%20evaluating%20the%20physical%20robustness%20of%20VLAMs.%20The%20physical%20threats%0Ain%20PVEP%20specifically%20include%20Out-of-Distribution%2C%20Typography-based%20Visual%0APrompt%2C%20and%20Adversarial%20Patch%20Attacks.%20By%20comparing%20the%20performance%0Afluctuations%20of%20VLAMs%20before%20and%20after%20being%20attacked%2C%20we%20provide%20generalizable%0A%5Ctextbf%7B%5Ctextit%7BAnalyses%7D%7D%20of%20how%20VLAMs%20respond%20to%20different%20physical%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13174v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManipulation%2520Facing%2520Threats%253A%2520Evaluating%2520Physical%2520Vulnerabilities%2520in%250A%2520%2520End-to-End%2520Vision%2520Language%2520Action%2520Models%26entry.906535625%3DHao%2520Cheng%2520and%2520Erjia%2520Xiao%2520and%2520Yichi%2520Wang%2520and%2520Chengyuan%2520Yu%2520and%2520Mengshu%2520Sun%2520and%2520Qiang%2520Zhang%2520and%2520Yijie%2520Guo%2520and%2520Kaidi%2520Xu%2520and%2520Jize%2520Zhang%2520and%2520Chao%2520Shen%2520and%2520Philip%2520Torr%2520and%2520Jindong%2520Gu%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Recently%252C%2520driven%2520by%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%250AVision%2520Language%2520Action%2520Models%2520%2528VLAMs%2529%2520are%2520being%2520proposed%2520to%2520achieve%2520better%250Aperformance%2520in%2520open-vocabulary%2520scenarios%2520for%2520robotic%2520manipulation%2520tasks.%2520Since%250Amanipulation%2520tasks%2520involve%2520direct%2520interaction%2520with%2520the%2520physical%2520world%252C%2520ensuring%250Arobustness%2520and%2520safety%2520during%2520the%2520execution%2520of%2520this%2520task%2520is%2520always%2520a%2520very%250Acritical%2520issue.%2520In%2520this%2520paper%252C%2520by%2520synthesizing%2520current%2520safety%2520research%2520on%2520MLLMs%250Aand%2520the%2520specific%2520application%2520scenarios%2520of%2520the%2520manipulation%2520task%2520in%2520the%2520physical%250Aworld%252C%2520we%2520comprehensively%2520evaluate%2520VLAMs%2520in%2520the%2520face%2520of%2520potential%2520physical%250Athreats.%2520Specifically%252C%2520we%2520propose%2520the%2520Physical%2520Vulnerability%2520Evaluating%250APipeline%2520%2528PVEP%2529%2520that%2520can%2520incorporate%2520as%2520many%2520visual%2520modal%2520physical%2520threats%2520as%250Apossible%2520for%2520evaluating%2520the%2520physical%2520robustness%2520of%2520VLAMs.%2520The%2520physical%2520threats%250Ain%2520PVEP%2520specifically%2520include%2520Out-of-Distribution%252C%2520Typography-based%2520Visual%250APrompt%252C%2520and%2520Adversarial%2520Patch%2520Attacks.%2520By%2520comparing%2520the%2520performance%250Afluctuations%2520of%2520VLAMs%2520before%2520and%2520after%2520being%2520attacked%252C%2520we%2520provide%2520generalizable%250A%255Ctextbf%257B%255Ctextit%257BAnalyses%257D%257D%2520of%2520how%2520VLAMs%2520respond%2520to%2520different%2520physical%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13174v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manipulation%20Facing%20Threats%3A%20Evaluating%20Physical%20Vulnerabilities%20in%0A%20%20End-to-End%20Vision%20Language%20Action%20Models&entry.906535625=Hao%20Cheng%20and%20Erjia%20Xiao%20and%20Yichi%20Wang%20and%20Chengyuan%20Yu%20and%20Mengshu%20Sun%20and%20Qiang%20Zhang%20and%20Yijie%20Guo%20and%20Kaidi%20Xu%20and%20Jize%20Zhang%20and%20Chao%20Shen%20and%20Philip%20Torr%20and%20Jindong%20Gu%20and%20Renjing%20Xu&entry.1292438233=%20%20Recently%2C%20driven%20by%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0AVision%20Language%20Action%20Models%20%28VLAMs%29%20are%20being%20proposed%20to%20achieve%20better%0Aperformance%20in%20open-vocabulary%20scenarios%20for%20robotic%20manipulation%20tasks.%20Since%0Amanipulation%20tasks%20involve%20direct%20interaction%20with%20the%20physical%20world%2C%20ensuring%0Arobustness%20and%20safety%20during%20the%20execution%20of%20this%20task%20is%20always%20a%20very%0Acritical%20issue.%20In%20this%20paper%2C%20by%20synthesizing%20current%20safety%20research%20on%20MLLMs%0Aand%20the%20specific%20application%20scenarios%20of%20the%20manipulation%20task%20in%20the%20physical%0Aworld%2C%20we%20comprehensively%20evaluate%20VLAMs%20in%20the%20face%20of%20potential%20physical%0Athreats.%20Specifically%2C%20we%20propose%20the%20Physical%20Vulnerability%20Evaluating%0APipeline%20%28PVEP%29%20that%20can%20incorporate%20as%20many%20visual%20modal%20physical%20threats%20as%0Apossible%20for%20evaluating%20the%20physical%20robustness%20of%20VLAMs.%20The%20physical%20threats%0Ain%20PVEP%20specifically%20include%20Out-of-Distribution%2C%20Typography-based%20Visual%0APrompt%2C%20and%20Adversarial%20Patch%20Attacks.%20By%20comparing%20the%20performance%0Afluctuations%20of%20VLAMs%20before%20and%20after%20being%20attacked%2C%20we%20provide%20generalizable%0A%5Ctextbf%7B%5Ctextit%7BAnalyses%7D%7D%20of%20how%20VLAMs%20respond%20to%20different%20physical%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13174v3&entry.124074799=Read"},
{"title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool\n  Use", "author": "Yating Lin and Zixuan Huang and Fan Yang and Dmitry Berenson", "abstract": "  Multivariate time-series anomaly detection, which is critical for identifying\nunexpected events, has been explored in the field of machine learning for\nseveral decades. However, directly applying these methods to data from forceful\ntool use tasks is challenging because streaming sensor data in the real world\ntends to be inherently noisy, exhibits non-stationary behavior, and varies\nacross different tasks and tools. To address these challenges, we propose a\nmethod, AnoF-Diff, based on the diffusion model to extract force-torque\nfeatures from time-series data and use force-torque features to detect\nanomalies. We compare our method with other state-of-the-art methods in terms\nof F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)\non four forceful tool-use tasks, demonstrating that our method has better\nperformance and is more robust to a noisy dataset. We also propose the method\nof parallel anomaly score evaluation based on one-step diffusion and\ndemonstrate how our method can be used for online anomaly detection in several\nforceful tool use experiments.\n", "link": "http://arxiv.org/abs/2509.15153v1", "date": "2025-09-18", "relevancy": 0.9521, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4696}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnoF-Diff%3A%20One-Step%20Diffusion-Based%20Anomaly%20Detection%20for%20Forceful%20Tool%0A%20%20Use&body=Title%3A%20AnoF-Diff%3A%20One-Step%20Diffusion-Based%20Anomaly%20Detection%20for%20Forceful%20Tool%0A%20%20Use%0AAuthor%3A%20Yating%20Lin%20and%20Zixuan%20Huang%20and%20Fan%20Yang%20and%20Dmitry%20Berenson%0AAbstract%3A%20%20%20Multivariate%20time-series%20anomaly%20detection%2C%20which%20is%20critical%20for%20identifying%0Aunexpected%20events%2C%20has%20been%20explored%20in%20the%20field%20of%20machine%20learning%20for%0Aseveral%20decades.%20However%2C%20directly%20applying%20these%20methods%20to%20data%20from%20forceful%0Atool%20use%20tasks%20is%20challenging%20because%20streaming%20sensor%20data%20in%20the%20real%20world%0Atends%20to%20be%20inherently%20noisy%2C%20exhibits%20non-stationary%20behavior%2C%20and%20varies%0Aacross%20different%20tasks%20and%20tools.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Amethod%2C%20AnoF-Diff%2C%20based%20on%20the%20diffusion%20model%20to%20extract%20force-torque%0Afeatures%20from%20time-series%20data%20and%20use%20force-torque%20features%20to%20detect%0Aanomalies.%20We%20compare%20our%20method%20with%20other%20state-of-the-art%20methods%20in%20terms%0Aof%20F1-score%20and%20Area%20Under%20the%20Receiver%20Operating%20Characteristic%20curve%20%28AUROC%29%0Aon%20four%20forceful%20tool-use%20tasks%2C%20demonstrating%20that%20our%20method%20has%20better%0Aperformance%20and%20is%20more%20robust%20to%20a%20noisy%20dataset.%20We%20also%20propose%20the%20method%0Aof%20parallel%20anomaly%20score%20evaluation%20based%20on%20one-step%20diffusion%20and%0Ademonstrate%20how%20our%20method%20can%20be%20used%20for%20online%20anomaly%20detection%20in%20several%0Aforceful%20tool%20use%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnoF-Diff%253A%2520One-Step%2520Diffusion-Based%2520Anomaly%2520Detection%2520for%2520Forceful%2520Tool%250A%2520%2520Use%26entry.906535625%3DYating%2520Lin%2520and%2520Zixuan%2520Huang%2520and%2520Fan%2520Yang%2520and%2520Dmitry%2520Berenson%26entry.1292438233%3D%2520%2520Multivariate%2520time-series%2520anomaly%2520detection%252C%2520which%2520is%2520critical%2520for%2520identifying%250Aunexpected%2520events%252C%2520has%2520been%2520explored%2520in%2520the%2520field%2520of%2520machine%2520learning%2520for%250Aseveral%2520decades.%2520However%252C%2520directly%2520applying%2520these%2520methods%2520to%2520data%2520from%2520forceful%250Atool%2520use%2520tasks%2520is%2520challenging%2520because%2520streaming%2520sensor%2520data%2520in%2520the%2520real%2520world%250Atends%2520to%2520be%2520inherently%2520noisy%252C%2520exhibits%2520non-stationary%2520behavior%252C%2520and%2520varies%250Aacross%2520different%2520tasks%2520and%2520tools.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250Amethod%252C%2520AnoF-Diff%252C%2520based%2520on%2520the%2520diffusion%2520model%2520to%2520extract%2520force-torque%250Afeatures%2520from%2520time-series%2520data%2520and%2520use%2520force-torque%2520features%2520to%2520detect%250Aanomalies.%2520We%2520compare%2520our%2520method%2520with%2520other%2520state-of-the-art%2520methods%2520in%2520terms%250Aof%2520F1-score%2520and%2520Area%2520Under%2520the%2520Receiver%2520Operating%2520Characteristic%2520curve%2520%2528AUROC%2529%250Aon%2520four%2520forceful%2520tool-use%2520tasks%252C%2520demonstrating%2520that%2520our%2520method%2520has%2520better%250Aperformance%2520and%2520is%2520more%2520robust%2520to%2520a%2520noisy%2520dataset.%2520We%2520also%2520propose%2520the%2520method%250Aof%2520parallel%2520anomaly%2520score%2520evaluation%2520based%2520on%2520one-step%2520diffusion%2520and%250Ademonstrate%2520how%2520our%2520method%2520can%2520be%2520used%2520for%2520online%2520anomaly%2520detection%2520in%2520several%250Aforceful%2520tool%2520use%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnoF-Diff%3A%20One-Step%20Diffusion-Based%20Anomaly%20Detection%20for%20Forceful%20Tool%0A%20%20Use&entry.906535625=Yating%20Lin%20and%20Zixuan%20Huang%20and%20Fan%20Yang%20and%20Dmitry%20Berenson&entry.1292438233=%20%20Multivariate%20time-series%20anomaly%20detection%2C%20which%20is%20critical%20for%20identifying%0Aunexpected%20events%2C%20has%20been%20explored%20in%20the%20field%20of%20machine%20learning%20for%0Aseveral%20decades.%20However%2C%20directly%20applying%20these%20methods%20to%20data%20from%20forceful%0Atool%20use%20tasks%20is%20challenging%20because%20streaming%20sensor%20data%20in%20the%20real%20world%0Atends%20to%20be%20inherently%20noisy%2C%20exhibits%20non-stationary%20behavior%2C%20and%20varies%0Aacross%20different%20tasks%20and%20tools.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Amethod%2C%20AnoF-Diff%2C%20based%20on%20the%20diffusion%20model%20to%20extract%20force-torque%0Afeatures%20from%20time-series%20data%20and%20use%20force-torque%20features%20to%20detect%0Aanomalies.%20We%20compare%20our%20method%20with%20other%20state-of-the-art%20methods%20in%20terms%0Aof%20F1-score%20and%20Area%20Under%20the%20Receiver%20Operating%20Characteristic%20curve%20%28AUROC%29%0Aon%20four%20forceful%20tool-use%20tasks%2C%20demonstrating%20that%20our%20method%20has%20better%0Aperformance%20and%20is%20more%20robust%20to%20a%20noisy%20dataset.%20We%20also%20propose%20the%20method%0Aof%20parallel%20anomaly%20score%20evaluation%20based%20on%20one-step%20diffusion%20and%0Ademonstrate%20how%20our%20method%20can%20be%20used%20for%20online%20anomaly%20detection%20in%20several%0Aforceful%20tool%20use%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15153v1&entry.124074799=Read"},
{"title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large\n  Language Models", "author": "Chenzhuo Zhao and Ziqian Liu and Xinda Wang and Junting Lu and Chaoyi Ruan", "abstract": "  Prompt optimization is a practical and widely applicable alternative to fine\ntuning for improving large language model performance. Yet many existing\nmethods evaluate candidate prompts by sampling full outputs, often coupled with\nself critique or human annotated preferences, which limits scalability,\nespecially for smaller models or models that are not instruction tuned. We\npresent PMPO (Probabilistic Metric Prompt Optimization), a unified framework\nthat uses token level cross entropy as a direct, lightweight evaluation signal.\nPMPO locates low quality prompt segments via a masking based analysis and\niteratively rewrites them to propose improved variants. Crucially, during\nevaluation, PMPO selects among variants by minimizing loss in a single forward\npass, eliminating output sampling and human or judge based scoring for\nselection while still using standard generation only to propose rewrites. This\nunified, loss based strategy supports both supervised and preference based\ntasks. Across model sizes and datasets, PMPO outperforms prior prompt\noptimizers: it achieves the highest average accuracy on BBH, performs strongly\non GSM8K and AQUA RAT, and raises AlpacaEval 2.0 win rates by over 19 points.\nThese results demonstrate PMPO's effectiveness, efficiency, and broad\napplicability.\n", "link": "http://arxiv.org/abs/2505.16307v2", "date": "2025-09-18", "relevancy": 1.3523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4478}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMPO%3A%20Probabilistic%20Metric%20Prompt%20Optimization%20for%20Small%20and%20Large%0A%20%20Language%20Models&body=Title%3A%20PMPO%3A%20Probabilistic%20Metric%20Prompt%20Optimization%20for%20Small%20and%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Chenzhuo%20Zhao%20and%20Ziqian%20Liu%20and%20Xinda%20Wang%20and%20Junting%20Lu%20and%20Chaoyi%20Ruan%0AAbstract%3A%20%20%20Prompt%20optimization%20is%20a%20practical%20and%20widely%20applicable%20alternative%20to%20fine%0Atuning%20for%20improving%20large%20language%20model%20performance.%20Yet%20many%20existing%0Amethods%20evaluate%20candidate%20prompts%20by%20sampling%20full%20outputs%2C%20often%20coupled%20with%0Aself%20critique%20or%20human%20annotated%20preferences%2C%20which%20limits%20scalability%2C%0Aespecially%20for%20smaller%20models%20or%20models%20that%20are%20not%20instruction%20tuned.%20We%0Apresent%20PMPO%20%28Probabilistic%20Metric%20Prompt%20Optimization%29%2C%20a%20unified%20framework%0Athat%20uses%20token%20level%20cross%20entropy%20as%20a%20direct%2C%20lightweight%20evaluation%20signal.%0APMPO%20locates%20low%20quality%20prompt%20segments%20via%20a%20masking%20based%20analysis%20and%0Aiteratively%20rewrites%20them%20to%20propose%20improved%20variants.%20Crucially%2C%20during%0Aevaluation%2C%20PMPO%20selects%20among%20variants%20by%20minimizing%20loss%20in%20a%20single%20forward%0Apass%2C%20eliminating%20output%20sampling%20and%20human%20or%20judge%20based%20scoring%20for%0Aselection%20while%20still%20using%20standard%20generation%20only%20to%20propose%20rewrites.%20This%0Aunified%2C%20loss%20based%20strategy%20supports%20both%20supervised%20and%20preference%20based%0Atasks.%20Across%20model%20sizes%20and%20datasets%2C%20PMPO%20outperforms%20prior%20prompt%0Aoptimizers%3A%20it%20achieves%20the%20highest%20average%20accuracy%20on%20BBH%2C%20performs%20strongly%0Aon%20GSM8K%20and%20AQUA%20RAT%2C%20and%20raises%20AlpacaEval%202.0%20win%20rates%20by%20over%2019%20points.%0AThese%20results%20demonstrate%20PMPO%27s%20effectiveness%2C%20efficiency%2C%20and%20broad%0Aapplicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMPO%253A%2520Probabilistic%2520Metric%2520Prompt%2520Optimization%2520for%2520Small%2520and%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DChenzhuo%2520Zhao%2520and%2520Ziqian%2520Liu%2520and%2520Xinda%2520Wang%2520and%2520Junting%2520Lu%2520and%2520Chaoyi%2520Ruan%26entry.1292438233%3D%2520%2520Prompt%2520optimization%2520is%2520a%2520practical%2520and%2520widely%2520applicable%2520alternative%2520to%2520fine%250Atuning%2520for%2520improving%2520large%2520language%2520model%2520performance.%2520Yet%2520many%2520existing%250Amethods%2520evaluate%2520candidate%2520prompts%2520by%2520sampling%2520full%2520outputs%252C%2520often%2520coupled%2520with%250Aself%2520critique%2520or%2520human%2520annotated%2520preferences%252C%2520which%2520limits%2520scalability%252C%250Aespecially%2520for%2520smaller%2520models%2520or%2520models%2520that%2520are%2520not%2520instruction%2520tuned.%2520We%250Apresent%2520PMPO%2520%2528Probabilistic%2520Metric%2520Prompt%2520Optimization%2529%252C%2520a%2520unified%2520framework%250Athat%2520uses%2520token%2520level%2520cross%2520entropy%2520as%2520a%2520direct%252C%2520lightweight%2520evaluation%2520signal.%250APMPO%2520locates%2520low%2520quality%2520prompt%2520segments%2520via%2520a%2520masking%2520based%2520analysis%2520and%250Aiteratively%2520rewrites%2520them%2520to%2520propose%2520improved%2520variants.%2520Crucially%252C%2520during%250Aevaluation%252C%2520PMPO%2520selects%2520among%2520variants%2520by%2520minimizing%2520loss%2520in%2520a%2520single%2520forward%250Apass%252C%2520eliminating%2520output%2520sampling%2520and%2520human%2520or%2520judge%2520based%2520scoring%2520for%250Aselection%2520while%2520still%2520using%2520standard%2520generation%2520only%2520to%2520propose%2520rewrites.%2520This%250Aunified%252C%2520loss%2520based%2520strategy%2520supports%2520both%2520supervised%2520and%2520preference%2520based%250Atasks.%2520Across%2520model%2520sizes%2520and%2520datasets%252C%2520PMPO%2520outperforms%2520prior%2520prompt%250Aoptimizers%253A%2520it%2520achieves%2520the%2520highest%2520average%2520accuracy%2520on%2520BBH%252C%2520performs%2520strongly%250Aon%2520GSM8K%2520and%2520AQUA%2520RAT%252C%2520and%2520raises%2520AlpacaEval%25202.0%2520win%2520rates%2520by%2520over%252019%2520points.%250AThese%2520results%2520demonstrate%2520PMPO%2527s%2520effectiveness%252C%2520efficiency%252C%2520and%2520broad%250Aapplicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMPO%3A%20Probabilistic%20Metric%20Prompt%20Optimization%20for%20Small%20and%20Large%0A%20%20Language%20Models&entry.906535625=Chenzhuo%20Zhao%20and%20Ziqian%20Liu%20and%20Xinda%20Wang%20and%20Junting%20Lu%20and%20Chaoyi%20Ruan&entry.1292438233=%20%20Prompt%20optimization%20is%20a%20practical%20and%20widely%20applicable%20alternative%20to%20fine%0Atuning%20for%20improving%20large%20language%20model%20performance.%20Yet%20many%20existing%0Amethods%20evaluate%20candidate%20prompts%20by%20sampling%20full%20outputs%2C%20often%20coupled%20with%0Aself%20critique%20or%20human%20annotated%20preferences%2C%20which%20limits%20scalability%2C%0Aespecially%20for%20smaller%20models%20or%20models%20that%20are%20not%20instruction%20tuned.%20We%0Apresent%20PMPO%20%28Probabilistic%20Metric%20Prompt%20Optimization%29%2C%20a%20unified%20framework%0Athat%20uses%20token%20level%20cross%20entropy%20as%20a%20direct%2C%20lightweight%20evaluation%20signal.%0APMPO%20locates%20low%20quality%20prompt%20segments%20via%20a%20masking%20based%20analysis%20and%0Aiteratively%20rewrites%20them%20to%20propose%20improved%20variants.%20Crucially%2C%20during%0Aevaluation%2C%20PMPO%20selects%20among%20variants%20by%20minimizing%20loss%20in%20a%20single%20forward%0Apass%2C%20eliminating%20output%20sampling%20and%20human%20or%20judge%20based%20scoring%20for%0Aselection%20while%20still%20using%20standard%20generation%20only%20to%20propose%20rewrites.%20This%0Aunified%2C%20loss%20based%20strategy%20supports%20both%20supervised%20and%20preference%20based%0Atasks.%20Across%20model%20sizes%20and%20datasets%2C%20PMPO%20outperforms%20prior%20prompt%0Aoptimizers%3A%20it%20achieves%20the%20highest%20average%20accuracy%20on%20BBH%2C%20performs%20strongly%0Aon%20GSM8K%20and%20AQUA%20RAT%2C%20and%20raises%20AlpacaEval%202.0%20win%20rates%20by%20over%2019%20points.%0AThese%20results%20demonstrate%20PMPO%27s%20effectiveness%2C%20efficiency%2C%20and%20broad%0Aapplicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16307v2&entry.124074799=Read"},
{"title": "Doppler Radiance Field-Guided Antenna Selection for Improved\n  Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition", "author": "Navid Hasanzadeh and Shahrokh Valaee", "abstract": "  With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard\nfor advanced sensing, interest in using Wi-Fi Channel State Information (CSI)\nfor remote sensing has surged. Recent findings indicate that learning a unified\nthree-dimensional motion representation through Doppler Radiance Fields (DoRFs)\nderived from CSI significantly improves the generalization capabilities of\nWi-Fi-based human activity recognition (HAR). Despite this progress, CSI\nsignals remain affected by asynchronous access point (AP) clocks and additive\nnoise from environmental and hardware sources. Consequently, even with existing\npreprocessing techniques, both the CSI data and Doppler velocity projections\nused in DoRFs are still susceptible to noise and outliers, limiting HAR\nperformance. To address this challenge, we propose a novel framework for\nmulti-antenna APs to suppress noise and identify the most informative antennas\nbased on DoRF fitting errors, which capture inconsistencies among Doppler\nvelocity projections. Experimental results on a challenging small-scale hand\ngesture recognition dataset demonstrate that the proposed DoRF-guided\nWi-Fi-based HAR approach significantly improves generalization capability,\npaving the way for robust real-world sensing deployments.\n", "link": "http://arxiv.org/abs/2509.15129v1", "date": "2025-09-18", "relevancy": 1.9391, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4892}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doppler%20Radiance%20Field-Guided%20Antenna%20Selection%20for%20Improved%0A%20%20Generalization%20in%20Multi-Antenna%20Wi-Fi-based%20Human%20Activity%20Recognition&body=Title%3A%20Doppler%20Radiance%20Field-Guided%20Antenna%20Selection%20for%20Improved%0A%20%20Generalization%20in%20Multi-Antenna%20Wi-Fi-based%20Human%20Activity%20Recognition%0AAuthor%3A%20Navid%20Hasanzadeh%20and%20Shahrokh%20Valaee%0AAbstract%3A%20%20%20With%20the%20IEEE%20802.11bf%20Task%20Group%20introducing%20amendments%20to%20the%20WLAN%20standard%0Afor%20advanced%20sensing%2C%20interest%20in%20using%20Wi-Fi%20Channel%20State%20Information%20%28CSI%29%0Afor%20remote%20sensing%20has%20surged.%20Recent%20findings%20indicate%20that%20learning%20a%20unified%0Athree-dimensional%20motion%20representation%20through%20Doppler%20Radiance%20Fields%20%28DoRFs%29%0Aderived%20from%20CSI%20significantly%20improves%20the%20generalization%20capabilities%20of%0AWi-Fi-based%20human%20activity%20recognition%20%28HAR%29.%20Despite%20this%20progress%2C%20CSI%0Asignals%20remain%20affected%20by%20asynchronous%20access%20point%20%28AP%29%20clocks%20and%20additive%0Anoise%20from%20environmental%20and%20hardware%20sources.%20Consequently%2C%20even%20with%20existing%0Apreprocessing%20techniques%2C%20both%20the%20CSI%20data%20and%20Doppler%20velocity%20projections%0Aused%20in%20DoRFs%20are%20still%20susceptible%20to%20noise%20and%20outliers%2C%20limiting%20HAR%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20for%0Amulti-antenna%20APs%20to%20suppress%20noise%20and%20identify%20the%20most%20informative%20antennas%0Abased%20on%20DoRF%20fitting%20errors%2C%20which%20capture%20inconsistencies%20among%20Doppler%0Avelocity%20projections.%20Experimental%20results%20on%20a%20challenging%20small-scale%20hand%0Agesture%20recognition%20dataset%20demonstrate%20that%20the%20proposed%20DoRF-guided%0AWi-Fi-based%20HAR%20approach%20significantly%20improves%20generalization%20capability%2C%0Apaving%20the%20way%20for%20robust%20real-world%20sensing%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoppler%2520Radiance%2520Field-Guided%2520Antenna%2520Selection%2520for%2520Improved%250A%2520%2520Generalization%2520in%2520Multi-Antenna%2520Wi-Fi-based%2520Human%2520Activity%2520Recognition%26entry.906535625%3DNavid%2520Hasanzadeh%2520and%2520Shahrokh%2520Valaee%26entry.1292438233%3D%2520%2520With%2520the%2520IEEE%2520802.11bf%2520Task%2520Group%2520introducing%2520amendments%2520to%2520the%2520WLAN%2520standard%250Afor%2520advanced%2520sensing%252C%2520interest%2520in%2520using%2520Wi-Fi%2520Channel%2520State%2520Information%2520%2528CSI%2529%250Afor%2520remote%2520sensing%2520has%2520surged.%2520Recent%2520findings%2520indicate%2520that%2520learning%2520a%2520unified%250Athree-dimensional%2520motion%2520representation%2520through%2520Doppler%2520Radiance%2520Fields%2520%2528DoRFs%2529%250Aderived%2520from%2520CSI%2520significantly%2520improves%2520the%2520generalization%2520capabilities%2520of%250AWi-Fi-based%2520human%2520activity%2520recognition%2520%2528HAR%2529.%2520Despite%2520this%2520progress%252C%2520CSI%250Asignals%2520remain%2520affected%2520by%2520asynchronous%2520access%2520point%2520%2528AP%2529%2520clocks%2520and%2520additive%250Anoise%2520from%2520environmental%2520and%2520hardware%2520sources.%2520Consequently%252C%2520even%2520with%2520existing%250Apreprocessing%2520techniques%252C%2520both%2520the%2520CSI%2520data%2520and%2520Doppler%2520velocity%2520projections%250Aused%2520in%2520DoRFs%2520are%2520still%2520susceptible%2520to%2520noise%2520and%2520outliers%252C%2520limiting%2520HAR%250Aperformance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%250Amulti-antenna%2520APs%2520to%2520suppress%2520noise%2520and%2520identify%2520the%2520most%2520informative%2520antennas%250Abased%2520on%2520DoRF%2520fitting%2520errors%252C%2520which%2520capture%2520inconsistencies%2520among%2520Doppler%250Avelocity%2520projections.%2520Experimental%2520results%2520on%2520a%2520challenging%2520small-scale%2520hand%250Agesture%2520recognition%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520DoRF-guided%250AWi-Fi-based%2520HAR%2520approach%2520significantly%2520improves%2520generalization%2520capability%252C%250Apaving%2520the%2520way%2520for%2520robust%2520real-world%2520sensing%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doppler%20Radiance%20Field-Guided%20Antenna%20Selection%20for%20Improved%0A%20%20Generalization%20in%20Multi-Antenna%20Wi-Fi-based%20Human%20Activity%20Recognition&entry.906535625=Navid%20Hasanzadeh%20and%20Shahrokh%20Valaee&entry.1292438233=%20%20With%20the%20IEEE%20802.11bf%20Task%20Group%20introducing%20amendments%20to%20the%20WLAN%20standard%0Afor%20advanced%20sensing%2C%20interest%20in%20using%20Wi-Fi%20Channel%20State%20Information%20%28CSI%29%0Afor%20remote%20sensing%20has%20surged.%20Recent%20findings%20indicate%20that%20learning%20a%20unified%0Athree-dimensional%20motion%20representation%20through%20Doppler%20Radiance%20Fields%20%28DoRFs%29%0Aderived%20from%20CSI%20significantly%20improves%20the%20generalization%20capabilities%20of%0AWi-Fi-based%20human%20activity%20recognition%20%28HAR%29.%20Despite%20this%20progress%2C%20CSI%0Asignals%20remain%20affected%20by%20asynchronous%20access%20point%20%28AP%29%20clocks%20and%20additive%0Anoise%20from%20environmental%20and%20hardware%20sources.%20Consequently%2C%20even%20with%20existing%0Apreprocessing%20techniques%2C%20both%20the%20CSI%20data%20and%20Doppler%20velocity%20projections%0Aused%20in%20DoRFs%20are%20still%20susceptible%20to%20noise%20and%20outliers%2C%20limiting%20HAR%0Aperformance.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20for%0Amulti-antenna%20APs%20to%20suppress%20noise%20and%20identify%20the%20most%20informative%20antennas%0Abased%20on%20DoRF%20fitting%20errors%2C%20which%20capture%20inconsistencies%20among%20Doppler%0Avelocity%20projections.%20Experimental%20results%20on%20a%20challenging%20small-scale%20hand%0Agesture%20recognition%20dataset%20demonstrate%20that%20the%20proposed%20DoRF-guided%0AWi-Fi-based%20HAR%20approach%20significantly%20improves%20generalization%20capability%2C%0Apaving%20the%20way%20for%20robust%20real-world%20sensing%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15129v1&entry.124074799=Read"},
{"title": "Set Contribution Functions for Quantitative Bipolar Argumentation and\n  their Principles", "author": "Filip Naudot and Andreas Br\u00e4nnstr\u00f6m and Vicen\u00e7 Torra and Timotheus Kampik", "abstract": "  We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario.\n", "link": "http://arxiv.org/abs/2509.14963v1", "date": "2025-09-18", "relevancy": 1.4532, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Set%20Contribution%20Functions%20for%20Quantitative%20Bipolar%20Argumentation%20and%0A%20%20their%20Principles&body=Title%3A%20Set%20Contribution%20Functions%20for%20Quantitative%20Bipolar%20Argumentation%20and%0A%20%20their%20Principles%0AAuthor%3A%20Filip%20Naudot%20and%20Andreas%20Br%C3%A4nnstr%C3%B6m%20and%20Vicen%C3%A7%20Torra%20and%20Timotheus%20Kampik%0AAbstract%3A%20%20%20We%20present%20functions%20that%20quantify%20the%20contribution%20of%20a%20set%20of%20arguments%20in%0Aquantitative%20bipolar%20argumentation%20graphs%20to%20%28the%20final%20strength%20of%29%20an%0Aargument%20of%20interest%2C%20a%20so-called%20topic.%20Our%20set%20contribution%20functions%20are%0Ageneralizations%20of%20existing%20functions%20that%20quantify%20the%20contribution%20of%20a%0Asingle%20contributing%20argument%20to%20a%20topic.%20Accordingly%2C%20we%20generalize%20existing%0Acontribution%20function%20principles%20for%20set%20contribution%20functions%20and%20provide%20a%0Acorresponding%20principle-based%20analysis.%20We%20introduce%20new%20principles%20specific%20to%0Aset-based%20functions%20that%20focus%20on%20properties%20pertaining%20to%20the%20interaction%20of%0Aarguments%20within%20a%20set.%20Finally%2C%20we%20sketch%20how%20the%20principles%20play%20out%20across%0Adifferent%20set%20contribution%20functions%20given%20a%20recommendation%20system%20application%0Ascenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSet%2520Contribution%2520Functions%2520for%2520Quantitative%2520Bipolar%2520Argumentation%2520and%250A%2520%2520their%2520Principles%26entry.906535625%3DFilip%2520Naudot%2520and%2520Andreas%2520Br%25C3%25A4nnstr%25C3%25B6m%2520and%2520Vicen%25C3%25A7%2520Torra%2520and%2520Timotheus%2520Kampik%26entry.1292438233%3D%2520%2520We%2520present%2520functions%2520that%2520quantify%2520the%2520contribution%2520of%2520a%2520set%2520of%2520arguments%2520in%250Aquantitative%2520bipolar%2520argumentation%2520graphs%2520to%2520%2528the%2520final%2520strength%2520of%2529%2520an%250Aargument%2520of%2520interest%252C%2520a%2520so-called%2520topic.%2520Our%2520set%2520contribution%2520functions%2520are%250Ageneralizations%2520of%2520existing%2520functions%2520that%2520quantify%2520the%2520contribution%2520of%2520a%250Asingle%2520contributing%2520argument%2520to%2520a%2520topic.%2520Accordingly%252C%2520we%2520generalize%2520existing%250Acontribution%2520function%2520principles%2520for%2520set%2520contribution%2520functions%2520and%2520provide%2520a%250Acorresponding%2520principle-based%2520analysis.%2520We%2520introduce%2520new%2520principles%2520specific%2520to%250Aset-based%2520functions%2520that%2520focus%2520on%2520properties%2520pertaining%2520to%2520the%2520interaction%2520of%250Aarguments%2520within%2520a%2520set.%2520Finally%252C%2520we%2520sketch%2520how%2520the%2520principles%2520play%2520out%2520across%250Adifferent%2520set%2520contribution%2520functions%2520given%2520a%2520recommendation%2520system%2520application%250Ascenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Set%20Contribution%20Functions%20for%20Quantitative%20Bipolar%20Argumentation%20and%0A%20%20their%20Principles&entry.906535625=Filip%20Naudot%20and%20Andreas%20Br%C3%A4nnstr%C3%B6m%20and%20Vicen%C3%A7%20Torra%20and%20Timotheus%20Kampik&entry.1292438233=%20%20We%20present%20functions%20that%20quantify%20the%20contribution%20of%20a%20set%20of%20arguments%20in%0Aquantitative%20bipolar%20argumentation%20graphs%20to%20%28the%20final%20strength%20of%29%20an%0Aargument%20of%20interest%2C%20a%20so-called%20topic.%20Our%20set%20contribution%20functions%20are%0Ageneralizations%20of%20existing%20functions%20that%20quantify%20the%20contribution%20of%20a%0Asingle%20contributing%20argument%20to%20a%20topic.%20Accordingly%2C%20we%20generalize%20existing%0Acontribution%20function%20principles%20for%20set%20contribution%20functions%20and%20provide%20a%0Acorresponding%20principle-based%20analysis.%20We%20introduce%20new%20principles%20specific%20to%0Aset-based%20functions%20that%20focus%20on%20properties%20pertaining%20to%20the%20interaction%20of%0Aarguments%20within%20a%20set.%20Finally%2C%20we%20sketch%20how%20the%20principles%20play%20out%20across%0Adifferent%20set%20contribution%20functions%20given%20a%20recommendation%20system%20application%0Ascenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14963v1&entry.124074799=Read"},
{"title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by\n  Large Language Models", "author": "Thomas Huber and Christina Niklaus", "abstract": "  While LLMs have been extensively studied on general text generation tasks,\nthere is less research on text rewriting, a task related to general text\ngeneration, and particularly on the behavior of models on this task. In this\npaper we analyze what changes LLMs make in a text rewriting setting. We focus\nspecifically on argumentative texts and their improvement, a task named\nArgument Improvement (ArgImp). We present CLEAR: an evaluation pipeline\nconsisting of 57 metrics mapped to four linguistic levels: lexical, syntactic,\nsemantic and pragmatic. This pipeline is used to examine the qualities of\nLLM-rewritten arguments on a broad set of argumentation corpora and compare the\nbehavior of different LLMs on this task and analyze the behavior of different\nLLMs on this task in terms of linguistic levels. By taking all four linguistic\nlevels into consideration, we find that the models perform ArgImp by shortening\nthe texts while simultaneously increasing average word length and merging\nsentences. Overall we note an increase in the persuasion and coherence\ndimensions.\n", "link": "http://arxiv.org/abs/2509.15027v1", "date": "2025-09-18", "relevancy": 1.8326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4626}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEAR%3A%20A%20Comprehensive%20Linguistic%20Evaluation%20of%20Argument%20Rewriting%20by%0A%20%20Large%20Language%20Models&body=Title%3A%20CLEAR%3A%20A%20Comprehensive%20Linguistic%20Evaluation%20of%20Argument%20Rewriting%20by%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Thomas%20Huber%20and%20Christina%20Niklaus%0AAbstract%3A%20%20%20While%20LLMs%20have%20been%20extensively%20studied%20on%20general%20text%20generation%20tasks%2C%0Athere%20is%20less%20research%20on%20text%20rewriting%2C%20a%20task%20related%20to%20general%20text%0Ageneration%2C%20and%20particularly%20on%20the%20behavior%20of%20models%20on%20this%20task.%20In%20this%0Apaper%20we%20analyze%20what%20changes%20LLMs%20make%20in%20a%20text%20rewriting%20setting.%20We%20focus%0Aspecifically%20on%20argumentative%20texts%20and%20their%20improvement%2C%20a%20task%20named%0AArgument%20Improvement%20%28ArgImp%29.%20We%20present%20CLEAR%3A%20an%20evaluation%20pipeline%0Aconsisting%20of%2057%20metrics%20mapped%20to%20four%20linguistic%20levels%3A%20lexical%2C%20syntactic%2C%0Asemantic%20and%20pragmatic.%20This%20pipeline%20is%20used%20to%20examine%20the%20qualities%20of%0ALLM-rewritten%20arguments%20on%20a%20broad%20set%20of%20argumentation%20corpora%20and%20compare%20the%0Abehavior%20of%20different%20LLMs%20on%20this%20task%20and%20analyze%20the%20behavior%20of%20different%0ALLMs%20on%20this%20task%20in%20terms%20of%20linguistic%20levels.%20By%20taking%20all%20four%20linguistic%0Alevels%20into%20consideration%2C%20we%20find%20that%20the%20models%20perform%20ArgImp%20by%20shortening%0Athe%20texts%20while%20simultaneously%20increasing%20average%20word%20length%20and%20merging%0Asentences.%20Overall%20we%20note%20an%20increase%20in%20the%20persuasion%20and%20coherence%0Adimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEAR%253A%2520A%2520Comprehensive%2520Linguistic%2520Evaluation%2520of%2520Argument%2520Rewriting%2520by%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DThomas%2520Huber%2520and%2520Christina%2520Niklaus%26entry.1292438233%3D%2520%2520While%2520LLMs%2520have%2520been%2520extensively%2520studied%2520on%2520general%2520text%2520generation%2520tasks%252C%250Athere%2520is%2520less%2520research%2520on%2520text%2520rewriting%252C%2520a%2520task%2520related%2520to%2520general%2520text%250Ageneration%252C%2520and%2520particularly%2520on%2520the%2520behavior%2520of%2520models%2520on%2520this%2520task.%2520In%2520this%250Apaper%2520we%2520analyze%2520what%2520changes%2520LLMs%2520make%2520in%2520a%2520text%2520rewriting%2520setting.%2520We%2520focus%250Aspecifically%2520on%2520argumentative%2520texts%2520and%2520their%2520improvement%252C%2520a%2520task%2520named%250AArgument%2520Improvement%2520%2528ArgImp%2529.%2520We%2520present%2520CLEAR%253A%2520an%2520evaluation%2520pipeline%250Aconsisting%2520of%252057%2520metrics%2520mapped%2520to%2520four%2520linguistic%2520levels%253A%2520lexical%252C%2520syntactic%252C%250Asemantic%2520and%2520pragmatic.%2520This%2520pipeline%2520is%2520used%2520to%2520examine%2520the%2520qualities%2520of%250ALLM-rewritten%2520arguments%2520on%2520a%2520broad%2520set%2520of%2520argumentation%2520corpora%2520and%2520compare%2520the%250Abehavior%2520of%2520different%2520LLMs%2520on%2520this%2520task%2520and%2520analyze%2520the%2520behavior%2520of%2520different%250ALLMs%2520on%2520this%2520task%2520in%2520terms%2520of%2520linguistic%2520levels.%2520By%2520taking%2520all%2520four%2520linguistic%250Alevels%2520into%2520consideration%252C%2520we%2520find%2520that%2520the%2520models%2520perform%2520ArgImp%2520by%2520shortening%250Athe%2520texts%2520while%2520simultaneously%2520increasing%2520average%2520word%2520length%2520and%2520merging%250Asentences.%2520Overall%2520we%2520note%2520an%2520increase%2520in%2520the%2520persuasion%2520and%2520coherence%250Adimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEAR%3A%20A%20Comprehensive%20Linguistic%20Evaluation%20of%20Argument%20Rewriting%20by%0A%20%20Large%20Language%20Models&entry.906535625=Thomas%20Huber%20and%20Christina%20Niklaus&entry.1292438233=%20%20While%20LLMs%20have%20been%20extensively%20studied%20on%20general%20text%20generation%20tasks%2C%0Athere%20is%20less%20research%20on%20text%20rewriting%2C%20a%20task%20related%20to%20general%20text%0Ageneration%2C%20and%20particularly%20on%20the%20behavior%20of%20models%20on%20this%20task.%20In%20this%0Apaper%20we%20analyze%20what%20changes%20LLMs%20make%20in%20a%20text%20rewriting%20setting.%20We%20focus%0Aspecifically%20on%20argumentative%20texts%20and%20their%20improvement%2C%20a%20task%20named%0AArgument%20Improvement%20%28ArgImp%29.%20We%20present%20CLEAR%3A%20an%20evaluation%20pipeline%0Aconsisting%20of%2057%20metrics%20mapped%20to%20four%20linguistic%20levels%3A%20lexical%2C%20syntactic%2C%0Asemantic%20and%20pragmatic.%20This%20pipeline%20is%20used%20to%20examine%20the%20qualities%20of%0ALLM-rewritten%20arguments%20on%20a%20broad%20set%20of%20argumentation%20corpora%20and%20compare%20the%0Abehavior%20of%20different%20LLMs%20on%20this%20task%20and%20analyze%20the%20behavior%20of%20different%0ALLMs%20on%20this%20task%20in%20terms%20of%20linguistic%20levels.%20By%20taking%20all%20four%20linguistic%0Alevels%20into%20consideration%2C%20we%20find%20that%20the%20models%20perform%20ArgImp%20by%20shortening%0Athe%20texts%20while%20simultaneously%20increasing%20average%20word%20length%20and%20merging%0Asentences.%20Overall%20we%20note%20an%20increase%20in%20the%20persuasion%20and%20coherence%0Adimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15027v1&entry.124074799=Read"},
{"title": "Who to Trust? Aggregating Client Knowledge in Logit-Based Federated\n  Learning", "author": "Viktor Kovalchuk and Nikita Kotelevskii and Maxim Panov and Samuel Horv\u00e1th and Martin Tak\u00e1\u010d", "abstract": "  Federated learning (FL) usually shares model weights or gradients, which is\ncostly for large models. Logit-based FL reduces this cost by sharing only\nlogits computed on a public proxy dataset. However, aggregating information\nfrom heterogeneous clients is still challenging. This paper studies this\nproblem, introduces and compares three logit aggregation methods: simple\naveraging, uncertainty-weighted averaging, and a learned meta-aggregator.\nEvaluated on MNIST and CIFAR-10, these methods reduce communication overhead,\nimprove robustness under non-IID data, and achieve accuracy competitive with\ncentralized training.\n", "link": "http://arxiv.org/abs/2509.15147v1", "date": "2025-09-18", "relevancy": 1.955, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4883}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20to%20Trust%3F%20Aggregating%20Client%20Knowledge%20in%20Logit-Based%20Federated%0A%20%20Learning&body=Title%3A%20Who%20to%20Trust%3F%20Aggregating%20Client%20Knowledge%20in%20Logit-Based%20Federated%0A%20%20Learning%0AAuthor%3A%20Viktor%20Kovalchuk%20and%20Nikita%20Kotelevskii%20and%20Maxim%20Panov%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20usually%20shares%20model%20weights%20or%20gradients%2C%20which%20is%0Acostly%20for%20large%20models.%20Logit-based%20FL%20reduces%20this%20cost%20by%20sharing%20only%0Alogits%20computed%20on%20a%20public%20proxy%20dataset.%20However%2C%20aggregating%20information%0Afrom%20heterogeneous%20clients%20is%20still%20challenging.%20This%20paper%20studies%20this%0Aproblem%2C%20introduces%20and%20compares%20three%20logit%20aggregation%20methods%3A%20simple%0Aaveraging%2C%20uncertainty-weighted%20averaging%2C%20and%20a%20learned%20meta-aggregator.%0AEvaluated%20on%20MNIST%20and%20CIFAR-10%2C%20these%20methods%20reduce%20communication%20overhead%2C%0Aimprove%20robustness%20under%20non-IID%20data%2C%20and%20achieve%20accuracy%20competitive%20with%0Acentralized%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520to%2520Trust%253F%2520Aggregating%2520Client%2520Knowledge%2520in%2520Logit-Based%2520Federated%250A%2520%2520Learning%26entry.906535625%3DViktor%2520Kovalchuk%2520and%2520Nikita%2520Kotelevskii%2520and%2520Maxim%2520Panov%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520usually%2520shares%2520model%2520weights%2520or%2520gradients%252C%2520which%2520is%250Acostly%2520for%2520large%2520models.%2520Logit-based%2520FL%2520reduces%2520this%2520cost%2520by%2520sharing%2520only%250Alogits%2520computed%2520on%2520a%2520public%2520proxy%2520dataset.%2520However%252C%2520aggregating%2520information%250Afrom%2520heterogeneous%2520clients%2520is%2520still%2520challenging.%2520This%2520paper%2520studies%2520this%250Aproblem%252C%2520introduces%2520and%2520compares%2520three%2520logit%2520aggregation%2520methods%253A%2520simple%250Aaveraging%252C%2520uncertainty-weighted%2520averaging%252C%2520and%2520a%2520learned%2520meta-aggregator.%250AEvaluated%2520on%2520MNIST%2520and%2520CIFAR-10%252C%2520these%2520methods%2520reduce%2520communication%2520overhead%252C%250Aimprove%2520robustness%2520under%2520non-IID%2520data%252C%2520and%2520achieve%2520accuracy%2520competitive%2520with%250Acentralized%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20to%20Trust%3F%20Aggregating%20Client%20Knowledge%20in%20Logit-Based%20Federated%0A%20%20Learning&entry.906535625=Viktor%20Kovalchuk%20and%20Nikita%20Kotelevskii%20and%20Maxim%20Panov%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20usually%20shares%20model%20weights%20or%20gradients%2C%20which%20is%0Acostly%20for%20large%20models.%20Logit-based%20FL%20reduces%20this%20cost%20by%20sharing%20only%0Alogits%20computed%20on%20a%20public%20proxy%20dataset.%20However%2C%20aggregating%20information%0Afrom%20heterogeneous%20clients%20is%20still%20challenging.%20This%20paper%20studies%20this%0Aproblem%2C%20introduces%20and%20compares%20three%20logit%20aggregation%20methods%3A%20simple%0Aaveraging%2C%20uncertainty-weighted%20averaging%2C%20and%20a%20learned%20meta-aggregator.%0AEvaluated%20on%20MNIST%20and%20CIFAR-10%2C%20these%20methods%20reduce%20communication%20overhead%2C%0Aimprove%20robustness%20under%20non-IID%20data%2C%20and%20achieve%20accuracy%20competitive%20with%0Acentralized%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15147v1&entry.124074799=Read"},
{"title": "The mechanization of science illustrated by the Lean formalization of\n  the multi-graded Proj construction", "author": "Arnaud Mayeux and Jujian Zhang", "abstract": "  We formalize the multi-graded Proj construction in Lean4, illustrating\nmechanized mathematics and formalization.\n", "link": "http://arxiv.org/abs/2509.15116v1", "date": "2025-09-18", "relevancy": 1.2372, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.439}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4117}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20mechanization%20of%20science%20illustrated%20by%20the%20Lean%20formalization%20of%0A%20%20the%20multi-graded%20Proj%20construction&body=Title%3A%20The%20mechanization%20of%20science%20illustrated%20by%20the%20Lean%20formalization%20of%0A%20%20the%20multi-graded%20Proj%20construction%0AAuthor%3A%20Arnaud%20Mayeux%20and%20Jujian%20Zhang%0AAbstract%3A%20%20%20We%20formalize%20the%20multi-graded%20Proj%20construction%20in%20Lean4%2C%20illustrating%0Amechanized%20mathematics%20and%20formalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520mechanization%2520of%2520science%2520illustrated%2520by%2520the%2520Lean%2520formalization%2520of%250A%2520%2520the%2520multi-graded%2520Proj%2520construction%26entry.906535625%3DArnaud%2520Mayeux%2520and%2520Jujian%2520Zhang%26entry.1292438233%3D%2520%2520We%2520formalize%2520the%2520multi-graded%2520Proj%2520construction%2520in%2520Lean4%252C%2520illustrating%250Amechanized%2520mathematics%2520and%2520formalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20mechanization%20of%20science%20illustrated%20by%20the%20Lean%20formalization%20of%0A%20%20the%20multi-graded%20Proj%20construction&entry.906535625=Arnaud%20Mayeux%20and%20Jujian%20Zhang&entry.1292438233=%20%20We%20formalize%20the%20multi-graded%20Proj%20construction%20in%20Lean4%2C%20illustrating%0Amechanized%20mathematics%20and%20formalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15116v1&entry.124074799=Read"},
{"title": "Debias your Large Multi-Modal Model at Test-Time via Non-Contrastive\n  Visual Attribute Steering", "author": "Neale Ratzlaff and Matthew Lyle Olson and Musashi Hinck and Estelle Aflalo and Shao-Yen Tseng and Vasudev Lal and Phillip Howard", "abstract": "  Large Multi-Modal Models (LMMs) have demonstrated impressive capabilities as\ngeneral-purpose chatbots able to engage in conversations about visual inputs.\nHowever, their responses are influenced by societal biases present in their\ntraining datasets, leading to undesirable differences in how the model responds\nwhen presented with images depicting people of different demographics. In this\nwork, we propose a training-free debiasing framework for LMMs that intervenes\non the model's representations during text generation by constructing a\nsteering vector that reduces reference on protected attributes. Our framework\nintroduces two complementary methods: (1) a dataset-based approach that\nconstructs a steering vector by contrasting model activations on biased and\nneutral inputs, and (2) a novel optimization-based approach designed for\nlow-resource settings, which constructs the steering vector using a single step\nof gradient-based perturbation without requiring additional data. Our\nexperiments show that these interventions effectively reduce the propensity of\nLMMs to generate text related to protected attributes while maintaining\nsentiment and fluency. Furthermore, we demonstrate that debiased LMMs achieve\ncomparable accuracy to their unmodified counterparts on downstream tasks,\nindicating that bias mitigation can be achieved without sacrificing model\nperformance.\n", "link": "http://arxiv.org/abs/2411.12590v3", "date": "2025-09-18", "relevancy": 1.684, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debias%20your%20Large%20Multi-Modal%20Model%20at%20Test-Time%20via%20Non-Contrastive%0A%20%20Visual%20Attribute%20Steering&body=Title%3A%20Debias%20your%20Large%20Multi-Modal%20Model%20at%20Test-Time%20via%20Non-Contrastive%0A%20%20Visual%20Attribute%20Steering%0AAuthor%3A%20Neale%20Ratzlaff%20and%20Matthew%20Lyle%20Olson%20and%20Musashi%20Hinck%20and%20Estelle%20Aflalo%20and%20Shao-Yen%20Tseng%20and%20Vasudev%20Lal%20and%20Phillip%20Howard%0AAbstract%3A%20%20%20Large%20Multi-Modal%20Models%20%28LMMs%29%20have%20demonstrated%20impressive%20capabilities%20as%0Ageneral-purpose%20chatbots%20able%20to%20engage%20in%20conversations%20about%20visual%20inputs.%0AHowever%2C%20their%20responses%20are%20influenced%20by%20societal%20biases%20present%20in%20their%0Atraining%20datasets%2C%20leading%20to%20undesirable%20differences%20in%20how%20the%20model%20responds%0Awhen%20presented%20with%20images%20depicting%20people%20of%20different%20demographics.%20In%20this%0Awork%2C%20we%20propose%20a%20training-free%20debiasing%20framework%20for%20LMMs%20that%20intervenes%0Aon%20the%20model%27s%20representations%20during%20text%20generation%20by%20constructing%20a%0Asteering%20vector%20that%20reduces%20reference%20on%20protected%20attributes.%20Our%20framework%0Aintroduces%20two%20complementary%20methods%3A%20%281%29%20a%20dataset-based%20approach%20that%0Aconstructs%20a%20steering%20vector%20by%20contrasting%20model%20activations%20on%20biased%20and%0Aneutral%20inputs%2C%20and%20%282%29%20a%20novel%20optimization-based%20approach%20designed%20for%0Alow-resource%20settings%2C%20which%20constructs%20the%20steering%20vector%20using%20a%20single%20step%0Aof%20gradient-based%20perturbation%20without%20requiring%20additional%20data.%20Our%0Aexperiments%20show%20that%20these%20interventions%20effectively%20reduce%20the%20propensity%20of%0ALMMs%20to%20generate%20text%20related%20to%20protected%20attributes%20while%20maintaining%0Asentiment%20and%20fluency.%20Furthermore%2C%20we%20demonstrate%20that%20debiased%20LMMs%20achieve%0Acomparable%20accuracy%20to%20their%20unmodified%20counterparts%20on%20downstream%20tasks%2C%0Aindicating%20that%20bias%20mitigation%20can%20be%20achieved%20without%20sacrificing%20model%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12590v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebias%2520your%2520Large%2520Multi-Modal%2520Model%2520at%2520Test-Time%2520via%2520Non-Contrastive%250A%2520%2520Visual%2520Attribute%2520Steering%26entry.906535625%3DNeale%2520Ratzlaff%2520and%2520Matthew%2520Lyle%2520Olson%2520and%2520Musashi%2520Hinck%2520and%2520Estelle%2520Aflalo%2520and%2520Shao-Yen%2520Tseng%2520and%2520Vasudev%2520Lal%2520and%2520Phillip%2520Howard%26entry.1292438233%3D%2520%2520Large%2520Multi-Modal%2520Models%2520%2528LMMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520as%250Ageneral-purpose%2520chatbots%2520able%2520to%2520engage%2520in%2520conversations%2520about%2520visual%2520inputs.%250AHowever%252C%2520their%2520responses%2520are%2520influenced%2520by%2520societal%2520biases%2520present%2520in%2520their%250Atraining%2520datasets%252C%2520leading%2520to%2520undesirable%2520differences%2520in%2520how%2520the%2520model%2520responds%250Awhen%2520presented%2520with%2520images%2520depicting%2520people%2520of%2520different%2520demographics.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520training-free%2520debiasing%2520framework%2520for%2520LMMs%2520that%2520intervenes%250Aon%2520the%2520model%2527s%2520representations%2520during%2520text%2520generation%2520by%2520constructing%2520a%250Asteering%2520vector%2520that%2520reduces%2520reference%2520on%2520protected%2520attributes.%2520Our%2520framework%250Aintroduces%2520two%2520complementary%2520methods%253A%2520%25281%2529%2520a%2520dataset-based%2520approach%2520that%250Aconstructs%2520a%2520steering%2520vector%2520by%2520contrasting%2520model%2520activations%2520on%2520biased%2520and%250Aneutral%2520inputs%252C%2520and%2520%25282%2529%2520a%2520novel%2520optimization-based%2520approach%2520designed%2520for%250Alow-resource%2520settings%252C%2520which%2520constructs%2520the%2520steering%2520vector%2520using%2520a%2520single%2520step%250Aof%2520gradient-based%2520perturbation%2520without%2520requiring%2520additional%2520data.%2520Our%250Aexperiments%2520show%2520that%2520these%2520interventions%2520effectively%2520reduce%2520the%2520propensity%2520of%250ALMMs%2520to%2520generate%2520text%2520related%2520to%2520protected%2520attributes%2520while%2520maintaining%250Asentiment%2520and%2520fluency.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520debiased%2520LMMs%2520achieve%250Acomparable%2520accuracy%2520to%2520their%2520unmodified%2520counterparts%2520on%2520downstream%2520tasks%252C%250Aindicating%2520that%2520bias%2520mitigation%2520can%2520be%2520achieved%2520without%2520sacrificing%2520model%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12590v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debias%20your%20Large%20Multi-Modal%20Model%20at%20Test-Time%20via%20Non-Contrastive%0A%20%20Visual%20Attribute%20Steering&entry.906535625=Neale%20Ratzlaff%20and%20Matthew%20Lyle%20Olson%20and%20Musashi%20Hinck%20and%20Estelle%20Aflalo%20and%20Shao-Yen%20Tseng%20and%20Vasudev%20Lal%20and%20Phillip%20Howard&entry.1292438233=%20%20Large%20Multi-Modal%20Models%20%28LMMs%29%20have%20demonstrated%20impressive%20capabilities%20as%0Ageneral-purpose%20chatbots%20able%20to%20engage%20in%20conversations%20about%20visual%20inputs.%0AHowever%2C%20their%20responses%20are%20influenced%20by%20societal%20biases%20present%20in%20their%0Atraining%20datasets%2C%20leading%20to%20undesirable%20differences%20in%20how%20the%20model%20responds%0Awhen%20presented%20with%20images%20depicting%20people%20of%20different%20demographics.%20In%20this%0Awork%2C%20we%20propose%20a%20training-free%20debiasing%20framework%20for%20LMMs%20that%20intervenes%0Aon%20the%20model%27s%20representations%20during%20text%20generation%20by%20constructing%20a%0Asteering%20vector%20that%20reduces%20reference%20on%20protected%20attributes.%20Our%20framework%0Aintroduces%20two%20complementary%20methods%3A%20%281%29%20a%20dataset-based%20approach%20that%0Aconstructs%20a%20steering%20vector%20by%20contrasting%20model%20activations%20on%20biased%20and%0Aneutral%20inputs%2C%20and%20%282%29%20a%20novel%20optimization-based%20approach%20designed%20for%0Alow-resource%20settings%2C%20which%20constructs%20the%20steering%20vector%20using%20a%20single%20step%0Aof%20gradient-based%20perturbation%20without%20requiring%20additional%20data.%20Our%0Aexperiments%20show%20that%20these%20interventions%20effectively%20reduce%20the%20propensity%20of%0ALMMs%20to%20generate%20text%20related%20to%20protected%20attributes%20while%20maintaining%0Asentiment%20and%20fluency.%20Furthermore%2C%20we%20demonstrate%20that%20debiased%20LMMs%20achieve%0Acomparable%20accuracy%20to%20their%20unmodified%20counterparts%20on%20downstream%20tasks%2C%0Aindicating%20that%20bias%20mitigation%20can%20be%20achieved%20without%20sacrificing%20model%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12590v3&entry.124074799=Read"},
{"title": "On the Role of Individual Differences in Current Approaches to\n  Computational Image Aesthetics", "author": "Li-Wei Chen and Ombretta Strafforello and Anne-Sofie Maerten and Tinne Tuytelaars and Johan Wagemans", "abstract": "  Image aesthetic assessment (IAA) evaluates image aesthetics, a task\ncomplicated by image diversity and user subjectivity. Current approaches\naddress this in two stages: Generic IAA (GIAA) models estimate mean aesthetic\nscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to\nincorporate user subjectivity. However, a theoretical understanding of transfer\nlearning between GIAA and PIAA, particularly concerning the impact of group\ncomposition, group size, aesthetic differences between groups and individuals,\nand demographic correlations, is lacking. This work establishes a theoretical\nfoundation for IAA, proposing a unified model that encodes individual\ncharacteristics in a distributional format for both individual and group\nassessments. We show that transferring from GIAA to PIAA involves\nextrapolation, while the reverse involves interpolation, which is generally\nmore effective for machine learning. Extensive experiments with varying group\ncompositions, including sub-sampling by group size and disjoint demographics,\nreveal substantial performance variation even for GIAA, challenging the\nassumption that averaging scores eliminates individual subjectivity.\nScore-distribution analysis using Earth Mover's Distance (EMD) and the Gini\nindex identifies education, photography experience, and art experience as key\nfactors in aesthetic differences, with greater subjectivity in artworks than in\nphotographs. Code is available at\nhttps://github.com/lwchen6309/aesthetics_transfer_learning.\n", "link": "http://arxiv.org/abs/2502.20518v2", "date": "2025-09-18", "relevancy": 2.0302, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.512}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5073}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Role%20of%20Individual%20Differences%20in%20Current%20Approaches%20to%0A%20%20Computational%20Image%20Aesthetics&body=Title%3A%20On%20the%20Role%20of%20Individual%20Differences%20in%20Current%20Approaches%20to%0A%20%20Computational%20Image%20Aesthetics%0AAuthor%3A%20Li-Wei%20Chen%20and%20Ombretta%20Strafforello%20and%20Anne-Sofie%20Maerten%20and%20Tinne%20Tuytelaars%20and%20Johan%20Wagemans%0AAbstract%3A%20%20%20Image%20aesthetic%20assessment%20%28IAA%29%20evaluates%20image%20aesthetics%2C%20a%20task%0Acomplicated%20by%20image%20diversity%20and%20user%20subjectivity.%20Current%20approaches%0Aaddress%20this%20in%20two%20stages%3A%20Generic%20IAA%20%28GIAA%29%20models%20estimate%20mean%20aesthetic%0Ascores%2C%20while%20Personal%20IAA%20%28PIAA%29%20models%20adapt%20GIAA%20using%20transfer%20learning%20to%0Aincorporate%20user%20subjectivity.%20However%2C%20a%20theoretical%20understanding%20of%20transfer%0Alearning%20between%20GIAA%20and%20PIAA%2C%20particularly%20concerning%20the%20impact%20of%20group%0Acomposition%2C%20group%20size%2C%20aesthetic%20differences%20between%20groups%20and%20individuals%2C%0Aand%20demographic%20correlations%2C%20is%20lacking.%20This%20work%20establishes%20a%20theoretical%0Afoundation%20for%20IAA%2C%20proposing%20a%20unified%20model%20that%20encodes%20individual%0Acharacteristics%20in%20a%20distributional%20format%20for%20both%20individual%20and%20group%0Aassessments.%20We%20show%20that%20transferring%20from%20GIAA%20to%20PIAA%20involves%0Aextrapolation%2C%20while%20the%20reverse%20involves%20interpolation%2C%20which%20is%20generally%0Amore%20effective%20for%20machine%20learning.%20Extensive%20experiments%20with%20varying%20group%0Acompositions%2C%20including%20sub-sampling%20by%20group%20size%20and%20disjoint%20demographics%2C%0Areveal%20substantial%20performance%20variation%20even%20for%20GIAA%2C%20challenging%20the%0Aassumption%20that%20averaging%20scores%20eliminates%20individual%20subjectivity.%0AScore-distribution%20analysis%20using%20Earth%20Mover%27s%20Distance%20%28EMD%29%20and%20the%20Gini%0Aindex%20identifies%20education%2C%20photography%20experience%2C%20and%20art%20experience%20as%20key%0Afactors%20in%20aesthetic%20differences%2C%20with%20greater%20subjectivity%20in%20artworks%20than%20in%0Aphotographs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lwchen6309/aesthetics_transfer_learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Role%2520of%2520Individual%2520Differences%2520in%2520Current%2520Approaches%2520to%250A%2520%2520Computational%2520Image%2520Aesthetics%26entry.906535625%3DLi-Wei%2520Chen%2520and%2520Ombretta%2520Strafforello%2520and%2520Anne-Sofie%2520Maerten%2520and%2520Tinne%2520Tuytelaars%2520and%2520Johan%2520Wagemans%26entry.1292438233%3D%2520%2520Image%2520aesthetic%2520assessment%2520%2528IAA%2529%2520evaluates%2520image%2520aesthetics%252C%2520a%2520task%250Acomplicated%2520by%2520image%2520diversity%2520and%2520user%2520subjectivity.%2520Current%2520approaches%250Aaddress%2520this%2520in%2520two%2520stages%253A%2520Generic%2520IAA%2520%2528GIAA%2529%2520models%2520estimate%2520mean%2520aesthetic%250Ascores%252C%2520while%2520Personal%2520IAA%2520%2528PIAA%2529%2520models%2520adapt%2520GIAA%2520using%2520transfer%2520learning%2520to%250Aincorporate%2520user%2520subjectivity.%2520However%252C%2520a%2520theoretical%2520understanding%2520of%2520transfer%250Alearning%2520between%2520GIAA%2520and%2520PIAA%252C%2520particularly%2520concerning%2520the%2520impact%2520of%2520group%250Acomposition%252C%2520group%2520size%252C%2520aesthetic%2520differences%2520between%2520groups%2520and%2520individuals%252C%250Aand%2520demographic%2520correlations%252C%2520is%2520lacking.%2520This%2520work%2520establishes%2520a%2520theoretical%250Afoundation%2520for%2520IAA%252C%2520proposing%2520a%2520unified%2520model%2520that%2520encodes%2520individual%250Acharacteristics%2520in%2520a%2520distributional%2520format%2520for%2520both%2520individual%2520and%2520group%250Aassessments.%2520We%2520show%2520that%2520transferring%2520from%2520GIAA%2520to%2520PIAA%2520involves%250Aextrapolation%252C%2520while%2520the%2520reverse%2520involves%2520interpolation%252C%2520which%2520is%2520generally%250Amore%2520effective%2520for%2520machine%2520learning.%2520Extensive%2520experiments%2520with%2520varying%2520group%250Acompositions%252C%2520including%2520sub-sampling%2520by%2520group%2520size%2520and%2520disjoint%2520demographics%252C%250Areveal%2520substantial%2520performance%2520variation%2520even%2520for%2520GIAA%252C%2520challenging%2520the%250Aassumption%2520that%2520averaging%2520scores%2520eliminates%2520individual%2520subjectivity.%250AScore-distribution%2520analysis%2520using%2520Earth%2520Mover%2527s%2520Distance%2520%2528EMD%2529%2520and%2520the%2520Gini%250Aindex%2520identifies%2520education%252C%2520photography%2520experience%252C%2520and%2520art%2520experience%2520as%2520key%250Afactors%2520in%2520aesthetic%2520differences%252C%2520with%2520greater%2520subjectivity%2520in%2520artworks%2520than%2520in%250Aphotographs.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/lwchen6309/aesthetics_transfer_learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Role%20of%20Individual%20Differences%20in%20Current%20Approaches%20to%0A%20%20Computational%20Image%20Aesthetics&entry.906535625=Li-Wei%20Chen%20and%20Ombretta%20Strafforello%20and%20Anne-Sofie%20Maerten%20and%20Tinne%20Tuytelaars%20and%20Johan%20Wagemans&entry.1292438233=%20%20Image%20aesthetic%20assessment%20%28IAA%29%20evaluates%20image%20aesthetics%2C%20a%20task%0Acomplicated%20by%20image%20diversity%20and%20user%20subjectivity.%20Current%20approaches%0Aaddress%20this%20in%20two%20stages%3A%20Generic%20IAA%20%28GIAA%29%20models%20estimate%20mean%20aesthetic%0Ascores%2C%20while%20Personal%20IAA%20%28PIAA%29%20models%20adapt%20GIAA%20using%20transfer%20learning%20to%0Aincorporate%20user%20subjectivity.%20However%2C%20a%20theoretical%20understanding%20of%20transfer%0Alearning%20between%20GIAA%20and%20PIAA%2C%20particularly%20concerning%20the%20impact%20of%20group%0Acomposition%2C%20group%20size%2C%20aesthetic%20differences%20between%20groups%20and%20individuals%2C%0Aand%20demographic%20correlations%2C%20is%20lacking.%20This%20work%20establishes%20a%20theoretical%0Afoundation%20for%20IAA%2C%20proposing%20a%20unified%20model%20that%20encodes%20individual%0Acharacteristics%20in%20a%20distributional%20format%20for%20both%20individual%20and%20group%0Aassessments.%20We%20show%20that%20transferring%20from%20GIAA%20to%20PIAA%20involves%0Aextrapolation%2C%20while%20the%20reverse%20involves%20interpolation%2C%20which%20is%20generally%0Amore%20effective%20for%20machine%20learning.%20Extensive%20experiments%20with%20varying%20group%0Acompositions%2C%20including%20sub-sampling%20by%20group%20size%20and%20disjoint%20demographics%2C%0Areveal%20substantial%20performance%20variation%20even%20for%20GIAA%2C%20challenging%20the%0Aassumption%20that%20averaging%20scores%20eliminates%20individual%20subjectivity.%0AScore-distribution%20analysis%20using%20Earth%20Mover%27s%20Distance%20%28EMD%29%20and%20the%20Gini%0Aindex%20identifies%20education%2C%20photography%20experience%2C%20and%20art%20experience%20as%20key%0Afactors%20in%20aesthetic%20differences%2C%20with%20greater%20subjectivity%20in%20artworks%20than%20in%0Aphotographs.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lwchen6309/aesthetics_transfer_learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20518v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


