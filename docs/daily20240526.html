<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240522.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap", "author": "Mingrui Li and Jingwei Huang and Lei Sun and Aaron Xuxiang Tian and Tianchen Deng and Hongyu Wang", "abstract": "  SLAM systems based on Gaussian Splatting have garnered attention due to their\ncapabilities for rapid real-time rendering and high-fidelity mapping. However,\ncurrent Gaussian Splatting SLAM systems usually struggle with large scene\nrepresentation and lack effective loop closure detection. To address these\nissues, we introduce NGM-SLAM, the first 3DGS based SLAM system that utilizes\nneural radiance field submaps for progressive scene expression, effectively\nintegrating the strengths of neural radiance fields and 3D Gaussian Splatting.\nWe utilize neural radiance field submaps as supervision and achieve\nhigh-quality scene expression and online loop closure adjustments through\nGaussian rendering of fused submaps. Our results on multiple real-world scenes\nand large-scale scene datasets demonstrate that our method can achieve accurate\nhole filling and high-quality scene expression, supporting monocular, stereo,\nand RGB-D inputs, and achieving state-of-the-art scene reconstruction and\ntracking performance.\n", "link": "http://arxiv.org/abs/2405.05702v3", "date": "2024-05-23", "relevancy": 3.4108, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8163}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6478}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&body=Title%3A%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap%0AAuthor%3A%20Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang%0AAbstract%3A%20%20%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%20garnered%20attention%20due%20to%20their%0Acapabilities%20for%20rapid%20real-time%20rendering%20and%20high-fidelity%20mapping.%20However%2C%0Acurrent%20Gaussian%20Splatting%20SLAM%20systems%20usually%20struggle%20with%20large%20scene%0Arepresentation%20and%20lack%20effective%20loop%20closure%20detection.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NGM-SLAM%2C%20the%20first%203DGS%20based%20SLAM%20system%20that%20utilizes%0Aneural%20radiance%20field%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%0Aintegrating%20the%20strengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%0AWe%20utilize%20neural%20radiance%20field%20submaps%20as%20supervision%20and%20achieve%0Ahigh-quality%20scene%20expression%20and%20online%20loop%20closure%20adjustments%20through%0AGaussian%20rendering%20of%20fused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%0Aand%20large-scale%20scene%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%0Ahole%20filling%20and%20high-quality%20scene%20expression%2C%20supporting%20monocular%2C%20stereo%2C%0Aand%20RGB-D%20inputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%0Atracking%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05702v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNGM-SLAM%253A%2520Gaussian%2520Splatting%2520SLAM%2520with%2520Radiance%2520Field%2520Submap%26entry.906535625%3DMingrui%2520Li%2520and%2520Jingwei%2520Huang%2520and%2520Lei%2520Sun%2520and%2520Aaron%2520Xuxiang%2520Tian%2520and%2520Tianchen%2520Deng%2520and%2520Hongyu%2520Wang%26entry.1292438233%3D%2520%2520SLAM%2520systems%2520based%2520on%2520Gaussian%2520Splatting%2520have%2520garnered%2520attention%2520due%2520to%2520their%250Acapabilities%2520for%2520rapid%2520real-time%2520rendering%2520and%2520high-fidelity%2520mapping.%2520However%252C%250Acurrent%2520Gaussian%2520Splatting%2520SLAM%2520systems%2520usually%2520struggle%2520with%2520large%2520scene%250Arepresentation%2520and%2520lack%2520effective%2520loop%2520closure%2520detection.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520NGM-SLAM%252C%2520the%2520first%25203DGS%2520based%2520SLAM%2520system%2520that%2520utilizes%250Aneural%2520radiance%2520field%2520submaps%2520for%2520progressive%2520scene%2520expression%252C%2520effectively%250Aintegrating%2520the%2520strengths%2520of%2520neural%2520radiance%2520fields%2520and%25203D%2520Gaussian%2520Splatting.%250AWe%2520utilize%2520neural%2520radiance%2520field%2520submaps%2520as%2520supervision%2520and%2520achieve%250Ahigh-quality%2520scene%2520expression%2520and%2520online%2520loop%2520closure%2520adjustments%2520through%250AGaussian%2520rendering%2520of%2520fused%2520submaps.%2520Our%2520results%2520on%2520multiple%2520real-world%2520scenes%250Aand%2520large-scale%2520scene%2520datasets%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%2520accurate%250Ahole%2520filling%2520and%2520high-quality%2520scene%2520expression%252C%2520supporting%2520monocular%252C%2520stereo%252C%250Aand%2520RGB-D%2520inputs%252C%2520and%2520achieving%2520state-of-the-art%2520scene%2520reconstruction%2520and%250Atracking%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05702v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&entry.906535625=Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang&entry.1292438233=%20%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%20garnered%20attention%20due%20to%20their%0Acapabilities%20for%20rapid%20real-time%20rendering%20and%20high-fidelity%20mapping.%20However%2C%0Acurrent%20Gaussian%20Splatting%20SLAM%20systems%20usually%20struggle%20with%20large%20scene%0Arepresentation%20and%20lack%20effective%20loop%20closure%20detection.%20To%20address%20these%0Aissues%2C%20we%20introduce%20NGM-SLAM%2C%20the%20first%203DGS%20based%20SLAM%20system%20that%20utilizes%0Aneural%20radiance%20field%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%0Aintegrating%20the%20strengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%0AWe%20utilize%20neural%20radiance%20field%20submaps%20as%20supervision%20and%20achieve%0Ahigh-quality%20scene%20expression%20and%20online%20loop%20closure%20adjustments%20through%0AGaussian%20rendering%20of%20fused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%0Aand%20large-scale%20scene%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%0Ahole%20filling%20and%20high-quality%20scene%20expression%2C%20supporting%20monocular%2C%20stereo%2C%0Aand%20RGB-D%20inputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%0Atracking%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05702v3&entry.124074799=Read"},
{"title": "MagicDrive3D: Controllable 3D Generation for Any-View Rendering in\n  Street Scenes", "author": "Ruiyuan Gao and Kai Chen and Zhihao Li and Lanqing Hong and Zhenguo Li and Qiang Xu", "abstract": "  While controllable generative models for images and videos have achieved\nremarkable success, high-quality models for 3D scenes, particularly in\nunbounded scenarios like autonomous driving, remain underdeveloped due to high\ndata acquisition costs. In this paper, we introduce MagicDrive3D, a novel\npipeline for controllable 3D street scene generation that supports\nmulti-condition control, including BEV maps, 3D objects, and text descriptions.\nUnlike previous methods that reconstruct before training the generative models,\nMagicDrive3D first trains a video generation model and then reconstructs from\nthe generated data. This innovative approach enables easily controllable\ngeneration and static scene acquisition, resulting in high-quality scene\nreconstruction. To address the minor errors in generated content, we propose\ndeformable Gaussian splatting with monocular depth initialization and\nappearance modeling to manage exposure discrepancies across viewpoints.\nValidated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality\n3D driving scenes that support any-view rendering and enhance downstream tasks\nlike BEV segmentation. Our results demonstrate the framework's superior\nperformance, showcasing its transformative potential for autonomous driving\nsimulation and beyond.\n", "link": "http://arxiv.org/abs/2405.14475v1", "date": "2024-05-23", "relevancy": 3.3136, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6755}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6755}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicDrive3D%3A%20Controllable%203D%20Generation%20for%20Any-View%20Rendering%20in%0A%20%20Street%20Scenes&body=Title%3A%20MagicDrive3D%3A%20Controllable%203D%20Generation%20for%20Any-View%20Rendering%20in%0A%20%20Street%20Scenes%0AAuthor%3A%20Ruiyuan%20Gao%20and%20Kai%20Chen%20and%20Zhihao%20Li%20and%20Lanqing%20Hong%20and%20Zhenguo%20Li%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20While%20controllable%20generative%20models%20for%20images%20and%20videos%20have%20achieved%0Aremarkable%20success%2C%20high-quality%20models%20for%203D%20scenes%2C%20particularly%20in%0Aunbounded%20scenarios%20like%20autonomous%20driving%2C%20remain%20underdeveloped%20due%20to%20high%0Adata%20acquisition%20costs.%20In%20this%20paper%2C%20we%20introduce%20MagicDrive3D%2C%20a%20novel%0Apipeline%20for%20controllable%203D%20street%20scene%20generation%20that%20supports%0Amulti-condition%20control%2C%20including%20BEV%20maps%2C%203D%20objects%2C%20and%20text%20descriptions.%0AUnlike%20previous%20methods%20that%20reconstruct%20before%20training%20the%20generative%20models%2C%0AMagicDrive3D%20first%20trains%20a%20video%20generation%20model%20and%20then%20reconstructs%20from%0Athe%20generated%20data.%20This%20innovative%20approach%20enables%20easily%20controllable%0Ageneration%20and%20static%20scene%20acquisition%2C%20resulting%20in%20high-quality%20scene%0Areconstruction.%20To%20address%20the%20minor%20errors%20in%20generated%20content%2C%20we%20propose%0Adeformable%20Gaussian%20splatting%20with%20monocular%20depth%20initialization%20and%0Aappearance%20modeling%20to%20manage%20exposure%20discrepancies%20across%20viewpoints.%0AValidated%20on%20the%20nuScenes%20dataset%2C%20MagicDrive3D%20generates%20diverse%2C%20high-quality%0A3D%20driving%20scenes%20that%20support%20any-view%20rendering%20and%20enhance%20downstream%20tasks%0Alike%20BEV%20segmentation.%20Our%20results%20demonstrate%20the%20framework%27s%20superior%0Aperformance%2C%20showcasing%20its%20transformative%20potential%20for%20autonomous%20driving%0Asimulation%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicDrive3D%253A%2520Controllable%25203D%2520Generation%2520for%2520Any-View%2520Rendering%2520in%250A%2520%2520Street%2520Scenes%26entry.906535625%3DRuiyuan%2520Gao%2520and%2520Kai%2520Chen%2520and%2520Zhihao%2520Li%2520and%2520Lanqing%2520Hong%2520and%2520Zhenguo%2520Li%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520While%2520controllable%2520generative%2520models%2520for%2520images%2520and%2520videos%2520have%2520achieved%250Aremarkable%2520success%252C%2520high-quality%2520models%2520for%25203D%2520scenes%252C%2520particularly%2520in%250Aunbounded%2520scenarios%2520like%2520autonomous%2520driving%252C%2520remain%2520underdeveloped%2520due%2520to%2520high%250Adata%2520acquisition%2520costs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MagicDrive3D%252C%2520a%2520novel%250Apipeline%2520for%2520controllable%25203D%2520street%2520scene%2520generation%2520that%2520supports%250Amulti-condition%2520control%252C%2520including%2520BEV%2520maps%252C%25203D%2520objects%252C%2520and%2520text%2520descriptions.%250AUnlike%2520previous%2520methods%2520that%2520reconstruct%2520before%2520training%2520the%2520generative%2520models%252C%250AMagicDrive3D%2520first%2520trains%2520a%2520video%2520generation%2520model%2520and%2520then%2520reconstructs%2520from%250Athe%2520generated%2520data.%2520This%2520innovative%2520approach%2520enables%2520easily%2520controllable%250Ageneration%2520and%2520static%2520scene%2520acquisition%252C%2520resulting%2520in%2520high-quality%2520scene%250Areconstruction.%2520To%2520address%2520the%2520minor%2520errors%2520in%2520generated%2520content%252C%2520we%2520propose%250Adeformable%2520Gaussian%2520splatting%2520with%2520monocular%2520depth%2520initialization%2520and%250Aappearance%2520modeling%2520to%2520manage%2520exposure%2520discrepancies%2520across%2520viewpoints.%250AValidated%2520on%2520the%2520nuScenes%2520dataset%252C%2520MagicDrive3D%2520generates%2520diverse%252C%2520high-quality%250A3D%2520driving%2520scenes%2520that%2520support%2520any-view%2520rendering%2520and%2520enhance%2520downstream%2520tasks%250Alike%2520BEV%2520segmentation.%2520Our%2520results%2520demonstrate%2520the%2520framework%2527s%2520superior%250Aperformance%252C%2520showcasing%2520its%2520transformative%2520potential%2520for%2520autonomous%2520driving%250Asimulation%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicDrive3D%3A%20Controllable%203D%20Generation%20for%20Any-View%20Rendering%20in%0A%20%20Street%20Scenes&entry.906535625=Ruiyuan%20Gao%20and%20Kai%20Chen%20and%20Zhihao%20Li%20and%20Lanqing%20Hong%20and%20Zhenguo%20Li%20and%20Qiang%20Xu&entry.1292438233=%20%20While%20controllable%20generative%20models%20for%20images%20and%20videos%20have%20achieved%0Aremarkable%20success%2C%20high-quality%20models%20for%203D%20scenes%2C%20particularly%20in%0Aunbounded%20scenarios%20like%20autonomous%20driving%2C%20remain%20underdeveloped%20due%20to%20high%0Adata%20acquisition%20costs.%20In%20this%20paper%2C%20we%20introduce%20MagicDrive3D%2C%20a%20novel%0Apipeline%20for%20controllable%203D%20street%20scene%20generation%20that%20supports%0Amulti-condition%20control%2C%20including%20BEV%20maps%2C%203D%20objects%2C%20and%20text%20descriptions.%0AUnlike%20previous%20methods%20that%20reconstruct%20before%20training%20the%20generative%20models%2C%0AMagicDrive3D%20first%20trains%20a%20video%20generation%20model%20and%20then%20reconstructs%20from%0Athe%20generated%20data.%20This%20innovative%20approach%20enables%20easily%20controllable%0Ageneration%20and%20static%20scene%20acquisition%2C%20resulting%20in%20high-quality%20scene%0Areconstruction.%20To%20address%20the%20minor%20errors%20in%20generated%20content%2C%20we%20propose%0Adeformable%20Gaussian%20splatting%20with%20monocular%20depth%20initialization%20and%0Aappearance%20modeling%20to%20manage%20exposure%20discrepancies%20across%20viewpoints.%0AValidated%20on%20the%20nuScenes%20dataset%2C%20MagicDrive3D%20generates%20diverse%2C%20high-quality%0A3D%20driving%20scenes%20that%20support%20any-view%20rendering%20and%20enhance%20downstream%20tasks%0Alike%20BEV%20segmentation.%20Our%20results%20demonstrate%20the%20framework%27s%20superior%0Aperformance%2C%20showcasing%20its%20transformative%20potential%20for%20autonomous%20driving%0Asimulation%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14475v1&entry.124074799=Read"},
{"title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis", "author": "Basile Van Hoorick and Rundi Wu and Ege Ozguroglu and Kyle Sargent and Ruoshi Liu and Pavel Tokmakov and Achal Dave and Changxi Zheng and Carl Vondrick", "abstract": "  Accurate reconstruction of complex dynamic scenes from just a single\nviewpoint continues to be a challenging task in computer vision. Current\ndynamic novel view synthesis methods typically require videos from many\ndifferent camera viewpoints, necessitating careful recording setups, and\nsignificantly restricting their utility in the wild as well as in terms of\nembodied AI applications. In this paper, we propose $\\textbf{GCD}$, a\ncontrollable monocular dynamic view synthesis pipeline that leverages\nlarge-scale diffusion priors to, given a video of any scene, generate a\nsynchronous video from any other chosen perspective, conditioned on a set of\nrelative camera pose parameters. Our model does not require depth as input, and\ndoes not explicitly model 3D scene geometry, instead performing end-to-end\nvideo-to-video translation in order to achieve its goal efficiently. Despite\nbeing trained on synthetic multi-view video data only, zero-shot real-world\ngeneralization experiments show promising results in multiple domains,\nincluding robotics, object permanence, and driving environments. We believe our\nframework can potentially unlock powerful applications in rich dynamic scene\nunderstanding, perception for robotics, and interactive 3D video viewing\nexperiences for virtual reality.\n", "link": "http://arxiv.org/abs/2405.14868v1", "date": "2024-05-23", "relevancy": 3.3042, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6702}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6702}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis&body=Title%3A%20Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis%0AAuthor%3A%20Basile%20Van%20Hoorick%20and%20Rundi%20Wu%20and%20Ege%20Ozguroglu%20and%20Kyle%20Sargent%20and%20Ruoshi%20Liu%20and%20Pavel%20Tokmakov%20and%20Achal%20Dave%20and%20Changxi%20Zheng%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20Accurate%20reconstruction%20of%20complex%20dynamic%20scenes%20from%20just%20a%20single%0Aviewpoint%20continues%20to%20be%20a%20challenging%20task%20in%20computer%20vision.%20Current%0Adynamic%20novel%20view%20synthesis%20methods%20typically%20require%20videos%20from%20many%0Adifferent%20camera%20viewpoints%2C%20necessitating%20careful%20recording%20setups%2C%20and%0Asignificantly%20restricting%20their%20utility%20in%20the%20wild%20as%20well%20as%20in%20terms%20of%0Aembodied%20AI%20applications.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BGCD%7D%24%2C%20a%0Acontrollable%20monocular%20dynamic%20view%20synthesis%20pipeline%20that%20leverages%0Alarge-scale%20diffusion%20priors%20to%2C%20given%20a%20video%20of%20any%20scene%2C%20generate%20a%0Asynchronous%20video%20from%20any%20other%20chosen%20perspective%2C%20conditioned%20on%20a%20set%20of%0Arelative%20camera%20pose%20parameters.%20Our%20model%20does%20not%20require%20depth%20as%20input%2C%20and%0Adoes%20not%20explicitly%20model%203D%20scene%20geometry%2C%20instead%20performing%20end-to-end%0Avideo-to-video%20translation%20in%20order%20to%20achieve%20its%20goal%20efficiently.%20Despite%0Abeing%20trained%20on%20synthetic%20multi-view%20video%20data%20only%2C%20zero-shot%20real-world%0Ageneralization%20experiments%20show%20promising%20results%20in%20multiple%20domains%2C%0Aincluding%20robotics%2C%20object%20permanence%2C%20and%20driving%20environments.%20We%20believe%20our%0Aframework%20can%20potentially%20unlock%20powerful%20applications%20in%20rich%20dynamic%20scene%0Aunderstanding%2C%20perception%20for%20robotics%2C%20and%20interactive%203D%20video%20viewing%0Aexperiences%20for%20virtual%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Camera%2520Dolly%253A%2520Extreme%2520Monocular%2520Dynamic%2520Novel%2520View%2520Synthesis%26entry.906535625%3DBasile%2520Van%2520Hoorick%2520and%2520Rundi%2520Wu%2520and%2520Ege%2520Ozguroglu%2520and%2520Kyle%2520Sargent%2520and%2520Ruoshi%2520Liu%2520and%2520Pavel%2520Tokmakov%2520and%2520Achal%2520Dave%2520and%2520Changxi%2520Zheng%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520Accurate%2520reconstruction%2520of%2520complex%2520dynamic%2520scenes%2520from%2520just%2520a%2520single%250Aviewpoint%2520continues%2520to%2520be%2520a%2520challenging%2520task%2520in%2520computer%2520vision.%2520Current%250Adynamic%2520novel%2520view%2520synthesis%2520methods%2520typically%2520require%2520videos%2520from%2520many%250Adifferent%2520camera%2520viewpoints%252C%2520necessitating%2520careful%2520recording%2520setups%252C%2520and%250Asignificantly%2520restricting%2520their%2520utility%2520in%2520the%2520wild%2520as%2520well%2520as%2520in%2520terms%2520of%250Aembodied%2520AI%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%2524%255Ctextbf%257BGCD%257D%2524%252C%2520a%250Acontrollable%2520monocular%2520dynamic%2520view%2520synthesis%2520pipeline%2520that%2520leverages%250Alarge-scale%2520diffusion%2520priors%2520to%252C%2520given%2520a%2520video%2520of%2520any%2520scene%252C%2520generate%2520a%250Asynchronous%2520video%2520from%2520any%2520other%2520chosen%2520perspective%252C%2520conditioned%2520on%2520a%2520set%2520of%250Arelative%2520camera%2520pose%2520parameters.%2520Our%2520model%2520does%2520not%2520require%2520depth%2520as%2520input%252C%2520and%250Adoes%2520not%2520explicitly%2520model%25203D%2520scene%2520geometry%252C%2520instead%2520performing%2520end-to-end%250Avideo-to-video%2520translation%2520in%2520order%2520to%2520achieve%2520its%2520goal%2520efficiently.%2520Despite%250Abeing%2520trained%2520on%2520synthetic%2520multi-view%2520video%2520data%2520only%252C%2520zero-shot%2520real-world%250Ageneralization%2520experiments%2520show%2520promising%2520results%2520in%2520multiple%2520domains%252C%250Aincluding%2520robotics%252C%2520object%2520permanence%252C%2520and%2520driving%2520environments.%2520We%2520believe%2520our%250Aframework%2520can%2520potentially%2520unlock%2520powerful%2520applications%2520in%2520rich%2520dynamic%2520scene%250Aunderstanding%252C%2520perception%2520for%2520robotics%252C%2520and%2520interactive%25203D%2520video%2520viewing%250Aexperiences%2520for%2520virtual%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis&entry.906535625=Basile%20Van%20Hoorick%20and%20Rundi%20Wu%20and%20Ege%20Ozguroglu%20and%20Kyle%20Sargent%20and%20Ruoshi%20Liu%20and%20Pavel%20Tokmakov%20and%20Achal%20Dave%20and%20Changxi%20Zheng%20and%20Carl%20Vondrick&entry.1292438233=%20%20Accurate%20reconstruction%20of%20complex%20dynamic%20scenes%20from%20just%20a%20single%0Aviewpoint%20continues%20to%20be%20a%20challenging%20task%20in%20computer%20vision.%20Current%0Adynamic%20novel%20view%20synthesis%20methods%20typically%20require%20videos%20from%20many%0Adifferent%20camera%20viewpoints%2C%20necessitating%20careful%20recording%20setups%2C%20and%0Asignificantly%20restricting%20their%20utility%20in%20the%20wild%20as%20well%20as%20in%20terms%20of%0Aembodied%20AI%20applications.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BGCD%7D%24%2C%20a%0Acontrollable%20monocular%20dynamic%20view%20synthesis%20pipeline%20that%20leverages%0Alarge-scale%20diffusion%20priors%20to%2C%20given%20a%20video%20of%20any%20scene%2C%20generate%20a%0Asynchronous%20video%20from%20any%20other%20chosen%20perspective%2C%20conditioned%20on%20a%20set%20of%0Arelative%20camera%20pose%20parameters.%20Our%20model%20does%20not%20require%20depth%20as%20input%2C%20and%0Adoes%20not%20explicitly%20model%203D%20scene%20geometry%2C%20instead%20performing%20end-to-end%0Avideo-to-video%20translation%20in%20order%20to%20achieve%20its%20goal%20efficiently.%20Despite%0Abeing%20trained%20on%20synthetic%20multi-view%20video%20data%20only%2C%20zero-shot%20real-world%0Ageneralization%20experiments%20show%20promising%20results%20in%20multiple%20domains%2C%0Aincluding%20robotics%2C%20object%20permanence%2C%20and%20driving%20environments.%20We%20believe%20our%0Aframework%20can%20potentially%20unlock%20powerful%20applications%20in%20rich%20dynamic%20scene%0Aunderstanding%2C%20perception%20for%20robotics%2C%20and%20interactive%203D%20video%20viewing%0Aexperiences%20for%20virtual%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14868v1&entry.124074799=Read"},
{"title": "DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular\n  Videos", "author": "Wen-Hsuan Chu and Lei Ke and Katerina Fragkiadaki", "abstract": "  View-predictive generative models provide strong priors for lifting\nobject-centric images and videos into 3D and 4D through rendering and score\ndistillation objectives. A question then remains: what about lifting complete\nmulti-object dynamic scenes? There are two challenges in this direction: First,\nrendering error gradients are often insufficient to recover fast object motion,\nand second, view predictive generative models work much better for objects than\nwhole scenes, so, score distillation objectives cannot currently be applied at\nthe scene level directly. We present DreamScene4D, the first approach to\ngenerate 3D dynamic scenes of multiple objects from monocular videos via\n360-degree novel view synthesis. Our key insight is a \"decompose-recompose\"\napproach that factorizes the video scene into the background and object tracks,\nwhile also factorizing object motion into 3 components: object-centric\ndeformation, object-to-world-frame transformation, and camera motion. Such\ndecomposition permits rendering error gradients and object view-predictive\nmodels to recover object 3D completions and deformations while bounding box\ntracks guide the large object movements in the scene. We show extensive results\non challenging DAVIS, Kubric, and self-captured videos with quantitative\ncomparisons and a user preference study. Besides 4D scene generation,\nDreamScene4D obtains accurate 2D persistent point track by projecting the\ninferred 3D trajectories to 2D. We will release our code and hope our work will\nstimulate more research on fine-grained 4D understanding from videos.\n", "link": "http://arxiv.org/abs/2405.02280v2", "date": "2024-05-23", "relevancy": 3.2671, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.663}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.663}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos&body=Title%3A%20DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos%0AAuthor%3A%20Wen-Hsuan%20Chu%20and%20Lei%20Ke%20and%20Katerina%20Fragkiadaki%0AAbstract%3A%20%20%20View-predictive%20generative%20models%20provide%20strong%20priors%20for%20lifting%0Aobject-centric%20images%20and%20videos%20into%203D%20and%204D%20through%20rendering%20and%20score%0Adistillation%20objectives.%20A%20question%20then%20remains%3A%20what%20about%20lifting%20complete%0Amulti-object%20dynamic%20scenes%3F%20There%20are%20two%20challenges%20in%20this%20direction%3A%20First%2C%0Arendering%20error%20gradients%20are%20often%20insufficient%20to%20recover%20fast%20object%20motion%2C%0Aand%20second%2C%20view%20predictive%20generative%20models%20work%20much%20better%20for%20objects%20than%0Awhole%20scenes%2C%20so%2C%20score%20distillation%20objectives%20cannot%20currently%20be%20applied%20at%0Athe%20scene%20level%20directly.%20We%20present%20DreamScene4D%2C%20the%20first%20approach%20to%0Agenerate%203D%20dynamic%20scenes%20of%20multiple%20objects%20from%20monocular%20videos%20via%0A360-degree%20novel%20view%20synthesis.%20Our%20key%20insight%20is%20a%20%22decompose-recompose%22%0Aapproach%20that%20factorizes%20the%20video%20scene%20into%20the%20background%20and%20object%20tracks%2C%0Awhile%20also%20factorizing%20object%20motion%20into%203%20components%3A%20object-centric%0Adeformation%2C%20object-to-world-frame%20transformation%2C%20and%20camera%20motion.%20Such%0Adecomposition%20permits%20rendering%20error%20gradients%20and%20object%20view-predictive%0Amodels%20to%20recover%20object%203D%20completions%20and%20deformations%20while%20bounding%20box%0Atracks%20guide%20the%20large%20object%20movements%20in%20the%20scene.%20We%20show%20extensive%20results%0Aon%20challenging%20DAVIS%2C%20Kubric%2C%20and%20self-captured%20videos%20with%20quantitative%0Acomparisons%20and%20a%20user%20preference%20study.%20Besides%204D%20scene%20generation%2C%0ADreamScene4D%20obtains%20accurate%202D%20persistent%20point%20track%20by%20projecting%20the%0Ainferred%203D%20trajectories%20to%202D.%20We%20will%20release%20our%20code%20and%20hope%20our%20work%20will%0Astimulate%20more%20research%20on%20fine-grained%204D%20understanding%20from%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02280v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamScene4D%253A%2520Dynamic%2520Multi-Object%2520Scene%2520Generation%2520from%2520Monocular%250A%2520%2520Videos%26entry.906535625%3DWen-Hsuan%2520Chu%2520and%2520Lei%2520Ke%2520and%2520Katerina%2520Fragkiadaki%26entry.1292438233%3D%2520%2520View-predictive%2520generative%2520models%2520provide%2520strong%2520priors%2520for%2520lifting%250Aobject-centric%2520images%2520and%2520videos%2520into%25203D%2520and%25204D%2520through%2520rendering%2520and%2520score%250Adistillation%2520objectives.%2520A%2520question%2520then%2520remains%253A%2520what%2520about%2520lifting%2520complete%250Amulti-object%2520dynamic%2520scenes%253F%2520There%2520are%2520two%2520challenges%2520in%2520this%2520direction%253A%2520First%252C%250Arendering%2520error%2520gradients%2520are%2520often%2520insufficient%2520to%2520recover%2520fast%2520object%2520motion%252C%250Aand%2520second%252C%2520view%2520predictive%2520generative%2520models%2520work%2520much%2520better%2520for%2520objects%2520than%250Awhole%2520scenes%252C%2520so%252C%2520score%2520distillation%2520objectives%2520cannot%2520currently%2520be%2520applied%2520at%250Athe%2520scene%2520level%2520directly.%2520We%2520present%2520DreamScene4D%252C%2520the%2520first%2520approach%2520to%250Agenerate%25203D%2520dynamic%2520scenes%2520of%2520multiple%2520objects%2520from%2520monocular%2520videos%2520via%250A360-degree%2520novel%2520view%2520synthesis.%2520Our%2520key%2520insight%2520is%2520a%2520%2522decompose-recompose%2522%250Aapproach%2520that%2520factorizes%2520the%2520video%2520scene%2520into%2520the%2520background%2520and%2520object%2520tracks%252C%250Awhile%2520also%2520factorizing%2520object%2520motion%2520into%25203%2520components%253A%2520object-centric%250Adeformation%252C%2520object-to-world-frame%2520transformation%252C%2520and%2520camera%2520motion.%2520Such%250Adecomposition%2520permits%2520rendering%2520error%2520gradients%2520and%2520object%2520view-predictive%250Amodels%2520to%2520recover%2520object%25203D%2520completions%2520and%2520deformations%2520while%2520bounding%2520box%250Atracks%2520guide%2520the%2520large%2520object%2520movements%2520in%2520the%2520scene.%2520We%2520show%2520extensive%2520results%250Aon%2520challenging%2520DAVIS%252C%2520Kubric%252C%2520and%2520self-captured%2520videos%2520with%2520quantitative%250Acomparisons%2520and%2520a%2520user%2520preference%2520study.%2520Besides%25204D%2520scene%2520generation%252C%250ADreamScene4D%2520obtains%2520accurate%25202D%2520persistent%2520point%2520track%2520by%2520projecting%2520the%250Ainferred%25203D%2520trajectories%2520to%25202D.%2520We%2520will%2520release%2520our%2520code%2520and%2520hope%2520our%2520work%2520will%250Astimulate%2520more%2520research%2520on%2520fine-grained%25204D%2520understanding%2520from%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02280v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamScene4D%3A%20Dynamic%20Multi-Object%20Scene%20Generation%20from%20Monocular%0A%20%20Videos&entry.906535625=Wen-Hsuan%20Chu%20and%20Lei%20Ke%20and%20Katerina%20Fragkiadaki&entry.1292438233=%20%20View-predictive%20generative%20models%20provide%20strong%20priors%20for%20lifting%0Aobject-centric%20images%20and%20videos%20into%203D%20and%204D%20through%20rendering%20and%20score%0Adistillation%20objectives.%20A%20question%20then%20remains%3A%20what%20about%20lifting%20complete%0Amulti-object%20dynamic%20scenes%3F%20There%20are%20two%20challenges%20in%20this%20direction%3A%20First%2C%0Arendering%20error%20gradients%20are%20often%20insufficient%20to%20recover%20fast%20object%20motion%2C%0Aand%20second%2C%20view%20predictive%20generative%20models%20work%20much%20better%20for%20objects%20than%0Awhole%20scenes%2C%20so%2C%20score%20distillation%20objectives%20cannot%20currently%20be%20applied%20at%0Athe%20scene%20level%20directly.%20We%20present%20DreamScene4D%2C%20the%20first%20approach%20to%0Agenerate%203D%20dynamic%20scenes%20of%20multiple%20objects%20from%20monocular%20videos%20via%0A360-degree%20novel%20view%20synthesis.%20Our%20key%20insight%20is%20a%20%22decompose-recompose%22%0Aapproach%20that%20factorizes%20the%20video%20scene%20into%20the%20background%20and%20object%20tracks%2C%0Awhile%20also%20factorizing%20object%20motion%20into%203%20components%3A%20object-centric%0Adeformation%2C%20object-to-world-frame%20transformation%2C%20and%20camera%20motion.%20Such%0Adecomposition%20permits%20rendering%20error%20gradients%20and%20object%20view-predictive%0Amodels%20to%20recover%20object%203D%20completions%20and%20deformations%20while%20bounding%20box%0Atracks%20guide%20the%20large%20object%20movements%20in%20the%20scene.%20We%20show%20extensive%20results%0Aon%20challenging%20DAVIS%2C%20Kubric%2C%20and%20self-captured%20videos%20with%20quantitative%0Acomparisons%20and%20a%20user%20preference%20study.%20Besides%204D%20scene%20generation%2C%0ADreamScene4D%20obtains%20accurate%202D%20persistent%20point%20track%20by%20projecting%20the%0Ainferred%203D%20trajectories%20to%202D.%20We%20will%20release%20our%20code%20and%20hope%20our%20work%20will%0Astimulate%20more%20research%20on%20fine-grained%204D%20understanding%20from%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02280v2&entry.124074799=Read"},
{"title": "IPDreamer: Appearance-Controllable 3D Object Generation with Complex\n  Image Prompts", "author": "Bohan Zeng and Shanglin Li and Yutang Feng and Ling Yang and Hong Li and Sicheng Gao and Jiaming Liu and Conghui He and Wentao Zhang and Jianzhuang Liu and Baochang Zhang and Shuicheng Yan", "abstract": "  Recent advances in 3D generation have been remarkable, with methods such as\nDreamFusion leveraging large-scale text-to-image diffusion-based models to\nsupervise 3D object generation. These methods enable the synthesis of detailed\nand photorealistic textured objects. However, the appearance of 3D objects\nproduced by these text-to-3D models is unpredictable, and it is hard for the\nsingle-image-to-3D methods to deal with complex images, thus posing a challenge\nin generating appearance-controllable 3D objects. To achieve controllable\ncomplex 3D object synthesis, we propose IPDreamer, a novel approach that\nincorporates image prompt adaption to extract detailed and comprehensive\nappearance features from complex images, which are then utilized for 3D object\ngeneration. Our results demonstrate that IPDreamer effectively generates\nhigh-quality 3D objects that are consistent with both the provided text and the\nappearance of complex image prompts, demonstrating its promising capability in\nappearance-controllable 3D object generation. Our code is available at\nhttps://github.com/zengbohan0217/IPDreamer.\n", "link": "http://arxiv.org/abs/2310.05375v4", "date": "2024-05-23", "relevancy": 3.1233, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6268}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6236}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts&body=Title%3A%20IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts%0AAuthor%3A%20Bohan%20Zeng%20and%20Shanglin%20Li%20and%20Yutang%20Feng%20and%20Ling%20Yang%20and%20Hong%20Li%20and%20Sicheng%20Gao%20and%20Jiaming%20Liu%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Jianzhuang%20Liu%20and%20Baochang%20Zhang%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Recent%20advances%20in%203D%20generation%20have%20been%20remarkable%2C%20with%20methods%20such%20as%0ADreamFusion%20leveraging%20large-scale%20text-to-image%20diffusion-based%20models%20to%0Asupervise%203D%20object%20generation.%20These%20methods%20enable%20the%20synthesis%20of%20detailed%0Aand%20photorealistic%20textured%20objects.%20However%2C%20the%20appearance%20of%203D%20objects%0Aproduced%20by%20these%20text-to-3D%20models%20is%20unpredictable%2C%20and%20it%20is%20hard%20for%20the%0Asingle-image-to-3D%20methods%20to%20deal%20with%20complex%20images%2C%20thus%20posing%20a%20challenge%0Ain%20generating%20appearance-controllable%203D%20objects.%20To%20achieve%20controllable%0Acomplex%203D%20object%20synthesis%2C%20we%20propose%20IPDreamer%2C%20a%20novel%20approach%20that%0Aincorporates%20image%20prompt%20adaption%20to%20extract%20detailed%20and%20comprehensive%0Aappearance%20features%20from%20complex%20images%2C%20which%20are%20then%20utilized%20for%203D%20object%0Ageneration.%20Our%20results%20demonstrate%20that%20IPDreamer%20effectively%20generates%0Ahigh-quality%203D%20objects%20that%20are%20consistent%20with%20both%20the%20provided%20text%20and%20the%0Aappearance%20of%20complex%20image%20prompts%2C%20demonstrating%20its%20promising%20capability%20in%0Aappearance-controllable%203D%20object%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zengbohan0217/IPDreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05375v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPDreamer%253A%2520Appearance-Controllable%25203D%2520Object%2520Generation%2520with%2520Complex%250A%2520%2520Image%2520Prompts%26entry.906535625%3DBohan%2520Zeng%2520and%2520Shanglin%2520Li%2520and%2520Yutang%2520Feng%2520and%2520Ling%2520Yang%2520and%2520Hong%2520Li%2520and%2520Sicheng%2520Gao%2520and%2520Jiaming%2520Liu%2520and%2520Conghui%2520He%2520and%2520Wentao%2520Zhang%2520and%2520Jianzhuang%2520Liu%2520and%2520Baochang%2520Zhang%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%25203D%2520generation%2520have%2520been%2520remarkable%252C%2520with%2520methods%2520such%2520as%250ADreamFusion%2520leveraging%2520large-scale%2520text-to-image%2520diffusion-based%2520models%2520to%250Asupervise%25203D%2520object%2520generation.%2520These%2520methods%2520enable%2520the%2520synthesis%2520of%2520detailed%250Aand%2520photorealistic%2520textured%2520objects.%2520However%252C%2520the%2520appearance%2520of%25203D%2520objects%250Aproduced%2520by%2520these%2520text-to-3D%2520models%2520is%2520unpredictable%252C%2520and%2520it%2520is%2520hard%2520for%2520the%250Asingle-image-to-3D%2520methods%2520to%2520deal%2520with%2520complex%2520images%252C%2520thus%2520posing%2520a%2520challenge%250Ain%2520generating%2520appearance-controllable%25203D%2520objects.%2520To%2520achieve%2520controllable%250Acomplex%25203D%2520object%2520synthesis%252C%2520we%2520propose%2520IPDreamer%252C%2520a%2520novel%2520approach%2520that%250Aincorporates%2520image%2520prompt%2520adaption%2520to%2520extract%2520detailed%2520and%2520comprehensive%250Aappearance%2520features%2520from%2520complex%2520images%252C%2520which%2520are%2520then%2520utilized%2520for%25203D%2520object%250Ageneration.%2520Our%2520results%2520demonstrate%2520that%2520IPDreamer%2520effectively%2520generates%250Ahigh-quality%25203D%2520objects%2520that%2520are%2520consistent%2520with%2520both%2520the%2520provided%2520text%2520and%2520the%250Aappearance%2520of%2520complex%2520image%2520prompts%252C%2520demonstrating%2520its%2520promising%2520capability%2520in%250Aappearance-controllable%25203D%2520object%2520generation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zengbohan0217/IPDreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05375v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPDreamer%3A%20Appearance-Controllable%203D%20Object%20Generation%20with%20Complex%0A%20%20Image%20Prompts&entry.906535625=Bohan%20Zeng%20and%20Shanglin%20Li%20and%20Yutang%20Feng%20and%20Ling%20Yang%20and%20Hong%20Li%20and%20Sicheng%20Gao%20and%20Jiaming%20Liu%20and%20Conghui%20He%20and%20Wentao%20Zhang%20and%20Jianzhuang%20Liu%20and%20Baochang%20Zhang%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Recent%20advances%20in%203D%20generation%20have%20been%20remarkable%2C%20with%20methods%20such%20as%0ADreamFusion%20leveraging%20large-scale%20text-to-image%20diffusion-based%20models%20to%0Asupervise%203D%20object%20generation.%20These%20methods%20enable%20the%20synthesis%20of%20detailed%0Aand%20photorealistic%20textured%20objects.%20However%2C%20the%20appearance%20of%203D%20objects%0Aproduced%20by%20these%20text-to-3D%20models%20is%20unpredictable%2C%20and%20it%20is%20hard%20for%20the%0Asingle-image-to-3D%20methods%20to%20deal%20with%20complex%20images%2C%20thus%20posing%20a%20challenge%0Ain%20generating%20appearance-controllable%203D%20objects.%20To%20achieve%20controllable%0Acomplex%203D%20object%20synthesis%2C%20we%20propose%20IPDreamer%2C%20a%20novel%20approach%20that%0Aincorporates%20image%20prompt%20adaption%20to%20extract%20detailed%20and%20comprehensive%0Aappearance%20features%20from%20complex%20images%2C%20which%20are%20then%20utilized%20for%203D%20object%0Ageneration.%20Our%20results%20demonstrate%20that%20IPDreamer%20effectively%20generates%0Ahigh-quality%203D%20objects%20that%20are%20consistent%20with%20both%20the%20provided%20text%20and%20the%0Aappearance%20of%20complex%20image%20prompts%2C%20demonstrating%20its%20promising%20capability%20in%0Aappearance-controllable%203D%20object%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zengbohan0217/IPDreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05375v4&entry.124074799=Read"},
{"title": "Synergistic Global-space Camera and Human Reconstruction from Videos", "author": "Yizhou Zhao and Tuanfeng Y. Wang and Bhiksha Raj and Min Xu and Jimei Yang and Chun-Hao Paul Huang", "abstract": "  Remarkable strides have been made in reconstructing static scenes or human\nbodies from monocular videos. Yet, the two problems have largely been\napproached independently, without much synergy. Most visual SLAM methods can\nonly reconstruct camera trajectories and scene structures up to scale, while\nmost HMR methods reconstruct human meshes in metric scale but fall short in\nreasoning with cameras and scenes. This work introduces Synergistic Camera and\nHuman Reconstruction (SynCHMR) to marry the best of both worlds. Specifically,\nwe design Human-aware Metric SLAM to reconstruct metric-scale camera poses and\nscene point clouds using camera-frame HMR as a strong prior, addressing depth,\nscale, and dynamic ambiguities. Conditioning on the dense scene recovered, we\nfurther learn a Scene-aware SMPL Denoiser to enhance world-frame HMR by\nincorporating spatio-temporal coherency and dynamic scene constraints.\nTogether, they lead to consistent reconstructions of camera trajectories, human\nmeshes, and dense scene point clouds in a common world frame. Project page:\nhttps://paulchhuang.github.io/synchmr\n", "link": "http://arxiv.org/abs/2405.14855v1", "date": "2024-05-23", "relevancy": 3.0157, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6319}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergistic%20Global-space%20Camera%20and%20Human%20Reconstruction%20from%20Videos&body=Title%3A%20Synergistic%20Global-space%20Camera%20and%20Human%20Reconstruction%20from%20Videos%0AAuthor%3A%20Yizhou%20Zhao%20and%20Tuanfeng%20Y.%20Wang%20and%20Bhiksha%20Raj%20and%20Min%20Xu%20and%20Jimei%20Yang%20and%20Chun-Hao%20Paul%20Huang%0AAbstract%3A%20%20%20Remarkable%20strides%20have%20been%20made%20in%20reconstructing%20static%20scenes%20or%20human%0Abodies%20from%20monocular%20videos.%20Yet%2C%20the%20two%20problems%20have%20largely%20been%0Aapproached%20independently%2C%20without%20much%20synergy.%20Most%20visual%20SLAM%20methods%20can%0Aonly%20reconstruct%20camera%20trajectories%20and%20scene%20structures%20up%20to%20scale%2C%20while%0Amost%20HMR%20methods%20reconstruct%20human%20meshes%20in%20metric%20scale%20but%20fall%20short%20in%0Areasoning%20with%20cameras%20and%20scenes.%20This%20work%20introduces%20Synergistic%20Camera%20and%0AHuman%20Reconstruction%20%28SynCHMR%29%20to%20marry%20the%20best%20of%20both%20worlds.%20Specifically%2C%0Awe%20design%20Human-aware%20Metric%20SLAM%20to%20reconstruct%20metric-scale%20camera%20poses%20and%0Ascene%20point%20clouds%20using%20camera-frame%20HMR%20as%20a%20strong%20prior%2C%20addressing%20depth%2C%0Ascale%2C%20and%20dynamic%20ambiguities.%20Conditioning%20on%20the%20dense%20scene%20recovered%2C%20we%0Afurther%20learn%20a%20Scene-aware%20SMPL%20Denoiser%20to%20enhance%20world-frame%20HMR%20by%0Aincorporating%20spatio-temporal%20coherency%20and%20dynamic%20scene%20constraints.%0ATogether%2C%20they%20lead%20to%20consistent%20reconstructions%20of%20camera%20trajectories%2C%20human%0Ameshes%2C%20and%20dense%20scene%20point%20clouds%20in%20a%20common%20world%20frame.%20Project%20page%3A%0Ahttps%3A//paulchhuang.github.io/synchmr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergistic%2520Global-space%2520Camera%2520and%2520Human%2520Reconstruction%2520from%2520Videos%26entry.906535625%3DYizhou%2520Zhao%2520and%2520Tuanfeng%2520Y.%2520Wang%2520and%2520Bhiksha%2520Raj%2520and%2520Min%2520Xu%2520and%2520Jimei%2520Yang%2520and%2520Chun-Hao%2520Paul%2520Huang%26entry.1292438233%3D%2520%2520Remarkable%2520strides%2520have%2520been%2520made%2520in%2520reconstructing%2520static%2520scenes%2520or%2520human%250Abodies%2520from%2520monocular%2520videos.%2520Yet%252C%2520the%2520two%2520problems%2520have%2520largely%2520been%250Aapproached%2520independently%252C%2520without%2520much%2520synergy.%2520Most%2520visual%2520SLAM%2520methods%2520can%250Aonly%2520reconstruct%2520camera%2520trajectories%2520and%2520scene%2520structures%2520up%2520to%2520scale%252C%2520while%250Amost%2520HMR%2520methods%2520reconstruct%2520human%2520meshes%2520in%2520metric%2520scale%2520but%2520fall%2520short%2520in%250Areasoning%2520with%2520cameras%2520and%2520scenes.%2520This%2520work%2520introduces%2520Synergistic%2520Camera%2520and%250AHuman%2520Reconstruction%2520%2528SynCHMR%2529%2520to%2520marry%2520the%2520best%2520of%2520both%2520worlds.%2520Specifically%252C%250Awe%2520design%2520Human-aware%2520Metric%2520SLAM%2520to%2520reconstruct%2520metric-scale%2520camera%2520poses%2520and%250Ascene%2520point%2520clouds%2520using%2520camera-frame%2520HMR%2520as%2520a%2520strong%2520prior%252C%2520addressing%2520depth%252C%250Ascale%252C%2520and%2520dynamic%2520ambiguities.%2520Conditioning%2520on%2520the%2520dense%2520scene%2520recovered%252C%2520we%250Afurther%2520learn%2520a%2520Scene-aware%2520SMPL%2520Denoiser%2520to%2520enhance%2520world-frame%2520HMR%2520by%250Aincorporating%2520spatio-temporal%2520coherency%2520and%2520dynamic%2520scene%2520constraints.%250ATogether%252C%2520they%2520lead%2520to%2520consistent%2520reconstructions%2520of%2520camera%2520trajectories%252C%2520human%250Ameshes%252C%2520and%2520dense%2520scene%2520point%2520clouds%2520in%2520a%2520common%2520world%2520frame.%2520Project%2520page%253A%250Ahttps%253A//paulchhuang.github.io/synchmr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergistic%20Global-space%20Camera%20and%20Human%20Reconstruction%20from%20Videos&entry.906535625=Yizhou%20Zhao%20and%20Tuanfeng%20Y.%20Wang%20and%20Bhiksha%20Raj%20and%20Min%20Xu%20and%20Jimei%20Yang%20and%20Chun-Hao%20Paul%20Huang&entry.1292438233=%20%20Remarkable%20strides%20have%20been%20made%20in%20reconstructing%20static%20scenes%20or%20human%0Abodies%20from%20monocular%20videos.%20Yet%2C%20the%20two%20problems%20have%20largely%20been%0Aapproached%20independently%2C%20without%20much%20synergy.%20Most%20visual%20SLAM%20methods%20can%0Aonly%20reconstruct%20camera%20trajectories%20and%20scene%20structures%20up%20to%20scale%2C%20while%0Amost%20HMR%20methods%20reconstruct%20human%20meshes%20in%20metric%20scale%20but%20fall%20short%20in%0Areasoning%20with%20cameras%20and%20scenes.%20This%20work%20introduces%20Synergistic%20Camera%20and%0AHuman%20Reconstruction%20%28SynCHMR%29%20to%20marry%20the%20best%20of%20both%20worlds.%20Specifically%2C%0Awe%20design%20Human-aware%20Metric%20SLAM%20to%20reconstruct%20metric-scale%20camera%20poses%20and%0Ascene%20point%20clouds%20using%20camera-frame%20HMR%20as%20a%20strong%20prior%2C%20addressing%20depth%2C%0Ascale%2C%20and%20dynamic%20ambiguities.%20Conditioning%20on%20the%20dense%20scene%20recovered%2C%20we%0Afurther%20learn%20a%20Scene-aware%20SMPL%20Denoiser%20to%20enhance%20world-frame%20HMR%20by%0Aincorporating%20spatio-temporal%20coherency%20and%20dynamic%20scene%20constraints.%0ATogether%2C%20they%20lead%20to%20consistent%20reconstructions%20of%20camera%20trajectories%2C%20human%0Ameshes%2C%20and%20dense%20scene%20point%20clouds%20in%20a%20common%20world%20frame.%20Project%20page%3A%0Ahttps%3A//paulchhuang.github.io/synchmr%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14855v1&entry.124074799=Read"},
{"title": "TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing", "author": "Teng Xu and Jiamin Chen and Peng Chen and Youjia Zhang and Junqing Yu and Wei Yang", "abstract": "  Editing objects within a scene is a critical functionality required across a\nbroad spectrum of applications in computer vision and graphics. As 3D Gaussian\nSplatting (3DGS) emerges as a frontier in scene representation, the effective\nmodification of 3D Gaussian scenes has become increasingly vital. This process\nentails accurately retrieve the target objects and subsequently performing\nmodifications based on instructions. Though available in pieces, existing\ntechniques mainly embed sparse semantics into Gaussians for retrieval, and rely\non an iterative dataset update paradigm for editing, leading to over-smoothing\nor inconsistency issues. To this end, this paper proposes a systematic\napproach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and\nediting. In contrast to the top-down language grounding approach for 3D\nGaussians, we adopt a bottom-up language aggregation strategy to generate a\ndenser language embedded 3D Gaussians that supports open-vocabulary retrieval.\nTo overcome the over-smoothing and inconsistency issues in editing, we propose\na Coherent Score Distillation (CSD) that aggregates a 2D image editing\ndiffusion model and a multi-view diffusion model for score distillation,\nproducing multi-view consistent editing with much finer details. In various\nexperiments, we demonstrate that our TIGER is able to accomplish more\nconsistent and realistic edits than prior work.\n", "link": "http://arxiv.org/abs/2405.14455v1", "date": "2024-05-23", "relevancy": 3.0017, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6281}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6206}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIGER%3A%20Text-Instructed%203D%20Gaussian%20Retrieval%20and%20Coherent%20Editing&body=Title%3A%20TIGER%3A%20Text-Instructed%203D%20Gaussian%20Retrieval%20and%20Coherent%20Editing%0AAuthor%3A%20Teng%20Xu%20and%20Jiamin%20Chen%20and%20Peng%20Chen%20and%20Youjia%20Zhang%20and%20Junqing%20Yu%20and%20Wei%20Yang%0AAbstract%3A%20%20%20Editing%20objects%20within%20a%20scene%20is%20a%20critical%20functionality%20required%20across%20a%0Abroad%20spectrum%20of%20applications%20in%20computer%20vision%20and%20graphics.%20As%203D%20Gaussian%0ASplatting%20%283DGS%29%20emerges%20as%20a%20frontier%20in%20scene%20representation%2C%20the%20effective%0Amodification%20of%203D%20Gaussian%20scenes%20has%20become%20increasingly%20vital.%20This%20process%0Aentails%20accurately%20retrieve%20the%20target%20objects%20and%20subsequently%20performing%0Amodifications%20based%20on%20instructions.%20Though%20available%20in%20pieces%2C%20existing%0Atechniques%20mainly%20embed%20sparse%20semantics%20into%20Gaussians%20for%20retrieval%2C%20and%20rely%0Aon%20an%20iterative%20dataset%20update%20paradigm%20for%20editing%2C%20leading%20to%20over-smoothing%0Aor%20inconsistency%20issues.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20systematic%0Aapproach%2C%20namely%20TIGER%2C%20for%20coherent%20text-instructed%203D%20Gaussian%20retrieval%20and%0Aediting.%20In%20contrast%20to%20the%20top-down%20language%20grounding%20approach%20for%203D%0AGaussians%2C%20we%20adopt%20a%20bottom-up%20language%20aggregation%20strategy%20to%20generate%20a%0Adenser%20language%20embedded%203D%20Gaussians%20that%20supports%20open-vocabulary%20retrieval.%0ATo%20overcome%20the%20over-smoothing%20and%20inconsistency%20issues%20in%20editing%2C%20we%20propose%0Aa%20Coherent%20Score%20Distillation%20%28CSD%29%20that%20aggregates%20a%202D%20image%20editing%0Adiffusion%20model%20and%20a%20multi-view%20diffusion%20model%20for%20score%20distillation%2C%0Aproducing%20multi-view%20consistent%20editing%20with%20much%20finer%20details.%20In%20various%0Aexperiments%2C%20we%20demonstrate%20that%20our%20TIGER%20is%20able%20to%20accomplish%20more%0Aconsistent%20and%20realistic%20edits%20than%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIGER%253A%2520Text-Instructed%25203D%2520Gaussian%2520Retrieval%2520and%2520Coherent%2520Editing%26entry.906535625%3DTeng%2520Xu%2520and%2520Jiamin%2520Chen%2520and%2520Peng%2520Chen%2520and%2520Youjia%2520Zhang%2520and%2520Junqing%2520Yu%2520and%2520Wei%2520Yang%26entry.1292438233%3D%2520%2520Editing%2520objects%2520within%2520a%2520scene%2520is%2520a%2520critical%2520functionality%2520required%2520across%2520a%250Abroad%2520spectrum%2520of%2520applications%2520in%2520computer%2520vision%2520and%2520graphics.%2520As%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520emerges%2520as%2520a%2520frontier%2520in%2520scene%2520representation%252C%2520the%2520effective%250Amodification%2520of%25203D%2520Gaussian%2520scenes%2520has%2520become%2520increasingly%2520vital.%2520This%2520process%250Aentails%2520accurately%2520retrieve%2520the%2520target%2520objects%2520and%2520subsequently%2520performing%250Amodifications%2520based%2520on%2520instructions.%2520Though%2520available%2520in%2520pieces%252C%2520existing%250Atechniques%2520mainly%2520embed%2520sparse%2520semantics%2520into%2520Gaussians%2520for%2520retrieval%252C%2520and%2520rely%250Aon%2520an%2520iterative%2520dataset%2520update%2520paradigm%2520for%2520editing%252C%2520leading%2520to%2520over-smoothing%250Aor%2520inconsistency%2520issues.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520systematic%250Aapproach%252C%2520namely%2520TIGER%252C%2520for%2520coherent%2520text-instructed%25203D%2520Gaussian%2520retrieval%2520and%250Aediting.%2520In%2520contrast%2520to%2520the%2520top-down%2520language%2520grounding%2520approach%2520for%25203D%250AGaussians%252C%2520we%2520adopt%2520a%2520bottom-up%2520language%2520aggregation%2520strategy%2520to%2520generate%2520a%250Adenser%2520language%2520embedded%25203D%2520Gaussians%2520that%2520supports%2520open-vocabulary%2520retrieval.%250ATo%2520overcome%2520the%2520over-smoothing%2520and%2520inconsistency%2520issues%2520in%2520editing%252C%2520we%2520propose%250Aa%2520Coherent%2520Score%2520Distillation%2520%2528CSD%2529%2520that%2520aggregates%2520a%25202D%2520image%2520editing%250Adiffusion%2520model%2520and%2520a%2520multi-view%2520diffusion%2520model%2520for%2520score%2520distillation%252C%250Aproducing%2520multi-view%2520consistent%2520editing%2520with%2520much%2520finer%2520details.%2520In%2520various%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520TIGER%2520is%2520able%2520to%2520accomplish%2520more%250Aconsistent%2520and%2520realistic%2520edits%2520than%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIGER%3A%20Text-Instructed%203D%20Gaussian%20Retrieval%20and%20Coherent%20Editing&entry.906535625=Teng%20Xu%20and%20Jiamin%20Chen%20and%20Peng%20Chen%20and%20Youjia%20Zhang%20and%20Junqing%20Yu%20and%20Wei%20Yang&entry.1292438233=%20%20Editing%20objects%20within%20a%20scene%20is%20a%20critical%20functionality%20required%20across%20a%0Abroad%20spectrum%20of%20applications%20in%20computer%20vision%20and%20graphics.%20As%203D%20Gaussian%0ASplatting%20%283DGS%29%20emerges%20as%20a%20frontier%20in%20scene%20representation%2C%20the%20effective%0Amodification%20of%203D%20Gaussian%20scenes%20has%20become%20increasingly%20vital.%20This%20process%0Aentails%20accurately%20retrieve%20the%20target%20objects%20and%20subsequently%20performing%0Amodifications%20based%20on%20instructions.%20Though%20available%20in%20pieces%2C%20existing%0Atechniques%20mainly%20embed%20sparse%20semantics%20into%20Gaussians%20for%20retrieval%2C%20and%20rely%0Aon%20an%20iterative%20dataset%20update%20paradigm%20for%20editing%2C%20leading%20to%20over-smoothing%0Aor%20inconsistency%20issues.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20systematic%0Aapproach%2C%20namely%20TIGER%2C%20for%20coherent%20text-instructed%203D%20Gaussian%20retrieval%20and%0Aediting.%20In%20contrast%20to%20the%20top-down%20language%20grounding%20approach%20for%203D%0AGaussians%2C%20we%20adopt%20a%20bottom-up%20language%20aggregation%20strategy%20to%20generate%20a%0Adenser%20language%20embedded%203D%20Gaussians%20that%20supports%20open-vocabulary%20retrieval.%0ATo%20overcome%20the%20over-smoothing%20and%20inconsistency%20issues%20in%20editing%2C%20we%20propose%0Aa%20Coherent%20Score%20Distillation%20%28CSD%29%20that%20aggregates%20a%202D%20image%20editing%0Adiffusion%20model%20and%20a%20multi-view%20diffusion%20model%20for%20score%20distillation%2C%0Aproducing%20multi-view%20consistent%20editing%20with%20much%20finer%20details.%20In%20various%0Aexperiments%2C%20we%20demonstrate%20that%20our%20TIGER%20is%20able%20to%20accomplish%20more%0Aconsistent%20and%20realistic%20edits%20than%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14455v1&entry.124074799=Read"},
{"title": "G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n  Using Large Multi-Modality Models", "author": "Pengyue Jia and Yiding Liu and Xiaopeng Li and Xiangyu Zhao and Yuhao Wang and Yantong Du and Xiao Han and Xuetao Wei and Shuaiqiang Wang and Dawei Yin", "abstract": "  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.14702v1", "date": "2024-05-23", "relevancy": 2.9339, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.624}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G3%3A%20An%20Effective%20and%20Adaptive%20Framework%20for%20Worldwide%20Geolocalization%0A%20%20Using%20Large%20Multi-Modality%20Models&body=Title%3A%20G3%3A%20An%20Effective%20and%20Adaptive%20Framework%20for%20Worldwide%20Geolocalization%0A%20%20Using%20Large%20Multi-Modality%20Models%0AAuthor%3A%20Pengyue%20Jia%20and%20Yiding%20Liu%20and%20Xiaopeng%20Li%20and%20Xiangyu%20Zhao%20and%20Yuhao%20Wang%20and%20Yantong%20Du%20and%20Xiao%20Han%20and%20Xuetao%20Wei%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%0AAbstract%3A%20%20%20Worldwide%20geolocalization%20aims%20to%20locate%20the%20precise%20location%20at%20the%0Acoordinate%20level%20of%20photos%20taken%20anywhere%20on%20the%20Earth.%20It%20is%20very%20challenging%0Adue%20to%201%29%20the%20difficulty%20of%20capturing%20subtle%20location-aware%20visual%20semantics%2C%0Aand%202%29%20the%20heterogeneous%20geographical%20distribution%20of%20image%20data.%20As%20a%20result%2C%0Aexisting%20studies%20have%20clear%20limitations%20when%20scaled%20to%20a%20worldwide%20context.%0AThey%20may%20easily%20confuse%20distant%20images%20with%20similar%20visual%20contents%2C%20or%20cannot%0Aadapt%20to%20various%20locations%20worldwide%20with%20different%20amounts%20of%20relevant%20data.%0ATo%20resolve%20these%20limitations%2C%20we%20propose%20G3%2C%20a%20novel%20framework%20based%20on%0ARetrieval-Augmented%20Generation%20%28RAG%29.%20In%20particular%2C%20G3%20consists%20of%20three%0Asteps%2C%20i.e.%2C%20Geo-alignment%2C%20Geo-diversification%2C%20and%20Geo-verification%20to%0Aoptimize%20both%20retrieval%20and%20generation%20phases%20of%20worldwide%20geolocalization.%0ADuring%20Geo-alignment%2C%20our%20solution%20jointly%20learns%20expressive%20multi-modal%0Arepresentations%20for%20images%2C%20GPS%20and%20textual%20descriptions%2C%20which%20allows%20us%20to%0Acapture%20location-aware%20semantics%20for%20retrieving%20nearby%20images%20for%20a%20given%0Aquery.%20During%20Geo-diversification%2C%20we%20leverage%20a%20prompt%20ensembling%20method%20that%0Ais%20robust%20to%20inconsistent%20retrieval%20performance%20for%20different%20image%20queries.%0AFinally%2C%20we%20combine%20both%20retrieved%20and%20generated%20GPS%20candidates%20in%0AGeo-verification%20for%20location%20prediction.%20Experiments%20on%20two%20well-established%0Adatasets%20IM2GPS3k%20and%20YFCC4k%20verify%20the%20superiority%20of%20G3%20compared%20to%20other%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG3%253A%2520An%2520Effective%2520and%2520Adaptive%2520Framework%2520for%2520Worldwide%2520Geolocalization%250A%2520%2520Using%2520Large%2520Multi-Modality%2520Models%26entry.906535625%3DPengyue%2520Jia%2520and%2520Yiding%2520Liu%2520and%2520Xiaopeng%2520Li%2520and%2520Xiangyu%2520Zhao%2520and%2520Yuhao%2520Wang%2520and%2520Yantong%2520Du%2520and%2520Xiao%2520Han%2520and%2520Xuetao%2520Wei%2520and%2520Shuaiqiang%2520Wang%2520and%2520Dawei%2520Yin%26entry.1292438233%3D%2520%2520Worldwide%2520geolocalization%2520aims%2520to%2520locate%2520the%2520precise%2520location%2520at%2520the%250Acoordinate%2520level%2520of%2520photos%2520taken%2520anywhere%2520on%2520the%2520Earth.%2520It%2520is%2520very%2520challenging%250Adue%2520to%25201%2529%2520the%2520difficulty%2520of%2520capturing%2520subtle%2520location-aware%2520visual%2520semantics%252C%250Aand%25202%2529%2520the%2520heterogeneous%2520geographical%2520distribution%2520of%2520image%2520data.%2520As%2520a%2520result%252C%250Aexisting%2520studies%2520have%2520clear%2520limitations%2520when%2520scaled%2520to%2520a%2520worldwide%2520context.%250AThey%2520may%2520easily%2520confuse%2520distant%2520images%2520with%2520similar%2520visual%2520contents%252C%2520or%2520cannot%250Aadapt%2520to%2520various%2520locations%2520worldwide%2520with%2520different%2520amounts%2520of%2520relevant%2520data.%250ATo%2520resolve%2520these%2520limitations%252C%2520we%2520propose%2520G3%252C%2520a%2520novel%2520framework%2520based%2520on%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520In%2520particular%252C%2520G3%2520consists%2520of%2520three%250Asteps%252C%2520i.e.%252C%2520Geo-alignment%252C%2520Geo-diversification%252C%2520and%2520Geo-verification%2520to%250Aoptimize%2520both%2520retrieval%2520and%2520generation%2520phases%2520of%2520worldwide%2520geolocalization.%250ADuring%2520Geo-alignment%252C%2520our%2520solution%2520jointly%2520learns%2520expressive%2520multi-modal%250Arepresentations%2520for%2520images%252C%2520GPS%2520and%2520textual%2520descriptions%252C%2520which%2520allows%2520us%2520to%250Acapture%2520location-aware%2520semantics%2520for%2520retrieving%2520nearby%2520images%2520for%2520a%2520given%250Aquery.%2520During%2520Geo-diversification%252C%2520we%2520leverage%2520a%2520prompt%2520ensembling%2520method%2520that%250Ais%2520robust%2520to%2520inconsistent%2520retrieval%2520performance%2520for%2520different%2520image%2520queries.%250AFinally%252C%2520we%2520combine%2520both%2520retrieved%2520and%2520generated%2520GPS%2520candidates%2520in%250AGeo-verification%2520for%2520location%2520prediction.%2520Experiments%2520on%2520two%2520well-established%250Adatasets%2520IM2GPS3k%2520and%2520YFCC4k%2520verify%2520the%2520superiority%2520of%2520G3%2520compared%2520to%2520other%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3%3A%20An%20Effective%20and%20Adaptive%20Framework%20for%20Worldwide%20Geolocalization%0A%20%20Using%20Large%20Multi-Modality%20Models&entry.906535625=Pengyue%20Jia%20and%20Yiding%20Liu%20and%20Xiaopeng%20Li%20and%20Xiangyu%20Zhao%20and%20Yuhao%20Wang%20and%20Yantong%20Du%20and%20Xiao%20Han%20and%20Xuetao%20Wei%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin&entry.1292438233=%20%20Worldwide%20geolocalization%20aims%20to%20locate%20the%20precise%20location%20at%20the%0Acoordinate%20level%20of%20photos%20taken%20anywhere%20on%20the%20Earth.%20It%20is%20very%20challenging%0Adue%20to%201%29%20the%20difficulty%20of%20capturing%20subtle%20location-aware%20visual%20semantics%2C%0Aand%202%29%20the%20heterogeneous%20geographical%20distribution%20of%20image%20data.%20As%20a%20result%2C%0Aexisting%20studies%20have%20clear%20limitations%20when%20scaled%20to%20a%20worldwide%20context.%0AThey%20may%20easily%20confuse%20distant%20images%20with%20similar%20visual%20contents%2C%20or%20cannot%0Aadapt%20to%20various%20locations%20worldwide%20with%20different%20amounts%20of%20relevant%20data.%0ATo%20resolve%20these%20limitations%2C%20we%20propose%20G3%2C%20a%20novel%20framework%20based%20on%0ARetrieval-Augmented%20Generation%20%28RAG%29.%20In%20particular%2C%20G3%20consists%20of%20three%0Asteps%2C%20i.e.%2C%20Geo-alignment%2C%20Geo-diversification%2C%20and%20Geo-verification%20to%0Aoptimize%20both%20retrieval%20and%20generation%20phases%20of%20worldwide%20geolocalization.%0ADuring%20Geo-alignment%2C%20our%20solution%20jointly%20learns%20expressive%20multi-modal%0Arepresentations%20for%20images%2C%20GPS%20and%20textual%20descriptions%2C%20which%20allows%20us%20to%0Acapture%20location-aware%20semantics%20for%20retrieving%20nearby%20images%20for%20a%20given%0Aquery.%20During%20Geo-diversification%2C%20we%20leverage%20a%20prompt%20ensembling%20method%20that%0Ais%20robust%20to%20inconsistent%20retrieval%20performance%20for%20different%20image%20queries.%0AFinally%2C%20we%20combine%20both%20retrieved%20and%20generated%20GPS%20candidates%20in%0AGeo-verification%20for%20location%20prediction.%20Experiments%20on%20two%20well-established%0Adatasets%20IM2GPS3k%20and%20YFCC4k%20verify%20the%20superiority%20of%20G3%20compared%20to%20other%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14702v1&entry.124074799=Read"},
{"title": "DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation", "author": "Jinxin Liu and Xinghong Guo and Zifeng Zhuang and Donglin Wang", "abstract": "  In this paper, we propose a novel approach called DIffusion-guided DIversity\n(DIDI) for offline behavioral generation. The goal of DIDI is to learn a\ndiverse set of skills from a mixture of label-free offline data. We achieve\nthis by leveraging diffusion probabilistic models as priors to guide the\nlearning process and regularize the policy. By optimizing a joint objective\nthat incorporates diversity and diffusion-guided regularization, we encourage\nthe emergence of diverse behaviors while maintaining the similarity to the\noffline data. Experimental results in four decision-making domains (Push,\nKitchen, Humanoid, and D4RL tasks) show that DIDI is effective in discovering\ndiverse and discriminative skills. We also introduce skill stitching and skill\ninterpolation, which highlight the generalist nature of the learned skill\nspace. Further, by incorporating an extrinsic reward function, DIDI enables\nreward-guided behavior generation, facilitating the learning of diverse and\noptimal behaviors from sub-optimal data.\n", "link": "http://arxiv.org/abs/2405.14790v1", "date": "2024-05-23", "relevancy": 2.8278, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5815}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5696}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIDI%3A%20Diffusion-Guided%20Diversity%20for%20Offline%20Behavioral%20Generation&body=Title%3A%20DIDI%3A%20Diffusion-Guided%20Diversity%20for%20Offline%20Behavioral%20Generation%0AAuthor%3A%20Jinxin%20Liu%20and%20Xinghong%20Guo%20and%20Zifeng%20Zhuang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20DIffusion-guided%20DIversity%0A%28DIDI%29%20for%20offline%20behavioral%20generation.%20The%20goal%20of%20DIDI%20is%20to%20learn%20a%0Adiverse%20set%20of%20skills%20from%20a%20mixture%20of%20label-free%20offline%20data.%20We%20achieve%0Athis%20by%20leveraging%20diffusion%20probabilistic%20models%20as%20priors%20to%20guide%20the%0Alearning%20process%20and%20regularize%20the%20policy.%20By%20optimizing%20a%20joint%20objective%0Athat%20incorporates%20diversity%20and%20diffusion-guided%20regularization%2C%20we%20encourage%0Athe%20emergence%20of%20diverse%20behaviors%20while%20maintaining%20the%20similarity%20to%20the%0Aoffline%20data.%20Experimental%20results%20in%20four%20decision-making%20domains%20%28Push%2C%0AKitchen%2C%20Humanoid%2C%20and%20D4RL%20tasks%29%20show%20that%20DIDI%20is%20effective%20in%20discovering%0Adiverse%20and%20discriminative%20skills.%20We%20also%20introduce%20skill%20stitching%20and%20skill%0Ainterpolation%2C%20which%20highlight%20the%20generalist%20nature%20of%20the%20learned%20skill%0Aspace.%20Further%2C%20by%20incorporating%20an%20extrinsic%20reward%20function%2C%20DIDI%20enables%0Areward-guided%20behavior%20generation%2C%20facilitating%20the%20learning%20of%20diverse%20and%0Aoptimal%20behaviors%20from%20sub-optimal%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIDI%253A%2520Diffusion-Guided%2520Diversity%2520for%2520Offline%2520Behavioral%2520Generation%26entry.906535625%3DJinxin%2520Liu%2520and%2520Xinghong%2520Guo%2520and%2520Zifeng%2520Zhuang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520called%2520DIffusion-guided%2520DIversity%250A%2528DIDI%2529%2520for%2520offline%2520behavioral%2520generation.%2520The%2520goal%2520of%2520DIDI%2520is%2520to%2520learn%2520a%250Adiverse%2520set%2520of%2520skills%2520from%2520a%2520mixture%2520of%2520label-free%2520offline%2520data.%2520We%2520achieve%250Athis%2520by%2520leveraging%2520diffusion%2520probabilistic%2520models%2520as%2520priors%2520to%2520guide%2520the%250Alearning%2520process%2520and%2520regularize%2520the%2520policy.%2520By%2520optimizing%2520a%2520joint%2520objective%250Athat%2520incorporates%2520diversity%2520and%2520diffusion-guided%2520regularization%252C%2520we%2520encourage%250Athe%2520emergence%2520of%2520diverse%2520behaviors%2520while%2520maintaining%2520the%2520similarity%2520to%2520the%250Aoffline%2520data.%2520Experimental%2520results%2520in%2520four%2520decision-making%2520domains%2520%2528Push%252C%250AKitchen%252C%2520Humanoid%252C%2520and%2520D4RL%2520tasks%2529%2520show%2520that%2520DIDI%2520is%2520effective%2520in%2520discovering%250Adiverse%2520and%2520discriminative%2520skills.%2520We%2520also%2520introduce%2520skill%2520stitching%2520and%2520skill%250Ainterpolation%252C%2520which%2520highlight%2520the%2520generalist%2520nature%2520of%2520the%2520learned%2520skill%250Aspace.%2520Further%252C%2520by%2520incorporating%2520an%2520extrinsic%2520reward%2520function%252C%2520DIDI%2520enables%250Areward-guided%2520behavior%2520generation%252C%2520facilitating%2520the%2520learning%2520of%2520diverse%2520and%250Aoptimal%2520behaviors%2520from%2520sub-optimal%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIDI%3A%20Diffusion-Guided%20Diversity%20for%20Offline%20Behavioral%20Generation&entry.906535625=Jinxin%20Liu%20and%20Xinghong%20Guo%20and%20Zifeng%20Zhuang%20and%20Donglin%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20called%20DIffusion-guided%20DIversity%0A%28DIDI%29%20for%20offline%20behavioral%20generation.%20The%20goal%20of%20DIDI%20is%20to%20learn%20a%0Adiverse%20set%20of%20skills%20from%20a%20mixture%20of%20label-free%20offline%20data.%20We%20achieve%0Athis%20by%20leveraging%20diffusion%20probabilistic%20models%20as%20priors%20to%20guide%20the%0Alearning%20process%20and%20regularize%20the%20policy.%20By%20optimizing%20a%20joint%20objective%0Athat%20incorporates%20diversity%20and%20diffusion-guided%20regularization%2C%20we%20encourage%0Athe%20emergence%20of%20diverse%20behaviors%20while%20maintaining%20the%20similarity%20to%20the%0Aoffline%20data.%20Experimental%20results%20in%20four%20decision-making%20domains%20%28Push%2C%0AKitchen%2C%20Humanoid%2C%20and%20D4RL%20tasks%29%20show%20that%20DIDI%20is%20effective%20in%20discovering%0Adiverse%20and%20discriminative%20skills.%20We%20also%20introduce%20skill%20stitching%20and%20skill%0Ainterpolation%2C%20which%20highlight%20the%20generalist%20nature%20of%20the%20learned%20skill%0Aspace.%20Further%2C%20by%20incorporating%20an%20extrinsic%20reward%20function%2C%20DIDI%20enables%0Areward-guided%20behavior%20generation%2C%20facilitating%20the%20learning%20of%20diverse%20and%0Aoptimal%20behaviors%20from%20sub-optimal%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14790v1&entry.124074799=Read"},
{"title": "Camera Relocalization in Shadow-free Neural Radiance Fields", "author": "Shiyao Xu and Caiyun Liu and Yuantao Chen and Zhenxin Zhu and Zike Yan and Yongliang Shi and Hao Zhao and Guyue Zhou", "abstract": "  Camera relocalization is a crucial problem in computer vision and robotics.\nRecent advancements in neural radiance fields (NeRFs) have shown promise in\nsynthesizing photo-realistic images. Several works have utilized NeRFs for\nrefining camera poses, but they do not account for lighting changes that can\naffect scene appearance and shadow regions, causing a degraded pose\noptimization process. In this paper, we propose a two-staged pipeline that\nnormalizes images with varying lighting and shadow conditions to improve camera\nrelocalization. We implement our scene representation upon a hash-encoded NeRF\nwhich significantly boosts up the pose optimization process. To account for the\nnoisy image gradient computing problem in grid-based NeRFs, we further propose\na re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient\naveraging technique to smoothen the process. Experimental results on several\ndatasets with varying lighting conditions demonstrate that our method achieves\nstate-of-the-art results in camera relocalization under varying lighting\nconditions. Code and data will be made publicly available.\n", "link": "http://arxiv.org/abs/2405.14824v1", "date": "2024-05-23", "relevancy": 2.8145, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5796}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5547}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Camera%20Relocalization%20in%20Shadow-free%20Neural%20Radiance%20Fields&body=Title%3A%20Camera%20Relocalization%20in%20Shadow-free%20Neural%20Radiance%20Fields%0AAuthor%3A%20Shiyao%20Xu%20and%20Caiyun%20Liu%20and%20Yuantao%20Chen%20and%20Zhenxin%20Zhu%20and%20Zike%20Yan%20and%20Yongliang%20Shi%20and%20Hao%20Zhao%20and%20Guyue%20Zhou%0AAbstract%3A%20%20%20Camera%20relocalization%20is%20a%20crucial%20problem%20in%20computer%20vision%20and%20robotics.%0ARecent%20advancements%20in%20neural%20radiance%20fields%20%28NeRFs%29%20have%20shown%20promise%20in%0Asynthesizing%20photo-realistic%20images.%20Several%20works%20have%20utilized%20NeRFs%20for%0Arefining%20camera%20poses%2C%20but%20they%20do%20not%20account%20for%20lighting%20changes%20that%20can%0Aaffect%20scene%20appearance%20and%20shadow%20regions%2C%20causing%20a%20degraded%20pose%0Aoptimization%20process.%20In%20this%20paper%2C%20we%20propose%20a%20two-staged%20pipeline%20that%0Anormalizes%20images%20with%20varying%20lighting%20and%20shadow%20conditions%20to%20improve%20camera%0Arelocalization.%20We%20implement%20our%20scene%20representation%20upon%20a%20hash-encoded%20NeRF%0Awhich%20significantly%20boosts%20up%20the%20pose%20optimization%20process.%20To%20account%20for%20the%0Anoisy%20image%20gradient%20computing%20problem%20in%20grid-based%20NeRFs%2C%20we%20further%20propose%0Aa%20re-devised%20truncated%20dynamic%20low-pass%20filter%20%28TDLF%29%20and%20a%20numerical%20gradient%0Aaveraging%20technique%20to%20smoothen%20the%20process.%20Experimental%20results%20on%20several%0Adatasets%20with%20varying%20lighting%20conditions%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%20in%20camera%20relocalization%20under%20varying%20lighting%0Aconditions.%20Code%20and%20data%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCamera%2520Relocalization%2520in%2520Shadow-free%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DShiyao%2520Xu%2520and%2520Caiyun%2520Liu%2520and%2520Yuantao%2520Chen%2520and%2520Zhenxin%2520Zhu%2520and%2520Zike%2520Yan%2520and%2520Yongliang%2520Shi%2520and%2520Hao%2520Zhao%2520and%2520Guyue%2520Zhou%26entry.1292438233%3D%2520%2520Camera%2520relocalization%2520is%2520a%2520crucial%2520problem%2520in%2520computer%2520vision%2520and%2520robotics.%250ARecent%2520advancements%2520in%2520neural%2520radiance%2520fields%2520%2528NeRFs%2529%2520have%2520shown%2520promise%2520in%250Asynthesizing%2520photo-realistic%2520images.%2520Several%2520works%2520have%2520utilized%2520NeRFs%2520for%250Arefining%2520camera%2520poses%252C%2520but%2520they%2520do%2520not%2520account%2520for%2520lighting%2520changes%2520that%2520can%250Aaffect%2520scene%2520appearance%2520and%2520shadow%2520regions%252C%2520causing%2520a%2520degraded%2520pose%250Aoptimization%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520two-staged%2520pipeline%2520that%250Anormalizes%2520images%2520with%2520varying%2520lighting%2520and%2520shadow%2520conditions%2520to%2520improve%2520camera%250Arelocalization.%2520We%2520implement%2520our%2520scene%2520representation%2520upon%2520a%2520hash-encoded%2520NeRF%250Awhich%2520significantly%2520boosts%2520up%2520the%2520pose%2520optimization%2520process.%2520To%2520account%2520for%2520the%250Anoisy%2520image%2520gradient%2520computing%2520problem%2520in%2520grid-based%2520NeRFs%252C%2520we%2520further%2520propose%250Aa%2520re-devised%2520truncated%2520dynamic%2520low-pass%2520filter%2520%2528TDLF%2529%2520and%2520a%2520numerical%2520gradient%250Aaveraging%2520technique%2520to%2520smoothen%2520the%2520process.%2520Experimental%2520results%2520on%2520several%250Adatasets%2520with%2520varying%2520lighting%2520conditions%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520results%2520in%2520camera%2520relocalization%2520under%2520varying%2520lighting%250Aconditions.%2520Code%2520and%2520data%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Camera%20Relocalization%20in%20Shadow-free%20Neural%20Radiance%20Fields&entry.906535625=Shiyao%20Xu%20and%20Caiyun%20Liu%20and%20Yuantao%20Chen%20and%20Zhenxin%20Zhu%20and%20Zike%20Yan%20and%20Yongliang%20Shi%20and%20Hao%20Zhao%20and%20Guyue%20Zhou&entry.1292438233=%20%20Camera%20relocalization%20is%20a%20crucial%20problem%20in%20computer%20vision%20and%20robotics.%0ARecent%20advancements%20in%20neural%20radiance%20fields%20%28NeRFs%29%20have%20shown%20promise%20in%0Asynthesizing%20photo-realistic%20images.%20Several%20works%20have%20utilized%20NeRFs%20for%0Arefining%20camera%20poses%2C%20but%20they%20do%20not%20account%20for%20lighting%20changes%20that%20can%0Aaffect%20scene%20appearance%20and%20shadow%20regions%2C%20causing%20a%20degraded%20pose%0Aoptimization%20process.%20In%20this%20paper%2C%20we%20propose%20a%20two-staged%20pipeline%20that%0Anormalizes%20images%20with%20varying%20lighting%20and%20shadow%20conditions%20to%20improve%20camera%0Arelocalization.%20We%20implement%20our%20scene%20representation%20upon%20a%20hash-encoded%20NeRF%0Awhich%20significantly%20boosts%20up%20the%20pose%20optimization%20process.%20To%20account%20for%20the%0Anoisy%20image%20gradient%20computing%20problem%20in%20grid-based%20NeRFs%2C%20we%20further%20propose%0Aa%20re-devised%20truncated%20dynamic%20low-pass%20filter%20%28TDLF%29%20and%20a%20numerical%20gradient%0Aaveraging%20technique%20to%20smoothen%20the%20process.%20Experimental%20results%20on%20several%0Adatasets%20with%20varying%20lighting%20conditions%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%20in%20camera%20relocalization%20under%20varying%20lighting%0Aconditions.%20Code%20and%20data%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14824v1&entry.124074799=Read"},
{"title": "Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning\n  for Dynamic Graph Neural Networks", "author": "He Zhang and Bang Wu and Xiangwen Yang and Xingliang Yuan and Chengqi Zhang and Shirui Pan", "abstract": "  Graph unlearning has emerged as an essential tool for safeguarding user\nprivacy and mitigating the negative impacts of undesirable data. Meanwhile, the\nadvent of dynamic graph neural networks (DGNNs) marks a significant advancement\ndue to their superior capability in learning from dynamic graphs, which\nencapsulate spatial-temporal variations in diverse real-world applications\n(e.g., traffic forecasting). With the increasing prevalence of DGNNs, it\nbecomes imperative to investigate the implementation of dynamic graph\nunlearning. However, current graph unlearning methodologies are designed for\nGNNs operating on static graphs and exhibit limitations including their serving\nin a pre-processing manner and impractical resource demands. Furthermore, the\nadaptation of these methods to DGNNs presents non-trivial challenges, owing to\nthe distinctive nature of dynamic graphs. To this end, we propose an effective,\nefficient, model-agnostic, and post-processing method to implement DGNN\nunlearning. Specifically, we first define the unlearning requests and formulate\ndynamic graph unlearning in the context of continuous-time dynamic graphs.\nAfter conducting a role analysis on the unlearning data, the remaining data,\nand the target DGNN model, we propose a method called Gradient Transformation\nand a loss function to map the unlearning request to the desired parameter\nupdate. Evaluations on six real-world datasets and state-of-the-art DGNN\nbackbones demonstrate its effectiveness (e.g., limited performance drop even\nobvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up)\noutperformance, and potential advantages in handling future unlearning requests\n(e.g., at most 32.59$\\times$ speed-up).\n", "link": "http://arxiv.org/abs/2405.14407v1", "date": "2024-05-23", "relevancy": 2.8134, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5916}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5507}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Transformation%3A%20Towards%20Efficient%20and%20Model-Agnostic%20Unlearning%0A%20%20for%20Dynamic%20Graph%20Neural%20Networks&body=Title%3A%20Gradient%20Transformation%3A%20Towards%20Efficient%20and%20Model-Agnostic%20Unlearning%0A%20%20for%20Dynamic%20Graph%20Neural%20Networks%0AAuthor%3A%20He%20Zhang%20and%20Bang%20Wu%20and%20Xiangwen%20Yang%20and%20Xingliang%20Yuan%20and%20Chengqi%20Zhang%20and%20Shirui%20Pan%0AAbstract%3A%20%20%20Graph%20unlearning%20has%20emerged%20as%20an%20essential%20tool%20for%20safeguarding%20user%0Aprivacy%20and%20mitigating%20the%20negative%20impacts%20of%20undesirable%20data.%20Meanwhile%2C%20the%0Aadvent%20of%20dynamic%20graph%20neural%20networks%20%28DGNNs%29%20marks%20a%20significant%20advancement%0Adue%20to%20their%20superior%20capability%20in%20learning%20from%20dynamic%20graphs%2C%20which%0Aencapsulate%20spatial-temporal%20variations%20in%20diverse%20real-world%20applications%0A%28e.g.%2C%20traffic%20forecasting%29.%20With%20the%20increasing%20prevalence%20of%20DGNNs%2C%20it%0Abecomes%20imperative%20to%20investigate%20the%20implementation%20of%20dynamic%20graph%0Aunlearning.%20However%2C%20current%20graph%20unlearning%20methodologies%20are%20designed%20for%0AGNNs%20operating%20on%20static%20graphs%20and%20exhibit%20limitations%20including%20their%20serving%0Ain%20a%20pre-processing%20manner%20and%20impractical%20resource%20demands.%20Furthermore%2C%20the%0Aadaptation%20of%20these%20methods%20to%20DGNNs%20presents%20non-trivial%20challenges%2C%20owing%20to%0Athe%20distinctive%20nature%20of%20dynamic%20graphs.%20To%20this%20end%2C%20we%20propose%20an%20effective%2C%0Aefficient%2C%20model-agnostic%2C%20and%20post-processing%20method%20to%20implement%20DGNN%0Aunlearning.%20Specifically%2C%20we%20first%20define%20the%20unlearning%20requests%20and%20formulate%0Adynamic%20graph%20unlearning%20in%20the%20context%20of%20continuous-time%20dynamic%20graphs.%0AAfter%20conducting%20a%20role%20analysis%20on%20the%20unlearning%20data%2C%20the%20remaining%20data%2C%0Aand%20the%20target%20DGNN%20model%2C%20we%20propose%20a%20method%20called%20Gradient%20Transformation%0Aand%20a%20loss%20function%20to%20map%20the%20unlearning%20request%20to%20the%20desired%20parameter%0Aupdate.%20Evaluations%20on%20six%20real-world%20datasets%20and%20state-of-the-art%20DGNN%0Abackbones%20demonstrate%20its%20effectiveness%20%28e.g.%2C%20limited%20performance%20drop%20even%0Aobvious%20improvement%29%20and%20efficiency%20%28e.g.%2C%20at%20most%207.23%24%5Ctimes%24%20speed-up%29%0Aoutperformance%2C%20and%20potential%20advantages%20in%20handling%20future%20unlearning%20requests%0A%28e.g.%2C%20at%20most%2032.59%24%5Ctimes%24%20speed-up%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Transformation%253A%2520Towards%2520Efficient%2520and%2520Model-Agnostic%2520Unlearning%250A%2520%2520for%2520Dynamic%2520Graph%2520Neural%2520Networks%26entry.906535625%3DHe%2520Zhang%2520and%2520Bang%2520Wu%2520and%2520Xiangwen%2520Yang%2520and%2520Xingliang%2520Yuan%2520and%2520Chengqi%2520Zhang%2520and%2520Shirui%2520Pan%26entry.1292438233%3D%2520%2520Graph%2520unlearning%2520has%2520emerged%2520as%2520an%2520essential%2520tool%2520for%2520safeguarding%2520user%250Aprivacy%2520and%2520mitigating%2520the%2520negative%2520impacts%2520of%2520undesirable%2520data.%2520Meanwhile%252C%2520the%250Aadvent%2520of%2520dynamic%2520graph%2520neural%2520networks%2520%2528DGNNs%2529%2520marks%2520a%2520significant%2520advancement%250Adue%2520to%2520their%2520superior%2520capability%2520in%2520learning%2520from%2520dynamic%2520graphs%252C%2520which%250Aencapsulate%2520spatial-temporal%2520variations%2520in%2520diverse%2520real-world%2520applications%250A%2528e.g.%252C%2520traffic%2520forecasting%2529.%2520With%2520the%2520increasing%2520prevalence%2520of%2520DGNNs%252C%2520it%250Abecomes%2520imperative%2520to%2520investigate%2520the%2520implementation%2520of%2520dynamic%2520graph%250Aunlearning.%2520However%252C%2520current%2520graph%2520unlearning%2520methodologies%2520are%2520designed%2520for%250AGNNs%2520operating%2520on%2520static%2520graphs%2520and%2520exhibit%2520limitations%2520including%2520their%2520serving%250Ain%2520a%2520pre-processing%2520manner%2520and%2520impractical%2520resource%2520demands.%2520Furthermore%252C%2520the%250Aadaptation%2520of%2520these%2520methods%2520to%2520DGNNs%2520presents%2520non-trivial%2520challenges%252C%2520owing%2520to%250Athe%2520distinctive%2520nature%2520of%2520dynamic%2520graphs.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520effective%252C%250Aefficient%252C%2520model-agnostic%252C%2520and%2520post-processing%2520method%2520to%2520implement%2520DGNN%250Aunlearning.%2520Specifically%252C%2520we%2520first%2520define%2520the%2520unlearning%2520requests%2520and%2520formulate%250Adynamic%2520graph%2520unlearning%2520in%2520the%2520context%2520of%2520continuous-time%2520dynamic%2520graphs.%250AAfter%2520conducting%2520a%2520role%2520analysis%2520on%2520the%2520unlearning%2520data%252C%2520the%2520remaining%2520data%252C%250Aand%2520the%2520target%2520DGNN%2520model%252C%2520we%2520propose%2520a%2520method%2520called%2520Gradient%2520Transformation%250Aand%2520a%2520loss%2520function%2520to%2520map%2520the%2520unlearning%2520request%2520to%2520the%2520desired%2520parameter%250Aupdate.%2520Evaluations%2520on%2520six%2520real-world%2520datasets%2520and%2520state-of-the-art%2520DGNN%250Abackbones%2520demonstrate%2520its%2520effectiveness%2520%2528e.g.%252C%2520limited%2520performance%2520drop%2520even%250Aobvious%2520improvement%2529%2520and%2520efficiency%2520%2528e.g.%252C%2520at%2520most%25207.23%2524%255Ctimes%2524%2520speed-up%2529%250Aoutperformance%252C%2520and%2520potential%2520advantages%2520in%2520handling%2520future%2520unlearning%2520requests%250A%2528e.g.%252C%2520at%2520most%252032.59%2524%255Ctimes%2524%2520speed-up%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Transformation%3A%20Towards%20Efficient%20and%20Model-Agnostic%20Unlearning%0A%20%20for%20Dynamic%20Graph%20Neural%20Networks&entry.906535625=He%20Zhang%20and%20Bang%20Wu%20and%20Xiangwen%20Yang%20and%20Xingliang%20Yuan%20and%20Chengqi%20Zhang%20and%20Shirui%20Pan&entry.1292438233=%20%20Graph%20unlearning%20has%20emerged%20as%20an%20essential%20tool%20for%20safeguarding%20user%0Aprivacy%20and%20mitigating%20the%20negative%20impacts%20of%20undesirable%20data.%20Meanwhile%2C%20the%0Aadvent%20of%20dynamic%20graph%20neural%20networks%20%28DGNNs%29%20marks%20a%20significant%20advancement%0Adue%20to%20their%20superior%20capability%20in%20learning%20from%20dynamic%20graphs%2C%20which%0Aencapsulate%20spatial-temporal%20variations%20in%20diverse%20real-world%20applications%0A%28e.g.%2C%20traffic%20forecasting%29.%20With%20the%20increasing%20prevalence%20of%20DGNNs%2C%20it%0Abecomes%20imperative%20to%20investigate%20the%20implementation%20of%20dynamic%20graph%0Aunlearning.%20However%2C%20current%20graph%20unlearning%20methodologies%20are%20designed%20for%0AGNNs%20operating%20on%20static%20graphs%20and%20exhibit%20limitations%20including%20their%20serving%0Ain%20a%20pre-processing%20manner%20and%20impractical%20resource%20demands.%20Furthermore%2C%20the%0Aadaptation%20of%20these%20methods%20to%20DGNNs%20presents%20non-trivial%20challenges%2C%20owing%20to%0Athe%20distinctive%20nature%20of%20dynamic%20graphs.%20To%20this%20end%2C%20we%20propose%20an%20effective%2C%0Aefficient%2C%20model-agnostic%2C%20and%20post-processing%20method%20to%20implement%20DGNN%0Aunlearning.%20Specifically%2C%20we%20first%20define%20the%20unlearning%20requests%20and%20formulate%0Adynamic%20graph%20unlearning%20in%20the%20context%20of%20continuous-time%20dynamic%20graphs.%0AAfter%20conducting%20a%20role%20analysis%20on%20the%20unlearning%20data%2C%20the%20remaining%20data%2C%0Aand%20the%20target%20DGNN%20model%2C%20we%20propose%20a%20method%20called%20Gradient%20Transformation%0Aand%20a%20loss%20function%20to%20map%20the%20unlearning%20request%20to%20the%20desired%20parameter%0Aupdate.%20Evaluations%20on%20six%20real-world%20datasets%20and%20state-of-the-art%20DGNN%0Abackbones%20demonstrate%20its%20effectiveness%20%28e.g.%2C%20limited%20performance%20drop%20even%0Aobvious%20improvement%29%20and%20efficiency%20%28e.g.%2C%20at%20most%207.23%24%5Ctimes%24%20speed-up%29%0Aoutperformance%2C%20and%20potential%20advantages%20in%20handling%20future%20unlearning%20requests%0A%28e.g.%2C%20at%20most%2032.59%24%5Ctimes%24%20speed-up%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14407v1&entry.124074799=Read"},
{"title": "Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using\n  Sparse RGB Cameras", "author": "Hanzhang Tu and Ruizhi Shao and Xue Dong and Shunyuan Zheng and Hao Zhang and Lili Chen and Meili Wang and Wenyu Li and Siyan Ma and Shengping Zhang and Boyao Zhou and Yebin Liu", "abstract": "  In this paper, we present a low-budget and high-authenticity bidirectional\ntelepresence system, Tele-Aloha, targeting peer-to-peer communication\nscenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse\nRGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve\nhigh-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms)\nand robust distant communication. As the core of Tele-Aloha, we propose an\nefficient novel view synthesis algorithm for upper-body. Firstly, we design a\ncascaded disparity estimator for obtaining a robust geometry cue. Additionally\na neural rasterizer via Gaussian Splatting is introduced to project latent\nfeatures onto target view and to decode them into a reduced resolution.\nFurther, given the high-quality captured data, we leverage weighted blending\nmechanism to refine the decoded image into the final resolution of 2K.\nExploiting world-leading autostereoscopic display and low-latency iris\ntracking, users are able to experience a strong three-dimensional sense even\nwithout any wearable head-mounted display device. Altogether, our telepresence\nsystem demonstrates the sense of co-presence in real-life experiments,\ninspiring the next generation of communication.\n", "link": "http://arxiv.org/abs/2405.14866v1", "date": "2024-05-23", "relevancy": 2.7736, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.593}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tele-Aloha%3A%20A%20Low-budget%20and%20High-authenticity%20Telepresence%20System%20Using%0A%20%20Sparse%20RGB%20Cameras&body=Title%3A%20Tele-Aloha%3A%20A%20Low-budget%20and%20High-authenticity%20Telepresence%20System%20Using%0A%20%20Sparse%20RGB%20Cameras%0AAuthor%3A%20Hanzhang%20Tu%20and%20Ruizhi%20Shao%20and%20Xue%20Dong%20and%20Shunyuan%20Zheng%20and%20Hao%20Zhang%20and%20Lili%20Chen%20and%20Meili%20Wang%20and%20Wenyu%20Li%20and%20Siyan%20Ma%20and%20Shengping%20Zhang%20and%20Boyao%20Zhou%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20low-budget%20and%20high-authenticity%20bidirectional%0Atelepresence%20system%2C%20Tele-Aloha%2C%20targeting%20peer-to-peer%20communication%0Ascenarios.%20Compared%20to%20previous%20systems%2C%20Tele-Aloha%20utilizes%20only%20four%20sparse%0ARGB%20cameras%2C%20one%20consumer-grade%20GPU%2C%20and%20one%20autostereoscopic%20screen%20to%20achieve%0Ahigh-resolution%20%282048x2048%29%2C%20real-time%20%2830%20fps%29%2C%20low-latency%20%28less%20than%20150ms%29%0Aand%20robust%20distant%20communication.%20As%20the%20core%20of%20Tele-Aloha%2C%20we%20propose%20an%0Aefficient%20novel%20view%20synthesis%20algorithm%20for%20upper-body.%20Firstly%2C%20we%20design%20a%0Acascaded%20disparity%20estimator%20for%20obtaining%20a%20robust%20geometry%20cue.%20Additionally%0Aa%20neural%20rasterizer%20via%20Gaussian%20Splatting%20is%20introduced%20to%20project%20latent%0Afeatures%20onto%20target%20view%20and%20to%20decode%20them%20into%20a%20reduced%20resolution.%0AFurther%2C%20given%20the%20high-quality%20captured%20data%2C%20we%20leverage%20weighted%20blending%0Amechanism%20to%20refine%20the%20decoded%20image%20into%20the%20final%20resolution%20of%202K.%0AExploiting%20world-leading%20autostereoscopic%20display%20and%20low-latency%20iris%0Atracking%2C%20users%20are%20able%20to%20experience%20a%20strong%20three-dimensional%20sense%20even%0Awithout%20any%20wearable%20head-mounted%20display%20device.%20Altogether%2C%20our%20telepresence%0Asystem%20demonstrates%20the%20sense%20of%20co-presence%20in%20real-life%20experiments%2C%0Ainspiring%20the%20next%20generation%20of%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTele-Aloha%253A%2520A%2520Low-budget%2520and%2520High-authenticity%2520Telepresence%2520System%2520Using%250A%2520%2520Sparse%2520RGB%2520Cameras%26entry.906535625%3DHanzhang%2520Tu%2520and%2520Ruizhi%2520Shao%2520and%2520Xue%2520Dong%2520and%2520Shunyuan%2520Zheng%2520and%2520Hao%2520Zhang%2520and%2520Lili%2520Chen%2520and%2520Meili%2520Wang%2520and%2520Wenyu%2520Li%2520and%2520Siyan%2520Ma%2520and%2520Shengping%2520Zhang%2520and%2520Boyao%2520Zhou%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520low-budget%2520and%2520high-authenticity%2520bidirectional%250Atelepresence%2520system%252C%2520Tele-Aloha%252C%2520targeting%2520peer-to-peer%2520communication%250Ascenarios.%2520Compared%2520to%2520previous%2520systems%252C%2520Tele-Aloha%2520utilizes%2520only%2520four%2520sparse%250ARGB%2520cameras%252C%2520one%2520consumer-grade%2520GPU%252C%2520and%2520one%2520autostereoscopic%2520screen%2520to%2520achieve%250Ahigh-resolution%2520%25282048x2048%2529%252C%2520real-time%2520%252830%2520fps%2529%252C%2520low-latency%2520%2528less%2520than%2520150ms%2529%250Aand%2520robust%2520distant%2520communication.%2520As%2520the%2520core%2520of%2520Tele-Aloha%252C%2520we%2520propose%2520an%250Aefficient%2520novel%2520view%2520synthesis%2520algorithm%2520for%2520upper-body.%2520Firstly%252C%2520we%2520design%2520a%250Acascaded%2520disparity%2520estimator%2520for%2520obtaining%2520a%2520robust%2520geometry%2520cue.%2520Additionally%250Aa%2520neural%2520rasterizer%2520via%2520Gaussian%2520Splatting%2520is%2520introduced%2520to%2520project%2520latent%250Afeatures%2520onto%2520target%2520view%2520and%2520to%2520decode%2520them%2520into%2520a%2520reduced%2520resolution.%250AFurther%252C%2520given%2520the%2520high-quality%2520captured%2520data%252C%2520we%2520leverage%2520weighted%2520blending%250Amechanism%2520to%2520refine%2520the%2520decoded%2520image%2520into%2520the%2520final%2520resolution%2520of%25202K.%250AExploiting%2520world-leading%2520autostereoscopic%2520display%2520and%2520low-latency%2520iris%250Atracking%252C%2520users%2520are%2520able%2520to%2520experience%2520a%2520strong%2520three-dimensional%2520sense%2520even%250Awithout%2520any%2520wearable%2520head-mounted%2520display%2520device.%2520Altogether%252C%2520our%2520telepresence%250Asystem%2520demonstrates%2520the%2520sense%2520of%2520co-presence%2520in%2520real-life%2520experiments%252C%250Ainspiring%2520the%2520next%2520generation%2520of%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tele-Aloha%3A%20A%20Low-budget%20and%20High-authenticity%20Telepresence%20System%20Using%0A%20%20Sparse%20RGB%20Cameras&entry.906535625=Hanzhang%20Tu%20and%20Ruizhi%20Shao%20and%20Xue%20Dong%20and%20Shunyuan%20Zheng%20and%20Hao%20Zhang%20and%20Lili%20Chen%20and%20Meili%20Wang%20and%20Wenyu%20Li%20and%20Siyan%20Ma%20and%20Shengping%20Zhang%20and%20Boyao%20Zhou%20and%20Yebin%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20low-budget%20and%20high-authenticity%20bidirectional%0Atelepresence%20system%2C%20Tele-Aloha%2C%20targeting%20peer-to-peer%20communication%0Ascenarios.%20Compared%20to%20previous%20systems%2C%20Tele-Aloha%20utilizes%20only%20four%20sparse%0ARGB%20cameras%2C%20one%20consumer-grade%20GPU%2C%20and%20one%20autostereoscopic%20screen%20to%20achieve%0Ahigh-resolution%20%282048x2048%29%2C%20real-time%20%2830%20fps%29%2C%20low-latency%20%28less%20than%20150ms%29%0Aand%20robust%20distant%20communication.%20As%20the%20core%20of%20Tele-Aloha%2C%20we%20propose%20an%0Aefficient%20novel%20view%20synthesis%20algorithm%20for%20upper-body.%20Firstly%2C%20we%20design%20a%0Acascaded%20disparity%20estimator%20for%20obtaining%20a%20robust%20geometry%20cue.%20Additionally%0Aa%20neural%20rasterizer%20via%20Gaussian%20Splatting%20is%20introduced%20to%20project%20latent%0Afeatures%20onto%20target%20view%20and%20to%20decode%20them%20into%20a%20reduced%20resolution.%0AFurther%2C%20given%20the%20high-quality%20captured%20data%2C%20we%20leverage%20weighted%20blending%0Amechanism%20to%20refine%20the%20decoded%20image%20into%20the%20final%20resolution%20of%202K.%0AExploiting%20world-leading%20autostereoscopic%20display%20and%20low-latency%20iris%0Atracking%2C%20users%20are%20able%20to%20experience%20a%20strong%20three-dimensional%20sense%20even%0Awithout%20any%20wearable%20head-mounted%20display%20device.%20Altogether%2C%20our%20telepresence%0Asystem%20demonstrates%20the%20sense%20of%20co-presence%20in%20real-life%20experiments%2C%0Ainspiring%20the%20next%20generation%20of%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14866v1&entry.124074799=Read"},
{"title": "Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion\n  Transformer", "author": "Shuang Wu and Youtian Lin and Feihu Zhang and Yifei Zeng and Jingxi Xu and Philip Torr and Xun Cao and Yao Yao", "abstract": "  Generating high-quality 3D assets from text and images has long been\nchallenging, primarily due to the absence of scalable 3D representations\ncapable of capturing intricate geometry distributions. In this work, we\nintroduce Direct3D, a native 3D generative model scalable to in-the-wild input\nimages, without requiring a multiview diffusion model or SDS optimization. Our\napproach comprises two primary components: a Direct 3D Variational Auto-Encoder\n(D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT). D3D-VAE efficiently\nencodes high-resolution 3D shapes into a compact and continuous latent triplane\nspace. Notably, our method directly supervises the decoded geometry using a\nsemi-continuous surface sampling strategy, diverging from previous methods\nrelying on rendered images as supervision signals. D3D-DiT models the\ndistribution of encoded 3D latents and is specifically designed to fuse\npositional information from the three feature maps of the triplane latent,\nenabling a native 3D generative model scalable to large-scale 3D datasets.\nAdditionally, we introduce an innovative image-to-3D generation pipeline\nincorporating semantic and pixel-level image conditions, allowing the model to\nproduce 3D shapes consistent with the provided conditional image input.\nExtensive experiments demonstrate the superiority of our large-scale\npre-trained Direct3D over previous image-to-3D approaches, achieving\nsignificantly better generation quality and generalization ability, thus\nestablishing a new state-of-the-art for 3D content creation. Project page:\nhttps://nju-3dv.github.io/projects/Direct3D/.\n", "link": "http://arxiv.org/abs/2405.14832v1", "date": "2024-05-23", "relevancy": 2.7408, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6865}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6865}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct3D%3A%20Scalable%20Image-to-3D%20Generation%20via%203D%20Latent%20Diffusion%0A%20%20Transformer&body=Title%3A%20Direct3D%3A%20Scalable%20Image-to-3D%20Generation%20via%203D%20Latent%20Diffusion%0A%20%20Transformer%0AAuthor%3A%20Shuang%20Wu%20and%20Youtian%20Lin%20and%20Feihu%20Zhang%20and%20Yifei%20Zeng%20and%20Jingxi%20Xu%20and%20Philip%20Torr%20and%20Xun%20Cao%20and%20Yao%20Yao%0AAbstract%3A%20%20%20Generating%20high-quality%203D%20assets%20from%20text%20and%20images%20has%20long%20been%0Achallenging%2C%20primarily%20due%20to%20the%20absence%20of%20scalable%203D%20representations%0Acapable%20of%20capturing%20intricate%20geometry%20distributions.%20In%20this%20work%2C%20we%0Aintroduce%20Direct3D%2C%20a%20native%203D%20generative%20model%20scalable%20to%20in-the-wild%20input%0Aimages%2C%20without%20requiring%20a%20multiview%20diffusion%20model%20or%20SDS%20optimization.%20Our%0Aapproach%20comprises%20two%20primary%20components%3A%20a%20Direct%203D%20Variational%20Auto-Encoder%0A%28D3D-VAE%29%20and%20a%20Direct%203D%20Diffusion%20Transformer%20%28D3D-DiT%29.%20D3D-VAE%20efficiently%0Aencodes%20high-resolution%203D%20shapes%20into%20a%20compact%20and%20continuous%20latent%20triplane%0Aspace.%20Notably%2C%20our%20method%20directly%20supervises%20the%20decoded%20geometry%20using%20a%0Asemi-continuous%20surface%20sampling%20strategy%2C%20diverging%20from%20previous%20methods%0Arelying%20on%20rendered%20images%20as%20supervision%20signals.%20D3D-DiT%20models%20the%0Adistribution%20of%20encoded%203D%20latents%20and%20is%20specifically%20designed%20to%20fuse%0Apositional%20information%20from%20the%20three%20feature%20maps%20of%20the%20triplane%20latent%2C%0Aenabling%20a%20native%203D%20generative%20model%20scalable%20to%20large-scale%203D%20datasets.%0AAdditionally%2C%20we%20introduce%20an%20innovative%20image-to-3D%20generation%20pipeline%0Aincorporating%20semantic%20and%20pixel-level%20image%20conditions%2C%20allowing%20the%20model%20to%0Aproduce%203D%20shapes%20consistent%20with%20the%20provided%20conditional%20image%20input.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20large-scale%0Apre-trained%20Direct3D%20over%20previous%20image-to-3D%20approaches%2C%20achieving%0Asignificantly%20better%20generation%20quality%20and%20generalization%20ability%2C%20thus%0Aestablishing%20a%20new%20state-of-the-art%20for%203D%20content%20creation.%20Project%20page%3A%0Ahttps%3A//nju-3dv.github.io/projects/Direct3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect3D%253A%2520Scalable%2520Image-to-3D%2520Generation%2520via%25203D%2520Latent%2520Diffusion%250A%2520%2520Transformer%26entry.906535625%3DShuang%2520Wu%2520and%2520Youtian%2520Lin%2520and%2520Feihu%2520Zhang%2520and%2520Yifei%2520Zeng%2520and%2520Jingxi%2520Xu%2520and%2520Philip%2520Torr%2520and%2520Xun%2520Cao%2520and%2520Yao%2520Yao%26entry.1292438233%3D%2520%2520Generating%2520high-quality%25203D%2520assets%2520from%2520text%2520and%2520images%2520has%2520long%2520been%250Achallenging%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%2520scalable%25203D%2520representations%250Acapable%2520of%2520capturing%2520intricate%2520geometry%2520distributions.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Direct3D%252C%2520a%2520native%25203D%2520generative%2520model%2520scalable%2520to%2520in-the-wild%2520input%250Aimages%252C%2520without%2520requiring%2520a%2520multiview%2520diffusion%2520model%2520or%2520SDS%2520optimization.%2520Our%250Aapproach%2520comprises%2520two%2520primary%2520components%253A%2520a%2520Direct%25203D%2520Variational%2520Auto-Encoder%250A%2528D3D-VAE%2529%2520and%2520a%2520Direct%25203D%2520Diffusion%2520Transformer%2520%2528D3D-DiT%2529.%2520D3D-VAE%2520efficiently%250Aencodes%2520high-resolution%25203D%2520shapes%2520into%2520a%2520compact%2520and%2520continuous%2520latent%2520triplane%250Aspace.%2520Notably%252C%2520our%2520method%2520directly%2520supervises%2520the%2520decoded%2520geometry%2520using%2520a%250Asemi-continuous%2520surface%2520sampling%2520strategy%252C%2520diverging%2520from%2520previous%2520methods%250Arelying%2520on%2520rendered%2520images%2520as%2520supervision%2520signals.%2520D3D-DiT%2520models%2520the%250Adistribution%2520of%2520encoded%25203D%2520latents%2520and%2520is%2520specifically%2520designed%2520to%2520fuse%250Apositional%2520information%2520from%2520the%2520three%2520feature%2520maps%2520of%2520the%2520triplane%2520latent%252C%250Aenabling%2520a%2520native%25203D%2520generative%2520model%2520scalable%2520to%2520large-scale%25203D%2520datasets.%250AAdditionally%252C%2520we%2520introduce%2520an%2520innovative%2520image-to-3D%2520generation%2520pipeline%250Aincorporating%2520semantic%2520and%2520pixel-level%2520image%2520conditions%252C%2520allowing%2520the%2520model%2520to%250Aproduce%25203D%2520shapes%2520consistent%2520with%2520the%2520provided%2520conditional%2520image%2520input.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520large-scale%250Apre-trained%2520Direct3D%2520over%2520previous%2520image-to-3D%2520approaches%252C%2520achieving%250Asignificantly%2520better%2520generation%2520quality%2520and%2520generalization%2520ability%252C%2520thus%250Aestablishing%2520a%2520new%2520state-of-the-art%2520for%25203D%2520content%2520creation.%2520Project%2520page%253A%250Ahttps%253A//nju-3dv.github.io/projects/Direct3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct3D%3A%20Scalable%20Image-to-3D%20Generation%20via%203D%20Latent%20Diffusion%0A%20%20Transformer&entry.906535625=Shuang%20Wu%20and%20Youtian%20Lin%20and%20Feihu%20Zhang%20and%20Yifei%20Zeng%20and%20Jingxi%20Xu%20and%20Philip%20Torr%20and%20Xun%20Cao%20and%20Yao%20Yao&entry.1292438233=%20%20Generating%20high-quality%203D%20assets%20from%20text%20and%20images%20has%20long%20been%0Achallenging%2C%20primarily%20due%20to%20the%20absence%20of%20scalable%203D%20representations%0Acapable%20of%20capturing%20intricate%20geometry%20distributions.%20In%20this%20work%2C%20we%0Aintroduce%20Direct3D%2C%20a%20native%203D%20generative%20model%20scalable%20to%20in-the-wild%20input%0Aimages%2C%20without%20requiring%20a%20multiview%20diffusion%20model%20or%20SDS%20optimization.%20Our%0Aapproach%20comprises%20two%20primary%20components%3A%20a%20Direct%203D%20Variational%20Auto-Encoder%0A%28D3D-VAE%29%20and%20a%20Direct%203D%20Diffusion%20Transformer%20%28D3D-DiT%29.%20D3D-VAE%20efficiently%0Aencodes%20high-resolution%203D%20shapes%20into%20a%20compact%20and%20continuous%20latent%20triplane%0Aspace.%20Notably%2C%20our%20method%20directly%20supervises%20the%20decoded%20geometry%20using%20a%0Asemi-continuous%20surface%20sampling%20strategy%2C%20diverging%20from%20previous%20methods%0Arelying%20on%20rendered%20images%20as%20supervision%20signals.%20D3D-DiT%20models%20the%0Adistribution%20of%20encoded%203D%20latents%20and%20is%20specifically%20designed%20to%20fuse%0Apositional%20information%20from%20the%20three%20feature%20maps%20of%20the%20triplane%20latent%2C%0Aenabling%20a%20native%203D%20generative%20model%20scalable%20to%20large-scale%203D%20datasets.%0AAdditionally%2C%20we%20introduce%20an%20innovative%20image-to-3D%20generation%20pipeline%0Aincorporating%20semantic%20and%20pixel-level%20image%20conditions%2C%20allowing%20the%20model%20to%0Aproduce%203D%20shapes%20consistent%20with%20the%20provided%20conditional%20image%20input.%0AExtensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20large-scale%0Apre-trained%20Direct3D%20over%20previous%20image-to-3D%20approaches%2C%20achieving%0Asignificantly%20better%20generation%20quality%20and%20generalization%20ability%2C%20thus%0Aestablishing%20a%20new%20state-of-the-art%20for%203D%20content%20creation.%20Project%20page%3A%0Ahttps%3A//nju-3dv.github.io/projects/Direct3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14832v1&entry.124074799=Read"},
{"title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP", "author": "Beichen Zhang and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Jiaqi Wang", "abstract": "  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.\n", "link": "http://arxiv.org/abs/2403.15378v2", "date": "2024-05-23", "relevancy": 2.7196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP&body=Title%3A%20Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP%0AAuthor%3A%20Beichen%20Zhang%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20the%20cornerstone%20for%0Azero-shot%20classification%2C%20text-image%20retrieval%2C%20and%20text-image%20generation%20by%0Aaligning%20image%20and%20text%20modalities.%20Despite%20its%20widespread%20adoption%2C%20a%0Asignificant%20limitation%20of%20CLIP%20lies%20in%20the%20inadequate%20length%20of%20text%20input.%20The%0Alength%20of%20the%20text%20token%20is%20restricted%20to%2077%2C%20and%20an%20empirical%20study%20shows%20the%0Aactual%20effective%20length%20is%20even%20less%20than%2020.%20This%20prevents%20CLIP%20from%20handling%0Adetailed%20descriptions%2C%20limiting%20its%20applications%20for%20image%20retrieval%20and%0Atext-to-image%20generation%20with%20extensive%20prerequisites.%20To%20this%20end%2C%20we%20propose%0ALong-CLIP%20as%20a%20plug-and-play%20alternative%20to%20CLIP%20that%20supports%20long-text%20input%2C%0Aretains%20or%20even%20surpasses%20its%20zero-shot%20generalizability%2C%20and%20aligns%20the%20CLIP%0Alatent%20space%2C%20making%20it%20readily%20replace%20CLIP%20without%20any%20further%20adaptation%20in%0Adownstream%20frameworks.%20Nevertheless%2C%20achieving%20this%20goal%20is%20far%20from%0Astraightforward%2C%20as%20simplistic%20fine-tuning%20can%20result%20in%20a%20significant%0Adegradation%20of%20CLIP%27s%20performance.%20Moreover%2C%20substituting%20the%20text%20encoder%20with%0Aa%20language%20model%20supporting%20longer%20contexts%20necessitates%20pretraining%20with%20vast%0Aamounts%20of%20data%2C%20incurring%20significant%20expenses.%20Accordingly%2C%20Long-CLIP%0Aintroduces%20an%20efficient%20fine-tuning%20solution%20on%20CLIP%20with%20two%20novel%20strategies%0Adesigned%20to%20maintain%20the%20original%20capabilities%2C%20including%20%281%29%20a%0Aknowledge-preserved%20stretching%20of%20positional%20embedding%20and%20%282%29%20a%20primary%0Acomponent%20matching%20of%20CLIP%20features.%20With%20leveraging%20just%20one%20million%20extra%0Along%20text-image%20pairs%2C%20Long-CLIP%20has%20shown%20the%20superiority%20to%20CLIP%20for%20about%0A20%25%20in%20long%20caption%20text-image%20retrieval%20and%206%25%20in%20traditional%20text-image%0Aretrieval%20tasks%2C%20e.g.%2C%20COCO%20and%20Flickr30k.%20Furthermore%2C%20Long-CLIP%20offers%0Aenhanced%20capabilities%20for%20generating%20images%20from%20detailed%20text%20descriptions%20by%0Areplacing%20CLIP%20in%20a%20plug-and-play%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-CLIP%253A%2520Unlocking%2520the%2520Long-Text%2520Capability%2520of%2520CLIP%26entry.906535625%3DBeichen%2520Zhang%2520and%2520Pan%2520Zhang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520been%2520the%2520cornerstone%2520for%250Azero-shot%2520classification%252C%2520text-image%2520retrieval%252C%2520and%2520text-image%2520generation%2520by%250Aaligning%2520image%2520and%2520text%2520modalities.%2520Despite%2520its%2520widespread%2520adoption%252C%2520a%250Asignificant%2520limitation%2520of%2520CLIP%2520lies%2520in%2520the%2520inadequate%2520length%2520of%2520text%2520input.%2520The%250Alength%2520of%2520the%2520text%2520token%2520is%2520restricted%2520to%252077%252C%2520and%2520an%2520empirical%2520study%2520shows%2520the%250Aactual%2520effective%2520length%2520is%2520even%2520less%2520than%252020.%2520This%2520prevents%2520CLIP%2520from%2520handling%250Adetailed%2520descriptions%252C%2520limiting%2520its%2520applications%2520for%2520image%2520retrieval%2520and%250Atext-to-image%2520generation%2520with%2520extensive%2520prerequisites.%2520To%2520this%2520end%252C%2520we%2520propose%250ALong-CLIP%2520as%2520a%2520plug-and-play%2520alternative%2520to%2520CLIP%2520that%2520supports%2520long-text%2520input%252C%250Aretains%2520or%2520even%2520surpasses%2520its%2520zero-shot%2520generalizability%252C%2520and%2520aligns%2520the%2520CLIP%250Alatent%2520space%252C%2520making%2520it%2520readily%2520replace%2520CLIP%2520without%2520any%2520further%2520adaptation%2520in%250Adownstream%2520frameworks.%2520Nevertheless%252C%2520achieving%2520this%2520goal%2520is%2520far%2520from%250Astraightforward%252C%2520as%2520simplistic%2520fine-tuning%2520can%2520result%2520in%2520a%2520significant%250Adegradation%2520of%2520CLIP%2527s%2520performance.%2520Moreover%252C%2520substituting%2520the%2520text%2520encoder%2520with%250Aa%2520language%2520model%2520supporting%2520longer%2520contexts%2520necessitates%2520pretraining%2520with%2520vast%250Aamounts%2520of%2520data%252C%2520incurring%2520significant%2520expenses.%2520Accordingly%252C%2520Long-CLIP%250Aintroduces%2520an%2520efficient%2520fine-tuning%2520solution%2520on%2520CLIP%2520with%2520two%2520novel%2520strategies%250Adesigned%2520to%2520maintain%2520the%2520original%2520capabilities%252C%2520including%2520%25281%2529%2520a%250Aknowledge-preserved%2520stretching%2520of%2520positional%2520embedding%2520and%2520%25282%2529%2520a%2520primary%250Acomponent%2520matching%2520of%2520CLIP%2520features.%2520With%2520leveraging%2520just%2520one%2520million%2520extra%250Along%2520text-image%2520pairs%252C%2520Long-CLIP%2520has%2520shown%2520the%2520superiority%2520to%2520CLIP%2520for%2520about%250A20%2525%2520in%2520long%2520caption%2520text-image%2520retrieval%2520and%25206%2525%2520in%2520traditional%2520text-image%250Aretrieval%2520tasks%252C%2520e.g.%252C%2520COCO%2520and%2520Flickr30k.%2520Furthermore%252C%2520Long-CLIP%2520offers%250Aenhanced%2520capabilities%2520for%2520generating%2520images%2520from%2520detailed%2520text%2520descriptions%2520by%250Areplacing%2520CLIP%2520in%2520a%2520plug-and-play%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-CLIP%3A%20Unlocking%20the%20Long-Text%20Capability%20of%20CLIP&entry.906535625=Beichen%20Zhang%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20been%20the%20cornerstone%20for%0Azero-shot%20classification%2C%20text-image%20retrieval%2C%20and%20text-image%20generation%20by%0Aaligning%20image%20and%20text%20modalities.%20Despite%20its%20widespread%20adoption%2C%20a%0Asignificant%20limitation%20of%20CLIP%20lies%20in%20the%20inadequate%20length%20of%20text%20input.%20The%0Alength%20of%20the%20text%20token%20is%20restricted%20to%2077%2C%20and%20an%20empirical%20study%20shows%20the%0Aactual%20effective%20length%20is%20even%20less%20than%2020.%20This%20prevents%20CLIP%20from%20handling%0Adetailed%20descriptions%2C%20limiting%20its%20applications%20for%20image%20retrieval%20and%0Atext-to-image%20generation%20with%20extensive%20prerequisites.%20To%20this%20end%2C%20we%20propose%0ALong-CLIP%20as%20a%20plug-and-play%20alternative%20to%20CLIP%20that%20supports%20long-text%20input%2C%0Aretains%20or%20even%20surpasses%20its%20zero-shot%20generalizability%2C%20and%20aligns%20the%20CLIP%0Alatent%20space%2C%20making%20it%20readily%20replace%20CLIP%20without%20any%20further%20adaptation%20in%0Adownstream%20frameworks.%20Nevertheless%2C%20achieving%20this%20goal%20is%20far%20from%0Astraightforward%2C%20as%20simplistic%20fine-tuning%20can%20result%20in%20a%20significant%0Adegradation%20of%20CLIP%27s%20performance.%20Moreover%2C%20substituting%20the%20text%20encoder%20with%0Aa%20language%20model%20supporting%20longer%20contexts%20necessitates%20pretraining%20with%20vast%0Aamounts%20of%20data%2C%20incurring%20significant%20expenses.%20Accordingly%2C%20Long-CLIP%0Aintroduces%20an%20efficient%20fine-tuning%20solution%20on%20CLIP%20with%20two%20novel%20strategies%0Adesigned%20to%20maintain%20the%20original%20capabilities%2C%20including%20%281%29%20a%0Aknowledge-preserved%20stretching%20of%20positional%20embedding%20and%20%282%29%20a%20primary%0Acomponent%20matching%20of%20CLIP%20features.%20With%20leveraging%20just%20one%20million%20extra%0Along%20text-image%20pairs%2C%20Long-CLIP%20has%20shown%20the%20superiority%20to%20CLIP%20for%20about%0A20%25%20in%20long%20caption%20text-image%20retrieval%20and%206%25%20in%20traditional%20text-image%0Aretrieval%20tasks%2C%20e.g.%2C%20COCO%20and%20Flickr30k.%20Furthermore%2C%20Long-CLIP%20offers%0Aenhanced%20capabilities%20for%20generating%20images%20from%20detailed%20text%20descriptions%20by%0Areplacing%20CLIP%20in%20a%20plug-and-play%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15378v2&entry.124074799=Read"},
{"title": "HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph\n  Representation Learning", "author": "Zhuo Xu and Lu Bai and Lixin Cui and Ming Li and Yue Wang and Edwin R. Hancock", "abstract": "  Graph Auto-Encoders (GAEs) are powerful tools for graph representation\nlearning. In this paper, we develop a novel Hierarchical Cluster-based GAE\n(HC-GAE), that can learn effective structural characteristics for graph data\nanalysis. To this end, during the encoding process, we commence by utilizing\nthe hard node assignment to decompose a sample graph into a family of separated\nsubgraphs. We compress each subgraph into a coarsened node, transforming the\noriginal graph into a coarsened graph. On the other hand, during the decoding\nprocess, we adopt the soft node assignment to reconstruct the original graph\nstructure by expanding the coarsened nodes. By hierarchically performing the\nabove compressing procedure during the decoding process as well as the\nexpanding procedure during the decoding process, the proposed HC-GAE can\neffectively extract bidirectionally hierarchical structural features of the\noriginal sample graph. Furthermore, we re-design the loss function that can\nintegrate the information from either the encoder or the decoder. Since the\nassociated graph convolution operation of the proposed HC-GAE is restricted in\neach individual separated subgraph and cannot propagate the node information\nbetween different subgraphs, the proposed HC-GAE can significantly reduce the\nover-smoothing problem arising in the classical convolution-based GAEs. The\nproposed HC-GAE can generate effective representations for either node\nclassification or graph classification, and the experiments demonstrate the\neffectiveness on real-world datasets.\n", "link": "http://arxiv.org/abs/2405.14742v1", "date": "2024-05-23", "relevancy": 2.7123, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5918}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5659}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HC-GAE%3A%20The%20Hierarchical%20Cluster-based%20Graph%20Auto-Encoder%20for%20Graph%0A%20%20Representation%20Learning&body=Title%3A%20HC-GAE%3A%20The%20Hierarchical%20Cluster-based%20Graph%20Auto-Encoder%20for%20Graph%0A%20%20Representation%20Learning%0AAuthor%3A%20Zhuo%20Xu%20and%20Lu%20Bai%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Edwin%20R.%20Hancock%0AAbstract%3A%20%20%20Graph%20Auto-Encoders%20%28GAEs%29%20are%20powerful%20tools%20for%20graph%20representation%0Alearning.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20Hierarchical%20Cluster-based%20GAE%0A%28HC-GAE%29%2C%20that%20can%20learn%20effective%20structural%20characteristics%20for%20graph%20data%0Aanalysis.%20To%20this%20end%2C%20during%20the%20encoding%20process%2C%20we%20commence%20by%20utilizing%0Athe%20hard%20node%20assignment%20to%20decompose%20a%20sample%20graph%20into%20a%20family%20of%20separated%0Asubgraphs.%20We%20compress%20each%20subgraph%20into%20a%20coarsened%20node%2C%20transforming%20the%0Aoriginal%20graph%20into%20a%20coarsened%20graph.%20On%20the%20other%20hand%2C%20during%20the%20decoding%0Aprocess%2C%20we%20adopt%20the%20soft%20node%20assignment%20to%20reconstruct%20the%20original%20graph%0Astructure%20by%20expanding%20the%20coarsened%20nodes.%20By%20hierarchically%20performing%20the%0Aabove%20compressing%20procedure%20during%20the%20decoding%20process%20as%20well%20as%20the%0Aexpanding%20procedure%20during%20the%20decoding%20process%2C%20the%20proposed%20HC-GAE%20can%0Aeffectively%20extract%20bidirectionally%20hierarchical%20structural%20features%20of%20the%0Aoriginal%20sample%20graph.%20Furthermore%2C%20we%20re-design%20the%20loss%20function%20that%20can%0Aintegrate%20the%20information%20from%20either%20the%20encoder%20or%20the%20decoder.%20Since%20the%0Aassociated%20graph%20convolution%20operation%20of%20the%20proposed%20HC-GAE%20is%20restricted%20in%0Aeach%20individual%20separated%20subgraph%20and%20cannot%20propagate%20the%20node%20information%0Abetween%20different%20subgraphs%2C%20the%20proposed%20HC-GAE%20can%20significantly%20reduce%20the%0Aover-smoothing%20problem%20arising%20in%20the%20classical%20convolution-based%20GAEs.%20The%0Aproposed%20HC-GAE%20can%20generate%20effective%20representations%20for%20either%20node%0Aclassification%20or%20graph%20classification%2C%20and%20the%20experiments%20demonstrate%20the%0Aeffectiveness%20on%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHC-GAE%253A%2520The%2520Hierarchical%2520Cluster-based%2520Graph%2520Auto-Encoder%2520for%2520Graph%250A%2520%2520Representation%2520Learning%26entry.906535625%3DZhuo%2520Xu%2520and%2520Lu%2520Bai%2520and%2520Lixin%2520Cui%2520and%2520Ming%2520Li%2520and%2520Yue%2520Wang%2520and%2520Edwin%2520R.%2520Hancock%26entry.1292438233%3D%2520%2520Graph%2520Auto-Encoders%2520%2528GAEs%2529%2520are%2520powerful%2520tools%2520for%2520graph%2520representation%250Alearning.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520Hierarchical%2520Cluster-based%2520GAE%250A%2528HC-GAE%2529%252C%2520that%2520can%2520learn%2520effective%2520structural%2520characteristics%2520for%2520graph%2520data%250Aanalysis.%2520To%2520this%2520end%252C%2520during%2520the%2520encoding%2520process%252C%2520we%2520commence%2520by%2520utilizing%250Athe%2520hard%2520node%2520assignment%2520to%2520decompose%2520a%2520sample%2520graph%2520into%2520a%2520family%2520of%2520separated%250Asubgraphs.%2520We%2520compress%2520each%2520subgraph%2520into%2520a%2520coarsened%2520node%252C%2520transforming%2520the%250Aoriginal%2520graph%2520into%2520a%2520coarsened%2520graph.%2520On%2520the%2520other%2520hand%252C%2520during%2520the%2520decoding%250Aprocess%252C%2520we%2520adopt%2520the%2520soft%2520node%2520assignment%2520to%2520reconstruct%2520the%2520original%2520graph%250Astructure%2520by%2520expanding%2520the%2520coarsened%2520nodes.%2520By%2520hierarchically%2520performing%2520the%250Aabove%2520compressing%2520procedure%2520during%2520the%2520decoding%2520process%2520as%2520well%2520as%2520the%250Aexpanding%2520procedure%2520during%2520the%2520decoding%2520process%252C%2520the%2520proposed%2520HC-GAE%2520can%250Aeffectively%2520extract%2520bidirectionally%2520hierarchical%2520structural%2520features%2520of%2520the%250Aoriginal%2520sample%2520graph.%2520Furthermore%252C%2520we%2520re-design%2520the%2520loss%2520function%2520that%2520can%250Aintegrate%2520the%2520information%2520from%2520either%2520the%2520encoder%2520or%2520the%2520decoder.%2520Since%2520the%250Aassociated%2520graph%2520convolution%2520operation%2520of%2520the%2520proposed%2520HC-GAE%2520is%2520restricted%2520in%250Aeach%2520individual%2520separated%2520subgraph%2520and%2520cannot%2520propagate%2520the%2520node%2520information%250Abetween%2520different%2520subgraphs%252C%2520the%2520proposed%2520HC-GAE%2520can%2520significantly%2520reduce%2520the%250Aover-smoothing%2520problem%2520arising%2520in%2520the%2520classical%2520convolution-based%2520GAEs.%2520The%250Aproposed%2520HC-GAE%2520can%2520generate%2520effective%2520representations%2520for%2520either%2520node%250Aclassification%2520or%2520graph%2520classification%252C%2520and%2520the%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520on%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HC-GAE%3A%20The%20Hierarchical%20Cluster-based%20Graph%20Auto-Encoder%20for%20Graph%0A%20%20Representation%20Learning&entry.906535625=Zhuo%20Xu%20and%20Lu%20Bai%20and%20Lixin%20Cui%20and%20Ming%20Li%20and%20Yue%20Wang%20and%20Edwin%20R.%20Hancock&entry.1292438233=%20%20Graph%20Auto-Encoders%20%28GAEs%29%20are%20powerful%20tools%20for%20graph%20representation%0Alearning.%20In%20this%20paper%2C%20we%20develop%20a%20novel%20Hierarchical%20Cluster-based%20GAE%0A%28HC-GAE%29%2C%20that%20can%20learn%20effective%20structural%20characteristics%20for%20graph%20data%0Aanalysis.%20To%20this%20end%2C%20during%20the%20encoding%20process%2C%20we%20commence%20by%20utilizing%0Athe%20hard%20node%20assignment%20to%20decompose%20a%20sample%20graph%20into%20a%20family%20of%20separated%0Asubgraphs.%20We%20compress%20each%20subgraph%20into%20a%20coarsened%20node%2C%20transforming%20the%0Aoriginal%20graph%20into%20a%20coarsened%20graph.%20On%20the%20other%20hand%2C%20during%20the%20decoding%0Aprocess%2C%20we%20adopt%20the%20soft%20node%20assignment%20to%20reconstruct%20the%20original%20graph%0Astructure%20by%20expanding%20the%20coarsened%20nodes.%20By%20hierarchically%20performing%20the%0Aabove%20compressing%20procedure%20during%20the%20decoding%20process%20as%20well%20as%20the%0Aexpanding%20procedure%20during%20the%20decoding%20process%2C%20the%20proposed%20HC-GAE%20can%0Aeffectively%20extract%20bidirectionally%20hierarchical%20structural%20features%20of%20the%0Aoriginal%20sample%20graph.%20Furthermore%2C%20we%20re-design%20the%20loss%20function%20that%20can%0Aintegrate%20the%20information%20from%20either%20the%20encoder%20or%20the%20decoder.%20Since%20the%0Aassociated%20graph%20convolution%20operation%20of%20the%20proposed%20HC-GAE%20is%20restricted%20in%0Aeach%20individual%20separated%20subgraph%20and%20cannot%20propagate%20the%20node%20information%0Abetween%20different%20subgraphs%2C%20the%20proposed%20HC-GAE%20can%20significantly%20reduce%20the%0Aover-smoothing%20problem%20arising%20in%20the%20classical%20convolution-based%20GAEs.%20The%0Aproposed%20HC-GAE%20can%20generate%20effective%20representations%20for%20either%20node%0Aclassification%20or%20graph%20classification%2C%20and%20the%20experiments%20demonstrate%20the%0Aeffectiveness%20on%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14742v1&entry.124074799=Read"},
{"title": "Capsule Network Projectors are Equivariant and Invariant Learners", "author": "Miles Everett and Aiden Durrant and Mingjun Zhong and Georgios Leontidis", "abstract": "  Learning invariant representations has been the longstanding approach to\nself-supervised learning. However, recently progress has been made in\npreserving equivariant properties in representations, yet do so with highly\nprescribed architectures. In this work, we propose an invariant-equivariant\nself-supervised architecture that employs Capsule Networks (CapsNets) which\nhave been shown to capture equivariance with respect to novel viewpoints. We\ndemonstrate that the use of CapsNets in equivariant self-supervised\narchitectures achieves improved downstream performance on equivariant tasks\nwith higher efficiency and fewer network parameters. To accommodate the\narchitectural changes of CapsNets, we introduce a new objective function based\non entropy minimisation. This approach, which we name CapsIE (Capsule Invariant\nEquivariant Network), achieves state-of-the-art performance across all\ninvariant and equivariant downstream tasks on the 3DIEBench dataset, while\noutperforming supervised baselines. Our results demonstrate the ability of\nCapsNets to learn complex and generalised representations for large-scale,\nmulti-task datasets compared to previous CapsNet benchmarks. Code is available\nat https://github.com/AberdeenML/CapsIE.\n", "link": "http://arxiv.org/abs/2405.14386v1", "date": "2024-05-23", "relevancy": 2.6987, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5432}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5425}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capsule%20Network%20Projectors%20are%20Equivariant%20and%20Invariant%20Learners&body=Title%3A%20Capsule%20Network%20Projectors%20are%20Equivariant%20and%20Invariant%20Learners%0AAuthor%3A%20Miles%20Everett%20and%20Aiden%20Durrant%20and%20Mingjun%20Zhong%20and%20Georgios%20Leontidis%0AAbstract%3A%20%20%20Learning%20invariant%20representations%20has%20been%20the%20longstanding%20approach%20to%0Aself-supervised%20learning.%20However%2C%20recently%20progress%20has%20been%20made%20in%0Apreserving%20equivariant%20properties%20in%20representations%2C%20yet%20do%20so%20with%20highly%0Aprescribed%20architectures.%20In%20this%20work%2C%20we%20propose%20an%20invariant-equivariant%0Aself-supervised%20architecture%20that%20employs%20Capsule%20Networks%20%28CapsNets%29%20which%0Ahave%20been%20shown%20to%20capture%20equivariance%20with%20respect%20to%20novel%20viewpoints.%20We%0Ademonstrate%20that%20the%20use%20of%20CapsNets%20in%20equivariant%20self-supervised%0Aarchitectures%20achieves%20improved%20downstream%20performance%20on%20equivariant%20tasks%0Awith%20higher%20efficiency%20and%20fewer%20network%20parameters.%20To%20accommodate%20the%0Aarchitectural%20changes%20of%20CapsNets%2C%20we%20introduce%20a%20new%20objective%20function%20based%0Aon%20entropy%20minimisation.%20This%20approach%2C%20which%20we%20name%20CapsIE%20%28Capsule%20Invariant%0AEquivariant%20Network%29%2C%20achieves%20state-of-the-art%20performance%20across%20all%0Ainvariant%20and%20equivariant%20downstream%20tasks%20on%20the%203DIEBench%20dataset%2C%20while%0Aoutperforming%20supervised%20baselines.%20Our%20results%20demonstrate%20the%20ability%20of%0ACapsNets%20to%20learn%20complex%20and%20generalised%20representations%20for%20large-scale%2C%0Amulti-task%20datasets%20compared%20to%20previous%20CapsNet%20benchmarks.%20Code%20is%20available%0Aat%20https%3A//github.com/AberdeenML/CapsIE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapsule%2520Network%2520Projectors%2520are%2520Equivariant%2520and%2520Invariant%2520Learners%26entry.906535625%3DMiles%2520Everett%2520and%2520Aiden%2520Durrant%2520and%2520Mingjun%2520Zhong%2520and%2520Georgios%2520Leontidis%26entry.1292438233%3D%2520%2520Learning%2520invariant%2520representations%2520has%2520been%2520the%2520longstanding%2520approach%2520to%250Aself-supervised%2520learning.%2520However%252C%2520recently%2520progress%2520has%2520been%2520made%2520in%250Apreserving%2520equivariant%2520properties%2520in%2520representations%252C%2520yet%2520do%2520so%2520with%2520highly%250Aprescribed%2520architectures.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520invariant-equivariant%250Aself-supervised%2520architecture%2520that%2520employs%2520Capsule%2520Networks%2520%2528CapsNets%2529%2520which%250Ahave%2520been%2520shown%2520to%2520capture%2520equivariance%2520with%2520respect%2520to%2520novel%2520viewpoints.%2520We%250Ademonstrate%2520that%2520the%2520use%2520of%2520CapsNets%2520in%2520equivariant%2520self-supervised%250Aarchitectures%2520achieves%2520improved%2520downstream%2520performance%2520on%2520equivariant%2520tasks%250Awith%2520higher%2520efficiency%2520and%2520fewer%2520network%2520parameters.%2520To%2520accommodate%2520the%250Aarchitectural%2520changes%2520of%2520CapsNets%252C%2520we%2520introduce%2520a%2520new%2520objective%2520function%2520based%250Aon%2520entropy%2520minimisation.%2520This%2520approach%252C%2520which%2520we%2520name%2520CapsIE%2520%2528Capsule%2520Invariant%250AEquivariant%2520Network%2529%252C%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%250Ainvariant%2520and%2520equivariant%2520downstream%2520tasks%2520on%2520the%25203DIEBench%2520dataset%252C%2520while%250Aoutperforming%2520supervised%2520baselines.%2520Our%2520results%2520demonstrate%2520the%2520ability%2520of%250ACapsNets%2520to%2520learn%2520complex%2520and%2520generalised%2520representations%2520for%2520large-scale%252C%250Amulti-task%2520datasets%2520compared%2520to%2520previous%2520CapsNet%2520benchmarks.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/AberdeenML/CapsIE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capsule%20Network%20Projectors%20are%20Equivariant%20and%20Invariant%20Learners&entry.906535625=Miles%20Everett%20and%20Aiden%20Durrant%20and%20Mingjun%20Zhong%20and%20Georgios%20Leontidis&entry.1292438233=%20%20Learning%20invariant%20representations%20has%20been%20the%20longstanding%20approach%20to%0Aself-supervised%20learning.%20However%2C%20recently%20progress%20has%20been%20made%20in%0Apreserving%20equivariant%20properties%20in%20representations%2C%20yet%20do%20so%20with%20highly%0Aprescribed%20architectures.%20In%20this%20work%2C%20we%20propose%20an%20invariant-equivariant%0Aself-supervised%20architecture%20that%20employs%20Capsule%20Networks%20%28CapsNets%29%20which%0Ahave%20been%20shown%20to%20capture%20equivariance%20with%20respect%20to%20novel%20viewpoints.%20We%0Ademonstrate%20that%20the%20use%20of%20CapsNets%20in%20equivariant%20self-supervised%0Aarchitectures%20achieves%20improved%20downstream%20performance%20on%20equivariant%20tasks%0Awith%20higher%20efficiency%20and%20fewer%20network%20parameters.%20To%20accommodate%20the%0Aarchitectural%20changes%20of%20CapsNets%2C%20we%20introduce%20a%20new%20objective%20function%20based%0Aon%20entropy%20minimisation.%20This%20approach%2C%20which%20we%20name%20CapsIE%20%28Capsule%20Invariant%0AEquivariant%20Network%29%2C%20achieves%20state-of-the-art%20performance%20across%20all%0Ainvariant%20and%20equivariant%20downstream%20tasks%20on%20the%203DIEBench%20dataset%2C%20while%0Aoutperforming%20supervised%20baselines.%20Our%20results%20demonstrate%20the%20ability%20of%0ACapsNets%20to%20learn%20complex%20and%20generalised%20representations%20for%20large-scale%2C%0Amulti-task%20datasets%20compared%20to%20previous%20CapsNet%20benchmarks.%20Code%20is%20available%0Aat%20https%3A//github.com/AberdeenML/CapsIE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14386v1&entry.124074799=Read"},
{"title": "Neural Bounding", "author": "Stephanie Wenxin Liu and Michael Fischer and Paul D. Yoo and Tobias Ritschel", "abstract": "  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free or occupied. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods. In addition,\nwe propose an extension of our bounding method using early exits that\naccelerates query speeds by 25%. We also demonstrate that our approach is\napplicable to non-deep learning models that train within seconds. Our project\npage is at: https://wenxin-liu.github.io/neural_bounding/.\n", "link": "http://arxiv.org/abs/2310.06822v4", "date": "2024-05-23", "relevancy": 2.6956, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5447}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5442}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Bounding&body=Title%3A%20Neural%20Bounding%0AAuthor%3A%20Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06822v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Bounding%26entry.906535625%3DStephanie%2520Wenxin%2520Liu%2520and%2520Michael%2520Fischer%2520and%2520Paul%2520D.%2520Yoo%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520Bounding%2520volumes%2520are%2520an%2520established%2520concept%2520in%2520computer%2520graphics%2520and%2520vision%250Atasks%2520but%2520have%2520seen%2520little%2520change%2520since%2520their%2520early%2520inception.%2520In%2520this%2520work%252C%2520we%250Astudy%2520the%2520use%2520of%2520neural%2520networks%2520as%2520bounding%2520volumes.%2520Our%2520key%2520observation%2520is%250Athat%2520bounding%252C%2520which%2520so%2520far%2520has%2520primarily%2520been%2520considered%2520a%2520problem%2520of%250Acomputational%2520geometry%252C%2520can%2520be%2520redefined%2520as%2520a%2520problem%2520of%2520learning%2520to%2520classify%250Aspace%2520into%2520free%2520or%2520occupied.%2520This%2520learning-based%2520approach%2520is%2520particularly%250Aadvantageous%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520animated%2520scenes%2520with%2520complex%250Aqueries%252C%2520where%2520neural%2520networks%2520are%2520known%2520to%2520excel.%2520However%252C%2520unlocking%2520neural%250Abounding%2520requires%2520a%2520twist%253A%2520allowing%2520--%2520but%2520also%2520limiting%2520--%2520false%2520positives%252C%250Awhile%2520ensuring%2520that%2520the%2520number%2520of%2520false%2520negatives%2520is%2520strictly%2520zero.%2520We%2520enable%250Asuch%2520tight%2520and%2520conservative%2520results%2520using%2520a%2520dynamically-weighted%2520asymmetric%250Aloss%2520function.%2520Our%2520results%2520show%2520that%2520our%2520neural%2520bounding%2520produces%2520up%2520to%2520an%250Aorder%2520of%2520magnitude%2520fewer%2520false%2520positives%2520than%2520traditional%2520methods.%2520In%2520addition%252C%250Awe%2520propose%2520an%2520extension%2520of%2520our%2520bounding%2520method%2520using%2520early%2520exits%2520that%250Aaccelerates%2520query%2520speeds%2520by%252025%2525.%2520We%2520also%2520demonstrate%2520that%2520our%2520approach%2520is%250Aapplicable%2520to%2520non-deep%2520learning%2520models%2520that%2520train%2520within%2520seconds.%2520Our%2520project%250Apage%2520is%2520at%253A%2520https%253A//wenxin-liu.github.io/neural_bounding/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06822v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Bounding&entry.906535625=Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel&entry.1292438233=%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06822v4&entry.124074799=Read"},
{"title": "PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible\n  Poses", "author": "Yong Zhong and Min Zhao and Zebin You and Xiaofeng Yu and Changwang Zhang and Chongxuan Li", "abstract": "  In this paper, we introduce PoseCrafter, a one-shot method for personalized\nvideo generation following the control of flexible poses. Built upon Stable\nDiffusion and ControlNet, we carefully design an inference process to produce\nhigh-quality videos without the corresponding ground-truth frames. First, we\nselect an appropriate reference frame from the training video and invert it to\ninitialize all latent variables for generation. Then, we insert the\ncorresponding training pose into the target pose sequences to enhance\nfaithfulness through a trained temporal attention module. Furthermore, to\nalleviate the face and hand degradation resulting from discrepancies between\nposes of training videos and inference poses, we implement simple latent\nediting through an affine transformation matrix involving facial and hand\nlandmarks. Extensive experiments on several datasets demonstrate that\nPoseCrafter achieves superior results to baselines pre-trained on a vast\ncollection of videos under 8 commonly used metrics. Besides, PoseCrafter can\nfollow poses from different individuals or artificial edits and simultaneously\nretain the human identity in an open-domain training video.\n", "link": "http://arxiv.org/abs/2405.14582v1", "date": "2024-05-23", "relevancy": 2.6786, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6989}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6697}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Poses&body=Title%3A%20PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Poses%0AAuthor%3A%20Yong%20Zhong%20and%20Min%20Zhao%20and%20Zebin%20You%20and%20Xiaofeng%20Yu%20and%20Changwang%20Zhang%20and%20Chongxuan%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20PoseCrafter%2C%20a%20one-shot%20method%20for%20personalized%0Avideo%20generation%20following%20the%20control%20of%20flexible%20poses.%20Built%20upon%20Stable%0ADiffusion%20and%20ControlNet%2C%20we%20carefully%20design%20an%20inference%20process%20to%20produce%0Ahigh-quality%20videos%20without%20the%20corresponding%20ground-truth%20frames.%20First%2C%20we%0Aselect%20an%20appropriate%20reference%20frame%20from%20the%20training%20video%20and%20invert%20it%20to%0Ainitialize%20all%20latent%20variables%20for%20generation.%20Then%2C%20we%20insert%20the%0Acorresponding%20training%20pose%20into%20the%20target%20pose%20sequences%20to%20enhance%0Afaithfulness%20through%20a%20trained%20temporal%20attention%20module.%20Furthermore%2C%20to%0Aalleviate%20the%20face%20and%20hand%20degradation%20resulting%20from%20discrepancies%20between%0Aposes%20of%20training%20videos%20and%20inference%20poses%2C%20we%20implement%20simple%20latent%0Aediting%20through%20an%20affine%20transformation%20matrix%20involving%20facial%20and%20hand%0Alandmarks.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%0APoseCrafter%20achieves%20superior%20results%20to%20baselines%20pre-trained%20on%20a%20vast%0Acollection%20of%20videos%20under%208%20commonly%20used%20metrics.%20Besides%2C%20PoseCrafter%20can%0Afollow%20poses%20from%20different%20individuals%20or%20artificial%20edits%20and%20simultaneously%0Aretain%20the%20human%20identity%20in%20an%20open-domain%20training%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseCrafter%253A%2520One-Shot%2520Personalized%2520Video%2520Synthesis%2520Following%2520Flexible%250A%2520%2520Poses%26entry.906535625%3DYong%2520Zhong%2520and%2520Min%2520Zhao%2520and%2520Zebin%2520You%2520and%2520Xiaofeng%2520Yu%2520and%2520Changwang%2520Zhang%2520and%2520Chongxuan%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PoseCrafter%252C%2520a%2520one-shot%2520method%2520for%2520personalized%250Avideo%2520generation%2520following%2520the%2520control%2520of%2520flexible%2520poses.%2520Built%2520upon%2520Stable%250ADiffusion%2520and%2520ControlNet%252C%2520we%2520carefully%2520design%2520an%2520inference%2520process%2520to%2520produce%250Ahigh-quality%2520videos%2520without%2520the%2520corresponding%2520ground-truth%2520frames.%2520First%252C%2520we%250Aselect%2520an%2520appropriate%2520reference%2520frame%2520from%2520the%2520training%2520video%2520and%2520invert%2520it%2520to%250Ainitialize%2520all%2520latent%2520variables%2520for%2520generation.%2520Then%252C%2520we%2520insert%2520the%250Acorresponding%2520training%2520pose%2520into%2520the%2520target%2520pose%2520sequences%2520to%2520enhance%250Afaithfulness%2520through%2520a%2520trained%2520temporal%2520attention%2520module.%2520Furthermore%252C%2520to%250Aalleviate%2520the%2520face%2520and%2520hand%2520degradation%2520resulting%2520from%2520discrepancies%2520between%250Aposes%2520of%2520training%2520videos%2520and%2520inference%2520poses%252C%2520we%2520implement%2520simple%2520latent%250Aediting%2520through%2520an%2520affine%2520transformation%2520matrix%2520involving%2520facial%2520and%2520hand%250Alandmarks.%2520Extensive%2520experiments%2520on%2520several%2520datasets%2520demonstrate%2520that%250APoseCrafter%2520achieves%2520superior%2520results%2520to%2520baselines%2520pre-trained%2520on%2520a%2520vast%250Acollection%2520of%2520videos%2520under%25208%2520commonly%2520used%2520metrics.%2520Besides%252C%2520PoseCrafter%2520can%250Afollow%2520poses%2520from%2520different%2520individuals%2520or%2520artificial%2520edits%2520and%2520simultaneously%250Aretain%2520the%2520human%2520identity%2520in%2520an%2520open-domain%2520training%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseCrafter%3A%20One-Shot%20Personalized%20Video%20Synthesis%20Following%20Flexible%0A%20%20Poses&entry.906535625=Yong%20Zhong%20and%20Min%20Zhao%20and%20Zebin%20You%20and%20Xiaofeng%20Yu%20and%20Changwang%20Zhang%20and%20Chongxuan%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20PoseCrafter%2C%20a%20one-shot%20method%20for%20personalized%0Avideo%20generation%20following%20the%20control%20of%20flexible%20poses.%20Built%20upon%20Stable%0ADiffusion%20and%20ControlNet%2C%20we%20carefully%20design%20an%20inference%20process%20to%20produce%0Ahigh-quality%20videos%20without%20the%20corresponding%20ground-truth%20frames.%20First%2C%20we%0Aselect%20an%20appropriate%20reference%20frame%20from%20the%20training%20video%20and%20invert%20it%20to%0Ainitialize%20all%20latent%20variables%20for%20generation.%20Then%2C%20we%20insert%20the%0Acorresponding%20training%20pose%20into%20the%20target%20pose%20sequences%20to%20enhance%0Afaithfulness%20through%20a%20trained%20temporal%20attention%20module.%20Furthermore%2C%20to%0Aalleviate%20the%20face%20and%20hand%20degradation%20resulting%20from%20discrepancies%20between%0Aposes%20of%20training%20videos%20and%20inference%20poses%2C%20we%20implement%20simple%20latent%0Aediting%20through%20an%20affine%20transformation%20matrix%20involving%20facial%20and%20hand%0Alandmarks.%20Extensive%20experiments%20on%20several%20datasets%20demonstrate%20that%0APoseCrafter%20achieves%20superior%20results%20to%20baselines%20pre-trained%20on%20a%20vast%0Acollection%20of%20videos%20under%208%20commonly%20used%20metrics.%20Besides%2C%20PoseCrafter%20can%0Afollow%20poses%20from%20different%20individuals%20or%20artificial%20edits%20and%20simultaneously%0Aretain%20the%20human%20identity%20in%20an%20open-domain%20training%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14582v1&entry.124074799=Read"},
{"title": "K-band: Self-supervised MRI Reconstruction via Stochastic Gradient\n  Descent over K-space Subsets", "author": "Frederic Wang and Han Qi and Alfredo De Goyeneche and Reinhard Heckel and Michael Lustig and Efrat Shimron", "abstract": "  Although deep learning (DL) methods are powerful for solving inverse\nproblems, their reliance on high-quality training data is a major hurdle. This\nis significant in high-dimensional (dynamic/volumetric) magnetic resonance\nimaging (MRI), where acquisition of high-resolution fully sampled k-space data\nis impractical. We introduce a novel mathematical framework, dubbed k-band,\nthat enables training DL models using only partial, limited-resolution k-space\ndata. Specifically, we introduce training with stochastic gradient descent\n(SGD) over k-space subsets. In each training iteration, rather than using the\nfully sampled k-space for computing gradients, we use only a small k-space\nportion. This concept is compatible with different sampling strategies; here we\ndemonstrate the method for k-space \"bands\", which have limited resolution in\none dimension and can hence be acquired rapidly. We prove analytically that our\nmethod stochastically approximates the gradients computed in a fully-supervised\nsetup, when two simple conditions are met: (i) the limited-resolution axis is\nchosen randomly-uniformly for every new scan, hence k-space is fully covered\nacross the entire training set, and (ii) the loss function is weighed with a\nmask, derived here analytically, which facilitates accurate reconstruction of\nhigh-resolution details. Numerical experiments with raw MRI data indicate that\nk-band outperforms two other methods trained on limited-resolution data and\nperforms comparably to state-of-the-art (SoTA) methods trained on\nhigh-resolution data. k-band hence obtains SoTA performance, with the advantage\nof training using only limited-resolution data. This work hence introduces a\npractical, easy-to-implement, self-supervised training framework, which\ninvolves fast acquisition and self-supervised reconstruction and offers\ntheoretical guarantees.\n", "link": "http://arxiv.org/abs/2308.02958v3", "date": "2024-05-23", "relevancy": 2.6668, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5244}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K-band%3A%20Self-supervised%20MRI%20Reconstruction%20via%20Stochastic%20Gradient%0A%20%20Descent%20over%20K-space%20Subsets&body=Title%3A%20K-band%3A%20Self-supervised%20MRI%20Reconstruction%20via%20Stochastic%20Gradient%0A%20%20Descent%20over%20K-space%20Subsets%0AAuthor%3A%20Frederic%20Wang%20and%20Han%20Qi%20and%20Alfredo%20De%20Goyeneche%20and%20Reinhard%20Heckel%20and%20Michael%20Lustig%20and%20Efrat%20Shimron%0AAbstract%3A%20%20%20Although%20deep%20learning%20%28DL%29%20methods%20are%20powerful%20for%20solving%20inverse%0Aproblems%2C%20their%20reliance%20on%20high-quality%20training%20data%20is%20a%20major%20hurdle.%20This%0Ais%20significant%20in%20high-dimensional%20%28dynamic/volumetric%29%20magnetic%20resonance%0Aimaging%20%28MRI%29%2C%20where%20acquisition%20of%20high-resolution%20fully%20sampled%20k-space%20data%0Ais%20impractical.%20We%20introduce%20a%20novel%20mathematical%20framework%2C%20dubbed%20k-band%2C%0Athat%20enables%20training%20DL%20models%20using%20only%20partial%2C%20limited-resolution%20k-space%0Adata.%20Specifically%2C%20we%20introduce%20training%20with%20stochastic%20gradient%20descent%0A%28SGD%29%20over%20k-space%20subsets.%20In%20each%20training%20iteration%2C%20rather%20than%20using%20the%0Afully%20sampled%20k-space%20for%20computing%20gradients%2C%20we%20use%20only%20a%20small%20k-space%0Aportion.%20This%20concept%20is%20compatible%20with%20different%20sampling%20strategies%3B%20here%20we%0Ademonstrate%20the%20method%20for%20k-space%20%22bands%22%2C%20which%20have%20limited%20resolution%20in%0Aone%20dimension%20and%20can%20hence%20be%20acquired%20rapidly.%20We%20prove%20analytically%20that%20our%0Amethod%20stochastically%20approximates%20the%20gradients%20computed%20in%20a%20fully-supervised%0Asetup%2C%20when%20two%20simple%20conditions%20are%20met%3A%20%28i%29%20the%20limited-resolution%20axis%20is%0Achosen%20randomly-uniformly%20for%20every%20new%20scan%2C%20hence%20k-space%20is%20fully%20covered%0Aacross%20the%20entire%20training%20set%2C%20and%20%28ii%29%20the%20loss%20function%20is%20weighed%20with%20a%0Amask%2C%20derived%20here%20analytically%2C%20which%20facilitates%20accurate%20reconstruction%20of%0Ahigh-resolution%20details.%20Numerical%20experiments%20with%20raw%20MRI%20data%20indicate%20that%0Ak-band%20outperforms%20two%20other%20methods%20trained%20on%20limited-resolution%20data%20and%0Aperforms%20comparably%20to%20state-of-the-art%20%28SoTA%29%20methods%20trained%20on%0Ahigh-resolution%20data.%20k-band%20hence%20obtains%20SoTA%20performance%2C%20with%20the%20advantage%0Aof%20training%20using%20only%20limited-resolution%20data.%20This%20work%20hence%20introduces%20a%0Apractical%2C%20easy-to-implement%2C%20self-supervised%20training%20framework%2C%20which%0Ainvolves%20fast%20acquisition%20and%20self-supervised%20reconstruction%20and%20offers%0Atheoretical%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.02958v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK-band%253A%2520Self-supervised%2520MRI%2520Reconstruction%2520via%2520Stochastic%2520Gradient%250A%2520%2520Descent%2520over%2520K-space%2520Subsets%26entry.906535625%3DFrederic%2520Wang%2520and%2520Han%2520Qi%2520and%2520Alfredo%2520De%2520Goyeneche%2520and%2520Reinhard%2520Heckel%2520and%2520Michael%2520Lustig%2520and%2520Efrat%2520Shimron%26entry.1292438233%3D%2520%2520Although%2520deep%2520learning%2520%2528DL%2529%2520methods%2520are%2520powerful%2520for%2520solving%2520inverse%250Aproblems%252C%2520their%2520reliance%2520on%2520high-quality%2520training%2520data%2520is%2520a%2520major%2520hurdle.%2520This%250Ais%2520significant%2520in%2520high-dimensional%2520%2528dynamic/volumetric%2529%2520magnetic%2520resonance%250Aimaging%2520%2528MRI%2529%252C%2520where%2520acquisition%2520of%2520high-resolution%2520fully%2520sampled%2520k-space%2520data%250Ais%2520impractical.%2520We%2520introduce%2520a%2520novel%2520mathematical%2520framework%252C%2520dubbed%2520k-band%252C%250Athat%2520enables%2520training%2520DL%2520models%2520using%2520only%2520partial%252C%2520limited-resolution%2520k-space%250Adata.%2520Specifically%252C%2520we%2520introduce%2520training%2520with%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529%2520over%2520k-space%2520subsets.%2520In%2520each%2520training%2520iteration%252C%2520rather%2520than%2520using%2520the%250Afully%2520sampled%2520k-space%2520for%2520computing%2520gradients%252C%2520we%2520use%2520only%2520a%2520small%2520k-space%250Aportion.%2520This%2520concept%2520is%2520compatible%2520with%2520different%2520sampling%2520strategies%253B%2520here%2520we%250Ademonstrate%2520the%2520method%2520for%2520k-space%2520%2522bands%2522%252C%2520which%2520have%2520limited%2520resolution%2520in%250Aone%2520dimension%2520and%2520can%2520hence%2520be%2520acquired%2520rapidly.%2520We%2520prove%2520analytically%2520that%2520our%250Amethod%2520stochastically%2520approximates%2520the%2520gradients%2520computed%2520in%2520a%2520fully-supervised%250Asetup%252C%2520when%2520two%2520simple%2520conditions%2520are%2520met%253A%2520%2528i%2529%2520the%2520limited-resolution%2520axis%2520is%250Achosen%2520randomly-uniformly%2520for%2520every%2520new%2520scan%252C%2520hence%2520k-space%2520is%2520fully%2520covered%250Aacross%2520the%2520entire%2520training%2520set%252C%2520and%2520%2528ii%2529%2520the%2520loss%2520function%2520is%2520weighed%2520with%2520a%250Amask%252C%2520derived%2520here%2520analytically%252C%2520which%2520facilitates%2520accurate%2520reconstruction%2520of%250Ahigh-resolution%2520details.%2520Numerical%2520experiments%2520with%2520raw%2520MRI%2520data%2520indicate%2520that%250Ak-band%2520outperforms%2520two%2520other%2520methods%2520trained%2520on%2520limited-resolution%2520data%2520and%250Aperforms%2520comparably%2520to%2520state-of-the-art%2520%2528SoTA%2529%2520methods%2520trained%2520on%250Ahigh-resolution%2520data.%2520k-band%2520hence%2520obtains%2520SoTA%2520performance%252C%2520with%2520the%2520advantage%250Aof%2520training%2520using%2520only%2520limited-resolution%2520data.%2520This%2520work%2520hence%2520introduces%2520a%250Apractical%252C%2520easy-to-implement%252C%2520self-supervised%2520training%2520framework%252C%2520which%250Ainvolves%2520fast%2520acquisition%2520and%2520self-supervised%2520reconstruction%2520and%2520offers%250Atheoretical%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.02958v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K-band%3A%20Self-supervised%20MRI%20Reconstruction%20via%20Stochastic%20Gradient%0A%20%20Descent%20over%20K-space%20Subsets&entry.906535625=Frederic%20Wang%20and%20Han%20Qi%20and%20Alfredo%20De%20Goyeneche%20and%20Reinhard%20Heckel%20and%20Michael%20Lustig%20and%20Efrat%20Shimron&entry.1292438233=%20%20Although%20deep%20learning%20%28DL%29%20methods%20are%20powerful%20for%20solving%20inverse%0Aproblems%2C%20their%20reliance%20on%20high-quality%20training%20data%20is%20a%20major%20hurdle.%20This%0Ais%20significant%20in%20high-dimensional%20%28dynamic/volumetric%29%20magnetic%20resonance%0Aimaging%20%28MRI%29%2C%20where%20acquisition%20of%20high-resolution%20fully%20sampled%20k-space%20data%0Ais%20impractical.%20We%20introduce%20a%20novel%20mathematical%20framework%2C%20dubbed%20k-band%2C%0Athat%20enables%20training%20DL%20models%20using%20only%20partial%2C%20limited-resolution%20k-space%0Adata.%20Specifically%2C%20we%20introduce%20training%20with%20stochastic%20gradient%20descent%0A%28SGD%29%20over%20k-space%20subsets.%20In%20each%20training%20iteration%2C%20rather%20than%20using%20the%0Afully%20sampled%20k-space%20for%20computing%20gradients%2C%20we%20use%20only%20a%20small%20k-space%0Aportion.%20This%20concept%20is%20compatible%20with%20different%20sampling%20strategies%3B%20here%20we%0Ademonstrate%20the%20method%20for%20k-space%20%22bands%22%2C%20which%20have%20limited%20resolution%20in%0Aone%20dimension%20and%20can%20hence%20be%20acquired%20rapidly.%20We%20prove%20analytically%20that%20our%0Amethod%20stochastically%20approximates%20the%20gradients%20computed%20in%20a%20fully-supervised%0Asetup%2C%20when%20two%20simple%20conditions%20are%20met%3A%20%28i%29%20the%20limited-resolution%20axis%20is%0Achosen%20randomly-uniformly%20for%20every%20new%20scan%2C%20hence%20k-space%20is%20fully%20covered%0Aacross%20the%20entire%20training%20set%2C%20and%20%28ii%29%20the%20loss%20function%20is%20weighed%20with%20a%0Amask%2C%20derived%20here%20analytically%2C%20which%20facilitates%20accurate%20reconstruction%20of%0Ahigh-resolution%20details.%20Numerical%20experiments%20with%20raw%20MRI%20data%20indicate%20that%0Ak-band%20outperforms%20two%20other%20methods%20trained%20on%20limited-resolution%20data%20and%0Aperforms%20comparably%20to%20state-of-the-art%20%28SoTA%29%20methods%20trained%20on%0Ahigh-resolution%20data.%20k-band%20hence%20obtains%20SoTA%20performance%2C%20with%20the%20advantage%0Aof%20training%20using%20only%20limited-resolution%20data.%20This%20work%20hence%20introduces%20a%0Apractical%2C%20easy-to-implement%2C%20self-supervised%20training%20framework%2C%20which%0Ainvolves%20fast%20acquisition%20and%20self-supervised%20reconstruction%20and%20offers%0Atheoretical%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02958v3&entry.124074799=Read"},
{"title": "Neural Directional Encoding for Efficient and Accurate View-Dependent\n  Appearance Modeling", "author": "Liwen Wu and Sai Bi and Zexiang Xu and Fujun Luan and Kai Zhang and Iliyan Georgiev and Kalyan Sunkavalli and Ravi Ramamoorthi", "abstract": "  Novel-view synthesis of specular objects like shiny metals or glossy paints\nremains a significant challenge. Not only the glossy appearance but also global\nillumination effects, including reflections of other objects in the\nenvironment, are critical components to faithfully reproduce a scene. In this\npaper, we present Neural Directional Encoding (NDE), a view-dependent\nappearance encoding of neural radiance fields (NeRF) for rendering specular\nobjects. NDE transfers the concept of feature-grid-based spatial encoding to\nthe angular domain, significantly improving the ability to model high-frequency\nangular signals. In contrast to previous methods that use encoding functions\nwith only angular input, we additionally cone-trace spatial features to obtain\na spatially varying directional encoding, which addresses the challenging\ninterreflection effects. Extensive experiments on both synthetic and real\ndatasets show that a NeRF model with NDE (1) outperforms the state of the art\non view synthesis of specular objects, and (2) works with small networks to\nallow fast (real-time) inference. The project webpage and source code are\navailable at: \\url{https://lwwu2.github.io/nde/}.\n", "link": "http://arxiv.org/abs/2405.14847v1", "date": "2024-05-23", "relevancy": 2.6609, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5419}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5312}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Directional%20Encoding%20for%20Efficient%20and%20Accurate%20View-Dependent%0A%20%20Appearance%20Modeling&body=Title%3A%20Neural%20Directional%20Encoding%20for%20Efficient%20and%20Accurate%20View-Dependent%0A%20%20Appearance%20Modeling%0AAuthor%3A%20Liwen%20Wu%20and%20Sai%20Bi%20and%20Zexiang%20Xu%20and%20Fujun%20Luan%20and%20Kai%20Zhang%20and%20Iliyan%20Georgiev%20and%20Kalyan%20Sunkavalli%20and%20Ravi%20Ramamoorthi%0AAbstract%3A%20%20%20Novel-view%20synthesis%20of%20specular%20objects%20like%20shiny%20metals%20or%20glossy%20paints%0Aremains%20a%20significant%20challenge.%20Not%20only%20the%20glossy%20appearance%20but%20also%20global%0Aillumination%20effects%2C%20including%20reflections%20of%20other%20objects%20in%20the%0Aenvironment%2C%20are%20critical%20components%20to%20faithfully%20reproduce%20a%20scene.%20In%20this%0Apaper%2C%20we%20present%20Neural%20Directional%20Encoding%20%28NDE%29%2C%20a%20view-dependent%0Aappearance%20encoding%20of%20neural%20radiance%20fields%20%28NeRF%29%20for%20rendering%20specular%0Aobjects.%20NDE%20transfers%20the%20concept%20of%20feature-grid-based%20spatial%20encoding%20to%0Athe%20angular%20domain%2C%20significantly%20improving%20the%20ability%20to%20model%20high-frequency%0Aangular%20signals.%20In%20contrast%20to%20previous%20methods%20that%20use%20encoding%20functions%0Awith%20only%20angular%20input%2C%20we%20additionally%20cone-trace%20spatial%20features%20to%20obtain%0Aa%20spatially%20varying%20directional%20encoding%2C%20which%20addresses%20the%20challenging%0Ainterreflection%20effects.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20show%20that%20a%20NeRF%20model%20with%20NDE%20%281%29%20outperforms%20the%20state%20of%20the%20art%0Aon%20view%20synthesis%20of%20specular%20objects%2C%20and%20%282%29%20works%20with%20small%20networks%20to%0Aallow%20fast%20%28real-time%29%20inference.%20The%20project%20webpage%20and%20source%20code%20are%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//lwwu2.github.io/nde/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Directional%2520Encoding%2520for%2520Efficient%2520and%2520Accurate%2520View-Dependent%250A%2520%2520Appearance%2520Modeling%26entry.906535625%3DLiwen%2520Wu%2520and%2520Sai%2520Bi%2520and%2520Zexiang%2520Xu%2520and%2520Fujun%2520Luan%2520and%2520Kai%2520Zhang%2520and%2520Iliyan%2520Georgiev%2520and%2520Kalyan%2520Sunkavalli%2520and%2520Ravi%2520Ramamoorthi%26entry.1292438233%3D%2520%2520Novel-view%2520synthesis%2520of%2520specular%2520objects%2520like%2520shiny%2520metals%2520or%2520glossy%2520paints%250Aremains%2520a%2520significant%2520challenge.%2520Not%2520only%2520the%2520glossy%2520appearance%2520but%2520also%2520global%250Aillumination%2520effects%252C%2520including%2520reflections%2520of%2520other%2520objects%2520in%2520the%250Aenvironment%252C%2520are%2520critical%2520components%2520to%2520faithfully%2520reproduce%2520a%2520scene.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Neural%2520Directional%2520Encoding%2520%2528NDE%2529%252C%2520a%2520view-dependent%250Aappearance%2520encoding%2520of%2520neural%2520radiance%2520fields%2520%2528NeRF%2529%2520for%2520rendering%2520specular%250Aobjects.%2520NDE%2520transfers%2520the%2520concept%2520of%2520feature-grid-based%2520spatial%2520encoding%2520to%250Athe%2520angular%2520domain%252C%2520significantly%2520improving%2520the%2520ability%2520to%2520model%2520high-frequency%250Aangular%2520signals.%2520In%2520contrast%2520to%2520previous%2520methods%2520that%2520use%2520encoding%2520functions%250Awith%2520only%2520angular%2520input%252C%2520we%2520additionally%2520cone-trace%2520spatial%2520features%2520to%2520obtain%250Aa%2520spatially%2520varying%2520directional%2520encoding%252C%2520which%2520addresses%2520the%2520challenging%250Ainterreflection%2520effects.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%250Adatasets%2520show%2520that%2520a%2520NeRF%2520model%2520with%2520NDE%2520%25281%2529%2520outperforms%2520the%2520state%2520of%2520the%2520art%250Aon%2520view%2520synthesis%2520of%2520specular%2520objects%252C%2520and%2520%25282%2529%2520works%2520with%2520small%2520networks%2520to%250Aallow%2520fast%2520%2528real-time%2529%2520inference.%2520The%2520project%2520webpage%2520and%2520source%2520code%2520are%250Aavailable%2520at%253A%2520%255Curl%257Bhttps%253A//lwwu2.github.io/nde/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Directional%20Encoding%20for%20Efficient%20and%20Accurate%20View-Dependent%0A%20%20Appearance%20Modeling&entry.906535625=Liwen%20Wu%20and%20Sai%20Bi%20and%20Zexiang%20Xu%20and%20Fujun%20Luan%20and%20Kai%20Zhang%20and%20Iliyan%20Georgiev%20and%20Kalyan%20Sunkavalli%20and%20Ravi%20Ramamoorthi&entry.1292438233=%20%20Novel-view%20synthesis%20of%20specular%20objects%20like%20shiny%20metals%20or%20glossy%20paints%0Aremains%20a%20significant%20challenge.%20Not%20only%20the%20glossy%20appearance%20but%20also%20global%0Aillumination%20effects%2C%20including%20reflections%20of%20other%20objects%20in%20the%0Aenvironment%2C%20are%20critical%20components%20to%20faithfully%20reproduce%20a%20scene.%20In%20this%0Apaper%2C%20we%20present%20Neural%20Directional%20Encoding%20%28NDE%29%2C%20a%20view-dependent%0Aappearance%20encoding%20of%20neural%20radiance%20fields%20%28NeRF%29%20for%20rendering%20specular%0Aobjects.%20NDE%20transfers%20the%20concept%20of%20feature-grid-based%20spatial%20encoding%20to%0Athe%20angular%20domain%2C%20significantly%20improving%20the%20ability%20to%20model%20high-frequency%0Aangular%20signals.%20In%20contrast%20to%20previous%20methods%20that%20use%20encoding%20functions%0Awith%20only%20angular%20input%2C%20we%20additionally%20cone-trace%20spatial%20features%20to%20obtain%0Aa%20spatially%20varying%20directional%20encoding%2C%20which%20addresses%20the%20challenging%0Ainterreflection%20effects.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20show%20that%20a%20NeRF%20model%20with%20NDE%20%281%29%20outperforms%20the%20state%20of%20the%20art%0Aon%20view%20synthesis%20of%20specular%20objects%2C%20and%20%282%29%20works%20with%20small%20networks%20to%0Aallow%20fast%20%28real-time%29%20inference.%20The%20project%20webpage%20and%20source%20code%20are%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//lwwu2.github.io/nde/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14847v1&entry.124074799=Read"},
{"title": "PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal\n  Prediction Hypothesis", "author": "Satoki Ishikawa and Makoto Yamada and Han Bao and Yuki Takezawa", "abstract": "  SimSiam is a prominent self-supervised learning method that achieves\nimpressive results in various vision tasks under static environments. However,\nit has two critical issues: high sensitivity to hyperparameters, especially\nweight decay, and unsatisfactory performance in online and continual learning,\nwhere neuroscientists believe that powerful memory functions are necessary, as\nin brains. In this paper, we propose PhiNet, inspired by a hippocampal model\nbased on the temporal prediction hypothesis. Unlike SimSiam, which aligns two\naugmented views of the original image, PhiNet integrates an additional\npredictor block that estimates the original image representation to imitate the\nCA1 region in the hippocampus. Moreover, we model the neocortex inspired by the\nComplementary Learning Systems theory with a momentum encoder block as a slow\nlearner, which works as long-term memory. We demonstrate through analysing the\nlearning dynamics that PhiNet benefits from the additional predictor to prevent\nthe complete collapse of learned representations, a notorious challenge in\nnon-contrastive learning. This dynamics analysis may partially corroborate why\nthis hippocampal model is biologically plausible. Experimental results\ndemonstrate that PhiNet is more robust to weight decay and performs better than\nSimSiam in memory-intensive tasks like online and continual learning.\n", "link": "http://arxiv.org/abs/2405.14650v1", "date": "2024-05-23", "relevancy": 2.646, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5407}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhiNets%3A%20Brain-inspired%20Non-contrastive%20Learning%20Based%20on%20Temporal%0A%20%20Prediction%20Hypothesis&body=Title%3A%20PhiNets%3A%20Brain-inspired%20Non-contrastive%20Learning%20Based%20on%20Temporal%0A%20%20Prediction%20Hypothesis%0AAuthor%3A%20Satoki%20Ishikawa%20and%20Makoto%20Yamada%20and%20Han%20Bao%20and%20Yuki%20Takezawa%0AAbstract%3A%20%20%20SimSiam%20is%20a%20prominent%20self-supervised%20learning%20method%20that%20achieves%0Aimpressive%20results%20in%20various%20vision%20tasks%20under%20static%20environments.%20However%2C%0Ait%20has%20two%20critical%20issues%3A%20high%20sensitivity%20to%20hyperparameters%2C%20especially%0Aweight%20decay%2C%20and%20unsatisfactory%20performance%20in%20online%20and%20continual%20learning%2C%0Awhere%20neuroscientists%20believe%20that%20powerful%20memory%20functions%20are%20necessary%2C%20as%0Ain%20brains.%20In%20this%20paper%2C%20we%20propose%20PhiNet%2C%20inspired%20by%20a%20hippocampal%20model%0Abased%20on%20the%20temporal%20prediction%20hypothesis.%20Unlike%20SimSiam%2C%20which%20aligns%20two%0Aaugmented%20views%20of%20the%20original%20image%2C%20PhiNet%20integrates%20an%20additional%0Apredictor%20block%20that%20estimates%20the%20original%20image%20representation%20to%20imitate%20the%0ACA1%20region%20in%20the%20hippocampus.%20Moreover%2C%20we%20model%20the%20neocortex%20inspired%20by%20the%0AComplementary%20Learning%20Systems%20theory%20with%20a%20momentum%20encoder%20block%20as%20a%20slow%0Alearner%2C%20which%20works%20as%20long-term%20memory.%20We%20demonstrate%20through%20analysing%20the%0Alearning%20dynamics%20that%20PhiNet%20benefits%20from%20the%20additional%20predictor%20to%20prevent%0Athe%20complete%20collapse%20of%20learned%20representations%2C%20a%20notorious%20challenge%20in%0Anon-contrastive%20learning.%20This%20dynamics%20analysis%20may%20partially%20corroborate%20why%0Athis%20hippocampal%20model%20is%20biologically%20plausible.%20Experimental%20results%0Ademonstrate%20that%20PhiNet%20is%20more%20robust%20to%20weight%20decay%20and%20performs%20better%20than%0ASimSiam%20in%20memory-intensive%20tasks%20like%20online%20and%20continual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhiNets%253A%2520Brain-inspired%2520Non-contrastive%2520Learning%2520Based%2520on%2520Temporal%250A%2520%2520Prediction%2520Hypothesis%26entry.906535625%3DSatoki%2520Ishikawa%2520and%2520Makoto%2520Yamada%2520and%2520Han%2520Bao%2520and%2520Yuki%2520Takezawa%26entry.1292438233%3D%2520%2520SimSiam%2520is%2520a%2520prominent%2520self-supervised%2520learning%2520method%2520that%2520achieves%250Aimpressive%2520results%2520in%2520various%2520vision%2520tasks%2520under%2520static%2520environments.%2520However%252C%250Ait%2520has%2520two%2520critical%2520issues%253A%2520high%2520sensitivity%2520to%2520hyperparameters%252C%2520especially%250Aweight%2520decay%252C%2520and%2520unsatisfactory%2520performance%2520in%2520online%2520and%2520continual%2520learning%252C%250Awhere%2520neuroscientists%2520believe%2520that%2520powerful%2520memory%2520functions%2520are%2520necessary%252C%2520as%250Ain%2520brains.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PhiNet%252C%2520inspired%2520by%2520a%2520hippocampal%2520model%250Abased%2520on%2520the%2520temporal%2520prediction%2520hypothesis.%2520Unlike%2520SimSiam%252C%2520which%2520aligns%2520two%250Aaugmented%2520views%2520of%2520the%2520original%2520image%252C%2520PhiNet%2520integrates%2520an%2520additional%250Apredictor%2520block%2520that%2520estimates%2520the%2520original%2520image%2520representation%2520to%2520imitate%2520the%250ACA1%2520region%2520in%2520the%2520hippocampus.%2520Moreover%252C%2520we%2520model%2520the%2520neocortex%2520inspired%2520by%2520the%250AComplementary%2520Learning%2520Systems%2520theory%2520with%2520a%2520momentum%2520encoder%2520block%2520as%2520a%2520slow%250Alearner%252C%2520which%2520works%2520as%2520long-term%2520memory.%2520We%2520demonstrate%2520through%2520analysing%2520the%250Alearning%2520dynamics%2520that%2520PhiNet%2520benefits%2520from%2520the%2520additional%2520predictor%2520to%2520prevent%250Athe%2520complete%2520collapse%2520of%2520learned%2520representations%252C%2520a%2520notorious%2520challenge%2520in%250Anon-contrastive%2520learning.%2520This%2520dynamics%2520analysis%2520may%2520partially%2520corroborate%2520why%250Athis%2520hippocampal%2520model%2520is%2520biologically%2520plausible.%2520Experimental%2520results%250Ademonstrate%2520that%2520PhiNet%2520is%2520more%2520robust%2520to%2520weight%2520decay%2520and%2520performs%2520better%2520than%250ASimSiam%2520in%2520memory-intensive%2520tasks%2520like%2520online%2520and%2520continual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhiNets%3A%20Brain-inspired%20Non-contrastive%20Learning%20Based%20on%20Temporal%0A%20%20Prediction%20Hypothesis&entry.906535625=Satoki%20Ishikawa%20and%20Makoto%20Yamada%20and%20Han%20Bao%20and%20Yuki%20Takezawa&entry.1292438233=%20%20SimSiam%20is%20a%20prominent%20self-supervised%20learning%20method%20that%20achieves%0Aimpressive%20results%20in%20various%20vision%20tasks%20under%20static%20environments.%20However%2C%0Ait%20has%20two%20critical%20issues%3A%20high%20sensitivity%20to%20hyperparameters%2C%20especially%0Aweight%20decay%2C%20and%20unsatisfactory%20performance%20in%20online%20and%20continual%20learning%2C%0Awhere%20neuroscientists%20believe%20that%20powerful%20memory%20functions%20are%20necessary%2C%20as%0Ain%20brains.%20In%20this%20paper%2C%20we%20propose%20PhiNet%2C%20inspired%20by%20a%20hippocampal%20model%0Abased%20on%20the%20temporal%20prediction%20hypothesis.%20Unlike%20SimSiam%2C%20which%20aligns%20two%0Aaugmented%20views%20of%20the%20original%20image%2C%20PhiNet%20integrates%20an%20additional%0Apredictor%20block%20that%20estimates%20the%20original%20image%20representation%20to%20imitate%20the%0ACA1%20region%20in%20the%20hippocampus.%20Moreover%2C%20we%20model%20the%20neocortex%20inspired%20by%20the%0AComplementary%20Learning%20Systems%20theory%20with%20a%20momentum%20encoder%20block%20as%20a%20slow%0Alearner%2C%20which%20works%20as%20long-term%20memory.%20We%20demonstrate%20through%20analysing%20the%0Alearning%20dynamics%20that%20PhiNet%20benefits%20from%20the%20additional%20predictor%20to%20prevent%0Athe%20complete%20collapse%20of%20learned%20representations%2C%20a%20notorious%20challenge%20in%0Anon-contrastive%20learning.%20This%20dynamics%20analysis%20may%20partially%20corroborate%20why%0Athis%20hippocampal%20model%20is%20biologically%20plausible.%20Experimental%20results%0Ademonstrate%20that%20PhiNet%20is%20more%20robust%20to%20weight%20decay%20and%20performs%20better%20than%0ASimSiam%20in%20memory-intensive%20tasks%20like%20online%20and%20continual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14650v1&entry.124074799=Read"},
{"title": "AdaAugment: A Tuning-Free and Adaptive Approach to Enhance Data\n  Augmentation", "author": "Suorong Yang and Peijia Li and Xin Xiong and Furao Shen and Jian Zhao", "abstract": "  Data augmentation (DA) is widely employed to improve the generalization\nperformance of deep models. However, most existing DA methods use augmentation\noperations with random magnitudes throughout training. While this fosters\ndiversity, it can also inevitably introduce uncontrolled variability in\naugmented data, which may cause misalignment with the evolving training status\nof the target models. Both theoretical and empirical findings suggest that this\nmisalignment increases the risks of underfitting and overfitting. To address\nthese limitations, we propose AdaAugment, an innovative and tuning-free\nAdaptive Augmentation method that utilizes reinforcement learning to\ndynamically adjust augmentation magnitudes for individual training samples\nbased on real-time feedback from the target network. Specifically, AdaAugment\nfeatures a dual-model architecture consisting of a policy network and a target\nnetwork, which are jointly optimized to effectively adapt augmentation\nmagnitudes. The policy network optimizes the variability within the augmented\ndata, while the target network utilizes the adaptively augmented samples for\ntraining. Extensive experiments across benchmark datasets and deep\narchitectures demonstrate that AdaAugment consistently outperforms other\nstate-of-the-art DA methods in effectiveness while maintaining remarkable\nefficiency.\n", "link": "http://arxiv.org/abs/2405.11467v2", "date": "2024-05-23", "relevancy": 2.6392, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5438}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5231}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaAugment%3A%20A%20Tuning-Free%20and%20Adaptive%20Approach%20to%20Enhance%20Data%0A%20%20Augmentation&body=Title%3A%20AdaAugment%3A%20A%20Tuning-Free%20and%20Adaptive%20Approach%20to%20Enhance%20Data%0A%20%20Augmentation%0AAuthor%3A%20Suorong%20Yang%20and%20Peijia%20Li%20and%20Xin%20Xiong%20and%20Furao%20Shen%20and%20Jian%20Zhao%0AAbstract%3A%20%20%20Data%20augmentation%20%28DA%29%20is%20widely%20employed%20to%20improve%20the%20generalization%0Aperformance%20of%20deep%20models.%20However%2C%20most%20existing%20DA%20methods%20use%20augmentation%0Aoperations%20with%20random%20magnitudes%20throughout%20training.%20While%20this%20fosters%0Adiversity%2C%20it%20can%20also%20inevitably%20introduce%20uncontrolled%20variability%20in%0Aaugmented%20data%2C%20which%20may%20cause%20misalignment%20with%20the%20evolving%20training%20status%0Aof%20the%20target%20models.%20Both%20theoretical%20and%20empirical%20findings%20suggest%20that%20this%0Amisalignment%20increases%20the%20risks%20of%20underfitting%20and%20overfitting.%20To%20address%0Athese%20limitations%2C%20we%20propose%20AdaAugment%2C%20an%20innovative%20and%20tuning-free%0AAdaptive%20Augmentation%20method%20that%20utilizes%20reinforcement%20learning%20to%0Adynamically%20adjust%20augmentation%20magnitudes%20for%20individual%20training%20samples%0Abased%20on%20real-time%20feedback%20from%20the%20target%20network.%20Specifically%2C%20AdaAugment%0Afeatures%20a%20dual-model%20architecture%20consisting%20of%20a%20policy%20network%20and%20a%20target%0Anetwork%2C%20which%20are%20jointly%20optimized%20to%20effectively%20adapt%20augmentation%0Amagnitudes.%20The%20policy%20network%20optimizes%20the%20variability%20within%20the%20augmented%0Adata%2C%20while%20the%20target%20network%20utilizes%20the%20adaptively%20augmented%20samples%20for%0Atraining.%20Extensive%20experiments%20across%20benchmark%20datasets%20and%20deep%0Aarchitectures%20demonstrate%20that%20AdaAugment%20consistently%20outperforms%20other%0Astate-of-the-art%20DA%20methods%20in%20effectiveness%20while%20maintaining%20remarkable%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaAugment%253A%2520A%2520Tuning-Free%2520and%2520Adaptive%2520Approach%2520to%2520Enhance%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DSuorong%2520Yang%2520and%2520Peijia%2520Li%2520and%2520Xin%2520Xiong%2520and%2520Furao%2520Shen%2520and%2520Jian%2520Zhao%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520%2528DA%2529%2520is%2520widely%2520employed%2520to%2520improve%2520the%2520generalization%250Aperformance%2520of%2520deep%2520models.%2520However%252C%2520most%2520existing%2520DA%2520methods%2520use%2520augmentation%250Aoperations%2520with%2520random%2520magnitudes%2520throughout%2520training.%2520While%2520this%2520fosters%250Adiversity%252C%2520it%2520can%2520also%2520inevitably%2520introduce%2520uncontrolled%2520variability%2520in%250Aaugmented%2520data%252C%2520which%2520may%2520cause%2520misalignment%2520with%2520the%2520evolving%2520training%2520status%250Aof%2520the%2520target%2520models.%2520Both%2520theoretical%2520and%2520empirical%2520findings%2520suggest%2520that%2520this%250Amisalignment%2520increases%2520the%2520risks%2520of%2520underfitting%2520and%2520overfitting.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520AdaAugment%252C%2520an%2520innovative%2520and%2520tuning-free%250AAdaptive%2520Augmentation%2520method%2520that%2520utilizes%2520reinforcement%2520learning%2520to%250Adynamically%2520adjust%2520augmentation%2520magnitudes%2520for%2520individual%2520training%2520samples%250Abased%2520on%2520real-time%2520feedback%2520from%2520the%2520target%2520network.%2520Specifically%252C%2520AdaAugment%250Afeatures%2520a%2520dual-model%2520architecture%2520consisting%2520of%2520a%2520policy%2520network%2520and%2520a%2520target%250Anetwork%252C%2520which%2520are%2520jointly%2520optimized%2520to%2520effectively%2520adapt%2520augmentation%250Amagnitudes.%2520The%2520policy%2520network%2520optimizes%2520the%2520variability%2520within%2520the%2520augmented%250Adata%252C%2520while%2520the%2520target%2520network%2520utilizes%2520the%2520adaptively%2520augmented%2520samples%2520for%250Atraining.%2520Extensive%2520experiments%2520across%2520benchmark%2520datasets%2520and%2520deep%250Aarchitectures%2520demonstrate%2520that%2520AdaAugment%2520consistently%2520outperforms%2520other%250Astate-of-the-art%2520DA%2520methods%2520in%2520effectiveness%2520while%2520maintaining%2520remarkable%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaAugment%3A%20A%20Tuning-Free%20and%20Adaptive%20Approach%20to%20Enhance%20Data%0A%20%20Augmentation&entry.906535625=Suorong%20Yang%20and%20Peijia%20Li%20and%20Xin%20Xiong%20and%20Furao%20Shen%20and%20Jian%20Zhao&entry.1292438233=%20%20Data%20augmentation%20%28DA%29%20is%20widely%20employed%20to%20improve%20the%20generalization%0Aperformance%20of%20deep%20models.%20However%2C%20most%20existing%20DA%20methods%20use%20augmentation%0Aoperations%20with%20random%20magnitudes%20throughout%20training.%20While%20this%20fosters%0Adiversity%2C%20it%20can%20also%20inevitably%20introduce%20uncontrolled%20variability%20in%0Aaugmented%20data%2C%20which%20may%20cause%20misalignment%20with%20the%20evolving%20training%20status%0Aof%20the%20target%20models.%20Both%20theoretical%20and%20empirical%20findings%20suggest%20that%20this%0Amisalignment%20increases%20the%20risks%20of%20underfitting%20and%20overfitting.%20To%20address%0Athese%20limitations%2C%20we%20propose%20AdaAugment%2C%20an%20innovative%20and%20tuning-free%0AAdaptive%20Augmentation%20method%20that%20utilizes%20reinforcement%20learning%20to%0Adynamically%20adjust%20augmentation%20magnitudes%20for%20individual%20training%20samples%0Abased%20on%20real-time%20feedback%20from%20the%20target%20network.%20Specifically%2C%20AdaAugment%0Afeatures%20a%20dual-model%20architecture%20consisting%20of%20a%20policy%20network%20and%20a%20target%0Anetwork%2C%20which%20are%20jointly%20optimized%20to%20effectively%20adapt%20augmentation%0Amagnitudes.%20The%20policy%20network%20optimizes%20the%20variability%20within%20the%20augmented%0Adata%2C%20while%20the%20target%20network%20utilizes%20the%20adaptively%20augmented%20samples%20for%0Atraining.%20Extensive%20experiments%20across%20benchmark%20datasets%20and%20deep%0Aarchitectures%20demonstrate%20that%20AdaAugment%20consistently%20outperforms%20other%0Astate-of-the-art%20DA%20methods%20in%20effectiveness%20while%20maintaining%20remarkable%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11467v2&entry.124074799=Read"},
{"title": "Towards Imperceptible Backdoor Attack in Self-supervised Learning", "author": "Hanrong Zhang and Zhenting Wang and Tingxu Han and Mingyu Jin and Chenlu Zhan and Mengnan Du and Hongwei Wang and Shiqing Ma", "abstract": "  Self-supervised learning models are vulnerable to backdoor attacks. Existing\nbackdoor attacks that are effective in self-supervised learning often involve\nnoticeable triggers, like colored patches, which are vulnerable to human\ninspection. In this paper, we propose an imperceptible and effective backdoor\nattack against self-supervised models. We first find that existing\nimperceptible triggers designed for supervised learning are not as effective in\ncompromising self-supervised models. We then identify this ineffectiveness is\nattributed to the overlap in distributions between the backdoor and augmented\nsamples used in self-supervised learning. Building on this insight, we design\nan attack using optimized triggers that are disentangled to the augmented\ntransformation in the self-supervised learning, while also remaining\nimperceptible to human vision. Experiments on five datasets and seven SSL\nalgorithms demonstrate our attack is highly effective and stealthy. It also has\nstrong resistance to existing backdoor defenses. Our code can be found at\nhttps://github.com/Zhang-Henry/IMPERATIVE.\n", "link": "http://arxiv.org/abs/2405.14672v1", "date": "2024-05-23", "relevancy": 2.6245, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5434}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5165}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Imperceptible%20Backdoor%20Attack%20in%20Self-supervised%20Learning&body=Title%3A%20Towards%20Imperceptible%20Backdoor%20Attack%20in%20Self-supervised%20Learning%0AAuthor%3A%20Hanrong%20Zhang%20and%20Zhenting%20Wang%20and%20Tingxu%20Han%20and%20Mingyu%20Jin%20and%20Chenlu%20Zhan%20and%20Mengnan%20Du%20and%20Hongwei%20Wang%20and%20Shiqing%20Ma%0AAbstract%3A%20%20%20Self-supervised%20learning%20models%20are%20vulnerable%20to%20backdoor%20attacks.%20Existing%0Abackdoor%20attacks%20that%20are%20effective%20in%20self-supervised%20learning%20often%20involve%0Anoticeable%20triggers%2C%20like%20colored%20patches%2C%20which%20are%20vulnerable%20to%20human%0Ainspection.%20In%20this%20paper%2C%20we%20propose%20an%20imperceptible%20and%20effective%20backdoor%0Aattack%20against%20self-supervised%20models.%20We%20first%20find%20that%20existing%0Aimperceptible%20triggers%20designed%20for%20supervised%20learning%20are%20not%20as%20effective%20in%0Acompromising%20self-supervised%20models.%20We%20then%20identify%20this%20ineffectiveness%20is%0Aattributed%20to%20the%20overlap%20in%20distributions%20between%20the%20backdoor%20and%20augmented%0Asamples%20used%20in%20self-supervised%20learning.%20Building%20on%20this%20insight%2C%20we%20design%0Aan%20attack%20using%20optimized%20triggers%20that%20are%20disentangled%20to%20the%20augmented%0Atransformation%20in%20the%20self-supervised%20learning%2C%20while%20also%20remaining%0Aimperceptible%20to%20human%20vision.%20Experiments%20on%20five%20datasets%20and%20seven%20SSL%0Aalgorithms%20demonstrate%20our%20attack%20is%20highly%20effective%20and%20stealthy.%20It%20also%20has%0Astrong%20resistance%20to%20existing%20backdoor%20defenses.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Zhang-Henry/IMPERATIVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Imperceptible%2520Backdoor%2520Attack%2520in%2520Self-supervised%2520Learning%26entry.906535625%3DHanrong%2520Zhang%2520and%2520Zhenting%2520Wang%2520and%2520Tingxu%2520Han%2520and%2520Mingyu%2520Jin%2520and%2520Chenlu%2520Zhan%2520and%2520Mengnan%2520Du%2520and%2520Hongwei%2520Wang%2520and%2520Shiqing%2520Ma%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520models%2520are%2520vulnerable%2520to%2520backdoor%2520attacks.%2520Existing%250Abackdoor%2520attacks%2520that%2520are%2520effective%2520in%2520self-supervised%2520learning%2520often%2520involve%250Anoticeable%2520triggers%252C%2520like%2520colored%2520patches%252C%2520which%2520are%2520vulnerable%2520to%2520human%250Ainspection.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520imperceptible%2520and%2520effective%2520backdoor%250Aattack%2520against%2520self-supervised%2520models.%2520We%2520first%2520find%2520that%2520existing%250Aimperceptible%2520triggers%2520designed%2520for%2520supervised%2520learning%2520are%2520not%2520as%2520effective%2520in%250Acompromising%2520self-supervised%2520models.%2520We%2520then%2520identify%2520this%2520ineffectiveness%2520is%250Aattributed%2520to%2520the%2520overlap%2520in%2520distributions%2520between%2520the%2520backdoor%2520and%2520augmented%250Asamples%2520used%2520in%2520self-supervised%2520learning.%2520Building%2520on%2520this%2520insight%252C%2520we%2520design%250Aan%2520attack%2520using%2520optimized%2520triggers%2520that%2520are%2520disentangled%2520to%2520the%2520augmented%250Atransformation%2520in%2520the%2520self-supervised%2520learning%252C%2520while%2520also%2520remaining%250Aimperceptible%2520to%2520human%2520vision.%2520Experiments%2520on%2520five%2520datasets%2520and%2520seven%2520SSL%250Aalgorithms%2520demonstrate%2520our%2520attack%2520is%2520highly%2520effective%2520and%2520stealthy.%2520It%2520also%2520has%250Astrong%2520resistance%2520to%2520existing%2520backdoor%2520defenses.%2520Our%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/Zhang-Henry/IMPERATIVE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Imperceptible%20Backdoor%20Attack%20in%20Self-supervised%20Learning&entry.906535625=Hanrong%20Zhang%20and%20Zhenting%20Wang%20and%20Tingxu%20Han%20and%20Mingyu%20Jin%20and%20Chenlu%20Zhan%20and%20Mengnan%20Du%20and%20Hongwei%20Wang%20and%20Shiqing%20Ma&entry.1292438233=%20%20Self-supervised%20learning%20models%20are%20vulnerable%20to%20backdoor%20attacks.%20Existing%0Abackdoor%20attacks%20that%20are%20effective%20in%20self-supervised%20learning%20often%20involve%0Anoticeable%20triggers%2C%20like%20colored%20patches%2C%20which%20are%20vulnerable%20to%20human%0Ainspection.%20In%20this%20paper%2C%20we%20propose%20an%20imperceptible%20and%20effective%20backdoor%0Aattack%20against%20self-supervised%20models.%20We%20first%20find%20that%20existing%0Aimperceptible%20triggers%20designed%20for%20supervised%20learning%20are%20not%20as%20effective%20in%0Acompromising%20self-supervised%20models.%20We%20then%20identify%20this%20ineffectiveness%20is%0Aattributed%20to%20the%20overlap%20in%20distributions%20between%20the%20backdoor%20and%20augmented%0Asamples%20used%20in%20self-supervised%20learning.%20Building%20on%20this%20insight%2C%20we%20design%0Aan%20attack%20using%20optimized%20triggers%20that%20are%20disentangled%20to%20the%20augmented%0Atransformation%20in%20the%20self-supervised%20learning%2C%20while%20also%20remaining%0Aimperceptible%20to%20human%20vision.%20Experiments%20on%20five%20datasets%20and%20seven%20SSL%0Aalgorithms%20demonstrate%20our%20attack%20is%20highly%20effective%20and%20stealthy.%20It%20also%20has%0Astrong%20resistance%20to%20existing%20backdoor%20defenses.%20Our%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/Zhang-Henry/IMPERATIVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14672v1&entry.124074799=Read"},
{"title": "Exploring Alignment in Shared Cross-lingual Spaces", "author": "Basel Mousi and Nadir Durrani and Fahim Dalvi and Majd Hawasly and Ahmed Abdelali", "abstract": "  Despite their remarkable ability to capture linguistic nuances across diverse\nlanguages, questions persist regarding the degree of alignment between\nlanguages in multilingual embeddings. Drawing inspiration from research on\nhigh-dimensional representations in neural language models, we employ\nclustering to uncover latent concepts within multilingual models. Our analysis\nfocuses on quantifying the \\textit{alignment} and \\textit{overlap} of these\nconcepts across various languages within the latent space. To this end, we\nintroduce two metrics \\CA{} and \\CO{} aimed at quantifying these aspects,\nenabling a deeper exploration of multilingual embeddings. Our study encompasses\nthree multilingual models (\\texttt{mT5}, \\texttt{mBERT}, and \\texttt{XLM-R})\nand three downstream tasks (Machine Translation, Named Entity Recognition, and\nSentiment Analysis). Key findings from our analysis include: i) deeper layers\nin the network demonstrate increased cross-lingual \\textit{alignment} due to\nthe presence of language-agnostic concepts, ii) fine-tuning of the models\nenhances \\textit{alignment} within the latent space, and iii) such\ntask-specific calibration helps in explaining the emergence of zero-shot\ncapabilities in the models.\\footnote{The code is available at\n\\url{https://github.com/baselmousi/multilingual-latent-concepts}}\n", "link": "http://arxiv.org/abs/2405.14535v1", "date": "2024-05-23", "relevancy": 2.6238, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Alignment%20in%20Shared%20Cross-lingual%20Spaces&body=Title%3A%20Exploring%20Alignment%20in%20Shared%20Cross-lingual%20Spaces%0AAuthor%3A%20Basel%20Mousi%20and%20Nadir%20Durrani%20and%20Fahim%20Dalvi%20and%20Majd%20Hawasly%20and%20Ahmed%20Abdelali%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20ability%20to%20capture%20linguistic%20nuances%20across%20diverse%0Alanguages%2C%20questions%20persist%20regarding%20the%20degree%20of%20alignment%20between%0Alanguages%20in%20multilingual%20embeddings.%20Drawing%20inspiration%20from%20research%20on%0Ahigh-dimensional%20representations%20in%20neural%20language%20models%2C%20we%20employ%0Aclustering%20to%20uncover%20latent%20concepts%20within%20multilingual%20models.%20Our%20analysis%0Afocuses%20on%20quantifying%20the%20%5Ctextit%7Balignment%7D%20and%20%5Ctextit%7Boverlap%7D%20of%20these%0Aconcepts%20across%20various%20languages%20within%20the%20latent%20space.%20To%20this%20end%2C%20we%0Aintroduce%20two%20metrics%20%5CCA%7B%7D%20and%20%5CCO%7B%7D%20aimed%20at%20quantifying%20these%20aspects%2C%0Aenabling%20a%20deeper%20exploration%20of%20multilingual%20embeddings.%20Our%20study%20encompasses%0Athree%20multilingual%20models%20%28%5Ctexttt%7BmT5%7D%2C%20%5Ctexttt%7BmBERT%7D%2C%20and%20%5Ctexttt%7BXLM-R%7D%29%0Aand%20three%20downstream%20tasks%20%28Machine%20Translation%2C%20Named%20Entity%20Recognition%2C%20and%0ASentiment%20Analysis%29.%20Key%20findings%20from%20our%20analysis%20include%3A%20i%29%20deeper%20layers%0Ain%20the%20network%20demonstrate%20increased%20cross-lingual%20%5Ctextit%7Balignment%7D%20due%20to%0Athe%20presence%20of%20language-agnostic%20concepts%2C%20ii%29%20fine-tuning%20of%20the%20models%0Aenhances%20%5Ctextit%7Balignment%7D%20within%20the%20latent%20space%2C%20and%20iii%29%20such%0Atask-specific%20calibration%20helps%20in%20explaining%20the%20emergence%20of%20zero-shot%0Acapabilities%20in%20the%20models.%5Cfootnote%7BThe%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/baselmousi/multilingual-latent-concepts%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Alignment%2520in%2520Shared%2520Cross-lingual%2520Spaces%26entry.906535625%3DBasel%2520Mousi%2520and%2520Nadir%2520Durrani%2520and%2520Fahim%2520Dalvi%2520and%2520Majd%2520Hawasly%2520and%2520Ahmed%2520Abdelali%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520ability%2520to%2520capture%2520linguistic%2520nuances%2520across%2520diverse%250Alanguages%252C%2520questions%2520persist%2520regarding%2520the%2520degree%2520of%2520alignment%2520between%250Alanguages%2520in%2520multilingual%2520embeddings.%2520Drawing%2520inspiration%2520from%2520research%2520on%250Ahigh-dimensional%2520representations%2520in%2520neural%2520language%2520models%252C%2520we%2520employ%250Aclustering%2520to%2520uncover%2520latent%2520concepts%2520within%2520multilingual%2520models.%2520Our%2520analysis%250Afocuses%2520on%2520quantifying%2520the%2520%255Ctextit%257Balignment%257D%2520and%2520%255Ctextit%257Boverlap%257D%2520of%2520these%250Aconcepts%2520across%2520various%2520languages%2520within%2520the%2520latent%2520space.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520two%2520metrics%2520%255CCA%257B%257D%2520and%2520%255CCO%257B%257D%2520aimed%2520at%2520quantifying%2520these%2520aspects%252C%250Aenabling%2520a%2520deeper%2520exploration%2520of%2520multilingual%2520embeddings.%2520Our%2520study%2520encompasses%250Athree%2520multilingual%2520models%2520%2528%255Ctexttt%257BmT5%257D%252C%2520%255Ctexttt%257BmBERT%257D%252C%2520and%2520%255Ctexttt%257BXLM-R%257D%2529%250Aand%2520three%2520downstream%2520tasks%2520%2528Machine%2520Translation%252C%2520Named%2520Entity%2520Recognition%252C%2520and%250ASentiment%2520Analysis%2529.%2520Key%2520findings%2520from%2520our%2520analysis%2520include%253A%2520i%2529%2520deeper%2520layers%250Ain%2520the%2520network%2520demonstrate%2520increased%2520cross-lingual%2520%255Ctextit%257Balignment%257D%2520due%2520to%250Athe%2520presence%2520of%2520language-agnostic%2520concepts%252C%2520ii%2529%2520fine-tuning%2520of%2520the%2520models%250Aenhances%2520%255Ctextit%257Balignment%257D%2520within%2520the%2520latent%2520space%252C%2520and%2520iii%2529%2520such%250Atask-specific%2520calibration%2520helps%2520in%2520explaining%2520the%2520emergence%2520of%2520zero-shot%250Acapabilities%2520in%2520the%2520models.%255Cfootnote%257BThe%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/baselmousi/multilingual-latent-concepts%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Alignment%20in%20Shared%20Cross-lingual%20Spaces&entry.906535625=Basel%20Mousi%20and%20Nadir%20Durrani%20and%20Fahim%20Dalvi%20and%20Majd%20Hawasly%20and%20Ahmed%20Abdelali&entry.1292438233=%20%20Despite%20their%20remarkable%20ability%20to%20capture%20linguistic%20nuances%20across%20diverse%0Alanguages%2C%20questions%20persist%20regarding%20the%20degree%20of%20alignment%20between%0Alanguages%20in%20multilingual%20embeddings.%20Drawing%20inspiration%20from%20research%20on%0Ahigh-dimensional%20representations%20in%20neural%20language%20models%2C%20we%20employ%0Aclustering%20to%20uncover%20latent%20concepts%20within%20multilingual%20models.%20Our%20analysis%0Afocuses%20on%20quantifying%20the%20%5Ctextit%7Balignment%7D%20and%20%5Ctextit%7Boverlap%7D%20of%20these%0Aconcepts%20across%20various%20languages%20within%20the%20latent%20space.%20To%20this%20end%2C%20we%0Aintroduce%20two%20metrics%20%5CCA%7B%7D%20and%20%5CCO%7B%7D%20aimed%20at%20quantifying%20these%20aspects%2C%0Aenabling%20a%20deeper%20exploration%20of%20multilingual%20embeddings.%20Our%20study%20encompasses%0Athree%20multilingual%20models%20%28%5Ctexttt%7BmT5%7D%2C%20%5Ctexttt%7BmBERT%7D%2C%20and%20%5Ctexttt%7BXLM-R%7D%29%0Aand%20three%20downstream%20tasks%20%28Machine%20Translation%2C%20Named%20Entity%20Recognition%2C%20and%0ASentiment%20Analysis%29.%20Key%20findings%20from%20our%20analysis%20include%3A%20i%29%20deeper%20layers%0Ain%20the%20network%20demonstrate%20increased%20cross-lingual%20%5Ctextit%7Balignment%7D%20due%20to%0Athe%20presence%20of%20language-agnostic%20concepts%2C%20ii%29%20fine-tuning%20of%20the%20models%0Aenhances%20%5Ctextit%7Balignment%7D%20within%20the%20latent%20space%2C%20and%20iii%29%20such%0Atask-specific%20calibration%20helps%20in%20explaining%20the%20emergence%20of%20zero-shot%0Acapabilities%20in%20the%20models.%5Cfootnote%7BThe%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/baselmousi/multilingual-latent-concepts%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14535v1&entry.124074799=Read"},
{"title": "Implicit In-context Learning", "author": "Zhuowei Li and Zihao Xu and Ligong Han and Yunhe Gao and Song Wen and Di Liu and Hao Wang and Dimitris N. Metaxas", "abstract": "  In-context Learning (ICL) empowers large language models (LLMs) to adapt to\nunseen tasks during inference by prefixing a few demonstration examples prior\nto test queries. Despite its versatility, ICL incurs substantial computational\nand memory overheads compared to zero-shot learning and is susceptible to the\nselection and order of demonstration examples. In this work, we introduce\nImplicit In-context Learning (I2CL), an innovative paradigm that addresses the\nchallenges associated with traditional ICL by absorbing demonstration examples\nwithin the activation space. I2CL first generates a condensed vector\nrepresentation, namely a context vector, from the demonstration examples. It\nthen integrates the context vector during inference by injecting a linear\ncombination of the context vector and query activations into the model's\nresidual streams. Empirical evaluation on nine real-world tasks across three\nmodel architectures demonstrates that I2CL achieves few-shot performance with\nzero-shot cost and exhibits robustness against the variation of demonstration\nexamples. Furthermore, I2CL facilitates a novel representation of \"task-ids\",\nenhancing task similarity detection and enabling effective transfer learning.\nWe provide a comprehensive analysis of I2CL, offering deeper insights into its\nmechanisms and broader implications for ICL. The source code is available at:\nhttps://github.com/LzVv123456/I2CL.\n", "link": "http://arxiv.org/abs/2405.14660v1", "date": "2024-05-23", "relevancy": 2.6166, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5173}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20In-context%20Learning&body=Title%3A%20Implicit%20In-context%20Learning%0AAuthor%3A%20Zhuowei%20Li%20and%20Zihao%20Xu%20and%20Ligong%20Han%20and%20Yunhe%20Gao%20and%20Song%20Wen%20and%20Di%20Liu%20and%20Hao%20Wang%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20In-context%20Learning%20%28ICL%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20adapt%20to%0Aunseen%20tasks%20during%20inference%20by%20prefixing%20a%20few%20demonstration%20examples%20prior%0Ato%20test%20queries.%20Despite%20its%20versatility%2C%20ICL%20incurs%20substantial%20computational%0Aand%20memory%20overheads%20compared%20to%20zero-shot%20learning%20and%20is%20susceptible%20to%20the%0Aselection%20and%20order%20of%20demonstration%20examples.%20In%20this%20work%2C%20we%20introduce%0AImplicit%20In-context%20Learning%20%28I2CL%29%2C%20an%20innovative%20paradigm%20that%20addresses%20the%0Achallenges%20associated%20with%20traditional%20ICL%20by%20absorbing%20demonstration%20examples%0Awithin%20the%20activation%20space.%20I2CL%20first%20generates%20a%20condensed%20vector%0Arepresentation%2C%20namely%20a%20context%20vector%2C%20from%20the%20demonstration%20examples.%20It%0Athen%20integrates%20the%20context%20vector%20during%20inference%20by%20injecting%20a%20linear%0Acombination%20of%20the%20context%20vector%20and%20query%20activations%20into%20the%20model%27s%0Aresidual%20streams.%20Empirical%20evaluation%20on%20nine%20real-world%20tasks%20across%20three%0Amodel%20architectures%20demonstrates%20that%20I2CL%20achieves%20few-shot%20performance%20with%0Azero-shot%20cost%20and%20exhibits%20robustness%20against%20the%20variation%20of%20demonstration%0Aexamples.%20Furthermore%2C%20I2CL%20facilitates%20a%20novel%20representation%20of%20%22task-ids%22%2C%0Aenhancing%20task%20similarity%20detection%20and%20enabling%20effective%20transfer%20learning.%0AWe%20provide%20a%20comprehensive%20analysis%20of%20I2CL%2C%20offering%20deeper%20insights%20into%20its%0Amechanisms%20and%20broader%20implications%20for%20ICL.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/LzVv123456/I2CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520In-context%2520Learning%26entry.906535625%3DZhuowei%2520Li%2520and%2520Zihao%2520Xu%2520and%2520Ligong%2520Han%2520and%2520Yunhe%2520Gao%2520and%2520Song%2520Wen%2520and%2520Di%2520Liu%2520and%2520Hao%2520Wang%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520In-context%2520Learning%2520%2528ICL%2529%2520empowers%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520adapt%2520to%250Aunseen%2520tasks%2520during%2520inference%2520by%2520prefixing%2520a%2520few%2520demonstration%2520examples%2520prior%250Ato%2520test%2520queries.%2520Despite%2520its%2520versatility%252C%2520ICL%2520incurs%2520substantial%2520computational%250Aand%2520memory%2520overheads%2520compared%2520to%2520zero-shot%2520learning%2520and%2520is%2520susceptible%2520to%2520the%250Aselection%2520and%2520order%2520of%2520demonstration%2520examples.%2520In%2520this%2520work%252C%2520we%2520introduce%250AImplicit%2520In-context%2520Learning%2520%2528I2CL%2529%252C%2520an%2520innovative%2520paradigm%2520that%2520addresses%2520the%250Achallenges%2520associated%2520with%2520traditional%2520ICL%2520by%2520absorbing%2520demonstration%2520examples%250Awithin%2520the%2520activation%2520space.%2520I2CL%2520first%2520generates%2520a%2520condensed%2520vector%250Arepresentation%252C%2520namely%2520a%2520context%2520vector%252C%2520from%2520the%2520demonstration%2520examples.%2520It%250Athen%2520integrates%2520the%2520context%2520vector%2520during%2520inference%2520by%2520injecting%2520a%2520linear%250Acombination%2520of%2520the%2520context%2520vector%2520and%2520query%2520activations%2520into%2520the%2520model%2527s%250Aresidual%2520streams.%2520Empirical%2520evaluation%2520on%2520nine%2520real-world%2520tasks%2520across%2520three%250Amodel%2520architectures%2520demonstrates%2520that%2520I2CL%2520achieves%2520few-shot%2520performance%2520with%250Azero-shot%2520cost%2520and%2520exhibits%2520robustness%2520against%2520the%2520variation%2520of%2520demonstration%250Aexamples.%2520Furthermore%252C%2520I2CL%2520facilitates%2520a%2520novel%2520representation%2520of%2520%2522task-ids%2522%252C%250Aenhancing%2520task%2520similarity%2520detection%2520and%2520enabling%2520effective%2520transfer%2520learning.%250AWe%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520I2CL%252C%2520offering%2520deeper%2520insights%2520into%2520its%250Amechanisms%2520and%2520broader%2520implications%2520for%2520ICL.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/LzVv123456/I2CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20In-context%20Learning&entry.906535625=Zhuowei%20Li%20and%20Zihao%20Xu%20and%20Ligong%20Han%20and%20Yunhe%20Gao%20and%20Song%20Wen%20and%20Di%20Liu%20and%20Hao%20Wang%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20In-context%20Learning%20%28ICL%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20adapt%20to%0Aunseen%20tasks%20during%20inference%20by%20prefixing%20a%20few%20demonstration%20examples%20prior%0Ato%20test%20queries.%20Despite%20its%20versatility%2C%20ICL%20incurs%20substantial%20computational%0Aand%20memory%20overheads%20compared%20to%20zero-shot%20learning%20and%20is%20susceptible%20to%20the%0Aselection%20and%20order%20of%20demonstration%20examples.%20In%20this%20work%2C%20we%20introduce%0AImplicit%20In-context%20Learning%20%28I2CL%29%2C%20an%20innovative%20paradigm%20that%20addresses%20the%0Achallenges%20associated%20with%20traditional%20ICL%20by%20absorbing%20demonstration%20examples%0Awithin%20the%20activation%20space.%20I2CL%20first%20generates%20a%20condensed%20vector%0Arepresentation%2C%20namely%20a%20context%20vector%2C%20from%20the%20demonstration%20examples.%20It%0Athen%20integrates%20the%20context%20vector%20during%20inference%20by%20injecting%20a%20linear%0Acombination%20of%20the%20context%20vector%20and%20query%20activations%20into%20the%20model%27s%0Aresidual%20streams.%20Empirical%20evaluation%20on%20nine%20real-world%20tasks%20across%20three%0Amodel%20architectures%20demonstrates%20that%20I2CL%20achieves%20few-shot%20performance%20with%0Azero-shot%20cost%20and%20exhibits%20robustness%20against%20the%20variation%20of%20demonstration%0Aexamples.%20Furthermore%2C%20I2CL%20facilitates%20a%20novel%20representation%20of%20%22task-ids%22%2C%0Aenhancing%20task%20similarity%20detection%20and%20enabling%20effective%20transfer%20learning.%0AWe%20provide%20a%20comprehensive%20analysis%20of%20I2CL%2C%20offering%20deeper%20insights%20into%20its%0Amechanisms%20and%20broader%20implications%20for%20ICL.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/LzVv123456/I2CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14660v1&entry.124074799=Read"},
{"title": "NeRF-Casting: Improved View-Dependent Appearance with Consistent\n  Reflections", "author": "Dor Verbin and Pratul P. Srinivasan and Peter Hedman and Ben Mildenhall and Benjamin Attal and Richard Szeliski and Jonathan T. Barron", "abstract": "  Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render\nhighly specular objects, whose appearance varies quickly with changes in\nviewpoint. Recent works have improved NeRF's ability to render detailed\nspecular appearance of distant environment illumination, but are unable to\nsynthesize consistent reflections of closer content. Moreover, these techniques\nrely on large computationally-expensive neural networks to model outgoing\nradiance, which severely limits optimization and rendering speed. We address\nthese issues with an approach based on ray tracing: instead of querying an\nexpensive neural network for the outgoing view-dependent radiance at points\nalong each camera ray, our model casts reflection rays from these points and\ntraces them through the NeRF representation to render feature vectors which are\ndecoded into color using a small inexpensive network. We demonstrate that our\nmodel outperforms prior methods for view synthesis of scenes containing shiny\nobjects, and that it is the only existing NeRF method that can synthesize\nphotorealistic specular appearance and reflections in real-world scenes, while\nrequiring comparable optimization time to current state-of-the-art view\nsynthesis models.\n", "link": "http://arxiv.org/abs/2405.14871v1", "date": "2024-05-23", "relevancy": 2.6099, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5395}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5167}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-Casting%3A%20Improved%20View-Dependent%20Appearance%20with%20Consistent%0A%20%20Reflections&body=Title%3A%20NeRF-Casting%3A%20Improved%20View-Dependent%20Appearance%20with%20Consistent%0A%20%20Reflections%0AAuthor%3A%20Dor%20Verbin%20and%20Pratul%20P.%20Srinivasan%20and%20Peter%20Hedman%20and%20Ben%20Mildenhall%20and%20Benjamin%20Attal%20and%20Richard%20Szeliski%20and%20Jonathan%20T.%20Barron%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20typically%20struggle%20to%20reconstruct%20and%20render%0Ahighly%20specular%20objects%2C%20whose%20appearance%20varies%20quickly%20with%20changes%20in%0Aviewpoint.%20Recent%20works%20have%20improved%20NeRF%27s%20ability%20to%20render%20detailed%0Aspecular%20appearance%20of%20distant%20environment%20illumination%2C%20but%20are%20unable%20to%0Asynthesize%20consistent%20reflections%20of%20closer%20content.%20Moreover%2C%20these%20techniques%0Arely%20on%20large%20computationally-expensive%20neural%20networks%20to%20model%20outgoing%0Aradiance%2C%20which%20severely%20limits%20optimization%20and%20rendering%20speed.%20We%20address%0Athese%20issues%20with%20an%20approach%20based%20on%20ray%20tracing%3A%20instead%20of%20querying%20an%0Aexpensive%20neural%20network%20for%20the%20outgoing%20view-dependent%20radiance%20at%20points%0Aalong%20each%20camera%20ray%2C%20our%20model%20casts%20reflection%20rays%20from%20these%20points%20and%0Atraces%20them%20through%20the%20NeRF%20representation%20to%20render%20feature%20vectors%20which%20are%0Adecoded%20into%20color%20using%20a%20small%20inexpensive%20network.%20We%20demonstrate%20that%20our%0Amodel%20outperforms%20prior%20methods%20for%20view%20synthesis%20of%20scenes%20containing%20shiny%0Aobjects%2C%20and%20that%20it%20is%20the%20only%20existing%20NeRF%20method%20that%20can%20synthesize%0Aphotorealistic%20specular%20appearance%20and%20reflections%20in%20real-world%20scenes%2C%20while%0Arequiring%20comparable%20optimization%20time%20to%20current%20state-of-the-art%20view%0Asynthesis%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-Casting%253A%2520Improved%2520View-Dependent%2520Appearance%2520with%2520Consistent%250A%2520%2520Reflections%26entry.906535625%3DDor%2520Verbin%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Peter%2520Hedman%2520and%2520Ben%2520Mildenhall%2520and%2520Benjamin%2520Attal%2520and%2520Richard%2520Szeliski%2520and%2520Jonathan%2520T.%2520Barron%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520typically%2520struggle%2520to%2520reconstruct%2520and%2520render%250Ahighly%2520specular%2520objects%252C%2520whose%2520appearance%2520varies%2520quickly%2520with%2520changes%2520in%250Aviewpoint.%2520Recent%2520works%2520have%2520improved%2520NeRF%2527s%2520ability%2520to%2520render%2520detailed%250Aspecular%2520appearance%2520of%2520distant%2520environment%2520illumination%252C%2520but%2520are%2520unable%2520to%250Asynthesize%2520consistent%2520reflections%2520of%2520closer%2520content.%2520Moreover%252C%2520these%2520techniques%250Arely%2520on%2520large%2520computationally-expensive%2520neural%2520networks%2520to%2520model%2520outgoing%250Aradiance%252C%2520which%2520severely%2520limits%2520optimization%2520and%2520rendering%2520speed.%2520We%2520address%250Athese%2520issues%2520with%2520an%2520approach%2520based%2520on%2520ray%2520tracing%253A%2520instead%2520of%2520querying%2520an%250Aexpensive%2520neural%2520network%2520for%2520the%2520outgoing%2520view-dependent%2520radiance%2520at%2520points%250Aalong%2520each%2520camera%2520ray%252C%2520our%2520model%2520casts%2520reflection%2520rays%2520from%2520these%2520points%2520and%250Atraces%2520them%2520through%2520the%2520NeRF%2520representation%2520to%2520render%2520feature%2520vectors%2520which%2520are%250Adecoded%2520into%2520color%2520using%2520a%2520small%2520inexpensive%2520network.%2520We%2520demonstrate%2520that%2520our%250Amodel%2520outperforms%2520prior%2520methods%2520for%2520view%2520synthesis%2520of%2520scenes%2520containing%2520shiny%250Aobjects%252C%2520and%2520that%2520it%2520is%2520the%2520only%2520existing%2520NeRF%2520method%2520that%2520can%2520synthesize%250Aphotorealistic%2520specular%2520appearance%2520and%2520reflections%2520in%2520real-world%2520scenes%252C%2520while%250Arequiring%2520comparable%2520optimization%2520time%2520to%2520current%2520state-of-the-art%2520view%250Asynthesis%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-Casting%3A%20Improved%20View-Dependent%20Appearance%20with%20Consistent%0A%20%20Reflections&entry.906535625=Dor%20Verbin%20and%20Pratul%20P.%20Srinivasan%20and%20Peter%20Hedman%20and%20Ben%20Mildenhall%20and%20Benjamin%20Attal%20and%20Richard%20Szeliski%20and%20Jonathan%20T.%20Barron&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20typically%20struggle%20to%20reconstruct%20and%20render%0Ahighly%20specular%20objects%2C%20whose%20appearance%20varies%20quickly%20with%20changes%20in%0Aviewpoint.%20Recent%20works%20have%20improved%20NeRF%27s%20ability%20to%20render%20detailed%0Aspecular%20appearance%20of%20distant%20environment%20illumination%2C%20but%20are%20unable%20to%0Asynthesize%20consistent%20reflections%20of%20closer%20content.%20Moreover%2C%20these%20techniques%0Arely%20on%20large%20computationally-expensive%20neural%20networks%20to%20model%20outgoing%0Aradiance%2C%20which%20severely%20limits%20optimization%20and%20rendering%20speed.%20We%20address%0Athese%20issues%20with%20an%20approach%20based%20on%20ray%20tracing%3A%20instead%20of%20querying%20an%0Aexpensive%20neural%20network%20for%20the%20outgoing%20view-dependent%20radiance%20at%20points%0Aalong%20each%20camera%20ray%2C%20our%20model%20casts%20reflection%20rays%20from%20these%20points%20and%0Atraces%20them%20through%20the%20NeRF%20representation%20to%20render%20feature%20vectors%20which%20are%0Adecoded%20into%20color%20using%20a%20small%20inexpensive%20network.%20We%20demonstrate%20that%20our%0Amodel%20outperforms%20prior%20methods%20for%20view%20synthesis%20of%20scenes%20containing%20shiny%0Aobjects%2C%20and%20that%20it%20is%20the%20only%20existing%20NeRF%20method%20that%20can%20synthesize%0Aphotorealistic%20specular%20appearance%20and%20reflections%20in%20real-world%20scenes%2C%20while%0Arequiring%20comparable%20optimization%20time%20to%20current%20state-of-the-art%20view%0Asynthesis%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14871v1&entry.124074799=Read"},
{"title": "Exact Gauss-Newton Optimization for Training Deep Neural Networks", "author": "Mikalai Korbit and Adeyemi D. Adeoye and Alberto Bemporad and Mario Zanon", "abstract": "  We present EGN, a stochastic second-order optimization algorithm that\ncombines the generalized Gauss-Newton (GN) Hessian approximation with low-rank\nlinear algebra to compute the descent direction. Leveraging the Duncan-Guttman\nmatrix identity, the parameter update is obtained by factorizing a matrix which\nhas the size of the mini-batch. This is particularly advantageous for\nlarge-scale machine learning problems where the dimension of the neural network\nparameter vector is several orders of magnitude larger than the batch size.\nAdditionally, we show how improvements such as line search, adaptive\nregularization, and momentum can be seamlessly added to EGN to further\naccelerate the algorithm. Moreover, under mild assumptions, we prove that our\nalgorithm converges to an $\\epsilon$-stationary point at a linear rate.\nFinally, our numerical experiments demonstrate that EGN consistently exceeds,\nor at most matches the generalization performance of well-tuned SGD, Adam, and\nSGN optimizers across various supervised and reinforcement learning tasks.\n", "link": "http://arxiv.org/abs/2405.14402v1", "date": "2024-05-23", "relevancy": 2.6097, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5825}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5033}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Gauss-Newton%20Optimization%20for%20Training%20Deep%20Neural%20Networks&body=Title%3A%20Exact%20Gauss-Newton%20Optimization%20for%20Training%20Deep%20Neural%20Networks%0AAuthor%3A%20Mikalai%20Korbit%20and%20Adeyemi%20D.%20Adeoye%20and%20Alberto%20Bemporad%20and%20Mario%20Zanon%0AAbstract%3A%20%20%20We%20present%20EGN%2C%20a%20stochastic%20second-order%20optimization%20algorithm%20that%0Acombines%20the%20generalized%20Gauss-Newton%20%28GN%29%20Hessian%20approximation%20with%20low-rank%0Alinear%20algebra%20to%20compute%20the%20descent%20direction.%20Leveraging%20the%20Duncan-Guttman%0Amatrix%20identity%2C%20the%20parameter%20update%20is%20obtained%20by%20factorizing%20a%20matrix%20which%0Ahas%20the%20size%20of%20the%20mini-batch.%20This%20is%20particularly%20advantageous%20for%0Alarge-scale%20machine%20learning%20problems%20where%20the%20dimension%20of%20the%20neural%20network%0Aparameter%20vector%20is%20several%20orders%20of%20magnitude%20larger%20than%20the%20batch%20size.%0AAdditionally%2C%20we%20show%20how%20improvements%20such%20as%20line%20search%2C%20adaptive%0Aregularization%2C%20and%20momentum%20can%20be%20seamlessly%20added%20to%20EGN%20to%20further%0Aaccelerate%20the%20algorithm.%20Moreover%2C%20under%20mild%20assumptions%2C%20we%20prove%20that%20our%0Aalgorithm%20converges%20to%20an%20%24%5Cepsilon%24-stationary%20point%20at%20a%20linear%20rate.%0AFinally%2C%20our%20numerical%20experiments%20demonstrate%20that%20EGN%20consistently%20exceeds%2C%0Aor%20at%20most%20matches%20the%20generalization%20performance%20of%20well-tuned%20SGD%2C%20Adam%2C%20and%0ASGN%20optimizers%20across%20various%20supervised%20and%20reinforcement%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Gauss-Newton%2520Optimization%2520for%2520Training%2520Deep%2520Neural%2520Networks%26entry.906535625%3DMikalai%2520Korbit%2520and%2520Adeyemi%2520D.%2520Adeoye%2520and%2520Alberto%2520Bemporad%2520and%2520Mario%2520Zanon%26entry.1292438233%3D%2520%2520We%2520present%2520EGN%252C%2520a%2520stochastic%2520second-order%2520optimization%2520algorithm%2520that%250Acombines%2520the%2520generalized%2520Gauss-Newton%2520%2528GN%2529%2520Hessian%2520approximation%2520with%2520low-rank%250Alinear%2520algebra%2520to%2520compute%2520the%2520descent%2520direction.%2520Leveraging%2520the%2520Duncan-Guttman%250Amatrix%2520identity%252C%2520the%2520parameter%2520update%2520is%2520obtained%2520by%2520factorizing%2520a%2520matrix%2520which%250Ahas%2520the%2520size%2520of%2520the%2520mini-batch.%2520This%2520is%2520particularly%2520advantageous%2520for%250Alarge-scale%2520machine%2520learning%2520problems%2520where%2520the%2520dimension%2520of%2520the%2520neural%2520network%250Aparameter%2520vector%2520is%2520several%2520orders%2520of%2520magnitude%2520larger%2520than%2520the%2520batch%2520size.%250AAdditionally%252C%2520we%2520show%2520how%2520improvements%2520such%2520as%2520line%2520search%252C%2520adaptive%250Aregularization%252C%2520and%2520momentum%2520can%2520be%2520seamlessly%2520added%2520to%2520EGN%2520to%2520further%250Aaccelerate%2520the%2520algorithm.%2520Moreover%252C%2520under%2520mild%2520assumptions%252C%2520we%2520prove%2520that%2520our%250Aalgorithm%2520converges%2520to%2520an%2520%2524%255Cepsilon%2524-stationary%2520point%2520at%2520a%2520linear%2520rate.%250AFinally%252C%2520our%2520numerical%2520experiments%2520demonstrate%2520that%2520EGN%2520consistently%2520exceeds%252C%250Aor%2520at%2520most%2520matches%2520the%2520generalization%2520performance%2520of%2520well-tuned%2520SGD%252C%2520Adam%252C%2520and%250ASGN%2520optimizers%2520across%2520various%2520supervised%2520and%2520reinforcement%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Gauss-Newton%20Optimization%20for%20Training%20Deep%20Neural%20Networks&entry.906535625=Mikalai%20Korbit%20and%20Adeyemi%20D.%20Adeoye%20and%20Alberto%20Bemporad%20and%20Mario%20Zanon&entry.1292438233=%20%20We%20present%20EGN%2C%20a%20stochastic%20second-order%20optimization%20algorithm%20that%0Acombines%20the%20generalized%20Gauss-Newton%20%28GN%29%20Hessian%20approximation%20with%20low-rank%0Alinear%20algebra%20to%20compute%20the%20descent%20direction.%20Leveraging%20the%20Duncan-Guttman%0Amatrix%20identity%2C%20the%20parameter%20update%20is%20obtained%20by%20factorizing%20a%20matrix%20which%0Ahas%20the%20size%20of%20the%20mini-batch.%20This%20is%20particularly%20advantageous%20for%0Alarge-scale%20machine%20learning%20problems%20where%20the%20dimension%20of%20the%20neural%20network%0Aparameter%20vector%20is%20several%20orders%20of%20magnitude%20larger%20than%20the%20batch%20size.%0AAdditionally%2C%20we%20show%20how%20improvements%20such%20as%20line%20search%2C%20adaptive%0Aregularization%2C%20and%20momentum%20can%20be%20seamlessly%20added%20to%20EGN%20to%20further%0Aaccelerate%20the%20algorithm.%20Moreover%2C%20under%20mild%20assumptions%2C%20we%20prove%20that%20our%0Aalgorithm%20converges%20to%20an%20%24%5Cepsilon%24-stationary%20point%20at%20a%20linear%20rate.%0AFinally%2C%20our%20numerical%20experiments%20demonstrate%20that%20EGN%20consistently%20exceeds%2C%0Aor%20at%20most%20matches%20the%20generalization%20performance%20of%20well-tuned%20SGD%2C%20Adam%2C%20and%0ASGN%20optimizers%20across%20various%20supervised%20and%20reinforcement%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14402v1&entry.124074799=Read"},
{"title": "Aligning Embeddings and Geometric Random Graphs: Informational Results\n  and Computational Approaches for the Procrustes-Wasserstein Problem", "author": "Mathieu Even and Luca Ganassali and Jakob Maier and Laurent Massouli\u00e9", "abstract": "  The Procrustes-Wasserstein problem consists in matching two high-dimensional\npoint clouds in an unsupervised setting, and has many applications in natural\nlanguage processing and computer vision. We consider a planted model with two\ndatasets $X,Y$ that consist of $n$ datapoints in $\\mathbb{R}^d$, where $Y$ is a\nnoisy version of $X$, up to an orthogonal transformation and a relabeling of\nthe data points. This setting is related to the graph alignment problem in\ngeometric models. In this work, we focus on the euclidean transport cost\nbetween the point clouds as a measure of performance for the alignment. We\nfirst establish information-theoretic results, in the high ($d \\gg \\log n$) and\nlow ($d \\ll \\log n$) dimensional regimes. We then study computational aspects\nand propose the Ping-Pong algorithm, alternatively estimating the orthogonal\ntransformation and the relabeling, initialized via a Franke-Wolfe convex\nrelaxation. We give sufficient conditions for the method to retrieve the\nplanted signal after one single step. We provide experimental results to\ncompare the proposed approach with the state-of-the-art method of Grave et al.\n(2019).\n", "link": "http://arxiv.org/abs/2405.14532v1", "date": "2024-05-23", "relevancy": 2.5896, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Embeddings%20and%20Geometric%20Random%20Graphs%3A%20Informational%20Results%0A%20%20and%20Computational%20Approaches%20for%20the%20Procrustes-Wasserstein%20Problem&body=Title%3A%20Aligning%20Embeddings%20and%20Geometric%20Random%20Graphs%3A%20Informational%20Results%0A%20%20and%20Computational%20Approaches%20for%20the%20Procrustes-Wasserstein%20Problem%0AAuthor%3A%20Mathieu%20Even%20and%20Luca%20Ganassali%20and%20Jakob%20Maier%20and%20Laurent%20Massouli%C3%A9%0AAbstract%3A%20%20%20The%20Procrustes-Wasserstein%20problem%20consists%20in%20matching%20two%20high-dimensional%0Apoint%20clouds%20in%20an%20unsupervised%20setting%2C%20and%20has%20many%20applications%20in%20natural%0Alanguage%20processing%20and%20computer%20vision.%20We%20consider%20a%20planted%20model%20with%20two%0Adatasets%20%24X%2CY%24%20that%20consist%20of%20%24n%24%20datapoints%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20where%20%24Y%24%20is%20a%0Anoisy%20version%20of%20%24X%24%2C%20up%20to%20an%20orthogonal%20transformation%20and%20a%20relabeling%20of%0Athe%20data%20points.%20This%20setting%20is%20related%20to%20the%20graph%20alignment%20problem%20in%0Ageometric%20models.%20In%20this%20work%2C%20we%20focus%20on%20the%20euclidean%20transport%20cost%0Abetween%20the%20point%20clouds%20as%20a%20measure%20of%20performance%20for%20the%20alignment.%20We%0Afirst%20establish%20information-theoretic%20results%2C%20in%20the%20high%20%28%24d%20%5Cgg%20%5Clog%20n%24%29%20and%0Alow%20%28%24d%20%5Cll%20%5Clog%20n%24%29%20dimensional%20regimes.%20We%20then%20study%20computational%20aspects%0Aand%20propose%20the%20Ping-Pong%20algorithm%2C%20alternatively%20estimating%20the%20orthogonal%0Atransformation%20and%20the%20relabeling%2C%20initialized%20via%20a%20Franke-Wolfe%20convex%0Arelaxation.%20We%20give%20sufficient%20conditions%20for%20the%20method%20to%20retrieve%20the%0Aplanted%20signal%20after%20one%20single%20step.%20We%20provide%20experimental%20results%20to%0Acompare%20the%20proposed%20approach%20with%20the%20state-of-the-art%20method%20of%20Grave%20et%20al.%0A%282019%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Embeddings%2520and%2520Geometric%2520Random%2520Graphs%253A%2520Informational%2520Results%250A%2520%2520and%2520Computational%2520Approaches%2520for%2520the%2520Procrustes-Wasserstein%2520Problem%26entry.906535625%3DMathieu%2520Even%2520and%2520Luca%2520Ganassali%2520and%2520Jakob%2520Maier%2520and%2520Laurent%2520Massouli%25C3%25A9%26entry.1292438233%3D%2520%2520The%2520Procrustes-Wasserstein%2520problem%2520consists%2520in%2520matching%2520two%2520high-dimensional%250Apoint%2520clouds%2520in%2520an%2520unsupervised%2520setting%252C%2520and%2520has%2520many%2520applications%2520in%2520natural%250Alanguage%2520processing%2520and%2520computer%2520vision.%2520We%2520consider%2520a%2520planted%2520model%2520with%2520two%250Adatasets%2520%2524X%252CY%2524%2520that%2520consist%2520of%2520%2524n%2524%2520datapoints%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520where%2520%2524Y%2524%2520is%2520a%250Anoisy%2520version%2520of%2520%2524X%2524%252C%2520up%2520to%2520an%2520orthogonal%2520transformation%2520and%2520a%2520relabeling%2520of%250Athe%2520data%2520points.%2520This%2520setting%2520is%2520related%2520to%2520the%2520graph%2520alignment%2520problem%2520in%250Ageometric%2520models.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520euclidean%2520transport%2520cost%250Abetween%2520the%2520point%2520clouds%2520as%2520a%2520measure%2520of%2520performance%2520for%2520the%2520alignment.%2520We%250Afirst%2520establish%2520information-theoretic%2520results%252C%2520in%2520the%2520high%2520%2528%2524d%2520%255Cgg%2520%255Clog%2520n%2524%2529%2520and%250Alow%2520%2528%2524d%2520%255Cll%2520%255Clog%2520n%2524%2529%2520dimensional%2520regimes.%2520We%2520then%2520study%2520computational%2520aspects%250Aand%2520propose%2520the%2520Ping-Pong%2520algorithm%252C%2520alternatively%2520estimating%2520the%2520orthogonal%250Atransformation%2520and%2520the%2520relabeling%252C%2520initialized%2520via%2520a%2520Franke-Wolfe%2520convex%250Arelaxation.%2520We%2520give%2520sufficient%2520conditions%2520for%2520the%2520method%2520to%2520retrieve%2520the%250Aplanted%2520signal%2520after%2520one%2520single%2520step.%2520We%2520provide%2520experimental%2520results%2520to%250Acompare%2520the%2520proposed%2520approach%2520with%2520the%2520state-of-the-art%2520method%2520of%2520Grave%2520et%2520al.%250A%25282019%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Embeddings%20and%20Geometric%20Random%20Graphs%3A%20Informational%20Results%0A%20%20and%20Computational%20Approaches%20for%20the%20Procrustes-Wasserstein%20Problem&entry.906535625=Mathieu%20Even%20and%20Luca%20Ganassali%20and%20Jakob%20Maier%20and%20Laurent%20Massouli%C3%A9&entry.1292438233=%20%20The%20Procrustes-Wasserstein%20problem%20consists%20in%20matching%20two%20high-dimensional%0Apoint%20clouds%20in%20an%20unsupervised%20setting%2C%20and%20has%20many%20applications%20in%20natural%0Alanguage%20processing%20and%20computer%20vision.%20We%20consider%20a%20planted%20model%20with%20two%0Adatasets%20%24X%2CY%24%20that%20consist%20of%20%24n%24%20datapoints%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20where%20%24Y%24%20is%20a%0Anoisy%20version%20of%20%24X%24%2C%20up%20to%20an%20orthogonal%20transformation%20and%20a%20relabeling%20of%0Athe%20data%20points.%20This%20setting%20is%20related%20to%20the%20graph%20alignment%20problem%20in%0Ageometric%20models.%20In%20this%20work%2C%20we%20focus%20on%20the%20euclidean%20transport%20cost%0Abetween%20the%20point%20clouds%20as%20a%20measure%20of%20performance%20for%20the%20alignment.%20We%0Afirst%20establish%20information-theoretic%20results%2C%20in%20the%20high%20%28%24d%20%5Cgg%20%5Clog%20n%24%29%20and%0Alow%20%28%24d%20%5Cll%20%5Clog%20n%24%29%20dimensional%20regimes.%20We%20then%20study%20computational%20aspects%0Aand%20propose%20the%20Ping-Pong%20algorithm%2C%20alternatively%20estimating%20the%20orthogonal%0Atransformation%20and%20the%20relabeling%2C%20initialized%20via%20a%20Franke-Wolfe%20convex%0Arelaxation.%20We%20give%20sufficient%20conditions%20for%20the%20method%20to%20retrieve%20the%0Aplanted%20signal%20after%20one%20single%20step.%20We%20provide%20experimental%20results%20to%0Acompare%20the%20proposed%20approach%20with%20the%20state-of-the-art%20method%20of%20Grave%20et%20al.%0A%282019%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14532v1&entry.124074799=Read"},
{"title": "Ghost-Stereo: GhostNet-based Cost Volume Enhancement and Aggregation for\n  Stereo Matching Networks", "author": "Xingguang Jiang and Xiaofeng Bian and Chenggang Guo", "abstract": "  Depth estimation based on stereo matching is a classic but popular computer\nvision problem, which has a wide range of real-world applications. Current\nstereo matching methods generally adopt the deep Siamese neural network\narchitecture, and have achieved impressing performance by constructing feature\nmatching cost volumes and using 3D convolutions for cost aggregation. However,\nmost existing methods suffer from large number of parameters and slow running\ntime due to the sequential use of 3D convolutions. In this paper, we propose\nGhost-Stereo, a novel end-to-end stereo matching network. The feature\nextraction part of the network uses the GhostNet to form a U-shaped structure.\nThe core of Ghost-Stereo is a GhostNet feature-based cost volume enhancement\n(Ghost-CVE) module and a GhostNet-inspired lightweight cost volume aggregation\n(Ghost-CVA) module. For the Ghost-CVE part, cost volumes are constructed and\nfused by the GhostNet-based features to enhance the spatial context awareness.\nFor the Ghost-CVA part, a lightweight 3D convolution bottleneck block based on\nthe GhostNet is proposed to reduce the computational complexity in this module.\nBy combining with the context and geometry fusion module, a classical\nhourglass-shaped cost volume aggregate structure is constructed. Ghost-Stereo\nachieves a comparable performance than state-of-the-art real-time methods on\nseveral publicly benchmarks, and shows a better generalization ability.\n", "link": "http://arxiv.org/abs/2405.14520v1", "date": "2024-05-23", "relevancy": 2.5847, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5498}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5155}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ghost-Stereo%3A%20GhostNet-based%20Cost%20Volume%20Enhancement%20and%20Aggregation%20for%0A%20%20Stereo%20Matching%20Networks&body=Title%3A%20Ghost-Stereo%3A%20GhostNet-based%20Cost%20Volume%20Enhancement%20and%20Aggregation%20for%0A%20%20Stereo%20Matching%20Networks%0AAuthor%3A%20Xingguang%20Jiang%20and%20Xiaofeng%20Bian%20and%20Chenggang%20Guo%0AAbstract%3A%20%20%20Depth%20estimation%20based%20on%20stereo%20matching%20is%20a%20classic%20but%20popular%20computer%0Avision%20problem%2C%20which%20has%20a%20wide%20range%20of%20real-world%20applications.%20Current%0Astereo%20matching%20methods%20generally%20adopt%20the%20deep%20Siamese%20neural%20network%0Aarchitecture%2C%20and%20have%20achieved%20impressing%20performance%20by%20constructing%20feature%0Amatching%20cost%20volumes%20and%20using%203D%20convolutions%20for%20cost%20aggregation.%20However%2C%0Amost%20existing%20methods%20suffer%20from%20large%20number%20of%20parameters%20and%20slow%20running%0Atime%20due%20to%20the%20sequential%20use%20of%203D%20convolutions.%20In%20this%20paper%2C%20we%20propose%0AGhost-Stereo%2C%20a%20novel%20end-to-end%20stereo%20matching%20network.%20The%20feature%0Aextraction%20part%20of%20the%20network%20uses%20the%20GhostNet%20to%20form%20a%20U-shaped%20structure.%0AThe%20core%20of%20Ghost-Stereo%20is%20a%20GhostNet%20feature-based%20cost%20volume%20enhancement%0A%28Ghost-CVE%29%20module%20and%20a%20GhostNet-inspired%20lightweight%20cost%20volume%20aggregation%0A%28Ghost-CVA%29%20module.%20For%20the%20Ghost-CVE%20part%2C%20cost%20volumes%20are%20constructed%20and%0Afused%20by%20the%20GhostNet-based%20features%20to%20enhance%20the%20spatial%20context%20awareness.%0AFor%20the%20Ghost-CVA%20part%2C%20a%20lightweight%203D%20convolution%20bottleneck%20block%20based%20on%0Athe%20GhostNet%20is%20proposed%20to%20reduce%20the%20computational%20complexity%20in%20this%20module.%0ABy%20combining%20with%20the%20context%20and%20geometry%20fusion%20module%2C%20a%20classical%0Ahourglass-shaped%20cost%20volume%20aggregate%20structure%20is%20constructed.%20Ghost-Stereo%0Aachieves%20a%20comparable%20performance%20than%20state-of-the-art%20real-time%20methods%20on%0Aseveral%20publicly%20benchmarks%2C%20and%20shows%20a%20better%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGhost-Stereo%253A%2520GhostNet-based%2520Cost%2520Volume%2520Enhancement%2520and%2520Aggregation%2520for%250A%2520%2520Stereo%2520Matching%2520Networks%26entry.906535625%3DXingguang%2520Jiang%2520and%2520Xiaofeng%2520Bian%2520and%2520Chenggang%2520Guo%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520based%2520on%2520stereo%2520matching%2520is%2520a%2520classic%2520but%2520popular%2520computer%250Avision%2520problem%252C%2520which%2520has%2520a%2520wide%2520range%2520of%2520real-world%2520applications.%2520Current%250Astereo%2520matching%2520methods%2520generally%2520adopt%2520the%2520deep%2520Siamese%2520neural%2520network%250Aarchitecture%252C%2520and%2520have%2520achieved%2520impressing%2520performance%2520by%2520constructing%2520feature%250Amatching%2520cost%2520volumes%2520and%2520using%25203D%2520convolutions%2520for%2520cost%2520aggregation.%2520However%252C%250Amost%2520existing%2520methods%2520suffer%2520from%2520large%2520number%2520of%2520parameters%2520and%2520slow%2520running%250Atime%2520due%2520to%2520the%2520sequential%2520use%2520of%25203D%2520convolutions.%2520In%2520this%2520paper%252C%2520we%2520propose%250AGhost-Stereo%252C%2520a%2520novel%2520end-to-end%2520stereo%2520matching%2520network.%2520The%2520feature%250Aextraction%2520part%2520of%2520the%2520network%2520uses%2520the%2520GhostNet%2520to%2520form%2520a%2520U-shaped%2520structure.%250AThe%2520core%2520of%2520Ghost-Stereo%2520is%2520a%2520GhostNet%2520feature-based%2520cost%2520volume%2520enhancement%250A%2528Ghost-CVE%2529%2520module%2520and%2520a%2520GhostNet-inspired%2520lightweight%2520cost%2520volume%2520aggregation%250A%2528Ghost-CVA%2529%2520module.%2520For%2520the%2520Ghost-CVE%2520part%252C%2520cost%2520volumes%2520are%2520constructed%2520and%250Afused%2520by%2520the%2520GhostNet-based%2520features%2520to%2520enhance%2520the%2520spatial%2520context%2520awareness.%250AFor%2520the%2520Ghost-CVA%2520part%252C%2520a%2520lightweight%25203D%2520convolution%2520bottleneck%2520block%2520based%2520on%250Athe%2520GhostNet%2520is%2520proposed%2520to%2520reduce%2520the%2520computational%2520complexity%2520in%2520this%2520module.%250ABy%2520combining%2520with%2520the%2520context%2520and%2520geometry%2520fusion%2520module%252C%2520a%2520classical%250Ahourglass-shaped%2520cost%2520volume%2520aggregate%2520structure%2520is%2520constructed.%2520Ghost-Stereo%250Aachieves%2520a%2520comparable%2520performance%2520than%2520state-of-the-art%2520real-time%2520methods%2520on%250Aseveral%2520publicly%2520benchmarks%252C%2520and%2520shows%2520a%2520better%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ghost-Stereo%3A%20GhostNet-based%20Cost%20Volume%20Enhancement%20and%20Aggregation%20for%0A%20%20Stereo%20Matching%20Networks&entry.906535625=Xingguang%20Jiang%20and%20Xiaofeng%20Bian%20and%20Chenggang%20Guo&entry.1292438233=%20%20Depth%20estimation%20based%20on%20stereo%20matching%20is%20a%20classic%20but%20popular%20computer%0Avision%20problem%2C%20which%20has%20a%20wide%20range%20of%20real-world%20applications.%20Current%0Astereo%20matching%20methods%20generally%20adopt%20the%20deep%20Siamese%20neural%20network%0Aarchitecture%2C%20and%20have%20achieved%20impressing%20performance%20by%20constructing%20feature%0Amatching%20cost%20volumes%20and%20using%203D%20convolutions%20for%20cost%20aggregation.%20However%2C%0Amost%20existing%20methods%20suffer%20from%20large%20number%20of%20parameters%20and%20slow%20running%0Atime%20due%20to%20the%20sequential%20use%20of%203D%20convolutions.%20In%20this%20paper%2C%20we%20propose%0AGhost-Stereo%2C%20a%20novel%20end-to-end%20stereo%20matching%20network.%20The%20feature%0Aextraction%20part%20of%20the%20network%20uses%20the%20GhostNet%20to%20form%20a%20U-shaped%20structure.%0AThe%20core%20of%20Ghost-Stereo%20is%20a%20GhostNet%20feature-based%20cost%20volume%20enhancement%0A%28Ghost-CVE%29%20module%20and%20a%20GhostNet-inspired%20lightweight%20cost%20volume%20aggregation%0A%28Ghost-CVA%29%20module.%20For%20the%20Ghost-CVE%20part%2C%20cost%20volumes%20are%20constructed%20and%0Afused%20by%20the%20GhostNet-based%20features%20to%20enhance%20the%20spatial%20context%20awareness.%0AFor%20the%20Ghost-CVA%20part%2C%20a%20lightweight%203D%20convolution%20bottleneck%20block%20based%20on%0Athe%20GhostNet%20is%20proposed%20to%20reduce%20the%20computational%20complexity%20in%20this%20module.%0ABy%20combining%20with%20the%20context%20and%20geometry%20fusion%20module%2C%20a%20classical%0Ahourglass-shaped%20cost%20volume%20aggregate%20structure%20is%20constructed.%20Ghost-Stereo%0Aachieves%20a%20comparable%20performance%20than%20state-of-the-art%20real-time%20methods%20on%0Aseveral%20publicly%20benchmarks%2C%20and%20shows%20a%20better%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14520v1&entry.124074799=Read"},
{"title": "Flatten Anything: Unsupervised Neural Surface Parameterization", "author": "Qijian Zhang and Junhui Hou and Wenping Wang and Ying He", "abstract": "  Surface parameterization plays an essential role in numerous computer\ngraphics and geometry processing applications. Traditional parameterization\napproaches are designed for high-quality meshes laboriously created by\nspecialized 3D modelers, thus unable to meet the processing demand for the\ncurrent explosion of ordinary 3D data. Moreover, their working mechanisms are\ntypically restricted to certain simple topologies, thus relying on cumbersome\nmanual efforts (e.g., surface cutting, part segmentation) for pre-processing.\nIn this paper, we introduce the Flatten Anything Model (FAM), an unsupervised\nneural architecture to achieve global free-boundary surface parameterization\nvia learning point-wise mappings between 3D points on the target geometric\nsurface and adaptively-deformed UV coordinates within the 2D parameter domain.\nTo mimic the actual physical procedures, we ingeniously construct\ngeometrically-interpretable sub-networks with specific functionalities of\nsurface cutting, UV deforming, unwrapping, and wrapping, which are assembled\ninto a bi-directional cycle mapping framework. Compared with previous methods,\nour FAM directly operates on discrete surface points without utilizing\nconnectivity information, thus significantly reducing the strict requirements\nfor mesh quality and even applicable to unstructured point cloud data. More\nimportantly, our FAM is fully-automated without the need for pre-cutting and\ncan deal with highly-complex topologies, since its learning process adaptively\nfinds reasonable cutting seams and UV boundaries. Extensive experiments\ndemonstrate the universality, superiority, and inspiring potential of our\nproposed neural surface parameterization paradigm. The code will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2405.14633v1", "date": "2024-05-23", "relevancy": 2.5704, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5277}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flatten%20Anything%3A%20Unsupervised%20Neural%20Surface%20Parameterization&body=Title%3A%20Flatten%20Anything%3A%20Unsupervised%20Neural%20Surface%20Parameterization%0AAuthor%3A%20Qijian%20Zhang%20and%20Junhui%20Hou%20and%20Wenping%20Wang%20and%20Ying%20He%0AAbstract%3A%20%20%20Surface%20parameterization%20plays%20an%20essential%20role%20in%20numerous%20computer%0Agraphics%20and%20geometry%20processing%20applications.%20Traditional%20parameterization%0Aapproaches%20are%20designed%20for%20high-quality%20meshes%20laboriously%20created%20by%0Aspecialized%203D%20modelers%2C%20thus%20unable%20to%20meet%20the%20processing%20demand%20for%20the%0Acurrent%20explosion%20of%20ordinary%203D%20data.%20Moreover%2C%20their%20working%20mechanisms%20are%0Atypically%20restricted%20to%20certain%20simple%20topologies%2C%20thus%20relying%20on%20cumbersome%0Amanual%20efforts%20%28e.g.%2C%20surface%20cutting%2C%20part%20segmentation%29%20for%20pre-processing.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Flatten%20Anything%20Model%20%28FAM%29%2C%20an%20unsupervised%0Aneural%20architecture%20to%20achieve%20global%20free-boundary%20surface%20parameterization%0Avia%20learning%20point-wise%20mappings%20between%203D%20points%20on%20the%20target%20geometric%0Asurface%20and%20adaptively-deformed%20UV%20coordinates%20within%20the%202D%20parameter%20domain.%0ATo%20mimic%20the%20actual%20physical%20procedures%2C%20we%20ingeniously%20construct%0Ageometrically-interpretable%20sub-networks%20with%20specific%20functionalities%20of%0Asurface%20cutting%2C%20UV%20deforming%2C%20unwrapping%2C%20and%20wrapping%2C%20which%20are%20assembled%0Ainto%20a%20bi-directional%20cycle%20mapping%20framework.%20Compared%20with%20previous%20methods%2C%0Aour%20FAM%20directly%20operates%20on%20discrete%20surface%20points%20without%20utilizing%0Aconnectivity%20information%2C%20thus%20significantly%20reducing%20the%20strict%20requirements%0Afor%20mesh%20quality%20and%20even%20applicable%20to%20unstructured%20point%20cloud%20data.%20More%0Aimportantly%2C%20our%20FAM%20is%20fully-automated%20without%20the%20need%20for%20pre-cutting%20and%0Acan%20deal%20with%20highly-complex%20topologies%2C%20since%20its%20learning%20process%20adaptively%0Afinds%20reasonable%20cutting%20seams%20and%20UV%20boundaries.%20Extensive%20experiments%0Ademonstrate%20the%20universality%2C%20superiority%2C%20and%20inspiring%20potential%20of%20our%0Aproposed%20neural%20surface%20parameterization%20paradigm.%20The%20code%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlatten%2520Anything%253A%2520Unsupervised%2520Neural%2520Surface%2520Parameterization%26entry.906535625%3DQijian%2520Zhang%2520and%2520Junhui%2520Hou%2520and%2520Wenping%2520Wang%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520Surface%2520parameterization%2520plays%2520an%2520essential%2520role%2520in%2520numerous%2520computer%250Agraphics%2520and%2520geometry%2520processing%2520applications.%2520Traditional%2520parameterization%250Aapproaches%2520are%2520designed%2520for%2520high-quality%2520meshes%2520laboriously%2520created%2520by%250Aspecialized%25203D%2520modelers%252C%2520thus%2520unable%2520to%2520meet%2520the%2520processing%2520demand%2520for%2520the%250Acurrent%2520explosion%2520of%2520ordinary%25203D%2520data.%2520Moreover%252C%2520their%2520working%2520mechanisms%2520are%250Atypically%2520restricted%2520to%2520certain%2520simple%2520topologies%252C%2520thus%2520relying%2520on%2520cumbersome%250Amanual%2520efforts%2520%2528e.g.%252C%2520surface%2520cutting%252C%2520part%2520segmentation%2529%2520for%2520pre-processing.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Flatten%2520Anything%2520Model%2520%2528FAM%2529%252C%2520an%2520unsupervised%250Aneural%2520architecture%2520to%2520achieve%2520global%2520free-boundary%2520surface%2520parameterization%250Avia%2520learning%2520point-wise%2520mappings%2520between%25203D%2520points%2520on%2520the%2520target%2520geometric%250Asurface%2520and%2520adaptively-deformed%2520UV%2520coordinates%2520within%2520the%25202D%2520parameter%2520domain.%250ATo%2520mimic%2520the%2520actual%2520physical%2520procedures%252C%2520we%2520ingeniously%2520construct%250Ageometrically-interpretable%2520sub-networks%2520with%2520specific%2520functionalities%2520of%250Asurface%2520cutting%252C%2520UV%2520deforming%252C%2520unwrapping%252C%2520and%2520wrapping%252C%2520which%2520are%2520assembled%250Ainto%2520a%2520bi-directional%2520cycle%2520mapping%2520framework.%2520Compared%2520with%2520previous%2520methods%252C%250Aour%2520FAM%2520directly%2520operates%2520on%2520discrete%2520surface%2520points%2520without%2520utilizing%250Aconnectivity%2520information%252C%2520thus%2520significantly%2520reducing%2520the%2520strict%2520requirements%250Afor%2520mesh%2520quality%2520and%2520even%2520applicable%2520to%2520unstructured%2520point%2520cloud%2520data.%2520More%250Aimportantly%252C%2520our%2520FAM%2520is%2520fully-automated%2520without%2520the%2520need%2520for%2520pre-cutting%2520and%250Acan%2520deal%2520with%2520highly-complex%2520topologies%252C%2520since%2520its%2520learning%2520process%2520adaptively%250Afinds%2520reasonable%2520cutting%2520seams%2520and%2520UV%2520boundaries.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520universality%252C%2520superiority%252C%2520and%2520inspiring%2520potential%2520of%2520our%250Aproposed%2520neural%2520surface%2520parameterization%2520paradigm.%2520The%2520code%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flatten%20Anything%3A%20Unsupervised%20Neural%20Surface%20Parameterization&entry.906535625=Qijian%20Zhang%20and%20Junhui%20Hou%20and%20Wenping%20Wang%20and%20Ying%20He&entry.1292438233=%20%20Surface%20parameterization%20plays%20an%20essential%20role%20in%20numerous%20computer%0Agraphics%20and%20geometry%20processing%20applications.%20Traditional%20parameterization%0Aapproaches%20are%20designed%20for%20high-quality%20meshes%20laboriously%20created%20by%0Aspecialized%203D%20modelers%2C%20thus%20unable%20to%20meet%20the%20processing%20demand%20for%20the%0Acurrent%20explosion%20of%20ordinary%203D%20data.%20Moreover%2C%20their%20working%20mechanisms%20are%0Atypically%20restricted%20to%20certain%20simple%20topologies%2C%20thus%20relying%20on%20cumbersome%0Amanual%20efforts%20%28e.g.%2C%20surface%20cutting%2C%20part%20segmentation%29%20for%20pre-processing.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Flatten%20Anything%20Model%20%28FAM%29%2C%20an%20unsupervised%0Aneural%20architecture%20to%20achieve%20global%20free-boundary%20surface%20parameterization%0Avia%20learning%20point-wise%20mappings%20between%203D%20points%20on%20the%20target%20geometric%0Asurface%20and%20adaptively-deformed%20UV%20coordinates%20within%20the%202D%20parameter%20domain.%0ATo%20mimic%20the%20actual%20physical%20procedures%2C%20we%20ingeniously%20construct%0Ageometrically-interpretable%20sub-networks%20with%20specific%20functionalities%20of%0Asurface%20cutting%2C%20UV%20deforming%2C%20unwrapping%2C%20and%20wrapping%2C%20which%20are%20assembled%0Ainto%20a%20bi-directional%20cycle%20mapping%20framework.%20Compared%20with%20previous%20methods%2C%0Aour%20FAM%20directly%20operates%20on%20discrete%20surface%20points%20without%20utilizing%0Aconnectivity%20information%2C%20thus%20significantly%20reducing%20the%20strict%20requirements%0Afor%20mesh%20quality%20and%20even%20applicable%20to%20unstructured%20point%20cloud%20data.%20More%0Aimportantly%2C%20our%20FAM%20is%20fully-automated%20without%20the%20need%20for%20pre-cutting%20and%0Acan%20deal%20with%20highly-complex%20topologies%2C%20since%20its%20learning%20process%20adaptively%0Afinds%20reasonable%20cutting%20seams%20and%20UV%20boundaries.%20Extensive%20experiments%0Ademonstrate%20the%20universality%2C%20superiority%2C%20and%20inspiring%20potential%20of%20our%0Aproposed%20neural%20surface%20parameterization%20paradigm.%20The%20code%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14633v1&entry.124074799=Read"},
{"title": "Discretization of continuous input spaces in the hippocampal autoencoder", "author": "Adrian F. Amil and Ismael T. Freire and Paul F. M. J. Verschure", "abstract": "  The hippocampus has been associated with both spatial cognition and episodic\nmemory formation, but integrating these functions into a unified framework\nremains challenging. Here, we demonstrate that forming discrete memories of\nvisual events in sparse autoencoder neurons can produce spatial tuning similar\nto hippocampal place cells. We then show that the resulting very\nhigh-dimensional code enables neurons to discretize and tile the underlying\nimage space with minimal overlap. Additionally, we extend our results to the\nauditory domain, showing that neurons similarly tile the frequency space in an\nexperience-dependent manner. Lastly, we show that reinforcement learning agents\ncan effectively perform various visuo-spatial cognitive tasks using these\nsparse, very high-dimensional representations.\n", "link": "http://arxiv.org/abs/2405.14600v1", "date": "2024-05-23", "relevancy": 2.5495, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.496}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discretization%20of%20continuous%20input%20spaces%20in%20the%20hippocampal%20autoencoder&body=Title%3A%20Discretization%20of%20continuous%20input%20spaces%20in%20the%20hippocampal%20autoencoder%0AAuthor%3A%20Adrian%20F.%20Amil%20and%20Ismael%20T.%20Freire%20and%20Paul%20F.%20M.%20J.%20Verschure%0AAbstract%3A%20%20%20The%20hippocampus%20has%20been%20associated%20with%20both%20spatial%20cognition%20and%20episodic%0Amemory%20formation%2C%20but%20integrating%20these%20functions%20into%20a%20unified%20framework%0Aremains%20challenging.%20Here%2C%20we%20demonstrate%20that%20forming%20discrete%20memories%20of%0Avisual%20events%20in%20sparse%20autoencoder%20neurons%20can%20produce%20spatial%20tuning%20similar%0Ato%20hippocampal%20place%20cells.%20We%20then%20show%20that%20the%20resulting%20very%0Ahigh-dimensional%20code%20enables%20neurons%20to%20discretize%20and%20tile%20the%20underlying%0Aimage%20space%20with%20minimal%20overlap.%20Additionally%2C%20we%20extend%20our%20results%20to%20the%0Aauditory%20domain%2C%20showing%20that%20neurons%20similarly%20tile%20the%20frequency%20space%20in%20an%0Aexperience-dependent%20manner.%20Lastly%2C%20we%20show%20that%20reinforcement%20learning%20agents%0Acan%20effectively%20perform%20various%20visuo-spatial%20cognitive%20tasks%20using%20these%0Asparse%2C%20very%20high-dimensional%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscretization%2520of%2520continuous%2520input%2520spaces%2520in%2520the%2520hippocampal%2520autoencoder%26entry.906535625%3DAdrian%2520F.%2520Amil%2520and%2520Ismael%2520T.%2520Freire%2520and%2520Paul%2520F.%2520M.%2520J.%2520Verschure%26entry.1292438233%3D%2520%2520The%2520hippocampus%2520has%2520been%2520associated%2520with%2520both%2520spatial%2520cognition%2520and%2520episodic%250Amemory%2520formation%252C%2520but%2520integrating%2520these%2520functions%2520into%2520a%2520unified%2520framework%250Aremains%2520challenging.%2520Here%252C%2520we%2520demonstrate%2520that%2520forming%2520discrete%2520memories%2520of%250Avisual%2520events%2520in%2520sparse%2520autoencoder%2520neurons%2520can%2520produce%2520spatial%2520tuning%2520similar%250Ato%2520hippocampal%2520place%2520cells.%2520We%2520then%2520show%2520that%2520the%2520resulting%2520very%250Ahigh-dimensional%2520code%2520enables%2520neurons%2520to%2520discretize%2520and%2520tile%2520the%2520underlying%250Aimage%2520space%2520with%2520minimal%2520overlap.%2520Additionally%252C%2520we%2520extend%2520our%2520results%2520to%2520the%250Aauditory%2520domain%252C%2520showing%2520that%2520neurons%2520similarly%2520tile%2520the%2520frequency%2520space%2520in%2520an%250Aexperience-dependent%2520manner.%2520Lastly%252C%2520we%2520show%2520that%2520reinforcement%2520learning%2520agents%250Acan%2520effectively%2520perform%2520various%2520visuo-spatial%2520cognitive%2520tasks%2520using%2520these%250Asparse%252C%2520very%2520high-dimensional%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discretization%20of%20continuous%20input%20spaces%20in%20the%20hippocampal%20autoencoder&entry.906535625=Adrian%20F.%20Amil%20and%20Ismael%20T.%20Freire%20and%20Paul%20F.%20M.%20J.%20Verschure&entry.1292438233=%20%20The%20hippocampus%20has%20been%20associated%20with%20both%20spatial%20cognition%20and%20episodic%0Amemory%20formation%2C%20but%20integrating%20these%20functions%20into%20a%20unified%20framework%0Aremains%20challenging.%20Here%2C%20we%20demonstrate%20that%20forming%20discrete%20memories%20of%0Avisual%20events%20in%20sparse%20autoencoder%20neurons%20can%20produce%20spatial%20tuning%20similar%0Ato%20hippocampal%20place%20cells.%20We%20then%20show%20that%20the%20resulting%20very%0Ahigh-dimensional%20code%20enables%20neurons%20to%20discretize%20and%20tile%20the%20underlying%0Aimage%20space%20with%20minimal%20overlap.%20Additionally%2C%20we%20extend%20our%20results%20to%20the%0Aauditory%20domain%2C%20showing%20that%20neurons%20similarly%20tile%20the%20frequency%20space%20in%20an%0Aexperience-dependent%20manner.%20Lastly%2C%20we%20show%20that%20reinforcement%20learning%20agents%0Acan%20effectively%20perform%20various%20visuo-spatial%20cognitive%20tasks%20using%20these%0Asparse%2C%20very%20high-dimensional%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14600v1&entry.124074799=Read"},
{"title": "Boosting Robustness by Clipping Gradients in Distributed Learning", "author": "Youssef Allouah and Rachid Guerraoui and Nirupam Gupta and Ahmed Jellouli and Geovani Rizk and John Stephan", "abstract": "  Robust distributed learning consists in achieving good learning performance\ndespite the presence of misbehaving workers. State-of-the-art (SOTA) robust\ndistributed gradient descent (Robust-DGD) methods, relying on robust\naggregation, have been proven to be optimal: Their learning error matches the\nlower bound established under the standard heterogeneity model of $(G,\nB)$-gradient dissimilarity. The learning guarantee of SOTA Robust-DGD cannot be\nfurther improved when model initialization is done arbitrarily. However, we\nshow that it is possible to circumvent the lower bound, and improve the\nlearning performance, when the workers' gradients at model initialization are\nassumed to be bounded. We prove this by proposing pre-aggregation clipping of\nworkers' gradients, using a novel scheme called adaptive robust clipping (ARC).\nIncorporating ARC in Robust-DGD provably improves the learning, under the\naforementioned assumption on model initialization. The factor of improvement is\nprominent when the tolerable fraction of misbehaving workers approaches the\nbreakdown point. ARC induces this improvement by constricting the search space,\nwhile preserving the robustness property of the original aggregation scheme at\nthe same time. We validate this theoretical finding through exhaustive\nexperiments on benchmark image classification tasks.\n", "link": "http://arxiv.org/abs/2405.14432v1", "date": "2024-05-23", "relevancy": 2.5456, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5203}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Robustness%20by%20Clipping%20Gradients%20in%20Distributed%20Learning&body=Title%3A%20Boosting%20Robustness%20by%20Clipping%20Gradients%20in%20Distributed%20Learning%0AAuthor%3A%20Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan%0AAbstract%3A%20%20%20Robust%20distributed%20learning%20consists%20in%20achieving%20good%20learning%20performance%0Adespite%20the%20presence%20of%20misbehaving%20workers.%20State-of-the-art%20%28SOTA%29%20robust%0Adistributed%20gradient%20descent%20%28Robust-DGD%29%20methods%2C%20relying%20on%20robust%0Aaggregation%2C%20have%20been%20proven%20to%20be%20optimal%3A%20Their%20learning%20error%20matches%20the%0Alower%20bound%20established%20under%20the%20standard%20heterogeneity%20model%20of%20%24%28G%2C%0AB%29%24-gradient%20dissimilarity.%20The%20learning%20guarantee%20of%20SOTA%20Robust-DGD%20cannot%20be%0Afurther%20improved%20when%20model%20initialization%20is%20done%20arbitrarily.%20However%2C%20we%0Ashow%20that%20it%20is%20possible%20to%20circumvent%20the%20lower%20bound%2C%20and%20improve%20the%0Alearning%20performance%2C%20when%20the%20workers%27%20gradients%20at%20model%20initialization%20are%0Aassumed%20to%20be%20bounded.%20We%20prove%20this%20by%20proposing%20pre-aggregation%20clipping%20of%0Aworkers%27%20gradients%2C%20using%20a%20novel%20scheme%20called%20adaptive%20robust%20clipping%20%28ARC%29.%0AIncorporating%20ARC%20in%20Robust-DGD%20provably%20improves%20the%20learning%2C%20under%20the%0Aaforementioned%20assumption%20on%20model%20initialization.%20The%20factor%20of%20improvement%20is%0Aprominent%20when%20the%20tolerable%20fraction%20of%20misbehaving%20workers%20approaches%20the%0Abreakdown%20point.%20ARC%20induces%20this%20improvement%20by%20constricting%20the%20search%20space%2C%0Awhile%20preserving%20the%20robustness%20property%20of%20the%20original%20aggregation%20scheme%20at%0Athe%20same%20time.%20We%20validate%20this%20theoretical%20finding%20through%20exhaustive%0Aexperiments%20on%20benchmark%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Robustness%2520by%2520Clipping%2520Gradients%2520in%2520Distributed%2520Learning%26entry.906535625%3DYoussef%2520Allouah%2520and%2520Rachid%2520Guerraoui%2520and%2520Nirupam%2520Gupta%2520and%2520Ahmed%2520Jellouli%2520and%2520Geovani%2520Rizk%2520and%2520John%2520Stephan%26entry.1292438233%3D%2520%2520Robust%2520distributed%2520learning%2520consists%2520in%2520achieving%2520good%2520learning%2520performance%250Adespite%2520the%2520presence%2520of%2520misbehaving%2520workers.%2520State-of-the-art%2520%2528SOTA%2529%2520robust%250Adistributed%2520gradient%2520descent%2520%2528Robust-DGD%2529%2520methods%252C%2520relying%2520on%2520robust%250Aaggregation%252C%2520have%2520been%2520proven%2520to%2520be%2520optimal%253A%2520Their%2520learning%2520error%2520matches%2520the%250Alower%2520bound%2520established%2520under%2520the%2520standard%2520heterogeneity%2520model%2520of%2520%2524%2528G%252C%250AB%2529%2524-gradient%2520dissimilarity.%2520The%2520learning%2520guarantee%2520of%2520SOTA%2520Robust-DGD%2520cannot%2520be%250Afurther%2520improved%2520when%2520model%2520initialization%2520is%2520done%2520arbitrarily.%2520However%252C%2520we%250Ashow%2520that%2520it%2520is%2520possible%2520to%2520circumvent%2520the%2520lower%2520bound%252C%2520and%2520improve%2520the%250Alearning%2520performance%252C%2520when%2520the%2520workers%2527%2520gradients%2520at%2520model%2520initialization%2520are%250Aassumed%2520to%2520be%2520bounded.%2520We%2520prove%2520this%2520by%2520proposing%2520pre-aggregation%2520clipping%2520of%250Aworkers%2527%2520gradients%252C%2520using%2520a%2520novel%2520scheme%2520called%2520adaptive%2520robust%2520clipping%2520%2528ARC%2529.%250AIncorporating%2520ARC%2520in%2520Robust-DGD%2520provably%2520improves%2520the%2520learning%252C%2520under%2520the%250Aaforementioned%2520assumption%2520on%2520model%2520initialization.%2520The%2520factor%2520of%2520improvement%2520is%250Aprominent%2520when%2520the%2520tolerable%2520fraction%2520of%2520misbehaving%2520workers%2520approaches%2520the%250Abreakdown%2520point.%2520ARC%2520induces%2520this%2520improvement%2520by%2520constricting%2520the%2520search%2520space%252C%250Awhile%2520preserving%2520the%2520robustness%2520property%2520of%2520the%2520original%2520aggregation%2520scheme%2520at%250Athe%2520same%2520time.%2520We%2520validate%2520this%2520theoretical%2520finding%2520through%2520exhaustive%250Aexperiments%2520on%2520benchmark%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Robustness%20by%20Clipping%20Gradients%20in%20Distributed%20Learning&entry.906535625=Youssef%20Allouah%20and%20Rachid%20Guerraoui%20and%20Nirupam%20Gupta%20and%20Ahmed%20Jellouli%20and%20Geovani%20Rizk%20and%20John%20Stephan&entry.1292438233=%20%20Robust%20distributed%20learning%20consists%20in%20achieving%20good%20learning%20performance%0Adespite%20the%20presence%20of%20misbehaving%20workers.%20State-of-the-art%20%28SOTA%29%20robust%0Adistributed%20gradient%20descent%20%28Robust-DGD%29%20methods%2C%20relying%20on%20robust%0Aaggregation%2C%20have%20been%20proven%20to%20be%20optimal%3A%20Their%20learning%20error%20matches%20the%0Alower%20bound%20established%20under%20the%20standard%20heterogeneity%20model%20of%20%24%28G%2C%0AB%29%24-gradient%20dissimilarity.%20The%20learning%20guarantee%20of%20SOTA%20Robust-DGD%20cannot%20be%0Afurther%20improved%20when%20model%20initialization%20is%20done%20arbitrarily.%20However%2C%20we%0Ashow%20that%20it%20is%20possible%20to%20circumvent%20the%20lower%20bound%2C%20and%20improve%20the%0Alearning%20performance%2C%20when%20the%20workers%27%20gradients%20at%20model%20initialization%20are%0Aassumed%20to%20be%20bounded.%20We%20prove%20this%20by%20proposing%20pre-aggregation%20clipping%20of%0Aworkers%27%20gradients%2C%20using%20a%20novel%20scheme%20called%20adaptive%20robust%20clipping%20%28ARC%29.%0AIncorporating%20ARC%20in%20Robust-DGD%20provably%20improves%20the%20learning%2C%20under%20the%0Aaforementioned%20assumption%20on%20model%20initialization.%20The%20factor%20of%20improvement%20is%0Aprominent%20when%20the%20tolerable%20fraction%20of%20misbehaving%20workers%20approaches%20the%0Abreakdown%20point.%20ARC%20induces%20this%20improvement%20by%20constricting%20the%20search%20space%2C%0Awhile%20preserving%20the%20robustness%20property%20of%20the%20original%20aggregation%20scheme%20at%0Athe%20same%20time.%20We%20validate%20this%20theoretical%20finding%20through%20exhaustive%0Aexperiments%20on%20benchmark%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14432v1&entry.124074799=Read"},
{"title": "Video Diffusion Models are Training-free Motion Interpreter and\n  Controller", "author": "Zeqi Xiao and Yifan Zhou and Shuai Yang and Xingang Pan", "abstract": "  Video generation primarily aims to model authentic and customized motion\nacross frames, making understanding and controlling the motion a crucial topic.\nMost diffusion-based studies on video motion focus on motion customization with\ntraining-based paradigms, which, however, demands substantial training\nresources and necessitates retraining for diverse models. Crucially, these\napproaches do not explore how video diffusion models encode cross-frame motion\ninformation in their features, lacking interpretability and transparency in\ntheir effectiveness. To answer this question, this paper introduces a novel\nperspective to understand, localize, and manipulate motion-aware features in\nvideo diffusion models. Through analysis using Principal Component Analysis\n(PCA), our work discloses that robust motion-aware feature already exists in\nvideo diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating\ncontent correlation information and filtering motion channels. MOFT provides a\ndistinct set of benefits, including the ability to encode comprehensive motion\ninformation with clear interpretability, extraction without the need for\ntraining, and generalizability across diverse architectures. Leveraging MOFT,\nwe propose a novel training-free video motion control framework. Our method\ndemonstrates competitive performance in generating natural and faithful motion,\nproviding architecture-agnostic insights and applicability in a variety of\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2405.14864v1", "date": "2024-05-23", "relevancy": 2.545, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6304}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller&body=Title%3A%20Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller%0AAuthor%3A%20Zeqi%20Xiao%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Xingang%20Pan%0AAbstract%3A%20%20%20Video%20generation%20primarily%20aims%20to%20model%20authentic%20and%20customized%20motion%0Aacross%20frames%2C%20making%20understanding%20and%20controlling%20the%20motion%20a%20crucial%20topic.%0AMost%20diffusion-based%20studies%20on%20video%20motion%20focus%20on%20motion%20customization%20with%0Atraining-based%20paradigms%2C%20which%2C%20however%2C%20demands%20substantial%20training%0Aresources%20and%20necessitates%20retraining%20for%20diverse%20models.%20Crucially%2C%20these%0Aapproaches%20do%20not%20explore%20how%20video%20diffusion%20models%20encode%20cross-frame%20motion%0Ainformation%20in%20their%20features%2C%20lacking%20interpretability%20and%20transparency%20in%0Atheir%20effectiveness.%20To%20answer%20this%20question%2C%20this%20paper%20introduces%20a%20novel%0Aperspective%20to%20understand%2C%20localize%2C%20and%20manipulate%20motion-aware%20features%20in%0Avideo%20diffusion%20models.%20Through%20analysis%20using%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20our%20work%20discloses%20that%20robust%20motion-aware%20feature%20already%20exists%20in%0Avideo%20diffusion%20models.%20We%20present%20a%20new%20MOtion%20FeaTure%20%28MOFT%29%20by%20eliminating%0Acontent%20correlation%20information%20and%20filtering%20motion%20channels.%20MOFT%20provides%20a%0Adistinct%20set%20of%20benefits%2C%20including%20the%20ability%20to%20encode%20comprehensive%20motion%0Ainformation%20with%20clear%20interpretability%2C%20extraction%20without%20the%20need%20for%0Atraining%2C%20and%20generalizability%20across%20diverse%20architectures.%20Leveraging%20MOFT%2C%0Awe%20propose%20a%20novel%20training-free%20video%20motion%20control%20framework.%20Our%20method%0Ademonstrates%20competitive%20performance%20in%20generating%20natural%20and%20faithful%20motion%2C%0Aproviding%20architecture-agnostic%20insights%20and%20applicability%20in%20a%20variety%20of%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Diffusion%2520Models%2520are%2520Training-free%2520Motion%2520Interpreter%2520and%250A%2520%2520Controller%26entry.906535625%3DZeqi%2520Xiao%2520and%2520Yifan%2520Zhou%2520and%2520Shuai%2520Yang%2520and%2520Xingang%2520Pan%26entry.1292438233%3D%2520%2520Video%2520generation%2520primarily%2520aims%2520to%2520model%2520authentic%2520and%2520customized%2520motion%250Aacross%2520frames%252C%2520making%2520understanding%2520and%2520controlling%2520the%2520motion%2520a%2520crucial%2520topic.%250AMost%2520diffusion-based%2520studies%2520on%2520video%2520motion%2520focus%2520on%2520motion%2520customization%2520with%250Atraining-based%2520paradigms%252C%2520which%252C%2520however%252C%2520demands%2520substantial%2520training%250Aresources%2520and%2520necessitates%2520retraining%2520for%2520diverse%2520models.%2520Crucially%252C%2520these%250Aapproaches%2520do%2520not%2520explore%2520how%2520video%2520diffusion%2520models%2520encode%2520cross-frame%2520motion%250Ainformation%2520in%2520their%2520features%252C%2520lacking%2520interpretability%2520and%2520transparency%2520in%250Atheir%2520effectiveness.%2520To%2520answer%2520this%2520question%252C%2520this%2520paper%2520introduces%2520a%2520novel%250Aperspective%2520to%2520understand%252C%2520localize%252C%2520and%2520manipulate%2520motion-aware%2520features%2520in%250Avideo%2520diffusion%2520models.%2520Through%2520analysis%2520using%2520Principal%2520Component%2520Analysis%250A%2528PCA%2529%252C%2520our%2520work%2520discloses%2520that%2520robust%2520motion-aware%2520feature%2520already%2520exists%2520in%250Avideo%2520diffusion%2520models.%2520We%2520present%2520a%2520new%2520MOtion%2520FeaTure%2520%2528MOFT%2529%2520by%2520eliminating%250Acontent%2520correlation%2520information%2520and%2520filtering%2520motion%2520channels.%2520MOFT%2520provides%2520a%250Adistinct%2520set%2520of%2520benefits%252C%2520including%2520the%2520ability%2520to%2520encode%2520comprehensive%2520motion%250Ainformation%2520with%2520clear%2520interpretability%252C%2520extraction%2520without%2520the%2520need%2520for%250Atraining%252C%2520and%2520generalizability%2520across%2520diverse%2520architectures.%2520Leveraging%2520MOFT%252C%250Awe%2520propose%2520a%2520novel%2520training-free%2520video%2520motion%2520control%2520framework.%2520Our%2520method%250Ademonstrates%2520competitive%2520performance%2520in%2520generating%2520natural%2520and%2520faithful%2520motion%252C%250Aproviding%2520architecture-agnostic%2520insights%2520and%2520applicability%2520in%2520a%2520variety%2520of%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Diffusion%20Models%20are%20Training-free%20Motion%20Interpreter%20and%0A%20%20Controller&entry.906535625=Zeqi%20Xiao%20and%20Yifan%20Zhou%20and%20Shuai%20Yang%20and%20Xingang%20Pan&entry.1292438233=%20%20Video%20generation%20primarily%20aims%20to%20model%20authentic%20and%20customized%20motion%0Aacross%20frames%2C%20making%20understanding%20and%20controlling%20the%20motion%20a%20crucial%20topic.%0AMost%20diffusion-based%20studies%20on%20video%20motion%20focus%20on%20motion%20customization%20with%0Atraining-based%20paradigms%2C%20which%2C%20however%2C%20demands%20substantial%20training%0Aresources%20and%20necessitates%20retraining%20for%20diverse%20models.%20Crucially%2C%20these%0Aapproaches%20do%20not%20explore%20how%20video%20diffusion%20models%20encode%20cross-frame%20motion%0Ainformation%20in%20their%20features%2C%20lacking%20interpretability%20and%20transparency%20in%0Atheir%20effectiveness.%20To%20answer%20this%20question%2C%20this%20paper%20introduces%20a%20novel%0Aperspective%20to%20understand%2C%20localize%2C%20and%20manipulate%20motion-aware%20features%20in%0Avideo%20diffusion%20models.%20Through%20analysis%20using%20Principal%20Component%20Analysis%0A%28PCA%29%2C%20our%20work%20discloses%20that%20robust%20motion-aware%20feature%20already%20exists%20in%0Avideo%20diffusion%20models.%20We%20present%20a%20new%20MOtion%20FeaTure%20%28MOFT%29%20by%20eliminating%0Acontent%20correlation%20information%20and%20filtering%20motion%20channels.%20MOFT%20provides%20a%0Adistinct%20set%20of%20benefits%2C%20including%20the%20ability%20to%20encode%20comprehensive%20motion%0Ainformation%20with%20clear%20interpretability%2C%20extraction%20without%20the%20need%20for%0Atraining%2C%20and%20generalizability%20across%20diverse%20architectures.%20Leveraging%20MOFT%2C%0Awe%20propose%20a%20novel%20training-free%20video%20motion%20control%20framework.%20Our%20method%0Ademonstrates%20competitive%20performance%20in%20generating%20natural%20and%20faithful%20motion%2C%0Aproviding%20architecture-agnostic%20insights%20and%20applicability%20in%20a%20variety%20of%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14864v1&entry.124074799=Read"},
{"title": "O$n$ Learning Deep O($n$)-Equivariant Hyperspheres", "author": "Pavlo Melnyk and Michael Felsberg and M\u00e5rten Wadenb\u00e4ck and Andreas Robinson and Cuong Le", "abstract": "  In this paper, we utilize hyperspheres and regular $n$-simplexes and propose\nan approach to learning deep features equivariant under the transformations of\n$n$D reflections and rotations, encompassed by the powerful group of O$(n)$.\nNamely, we propose O$(n)$-equivariant neurons with spherical decision surfaces\nthat generalize to any dimension $n$, which we call Deep Equivariant\nHyperspheres. We demonstrate how to combine them in a network that directly\noperates on the basis of the input points and propose an invariant operator\nbased on the relation between two points and a sphere, which as we show, turns\nout to be a Gram matrix. Using synthetic and real-world data in $n$D, we\nexperimentally verify our theoretical contributions and find that our approach\nis superior to the competing methods for O$(n)$-equivariant benchmark datasets\n(classification and regression), demonstrating a favorable speed/performance\ntrade-off. The code is available at\nhttps://github.com/pavlo-melnyk/equivariant-hyperspheres.\n", "link": "http://arxiv.org/abs/2305.15613v6", "date": "2024-05-23", "relevancy": 2.5384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O%24n%24%20Learning%20Deep%20O%28%24n%24%29-Equivariant%20Hyperspheres&body=Title%3A%20O%24n%24%20Learning%20Deep%20O%28%24n%24%29-Equivariant%20Hyperspheres%0AAuthor%3A%20Pavlo%20Melnyk%20and%20Michael%20Felsberg%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Andreas%20Robinson%20and%20Cuong%20Le%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20utilize%20hyperspheres%20and%20regular%20%24n%24-simplexes%20and%20propose%0Aan%20approach%20to%20learning%20deep%20features%20equivariant%20under%20the%20transformations%20of%0A%24n%24D%20reflections%20and%20rotations%2C%20encompassed%20by%20the%20powerful%20group%20of%20O%24%28n%29%24.%0ANamely%2C%20we%20propose%20O%24%28n%29%24-equivariant%20neurons%20with%20spherical%20decision%20surfaces%0Athat%20generalize%20to%20any%20dimension%20%24n%24%2C%20which%20we%20call%20Deep%20Equivariant%0AHyperspheres.%20We%20demonstrate%20how%20to%20combine%20them%20in%20a%20network%20that%20directly%0Aoperates%20on%20the%20basis%20of%20the%20input%20points%20and%20propose%20an%20invariant%20operator%0Abased%20on%20the%20relation%20between%20two%20points%20and%20a%20sphere%2C%20which%20as%20we%20show%2C%20turns%0Aout%20to%20be%20a%20Gram%20matrix.%20Using%20synthetic%20and%20real-world%20data%20in%20%24n%24D%2C%20we%0Aexperimentally%20verify%20our%20theoretical%20contributions%20and%20find%20that%20our%20approach%0Ais%20superior%20to%20the%20competing%20methods%20for%20O%24%28n%29%24-equivariant%20benchmark%20datasets%0A%28classification%20and%20regression%29%2C%20demonstrating%20a%20favorable%20speed/performance%0Atrade-off.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/pavlo-melnyk/equivariant-hyperspheres.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15613v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO%2524n%2524%2520Learning%2520Deep%2520O%2528%2524n%2524%2529-Equivariant%2520Hyperspheres%26entry.906535625%3DPavlo%2520Melnyk%2520and%2520Michael%2520Felsberg%2520and%2520M%25C3%25A5rten%2520Wadenb%25C3%25A4ck%2520and%2520Andreas%2520Robinson%2520and%2520Cuong%2520Le%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520utilize%2520hyperspheres%2520and%2520regular%2520%2524n%2524-simplexes%2520and%2520propose%250Aan%2520approach%2520to%2520learning%2520deep%2520features%2520equivariant%2520under%2520the%2520transformations%2520of%250A%2524n%2524D%2520reflections%2520and%2520rotations%252C%2520encompassed%2520by%2520the%2520powerful%2520group%2520of%2520O%2524%2528n%2529%2524.%250ANamely%252C%2520we%2520propose%2520O%2524%2528n%2529%2524-equivariant%2520neurons%2520with%2520spherical%2520decision%2520surfaces%250Athat%2520generalize%2520to%2520any%2520dimension%2520%2524n%2524%252C%2520which%2520we%2520call%2520Deep%2520Equivariant%250AHyperspheres.%2520We%2520demonstrate%2520how%2520to%2520combine%2520them%2520in%2520a%2520network%2520that%2520directly%250Aoperates%2520on%2520the%2520basis%2520of%2520the%2520input%2520points%2520and%2520propose%2520an%2520invariant%2520operator%250Abased%2520on%2520the%2520relation%2520between%2520two%2520points%2520and%2520a%2520sphere%252C%2520which%2520as%2520we%2520show%252C%2520turns%250Aout%2520to%2520be%2520a%2520Gram%2520matrix.%2520Using%2520synthetic%2520and%2520real-world%2520data%2520in%2520%2524n%2524D%252C%2520we%250Aexperimentally%2520verify%2520our%2520theoretical%2520contributions%2520and%2520find%2520that%2520our%2520approach%250Ais%2520superior%2520to%2520the%2520competing%2520methods%2520for%2520O%2524%2528n%2529%2524-equivariant%2520benchmark%2520datasets%250A%2528classification%2520and%2520regression%2529%252C%2520demonstrating%2520a%2520favorable%2520speed/performance%250Atrade-off.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/pavlo-melnyk/equivariant-hyperspheres.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15613v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O%24n%24%20Learning%20Deep%20O%28%24n%24%29-Equivariant%20Hyperspheres&entry.906535625=Pavlo%20Melnyk%20and%20Michael%20Felsberg%20and%20M%C3%A5rten%20Wadenb%C3%A4ck%20and%20Andreas%20Robinson%20and%20Cuong%20Le&entry.1292438233=%20%20In%20this%20paper%2C%20we%20utilize%20hyperspheres%20and%20regular%20%24n%24-simplexes%20and%20propose%0Aan%20approach%20to%20learning%20deep%20features%20equivariant%20under%20the%20transformations%20of%0A%24n%24D%20reflections%20and%20rotations%2C%20encompassed%20by%20the%20powerful%20group%20of%20O%24%28n%29%24.%0ANamely%2C%20we%20propose%20O%24%28n%29%24-equivariant%20neurons%20with%20spherical%20decision%20surfaces%0Athat%20generalize%20to%20any%20dimension%20%24n%24%2C%20which%20we%20call%20Deep%20Equivariant%0AHyperspheres.%20We%20demonstrate%20how%20to%20combine%20them%20in%20a%20network%20that%20directly%0Aoperates%20on%20the%20basis%20of%20the%20input%20points%20and%20propose%20an%20invariant%20operator%0Abased%20on%20the%20relation%20between%20two%20points%20and%20a%20sphere%2C%20which%20as%20we%20show%2C%20turns%0Aout%20to%20be%20a%20Gram%20matrix.%20Using%20synthetic%20and%20real-world%20data%20in%20%24n%24D%2C%20we%0Aexperimentally%20verify%20our%20theoretical%20contributions%20and%20find%20that%20our%20approach%0Ais%20superior%20to%20the%20competing%20methods%20for%20O%24%28n%29%24-equivariant%20benchmark%20datasets%0A%28classification%20and%20regression%29%2C%20demonstrating%20a%20favorable%20speed/performance%0Atrade-off.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/pavlo-melnyk/equivariant-hyperspheres.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15613v6&entry.124074799=Read"},
{"title": "Federated Online Adaptation for Deep Stereo", "author": "Matteo Poggi and Fabio Tosi", "abstract": "  We introduce a novel approach for adapting deep stereo networks in a\ncollaborative manner. By building over principles of federated learning, we\ndevelop a distributed framework allowing for demanding the optimization process\nto a number of clients deployed in different environments. This makes it\npossible, for a deep stereo network running on resourced-constrained devices,\nto capitalize on the adaptation process carried out by other instances of the\nsame architecture, and thus improve its accuracy in challenging environments\neven when it cannot carry out adaptation on its own. Experimental results show\nhow federated adaptation performs equivalently to on-device adaptation, and\neven better when dealing with challenging environments.\n", "link": "http://arxiv.org/abs/2405.14873v1", "date": "2024-05-23", "relevancy": 2.5244, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5113}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Online%20Adaptation%20for%20Deep%20Stereo&body=Title%3A%20Federated%20Online%20Adaptation%20for%20Deep%20Stereo%0AAuthor%3A%20Matteo%20Poggi%20and%20Fabio%20Tosi%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20approach%20for%20adapting%20deep%20stereo%20networks%20in%20a%0Acollaborative%20manner.%20By%20building%20over%20principles%20of%20federated%20learning%2C%20we%0Adevelop%20a%20distributed%20framework%20allowing%20for%20demanding%20the%20optimization%20process%0Ato%20a%20number%20of%20clients%20deployed%20in%20different%20environments.%20This%20makes%20it%0Apossible%2C%20for%20a%20deep%20stereo%20network%20running%20on%20resourced-constrained%20devices%2C%0Ato%20capitalize%20on%20the%20adaptation%20process%20carried%20out%20by%20other%20instances%20of%20the%0Asame%20architecture%2C%20and%20thus%20improve%20its%20accuracy%20in%20challenging%20environments%0Aeven%20when%20it%20cannot%20carry%20out%20adaptation%20on%20its%20own.%20Experimental%20results%20show%0Ahow%20federated%20adaptation%20performs%20equivalently%20to%20on-device%20adaptation%2C%20and%0Aeven%20better%20when%20dealing%20with%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14873v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Online%2520Adaptation%2520for%2520Deep%2520Stereo%26entry.906535625%3DMatteo%2520Poggi%2520and%2520Fabio%2520Tosi%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520approach%2520for%2520adapting%2520deep%2520stereo%2520networks%2520in%2520a%250Acollaborative%2520manner.%2520By%2520building%2520over%2520principles%2520of%2520federated%2520learning%252C%2520we%250Adevelop%2520a%2520distributed%2520framework%2520allowing%2520for%2520demanding%2520the%2520optimization%2520process%250Ato%2520a%2520number%2520of%2520clients%2520deployed%2520in%2520different%2520environments.%2520This%2520makes%2520it%250Apossible%252C%2520for%2520a%2520deep%2520stereo%2520network%2520running%2520on%2520resourced-constrained%2520devices%252C%250Ato%2520capitalize%2520on%2520the%2520adaptation%2520process%2520carried%2520out%2520by%2520other%2520instances%2520of%2520the%250Asame%2520architecture%252C%2520and%2520thus%2520improve%2520its%2520accuracy%2520in%2520challenging%2520environments%250Aeven%2520when%2520it%2520cannot%2520carry%2520out%2520adaptation%2520on%2520its%2520own.%2520Experimental%2520results%2520show%250Ahow%2520federated%2520adaptation%2520performs%2520equivalently%2520to%2520on-device%2520adaptation%252C%2520and%250Aeven%2520better%2520when%2520dealing%2520with%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14873v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Online%20Adaptation%20for%20Deep%20Stereo&entry.906535625=Matteo%20Poggi%20and%20Fabio%20Tosi&entry.1292438233=%20%20We%20introduce%20a%20novel%20approach%20for%20adapting%20deep%20stereo%20networks%20in%20a%0Acollaborative%20manner.%20By%20building%20over%20principles%20of%20federated%20learning%2C%20we%0Adevelop%20a%20distributed%20framework%20allowing%20for%20demanding%20the%20optimization%20process%0Ato%20a%20number%20of%20clients%20deployed%20in%20different%20environments.%20This%20makes%20it%0Apossible%2C%20for%20a%20deep%20stereo%20network%20running%20on%20resourced-constrained%20devices%2C%0Ato%20capitalize%20on%20the%20adaptation%20process%20carried%20out%20by%20other%20instances%20of%20the%0Asame%20architecture%2C%20and%20thus%20improve%20its%20accuracy%20in%20challenging%20environments%0Aeven%20when%20it%20cannot%20carry%20out%20adaptation%20on%20its%20own.%20Experimental%20results%20show%0Ahow%20federated%20adaptation%20performs%20equivalently%20to%20on-device%20adaptation%2C%20and%0Aeven%20better%20when%20dealing%20with%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14873v1&entry.124074799=Read"},
{"title": "Convolutional Neural Network Model Observers Discount Signal-like\n  Anatomical Structures During Search in Virtual Digital Breast Tomosynthesis\n  Phantoms", "author": "Aditya Jonnalagadda and Bruno B. Barufaldi and Andrew D. A. Maidment and Susan P. Weinstein and Craig K. Abbey and Miguel P. Eckstein", "abstract": "  Model observers are computational tools to evaluate and optimize task-based\nmedical image quality. Linear model observers, such as the Channelized\nHotelling Observer (CHO), predict human accuracy in detection tasks with a few\npossible signal locations in clinical phantoms or real anatomic backgrounds. In\nrecent years, Convolutional Neural Networks (CNNs) have been proposed as a new\ntype of model observer. What is not well understood is what CNNs add over the\nmore common linear model observer approaches. We compare the CHO and CNN\ndetection accuracy to the radiologist's accuracy in searching for two types of\nsignals (mass and microcalcification) embedded in 2D/3D breast tomosynthesis\nphantoms (DBT). We show that the CHO model's accuracy is comparable to the\nCNN's performance for a location-known-exactly detection task. However, for the\nsearch task with 2D/3D DBT phantoms, the CHO's detection accuracy was\nsignificantly lower than the CNN accuracy. A comparison to the radiologist's\naccuracy showed that the CNN but not the CHO could match or exceed the\nradiologist's accuracy in the 2D microcalcification and 3D mass search\nconditions. An analysis of the eye position showed that radiologists fixated\nmore often and longer at the locations corresponding to CNN false positives.\nMost CHO false positives were the phantom's normal anatomy and were not fixated\nby radiologists. In conclusion, we show that CNNs can be used as an\nanthropomorphic model observer for the search task for which traditional linear\nmodel observers fail due to their inability to discount false positives arising\nfrom the anatomical backgrounds.\n", "link": "http://arxiv.org/abs/2405.14720v1", "date": "2024-05-23", "relevancy": 2.5211, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5083}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5083}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convolutional%20Neural%20Network%20Model%20Observers%20Discount%20Signal-like%0A%20%20Anatomical%20Structures%20During%20Search%20in%20Virtual%20Digital%20Breast%20Tomosynthesis%0A%20%20Phantoms&body=Title%3A%20Convolutional%20Neural%20Network%20Model%20Observers%20Discount%20Signal-like%0A%20%20Anatomical%20Structures%20During%20Search%20in%20Virtual%20Digital%20Breast%20Tomosynthesis%0A%20%20Phantoms%0AAuthor%3A%20Aditya%20Jonnalagadda%20and%20Bruno%20B.%20Barufaldi%20and%20Andrew%20D.%20A.%20Maidment%20and%20Susan%20P.%20Weinstein%20and%20Craig%20K.%20Abbey%20and%20Miguel%20P.%20Eckstein%0AAbstract%3A%20%20%20Model%20observers%20are%20computational%20tools%20to%20evaluate%20and%20optimize%20task-based%0Amedical%20image%20quality.%20Linear%20model%20observers%2C%20such%20as%20the%20Channelized%0AHotelling%20Observer%20%28CHO%29%2C%20predict%20human%20accuracy%20in%20detection%20tasks%20with%20a%20few%0Apossible%20signal%20locations%20in%20clinical%20phantoms%20or%20real%20anatomic%20backgrounds.%20In%0Arecent%20years%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20proposed%20as%20a%20new%0Atype%20of%20model%20observer.%20What%20is%20not%20well%20understood%20is%20what%20CNNs%20add%20over%20the%0Amore%20common%20linear%20model%20observer%20approaches.%20We%20compare%20the%20CHO%20and%20CNN%0Adetection%20accuracy%20to%20the%20radiologist%27s%20accuracy%20in%20searching%20for%20two%20types%20of%0Asignals%20%28mass%20and%20microcalcification%29%20embedded%20in%202D/3D%20breast%20tomosynthesis%0Aphantoms%20%28DBT%29.%20We%20show%20that%20the%20CHO%20model%27s%20accuracy%20is%20comparable%20to%20the%0ACNN%27s%20performance%20for%20a%20location-known-exactly%20detection%20task.%20However%2C%20for%20the%0Asearch%20task%20with%202D/3D%20DBT%20phantoms%2C%20the%20CHO%27s%20detection%20accuracy%20was%0Asignificantly%20lower%20than%20the%20CNN%20accuracy.%20A%20comparison%20to%20the%20radiologist%27s%0Aaccuracy%20showed%20that%20the%20CNN%20but%20not%20the%20CHO%20could%20match%20or%20exceed%20the%0Aradiologist%27s%20accuracy%20in%20the%202D%20microcalcification%20and%203D%20mass%20search%0Aconditions.%20An%20analysis%20of%20the%20eye%20position%20showed%20that%20radiologists%20fixated%0Amore%20often%20and%20longer%20at%20the%20locations%20corresponding%20to%20CNN%20false%20positives.%0AMost%20CHO%20false%20positives%20were%20the%20phantom%27s%20normal%20anatomy%20and%20were%20not%20fixated%0Aby%20radiologists.%20In%20conclusion%2C%20we%20show%20that%20CNNs%20can%20be%20used%20as%20an%0Aanthropomorphic%20model%20observer%20for%20the%20search%20task%20for%20which%20traditional%20linear%0Amodel%20observers%20fail%20due%20to%20their%20inability%20to%20discount%20false%20positives%20arising%0Afrom%20the%20anatomical%20backgrounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvolutional%2520Neural%2520Network%2520Model%2520Observers%2520Discount%2520Signal-like%250A%2520%2520Anatomical%2520Structures%2520During%2520Search%2520in%2520Virtual%2520Digital%2520Breast%2520Tomosynthesis%250A%2520%2520Phantoms%26entry.906535625%3DAditya%2520Jonnalagadda%2520and%2520Bruno%2520B.%2520Barufaldi%2520and%2520Andrew%2520D.%2520A.%2520Maidment%2520and%2520Susan%2520P.%2520Weinstein%2520and%2520Craig%2520K.%2520Abbey%2520and%2520Miguel%2520P.%2520Eckstein%26entry.1292438233%3D%2520%2520Model%2520observers%2520are%2520computational%2520tools%2520to%2520evaluate%2520and%2520optimize%2520task-based%250Amedical%2520image%2520quality.%2520Linear%2520model%2520observers%252C%2520such%2520as%2520the%2520Channelized%250AHotelling%2520Observer%2520%2528CHO%2529%252C%2520predict%2520human%2520accuracy%2520in%2520detection%2520tasks%2520with%2520a%2520few%250Apossible%2520signal%2520locations%2520in%2520clinical%2520phantoms%2520or%2520real%2520anatomic%2520backgrounds.%2520In%250Arecent%2520years%252C%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520been%2520proposed%2520as%2520a%2520new%250Atype%2520of%2520model%2520observer.%2520What%2520is%2520not%2520well%2520understood%2520is%2520what%2520CNNs%2520add%2520over%2520the%250Amore%2520common%2520linear%2520model%2520observer%2520approaches.%2520We%2520compare%2520the%2520CHO%2520and%2520CNN%250Adetection%2520accuracy%2520to%2520the%2520radiologist%2527s%2520accuracy%2520in%2520searching%2520for%2520two%2520types%2520of%250Asignals%2520%2528mass%2520and%2520microcalcification%2529%2520embedded%2520in%25202D/3D%2520breast%2520tomosynthesis%250Aphantoms%2520%2528DBT%2529.%2520We%2520show%2520that%2520the%2520CHO%2520model%2527s%2520accuracy%2520is%2520comparable%2520to%2520the%250ACNN%2527s%2520performance%2520for%2520a%2520location-known-exactly%2520detection%2520task.%2520However%252C%2520for%2520the%250Asearch%2520task%2520with%25202D/3D%2520DBT%2520phantoms%252C%2520the%2520CHO%2527s%2520detection%2520accuracy%2520was%250Asignificantly%2520lower%2520than%2520the%2520CNN%2520accuracy.%2520A%2520comparison%2520to%2520the%2520radiologist%2527s%250Aaccuracy%2520showed%2520that%2520the%2520CNN%2520but%2520not%2520the%2520CHO%2520could%2520match%2520or%2520exceed%2520the%250Aradiologist%2527s%2520accuracy%2520in%2520the%25202D%2520microcalcification%2520and%25203D%2520mass%2520search%250Aconditions.%2520An%2520analysis%2520of%2520the%2520eye%2520position%2520showed%2520that%2520radiologists%2520fixated%250Amore%2520often%2520and%2520longer%2520at%2520the%2520locations%2520corresponding%2520to%2520CNN%2520false%2520positives.%250AMost%2520CHO%2520false%2520positives%2520were%2520the%2520phantom%2527s%2520normal%2520anatomy%2520and%2520were%2520not%2520fixated%250Aby%2520radiologists.%2520In%2520conclusion%252C%2520we%2520show%2520that%2520CNNs%2520can%2520be%2520used%2520as%2520an%250Aanthropomorphic%2520model%2520observer%2520for%2520the%2520search%2520task%2520for%2520which%2520traditional%2520linear%250Amodel%2520observers%2520fail%2520due%2520to%2520their%2520inability%2520to%2520discount%2520false%2520positives%2520arising%250Afrom%2520the%2520anatomical%2520backgrounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convolutional%20Neural%20Network%20Model%20Observers%20Discount%20Signal-like%0A%20%20Anatomical%20Structures%20During%20Search%20in%20Virtual%20Digital%20Breast%20Tomosynthesis%0A%20%20Phantoms&entry.906535625=Aditya%20Jonnalagadda%20and%20Bruno%20B.%20Barufaldi%20and%20Andrew%20D.%20A.%20Maidment%20and%20Susan%20P.%20Weinstein%20and%20Craig%20K.%20Abbey%20and%20Miguel%20P.%20Eckstein&entry.1292438233=%20%20Model%20observers%20are%20computational%20tools%20to%20evaluate%20and%20optimize%20task-based%0Amedical%20image%20quality.%20Linear%20model%20observers%2C%20such%20as%20the%20Channelized%0AHotelling%20Observer%20%28CHO%29%2C%20predict%20human%20accuracy%20in%20detection%20tasks%20with%20a%20few%0Apossible%20signal%20locations%20in%20clinical%20phantoms%20or%20real%20anatomic%20backgrounds.%20In%0Arecent%20years%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20proposed%20as%20a%20new%0Atype%20of%20model%20observer.%20What%20is%20not%20well%20understood%20is%20what%20CNNs%20add%20over%20the%0Amore%20common%20linear%20model%20observer%20approaches.%20We%20compare%20the%20CHO%20and%20CNN%0Adetection%20accuracy%20to%20the%20radiologist%27s%20accuracy%20in%20searching%20for%20two%20types%20of%0Asignals%20%28mass%20and%20microcalcification%29%20embedded%20in%202D/3D%20breast%20tomosynthesis%0Aphantoms%20%28DBT%29.%20We%20show%20that%20the%20CHO%20model%27s%20accuracy%20is%20comparable%20to%20the%0ACNN%27s%20performance%20for%20a%20location-known-exactly%20detection%20task.%20However%2C%20for%20the%0Asearch%20task%20with%202D/3D%20DBT%20phantoms%2C%20the%20CHO%27s%20detection%20accuracy%20was%0Asignificantly%20lower%20than%20the%20CNN%20accuracy.%20A%20comparison%20to%20the%20radiologist%27s%0Aaccuracy%20showed%20that%20the%20CNN%20but%20not%20the%20CHO%20could%20match%20or%20exceed%20the%0Aradiologist%27s%20accuracy%20in%20the%202D%20microcalcification%20and%203D%20mass%20search%0Aconditions.%20An%20analysis%20of%20the%20eye%20position%20showed%20that%20radiologists%20fixated%0Amore%20often%20and%20longer%20at%20the%20locations%20corresponding%20to%20CNN%20false%20positives.%0AMost%20CHO%20false%20positives%20were%20the%20phantom%27s%20normal%20anatomy%20and%20were%20not%20fixated%0Aby%20radiologists.%20In%20conclusion%2C%20we%20show%20that%20CNNs%20can%20be%20used%20as%20an%0Aanthropomorphic%20model%20observer%20for%20the%20search%20task%20for%20which%20traditional%20linear%0Amodel%20observers%20fail%20due%20to%20their%20inability%20to%20discount%20false%20positives%20arising%0Afrom%20the%20anatomical%20backgrounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14720v1&entry.124074799=Read"},
{"title": "Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image\n  Generation", "author": "Hongxu Jiang and Muhammad Imran and Linhai Ma and Teng Zhang and Yuyin Zhou and Muxuan Liang and Kuang Gong and Wei Shao", "abstract": "  Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented\nsuccess in computer vision. However, they remain underutilized in medical\nimaging, a field crucial for disease diagnosis and treatment planning. This is\nprimarily due to the high computational cost associated with (1) the use of\nlarge number of time steps (e.g., 1,000) in diffusion processes and (2) the\nincreased dimensionality of medical images, which are often 3D or 4D. Training\na diffusion model on medical images typically takes days to weeks, while\nsampling each image volume takes minutes to hours. To address this challenge,\nwe introduce Fast-DDPM, a simple yet effective approach capable of improving\ntraining speed, sampling speed, and generation quality simultaneously. Unlike\nDDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains\nand samples using only 10 time steps. The key to our method lies in aligning\nthe training and sampling procedures. We introduced two efficient noise\nschedulers with 10 time steps: one with uniform time step sampling and another\nwith non-uniform sampling. We evaluated Fast-DDPM across three medical\nimage-to-image generation tasks: multi-image super-resolution, image denoising,\nand image-to-image translation. Fast-DDPM outperformed DDPM and current\nstate-of-the-art methods based on convolutional networks and generative\nadversarial networks in all tasks. Additionally, Fast-DDPM reduced training\ntime by a factor of 5 and sampling time by a factor of 100 compared to DDPM.\nOur code is publicly available at: https://github.com/mirthAI/Fast-DDPM.\n", "link": "http://arxiv.org/abs/2405.14802v1", "date": "2024-05-23", "relevancy": 2.52, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.688}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6321}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%20Image-to-Image%0A%20%20Generation&body=Title%3A%20Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%20Image-to-Image%0A%20%20Generation%0AAuthor%3A%20Hongxu%20Jiang%20and%20Muhammad%20Imran%20and%20Linhai%20Ma%20and%20Teng%20Zhang%20and%20Yuyin%20Zhou%20and%20Muxuan%20Liang%20and%20Kuang%20Gong%20and%20Wei%20Shao%0AAbstract%3A%20%20%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20have%20achieved%20unprecedented%0Asuccess%20in%20computer%20vision.%20However%2C%20they%20remain%20underutilized%20in%20medical%0Aimaging%2C%20a%20field%20crucial%20for%20disease%20diagnosis%20and%20treatment%20planning.%20This%20is%0Aprimarily%20due%20to%20the%20high%20computational%20cost%20associated%20with%20%281%29%20the%20use%20of%0Alarge%20number%20of%20time%20steps%20%28e.g.%2C%201%2C000%29%20in%20diffusion%20processes%20and%20%282%29%20the%0Aincreased%20dimensionality%20of%20medical%20images%2C%20which%20are%20often%203D%20or%204D.%20Training%0Aa%20diffusion%20model%20on%20medical%20images%20typically%20takes%20days%20to%20weeks%2C%20while%0Asampling%20each%20image%20volume%20takes%20minutes%20to%20hours.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20Fast-DDPM%2C%20a%20simple%20yet%20effective%20approach%20capable%20of%20improving%0Atraining%20speed%2C%20sampling%20speed%2C%20and%20generation%20quality%20simultaneously.%20Unlike%0ADDPM%2C%20which%20trains%20the%20image%20denoiser%20across%201%2C000%20time%20steps%2C%20Fast-DDPM%20trains%0Aand%20samples%20using%20only%2010%20time%20steps.%20The%20key%20to%20our%20method%20lies%20in%20aligning%0Athe%20training%20and%20sampling%20procedures.%20We%20introduced%20two%20efficient%20noise%0Aschedulers%20with%2010%20time%20steps%3A%20one%20with%20uniform%20time%20step%20sampling%20and%20another%0Awith%20non-uniform%20sampling.%20We%20evaluated%20Fast-DDPM%20across%20three%20medical%0Aimage-to-image%20generation%20tasks%3A%20multi-image%20super-resolution%2C%20image%20denoising%2C%0Aand%20image-to-image%20translation.%20Fast-DDPM%20outperformed%20DDPM%20and%20current%0Astate-of-the-art%20methods%20based%20on%20convolutional%20networks%20and%20generative%0Aadversarial%20networks%20in%20all%20tasks.%20Additionally%2C%20Fast-DDPM%20reduced%20training%0Atime%20by%20a%20factor%20of%205%20and%20sampling%20time%20by%20a%20factor%20of%20100%20compared%20to%20DDPM.%0AOur%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/mirthAI/Fast-DDPM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520for%2520Medical%2520Image-to-Image%250A%2520%2520Generation%26entry.906535625%3DHongxu%2520Jiang%2520and%2520Muhammad%2520Imran%2520and%2520Linhai%2520Ma%2520and%2520Teng%2520Zhang%2520and%2520Yuyin%2520Zhou%2520and%2520Muxuan%2520Liang%2520and%2520Kuang%2520Gong%2520and%2520Wei%2520Shao%26entry.1292438233%3D%2520%2520Denoising%2520diffusion%2520probabilistic%2520models%2520%2528DDPMs%2529%2520have%2520achieved%2520unprecedented%250Asuccess%2520in%2520computer%2520vision.%2520However%252C%2520they%2520remain%2520underutilized%2520in%2520medical%250Aimaging%252C%2520a%2520field%2520crucial%2520for%2520disease%2520diagnosis%2520and%2520treatment%2520planning.%2520This%2520is%250Aprimarily%2520due%2520to%2520the%2520high%2520computational%2520cost%2520associated%2520with%2520%25281%2529%2520the%2520use%2520of%250Alarge%2520number%2520of%2520time%2520steps%2520%2528e.g.%252C%25201%252C000%2529%2520in%2520diffusion%2520processes%2520and%2520%25282%2529%2520the%250Aincreased%2520dimensionality%2520of%2520medical%2520images%252C%2520which%2520are%2520often%25203D%2520or%25204D.%2520Training%250Aa%2520diffusion%2520model%2520on%2520medical%2520images%2520typically%2520takes%2520days%2520to%2520weeks%252C%2520while%250Asampling%2520each%2520image%2520volume%2520takes%2520minutes%2520to%2520hours.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520introduce%2520Fast-DDPM%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520capable%2520of%2520improving%250Atraining%2520speed%252C%2520sampling%2520speed%252C%2520and%2520generation%2520quality%2520simultaneously.%2520Unlike%250ADDPM%252C%2520which%2520trains%2520the%2520image%2520denoiser%2520across%25201%252C000%2520time%2520steps%252C%2520Fast-DDPM%2520trains%250Aand%2520samples%2520using%2520only%252010%2520time%2520steps.%2520The%2520key%2520to%2520our%2520method%2520lies%2520in%2520aligning%250Athe%2520training%2520and%2520sampling%2520procedures.%2520We%2520introduced%2520two%2520efficient%2520noise%250Aschedulers%2520with%252010%2520time%2520steps%253A%2520one%2520with%2520uniform%2520time%2520step%2520sampling%2520and%2520another%250Awith%2520non-uniform%2520sampling.%2520We%2520evaluated%2520Fast-DDPM%2520across%2520three%2520medical%250Aimage-to-image%2520generation%2520tasks%253A%2520multi-image%2520super-resolution%252C%2520image%2520denoising%252C%250Aand%2520image-to-image%2520translation.%2520Fast-DDPM%2520outperformed%2520DDPM%2520and%2520current%250Astate-of-the-art%2520methods%2520based%2520on%2520convolutional%2520networks%2520and%2520generative%250Aadversarial%2520networks%2520in%2520all%2520tasks.%2520Additionally%252C%2520Fast-DDPM%2520reduced%2520training%250Atime%2520by%2520a%2520factor%2520of%25205%2520and%2520sampling%2520time%2520by%2520a%2520factor%2520of%2520100%2520compared%2520to%2520DDPM.%250AOur%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/mirthAI/Fast-DDPM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Medical%20Image-to-Image%0A%20%20Generation&entry.906535625=Hongxu%20Jiang%20and%20Muhammad%20Imran%20and%20Linhai%20Ma%20and%20Teng%20Zhang%20and%20Yuyin%20Zhou%20and%20Muxuan%20Liang%20and%20Kuang%20Gong%20and%20Wei%20Shao&entry.1292438233=%20%20Denoising%20diffusion%20probabilistic%20models%20%28DDPMs%29%20have%20achieved%20unprecedented%0Asuccess%20in%20computer%20vision.%20However%2C%20they%20remain%20underutilized%20in%20medical%0Aimaging%2C%20a%20field%20crucial%20for%20disease%20diagnosis%20and%20treatment%20planning.%20This%20is%0Aprimarily%20due%20to%20the%20high%20computational%20cost%20associated%20with%20%281%29%20the%20use%20of%0Alarge%20number%20of%20time%20steps%20%28e.g.%2C%201%2C000%29%20in%20diffusion%20processes%20and%20%282%29%20the%0Aincreased%20dimensionality%20of%20medical%20images%2C%20which%20are%20often%203D%20or%204D.%20Training%0Aa%20diffusion%20model%20on%20medical%20images%20typically%20takes%20days%20to%20weeks%2C%20while%0Asampling%20each%20image%20volume%20takes%20minutes%20to%20hours.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20Fast-DDPM%2C%20a%20simple%20yet%20effective%20approach%20capable%20of%20improving%0Atraining%20speed%2C%20sampling%20speed%2C%20and%20generation%20quality%20simultaneously.%20Unlike%0ADDPM%2C%20which%20trains%20the%20image%20denoiser%20across%201%2C000%20time%20steps%2C%20Fast-DDPM%20trains%0Aand%20samples%20using%20only%2010%20time%20steps.%20The%20key%20to%20our%20method%20lies%20in%20aligning%0Athe%20training%20and%20sampling%20procedures.%20We%20introduced%20two%20efficient%20noise%0Aschedulers%20with%2010%20time%20steps%3A%20one%20with%20uniform%20time%20step%20sampling%20and%20another%0Awith%20non-uniform%20sampling.%20We%20evaluated%20Fast-DDPM%20across%20three%20medical%0Aimage-to-image%20generation%20tasks%3A%20multi-image%20super-resolution%2C%20image%20denoising%2C%0Aand%20image-to-image%20translation.%20Fast-DDPM%20outperformed%20DDPM%20and%20current%0Astate-of-the-art%20methods%20based%20on%20convolutional%20networks%20and%20generative%0Aadversarial%20networks%20in%20all%20tasks.%20Additionally%2C%20Fast-DDPM%20reduced%20training%0Atime%20by%20a%20factor%20of%205%20and%20sampling%20time%20by%20a%20factor%20of%20100%20compared%20to%20DDPM.%0AOur%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/mirthAI/Fast-DDPM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14802v1&entry.124074799=Read"},
{"title": "Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation", "author": "Shiqi Yang and Zhi Zhong and Mengjie Zhao and Shusuke Takahashi and Masato Ishii and Takashi Shibuya and Yuki Mitsufuji", "abstract": "  In recent years, with the realistic generation results and a wide range of\npersonalized applications, diffusion-based generative models gain huge\nattention in both visual and audio generation areas. Compared to the\nconsiderable advancements of text2image or text2audio generation, research in\naudio2visual or visual2audio generation has been relatively slow. The recent\naudio-visual generation methods usually resort to huge large language model or\ncomposable diffusion models. Instead of designing another giant model for\naudio-visual generation, in this paper we take a step back showing a simple and\nlightweight generative transformer, which is not fully investigated in\nmulti-modal generation, can achieve excellent results on image2audio\ngeneration. The transformer operates in the discrete audio and visual\nVector-Quantized GAN space, and is trained in the mask denoising manner. After\ntraining, the classifier-free guidance could be deployed off-the-shelf\nachieving better performance, without any extra training or modification. Since\nthe transformer model is modality symmetrical, it could also be directly\ndeployed for audio2image generation and co-generation. In the experiments, we\nshow that our simple method surpasses recent image2audio generation methods.\nGenerated audio samples can be found at\nhttps://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ\n", "link": "http://arxiv.org/abs/2405.14598v1", "date": "2024-05-23", "relevancy": 2.4664, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6183}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation&body=Title%3A%20Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation%0AAuthor%3A%20Shiqi%20Yang%20and%20Zhi%20Zhong%20and%20Mengjie%20Zhao%20and%20Shusuke%20Takahashi%20and%20Masato%20Ishii%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20In%20recent%20years%2C%20with%20the%20realistic%20generation%20results%20and%20a%20wide%20range%20of%0Apersonalized%20applications%2C%20diffusion-based%20generative%20models%20gain%20huge%0Aattention%20in%20both%20visual%20and%20audio%20generation%20areas.%20Compared%20to%20the%0Aconsiderable%20advancements%20of%20text2image%20or%20text2audio%20generation%2C%20research%20in%0Aaudio2visual%20or%20visual2audio%20generation%20has%20been%20relatively%20slow.%20The%20recent%0Aaudio-visual%20generation%20methods%20usually%20resort%20to%20huge%20large%20language%20model%20or%0Acomposable%20diffusion%20models.%20Instead%20of%20designing%20another%20giant%20model%20for%0Aaudio-visual%20generation%2C%20in%20this%20paper%20we%20take%20a%20step%20back%20showing%20a%20simple%20and%0Alightweight%20generative%20transformer%2C%20which%20is%20not%20fully%20investigated%20in%0Amulti-modal%20generation%2C%20can%20achieve%20excellent%20results%20on%20image2audio%0Ageneration.%20The%20transformer%20operates%20in%20the%20discrete%20audio%20and%20visual%0AVector-Quantized%20GAN%20space%2C%20and%20is%20trained%20in%20the%20mask%20denoising%20manner.%20After%0Atraining%2C%20the%20classifier-free%20guidance%20could%20be%20deployed%20off-the-shelf%0Aachieving%20better%20performance%2C%20without%20any%20extra%20training%20or%20modification.%20Since%0Athe%20transformer%20model%20is%20modality%20symmetrical%2C%20it%20could%20also%20be%20directly%0Adeployed%20for%20audio2image%20generation%20and%20co-generation.%20In%20the%20experiments%2C%20we%0Ashow%20that%20our%20simple%20method%20surpasses%20recent%20image2audio%20generation%20methods.%0AGenerated%20audio%20samples%20can%20be%20found%20at%0Ahttps%3A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Echoes%253A%2520A%2520Simple%2520Unified%2520Transformer%2520for%2520Audio-Visual%2520Generation%26entry.906535625%3DShiqi%2520Yang%2520and%2520Zhi%2520Zhong%2520and%2520Mengjie%2520Zhao%2520and%2520Shusuke%2520Takahashi%2520and%2520Masato%2520Ishii%2520and%2520Takashi%2520Shibuya%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520with%2520the%2520realistic%2520generation%2520results%2520and%2520a%2520wide%2520range%2520of%250Apersonalized%2520applications%252C%2520diffusion-based%2520generative%2520models%2520gain%2520huge%250Aattention%2520in%2520both%2520visual%2520and%2520audio%2520generation%2520areas.%2520Compared%2520to%2520the%250Aconsiderable%2520advancements%2520of%2520text2image%2520or%2520text2audio%2520generation%252C%2520research%2520in%250Aaudio2visual%2520or%2520visual2audio%2520generation%2520has%2520been%2520relatively%2520slow.%2520The%2520recent%250Aaudio-visual%2520generation%2520methods%2520usually%2520resort%2520to%2520huge%2520large%2520language%2520model%2520or%250Acomposable%2520diffusion%2520models.%2520Instead%2520of%2520designing%2520another%2520giant%2520model%2520for%250Aaudio-visual%2520generation%252C%2520in%2520this%2520paper%2520we%2520take%2520a%2520step%2520back%2520showing%2520a%2520simple%2520and%250Alightweight%2520generative%2520transformer%252C%2520which%2520is%2520not%2520fully%2520investigated%2520in%250Amulti-modal%2520generation%252C%2520can%2520achieve%2520excellent%2520results%2520on%2520image2audio%250Ageneration.%2520The%2520transformer%2520operates%2520in%2520the%2520discrete%2520audio%2520and%2520visual%250AVector-Quantized%2520GAN%2520space%252C%2520and%2520is%2520trained%2520in%2520the%2520mask%2520denoising%2520manner.%2520After%250Atraining%252C%2520the%2520classifier-free%2520guidance%2520could%2520be%2520deployed%2520off-the-shelf%250Aachieving%2520better%2520performance%252C%2520without%2520any%2520extra%2520training%2520or%2520modification.%2520Since%250Athe%2520transformer%2520model%2520is%2520modality%2520symmetrical%252C%2520it%2520could%2520also%2520be%2520directly%250Adeployed%2520for%2520audio2image%2520generation%2520and%2520co-generation.%2520In%2520the%2520experiments%252C%2520we%250Ashow%2520that%2520our%2520simple%2520method%2520surpasses%2520recent%2520image2audio%2520generation%2520methods.%250AGenerated%2520audio%2520samples%2520can%2520be%2520found%2520at%250Ahttps%253A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Echoes%3A%20A%20Simple%20Unified%20Transformer%20for%20Audio-Visual%20Generation&entry.906535625=Shiqi%20Yang%20and%20Zhi%20Zhong%20and%20Mengjie%20Zhao%20and%20Shusuke%20Takahashi%20and%20Masato%20Ishii%20and%20Takashi%20Shibuya%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20In%20recent%20years%2C%20with%20the%20realistic%20generation%20results%20and%20a%20wide%20range%20of%0Apersonalized%20applications%2C%20diffusion-based%20generative%20models%20gain%20huge%0Aattention%20in%20both%20visual%20and%20audio%20generation%20areas.%20Compared%20to%20the%0Aconsiderable%20advancements%20of%20text2image%20or%20text2audio%20generation%2C%20research%20in%0Aaudio2visual%20or%20visual2audio%20generation%20has%20been%20relatively%20slow.%20The%20recent%0Aaudio-visual%20generation%20methods%20usually%20resort%20to%20huge%20large%20language%20model%20or%0Acomposable%20diffusion%20models.%20Instead%20of%20designing%20another%20giant%20model%20for%0Aaudio-visual%20generation%2C%20in%20this%20paper%20we%20take%20a%20step%20back%20showing%20a%20simple%20and%0Alightweight%20generative%20transformer%2C%20which%20is%20not%20fully%20investigated%20in%0Amulti-modal%20generation%2C%20can%20achieve%20excellent%20results%20on%20image2audio%0Ageneration.%20The%20transformer%20operates%20in%20the%20discrete%20audio%20and%20visual%0AVector-Quantized%20GAN%20space%2C%20and%20is%20trained%20in%20the%20mask%20denoising%20manner.%20After%0Atraining%2C%20the%20classifier-free%20guidance%20could%20be%20deployed%20off-the-shelf%0Aachieving%20better%20performance%2C%20without%20any%20extra%20training%20or%20modification.%20Since%0Athe%20transformer%20model%20is%20modality%20symmetrical%2C%20it%20could%20also%20be%20directly%0Adeployed%20for%20audio2image%20generation%20and%20co-generation.%20In%20the%20experiments%2C%20we%0Ashow%20that%20our%20simple%20method%20surpasses%20recent%20image2audio%20generation%20methods.%0AGenerated%20audio%20samples%20can%20be%20found%20at%0Ahttps%3A//docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14598v1&entry.124074799=Read"},
{"title": "ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation", "author": "Bo Peng and Xinyuan Chen and Yaohui Wang and Chaochao Lu and Yu Qiao", "abstract": "  Recent works have successfully extended large-scale text-to-image models to\nthe video domain, producing promising results but at a high computational cost\nand requiring a large amount of video data. In this work, we introduce\nConditionVideo, a training-free approach to text-to-video generation based on\nthe provided condition, video, and input text, by leveraging the power of\noff-the-shelf text-to-image generation methods (e.g., Stable Diffusion).\nConditionVideo generates realistic dynamic videos from random noise or given\nscene videos. Our method explicitly disentangles the motion representation into\ncondition-guided and scenery motion components. To this end, the ConditionVideo\nmodel is designed with a UNet branch and a control branch. To improve temporal\ncoherence, we introduce sparse bi-directional spatial-temporal attention\n(sBiST-Attn). The 3D control network extends the conventional 2D controlnet\nmodel, aiming to strengthen conditional generation accuracy by additionally\nleveraging the bi-directional frames in the temporal domain. Our method\nexhibits superior performance in terms of frame consistency, clip score, and\nconditional accuracy, outperforming other compared methods.\n", "link": "http://arxiv.org/abs/2310.07697v2", "date": "2024-05-23", "relevancy": 2.4487, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6301}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6151}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConditionVideo%3A%20Training-Free%20Condition-Guided%20Text-to-Video%20Generation&body=Title%3A%20ConditionVideo%3A%20Training-Free%20Condition-Guided%20Text-to-Video%20Generation%0AAuthor%3A%20Bo%20Peng%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Chaochao%20Lu%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Recent%20works%20have%20successfully%20extended%20large-scale%20text-to-image%20models%20to%0Athe%20video%20domain%2C%20producing%20promising%20results%20but%20at%20a%20high%20computational%20cost%0Aand%20requiring%20a%20large%20amount%20of%20video%20data.%20In%20this%20work%2C%20we%20introduce%0AConditionVideo%2C%20a%20training-free%20approach%20to%20text-to-video%20generation%20based%20on%0Athe%20provided%20condition%2C%20video%2C%20and%20input%20text%2C%20by%20leveraging%20the%20power%20of%0Aoff-the-shelf%20text-to-image%20generation%20methods%20%28e.g.%2C%20Stable%20Diffusion%29.%0AConditionVideo%20generates%20realistic%20dynamic%20videos%20from%20random%20noise%20or%20given%0Ascene%20videos.%20Our%20method%20explicitly%20disentangles%20the%20motion%20representation%20into%0Acondition-guided%20and%20scenery%20motion%20components.%20To%20this%20end%2C%20the%20ConditionVideo%0Amodel%20is%20designed%20with%20a%20UNet%20branch%20and%20a%20control%20branch.%20To%20improve%20temporal%0Acoherence%2C%20we%20introduce%20sparse%20bi-directional%20spatial-temporal%20attention%0A%28sBiST-Attn%29.%20The%203D%20control%20network%20extends%20the%20conventional%202D%20controlnet%0Amodel%2C%20aiming%20to%20strengthen%20conditional%20generation%20accuracy%20by%20additionally%0Aleveraging%20the%20bi-directional%20frames%20in%20the%20temporal%20domain.%20Our%20method%0Aexhibits%20superior%20performance%20in%20terms%20of%20frame%20consistency%2C%20clip%20score%2C%20and%0Aconditional%20accuracy%2C%20outperforming%20other%20compared%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditionVideo%253A%2520Training-Free%2520Condition-Guided%2520Text-to-Video%2520Generation%26entry.906535625%3DBo%2520Peng%2520and%2520Xinyuan%2520Chen%2520and%2520Yaohui%2520Wang%2520and%2520Chaochao%2520Lu%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Recent%2520works%2520have%2520successfully%2520extended%2520large-scale%2520text-to-image%2520models%2520to%250Athe%2520video%2520domain%252C%2520producing%2520promising%2520results%2520but%2520at%2520a%2520high%2520computational%2520cost%250Aand%2520requiring%2520a%2520large%2520amount%2520of%2520video%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%250AConditionVideo%252C%2520a%2520training-free%2520approach%2520to%2520text-to-video%2520generation%2520based%2520on%250Athe%2520provided%2520condition%252C%2520video%252C%2520and%2520input%2520text%252C%2520by%2520leveraging%2520the%2520power%2520of%250Aoff-the-shelf%2520text-to-image%2520generation%2520methods%2520%2528e.g.%252C%2520Stable%2520Diffusion%2529.%250AConditionVideo%2520generates%2520realistic%2520dynamic%2520videos%2520from%2520random%2520noise%2520or%2520given%250Ascene%2520videos.%2520Our%2520method%2520explicitly%2520disentangles%2520the%2520motion%2520representation%2520into%250Acondition-guided%2520and%2520scenery%2520motion%2520components.%2520To%2520this%2520end%252C%2520the%2520ConditionVideo%250Amodel%2520is%2520designed%2520with%2520a%2520UNet%2520branch%2520and%2520a%2520control%2520branch.%2520To%2520improve%2520temporal%250Acoherence%252C%2520we%2520introduce%2520sparse%2520bi-directional%2520spatial-temporal%2520attention%250A%2528sBiST-Attn%2529.%2520The%25203D%2520control%2520network%2520extends%2520the%2520conventional%25202D%2520controlnet%250Amodel%252C%2520aiming%2520to%2520strengthen%2520conditional%2520generation%2520accuracy%2520by%2520additionally%250Aleveraging%2520the%2520bi-directional%2520frames%2520in%2520the%2520temporal%2520domain.%2520Our%2520method%250Aexhibits%2520superior%2520performance%2520in%2520terms%2520of%2520frame%2520consistency%252C%2520clip%2520score%252C%2520and%250Aconditional%2520accuracy%252C%2520outperforming%2520other%2520compared%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConditionVideo%3A%20Training-Free%20Condition-Guided%20Text-to-Video%20Generation&entry.906535625=Bo%20Peng%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Chaochao%20Lu%20and%20Yu%20Qiao&entry.1292438233=%20%20Recent%20works%20have%20successfully%20extended%20large-scale%20text-to-image%20models%20to%0Athe%20video%20domain%2C%20producing%20promising%20results%20but%20at%20a%20high%20computational%20cost%0Aand%20requiring%20a%20large%20amount%20of%20video%20data.%20In%20this%20work%2C%20we%20introduce%0AConditionVideo%2C%20a%20training-free%20approach%20to%20text-to-video%20generation%20based%20on%0Athe%20provided%20condition%2C%20video%2C%20and%20input%20text%2C%20by%20leveraging%20the%20power%20of%0Aoff-the-shelf%20text-to-image%20generation%20methods%20%28e.g.%2C%20Stable%20Diffusion%29.%0AConditionVideo%20generates%20realistic%20dynamic%20videos%20from%20random%20noise%20or%20given%0Ascene%20videos.%20Our%20method%20explicitly%20disentangles%20the%20motion%20representation%20into%0Acondition-guided%20and%20scenery%20motion%20components.%20To%20this%20end%2C%20the%20ConditionVideo%0Amodel%20is%20designed%20with%20a%20UNet%20branch%20and%20a%20control%20branch.%20To%20improve%20temporal%0Acoherence%2C%20we%20introduce%20sparse%20bi-directional%20spatial-temporal%20attention%0A%28sBiST-Attn%29.%20The%203D%20control%20network%20extends%20the%20conventional%202D%20controlnet%0Amodel%2C%20aiming%20to%20strengthen%20conditional%20generation%20accuracy%20by%20additionally%0Aleveraging%20the%20bi-directional%20frames%20in%20the%20temporal%20domain.%20Our%20method%0Aexhibits%20superior%20performance%20in%20terms%20of%20frame%20consistency%2C%20clip%20score%2C%20and%0Aconditional%20accuracy%2C%20outperforming%20other%20compared%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07697v2&entry.124074799=Read"},
{"title": "Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by\n  Self-Contrast", "author": "Chufan Shi and Cheng Yang and Xinyu Zhu and Jiahao Wang and Taiqiang Wu and Siheng Li and Deng Cai and Yujiu Yang and Yu Meng", "abstract": "  Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling\nmodel size while maintaining computational efficiency. In MoE, each token in\nthe input sequence activates a different subset of experts determined by a\nrouting mechanism. However, the unchosen experts in MoE models do not\ncontribute to the output, potentially leading to underutilization of the\nmodel's capacity. In this work, we first conduct exploratory studies to\ndemonstrate that increasing the number of activated experts does not\nnecessarily improve and can even degrade the output quality. Then, we show that\noutput distributions from an MoE model using different routing strategies\nsubstantially differ, indicating that different experts do not always act\nsynergistically. Motivated by these findings, we propose Self-Contrast\nMixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen\nexperts in a self-contrast manner during inference. In SCMoE, the next-token\nprobabilities are determined by contrasting the outputs from strong and weak\nactivation using the same MoE model. Our method is conceptually simple and\ncomputationally lightweight, as it incurs minimal latency compared to greedy\ndecoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and\nHumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's\nreasoning capability across various domains. For example, it improves the\naccuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with\nself-consistency yields additional gains, increasing major@20 accuracy from\n75.59 to 78.31.\n", "link": "http://arxiv.org/abs/2405.14507v1", "date": "2024-05-23", "relevancy": 2.4422, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.492}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4901}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unchosen%20Experts%20Can%20Contribute%20Too%3A%20Unleashing%20MoE%20Models%27%20Power%20by%0A%20%20Self-Contrast&body=Title%3A%20Unchosen%20Experts%20Can%20Contribute%20Too%3A%20Unleashing%20MoE%20Models%27%20Power%20by%0A%20%20Self-Contrast%0AAuthor%3A%20Chufan%20Shi%20and%20Cheng%20Yang%20and%20Xinyu%20Zhu%20and%20Jiahao%20Wang%20and%20Taiqiang%20Wu%20and%20Siheng%20Li%20and%20Deng%20Cai%20and%20Yujiu%20Yang%20and%20Yu%20Meng%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20has%20emerged%20as%20a%20prominent%20architecture%20for%20scaling%0Amodel%20size%20while%20maintaining%20computational%20efficiency.%20In%20MoE%2C%20each%20token%20in%0Athe%20input%20sequence%20activates%20a%20different%20subset%20of%20experts%20determined%20by%20a%0Arouting%20mechanism.%20However%2C%20the%20unchosen%20experts%20in%20MoE%20models%20do%20not%0Acontribute%20to%20the%20output%2C%20potentially%20leading%20to%20underutilization%20of%20the%0Amodel%27s%20capacity.%20In%20this%20work%2C%20we%20first%20conduct%20exploratory%20studies%20to%0Ademonstrate%20that%20increasing%20the%20number%20of%20activated%20experts%20does%20not%0Anecessarily%20improve%20and%20can%20even%20degrade%20the%20output%20quality.%20Then%2C%20we%20show%20that%0Aoutput%20distributions%20from%20an%20MoE%20model%20using%20different%20routing%20strategies%0Asubstantially%20differ%2C%20indicating%20that%20different%20experts%20do%20not%20always%20act%0Asynergistically.%20Motivated%20by%20these%20findings%2C%20we%20propose%20Self-Contrast%0AMixture-of-Experts%20%28SCMoE%29%2C%20a%20training-free%20strategy%20that%20utilizes%20unchosen%0Aexperts%20in%20a%20self-contrast%20manner%20during%20inference.%20In%20SCMoE%2C%20the%20next-token%0Aprobabilities%20are%20determined%20by%20contrasting%20the%20outputs%20from%20strong%20and%20weak%0Aactivation%20using%20the%20same%20MoE%20model.%20Our%20method%20is%20conceptually%20simple%20and%0Acomputationally%20lightweight%2C%20as%20it%20incurs%20minimal%20latency%20compared%20to%20greedy%0Adecoding.%20Experiments%20on%20several%20benchmarks%20%28GSM8K%2C%20StrategyQA%2C%20MBPP%20and%0AHumanEval%29%20demonstrate%20that%20SCMoE%20can%20consistently%20enhance%20Mixtral%208x7B%27s%0Areasoning%20capability%20across%20various%20domains.%20For%20example%2C%20it%20improves%20the%0Aaccuracy%20on%20GSM8K%20from%2061.79%20to%2066.94.%20Moreover%2C%20combining%20SCMoE%20with%0Aself-consistency%20yields%20additional%20gains%2C%20increasing%20major%4020%20accuracy%20from%0A75.59%20to%2078.31.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnchosen%2520Experts%2520Can%2520Contribute%2520Too%253A%2520Unleashing%2520MoE%2520Models%2527%2520Power%2520by%250A%2520%2520Self-Contrast%26entry.906535625%3DChufan%2520Shi%2520and%2520Cheng%2520Yang%2520and%2520Xinyu%2520Zhu%2520and%2520Jiahao%2520Wang%2520and%2520Taiqiang%2520Wu%2520and%2520Siheng%2520Li%2520and%2520Deng%2520Cai%2520and%2520Yujiu%2520Yang%2520and%2520Yu%2520Meng%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520has%2520emerged%2520as%2520a%2520prominent%2520architecture%2520for%2520scaling%250Amodel%2520size%2520while%2520maintaining%2520computational%2520efficiency.%2520In%2520MoE%252C%2520each%2520token%2520in%250Athe%2520input%2520sequence%2520activates%2520a%2520different%2520subset%2520of%2520experts%2520determined%2520by%2520a%250Arouting%2520mechanism.%2520However%252C%2520the%2520unchosen%2520experts%2520in%2520MoE%2520models%2520do%2520not%250Acontribute%2520to%2520the%2520output%252C%2520potentially%2520leading%2520to%2520underutilization%2520of%2520the%250Amodel%2527s%2520capacity.%2520In%2520this%2520work%252C%2520we%2520first%2520conduct%2520exploratory%2520studies%2520to%250Ademonstrate%2520that%2520increasing%2520the%2520number%2520of%2520activated%2520experts%2520does%2520not%250Anecessarily%2520improve%2520and%2520can%2520even%2520degrade%2520the%2520output%2520quality.%2520Then%252C%2520we%2520show%2520that%250Aoutput%2520distributions%2520from%2520an%2520MoE%2520model%2520using%2520different%2520routing%2520strategies%250Asubstantially%2520differ%252C%2520indicating%2520that%2520different%2520experts%2520do%2520not%2520always%2520act%250Asynergistically.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520Self-Contrast%250AMixture-of-Experts%2520%2528SCMoE%2529%252C%2520a%2520training-free%2520strategy%2520that%2520utilizes%2520unchosen%250Aexperts%2520in%2520a%2520self-contrast%2520manner%2520during%2520inference.%2520In%2520SCMoE%252C%2520the%2520next-token%250Aprobabilities%2520are%2520determined%2520by%2520contrasting%2520the%2520outputs%2520from%2520strong%2520and%2520weak%250Aactivation%2520using%2520the%2520same%2520MoE%2520model.%2520Our%2520method%2520is%2520conceptually%2520simple%2520and%250Acomputationally%2520lightweight%252C%2520as%2520it%2520incurs%2520minimal%2520latency%2520compared%2520to%2520greedy%250Adecoding.%2520Experiments%2520on%2520several%2520benchmarks%2520%2528GSM8K%252C%2520StrategyQA%252C%2520MBPP%2520and%250AHumanEval%2529%2520demonstrate%2520that%2520SCMoE%2520can%2520consistently%2520enhance%2520Mixtral%25208x7B%2527s%250Areasoning%2520capability%2520across%2520various%2520domains.%2520For%2520example%252C%2520it%2520improves%2520the%250Aaccuracy%2520on%2520GSM8K%2520from%252061.79%2520to%252066.94.%2520Moreover%252C%2520combining%2520SCMoE%2520with%250Aself-consistency%2520yields%2520additional%2520gains%252C%2520increasing%2520major%254020%2520accuracy%2520from%250A75.59%2520to%252078.31.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unchosen%20Experts%20Can%20Contribute%20Too%3A%20Unleashing%20MoE%20Models%27%20Power%20by%0A%20%20Self-Contrast&entry.906535625=Chufan%20Shi%20and%20Cheng%20Yang%20and%20Xinyu%20Zhu%20and%20Jiahao%20Wang%20and%20Taiqiang%20Wu%20and%20Siheng%20Li%20and%20Deng%20Cai%20and%20Yujiu%20Yang%20and%20Yu%20Meng&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20has%20emerged%20as%20a%20prominent%20architecture%20for%20scaling%0Amodel%20size%20while%20maintaining%20computational%20efficiency.%20In%20MoE%2C%20each%20token%20in%0Athe%20input%20sequence%20activates%20a%20different%20subset%20of%20experts%20determined%20by%20a%0Arouting%20mechanism.%20However%2C%20the%20unchosen%20experts%20in%20MoE%20models%20do%20not%0Acontribute%20to%20the%20output%2C%20potentially%20leading%20to%20underutilization%20of%20the%0Amodel%27s%20capacity.%20In%20this%20work%2C%20we%20first%20conduct%20exploratory%20studies%20to%0Ademonstrate%20that%20increasing%20the%20number%20of%20activated%20experts%20does%20not%0Anecessarily%20improve%20and%20can%20even%20degrade%20the%20output%20quality.%20Then%2C%20we%20show%20that%0Aoutput%20distributions%20from%20an%20MoE%20model%20using%20different%20routing%20strategies%0Asubstantially%20differ%2C%20indicating%20that%20different%20experts%20do%20not%20always%20act%0Asynergistically.%20Motivated%20by%20these%20findings%2C%20we%20propose%20Self-Contrast%0AMixture-of-Experts%20%28SCMoE%29%2C%20a%20training-free%20strategy%20that%20utilizes%20unchosen%0Aexperts%20in%20a%20self-contrast%20manner%20during%20inference.%20In%20SCMoE%2C%20the%20next-token%0Aprobabilities%20are%20determined%20by%20contrasting%20the%20outputs%20from%20strong%20and%20weak%0Aactivation%20using%20the%20same%20MoE%20model.%20Our%20method%20is%20conceptually%20simple%20and%0Acomputationally%20lightweight%2C%20as%20it%20incurs%20minimal%20latency%20compared%20to%20greedy%0Adecoding.%20Experiments%20on%20several%20benchmarks%20%28GSM8K%2C%20StrategyQA%2C%20MBPP%20and%0AHumanEval%29%20demonstrate%20that%20SCMoE%20can%20consistently%20enhance%20Mixtral%208x7B%27s%0Areasoning%20capability%20across%20various%20domains.%20For%20example%2C%20it%20improves%20the%0Aaccuracy%20on%20GSM8K%20from%2061.79%20to%2066.94.%20Moreover%2C%20combining%20SCMoE%20with%0Aself-consistency%20yields%20additional%20gains%2C%20increasing%20major%4020%20accuracy%20from%0A75.59%20to%2078.31.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14507v1&entry.124074799=Read"},
{"title": "Recurrent Early Exits for Federated Learning with Heterogeneous Clients", "author": "Royson Lee and Javier Fernandez-Marques and Shell Xu Hu and Da Li and Stefanos Laskaridis and \u0141ukasz Dudziak and Timothy Hospedales and Ferenc Husz\u00e1r and Nicholas D. Lane", "abstract": "  Federated learning (FL) has enabled distributed learning of a model across\nmultiple clients in a privacy-preserving manner. One of the main challenges of\nFL is to accommodate clients with varying hardware capacities; clients have\ndiffering compute and memory requirements. To tackle this challenge, recent\nstate-of-the-art approaches leverage the use of early exits. Nonetheless, these\napproaches fall short of mitigating the challenges of joint learning multiple\nexit classifiers, often relying on hand-picked heuristic solutions for\nknowledge distillation among classifiers and/or utilizing additional layers for\nweaker classifiers. In this work, instead of utilizing multiple classifiers, we\npropose a recurrent early exit approach named ReeFL that fuses features from\ndifferent sub-models into a single shared classifier. Specifically, we use a\ntransformer-based early-exit module shared among sub-models to i) better\nexploit multi-layer feature representations for task-specific prediction and\nii) modulate the feature representation of the backbone model for subsequent\npredictions. We additionally present a per-client self-distillation approach\nwhere the best sub-model is automatically selected as the teacher of the other\nsub-models at each client. Our experiments on standard image and speech\nclassification benchmarks across various emerging federated fine-tuning\nbaselines demonstrate ReeFL's effectiveness over previous works.\n", "link": "http://arxiv.org/abs/2405.14791v1", "date": "2024-05-23", "relevancy": 2.4173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4956}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients&body=Title%3A%20Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients%0AAuthor%3A%20Royson%20Lee%20and%20Javier%20Fernandez-Marques%20and%20Shell%20Xu%20Hu%20and%20Da%20Li%20and%20Stefanos%20Laskaridis%20and%20%C5%81ukasz%20Dudziak%20and%20Timothy%20Hospedales%20and%20Ferenc%20Husz%C3%A1r%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20enabled%20distributed%20learning%20of%20a%20model%20across%0Amultiple%20clients%20in%20a%20privacy-preserving%20manner.%20One%20of%20the%20main%20challenges%20of%0AFL%20is%20to%20accommodate%20clients%20with%20varying%20hardware%20capacities%3B%20clients%20have%0Adiffering%20compute%20and%20memory%20requirements.%20To%20tackle%20this%20challenge%2C%20recent%0Astate-of-the-art%20approaches%20leverage%20the%20use%20of%20early%20exits.%20Nonetheless%2C%20these%0Aapproaches%20fall%20short%20of%20mitigating%20the%20challenges%20of%20joint%20learning%20multiple%0Aexit%20classifiers%2C%20often%20relying%20on%20hand-picked%20heuristic%20solutions%20for%0Aknowledge%20distillation%20among%20classifiers%20and/or%20utilizing%20additional%20layers%20for%0Aweaker%20classifiers.%20In%20this%20work%2C%20instead%20of%20utilizing%20multiple%20classifiers%2C%20we%0Apropose%20a%20recurrent%20early%20exit%20approach%20named%20ReeFL%20that%20fuses%20features%20from%0Adifferent%20sub-models%20into%20a%20single%20shared%20classifier.%20Specifically%2C%20we%20use%20a%0Atransformer-based%20early-exit%20module%20shared%20among%20sub-models%20to%20i%29%20better%0Aexploit%20multi-layer%20feature%20representations%20for%20task-specific%20prediction%20and%0Aii%29%20modulate%20the%20feature%20representation%20of%20the%20backbone%20model%20for%20subsequent%0Apredictions.%20We%20additionally%20present%20a%20per-client%20self-distillation%20approach%0Awhere%20the%20best%20sub-model%20is%20automatically%20selected%20as%20the%20teacher%20of%20the%20other%0Asub-models%20at%20each%20client.%20Our%20experiments%20on%20standard%20image%20and%20speech%0Aclassification%20benchmarks%20across%20various%20emerging%20federated%20fine-tuning%0Abaselines%20demonstrate%20ReeFL%27s%20effectiveness%20over%20previous%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Early%2520Exits%2520for%2520Federated%2520Learning%2520with%2520Heterogeneous%2520Clients%26entry.906535625%3DRoyson%2520Lee%2520and%2520Javier%2520Fernandez-Marques%2520and%2520Shell%2520Xu%2520Hu%2520and%2520Da%2520Li%2520and%2520Stefanos%2520Laskaridis%2520and%2520%25C5%2581ukasz%2520Dudziak%2520and%2520Timothy%2520Hospedales%2520and%2520Ferenc%2520Husz%25C3%25A1r%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520enabled%2520distributed%2520learning%2520of%2520a%2520model%2520across%250Amultiple%2520clients%2520in%2520a%2520privacy-preserving%2520manner.%2520One%2520of%2520the%2520main%2520challenges%2520of%250AFL%2520is%2520to%2520accommodate%2520clients%2520with%2520varying%2520hardware%2520capacities%253B%2520clients%2520have%250Adiffering%2520compute%2520and%2520memory%2520requirements.%2520To%2520tackle%2520this%2520challenge%252C%2520recent%250Astate-of-the-art%2520approaches%2520leverage%2520the%2520use%2520of%2520early%2520exits.%2520Nonetheless%252C%2520these%250Aapproaches%2520fall%2520short%2520of%2520mitigating%2520the%2520challenges%2520of%2520joint%2520learning%2520multiple%250Aexit%2520classifiers%252C%2520often%2520relying%2520on%2520hand-picked%2520heuristic%2520solutions%2520for%250Aknowledge%2520distillation%2520among%2520classifiers%2520and/or%2520utilizing%2520additional%2520layers%2520for%250Aweaker%2520classifiers.%2520In%2520this%2520work%252C%2520instead%2520of%2520utilizing%2520multiple%2520classifiers%252C%2520we%250Apropose%2520a%2520recurrent%2520early%2520exit%2520approach%2520named%2520ReeFL%2520that%2520fuses%2520features%2520from%250Adifferent%2520sub-models%2520into%2520a%2520single%2520shared%2520classifier.%2520Specifically%252C%2520we%2520use%2520a%250Atransformer-based%2520early-exit%2520module%2520shared%2520among%2520sub-models%2520to%2520i%2529%2520better%250Aexploit%2520multi-layer%2520feature%2520representations%2520for%2520task-specific%2520prediction%2520and%250Aii%2529%2520modulate%2520the%2520feature%2520representation%2520of%2520the%2520backbone%2520model%2520for%2520subsequent%250Apredictions.%2520We%2520additionally%2520present%2520a%2520per-client%2520self-distillation%2520approach%250Awhere%2520the%2520best%2520sub-model%2520is%2520automatically%2520selected%2520as%2520the%2520teacher%2520of%2520the%2520other%250Asub-models%2520at%2520each%2520client.%2520Our%2520experiments%2520on%2520standard%2520image%2520and%2520speech%250Aclassification%2520benchmarks%2520across%2520various%2520emerging%2520federated%2520fine-tuning%250Abaselines%2520demonstrate%2520ReeFL%2527s%2520effectiveness%2520over%2520previous%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Early%20Exits%20for%20Federated%20Learning%20with%20Heterogeneous%20Clients&entry.906535625=Royson%20Lee%20and%20Javier%20Fernandez-Marques%20and%20Shell%20Xu%20Hu%20and%20Da%20Li%20and%20Stefanos%20Laskaridis%20and%20%C5%81ukasz%20Dudziak%20and%20Timothy%20Hospedales%20and%20Ferenc%20Husz%C3%A1r%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20enabled%20distributed%20learning%20of%20a%20model%20across%0Amultiple%20clients%20in%20a%20privacy-preserving%20manner.%20One%20of%20the%20main%20challenges%20of%0AFL%20is%20to%20accommodate%20clients%20with%20varying%20hardware%20capacities%3B%20clients%20have%0Adiffering%20compute%20and%20memory%20requirements.%20To%20tackle%20this%20challenge%2C%20recent%0Astate-of-the-art%20approaches%20leverage%20the%20use%20of%20early%20exits.%20Nonetheless%2C%20these%0Aapproaches%20fall%20short%20of%20mitigating%20the%20challenges%20of%20joint%20learning%20multiple%0Aexit%20classifiers%2C%20often%20relying%20on%20hand-picked%20heuristic%20solutions%20for%0Aknowledge%20distillation%20among%20classifiers%20and/or%20utilizing%20additional%20layers%20for%0Aweaker%20classifiers.%20In%20this%20work%2C%20instead%20of%20utilizing%20multiple%20classifiers%2C%20we%0Apropose%20a%20recurrent%20early%20exit%20approach%20named%20ReeFL%20that%20fuses%20features%20from%0Adifferent%20sub-models%20into%20a%20single%20shared%20classifier.%20Specifically%2C%20we%20use%20a%0Atransformer-based%20early-exit%20module%20shared%20among%20sub-models%20to%20i%29%20better%0Aexploit%20multi-layer%20feature%20representations%20for%20task-specific%20prediction%20and%0Aii%29%20modulate%20the%20feature%20representation%20of%20the%20backbone%20model%20for%20subsequent%0Apredictions.%20We%20additionally%20present%20a%20per-client%20self-distillation%20approach%0Awhere%20the%20best%20sub-model%20is%20automatically%20selected%20as%20the%20teacher%20of%20the%20other%0Asub-models%20at%20each%20client.%20Our%20experiments%20on%20standard%20image%20and%20speech%0Aclassification%20benchmarks%20across%20various%20emerging%20federated%20fine-tuning%0Abaselines%20demonstrate%20ReeFL%27s%20effectiveness%20over%20previous%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14791v1&entry.124074799=Read"},
{"title": "RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance", "author": "Zhicheng Sun and Zhenhao Yang and Yang Jin and Haozhe Chi and Kun Xu and Kun Xu and Liwei Chen and Hao Jiang and Di Zhang and Yang Song and Kun Gai and Yadong Mu", "abstract": "  Customizing diffusion models to generate identity-preserving images from\nuser-provided reference images is an intriguing new problem. The prevalent\napproaches typically require training on extensive domain-specific images to\nachieve identity preservation, which lacks flexibility across different use\ncases. To address this issue, we exploit classifier guidance, a training-free\ntechnique that steers diffusion models using an existing classifier, for\npersonalized image generation. Our study shows that based on a recent rectified\nflow framework, the major limitation of vanilla classifier guidance in\nrequiring a special classifier can be resolved with a simple fixed-point\nsolution, allowing flexible personalization with off-the-shelf image\ndiscriminators. Moreover, its solving procedure proves to be stable when\nanchored to a reference flow trajectory, with a convergence guarantee. The\nderived method is implemented on rectified flow with different off-the-shelf\nimage discriminators, delivering advantageous personalization results for human\nfaces, live subjects, and certain objects. Code is available at\nhttps://github.com/feifeiobama/RectifID.\n", "link": "http://arxiv.org/abs/2405.14677v1", "date": "2024-05-23", "relevancy": 2.4158, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6671}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6132}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance&body=Title%3A%20RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance%0AAuthor%3A%20Zhicheng%20Sun%20and%20Zhenhao%20Yang%20and%20Yang%20Jin%20and%20Haozhe%20Chi%20and%20Kun%20Xu%20and%20Kun%20Xu%20and%20Liwei%20Chen%20and%20Hao%20Jiang%20and%20Di%20Zhang%20and%20Yang%20Song%20and%20Kun%20Gai%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20Customizing%20diffusion%20models%20to%20generate%20identity-preserving%20images%20from%0Auser-provided%20reference%20images%20is%20an%20intriguing%20new%20problem.%20The%20prevalent%0Aapproaches%20typically%20require%20training%20on%20extensive%20domain-specific%20images%20to%0Aachieve%20identity%20preservation%2C%20which%20lacks%20flexibility%20across%20different%20use%0Acases.%20To%20address%20this%20issue%2C%20we%20exploit%20classifier%20guidance%2C%20a%20training-free%0Atechnique%20that%20steers%20diffusion%20models%20using%20an%20existing%20classifier%2C%20for%0Apersonalized%20image%20generation.%20Our%20study%20shows%20that%20based%20on%20a%20recent%20rectified%0Aflow%20framework%2C%20the%20major%20limitation%20of%20vanilla%20classifier%20guidance%20in%0Arequiring%20a%20special%20classifier%20can%20be%20resolved%20with%20a%20simple%20fixed-point%0Asolution%2C%20allowing%20flexible%20personalization%20with%20off-the-shelf%20image%0Adiscriminators.%20Moreover%2C%20its%20solving%20procedure%20proves%20to%20be%20stable%20when%0Aanchored%20to%20a%20reference%20flow%20trajectory%2C%20with%20a%20convergence%20guarantee.%20The%0Aderived%20method%20is%20implemented%20on%20rectified%20flow%20with%20different%20off-the-shelf%0Aimage%20discriminators%2C%20delivering%20advantageous%20personalization%20results%20for%20human%0Afaces%2C%20live%20subjects%2C%20and%20certain%20objects.%20Code%20is%20available%20at%0Ahttps%3A//github.com/feifeiobama/RectifID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRectifID%253A%2520Personalizing%2520Rectified%2520Flow%2520with%2520Anchored%2520Classifier%2520Guidance%26entry.906535625%3DZhicheng%2520Sun%2520and%2520Zhenhao%2520Yang%2520and%2520Yang%2520Jin%2520and%2520Haozhe%2520Chi%2520and%2520Kun%2520Xu%2520and%2520Kun%2520Xu%2520and%2520Liwei%2520Chen%2520and%2520Hao%2520Jiang%2520and%2520Di%2520Zhang%2520and%2520Yang%2520Song%2520and%2520Kun%2520Gai%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520Customizing%2520diffusion%2520models%2520to%2520generate%2520identity-preserving%2520images%2520from%250Auser-provided%2520reference%2520images%2520is%2520an%2520intriguing%2520new%2520problem.%2520The%2520prevalent%250Aapproaches%2520typically%2520require%2520training%2520on%2520extensive%2520domain-specific%2520images%2520to%250Aachieve%2520identity%2520preservation%252C%2520which%2520lacks%2520flexibility%2520across%2520different%2520use%250Acases.%2520To%2520address%2520this%2520issue%252C%2520we%2520exploit%2520classifier%2520guidance%252C%2520a%2520training-free%250Atechnique%2520that%2520steers%2520diffusion%2520models%2520using%2520an%2520existing%2520classifier%252C%2520for%250Apersonalized%2520image%2520generation.%2520Our%2520study%2520shows%2520that%2520based%2520on%2520a%2520recent%2520rectified%250Aflow%2520framework%252C%2520the%2520major%2520limitation%2520of%2520vanilla%2520classifier%2520guidance%2520in%250Arequiring%2520a%2520special%2520classifier%2520can%2520be%2520resolved%2520with%2520a%2520simple%2520fixed-point%250Asolution%252C%2520allowing%2520flexible%2520personalization%2520with%2520off-the-shelf%2520image%250Adiscriminators.%2520Moreover%252C%2520its%2520solving%2520procedure%2520proves%2520to%2520be%2520stable%2520when%250Aanchored%2520to%2520a%2520reference%2520flow%2520trajectory%252C%2520with%2520a%2520convergence%2520guarantee.%2520The%250Aderived%2520method%2520is%2520implemented%2520on%2520rectified%2520flow%2520with%2520different%2520off-the-shelf%250Aimage%2520discriminators%252C%2520delivering%2520advantageous%2520personalization%2520results%2520for%2520human%250Afaces%252C%2520live%2520subjects%252C%2520and%2520certain%2520objects.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/feifeiobama/RectifID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RectifID%3A%20Personalizing%20Rectified%20Flow%20with%20Anchored%20Classifier%20Guidance&entry.906535625=Zhicheng%20Sun%20and%20Zhenhao%20Yang%20and%20Yang%20Jin%20and%20Haozhe%20Chi%20and%20Kun%20Xu%20and%20Kun%20Xu%20and%20Liwei%20Chen%20and%20Hao%20Jiang%20and%20Di%20Zhang%20and%20Yang%20Song%20and%20Kun%20Gai%20and%20Yadong%20Mu&entry.1292438233=%20%20Customizing%20diffusion%20models%20to%20generate%20identity-preserving%20images%20from%0Auser-provided%20reference%20images%20is%20an%20intriguing%20new%20problem.%20The%20prevalent%0Aapproaches%20typically%20require%20training%20on%20extensive%20domain-specific%20images%20to%0Aachieve%20identity%20preservation%2C%20which%20lacks%20flexibility%20across%20different%20use%0Acases.%20To%20address%20this%20issue%2C%20we%20exploit%20classifier%20guidance%2C%20a%20training-free%0Atechnique%20that%20steers%20diffusion%20models%20using%20an%20existing%20classifier%2C%20for%0Apersonalized%20image%20generation.%20Our%20study%20shows%20that%20based%20on%20a%20recent%20rectified%0Aflow%20framework%2C%20the%20major%20limitation%20of%20vanilla%20classifier%20guidance%20in%0Arequiring%20a%20special%20classifier%20can%20be%20resolved%20with%20a%20simple%20fixed-point%0Asolution%2C%20allowing%20flexible%20personalization%20with%20off-the-shelf%20image%0Adiscriminators.%20Moreover%2C%20its%20solving%20procedure%20proves%20to%20be%20stable%20when%0Aanchored%20to%20a%20reference%20flow%20trajectory%2C%20with%20a%20convergence%20guarantee.%20The%0Aderived%20method%20is%20implemented%20on%20rectified%20flow%20with%20different%20off-the-shelf%0Aimage%20discriminators%2C%20delivering%20advantageous%20personalization%20results%20for%20human%0Afaces%2C%20live%20subjects%2C%20and%20certain%20objects.%20Code%20is%20available%20at%0Ahttps%3A//github.com/feifeiobama/RectifID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14677v1&entry.124074799=Read"},
{"title": "ClusterTabNet: Supervised clustering method for table detection and\n  table structure recognition", "author": "Marek Polewczyk and Marco Spinaci", "abstract": "  We present a novel deep-learning-based method to cluster words in documents\nwhich we apply to detect and recognize tables given the OCR output. We\ninterpret table structure bottom-up as a graph of relations between pairs of\nwords (belonging to the same row, column, header, as well as to the same table)\nand use a transformer encoder model to predict its adjacency matrix. We\ndemonstrate the performance of our method on the PubTables-1M dataset as well\nas PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art\ndetection methods such as DETR and Faster R-CNN, our method achieves similar or\nbetter accuracy, while requiring a significantly smaller model.\n", "link": "http://arxiv.org/abs/2402.07502v2", "date": "2024-05-23", "relevancy": 2.3956, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClusterTabNet%3A%20Supervised%20clustering%20method%20for%20table%20detection%20and%0A%20%20table%20structure%20recognition&body=Title%3A%20ClusterTabNet%3A%20Supervised%20clustering%20method%20for%20table%20detection%20and%0A%20%20table%20structure%20recognition%0AAuthor%3A%20Marek%20Polewczyk%20and%20Marco%20Spinaci%0AAbstract%3A%20%20%20We%20present%20a%20novel%20deep-learning-based%20method%20to%20cluster%20words%20in%20documents%0Awhich%20we%20apply%20to%20detect%20and%20recognize%20tables%20given%20the%20OCR%20output.%20We%0Ainterpret%20table%20structure%20bottom-up%20as%20a%20graph%20of%20relations%20between%20pairs%20of%0Awords%20%28belonging%20to%20the%20same%20row%2C%20column%2C%20header%2C%20as%20well%20as%20to%20the%20same%20table%29%0Aand%20use%20a%20transformer%20encoder%20model%20to%20predict%20its%20adjacency%20matrix.%20We%0Ademonstrate%20the%20performance%20of%20our%20method%20on%20the%20PubTables-1M%20dataset%20as%20well%0Aas%20PubTabNet%20and%20FinTabNet%20datasets.%20Compared%20to%20the%20current%20state-of-the-art%0Adetection%20methods%20such%20as%20DETR%20and%20Faster%20R-CNN%2C%20our%20method%20achieves%20similar%20or%0Abetter%20accuracy%2C%20while%20requiring%20a%20significantly%20smaller%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClusterTabNet%253A%2520Supervised%2520clustering%2520method%2520for%2520table%2520detection%2520and%250A%2520%2520table%2520structure%2520recognition%26entry.906535625%3DMarek%2520Polewczyk%2520and%2520Marco%2520Spinaci%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520deep-learning-based%2520method%2520to%2520cluster%2520words%2520in%2520documents%250Awhich%2520we%2520apply%2520to%2520detect%2520and%2520recognize%2520tables%2520given%2520the%2520OCR%2520output.%2520We%250Ainterpret%2520table%2520structure%2520bottom-up%2520as%2520a%2520graph%2520of%2520relations%2520between%2520pairs%2520of%250Awords%2520%2528belonging%2520to%2520the%2520same%2520row%252C%2520column%252C%2520header%252C%2520as%2520well%2520as%2520to%2520the%2520same%2520table%2529%250Aand%2520use%2520a%2520transformer%2520encoder%2520model%2520to%2520predict%2520its%2520adjacency%2520matrix.%2520We%250Ademonstrate%2520the%2520performance%2520of%2520our%2520method%2520on%2520the%2520PubTables-1M%2520dataset%2520as%2520well%250Aas%2520PubTabNet%2520and%2520FinTabNet%2520datasets.%2520Compared%2520to%2520the%2520current%2520state-of-the-art%250Adetection%2520methods%2520such%2520as%2520DETR%2520and%2520Faster%2520R-CNN%252C%2520our%2520method%2520achieves%2520similar%2520or%250Abetter%2520accuracy%252C%2520while%2520requiring%2520a%2520significantly%2520smaller%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClusterTabNet%3A%20Supervised%20clustering%20method%20for%20table%20detection%20and%0A%20%20table%20structure%20recognition&entry.906535625=Marek%20Polewczyk%20and%20Marco%20Spinaci&entry.1292438233=%20%20We%20present%20a%20novel%20deep-learning-based%20method%20to%20cluster%20words%20in%20documents%0Awhich%20we%20apply%20to%20detect%20and%20recognize%20tables%20given%20the%20OCR%20output.%20We%0Ainterpret%20table%20structure%20bottom-up%20as%20a%20graph%20of%20relations%20between%20pairs%20of%0Awords%20%28belonging%20to%20the%20same%20row%2C%20column%2C%20header%2C%20as%20well%20as%20to%20the%20same%20table%29%0Aand%20use%20a%20transformer%20encoder%20model%20to%20predict%20its%20adjacency%20matrix.%20We%0Ademonstrate%20the%20performance%20of%20our%20method%20on%20the%20PubTables-1M%20dataset%20as%20well%0Aas%20PubTabNet%20and%20FinTabNet%20datasets.%20Compared%20to%20the%20current%20state-of-the-art%0Adetection%20methods%20such%20as%20DETR%20and%20Faster%20R-CNN%2C%20our%20method%20achieves%20similar%20or%0Abetter%20accuracy%2C%20while%20requiring%20a%20significantly%20smaller%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07502v2&entry.124074799=Read"},
{"title": "Synthetic Data Generation for Intersectional Fairness by Leveraging\n  Hierarchical Group Structure", "author": "Gaurav Maheshwari and Aur\u00e9lien Bellet and Pascal Denis and Mikaela Keller", "abstract": "  In this paper, we introduce a data augmentation approach specifically\ntailored to enhance intersectional fairness in classification tasks. Our method\ncapitalizes on the hierarchical structure inherent to intersectionality, by\nviewing groups as intersections of their parent categories. This perspective\nallows us to augment data for smaller groups by learning a transformation\nfunction that combines data from these parent groups. Our empirical analysis,\nconducted on four diverse datasets including both text and images, reveals that\nclassifiers trained with this data augmentation approach achieve superior\nintersectional fairness and are more robust to ``leveling down'' when compared\nto methods optimizing traditional group fairness metrics.\n", "link": "http://arxiv.org/abs/2405.14521v1", "date": "2024-05-23", "relevancy": 2.3878, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4861}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4768}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20Generation%20for%20Intersectional%20Fairness%20by%20Leveraging%0A%20%20Hierarchical%20Group%20Structure&body=Title%3A%20Synthetic%20Data%20Generation%20for%20Intersectional%20Fairness%20by%20Leveraging%0A%20%20Hierarchical%20Group%20Structure%0AAuthor%3A%20Gaurav%20Maheshwari%20and%20Aur%C3%A9lien%20Bellet%20and%20Pascal%20Denis%20and%20Mikaela%20Keller%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20data%20augmentation%20approach%20specifically%0Atailored%20to%20enhance%20intersectional%20fairness%20in%20classification%20tasks.%20Our%20method%0Acapitalizes%20on%20the%20hierarchical%20structure%20inherent%20to%20intersectionality%2C%20by%0Aviewing%20groups%20as%20intersections%20of%20their%20parent%20categories.%20This%20perspective%0Aallows%20us%20to%20augment%20data%20for%20smaller%20groups%20by%20learning%20a%20transformation%0Afunction%20that%20combines%20data%20from%20these%20parent%20groups.%20Our%20empirical%20analysis%2C%0Aconducted%20on%20four%20diverse%20datasets%20including%20both%20text%20and%20images%2C%20reveals%20that%0Aclassifiers%20trained%20with%20this%20data%20augmentation%20approach%20achieve%20superior%0Aintersectional%20fairness%20and%20are%20more%20robust%20to%20%60%60leveling%20down%27%27%20when%20compared%0Ato%20methods%20optimizing%20traditional%20group%20fairness%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520Generation%2520for%2520Intersectional%2520Fairness%2520by%2520Leveraging%250A%2520%2520Hierarchical%2520Group%2520Structure%26entry.906535625%3DGaurav%2520Maheshwari%2520and%2520Aur%25C3%25A9lien%2520Bellet%2520and%2520Pascal%2520Denis%2520and%2520Mikaela%2520Keller%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520data%2520augmentation%2520approach%2520specifically%250Atailored%2520to%2520enhance%2520intersectional%2520fairness%2520in%2520classification%2520tasks.%2520Our%2520method%250Acapitalizes%2520on%2520the%2520hierarchical%2520structure%2520inherent%2520to%2520intersectionality%252C%2520by%250Aviewing%2520groups%2520as%2520intersections%2520of%2520their%2520parent%2520categories.%2520This%2520perspective%250Aallows%2520us%2520to%2520augment%2520data%2520for%2520smaller%2520groups%2520by%2520learning%2520a%2520transformation%250Afunction%2520that%2520combines%2520data%2520from%2520these%2520parent%2520groups.%2520Our%2520empirical%2520analysis%252C%250Aconducted%2520on%2520four%2520diverse%2520datasets%2520including%2520both%2520text%2520and%2520images%252C%2520reveals%2520that%250Aclassifiers%2520trained%2520with%2520this%2520data%2520augmentation%2520approach%2520achieve%2520superior%250Aintersectional%2520fairness%2520and%2520are%2520more%2520robust%2520to%2520%2560%2560leveling%2520down%2527%2527%2520when%2520compared%250Ato%2520methods%2520optimizing%2520traditional%2520group%2520fairness%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20Generation%20for%20Intersectional%20Fairness%20by%20Leveraging%0A%20%20Hierarchical%20Group%20Structure&entry.906535625=Gaurav%20Maheshwari%20and%20Aur%C3%A9lien%20Bellet%20and%20Pascal%20Denis%20and%20Mikaela%20Keller&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20data%20augmentation%20approach%20specifically%0Atailored%20to%20enhance%20intersectional%20fairness%20in%20classification%20tasks.%20Our%20method%0Acapitalizes%20on%20the%20hierarchical%20structure%20inherent%20to%20intersectionality%2C%20by%0Aviewing%20groups%20as%20intersections%20of%20their%20parent%20categories.%20This%20perspective%0Aallows%20us%20to%20augment%20data%20for%20smaller%20groups%20by%20learning%20a%20transformation%0Afunction%20that%20combines%20data%20from%20these%20parent%20groups.%20Our%20empirical%20analysis%2C%0Aconducted%20on%20four%20diverse%20datasets%20including%20both%20text%20and%20images%2C%20reveals%20that%0Aclassifiers%20trained%20with%20this%20data%20augmentation%20approach%20achieve%20superior%0Aintersectional%20fairness%20and%20are%20more%20robust%20to%20%60%60leveling%20down%27%27%20when%20compared%0Ato%20methods%20optimizing%20traditional%20group%20fairness%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14521v1&entry.124074799=Read"},
{"title": "Bagging Improves Generalization Exponentially", "author": "Huaqian Jie and Donghao Ying and Henry Lam and Wotao Yin", "abstract": "  Bagging is a popular ensemble technique to improve the accuracy of machine\nlearning models. It hinges on the well-established rationale that, by\nrepeatedly retraining on resampled data, the aggregated model exhibits lower\nvariance and hence higher stability, especially for discontinuous base\nlearners. In this paper, we provide a new perspective on bagging: By suitably\naggregating the base learners at the parametrization instead of the output\nlevel, bagging improves generalization performances exponentially, a strength\nthat is significantly more powerful than variance reduction. More precisely, we\nshow that for general stochastic optimization problems that suffer from slowly\n(i.e., polynomially) decaying generalization errors, bagging can effectively\nreduce these errors to an exponential decay. Moreover, this power of bagging is\nagnostic to the solution schemes, including common empirical risk minimization,\ndistributionally robust optimization, and various regularizations. We\ndemonstrate how bagging can substantially improve generalization performances\nin a range of examples involving heavy-tailed data that suffer from\nintrinsically slow rates.\n", "link": "http://arxiv.org/abs/2405.14741v1", "date": "2024-05-23", "relevancy": 2.3819, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bagging%20Improves%20Generalization%20Exponentially&body=Title%3A%20Bagging%20Improves%20Generalization%20Exponentially%0AAuthor%3A%20Huaqian%20Jie%20and%20Donghao%20Ying%20and%20Henry%20Lam%20and%20Wotao%20Yin%0AAbstract%3A%20%20%20Bagging%20is%20a%20popular%20ensemble%20technique%20to%20improve%20the%20accuracy%20of%20machine%0Alearning%20models.%20It%20hinges%20on%20the%20well-established%20rationale%20that%2C%20by%0Arepeatedly%20retraining%20on%20resampled%20data%2C%20the%20aggregated%20model%20exhibits%20lower%0Avariance%20and%20hence%20higher%20stability%2C%20especially%20for%20discontinuous%20base%0Alearners.%20In%20this%20paper%2C%20we%20provide%20a%20new%20perspective%20on%20bagging%3A%20By%20suitably%0Aaggregating%20the%20base%20learners%20at%20the%20parametrization%20instead%20of%20the%20output%0Alevel%2C%20bagging%20improves%20generalization%20performances%20exponentially%2C%20a%20strength%0Athat%20is%20significantly%20more%20powerful%20than%20variance%20reduction.%20More%20precisely%2C%20we%0Ashow%20that%20for%20general%20stochastic%20optimization%20problems%20that%20suffer%20from%20slowly%0A%28i.e.%2C%20polynomially%29%20decaying%20generalization%20errors%2C%20bagging%20can%20effectively%0Areduce%20these%20errors%20to%20an%20exponential%20decay.%20Moreover%2C%20this%20power%20of%20bagging%20is%0Aagnostic%20to%20the%20solution%20schemes%2C%20including%20common%20empirical%20risk%20minimization%2C%0Adistributionally%20robust%20optimization%2C%20and%20various%20regularizations.%20We%0Ademonstrate%20how%20bagging%20can%20substantially%20improve%20generalization%20performances%0Ain%20a%20range%20of%20examples%20involving%20heavy-tailed%20data%20that%20suffer%20from%0Aintrinsically%20slow%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBagging%2520Improves%2520Generalization%2520Exponentially%26entry.906535625%3DHuaqian%2520Jie%2520and%2520Donghao%2520Ying%2520and%2520Henry%2520Lam%2520and%2520Wotao%2520Yin%26entry.1292438233%3D%2520%2520Bagging%2520is%2520a%2520popular%2520ensemble%2520technique%2520to%2520improve%2520the%2520accuracy%2520of%2520machine%250Alearning%2520models.%2520It%2520hinges%2520on%2520the%2520well-established%2520rationale%2520that%252C%2520by%250Arepeatedly%2520retraining%2520on%2520resampled%2520data%252C%2520the%2520aggregated%2520model%2520exhibits%2520lower%250Avariance%2520and%2520hence%2520higher%2520stability%252C%2520especially%2520for%2520discontinuous%2520base%250Alearners.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520new%2520perspective%2520on%2520bagging%253A%2520By%2520suitably%250Aaggregating%2520the%2520base%2520learners%2520at%2520the%2520parametrization%2520instead%2520of%2520the%2520output%250Alevel%252C%2520bagging%2520improves%2520generalization%2520performances%2520exponentially%252C%2520a%2520strength%250Athat%2520is%2520significantly%2520more%2520powerful%2520than%2520variance%2520reduction.%2520More%2520precisely%252C%2520we%250Ashow%2520that%2520for%2520general%2520stochastic%2520optimization%2520problems%2520that%2520suffer%2520from%2520slowly%250A%2528i.e.%252C%2520polynomially%2529%2520decaying%2520generalization%2520errors%252C%2520bagging%2520can%2520effectively%250Areduce%2520these%2520errors%2520to%2520an%2520exponential%2520decay.%2520Moreover%252C%2520this%2520power%2520of%2520bagging%2520is%250Aagnostic%2520to%2520the%2520solution%2520schemes%252C%2520including%2520common%2520empirical%2520risk%2520minimization%252C%250Adistributionally%2520robust%2520optimization%252C%2520and%2520various%2520regularizations.%2520We%250Ademonstrate%2520how%2520bagging%2520can%2520substantially%2520improve%2520generalization%2520performances%250Ain%2520a%2520range%2520of%2520examples%2520involving%2520heavy-tailed%2520data%2520that%2520suffer%2520from%250Aintrinsically%2520slow%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bagging%20Improves%20Generalization%20Exponentially&entry.906535625=Huaqian%20Jie%20and%20Donghao%20Ying%20and%20Henry%20Lam%20and%20Wotao%20Yin&entry.1292438233=%20%20Bagging%20is%20a%20popular%20ensemble%20technique%20to%20improve%20the%20accuracy%20of%20machine%0Alearning%20models.%20It%20hinges%20on%20the%20well-established%20rationale%20that%2C%20by%0Arepeatedly%20retraining%20on%20resampled%20data%2C%20the%20aggregated%20model%20exhibits%20lower%0Avariance%20and%20hence%20higher%20stability%2C%20especially%20for%20discontinuous%20base%0Alearners.%20In%20this%20paper%2C%20we%20provide%20a%20new%20perspective%20on%20bagging%3A%20By%20suitably%0Aaggregating%20the%20base%20learners%20at%20the%20parametrization%20instead%20of%20the%20output%0Alevel%2C%20bagging%20improves%20generalization%20performances%20exponentially%2C%20a%20strength%0Athat%20is%20significantly%20more%20powerful%20than%20variance%20reduction.%20More%20precisely%2C%20we%0Ashow%20that%20for%20general%20stochastic%20optimization%20problems%20that%20suffer%20from%20slowly%0A%28i.e.%2C%20polynomially%29%20decaying%20generalization%20errors%2C%20bagging%20can%20effectively%0Areduce%20these%20errors%20to%20an%20exponential%20decay.%20Moreover%2C%20this%20power%20of%20bagging%20is%0Aagnostic%20to%20the%20solution%20schemes%2C%20including%20common%20empirical%20risk%20minimization%2C%0Adistributionally%20robust%20optimization%2C%20and%20various%20regularizations.%20We%0Ademonstrate%20how%20bagging%20can%20substantially%20improve%20generalization%20performances%0Ain%20a%20range%20of%20examples%20involving%20heavy-tailed%20data%20that%20suffer%20from%0Aintrinsically%20slow%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14741v1&entry.124074799=Read"},
{"title": "PuzzleAvatar: Assembling 3D Avatars from Personal Albums", "author": "Yuliang Xiu and Yufei Ye and Zhen Liu and Dimitrios Tzionas and Michael J. Black", "abstract": "  Generating personalized 3D avatars is crucial for AR/VR. However, recent\ntext-to-3D methods that generate avatars for celebrities or fictional\ncharacters, struggle with everyday people. Methods for faithful reconstruction\ntypically require full-body images in controlled settings. What if a user could\njust upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get\na faithful avatar in return? The challenge is that such casual photo\ncollections contain diverse poses, challenging viewpoints, cropped views, and\nocclusion (albeit with a consistent outfit, accessories and hairstyle). We\naddress this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model\nthat generates a faithful 3D avatar (in a canonical pose) from a personal OOTD\nalbum, while bypassing the challenging estimation of body and camera pose. To\nthis end, we fine-tune a foundational vision-language model (VLM) on such\nphotos, encoding the appearance, identity, garments, hairstyles, and\naccessories of a person into (separate) learned tokens and instilling these\ncues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\"\nfrom which we assemble a faithful, personalized 3D avatar. Importantly, we can\ncustomize avatars by simply inter-changing tokens. As a benchmark for this new\ntask, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total\nof nearly 1K OOTD configurations, in challenging partial photos with paired\nground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high\nreconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique\nscalability to album photos, and strong robustness. Our model and data will be\npublic.\n", "link": "http://arxiv.org/abs/2405.14869v1", "date": "2024-05-23", "relevancy": 2.3653, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5992}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5972}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PuzzleAvatar%3A%20Assembling%203D%20Avatars%20from%20Personal%20Albums&body=Title%3A%20PuzzleAvatar%3A%20Assembling%203D%20Avatars%20from%20Personal%20Albums%0AAuthor%3A%20Yuliang%20Xiu%20and%20Yufei%20Ye%20and%20Zhen%20Liu%20and%20Dimitrios%20Tzionas%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Generating%20personalized%203D%20avatars%20is%20crucial%20for%20AR/VR.%20However%2C%20recent%0Atext-to-3D%20methods%20that%20generate%20avatars%20for%20celebrities%20or%20fictional%0Acharacters%2C%20struggle%20with%20everyday%20people.%20Methods%20for%20faithful%20reconstruction%0Atypically%20require%20full-body%20images%20in%20controlled%20settings.%20What%20if%20a%20user%20could%0Ajust%20upload%20their%20personal%20%22OOTD%22%20%28Outfit%20Of%20The%20Day%29%20photo%20collection%20and%20get%0Aa%20faithful%20avatar%20in%20return%3F%20The%20challenge%20is%20that%20such%20casual%20photo%0Acollections%20contain%20diverse%20poses%2C%20challenging%20viewpoints%2C%20cropped%20views%2C%20and%0Aocclusion%20%28albeit%20with%20a%20consistent%20outfit%2C%20accessories%20and%20hairstyle%29.%20We%0Aaddress%20this%20novel%20%22Album2Human%22%20task%20by%20developing%20PuzzleAvatar%2C%20a%20novel%20model%0Athat%20generates%20a%20faithful%203D%20avatar%20%28in%20a%20canonical%20pose%29%20from%20a%20personal%20OOTD%0Aalbum%2C%20while%20bypassing%20the%20challenging%20estimation%20of%20body%20and%20camera%20pose.%20To%0Athis%20end%2C%20we%20fine-tune%20a%20foundational%20vision-language%20model%20%28VLM%29%20on%20such%0Aphotos%2C%20encoding%20the%20appearance%2C%20identity%2C%20garments%2C%20hairstyles%2C%20and%0Aaccessories%20of%20a%20person%20into%20%28separate%29%20learned%20tokens%20and%20instilling%20these%0Acues%20into%20the%20VLM.%20In%20effect%2C%20we%20exploit%20the%20learned%20tokens%20as%20%22puzzle%20pieces%22%0Afrom%20which%20we%20assemble%20a%20faithful%2C%20personalized%203D%20avatar.%20Importantly%2C%20we%20can%0Acustomize%20avatars%20by%20simply%20inter-changing%20tokens.%20As%20a%20benchmark%20for%20this%20new%0Atask%2C%20we%20collect%20a%20new%20dataset%2C%20called%20PuzzleIOI%2C%20with%2041%20subjects%20in%20a%20total%0Aof%20nearly%201K%20OOTD%20configurations%2C%20in%20challenging%20partial%20photos%20with%20paired%0Aground-truth%203D%20bodies.%20Evaluation%20shows%20that%20PuzzleAvatar%20not%20only%20has%20high%0Areconstruction%20accuracy%2C%20outperforming%20TeCH%20and%20MVDreamBooth%2C%20but%20also%20a%20unique%0Ascalability%20to%20album%20photos%2C%20and%20strong%20robustness.%20Our%20model%20and%20data%20will%20be%0Apublic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPuzzleAvatar%253A%2520Assembling%25203D%2520Avatars%2520from%2520Personal%2520Albums%26entry.906535625%3DYuliang%2520Xiu%2520and%2520Yufei%2520Ye%2520and%2520Zhen%2520Liu%2520and%2520Dimitrios%2520Tzionas%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Generating%2520personalized%25203D%2520avatars%2520is%2520crucial%2520for%2520AR/VR.%2520However%252C%2520recent%250Atext-to-3D%2520methods%2520that%2520generate%2520avatars%2520for%2520celebrities%2520or%2520fictional%250Acharacters%252C%2520struggle%2520with%2520everyday%2520people.%2520Methods%2520for%2520faithful%2520reconstruction%250Atypically%2520require%2520full-body%2520images%2520in%2520controlled%2520settings.%2520What%2520if%2520a%2520user%2520could%250Ajust%2520upload%2520their%2520personal%2520%2522OOTD%2522%2520%2528Outfit%2520Of%2520The%2520Day%2529%2520photo%2520collection%2520and%2520get%250Aa%2520faithful%2520avatar%2520in%2520return%253F%2520The%2520challenge%2520is%2520that%2520such%2520casual%2520photo%250Acollections%2520contain%2520diverse%2520poses%252C%2520challenging%2520viewpoints%252C%2520cropped%2520views%252C%2520and%250Aocclusion%2520%2528albeit%2520with%2520a%2520consistent%2520outfit%252C%2520accessories%2520and%2520hairstyle%2529.%2520We%250Aaddress%2520this%2520novel%2520%2522Album2Human%2522%2520task%2520by%2520developing%2520PuzzleAvatar%252C%2520a%2520novel%2520model%250Athat%2520generates%2520a%2520faithful%25203D%2520avatar%2520%2528in%2520a%2520canonical%2520pose%2529%2520from%2520a%2520personal%2520OOTD%250Aalbum%252C%2520while%2520bypassing%2520the%2520challenging%2520estimation%2520of%2520body%2520and%2520camera%2520pose.%2520To%250Athis%2520end%252C%2520we%2520fine-tune%2520a%2520foundational%2520vision-language%2520model%2520%2528VLM%2529%2520on%2520such%250Aphotos%252C%2520encoding%2520the%2520appearance%252C%2520identity%252C%2520garments%252C%2520hairstyles%252C%2520and%250Aaccessories%2520of%2520a%2520person%2520into%2520%2528separate%2529%2520learned%2520tokens%2520and%2520instilling%2520these%250Acues%2520into%2520the%2520VLM.%2520In%2520effect%252C%2520we%2520exploit%2520the%2520learned%2520tokens%2520as%2520%2522puzzle%2520pieces%2522%250Afrom%2520which%2520we%2520assemble%2520a%2520faithful%252C%2520personalized%25203D%2520avatar.%2520Importantly%252C%2520we%2520can%250Acustomize%2520avatars%2520by%2520simply%2520inter-changing%2520tokens.%2520As%2520a%2520benchmark%2520for%2520this%2520new%250Atask%252C%2520we%2520collect%2520a%2520new%2520dataset%252C%2520called%2520PuzzleIOI%252C%2520with%252041%2520subjects%2520in%2520a%2520total%250Aof%2520nearly%25201K%2520OOTD%2520configurations%252C%2520in%2520challenging%2520partial%2520photos%2520with%2520paired%250Aground-truth%25203D%2520bodies.%2520Evaluation%2520shows%2520that%2520PuzzleAvatar%2520not%2520only%2520has%2520high%250Areconstruction%2520accuracy%252C%2520outperforming%2520TeCH%2520and%2520MVDreamBooth%252C%2520but%2520also%2520a%2520unique%250Ascalability%2520to%2520album%2520photos%252C%2520and%2520strong%2520robustness.%2520Our%2520model%2520and%2520data%2520will%2520be%250Apublic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PuzzleAvatar%3A%20Assembling%203D%20Avatars%20from%20Personal%20Albums&entry.906535625=Yuliang%20Xiu%20and%20Yufei%20Ye%20and%20Zhen%20Liu%20and%20Dimitrios%20Tzionas%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Generating%20personalized%203D%20avatars%20is%20crucial%20for%20AR/VR.%20However%2C%20recent%0Atext-to-3D%20methods%20that%20generate%20avatars%20for%20celebrities%20or%20fictional%0Acharacters%2C%20struggle%20with%20everyday%20people.%20Methods%20for%20faithful%20reconstruction%0Atypically%20require%20full-body%20images%20in%20controlled%20settings.%20What%20if%20a%20user%20could%0Ajust%20upload%20their%20personal%20%22OOTD%22%20%28Outfit%20Of%20The%20Day%29%20photo%20collection%20and%20get%0Aa%20faithful%20avatar%20in%20return%3F%20The%20challenge%20is%20that%20such%20casual%20photo%0Acollections%20contain%20diverse%20poses%2C%20challenging%20viewpoints%2C%20cropped%20views%2C%20and%0Aocclusion%20%28albeit%20with%20a%20consistent%20outfit%2C%20accessories%20and%20hairstyle%29.%20We%0Aaddress%20this%20novel%20%22Album2Human%22%20task%20by%20developing%20PuzzleAvatar%2C%20a%20novel%20model%0Athat%20generates%20a%20faithful%203D%20avatar%20%28in%20a%20canonical%20pose%29%20from%20a%20personal%20OOTD%0Aalbum%2C%20while%20bypassing%20the%20challenging%20estimation%20of%20body%20and%20camera%20pose.%20To%0Athis%20end%2C%20we%20fine-tune%20a%20foundational%20vision-language%20model%20%28VLM%29%20on%20such%0Aphotos%2C%20encoding%20the%20appearance%2C%20identity%2C%20garments%2C%20hairstyles%2C%20and%0Aaccessories%20of%20a%20person%20into%20%28separate%29%20learned%20tokens%20and%20instilling%20these%0Acues%20into%20the%20VLM.%20In%20effect%2C%20we%20exploit%20the%20learned%20tokens%20as%20%22puzzle%20pieces%22%0Afrom%20which%20we%20assemble%20a%20faithful%2C%20personalized%203D%20avatar.%20Importantly%2C%20we%20can%0Acustomize%20avatars%20by%20simply%20inter-changing%20tokens.%20As%20a%20benchmark%20for%20this%20new%0Atask%2C%20we%20collect%20a%20new%20dataset%2C%20called%20PuzzleIOI%2C%20with%2041%20subjects%20in%20a%20total%0Aof%20nearly%201K%20OOTD%20configurations%2C%20in%20challenging%20partial%20photos%20with%20paired%0Aground-truth%203D%20bodies.%20Evaluation%20shows%20that%20PuzzleAvatar%20not%20only%20has%20high%0Areconstruction%20accuracy%2C%20outperforming%20TeCH%20and%20MVDreamBooth%2C%20but%20also%20a%20unique%0Ascalability%20to%20album%20photos%2C%20and%20strong%20robustness.%20Our%20model%20and%20data%20will%20be%0Apublic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14869v1&entry.124074799=Read"},
{"title": "Multistable Shape from Shading Emerges from Patch Diffusion", "author": "Xinran Nicole Han and Todd Zickler and Ko Nishino", "abstract": "  Models for monocular shape reconstruction of surfaces with diffuse reflection\n-- shape from shading -- ought to produce distributions of outputs, because\nthere are fundamental mathematical ambiguities of both continuous (e.g.,\nbas-relief) and discrete (e.g., convex/concave) varieties which are also\nexperienced by humans. Yet, the outputs of current models are limited to point\nestimates or tight distributions around single modes, which prevent them from\ncapturing these effects. We introduce a model that reconstructs a multimodal\ndistribution of shapes from a single shading image, which aligns with the human\nexperience of multistable perception. We train a small denoising diffusion\nprocess to generate surface normal fields from $16\\times 16$ patches of\nsynthetic images of everyday 3D objects. We deploy this model patch-wise at\nmultiple scales, with guidance from inter-patch shape consistency constraints.\nDespite its relatively small parameter count and predominantly bottom-up\nstructure, we show that multistable shape explanations emerge from this model\nfor ''ambiguous'' test images that humans experience as being multistable. At\nthe same time, the model produces veridical shape estimates for object-like\nimages that include distinctive occluding contours and appear less ambiguous.\nThis may inspire new architectures for stochastic 3D shape perception that are\nmore efficient and better aligned with human experience.\n", "link": "http://arxiv.org/abs/2405.14530v1", "date": "2024-05-23", "relevancy": 2.3353, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5933}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multistable%20Shape%20from%20Shading%20Emerges%20from%20Patch%20Diffusion&body=Title%3A%20Multistable%20Shape%20from%20Shading%20Emerges%20from%20Patch%20Diffusion%0AAuthor%3A%20Xinran%20Nicole%20Han%20and%20Todd%20Zickler%20and%20Ko%20Nishino%0AAbstract%3A%20%20%20Models%20for%20monocular%20shape%20reconstruction%20of%20surfaces%20with%20diffuse%20reflection%0A--%20shape%20from%20shading%20--%20ought%20to%20produce%20distributions%20of%20outputs%2C%20because%0Athere%20are%20fundamental%20mathematical%20ambiguities%20of%20both%20continuous%20%28e.g.%2C%0Abas-relief%29%20and%20discrete%20%28e.g.%2C%20convex/concave%29%20varieties%20which%20are%20also%0Aexperienced%20by%20humans.%20Yet%2C%20the%20outputs%20of%20current%20models%20are%20limited%20to%20point%0Aestimates%20or%20tight%20distributions%20around%20single%20modes%2C%20which%20prevent%20them%20from%0Acapturing%20these%20effects.%20We%20introduce%20a%20model%20that%20reconstructs%20a%20multimodal%0Adistribution%20of%20shapes%20from%20a%20single%20shading%20image%2C%20which%20aligns%20with%20the%20human%0Aexperience%20of%20multistable%20perception.%20We%20train%20a%20small%20denoising%20diffusion%0Aprocess%20to%20generate%20surface%20normal%20fields%20from%20%2416%5Ctimes%2016%24%20patches%20of%0Asynthetic%20images%20of%20everyday%203D%20objects.%20We%20deploy%20this%20model%20patch-wise%20at%0Amultiple%20scales%2C%20with%20guidance%20from%20inter-patch%20shape%20consistency%20constraints.%0ADespite%20its%20relatively%20small%20parameter%20count%20and%20predominantly%20bottom-up%0Astructure%2C%20we%20show%20that%20multistable%20shape%20explanations%20emerge%20from%20this%20model%0Afor%20%27%27ambiguous%27%27%20test%20images%20that%20humans%20experience%20as%20being%20multistable.%20At%0Athe%20same%20time%2C%20the%20model%20produces%20veridical%20shape%20estimates%20for%20object-like%0Aimages%20that%20include%20distinctive%20occluding%20contours%20and%20appear%20less%20ambiguous.%0AThis%20may%20inspire%20new%20architectures%20for%20stochastic%203D%20shape%20perception%20that%20are%0Amore%20efficient%20and%20better%20aligned%20with%20human%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultistable%2520Shape%2520from%2520Shading%2520Emerges%2520from%2520Patch%2520Diffusion%26entry.906535625%3DXinran%2520Nicole%2520Han%2520and%2520Todd%2520Zickler%2520and%2520Ko%2520Nishino%26entry.1292438233%3D%2520%2520Models%2520for%2520monocular%2520shape%2520reconstruction%2520of%2520surfaces%2520with%2520diffuse%2520reflection%250A--%2520shape%2520from%2520shading%2520--%2520ought%2520to%2520produce%2520distributions%2520of%2520outputs%252C%2520because%250Athere%2520are%2520fundamental%2520mathematical%2520ambiguities%2520of%2520both%2520continuous%2520%2528e.g.%252C%250Abas-relief%2529%2520and%2520discrete%2520%2528e.g.%252C%2520convex/concave%2529%2520varieties%2520which%2520are%2520also%250Aexperienced%2520by%2520humans.%2520Yet%252C%2520the%2520outputs%2520of%2520current%2520models%2520are%2520limited%2520to%2520point%250Aestimates%2520or%2520tight%2520distributions%2520around%2520single%2520modes%252C%2520which%2520prevent%2520them%2520from%250Acapturing%2520these%2520effects.%2520We%2520introduce%2520a%2520model%2520that%2520reconstructs%2520a%2520multimodal%250Adistribution%2520of%2520shapes%2520from%2520a%2520single%2520shading%2520image%252C%2520which%2520aligns%2520with%2520the%2520human%250Aexperience%2520of%2520multistable%2520perception.%2520We%2520train%2520a%2520small%2520denoising%2520diffusion%250Aprocess%2520to%2520generate%2520surface%2520normal%2520fields%2520from%2520%252416%255Ctimes%252016%2524%2520patches%2520of%250Asynthetic%2520images%2520of%2520everyday%25203D%2520objects.%2520We%2520deploy%2520this%2520model%2520patch-wise%2520at%250Amultiple%2520scales%252C%2520with%2520guidance%2520from%2520inter-patch%2520shape%2520consistency%2520constraints.%250ADespite%2520its%2520relatively%2520small%2520parameter%2520count%2520and%2520predominantly%2520bottom-up%250Astructure%252C%2520we%2520show%2520that%2520multistable%2520shape%2520explanations%2520emerge%2520from%2520this%2520model%250Afor%2520%2527%2527ambiguous%2527%2527%2520test%2520images%2520that%2520humans%2520experience%2520as%2520being%2520multistable.%2520At%250Athe%2520same%2520time%252C%2520the%2520model%2520produces%2520veridical%2520shape%2520estimates%2520for%2520object-like%250Aimages%2520that%2520include%2520distinctive%2520occluding%2520contours%2520and%2520appear%2520less%2520ambiguous.%250AThis%2520may%2520inspire%2520new%2520architectures%2520for%2520stochastic%25203D%2520shape%2520perception%2520that%2520are%250Amore%2520efficient%2520and%2520better%2520aligned%2520with%2520human%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multistable%20Shape%20from%20Shading%20Emerges%20from%20Patch%20Diffusion&entry.906535625=Xinran%20Nicole%20Han%20and%20Todd%20Zickler%20and%20Ko%20Nishino&entry.1292438233=%20%20Models%20for%20monocular%20shape%20reconstruction%20of%20surfaces%20with%20diffuse%20reflection%0A--%20shape%20from%20shading%20--%20ought%20to%20produce%20distributions%20of%20outputs%2C%20because%0Athere%20are%20fundamental%20mathematical%20ambiguities%20of%20both%20continuous%20%28e.g.%2C%0Abas-relief%29%20and%20discrete%20%28e.g.%2C%20convex/concave%29%20varieties%20which%20are%20also%0Aexperienced%20by%20humans.%20Yet%2C%20the%20outputs%20of%20current%20models%20are%20limited%20to%20point%0Aestimates%20or%20tight%20distributions%20around%20single%20modes%2C%20which%20prevent%20them%20from%0Acapturing%20these%20effects.%20We%20introduce%20a%20model%20that%20reconstructs%20a%20multimodal%0Adistribution%20of%20shapes%20from%20a%20single%20shading%20image%2C%20which%20aligns%20with%20the%20human%0Aexperience%20of%20multistable%20perception.%20We%20train%20a%20small%20denoising%20diffusion%0Aprocess%20to%20generate%20surface%20normal%20fields%20from%20%2416%5Ctimes%2016%24%20patches%20of%0Asynthetic%20images%20of%20everyday%203D%20objects.%20We%20deploy%20this%20model%20patch-wise%20at%0Amultiple%20scales%2C%20with%20guidance%20from%20inter-patch%20shape%20consistency%20constraints.%0ADespite%20its%20relatively%20small%20parameter%20count%20and%20predominantly%20bottom-up%0Astructure%2C%20we%20show%20that%20multistable%20shape%20explanations%20emerge%20from%20this%20model%0Afor%20%27%27ambiguous%27%27%20test%20images%20that%20humans%20experience%20as%20being%20multistable.%20At%0Athe%20same%20time%2C%20the%20model%20produces%20veridical%20shape%20estimates%20for%20object-like%0Aimages%20that%20include%20distinctive%20occluding%20contours%20and%20appear%20less%20ambiguous.%0AThis%20may%20inspire%20new%20architectures%20for%20stochastic%203D%20shape%20perception%20that%20are%0Amore%20efficient%20and%20better%20aligned%20with%20human%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14530v1&entry.124074799=Read"},
{"title": "Multiscale Vision Transformers meet Bipartite Matching for efficient\n  single-stage Action Localization", "author": "Ioanna Ntinou and Enrique Sanchez and Georgios Tzimiropoulos", "abstract": "  Action Localization is a challenging problem that combines detection and\nrecognition tasks, which are often addressed separately. State-of-the-art\nmethods rely on off-the-shelf bounding box detections pre-computed at high\nresolution, and propose transformer models that focus on the classification\ntask alone. Such two-stage solutions are prohibitive for real-time deployment.\nOn the other hand, single-stage methods target both tasks by devoting part of\nthe network (generally the backbone) to sharing the majority of the workload,\ncompromising performance for speed. These methods build on adding a DETR head\nwith learnable queries that after cross- and self-attention can be sent to\ncorresponding MLPs for detecting a person's bounding box and action. However,\nDETR-like architectures are challenging to train and can incur in big\ncomplexity.\n  In this paper, we observe that \\textbf{a straight bipartite matching loss can\nbe applied to the output tokens of a vision transformer}. This results in a\nbackbone + MLP architecture that can do both tasks without the need of an extra\nencoder-decoder head and learnable queries. We show that a single MViTv2-S\narchitecture trained with bipartite matching to perform both tasks surpasses\nthe same MViTv2-S when trained with RoI align on pre-computed bounding boxes.\nWith a careful design of token pooling and the proposed training pipeline, our\nBipartite-Matching Vision Transformer model, \\textbf{BMViT}, achieves +3 mAP on\nAVA2.2. w.r.t. the two-stage MViTv2-S counterpart. Code is available at\n\\href{https://github.com/IoannaNti/BMViT}{https://github.com/IoannaNti/BMViT}\n", "link": "http://arxiv.org/abs/2312.17686v2", "date": "2024-05-23", "relevancy": 2.3321, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5934}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiscale%20Vision%20Transformers%20meet%20Bipartite%20Matching%20for%20efficient%0A%20%20single-stage%20Action%20Localization&body=Title%3A%20Multiscale%20Vision%20Transformers%20meet%20Bipartite%20Matching%20for%20efficient%0A%20%20single-stage%20Action%20Localization%0AAuthor%3A%20Ioanna%20Ntinou%20and%20Enrique%20Sanchez%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20Action%20Localization%20is%20a%20challenging%20problem%20that%20combines%20detection%20and%0Arecognition%20tasks%2C%20which%20are%20often%20addressed%20separately.%20State-of-the-art%0Amethods%20rely%20on%20off-the-shelf%20bounding%20box%20detections%20pre-computed%20at%20high%0Aresolution%2C%20and%20propose%20transformer%20models%20that%20focus%20on%20the%20classification%0Atask%20alone.%20Such%20two-stage%20solutions%20are%20prohibitive%20for%20real-time%20deployment.%0AOn%20the%20other%20hand%2C%20single-stage%20methods%20target%20both%20tasks%20by%20devoting%20part%20of%0Athe%20network%20%28generally%20the%20backbone%29%20to%20sharing%20the%20majority%20of%20the%20workload%2C%0Acompromising%20performance%20for%20speed.%20These%20methods%20build%20on%20adding%20a%20DETR%20head%0Awith%20learnable%20queries%20that%20after%20cross-%20and%20self-attention%20can%20be%20sent%20to%0Acorresponding%20MLPs%20for%20detecting%20a%20person%27s%20bounding%20box%20and%20action.%20However%2C%0ADETR-like%20architectures%20are%20challenging%20to%20train%20and%20can%20incur%20in%20big%0Acomplexity.%0A%20%20In%20this%20paper%2C%20we%20observe%20that%20%5Ctextbf%7Ba%20straight%20bipartite%20matching%20loss%20can%0Abe%20applied%20to%20the%20output%20tokens%20of%20a%20vision%20transformer%7D.%20This%20results%20in%20a%0Abackbone%20%2B%20MLP%20architecture%20that%20can%20do%20both%20tasks%20without%20the%20need%20of%20an%20extra%0Aencoder-decoder%20head%20and%20learnable%20queries.%20We%20show%20that%20a%20single%20MViTv2-S%0Aarchitecture%20trained%20with%20bipartite%20matching%20to%20perform%20both%20tasks%20surpasses%0Athe%20same%20MViTv2-S%20when%20trained%20with%20RoI%20align%20on%20pre-computed%20bounding%20boxes.%0AWith%20a%20careful%20design%20of%20token%20pooling%20and%20the%20proposed%20training%20pipeline%2C%20our%0ABipartite-Matching%20Vision%20Transformer%20model%2C%20%5Ctextbf%7BBMViT%7D%2C%20achieves%20%2B3%20mAP%20on%0AAVA2.2.%20w.r.t.%20the%20two-stage%20MViTv2-S%20counterpart.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/IoannaNti/BMViT%7D%7Bhttps%3A//github.com/IoannaNti/BMViT%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiscale%2520Vision%2520Transformers%2520meet%2520Bipartite%2520Matching%2520for%2520efficient%250A%2520%2520single-stage%2520Action%2520Localization%26entry.906535625%3DIoanna%2520Ntinou%2520and%2520Enrique%2520Sanchez%2520and%2520Georgios%2520Tzimiropoulos%26entry.1292438233%3D%2520%2520Action%2520Localization%2520is%2520a%2520challenging%2520problem%2520that%2520combines%2520detection%2520and%250Arecognition%2520tasks%252C%2520which%2520are%2520often%2520addressed%2520separately.%2520State-of-the-art%250Amethods%2520rely%2520on%2520off-the-shelf%2520bounding%2520box%2520detections%2520pre-computed%2520at%2520high%250Aresolution%252C%2520and%2520propose%2520transformer%2520models%2520that%2520focus%2520on%2520the%2520classification%250Atask%2520alone.%2520Such%2520two-stage%2520solutions%2520are%2520prohibitive%2520for%2520real-time%2520deployment.%250AOn%2520the%2520other%2520hand%252C%2520single-stage%2520methods%2520target%2520both%2520tasks%2520by%2520devoting%2520part%2520of%250Athe%2520network%2520%2528generally%2520the%2520backbone%2529%2520to%2520sharing%2520the%2520majority%2520of%2520the%2520workload%252C%250Acompromising%2520performance%2520for%2520speed.%2520These%2520methods%2520build%2520on%2520adding%2520a%2520DETR%2520head%250Awith%2520learnable%2520queries%2520that%2520after%2520cross-%2520and%2520self-attention%2520can%2520be%2520sent%2520to%250Acorresponding%2520MLPs%2520for%2520detecting%2520a%2520person%2527s%2520bounding%2520box%2520and%2520action.%2520However%252C%250ADETR-like%2520architectures%2520are%2520challenging%2520to%2520train%2520and%2520can%2520incur%2520in%2520big%250Acomplexity.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520observe%2520that%2520%255Ctextbf%257Ba%2520straight%2520bipartite%2520matching%2520loss%2520can%250Abe%2520applied%2520to%2520the%2520output%2520tokens%2520of%2520a%2520vision%2520transformer%257D.%2520This%2520results%2520in%2520a%250Abackbone%2520%252B%2520MLP%2520architecture%2520that%2520can%2520do%2520both%2520tasks%2520without%2520the%2520need%2520of%2520an%2520extra%250Aencoder-decoder%2520head%2520and%2520learnable%2520queries.%2520We%2520show%2520that%2520a%2520single%2520MViTv2-S%250Aarchitecture%2520trained%2520with%2520bipartite%2520matching%2520to%2520perform%2520both%2520tasks%2520surpasses%250Athe%2520same%2520MViTv2-S%2520when%2520trained%2520with%2520RoI%2520align%2520on%2520pre-computed%2520bounding%2520boxes.%250AWith%2520a%2520careful%2520design%2520of%2520token%2520pooling%2520and%2520the%2520proposed%2520training%2520pipeline%252C%2520our%250ABipartite-Matching%2520Vision%2520Transformer%2520model%252C%2520%255Ctextbf%257BBMViT%257D%252C%2520achieves%2520%252B3%2520mAP%2520on%250AAVA2.2.%2520w.r.t.%2520the%2520two-stage%2520MViTv2-S%2520counterpart.%2520Code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/IoannaNti/BMViT%257D%257Bhttps%253A//github.com/IoannaNti/BMViT%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiscale%20Vision%20Transformers%20meet%20Bipartite%20Matching%20for%20efficient%0A%20%20single-stage%20Action%20Localization&entry.906535625=Ioanna%20Ntinou%20and%20Enrique%20Sanchez%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20Action%20Localization%20is%20a%20challenging%20problem%20that%20combines%20detection%20and%0Arecognition%20tasks%2C%20which%20are%20often%20addressed%20separately.%20State-of-the-art%0Amethods%20rely%20on%20off-the-shelf%20bounding%20box%20detections%20pre-computed%20at%20high%0Aresolution%2C%20and%20propose%20transformer%20models%20that%20focus%20on%20the%20classification%0Atask%20alone.%20Such%20two-stage%20solutions%20are%20prohibitive%20for%20real-time%20deployment.%0AOn%20the%20other%20hand%2C%20single-stage%20methods%20target%20both%20tasks%20by%20devoting%20part%20of%0Athe%20network%20%28generally%20the%20backbone%29%20to%20sharing%20the%20majority%20of%20the%20workload%2C%0Acompromising%20performance%20for%20speed.%20These%20methods%20build%20on%20adding%20a%20DETR%20head%0Awith%20learnable%20queries%20that%20after%20cross-%20and%20self-attention%20can%20be%20sent%20to%0Acorresponding%20MLPs%20for%20detecting%20a%20person%27s%20bounding%20box%20and%20action.%20However%2C%0ADETR-like%20architectures%20are%20challenging%20to%20train%20and%20can%20incur%20in%20big%0Acomplexity.%0A%20%20In%20this%20paper%2C%20we%20observe%20that%20%5Ctextbf%7Ba%20straight%20bipartite%20matching%20loss%20can%0Abe%20applied%20to%20the%20output%20tokens%20of%20a%20vision%20transformer%7D.%20This%20results%20in%20a%0Abackbone%20%2B%20MLP%20architecture%20that%20can%20do%20both%20tasks%20without%20the%20need%20of%20an%20extra%0Aencoder-decoder%20head%20and%20learnable%20queries.%20We%20show%20that%20a%20single%20MViTv2-S%0Aarchitecture%20trained%20with%20bipartite%20matching%20to%20perform%20both%20tasks%20surpasses%0Athe%20same%20MViTv2-S%20when%20trained%20with%20RoI%20align%20on%20pre-computed%20bounding%20boxes.%0AWith%20a%20careful%20design%20of%20token%20pooling%20and%20the%20proposed%20training%20pipeline%2C%20our%0ABipartite-Matching%20Vision%20Transformer%20model%2C%20%5Ctextbf%7BBMViT%7D%2C%20achieves%20%2B3%20mAP%20on%0AAVA2.2.%20w.r.t.%20the%20two-stage%20MViTv2-S%20counterpart.%20Code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/IoannaNti/BMViT%7D%7Bhttps%3A//github.com/IoannaNti/BMViT%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17686v2&entry.124074799=Read"},
{"title": "CLAP4CLIP: Continual Learning with Probabilistic Finetuning for\n  Vision-Language Models", "author": "Saurav Jha and Dong Gong and Lina Yao", "abstract": "  Continual learning (CL) aims to help deep neural networks to learn new\nknowledge while retaining what has been learned. Recently, pre-trained\nvision-language models such as CLIP, with powerful generalizability, have been\ngaining traction as practical CL candidates. However, the domain mismatch\nbetween the pre-training and the downstream CL tasks calls for finetuning of\nthe CLIP on the latter. The deterministic nature of the existing finetuning\nmethods makes them overlook the many possible interactions across the\nmodalities and deems them unsafe for high-risk CL tasks requiring reliable\nuncertainty estimation. To address these, our work proposes Continual LeArning\nwith Probabilistic finetuning (CLAP). CLAP develops probabilistic modeling over\ntask-specific modules with visual-guided text features, providing more\ncalibrated finetuning in CL. It further alleviates forgetting by exploiting the\nrich pre-trained knowledge of CLIP for weight initialization and distribution\nregularization of task-specific modules. Cooperating with the diverse range of\nexisting prompting methods, CLAP can surpass the predominant deterministic\nfinetuning approaches for CL with CLIP. We conclude with out-of-the-box\napplications of superior uncertainty estimation abilities of CLAP for novel\ndata detection and exemplar selection within CL setups. Our code is available\nat \\url{https://github.com/srvCodes/clap4clip}.\n", "link": "http://arxiv.org/abs/2403.19137v2", "date": "2024-05-23", "relevancy": 2.3255, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6036}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAP4CLIP%3A%20Continual%20Learning%20with%20Probabilistic%20Finetuning%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20CLAP4CLIP%3A%20Continual%20Learning%20with%20Probabilistic%20Finetuning%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Saurav%20Jha%20and%20Dong%20Gong%20and%20Lina%20Yao%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20aims%20to%20help%20deep%20neural%20networks%20to%20learn%20new%0Aknowledge%20while%20retaining%20what%20has%20been%20learned.%20Recently%2C%20pre-trained%0Avision-language%20models%20such%20as%20CLIP%2C%20with%20powerful%20generalizability%2C%20have%20been%0Againing%20traction%20as%20practical%20CL%20candidates.%20However%2C%20the%20domain%20mismatch%0Abetween%20the%20pre-training%20and%20the%20downstream%20CL%20tasks%20calls%20for%20finetuning%20of%0Athe%20CLIP%20on%20the%20latter.%20The%20deterministic%20nature%20of%20the%20existing%20finetuning%0Amethods%20makes%20them%20overlook%20the%20many%20possible%20interactions%20across%20the%0Amodalities%20and%20deems%20them%20unsafe%20for%20high-risk%20CL%20tasks%20requiring%20reliable%0Auncertainty%20estimation.%20To%20address%20these%2C%20our%20work%20proposes%20Continual%20LeArning%0Awith%20Probabilistic%20finetuning%20%28CLAP%29.%20CLAP%20develops%20probabilistic%20modeling%20over%0Atask-specific%20modules%20with%20visual-guided%20text%20features%2C%20providing%20more%0Acalibrated%20finetuning%20in%20CL.%20It%20further%20alleviates%20forgetting%20by%20exploiting%20the%0Arich%20pre-trained%20knowledge%20of%20CLIP%20for%20weight%20initialization%20and%20distribution%0Aregularization%20of%20task-specific%20modules.%20Cooperating%20with%20the%20diverse%20range%20of%0Aexisting%20prompting%20methods%2C%20CLAP%20can%20surpass%20the%20predominant%20deterministic%0Afinetuning%20approaches%20for%20CL%20with%20CLIP.%20We%20conclude%20with%20out-of-the-box%0Aapplications%20of%20superior%20uncertainty%20estimation%20abilities%20of%20CLAP%20for%20novel%0Adata%20detection%20and%20exemplar%20selection%20within%20CL%20setups.%20Our%20code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/srvCodes/clap4clip%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAP4CLIP%253A%2520Continual%2520Learning%2520with%2520Probabilistic%2520Finetuning%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DSaurav%2520Jha%2520and%2520Dong%2520Gong%2520and%2520Lina%2520Yao%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520aims%2520to%2520help%2520deep%2520neural%2520networks%2520to%2520learn%2520new%250Aknowledge%2520while%2520retaining%2520what%2520has%2520been%2520learned.%2520Recently%252C%2520pre-trained%250Avision-language%2520models%2520such%2520as%2520CLIP%252C%2520with%2520powerful%2520generalizability%252C%2520have%2520been%250Againing%2520traction%2520as%2520practical%2520CL%2520candidates.%2520However%252C%2520the%2520domain%2520mismatch%250Abetween%2520the%2520pre-training%2520and%2520the%2520downstream%2520CL%2520tasks%2520calls%2520for%2520finetuning%2520of%250Athe%2520CLIP%2520on%2520the%2520latter.%2520The%2520deterministic%2520nature%2520of%2520the%2520existing%2520finetuning%250Amethods%2520makes%2520them%2520overlook%2520the%2520many%2520possible%2520interactions%2520across%2520the%250Amodalities%2520and%2520deems%2520them%2520unsafe%2520for%2520high-risk%2520CL%2520tasks%2520requiring%2520reliable%250Auncertainty%2520estimation.%2520To%2520address%2520these%252C%2520our%2520work%2520proposes%2520Continual%2520LeArning%250Awith%2520Probabilistic%2520finetuning%2520%2528CLAP%2529.%2520CLAP%2520develops%2520probabilistic%2520modeling%2520over%250Atask-specific%2520modules%2520with%2520visual-guided%2520text%2520features%252C%2520providing%2520more%250Acalibrated%2520finetuning%2520in%2520CL.%2520It%2520further%2520alleviates%2520forgetting%2520by%2520exploiting%2520the%250Arich%2520pre-trained%2520knowledge%2520of%2520CLIP%2520for%2520weight%2520initialization%2520and%2520distribution%250Aregularization%2520of%2520task-specific%2520modules.%2520Cooperating%2520with%2520the%2520diverse%2520range%2520of%250Aexisting%2520prompting%2520methods%252C%2520CLAP%2520can%2520surpass%2520the%2520predominant%2520deterministic%250Afinetuning%2520approaches%2520for%2520CL%2520with%2520CLIP.%2520We%2520conclude%2520with%2520out-of-the-box%250Aapplications%2520of%2520superior%2520uncertainty%2520estimation%2520abilities%2520of%2520CLAP%2520for%2520novel%250Adata%2520detection%2520and%2520exemplar%2520selection%2520within%2520CL%2520setups.%2520Our%2520code%2520is%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/srvCodes/clap4clip%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAP4CLIP%3A%20Continual%20Learning%20with%20Probabilistic%20Finetuning%20for%0A%20%20Vision-Language%20Models&entry.906535625=Saurav%20Jha%20and%20Dong%20Gong%20and%20Lina%20Yao&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20aims%20to%20help%20deep%20neural%20networks%20to%20learn%20new%0Aknowledge%20while%20retaining%20what%20has%20been%20learned.%20Recently%2C%20pre-trained%0Avision-language%20models%20such%20as%20CLIP%2C%20with%20powerful%20generalizability%2C%20have%20been%0Againing%20traction%20as%20practical%20CL%20candidates.%20However%2C%20the%20domain%20mismatch%0Abetween%20the%20pre-training%20and%20the%20downstream%20CL%20tasks%20calls%20for%20finetuning%20of%0Athe%20CLIP%20on%20the%20latter.%20The%20deterministic%20nature%20of%20the%20existing%20finetuning%0Amethods%20makes%20them%20overlook%20the%20many%20possible%20interactions%20across%20the%0Amodalities%20and%20deems%20them%20unsafe%20for%20high-risk%20CL%20tasks%20requiring%20reliable%0Auncertainty%20estimation.%20To%20address%20these%2C%20our%20work%20proposes%20Continual%20LeArning%0Awith%20Probabilistic%20finetuning%20%28CLAP%29.%20CLAP%20develops%20probabilistic%20modeling%20over%0Atask-specific%20modules%20with%20visual-guided%20text%20features%2C%20providing%20more%0Acalibrated%20finetuning%20in%20CL.%20It%20further%20alleviates%20forgetting%20by%20exploiting%20the%0Arich%20pre-trained%20knowledge%20of%20CLIP%20for%20weight%20initialization%20and%20distribution%0Aregularization%20of%20task-specific%20modules.%20Cooperating%20with%20the%20diverse%20range%20of%0Aexisting%20prompting%20methods%2C%20CLAP%20can%20surpass%20the%20predominant%20deterministic%0Afinetuning%20approaches%20for%20CL%20with%20CLIP.%20We%20conclude%20with%20out-of-the-box%0Aapplications%20of%20superior%20uncertainty%20estimation%20abilities%20of%20CLAP%20for%20novel%0Adata%20detection%20and%20exemplar%20selection%20within%20CL%20setups.%20Our%20code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/srvCodes/clap4clip%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19137v2&entry.124074799=Read"},
{"title": "A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization\n  Inversion for Zero-Shot Video Editing", "author": "Maomao Li and Yu Li and Tianyu Yang and Yunfei Liu and Dongxu Yue and Zhihui Lin and Dong Xu", "abstract": "  This paper presents a video inversion approach for zero-shot video editing,\nwhich models the input video with low-rank representation during the inversion\nprocess. The existing video editing methods usually apply the typical 2D DDIM\ninversion or naive spatial-temporal DDIM inversion before editing, which\nleverages time-varying representation for each frame to derive noisy latent.\nUnlike most existing approaches, we propose a Spatial-Temporal\nExpectation-Maximization (STEM) inversion, which formulates the dense video\nfeature under an expectation-maximization manner and iteratively estimates a\nmore compact basis set to represent the whole video. Each frame applies the\nfixed and global representation for inversion, which is more friendly for\ntemporal consistency during reconstruction and editing. Extensive qualitative\nand quantitative experiments demonstrate that our STEM inversion can achieve\nconsistent improvement on two state-of-the-art video editing methods. Project\npage: https://stem-inv.github.io/page/.\n", "link": "http://arxiv.org/abs/2312.05856v2", "date": "2024-05-23", "relevancy": 2.3213, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6249}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5798}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Video%20is%20Worth%20256%20Bases%3A%20Spatial-Temporal%20Expectation-Maximization%0A%20%20Inversion%20for%20Zero-Shot%20Video%20Editing&body=Title%3A%20A%20Video%20is%20Worth%20256%20Bases%3A%20Spatial-Temporal%20Expectation-Maximization%0A%20%20Inversion%20for%20Zero-Shot%20Video%20Editing%0AAuthor%3A%20Maomao%20Li%20and%20Yu%20Li%20and%20Tianyu%20Yang%20and%20Yunfei%20Liu%20and%20Dongxu%20Yue%20and%20Zhihui%20Lin%20and%20Dong%20Xu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20video%20inversion%20approach%20for%20zero-shot%20video%20editing%2C%0Awhich%20models%20the%20input%20video%20with%20low-rank%20representation%20during%20the%20inversion%0Aprocess.%20The%20existing%20video%20editing%20methods%20usually%20apply%20the%20typical%202D%20DDIM%0Ainversion%20or%20naive%20spatial-temporal%20DDIM%20inversion%20before%20editing%2C%20which%0Aleverages%20time-varying%20representation%20for%20each%20frame%20to%20derive%20noisy%20latent.%0AUnlike%20most%20existing%20approaches%2C%20we%20propose%20a%20Spatial-Temporal%0AExpectation-Maximization%20%28STEM%29%20inversion%2C%20which%20formulates%20the%20dense%20video%0Afeature%20under%20an%20expectation-maximization%20manner%20and%20iteratively%20estimates%20a%0Amore%20compact%20basis%20set%20to%20represent%20the%20whole%20video.%20Each%20frame%20applies%20the%0Afixed%20and%20global%20representation%20for%20inversion%2C%20which%20is%20more%20friendly%20for%0Atemporal%20consistency%20during%20reconstruction%20and%20editing.%20Extensive%20qualitative%0Aand%20quantitative%20experiments%20demonstrate%20that%20our%20STEM%20inversion%20can%20achieve%0Aconsistent%20improvement%20on%20two%20state-of-the-art%20video%20editing%20methods.%20Project%0Apage%3A%20https%3A//stem-inv.github.io/page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Video%2520is%2520Worth%2520256%2520Bases%253A%2520Spatial-Temporal%2520Expectation-Maximization%250A%2520%2520Inversion%2520for%2520Zero-Shot%2520Video%2520Editing%26entry.906535625%3DMaomao%2520Li%2520and%2520Yu%2520Li%2520and%2520Tianyu%2520Yang%2520and%2520Yunfei%2520Liu%2520and%2520Dongxu%2520Yue%2520and%2520Zhihui%2520Lin%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520video%2520inversion%2520approach%2520for%2520zero-shot%2520video%2520editing%252C%250Awhich%2520models%2520the%2520input%2520video%2520with%2520low-rank%2520representation%2520during%2520the%2520inversion%250Aprocess.%2520The%2520existing%2520video%2520editing%2520methods%2520usually%2520apply%2520the%2520typical%25202D%2520DDIM%250Ainversion%2520or%2520naive%2520spatial-temporal%2520DDIM%2520inversion%2520before%2520editing%252C%2520which%250Aleverages%2520time-varying%2520representation%2520for%2520each%2520frame%2520to%2520derive%2520noisy%2520latent.%250AUnlike%2520most%2520existing%2520approaches%252C%2520we%2520propose%2520a%2520Spatial-Temporal%250AExpectation-Maximization%2520%2528STEM%2529%2520inversion%252C%2520which%2520formulates%2520the%2520dense%2520video%250Afeature%2520under%2520an%2520expectation-maximization%2520manner%2520and%2520iteratively%2520estimates%2520a%250Amore%2520compact%2520basis%2520set%2520to%2520represent%2520the%2520whole%2520video.%2520Each%2520frame%2520applies%2520the%250Afixed%2520and%2520global%2520representation%2520for%2520inversion%252C%2520which%2520is%2520more%2520friendly%2520for%250Atemporal%2520consistency%2520during%2520reconstruction%2520and%2520editing.%2520Extensive%2520qualitative%250Aand%2520quantitative%2520experiments%2520demonstrate%2520that%2520our%2520STEM%2520inversion%2520can%2520achieve%250Aconsistent%2520improvement%2520on%2520two%2520state-of-the-art%2520video%2520editing%2520methods.%2520Project%250Apage%253A%2520https%253A//stem-inv.github.io/page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Video%20is%20Worth%20256%20Bases%3A%20Spatial-Temporal%20Expectation-Maximization%0A%20%20Inversion%20for%20Zero-Shot%20Video%20Editing&entry.906535625=Maomao%20Li%20and%20Yu%20Li%20and%20Tianyu%20Yang%20and%20Yunfei%20Liu%20and%20Dongxu%20Yue%20and%20Zhihui%20Lin%20and%20Dong%20Xu&entry.1292438233=%20%20This%20paper%20presents%20a%20video%20inversion%20approach%20for%20zero-shot%20video%20editing%2C%0Awhich%20models%20the%20input%20video%20with%20low-rank%20representation%20during%20the%20inversion%0Aprocess.%20The%20existing%20video%20editing%20methods%20usually%20apply%20the%20typical%202D%20DDIM%0Ainversion%20or%20naive%20spatial-temporal%20DDIM%20inversion%20before%20editing%2C%20which%0Aleverages%20time-varying%20representation%20for%20each%20frame%20to%20derive%20noisy%20latent.%0AUnlike%20most%20existing%20approaches%2C%20we%20propose%20a%20Spatial-Temporal%0AExpectation-Maximization%20%28STEM%29%20inversion%2C%20which%20formulates%20the%20dense%20video%0Afeature%20under%20an%20expectation-maximization%20manner%20and%20iteratively%20estimates%20a%0Amore%20compact%20basis%20set%20to%20represent%20the%20whole%20video.%20Each%20frame%20applies%20the%0Afixed%20and%20global%20representation%20for%20inversion%2C%20which%20is%20more%20friendly%20for%0Atemporal%20consistency%20during%20reconstruction%20and%20editing.%20Extensive%20qualitative%0Aand%20quantitative%20experiments%20demonstrate%20that%20our%20STEM%20inversion%20can%20achieve%0Aconsistent%20improvement%20on%20two%20state-of-the-art%20video%20editing%20methods.%20Project%0Apage%3A%20https%3A//stem-inv.github.io/page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05856v2&entry.124074799=Read"},
{"title": "Seeing is not always believing: The Space of Harmless Perturbations", "author": "Lu Chen and Shaofeng Li and Benhao Huang and Fan Yang and Zheng Li and Jie Li and Yuan Luo", "abstract": "  Existing works have extensively studied adversarial examples, which are\nminimal perturbations that can mislead the output of deep neural networks\n(DNNs) while remaining imperceptible to humans. However, in this work, we\nreveal the existence of a harmless perturbation space, in which perturbations\ndrawn from this space, regardless of their magnitudes, leave the network output\nunchanged when applied to inputs. Essentially, the harmless perturbation space\nemerges from the usage of non-injective functions (linear or non-linear layers)\nwithin DNNs, enabling multiple distinct inputs to be mapped to the same output.\nFor linear layers with input dimensions exceeding output dimensions, any linear\ncombination of the orthogonal bases of the nullspace of the parameter\nconsistently yields no change in their output. For non-linear layers, the\nharmless perturbation space may expand, depending on the properties of the\nlayers and input samples. Inspired by this property of DNNs, we solve for a\nfamily of general perturbation spaces that are redundant for the DNN's\ndecision, and can be used to hide sensitive data and serve as a means of model\nidentification. Our work highlights the distinctive robustness of DNNs (i.e.,\nconsistency under large magnitude perturbations) in contrast to adversarial\nexamples (vulnerability for small imperceptible noises).\n", "link": "http://arxiv.org/abs/2402.02095v2", "date": "2024-05-23", "relevancy": 2.3151, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4703}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4645}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20is%20not%20always%20believing%3A%20The%20Space%20of%20Harmless%20Perturbations&body=Title%3A%20Seeing%20is%20not%20always%20believing%3A%20The%20Space%20of%20Harmless%20Perturbations%0AAuthor%3A%20Lu%20Chen%20and%20Shaofeng%20Li%20and%20Benhao%20Huang%20and%20Fan%20Yang%20and%20Zheng%20Li%20and%20Jie%20Li%20and%20Yuan%20Luo%0AAbstract%3A%20%20%20Existing%20works%20have%20extensively%20studied%20adversarial%20examples%2C%20which%20are%0Aminimal%20perturbations%20that%20can%20mislead%20the%20output%20of%20deep%20neural%20networks%0A%28DNNs%29%20while%20remaining%20imperceptible%20to%20humans.%20However%2C%20in%20this%20work%2C%20we%0Areveal%20the%20existence%20of%20a%20harmless%20perturbation%20space%2C%20in%20which%20perturbations%0Adrawn%20from%20this%20space%2C%20regardless%20of%20their%20magnitudes%2C%20leave%20the%20network%20output%0Aunchanged%20when%20applied%20to%20inputs.%20Essentially%2C%20the%20harmless%20perturbation%20space%0Aemerges%20from%20the%20usage%20of%20non-injective%20functions%20%28linear%20or%20non-linear%20layers%29%0Awithin%20DNNs%2C%20enabling%20multiple%20distinct%20inputs%20to%20be%20mapped%20to%20the%20same%20output.%0AFor%20linear%20layers%20with%20input%20dimensions%20exceeding%20output%20dimensions%2C%20any%20linear%0Acombination%20of%20the%20orthogonal%20bases%20of%20the%20nullspace%20of%20the%20parameter%0Aconsistently%20yields%20no%20change%20in%20their%20output.%20For%20non-linear%20layers%2C%20the%0Aharmless%20perturbation%20space%20may%20expand%2C%20depending%20on%20the%20properties%20of%20the%0Alayers%20and%20input%20samples.%20Inspired%20by%20this%20property%20of%20DNNs%2C%20we%20solve%20for%20a%0Afamily%20of%20general%20perturbation%20spaces%20that%20are%20redundant%20for%20the%20DNN%27s%0Adecision%2C%20and%20can%20be%20used%20to%20hide%20sensitive%20data%20and%20serve%20as%20a%20means%20of%20model%0Aidentification.%20Our%20work%20highlights%20the%20distinctive%20robustness%20of%20DNNs%20%28i.e.%2C%0Aconsistency%20under%20large%20magnitude%20perturbations%29%20in%20contrast%20to%20adversarial%0Aexamples%20%28vulnerability%20for%20small%20imperceptible%20noises%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520is%2520not%2520always%2520believing%253A%2520The%2520Space%2520of%2520Harmless%2520Perturbations%26entry.906535625%3DLu%2520Chen%2520and%2520Shaofeng%2520Li%2520and%2520Benhao%2520Huang%2520and%2520Fan%2520Yang%2520and%2520Zheng%2520Li%2520and%2520Jie%2520Li%2520and%2520Yuan%2520Luo%26entry.1292438233%3D%2520%2520Existing%2520works%2520have%2520extensively%2520studied%2520adversarial%2520examples%252C%2520which%2520are%250Aminimal%2520perturbations%2520that%2520can%2520mislead%2520the%2520output%2520of%2520deep%2520neural%2520networks%250A%2528DNNs%2529%2520while%2520remaining%2520imperceptible%2520to%2520humans.%2520However%252C%2520in%2520this%2520work%252C%2520we%250Areveal%2520the%2520existence%2520of%2520a%2520harmless%2520perturbation%2520space%252C%2520in%2520which%2520perturbations%250Adrawn%2520from%2520this%2520space%252C%2520regardless%2520of%2520their%2520magnitudes%252C%2520leave%2520the%2520network%2520output%250Aunchanged%2520when%2520applied%2520to%2520inputs.%2520Essentially%252C%2520the%2520harmless%2520perturbation%2520space%250Aemerges%2520from%2520the%2520usage%2520of%2520non-injective%2520functions%2520%2528linear%2520or%2520non-linear%2520layers%2529%250Awithin%2520DNNs%252C%2520enabling%2520multiple%2520distinct%2520inputs%2520to%2520be%2520mapped%2520to%2520the%2520same%2520output.%250AFor%2520linear%2520layers%2520with%2520input%2520dimensions%2520exceeding%2520output%2520dimensions%252C%2520any%2520linear%250Acombination%2520of%2520the%2520orthogonal%2520bases%2520of%2520the%2520nullspace%2520of%2520the%2520parameter%250Aconsistently%2520yields%2520no%2520change%2520in%2520their%2520output.%2520For%2520non-linear%2520layers%252C%2520the%250Aharmless%2520perturbation%2520space%2520may%2520expand%252C%2520depending%2520on%2520the%2520properties%2520of%2520the%250Alayers%2520and%2520input%2520samples.%2520Inspired%2520by%2520this%2520property%2520of%2520DNNs%252C%2520we%2520solve%2520for%2520a%250Afamily%2520of%2520general%2520perturbation%2520spaces%2520that%2520are%2520redundant%2520for%2520the%2520DNN%2527s%250Adecision%252C%2520and%2520can%2520be%2520used%2520to%2520hide%2520sensitive%2520data%2520and%2520serve%2520as%2520a%2520means%2520of%2520model%250Aidentification.%2520Our%2520work%2520highlights%2520the%2520distinctive%2520robustness%2520of%2520DNNs%2520%2528i.e.%252C%250Aconsistency%2520under%2520large%2520magnitude%2520perturbations%2529%2520in%2520contrast%2520to%2520adversarial%250Aexamples%2520%2528vulnerability%2520for%2520small%2520imperceptible%2520noises%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20is%20not%20always%20believing%3A%20The%20Space%20of%20Harmless%20Perturbations&entry.906535625=Lu%20Chen%20and%20Shaofeng%20Li%20and%20Benhao%20Huang%20and%20Fan%20Yang%20and%20Zheng%20Li%20and%20Jie%20Li%20and%20Yuan%20Luo&entry.1292438233=%20%20Existing%20works%20have%20extensively%20studied%20adversarial%20examples%2C%20which%20are%0Aminimal%20perturbations%20that%20can%20mislead%20the%20output%20of%20deep%20neural%20networks%0A%28DNNs%29%20while%20remaining%20imperceptible%20to%20humans.%20However%2C%20in%20this%20work%2C%20we%0Areveal%20the%20existence%20of%20a%20harmless%20perturbation%20space%2C%20in%20which%20perturbations%0Adrawn%20from%20this%20space%2C%20regardless%20of%20their%20magnitudes%2C%20leave%20the%20network%20output%0Aunchanged%20when%20applied%20to%20inputs.%20Essentially%2C%20the%20harmless%20perturbation%20space%0Aemerges%20from%20the%20usage%20of%20non-injective%20functions%20%28linear%20or%20non-linear%20layers%29%0Awithin%20DNNs%2C%20enabling%20multiple%20distinct%20inputs%20to%20be%20mapped%20to%20the%20same%20output.%0AFor%20linear%20layers%20with%20input%20dimensions%20exceeding%20output%20dimensions%2C%20any%20linear%0Acombination%20of%20the%20orthogonal%20bases%20of%20the%20nullspace%20of%20the%20parameter%0Aconsistently%20yields%20no%20change%20in%20their%20output.%20For%20non-linear%20layers%2C%20the%0Aharmless%20perturbation%20space%20may%20expand%2C%20depending%20on%20the%20properties%20of%20the%0Alayers%20and%20input%20samples.%20Inspired%20by%20this%20property%20of%20DNNs%2C%20we%20solve%20for%20a%0Afamily%20of%20general%20perturbation%20spaces%20that%20are%20redundant%20for%20the%20DNN%27s%0Adecision%2C%20and%20can%20be%20used%20to%20hide%20sensitive%20data%20and%20serve%20as%20a%20means%20of%20model%0Aidentification.%20Our%20work%20highlights%20the%20distinctive%20robustness%20of%20DNNs%20%28i.e.%2C%0Aconsistency%20under%20large%20magnitude%20perturbations%29%20in%20contrast%20to%20adversarial%0Aexamples%20%28vulnerability%20for%20small%20imperceptible%20noises%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02095v2&entry.124074799=Read"},
{"title": "Overcoming the Challenges of Batch Normalization in Federated Learning", "author": "Rachid Guerraoui and Rafael Pinot and Geovani Rizk and John Stephan and Fran\u00e7ois Taiani", "abstract": "  Batch normalization has proven to be a very beneficial mechanism to\naccelerate the training and improve the accuracy of deep neural networks in\ncentralized environments. Yet, the scheme faces significant challenges in\nfederated learning, especially under high data heterogeneity. Essentially, the\nmain challenges arise from external covariate shifts and inconsistent\nstatistics across clients. We introduce in this paper Federated BatchNorm\n(FBN), a novel scheme that restores the benefits of batch normalization in\nfederated learning. Essentially, FBN ensures that the batch normalization\nduring training is consistent with what would be achieved in a centralized\nexecution, hence preserving the distribution of the data, and providing running\nstatistics that accurately approximate the global statistics. FBN thereby\nreduces the external covariate shift and matches the evaluation performance of\nthe centralized setting. We also show that, with a slight increase in\ncomplexity, we can robustify FBN to mitigate erroneous statistics and\npotentially adversarial attacks.\n", "link": "http://arxiv.org/abs/2405.14670v1", "date": "2024-05-23", "relevancy": 2.3095, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20the%20Challenges%20of%20Batch%20Normalization%20in%20Federated%20Learning&body=Title%3A%20Overcoming%20the%20Challenges%20of%20Batch%20Normalization%20in%20Federated%20Learning%0AAuthor%3A%20Rachid%20Guerraoui%20and%20Rafael%20Pinot%20and%20Geovani%20Rizk%20and%20John%20Stephan%20and%20Fran%C3%A7ois%20Taiani%0AAbstract%3A%20%20%20Batch%20normalization%20has%20proven%20to%20be%20a%20very%20beneficial%20mechanism%20to%0Aaccelerate%20the%20training%20and%20improve%20the%20accuracy%20of%20deep%20neural%20networks%20in%0Acentralized%20environments.%20Yet%2C%20the%20scheme%20faces%20significant%20challenges%20in%0Afederated%20learning%2C%20especially%20under%20high%20data%20heterogeneity.%20Essentially%2C%20the%0Amain%20challenges%20arise%20from%20external%20covariate%20shifts%20and%20inconsistent%0Astatistics%20across%20clients.%20We%20introduce%20in%20this%20paper%20Federated%20BatchNorm%0A%28FBN%29%2C%20a%20novel%20scheme%20that%20restores%20the%20benefits%20of%20batch%20normalization%20in%0Afederated%20learning.%20Essentially%2C%20FBN%20ensures%20that%20the%20batch%20normalization%0Aduring%20training%20is%20consistent%20with%20what%20would%20be%20achieved%20in%20a%20centralized%0Aexecution%2C%20hence%20preserving%20the%20distribution%20of%20the%20data%2C%20and%20providing%20running%0Astatistics%20that%20accurately%20approximate%20the%20global%20statistics.%20FBN%20thereby%0Areduces%20the%20external%20covariate%20shift%20and%20matches%20the%20evaluation%20performance%20of%0Athe%20centralized%20setting.%20We%20also%20show%20that%2C%20with%20a%20slight%20increase%20in%0Acomplexity%2C%20we%20can%20robustify%20FBN%20to%20mitigate%20erroneous%20statistics%20and%0Apotentially%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520the%2520Challenges%2520of%2520Batch%2520Normalization%2520in%2520Federated%2520Learning%26entry.906535625%3DRachid%2520Guerraoui%2520and%2520Rafael%2520Pinot%2520and%2520Geovani%2520Rizk%2520and%2520John%2520Stephan%2520and%2520Fran%25C3%25A7ois%2520Taiani%26entry.1292438233%3D%2520%2520Batch%2520normalization%2520has%2520proven%2520to%2520be%2520a%2520very%2520beneficial%2520mechanism%2520to%250Aaccelerate%2520the%2520training%2520and%2520improve%2520the%2520accuracy%2520of%2520deep%2520neural%2520networks%2520in%250Acentralized%2520environments.%2520Yet%252C%2520the%2520scheme%2520faces%2520significant%2520challenges%2520in%250Afederated%2520learning%252C%2520especially%2520under%2520high%2520data%2520heterogeneity.%2520Essentially%252C%2520the%250Amain%2520challenges%2520arise%2520from%2520external%2520covariate%2520shifts%2520and%2520inconsistent%250Astatistics%2520across%2520clients.%2520We%2520introduce%2520in%2520this%2520paper%2520Federated%2520BatchNorm%250A%2528FBN%2529%252C%2520a%2520novel%2520scheme%2520that%2520restores%2520the%2520benefits%2520of%2520batch%2520normalization%2520in%250Afederated%2520learning.%2520Essentially%252C%2520FBN%2520ensures%2520that%2520the%2520batch%2520normalization%250Aduring%2520training%2520is%2520consistent%2520with%2520what%2520would%2520be%2520achieved%2520in%2520a%2520centralized%250Aexecution%252C%2520hence%2520preserving%2520the%2520distribution%2520of%2520the%2520data%252C%2520and%2520providing%2520running%250Astatistics%2520that%2520accurately%2520approximate%2520the%2520global%2520statistics.%2520FBN%2520thereby%250Areduces%2520the%2520external%2520covariate%2520shift%2520and%2520matches%2520the%2520evaluation%2520performance%2520of%250Athe%2520centralized%2520setting.%2520We%2520also%2520show%2520that%252C%2520with%2520a%2520slight%2520increase%2520in%250Acomplexity%252C%2520we%2520can%2520robustify%2520FBN%2520to%2520mitigate%2520erroneous%2520statistics%2520and%250Apotentially%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20the%20Challenges%20of%20Batch%20Normalization%20in%20Federated%20Learning&entry.906535625=Rachid%20Guerraoui%20and%20Rafael%20Pinot%20and%20Geovani%20Rizk%20and%20John%20Stephan%20and%20Fran%C3%A7ois%20Taiani&entry.1292438233=%20%20Batch%20normalization%20has%20proven%20to%20be%20a%20very%20beneficial%20mechanism%20to%0Aaccelerate%20the%20training%20and%20improve%20the%20accuracy%20of%20deep%20neural%20networks%20in%0Acentralized%20environments.%20Yet%2C%20the%20scheme%20faces%20significant%20challenges%20in%0Afederated%20learning%2C%20especially%20under%20high%20data%20heterogeneity.%20Essentially%2C%20the%0Amain%20challenges%20arise%20from%20external%20covariate%20shifts%20and%20inconsistent%0Astatistics%20across%20clients.%20We%20introduce%20in%20this%20paper%20Federated%20BatchNorm%0A%28FBN%29%2C%20a%20novel%20scheme%20that%20restores%20the%20benefits%20of%20batch%20normalization%20in%0Afederated%20learning.%20Essentially%2C%20FBN%20ensures%20that%20the%20batch%20normalization%0Aduring%20training%20is%20consistent%20with%20what%20would%20be%20achieved%20in%20a%20centralized%0Aexecution%2C%20hence%20preserving%20the%20distribution%20of%20the%20data%2C%20and%20providing%20running%0Astatistics%20that%20accurately%20approximate%20the%20global%20statistics.%20FBN%20thereby%0Areduces%20the%20external%20covariate%20shift%20and%20matches%20the%20evaluation%20performance%20of%0Athe%20centralized%20setting.%20We%20also%20show%20that%2C%20with%20a%20slight%20increase%20in%0Acomplexity%2C%20we%20can%20robustify%20FBN%20to%20mitigate%20erroneous%20statistics%20and%0Apotentially%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14670v1&entry.124074799=Read"},
{"title": "Masked Image Modelling for retinal OCT understanding", "author": "Theodoros Pissas and Pablo M\u00e1rquez-Neila and Sebastian Wolf and Martin Zinkernagel and Raphael Sznitman", "abstract": "  This work explores the effectiveness of masked image modelling for learning\nrepresentations of retinal OCT images. To this end, we leverage Masked\nAutoencoders (MAE), a simple and scalable method for self-supervised learning,\nto obtain a powerful and general representation for OCT images by training on\n700K OCT images from 41K patients collected under real world clinical settings.\nWe also provide the first extensive evaluation for a model of OCT on a\nchallenging battery of 6 downstream tasks. Our model achieves strong\nperformance when fully finetuned but can also serve as a versatile frozen\nfeature extractor for many tasks using lightweight adapters. Furthermore, we\npropose an extension of the MAE pretraining to fuse OCT with an auxiliary\nmodality, namely, IR fundus images and learn a joint model for both. We\ndemonstrate our approach improves performance on a multimodal downstream\napplication. Our experiments utilize most publicly available OCT datasets, thus\nenabling future comparisons. Our code and model weights are publicly available\nhttps://github.com/TheoPis/MIM_OCT.\n", "link": "http://arxiv.org/abs/2405.14788v1", "date": "2024-05-23", "relevancy": 2.284, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5889}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5628}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Image%20Modelling%20for%20retinal%20OCT%20understanding&body=Title%3A%20Masked%20Image%20Modelling%20for%20retinal%20OCT%20understanding%0AAuthor%3A%20Theodoros%20Pissas%20and%20Pablo%20M%C3%A1rquez-Neila%20and%20Sebastian%20Wolf%20and%20Martin%20Zinkernagel%20and%20Raphael%20Sznitman%0AAbstract%3A%20%20%20This%20work%20explores%20the%20effectiveness%20of%20masked%20image%20modelling%20for%20learning%0Arepresentations%20of%20retinal%20OCT%20images.%20To%20this%20end%2C%20we%20leverage%20Masked%0AAutoencoders%20%28MAE%29%2C%20a%20simple%20and%20scalable%20method%20for%20self-supervised%20learning%2C%0Ato%20obtain%20a%20powerful%20and%20general%20representation%20for%20OCT%20images%20by%20training%20on%0A700K%20OCT%20images%20from%2041K%20patients%20collected%20under%20real%20world%20clinical%20settings.%0AWe%20also%20provide%20the%20first%20extensive%20evaluation%20for%20a%20model%20of%20OCT%20on%20a%0Achallenging%20battery%20of%206%20downstream%20tasks.%20Our%20model%20achieves%20strong%0Aperformance%20when%20fully%20finetuned%20but%20can%20also%20serve%20as%20a%20versatile%20frozen%0Afeature%20extractor%20for%20many%20tasks%20using%20lightweight%20adapters.%20Furthermore%2C%20we%0Apropose%20an%20extension%20of%20the%20MAE%20pretraining%20to%20fuse%20OCT%20with%20an%20auxiliary%0Amodality%2C%20namely%2C%20IR%20fundus%20images%20and%20learn%20a%20joint%20model%20for%20both.%20We%0Ademonstrate%20our%20approach%20improves%20performance%20on%20a%20multimodal%20downstream%0Aapplication.%20Our%20experiments%20utilize%20most%20publicly%20available%20OCT%20datasets%2C%20thus%0Aenabling%20future%20comparisons.%20Our%20code%20and%20model%20weights%20are%20publicly%20available%0Ahttps%3A//github.com/TheoPis/MIM_OCT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Image%2520Modelling%2520for%2520retinal%2520OCT%2520understanding%26entry.906535625%3DTheodoros%2520Pissas%2520and%2520Pablo%2520M%25C3%25A1rquez-Neila%2520and%2520Sebastian%2520Wolf%2520and%2520Martin%2520Zinkernagel%2520and%2520Raphael%2520Sznitman%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520effectiveness%2520of%2520masked%2520image%2520modelling%2520for%2520learning%250Arepresentations%2520of%2520retinal%2520OCT%2520images.%2520To%2520this%2520end%252C%2520we%2520leverage%2520Masked%250AAutoencoders%2520%2528MAE%2529%252C%2520a%2520simple%2520and%2520scalable%2520method%2520for%2520self-supervised%2520learning%252C%250Ato%2520obtain%2520a%2520powerful%2520and%2520general%2520representation%2520for%2520OCT%2520images%2520by%2520training%2520on%250A700K%2520OCT%2520images%2520from%252041K%2520patients%2520collected%2520under%2520real%2520world%2520clinical%2520settings.%250AWe%2520also%2520provide%2520the%2520first%2520extensive%2520evaluation%2520for%2520a%2520model%2520of%2520OCT%2520on%2520a%250Achallenging%2520battery%2520of%25206%2520downstream%2520tasks.%2520Our%2520model%2520achieves%2520strong%250Aperformance%2520when%2520fully%2520finetuned%2520but%2520can%2520also%2520serve%2520as%2520a%2520versatile%2520frozen%250Afeature%2520extractor%2520for%2520many%2520tasks%2520using%2520lightweight%2520adapters.%2520Furthermore%252C%2520we%250Apropose%2520an%2520extension%2520of%2520the%2520MAE%2520pretraining%2520to%2520fuse%2520OCT%2520with%2520an%2520auxiliary%250Amodality%252C%2520namely%252C%2520IR%2520fundus%2520images%2520and%2520learn%2520a%2520joint%2520model%2520for%2520both.%2520We%250Ademonstrate%2520our%2520approach%2520improves%2520performance%2520on%2520a%2520multimodal%2520downstream%250Aapplication.%2520Our%2520experiments%2520utilize%2520most%2520publicly%2520available%2520OCT%2520datasets%252C%2520thus%250Aenabling%2520future%2520comparisons.%2520Our%2520code%2520and%2520model%2520weights%2520are%2520publicly%2520available%250Ahttps%253A//github.com/TheoPis/MIM_OCT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Image%20Modelling%20for%20retinal%20OCT%20understanding&entry.906535625=Theodoros%20Pissas%20and%20Pablo%20M%C3%A1rquez-Neila%20and%20Sebastian%20Wolf%20and%20Martin%20Zinkernagel%20and%20Raphael%20Sznitman&entry.1292438233=%20%20This%20work%20explores%20the%20effectiveness%20of%20masked%20image%20modelling%20for%20learning%0Arepresentations%20of%20retinal%20OCT%20images.%20To%20this%20end%2C%20we%20leverage%20Masked%0AAutoencoders%20%28MAE%29%2C%20a%20simple%20and%20scalable%20method%20for%20self-supervised%20learning%2C%0Ato%20obtain%20a%20powerful%20and%20general%20representation%20for%20OCT%20images%20by%20training%20on%0A700K%20OCT%20images%20from%2041K%20patients%20collected%20under%20real%20world%20clinical%20settings.%0AWe%20also%20provide%20the%20first%20extensive%20evaluation%20for%20a%20model%20of%20OCT%20on%20a%0Achallenging%20battery%20of%206%20downstream%20tasks.%20Our%20model%20achieves%20strong%0Aperformance%20when%20fully%20finetuned%20but%20can%20also%20serve%20as%20a%20versatile%20frozen%0Afeature%20extractor%20for%20many%20tasks%20using%20lightweight%20adapters.%20Furthermore%2C%20we%0Apropose%20an%20extension%20of%20the%20MAE%20pretraining%20to%20fuse%20OCT%20with%20an%20auxiliary%0Amodality%2C%20namely%2C%20IR%20fundus%20images%20and%20learn%20a%20joint%20model%20for%20both.%20We%0Ademonstrate%20our%20approach%20improves%20performance%20on%20a%20multimodal%20downstream%0Aapplication.%20Our%20experiments%20utilize%20most%20publicly%20available%20OCT%20datasets%2C%20thus%0Aenabling%20future%20comparisons.%20Our%20code%20and%20model%20weights%20are%20publicly%20available%0Ahttps%3A//github.com/TheoPis/MIM_OCT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14788v1&entry.124074799=Read"},
{"title": "A Semantic Segmentation-guided Approach for Ground-to-Aerial Image\n  Matching", "author": "Francesco Pro and Nikolaos Dionelis and Luca Maiano and Bertrand Le Saux and Irene Amerini", "abstract": "  Nowadays the accurate geo-localization of ground-view images has an important\nrole across domains as diverse as journalism, forensics analysis, transports,\nand Earth Observation. This work addresses the problem of matching a query\nground-view image with the corresponding satellite image without GPS data. This\nis done by comparing the features from a ground-view image and a satellite one,\ninnovatively leveraging the corresponding latter's segmentation mask through a\nthree-stream Siamese-like network. The proposed method, Semantic Align Net\n(SAN), focuses on limited Field-of-View (FoV) and ground panorama images\n(images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite\nimages in combination with their semantic segmentation masks, aimed at ensuring\nthat the model can extract useful features and focus on the significant parts\nof the images. This work shows how SAN through semantic analysis of images\nimproves the performance on the unlabelled CVUSA dataset for all the tested\nFoVs.\n", "link": "http://arxiv.org/abs/2404.11302v2", "date": "2024-05-23", "relevancy": 2.2758, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5858}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5647}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching&body=Title%3A%20A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching%0AAuthor%3A%20Francesco%20Pro%20and%20Nikolaos%20Dionelis%20and%20Luca%20Maiano%20and%20Bertrand%20Le%20Saux%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20Nowadays%20the%20accurate%20geo-localization%20of%20ground-view%20images%20has%20an%20important%0Arole%20across%20domains%20as%20diverse%20as%20journalism%2C%20forensics%20analysis%2C%20transports%2C%0Aand%20Earth%20Observation.%20This%20work%20addresses%20the%20problem%20of%20matching%20a%20query%0Aground-view%20image%20with%20the%20corresponding%20satellite%20image%20without%20GPS%20data.%20This%0Ais%20done%20by%20comparing%20the%20features%20from%20a%20ground-view%20image%20and%20a%20satellite%20one%2C%0Ainnovatively%20leveraging%20the%20corresponding%20latter%27s%20segmentation%20mask%20through%20a%0Athree-stream%20Siamese-like%20network.%20The%20proposed%20method%2C%20Semantic%20Align%20Net%0A%28SAN%29%2C%20focuses%20on%20limited%20Field-of-View%20%28FoV%29%20and%20ground%20panorama%20images%0A%28images%20with%20a%20FoV%20of%20360%7B%5Cdeg%7D%29.%20The%20novelty%20lies%20in%20the%20fusion%20of%20satellite%0Aimages%20in%20combination%20with%20their%20semantic%20segmentation%20masks%2C%20aimed%20at%20ensuring%0Athat%20the%20model%20can%20extract%20useful%20features%20and%20focus%20on%20the%20significant%20parts%0Aof%20the%20images.%20This%20work%20shows%20how%20SAN%20through%20semantic%20analysis%20of%20images%0Aimproves%20the%20performance%20on%20the%20unlabelled%20CVUSA%20dataset%20for%20all%20the%20tested%0AFoVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Semantic%2520Segmentation-guided%2520Approach%2520for%2520Ground-to-Aerial%2520Image%250A%2520%2520Matching%26entry.906535625%3DFrancesco%2520Pro%2520and%2520Nikolaos%2520Dionelis%2520and%2520Luca%2520Maiano%2520and%2520Bertrand%2520Le%2520Saux%2520and%2520Irene%2520Amerini%26entry.1292438233%3D%2520%2520Nowadays%2520the%2520accurate%2520geo-localization%2520of%2520ground-view%2520images%2520has%2520an%2520important%250Arole%2520across%2520domains%2520as%2520diverse%2520as%2520journalism%252C%2520forensics%2520analysis%252C%2520transports%252C%250Aand%2520Earth%2520Observation.%2520This%2520work%2520addresses%2520the%2520problem%2520of%2520matching%2520a%2520query%250Aground-view%2520image%2520with%2520the%2520corresponding%2520satellite%2520image%2520without%2520GPS%2520data.%2520This%250Ais%2520done%2520by%2520comparing%2520the%2520features%2520from%2520a%2520ground-view%2520image%2520and%2520a%2520satellite%2520one%252C%250Ainnovatively%2520leveraging%2520the%2520corresponding%2520latter%2527s%2520segmentation%2520mask%2520through%2520a%250Athree-stream%2520Siamese-like%2520network.%2520The%2520proposed%2520method%252C%2520Semantic%2520Align%2520Net%250A%2528SAN%2529%252C%2520focuses%2520on%2520limited%2520Field-of-View%2520%2528FoV%2529%2520and%2520ground%2520panorama%2520images%250A%2528images%2520with%2520a%2520FoV%2520of%2520360%257B%255Cdeg%257D%2529.%2520The%2520novelty%2520lies%2520in%2520the%2520fusion%2520of%2520satellite%250Aimages%2520in%2520combination%2520with%2520their%2520semantic%2520segmentation%2520masks%252C%2520aimed%2520at%2520ensuring%250Athat%2520the%2520model%2520can%2520extract%2520useful%2520features%2520and%2520focus%2520on%2520the%2520significant%2520parts%250Aof%2520the%2520images.%2520This%2520work%2520shows%2520how%2520SAN%2520through%2520semantic%2520analysis%2520of%2520images%250Aimproves%2520the%2520performance%2520on%2520the%2520unlabelled%2520CVUSA%2520dataset%2520for%2520all%2520the%2520tested%250AFoVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching&entry.906535625=Francesco%20Pro%20and%20Nikolaos%20Dionelis%20and%20Luca%20Maiano%20and%20Bertrand%20Le%20Saux%20and%20Irene%20Amerini&entry.1292438233=%20%20Nowadays%20the%20accurate%20geo-localization%20of%20ground-view%20images%20has%20an%20important%0Arole%20across%20domains%20as%20diverse%20as%20journalism%2C%20forensics%20analysis%2C%20transports%2C%0Aand%20Earth%20Observation.%20This%20work%20addresses%20the%20problem%20of%20matching%20a%20query%0Aground-view%20image%20with%20the%20corresponding%20satellite%20image%20without%20GPS%20data.%20This%0Ais%20done%20by%20comparing%20the%20features%20from%20a%20ground-view%20image%20and%20a%20satellite%20one%2C%0Ainnovatively%20leveraging%20the%20corresponding%20latter%27s%20segmentation%20mask%20through%20a%0Athree-stream%20Siamese-like%20network.%20The%20proposed%20method%2C%20Semantic%20Align%20Net%0A%28SAN%29%2C%20focuses%20on%20limited%20Field-of-View%20%28FoV%29%20and%20ground%20panorama%20images%0A%28images%20with%20a%20FoV%20of%20360%7B%5Cdeg%7D%29.%20The%20novelty%20lies%20in%20the%20fusion%20of%20satellite%0Aimages%20in%20combination%20with%20their%20semantic%20segmentation%20masks%2C%20aimed%20at%20ensuring%0Athat%20the%20model%20can%20extract%20useful%20features%20and%20focus%20on%20the%20significant%20parts%0Aof%20the%20images.%20This%20work%20shows%20how%20SAN%20through%20semantic%20analysis%20of%20images%0Aimproves%20the%20performance%20on%20the%20unlabelled%20CVUSA%20dataset%20for%20all%20the%20tested%0AFoVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11302v2&entry.124074799=Read"},
{"title": "MobilityGPT: Enhanced Human Mobility Modeling with a GPT model", "author": "Ammar Haydari and Dongjie Chen and Zhengfeng Lai and Michael Zhang and Chen-Nee Chuah", "abstract": "  Generative models have shown promising results in capturing human mobility\ncharacteristics and generating synthetic trajectories. However, it remains\nchallenging to ensure that the generated geospatial mobility data is\nsemantically realistic, including consistent location sequences, and reflects\nreal-world characteristics, such as constraining on geospatial limits. We\nreformat human mobility modeling as an autoregressive generation task to\naddress these issues, leveraging the Generative Pre-trained Transformer (GPT)\narchitecture. To ensure its controllable generation to alleviate the above\nchallenges, we propose a geospatially-aware generative model, MobilityGPT. We\npropose a gravity-based sampling method to train a transformer for semantic\nsequence similarity. Then, we constrained the training process via a road\nconnectivity matrix that provides the connectivity of sequences in trajectory\ngeneration, thereby keeping generated trajectories in geospatial limits.\nLastly, we proposed to construct a preference dataset for fine-tuning\nMobilityGPT via Reinforcement Learning from Trajectory Feedback (RLTF)\nmechanism, which minimizes the travel distance between training and the\nsynthetically generated trajectories. Experiments on real-world datasets\ndemonstrate MobilityGPT's superior performance over state-of-the-art methods in\ngenerating high-quality mobility trajectories that are closest to real data in\nterms of origin-destination similarity, trip length, travel radius, link, and\ngravity distributions.\n", "link": "http://arxiv.org/abs/2402.03264v2", "date": "2024-05-23", "relevancy": 2.2633, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5807}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MobilityGPT%3A%20Enhanced%20Human%20Mobility%20Modeling%20with%20a%20GPT%20model&body=Title%3A%20MobilityGPT%3A%20Enhanced%20Human%20Mobility%20Modeling%20with%20a%20GPT%20model%0AAuthor%3A%20Ammar%20Haydari%20and%20Dongjie%20Chen%20and%20Zhengfeng%20Lai%20and%20Michael%20Zhang%20and%20Chen-Nee%20Chuah%0AAbstract%3A%20%20%20Generative%20models%20have%20shown%20promising%20results%20in%20capturing%20human%20mobility%0Acharacteristics%20and%20generating%20synthetic%20trajectories.%20However%2C%20it%20remains%0Achallenging%20to%20ensure%20that%20the%20generated%20geospatial%20mobility%20data%20is%0Asemantically%20realistic%2C%20including%20consistent%20location%20sequences%2C%20and%20reflects%0Areal-world%20characteristics%2C%20such%20as%20constraining%20on%20geospatial%20limits.%20We%0Areformat%20human%20mobility%20modeling%20as%20an%20autoregressive%20generation%20task%20to%0Aaddress%20these%20issues%2C%20leveraging%20the%20Generative%20Pre-trained%20Transformer%20%28GPT%29%0Aarchitecture.%20To%20ensure%20its%20controllable%20generation%20to%20alleviate%20the%20above%0Achallenges%2C%20we%20propose%20a%20geospatially-aware%20generative%20model%2C%20MobilityGPT.%20We%0Apropose%20a%20gravity-based%20sampling%20method%20to%20train%20a%20transformer%20for%20semantic%0Asequence%20similarity.%20Then%2C%20we%20constrained%20the%20training%20process%20via%20a%20road%0Aconnectivity%20matrix%20that%20provides%20the%20connectivity%20of%20sequences%20in%20trajectory%0Ageneration%2C%20thereby%20keeping%20generated%20trajectories%20in%20geospatial%20limits.%0ALastly%2C%20we%20proposed%20to%20construct%20a%20preference%20dataset%20for%20fine-tuning%0AMobilityGPT%20via%20Reinforcement%20Learning%20from%20Trajectory%20Feedback%20%28RLTF%29%0Amechanism%2C%20which%20minimizes%20the%20travel%20distance%20between%20training%20and%20the%0Asynthetically%20generated%20trajectories.%20Experiments%20on%20real-world%20datasets%0Ademonstrate%20MobilityGPT%27s%20superior%20performance%20over%20state-of-the-art%20methods%20in%0Agenerating%20high-quality%20mobility%20trajectories%20that%20are%20closest%20to%20real%20data%20in%0Aterms%20of%20origin-destination%20similarity%2C%20trip%20length%2C%20travel%20radius%2C%20link%2C%20and%0Agravity%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMobilityGPT%253A%2520Enhanced%2520Human%2520Mobility%2520Modeling%2520with%2520a%2520GPT%2520model%26entry.906535625%3DAmmar%2520Haydari%2520and%2520Dongjie%2520Chen%2520and%2520Zhengfeng%2520Lai%2520and%2520Michael%2520Zhang%2520and%2520Chen-Nee%2520Chuah%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520shown%2520promising%2520results%2520in%2520capturing%2520human%2520mobility%250Acharacteristics%2520and%2520generating%2520synthetic%2520trajectories.%2520However%252C%2520it%2520remains%250Achallenging%2520to%2520ensure%2520that%2520the%2520generated%2520geospatial%2520mobility%2520data%2520is%250Asemantically%2520realistic%252C%2520including%2520consistent%2520location%2520sequences%252C%2520and%2520reflects%250Areal-world%2520characteristics%252C%2520such%2520as%2520constraining%2520on%2520geospatial%2520limits.%2520We%250Areformat%2520human%2520mobility%2520modeling%2520as%2520an%2520autoregressive%2520generation%2520task%2520to%250Aaddress%2520these%2520issues%252C%2520leveraging%2520the%2520Generative%2520Pre-trained%2520Transformer%2520%2528GPT%2529%250Aarchitecture.%2520To%2520ensure%2520its%2520controllable%2520generation%2520to%2520alleviate%2520the%2520above%250Achallenges%252C%2520we%2520propose%2520a%2520geospatially-aware%2520generative%2520model%252C%2520MobilityGPT.%2520We%250Apropose%2520a%2520gravity-based%2520sampling%2520method%2520to%2520train%2520a%2520transformer%2520for%2520semantic%250Asequence%2520similarity.%2520Then%252C%2520we%2520constrained%2520the%2520training%2520process%2520via%2520a%2520road%250Aconnectivity%2520matrix%2520that%2520provides%2520the%2520connectivity%2520of%2520sequences%2520in%2520trajectory%250Ageneration%252C%2520thereby%2520keeping%2520generated%2520trajectories%2520in%2520geospatial%2520limits.%250ALastly%252C%2520we%2520proposed%2520to%2520construct%2520a%2520preference%2520dataset%2520for%2520fine-tuning%250AMobilityGPT%2520via%2520Reinforcement%2520Learning%2520from%2520Trajectory%2520Feedback%2520%2528RLTF%2529%250Amechanism%252C%2520which%2520minimizes%2520the%2520travel%2520distance%2520between%2520training%2520and%2520the%250Asynthetically%2520generated%2520trajectories.%2520Experiments%2520on%2520real-world%2520datasets%250Ademonstrate%2520MobilityGPT%2527s%2520superior%2520performance%2520over%2520state-of-the-art%2520methods%2520in%250Agenerating%2520high-quality%2520mobility%2520trajectories%2520that%2520are%2520closest%2520to%2520real%2520data%2520in%250Aterms%2520of%2520origin-destination%2520similarity%252C%2520trip%2520length%252C%2520travel%2520radius%252C%2520link%252C%2520and%250Agravity%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MobilityGPT%3A%20Enhanced%20Human%20Mobility%20Modeling%20with%20a%20GPT%20model&entry.906535625=Ammar%20Haydari%20and%20Dongjie%20Chen%20and%20Zhengfeng%20Lai%20and%20Michael%20Zhang%20and%20Chen-Nee%20Chuah&entry.1292438233=%20%20Generative%20models%20have%20shown%20promising%20results%20in%20capturing%20human%20mobility%0Acharacteristics%20and%20generating%20synthetic%20trajectories.%20However%2C%20it%20remains%0Achallenging%20to%20ensure%20that%20the%20generated%20geospatial%20mobility%20data%20is%0Asemantically%20realistic%2C%20including%20consistent%20location%20sequences%2C%20and%20reflects%0Areal-world%20characteristics%2C%20such%20as%20constraining%20on%20geospatial%20limits.%20We%0Areformat%20human%20mobility%20modeling%20as%20an%20autoregressive%20generation%20task%20to%0Aaddress%20these%20issues%2C%20leveraging%20the%20Generative%20Pre-trained%20Transformer%20%28GPT%29%0Aarchitecture.%20To%20ensure%20its%20controllable%20generation%20to%20alleviate%20the%20above%0Achallenges%2C%20we%20propose%20a%20geospatially-aware%20generative%20model%2C%20MobilityGPT.%20We%0Apropose%20a%20gravity-based%20sampling%20method%20to%20train%20a%20transformer%20for%20semantic%0Asequence%20similarity.%20Then%2C%20we%20constrained%20the%20training%20process%20via%20a%20road%0Aconnectivity%20matrix%20that%20provides%20the%20connectivity%20of%20sequences%20in%20trajectory%0Ageneration%2C%20thereby%20keeping%20generated%20trajectories%20in%20geospatial%20limits.%0ALastly%2C%20we%20proposed%20to%20construct%20a%20preference%20dataset%20for%20fine-tuning%0AMobilityGPT%20via%20Reinforcement%20Learning%20from%20Trajectory%20Feedback%20%28RLTF%29%0Amechanism%2C%20which%20minimizes%20the%20travel%20distance%20between%20training%20and%20the%0Asynthetically%20generated%20trajectories.%20Experiments%20on%20real-world%20datasets%0Ademonstrate%20MobilityGPT%27s%20superior%20performance%20over%20state-of-the-art%20methods%20in%0Agenerating%20high-quality%20mobility%20trajectories%20that%20are%20closest%20to%20real%20data%20in%0Aterms%20of%20origin-destination%20similarity%2C%20trip%20length%2C%20travel%20radius%2C%20link%2C%20and%0Agravity%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03264v2&entry.124074799=Read"},
{"title": "SE3D: A Framework For Saliency Method Evaluation In 3D Imaging", "author": "Mariusz Wi\u015bniewski and Loris Giulivi and Giacomo Boracchi", "abstract": "  For more than a decade, deep learning models have been dominating in various\n2D imaging tasks. Their application is now extending to 3D imaging, with 3D\nConvolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and\nCT scans, with significant implications for fields such as autonomous driving\nand medical imaging. In these critical settings, explaining the model's\ndecisions is fundamental. Despite recent advances in Explainable Artificial\nIntelligence, however, little effort has been devoted to explaining 3D CNNs,\nand many works explain these models via inadequate extensions of 2D saliency\nmethods.\n  One fundamental limitation to the development of 3D saliency methods is the\nlack of a benchmark to quantitatively assess them on 3D data. To address this\nissue, we propose SE3D: a framework for Saliency method Evaluation in 3D\nimaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and\nevaluation metrics to assess saliency methods for 3D CNNs. We evaluate both\nstate-of-the-art saliency methods designed for 3D data and extensions of\npopular 2D saliency methods to 3D. Our experiments show that 3D saliency\nmethods do not provide explanations of sufficient quality, and that there is\nmargin for future improvements and safer applications of 3D CNNs in critical\nfields.\n", "link": "http://arxiv.org/abs/2405.14584v1", "date": "2024-05-23", "relevancy": 2.2628, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE3D%3A%20A%20Framework%20For%20Saliency%20Method%20Evaluation%20In%203D%20Imaging&body=Title%3A%20SE3D%3A%20A%20Framework%20For%20Saliency%20Method%20Evaluation%20In%203D%20Imaging%0AAuthor%3A%20Mariusz%20Wi%C5%9Bniewski%20and%20Loris%20Giulivi%20and%20Giacomo%20Boracchi%0AAbstract%3A%20%20%20For%20more%20than%20a%20decade%2C%20deep%20learning%20models%20have%20been%20dominating%20in%20various%0A2D%20imaging%20tasks.%20Their%20application%20is%20now%20extending%20to%203D%20imaging%2C%20with%203D%0AConvolutional%20Neural%20Networks%20%283D%20CNNs%29%20being%20able%20to%20process%20LIDAR%2C%20MRI%2C%20and%0ACT%20scans%2C%20with%20significant%20implications%20for%20fields%20such%20as%20autonomous%20driving%0Aand%20medical%20imaging.%20In%20these%20critical%20settings%2C%20explaining%20the%20model%27s%0Adecisions%20is%20fundamental.%20Despite%20recent%20advances%20in%20Explainable%20Artificial%0AIntelligence%2C%20however%2C%20little%20effort%20has%20been%20devoted%20to%20explaining%203D%20CNNs%2C%0Aand%20many%20works%20explain%20these%20models%20via%20inadequate%20extensions%20of%202D%20saliency%0Amethods.%0A%20%20One%20fundamental%20limitation%20to%20the%20development%20of%203D%20saliency%20methods%20is%20the%0Alack%20of%20a%20benchmark%20to%20quantitatively%20assess%20them%20on%203D%20data.%20To%20address%20this%0Aissue%2C%20we%20propose%20SE3D%3A%20a%20framework%20for%20Saliency%20method%20Evaluation%20in%203D%0Aimaging.%20We%20propose%20modifications%20to%20ShapeNet%2C%20ScanNet%2C%20and%20BraTS%20datasets%2C%20and%0Aevaluation%20metrics%20to%20assess%20saliency%20methods%20for%203D%20CNNs.%20We%20evaluate%20both%0Astate-of-the-art%20saliency%20methods%20designed%20for%203D%20data%20and%20extensions%20of%0Apopular%202D%20saliency%20methods%20to%203D.%20Our%20experiments%20show%20that%203D%20saliency%0Amethods%20do%20not%20provide%20explanations%20of%20sufficient%20quality%2C%20and%20that%20there%20is%0Amargin%20for%20future%20improvements%20and%20safer%20applications%20of%203D%20CNNs%20in%20critical%0Afields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE3D%253A%2520A%2520Framework%2520For%2520Saliency%2520Method%2520Evaluation%2520In%25203D%2520Imaging%26entry.906535625%3DMariusz%2520Wi%25C5%259Bniewski%2520and%2520Loris%2520Giulivi%2520and%2520Giacomo%2520Boracchi%26entry.1292438233%3D%2520%2520For%2520more%2520than%2520a%2520decade%252C%2520deep%2520learning%2520models%2520have%2520been%2520dominating%2520in%2520various%250A2D%2520imaging%2520tasks.%2520Their%2520application%2520is%2520now%2520extending%2520to%25203D%2520imaging%252C%2520with%25203D%250AConvolutional%2520Neural%2520Networks%2520%25283D%2520CNNs%2529%2520being%2520able%2520to%2520process%2520LIDAR%252C%2520MRI%252C%2520and%250ACT%2520scans%252C%2520with%2520significant%2520implications%2520for%2520fields%2520such%2520as%2520autonomous%2520driving%250Aand%2520medical%2520imaging.%2520In%2520these%2520critical%2520settings%252C%2520explaining%2520the%2520model%2527s%250Adecisions%2520is%2520fundamental.%2520Despite%2520recent%2520advances%2520in%2520Explainable%2520Artificial%250AIntelligence%252C%2520however%252C%2520little%2520effort%2520has%2520been%2520devoted%2520to%2520explaining%25203D%2520CNNs%252C%250Aand%2520many%2520works%2520explain%2520these%2520models%2520via%2520inadequate%2520extensions%2520of%25202D%2520saliency%250Amethods.%250A%2520%2520One%2520fundamental%2520limitation%2520to%2520the%2520development%2520of%25203D%2520saliency%2520methods%2520is%2520the%250Alack%2520of%2520a%2520benchmark%2520to%2520quantitatively%2520assess%2520them%2520on%25203D%2520data.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520SE3D%253A%2520a%2520framework%2520for%2520Saliency%2520method%2520Evaluation%2520in%25203D%250Aimaging.%2520We%2520propose%2520modifications%2520to%2520ShapeNet%252C%2520ScanNet%252C%2520and%2520BraTS%2520datasets%252C%2520and%250Aevaluation%2520metrics%2520to%2520assess%2520saliency%2520methods%2520for%25203D%2520CNNs.%2520We%2520evaluate%2520both%250Astate-of-the-art%2520saliency%2520methods%2520designed%2520for%25203D%2520data%2520and%2520extensions%2520of%250Apopular%25202D%2520saliency%2520methods%2520to%25203D.%2520Our%2520experiments%2520show%2520that%25203D%2520saliency%250Amethods%2520do%2520not%2520provide%2520explanations%2520of%2520sufficient%2520quality%252C%2520and%2520that%2520there%2520is%250Amargin%2520for%2520future%2520improvements%2520and%2520safer%2520applications%2520of%25203D%2520CNNs%2520in%2520critical%250Afields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE3D%3A%20A%20Framework%20For%20Saliency%20Method%20Evaluation%20In%203D%20Imaging&entry.906535625=Mariusz%20Wi%C5%9Bniewski%20and%20Loris%20Giulivi%20and%20Giacomo%20Boracchi&entry.1292438233=%20%20For%20more%20than%20a%20decade%2C%20deep%20learning%20models%20have%20been%20dominating%20in%20various%0A2D%20imaging%20tasks.%20Their%20application%20is%20now%20extending%20to%203D%20imaging%2C%20with%203D%0AConvolutional%20Neural%20Networks%20%283D%20CNNs%29%20being%20able%20to%20process%20LIDAR%2C%20MRI%2C%20and%0ACT%20scans%2C%20with%20significant%20implications%20for%20fields%20such%20as%20autonomous%20driving%0Aand%20medical%20imaging.%20In%20these%20critical%20settings%2C%20explaining%20the%20model%27s%0Adecisions%20is%20fundamental.%20Despite%20recent%20advances%20in%20Explainable%20Artificial%0AIntelligence%2C%20however%2C%20little%20effort%20has%20been%20devoted%20to%20explaining%203D%20CNNs%2C%0Aand%20many%20works%20explain%20these%20models%20via%20inadequate%20extensions%20of%202D%20saliency%0Amethods.%0A%20%20One%20fundamental%20limitation%20to%20the%20development%20of%203D%20saliency%20methods%20is%20the%0Alack%20of%20a%20benchmark%20to%20quantitatively%20assess%20them%20on%203D%20data.%20To%20address%20this%0Aissue%2C%20we%20propose%20SE3D%3A%20a%20framework%20for%20Saliency%20method%20Evaluation%20in%203D%0Aimaging.%20We%20propose%20modifications%20to%20ShapeNet%2C%20ScanNet%2C%20and%20BraTS%20datasets%2C%20and%0Aevaluation%20metrics%20to%20assess%20saliency%20methods%20for%203D%20CNNs.%20We%20evaluate%20both%0Astate-of-the-art%20saliency%20methods%20designed%20for%203D%20data%20and%20extensions%20of%0Apopular%202D%20saliency%20methods%20to%203D.%20Our%20experiments%20show%20that%203D%20saliency%0Amethods%20do%20not%20provide%20explanations%20of%20sufficient%20quality%2C%20and%20that%20there%20is%0Amargin%20for%20future%20improvements%20and%20safer%20applications%20of%203D%20CNNs%20in%20critical%0Afields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14584v1&entry.124074799=Read"},
{"title": "An Empirical Study of Training State-of-the-Art LiDAR Segmentation\n  Models", "author": "Jiahao Sun and Xiang Xu and Lingdong Kong and Youquan Liu and Li Li and Chenming Zhu and Jingwei Zhang and Zeqi Xiao and Runnan Chen and Tai Wang and Wenwei Zhang and Kai Chen and Chunmei Qing", "abstract": "  In the rapidly evolving field of autonomous driving, precise segmentation of\nLiDAR data is crucial for understanding complex 3D environments. Traditional\napproaches often rely on disparate, standalone codebases, hindering unified\nadvancements and fair benchmarking across models. To address these challenges,\nwe introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the\nefficient training and evaluation of state-of-the-art LiDAR segmentation\nmodels. We support a wide range of segmentation models and integrate advanced\ndata augmentation techniques to enhance robustness and generalization.\nAdditionally, the toolbox provides support for multiple leading sparse\nconvolution backends, optimizing computational efficiency and performance. By\nfostering a unified framework, MMDetection3D-lidarseg streamlines development\nand benchmarking, setting new standards for research and application. Our\nextensive benchmark experiments on widely-used datasets demonstrate the\neffectiveness of the toolbox. The codebase and trained models have been\npublicly available, promoting further research and innovation in the field of\nLiDAR segmentation for autonomous driving.\n", "link": "http://arxiv.org/abs/2405.14870v1", "date": "2024-05-23", "relevancy": 2.26, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5922}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models&body=Title%3A%20An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models%0AAuthor%3A%20Jiahao%20Sun%20and%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Youquan%20Liu%20and%20Li%20Li%20and%20Chenming%20Zhu%20and%20Jingwei%20Zhang%20and%20Zeqi%20Xiao%20and%20Runnan%20Chen%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Chunmei%20Qing%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%2C%20precise%20segmentation%20of%0ALiDAR%20data%20is%20crucial%20for%20understanding%20complex%203D%20environments.%20Traditional%0Aapproaches%20often%20rely%20on%20disparate%2C%20standalone%20codebases%2C%20hindering%20unified%0Aadvancements%20and%20fair%20benchmarking%20across%20models.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20MMDetection3D-lidarseg%2C%20a%20comprehensive%20toolbox%20designed%20for%20the%0Aefficient%20training%20and%20evaluation%20of%20state-of-the-art%20LiDAR%20segmentation%0Amodels.%20We%20support%20a%20wide%20range%20of%20segmentation%20models%20and%20integrate%20advanced%0Adata%20augmentation%20techniques%20to%20enhance%20robustness%20and%20generalization.%0AAdditionally%2C%20the%20toolbox%20provides%20support%20for%20multiple%20leading%20sparse%0Aconvolution%20backends%2C%20optimizing%20computational%20efficiency%20and%20performance.%20By%0Afostering%20a%20unified%20framework%2C%20MMDetection3D-lidarseg%20streamlines%20development%0Aand%20benchmarking%2C%20setting%20new%20standards%20for%20research%20and%20application.%20Our%0Aextensive%20benchmark%20experiments%20on%20widely-used%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20toolbox.%20The%20codebase%20and%20trained%20models%20have%20been%0Apublicly%20available%2C%20promoting%20further%20research%20and%20innovation%20in%20the%20field%20of%0ALiDAR%20segmentation%20for%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Training%2520State-of-the-Art%2520LiDAR%2520Segmentation%250A%2520%2520Models%26entry.906535625%3DJiahao%2520Sun%2520and%2520Xiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Youquan%2520Liu%2520and%2520Li%2520Li%2520and%2520Chenming%2520Zhu%2520and%2520Jingwei%2520Zhang%2520and%2520Zeqi%2520Xiao%2520and%2520Runnan%2520Chen%2520and%2520Tai%2520Wang%2520and%2520Wenwei%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Chunmei%2520Qing%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520autonomous%2520driving%252C%2520precise%2520segmentation%2520of%250ALiDAR%2520data%2520is%2520crucial%2520for%2520understanding%2520complex%25203D%2520environments.%2520Traditional%250Aapproaches%2520often%2520rely%2520on%2520disparate%252C%2520standalone%2520codebases%252C%2520hindering%2520unified%250Aadvancements%2520and%2520fair%2520benchmarking%2520across%2520models.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520MMDetection3D-lidarseg%252C%2520a%2520comprehensive%2520toolbox%2520designed%2520for%2520the%250Aefficient%2520training%2520and%2520evaluation%2520of%2520state-of-the-art%2520LiDAR%2520segmentation%250Amodels.%2520We%2520support%2520a%2520wide%2520range%2520of%2520segmentation%2520models%2520and%2520integrate%2520advanced%250Adata%2520augmentation%2520techniques%2520to%2520enhance%2520robustness%2520and%2520generalization.%250AAdditionally%252C%2520the%2520toolbox%2520provides%2520support%2520for%2520multiple%2520leading%2520sparse%250Aconvolution%2520backends%252C%2520optimizing%2520computational%2520efficiency%2520and%2520performance.%2520By%250Afostering%2520a%2520unified%2520framework%252C%2520MMDetection3D-lidarseg%2520streamlines%2520development%250Aand%2520benchmarking%252C%2520setting%2520new%2520standards%2520for%2520research%2520and%2520application.%2520Our%250Aextensive%2520benchmark%2520experiments%2520on%2520widely-used%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520toolbox.%2520The%2520codebase%2520and%2520trained%2520models%2520have%2520been%250Apublicly%2520available%252C%2520promoting%2520further%2520research%2520and%2520innovation%2520in%2520the%2520field%2520of%250ALiDAR%2520segmentation%2520for%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Training%20State-of-the-Art%20LiDAR%20Segmentation%0A%20%20Models&entry.906535625=Jiahao%20Sun%20and%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Youquan%20Liu%20and%20Li%20Li%20and%20Chenming%20Zhu%20and%20Jingwei%20Zhang%20and%20Zeqi%20Xiao%20and%20Runnan%20Chen%20and%20Tai%20Wang%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Chunmei%20Qing&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20autonomous%20driving%2C%20precise%20segmentation%20of%0ALiDAR%20data%20is%20crucial%20for%20understanding%20complex%203D%20environments.%20Traditional%0Aapproaches%20often%20rely%20on%20disparate%2C%20standalone%20codebases%2C%20hindering%20unified%0Aadvancements%20and%20fair%20benchmarking%20across%20models.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20MMDetection3D-lidarseg%2C%20a%20comprehensive%20toolbox%20designed%20for%20the%0Aefficient%20training%20and%20evaluation%20of%20state-of-the-art%20LiDAR%20segmentation%0Amodels.%20We%20support%20a%20wide%20range%20of%20segmentation%20models%20and%20integrate%20advanced%0Adata%20augmentation%20techniques%20to%20enhance%20robustness%20and%20generalization.%0AAdditionally%2C%20the%20toolbox%20provides%20support%20for%20multiple%20leading%20sparse%0Aconvolution%20backends%2C%20optimizing%20computational%20efficiency%20and%20performance.%20By%0Afostering%20a%20unified%20framework%2C%20MMDetection3D-lidarseg%20streamlines%20development%0Aand%20benchmarking%2C%20setting%20new%20standards%20for%20research%20and%20application.%20Our%0Aextensive%20benchmark%20experiments%20on%20widely-used%20datasets%20demonstrate%20the%0Aeffectiveness%20of%20the%20toolbox.%20The%20codebase%20and%20trained%20models%20have%20been%0Apublicly%20available%2C%20promoting%20further%20research%20and%20innovation%20in%20the%20field%20of%0ALiDAR%20segmentation%20for%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14870v1&entry.124074799=Read"},
{"title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long\n  unlabelled video", "author": "Shashanka Venkataramanan and Mamshad Nayeem Rizve and Jo\u00e3o Carreira and Yuki M. Asano and Yannis Avrithis", "abstract": "  Self-supervised learning has unlocked the potential of scaling up pretraining\nto billions of images, since annotation is unnecessary. But are we making the\nbest use of data? How more economical can we be? In this work, we attempt to\nanswer this question by making two contributions. First, we investigate\nfirst-person videos and introduce a \"Walking Tours\" dataset. These videos are\nhigh-resolution, hours-long, captured in a single uninterrupted take, depicting\na large number of objects and actions with natural scene transitions. They are\nunlabeled and uncurated, thus realistic for self-supervision and comparable\nwith human learning.\n  Second, we introduce a novel self-supervised image pretraining method\ntailored for learning from continuous videos. Existing methods typically adapt\nimage-based pretraining approaches to incorporate more frames. Instead, we\nadvocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\nleads to attention maps that Discover and tRAck objects over time in an\nend-to-end manner, using transformer cross-attention. We derive multiple views\nfrom the tracks and use them in a classical self-supervised distillation loss.\nUsing our novel approach, a single Walking Tours video remarkably becomes a\nstrong competitor to ImageNet for several image and video downstream tasks.\n", "link": "http://arxiv.org/abs/2310.08584v2", "date": "2024-05-23", "relevancy": 2.2593, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5851}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20ImageNet%20worth%201%20video%3F%20Learning%20strong%20image%20encoders%20from%201%20long%0A%20%20unlabelled%20video&body=Title%3A%20Is%20ImageNet%20worth%201%20video%3F%20Learning%20strong%20image%20encoders%20from%201%20long%0A%20%20unlabelled%20video%0AAuthor%3A%20Shashanka%20Venkataramanan%20and%20Mamshad%20Nayeem%20Rizve%20and%20Jo%C3%A3o%20Carreira%20and%20Yuki%20M.%20Asano%20and%20Yannis%20Avrithis%0AAbstract%3A%20%20%20Self-supervised%20learning%20has%20unlocked%20the%20potential%20of%20scaling%20up%20pretraining%0Ato%20billions%20of%20images%2C%20since%20annotation%20is%20unnecessary.%20But%20are%20we%20making%20the%0Abest%20use%20of%20data%3F%20How%20more%20economical%20can%20we%20be%3F%20In%20this%20work%2C%20we%20attempt%20to%0Aanswer%20this%20question%20by%20making%20two%20contributions.%20First%2C%20we%20investigate%0Afirst-person%20videos%20and%20introduce%20a%20%22Walking%20Tours%22%20dataset.%20These%20videos%20are%0Ahigh-resolution%2C%20hours-long%2C%20captured%20in%20a%20single%20uninterrupted%20take%2C%20depicting%0Aa%20large%20number%20of%20objects%20and%20actions%20with%20natural%20scene%20transitions.%20They%20are%0Aunlabeled%20and%20uncurated%2C%20thus%20realistic%20for%20self-supervision%20and%20comparable%0Awith%20human%20learning.%0A%20%20Second%2C%20we%20introduce%20a%20novel%20self-supervised%20image%20pretraining%20method%0Atailored%20for%20learning%20from%20continuous%20videos.%20Existing%20methods%20typically%20adapt%0Aimage-based%20pretraining%20approaches%20to%20incorporate%20more%20frames.%20Instead%2C%20we%0Aadvocate%20a%20%22tracking%20to%20learn%20to%20recognize%22%20approach.%20Our%20method%20called%20DoRA%2C%0Aleads%20to%20attention%20maps%20that%20Discover%20and%20tRAck%20objects%20over%20time%20in%20an%0Aend-to-end%20manner%2C%20using%20transformer%20cross-attention.%20We%20derive%20multiple%20views%0Afrom%20the%20tracks%20and%20use%20them%20in%20a%20classical%20self-supervised%20distillation%20loss.%0AUsing%20our%20novel%20approach%2C%20a%20single%20Walking%20Tours%20video%20remarkably%20becomes%20a%0Astrong%20competitor%20to%20ImageNet%20for%20several%20image%20and%20video%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520ImageNet%2520worth%25201%2520video%253F%2520Learning%2520strong%2520image%2520encoders%2520from%25201%2520long%250A%2520%2520unlabelled%2520video%26entry.906535625%3DShashanka%2520Venkataramanan%2520and%2520Mamshad%2520Nayeem%2520Rizve%2520and%2520Jo%25C3%25A3o%2520Carreira%2520and%2520Yuki%2520M.%2520Asano%2520and%2520Yannis%2520Avrithis%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520has%2520unlocked%2520the%2520potential%2520of%2520scaling%2520up%2520pretraining%250Ato%2520billions%2520of%2520images%252C%2520since%2520annotation%2520is%2520unnecessary.%2520But%2520are%2520we%2520making%2520the%250Abest%2520use%2520of%2520data%253F%2520How%2520more%2520economical%2520can%2520we%2520be%253F%2520In%2520this%2520work%252C%2520we%2520attempt%2520to%250Aanswer%2520this%2520question%2520by%2520making%2520two%2520contributions.%2520First%252C%2520we%2520investigate%250Afirst-person%2520videos%2520and%2520introduce%2520a%2520%2522Walking%2520Tours%2522%2520dataset.%2520These%2520videos%2520are%250Ahigh-resolution%252C%2520hours-long%252C%2520captured%2520in%2520a%2520single%2520uninterrupted%2520take%252C%2520depicting%250Aa%2520large%2520number%2520of%2520objects%2520and%2520actions%2520with%2520natural%2520scene%2520transitions.%2520They%2520are%250Aunlabeled%2520and%2520uncurated%252C%2520thus%2520realistic%2520for%2520self-supervision%2520and%2520comparable%250Awith%2520human%2520learning.%250A%2520%2520Second%252C%2520we%2520introduce%2520a%2520novel%2520self-supervised%2520image%2520pretraining%2520method%250Atailored%2520for%2520learning%2520from%2520continuous%2520videos.%2520Existing%2520methods%2520typically%2520adapt%250Aimage-based%2520pretraining%2520approaches%2520to%2520incorporate%2520more%2520frames.%2520Instead%252C%2520we%250Aadvocate%2520a%2520%2522tracking%2520to%2520learn%2520to%2520recognize%2522%2520approach.%2520Our%2520method%2520called%2520DoRA%252C%250Aleads%2520to%2520attention%2520maps%2520that%2520Discover%2520and%2520tRAck%2520objects%2520over%2520time%2520in%2520an%250Aend-to-end%2520manner%252C%2520using%2520transformer%2520cross-attention.%2520We%2520derive%2520multiple%2520views%250Afrom%2520the%2520tracks%2520and%2520use%2520them%2520in%2520a%2520classical%2520self-supervised%2520distillation%2520loss.%250AUsing%2520our%2520novel%2520approach%252C%2520a%2520single%2520Walking%2520Tours%2520video%2520remarkably%2520becomes%2520a%250Astrong%2520competitor%2520to%2520ImageNet%2520for%2520several%2520image%2520and%2520video%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20ImageNet%20worth%201%20video%3F%20Learning%20strong%20image%20encoders%20from%201%20long%0A%20%20unlabelled%20video&entry.906535625=Shashanka%20Venkataramanan%20and%20Mamshad%20Nayeem%20Rizve%20and%20Jo%C3%A3o%20Carreira%20and%20Yuki%20M.%20Asano%20and%20Yannis%20Avrithis&entry.1292438233=%20%20Self-supervised%20learning%20has%20unlocked%20the%20potential%20of%20scaling%20up%20pretraining%0Ato%20billions%20of%20images%2C%20since%20annotation%20is%20unnecessary.%20But%20are%20we%20making%20the%0Abest%20use%20of%20data%3F%20How%20more%20economical%20can%20we%20be%3F%20In%20this%20work%2C%20we%20attempt%20to%0Aanswer%20this%20question%20by%20making%20two%20contributions.%20First%2C%20we%20investigate%0Afirst-person%20videos%20and%20introduce%20a%20%22Walking%20Tours%22%20dataset.%20These%20videos%20are%0Ahigh-resolution%2C%20hours-long%2C%20captured%20in%20a%20single%20uninterrupted%20take%2C%20depicting%0Aa%20large%20number%20of%20objects%20and%20actions%20with%20natural%20scene%20transitions.%20They%20are%0Aunlabeled%20and%20uncurated%2C%20thus%20realistic%20for%20self-supervision%20and%20comparable%0Awith%20human%20learning.%0A%20%20Second%2C%20we%20introduce%20a%20novel%20self-supervised%20image%20pretraining%20method%0Atailored%20for%20learning%20from%20continuous%20videos.%20Existing%20methods%20typically%20adapt%0Aimage-based%20pretraining%20approaches%20to%20incorporate%20more%20frames.%20Instead%2C%20we%0Aadvocate%20a%20%22tracking%20to%20learn%20to%20recognize%22%20approach.%20Our%20method%20called%20DoRA%2C%0Aleads%20to%20attention%20maps%20that%20Discover%20and%20tRAck%20objects%20over%20time%20in%20an%0Aend-to-end%20manner%2C%20using%20transformer%20cross-attention.%20We%20derive%20multiple%20views%0Afrom%20the%20tracks%20and%20use%20them%20in%20a%20classical%20self-supervised%20distillation%20loss.%0AUsing%20our%20novel%20approach%2C%20a%20single%20Walking%20Tours%20video%20remarkably%20becomes%20a%0Astrong%20competitor%20to%20ImageNet%20for%20several%20image%20and%20video%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08584v2&entry.124074799=Read"},
{"title": "Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning\n  and Inference", "author": "Ting Liu and Xuyang Liu and Liangtao Shi and Zunnan Xu and Siteng Huang and Yi Xin and Quanjun Yin", "abstract": "  Parameter-efficient fine-tuning (PEFT) has emerged as a popular approach for\nadapting pre-trained Vision Transformer (ViT) models to downstream\napplications. While current PEFT methods achieve parameter efficiency, they\noverlook GPU memory and time efficiency during both fine-tuning and inference,\ndue to the repeated computation of redundant tokens in the ViT architecture.\nThis falls short of practical requirements for downstream task adaptation. In\nthis paper, we propose \\textbf{Sparse-Tuning}, a novel tuning paradigm that\nsubstantially enhances both fine-tuning and inference efficiency for\npre-trained ViT models. Sparse-Tuning efficiently fine-tunes the pre-trained\nViT by sparsely preserving the informative tokens and merging redundant ones,\nenabling the ViT to focus on the foreground while reducing computational costs\non background regions in the images. To accurately distinguish informative\ntokens from uninformative ones, we introduce a tailored Dense Adapter, which\nestablishes dense connections across different encoder layers in the ViT,\nthereby enhancing the representational capacity and quality of token\nsparsification. Empirical results on VTAB-1K, three complete image datasets,\nand two complete video datasets demonstrate that Sparse-Tuning reduces the\nGFLOPs to \\textbf{62\\%-70\\%} of the original ViT-B while achieving\nstate-of-the-art performance. Source code is available at\n\\url{https://github.com/liuting20/Sparse-Tuning}.\n", "link": "http://arxiv.org/abs/2405.14700v1", "date": "2024-05-23", "relevancy": 2.2572, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5859}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5528}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse-Tuning%3A%20Adapting%20Vision%20Transformers%20with%20Efficient%20Fine-tuning%0A%20%20and%20Inference&body=Title%3A%20Sparse-Tuning%3A%20Adapting%20Vision%20Transformers%20with%20Efficient%20Fine-tuning%0A%20%20and%20Inference%0AAuthor%3A%20Ting%20Liu%20and%20Xuyang%20Liu%20and%20Liangtao%20Shi%20and%20Zunnan%20Xu%20and%20Siteng%20Huang%20and%20Yi%20Xin%20and%20Quanjun%20Yin%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20has%20emerged%20as%20a%20popular%20approach%20for%0Aadapting%20pre-trained%20Vision%20Transformer%20%28ViT%29%20models%20to%20downstream%0Aapplications.%20While%20current%20PEFT%20methods%20achieve%20parameter%20efficiency%2C%20they%0Aoverlook%20GPU%20memory%20and%20time%20efficiency%20during%20both%20fine-tuning%20and%20inference%2C%0Adue%20to%20the%20repeated%20computation%20of%20redundant%20tokens%20in%20the%20ViT%20architecture.%0AThis%20falls%20short%20of%20practical%20requirements%20for%20downstream%20task%20adaptation.%20In%0Athis%20paper%2C%20we%20propose%20%5Ctextbf%7BSparse-Tuning%7D%2C%20a%20novel%20tuning%20paradigm%20that%0Asubstantially%20enhances%20both%20fine-tuning%20and%20inference%20efficiency%20for%0Apre-trained%20ViT%20models.%20Sparse-Tuning%20efficiently%20fine-tunes%20the%20pre-trained%0AViT%20by%20sparsely%20preserving%20the%20informative%20tokens%20and%20merging%20redundant%20ones%2C%0Aenabling%20the%20ViT%20to%20focus%20on%20the%20foreground%20while%20reducing%20computational%20costs%0Aon%20background%20regions%20in%20the%20images.%20To%20accurately%20distinguish%20informative%0Atokens%20from%20uninformative%20ones%2C%20we%20introduce%20a%20tailored%20Dense%20Adapter%2C%20which%0Aestablishes%20dense%20connections%20across%20different%20encoder%20layers%20in%20the%20ViT%2C%0Athereby%20enhancing%20the%20representational%20capacity%20and%20quality%20of%20token%0Asparsification.%20Empirical%20results%20on%20VTAB-1K%2C%20three%20complete%20image%20datasets%2C%0Aand%20two%20complete%20video%20datasets%20demonstrate%20that%20Sparse-Tuning%20reduces%20the%0AGFLOPs%20to%20%5Ctextbf%7B62%5C%25-70%5C%25%7D%20of%20the%20original%20ViT-B%20while%20achieving%0Astate-of-the-art%20performance.%20Source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/liuting20/Sparse-Tuning%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse-Tuning%253A%2520Adapting%2520Vision%2520Transformers%2520with%2520Efficient%2520Fine-tuning%250A%2520%2520and%2520Inference%26entry.906535625%3DTing%2520Liu%2520and%2520Xuyang%2520Liu%2520and%2520Liangtao%2520Shi%2520and%2520Zunnan%2520Xu%2520and%2520Siteng%2520Huang%2520and%2520Yi%2520Xin%2520and%2520Quanjun%2520Yin%26entry.1292438233%3D%2520%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520has%2520emerged%2520as%2520a%2520popular%2520approach%2520for%250Aadapting%2520pre-trained%2520Vision%2520Transformer%2520%2528ViT%2529%2520models%2520to%2520downstream%250Aapplications.%2520While%2520current%2520PEFT%2520methods%2520achieve%2520parameter%2520efficiency%252C%2520they%250Aoverlook%2520GPU%2520memory%2520and%2520time%2520efficiency%2520during%2520both%2520fine-tuning%2520and%2520inference%252C%250Adue%2520to%2520the%2520repeated%2520computation%2520of%2520redundant%2520tokens%2520in%2520the%2520ViT%2520architecture.%250AThis%2520falls%2520short%2520of%2520practical%2520requirements%2520for%2520downstream%2520task%2520adaptation.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BSparse-Tuning%257D%252C%2520a%2520novel%2520tuning%2520paradigm%2520that%250Asubstantially%2520enhances%2520both%2520fine-tuning%2520and%2520inference%2520efficiency%2520for%250Apre-trained%2520ViT%2520models.%2520Sparse-Tuning%2520efficiently%2520fine-tunes%2520the%2520pre-trained%250AViT%2520by%2520sparsely%2520preserving%2520the%2520informative%2520tokens%2520and%2520merging%2520redundant%2520ones%252C%250Aenabling%2520the%2520ViT%2520to%2520focus%2520on%2520the%2520foreground%2520while%2520reducing%2520computational%2520costs%250Aon%2520background%2520regions%2520in%2520the%2520images.%2520To%2520accurately%2520distinguish%2520informative%250Atokens%2520from%2520uninformative%2520ones%252C%2520we%2520introduce%2520a%2520tailored%2520Dense%2520Adapter%252C%2520which%250Aestablishes%2520dense%2520connections%2520across%2520different%2520encoder%2520layers%2520in%2520the%2520ViT%252C%250Athereby%2520enhancing%2520the%2520representational%2520capacity%2520and%2520quality%2520of%2520token%250Asparsification.%2520Empirical%2520results%2520on%2520VTAB-1K%252C%2520three%2520complete%2520image%2520datasets%252C%250Aand%2520two%2520complete%2520video%2520datasets%2520demonstrate%2520that%2520Sparse-Tuning%2520reduces%2520the%250AGFLOPs%2520to%2520%255Ctextbf%257B62%255C%2525-70%255C%2525%257D%2520of%2520the%2520original%2520ViT-B%2520while%2520achieving%250Astate-of-the-art%2520performance.%2520Source%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/liuting20/Sparse-Tuning%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse-Tuning%3A%20Adapting%20Vision%20Transformers%20with%20Efficient%20Fine-tuning%0A%20%20and%20Inference&entry.906535625=Ting%20Liu%20and%20Xuyang%20Liu%20and%20Liangtao%20Shi%20and%20Zunnan%20Xu%20and%20Siteng%20Huang%20and%20Yi%20Xin%20and%20Quanjun%20Yin&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20has%20emerged%20as%20a%20popular%20approach%20for%0Aadapting%20pre-trained%20Vision%20Transformer%20%28ViT%29%20models%20to%20downstream%0Aapplications.%20While%20current%20PEFT%20methods%20achieve%20parameter%20efficiency%2C%20they%0Aoverlook%20GPU%20memory%20and%20time%20efficiency%20during%20both%20fine-tuning%20and%20inference%2C%0Adue%20to%20the%20repeated%20computation%20of%20redundant%20tokens%20in%20the%20ViT%20architecture.%0AThis%20falls%20short%20of%20practical%20requirements%20for%20downstream%20task%20adaptation.%20In%0Athis%20paper%2C%20we%20propose%20%5Ctextbf%7BSparse-Tuning%7D%2C%20a%20novel%20tuning%20paradigm%20that%0Asubstantially%20enhances%20both%20fine-tuning%20and%20inference%20efficiency%20for%0Apre-trained%20ViT%20models.%20Sparse-Tuning%20efficiently%20fine-tunes%20the%20pre-trained%0AViT%20by%20sparsely%20preserving%20the%20informative%20tokens%20and%20merging%20redundant%20ones%2C%0Aenabling%20the%20ViT%20to%20focus%20on%20the%20foreground%20while%20reducing%20computational%20costs%0Aon%20background%20regions%20in%20the%20images.%20To%20accurately%20distinguish%20informative%0Atokens%20from%20uninformative%20ones%2C%20we%20introduce%20a%20tailored%20Dense%20Adapter%2C%20which%0Aestablishes%20dense%20connections%20across%20different%20encoder%20layers%20in%20the%20ViT%2C%0Athereby%20enhancing%20the%20representational%20capacity%20and%20quality%20of%20token%0Asparsification.%20Empirical%20results%20on%20VTAB-1K%2C%20three%20complete%20image%20datasets%2C%0Aand%20two%20complete%20video%20datasets%20demonstrate%20that%20Sparse-Tuning%20reduces%20the%0AGFLOPs%20to%20%5Ctextbf%7B62%5C%25-70%5C%25%7D%20of%20the%20original%20ViT-B%20while%20achieving%0Astate-of-the-art%20performance.%20Source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/liuting20/Sparse-Tuning%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14700v1&entry.124074799=Read"},
{"title": "HTN-Based Tutors: A New Intelligent Tutoring Framework Based on\n  Hierarchical Task Networks", "author": "Momin N. Siddiqui and Adit Gupta and Jennifer M. Reddig and Christopher J. Maclellan", "abstract": "  Intelligent tutors have shown success in delivering a personalized and\nadaptive learning experience. However, there exist challenges regarding the\ngranularity of knowledge in existing frameworks and the resulting instructions\nthey can provide. To address these issues, we propose HTN-based tutors, a new\nintelligent tutoring framework that represents expert models using Hierarchical\nTask Networks (HTNs). Like other tutoring frameworks, it allows flexible\nencoding of different problem-solving strategies while providing the additional\nbenefit of a hierarchical knowledge organization. We leverage the latter to\ncreate tutors that can adapt the granularity of their scaffolding. This\norganization also aligns well with the compositional nature of skills.\n", "link": "http://arxiv.org/abs/2405.14716v1", "date": "2024-05-23", "relevancy": 2.2505, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HTN-Based%20Tutors%3A%20A%20New%20Intelligent%20Tutoring%20Framework%20Based%20on%0A%20%20Hierarchical%20Task%20Networks&body=Title%3A%20HTN-Based%20Tutors%3A%20A%20New%20Intelligent%20Tutoring%20Framework%20Based%20on%0A%20%20Hierarchical%20Task%20Networks%0AAuthor%3A%20Momin%20N.%20Siddiqui%20and%20Adit%20Gupta%20and%20Jennifer%20M.%20Reddig%20and%20Christopher%20J.%20Maclellan%0AAbstract%3A%20%20%20Intelligent%20tutors%20have%20shown%20success%20in%20delivering%20a%20personalized%20and%0Aadaptive%20learning%20experience.%20However%2C%20there%20exist%20challenges%20regarding%20the%0Agranularity%20of%20knowledge%20in%20existing%20frameworks%20and%20the%20resulting%20instructions%0Athey%20can%20provide.%20To%20address%20these%20issues%2C%20we%20propose%20HTN-based%20tutors%2C%20a%20new%0Aintelligent%20tutoring%20framework%20that%20represents%20expert%20models%20using%20Hierarchical%0ATask%20Networks%20%28HTNs%29.%20Like%20other%20tutoring%20frameworks%2C%20it%20allows%20flexible%0Aencoding%20of%20different%20problem-solving%20strategies%20while%20providing%20the%20additional%0Abenefit%20of%20a%20hierarchical%20knowledge%20organization.%20We%20leverage%20the%20latter%20to%0Acreate%20tutors%20that%20can%20adapt%20the%20granularity%20of%20their%20scaffolding.%20This%0Aorganization%20also%20aligns%20well%20with%20the%20compositional%20nature%20of%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHTN-Based%2520Tutors%253A%2520A%2520New%2520Intelligent%2520Tutoring%2520Framework%2520Based%2520on%250A%2520%2520Hierarchical%2520Task%2520Networks%26entry.906535625%3DMomin%2520N.%2520Siddiqui%2520and%2520Adit%2520Gupta%2520and%2520Jennifer%2520M.%2520Reddig%2520and%2520Christopher%2520J.%2520Maclellan%26entry.1292438233%3D%2520%2520Intelligent%2520tutors%2520have%2520shown%2520success%2520in%2520delivering%2520a%2520personalized%2520and%250Aadaptive%2520learning%2520experience.%2520However%252C%2520there%2520exist%2520challenges%2520regarding%2520the%250Agranularity%2520of%2520knowledge%2520in%2520existing%2520frameworks%2520and%2520the%2520resulting%2520instructions%250Athey%2520can%2520provide.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520HTN-based%2520tutors%252C%2520a%2520new%250Aintelligent%2520tutoring%2520framework%2520that%2520represents%2520expert%2520models%2520using%2520Hierarchical%250ATask%2520Networks%2520%2528HTNs%2529.%2520Like%2520other%2520tutoring%2520frameworks%252C%2520it%2520allows%2520flexible%250Aencoding%2520of%2520different%2520problem-solving%2520strategies%2520while%2520providing%2520the%2520additional%250Abenefit%2520of%2520a%2520hierarchical%2520knowledge%2520organization.%2520We%2520leverage%2520the%2520latter%2520to%250Acreate%2520tutors%2520that%2520can%2520adapt%2520the%2520granularity%2520of%2520their%2520scaffolding.%2520This%250Aorganization%2520also%2520aligns%2520well%2520with%2520the%2520compositional%2520nature%2520of%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HTN-Based%20Tutors%3A%20A%20New%20Intelligent%20Tutoring%20Framework%20Based%20on%0A%20%20Hierarchical%20Task%20Networks&entry.906535625=Momin%20N.%20Siddiqui%20and%20Adit%20Gupta%20and%20Jennifer%20M.%20Reddig%20and%20Christopher%20J.%20Maclellan&entry.1292438233=%20%20Intelligent%20tutors%20have%20shown%20success%20in%20delivering%20a%20personalized%20and%0Aadaptive%20learning%20experience.%20However%2C%20there%20exist%20challenges%20regarding%20the%0Agranularity%20of%20knowledge%20in%20existing%20frameworks%20and%20the%20resulting%20instructions%0Athey%20can%20provide.%20To%20address%20these%20issues%2C%20we%20propose%20HTN-based%20tutors%2C%20a%20new%0Aintelligent%20tutoring%20framework%20that%20represents%20expert%20models%20using%20Hierarchical%0ATask%20Networks%20%28HTNs%29.%20Like%20other%20tutoring%20frameworks%2C%20it%20allows%20flexible%0Aencoding%20of%20different%20problem-solving%20strategies%20while%20providing%20the%20additional%0Abenefit%20of%20a%20hierarchical%20knowledge%20organization.%20We%20leverage%20the%20latter%20to%0Acreate%20tutors%20that%20can%20adapt%20the%20granularity%20of%20their%20scaffolding.%20This%0Aorganization%20also%20aligns%20well%20with%20the%20compositional%20nature%20of%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14716v1&entry.124074799=Read"},
{"title": "Efficiency for Free: Ideal Data Are Transportable Representations", "author": "Peng Sun and Yi Jiang and Tao Lin", "abstract": "  Data, the seminal opportunity and challenge in modern machine learning,\ncurrently constrains the scalability of representation learning and impedes the\npace of model evolution. Existing paradigms tackle the issue of learning\nefficiency over massive datasets from the perspective of self-supervised\nlearning and dataset distillation independently, while neglecting the untapped\npotential of accelerating representation learning from an intermediate\nstandpoint. In this work, we delve into defining the ideal data properties from\nboth optimization and generalization perspectives. We propose that\nmodel-generated representations, despite being trained on diverse tasks and\narchitectures, converge to a shared linear space, facilitating effective linear\ntransport between models. Furthermore, we demonstrate that these\nrepresentations exhibit properties conducive to the formation of ideal data.\nThe theoretical/empirical insights therein inspire us to propose a\nRepresentation Learning Accelerator (ReLA), which leverages a task- and\narchitecture-agnostic, yet publicly available, free model to form a dynamic\ndata subset and thus accelerate (self-)supervised learning. For instance,\nemploying a CLIP ViT B/16 as a prior model for dynamic data generation,\nReLA-aided BYOL can train a ResNet-50 from scratch with 50% of ImageNet-1K,\nyielding performance surpassing that of training on the full dataset.\nAdditionally, employing a ResNet-18 pre-trained on CIFAR-10 can enhance\nResNet-50 training on 10% of ImageNet-1K, resulting in a 7.7% increase in\naccuracy.\n", "link": "http://arxiv.org/abs/2405.14669v1", "date": "2024-05-23", "relevancy": 2.2475, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5743}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.557}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations&body=Title%3A%20Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations%0AAuthor%3A%20Peng%20Sun%20and%20Yi%20Jiang%20and%20Tao%20Lin%0AAbstract%3A%20%20%20Data%2C%20the%20seminal%20opportunity%20and%20challenge%20in%20modern%20machine%20learning%2C%0Acurrently%20constrains%20the%20scalability%20of%20representation%20learning%20and%20impedes%20the%0Apace%20of%20model%20evolution.%20Existing%20paradigms%20tackle%20the%20issue%20of%20learning%0Aefficiency%20over%20massive%20datasets%20from%20the%20perspective%20of%20self-supervised%0Alearning%20and%20dataset%20distillation%20independently%2C%20while%20neglecting%20the%20untapped%0Apotential%20of%20accelerating%20representation%20learning%20from%20an%20intermediate%0Astandpoint.%20In%20this%20work%2C%20we%20delve%20into%20defining%20the%20ideal%20data%20properties%20from%0Aboth%20optimization%20and%20generalization%20perspectives.%20We%20propose%20that%0Amodel-generated%20representations%2C%20despite%20being%20trained%20on%20diverse%20tasks%20and%0Aarchitectures%2C%20converge%20to%20a%20shared%20linear%20space%2C%20facilitating%20effective%20linear%0Atransport%20between%20models.%20Furthermore%2C%20we%20demonstrate%20that%20these%0Arepresentations%20exhibit%20properties%20conducive%20to%20the%20formation%20of%20ideal%20data.%0AThe%20theoretical/empirical%20insights%20therein%20inspire%20us%20to%20propose%20a%0ARepresentation%20Learning%20Accelerator%20%28ReLA%29%2C%20which%20leverages%20a%20task-%20and%0Aarchitecture-agnostic%2C%20yet%20publicly%20available%2C%20free%20model%20to%20form%20a%20dynamic%0Adata%20subset%20and%20thus%20accelerate%20%28self-%29supervised%20learning.%20For%20instance%2C%0Aemploying%20a%20CLIP%20ViT%20B/16%20as%20a%20prior%20model%20for%20dynamic%20data%20generation%2C%0AReLA-aided%20BYOL%20can%20train%20a%20ResNet-50%20from%20scratch%20with%2050%25%20of%20ImageNet-1K%2C%0Ayielding%20performance%20surpassing%20that%20of%20training%20on%20the%20full%20dataset.%0AAdditionally%2C%20employing%20a%20ResNet-18%20pre-trained%20on%20CIFAR-10%20can%20enhance%0AResNet-50%20training%20on%2010%25%20of%20ImageNet-1K%2C%20resulting%20in%20a%207.7%25%20increase%20in%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiency%2520for%2520Free%253A%2520Ideal%2520Data%2520Are%2520Transportable%2520Representations%26entry.906535625%3DPeng%2520Sun%2520and%2520Yi%2520Jiang%2520and%2520Tao%2520Lin%26entry.1292438233%3D%2520%2520Data%252C%2520the%2520seminal%2520opportunity%2520and%2520challenge%2520in%2520modern%2520machine%2520learning%252C%250Acurrently%2520constrains%2520the%2520scalability%2520of%2520representation%2520learning%2520and%2520impedes%2520the%250Apace%2520of%2520model%2520evolution.%2520Existing%2520paradigms%2520tackle%2520the%2520issue%2520of%2520learning%250Aefficiency%2520over%2520massive%2520datasets%2520from%2520the%2520perspective%2520of%2520self-supervised%250Alearning%2520and%2520dataset%2520distillation%2520independently%252C%2520while%2520neglecting%2520the%2520untapped%250Apotential%2520of%2520accelerating%2520representation%2520learning%2520from%2520an%2520intermediate%250Astandpoint.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520defining%2520the%2520ideal%2520data%2520properties%2520from%250Aboth%2520optimization%2520and%2520generalization%2520perspectives.%2520We%2520propose%2520that%250Amodel-generated%2520representations%252C%2520despite%2520being%2520trained%2520on%2520diverse%2520tasks%2520and%250Aarchitectures%252C%2520converge%2520to%2520a%2520shared%2520linear%2520space%252C%2520facilitating%2520effective%2520linear%250Atransport%2520between%2520models.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520these%250Arepresentations%2520exhibit%2520properties%2520conducive%2520to%2520the%2520formation%2520of%2520ideal%2520data.%250AThe%2520theoretical/empirical%2520insights%2520therein%2520inspire%2520us%2520to%2520propose%2520a%250ARepresentation%2520Learning%2520Accelerator%2520%2528ReLA%2529%252C%2520which%2520leverages%2520a%2520task-%2520and%250Aarchitecture-agnostic%252C%2520yet%2520publicly%2520available%252C%2520free%2520model%2520to%2520form%2520a%2520dynamic%250Adata%2520subset%2520and%2520thus%2520accelerate%2520%2528self-%2529supervised%2520learning.%2520For%2520instance%252C%250Aemploying%2520a%2520CLIP%2520ViT%2520B/16%2520as%2520a%2520prior%2520model%2520for%2520dynamic%2520data%2520generation%252C%250AReLA-aided%2520BYOL%2520can%2520train%2520a%2520ResNet-50%2520from%2520scratch%2520with%252050%2525%2520of%2520ImageNet-1K%252C%250Ayielding%2520performance%2520surpassing%2520that%2520of%2520training%2520on%2520the%2520full%2520dataset.%250AAdditionally%252C%2520employing%2520a%2520ResNet-18%2520pre-trained%2520on%2520CIFAR-10%2520can%2520enhance%250AResNet-50%2520training%2520on%252010%2525%2520of%2520ImageNet-1K%252C%2520resulting%2520in%2520a%25207.7%2525%2520increase%2520in%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiency%20for%20Free%3A%20Ideal%20Data%20Are%20Transportable%20Representations&entry.906535625=Peng%20Sun%20and%20Yi%20Jiang%20and%20Tao%20Lin&entry.1292438233=%20%20Data%2C%20the%20seminal%20opportunity%20and%20challenge%20in%20modern%20machine%20learning%2C%0Acurrently%20constrains%20the%20scalability%20of%20representation%20learning%20and%20impedes%20the%0Apace%20of%20model%20evolution.%20Existing%20paradigms%20tackle%20the%20issue%20of%20learning%0Aefficiency%20over%20massive%20datasets%20from%20the%20perspective%20of%20self-supervised%0Alearning%20and%20dataset%20distillation%20independently%2C%20while%20neglecting%20the%20untapped%0Apotential%20of%20accelerating%20representation%20learning%20from%20an%20intermediate%0Astandpoint.%20In%20this%20work%2C%20we%20delve%20into%20defining%20the%20ideal%20data%20properties%20from%0Aboth%20optimization%20and%20generalization%20perspectives.%20We%20propose%20that%0Amodel-generated%20representations%2C%20despite%20being%20trained%20on%20diverse%20tasks%20and%0Aarchitectures%2C%20converge%20to%20a%20shared%20linear%20space%2C%20facilitating%20effective%20linear%0Atransport%20between%20models.%20Furthermore%2C%20we%20demonstrate%20that%20these%0Arepresentations%20exhibit%20properties%20conducive%20to%20the%20formation%20of%20ideal%20data.%0AThe%20theoretical/empirical%20insights%20therein%20inspire%20us%20to%20propose%20a%0ARepresentation%20Learning%20Accelerator%20%28ReLA%29%2C%20which%20leverages%20a%20task-%20and%0Aarchitecture-agnostic%2C%20yet%20publicly%20available%2C%20free%20model%20to%20form%20a%20dynamic%0Adata%20subset%20and%20thus%20accelerate%20%28self-%29supervised%20learning.%20For%20instance%2C%0Aemploying%20a%20CLIP%20ViT%20B/16%20as%20a%20prior%20model%20for%20dynamic%20data%20generation%2C%0AReLA-aided%20BYOL%20can%20train%20a%20ResNet-50%20from%20scratch%20with%2050%25%20of%20ImageNet-1K%2C%0Ayielding%20performance%20surpassing%20that%20of%20training%20on%20the%20full%20dataset.%0AAdditionally%2C%20employing%20a%20ResNet-18%20pre-trained%20on%20CIFAR-10%20can%20enhance%0AResNet-50%20training%20on%2010%25%20of%20ImageNet-1K%2C%20resulting%20in%20a%207.7%25%20increase%20in%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14669v1&entry.124074799=Read"},
{"title": "JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field\n  Representation and Compression", "author": "Zihan Zheng and Houqiang Zhong and Qiang Hu and Xiaoyun Zhang and Li Song and Ya Zhang and Yanfeng Wang", "abstract": "  Neural Radiance Field (NeRF) excels in photo-realistically static scenes,\ninspiring numerous efforts to facilitate volumetric videos. However, rendering\ndynamic and long-sequence radiance fields remains challenging due to the\nsignificant data required to represent volumetric videos. In this paper, we\npropose a novel end-to-end joint optimization scheme of dynamic NeRF\nrepresentation and compression, called JointRF, thus achieving significantly\nimproved quality and compression efficiency against the previous methods.\nSpecifically, JointRF employs a compact residual feature grid and a coefficient\nfeature grid to represent the dynamic NeRF. This representation handles large\nmotions without compromising quality while concurrently diminishing temporal\nredundancy. We also introduce a sequential feature compression subnetwork to\nfurther reduce spatial-temporal redundancy. Finally, the representation and\ncompression subnetworks are end-to-end trained combined within the JointRF.\nExtensive experiments demonstrate that JointRF can achieve superior compression\nperformance across various datasets.\n", "link": "http://arxiv.org/abs/2405.14452v1", "date": "2024-05-23", "relevancy": 2.2455, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5798}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5488}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointRF%3A%20End-to-End%20Joint%20Optimization%20for%20Dynamic%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Compression&body=Title%3A%20JointRF%3A%20End-to-End%20Joint%20Optimization%20for%20Dynamic%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Compression%0AAuthor%3A%20Zihan%20Zheng%20and%20Houqiang%20Zhong%20and%20Qiang%20Hu%20and%20Xiaoyun%20Zhang%20and%20Li%20Song%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Neural%20Radiance%20Field%20%28NeRF%29%20excels%20in%20photo-realistically%20static%20scenes%2C%0Ainspiring%20numerous%20efforts%20to%20facilitate%20volumetric%20videos.%20However%2C%20rendering%0Adynamic%20and%20long-sequence%20radiance%20fields%20remains%20challenging%20due%20to%20the%0Asignificant%20data%20required%20to%20represent%20volumetric%20videos.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20end-to-end%20joint%20optimization%20scheme%20of%20dynamic%20NeRF%0Arepresentation%20and%20compression%2C%20called%20JointRF%2C%20thus%20achieving%20significantly%0Aimproved%20quality%20and%20compression%20efficiency%20against%20the%20previous%20methods.%0ASpecifically%2C%20JointRF%20employs%20a%20compact%20residual%20feature%20grid%20and%20a%20coefficient%0Afeature%20grid%20to%20represent%20the%20dynamic%20NeRF.%20This%20representation%20handles%20large%0Amotions%20without%20compromising%20quality%20while%20concurrently%20diminishing%20temporal%0Aredundancy.%20We%20also%20introduce%20a%20sequential%20feature%20compression%20subnetwork%20to%0Afurther%20reduce%20spatial-temporal%20redundancy.%20Finally%2C%20the%20representation%20and%0Acompression%20subnetworks%20are%20end-to-end%20trained%20combined%20within%20the%20JointRF.%0AExtensive%20experiments%20demonstrate%20that%20JointRF%20can%20achieve%20superior%20compression%0Aperformance%20across%20various%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointRF%253A%2520End-to-End%2520Joint%2520Optimization%2520for%2520Dynamic%2520Neural%2520Radiance%2520Field%250A%2520%2520Representation%2520and%2520Compression%26entry.906535625%3DZihan%2520Zheng%2520and%2520Houqiang%2520Zhong%2520and%2520Qiang%2520Hu%2520and%2520Xiaoyun%2520Zhang%2520and%2520Li%2520Song%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520excels%2520in%2520photo-realistically%2520static%2520scenes%252C%250Ainspiring%2520numerous%2520efforts%2520to%2520facilitate%2520volumetric%2520videos.%2520However%252C%2520rendering%250Adynamic%2520and%2520long-sequence%2520radiance%2520fields%2520remains%2520challenging%2520due%2520to%2520the%250Asignificant%2520data%2520required%2520to%2520represent%2520volumetric%2520videos.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520end-to-end%2520joint%2520optimization%2520scheme%2520of%2520dynamic%2520NeRF%250Arepresentation%2520and%2520compression%252C%2520called%2520JointRF%252C%2520thus%2520achieving%2520significantly%250Aimproved%2520quality%2520and%2520compression%2520efficiency%2520against%2520the%2520previous%2520methods.%250ASpecifically%252C%2520JointRF%2520employs%2520a%2520compact%2520residual%2520feature%2520grid%2520and%2520a%2520coefficient%250Afeature%2520grid%2520to%2520represent%2520the%2520dynamic%2520NeRF.%2520This%2520representation%2520handles%2520large%250Amotions%2520without%2520compromising%2520quality%2520while%2520concurrently%2520diminishing%2520temporal%250Aredundancy.%2520We%2520also%2520introduce%2520a%2520sequential%2520feature%2520compression%2520subnetwork%2520to%250Afurther%2520reduce%2520spatial-temporal%2520redundancy.%2520Finally%252C%2520the%2520representation%2520and%250Acompression%2520subnetworks%2520are%2520end-to-end%2520trained%2520combined%2520within%2520the%2520JointRF.%250AExtensive%2520experiments%2520demonstrate%2520that%2520JointRF%2520can%2520achieve%2520superior%2520compression%250Aperformance%2520across%2520various%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointRF%3A%20End-to-End%20Joint%20Optimization%20for%20Dynamic%20Neural%20Radiance%20Field%0A%20%20Representation%20and%20Compression&entry.906535625=Zihan%20Zheng%20and%20Houqiang%20Zhong%20and%20Qiang%20Hu%20and%20Xiaoyun%20Zhang%20and%20Li%20Song%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Neural%20Radiance%20Field%20%28NeRF%29%20excels%20in%20photo-realistically%20static%20scenes%2C%0Ainspiring%20numerous%20efforts%20to%20facilitate%20volumetric%20videos.%20However%2C%20rendering%0Adynamic%20and%20long-sequence%20radiance%20fields%20remains%20challenging%20due%20to%20the%0Asignificant%20data%20required%20to%20represent%20volumetric%20videos.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20end-to-end%20joint%20optimization%20scheme%20of%20dynamic%20NeRF%0Arepresentation%20and%20compression%2C%20called%20JointRF%2C%20thus%20achieving%20significantly%0Aimproved%20quality%20and%20compression%20efficiency%20against%20the%20previous%20methods.%0ASpecifically%2C%20JointRF%20employs%20a%20compact%20residual%20feature%20grid%20and%20a%20coefficient%0Afeature%20grid%20to%20represent%20the%20dynamic%20NeRF.%20This%20representation%20handles%20large%0Amotions%20without%20compromising%20quality%20while%20concurrently%20diminishing%20temporal%0Aredundancy.%20We%20also%20introduce%20a%20sequential%20feature%20compression%20subnetwork%20to%0Afurther%20reduce%20spatial-temporal%20redundancy.%20Finally%2C%20the%20representation%20and%0Acompression%20subnetworks%20are%20end-to-end%20trained%20combined%20within%20the%20JointRF.%0AExtensive%20experiments%20demonstrate%20that%20JointRF%20can%20achieve%20superior%20compression%0Aperformance%20across%20various%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14452v1&entry.124074799=Read"},
{"title": "Instruction Tuning With Loss Over Instructions", "author": "Zhengyan Shi and Adam X. Yang and Bin Wu and Laurence Aitchison and Emine Yilmaz and Aldo Lipani", "abstract": "  Instruction tuning plays a crucial role in shaping the outputs of language\nmodels (LMs) to desired styles. In this work, we propose a simple yet effective\nmethod, Instruction Modelling (IM), which trains LMs by applying a loss\nfunction to the instruction and prompt part rather than solely to the output\npart. Through experiments across 21 diverse benchmarks, we show that, in many\nscenarios, IM can effectively improve the LM performance on both NLP tasks\n(e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks\n(e.g., MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM\nboosts model performance on AlpacaEval 1.0 by over 100%. We identify two key\nfactors influencing the effectiveness of IM: (1) The ratio between instruction\nlength and output length in the training data; and (2) The number of training\nexamples. We observe that IM is especially beneficial when trained on datasets\nwith lengthy instructions paired with brief outputs, or under the Superficial\nAlignment Hypothesis (SAH) where a small amount of training examples are used\nfor instruction tuning. Further analysis substantiates our hypothesis that the\nimprovement can be attributed to reduced overfitting to instruction tuning\ndatasets. Our work provides practical guidance for instruction tuning LMs,\nespecially in low-resource scenarios.\n", "link": "http://arxiv.org/abs/2405.14394v1", "date": "2024-05-23", "relevancy": 2.2349, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4476}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction%20Tuning%20With%20Loss%20Over%20Instructions&body=Title%3A%20Instruction%20Tuning%20With%20Loss%20Over%20Instructions%0AAuthor%3A%20Zhengyan%20Shi%20and%20Adam%20X.%20Yang%20and%20Bin%20Wu%20and%20Laurence%20Aitchison%20and%20Emine%20Yilmaz%20and%20Aldo%20Lipani%0AAbstract%3A%20%20%20Instruction%20tuning%20plays%20a%20crucial%20role%20in%20shaping%20the%20outputs%20of%20language%0Amodels%20%28LMs%29%20to%20desired%20styles.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Amethod%2C%20Instruction%20Modelling%20%28IM%29%2C%20which%20trains%20LMs%20by%20applying%20a%20loss%0Afunction%20to%20the%20instruction%20and%20prompt%20part%20rather%20than%20solely%20to%20the%20output%0Apart.%20Through%20experiments%20across%2021%20diverse%20benchmarks%2C%20we%20show%20that%2C%20in%20many%0Ascenarios%2C%20IM%20can%20effectively%20improve%20the%20LM%20performance%20on%20both%20NLP%20tasks%0A%28e.g.%2C%20MMLU%2C%20TruthfulQA%2C%20and%20HumanEval%29%20and%20open-ended%20generation%20benchmarks%0A%28e.g.%2C%20MT-Bench%20and%20AlpacaEval%29.%20Remarkably%2C%20in%20the%20most%20advantageous%20case%2C%20IM%0Aboosts%20model%20performance%20on%20AlpacaEval%201.0%20by%20over%20100%25.%20We%20identify%20two%20key%0Afactors%20influencing%20the%20effectiveness%20of%20IM%3A%20%281%29%20The%20ratio%20between%20instruction%0Alength%20and%20output%20length%20in%20the%20training%20data%3B%20and%20%282%29%20The%20number%20of%20training%0Aexamples.%20We%20observe%20that%20IM%20is%20especially%20beneficial%20when%20trained%20on%20datasets%0Awith%20lengthy%20instructions%20paired%20with%20brief%20outputs%2C%20or%20under%20the%20Superficial%0AAlignment%20Hypothesis%20%28SAH%29%20where%20a%20small%20amount%20of%20training%20examples%20are%20used%0Afor%20instruction%20tuning.%20Further%20analysis%20substantiates%20our%20hypothesis%20that%20the%0Aimprovement%20can%20be%20attributed%20to%20reduced%20overfitting%20to%20instruction%20tuning%0Adatasets.%20Our%20work%20provides%20practical%20guidance%20for%20instruction%20tuning%20LMs%2C%0Aespecially%20in%20low-resource%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction%2520Tuning%2520With%2520Loss%2520Over%2520Instructions%26entry.906535625%3DZhengyan%2520Shi%2520and%2520Adam%2520X.%2520Yang%2520and%2520Bin%2520Wu%2520and%2520Laurence%2520Aitchison%2520and%2520Emine%2520Yilmaz%2520and%2520Aldo%2520Lipani%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520plays%2520a%2520crucial%2520role%2520in%2520shaping%2520the%2520outputs%2520of%2520language%250Amodels%2520%2528LMs%2529%2520to%2520desired%2520styles.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250Amethod%252C%2520Instruction%2520Modelling%2520%2528IM%2529%252C%2520which%2520trains%2520LMs%2520by%2520applying%2520a%2520loss%250Afunction%2520to%2520the%2520instruction%2520and%2520prompt%2520part%2520rather%2520than%2520solely%2520to%2520the%2520output%250Apart.%2520Through%2520experiments%2520across%252021%2520diverse%2520benchmarks%252C%2520we%2520show%2520that%252C%2520in%2520many%250Ascenarios%252C%2520IM%2520can%2520effectively%2520improve%2520the%2520LM%2520performance%2520on%2520both%2520NLP%2520tasks%250A%2528e.g.%252C%2520MMLU%252C%2520TruthfulQA%252C%2520and%2520HumanEval%2529%2520and%2520open-ended%2520generation%2520benchmarks%250A%2528e.g.%252C%2520MT-Bench%2520and%2520AlpacaEval%2529.%2520Remarkably%252C%2520in%2520the%2520most%2520advantageous%2520case%252C%2520IM%250Aboosts%2520model%2520performance%2520on%2520AlpacaEval%25201.0%2520by%2520over%2520100%2525.%2520We%2520identify%2520two%2520key%250Afactors%2520influencing%2520the%2520effectiveness%2520of%2520IM%253A%2520%25281%2529%2520The%2520ratio%2520between%2520instruction%250Alength%2520and%2520output%2520length%2520in%2520the%2520training%2520data%253B%2520and%2520%25282%2529%2520The%2520number%2520of%2520training%250Aexamples.%2520We%2520observe%2520that%2520IM%2520is%2520especially%2520beneficial%2520when%2520trained%2520on%2520datasets%250Awith%2520lengthy%2520instructions%2520paired%2520with%2520brief%2520outputs%252C%2520or%2520under%2520the%2520Superficial%250AAlignment%2520Hypothesis%2520%2528SAH%2529%2520where%2520a%2520small%2520amount%2520of%2520training%2520examples%2520are%2520used%250Afor%2520instruction%2520tuning.%2520Further%2520analysis%2520substantiates%2520our%2520hypothesis%2520that%2520the%250Aimprovement%2520can%2520be%2520attributed%2520to%2520reduced%2520overfitting%2520to%2520instruction%2520tuning%250Adatasets.%2520Our%2520work%2520provides%2520practical%2520guidance%2520for%2520instruction%2520tuning%2520LMs%252C%250Aespecially%2520in%2520low-resource%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction%20Tuning%20With%20Loss%20Over%20Instructions&entry.906535625=Zhengyan%20Shi%20and%20Adam%20X.%20Yang%20and%20Bin%20Wu%20and%20Laurence%20Aitchison%20and%20Emine%20Yilmaz%20and%20Aldo%20Lipani&entry.1292438233=%20%20Instruction%20tuning%20plays%20a%20crucial%20role%20in%20shaping%20the%20outputs%20of%20language%0Amodels%20%28LMs%29%20to%20desired%20styles.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%20effective%0Amethod%2C%20Instruction%20Modelling%20%28IM%29%2C%20which%20trains%20LMs%20by%20applying%20a%20loss%0Afunction%20to%20the%20instruction%20and%20prompt%20part%20rather%20than%20solely%20to%20the%20output%0Apart.%20Through%20experiments%20across%2021%20diverse%20benchmarks%2C%20we%20show%20that%2C%20in%20many%0Ascenarios%2C%20IM%20can%20effectively%20improve%20the%20LM%20performance%20on%20both%20NLP%20tasks%0A%28e.g.%2C%20MMLU%2C%20TruthfulQA%2C%20and%20HumanEval%29%20and%20open-ended%20generation%20benchmarks%0A%28e.g.%2C%20MT-Bench%20and%20AlpacaEval%29.%20Remarkably%2C%20in%20the%20most%20advantageous%20case%2C%20IM%0Aboosts%20model%20performance%20on%20AlpacaEval%201.0%20by%20over%20100%25.%20We%20identify%20two%20key%0Afactors%20influencing%20the%20effectiveness%20of%20IM%3A%20%281%29%20The%20ratio%20between%20instruction%0Alength%20and%20output%20length%20in%20the%20training%20data%3B%20and%20%282%29%20The%20number%20of%20training%0Aexamples.%20We%20observe%20that%20IM%20is%20especially%20beneficial%20when%20trained%20on%20datasets%0Awith%20lengthy%20instructions%20paired%20with%20brief%20outputs%2C%20or%20under%20the%20Superficial%0AAlignment%20Hypothesis%20%28SAH%29%20where%20a%20small%20amount%20of%20training%20examples%20are%20used%0Afor%20instruction%20tuning.%20Further%20analysis%20substantiates%20our%20hypothesis%20that%20the%0Aimprovement%20can%20be%20attributed%20to%20reduced%20overfitting%20to%20instruction%20tuning%0Adatasets.%20Our%20work%20provides%20practical%20guidance%20for%20instruction%20tuning%20LMs%2C%0Aespecially%20in%20low-resource%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14394v1&entry.124074799=Read"},
{"title": "Sketch-and-Project Meets Newton Method: Global $\\mathcal O(k^{-2})$\n  Convergence with Low-Rank Updates", "author": "Slavom\u00edr Hanzely", "abstract": "  In this paper, we propose the first sketch-and-project Newton method with\nfast $\\mathcal O(k^{-2})$ global convergence rate for self-concordant\nfunctions. Our method, SGN, can be viewed in three ways: i) as a\nsketch-and-project algorithm projecting updates of Newton method, ii) as a\ncubically regularized Newton ethod in sketched subspaces, and iii) as a damped\nNewton method in sketched subspaces. SGN inherits best of all three worlds:\ncheap iteration costs of sketch-and-project methods, state-of-the-art $\\mathcal\nO(k^{-2})$ global convergence rate of full-rank Newton-like methods and the\nalgorithm simplicity of damped Newton methods. Finally, we demonstrate its\ncomparable empirical performance to baseline algorithms.\n", "link": "http://arxiv.org/abs/2305.13082v3", "date": "2024-05-23", "relevancy": 2.2272, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.466}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4604}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates&body=Title%3A%20Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates%0AAuthor%3A%20Slavom%C3%ADr%20Hanzely%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20sketch-and-project%20Newton%20method%20with%0Afast%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20for%20self-concordant%0Afunctions.%20Our%20method%2C%20SGN%2C%20can%20be%20viewed%20in%20three%20ways%3A%20i%29%20as%20a%0Asketch-and-project%20algorithm%20projecting%20updates%20of%20Newton%20method%2C%20ii%29%20as%20a%0Acubically%20regularized%20Newton%20ethod%20in%20sketched%20subspaces%2C%20and%20iii%29%20as%20a%20damped%0ANewton%20method%20in%20sketched%20subspaces.%20SGN%20inherits%20best%20of%20all%20three%20worlds%3A%0Acheap%20iteration%20costs%20of%20sketch-and-project%20methods%2C%20state-of-the-art%20%24%5Cmathcal%0AO%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20of%20full-rank%20Newton-like%20methods%20and%20the%0Aalgorithm%20simplicity%20of%20damped%20Newton%20methods.%20Finally%2C%20we%20demonstrate%20its%0Acomparable%20empirical%20performance%20to%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch-and-Project%2520Meets%2520Newton%2520Method%253A%2520Global%2520%2524%255Cmathcal%2520O%2528k%255E%257B-2%257D%2529%2524%250A%2520%2520Convergence%2520with%2520Low-Rank%2520Updates%26entry.906535625%3DSlavom%25C3%25ADr%2520Hanzely%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520first%2520sketch-and-project%2520Newton%2520method%2520with%250Afast%2520%2524%255Cmathcal%2520O%2528k%255E%257B-2%257D%2529%2524%2520global%2520convergence%2520rate%2520for%2520self-concordant%250Afunctions.%2520Our%2520method%252C%2520SGN%252C%2520can%2520be%2520viewed%2520in%2520three%2520ways%253A%2520i%2529%2520as%2520a%250Asketch-and-project%2520algorithm%2520projecting%2520updates%2520of%2520Newton%2520method%252C%2520ii%2529%2520as%2520a%250Acubically%2520regularized%2520Newton%2520ethod%2520in%2520sketched%2520subspaces%252C%2520and%2520iii%2529%2520as%2520a%2520damped%250ANewton%2520method%2520in%2520sketched%2520subspaces.%2520SGN%2520inherits%2520best%2520of%2520all%2520three%2520worlds%253A%250Acheap%2520iteration%2520costs%2520of%2520sketch-and-project%2520methods%252C%2520state-of-the-art%2520%2524%255Cmathcal%250AO%2528k%255E%257B-2%257D%2529%2524%2520global%2520convergence%2520rate%2520of%2520full-rank%2520Newton-like%2520methods%2520and%2520the%250Aalgorithm%2520simplicity%2520of%2520damped%2520Newton%2520methods.%2520Finally%252C%2520we%2520demonstrate%2520its%250Acomparable%2520empirical%2520performance%2520to%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch-and-Project%20Meets%20Newton%20Method%3A%20Global%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%0A%20%20Convergence%20with%20Low-Rank%20Updates&entry.906535625=Slavom%C3%ADr%20Hanzely&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20the%20first%20sketch-and-project%20Newton%20method%20with%0Afast%20%24%5Cmathcal%20O%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20for%20self-concordant%0Afunctions.%20Our%20method%2C%20SGN%2C%20can%20be%20viewed%20in%20three%20ways%3A%20i%29%20as%20a%0Asketch-and-project%20algorithm%20projecting%20updates%20of%20Newton%20method%2C%20ii%29%20as%20a%0Acubically%20regularized%20Newton%20ethod%20in%20sketched%20subspaces%2C%20and%20iii%29%20as%20a%20damped%0ANewton%20method%20in%20sketched%20subspaces.%20SGN%20inherits%20best%20of%20all%20three%20worlds%3A%0Acheap%20iteration%20costs%20of%20sketch-and-project%20methods%2C%20state-of-the-art%20%24%5Cmathcal%0AO%28k%5E%7B-2%7D%29%24%20global%20convergence%20rate%20of%20full-rank%20Newton-like%20methods%20and%20the%0Aalgorithm%20simplicity%20of%20damped%20Newton%20methods.%20Finally%2C%20we%20demonstrate%20its%0Acomparable%20empirical%20performance%20to%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13082v3&entry.124074799=Read"},
{"title": "LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent\n  Diffusion Models", "author": "Seyedmorteza Sadat and Jakob Buhmann and Derek Bradley and Otmar Hilliges and Romann M. Weber", "abstract": "  Advances in latent diffusion models (LDMs) have revolutionized\nhigh-resolution image generation, but the design space of the autoencoder that\nis central to these systems remains underexplored. In this paper, we introduce\nLiteVAE, a family of autoencoders for LDMs that leverage the 2D discrete\nwavelet transform to enhance scalability and computational efficiency over\nstandard variational autoencoders (VAEs) with no sacrifice in output quality.\nWe also investigate the training methodologies and the decoder architecture of\nLiteVAE and propose several enhancements that improve the training dynamics and\nreconstruction quality. Our base LiteVAE model matches the quality of the\nestablished VAEs in current LDMs with a six-fold reduction in encoder\nparameters, leading to faster training and lower GPU memory requirements, while\nour larger model outperforms VAEs of comparable complexity across all evaluated\nmetrics (rFID, LPIPS, PSNR, and SSIM).\n", "link": "http://arxiv.org/abs/2405.14477v1", "date": "2024-05-23", "relevancy": 2.2091, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6367}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5356}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models&body=Title%3A%20LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models%0AAuthor%3A%20Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber%0AAbstract%3A%20%20%20Advances%20in%20latent%20diffusion%20models%20%28LDMs%29%20have%20revolutionized%0Ahigh-resolution%20image%20generation%2C%20but%20the%20design%20space%20of%20the%20autoencoder%20that%0Ais%20central%20to%20these%20systems%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%0ALiteVAE%2C%20a%20family%20of%20autoencoders%20for%20LDMs%20that%20leverage%20the%202D%20discrete%0Awavelet%20transform%20to%20enhance%20scalability%20and%20computational%20efficiency%20over%0Astandard%20variational%20autoencoders%20%28VAEs%29%20with%20no%20sacrifice%20in%20output%20quality.%0AWe%20also%20investigate%20the%20training%20methodologies%20and%20the%20decoder%20architecture%20of%0ALiteVAE%20and%20propose%20several%20enhancements%20that%20improve%20the%20training%20dynamics%20and%0Areconstruction%20quality.%20Our%20base%20LiteVAE%20model%20matches%20the%20quality%20of%20the%0Aestablished%20VAEs%20in%20current%20LDMs%20with%20a%20six-fold%20reduction%20in%20encoder%0Aparameters%2C%20leading%20to%20faster%20training%20and%20lower%20GPU%20memory%20requirements%2C%20while%0Aour%20larger%20model%20outperforms%20VAEs%20of%20comparable%20complexity%20across%20all%20evaluated%0Ametrics%20%28rFID%2C%20LPIPS%2C%20PSNR%2C%20and%20SSIM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteVAE%253A%2520Lightweight%2520and%2520Efficient%2520Variational%2520Autoencoders%2520for%2520Latent%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DSeyedmorteza%2520Sadat%2520and%2520Jakob%2520Buhmann%2520and%2520Derek%2520Bradley%2520and%2520Otmar%2520Hilliges%2520and%2520Romann%2520M.%2520Weber%26entry.1292438233%3D%2520%2520Advances%2520in%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520have%2520revolutionized%250Ahigh-resolution%2520image%2520generation%252C%2520but%2520the%2520design%2520space%2520of%2520the%2520autoencoder%2520that%250Ais%2520central%2520to%2520these%2520systems%2520remains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALiteVAE%252C%2520a%2520family%2520of%2520autoencoders%2520for%2520LDMs%2520that%2520leverage%2520the%25202D%2520discrete%250Awavelet%2520transform%2520to%2520enhance%2520scalability%2520and%2520computational%2520efficiency%2520over%250Astandard%2520variational%2520autoencoders%2520%2528VAEs%2529%2520with%2520no%2520sacrifice%2520in%2520output%2520quality.%250AWe%2520also%2520investigate%2520the%2520training%2520methodologies%2520and%2520the%2520decoder%2520architecture%2520of%250ALiteVAE%2520and%2520propose%2520several%2520enhancements%2520that%2520improve%2520the%2520training%2520dynamics%2520and%250Areconstruction%2520quality.%2520Our%2520base%2520LiteVAE%2520model%2520matches%2520the%2520quality%2520of%2520the%250Aestablished%2520VAEs%2520in%2520current%2520LDMs%2520with%2520a%2520six-fold%2520reduction%2520in%2520encoder%250Aparameters%252C%2520leading%2520to%2520faster%2520training%2520and%2520lower%2520GPU%2520memory%2520requirements%252C%2520while%250Aour%2520larger%2520model%2520outperforms%2520VAEs%2520of%2520comparable%2520complexity%2520across%2520all%2520evaluated%250Ametrics%2520%2528rFID%252C%2520LPIPS%252C%2520PSNR%252C%2520and%2520SSIM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteVAE%3A%20Lightweight%20and%20Efficient%20Variational%20Autoencoders%20for%20Latent%0A%20%20Diffusion%20Models&entry.906535625=Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber&entry.1292438233=%20%20Advances%20in%20latent%20diffusion%20models%20%28LDMs%29%20have%20revolutionized%0Ahigh-resolution%20image%20generation%2C%20but%20the%20design%20space%20of%20the%20autoencoder%20that%0Ais%20central%20to%20these%20systems%20remains%20underexplored.%20In%20this%20paper%2C%20we%20introduce%0ALiteVAE%2C%20a%20family%20of%20autoencoders%20for%20LDMs%20that%20leverage%20the%202D%20discrete%0Awavelet%20transform%20to%20enhance%20scalability%20and%20computational%20efficiency%20over%0Astandard%20variational%20autoencoders%20%28VAEs%29%20with%20no%20sacrifice%20in%20output%20quality.%0AWe%20also%20investigate%20the%20training%20methodologies%20and%20the%20decoder%20architecture%20of%0ALiteVAE%20and%20propose%20several%20enhancements%20that%20improve%20the%20training%20dynamics%20and%0Areconstruction%20quality.%20Our%20base%20LiteVAE%20model%20matches%20the%20quality%20of%20the%0Aestablished%20VAEs%20in%20current%20LDMs%20with%20a%20six-fold%20reduction%20in%20encoder%0Aparameters%2C%20leading%20to%20faster%20training%20and%20lower%20GPU%20memory%20requirements%2C%20while%0Aour%20larger%20model%20outperforms%20VAEs%20of%20comparable%20complexity%20across%20all%20evaluated%0Ametrics%20%28rFID%2C%20LPIPS%2C%20PSNR%2C%20and%20SSIM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14477v1&entry.124074799=Read"},
{"title": "LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention\n  Networks", "author": "Michelle Halbheer and Dominik J. M\u00fchlematter and Alexander Becker and Dominik Narnhofer and Helge Aasen and Konrad Schindler and Mehmet Ozgur Turkoglu", "abstract": "  Numerous crucial tasks in real-world decision-making rely on machine learning\nalgorithms with calibrated uncertainty estimates. However, modern methods often\nyield overconfident and uncalibrated predictions. Various approaches involve\ntraining an ensemble of separate models to quantify the uncertainty related to\nthe model itself, known as epistemic uncertainty. In an explicit\nimplementation, the ensemble approach has high computational cost and high\nmemory requirements. This particular challenge is evident in state-of-the-art\nneural networks such as transformers, where even a single network is already\ndemanding in terms of compute and memory. Consequently, efforts are made to\nemulate the ensemble model without actually instantiating separate ensemble\nmembers, referred to as implicit ensembling. We introduce LoRA-Ensemble, a\nparameter-efficient deep ensemble method for self-attention networks, which is\nbased on Low-Rank Adaptation (LoRA). Initially developed for efficient LLM\nfine-tuning, we extend LoRA to an implicit ensembling approach. By employing a\nsingle pre-trained self-attention network with weights shared across all\nmembers, we train member-specific low-rank matrices for the attention\nprojections. Our method exhibits superior calibration compared to explicit\nensembles and achieves similar or better accuracy across various prediction\ntasks and datasets.\n", "link": "http://arxiv.org/abs/2405.14438v1", "date": "2024-05-23", "relevancy": 2.1945, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5968}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks&body=Title%3A%20LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks%0AAuthor%3A%20Michelle%20Halbheer%20and%20Dominik%20J.%20M%C3%BChlematter%20and%20Alexander%20Becker%20and%20Dominik%20Narnhofer%20and%20Helge%20Aasen%20and%20Konrad%20Schindler%20and%20Mehmet%20Ozgur%20Turkoglu%0AAbstract%3A%20%20%20Numerous%20crucial%20tasks%20in%20real-world%20decision-making%20rely%20on%20machine%20learning%0Aalgorithms%20with%20calibrated%20uncertainty%20estimates.%20However%2C%20modern%20methods%20often%0Ayield%20overconfident%20and%20uncalibrated%20predictions.%20Various%20approaches%20involve%0Atraining%20an%20ensemble%20of%20separate%20models%20to%20quantify%20the%20uncertainty%20related%20to%0Athe%20model%20itself%2C%20known%20as%20epistemic%20uncertainty.%20In%20an%20explicit%0Aimplementation%2C%20the%20ensemble%20approach%20has%20high%20computational%20cost%20and%20high%0Amemory%20requirements.%20This%20particular%20challenge%20is%20evident%20in%20state-of-the-art%0Aneural%20networks%20such%20as%20transformers%2C%20where%20even%20a%20single%20network%20is%20already%0Ademanding%20in%20terms%20of%20compute%20and%20memory.%20Consequently%2C%20efforts%20are%20made%20to%0Aemulate%20the%20ensemble%20model%20without%20actually%20instantiating%20separate%20ensemble%0Amembers%2C%20referred%20to%20as%20implicit%20ensembling.%20We%20introduce%20LoRA-Ensemble%2C%20a%0Aparameter-efficient%20deep%20ensemble%20method%20for%20self-attention%20networks%2C%20which%20is%0Abased%20on%20Low-Rank%20Adaptation%20%28LoRA%29.%20Initially%20developed%20for%20efficient%20LLM%0Afine-tuning%2C%20we%20extend%20LoRA%20to%20an%20implicit%20ensembling%20approach.%20By%20employing%20a%0Asingle%20pre-trained%20self-attention%20network%20with%20weights%20shared%20across%20all%0Amembers%2C%20we%20train%20member-specific%20low-rank%20matrices%20for%20the%20attention%0Aprojections.%20Our%20method%20exhibits%20superior%20calibration%20compared%20to%20explicit%0Aensembles%20and%20achieves%20similar%20or%20better%20accuracy%20across%20various%20prediction%0Atasks%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Ensemble%253A%2520Efficient%2520Uncertainty%2520Modelling%2520for%2520Self-attention%250A%2520%2520Networks%26entry.906535625%3DMichelle%2520Halbheer%2520and%2520Dominik%2520J.%2520M%25C3%25BChlematter%2520and%2520Alexander%2520Becker%2520and%2520Dominik%2520Narnhofer%2520and%2520Helge%2520Aasen%2520and%2520Konrad%2520Schindler%2520and%2520Mehmet%2520Ozgur%2520Turkoglu%26entry.1292438233%3D%2520%2520Numerous%2520crucial%2520tasks%2520in%2520real-world%2520decision-making%2520rely%2520on%2520machine%2520learning%250Aalgorithms%2520with%2520calibrated%2520uncertainty%2520estimates.%2520However%252C%2520modern%2520methods%2520often%250Ayield%2520overconfident%2520and%2520uncalibrated%2520predictions.%2520Various%2520approaches%2520involve%250Atraining%2520an%2520ensemble%2520of%2520separate%2520models%2520to%2520quantify%2520the%2520uncertainty%2520related%2520to%250Athe%2520model%2520itself%252C%2520known%2520as%2520epistemic%2520uncertainty.%2520In%2520an%2520explicit%250Aimplementation%252C%2520the%2520ensemble%2520approach%2520has%2520high%2520computational%2520cost%2520and%2520high%250Amemory%2520requirements.%2520This%2520particular%2520challenge%2520is%2520evident%2520in%2520state-of-the-art%250Aneural%2520networks%2520such%2520as%2520transformers%252C%2520where%2520even%2520a%2520single%2520network%2520is%2520already%250Ademanding%2520in%2520terms%2520of%2520compute%2520and%2520memory.%2520Consequently%252C%2520efforts%2520are%2520made%2520to%250Aemulate%2520the%2520ensemble%2520model%2520without%2520actually%2520instantiating%2520separate%2520ensemble%250Amembers%252C%2520referred%2520to%2520as%2520implicit%2520ensembling.%2520We%2520introduce%2520LoRA-Ensemble%252C%2520a%250Aparameter-efficient%2520deep%2520ensemble%2520method%2520for%2520self-attention%2520networks%252C%2520which%2520is%250Abased%2520on%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520Initially%2520developed%2520for%2520efficient%2520LLM%250Afine-tuning%252C%2520we%2520extend%2520LoRA%2520to%2520an%2520implicit%2520ensembling%2520approach.%2520By%2520employing%2520a%250Asingle%2520pre-trained%2520self-attention%2520network%2520with%2520weights%2520shared%2520across%2520all%250Amembers%252C%2520we%2520train%2520member-specific%2520low-rank%2520matrices%2520for%2520the%2520attention%250Aprojections.%2520Our%2520method%2520exhibits%2520superior%2520calibration%2520compared%2520to%2520explicit%250Aensembles%2520and%2520achieves%2520similar%2520or%2520better%2520accuracy%2520across%2520various%2520prediction%250Atasks%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Ensemble%3A%20Efficient%20Uncertainty%20Modelling%20for%20Self-attention%0A%20%20Networks&entry.906535625=Michelle%20Halbheer%20and%20Dominik%20J.%20M%C3%BChlematter%20and%20Alexander%20Becker%20and%20Dominik%20Narnhofer%20and%20Helge%20Aasen%20and%20Konrad%20Schindler%20and%20Mehmet%20Ozgur%20Turkoglu&entry.1292438233=%20%20Numerous%20crucial%20tasks%20in%20real-world%20decision-making%20rely%20on%20machine%20learning%0Aalgorithms%20with%20calibrated%20uncertainty%20estimates.%20However%2C%20modern%20methods%20often%0Ayield%20overconfident%20and%20uncalibrated%20predictions.%20Various%20approaches%20involve%0Atraining%20an%20ensemble%20of%20separate%20models%20to%20quantify%20the%20uncertainty%20related%20to%0Athe%20model%20itself%2C%20known%20as%20epistemic%20uncertainty.%20In%20an%20explicit%0Aimplementation%2C%20the%20ensemble%20approach%20has%20high%20computational%20cost%20and%20high%0Amemory%20requirements.%20This%20particular%20challenge%20is%20evident%20in%20state-of-the-art%0Aneural%20networks%20such%20as%20transformers%2C%20where%20even%20a%20single%20network%20is%20already%0Ademanding%20in%20terms%20of%20compute%20and%20memory.%20Consequently%2C%20efforts%20are%20made%20to%0Aemulate%20the%20ensemble%20model%20without%20actually%20instantiating%20separate%20ensemble%0Amembers%2C%20referred%20to%20as%20implicit%20ensembling.%20We%20introduce%20LoRA-Ensemble%2C%20a%0Aparameter-efficient%20deep%20ensemble%20method%20for%20self-attention%20networks%2C%20which%20is%0Abased%20on%20Low-Rank%20Adaptation%20%28LoRA%29.%20Initially%20developed%20for%20efficient%20LLM%0Afine-tuning%2C%20we%20extend%20LoRA%20to%20an%20implicit%20ensembling%20approach.%20By%20employing%20a%0Asingle%20pre-trained%20self-attention%20network%20with%20weights%20shared%20across%20all%0Amembers%2C%20we%20train%20member-specific%20low-rank%20matrices%20for%20the%20attention%0Aprojections.%20Our%20method%20exhibits%20superior%20calibration%20compared%20to%20explicit%0Aensembles%20and%20achieves%20similar%20or%20better%20accuracy%20across%20various%20prediction%0Atasks%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14438v1&entry.124074799=Read"},
{"title": "Enhanced Spatiotemporal Prediction Using Physical-guided And\n  Frequency-enhanced Recurrent Neural Networks", "author": "Xuanle Zhao and Yue Sun and Tielin Zhang and Bo Xu", "abstract": "  Spatiotemporal prediction plays an important role in solving natural problems\nand processing video frames, especially in weather forecasting and human action\nrecognition. Recent advances attempt to incorporate prior physical knowledge\ninto the deep learning framework to estimate the unknown governing partial\ndifferential equations (PDEs), which have shown promising results in\nspatiotemporal prediction tasks. However, previous approaches only restrict\nneural network architectures or loss functions to acquire physical or PDE\nfeatures, which decreases the representative capacity of a neural network.\nMeanwhile, the updating process of the physical state cannot be effectively\nestimated. To solve the above mentioned problems, this paper proposes a\nphysical-guided neural network, which utilizes the frequency-enhanced Fourier\nmodule and moment loss to strengthen the model's ability to estimate the\nspatiotemporal dynamics. Furthermore, we propose an adaptive second-order\nRunge-Kutta method with physical constraints to model the physical states more\nprecisely. We evaluate our model on both spatiotemporal and video prediction\ntasks. The experimental results show that our model outperforms\nstate-of-the-art methods and performs best in several datasets, with a much\nsmaller parameter count.\n", "link": "http://arxiv.org/abs/2405.14504v1", "date": "2024-05-23", "relevancy": 2.1866, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5926}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5379}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Spatiotemporal%20Prediction%20Using%20Physical-guided%20And%0A%20%20Frequency-enhanced%20Recurrent%20Neural%20Networks&body=Title%3A%20Enhanced%20Spatiotemporal%20Prediction%20Using%20Physical-guided%20And%0A%20%20Frequency-enhanced%20Recurrent%20Neural%20Networks%0AAuthor%3A%20Xuanle%20Zhao%20and%20Yue%20Sun%20and%20Tielin%20Zhang%20and%20Bo%20Xu%0AAbstract%3A%20%20%20Spatiotemporal%20prediction%20plays%20an%20important%20role%20in%20solving%20natural%20problems%0Aand%20processing%20video%20frames%2C%20especially%20in%20weather%20forecasting%20and%20human%20action%0Arecognition.%20Recent%20advances%20attempt%20to%20incorporate%20prior%20physical%20knowledge%0Ainto%20the%20deep%20learning%20framework%20to%20estimate%20the%20unknown%20governing%20partial%0Adifferential%20equations%20%28PDEs%29%2C%20which%20have%20shown%20promising%20results%20in%0Aspatiotemporal%20prediction%20tasks.%20However%2C%20previous%20approaches%20only%20restrict%0Aneural%20network%20architectures%20or%20loss%20functions%20to%20acquire%20physical%20or%20PDE%0Afeatures%2C%20which%20decreases%20the%20representative%20capacity%20of%20a%20neural%20network.%0AMeanwhile%2C%20the%20updating%20process%20of%20the%20physical%20state%20cannot%20be%20effectively%0Aestimated.%20To%20solve%20the%20above%20mentioned%20problems%2C%20this%20paper%20proposes%20a%0Aphysical-guided%20neural%20network%2C%20which%20utilizes%20the%20frequency-enhanced%20Fourier%0Amodule%20and%20moment%20loss%20to%20strengthen%20the%20model%27s%20ability%20to%20estimate%20the%0Aspatiotemporal%20dynamics.%20Furthermore%2C%20we%20propose%20an%20adaptive%20second-order%0ARunge-Kutta%20method%20with%20physical%20constraints%20to%20model%20the%20physical%20states%20more%0Aprecisely.%20We%20evaluate%20our%20model%20on%20both%20spatiotemporal%20and%20video%20prediction%0Atasks.%20The%20experimental%20results%20show%20that%20our%20model%20outperforms%0Astate-of-the-art%20methods%20and%20performs%20best%20in%20several%20datasets%2C%20with%20a%20much%0Asmaller%20parameter%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Spatiotemporal%2520Prediction%2520Using%2520Physical-guided%2520And%250A%2520%2520Frequency-enhanced%2520Recurrent%2520Neural%2520Networks%26entry.906535625%3DXuanle%2520Zhao%2520and%2520Yue%2520Sun%2520and%2520Tielin%2520Zhang%2520and%2520Bo%2520Xu%26entry.1292438233%3D%2520%2520Spatiotemporal%2520prediction%2520plays%2520an%2520important%2520role%2520in%2520solving%2520natural%2520problems%250Aand%2520processing%2520video%2520frames%252C%2520especially%2520in%2520weather%2520forecasting%2520and%2520human%2520action%250Arecognition.%2520Recent%2520advances%2520attempt%2520to%2520incorporate%2520prior%2520physical%2520knowledge%250Ainto%2520the%2520deep%2520learning%2520framework%2520to%2520estimate%2520the%2520unknown%2520governing%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%252C%2520which%2520have%2520shown%2520promising%2520results%2520in%250Aspatiotemporal%2520prediction%2520tasks.%2520However%252C%2520previous%2520approaches%2520only%2520restrict%250Aneural%2520network%2520architectures%2520or%2520loss%2520functions%2520to%2520acquire%2520physical%2520or%2520PDE%250Afeatures%252C%2520which%2520decreases%2520the%2520representative%2520capacity%2520of%2520a%2520neural%2520network.%250AMeanwhile%252C%2520the%2520updating%2520process%2520of%2520the%2520physical%2520state%2520cannot%2520be%2520effectively%250Aestimated.%2520To%2520solve%2520the%2520above%2520mentioned%2520problems%252C%2520this%2520paper%2520proposes%2520a%250Aphysical-guided%2520neural%2520network%252C%2520which%2520utilizes%2520the%2520frequency-enhanced%2520Fourier%250Amodule%2520and%2520moment%2520loss%2520to%2520strengthen%2520the%2520model%2527s%2520ability%2520to%2520estimate%2520the%250Aspatiotemporal%2520dynamics.%2520Furthermore%252C%2520we%2520propose%2520an%2520adaptive%2520second-order%250ARunge-Kutta%2520method%2520with%2520physical%2520constraints%2520to%2520model%2520the%2520physical%2520states%2520more%250Aprecisely.%2520We%2520evaluate%2520our%2520model%2520on%2520both%2520spatiotemporal%2520and%2520video%2520prediction%250Atasks.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520model%2520outperforms%250Astate-of-the-art%2520methods%2520and%2520performs%2520best%2520in%2520several%2520datasets%252C%2520with%2520a%2520much%250Asmaller%2520parameter%2520count.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Spatiotemporal%20Prediction%20Using%20Physical-guided%20And%0A%20%20Frequency-enhanced%20Recurrent%20Neural%20Networks&entry.906535625=Xuanle%20Zhao%20and%20Yue%20Sun%20and%20Tielin%20Zhang%20and%20Bo%20Xu&entry.1292438233=%20%20Spatiotemporal%20prediction%20plays%20an%20important%20role%20in%20solving%20natural%20problems%0Aand%20processing%20video%20frames%2C%20especially%20in%20weather%20forecasting%20and%20human%20action%0Arecognition.%20Recent%20advances%20attempt%20to%20incorporate%20prior%20physical%20knowledge%0Ainto%20the%20deep%20learning%20framework%20to%20estimate%20the%20unknown%20governing%20partial%0Adifferential%20equations%20%28PDEs%29%2C%20which%20have%20shown%20promising%20results%20in%0Aspatiotemporal%20prediction%20tasks.%20However%2C%20previous%20approaches%20only%20restrict%0Aneural%20network%20architectures%20or%20loss%20functions%20to%20acquire%20physical%20or%20PDE%0Afeatures%2C%20which%20decreases%20the%20representative%20capacity%20of%20a%20neural%20network.%0AMeanwhile%2C%20the%20updating%20process%20of%20the%20physical%20state%20cannot%20be%20effectively%0Aestimated.%20To%20solve%20the%20above%20mentioned%20problems%2C%20this%20paper%20proposes%20a%0Aphysical-guided%20neural%20network%2C%20which%20utilizes%20the%20frequency-enhanced%20Fourier%0Amodule%20and%20moment%20loss%20to%20strengthen%20the%20model%27s%20ability%20to%20estimate%20the%0Aspatiotemporal%20dynamics.%20Furthermore%2C%20we%20propose%20an%20adaptive%20second-order%0ARunge-Kutta%20method%20with%20physical%20constraints%20to%20model%20the%20physical%20states%20more%0Aprecisely.%20We%20evaluate%20our%20model%20on%20both%20spatiotemporal%20and%20video%20prediction%0Atasks.%20The%20experimental%20results%20show%20that%20our%20model%20outperforms%0Astate-of-the-art%20methods%20and%20performs%20best%20in%20several%20datasets%2C%20with%20a%20much%0Asmaller%20parameter%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14504v1&entry.124074799=Read"},
{"title": "TopoLogic: An Interpretable Pipeline for Lane Topology Reasoning on\n  Driving Scenes", "author": "Yanping Fu and Wenbin Liao and Xinyuan Liu and Hang xu and Yike Ma and Feng Dai and Yucheng Zhang", "abstract": "  As an emerging task that integrates perception and reasoning, topology\nreasoning in autonomous driving scenes has recently garnered widespread\nattention. However, existing work often emphasizes \"perception over reasoning\":\nthey typically boost reasoning performance by enhancing the perception of lanes\nand directly adopt MLP to learn lane topology from lane query. This paradigm\noverlooks the geometric features intrinsic to the lanes themselves and are\nprone to being influenced by inherent endpoint shifts in lane detection.\n  To tackle this issue, we propose an interpretable method for lane topology\nreasoning based on lane geometric distance and lane query similarity, named\nTopoLogic.\n  This method mitigates the impact of endpoint shifts in geometric space, and\nintroduces explicit similarity calculation in semantic space as a complement.\nBy integrating results from both spaces, our methods provides more\ncomprehensive information for lane topology.\n  Ultimately, our approach significantly outperforms the existing\nstate-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s.\n10.9 in TOP$_{ll}$ and 44.1 v.s. 39.8 in OLS on subset_A. Additionally, our\nproposed geometric distance topology reasoning method can be incorporated into\nwell-trained models without re-training, significantly boost the performance of\nlane topology reasoning. The code is released at\nhttps://github.com/Franpin/TopoLogic.\n", "link": "http://arxiv.org/abs/2405.14747v1", "date": "2024-05-23", "relevancy": 2.1794, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.54}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoLogic%3A%20An%20Interpretable%20Pipeline%20for%20Lane%20Topology%20Reasoning%20on%0A%20%20Driving%20Scenes&body=Title%3A%20TopoLogic%3A%20An%20Interpretable%20Pipeline%20for%20Lane%20Topology%20Reasoning%20on%0A%20%20Driving%20Scenes%0AAuthor%3A%20Yanping%20Fu%20and%20Wenbin%20Liao%20and%20Xinyuan%20Liu%20and%20Hang%20xu%20and%20Yike%20Ma%20and%20Feng%20Dai%20and%20Yucheng%20Zhang%0AAbstract%3A%20%20%20As%20an%20emerging%20task%20that%20integrates%20perception%20and%20reasoning%2C%20topology%0Areasoning%20in%20autonomous%20driving%20scenes%20has%20recently%20garnered%20widespread%0Aattention.%20However%2C%20existing%20work%20often%20emphasizes%20%22perception%20over%20reasoning%22%3A%0Athey%20typically%20boost%20reasoning%20performance%20by%20enhancing%20the%20perception%20of%20lanes%0Aand%20directly%20adopt%20MLP%20to%20learn%20lane%20topology%20from%20lane%20query.%20This%20paradigm%0Aoverlooks%20the%20geometric%20features%20intrinsic%20to%20the%20lanes%20themselves%20and%20are%0Aprone%20to%20being%20influenced%20by%20inherent%20endpoint%20shifts%20in%20lane%20detection.%0A%20%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20interpretable%20method%20for%20lane%20topology%0Areasoning%20based%20on%20lane%20geometric%20distance%20and%20lane%20query%20similarity%2C%20named%0ATopoLogic.%0A%20%20This%20method%20mitigates%20the%20impact%20of%20endpoint%20shifts%20in%20geometric%20space%2C%20and%0Aintroduces%20explicit%20similarity%20calculation%20in%20semantic%20space%20as%20a%20complement.%0ABy%20integrating%20results%20from%20both%20spaces%2C%20our%20methods%20provides%20more%0Acomprehensive%20information%20for%20lane%20topology.%0A%20%20Ultimately%2C%20our%20approach%20significantly%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20on%20the%20mainstream%20benchmark%20OpenLane-V2%20%2823.9%20v.s.%0A10.9%20in%20TOP%24_%7Bll%7D%24%20and%2044.1%20v.s.%2039.8%20in%20OLS%20on%20subset_A.%20Additionally%2C%20our%0Aproposed%20geometric%20distance%20topology%20reasoning%20method%20can%20be%20incorporated%20into%0Awell-trained%20models%20without%20re-training%2C%20significantly%20boost%20the%20performance%20of%0Alane%20topology%20reasoning.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Franpin/TopoLogic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoLogic%253A%2520An%2520Interpretable%2520Pipeline%2520for%2520Lane%2520Topology%2520Reasoning%2520on%250A%2520%2520Driving%2520Scenes%26entry.906535625%3DYanping%2520Fu%2520and%2520Wenbin%2520Liao%2520and%2520Xinyuan%2520Liu%2520and%2520Hang%2520xu%2520and%2520Yike%2520Ma%2520and%2520Feng%2520Dai%2520and%2520Yucheng%2520Zhang%26entry.1292438233%3D%2520%2520As%2520an%2520emerging%2520task%2520that%2520integrates%2520perception%2520and%2520reasoning%252C%2520topology%250Areasoning%2520in%2520autonomous%2520driving%2520scenes%2520has%2520recently%2520garnered%2520widespread%250Aattention.%2520However%252C%2520existing%2520work%2520often%2520emphasizes%2520%2522perception%2520over%2520reasoning%2522%253A%250Athey%2520typically%2520boost%2520reasoning%2520performance%2520by%2520enhancing%2520the%2520perception%2520of%2520lanes%250Aand%2520directly%2520adopt%2520MLP%2520to%2520learn%2520lane%2520topology%2520from%2520lane%2520query.%2520This%2520paradigm%250Aoverlooks%2520the%2520geometric%2520features%2520intrinsic%2520to%2520the%2520lanes%2520themselves%2520and%2520are%250Aprone%2520to%2520being%2520influenced%2520by%2520inherent%2520endpoint%2520shifts%2520in%2520lane%2520detection.%250A%2520%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520an%2520interpretable%2520method%2520for%2520lane%2520topology%250Areasoning%2520based%2520on%2520lane%2520geometric%2520distance%2520and%2520lane%2520query%2520similarity%252C%2520named%250ATopoLogic.%250A%2520%2520This%2520method%2520mitigates%2520the%2520impact%2520of%2520endpoint%2520shifts%2520in%2520geometric%2520space%252C%2520and%250Aintroduces%2520explicit%2520similarity%2520calculation%2520in%2520semantic%2520space%2520as%2520a%2520complement.%250ABy%2520integrating%2520results%2520from%2520both%2520spaces%252C%2520our%2520methods%2520provides%2520more%250Acomprehensive%2520information%2520for%2520lane%2520topology.%250A%2520%2520Ultimately%252C%2520our%2520approach%2520significantly%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods%2520on%2520the%2520mainstream%2520benchmark%2520OpenLane-V2%2520%252823.9%2520v.s.%250A10.9%2520in%2520TOP%2524_%257Bll%257D%2524%2520and%252044.1%2520v.s.%252039.8%2520in%2520OLS%2520on%2520subset_A.%2520Additionally%252C%2520our%250Aproposed%2520geometric%2520distance%2520topology%2520reasoning%2520method%2520can%2520be%2520incorporated%2520into%250Awell-trained%2520models%2520without%2520re-training%252C%2520significantly%2520boost%2520the%2520performance%2520of%250Alane%2520topology%2520reasoning.%2520The%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/Franpin/TopoLogic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoLogic%3A%20An%20Interpretable%20Pipeline%20for%20Lane%20Topology%20Reasoning%20on%0A%20%20Driving%20Scenes&entry.906535625=Yanping%20Fu%20and%20Wenbin%20Liao%20and%20Xinyuan%20Liu%20and%20Hang%20xu%20and%20Yike%20Ma%20and%20Feng%20Dai%20and%20Yucheng%20Zhang&entry.1292438233=%20%20As%20an%20emerging%20task%20that%20integrates%20perception%20and%20reasoning%2C%20topology%0Areasoning%20in%20autonomous%20driving%20scenes%20has%20recently%20garnered%20widespread%0Aattention.%20However%2C%20existing%20work%20often%20emphasizes%20%22perception%20over%20reasoning%22%3A%0Athey%20typically%20boost%20reasoning%20performance%20by%20enhancing%20the%20perception%20of%20lanes%0Aand%20directly%20adopt%20MLP%20to%20learn%20lane%20topology%20from%20lane%20query.%20This%20paradigm%0Aoverlooks%20the%20geometric%20features%20intrinsic%20to%20the%20lanes%20themselves%20and%20are%0Aprone%20to%20being%20influenced%20by%20inherent%20endpoint%20shifts%20in%20lane%20detection.%0A%20%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20interpretable%20method%20for%20lane%20topology%0Areasoning%20based%20on%20lane%20geometric%20distance%20and%20lane%20query%20similarity%2C%20named%0ATopoLogic.%0A%20%20This%20method%20mitigates%20the%20impact%20of%20endpoint%20shifts%20in%20geometric%20space%2C%20and%0Aintroduces%20explicit%20similarity%20calculation%20in%20semantic%20space%20as%20a%20complement.%0ABy%20integrating%20results%20from%20both%20spaces%2C%20our%20methods%20provides%20more%0Acomprehensive%20information%20for%20lane%20topology.%0A%20%20Ultimately%2C%20our%20approach%20significantly%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20on%20the%20mainstream%20benchmark%20OpenLane-V2%20%2823.9%20v.s.%0A10.9%20in%20TOP%24_%7Bll%7D%24%20and%2044.1%20v.s.%2039.8%20in%20OLS%20on%20subset_A.%20Additionally%2C%20our%0Aproposed%20geometric%20distance%20topology%20reasoning%20method%20can%20be%20incorporated%20into%0Awell-trained%20models%20without%20re-training%2C%20significantly%20boost%20the%20performance%20of%0Alane%20topology%20reasoning.%20The%20code%20is%20released%20at%0Ahttps%3A//github.com/Franpin/TopoLogic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14747v1&entry.124074799=Read"},
{"title": "Drones Help Drones: A Collaborative Framework for Multi-Drone Object\n  Trajectory Prediction and Beyond", "author": "Zhechao Wang and Peirui Cheng and Mingxin Chen and Pengju Tian and Zhirui Wang and Xinming Li and Xue Yang and Xian Sun", "abstract": "  Collaborative trajectory prediction can comprehensively forecast the future\nmotion of objects through multi-view complementary information. However, it\nencounters two main challenges in multi-drone collaboration settings. The\nexpansive aerial observations make it difficult to generate precise Bird's Eye\nView (BEV) representations. Besides, excessive interactions can not meet\nreal-time prediction requirements within the constrained drone-based\ncommunication bandwidth. To address these problems, we propose a novel\nframework named \"Drones Help Drones\" (DHD). Firstly, we incorporate the ground\npriors provided by the drone's inclined observation to estimate the distance\nbetween objects and drones, leading to more precise BEV generation. Secondly,\nwe design a selective mechanism based on the local feature discrepancy to\nprioritize the critical information contributing to prediction tasks during\ninter-drone interactions. Additionally, we create the first dataset for\nmulti-drone collaborative prediction, named \"Air-Co-Pred\", and conduct\nquantitative and qualitative experiments to validate the effectiveness of our\nDHD framework.The results demonstrate that compared to state-of-the-art\napproaches, DHD reduces position deviation in BEV representations by over 20%\nand requires only a quarter of the transmission ratio for interactions while\nachieving comparable prediction performance. Moreover, DHD also shows promising\ngeneralization to the collaborative 3D object detection in CoPerception-UAVs.\n", "link": "http://arxiv.org/abs/2405.14674v1", "date": "2024-05-23", "relevancy": 2.1764, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5575}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.546}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drones%20Help%20Drones%3A%20A%20Collaborative%20Framework%20for%20Multi-Drone%20Object%0A%20%20Trajectory%20Prediction%20and%20Beyond&body=Title%3A%20Drones%20Help%20Drones%3A%20A%20Collaborative%20Framework%20for%20Multi-Drone%20Object%0A%20%20Trajectory%20Prediction%20and%20Beyond%0AAuthor%3A%20Zhechao%20Wang%20and%20Peirui%20Cheng%20and%20Mingxin%20Chen%20and%20Pengju%20Tian%20and%20Zhirui%20Wang%20and%20Xinming%20Li%20and%20Xue%20Yang%20and%20Xian%20Sun%0AAbstract%3A%20%20%20Collaborative%20trajectory%20prediction%20can%20comprehensively%20forecast%20the%20future%0Amotion%20of%20objects%20through%20multi-view%20complementary%20information.%20However%2C%20it%0Aencounters%20two%20main%20challenges%20in%20multi-drone%20collaboration%20settings.%20The%0Aexpansive%20aerial%20observations%20make%20it%20difficult%20to%20generate%20precise%20Bird%27s%20Eye%0AView%20%28BEV%29%20representations.%20Besides%2C%20excessive%20interactions%20can%20not%20meet%0Areal-time%20prediction%20requirements%20within%20the%20constrained%20drone-based%0Acommunication%20bandwidth.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aframework%20named%20%22Drones%20Help%20Drones%22%20%28DHD%29.%20Firstly%2C%20we%20incorporate%20the%20ground%0Apriors%20provided%20by%20the%20drone%27s%20inclined%20observation%20to%20estimate%20the%20distance%0Abetween%20objects%20and%20drones%2C%20leading%20to%20more%20precise%20BEV%20generation.%20Secondly%2C%0Awe%20design%20a%20selective%20mechanism%20based%20on%20the%20local%20feature%20discrepancy%20to%0Aprioritize%20the%20critical%20information%20contributing%20to%20prediction%20tasks%20during%0Ainter-drone%20interactions.%20Additionally%2C%20we%20create%20the%20first%20dataset%20for%0Amulti-drone%20collaborative%20prediction%2C%20named%20%22Air-Co-Pred%22%2C%20and%20conduct%0Aquantitative%20and%20qualitative%20experiments%20to%20validate%20the%20effectiveness%20of%20our%0ADHD%20framework.The%20results%20demonstrate%20that%20compared%20to%20state-of-the-art%0Aapproaches%2C%20DHD%20reduces%20position%20deviation%20in%20BEV%20representations%20by%20over%2020%25%0Aand%20requires%20only%20a%20quarter%20of%20the%20transmission%20ratio%20for%20interactions%20while%0Aachieving%20comparable%20prediction%20performance.%20Moreover%2C%20DHD%20also%20shows%20promising%0Ageneralization%20to%20the%20collaborative%203D%20object%20detection%20in%20CoPerception-UAVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrones%2520Help%2520Drones%253A%2520A%2520Collaborative%2520Framework%2520for%2520Multi-Drone%2520Object%250A%2520%2520Trajectory%2520Prediction%2520and%2520Beyond%26entry.906535625%3DZhechao%2520Wang%2520and%2520Peirui%2520Cheng%2520and%2520Mingxin%2520Chen%2520and%2520Pengju%2520Tian%2520and%2520Zhirui%2520Wang%2520and%2520Xinming%2520Li%2520and%2520Xue%2520Yang%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520Collaborative%2520trajectory%2520prediction%2520can%2520comprehensively%2520forecast%2520the%2520future%250Amotion%2520of%2520objects%2520through%2520multi-view%2520complementary%2520information.%2520However%252C%2520it%250Aencounters%2520two%2520main%2520challenges%2520in%2520multi-drone%2520collaboration%2520settings.%2520The%250Aexpansive%2520aerial%2520observations%2520make%2520it%2520difficult%2520to%2520generate%2520precise%2520Bird%2527s%2520Eye%250AView%2520%2528BEV%2529%2520representations.%2520Besides%252C%2520excessive%2520interactions%2520can%2520not%2520meet%250Areal-time%2520prediction%2520requirements%2520within%2520the%2520constrained%2520drone-based%250Acommunication%2520bandwidth.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520named%2520%2522Drones%2520Help%2520Drones%2522%2520%2528DHD%2529.%2520Firstly%252C%2520we%2520incorporate%2520the%2520ground%250Apriors%2520provided%2520by%2520the%2520drone%2527s%2520inclined%2520observation%2520to%2520estimate%2520the%2520distance%250Abetween%2520objects%2520and%2520drones%252C%2520leading%2520to%2520more%2520precise%2520BEV%2520generation.%2520Secondly%252C%250Awe%2520design%2520a%2520selective%2520mechanism%2520based%2520on%2520the%2520local%2520feature%2520discrepancy%2520to%250Aprioritize%2520the%2520critical%2520information%2520contributing%2520to%2520prediction%2520tasks%2520during%250Ainter-drone%2520interactions.%2520Additionally%252C%2520we%2520create%2520the%2520first%2520dataset%2520for%250Amulti-drone%2520collaborative%2520prediction%252C%2520named%2520%2522Air-Co-Pred%2522%252C%2520and%2520conduct%250Aquantitative%2520and%2520qualitative%2520experiments%2520to%2520validate%2520the%2520effectiveness%2520of%2520our%250ADHD%2520framework.The%2520results%2520demonstrate%2520that%2520compared%2520to%2520state-of-the-art%250Aapproaches%252C%2520DHD%2520reduces%2520position%2520deviation%2520in%2520BEV%2520representations%2520by%2520over%252020%2525%250Aand%2520requires%2520only%2520a%2520quarter%2520of%2520the%2520transmission%2520ratio%2520for%2520interactions%2520while%250Aachieving%2520comparable%2520prediction%2520performance.%2520Moreover%252C%2520DHD%2520also%2520shows%2520promising%250Ageneralization%2520to%2520the%2520collaborative%25203D%2520object%2520detection%2520in%2520CoPerception-UAVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drones%20Help%20Drones%3A%20A%20Collaborative%20Framework%20for%20Multi-Drone%20Object%0A%20%20Trajectory%20Prediction%20and%20Beyond&entry.906535625=Zhechao%20Wang%20and%20Peirui%20Cheng%20and%20Mingxin%20Chen%20and%20Pengju%20Tian%20and%20Zhirui%20Wang%20and%20Xinming%20Li%20and%20Xue%20Yang%20and%20Xian%20Sun&entry.1292438233=%20%20Collaborative%20trajectory%20prediction%20can%20comprehensively%20forecast%20the%20future%0Amotion%20of%20objects%20through%20multi-view%20complementary%20information.%20However%2C%20it%0Aencounters%20two%20main%20challenges%20in%20multi-drone%20collaboration%20settings.%20The%0Aexpansive%20aerial%20observations%20make%20it%20difficult%20to%20generate%20precise%20Bird%27s%20Eye%0AView%20%28BEV%29%20representations.%20Besides%2C%20excessive%20interactions%20can%20not%20meet%0Areal-time%20prediction%20requirements%20within%20the%20constrained%20drone-based%0Acommunication%20bandwidth.%20To%20address%20these%20problems%2C%20we%20propose%20a%20novel%0Aframework%20named%20%22Drones%20Help%20Drones%22%20%28DHD%29.%20Firstly%2C%20we%20incorporate%20the%20ground%0Apriors%20provided%20by%20the%20drone%27s%20inclined%20observation%20to%20estimate%20the%20distance%0Abetween%20objects%20and%20drones%2C%20leading%20to%20more%20precise%20BEV%20generation.%20Secondly%2C%0Awe%20design%20a%20selective%20mechanism%20based%20on%20the%20local%20feature%20discrepancy%20to%0Aprioritize%20the%20critical%20information%20contributing%20to%20prediction%20tasks%20during%0Ainter-drone%20interactions.%20Additionally%2C%20we%20create%20the%20first%20dataset%20for%0Amulti-drone%20collaborative%20prediction%2C%20named%20%22Air-Co-Pred%22%2C%20and%20conduct%0Aquantitative%20and%20qualitative%20experiments%20to%20validate%20the%20effectiveness%20of%20our%0ADHD%20framework.The%20results%20demonstrate%20that%20compared%20to%20state-of-the-art%0Aapproaches%2C%20DHD%20reduces%20position%20deviation%20in%20BEV%20representations%20by%20over%2020%25%0Aand%20requires%20only%20a%20quarter%20of%20the%20transmission%20ratio%20for%20interactions%20while%0Aachieving%20comparable%20prediction%20performance.%20Moreover%2C%20DHD%20also%20shows%20promising%0Ageneralization%20to%20the%20collaborative%203D%20object%20detection%20in%20CoPerception-UAVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14674v1&entry.124074799=Read"},
{"title": "Deep Learning Classification of Photoplethysmogram Signal for\n  Hypertension Levels", "author": "Nida Nasir and Mustafa Sameer and Feras Barneih and Omar Alshaltone and Muneeb Ahmed", "abstract": "  Continuous photoplethysmography (PPG)-based blood pressure monitoring is\nnecessary for healthcare and fitness applications. In Artificial Intelligence\n(AI), signal classification levels with the machine and deep learning\narrangements need to be explored further. Techniques based on time-frequency\nspectra, such as Short-time Fourier Transform (STFT), have been used to address\nthe challenges of motion artifact correction. Therefore, the proposed study\nworks with PPG signals of more than 200 patients (650+ signal samples) with\nhypertension, using STFT with various Neural Networks (Convolution Neural\nNetwork (CNN), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term\nMemory (Bi-LSTM), followed by machine learning classifiers, such as, Support\nVector Machine (SVM) and Random Forest (RF). The classification has been done\nfor two categories: Prehypertension (normal levels) and Hypertension (includes\nStage I and Stage II). Various performance metrics have been obtained with two\nbatch sizes of 3 and 16 for the fusion of the neural networks. With precision\nand specificity of 100% and recall of 82.1%, the LSTM model provides the best\nresults among all combinations of Neural Networks. However, the maximum\naccuracy of 71.9% is achieved by the LSTM-CNN model. Further stacked Ensemble\nmethod has been used to achieve 100% accuracy for Meta-LSTM-RF, Meta-\nLSTM-CNN-RF and Meta- STFT-CNN-SVM.\n", "link": "http://arxiv.org/abs/2405.14556v1", "date": "2024-05-23", "relevancy": 2.1718, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4391}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.434}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Classification%20of%20Photoplethysmogram%20Signal%20for%0A%20%20Hypertension%20Levels&body=Title%3A%20Deep%20Learning%20Classification%20of%20Photoplethysmogram%20Signal%20for%0A%20%20Hypertension%20Levels%0AAuthor%3A%20Nida%20Nasir%20and%20Mustafa%20Sameer%20and%20Feras%20Barneih%20and%20Omar%20Alshaltone%20and%20Muneeb%20Ahmed%0AAbstract%3A%20%20%20Continuous%20photoplethysmography%20%28PPG%29-based%20blood%20pressure%20monitoring%20is%0Anecessary%20for%20healthcare%20and%20fitness%20applications.%20In%20Artificial%20Intelligence%0A%28AI%29%2C%20signal%20classification%20levels%20with%20the%20machine%20and%20deep%20learning%0Aarrangements%20need%20to%20be%20explored%20further.%20Techniques%20based%20on%20time-frequency%0Aspectra%2C%20such%20as%20Short-time%20Fourier%20Transform%20%28STFT%29%2C%20have%20been%20used%20to%20address%0Athe%20challenges%20of%20motion%20artifact%20correction.%20Therefore%2C%20the%20proposed%20study%0Aworks%20with%20PPG%20signals%20of%20more%20than%20200%20patients%20%28650%2B%20signal%20samples%29%20with%0Ahypertension%2C%20using%20STFT%20with%20various%20Neural%20Networks%20%28Convolution%20Neural%0ANetwork%20%28CNN%29%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%2C%20Bidirectional%20Long%20Short-Term%0AMemory%20%28Bi-LSTM%29%2C%20followed%20by%20machine%20learning%20classifiers%2C%20such%20as%2C%20Support%0AVector%20Machine%20%28SVM%29%20and%20Random%20Forest%20%28RF%29.%20The%20classification%20has%20been%20done%0Afor%20two%20categories%3A%20Prehypertension%20%28normal%20levels%29%20and%20Hypertension%20%28includes%0AStage%20I%20and%20Stage%20II%29.%20Various%20performance%20metrics%20have%20been%20obtained%20with%20two%0Abatch%20sizes%20of%203%20and%2016%20for%20the%20fusion%20of%20the%20neural%20networks.%20With%20precision%0Aand%20specificity%20of%20100%25%20and%20recall%20of%2082.1%25%2C%20the%20LSTM%20model%20provides%20the%20best%0Aresults%20among%20all%20combinations%20of%20Neural%20Networks.%20However%2C%20the%20maximum%0Aaccuracy%20of%2071.9%25%20is%20achieved%20by%20the%20LSTM-CNN%20model.%20Further%20stacked%20Ensemble%0Amethod%20has%20been%20used%20to%20achieve%20100%25%20accuracy%20for%20Meta-LSTM-RF%2C%20Meta-%0ALSTM-CNN-RF%20and%20Meta-%20STFT-CNN-SVM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Classification%2520of%2520Photoplethysmogram%2520Signal%2520for%250A%2520%2520Hypertension%2520Levels%26entry.906535625%3DNida%2520Nasir%2520and%2520Mustafa%2520Sameer%2520and%2520Feras%2520Barneih%2520and%2520Omar%2520Alshaltone%2520and%2520Muneeb%2520Ahmed%26entry.1292438233%3D%2520%2520Continuous%2520photoplethysmography%2520%2528PPG%2529-based%2520blood%2520pressure%2520monitoring%2520is%250Anecessary%2520for%2520healthcare%2520and%2520fitness%2520applications.%2520In%2520Artificial%2520Intelligence%250A%2528AI%2529%252C%2520signal%2520classification%2520levels%2520with%2520the%2520machine%2520and%2520deep%2520learning%250Aarrangements%2520need%2520to%2520be%2520explored%2520further.%2520Techniques%2520based%2520on%2520time-frequency%250Aspectra%252C%2520such%2520as%2520Short-time%2520Fourier%2520Transform%2520%2528STFT%2529%252C%2520have%2520been%2520used%2520to%2520address%250Athe%2520challenges%2520of%2520motion%2520artifact%2520correction.%2520Therefore%252C%2520the%2520proposed%2520study%250Aworks%2520with%2520PPG%2520signals%2520of%2520more%2520than%2520200%2520patients%2520%2528650%252B%2520signal%2520samples%2529%2520with%250Ahypertension%252C%2520using%2520STFT%2520with%2520various%2520Neural%2520Networks%2520%2528Convolution%2520Neural%250ANetwork%2520%2528CNN%2529%252C%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%252C%2520Bidirectional%2520Long%2520Short-Term%250AMemory%2520%2528Bi-LSTM%2529%252C%2520followed%2520by%2520machine%2520learning%2520classifiers%252C%2520such%2520as%252C%2520Support%250AVector%2520Machine%2520%2528SVM%2529%2520and%2520Random%2520Forest%2520%2528RF%2529.%2520The%2520classification%2520has%2520been%2520done%250Afor%2520two%2520categories%253A%2520Prehypertension%2520%2528normal%2520levels%2529%2520and%2520Hypertension%2520%2528includes%250AStage%2520I%2520and%2520Stage%2520II%2529.%2520Various%2520performance%2520metrics%2520have%2520been%2520obtained%2520with%2520two%250Abatch%2520sizes%2520of%25203%2520and%252016%2520for%2520the%2520fusion%2520of%2520the%2520neural%2520networks.%2520With%2520precision%250Aand%2520specificity%2520of%2520100%2525%2520and%2520recall%2520of%252082.1%2525%252C%2520the%2520LSTM%2520model%2520provides%2520the%2520best%250Aresults%2520among%2520all%2520combinations%2520of%2520Neural%2520Networks.%2520However%252C%2520the%2520maximum%250Aaccuracy%2520of%252071.9%2525%2520is%2520achieved%2520by%2520the%2520LSTM-CNN%2520model.%2520Further%2520stacked%2520Ensemble%250Amethod%2520has%2520been%2520used%2520to%2520achieve%2520100%2525%2520accuracy%2520for%2520Meta-LSTM-RF%252C%2520Meta-%250ALSTM-CNN-RF%2520and%2520Meta-%2520STFT-CNN-SVM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Classification%20of%20Photoplethysmogram%20Signal%20for%0A%20%20Hypertension%20Levels&entry.906535625=Nida%20Nasir%20and%20Mustafa%20Sameer%20and%20Feras%20Barneih%20and%20Omar%20Alshaltone%20and%20Muneeb%20Ahmed&entry.1292438233=%20%20Continuous%20photoplethysmography%20%28PPG%29-based%20blood%20pressure%20monitoring%20is%0Anecessary%20for%20healthcare%20and%20fitness%20applications.%20In%20Artificial%20Intelligence%0A%28AI%29%2C%20signal%20classification%20levels%20with%20the%20machine%20and%20deep%20learning%0Aarrangements%20need%20to%20be%20explored%20further.%20Techniques%20based%20on%20time-frequency%0Aspectra%2C%20such%20as%20Short-time%20Fourier%20Transform%20%28STFT%29%2C%20have%20been%20used%20to%20address%0Athe%20challenges%20of%20motion%20artifact%20correction.%20Therefore%2C%20the%20proposed%20study%0Aworks%20with%20PPG%20signals%20of%20more%20than%20200%20patients%20%28650%2B%20signal%20samples%29%20with%0Ahypertension%2C%20using%20STFT%20with%20various%20Neural%20Networks%20%28Convolution%20Neural%0ANetwork%20%28CNN%29%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%2C%20Bidirectional%20Long%20Short-Term%0AMemory%20%28Bi-LSTM%29%2C%20followed%20by%20machine%20learning%20classifiers%2C%20such%20as%2C%20Support%0AVector%20Machine%20%28SVM%29%20and%20Random%20Forest%20%28RF%29.%20The%20classification%20has%20been%20done%0Afor%20two%20categories%3A%20Prehypertension%20%28normal%20levels%29%20and%20Hypertension%20%28includes%0AStage%20I%20and%20Stage%20II%29.%20Various%20performance%20metrics%20have%20been%20obtained%20with%20two%0Abatch%20sizes%20of%203%20and%2016%20for%20the%20fusion%20of%20the%20neural%20networks.%20With%20precision%0Aand%20specificity%20of%20100%25%20and%20recall%20of%2082.1%25%2C%20the%20LSTM%20model%20provides%20the%20best%0Aresults%20among%20all%20combinations%20of%20Neural%20Networks.%20However%2C%20the%20maximum%0Aaccuracy%20of%2071.9%25%20is%20achieved%20by%20the%20LSTM-CNN%20model.%20Further%20stacked%20Ensemble%0Amethod%20has%20been%20used%20to%20achieve%20100%25%20accuracy%20for%20Meta-LSTM-RF%2C%20Meta-%0ALSTM-CNN-RF%20and%20Meta-%20STFT-CNN-SVM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14556v1&entry.124074799=Read"},
{"title": "DuEDL: Dual-Branch Evidential Deep Learning for Scribble-Supervised\n  Medical Image Segmentation", "author": "Yitong Yang and Xinli Xu and Haigen Hu and Haixia Long and Qianwei Zhou and Qiu Guan", "abstract": "  Despite the recent progress in medical image segmentation with scribble-based\nannotations, the segmentation results of most models are still not ro-bust and\ngeneralizable enough in open environments. Evidential deep learn-ing (EDL) has\nrecently been proposed as a promising solution to model predictive uncertainty\nand improve the reliability of medical image segmen-tation. However directly\napplying EDL to scribble-supervised medical im-age segmentation faces a\ntradeoff between accuracy and reliability. To ad-dress the challenge, we\npropose a novel framework called Dual-Branch Evi-dential Deep Learning (DuEDL).\nFirstly, the decoder of the segmentation network is changed to two different\nbranches, and the evidence of the two branches is fused to generate\nhigh-quality pseudo-labels. Then the frame-work applies partial evidence loss\nand two-branch consistent loss for joint training of the model to adapt to the\nscribble supervision learning. The pro-posed method was tested on two cardiac\ndatasets: ACDC and MSCMRseg. The results show that our method significantly\nenhances the reliability and generalization ability of the model without\nsacrificing accuracy, outper-forming state-of-the-art baselines. The code is\navailable at https://github.com/Gardnery/DuEDL.\n", "link": "http://arxiv.org/abs/2405.14444v1", "date": "2024-05-23", "relevancy": 2.1706, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6644}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuEDL%3A%20Dual-Branch%20Evidential%20Deep%20Learning%20for%20Scribble-Supervised%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20DuEDL%3A%20Dual-Branch%20Evidential%20Deep%20Learning%20for%20Scribble-Supervised%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yitong%20Yang%20and%20Xinli%20Xu%20and%20Haigen%20Hu%20and%20Haixia%20Long%20and%20Qianwei%20Zhou%20and%20Qiu%20Guan%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%20in%20medical%20image%20segmentation%20with%20scribble-based%0Aannotations%2C%20the%20segmentation%20results%20of%20most%20models%20are%20still%20not%20ro-bust%20and%0Ageneralizable%20enough%20in%20open%20environments.%20Evidential%20deep%20learn-ing%20%28EDL%29%20has%0Arecently%20been%20proposed%20as%20a%20promising%20solution%20to%20model%20predictive%20uncertainty%0Aand%20improve%20the%20reliability%20of%20medical%20image%20segmen-tation.%20However%20directly%0Aapplying%20EDL%20to%20scribble-supervised%20medical%20im-age%20segmentation%20faces%20a%0Atradeoff%20between%20accuracy%20and%20reliability.%20To%20ad-dress%20the%20challenge%2C%20we%0Apropose%20a%20novel%20framework%20called%20Dual-Branch%20Evi-dential%20Deep%20Learning%20%28DuEDL%29.%0AFirstly%2C%20the%20decoder%20of%20the%20segmentation%20network%20is%20changed%20to%20two%20different%0Abranches%2C%20and%20the%20evidence%20of%20the%20two%20branches%20is%20fused%20to%20generate%0Ahigh-quality%20pseudo-labels.%20Then%20the%20frame-work%20applies%20partial%20evidence%20loss%0Aand%20two-branch%20consistent%20loss%20for%20joint%20training%20of%20the%20model%20to%20adapt%20to%20the%0Ascribble%20supervision%20learning.%20The%20pro-posed%20method%20was%20tested%20on%20two%20cardiac%0Adatasets%3A%20ACDC%20and%20MSCMRseg.%20The%20results%20show%20that%20our%20method%20significantly%0Aenhances%20the%20reliability%20and%20generalization%20ability%20of%20the%20model%20without%0Asacrificing%20accuracy%2C%20outper-forming%20state-of-the-art%20baselines.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/Gardnery/DuEDL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuEDL%253A%2520Dual-Branch%2520Evidential%2520Deep%2520Learning%2520for%2520Scribble-Supervised%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYitong%2520Yang%2520and%2520Xinli%2520Xu%2520and%2520Haigen%2520Hu%2520and%2520Haixia%2520Long%2520and%2520Qianwei%2520Zhou%2520and%2520Qiu%2520Guan%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%2520in%2520medical%2520image%2520segmentation%2520with%2520scribble-based%250Aannotations%252C%2520the%2520segmentation%2520results%2520of%2520most%2520models%2520are%2520still%2520not%2520ro-bust%2520and%250Ageneralizable%2520enough%2520in%2520open%2520environments.%2520Evidential%2520deep%2520learn-ing%2520%2528EDL%2529%2520has%250Arecently%2520been%2520proposed%2520as%2520a%2520promising%2520solution%2520to%2520model%2520predictive%2520uncertainty%250Aand%2520improve%2520the%2520reliability%2520of%2520medical%2520image%2520segmen-tation.%2520However%2520directly%250Aapplying%2520EDL%2520to%2520scribble-supervised%2520medical%2520im-age%2520segmentation%2520faces%2520a%250Atradeoff%2520between%2520accuracy%2520and%2520reliability.%2520To%2520ad-dress%2520the%2520challenge%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520called%2520Dual-Branch%2520Evi-dential%2520Deep%2520Learning%2520%2528DuEDL%2529.%250AFirstly%252C%2520the%2520decoder%2520of%2520the%2520segmentation%2520network%2520is%2520changed%2520to%2520two%2520different%250Abranches%252C%2520and%2520the%2520evidence%2520of%2520the%2520two%2520branches%2520is%2520fused%2520to%2520generate%250Ahigh-quality%2520pseudo-labels.%2520Then%2520the%2520frame-work%2520applies%2520partial%2520evidence%2520loss%250Aand%2520two-branch%2520consistent%2520loss%2520for%2520joint%2520training%2520of%2520the%2520model%2520to%2520adapt%2520to%2520the%250Ascribble%2520supervision%2520learning.%2520The%2520pro-posed%2520method%2520was%2520tested%2520on%2520two%2520cardiac%250Adatasets%253A%2520ACDC%2520and%2520MSCMRseg.%2520The%2520results%2520show%2520that%2520our%2520method%2520significantly%250Aenhances%2520the%2520reliability%2520and%2520generalization%2520ability%2520of%2520the%2520model%2520without%250Asacrificing%2520accuracy%252C%2520outper-forming%2520state-of-the-art%2520baselines.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Gardnery/DuEDL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuEDL%3A%20Dual-Branch%20Evidential%20Deep%20Learning%20for%20Scribble-Supervised%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yitong%20Yang%20and%20Xinli%20Xu%20and%20Haigen%20Hu%20and%20Haixia%20Long%20and%20Qianwei%20Zhou%20and%20Qiu%20Guan&entry.1292438233=%20%20Despite%20the%20recent%20progress%20in%20medical%20image%20segmentation%20with%20scribble-based%0Aannotations%2C%20the%20segmentation%20results%20of%20most%20models%20are%20still%20not%20ro-bust%20and%0Ageneralizable%20enough%20in%20open%20environments.%20Evidential%20deep%20learn-ing%20%28EDL%29%20has%0Arecently%20been%20proposed%20as%20a%20promising%20solution%20to%20model%20predictive%20uncertainty%0Aand%20improve%20the%20reliability%20of%20medical%20image%20segmen-tation.%20However%20directly%0Aapplying%20EDL%20to%20scribble-supervised%20medical%20im-age%20segmentation%20faces%20a%0Atradeoff%20between%20accuracy%20and%20reliability.%20To%20ad-dress%20the%20challenge%2C%20we%0Apropose%20a%20novel%20framework%20called%20Dual-Branch%20Evi-dential%20Deep%20Learning%20%28DuEDL%29.%0AFirstly%2C%20the%20decoder%20of%20the%20segmentation%20network%20is%20changed%20to%20two%20different%0Abranches%2C%20and%20the%20evidence%20of%20the%20two%20branches%20is%20fused%20to%20generate%0Ahigh-quality%20pseudo-labels.%20Then%20the%20frame-work%20applies%20partial%20evidence%20loss%0Aand%20two-branch%20consistent%20loss%20for%20joint%20training%20of%20the%20model%20to%20adapt%20to%20the%0Ascribble%20supervision%20learning.%20The%20pro-posed%20method%20was%20tested%20on%20two%20cardiac%0Adatasets%3A%20ACDC%20and%20MSCMRseg.%20The%20results%20show%20that%20our%20method%20significantly%0Aenhances%20the%20reliability%20and%20generalization%20ability%20of%20the%20model%20without%0Asacrificing%20accuracy%2C%20outper-forming%20state-of-the-art%20baselines.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/Gardnery/DuEDL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14444v1&entry.124074799=Read"},
{"title": "On provable privacy vulnerabilities of graph representations", "author": "Ruofan Wu and Guanhua Fang and Qiying Pan and Mingyang Zhang and Tengfei Liu and Weiqiang Wang", "abstract": "  Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\nsimilarity-based edge reconstruction attacks (SERA), furnishing a\nnon-asymptotic analysis of their reconstruction capacities. Moreover, we\npresent empirical corroboration indicating that such attacks can perfectly\nreconstruct sparse graphs as graph size increases. Conversely, we establish\nthat sparsity is a critical factor for SERA's effectiveness, as demonstrated\nthrough analysis and experiments on (dense) stochastic block models. Finally,\nwe explore the resilience of private graph representations produced via noisy\naggregation (NAG) mechanism against SERA. Through theoretical analysis and\nempirical assessments, we affirm the mitigation of SERA using NAG . In\nparallel, we also empirically delineate instances wherein SERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.\n", "link": "http://arxiv.org/abs/2402.04033v2", "date": "2024-05-23", "relevancy": 2.1685, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4562}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations&body=Title%3A%20On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations%0AAuthor%3A%20Ruofan%20Wu%20and%20Guanhua%20Fang%20and%20Qiying%20Pan%20and%20Mingyang%20Zhang%20and%20Tengfei%20Liu%20and%20Weiqiang%20Wang%0AAbstract%3A%20%20%20Graph%20representation%20learning%20%28GRL%29%20is%20critical%20for%20extracting%20insights%20from%0Acomplex%20network%20structures%2C%20but%20it%20also%20raises%20security%20concerns%20due%20to%0Apotential%20privacy%20vulnerabilities%20in%20these%20representations.%20This%20paper%0Ainvestigates%20the%20structural%20vulnerabilities%20in%20graph%20neural%20models%20where%0Asensitive%20topological%20information%20can%20be%20inferred%20through%20edge%20reconstruction%0Aattacks.%20Our%20research%20primarily%20addresses%20the%20theoretical%20underpinnings%20of%0Asimilarity-based%20edge%20reconstruction%20attacks%20%28SERA%29%2C%20furnishing%20a%0Anon-asymptotic%20analysis%20of%20their%20reconstruction%20capacities.%20Moreover%2C%20we%0Apresent%20empirical%20corroboration%20indicating%20that%20such%20attacks%20can%20perfectly%0Areconstruct%20sparse%20graphs%20as%20graph%20size%20increases.%20Conversely%2C%20we%20establish%0Athat%20sparsity%20is%20a%20critical%20factor%20for%20SERA%27s%20effectiveness%2C%20as%20demonstrated%0Athrough%20analysis%20and%20experiments%20on%20%28dense%29%20stochastic%20block%20models.%20Finally%2C%0Awe%20explore%20the%20resilience%20of%20private%20graph%20representations%20produced%20via%20noisy%0Aaggregation%20%28NAG%29%20mechanism%20against%20SERA.%20Through%20theoretical%20analysis%20and%0Aempirical%20assessments%2C%20we%20affirm%20the%20mitigation%20of%20SERA%20using%20NAG%20.%20In%0Aparallel%2C%20we%20also%20empirically%20delineate%20instances%20wherein%20SERA%20demonstrates%0Aboth%20efficacy%20and%20deficiency%20in%20its%20capacity%20to%20function%20as%20an%20instrument%20for%0Aelucidating%20the%20trade-off%20between%20privacy%20and%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520provable%2520privacy%2520vulnerabilities%2520of%2520graph%2520representations%26entry.906535625%3DRuofan%2520Wu%2520and%2520Guanhua%2520Fang%2520and%2520Qiying%2520Pan%2520and%2520Mingyang%2520Zhang%2520and%2520Tengfei%2520Liu%2520and%2520Weiqiang%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520%2528GRL%2529%2520is%2520critical%2520for%2520extracting%2520insights%2520from%250Acomplex%2520network%2520structures%252C%2520but%2520it%2520also%2520raises%2520security%2520concerns%2520due%2520to%250Apotential%2520privacy%2520vulnerabilities%2520in%2520these%2520representations.%2520This%2520paper%250Ainvestigates%2520the%2520structural%2520vulnerabilities%2520in%2520graph%2520neural%2520models%2520where%250Asensitive%2520topological%2520information%2520can%2520be%2520inferred%2520through%2520edge%2520reconstruction%250Aattacks.%2520Our%2520research%2520primarily%2520addresses%2520the%2520theoretical%2520underpinnings%2520of%250Asimilarity-based%2520edge%2520reconstruction%2520attacks%2520%2528SERA%2529%252C%2520furnishing%2520a%250Anon-asymptotic%2520analysis%2520of%2520their%2520reconstruction%2520capacities.%2520Moreover%252C%2520we%250Apresent%2520empirical%2520corroboration%2520indicating%2520that%2520such%2520attacks%2520can%2520perfectly%250Areconstruct%2520sparse%2520graphs%2520as%2520graph%2520size%2520increases.%2520Conversely%252C%2520we%2520establish%250Athat%2520sparsity%2520is%2520a%2520critical%2520factor%2520for%2520SERA%2527s%2520effectiveness%252C%2520as%2520demonstrated%250Athrough%2520analysis%2520and%2520experiments%2520on%2520%2528dense%2529%2520stochastic%2520block%2520models.%2520Finally%252C%250Awe%2520explore%2520the%2520resilience%2520of%2520private%2520graph%2520representations%2520produced%2520via%2520noisy%250Aaggregation%2520%2528NAG%2529%2520mechanism%2520against%2520SERA.%2520Through%2520theoretical%2520analysis%2520and%250Aempirical%2520assessments%252C%2520we%2520affirm%2520the%2520mitigation%2520of%2520SERA%2520using%2520NAG%2520.%2520In%250Aparallel%252C%2520we%2520also%2520empirically%2520delineate%2520instances%2520wherein%2520SERA%2520demonstrates%250Aboth%2520efficacy%2520and%2520deficiency%2520in%2520its%2520capacity%2520to%2520function%2520as%2520an%2520instrument%2520for%250Aelucidating%2520the%2520trade-off%2520between%2520privacy%2520and%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations&entry.906535625=Ruofan%20Wu%20and%20Guanhua%20Fang%20and%20Qiying%20Pan%20and%20Mingyang%20Zhang%20and%20Tengfei%20Liu%20and%20Weiqiang%20Wang&entry.1292438233=%20%20Graph%20representation%20learning%20%28GRL%29%20is%20critical%20for%20extracting%20insights%20from%0Acomplex%20network%20structures%2C%20but%20it%20also%20raises%20security%20concerns%20due%20to%0Apotential%20privacy%20vulnerabilities%20in%20these%20representations.%20This%20paper%0Ainvestigates%20the%20structural%20vulnerabilities%20in%20graph%20neural%20models%20where%0Asensitive%20topological%20information%20can%20be%20inferred%20through%20edge%20reconstruction%0Aattacks.%20Our%20research%20primarily%20addresses%20the%20theoretical%20underpinnings%20of%0Asimilarity-based%20edge%20reconstruction%20attacks%20%28SERA%29%2C%20furnishing%20a%0Anon-asymptotic%20analysis%20of%20their%20reconstruction%20capacities.%20Moreover%2C%20we%0Apresent%20empirical%20corroboration%20indicating%20that%20such%20attacks%20can%20perfectly%0Areconstruct%20sparse%20graphs%20as%20graph%20size%20increases.%20Conversely%2C%20we%20establish%0Athat%20sparsity%20is%20a%20critical%20factor%20for%20SERA%27s%20effectiveness%2C%20as%20demonstrated%0Athrough%20analysis%20and%20experiments%20on%20%28dense%29%20stochastic%20block%20models.%20Finally%2C%0Awe%20explore%20the%20resilience%20of%20private%20graph%20representations%20produced%20via%20noisy%0Aaggregation%20%28NAG%29%20mechanism%20against%20SERA.%20Through%20theoretical%20analysis%20and%0Aempirical%20assessments%2C%20we%20affirm%20the%20mitigation%20of%20SERA%20using%20NAG%20.%20In%0Aparallel%2C%20we%20also%20empirically%20delineate%20instances%20wherein%20SERA%20demonstrates%0Aboth%20efficacy%20and%20deficiency%20in%20its%20capacity%20to%20function%20as%20an%20instrument%20for%0Aelucidating%20the%20trade-off%20between%20privacy%20and%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04033v2&entry.124074799=Read"},
{"title": "Domain-specific augmentations with resolution agnostic self-attention\n  mechanism improves choroid segmentation in optical coherence tomography\n  images", "author": "Jamie Burke and Justin Engelmann and Charlene Hamid and Diana Moukaddem and Dan Pugh and Neeraj Dhaun and Amos Storkey and Niall Strang and Stuart King and Tom MacGillivray and Miguel O. Bernabeu and Ian J. C. MacCormick", "abstract": "  The choroid is a key vascular layer of the eye, supplying oxygen to the\nretinal photoreceptors. Non-invasive enhanced depth imaging optical coherence\ntomography (EDI-OCT) has recently improved access and visualisation of the\nchoroid, making it an exciting frontier for discovering novel vascular\nbiomarkers in ophthalmology and wider systemic health. However, current methods\nto measure the choroid often require use of multiple, independent\nsemi-automatic and deep learning-based algorithms which are not made\nopen-source. Previously, Choroidalyzer -- an open-source, fully automatic deep\nlearning method trained on 5,600 OCT B-scans from 385 eyes -- was developed to\nfully segment and quantify the choroid in EDI-OCT images, thus addressing these\nissues. Using the same dataset, we propose a Robust, Resolution-agnostic and\nEfficient Attention-based network for CHoroid segmentation (REACH). REACHNet\nleverages multi-resolution training with domain-specific data augmentation to\npromote generalisation, and uses a lightweight architecture with\nresolution-agnostic self-attention which is not only faster than\nChoroidalyzer's previous network (4 images/s vs. 2.75 images/s on a standard\nlaptop CPU), but has greater performance for segmenting the choroid region,\nvessels and fovea (Dice coefficient for region 0.9769 vs. 0.9749, vessels\n0.8612 vs. 0.8192 and fovea 0.8243 vs. 0.3783) due to its improved\nhyperparameter configuration and model training pipeline. REACHNet can be used\nwith Choroidalyzer as a drop-in replacement for the original model and will be\nmade available upon publication.\n", "link": "http://arxiv.org/abs/2405.14453v1", "date": "2024-05-23", "relevancy": 2.1474, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5499}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-specific%20augmentations%20with%20resolution%20agnostic%20self-attention%0A%20%20mechanism%20improves%20choroid%20segmentation%20in%20optical%20coherence%20tomography%0A%20%20images&body=Title%3A%20Domain-specific%20augmentations%20with%20resolution%20agnostic%20self-attention%0A%20%20mechanism%20improves%20choroid%20segmentation%20in%20optical%20coherence%20tomography%0A%20%20images%0AAuthor%3A%20Jamie%20Burke%20and%20Justin%20Engelmann%20and%20Charlene%20Hamid%20and%20Diana%20Moukaddem%20and%20Dan%20Pugh%20and%20Neeraj%20Dhaun%20and%20Amos%20Storkey%20and%20Niall%20Strang%20and%20Stuart%20King%20and%20Tom%20MacGillivray%20and%20Miguel%20O.%20Bernabeu%20and%20Ian%20J.%20C.%20MacCormick%0AAbstract%3A%20%20%20The%20choroid%20is%20a%20key%20vascular%20layer%20of%20the%20eye%2C%20supplying%20oxygen%20to%20the%0Aretinal%20photoreceptors.%20Non-invasive%20enhanced%20depth%20imaging%20optical%20coherence%0Atomography%20%28EDI-OCT%29%20has%20recently%20improved%20access%20and%20visualisation%20of%20the%0Achoroid%2C%20making%20it%20an%20exciting%20frontier%20for%20discovering%20novel%20vascular%0Abiomarkers%20in%20ophthalmology%20and%20wider%20systemic%20health.%20However%2C%20current%20methods%0Ato%20measure%20the%20choroid%20often%20require%20use%20of%20multiple%2C%20independent%0Asemi-automatic%20and%20deep%20learning-based%20algorithms%20which%20are%20not%20made%0Aopen-source.%20Previously%2C%20Choroidalyzer%20--%20an%20open-source%2C%20fully%20automatic%20deep%0Alearning%20method%20trained%20on%205%2C600%20OCT%20B-scans%20from%20385%20eyes%20--%20was%20developed%20to%0Afully%20segment%20and%20quantify%20the%20choroid%20in%20EDI-OCT%20images%2C%20thus%20addressing%20these%0Aissues.%20Using%20the%20same%20dataset%2C%20we%20propose%20a%20Robust%2C%20Resolution-agnostic%20and%0AEfficient%20Attention-based%20network%20for%20CHoroid%20segmentation%20%28REACH%29.%20REACHNet%0Aleverages%20multi-resolution%20training%20with%20domain-specific%20data%20augmentation%20to%0Apromote%20generalisation%2C%20and%20uses%20a%20lightweight%20architecture%20with%0Aresolution-agnostic%20self-attention%20which%20is%20not%20only%20faster%20than%0AChoroidalyzer%27s%20previous%20network%20%284%20images/s%20vs.%202.75%20images/s%20on%20a%20standard%0Alaptop%20CPU%29%2C%20but%20has%20greater%20performance%20for%20segmenting%20the%20choroid%20region%2C%0Avessels%20and%20fovea%20%28Dice%20coefficient%20for%20region%200.9769%20vs.%200.9749%2C%20vessels%0A0.8612%20vs.%200.8192%20and%20fovea%200.8243%20vs.%200.3783%29%20due%20to%20its%20improved%0Ahyperparameter%20configuration%20and%20model%20training%20pipeline.%20REACHNet%20can%20be%20used%0Awith%20Choroidalyzer%20as%20a%20drop-in%20replacement%20for%20the%20original%20model%20and%20will%20be%0Amade%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-specific%2520augmentations%2520with%2520resolution%2520agnostic%2520self-attention%250A%2520%2520mechanism%2520improves%2520choroid%2520segmentation%2520in%2520optical%2520coherence%2520tomography%250A%2520%2520images%26entry.906535625%3DJamie%2520Burke%2520and%2520Justin%2520Engelmann%2520and%2520Charlene%2520Hamid%2520and%2520Diana%2520Moukaddem%2520and%2520Dan%2520Pugh%2520and%2520Neeraj%2520Dhaun%2520and%2520Amos%2520Storkey%2520and%2520Niall%2520Strang%2520and%2520Stuart%2520King%2520and%2520Tom%2520MacGillivray%2520and%2520Miguel%2520O.%2520Bernabeu%2520and%2520Ian%2520J.%2520C.%2520MacCormick%26entry.1292438233%3D%2520%2520The%2520choroid%2520is%2520a%2520key%2520vascular%2520layer%2520of%2520the%2520eye%252C%2520supplying%2520oxygen%2520to%2520the%250Aretinal%2520photoreceptors.%2520Non-invasive%2520enhanced%2520depth%2520imaging%2520optical%2520coherence%250Atomography%2520%2528EDI-OCT%2529%2520has%2520recently%2520improved%2520access%2520and%2520visualisation%2520of%2520the%250Achoroid%252C%2520making%2520it%2520an%2520exciting%2520frontier%2520for%2520discovering%2520novel%2520vascular%250Abiomarkers%2520in%2520ophthalmology%2520and%2520wider%2520systemic%2520health.%2520However%252C%2520current%2520methods%250Ato%2520measure%2520the%2520choroid%2520often%2520require%2520use%2520of%2520multiple%252C%2520independent%250Asemi-automatic%2520and%2520deep%2520learning-based%2520algorithms%2520which%2520are%2520not%2520made%250Aopen-source.%2520Previously%252C%2520Choroidalyzer%2520--%2520an%2520open-source%252C%2520fully%2520automatic%2520deep%250Alearning%2520method%2520trained%2520on%25205%252C600%2520OCT%2520B-scans%2520from%2520385%2520eyes%2520--%2520was%2520developed%2520to%250Afully%2520segment%2520and%2520quantify%2520the%2520choroid%2520in%2520EDI-OCT%2520images%252C%2520thus%2520addressing%2520these%250Aissues.%2520Using%2520the%2520same%2520dataset%252C%2520we%2520propose%2520a%2520Robust%252C%2520Resolution-agnostic%2520and%250AEfficient%2520Attention-based%2520network%2520for%2520CHoroid%2520segmentation%2520%2528REACH%2529.%2520REACHNet%250Aleverages%2520multi-resolution%2520training%2520with%2520domain-specific%2520data%2520augmentation%2520to%250Apromote%2520generalisation%252C%2520and%2520uses%2520a%2520lightweight%2520architecture%2520with%250Aresolution-agnostic%2520self-attention%2520which%2520is%2520not%2520only%2520faster%2520than%250AChoroidalyzer%2527s%2520previous%2520network%2520%25284%2520images/s%2520vs.%25202.75%2520images/s%2520on%2520a%2520standard%250Alaptop%2520CPU%2529%252C%2520but%2520has%2520greater%2520performance%2520for%2520segmenting%2520the%2520choroid%2520region%252C%250Avessels%2520and%2520fovea%2520%2528Dice%2520coefficient%2520for%2520region%25200.9769%2520vs.%25200.9749%252C%2520vessels%250A0.8612%2520vs.%25200.8192%2520and%2520fovea%25200.8243%2520vs.%25200.3783%2529%2520due%2520to%2520its%2520improved%250Ahyperparameter%2520configuration%2520and%2520model%2520training%2520pipeline.%2520REACHNet%2520can%2520be%2520used%250Awith%2520Choroidalyzer%2520as%2520a%2520drop-in%2520replacement%2520for%2520the%2520original%2520model%2520and%2520will%2520be%250Amade%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-specific%20augmentations%20with%20resolution%20agnostic%20self-attention%0A%20%20mechanism%20improves%20choroid%20segmentation%20in%20optical%20coherence%20tomography%0A%20%20images&entry.906535625=Jamie%20Burke%20and%20Justin%20Engelmann%20and%20Charlene%20Hamid%20and%20Diana%20Moukaddem%20and%20Dan%20Pugh%20and%20Neeraj%20Dhaun%20and%20Amos%20Storkey%20and%20Niall%20Strang%20and%20Stuart%20King%20and%20Tom%20MacGillivray%20and%20Miguel%20O.%20Bernabeu%20and%20Ian%20J.%20C.%20MacCormick&entry.1292438233=%20%20The%20choroid%20is%20a%20key%20vascular%20layer%20of%20the%20eye%2C%20supplying%20oxygen%20to%20the%0Aretinal%20photoreceptors.%20Non-invasive%20enhanced%20depth%20imaging%20optical%20coherence%0Atomography%20%28EDI-OCT%29%20has%20recently%20improved%20access%20and%20visualisation%20of%20the%0Achoroid%2C%20making%20it%20an%20exciting%20frontier%20for%20discovering%20novel%20vascular%0Abiomarkers%20in%20ophthalmology%20and%20wider%20systemic%20health.%20However%2C%20current%20methods%0Ato%20measure%20the%20choroid%20often%20require%20use%20of%20multiple%2C%20independent%0Asemi-automatic%20and%20deep%20learning-based%20algorithms%20which%20are%20not%20made%0Aopen-source.%20Previously%2C%20Choroidalyzer%20--%20an%20open-source%2C%20fully%20automatic%20deep%0Alearning%20method%20trained%20on%205%2C600%20OCT%20B-scans%20from%20385%20eyes%20--%20was%20developed%20to%0Afully%20segment%20and%20quantify%20the%20choroid%20in%20EDI-OCT%20images%2C%20thus%20addressing%20these%0Aissues.%20Using%20the%20same%20dataset%2C%20we%20propose%20a%20Robust%2C%20Resolution-agnostic%20and%0AEfficient%20Attention-based%20network%20for%20CHoroid%20segmentation%20%28REACH%29.%20REACHNet%0Aleverages%20multi-resolution%20training%20with%20domain-specific%20data%20augmentation%20to%0Apromote%20generalisation%2C%20and%20uses%20a%20lightweight%20architecture%20with%0Aresolution-agnostic%20self-attention%20which%20is%20not%20only%20faster%20than%0AChoroidalyzer%27s%20previous%20network%20%284%20images/s%20vs.%202.75%20images/s%20on%20a%20standard%0Alaptop%20CPU%29%2C%20but%20has%20greater%20performance%20for%20segmenting%20the%20choroid%20region%2C%0Avessels%20and%20fovea%20%28Dice%20coefficient%20for%20region%200.9769%20vs.%200.9749%2C%20vessels%0A0.8612%20vs.%200.8192%20and%20fovea%200.8243%20vs.%200.3783%29%20due%20to%20its%20improved%0Ahyperparameter%20configuration%20and%20model%20training%20pipeline.%20REACHNet%20can%20be%20used%0Awith%20Choroidalyzer%20as%20a%20drop-in%20replacement%20for%20the%20original%20model%20and%20will%20be%0Amade%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14453v1&entry.124074799=Read"},
{"title": "Towards Realistic Long-tailed Semi-supervised Learning in an Open World", "author": "Yuanpeng He and Lijian Li", "abstract": "  Open-world long-tailed semi-supervised learning (OLSSL) has increasingly\nattracted attention. However, existing OLSSL algorithms generally assume that\nthe distributions between known and novel categories are nearly identical.\nAgainst this backdrop, we construct a more \\emph{Realistic Open-world\nLong-tailed Semi-supervised Learning} (\\textbf{ROLSSL}) setting where there is\nno premise on the distribution relationships between known and novel\ncategories. Furthermore, even within the known categories, the number of\nlabeled samples is significantly smaller than that of the unlabeled samples, as\nacquiring valid annotations is often prohibitively costly in the real world.\nUnder the proposed ROLSSL setting, we propose a simple yet potentially\neffective solution called dual-stage post-hoc logit adjustments. The proposed\napproach revisits the logit adjustment strategy by considering the\nrelationships among the frequency of samples, the total number of categories,\nand the overall size of data. Then, it estimates the distribution of unlabeled\ndata for both known and novel categories to dynamically readjust the\ncorresponding predictive probabilities, effectively mitigating category bias\nduring the learning of known and novel classes with more selective utilization\nof imbalanced unlabeled data. Extensive experiments on datasets such as\nCIFAR100 and ImageNet100 have demonstrated performance improvements of up to\n50.1\\%, validating the superiority of our proposed method and establishing a\nstrong baseline for this task. For further researches, the anonymous link to\nthe experimental code is at\n\\href{https://github.com/heyuanpengpku/ROLSSL}{\\textcolor{brightpink}{https://github.com/heyuanpengpku/ROLSSL}}\n", "link": "http://arxiv.org/abs/2405.14516v1", "date": "2024-05-23", "relevancy": 2.1472, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Realistic%20Long-tailed%20Semi-supervised%20Learning%20in%20an%20Open%20World&body=Title%3A%20Towards%20Realistic%20Long-tailed%20Semi-supervised%20Learning%20in%20an%20Open%20World%0AAuthor%3A%20Yuanpeng%20He%20and%20Lijian%20Li%0AAbstract%3A%20%20%20Open-world%20long-tailed%20semi-supervised%20learning%20%28OLSSL%29%20has%20increasingly%0Aattracted%20attention.%20However%2C%20existing%20OLSSL%20algorithms%20generally%20assume%20that%0Athe%20distributions%20between%20known%20and%20novel%20categories%20are%20nearly%20identical.%0AAgainst%20this%20backdrop%2C%20we%20construct%20a%20more%20%5Cemph%7BRealistic%20Open-world%0ALong-tailed%20Semi-supervised%20Learning%7D%20%28%5Ctextbf%7BROLSSL%7D%29%20setting%20where%20there%20is%0Ano%20premise%20on%20the%20distribution%20relationships%20between%20known%20and%20novel%0Acategories.%20Furthermore%2C%20even%20within%20the%20known%20categories%2C%20the%20number%20of%0Alabeled%20samples%20is%20significantly%20smaller%20than%20that%20of%20the%20unlabeled%20samples%2C%20as%0Aacquiring%20valid%20annotations%20is%20often%20prohibitively%20costly%20in%20the%20real%20world.%0AUnder%20the%20proposed%20ROLSSL%20setting%2C%20we%20propose%20a%20simple%20yet%20potentially%0Aeffective%20solution%20called%20dual-stage%20post-hoc%20logit%20adjustments.%20The%20proposed%0Aapproach%20revisits%20the%20logit%20adjustment%20strategy%20by%20considering%20the%0Arelationships%20among%20the%20frequency%20of%20samples%2C%20the%20total%20number%20of%20categories%2C%0Aand%20the%20overall%20size%20of%20data.%20Then%2C%20it%20estimates%20the%20distribution%20of%20unlabeled%0Adata%20for%20both%20known%20and%20novel%20categories%20to%20dynamically%20readjust%20the%0Acorresponding%20predictive%20probabilities%2C%20effectively%20mitigating%20category%20bias%0Aduring%20the%20learning%20of%20known%20and%20novel%20classes%20with%20more%20selective%20utilization%0Aof%20imbalanced%20unlabeled%20data.%20Extensive%20experiments%20on%20datasets%20such%20as%0ACIFAR100%20and%20ImageNet100%20have%20demonstrated%20performance%20improvements%20of%20up%20to%0A50.1%5C%25%2C%20validating%20the%20superiority%20of%20our%20proposed%20method%20and%20establishing%20a%0Astrong%20baseline%20for%20this%20task.%20For%20further%20researches%2C%20the%20anonymous%20link%20to%0Athe%20experimental%20code%20is%20at%0A%5Chref%7Bhttps%3A//github.com/heyuanpengpku/ROLSSL%7D%7B%5Ctextcolor%7Bbrightpink%7D%7Bhttps%3A//github.com/heyuanpengpku/ROLSSL%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Realistic%2520Long-tailed%2520Semi-supervised%2520Learning%2520in%2520an%2520Open%2520World%26entry.906535625%3DYuanpeng%2520He%2520and%2520Lijian%2520Li%26entry.1292438233%3D%2520%2520Open-world%2520long-tailed%2520semi-supervised%2520learning%2520%2528OLSSL%2529%2520has%2520increasingly%250Aattracted%2520attention.%2520However%252C%2520existing%2520OLSSL%2520algorithms%2520generally%2520assume%2520that%250Athe%2520distributions%2520between%2520known%2520and%2520novel%2520categories%2520are%2520nearly%2520identical.%250AAgainst%2520this%2520backdrop%252C%2520we%2520construct%2520a%2520more%2520%255Cemph%257BRealistic%2520Open-world%250ALong-tailed%2520Semi-supervised%2520Learning%257D%2520%2528%255Ctextbf%257BROLSSL%257D%2529%2520setting%2520where%2520there%2520is%250Ano%2520premise%2520on%2520the%2520distribution%2520relationships%2520between%2520known%2520and%2520novel%250Acategories.%2520Furthermore%252C%2520even%2520within%2520the%2520known%2520categories%252C%2520the%2520number%2520of%250Alabeled%2520samples%2520is%2520significantly%2520smaller%2520than%2520that%2520of%2520the%2520unlabeled%2520samples%252C%2520as%250Aacquiring%2520valid%2520annotations%2520is%2520often%2520prohibitively%2520costly%2520in%2520the%2520real%2520world.%250AUnder%2520the%2520proposed%2520ROLSSL%2520setting%252C%2520we%2520propose%2520a%2520simple%2520yet%2520potentially%250Aeffective%2520solution%2520called%2520dual-stage%2520post-hoc%2520logit%2520adjustments.%2520The%2520proposed%250Aapproach%2520revisits%2520the%2520logit%2520adjustment%2520strategy%2520by%2520considering%2520the%250Arelationships%2520among%2520the%2520frequency%2520of%2520samples%252C%2520the%2520total%2520number%2520of%2520categories%252C%250Aand%2520the%2520overall%2520size%2520of%2520data.%2520Then%252C%2520it%2520estimates%2520the%2520distribution%2520of%2520unlabeled%250Adata%2520for%2520both%2520known%2520and%2520novel%2520categories%2520to%2520dynamically%2520readjust%2520the%250Acorresponding%2520predictive%2520probabilities%252C%2520effectively%2520mitigating%2520category%2520bias%250Aduring%2520the%2520learning%2520of%2520known%2520and%2520novel%2520classes%2520with%2520more%2520selective%2520utilization%250Aof%2520imbalanced%2520unlabeled%2520data.%2520Extensive%2520experiments%2520on%2520datasets%2520such%2520as%250ACIFAR100%2520and%2520ImageNet100%2520have%2520demonstrated%2520performance%2520improvements%2520of%2520up%2520to%250A50.1%255C%2525%252C%2520validating%2520the%2520superiority%2520of%2520our%2520proposed%2520method%2520and%2520establishing%2520a%250Astrong%2520baseline%2520for%2520this%2520task.%2520For%2520further%2520researches%252C%2520the%2520anonymous%2520link%2520to%250Athe%2520experimental%2520code%2520is%2520at%250A%255Chref%257Bhttps%253A//github.com/heyuanpengpku/ROLSSL%257D%257B%255Ctextcolor%257Bbrightpink%257D%257Bhttps%253A//github.com/heyuanpengpku/ROLSSL%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Realistic%20Long-tailed%20Semi-supervised%20Learning%20in%20an%20Open%20World&entry.906535625=Yuanpeng%20He%20and%20Lijian%20Li&entry.1292438233=%20%20Open-world%20long-tailed%20semi-supervised%20learning%20%28OLSSL%29%20has%20increasingly%0Aattracted%20attention.%20However%2C%20existing%20OLSSL%20algorithms%20generally%20assume%20that%0Athe%20distributions%20between%20known%20and%20novel%20categories%20are%20nearly%20identical.%0AAgainst%20this%20backdrop%2C%20we%20construct%20a%20more%20%5Cemph%7BRealistic%20Open-world%0ALong-tailed%20Semi-supervised%20Learning%7D%20%28%5Ctextbf%7BROLSSL%7D%29%20setting%20where%20there%20is%0Ano%20premise%20on%20the%20distribution%20relationships%20between%20known%20and%20novel%0Acategories.%20Furthermore%2C%20even%20within%20the%20known%20categories%2C%20the%20number%20of%0Alabeled%20samples%20is%20significantly%20smaller%20than%20that%20of%20the%20unlabeled%20samples%2C%20as%0Aacquiring%20valid%20annotations%20is%20often%20prohibitively%20costly%20in%20the%20real%20world.%0AUnder%20the%20proposed%20ROLSSL%20setting%2C%20we%20propose%20a%20simple%20yet%20potentially%0Aeffective%20solution%20called%20dual-stage%20post-hoc%20logit%20adjustments.%20The%20proposed%0Aapproach%20revisits%20the%20logit%20adjustment%20strategy%20by%20considering%20the%0Arelationships%20among%20the%20frequency%20of%20samples%2C%20the%20total%20number%20of%20categories%2C%0Aand%20the%20overall%20size%20of%20data.%20Then%2C%20it%20estimates%20the%20distribution%20of%20unlabeled%0Adata%20for%20both%20known%20and%20novel%20categories%20to%20dynamically%20readjust%20the%0Acorresponding%20predictive%20probabilities%2C%20effectively%20mitigating%20category%20bias%0Aduring%20the%20learning%20of%20known%20and%20novel%20classes%20with%20more%20selective%20utilization%0Aof%20imbalanced%20unlabeled%20data.%20Extensive%20experiments%20on%20datasets%20such%20as%0ACIFAR100%20and%20ImageNet100%20have%20demonstrated%20performance%20improvements%20of%20up%20to%0A50.1%5C%25%2C%20validating%20the%20superiority%20of%20our%20proposed%20method%20and%20establishing%20a%0Astrong%20baseline%20for%20this%20task.%20For%20further%20researches%2C%20the%20anonymous%20link%20to%0Athe%20experimental%20code%20is%20at%0A%5Chref%7Bhttps%3A//github.com/heyuanpengpku/ROLSSL%7D%7B%5Ctextcolor%7Bbrightpink%7D%7Bhttps%3A//github.com/heyuanpengpku/ROLSSL%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14516v1&entry.124074799=Read"},
{"title": "Towards Educator-Driven Tutor Authoring: Generative AI Approaches for\n  Creating Intelligent Tutor Interfaces", "author": "Tommaso Calo and Christopher J. MacLellan", "abstract": "  Intelligent Tutoring Systems (ITSs) have shown great potential in delivering\npersonalized and adaptive education, but their widespread adoption has been\nhindered by the need for specialized programming and design skills. Existing\napproaches overcome the programming limitations with no-code authoring through\ndrag and drop, however they assume that educators possess the necessary skills\nto design effective and engaging tutor interfaces. To address this assumption\nwe introduce generative AI capabilities to assist educators in creating tutor\ninterfaces that meet their needs while adhering to design principles. Our\napproach leverages Large Language Models (LLMs) and prompt engineering to\ngenerate tutor layout and contents based on high-level requirements provided by\neducators as inputs. However, to allow them to actively participate in the\ndesign process, rather than relying entirely on AI-generated solutions, we\nallow generation both at the entire interface level and at the individual\ncomponent level. The former provides educators with a complete interface that\ncan be refined using direct manipulation, while the latter offers the ability\nto create specific elements to be added to the tutor interface. A small-scale\ncomparison shows the potential of our approach to enhance the efficiency of\ntutor interface design. Moving forward, we raise critical questions for\nassisting educators with generative AI capabilities to create personalized,\neffective, and engaging tutors, ultimately enhancing their adoption.\n", "link": "http://arxiv.org/abs/2405.14713v1", "date": "2024-05-23", "relevancy": 2.1432, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5577}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5322}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Educator-Driven%20Tutor%20Authoring%3A%20Generative%20AI%20Approaches%20for%0A%20%20Creating%20Intelligent%20Tutor%20Interfaces&body=Title%3A%20Towards%20Educator-Driven%20Tutor%20Authoring%3A%20Generative%20AI%20Approaches%20for%0A%20%20Creating%20Intelligent%20Tutor%20Interfaces%0AAuthor%3A%20Tommaso%20Calo%20and%20Christopher%20J.%20MacLellan%0AAbstract%3A%20%20%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20have%20shown%20great%20potential%20in%20delivering%0Apersonalized%20and%20adaptive%20education%2C%20but%20their%20widespread%20adoption%20has%20been%0Ahindered%20by%20the%20need%20for%20specialized%20programming%20and%20design%20skills.%20Existing%0Aapproaches%20overcome%20the%20programming%20limitations%20with%20no-code%20authoring%20through%0Adrag%20and%20drop%2C%20however%20they%20assume%20that%20educators%20possess%20the%20necessary%20skills%0Ato%20design%20effective%20and%20engaging%20tutor%20interfaces.%20To%20address%20this%20assumption%0Awe%20introduce%20generative%20AI%20capabilities%20to%20assist%20educators%20in%20creating%20tutor%0Ainterfaces%20that%20meet%20their%20needs%20while%20adhering%20to%20design%20principles.%20Our%0Aapproach%20leverages%20Large%20Language%20Models%20%28LLMs%29%20and%20prompt%20engineering%20to%0Agenerate%20tutor%20layout%20and%20contents%20based%20on%20high-level%20requirements%20provided%20by%0Aeducators%20as%20inputs.%20However%2C%20to%20allow%20them%20to%20actively%20participate%20in%20the%0Adesign%20process%2C%20rather%20than%20relying%20entirely%20on%20AI-generated%20solutions%2C%20we%0Aallow%20generation%20both%20at%20the%20entire%20interface%20level%20and%20at%20the%20individual%0Acomponent%20level.%20The%20former%20provides%20educators%20with%20a%20complete%20interface%20that%0Acan%20be%20refined%20using%20direct%20manipulation%2C%20while%20the%20latter%20offers%20the%20ability%0Ato%20create%20specific%20elements%20to%20be%20added%20to%20the%20tutor%20interface.%20A%20small-scale%0Acomparison%20shows%20the%20potential%20of%20our%20approach%20to%20enhance%20the%20efficiency%20of%0Atutor%20interface%20design.%20Moving%20forward%2C%20we%20raise%20critical%20questions%20for%0Aassisting%20educators%20with%20generative%20AI%20capabilities%20to%20create%20personalized%2C%0Aeffective%2C%20and%20engaging%20tutors%2C%20ultimately%20enhancing%20their%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Educator-Driven%2520Tutor%2520Authoring%253A%2520Generative%2520AI%2520Approaches%2520for%250A%2520%2520Creating%2520Intelligent%2520Tutor%2520Interfaces%26entry.906535625%3DTommaso%2520Calo%2520and%2520Christopher%2520J.%2520MacLellan%26entry.1292438233%3D%2520%2520Intelligent%2520Tutoring%2520Systems%2520%2528ITSs%2529%2520have%2520shown%2520great%2520potential%2520in%2520delivering%250Apersonalized%2520and%2520adaptive%2520education%252C%2520but%2520their%2520widespread%2520adoption%2520has%2520been%250Ahindered%2520by%2520the%2520need%2520for%2520specialized%2520programming%2520and%2520design%2520skills.%2520Existing%250Aapproaches%2520overcome%2520the%2520programming%2520limitations%2520with%2520no-code%2520authoring%2520through%250Adrag%2520and%2520drop%252C%2520however%2520they%2520assume%2520that%2520educators%2520possess%2520the%2520necessary%2520skills%250Ato%2520design%2520effective%2520and%2520engaging%2520tutor%2520interfaces.%2520To%2520address%2520this%2520assumption%250Awe%2520introduce%2520generative%2520AI%2520capabilities%2520to%2520assist%2520educators%2520in%2520creating%2520tutor%250Ainterfaces%2520that%2520meet%2520their%2520needs%2520while%2520adhering%2520to%2520design%2520principles.%2520Our%250Aapproach%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520prompt%2520engineering%2520to%250Agenerate%2520tutor%2520layout%2520and%2520contents%2520based%2520on%2520high-level%2520requirements%2520provided%2520by%250Aeducators%2520as%2520inputs.%2520However%252C%2520to%2520allow%2520them%2520to%2520actively%2520participate%2520in%2520the%250Adesign%2520process%252C%2520rather%2520than%2520relying%2520entirely%2520on%2520AI-generated%2520solutions%252C%2520we%250Aallow%2520generation%2520both%2520at%2520the%2520entire%2520interface%2520level%2520and%2520at%2520the%2520individual%250Acomponent%2520level.%2520The%2520former%2520provides%2520educators%2520with%2520a%2520complete%2520interface%2520that%250Acan%2520be%2520refined%2520using%2520direct%2520manipulation%252C%2520while%2520the%2520latter%2520offers%2520the%2520ability%250Ato%2520create%2520specific%2520elements%2520to%2520be%2520added%2520to%2520the%2520tutor%2520interface.%2520A%2520small-scale%250Acomparison%2520shows%2520the%2520potential%2520of%2520our%2520approach%2520to%2520enhance%2520the%2520efficiency%2520of%250Atutor%2520interface%2520design.%2520Moving%2520forward%252C%2520we%2520raise%2520critical%2520questions%2520for%250Aassisting%2520educators%2520with%2520generative%2520AI%2520capabilities%2520to%2520create%2520personalized%252C%250Aeffective%252C%2520and%2520engaging%2520tutors%252C%2520ultimately%2520enhancing%2520their%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Educator-Driven%20Tutor%20Authoring%3A%20Generative%20AI%20Approaches%20for%0A%20%20Creating%20Intelligent%20Tutor%20Interfaces&entry.906535625=Tommaso%20Calo%20and%20Christopher%20J.%20MacLellan&entry.1292438233=%20%20Intelligent%20Tutoring%20Systems%20%28ITSs%29%20have%20shown%20great%20potential%20in%20delivering%0Apersonalized%20and%20adaptive%20education%2C%20but%20their%20widespread%20adoption%20has%20been%0Ahindered%20by%20the%20need%20for%20specialized%20programming%20and%20design%20skills.%20Existing%0Aapproaches%20overcome%20the%20programming%20limitations%20with%20no-code%20authoring%20through%0Adrag%20and%20drop%2C%20however%20they%20assume%20that%20educators%20possess%20the%20necessary%20skills%0Ato%20design%20effective%20and%20engaging%20tutor%20interfaces.%20To%20address%20this%20assumption%0Awe%20introduce%20generative%20AI%20capabilities%20to%20assist%20educators%20in%20creating%20tutor%0Ainterfaces%20that%20meet%20their%20needs%20while%20adhering%20to%20design%20principles.%20Our%0Aapproach%20leverages%20Large%20Language%20Models%20%28LLMs%29%20and%20prompt%20engineering%20to%0Agenerate%20tutor%20layout%20and%20contents%20based%20on%20high-level%20requirements%20provided%20by%0Aeducators%20as%20inputs.%20However%2C%20to%20allow%20them%20to%20actively%20participate%20in%20the%0Adesign%20process%2C%20rather%20than%20relying%20entirely%20on%20AI-generated%20solutions%2C%20we%0Aallow%20generation%20both%20at%20the%20entire%20interface%20level%20and%20at%20the%20individual%0Acomponent%20level.%20The%20former%20provides%20educators%20with%20a%20complete%20interface%20that%0Acan%20be%20refined%20using%20direct%20manipulation%2C%20while%20the%20latter%20offers%20the%20ability%0Ato%20create%20specific%20elements%20to%20be%20added%20to%20the%20tutor%20interface.%20A%20small-scale%0Acomparison%20shows%20the%20potential%20of%20our%20approach%20to%20enhance%20the%20efficiency%20of%0Atutor%20interface%20design.%20Moving%20forward%2C%20we%20raise%20critical%20questions%20for%0Aassisting%20educators%20with%20generative%20AI%20capabilities%20to%20create%20personalized%2C%0Aeffective%2C%20and%20engaging%20tutors%2C%20ultimately%20enhancing%20their%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14713v1&entry.124074799=Read"},
{"title": "Improving Single Domain-Generalized Object Detection: A Focus on\n  Diversification and Alignment", "author": "Muhammad Sohail Danish and Muhammad Haris Khan and Muhammad Akhtar Munir and M. Saquib Sarfraz and Mohsen Ali", "abstract": "  In this work, we tackle the problem of domain generalization for object\ndetection, specifically focusing on the scenario where only a single source\ndomain is available. We propose an effective approach that involves two key\nsteps: diversifying the source domain and aligning detections based on class\nprediction confidence and localization. Firstly, we demonstrate that by\ncarefully selecting a set of augmentations, a base detector can outperform\nexisting methods for single domain generalization by a good margin. This\nhighlights the importance of domain diversification in improving the\nperformance of object detectors. Secondly, we introduce a method to align\ndetections from multiple views, considering both classification and\nlocalization outputs. This alignment procedure leads to better generalized and\nwell-calibrated object detector models, which are crucial for accurate\ndecision-making in safety-critical applications. Our approach is\ndetector-agnostic and can be seamlessly applied to both single-stage and\ntwo-stage detectors. To validate the effectiveness of our proposed methods, we\nconduct extensive experiments and ablations on challenging domain-shift\nscenarios. The results consistently demonstrate the superiority of our approach\ncompared to existing methods. Our code and models are available at:\nhttps://github.com/msohaildanish/DivAlign\n", "link": "http://arxiv.org/abs/2405.14497v1", "date": "2024-05-23", "relevancy": 2.143, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5476}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5283}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Single%20Domain-Generalized%20Object%20Detection%3A%20A%20Focus%20on%0A%20%20Diversification%20and%20Alignment&body=Title%3A%20Improving%20Single%20Domain-Generalized%20Object%20Detection%3A%20A%20Focus%20on%0A%20%20Diversification%20and%20Alignment%0AAuthor%3A%20Muhammad%20Sohail%20Danish%20and%20Muhammad%20Haris%20Khan%20and%20Muhammad%20Akhtar%20Munir%20and%20M.%20Saquib%20Sarfraz%20and%20Mohsen%20Ali%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20domain%20generalization%20for%20object%0Adetection%2C%20specifically%20focusing%20on%20the%20scenario%20where%20only%20a%20single%20source%0Adomain%20is%20available.%20We%20propose%20an%20effective%20approach%20that%20involves%20two%20key%0Asteps%3A%20diversifying%20the%20source%20domain%20and%20aligning%20detections%20based%20on%20class%0Aprediction%20confidence%20and%20localization.%20Firstly%2C%20we%20demonstrate%20that%20by%0Acarefully%20selecting%20a%20set%20of%20augmentations%2C%20a%20base%20detector%20can%20outperform%0Aexisting%20methods%20for%20single%20domain%20generalization%20by%20a%20good%20margin.%20This%0Ahighlights%20the%20importance%20of%20domain%20diversification%20in%20improving%20the%0Aperformance%20of%20object%20detectors.%20Secondly%2C%20we%20introduce%20a%20method%20to%20align%0Adetections%20from%20multiple%20views%2C%20considering%20both%20classification%20and%0Alocalization%20outputs.%20This%20alignment%20procedure%20leads%20to%20better%20generalized%20and%0Awell-calibrated%20object%20detector%20models%2C%20which%20are%20crucial%20for%20accurate%0Adecision-making%20in%20safety-critical%20applications.%20Our%20approach%20is%0Adetector-agnostic%20and%20can%20be%20seamlessly%20applied%20to%20both%20single-stage%20and%0Atwo-stage%20detectors.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20methods%2C%20we%0Aconduct%20extensive%20experiments%20and%20ablations%20on%20challenging%20domain-shift%0Ascenarios.%20The%20results%20consistently%20demonstrate%20the%20superiority%20of%20our%20approach%0Acompared%20to%20existing%20methods.%20Our%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/msohaildanish/DivAlign%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Single%2520Domain-Generalized%2520Object%2520Detection%253A%2520A%2520Focus%2520on%250A%2520%2520Diversification%2520and%2520Alignment%26entry.906535625%3DMuhammad%2520Sohail%2520Danish%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Muhammad%2520Akhtar%2520Munir%2520and%2520M.%2520Saquib%2520Sarfraz%2520and%2520Mohsen%2520Ali%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520problem%2520of%2520domain%2520generalization%2520for%2520object%250Adetection%252C%2520specifically%2520focusing%2520on%2520the%2520scenario%2520where%2520only%2520a%2520single%2520source%250Adomain%2520is%2520available.%2520We%2520propose%2520an%2520effective%2520approach%2520that%2520involves%2520two%2520key%250Asteps%253A%2520diversifying%2520the%2520source%2520domain%2520and%2520aligning%2520detections%2520based%2520on%2520class%250Aprediction%2520confidence%2520and%2520localization.%2520Firstly%252C%2520we%2520demonstrate%2520that%2520by%250Acarefully%2520selecting%2520a%2520set%2520of%2520augmentations%252C%2520a%2520base%2520detector%2520can%2520outperform%250Aexisting%2520methods%2520for%2520single%2520domain%2520generalization%2520by%2520a%2520good%2520margin.%2520This%250Ahighlights%2520the%2520importance%2520of%2520domain%2520diversification%2520in%2520improving%2520the%250Aperformance%2520of%2520object%2520detectors.%2520Secondly%252C%2520we%2520introduce%2520a%2520method%2520to%2520align%250Adetections%2520from%2520multiple%2520views%252C%2520considering%2520both%2520classification%2520and%250Alocalization%2520outputs.%2520This%2520alignment%2520procedure%2520leads%2520to%2520better%2520generalized%2520and%250Awell-calibrated%2520object%2520detector%2520models%252C%2520which%2520are%2520crucial%2520for%2520accurate%250Adecision-making%2520in%2520safety-critical%2520applications.%2520Our%2520approach%2520is%250Adetector-agnostic%2520and%2520can%2520be%2520seamlessly%2520applied%2520to%2520both%2520single-stage%2520and%250Atwo-stage%2520detectors.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520methods%252C%2520we%250Aconduct%2520extensive%2520experiments%2520and%2520ablations%2520on%2520challenging%2520domain-shift%250Ascenarios.%2520The%2520results%2520consistently%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%250Acompared%2520to%2520existing%2520methods.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/msohaildanish/DivAlign%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Single%20Domain-Generalized%20Object%20Detection%3A%20A%20Focus%20on%0A%20%20Diversification%20and%20Alignment&entry.906535625=Muhammad%20Sohail%20Danish%20and%20Muhammad%20Haris%20Khan%20and%20Muhammad%20Akhtar%20Munir%20and%20M.%20Saquib%20Sarfraz%20and%20Mohsen%20Ali&entry.1292438233=%20%20In%20this%20work%2C%20we%20tackle%20the%20problem%20of%20domain%20generalization%20for%20object%0Adetection%2C%20specifically%20focusing%20on%20the%20scenario%20where%20only%20a%20single%20source%0Adomain%20is%20available.%20We%20propose%20an%20effective%20approach%20that%20involves%20two%20key%0Asteps%3A%20diversifying%20the%20source%20domain%20and%20aligning%20detections%20based%20on%20class%0Aprediction%20confidence%20and%20localization.%20Firstly%2C%20we%20demonstrate%20that%20by%0Acarefully%20selecting%20a%20set%20of%20augmentations%2C%20a%20base%20detector%20can%20outperform%0Aexisting%20methods%20for%20single%20domain%20generalization%20by%20a%20good%20margin.%20This%0Ahighlights%20the%20importance%20of%20domain%20diversification%20in%20improving%20the%0Aperformance%20of%20object%20detectors.%20Secondly%2C%20we%20introduce%20a%20method%20to%20align%0Adetections%20from%20multiple%20views%2C%20considering%20both%20classification%20and%0Alocalization%20outputs.%20This%20alignment%20procedure%20leads%20to%20better%20generalized%20and%0Awell-calibrated%20object%20detector%20models%2C%20which%20are%20crucial%20for%20accurate%0Adecision-making%20in%20safety-critical%20applications.%20Our%20approach%20is%0Adetector-agnostic%20and%20can%20be%20seamlessly%20applied%20to%20both%20single-stage%20and%0Atwo-stage%20detectors.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20methods%2C%20we%0Aconduct%20extensive%20experiments%20and%20ablations%20on%20challenging%20domain-shift%0Ascenarios.%20The%20results%20consistently%20demonstrate%20the%20superiority%20of%20our%20approach%0Acompared%20to%20existing%20methods.%20Our%20code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/msohaildanish/DivAlign%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14497v1&entry.124074799=Read"},
{"title": "Identity Inference from CLIP Models using Only Textual Data", "author": "Songze Li and Ruoxi Cheng and Xiaojun Jia", "abstract": "  The widespread usage of large-scale multimodal models like CLIP has\nheightened concerns about the leakage of personally identifiable information\n(PII). Existing methods for identity inference in CLIP models, i.e., to detect\nthe presence of a person's PII used for training a CLIP model, require querying\nthe model with full PII, including textual descriptions of the person and\ncorresponding images (e.g., the name and the face photo of the person).\nHowever, this may lead to potential privacy breach of the image, as it may have\nnot been seen by the target model yet. Additionally, traditional membership\ninference attacks (MIAs) train shadow models to mimic the behaviors of the\ntarget model, which incurs high computational costs, especially for large CLIP\nmodels. To address these challenges, we propose a textual unimodal detector\n(TUNI) in CLIP models, a novel method for ID inference that 1) queries the\ntarget model with only text data; and 2) does not require training shadow\nmodels. Firstly, we develop a feature extraction algorithm, guided by the CLIP\nmodel, to extract features from a text description. TUNI starts with randomly\ngenerating textual gibberish that were clearly not utilized for training, and\nleverages their feature vectors to train a system of anomaly detectors. During\ninference, the feature vector of each test text is fed into the anomaly\ndetectors to determine if the person's PII is in the training set (abnormal) or\nnot (normal). Moreover, TUNI can be further strengthened integrating real\nimages associated with the tested individuals, if available at the detector.\nExtensive experiments of TUNI across various CLIP model architectures and\ndatasets demonstrate its superior performance over baselines, albeit with only\ntext data.\n", "link": "http://arxiv.org/abs/2405.14517v1", "date": "2024-05-23", "relevancy": 2.1394, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5456}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5304}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity%20Inference%20from%20CLIP%20Models%20using%20Only%20Textual%20Data&body=Title%3A%20Identity%20Inference%20from%20CLIP%20Models%20using%20Only%20Textual%20Data%0AAuthor%3A%20Songze%20Li%20and%20Ruoxi%20Cheng%20and%20Xiaojun%20Jia%0AAbstract%3A%20%20%20The%20widespread%20usage%20of%20large-scale%20multimodal%20models%20like%20CLIP%20has%0Aheightened%20concerns%20about%20the%20leakage%20of%20personally%20identifiable%20information%0A%28PII%29.%20Existing%20methods%20for%20identity%20inference%20in%20CLIP%20models%2C%20i.e.%2C%20to%20detect%0Athe%20presence%20of%20a%20person%27s%20PII%20used%20for%20training%20a%20CLIP%20model%2C%20require%20querying%0Athe%20model%20with%20full%20PII%2C%20including%20textual%20descriptions%20of%20the%20person%20and%0Acorresponding%20images%20%28e.g.%2C%20the%20name%20and%20the%20face%20photo%20of%20the%20person%29.%0AHowever%2C%20this%20may%20lead%20to%20potential%20privacy%20breach%20of%20the%20image%2C%20as%20it%20may%20have%0Anot%20been%20seen%20by%20the%20target%20model%20yet.%20Additionally%2C%20traditional%20membership%0Ainference%20attacks%20%28MIAs%29%20train%20shadow%20models%20to%20mimic%20the%20behaviors%20of%20the%0Atarget%20model%2C%20which%20incurs%20high%20computational%20costs%2C%20especially%20for%20large%20CLIP%0Amodels.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20textual%20unimodal%20detector%0A%28TUNI%29%20in%20CLIP%20models%2C%20a%20novel%20method%20for%20ID%20inference%20that%201%29%20queries%20the%0Atarget%20model%20with%20only%20text%20data%3B%20and%202%29%20does%20not%20require%20training%20shadow%0Amodels.%20Firstly%2C%20we%20develop%20a%20feature%20extraction%20algorithm%2C%20guided%20by%20the%20CLIP%0Amodel%2C%20to%20extract%20features%20from%20a%20text%20description.%20TUNI%20starts%20with%20randomly%0Agenerating%20textual%20gibberish%20that%20were%20clearly%20not%20utilized%20for%20training%2C%20and%0Aleverages%20their%20feature%20vectors%20to%20train%20a%20system%20of%20anomaly%20detectors.%20During%0Ainference%2C%20the%20feature%20vector%20of%20each%20test%20text%20is%20fed%20into%20the%20anomaly%0Adetectors%20to%20determine%20if%20the%20person%27s%20PII%20is%20in%20the%20training%20set%20%28abnormal%29%20or%0Anot%20%28normal%29.%20Moreover%2C%20TUNI%20can%20be%20further%20strengthened%20integrating%20real%0Aimages%20associated%20with%20the%20tested%20individuals%2C%20if%20available%20at%20the%20detector.%0AExtensive%20experiments%20of%20TUNI%20across%20various%20CLIP%20model%20architectures%20and%0Adatasets%20demonstrate%20its%20superior%20performance%20over%20baselines%2C%20albeit%20with%20only%0Atext%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity%2520Inference%2520from%2520CLIP%2520Models%2520using%2520Only%2520Textual%2520Data%26entry.906535625%3DSongze%2520Li%2520and%2520Ruoxi%2520Cheng%2520and%2520Xiaojun%2520Jia%26entry.1292438233%3D%2520%2520The%2520widespread%2520usage%2520of%2520large-scale%2520multimodal%2520models%2520like%2520CLIP%2520has%250Aheightened%2520concerns%2520about%2520the%2520leakage%2520of%2520personally%2520identifiable%2520information%250A%2528PII%2529.%2520Existing%2520methods%2520for%2520identity%2520inference%2520in%2520CLIP%2520models%252C%2520i.e.%252C%2520to%2520detect%250Athe%2520presence%2520of%2520a%2520person%2527s%2520PII%2520used%2520for%2520training%2520a%2520CLIP%2520model%252C%2520require%2520querying%250Athe%2520model%2520with%2520full%2520PII%252C%2520including%2520textual%2520descriptions%2520of%2520the%2520person%2520and%250Acorresponding%2520images%2520%2528e.g.%252C%2520the%2520name%2520and%2520the%2520face%2520photo%2520of%2520the%2520person%2529.%250AHowever%252C%2520this%2520may%2520lead%2520to%2520potential%2520privacy%2520breach%2520of%2520the%2520image%252C%2520as%2520it%2520may%2520have%250Anot%2520been%2520seen%2520by%2520the%2520target%2520model%2520yet.%2520Additionally%252C%2520traditional%2520membership%250Ainference%2520attacks%2520%2528MIAs%2529%2520train%2520shadow%2520models%2520to%2520mimic%2520the%2520behaviors%2520of%2520the%250Atarget%2520model%252C%2520which%2520incurs%2520high%2520computational%2520costs%252C%2520especially%2520for%2520large%2520CLIP%250Amodels.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520textual%2520unimodal%2520detector%250A%2528TUNI%2529%2520in%2520CLIP%2520models%252C%2520a%2520novel%2520method%2520for%2520ID%2520inference%2520that%25201%2529%2520queries%2520the%250Atarget%2520model%2520with%2520only%2520text%2520data%253B%2520and%25202%2529%2520does%2520not%2520require%2520training%2520shadow%250Amodels.%2520Firstly%252C%2520we%2520develop%2520a%2520feature%2520extraction%2520algorithm%252C%2520guided%2520by%2520the%2520CLIP%250Amodel%252C%2520to%2520extract%2520features%2520from%2520a%2520text%2520description.%2520TUNI%2520starts%2520with%2520randomly%250Agenerating%2520textual%2520gibberish%2520that%2520were%2520clearly%2520not%2520utilized%2520for%2520training%252C%2520and%250Aleverages%2520their%2520feature%2520vectors%2520to%2520train%2520a%2520system%2520of%2520anomaly%2520detectors.%2520During%250Ainference%252C%2520the%2520feature%2520vector%2520of%2520each%2520test%2520text%2520is%2520fed%2520into%2520the%2520anomaly%250Adetectors%2520to%2520determine%2520if%2520the%2520person%2527s%2520PII%2520is%2520in%2520the%2520training%2520set%2520%2528abnormal%2529%2520or%250Anot%2520%2528normal%2529.%2520Moreover%252C%2520TUNI%2520can%2520be%2520further%2520strengthened%2520integrating%2520real%250Aimages%2520associated%2520with%2520the%2520tested%2520individuals%252C%2520if%2520available%2520at%2520the%2520detector.%250AExtensive%2520experiments%2520of%2520TUNI%2520across%2520various%2520CLIP%2520model%2520architectures%2520and%250Adatasets%2520demonstrate%2520its%2520superior%2520performance%2520over%2520baselines%252C%2520albeit%2520with%2520only%250Atext%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Inference%20from%20CLIP%20Models%20using%20Only%20Textual%20Data&entry.906535625=Songze%20Li%20and%20Ruoxi%20Cheng%20and%20Xiaojun%20Jia&entry.1292438233=%20%20The%20widespread%20usage%20of%20large-scale%20multimodal%20models%20like%20CLIP%20has%0Aheightened%20concerns%20about%20the%20leakage%20of%20personally%20identifiable%20information%0A%28PII%29.%20Existing%20methods%20for%20identity%20inference%20in%20CLIP%20models%2C%20i.e.%2C%20to%20detect%0Athe%20presence%20of%20a%20person%27s%20PII%20used%20for%20training%20a%20CLIP%20model%2C%20require%20querying%0Athe%20model%20with%20full%20PII%2C%20including%20textual%20descriptions%20of%20the%20person%20and%0Acorresponding%20images%20%28e.g.%2C%20the%20name%20and%20the%20face%20photo%20of%20the%20person%29.%0AHowever%2C%20this%20may%20lead%20to%20potential%20privacy%20breach%20of%20the%20image%2C%20as%20it%20may%20have%0Anot%20been%20seen%20by%20the%20target%20model%20yet.%20Additionally%2C%20traditional%20membership%0Ainference%20attacks%20%28MIAs%29%20train%20shadow%20models%20to%20mimic%20the%20behaviors%20of%20the%0Atarget%20model%2C%20which%20incurs%20high%20computational%20costs%2C%20especially%20for%20large%20CLIP%0Amodels.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20textual%20unimodal%20detector%0A%28TUNI%29%20in%20CLIP%20models%2C%20a%20novel%20method%20for%20ID%20inference%20that%201%29%20queries%20the%0Atarget%20model%20with%20only%20text%20data%3B%20and%202%29%20does%20not%20require%20training%20shadow%0Amodels.%20Firstly%2C%20we%20develop%20a%20feature%20extraction%20algorithm%2C%20guided%20by%20the%20CLIP%0Amodel%2C%20to%20extract%20features%20from%20a%20text%20description.%20TUNI%20starts%20with%20randomly%0Agenerating%20textual%20gibberish%20that%20were%20clearly%20not%20utilized%20for%20training%2C%20and%0Aleverages%20their%20feature%20vectors%20to%20train%20a%20system%20of%20anomaly%20detectors.%20During%0Ainference%2C%20the%20feature%20vector%20of%20each%20test%20text%20is%20fed%20into%20the%20anomaly%0Adetectors%20to%20determine%20if%20the%20person%27s%20PII%20is%20in%20the%20training%20set%20%28abnormal%29%20or%0Anot%20%28normal%29.%20Moreover%2C%20TUNI%20can%20be%20further%20strengthened%20integrating%20real%0Aimages%20associated%20with%20the%20tested%20individuals%2C%20if%20available%20at%20the%20detector.%0AExtensive%20experiments%20of%20TUNI%20across%20various%20CLIP%20model%20architectures%20and%0Adatasets%20demonstrate%20its%20superior%20performance%20over%20baselines%2C%20albeit%20with%20only%0Atext%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14517v1&entry.124074799=Read"},
{"title": "Towards Cross-modal Backward-compatible Representation Learning for\n  Vision-Language Models", "author": "Young Kyun Jang and Ser-nam Lim", "abstract": "  Modern retrieval systems often struggle with upgrading to new and more\npowerful models due to the incompatibility of embeddings between the old and\nnew models. This necessitates a costly process known as backfilling, which\ninvolves re-computing the embeddings for a large number of data samples. In\nvision, Backward-compatible Training (BT) has been proposed to ensure that the\nnew model aligns with the old model's embeddings. This paper extends the\nconcept of vision-only BT to the field of cross-modal retrieval, marking the\nfirst attempt to address Cross-modal BT (XBT). Our goal is to achieve\nbackward-compatibility between Vision-Language Pretraining (VLP) models, such\nas CLIP, for the cross-modal retrieval task. To address XBT challenges, we\npropose an efficient solution: a projection module that maps the new model's\nembeddings to those of the old model. This module, pretrained solely with text\ndata, significantly reduces the number of image-text pairs required for XBT\nlearning, and, once it is pretrained, it avoids using the old model during\ntraining. Furthermore, we utilize parameter-efficient training strategies that\nimprove efficiency and preserve the off-the-shelf new model's knowledge by\navoiding any modifications. Experimental results on cross-modal retrieval\ndatasets demonstrate the effectiveness of XBT and its potential to enable\nbackfill-free upgrades when a new VLP model emerges.\n", "link": "http://arxiv.org/abs/2405.14715v1", "date": "2024-05-23", "relevancy": 2.1286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5832}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Cross-modal%20Backward-compatible%20Representation%20Learning%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20Towards%20Cross-modal%20Backward-compatible%20Representation%20Learning%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Young%20Kyun%20Jang%20and%20Ser-nam%20Lim%0AAbstract%3A%20%20%20Modern%20retrieval%20systems%20often%20struggle%20with%20upgrading%20to%20new%20and%20more%0Apowerful%20models%20due%20to%20the%20incompatibility%20of%20embeddings%20between%20the%20old%20and%0Anew%20models.%20This%20necessitates%20a%20costly%20process%20known%20as%20backfilling%2C%20which%0Ainvolves%20re-computing%20the%20embeddings%20for%20a%20large%20number%20of%20data%20samples.%20In%0Avision%2C%20Backward-compatible%20Training%20%28BT%29%20has%20been%20proposed%20to%20ensure%20that%20the%0Anew%20model%20aligns%20with%20the%20old%20model%27s%20embeddings.%20This%20paper%20extends%20the%0Aconcept%20of%20vision-only%20BT%20to%20the%20field%20of%20cross-modal%20retrieval%2C%20marking%20the%0Afirst%20attempt%20to%20address%20Cross-modal%20BT%20%28XBT%29.%20Our%20goal%20is%20to%20achieve%0Abackward-compatibility%20between%20Vision-Language%20Pretraining%20%28VLP%29%20models%2C%20such%0Aas%20CLIP%2C%20for%20the%20cross-modal%20retrieval%20task.%20To%20address%20XBT%20challenges%2C%20we%0Apropose%20an%20efficient%20solution%3A%20a%20projection%20module%20that%20maps%20the%20new%20model%27s%0Aembeddings%20to%20those%20of%20the%20old%20model.%20This%20module%2C%20pretrained%20solely%20with%20text%0Adata%2C%20significantly%20reduces%20the%20number%20of%20image-text%20pairs%20required%20for%20XBT%0Alearning%2C%20and%2C%20once%20it%20is%20pretrained%2C%20it%20avoids%20using%20the%20old%20model%20during%0Atraining.%20Furthermore%2C%20we%20utilize%20parameter-efficient%20training%20strategies%20that%0Aimprove%20efficiency%20and%20preserve%20the%20off-the-shelf%20new%20model%27s%20knowledge%20by%0Aavoiding%20any%20modifications.%20Experimental%20results%20on%20cross-modal%20retrieval%0Adatasets%20demonstrate%20the%20effectiveness%20of%20XBT%20and%20its%20potential%20to%20enable%0Abackfill-free%20upgrades%20when%20a%20new%20VLP%20model%20emerges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Cross-modal%2520Backward-compatible%2520Representation%2520Learning%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DYoung%2520Kyun%2520Jang%2520and%2520Ser-nam%2520Lim%26entry.1292438233%3D%2520%2520Modern%2520retrieval%2520systems%2520often%2520struggle%2520with%2520upgrading%2520to%2520new%2520and%2520more%250Apowerful%2520models%2520due%2520to%2520the%2520incompatibility%2520of%2520embeddings%2520between%2520the%2520old%2520and%250Anew%2520models.%2520This%2520necessitates%2520a%2520costly%2520process%2520known%2520as%2520backfilling%252C%2520which%250Ainvolves%2520re-computing%2520the%2520embeddings%2520for%2520a%2520large%2520number%2520of%2520data%2520samples.%2520In%250Avision%252C%2520Backward-compatible%2520Training%2520%2528BT%2529%2520has%2520been%2520proposed%2520to%2520ensure%2520that%2520the%250Anew%2520model%2520aligns%2520with%2520the%2520old%2520model%2527s%2520embeddings.%2520This%2520paper%2520extends%2520the%250Aconcept%2520of%2520vision-only%2520BT%2520to%2520the%2520field%2520of%2520cross-modal%2520retrieval%252C%2520marking%2520the%250Afirst%2520attempt%2520to%2520address%2520Cross-modal%2520BT%2520%2528XBT%2529.%2520Our%2520goal%2520is%2520to%2520achieve%250Abackward-compatibility%2520between%2520Vision-Language%2520Pretraining%2520%2528VLP%2529%2520models%252C%2520such%250Aas%2520CLIP%252C%2520for%2520the%2520cross-modal%2520retrieval%2520task.%2520To%2520address%2520XBT%2520challenges%252C%2520we%250Apropose%2520an%2520efficient%2520solution%253A%2520a%2520projection%2520module%2520that%2520maps%2520the%2520new%2520model%2527s%250Aembeddings%2520to%2520those%2520of%2520the%2520old%2520model.%2520This%2520module%252C%2520pretrained%2520solely%2520with%2520text%250Adata%252C%2520significantly%2520reduces%2520the%2520number%2520of%2520image-text%2520pairs%2520required%2520for%2520XBT%250Alearning%252C%2520and%252C%2520once%2520it%2520is%2520pretrained%252C%2520it%2520avoids%2520using%2520the%2520old%2520model%2520during%250Atraining.%2520Furthermore%252C%2520we%2520utilize%2520parameter-efficient%2520training%2520strategies%2520that%250Aimprove%2520efficiency%2520and%2520preserve%2520the%2520off-the-shelf%2520new%2520model%2527s%2520knowledge%2520by%250Aavoiding%2520any%2520modifications.%2520Experimental%2520results%2520on%2520cross-modal%2520retrieval%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520XBT%2520and%2520its%2520potential%2520to%2520enable%250Abackfill-free%2520upgrades%2520when%2520a%2520new%2520VLP%2520model%2520emerges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Cross-modal%20Backward-compatible%20Representation%20Learning%20for%0A%20%20Vision-Language%20Models&entry.906535625=Young%20Kyun%20Jang%20and%20Ser-nam%20Lim&entry.1292438233=%20%20Modern%20retrieval%20systems%20often%20struggle%20with%20upgrading%20to%20new%20and%20more%0Apowerful%20models%20due%20to%20the%20incompatibility%20of%20embeddings%20between%20the%20old%20and%0Anew%20models.%20This%20necessitates%20a%20costly%20process%20known%20as%20backfilling%2C%20which%0Ainvolves%20re-computing%20the%20embeddings%20for%20a%20large%20number%20of%20data%20samples.%20In%0Avision%2C%20Backward-compatible%20Training%20%28BT%29%20has%20been%20proposed%20to%20ensure%20that%20the%0Anew%20model%20aligns%20with%20the%20old%20model%27s%20embeddings.%20This%20paper%20extends%20the%0Aconcept%20of%20vision-only%20BT%20to%20the%20field%20of%20cross-modal%20retrieval%2C%20marking%20the%0Afirst%20attempt%20to%20address%20Cross-modal%20BT%20%28XBT%29.%20Our%20goal%20is%20to%20achieve%0Abackward-compatibility%20between%20Vision-Language%20Pretraining%20%28VLP%29%20models%2C%20such%0Aas%20CLIP%2C%20for%20the%20cross-modal%20retrieval%20task.%20To%20address%20XBT%20challenges%2C%20we%0Apropose%20an%20efficient%20solution%3A%20a%20projection%20module%20that%20maps%20the%20new%20model%27s%0Aembeddings%20to%20those%20of%20the%20old%20model.%20This%20module%2C%20pretrained%20solely%20with%20text%0Adata%2C%20significantly%20reduces%20the%20number%20of%20image-text%20pairs%20required%20for%20XBT%0Alearning%2C%20and%2C%20once%20it%20is%20pretrained%2C%20it%20avoids%20using%20the%20old%20model%20during%0Atraining.%20Furthermore%2C%20we%20utilize%20parameter-efficient%20training%20strategies%20that%0Aimprove%20efficiency%20and%20preserve%20the%20off-the-shelf%20new%20model%27s%20knowledge%20by%0Aavoiding%20any%20modifications.%20Experimental%20results%20on%20cross-modal%20retrieval%0Adatasets%20demonstrate%20the%20effectiveness%20of%20XBT%20and%20its%20potential%20to%20enable%0Abackfill-free%20upgrades%20when%20a%20new%20VLP%20model%20emerges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14715v1&entry.124074799=Read"},
{"title": "Distilling Vision-Language Pretraining for Efficient Cross-Modal\n  Retrieval", "author": "Young Kyun Jang and Donghyun Kim and Ser-nam Lim", "abstract": "  ``Learning to hash'' is a practical solution for efficient retrieval,\noffering fast search speed and low storage cost. It is widely applied in\nvarious applications, such as image-text cross-modal search. In this paper, we\nexplore the potential of enhancing the performance of learning to hash with the\nproliferation of powerful large pre-trained models, such as Vision-Language\nPre-training (VLP) models. We introduce a novel method named Distillation for\nCross-Modal Quantization (DCMQ), which leverages the rich semantic knowledge of\nVLP models to improve hash representation learning. Specifically, we use the\nVLP as a `teacher' to distill knowledge into a `student' hashing model equipped\nwith codebooks. This process involves the replacement of supervised labels,\nwhich are composed of multi-hot vectors and lack semantics, with the rich\nsemantics of VLP. In the end, we apply a transformation termed Normalization\nwith Paired Consistency (NPC) to achieve a discriminative target for\ndistillation. Further, we introduce a new quantization method, Product\nQuantization with Gumbel (PQG) that promotes balanced codebook learning,\nthereby improving the retrieval performance. Extensive benchmark testing\ndemonstrates that DCMQ consistently outperforms existing supervised cross-modal\nhashing approaches, showcasing its significant potential.\n", "link": "http://arxiv.org/abs/2405.14726v1", "date": "2024-05-23", "relevancy": 2.1258, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5219}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Vision-Language%20Pretraining%20for%20Efficient%20Cross-Modal%0A%20%20Retrieval&body=Title%3A%20Distilling%20Vision-Language%20Pretraining%20for%20Efficient%20Cross-Modal%0A%20%20Retrieval%0AAuthor%3A%20Young%20Kyun%20Jang%20and%20Donghyun%20Kim%20and%20Ser-nam%20Lim%0AAbstract%3A%20%20%20%60%60Learning%20to%20hash%27%27%20is%20a%20practical%20solution%20for%20efficient%20retrieval%2C%0Aoffering%20fast%20search%20speed%20and%20low%20storage%20cost.%20It%20is%20widely%20applied%20in%0Avarious%20applications%2C%20such%20as%20image-text%20cross-modal%20search.%20In%20this%20paper%2C%20we%0Aexplore%20the%20potential%20of%20enhancing%20the%20performance%20of%20learning%20to%20hash%20with%20the%0Aproliferation%20of%20powerful%20large%20pre-trained%20models%2C%20such%20as%20Vision-Language%0APre-training%20%28VLP%29%20models.%20We%20introduce%20a%20novel%20method%20named%20Distillation%20for%0ACross-Modal%20Quantization%20%28DCMQ%29%2C%20which%20leverages%20the%20rich%20semantic%20knowledge%20of%0AVLP%20models%20to%20improve%20hash%20representation%20learning.%20Specifically%2C%20we%20use%20the%0AVLP%20as%20a%20%60teacher%27%20to%20distill%20knowledge%20into%20a%20%60student%27%20hashing%20model%20equipped%0Awith%20codebooks.%20This%20process%20involves%20the%20replacement%20of%20supervised%20labels%2C%0Awhich%20are%20composed%20of%20multi-hot%20vectors%20and%20lack%20semantics%2C%20with%20the%20rich%0Asemantics%20of%20VLP.%20In%20the%20end%2C%20we%20apply%20a%20transformation%20termed%20Normalization%0Awith%20Paired%20Consistency%20%28NPC%29%20to%20achieve%20a%20discriminative%20target%20for%0Adistillation.%20Further%2C%20we%20introduce%20a%20new%20quantization%20method%2C%20Product%0AQuantization%20with%20Gumbel%20%28PQG%29%20that%20promotes%20balanced%20codebook%20learning%2C%0Athereby%20improving%20the%20retrieval%20performance.%20Extensive%20benchmark%20testing%0Ademonstrates%20that%20DCMQ%20consistently%20outperforms%20existing%20supervised%20cross-modal%0Ahashing%20approaches%2C%20showcasing%20its%20significant%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Vision-Language%2520Pretraining%2520for%2520Efficient%2520Cross-Modal%250A%2520%2520Retrieval%26entry.906535625%3DYoung%2520Kyun%2520Jang%2520and%2520Donghyun%2520Kim%2520and%2520Ser-nam%2520Lim%26entry.1292438233%3D%2520%2520%2560%2560Learning%2520to%2520hash%2527%2527%2520is%2520a%2520practical%2520solution%2520for%2520efficient%2520retrieval%252C%250Aoffering%2520fast%2520search%2520speed%2520and%2520low%2520storage%2520cost.%2520It%2520is%2520widely%2520applied%2520in%250Avarious%2520applications%252C%2520such%2520as%2520image-text%2520cross-modal%2520search.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520the%2520potential%2520of%2520enhancing%2520the%2520performance%2520of%2520learning%2520to%2520hash%2520with%2520the%250Aproliferation%2520of%2520powerful%2520large%2520pre-trained%2520models%252C%2520such%2520as%2520Vision-Language%250APre-training%2520%2528VLP%2529%2520models.%2520We%2520introduce%2520a%2520novel%2520method%2520named%2520Distillation%2520for%250ACross-Modal%2520Quantization%2520%2528DCMQ%2529%252C%2520which%2520leverages%2520the%2520rich%2520semantic%2520knowledge%2520of%250AVLP%2520models%2520to%2520improve%2520hash%2520representation%2520learning.%2520Specifically%252C%2520we%2520use%2520the%250AVLP%2520as%2520a%2520%2560teacher%2527%2520to%2520distill%2520knowledge%2520into%2520a%2520%2560student%2527%2520hashing%2520model%2520equipped%250Awith%2520codebooks.%2520This%2520process%2520involves%2520the%2520replacement%2520of%2520supervised%2520labels%252C%250Awhich%2520are%2520composed%2520of%2520multi-hot%2520vectors%2520and%2520lack%2520semantics%252C%2520with%2520the%2520rich%250Asemantics%2520of%2520VLP.%2520In%2520the%2520end%252C%2520we%2520apply%2520a%2520transformation%2520termed%2520Normalization%250Awith%2520Paired%2520Consistency%2520%2528NPC%2529%2520to%2520achieve%2520a%2520discriminative%2520target%2520for%250Adistillation.%2520Further%252C%2520we%2520introduce%2520a%2520new%2520quantization%2520method%252C%2520Product%250AQuantization%2520with%2520Gumbel%2520%2528PQG%2529%2520that%2520promotes%2520balanced%2520codebook%2520learning%252C%250Athereby%2520improving%2520the%2520retrieval%2520performance.%2520Extensive%2520benchmark%2520testing%250Ademonstrates%2520that%2520DCMQ%2520consistently%2520outperforms%2520existing%2520supervised%2520cross-modal%250Ahashing%2520approaches%252C%2520showcasing%2520its%2520significant%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Vision-Language%20Pretraining%20for%20Efficient%20Cross-Modal%0A%20%20Retrieval&entry.906535625=Young%20Kyun%20Jang%20and%20Donghyun%20Kim%20and%20Ser-nam%20Lim&entry.1292438233=%20%20%60%60Learning%20to%20hash%27%27%20is%20a%20practical%20solution%20for%20efficient%20retrieval%2C%0Aoffering%20fast%20search%20speed%20and%20low%20storage%20cost.%20It%20is%20widely%20applied%20in%0Avarious%20applications%2C%20such%20as%20image-text%20cross-modal%20search.%20In%20this%20paper%2C%20we%0Aexplore%20the%20potential%20of%20enhancing%20the%20performance%20of%20learning%20to%20hash%20with%20the%0Aproliferation%20of%20powerful%20large%20pre-trained%20models%2C%20such%20as%20Vision-Language%0APre-training%20%28VLP%29%20models.%20We%20introduce%20a%20novel%20method%20named%20Distillation%20for%0ACross-Modal%20Quantization%20%28DCMQ%29%2C%20which%20leverages%20the%20rich%20semantic%20knowledge%20of%0AVLP%20models%20to%20improve%20hash%20representation%20learning.%20Specifically%2C%20we%20use%20the%0AVLP%20as%20a%20%60teacher%27%20to%20distill%20knowledge%20into%20a%20%60student%27%20hashing%20model%20equipped%0Awith%20codebooks.%20This%20process%20involves%20the%20replacement%20of%20supervised%20labels%2C%0Awhich%20are%20composed%20of%20multi-hot%20vectors%20and%20lack%20semantics%2C%20with%20the%20rich%0Asemantics%20of%20VLP.%20In%20the%20end%2C%20we%20apply%20a%20transformation%20termed%20Normalization%0Awith%20Paired%20Consistency%20%28NPC%29%20to%20achieve%20a%20discriminative%20target%20for%0Adistillation.%20Further%2C%20we%20introduce%20a%20new%20quantization%20method%2C%20Product%0AQuantization%20with%20Gumbel%20%28PQG%29%20that%20promotes%20balanced%20codebook%20learning%2C%0Athereby%20improving%20the%20retrieval%20performance.%20Extensive%20benchmark%20testing%0Ademonstrates%20that%20DCMQ%20consistently%20outperforms%20existing%20supervised%20cross-modal%0Ahashing%20approaches%2C%20showcasing%20its%20significant%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14726v1&entry.124074799=Read"},
{"title": "Calibrated Self-Rewarding Vision Language Models", "author": "Yiyang Zhou and Zhiyuan Fan and Dongjie Cheng and Sihan Yang and Zhaorun Chen and Chenhang Cui and Xiyao Wang and Yun Li and Linjun Zhang and Huaxiu Yao", "abstract": "  Large Vision-Language Models (LVLMs) have made substantial progress by\nintegrating pre-trained large language models (LLMs) and vision models through\ninstruction tuning. Despite these advancements, LVLMs often exhibit the\nhallucination phenomenon, where generated text responses appear linguistically\nplausible but contradict the input image, indicating a misalignment between\nimage and text pairs. This misalignment arises because the model tends to\nprioritize textual information over visual input, even when both the language\nmodel and visual representations are of high quality. Existing methods leverage\nadditional models or human annotations to curate preference data and enhance\nmodality alignment through preference optimization. These approaches may not\neffectively reflect the target LVLM's preferences, making the curated\npreferences easily distinguishable. Our work addresses these challenges by\nproposing the Calibrated Self-Rewarding (CSR) approach, which enables the model\nto self-improve by iteratively generating candidate responses, evaluating the\nreward for each response, and curating preference data for fine-tuning. In the\nreward modeling, we employ a step-wise strategy and incorporate visual\nconstraints into the self-rewarding process to place greater emphasis on visual\ninput. Empirical results demonstrate that CSR enhances performance and reduces\nhallucinations across ten benchmarks and tasks, achieving substantial\nimprovements over existing methods by 7.62%. Our empirical results are further\nsupported by rigorous theoretical analysis, under mild assumptions, verifying\nthe effectiveness of introducing visual constraints into the self-rewarding\nparadigm. Additionally, CSR shows compatibility with different vision-language\nmodels and the ability to incrementally improve performance through iterative\nfine-tuning. Our data and code are available at\nhttps://github.com/YiyangZhou/CSR.\n", "link": "http://arxiv.org/abs/2405.14622v1", "date": "2024-05-23", "relevancy": 2.1102, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Self-Rewarding%20Vision%20Language%20Models&body=Title%3A%20Calibrated%20Self-Rewarding%20Vision%20Language%20Models%0AAuthor%3A%20Yiyang%20Zhou%20and%20Zhiyuan%20Fan%20and%20Dongjie%20Cheng%20and%20Sihan%20Yang%20and%20Zhaorun%20Chen%20and%20Chenhang%20Cui%20and%20Xiyao%20Wang%20and%20Yun%20Li%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20substantial%20progress%20by%0Aintegrating%20pre-trained%20large%20language%20models%20%28LLMs%29%20and%20vision%20models%20through%0Ainstruction%20tuning.%20Despite%20these%20advancements%2C%20LVLMs%20often%20exhibit%20the%0Ahallucination%20phenomenon%2C%20where%20generated%20text%20responses%20appear%20linguistically%0Aplausible%20but%20contradict%20the%20input%20image%2C%20indicating%20a%20misalignment%20between%0Aimage%20and%20text%20pairs.%20This%20misalignment%20arises%20because%20the%20model%20tends%20to%0Aprioritize%20textual%20information%20over%20visual%20input%2C%20even%20when%20both%20the%20language%0Amodel%20and%20visual%20representations%20are%20of%20high%20quality.%20Existing%20methods%20leverage%0Aadditional%20models%20or%20human%20annotations%20to%20curate%20preference%20data%20and%20enhance%0Amodality%20alignment%20through%20preference%20optimization.%20These%20approaches%20may%20not%0Aeffectively%20reflect%20the%20target%20LVLM%27s%20preferences%2C%20making%20the%20curated%0Apreferences%20easily%20distinguishable.%20Our%20work%20addresses%20these%20challenges%20by%0Aproposing%20the%20Calibrated%20Self-Rewarding%20%28CSR%29%20approach%2C%20which%20enables%20the%20model%0Ato%20self-improve%20by%20iteratively%20generating%20candidate%20responses%2C%20evaluating%20the%0Areward%20for%20each%20response%2C%20and%20curating%20preference%20data%20for%20fine-tuning.%20In%20the%0Areward%20modeling%2C%20we%20employ%20a%20step-wise%20strategy%20and%20incorporate%20visual%0Aconstraints%20into%20the%20self-rewarding%20process%20to%20place%20greater%20emphasis%20on%20visual%0Ainput.%20Empirical%20results%20demonstrate%20that%20CSR%20enhances%20performance%20and%20reduces%0Ahallucinations%20across%20ten%20benchmarks%20and%20tasks%2C%20achieving%20substantial%0Aimprovements%20over%20existing%20methods%20by%207.62%25.%20Our%20empirical%20results%20are%20further%0Asupported%20by%20rigorous%20theoretical%20analysis%2C%20under%20mild%20assumptions%2C%20verifying%0Athe%20effectiveness%20of%20introducing%20visual%20constraints%20into%20the%20self-rewarding%0Aparadigm.%20Additionally%2C%20CSR%20shows%20compatibility%20with%20different%20vision-language%0Amodels%20and%20the%20ability%20to%20incrementally%20improve%20performance%20through%20iterative%0Afine-tuning.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/YiyangZhou/CSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Self-Rewarding%2520Vision%2520Language%2520Models%26entry.906535625%3DYiyang%2520Zhou%2520and%2520Zhiyuan%2520Fan%2520and%2520Dongjie%2520Cheng%2520and%2520Sihan%2520Yang%2520and%2520Zhaorun%2520Chen%2520and%2520Chenhang%2520Cui%2520and%2520Xiyao%2520Wang%2520and%2520Yun%2520Li%2520and%2520Linjun%2520Zhang%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520made%2520substantial%2520progress%2520by%250Aintegrating%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision%2520models%2520through%250Ainstruction%2520tuning.%2520Despite%2520these%2520advancements%252C%2520LVLMs%2520often%2520exhibit%2520the%250Ahallucination%2520phenomenon%252C%2520where%2520generated%2520text%2520responses%2520appear%2520linguistically%250Aplausible%2520but%2520contradict%2520the%2520input%2520image%252C%2520indicating%2520a%2520misalignment%2520between%250Aimage%2520and%2520text%2520pairs.%2520This%2520misalignment%2520arises%2520because%2520the%2520model%2520tends%2520to%250Aprioritize%2520textual%2520information%2520over%2520visual%2520input%252C%2520even%2520when%2520both%2520the%2520language%250Amodel%2520and%2520visual%2520representations%2520are%2520of%2520high%2520quality.%2520Existing%2520methods%2520leverage%250Aadditional%2520models%2520or%2520human%2520annotations%2520to%2520curate%2520preference%2520data%2520and%2520enhance%250Amodality%2520alignment%2520through%2520preference%2520optimization.%2520These%2520approaches%2520may%2520not%250Aeffectively%2520reflect%2520the%2520target%2520LVLM%2527s%2520preferences%252C%2520making%2520the%2520curated%250Apreferences%2520easily%2520distinguishable.%2520Our%2520work%2520addresses%2520these%2520challenges%2520by%250Aproposing%2520the%2520Calibrated%2520Self-Rewarding%2520%2528CSR%2529%2520approach%252C%2520which%2520enables%2520the%2520model%250Ato%2520self-improve%2520by%2520iteratively%2520generating%2520candidate%2520responses%252C%2520evaluating%2520the%250Areward%2520for%2520each%2520response%252C%2520and%2520curating%2520preference%2520data%2520for%2520fine-tuning.%2520In%2520the%250Areward%2520modeling%252C%2520we%2520employ%2520a%2520step-wise%2520strategy%2520and%2520incorporate%2520visual%250Aconstraints%2520into%2520the%2520self-rewarding%2520process%2520to%2520place%2520greater%2520emphasis%2520on%2520visual%250Ainput.%2520Empirical%2520results%2520demonstrate%2520that%2520CSR%2520enhances%2520performance%2520and%2520reduces%250Ahallucinations%2520across%2520ten%2520benchmarks%2520and%2520tasks%252C%2520achieving%2520substantial%250Aimprovements%2520over%2520existing%2520methods%2520by%25207.62%2525.%2520Our%2520empirical%2520results%2520are%2520further%250Asupported%2520by%2520rigorous%2520theoretical%2520analysis%252C%2520under%2520mild%2520assumptions%252C%2520verifying%250Athe%2520effectiveness%2520of%2520introducing%2520visual%2520constraints%2520into%2520the%2520self-rewarding%250Aparadigm.%2520Additionally%252C%2520CSR%2520shows%2520compatibility%2520with%2520different%2520vision-language%250Amodels%2520and%2520the%2520ability%2520to%2520incrementally%2520improve%2520performance%2520through%2520iterative%250Afine-tuning.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/YiyangZhou/CSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Self-Rewarding%20Vision%20Language%20Models&entry.906535625=Yiyang%20Zhou%20and%20Zhiyuan%20Fan%20and%20Dongjie%20Cheng%20and%20Sihan%20Yang%20and%20Zhaorun%20Chen%20and%20Chenhang%20Cui%20and%20Xiyao%20Wang%20and%20Yun%20Li%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20substantial%20progress%20by%0Aintegrating%20pre-trained%20large%20language%20models%20%28LLMs%29%20and%20vision%20models%20through%0Ainstruction%20tuning.%20Despite%20these%20advancements%2C%20LVLMs%20often%20exhibit%20the%0Ahallucination%20phenomenon%2C%20where%20generated%20text%20responses%20appear%20linguistically%0Aplausible%20but%20contradict%20the%20input%20image%2C%20indicating%20a%20misalignment%20between%0Aimage%20and%20text%20pairs.%20This%20misalignment%20arises%20because%20the%20model%20tends%20to%0Aprioritize%20textual%20information%20over%20visual%20input%2C%20even%20when%20both%20the%20language%0Amodel%20and%20visual%20representations%20are%20of%20high%20quality.%20Existing%20methods%20leverage%0Aadditional%20models%20or%20human%20annotations%20to%20curate%20preference%20data%20and%20enhance%0Amodality%20alignment%20through%20preference%20optimization.%20These%20approaches%20may%20not%0Aeffectively%20reflect%20the%20target%20LVLM%27s%20preferences%2C%20making%20the%20curated%0Apreferences%20easily%20distinguishable.%20Our%20work%20addresses%20these%20challenges%20by%0Aproposing%20the%20Calibrated%20Self-Rewarding%20%28CSR%29%20approach%2C%20which%20enables%20the%20model%0Ato%20self-improve%20by%20iteratively%20generating%20candidate%20responses%2C%20evaluating%20the%0Areward%20for%20each%20response%2C%20and%20curating%20preference%20data%20for%20fine-tuning.%20In%20the%0Areward%20modeling%2C%20we%20employ%20a%20step-wise%20strategy%20and%20incorporate%20visual%0Aconstraints%20into%20the%20self-rewarding%20process%20to%20place%20greater%20emphasis%20on%20visual%0Ainput.%20Empirical%20results%20demonstrate%20that%20CSR%20enhances%20performance%20and%20reduces%0Ahallucinations%20across%20ten%20benchmarks%20and%20tasks%2C%20achieving%20substantial%0Aimprovements%20over%20existing%20methods%20by%207.62%25.%20Our%20empirical%20results%20are%20further%0Asupported%20by%20rigorous%20theoretical%20analysis%2C%20under%20mild%20assumptions%2C%20verifying%0Athe%20effectiveness%20of%20introducing%20visual%20constraints%20into%20the%20self-rewarding%0Aparadigm.%20Additionally%2C%20CSR%20shows%20compatibility%20with%20different%20vision-language%0Amodels%20and%20the%20ability%20to%20incrementally%20improve%20performance%20through%20iterative%0Afine-tuning.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/YiyangZhou/CSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14622v1&entry.124074799=Read"},
{"title": "Fisher Flow Matching for Generative Modeling over Discrete Data", "author": "Oscar Davis and Samuel Kessler and Mircea Petrache and {\u0130}smail {\u0130}lkan Ceylan and Avishek Joey Bose", "abstract": "  Generative modeling over discrete data has recently seen numerous success\nstories, with applications spanning language modeling, biological sequence\ndesign, and graph-structured molecular data. The predominant generative\nmodeling paradigm for discrete data is still autoregressive, with more recent\nalternatives based on diffusion or flow-matching falling short of their\nimpressive performance in continuous data settings, such as image or video\ngeneration. In this work, we introduce Fisher-Flow, a novel flow-matching model\nfor discrete data. Fisher-Flow takes a manifestly geometric perspective by\nconsidering categorical distributions over discrete data as points residing on\na statistical manifold equipped with its natural Riemannian metric: the\n$\\textit{Fisher-Rao metric}$. As a result, we demonstrate discrete data itself\ncan be continuously reparameterised to points on the positive orthant of the\n$d$-hypersphere $\\mathbb{S}^d_+$, which allows us to define flows that map any\nsource distribution to target in a principled manner by transporting mass along\n(closed-form) geodesics of $\\mathbb{S}^d_+$. Furthermore, the learned flows in\nFisher-Flow can be further bootstrapped by leveraging Riemannian optimal\ntransport leading to improved training dynamics. We prove that the gradient\nflow induced by Fisher-Flow is optimal in reducing the forward KL divergence.\n  We evaluate Fisher-Flow on an array of synthetic and diverse real-world\nbenchmarks, including designing DNA Promoter, and DNA Enhancer sequences.\nEmpirically, we find that Fisher-Flow improves over prior diffusion and\nflow-matching models on these benchmarks.\n", "link": "http://arxiv.org/abs/2405.14664v1", "date": "2024-05-23", "relevancy": 2.108, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6514}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.506}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fisher%20Flow%20Matching%20for%20Generative%20Modeling%20over%20Discrete%20Data&body=Title%3A%20Fisher%20Flow%20Matching%20for%20Generative%20Modeling%20over%20Discrete%20Data%0AAuthor%3A%20Oscar%20Davis%20and%20Samuel%20Kessler%20and%20Mircea%20Petrache%20and%20%7B%C4%B0%7Dsmail%20%7B%C4%B0%7Dlkan%20Ceylan%20and%20Avishek%20Joey%20Bose%0AAbstract%3A%20%20%20Generative%20modeling%20over%20discrete%20data%20has%20recently%20seen%20numerous%20success%0Astories%2C%20with%20applications%20spanning%20language%20modeling%2C%20biological%20sequence%0Adesign%2C%20and%20graph-structured%20molecular%20data.%20The%20predominant%20generative%0Amodeling%20paradigm%20for%20discrete%20data%20is%20still%20autoregressive%2C%20with%20more%20recent%0Aalternatives%20based%20on%20diffusion%20or%20flow-matching%20falling%20short%20of%20their%0Aimpressive%20performance%20in%20continuous%20data%20settings%2C%20such%20as%20image%20or%20video%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20Fisher-Flow%2C%20a%20novel%20flow-matching%20model%0Afor%20discrete%20data.%20Fisher-Flow%20takes%20a%20manifestly%20geometric%20perspective%20by%0Aconsidering%20categorical%20distributions%20over%20discrete%20data%20as%20points%20residing%20on%0Aa%20statistical%20manifold%20equipped%20with%20its%20natural%20Riemannian%20metric%3A%20the%0A%24%5Ctextit%7BFisher-Rao%20metric%7D%24.%20As%20a%20result%2C%20we%20demonstrate%20discrete%20data%20itself%0Acan%20be%20continuously%20reparameterised%20to%20points%20on%20the%20positive%20orthant%20of%20the%0A%24d%24-hypersphere%20%24%5Cmathbb%7BS%7D%5Ed_%2B%24%2C%20which%20allows%20us%20to%20define%20flows%20that%20map%20any%0Asource%20distribution%20to%20target%20in%20a%20principled%20manner%20by%20transporting%20mass%20along%0A%28closed-form%29%20geodesics%20of%20%24%5Cmathbb%7BS%7D%5Ed_%2B%24.%20Furthermore%2C%20the%20learned%20flows%20in%0AFisher-Flow%20can%20be%20further%20bootstrapped%20by%20leveraging%20Riemannian%20optimal%0Atransport%20leading%20to%20improved%20training%20dynamics.%20We%20prove%20that%20the%20gradient%0Aflow%20induced%20by%20Fisher-Flow%20is%20optimal%20in%20reducing%20the%20forward%20KL%20divergence.%0A%20%20We%20evaluate%20Fisher-Flow%20on%20an%20array%20of%20synthetic%20and%20diverse%20real-world%0Abenchmarks%2C%20including%20designing%20DNA%20Promoter%2C%20and%20DNA%20Enhancer%20sequences.%0AEmpirically%2C%20we%20find%20that%20Fisher-Flow%20improves%20over%20prior%20diffusion%20and%0Aflow-matching%20models%20on%20these%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFisher%2520Flow%2520Matching%2520for%2520Generative%2520Modeling%2520over%2520Discrete%2520Data%26entry.906535625%3DOscar%2520Davis%2520and%2520Samuel%2520Kessler%2520and%2520Mircea%2520Petrache%2520and%2520%257B%25C4%25B0%257Dsmail%2520%257B%25C4%25B0%257Dlkan%2520Ceylan%2520and%2520Avishek%2520Joey%2520Bose%26entry.1292438233%3D%2520%2520Generative%2520modeling%2520over%2520discrete%2520data%2520has%2520recently%2520seen%2520numerous%2520success%250Astories%252C%2520with%2520applications%2520spanning%2520language%2520modeling%252C%2520biological%2520sequence%250Adesign%252C%2520and%2520graph-structured%2520molecular%2520data.%2520The%2520predominant%2520generative%250Amodeling%2520paradigm%2520for%2520discrete%2520data%2520is%2520still%2520autoregressive%252C%2520with%2520more%2520recent%250Aalternatives%2520based%2520on%2520diffusion%2520or%2520flow-matching%2520falling%2520short%2520of%2520their%250Aimpressive%2520performance%2520in%2520continuous%2520data%2520settings%252C%2520such%2520as%2520image%2520or%2520video%250Ageneration.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Fisher-Flow%252C%2520a%2520novel%2520flow-matching%2520model%250Afor%2520discrete%2520data.%2520Fisher-Flow%2520takes%2520a%2520manifestly%2520geometric%2520perspective%2520by%250Aconsidering%2520categorical%2520distributions%2520over%2520discrete%2520data%2520as%2520points%2520residing%2520on%250Aa%2520statistical%2520manifold%2520equipped%2520with%2520its%2520natural%2520Riemannian%2520metric%253A%2520the%250A%2524%255Ctextit%257BFisher-Rao%2520metric%257D%2524.%2520As%2520a%2520result%252C%2520we%2520demonstrate%2520discrete%2520data%2520itself%250Acan%2520be%2520continuously%2520reparameterised%2520to%2520points%2520on%2520the%2520positive%2520orthant%2520of%2520the%250A%2524d%2524-hypersphere%2520%2524%255Cmathbb%257BS%257D%255Ed_%252B%2524%252C%2520which%2520allows%2520us%2520to%2520define%2520flows%2520that%2520map%2520any%250Asource%2520distribution%2520to%2520target%2520in%2520a%2520principled%2520manner%2520by%2520transporting%2520mass%2520along%250A%2528closed-form%2529%2520geodesics%2520of%2520%2524%255Cmathbb%257BS%257D%255Ed_%252B%2524.%2520Furthermore%252C%2520the%2520learned%2520flows%2520in%250AFisher-Flow%2520can%2520be%2520further%2520bootstrapped%2520by%2520leveraging%2520Riemannian%2520optimal%250Atransport%2520leading%2520to%2520improved%2520training%2520dynamics.%2520We%2520prove%2520that%2520the%2520gradient%250Aflow%2520induced%2520by%2520Fisher-Flow%2520is%2520optimal%2520in%2520reducing%2520the%2520forward%2520KL%2520divergence.%250A%2520%2520We%2520evaluate%2520Fisher-Flow%2520on%2520an%2520array%2520of%2520synthetic%2520and%2520diverse%2520real-world%250Abenchmarks%252C%2520including%2520designing%2520DNA%2520Promoter%252C%2520and%2520DNA%2520Enhancer%2520sequences.%250AEmpirically%252C%2520we%2520find%2520that%2520Fisher-Flow%2520improves%2520over%2520prior%2520diffusion%2520and%250Aflow-matching%2520models%2520on%2520these%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fisher%20Flow%20Matching%20for%20Generative%20Modeling%20over%20Discrete%20Data&entry.906535625=Oscar%20Davis%20and%20Samuel%20Kessler%20and%20Mircea%20Petrache%20and%20%7B%C4%B0%7Dsmail%20%7B%C4%B0%7Dlkan%20Ceylan%20and%20Avishek%20Joey%20Bose&entry.1292438233=%20%20Generative%20modeling%20over%20discrete%20data%20has%20recently%20seen%20numerous%20success%0Astories%2C%20with%20applications%20spanning%20language%20modeling%2C%20biological%20sequence%0Adesign%2C%20and%20graph-structured%20molecular%20data.%20The%20predominant%20generative%0Amodeling%20paradigm%20for%20discrete%20data%20is%20still%20autoregressive%2C%20with%20more%20recent%0Aalternatives%20based%20on%20diffusion%20or%20flow-matching%20falling%20short%20of%20their%0Aimpressive%20performance%20in%20continuous%20data%20settings%2C%20such%20as%20image%20or%20video%0Ageneration.%20In%20this%20work%2C%20we%20introduce%20Fisher-Flow%2C%20a%20novel%20flow-matching%20model%0Afor%20discrete%20data.%20Fisher-Flow%20takes%20a%20manifestly%20geometric%20perspective%20by%0Aconsidering%20categorical%20distributions%20over%20discrete%20data%20as%20points%20residing%20on%0Aa%20statistical%20manifold%20equipped%20with%20its%20natural%20Riemannian%20metric%3A%20the%0A%24%5Ctextit%7BFisher-Rao%20metric%7D%24.%20As%20a%20result%2C%20we%20demonstrate%20discrete%20data%20itself%0Acan%20be%20continuously%20reparameterised%20to%20points%20on%20the%20positive%20orthant%20of%20the%0A%24d%24-hypersphere%20%24%5Cmathbb%7BS%7D%5Ed_%2B%24%2C%20which%20allows%20us%20to%20define%20flows%20that%20map%20any%0Asource%20distribution%20to%20target%20in%20a%20principled%20manner%20by%20transporting%20mass%20along%0A%28closed-form%29%20geodesics%20of%20%24%5Cmathbb%7BS%7D%5Ed_%2B%24.%20Furthermore%2C%20the%20learned%20flows%20in%0AFisher-Flow%20can%20be%20further%20bootstrapped%20by%20leveraging%20Riemannian%20optimal%0Atransport%20leading%20to%20improved%20training%20dynamics.%20We%20prove%20that%20the%20gradient%0Aflow%20induced%20by%20Fisher-Flow%20is%20optimal%20in%20reducing%20the%20forward%20KL%20divergence.%0A%20%20We%20evaluate%20Fisher-Flow%20on%20an%20array%20of%20synthetic%20and%20diverse%20real-world%0Abenchmarks%2C%20including%20designing%20DNA%20Promoter%2C%20and%20DNA%20Enhancer%20sequences.%0AEmpirically%2C%20we%20find%20that%20Fisher-Flow%20improves%20over%20prior%20diffusion%20and%0Aflow-matching%20models%20on%20these%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14664v1&entry.124074799=Read"},
{"title": "Insight Into the Collocation of Multi-Source Satellite Imagery for\n  Multi-Scale Vessel Detection", "author": "Tran-Vu La and Minh-Tan Pham and Marco Chini", "abstract": "  Ship detection from satellite imagery using Deep Learning (DL) is an\nindispensable solution for maritime surveillance. However, applying DL models\ntrained on one dataset to others having differences in spatial resolution and\nradiometric features requires many adjustments. To overcome this issue, this\npaper focused on the DL models trained on datasets that consist of different\noptical images and a combination of radar and optical data. When dealing with a\nlimited number of training images, the performance of DL models via this\napproach was satisfactory. They could improve 5-20% of average precision,\ndepending on the optical images tested. Likewise, DL models trained on the\ncombined optical and radar dataset could be applied to both optical and radar\nimages. Our experiments showed that the models trained on an optical dataset\ncould be used for radar images, while those trained on a radar dataset offered\nvery poor scores when applied to optical images.\n", "link": "http://arxiv.org/abs/2403.13698v2", "date": "2024-05-23", "relevancy": 2.105, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5365}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5201}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection&body=Title%3A%20Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection%0AAuthor%3A%20Tran-Vu%20La%20and%20Minh-Tan%20Pham%20and%20Marco%20Chini%0AAbstract%3A%20%20%20Ship%20detection%20from%20satellite%20imagery%20using%20Deep%20Learning%20%28DL%29%20is%20an%0Aindispensable%20solution%20for%20maritime%20surveillance.%20However%2C%20applying%20DL%20models%0Atrained%20on%20one%20dataset%20to%20others%20having%20differences%20in%20spatial%20resolution%20and%0Aradiometric%20features%20requires%20many%20adjustments.%20To%20overcome%20this%20issue%2C%20this%0Apaper%20focused%20on%20the%20DL%20models%20trained%20on%20datasets%20that%20consist%20of%20different%0Aoptical%20images%20and%20a%20combination%20of%20radar%20and%20optical%20data.%20When%20dealing%20with%20a%0Alimited%20number%20of%20training%20images%2C%20the%20performance%20of%20DL%20models%20via%20this%0Aapproach%20was%20satisfactory.%20They%20could%20improve%205-20%25%20of%20average%20precision%2C%0Adepending%20on%20the%20optical%20images%20tested.%20Likewise%2C%20DL%20models%20trained%20on%20the%0Acombined%20optical%20and%20radar%20dataset%20could%20be%20applied%20to%20both%20optical%20and%20radar%0Aimages.%20Our%20experiments%20showed%20that%20the%20models%20trained%20on%20an%20optical%20dataset%0Acould%20be%20used%20for%20radar%20images%2C%20while%20those%20trained%20on%20a%20radar%20dataset%20offered%0Avery%20poor%20scores%20when%20applied%20to%20optical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsight%2520Into%2520the%2520Collocation%2520of%2520Multi-Source%2520Satellite%2520Imagery%2520for%250A%2520%2520Multi-Scale%2520Vessel%2520Detection%26entry.906535625%3DTran-Vu%2520La%2520and%2520Minh-Tan%2520Pham%2520and%2520Marco%2520Chini%26entry.1292438233%3D%2520%2520Ship%2520detection%2520from%2520satellite%2520imagery%2520using%2520Deep%2520Learning%2520%2528DL%2529%2520is%2520an%250Aindispensable%2520solution%2520for%2520maritime%2520surveillance.%2520However%252C%2520applying%2520DL%2520models%250Atrained%2520on%2520one%2520dataset%2520to%2520others%2520having%2520differences%2520in%2520spatial%2520resolution%2520and%250Aradiometric%2520features%2520requires%2520many%2520adjustments.%2520To%2520overcome%2520this%2520issue%252C%2520this%250Apaper%2520focused%2520on%2520the%2520DL%2520models%2520trained%2520on%2520datasets%2520that%2520consist%2520of%2520different%250Aoptical%2520images%2520and%2520a%2520combination%2520of%2520radar%2520and%2520optical%2520data.%2520When%2520dealing%2520with%2520a%250Alimited%2520number%2520of%2520training%2520images%252C%2520the%2520performance%2520of%2520DL%2520models%2520via%2520this%250Aapproach%2520was%2520satisfactory.%2520They%2520could%2520improve%25205-20%2525%2520of%2520average%2520precision%252C%250Adepending%2520on%2520the%2520optical%2520images%2520tested.%2520Likewise%252C%2520DL%2520models%2520trained%2520on%2520the%250Acombined%2520optical%2520and%2520radar%2520dataset%2520could%2520be%2520applied%2520to%2520both%2520optical%2520and%2520radar%250Aimages.%2520Our%2520experiments%2520showed%2520that%2520the%2520models%2520trained%2520on%2520an%2520optical%2520dataset%250Acould%2520be%2520used%2520for%2520radar%2520images%252C%2520while%2520those%2520trained%2520on%2520a%2520radar%2520dataset%2520offered%250Avery%2520poor%2520scores%2520when%2520applied%2520to%2520optical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight%20Into%20the%20Collocation%20of%20Multi-Source%20Satellite%20Imagery%20for%0A%20%20Multi-Scale%20Vessel%20Detection&entry.906535625=Tran-Vu%20La%20and%20Minh-Tan%20Pham%20and%20Marco%20Chini&entry.1292438233=%20%20Ship%20detection%20from%20satellite%20imagery%20using%20Deep%20Learning%20%28DL%29%20is%20an%0Aindispensable%20solution%20for%20maritime%20surveillance.%20However%2C%20applying%20DL%20models%0Atrained%20on%20one%20dataset%20to%20others%20having%20differences%20in%20spatial%20resolution%20and%0Aradiometric%20features%20requires%20many%20adjustments.%20To%20overcome%20this%20issue%2C%20this%0Apaper%20focused%20on%20the%20DL%20models%20trained%20on%20datasets%20that%20consist%20of%20different%0Aoptical%20images%20and%20a%20combination%20of%20radar%20and%20optical%20data.%20When%20dealing%20with%20a%0Alimited%20number%20of%20training%20images%2C%20the%20performance%20of%20DL%20models%20via%20this%0Aapproach%20was%20satisfactory.%20They%20could%20improve%205-20%25%20of%20average%20precision%2C%0Adepending%20on%20the%20optical%20images%20tested.%20Likewise%2C%20DL%20models%20trained%20on%20the%0Acombined%20optical%20and%20radar%20dataset%20could%20be%20applied%20to%20both%20optical%20and%20radar%0Aimages.%20Our%20experiments%20showed%20that%20the%20models%20trained%20on%20an%20optical%20dataset%0Acould%20be%20used%20for%20radar%20images%2C%20while%20those%20trained%20on%20a%20radar%20dataset%20offered%0Avery%20poor%20scores%20when%20applied%20to%20optical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13698v2&entry.124074799=Read"},
{"title": "VINS-Multi: A Robust Asynchronous Multi-camera-IMU State Estimator", "author": "Luqi Wang and Yang Xu and Shaojie Shen", "abstract": "  State estimation is a critical foundational module in robotics applications,\nwhere robustness and performance are paramount. Although in recent years, many\nworks have been focusing on improving one of the most widely adopted state\nestimation methods, visual inertial odometry (VIO), by incorporating multiple\ncameras, these efforts predominantly address synchronous camera systems.\nAsynchronous cameras, which offer simpler hardware configurations and enhanced\nresilience, have been largely overlooked. To fill this gap, this paper presents\nVINS-Multi, a novel multi-camera-IMU state estimator for asynchronous cameras.\nThe estimator comprises parallel front ends, a front end coordinator, and a\nback end optimization module capable of handling asynchronous input frames. It\nutilizes the frames effectively through a dynamic feature number allocation and\na frame priority coordination strategy. The proposed estimator is integrated\ninto a customized quadrotor platform and tested in multiple realistic and\nchallenging scenarios to validate its practicality. Additionally, comprehensive\nbenchmark results are provided to showcase the robustness and superior\nperformance of the proposed estimator.\n", "link": "http://arxiv.org/abs/2405.14539v1", "date": "2024-05-23", "relevancy": 2.1048, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5752}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.532}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VINS-Multi%3A%20A%20Robust%20Asynchronous%20Multi-camera-IMU%20State%20Estimator&body=Title%3A%20VINS-Multi%3A%20A%20Robust%20Asynchronous%20Multi-camera-IMU%20State%20Estimator%0AAuthor%3A%20Luqi%20Wang%20and%20Yang%20Xu%20and%20Shaojie%20Shen%0AAbstract%3A%20%20%20State%20estimation%20is%20a%20critical%20foundational%20module%20in%20robotics%20applications%2C%0Awhere%20robustness%20and%20performance%20are%20paramount.%20Although%20in%20recent%20years%2C%20many%0Aworks%20have%20been%20focusing%20on%20improving%20one%20of%20the%20most%20widely%20adopted%20state%0Aestimation%20methods%2C%20visual%20inertial%20odometry%20%28VIO%29%2C%20by%20incorporating%20multiple%0Acameras%2C%20these%20efforts%20predominantly%20address%20synchronous%20camera%20systems.%0AAsynchronous%20cameras%2C%20which%20offer%20simpler%20hardware%20configurations%20and%20enhanced%0Aresilience%2C%20have%20been%20largely%20overlooked.%20To%20fill%20this%20gap%2C%20this%20paper%20presents%0AVINS-Multi%2C%20a%20novel%20multi-camera-IMU%20state%20estimator%20for%20asynchronous%20cameras.%0AThe%20estimator%20comprises%20parallel%20front%20ends%2C%20a%20front%20end%20coordinator%2C%20and%20a%0Aback%20end%20optimization%20module%20capable%20of%20handling%20asynchronous%20input%20frames.%20It%0Autilizes%20the%20frames%20effectively%20through%20a%20dynamic%20feature%20number%20allocation%20and%0Aa%20frame%20priority%20coordination%20strategy.%20The%20proposed%20estimator%20is%20integrated%0Ainto%20a%20customized%20quadrotor%20platform%20and%20tested%20in%20multiple%20realistic%20and%0Achallenging%20scenarios%20to%20validate%20its%20practicality.%20Additionally%2C%20comprehensive%0Abenchmark%20results%20are%20provided%20to%20showcase%20the%20robustness%20and%20superior%0Aperformance%20of%20the%20proposed%20estimator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVINS-Multi%253A%2520A%2520Robust%2520Asynchronous%2520Multi-camera-IMU%2520State%2520Estimator%26entry.906535625%3DLuqi%2520Wang%2520and%2520Yang%2520Xu%2520and%2520Shaojie%2520Shen%26entry.1292438233%3D%2520%2520State%2520estimation%2520is%2520a%2520critical%2520foundational%2520module%2520in%2520robotics%2520applications%252C%250Awhere%2520robustness%2520and%2520performance%2520are%2520paramount.%2520Although%2520in%2520recent%2520years%252C%2520many%250Aworks%2520have%2520been%2520focusing%2520on%2520improving%2520one%2520of%2520the%2520most%2520widely%2520adopted%2520state%250Aestimation%2520methods%252C%2520visual%2520inertial%2520odometry%2520%2528VIO%2529%252C%2520by%2520incorporating%2520multiple%250Acameras%252C%2520these%2520efforts%2520predominantly%2520address%2520synchronous%2520camera%2520systems.%250AAsynchronous%2520cameras%252C%2520which%2520offer%2520simpler%2520hardware%2520configurations%2520and%2520enhanced%250Aresilience%252C%2520have%2520been%2520largely%2520overlooked.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520presents%250AVINS-Multi%252C%2520a%2520novel%2520multi-camera-IMU%2520state%2520estimator%2520for%2520asynchronous%2520cameras.%250AThe%2520estimator%2520comprises%2520parallel%2520front%2520ends%252C%2520a%2520front%2520end%2520coordinator%252C%2520and%2520a%250Aback%2520end%2520optimization%2520module%2520capable%2520of%2520handling%2520asynchronous%2520input%2520frames.%2520It%250Autilizes%2520the%2520frames%2520effectively%2520through%2520a%2520dynamic%2520feature%2520number%2520allocation%2520and%250Aa%2520frame%2520priority%2520coordination%2520strategy.%2520The%2520proposed%2520estimator%2520is%2520integrated%250Ainto%2520a%2520customized%2520quadrotor%2520platform%2520and%2520tested%2520in%2520multiple%2520realistic%2520and%250Achallenging%2520scenarios%2520to%2520validate%2520its%2520practicality.%2520Additionally%252C%2520comprehensive%250Abenchmark%2520results%2520are%2520provided%2520to%2520showcase%2520the%2520robustness%2520and%2520superior%250Aperformance%2520of%2520the%2520proposed%2520estimator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VINS-Multi%3A%20A%20Robust%20Asynchronous%20Multi-camera-IMU%20State%20Estimator&entry.906535625=Luqi%20Wang%20and%20Yang%20Xu%20and%20Shaojie%20Shen&entry.1292438233=%20%20State%20estimation%20is%20a%20critical%20foundational%20module%20in%20robotics%20applications%2C%0Awhere%20robustness%20and%20performance%20are%20paramount.%20Although%20in%20recent%20years%2C%20many%0Aworks%20have%20been%20focusing%20on%20improving%20one%20of%20the%20most%20widely%20adopted%20state%0Aestimation%20methods%2C%20visual%20inertial%20odometry%20%28VIO%29%2C%20by%20incorporating%20multiple%0Acameras%2C%20these%20efforts%20predominantly%20address%20synchronous%20camera%20systems.%0AAsynchronous%20cameras%2C%20which%20offer%20simpler%20hardware%20configurations%20and%20enhanced%0Aresilience%2C%20have%20been%20largely%20overlooked.%20To%20fill%20this%20gap%2C%20this%20paper%20presents%0AVINS-Multi%2C%20a%20novel%20multi-camera-IMU%20state%20estimator%20for%20asynchronous%20cameras.%0AThe%20estimator%20comprises%20parallel%20front%20ends%2C%20a%20front%20end%20coordinator%2C%20and%20a%0Aback%20end%20optimization%20module%20capable%20of%20handling%20asynchronous%20input%20frames.%20It%0Autilizes%20the%20frames%20effectively%20through%20a%20dynamic%20feature%20number%20allocation%20and%0Aa%20frame%20priority%20coordination%20strategy.%20The%20proposed%20estimator%20is%20integrated%0Ainto%20a%20customized%20quadrotor%20platform%20and%20tested%20in%20multiple%20realistic%20and%0Achallenging%20scenarios%20to%20validate%20its%20practicality.%20Additionally%2C%20comprehensive%0Abenchmark%20results%20are%20provided%20to%20showcase%20the%20robustness%20and%20superior%0Aperformance%20of%20the%20proposed%20estimator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14539v1&entry.124074799=Read"},
{"title": "CLIPScope: Enhancing Zero-Shot OOD Detection with Bayesian Scoring", "author": "Hao Fu and Naman Patel and Prashanth Krishnamurthy and Farshad Khorrami", "abstract": "  Detection of out-of-distribution (OOD) samples is crucial for safe real-world\ndeployment of machine learning models. Recent advances in vision language\nfoundation models have made them capable of detecting OOD samples without\nrequiring in-distribution (ID) images. However, these zero-shot methods often\nunderperform as they do not adequately consider ID class likelihoods in their\ndetection confidence scoring. Hence, we introduce CLIPScope, a zero-shot OOD\ndetection approach that normalizes the confidence score of a sample by class\nlikelihoods, akin to a Bayesian posterior update. Furthermore, CLIPScope\nincorporates a novel strategy to mine OOD classes from a large lexical\ndatabase. It selects class labels that are farthest and nearest to ID classes\nin terms of CLIP embedding distance to maximize coverage of OOD samples. We\nconduct extensive ablation studies and empirical evaluations, demonstrating\nstate of the art performance of CLIPScope across various OOD detection\nbenchmarks.\n", "link": "http://arxiv.org/abs/2405.14737v1", "date": "2024-05-23", "relevancy": 2.1029, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPScope%3A%20Enhancing%20Zero-Shot%20OOD%20Detection%20with%20Bayesian%20Scoring&body=Title%3A%20CLIPScope%3A%20Enhancing%20Zero-Shot%20OOD%20Detection%20with%20Bayesian%20Scoring%0AAuthor%3A%20Hao%20Fu%20and%20Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20Detection%20of%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20safe%20real-world%0Adeployment%20of%20machine%20learning%20models.%20Recent%20advances%20in%20vision%20language%0Afoundation%20models%20have%20made%20them%20capable%20of%20detecting%20OOD%20samples%20without%0Arequiring%20in-distribution%20%28ID%29%20images.%20However%2C%20these%20zero-shot%20methods%20often%0Aunderperform%20as%20they%20do%20not%20adequately%20consider%20ID%20class%20likelihoods%20in%20their%0Adetection%20confidence%20scoring.%20Hence%2C%20we%20introduce%20CLIPScope%2C%20a%20zero-shot%20OOD%0Adetection%20approach%20that%20normalizes%20the%20confidence%20score%20of%20a%20sample%20by%20class%0Alikelihoods%2C%20akin%20to%20a%20Bayesian%20posterior%20update.%20Furthermore%2C%20CLIPScope%0Aincorporates%20a%20novel%20strategy%20to%20mine%20OOD%20classes%20from%20a%20large%20lexical%0Adatabase.%20It%20selects%20class%20labels%20that%20are%20farthest%20and%20nearest%20to%20ID%20classes%0Ain%20terms%20of%20CLIP%20embedding%20distance%20to%20maximize%20coverage%20of%20OOD%20samples.%20We%0Aconduct%20extensive%20ablation%20studies%20and%20empirical%20evaluations%2C%20demonstrating%0Astate%20of%20the%20art%20performance%20of%20CLIPScope%20across%20various%20OOD%20detection%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPScope%253A%2520Enhancing%2520Zero-Shot%2520OOD%2520Detection%2520with%2520Bayesian%2520Scoring%26entry.906535625%3DHao%2520Fu%2520and%2520Naman%2520Patel%2520and%2520Prashanth%2520Krishnamurthy%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520Detection%2520of%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520crucial%2520for%2520safe%2520real-world%250Adeployment%2520of%2520machine%2520learning%2520models.%2520Recent%2520advances%2520in%2520vision%2520language%250Afoundation%2520models%2520have%2520made%2520them%2520capable%2520of%2520detecting%2520OOD%2520samples%2520without%250Arequiring%2520in-distribution%2520%2528ID%2529%2520images.%2520However%252C%2520these%2520zero-shot%2520methods%2520often%250Aunderperform%2520as%2520they%2520do%2520not%2520adequately%2520consider%2520ID%2520class%2520likelihoods%2520in%2520their%250Adetection%2520confidence%2520scoring.%2520Hence%252C%2520we%2520introduce%2520CLIPScope%252C%2520a%2520zero-shot%2520OOD%250Adetection%2520approach%2520that%2520normalizes%2520the%2520confidence%2520score%2520of%2520a%2520sample%2520by%2520class%250Alikelihoods%252C%2520akin%2520to%2520a%2520Bayesian%2520posterior%2520update.%2520Furthermore%252C%2520CLIPScope%250Aincorporates%2520a%2520novel%2520strategy%2520to%2520mine%2520OOD%2520classes%2520from%2520a%2520large%2520lexical%250Adatabase.%2520It%2520selects%2520class%2520labels%2520that%2520are%2520farthest%2520and%2520nearest%2520to%2520ID%2520classes%250Ain%2520terms%2520of%2520CLIP%2520embedding%2520distance%2520to%2520maximize%2520coverage%2520of%2520OOD%2520samples.%2520We%250Aconduct%2520extensive%2520ablation%2520studies%2520and%2520empirical%2520evaluations%252C%2520demonstrating%250Astate%2520of%2520the%2520art%2520performance%2520of%2520CLIPScope%2520across%2520various%2520OOD%2520detection%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPScope%3A%20Enhancing%20Zero-Shot%20OOD%20Detection%20with%20Bayesian%20Scoring&entry.906535625=Hao%20Fu%20and%20Naman%20Patel%20and%20Prashanth%20Krishnamurthy%20and%20Farshad%20Khorrami&entry.1292438233=%20%20Detection%20of%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20safe%20real-world%0Adeployment%20of%20machine%20learning%20models.%20Recent%20advances%20in%20vision%20language%0Afoundation%20models%20have%20made%20them%20capable%20of%20detecting%20OOD%20samples%20without%0Arequiring%20in-distribution%20%28ID%29%20images.%20However%2C%20these%20zero-shot%20methods%20often%0Aunderperform%20as%20they%20do%20not%20adequately%20consider%20ID%20class%20likelihoods%20in%20their%0Adetection%20confidence%20scoring.%20Hence%2C%20we%20introduce%20CLIPScope%2C%20a%20zero-shot%20OOD%0Adetection%20approach%20that%20normalizes%20the%20confidence%20score%20of%20a%20sample%20by%20class%0Alikelihoods%2C%20akin%20to%20a%20Bayesian%20posterior%20update.%20Furthermore%2C%20CLIPScope%0Aincorporates%20a%20novel%20strategy%20to%20mine%20OOD%20classes%20from%20a%20large%20lexical%0Adatabase.%20It%20selects%20class%20labels%20that%20are%20farthest%20and%20nearest%20to%20ID%20classes%0Ain%20terms%20of%20CLIP%20embedding%20distance%20to%20maximize%20coverage%20of%20OOD%20samples.%20We%0Aconduct%20extensive%20ablation%20studies%20and%20empirical%20evaluations%2C%20demonstrating%0Astate%20of%20the%20art%20performance%20of%20CLIPScope%20across%20various%20OOD%20detection%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14737v1&entry.124074799=Read"},
{"title": "Efficient Pre-training for Localized Instruction Generation of Videos", "author": "Anil Batra and Davide Moltisanti and Laura Sevilla-Lara and Marcus Rohrbach and Frank Keller", "abstract": "  Procedural videos, exemplified by recipe demonstrations, are instrumental in\nconveying step-by-step instructions. However, understanding such videos is\nchallenging as it involves the precise localization of steps and the generation\nof textual instructions. Manually annotating steps and writing instructions is\ncostly, which limits the size of current datasets and hinders effective\nlearning. Leveraging large but noisy video-transcript datasets for pre-training\ncan boost performance but demands significant computational resources.\nFurthermore, transcripts contain irrelevant content and differ in style from\nhuman-written instructions. To mitigate these issues, we propose a novel\ntechnique, Sieve-&-Swap, to automatically generate high quality training data\nfor the recipe domain: (i) Sieve filters irrelevant transcripts and (ii) Swap\nacquires high quality text by replacing transcripts with human-written\ninstruction from a text-only recipe dataset. The resulting dataset is three\norders of magnitude smaller than current web-scale datasets but enables\nefficient training of large-scale models. Alongside Sieve-&-Swap, we propose\nProcedure Transformer (ProcX), a model for end-to-end step localization and\ninstruction generation for procedural videos. When pre-trained on our curated\ndataset, this model achieves state-of-the-art performance on YouCook2 and Tasty\nwhile using a fraction of the training data. Our code and dataset will be\npublicly released.\n", "link": "http://arxiv.org/abs/2311.15964v3", "date": "2024-05-23", "relevancy": 2.1019, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5311}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.529}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos&body=Title%3A%20Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos%0AAuthor%3A%20Anil%20Batra%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller%0AAbstract%3A%20%20%20Procedural%20videos%2C%20exemplified%20by%20recipe%20demonstrations%2C%20are%20instrumental%20in%0Aconveying%20step-by-step%20instructions.%20However%2C%20understanding%20such%20videos%20is%0Achallenging%20as%20it%20involves%20the%20precise%20localization%20of%20steps%20and%20the%20generation%0Aof%20textual%20instructions.%20Manually%20annotating%20steps%20and%20writing%20instructions%20is%0Acostly%2C%20which%20limits%20the%20size%20of%20current%20datasets%20and%20hinders%20effective%0Alearning.%20Leveraging%20large%20but%20noisy%20video-transcript%20datasets%20for%20pre-training%0Acan%20boost%20performance%20but%20demands%20significant%20computational%20resources.%0AFurthermore%2C%20transcripts%20contain%20irrelevant%20content%20and%20differ%20in%20style%20from%0Ahuman-written%20instructions.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20novel%0Atechnique%2C%20Sieve-%26-Swap%2C%20to%20automatically%20generate%20high%20quality%20training%20data%0Afor%20the%20recipe%20domain%3A%20%28i%29%20Sieve%20filters%20irrelevant%20transcripts%20and%20%28ii%29%20Swap%0Aacquires%20high%20quality%20text%20by%20replacing%20transcripts%20with%20human-written%0Ainstruction%20from%20a%20text-only%20recipe%20dataset.%20The%20resulting%20dataset%20is%20three%0Aorders%20of%20magnitude%20smaller%20than%20current%20web-scale%20datasets%20but%20enables%0Aefficient%20training%20of%20large-scale%20models.%20Alongside%20Sieve-%26-Swap%2C%20we%20propose%0AProcedure%20Transformer%20%28ProcX%29%2C%20a%20model%20for%20end-to-end%20step%20localization%20and%0Ainstruction%20generation%20for%20procedural%20videos.%20When%20pre-trained%20on%20our%20curated%0Adataset%2C%20this%20model%20achieves%20state-of-the-art%20performance%20on%20YouCook2%20and%20Tasty%0Awhile%20using%20a%20fraction%20of%20the%20training%20data.%20Our%20code%20and%20dataset%20will%20be%0Apublicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15964v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Pre-training%2520for%2520Localized%2520Instruction%2520Generation%2520of%2520Videos%26entry.906535625%3DAnil%2520Batra%2520and%2520Davide%2520Moltisanti%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Marcus%2520Rohrbach%2520and%2520Frank%2520Keller%26entry.1292438233%3D%2520%2520Procedural%2520videos%252C%2520exemplified%2520by%2520recipe%2520demonstrations%252C%2520are%2520instrumental%2520in%250Aconveying%2520step-by-step%2520instructions.%2520However%252C%2520understanding%2520such%2520videos%2520is%250Achallenging%2520as%2520it%2520involves%2520the%2520precise%2520localization%2520of%2520steps%2520and%2520the%2520generation%250Aof%2520textual%2520instructions.%2520Manually%2520annotating%2520steps%2520and%2520writing%2520instructions%2520is%250Acostly%252C%2520which%2520limits%2520the%2520size%2520of%2520current%2520datasets%2520and%2520hinders%2520effective%250Alearning.%2520Leveraging%2520large%2520but%2520noisy%2520video-transcript%2520datasets%2520for%2520pre-training%250Acan%2520boost%2520performance%2520but%2520demands%2520significant%2520computational%2520resources.%250AFurthermore%252C%2520transcripts%2520contain%2520irrelevant%2520content%2520and%2520differ%2520in%2520style%2520from%250Ahuman-written%2520instructions.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250Atechnique%252C%2520Sieve-%2526-Swap%252C%2520to%2520automatically%2520generate%2520high%2520quality%2520training%2520data%250Afor%2520the%2520recipe%2520domain%253A%2520%2528i%2529%2520Sieve%2520filters%2520irrelevant%2520transcripts%2520and%2520%2528ii%2529%2520Swap%250Aacquires%2520high%2520quality%2520text%2520by%2520replacing%2520transcripts%2520with%2520human-written%250Ainstruction%2520from%2520a%2520text-only%2520recipe%2520dataset.%2520The%2520resulting%2520dataset%2520is%2520three%250Aorders%2520of%2520magnitude%2520smaller%2520than%2520current%2520web-scale%2520datasets%2520but%2520enables%250Aefficient%2520training%2520of%2520large-scale%2520models.%2520Alongside%2520Sieve-%2526-Swap%252C%2520we%2520propose%250AProcedure%2520Transformer%2520%2528ProcX%2529%252C%2520a%2520model%2520for%2520end-to-end%2520step%2520localization%2520and%250Ainstruction%2520generation%2520for%2520procedural%2520videos.%2520When%2520pre-trained%2520on%2520our%2520curated%250Adataset%252C%2520this%2520model%2520achieves%2520state-of-the-art%2520performance%2520on%2520YouCook2%2520and%2520Tasty%250Awhile%2520using%2520a%2520fraction%2520of%2520the%2520training%2520data.%2520Our%2520code%2520and%2520dataset%2520will%2520be%250Apublicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15964v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Pre-training%20for%20Localized%20Instruction%20Generation%20of%20Videos&entry.906535625=Anil%20Batra%20and%20Davide%20Moltisanti%20and%20Laura%20Sevilla-Lara%20and%20Marcus%20Rohrbach%20and%20Frank%20Keller&entry.1292438233=%20%20Procedural%20videos%2C%20exemplified%20by%20recipe%20demonstrations%2C%20are%20instrumental%20in%0Aconveying%20step-by-step%20instructions.%20However%2C%20understanding%20such%20videos%20is%0Achallenging%20as%20it%20involves%20the%20precise%20localization%20of%20steps%20and%20the%20generation%0Aof%20textual%20instructions.%20Manually%20annotating%20steps%20and%20writing%20instructions%20is%0Acostly%2C%20which%20limits%20the%20size%20of%20current%20datasets%20and%20hinders%20effective%0Alearning.%20Leveraging%20large%20but%20noisy%20video-transcript%20datasets%20for%20pre-training%0Acan%20boost%20performance%20but%20demands%20significant%20computational%20resources.%0AFurthermore%2C%20transcripts%20contain%20irrelevant%20content%20and%20differ%20in%20style%20from%0Ahuman-written%20instructions.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20novel%0Atechnique%2C%20Sieve-%26-Swap%2C%20to%20automatically%20generate%20high%20quality%20training%20data%0Afor%20the%20recipe%20domain%3A%20%28i%29%20Sieve%20filters%20irrelevant%20transcripts%20and%20%28ii%29%20Swap%0Aacquires%20high%20quality%20text%20by%20replacing%20transcripts%20with%20human-written%0Ainstruction%20from%20a%20text-only%20recipe%20dataset.%20The%20resulting%20dataset%20is%20three%0Aorders%20of%20magnitude%20smaller%20than%20current%20web-scale%20datasets%20but%20enables%0Aefficient%20training%20of%20large-scale%20models.%20Alongside%20Sieve-%26-Swap%2C%20we%20propose%0AProcedure%20Transformer%20%28ProcX%29%2C%20a%20model%20for%20end-to-end%20step%20localization%20and%0Ainstruction%20generation%20for%20procedural%20videos.%20When%20pre-trained%20on%20our%20curated%0Adataset%2C%20this%20model%20achieves%20state-of-the-art%20performance%20on%20YouCook2%20and%20Tasty%0Awhile%20using%20a%20fraction%20of%20the%20training%20data.%20Our%20code%20and%20dataset%20will%20be%0Apublicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15964v3&entry.124074799=Read"},
{"title": "UDKAG: Augmenting Large Vision-Language Models with Up-to-Date Knowledge", "author": "Chuanhao Li and Zhen Li and Chenchen Jing and Shuo Liu and Wenqi Shao and Yuwei Wu and Ping Luo and Yu Qiao and Kaipeng Zhang", "abstract": "  Large vision-language models (LVLMs) are ignorant of the up-to-date\nknowledge, such as LLaVA series, because they cannot be updated frequently due\nto the large amount of resources required, and therefore fail in many cases.\nFor example, if a LVLM was released on January 2024, and it wouldn't know the\ndetailed plot of the new movie Dune 2, which wasn't released until February\n2024. To solve the problem, a promising solution is to provide LVLMs with\nup-to-date knowledge via internet search during inference, i.e.,\ninternet-augmented generation (IAG), which is already integrated in some\nclosed-source commercial LVLMs such as GPT-4V. However, the specific mechanics\nunderpinning them remain a mystery. In this paper, we propose a plug-and-play\nframework, for augmenting existing LVLMs in handling visual question answering\n(VQA) about up-to-date knowledge, dubbed UDKAG. A hierarchical filtering model\nis trained to effectively and efficiently find the most helpful content from\nthe websites returned by a search engine to prompt LVLMs with up-to-date\nknowledge. To train the model and evaluate our framework's performance, we\npropose a pipeline to automatically generate news-related VQA samples to\nconstruct a dataset, dubbed UDK-VQA. A multi-model voting mechanism is\nintroduced to label the usefulness of website/content for VQA samples to\nconstruct the training set. Experimental results demonstrate the effectiveness\nof our framework, outperforming GPT-4V by about 25% in accuracy.\n", "link": "http://arxiv.org/abs/2405.14554v1", "date": "2024-05-23", "relevancy": 2.0998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.559}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UDKAG%3A%20Augmenting%20Large%20Vision-Language%20Models%20with%20Up-to-Date%20Knowledge&body=Title%3A%20UDKAG%3A%20Augmenting%20Large%20Vision-Language%20Models%20with%20Up-to-Date%20Knowledge%0AAuthor%3A%20Chuanhao%20Li%20and%20Zhen%20Li%20and%20Chenchen%20Jing%20and%20Shuo%20Liu%20and%20Wenqi%20Shao%20and%20Yuwei%20Wu%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20ignorant%20of%20the%20up-to-date%0Aknowledge%2C%20such%20as%20LLaVA%20series%2C%20because%20they%20cannot%20be%20updated%20frequently%20due%0Ato%20the%20large%20amount%20of%20resources%20required%2C%20and%20therefore%20fail%20in%20many%20cases.%0AFor%20example%2C%20if%20a%20LVLM%20was%20released%20on%20January%202024%2C%20and%20it%20wouldn%27t%20know%20the%0Adetailed%20plot%20of%20the%20new%20movie%20Dune%202%2C%20which%20wasn%27t%20released%20until%20February%0A2024.%20To%20solve%20the%20problem%2C%20a%20promising%20solution%20is%20to%20provide%20LVLMs%20with%0Aup-to-date%20knowledge%20via%20internet%20search%20during%20inference%2C%20i.e.%2C%0Ainternet-augmented%20generation%20%28IAG%29%2C%20which%20is%20already%20integrated%20in%20some%0Aclosed-source%20commercial%20LVLMs%20such%20as%20GPT-4V.%20However%2C%20the%20specific%20mechanics%0Aunderpinning%20them%20remain%20a%20mystery.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%0Aframework%2C%20for%20augmenting%20existing%20LVLMs%20in%20handling%20visual%20question%20answering%0A%28VQA%29%20about%20up-to-date%20knowledge%2C%20dubbed%20UDKAG.%20A%20hierarchical%20filtering%20model%0Ais%20trained%20to%20effectively%20and%20efficiently%20find%20the%20most%20helpful%20content%20from%0Athe%20websites%20returned%20by%20a%20search%20engine%20to%20prompt%20LVLMs%20with%20up-to-date%0Aknowledge.%20To%20train%20the%20model%20and%20evaluate%20our%20framework%27s%20performance%2C%20we%0Apropose%20a%20pipeline%20to%20automatically%20generate%20news-related%20VQA%20samples%20to%0Aconstruct%20a%20dataset%2C%20dubbed%20UDK-VQA.%20A%20multi-model%20voting%20mechanism%20is%0Aintroduced%20to%20label%20the%20usefulness%20of%20website/content%20for%20VQA%20samples%20to%0Aconstruct%20the%20training%20set.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20framework%2C%20outperforming%20GPT-4V%20by%20about%2025%25%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUDKAG%253A%2520Augmenting%2520Large%2520Vision-Language%2520Models%2520with%2520Up-to-Date%2520Knowledge%26entry.906535625%3DChuanhao%2520Li%2520and%2520Zhen%2520Li%2520and%2520Chenchen%2520Jing%2520and%2520Shuo%2520Liu%2520and%2520Wenqi%2520Shao%2520and%2520Yuwei%2520Wu%2520and%2520Ping%2520Luo%2520and%2520Yu%2520Qiao%2520and%2520Kaipeng%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520ignorant%2520of%2520the%2520up-to-date%250Aknowledge%252C%2520such%2520as%2520LLaVA%2520series%252C%2520because%2520they%2520cannot%2520be%2520updated%2520frequently%2520due%250Ato%2520the%2520large%2520amount%2520of%2520resources%2520required%252C%2520and%2520therefore%2520fail%2520in%2520many%2520cases.%250AFor%2520example%252C%2520if%2520a%2520LVLM%2520was%2520released%2520on%2520January%25202024%252C%2520and%2520it%2520wouldn%2527t%2520know%2520the%250Adetailed%2520plot%2520of%2520the%2520new%2520movie%2520Dune%25202%252C%2520which%2520wasn%2527t%2520released%2520until%2520February%250A2024.%2520To%2520solve%2520the%2520problem%252C%2520a%2520promising%2520solution%2520is%2520to%2520provide%2520LVLMs%2520with%250Aup-to-date%2520knowledge%2520via%2520internet%2520search%2520during%2520inference%252C%2520i.e.%252C%250Ainternet-augmented%2520generation%2520%2528IAG%2529%252C%2520which%2520is%2520already%2520integrated%2520in%2520some%250Aclosed-source%2520commercial%2520LVLMs%2520such%2520as%2520GPT-4V.%2520However%252C%2520the%2520specific%2520mechanics%250Aunderpinning%2520them%2520remain%2520a%2520mystery.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520plug-and-play%250Aframework%252C%2520for%2520augmenting%2520existing%2520LVLMs%2520in%2520handling%2520visual%2520question%2520answering%250A%2528VQA%2529%2520about%2520up-to-date%2520knowledge%252C%2520dubbed%2520UDKAG.%2520A%2520hierarchical%2520filtering%2520model%250Ais%2520trained%2520to%2520effectively%2520and%2520efficiently%2520find%2520the%2520most%2520helpful%2520content%2520from%250Athe%2520websites%2520returned%2520by%2520a%2520search%2520engine%2520to%2520prompt%2520LVLMs%2520with%2520up-to-date%250Aknowledge.%2520To%2520train%2520the%2520model%2520and%2520evaluate%2520our%2520framework%2527s%2520performance%252C%2520we%250Apropose%2520a%2520pipeline%2520to%2520automatically%2520generate%2520news-related%2520VQA%2520samples%2520to%250Aconstruct%2520a%2520dataset%252C%2520dubbed%2520UDK-VQA.%2520A%2520multi-model%2520voting%2520mechanism%2520is%250Aintroduced%2520to%2520label%2520the%2520usefulness%2520of%2520website/content%2520for%2520VQA%2520samples%2520to%250Aconstruct%2520the%2520training%2520set.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520framework%252C%2520outperforming%2520GPT-4V%2520by%2520about%252025%2525%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UDKAG%3A%20Augmenting%20Large%20Vision-Language%20Models%20with%20Up-to-Date%20Knowledge&entry.906535625=Chuanhao%20Li%20and%20Zhen%20Li%20and%20Chenchen%20Jing%20and%20Shuo%20Liu%20and%20Wenqi%20Shao%20and%20Yuwei%20Wu%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Kaipeng%20Zhang&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20ignorant%20of%20the%20up-to-date%0Aknowledge%2C%20such%20as%20LLaVA%20series%2C%20because%20they%20cannot%20be%20updated%20frequently%20due%0Ato%20the%20large%20amount%20of%20resources%20required%2C%20and%20therefore%20fail%20in%20many%20cases.%0AFor%20example%2C%20if%20a%20LVLM%20was%20released%20on%20January%202024%2C%20and%20it%20wouldn%27t%20know%20the%0Adetailed%20plot%20of%20the%20new%20movie%20Dune%202%2C%20which%20wasn%27t%20released%20until%20February%0A2024.%20To%20solve%20the%20problem%2C%20a%20promising%20solution%20is%20to%20provide%20LVLMs%20with%0Aup-to-date%20knowledge%20via%20internet%20search%20during%20inference%2C%20i.e.%2C%0Ainternet-augmented%20generation%20%28IAG%29%2C%20which%20is%20already%20integrated%20in%20some%0Aclosed-source%20commercial%20LVLMs%20such%20as%20GPT-4V.%20However%2C%20the%20specific%20mechanics%0Aunderpinning%20them%20remain%20a%20mystery.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%0Aframework%2C%20for%20augmenting%20existing%20LVLMs%20in%20handling%20visual%20question%20answering%0A%28VQA%29%20about%20up-to-date%20knowledge%2C%20dubbed%20UDKAG.%20A%20hierarchical%20filtering%20model%0Ais%20trained%20to%20effectively%20and%20efficiently%20find%20the%20most%20helpful%20content%20from%0Athe%20websites%20returned%20by%20a%20search%20engine%20to%20prompt%20LVLMs%20with%20up-to-date%0Aknowledge.%20To%20train%20the%20model%20and%20evaluate%20our%20framework%27s%20performance%2C%20we%0Apropose%20a%20pipeline%20to%20automatically%20generate%20news-related%20VQA%20samples%20to%0Aconstruct%20a%20dataset%2C%20dubbed%20UDK-VQA.%20A%20multi-model%20voting%20mechanism%20is%0Aintroduced%20to%20label%20the%20usefulness%20of%20website/content%20for%20VQA%20samples%20to%0Aconstruct%20the%20training%20set.%20Experimental%20results%20demonstrate%20the%20effectiveness%0Aof%20our%20framework%2C%20outperforming%20GPT-4V%20by%20about%2025%25%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14554v1&entry.124074799=Read"},
{"title": "U-TELL: Unsupervised Task Expert Lifelong Learning", "author": "Indu Solomon and Aye Phyu Phyu Aung and Uttam Kumar and Senthilnath Jayavelu", "abstract": "  Continual learning (CL) models are designed to learn new tasks arriving\nsequentially without re-training the network. However, real-world ML\napplications have very limited label information and these models suffer from\ncatastrophic forgetting. To address these issues, we propose an unsupervised CL\nmodel with task experts called Unsupervised Task Expert Lifelong Learning\n(U-TELL) to continually learn the data arriving in a sequence addressing\ncatastrophic forgetting. During training of U-TELL, we introduce a new expert\non arrival of a new task. Our proposed architecture has task experts, a\nstructured data generator and a task assigner. Each task expert is composed of\n3 blocks; i) a variational autoencoder to capture the task distribution and\nperform data abstraction, ii) a k-means clustering module, and iii) a structure\nextractor to preserve latent task data signature. During testing, task assigner\nselects a suitable expert to perform clustering. U-TELL does not store or\nreplay task samples, instead, we use generated structured samples to train the\ntask assigner. We compared U-TELL with five SOTA unsupervised CL methods.\nU-TELL outperformed all baselines on seven benchmarks and one industry dataset\nfor various CL scenarios with a training time over 6 times faster than the best\nperforming baseline.\n", "link": "http://arxiv.org/abs/2405.14623v1", "date": "2024-05-23", "relevancy": 2.0934, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning&body=Title%3A%20U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0AAuthor%3A%20Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20models%20are%20designed%20to%20learn%20new%20tasks%20arriving%0Asequentially%20without%20re-training%20the%20network.%20However%2C%20real-world%20ML%0Aapplications%20have%20very%20limited%20label%20information%20and%20these%20models%20suffer%20from%0Acatastrophic%20forgetting.%20To%20address%20these%20issues%2C%20we%20propose%20an%20unsupervised%20CL%0Amodel%20with%20task%20experts%20called%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0A%28U-TELL%29%20to%20continually%20learn%20the%20data%20arriving%20in%20a%20sequence%20addressing%0Acatastrophic%20forgetting.%20During%20training%20of%20U-TELL%2C%20we%20introduce%20a%20new%20expert%0Aon%20arrival%20of%20a%20new%20task.%20Our%20proposed%20architecture%20has%20task%20experts%2C%20a%0Astructured%20data%20generator%20and%20a%20task%20assigner.%20Each%20task%20expert%20is%20composed%20of%0A3%20blocks%3B%20i%29%20a%20variational%20autoencoder%20to%20capture%20the%20task%20distribution%20and%0Aperform%20data%20abstraction%2C%20ii%29%20a%20k-means%20clustering%20module%2C%20and%20iii%29%20a%20structure%0Aextractor%20to%20preserve%20latent%20task%20data%20signature.%20During%20testing%2C%20task%20assigner%0Aselects%20a%20suitable%20expert%20to%20perform%20clustering.%20U-TELL%20does%20not%20store%20or%0Areplay%20task%20samples%2C%20instead%2C%20we%20use%20generated%20structured%20samples%20to%20train%20the%0Atask%20assigner.%20We%20compared%20U-TELL%20with%20five%20SOTA%20unsupervised%20CL%20methods.%0AU-TELL%20outperformed%20all%20baselines%20on%20seven%20benchmarks%20and%20one%20industry%20dataset%0Afor%20various%20CL%20scenarios%20with%20a%20training%20time%20over%206%20times%20faster%20than%20the%20best%0Aperforming%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-TELL%253A%2520Unsupervised%2520Task%2520Expert%2520Lifelong%2520Learning%26entry.906535625%3DIndu%2520Solomon%2520and%2520Aye%2520Phyu%2520Phyu%2520Aung%2520and%2520Uttam%2520Kumar%2520and%2520Senthilnath%2520Jayavelu%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520models%2520are%2520designed%2520to%2520learn%2520new%2520tasks%2520arriving%250Asequentially%2520without%2520re-training%2520the%2520network.%2520However%252C%2520real-world%2520ML%250Aapplications%2520have%2520very%2520limited%2520label%2520information%2520and%2520these%2520models%2520suffer%2520from%250Acatastrophic%2520forgetting.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%2520unsupervised%2520CL%250Amodel%2520with%2520task%2520experts%2520called%2520Unsupervised%2520Task%2520Expert%2520Lifelong%2520Learning%250A%2528U-TELL%2529%2520to%2520continually%2520learn%2520the%2520data%2520arriving%2520in%2520a%2520sequence%2520addressing%250Acatastrophic%2520forgetting.%2520During%2520training%2520of%2520U-TELL%252C%2520we%2520introduce%2520a%2520new%2520expert%250Aon%2520arrival%2520of%2520a%2520new%2520task.%2520Our%2520proposed%2520architecture%2520has%2520task%2520experts%252C%2520a%250Astructured%2520data%2520generator%2520and%2520a%2520task%2520assigner.%2520Each%2520task%2520expert%2520is%2520composed%2520of%250A3%2520blocks%253B%2520i%2529%2520a%2520variational%2520autoencoder%2520to%2520capture%2520the%2520task%2520distribution%2520and%250Aperform%2520data%2520abstraction%252C%2520ii%2529%2520a%2520k-means%2520clustering%2520module%252C%2520and%2520iii%2529%2520a%2520structure%250Aextractor%2520to%2520preserve%2520latent%2520task%2520data%2520signature.%2520During%2520testing%252C%2520task%2520assigner%250Aselects%2520a%2520suitable%2520expert%2520to%2520perform%2520clustering.%2520U-TELL%2520does%2520not%2520store%2520or%250Areplay%2520task%2520samples%252C%2520instead%252C%2520we%2520use%2520generated%2520structured%2520samples%2520to%2520train%2520the%250Atask%2520assigner.%2520We%2520compared%2520U-TELL%2520with%2520five%2520SOTA%2520unsupervised%2520CL%2520methods.%250AU-TELL%2520outperformed%2520all%2520baselines%2520on%2520seven%2520benchmarks%2520and%2520one%2520industry%2520dataset%250Afor%2520various%2520CL%2520scenarios%2520with%2520a%2520training%2520time%2520over%25206%2520times%2520faster%2520than%2520the%2520best%250Aperforming%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-TELL%3A%20Unsupervised%20Task%20Expert%20Lifelong%20Learning&entry.906535625=Indu%20Solomon%20and%20Aye%20Phyu%20Phyu%20Aung%20and%20Uttam%20Kumar%20and%20Senthilnath%20Jayavelu&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20models%20are%20designed%20to%20learn%20new%20tasks%20arriving%0Asequentially%20without%20re-training%20the%20network.%20However%2C%20real-world%20ML%0Aapplications%20have%20very%20limited%20label%20information%20and%20these%20models%20suffer%20from%0Acatastrophic%20forgetting.%20To%20address%20these%20issues%2C%20we%20propose%20an%20unsupervised%20CL%0Amodel%20with%20task%20experts%20called%20Unsupervised%20Task%20Expert%20Lifelong%20Learning%0A%28U-TELL%29%20to%20continually%20learn%20the%20data%20arriving%20in%20a%20sequence%20addressing%0Acatastrophic%20forgetting.%20During%20training%20of%20U-TELL%2C%20we%20introduce%20a%20new%20expert%0Aon%20arrival%20of%20a%20new%20task.%20Our%20proposed%20architecture%20has%20task%20experts%2C%20a%0Astructured%20data%20generator%20and%20a%20task%20assigner.%20Each%20task%20expert%20is%20composed%20of%0A3%20blocks%3B%20i%29%20a%20variational%20autoencoder%20to%20capture%20the%20task%20distribution%20and%0Aperform%20data%20abstraction%2C%20ii%29%20a%20k-means%20clustering%20module%2C%20and%20iii%29%20a%20structure%0Aextractor%20to%20preserve%20latent%20task%20data%20signature.%20During%20testing%2C%20task%20assigner%0Aselects%20a%20suitable%20expert%20to%20perform%20clustering.%20U-TELL%20does%20not%20store%20or%0Areplay%20task%20samples%2C%20instead%2C%20we%20use%20generated%20structured%20samples%20to%20train%20the%0Atask%20assigner.%20We%20compared%20U-TELL%20with%20five%20SOTA%20unsupervised%20CL%20methods.%0AU-TELL%20outperformed%20all%20baselines%20on%20seven%20benchmarks%20and%20one%20industry%20dataset%0Afor%20various%20CL%20scenarios%20with%20a%20training%20time%20over%206%20times%20faster%20than%20the%20best%0Aperforming%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14623v1&entry.124074799=Read"},
{"title": "Iterative Methods for Full-Scale Gaussian Process Approximations for\n  Large Spatial Data", "author": "Tim Gyger and Reinhard Furrer and Fabio Sigrist", "abstract": "  Gaussian processes are flexible probabilistic regression models which are\nwidely used in statistics and machine learning. However, a drawback is their\nlimited scalability to large data sets. To alleviate this, we consider\nfull-scale approximations (FSAs) that combine predictive process methods and\ncovariance tapering, thus approximating both global and local structures. We\nshow how iterative methods can be used to reduce the computational costs for\ncalculating likelihoods, gradients, and predictive distributions with FSAs. We\nintroduce a novel preconditioner and show that it accelerates the conjugate\ngradient method's convergence speed and mitigates its sensitivity with respect\nto the FSA parameters and the eigenvalue structure of the original covariance\nmatrix, and we demonstrate empirically that it outperforms a state-of-the-art\npivoted Cholesky preconditioner. Further, we present a novel, accurate, and\nfast way to calculate predictive variances relying on stochastic estimations\nand iterative methods. In both simulated and real-world data experiments, we\nfind that our proposed methodology achieves the same accuracy as Cholesky-based\ncomputations with a substantial reduction in computational time. Finally, we\nalso compare different approaches for determining inducing points in predictive\nprocess and FSA models. All methods are implemented in a free C++ software\nlibrary with high-level Python and R packages.\n", "link": "http://arxiv.org/abs/2405.14492v1", "date": "2024-05-23", "relevancy": 2.0823, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5535}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.507}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data&body=Title%3A%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data%0AAuthor%3A%20Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%0Awidely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%0Alimited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20we%20consider%0Afull-scale%20approximations%20%28FSAs%29%20that%20combine%20predictive%20process%20methods%20and%0Acovariance%20tapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%0Ashow%20how%20iterative%20methods%20can%20be%20used%20to%20reduce%20the%20computational%20costs%20for%0Acalculating%20likelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20We%0Aintroduce%20a%20novel%20preconditioner%20and%20show%20that%20it%20accelerates%20the%20conjugate%0Agradient%20method%27s%20convergence%20speed%20and%20mitigates%20its%20sensitivity%20with%20respect%0Ato%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%20of%20the%20original%20covariance%0Amatrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%20outperforms%20a%20state-of-the-art%0Apivoted%20Cholesky%20preconditioner.%20Further%2C%20we%20present%20a%20novel%2C%20accurate%2C%20and%0Afast%20way%20to%20calculate%20predictive%20variances%20relying%20on%20stochastic%20estimations%0Aand%20iterative%20methods.%20In%20both%20simulated%20and%20real-world%20data%20experiments%2C%20we%0Afind%20that%20our%20proposed%20methodology%20achieves%20the%20same%20accuracy%20as%20Cholesky-based%0Acomputations%20with%20a%20substantial%20reduction%20in%20computational%20time.%20Finally%2C%20we%0Aalso%20compare%20different%20approaches%20for%20determining%20inducing%20points%20in%20predictive%0Aprocess%20and%20FSA%20models.%20All%20methods%20are%20implemented%20in%20a%20free%20C%2B%2B%20software%0Alibrary%20with%20high-level%20Python%20and%20R%20packages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Methods%2520for%2520Full-Scale%2520Gaussian%2520Process%2520Approximations%2520for%250A%2520%2520Large%2520Spatial%2520Data%26entry.906535625%3DTim%2520Gyger%2520and%2520Reinhard%2520Furrer%2520and%2520Fabio%2520Sigrist%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520flexible%2520probabilistic%2520regression%2520models%2520which%2520are%250Awidely%2520used%2520in%2520statistics%2520and%2520machine%2520learning.%2520However%252C%2520a%2520drawback%2520is%2520their%250Alimited%2520scalability%2520to%2520large%2520data%2520sets.%2520To%2520alleviate%2520this%252C%2520we%2520consider%250Afull-scale%2520approximations%2520%2528FSAs%2529%2520that%2520combine%2520predictive%2520process%2520methods%2520and%250Acovariance%2520tapering%252C%2520thus%2520approximating%2520both%2520global%2520and%2520local%2520structures.%2520We%250Ashow%2520how%2520iterative%2520methods%2520can%2520be%2520used%2520to%2520reduce%2520the%2520computational%2520costs%2520for%250Acalculating%2520likelihoods%252C%2520gradients%252C%2520and%2520predictive%2520distributions%2520with%2520FSAs.%2520We%250Aintroduce%2520a%2520novel%2520preconditioner%2520and%2520show%2520that%2520it%2520accelerates%2520the%2520conjugate%250Agradient%2520method%2527s%2520convergence%2520speed%2520and%2520mitigates%2520its%2520sensitivity%2520with%2520respect%250Ato%2520the%2520FSA%2520parameters%2520and%2520the%2520eigenvalue%2520structure%2520of%2520the%2520original%2520covariance%250Amatrix%252C%2520and%2520we%2520demonstrate%2520empirically%2520that%2520it%2520outperforms%2520a%2520state-of-the-art%250Apivoted%2520Cholesky%2520preconditioner.%2520Further%252C%2520we%2520present%2520a%2520novel%252C%2520accurate%252C%2520and%250Afast%2520way%2520to%2520calculate%2520predictive%2520variances%2520relying%2520on%2520stochastic%2520estimations%250Aand%2520iterative%2520methods.%2520In%2520both%2520simulated%2520and%2520real-world%2520data%2520experiments%252C%2520we%250Afind%2520that%2520our%2520proposed%2520methodology%2520achieves%2520the%2520same%2520accuracy%2520as%2520Cholesky-based%250Acomputations%2520with%2520a%2520substantial%2520reduction%2520in%2520computational%2520time.%2520Finally%252C%2520we%250Aalso%2520compare%2520different%2520approaches%2520for%2520determining%2520inducing%2520points%2520in%2520predictive%250Aprocess%2520and%2520FSA%2520models.%2520All%2520methods%2520are%2520implemented%2520in%2520a%2520free%2520C%252B%252B%2520software%250Alibrary%2520with%2520high-level%2520Python%2520and%2520R%2520packages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data&entry.906535625=Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist&entry.1292438233=%20%20Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%0Awidely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%0Alimited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20we%20consider%0Afull-scale%20approximations%20%28FSAs%29%20that%20combine%20predictive%20process%20methods%20and%0Acovariance%20tapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%0Ashow%20how%20iterative%20methods%20can%20be%20used%20to%20reduce%20the%20computational%20costs%20for%0Acalculating%20likelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20We%0Aintroduce%20a%20novel%20preconditioner%20and%20show%20that%20it%20accelerates%20the%20conjugate%0Agradient%20method%27s%20convergence%20speed%20and%20mitigates%20its%20sensitivity%20with%20respect%0Ato%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%20of%20the%20original%20covariance%0Amatrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%20outperforms%20a%20state-of-the-art%0Apivoted%20Cholesky%20preconditioner.%20Further%2C%20we%20present%20a%20novel%2C%20accurate%2C%20and%0Afast%20way%20to%20calculate%20predictive%20variances%20relying%20on%20stochastic%20estimations%0Aand%20iterative%20methods.%20In%20both%20simulated%20and%20real-world%20data%20experiments%2C%20we%0Afind%20that%20our%20proposed%20methodology%20achieves%20the%20same%20accuracy%20as%20Cholesky-based%0Acomputations%20with%20a%20substantial%20reduction%20in%20computational%20time.%20Finally%2C%20we%0Aalso%20compare%20different%20approaches%20for%20determining%20inducing%20points%20in%20predictive%0Aprocess%20and%20FSA%20models.%20All%20methods%20are%20implemented%20in%20a%20free%20C%2B%2B%20software%0Alibrary%20with%20high-level%20Python%20and%20R%20packages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14492v1&entry.124074799=Read"},
{"title": "HyenaPixel: Global Image Context with Convolutions", "author": "Julian Spravil and Sebastian Houben and Sven Behnke", "abstract": "  In computer vision, a larger effective receptive field (ERF) is associated\nwith better performance. While attention natively supports global context, its\nquadratic complexity limits its applicability to tasks that benefit from\nhigh-resolution input. In this work, we extend Hyena, a convolution-based\nattention replacement, from causal sequences to bidirectional data and\ntwo-dimensional image space. We scale Hyena's convolution kernels beyond the\nfeature map size, up to 191$\\times$191, to maximize ERF while maintaining\nsub-quadratic complexity in the number of pixels. We integrate our\ntwo-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer\nframework. For image categorization, HyenaPixel and bidirectional Hyena achieve\na competitive ImageNet-1k top-1 accuracy of 84.9% and 85.2%, respectively, with\nno additional training data, while outperforming other convolutional and\nlarge-kernel networks. Combining HyenaPixel with attention further improves\naccuracy. We attribute the success of bidirectional Hyena to learning the\ndata-dependent geometric arrangement of pixels without a fixed neighborhood\ndefinition. Experimental results on downstream tasks suggest that HyenaPixel\nwith large filters and a fixed neighborhood leads to better localization\nperformance.\n", "link": "http://arxiv.org/abs/2402.19305v2", "date": "2024-05-23", "relevancy": 2.0793, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5248}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5164}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyenaPixel%3A%20Global%20Image%20Context%20with%20Convolutions&body=Title%3A%20HyenaPixel%3A%20Global%20Image%20Context%20with%20Convolutions%0AAuthor%3A%20Julian%20Spravil%20and%20Sebastian%20Houben%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20In%20computer%20vision%2C%20a%20larger%20effective%20receptive%20field%20%28ERF%29%20is%20associated%0Awith%20better%20performance.%20While%20attention%20natively%20supports%20global%20context%2C%20its%0Aquadratic%20complexity%20limits%20its%20applicability%20to%20tasks%20that%20benefit%20from%0Ahigh-resolution%20input.%20In%20this%20work%2C%20we%20extend%20Hyena%2C%20a%20convolution-based%0Aattention%20replacement%2C%20from%20causal%20sequences%20to%20bidirectional%20data%20and%0Atwo-dimensional%20image%20space.%20We%20scale%20Hyena%27s%20convolution%20kernels%20beyond%20the%0Afeature%20map%20size%2C%20up%20to%20191%24%5Ctimes%24191%2C%20to%20maximize%20ERF%20while%20maintaining%0Asub-quadratic%20complexity%20in%20the%20number%20of%20pixels.%20We%20integrate%20our%0Atwo-dimensional%20Hyena%2C%20HyenaPixel%2C%20and%20bidirectional%20Hyena%20into%20the%20MetaFormer%0Aframework.%20For%20image%20categorization%2C%20HyenaPixel%20and%20bidirectional%20Hyena%20achieve%0Aa%20competitive%20ImageNet-1k%20top-1%20accuracy%20of%2084.9%25%20and%2085.2%25%2C%20respectively%2C%20with%0Ano%20additional%20training%20data%2C%20while%20outperforming%20other%20convolutional%20and%0Alarge-kernel%20networks.%20Combining%20HyenaPixel%20with%20attention%20further%20improves%0Aaccuracy.%20We%20attribute%20the%20success%20of%20bidirectional%20Hyena%20to%20learning%20the%0Adata-dependent%20geometric%20arrangement%20of%20pixels%20without%20a%20fixed%20neighborhood%0Adefinition.%20Experimental%20results%20on%20downstream%20tasks%20suggest%20that%20HyenaPixel%0Awith%20large%20filters%20and%20a%20fixed%20neighborhood%20leads%20to%20better%20localization%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyenaPixel%253A%2520Global%2520Image%2520Context%2520with%2520Convolutions%26entry.906535625%3DJulian%2520Spravil%2520and%2520Sebastian%2520Houben%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520In%2520computer%2520vision%252C%2520a%2520larger%2520effective%2520receptive%2520field%2520%2528ERF%2529%2520is%2520associated%250Awith%2520better%2520performance.%2520While%2520attention%2520natively%2520supports%2520global%2520context%252C%2520its%250Aquadratic%2520complexity%2520limits%2520its%2520applicability%2520to%2520tasks%2520that%2520benefit%2520from%250Ahigh-resolution%2520input.%2520In%2520this%2520work%252C%2520we%2520extend%2520Hyena%252C%2520a%2520convolution-based%250Aattention%2520replacement%252C%2520from%2520causal%2520sequences%2520to%2520bidirectional%2520data%2520and%250Atwo-dimensional%2520image%2520space.%2520We%2520scale%2520Hyena%2527s%2520convolution%2520kernels%2520beyond%2520the%250Afeature%2520map%2520size%252C%2520up%2520to%2520191%2524%255Ctimes%2524191%252C%2520to%2520maximize%2520ERF%2520while%2520maintaining%250Asub-quadratic%2520complexity%2520in%2520the%2520number%2520of%2520pixels.%2520We%2520integrate%2520our%250Atwo-dimensional%2520Hyena%252C%2520HyenaPixel%252C%2520and%2520bidirectional%2520Hyena%2520into%2520the%2520MetaFormer%250Aframework.%2520For%2520image%2520categorization%252C%2520HyenaPixel%2520and%2520bidirectional%2520Hyena%2520achieve%250Aa%2520competitive%2520ImageNet-1k%2520top-1%2520accuracy%2520of%252084.9%2525%2520and%252085.2%2525%252C%2520respectively%252C%2520with%250Ano%2520additional%2520training%2520data%252C%2520while%2520outperforming%2520other%2520convolutional%2520and%250Alarge-kernel%2520networks.%2520Combining%2520HyenaPixel%2520with%2520attention%2520further%2520improves%250Aaccuracy.%2520We%2520attribute%2520the%2520success%2520of%2520bidirectional%2520Hyena%2520to%2520learning%2520the%250Adata-dependent%2520geometric%2520arrangement%2520of%2520pixels%2520without%2520a%2520fixed%2520neighborhood%250Adefinition.%2520Experimental%2520results%2520on%2520downstream%2520tasks%2520suggest%2520that%2520HyenaPixel%250Awith%2520large%2520filters%2520and%2520a%2520fixed%2520neighborhood%2520leads%2520to%2520better%2520localization%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyenaPixel%3A%20Global%20Image%20Context%20with%20Convolutions&entry.906535625=Julian%20Spravil%20and%20Sebastian%20Houben%20and%20Sven%20Behnke&entry.1292438233=%20%20In%20computer%20vision%2C%20a%20larger%20effective%20receptive%20field%20%28ERF%29%20is%20associated%0Awith%20better%20performance.%20While%20attention%20natively%20supports%20global%20context%2C%20its%0Aquadratic%20complexity%20limits%20its%20applicability%20to%20tasks%20that%20benefit%20from%0Ahigh-resolution%20input.%20In%20this%20work%2C%20we%20extend%20Hyena%2C%20a%20convolution-based%0Aattention%20replacement%2C%20from%20causal%20sequences%20to%20bidirectional%20data%20and%0Atwo-dimensional%20image%20space.%20We%20scale%20Hyena%27s%20convolution%20kernels%20beyond%20the%0Afeature%20map%20size%2C%20up%20to%20191%24%5Ctimes%24191%2C%20to%20maximize%20ERF%20while%20maintaining%0Asub-quadratic%20complexity%20in%20the%20number%20of%20pixels.%20We%20integrate%20our%0Atwo-dimensional%20Hyena%2C%20HyenaPixel%2C%20and%20bidirectional%20Hyena%20into%20the%20MetaFormer%0Aframework.%20For%20image%20categorization%2C%20HyenaPixel%20and%20bidirectional%20Hyena%20achieve%0Aa%20competitive%20ImageNet-1k%20top-1%20accuracy%20of%2084.9%25%20and%2085.2%25%2C%20respectively%2C%20with%0Ano%20additional%20training%20data%2C%20while%20outperforming%20other%20convolutional%20and%0Alarge-kernel%20networks.%20Combining%20HyenaPixel%20with%20attention%20further%20improves%0Aaccuracy.%20We%20attribute%20the%20success%20of%20bidirectional%20Hyena%20to%20learning%20the%0Adata-dependent%20geometric%20arrangement%20of%20pixels%20without%20a%20fixed%20neighborhood%0Adefinition.%20Experimental%20results%20on%20downstream%20tasks%20suggest%20that%20HyenaPixel%0Awith%20large%20filters%20and%20a%20fixed%20neighborhood%20leads%20to%20better%20localization%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19305v2&entry.124074799=Read"},
{"title": "From Identifiable Causal Representations to Controllable Counterfactual\n  Generation: A Survey on Causal Generative Modeling", "author": "Aneesh Komanduri and Xintao Wu and Yongkai Wu and Feng Chen", "abstract": "  Deep generative models have shown tremendous capability in data density\nestimation and data generation from finite samples. While these models have\nshown impressive performance by learning correlations among features in the\ndata, some fundamental shortcomings are their lack of explainability, tendency\nto induce spurious correlations, and poor out-of-distribution extrapolation. To\nremedy such challenges, recent work has proposed a shift toward causal\ngenerative models. Causal models offer several beneficial properties to deep\ngenerative models, such as distribution shift robustness, fairness, and\ninterpretability. Structural causal models (SCMs) describe data-generating\nprocesses and model complex causal relationships and mechanisms among variables\nin a system. Thus, SCMs can naturally be combined with deep generative models.\nWe provide a technical survey on causal generative modeling categorized into\ncausal representation learning and controllable counterfactual generation\nmethods. We focus on fundamental theory, methodology, drawbacks, datasets, and\nmetrics. Then, we cover applications of causal generative models in fairness,\nprivacy, out-of-distribution generalization, precision medicine, and biological\nsciences. Lastly, we discuss open problems and fruitful research directions for\nfuture work in the field.\n", "link": "http://arxiv.org/abs/2310.11011v2", "date": "2024-05-23", "relevancy": 2.0726, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5404}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5046}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Identifiable%20Causal%20Representations%20to%20Controllable%20Counterfactual%0A%20%20Generation%3A%20A%20Survey%20on%20Causal%20Generative%20Modeling&body=Title%3A%20From%20Identifiable%20Causal%20Representations%20to%20Controllable%20Counterfactual%0A%20%20Generation%3A%20A%20Survey%20on%20Causal%20Generative%20Modeling%0AAuthor%3A%20Aneesh%20Komanduri%20and%20Xintao%20Wu%20and%20Yongkai%20Wu%20and%20Feng%20Chen%0AAbstract%3A%20%20%20Deep%20generative%20models%20have%20shown%20tremendous%20capability%20in%20data%20density%0Aestimation%20and%20data%20generation%20from%20finite%20samples.%20While%20these%20models%20have%0Ashown%20impressive%20performance%20by%20learning%20correlations%20among%20features%20in%20the%0Adata%2C%20some%20fundamental%20shortcomings%20are%20their%20lack%20of%20explainability%2C%20tendency%0Ato%20induce%20spurious%20correlations%2C%20and%20poor%20out-of-distribution%20extrapolation.%20To%0Aremedy%20such%20challenges%2C%20recent%20work%20has%20proposed%20a%20shift%20toward%20causal%0Agenerative%20models.%20Causal%20models%20offer%20several%20beneficial%20properties%20to%20deep%0Agenerative%20models%2C%20such%20as%20distribution%20shift%20robustness%2C%20fairness%2C%20and%0Ainterpretability.%20Structural%20causal%20models%20%28SCMs%29%20describe%20data-generating%0Aprocesses%20and%20model%20complex%20causal%20relationships%20and%20mechanisms%20among%20variables%0Ain%20a%20system.%20Thus%2C%20SCMs%20can%20naturally%20be%20combined%20with%20deep%20generative%20models.%0AWe%20provide%20a%20technical%20survey%20on%20causal%20generative%20modeling%20categorized%20into%0Acausal%20representation%20learning%20and%20controllable%20counterfactual%20generation%0Amethods.%20We%20focus%20on%20fundamental%20theory%2C%20methodology%2C%20drawbacks%2C%20datasets%2C%20and%0Ametrics.%20Then%2C%20we%20cover%20applications%20of%20causal%20generative%20models%20in%20fairness%2C%0Aprivacy%2C%20out-of-distribution%20generalization%2C%20precision%20medicine%2C%20and%20biological%0Asciences.%20Lastly%2C%20we%20discuss%20open%20problems%20and%20fruitful%20research%20directions%20for%0Afuture%20work%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Identifiable%2520Causal%2520Representations%2520to%2520Controllable%2520Counterfactual%250A%2520%2520Generation%253A%2520A%2520Survey%2520on%2520Causal%2520Generative%2520Modeling%26entry.906535625%3DAneesh%2520Komanduri%2520and%2520Xintao%2520Wu%2520and%2520Yongkai%2520Wu%2520and%2520Feng%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520have%2520shown%2520tremendous%2520capability%2520in%2520data%2520density%250Aestimation%2520and%2520data%2520generation%2520from%2520finite%2520samples.%2520While%2520these%2520models%2520have%250Ashown%2520impressive%2520performance%2520by%2520learning%2520correlations%2520among%2520features%2520in%2520the%250Adata%252C%2520some%2520fundamental%2520shortcomings%2520are%2520their%2520lack%2520of%2520explainability%252C%2520tendency%250Ato%2520induce%2520spurious%2520correlations%252C%2520and%2520poor%2520out-of-distribution%2520extrapolation.%2520To%250Aremedy%2520such%2520challenges%252C%2520recent%2520work%2520has%2520proposed%2520a%2520shift%2520toward%2520causal%250Agenerative%2520models.%2520Causal%2520models%2520offer%2520several%2520beneficial%2520properties%2520to%2520deep%250Agenerative%2520models%252C%2520such%2520as%2520distribution%2520shift%2520robustness%252C%2520fairness%252C%2520and%250Ainterpretability.%2520Structural%2520causal%2520models%2520%2528SCMs%2529%2520describe%2520data-generating%250Aprocesses%2520and%2520model%2520complex%2520causal%2520relationships%2520and%2520mechanisms%2520among%2520variables%250Ain%2520a%2520system.%2520Thus%252C%2520SCMs%2520can%2520naturally%2520be%2520combined%2520with%2520deep%2520generative%2520models.%250AWe%2520provide%2520a%2520technical%2520survey%2520on%2520causal%2520generative%2520modeling%2520categorized%2520into%250Acausal%2520representation%2520learning%2520and%2520controllable%2520counterfactual%2520generation%250Amethods.%2520We%2520focus%2520on%2520fundamental%2520theory%252C%2520methodology%252C%2520drawbacks%252C%2520datasets%252C%2520and%250Ametrics.%2520Then%252C%2520we%2520cover%2520applications%2520of%2520causal%2520generative%2520models%2520in%2520fairness%252C%250Aprivacy%252C%2520out-of-distribution%2520generalization%252C%2520precision%2520medicine%252C%2520and%2520biological%250Asciences.%2520Lastly%252C%2520we%2520discuss%2520open%2520problems%2520and%2520fruitful%2520research%2520directions%2520for%250Afuture%2520work%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Identifiable%20Causal%20Representations%20to%20Controllable%20Counterfactual%0A%20%20Generation%3A%20A%20Survey%20on%20Causal%20Generative%20Modeling&entry.906535625=Aneesh%20Komanduri%20and%20Xintao%20Wu%20and%20Yongkai%20Wu%20and%20Feng%20Chen&entry.1292438233=%20%20Deep%20generative%20models%20have%20shown%20tremendous%20capability%20in%20data%20density%0Aestimation%20and%20data%20generation%20from%20finite%20samples.%20While%20these%20models%20have%0Ashown%20impressive%20performance%20by%20learning%20correlations%20among%20features%20in%20the%0Adata%2C%20some%20fundamental%20shortcomings%20are%20their%20lack%20of%20explainability%2C%20tendency%0Ato%20induce%20spurious%20correlations%2C%20and%20poor%20out-of-distribution%20extrapolation.%20To%0Aremedy%20such%20challenges%2C%20recent%20work%20has%20proposed%20a%20shift%20toward%20causal%0Agenerative%20models.%20Causal%20models%20offer%20several%20beneficial%20properties%20to%20deep%0Agenerative%20models%2C%20such%20as%20distribution%20shift%20robustness%2C%20fairness%2C%20and%0Ainterpretability.%20Structural%20causal%20models%20%28SCMs%29%20describe%20data-generating%0Aprocesses%20and%20model%20complex%20causal%20relationships%20and%20mechanisms%20among%20variables%0Ain%20a%20system.%20Thus%2C%20SCMs%20can%20naturally%20be%20combined%20with%20deep%20generative%20models.%0AWe%20provide%20a%20technical%20survey%20on%20causal%20generative%20modeling%20categorized%20into%0Acausal%20representation%20learning%20and%20controllable%20counterfactual%20generation%0Amethods.%20We%20focus%20on%20fundamental%20theory%2C%20methodology%2C%20drawbacks%2C%20datasets%2C%20and%0Ametrics.%20Then%2C%20we%20cover%20applications%20of%20causal%20generative%20models%20in%20fairness%2C%0Aprivacy%2C%20out-of-distribution%20generalization%2C%20precision%20medicine%2C%20and%20biological%0Asciences.%20Lastly%2C%20we%20discuss%20open%20problems%20and%20fruitful%20research%20directions%20for%0Afuture%20work%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11011v2&entry.124074799=Read"},
{"title": "A Matrix-based Distance of Pythagorean Fuzzy Set and its Application in\n  Medical Diagnosis", "author": "Yuanpeng He and Lijian Li and Tianxiang Zhan", "abstract": "  The pythagorean fuzzy set (PFS) which is developed based on intuitionistic\nfuzzy set, is more efficient in elaborating and disposing uncertainties in\nindeterminate situations, which is a very reason of that PFS is applied in\nvarious kinds of fields. How to measure the distance between two pythagorean\nfuzzy sets is still an open issue. Mnay kinds of methods have been proposed to\npresent the of the question in former reaserches. However, not all of existing\nmethods can accurately manifest differences among pythagorean fuzzy sets and\nsatisfy the property of similarity. And some other kinds of methods neglect the\nrelationship among three variables of pythagorean fuzzy set. To addrees the\nproplem, a new method of measuring distance is proposed which meets the\nrequirements of axiom of distance measurement and is able to indicate the\ndegree of distinction of PFSs well. Then some numerical examples are offered to\nto verify that the method of measuring distances can avoid the situation that\nsome counter? intuitive and irrational results are produced and is more\neffective, reasonable and advanced than other similar methods. Besides, the\nproposed method of measuring distances between PFSs is applied in a real\nenvironment of application which is the medical diagnosis and is compared with\nother previous methods to demonstrate its superiority and efficiency. And the\nfeasibility of the proposed method in handling uncertainties in practice is\nalso proved at the same time.\n", "link": "http://arxiv.org/abs/2102.01538v2", "date": "2024-05-23", "relevancy": 1.1983, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4141}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3953}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Matrix-based%20Distance%20of%20Pythagorean%20Fuzzy%20Set%20and%20its%20Application%20in%0A%20%20Medical%20Diagnosis&body=Title%3A%20A%20Matrix-based%20Distance%20of%20Pythagorean%20Fuzzy%20Set%20and%20its%20Application%20in%0A%20%20Medical%20Diagnosis%0AAuthor%3A%20Yuanpeng%20He%20and%20Lijian%20Li%20and%20Tianxiang%20Zhan%0AAbstract%3A%20%20%20The%20pythagorean%20fuzzy%20set%20%28PFS%29%20which%20is%20developed%20based%20on%20intuitionistic%0Afuzzy%20set%2C%20is%20more%20efficient%20in%20elaborating%20and%20disposing%20uncertainties%20in%0Aindeterminate%20situations%2C%20which%20is%20a%20very%20reason%20of%20that%20PFS%20is%20applied%20in%0Avarious%20kinds%20of%20fields.%20How%20to%20measure%20the%20distance%20between%20two%20pythagorean%0Afuzzy%20sets%20is%20still%20an%20open%20issue.%20Mnay%20kinds%20of%20methods%20have%20been%20proposed%20to%0Apresent%20the%20of%20the%20question%20in%20former%20reaserches.%20However%2C%20not%20all%20of%20existing%0Amethods%20can%20accurately%20manifest%20differences%20among%20pythagorean%20fuzzy%20sets%20and%0Asatisfy%20the%20property%20of%20similarity.%20And%20some%20other%20kinds%20of%20methods%20neglect%20the%0Arelationship%20among%20three%20variables%20of%20pythagorean%20fuzzy%20set.%20To%20addrees%20the%0Aproplem%2C%20a%20new%20method%20of%20measuring%20distance%20is%20proposed%20which%20meets%20the%0Arequirements%20of%20axiom%20of%20distance%20measurement%20and%20is%20able%20to%20indicate%20the%0Adegree%20of%20distinction%20of%20PFSs%20well.%20Then%20some%20numerical%20examples%20are%20offered%20to%0Ato%20verify%20that%20the%20method%20of%20measuring%20distances%20can%20avoid%20the%20situation%20that%0Asome%20counter%3F%20intuitive%20and%20irrational%20results%20are%20produced%20and%20is%20more%0Aeffective%2C%20reasonable%20and%20advanced%20than%20other%20similar%20methods.%20Besides%2C%20the%0Aproposed%20method%20of%20measuring%20distances%20between%20PFSs%20is%20applied%20in%20a%20real%0Aenvironment%20of%20application%20which%20is%20the%20medical%20diagnosis%20and%20is%20compared%20with%0Aother%20previous%20methods%20to%20demonstrate%20its%20superiority%20and%20efficiency.%20And%20the%0Afeasibility%20of%20the%20proposed%20method%20in%20handling%20uncertainties%20in%20practice%20is%0Aalso%20proved%20at%20the%20same%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2102.01538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Matrix-based%2520Distance%2520of%2520Pythagorean%2520Fuzzy%2520Set%2520and%2520its%2520Application%2520in%250A%2520%2520Medical%2520Diagnosis%26entry.906535625%3DYuanpeng%2520He%2520and%2520Lijian%2520Li%2520and%2520Tianxiang%2520Zhan%26entry.1292438233%3D%2520%2520The%2520pythagorean%2520fuzzy%2520set%2520%2528PFS%2529%2520which%2520is%2520developed%2520based%2520on%2520intuitionistic%250Afuzzy%2520set%252C%2520is%2520more%2520efficient%2520in%2520elaborating%2520and%2520disposing%2520uncertainties%2520in%250Aindeterminate%2520situations%252C%2520which%2520is%2520a%2520very%2520reason%2520of%2520that%2520PFS%2520is%2520applied%2520in%250Avarious%2520kinds%2520of%2520fields.%2520How%2520to%2520measure%2520the%2520distance%2520between%2520two%2520pythagorean%250Afuzzy%2520sets%2520is%2520still%2520an%2520open%2520issue.%2520Mnay%2520kinds%2520of%2520methods%2520have%2520been%2520proposed%2520to%250Apresent%2520the%2520of%2520the%2520question%2520in%2520former%2520reaserches.%2520However%252C%2520not%2520all%2520of%2520existing%250Amethods%2520can%2520accurately%2520manifest%2520differences%2520among%2520pythagorean%2520fuzzy%2520sets%2520and%250Asatisfy%2520the%2520property%2520of%2520similarity.%2520And%2520some%2520other%2520kinds%2520of%2520methods%2520neglect%2520the%250Arelationship%2520among%2520three%2520variables%2520of%2520pythagorean%2520fuzzy%2520set.%2520To%2520addrees%2520the%250Aproplem%252C%2520a%2520new%2520method%2520of%2520measuring%2520distance%2520is%2520proposed%2520which%2520meets%2520the%250Arequirements%2520of%2520axiom%2520of%2520distance%2520measurement%2520and%2520is%2520able%2520to%2520indicate%2520the%250Adegree%2520of%2520distinction%2520of%2520PFSs%2520well.%2520Then%2520some%2520numerical%2520examples%2520are%2520offered%2520to%250Ato%2520verify%2520that%2520the%2520method%2520of%2520measuring%2520distances%2520can%2520avoid%2520the%2520situation%2520that%250Asome%2520counter%253F%2520intuitive%2520and%2520irrational%2520results%2520are%2520produced%2520and%2520is%2520more%250Aeffective%252C%2520reasonable%2520and%2520advanced%2520than%2520other%2520similar%2520methods.%2520Besides%252C%2520the%250Aproposed%2520method%2520of%2520measuring%2520distances%2520between%2520PFSs%2520is%2520applied%2520in%2520a%2520real%250Aenvironment%2520of%2520application%2520which%2520is%2520the%2520medical%2520diagnosis%2520and%2520is%2520compared%2520with%250Aother%2520previous%2520methods%2520to%2520demonstrate%2520its%2520superiority%2520and%2520efficiency.%2520And%2520the%250Afeasibility%2520of%2520the%2520proposed%2520method%2520in%2520handling%2520uncertainties%2520in%2520practice%2520is%250Aalso%2520proved%2520at%2520the%2520same%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2102.01538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Matrix-based%20Distance%20of%20Pythagorean%20Fuzzy%20Set%20and%20its%20Application%20in%0A%20%20Medical%20Diagnosis&entry.906535625=Yuanpeng%20He%20and%20Lijian%20Li%20and%20Tianxiang%20Zhan&entry.1292438233=%20%20The%20pythagorean%20fuzzy%20set%20%28PFS%29%20which%20is%20developed%20based%20on%20intuitionistic%0Afuzzy%20set%2C%20is%20more%20efficient%20in%20elaborating%20and%20disposing%20uncertainties%20in%0Aindeterminate%20situations%2C%20which%20is%20a%20very%20reason%20of%20that%20PFS%20is%20applied%20in%0Avarious%20kinds%20of%20fields.%20How%20to%20measure%20the%20distance%20between%20two%20pythagorean%0Afuzzy%20sets%20is%20still%20an%20open%20issue.%20Mnay%20kinds%20of%20methods%20have%20been%20proposed%20to%0Apresent%20the%20of%20the%20question%20in%20former%20reaserches.%20However%2C%20not%20all%20of%20existing%0Amethods%20can%20accurately%20manifest%20differences%20among%20pythagorean%20fuzzy%20sets%20and%0Asatisfy%20the%20property%20of%20similarity.%20And%20some%20other%20kinds%20of%20methods%20neglect%20the%0Arelationship%20among%20three%20variables%20of%20pythagorean%20fuzzy%20set.%20To%20addrees%20the%0Aproplem%2C%20a%20new%20method%20of%20measuring%20distance%20is%20proposed%20which%20meets%20the%0Arequirements%20of%20axiom%20of%20distance%20measurement%20and%20is%20able%20to%20indicate%20the%0Adegree%20of%20distinction%20of%20PFSs%20well.%20Then%20some%20numerical%20examples%20are%20offered%20to%0Ato%20verify%20that%20the%20method%20of%20measuring%20distances%20can%20avoid%20the%20situation%20that%0Asome%20counter%3F%20intuitive%20and%20irrational%20results%20are%20produced%20and%20is%20more%0Aeffective%2C%20reasonable%20and%20advanced%20than%20other%20similar%20methods.%20Besides%2C%20the%0Aproposed%20method%20of%20measuring%20distances%20between%20PFSs%20is%20applied%20in%20a%20real%0Aenvironment%20of%20application%20which%20is%20the%20medical%20diagnosis%20and%20is%20compared%20with%0Aother%20previous%20methods%20to%20demonstrate%20its%20superiority%20and%20efficiency.%20And%20the%0Afeasibility%20of%20the%20proposed%20method%20in%20handling%20uncertainties%20in%20practice%20is%0Aalso%20proved%20at%20the%20same%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2102.01538v2&entry.124074799=Read"},
{"title": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based\n  LLMs", "author": "Jaewoo Yang and Hayun Kim and Younghoon Kim", "abstract": "  Modern large language models (LLMs) have established state-of-the-art\nperformance through architectural improvements, but still require significant\ncomputational cost for inference. In an effort to reduce the inference cost,\npost-training quantization (PTQ) has become a popular approach, quantizing\nweights and activations to lower precision, such as INT8. In this paper, we\nreveal the challenges of activation quantization in GLU variants, which are\nwidely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\nThe problem is that severe local quantization errors, caused by excessive\nmagnitudes of activation in GLU variants, significantly degrade the performance\nof the quantized LLM. We denote these activations as activation spikes. Our\nfurther observations provide a systematic pattern of activation spikes: 1) The\nactivation spikes occur in the FFN of specific layers, particularly in the\nearly and late layers, 2) The activation spikes are dedicated to a couple of\ntokens, rather than being shared across a sequence. Based on our observations,\nwe propose two empirical methods, Quantization-free Module (QFeM) and\nQuantization-free Prefix (QFeP), to isolate the activation spikes during\nquantization. Our extensive experiments validate the effectiveness of the\nproposed methods for the activation quantization, especially with\ncoarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3,\nMistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the\ncurrent alleviation techniques (e.g., SmoothQuant) that fail to control the\nactivation spikes. Code is available at\nhttps://github.com/onnoo/activation-spikes.\n", "link": "http://arxiv.org/abs/2405.14428v1", "date": "2024-05-23", "relevancy": 1.8814, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.486}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4639}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Quantization%20Errors%20Due%20to%20Activation%20Spikes%20in%20GLU-Based%0A%20%20LLMs&body=Title%3A%20Mitigating%20Quantization%20Errors%20Due%20to%20Activation%20Spikes%20in%20GLU-Based%0A%20%20LLMs%0AAuthor%3A%20Jaewoo%20Yang%20and%20Hayun%20Kim%20and%20Younghoon%20Kim%0AAbstract%3A%20%20%20Modern%20large%20language%20models%20%28LLMs%29%20have%20established%20state-of-the-art%0Aperformance%20through%20architectural%20improvements%2C%20but%20still%20require%20significant%0Acomputational%20cost%20for%20inference.%20In%20an%20effort%20to%20reduce%20the%20inference%20cost%2C%0Apost-training%20quantization%20%28PTQ%29%20has%20become%20a%20popular%20approach%2C%20quantizing%0Aweights%20and%20activations%20to%20lower%20precision%2C%20such%20as%20INT8.%20In%20this%20paper%2C%20we%0Areveal%20the%20challenges%20of%20activation%20quantization%20in%20GLU%20variants%2C%20which%20are%0Awidely%20used%20in%20feed-forward%20network%20%28FFN%29%20of%20modern%20LLMs%2C%20such%20as%20LLaMA%20family.%0AThe%20problem%20is%20that%20severe%20local%20quantization%20errors%2C%20caused%20by%20excessive%0Amagnitudes%20of%20activation%20in%20GLU%20variants%2C%20significantly%20degrade%20the%20performance%0Aof%20the%20quantized%20LLM.%20We%20denote%20these%20activations%20as%20activation%20spikes.%20Our%0Afurther%20observations%20provide%20a%20systematic%20pattern%20of%20activation%20spikes%3A%201%29%20The%0Aactivation%20spikes%20occur%20in%20the%20FFN%20of%20specific%20layers%2C%20particularly%20in%20the%0Aearly%20and%20late%20layers%2C%202%29%20The%20activation%20spikes%20are%20dedicated%20to%20a%20couple%20of%0Atokens%2C%20rather%20than%20being%20shared%20across%20a%20sequence.%20Based%20on%20our%20observations%2C%0Awe%20propose%20two%20empirical%20methods%2C%20Quantization-free%20Module%20%28QFeM%29%20and%0AQuantization-free%20Prefix%20%28QFeP%29%2C%20to%20isolate%20the%20activation%20spikes%20during%0Aquantization.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%20of%20the%0Aproposed%20methods%20for%20the%20activation%20quantization%2C%20especially%20with%0Acoarse-grained%20scheme%2C%20of%20latest%20LLMs%20with%20GLU%20variants%2C%20including%20LLaMA-2/3%2C%0AMistral%2C%20Mixtral%2C%20SOLAR%2C%20and%20Gemma.%20In%20particular%2C%20our%20methods%20enhance%20the%0Acurrent%20alleviation%20techniques%20%28e.g.%2C%20SmoothQuant%29%20that%20fail%20to%20control%20the%0Aactivation%20spikes.%20Code%20is%20available%20at%0Ahttps%3A//github.com/onnoo/activation-spikes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Quantization%2520Errors%2520Due%2520to%2520Activation%2520Spikes%2520in%2520GLU-Based%250A%2520%2520LLMs%26entry.906535625%3DJaewoo%2520Yang%2520and%2520Hayun%2520Kim%2520and%2520Younghoon%2520Kim%26entry.1292438233%3D%2520%2520Modern%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520established%2520state-of-the-art%250Aperformance%2520through%2520architectural%2520improvements%252C%2520but%2520still%2520require%2520significant%250Acomputational%2520cost%2520for%2520inference.%2520In%2520an%2520effort%2520to%2520reduce%2520the%2520inference%2520cost%252C%250Apost-training%2520quantization%2520%2528PTQ%2529%2520has%2520become%2520a%2520popular%2520approach%252C%2520quantizing%250Aweights%2520and%2520activations%2520to%2520lower%2520precision%252C%2520such%2520as%2520INT8.%2520In%2520this%2520paper%252C%2520we%250Areveal%2520the%2520challenges%2520of%2520activation%2520quantization%2520in%2520GLU%2520variants%252C%2520which%2520are%250Awidely%2520used%2520in%2520feed-forward%2520network%2520%2528FFN%2529%2520of%2520modern%2520LLMs%252C%2520such%2520as%2520LLaMA%2520family.%250AThe%2520problem%2520is%2520that%2520severe%2520local%2520quantization%2520errors%252C%2520caused%2520by%2520excessive%250Amagnitudes%2520of%2520activation%2520in%2520GLU%2520variants%252C%2520significantly%2520degrade%2520the%2520performance%250Aof%2520the%2520quantized%2520LLM.%2520We%2520denote%2520these%2520activations%2520as%2520activation%2520spikes.%2520Our%250Afurther%2520observations%2520provide%2520a%2520systematic%2520pattern%2520of%2520activation%2520spikes%253A%25201%2529%2520The%250Aactivation%2520spikes%2520occur%2520in%2520the%2520FFN%2520of%2520specific%2520layers%252C%2520particularly%2520in%2520the%250Aearly%2520and%2520late%2520layers%252C%25202%2529%2520The%2520activation%2520spikes%2520are%2520dedicated%2520to%2520a%2520couple%2520of%250Atokens%252C%2520rather%2520than%2520being%2520shared%2520across%2520a%2520sequence.%2520Based%2520on%2520our%2520observations%252C%250Awe%2520propose%2520two%2520empirical%2520methods%252C%2520Quantization-free%2520Module%2520%2528QFeM%2529%2520and%250AQuantization-free%2520Prefix%2520%2528QFeP%2529%252C%2520to%2520isolate%2520the%2520activation%2520spikes%2520during%250Aquantization.%2520Our%2520extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520methods%2520for%2520the%2520activation%2520quantization%252C%2520especially%2520with%250Acoarse-grained%2520scheme%252C%2520of%2520latest%2520LLMs%2520with%2520GLU%2520variants%252C%2520including%2520LLaMA-2/3%252C%250AMistral%252C%2520Mixtral%252C%2520SOLAR%252C%2520and%2520Gemma.%2520In%2520particular%252C%2520our%2520methods%2520enhance%2520the%250Acurrent%2520alleviation%2520techniques%2520%2528e.g.%252C%2520SmoothQuant%2529%2520that%2520fail%2520to%2520control%2520the%250Aactivation%2520spikes.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/onnoo/activation-spikes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Quantization%20Errors%20Due%20to%20Activation%20Spikes%20in%20GLU-Based%0A%20%20LLMs&entry.906535625=Jaewoo%20Yang%20and%20Hayun%20Kim%20and%20Younghoon%20Kim&entry.1292438233=%20%20Modern%20large%20language%20models%20%28LLMs%29%20have%20established%20state-of-the-art%0Aperformance%20through%20architectural%20improvements%2C%20but%20still%20require%20significant%0Acomputational%20cost%20for%20inference.%20In%20an%20effort%20to%20reduce%20the%20inference%20cost%2C%0Apost-training%20quantization%20%28PTQ%29%20has%20become%20a%20popular%20approach%2C%20quantizing%0Aweights%20and%20activations%20to%20lower%20precision%2C%20such%20as%20INT8.%20In%20this%20paper%2C%20we%0Areveal%20the%20challenges%20of%20activation%20quantization%20in%20GLU%20variants%2C%20which%20are%0Awidely%20used%20in%20feed-forward%20network%20%28FFN%29%20of%20modern%20LLMs%2C%20such%20as%20LLaMA%20family.%0AThe%20problem%20is%20that%20severe%20local%20quantization%20errors%2C%20caused%20by%20excessive%0Amagnitudes%20of%20activation%20in%20GLU%20variants%2C%20significantly%20degrade%20the%20performance%0Aof%20the%20quantized%20LLM.%20We%20denote%20these%20activations%20as%20activation%20spikes.%20Our%0Afurther%20observations%20provide%20a%20systematic%20pattern%20of%20activation%20spikes%3A%201%29%20The%0Aactivation%20spikes%20occur%20in%20the%20FFN%20of%20specific%20layers%2C%20particularly%20in%20the%0Aearly%20and%20late%20layers%2C%202%29%20The%20activation%20spikes%20are%20dedicated%20to%20a%20couple%20of%0Atokens%2C%20rather%20than%20being%20shared%20across%20a%20sequence.%20Based%20on%20our%20observations%2C%0Awe%20propose%20two%20empirical%20methods%2C%20Quantization-free%20Module%20%28QFeM%29%20and%0AQuantization-free%20Prefix%20%28QFeP%29%2C%20to%20isolate%20the%20activation%20spikes%20during%0Aquantization.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%20of%20the%0Aproposed%20methods%20for%20the%20activation%20quantization%2C%20especially%20with%0Acoarse-grained%20scheme%2C%20of%20latest%20LLMs%20with%20GLU%20variants%2C%20including%20LLaMA-2/3%2C%0AMistral%2C%20Mixtral%2C%20SOLAR%2C%20and%20Gemma.%20In%20particular%2C%20our%20methods%20enhance%20the%0Acurrent%20alleviation%20techniques%20%28e.g.%2C%20SmoothQuant%29%20that%20fail%20to%20control%20the%0Aactivation%20spikes.%20Code%20is%20available%20at%0Ahttps%3A//github.com/onnoo/activation-spikes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14428v1&entry.124074799=Read"},
{"title": "Causal Discovery under Latent Class Confounding", "author": "Bijan Mazaheri and Spencer Gordon and Yuval Rabani and Leonard Schulman", "abstract": "  An acyclic causal structure can be described using a directed acyclic graph\n(DAG) with arrows indicating causation. The task of learning this structure\nfrom data is known as \"causal discovery.\" Diverse populations or changing\nenvironments can sometimes give rise to heterogeneous data. This heterogeneity\ncan be thought of as a mixture model with multiple \"sources,\" each exerting\ntheir own distinct signature on the observed variables. From this perspective,\nthe source is a latent common cause for every observed variable. While some\nmethods for causal discovery are able to work around unobserved confounding in\nspecial cases, the only known ways to deal with a global confounder (such as a\nlatent class) involve parametric assumptions. Focusing on discrete observables,\nwe demonstrate that globally confounded causal structures can still be\nidentifiable without parametric assumptions, so long as the number of latent\nclasses remains small relative to the size and sparsity of the underlying DAG.\n", "link": "http://arxiv.org/abs/2311.07454v4", "date": "2024-05-23", "relevancy": 1.6663, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4315}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4115}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Discovery%20under%20Latent%20Class%20Confounding&body=Title%3A%20Causal%20Discovery%20under%20Latent%20Class%20Confounding%0AAuthor%3A%20Bijan%20Mazaheri%20and%20Spencer%20Gordon%20and%20Yuval%20Rabani%20and%20Leonard%20Schulman%0AAbstract%3A%20%20%20An%20acyclic%20causal%20structure%20can%20be%20described%20using%20a%20directed%20acyclic%20graph%0A%28DAG%29%20with%20arrows%20indicating%20causation.%20The%20task%20of%20learning%20this%20structure%0Afrom%20data%20is%20known%20as%20%22causal%20discovery.%22%20Diverse%20populations%20or%20changing%0Aenvironments%20can%20sometimes%20give%20rise%20to%20heterogeneous%20data.%20This%20heterogeneity%0Acan%20be%20thought%20of%20as%20a%20mixture%20model%20with%20multiple%20%22sources%2C%22%20each%20exerting%0Atheir%20own%20distinct%20signature%20on%20the%20observed%20variables.%20From%20this%20perspective%2C%0Athe%20source%20is%20a%20latent%20common%20cause%20for%20every%20observed%20variable.%20While%20some%0Amethods%20for%20causal%20discovery%20are%20able%20to%20work%20around%20unobserved%20confounding%20in%0Aspecial%20cases%2C%20the%20only%20known%20ways%20to%20deal%20with%20a%20global%20confounder%20%28such%20as%20a%0Alatent%20class%29%20involve%20parametric%20assumptions.%20Focusing%20on%20discrete%20observables%2C%0Awe%20demonstrate%20that%20globally%20confounded%20causal%20structures%20can%20still%20be%0Aidentifiable%20without%20parametric%20assumptions%2C%20so%20long%20as%20the%20number%20of%20latent%0Aclasses%20remains%20small%20relative%20to%20the%20size%20and%20sparsity%20of%20the%20underlying%20DAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07454v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Discovery%2520under%2520Latent%2520Class%2520Confounding%26entry.906535625%3DBijan%2520Mazaheri%2520and%2520Spencer%2520Gordon%2520and%2520Yuval%2520Rabani%2520and%2520Leonard%2520Schulman%26entry.1292438233%3D%2520%2520An%2520acyclic%2520causal%2520structure%2520can%2520be%2520described%2520using%2520a%2520directed%2520acyclic%2520graph%250A%2528DAG%2529%2520with%2520arrows%2520indicating%2520causation.%2520The%2520task%2520of%2520learning%2520this%2520structure%250Afrom%2520data%2520is%2520known%2520as%2520%2522causal%2520discovery.%2522%2520Diverse%2520populations%2520or%2520changing%250Aenvironments%2520can%2520sometimes%2520give%2520rise%2520to%2520heterogeneous%2520data.%2520This%2520heterogeneity%250Acan%2520be%2520thought%2520of%2520as%2520a%2520mixture%2520model%2520with%2520multiple%2520%2522sources%252C%2522%2520each%2520exerting%250Atheir%2520own%2520distinct%2520signature%2520on%2520the%2520observed%2520variables.%2520From%2520this%2520perspective%252C%250Athe%2520source%2520is%2520a%2520latent%2520common%2520cause%2520for%2520every%2520observed%2520variable.%2520While%2520some%250Amethods%2520for%2520causal%2520discovery%2520are%2520able%2520to%2520work%2520around%2520unobserved%2520confounding%2520in%250Aspecial%2520cases%252C%2520the%2520only%2520known%2520ways%2520to%2520deal%2520with%2520a%2520global%2520confounder%2520%2528such%2520as%2520a%250Alatent%2520class%2529%2520involve%2520parametric%2520assumptions.%2520Focusing%2520on%2520discrete%2520observables%252C%250Awe%2520demonstrate%2520that%2520globally%2520confounded%2520causal%2520structures%2520can%2520still%2520be%250Aidentifiable%2520without%2520parametric%2520assumptions%252C%2520so%2520long%2520as%2520the%2520number%2520of%2520latent%250Aclasses%2520remains%2520small%2520relative%2520to%2520the%2520size%2520and%2520sparsity%2520of%2520the%2520underlying%2520DAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07454v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Discovery%20under%20Latent%20Class%20Confounding&entry.906535625=Bijan%20Mazaheri%20and%20Spencer%20Gordon%20and%20Yuval%20Rabani%20and%20Leonard%20Schulman&entry.1292438233=%20%20An%20acyclic%20causal%20structure%20can%20be%20described%20using%20a%20directed%20acyclic%20graph%0A%28DAG%29%20with%20arrows%20indicating%20causation.%20The%20task%20of%20learning%20this%20structure%0Afrom%20data%20is%20known%20as%20%22causal%20discovery.%22%20Diverse%20populations%20or%20changing%0Aenvironments%20can%20sometimes%20give%20rise%20to%20heterogeneous%20data.%20This%20heterogeneity%0Acan%20be%20thought%20of%20as%20a%20mixture%20model%20with%20multiple%20%22sources%2C%22%20each%20exerting%0Atheir%20own%20distinct%20signature%20on%20the%20observed%20variables.%20From%20this%20perspective%2C%0Athe%20source%20is%20a%20latent%20common%20cause%20for%20every%20observed%20variable.%20While%20some%0Amethods%20for%20causal%20discovery%20are%20able%20to%20work%20around%20unobserved%20confounding%20in%0Aspecial%20cases%2C%20the%20only%20known%20ways%20to%20deal%20with%20a%20global%20confounder%20%28such%20as%20a%0Alatent%20class%29%20involve%20parametric%20assumptions.%20Focusing%20on%20discrete%20observables%2C%0Awe%20demonstrate%20that%20globally%20confounded%20causal%20structures%20can%20still%20be%0Aidentifiable%20without%20parametric%20assumptions%2C%20so%20long%20as%20the%20number%20of%20latent%0Aclasses%20remains%20small%20relative%20to%20the%20size%20and%20sparsity%20of%20the%20underlying%20DAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07454v4&entry.124074799=Read"},
{"title": "Soft insoles for estimating 3D ground reaction forces using 3D printed\n  foam-like sensors", "author": "Nick Willemstein and Saivimal Sridar and Herman van der Kooij and Ali Sadeghi", "abstract": "  Sensorized insoles provide a tool for gait studies and health monitoring\nduring daily life. For users to accept such insoles they need to be comfortable\nand lightweight. Previous work has already demonstrated that estimation of\nground reaction forces (GRFs) is possible with insoles. However, these are\noften assemblies of commercial components restricting design freedom and\ncustomization. Within this work, we investigate using four 3D-printed soft\nfoam-like sensors to sensorize an insole. These sensors were combined with\nsystem identification of Hammerstein-Wiener models to estimate the 3D GRFs,\nwhich were compared to values from an instrumented treadmill as the golden\nstandard. It was observed that the four sensors behaved in line with the\nexpected change in pressure distribution during the gait cycle. In addition,\nthe identified (personalized) Hammerstein-Wiener models showed the best\nestimation performance (on average RMS error 9.3%, R^2=0.85 and mean absolute\nerror (MAE) 7%) of the vertical, mediolateral, and anteroposterior GRFs.\nThereby showing that these sensors can estimate the resulting 3D force\nreasonably well. These results for nine participants were comparable to or\noutperformed other works that used commercial FSRs with machine learning. The\nidentified models did decrease in estimation performance over time but stayed\non average 11.35% RMS and 8.6% MAE after a week with the Hammerstein-Wiener\nmodel seeming consistent between days two and seven. These results show promise\nfor using 3D-printed soft piezoresistive foam-like sensors with system\nidentification to be a viable approach for applications that require softness,\nlightweight, and customization such as wearable (force) sensors.\n", "link": "http://arxiv.org/abs/2303.04719v2", "date": "2024-05-23", "relevancy": 1.7773, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4329}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20insoles%20for%20estimating%203D%20ground%20reaction%20forces%20using%203D%20printed%0A%20%20foam-like%20sensors&body=Title%3A%20Soft%20insoles%20for%20estimating%203D%20ground%20reaction%20forces%20using%203D%20printed%0A%20%20foam-like%20sensors%0AAuthor%3A%20Nick%20Willemstein%20and%20Saivimal%20Sridar%20and%20Herman%20van%20der%20Kooij%20and%20Ali%20Sadeghi%0AAbstract%3A%20%20%20Sensorized%20insoles%20provide%20a%20tool%20for%20gait%20studies%20and%20health%20monitoring%0Aduring%20daily%20life.%20For%20users%20to%20accept%20such%20insoles%20they%20need%20to%20be%20comfortable%0Aand%20lightweight.%20Previous%20work%20has%20already%20demonstrated%20that%20estimation%20of%0Aground%20reaction%20forces%20%28GRFs%29%20is%20possible%20with%20insoles.%20However%2C%20these%20are%0Aoften%20assemblies%20of%20commercial%20components%20restricting%20design%20freedom%20and%0Acustomization.%20Within%20this%20work%2C%20we%20investigate%20using%20four%203D-printed%20soft%0Afoam-like%20sensors%20to%20sensorize%20an%20insole.%20These%20sensors%20were%20combined%20with%0Asystem%20identification%20of%20Hammerstein-Wiener%20models%20to%20estimate%20the%203D%20GRFs%2C%0Awhich%20were%20compared%20to%20values%20from%20an%20instrumented%20treadmill%20as%20the%20golden%0Astandard.%20It%20was%20observed%20that%20the%20four%20sensors%20behaved%20in%20line%20with%20the%0Aexpected%20change%20in%20pressure%20distribution%20during%20the%20gait%20cycle.%20In%20addition%2C%0Athe%20identified%20%28personalized%29%20Hammerstein-Wiener%20models%20showed%20the%20best%0Aestimation%20performance%20%28on%20average%20RMS%20error%209.3%25%2C%20R%5E2%3D0.85%20and%20mean%20absolute%0Aerror%20%28MAE%29%207%25%29%20of%20the%20vertical%2C%20mediolateral%2C%20and%20anteroposterior%20GRFs.%0AThereby%20showing%20that%20these%20sensors%20can%20estimate%20the%20resulting%203D%20force%0Areasonably%20well.%20These%20results%20for%20nine%20participants%20were%20comparable%20to%20or%0Aoutperformed%20other%20works%20that%20used%20commercial%20FSRs%20with%20machine%20learning.%20The%0Aidentified%20models%20did%20decrease%20in%20estimation%20performance%20over%20time%20but%20stayed%0Aon%20average%2011.35%25%20RMS%20and%208.6%25%20MAE%20after%20a%20week%20with%20the%20Hammerstein-Wiener%0Amodel%20seeming%20consistent%20between%20days%20two%20and%20seven.%20These%20results%20show%20promise%0Afor%20using%203D-printed%20soft%20piezoresistive%20foam-like%20sensors%20with%20system%0Aidentification%20to%20be%20a%20viable%20approach%20for%20applications%20that%20require%20softness%2C%0Alightweight%2C%20and%20customization%20such%20as%20wearable%20%28force%29%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.04719v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520insoles%2520for%2520estimating%25203D%2520ground%2520reaction%2520forces%2520using%25203D%2520printed%250A%2520%2520foam-like%2520sensors%26entry.906535625%3DNick%2520Willemstein%2520and%2520Saivimal%2520Sridar%2520and%2520Herman%2520van%2520der%2520Kooij%2520and%2520Ali%2520Sadeghi%26entry.1292438233%3D%2520%2520Sensorized%2520insoles%2520provide%2520a%2520tool%2520for%2520gait%2520studies%2520and%2520health%2520monitoring%250Aduring%2520daily%2520life.%2520For%2520users%2520to%2520accept%2520such%2520insoles%2520they%2520need%2520to%2520be%2520comfortable%250Aand%2520lightweight.%2520Previous%2520work%2520has%2520already%2520demonstrated%2520that%2520estimation%2520of%250Aground%2520reaction%2520forces%2520%2528GRFs%2529%2520is%2520possible%2520with%2520insoles.%2520However%252C%2520these%2520are%250Aoften%2520assemblies%2520of%2520commercial%2520components%2520restricting%2520design%2520freedom%2520and%250Acustomization.%2520Within%2520this%2520work%252C%2520we%2520investigate%2520using%2520four%25203D-printed%2520soft%250Afoam-like%2520sensors%2520to%2520sensorize%2520an%2520insole.%2520These%2520sensors%2520were%2520combined%2520with%250Asystem%2520identification%2520of%2520Hammerstein-Wiener%2520models%2520to%2520estimate%2520the%25203D%2520GRFs%252C%250Awhich%2520were%2520compared%2520to%2520values%2520from%2520an%2520instrumented%2520treadmill%2520as%2520the%2520golden%250Astandard.%2520It%2520was%2520observed%2520that%2520the%2520four%2520sensors%2520behaved%2520in%2520line%2520with%2520the%250Aexpected%2520change%2520in%2520pressure%2520distribution%2520during%2520the%2520gait%2520cycle.%2520In%2520addition%252C%250Athe%2520identified%2520%2528personalized%2529%2520Hammerstein-Wiener%2520models%2520showed%2520the%2520best%250Aestimation%2520performance%2520%2528on%2520average%2520RMS%2520error%25209.3%2525%252C%2520R%255E2%253D0.85%2520and%2520mean%2520absolute%250Aerror%2520%2528MAE%2529%25207%2525%2529%2520of%2520the%2520vertical%252C%2520mediolateral%252C%2520and%2520anteroposterior%2520GRFs.%250AThereby%2520showing%2520that%2520these%2520sensors%2520can%2520estimate%2520the%2520resulting%25203D%2520force%250Areasonably%2520well.%2520These%2520results%2520for%2520nine%2520participants%2520were%2520comparable%2520to%2520or%250Aoutperformed%2520other%2520works%2520that%2520used%2520commercial%2520FSRs%2520with%2520machine%2520learning.%2520The%250Aidentified%2520models%2520did%2520decrease%2520in%2520estimation%2520performance%2520over%2520time%2520but%2520stayed%250Aon%2520average%252011.35%2525%2520RMS%2520and%25208.6%2525%2520MAE%2520after%2520a%2520week%2520with%2520the%2520Hammerstein-Wiener%250Amodel%2520seeming%2520consistent%2520between%2520days%2520two%2520and%2520seven.%2520These%2520results%2520show%2520promise%250Afor%2520using%25203D-printed%2520soft%2520piezoresistive%2520foam-like%2520sensors%2520with%2520system%250Aidentification%2520to%2520be%2520a%2520viable%2520approach%2520for%2520applications%2520that%2520require%2520softness%252C%250Alightweight%252C%2520and%2520customization%2520such%2520as%2520wearable%2520%2528force%2529%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.04719v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20insoles%20for%20estimating%203D%20ground%20reaction%20forces%20using%203D%20printed%0A%20%20foam-like%20sensors&entry.906535625=Nick%20Willemstein%20and%20Saivimal%20Sridar%20and%20Herman%20van%20der%20Kooij%20and%20Ali%20Sadeghi&entry.1292438233=%20%20Sensorized%20insoles%20provide%20a%20tool%20for%20gait%20studies%20and%20health%20monitoring%0Aduring%20daily%20life.%20For%20users%20to%20accept%20such%20insoles%20they%20need%20to%20be%20comfortable%0Aand%20lightweight.%20Previous%20work%20has%20already%20demonstrated%20that%20estimation%20of%0Aground%20reaction%20forces%20%28GRFs%29%20is%20possible%20with%20insoles.%20However%2C%20these%20are%0Aoften%20assemblies%20of%20commercial%20components%20restricting%20design%20freedom%20and%0Acustomization.%20Within%20this%20work%2C%20we%20investigate%20using%20four%203D-printed%20soft%0Afoam-like%20sensors%20to%20sensorize%20an%20insole.%20These%20sensors%20were%20combined%20with%0Asystem%20identification%20of%20Hammerstein-Wiener%20models%20to%20estimate%20the%203D%20GRFs%2C%0Awhich%20were%20compared%20to%20values%20from%20an%20instrumented%20treadmill%20as%20the%20golden%0Astandard.%20It%20was%20observed%20that%20the%20four%20sensors%20behaved%20in%20line%20with%20the%0Aexpected%20change%20in%20pressure%20distribution%20during%20the%20gait%20cycle.%20In%20addition%2C%0Athe%20identified%20%28personalized%29%20Hammerstein-Wiener%20models%20showed%20the%20best%0Aestimation%20performance%20%28on%20average%20RMS%20error%209.3%25%2C%20R%5E2%3D0.85%20and%20mean%20absolute%0Aerror%20%28MAE%29%207%25%29%20of%20the%20vertical%2C%20mediolateral%2C%20and%20anteroposterior%20GRFs.%0AThereby%20showing%20that%20these%20sensors%20can%20estimate%20the%20resulting%203D%20force%0Areasonably%20well.%20These%20results%20for%20nine%20participants%20were%20comparable%20to%20or%0Aoutperformed%20other%20works%20that%20used%20commercial%20FSRs%20with%20machine%20learning.%20The%0Aidentified%20models%20did%20decrease%20in%20estimation%20performance%20over%20time%20but%20stayed%0Aon%20average%2011.35%25%20RMS%20and%208.6%25%20MAE%20after%20a%20week%20with%20the%20Hammerstein-Wiener%0Amodel%20seeming%20consistent%20between%20days%20two%20and%20seven.%20These%20results%20show%20promise%0Afor%20using%203D-printed%20soft%20piezoresistive%20foam-like%20sensors%20with%20system%0Aidentification%20to%20be%20a%20viable%20approach%20for%20applications%20that%20require%20softness%2C%0Alightweight%2C%20and%20customization%20such%20as%20wearable%20%28force%29%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.04719v2&entry.124074799=Read"},
{"title": "Heteroscedastic Preferential Bayesian Optimization with Informative\n  Noise Distributions", "author": "Marshal Arijona Sinaga and Julien Martinelli and Vikas Garg and Samuel Kaski", "abstract": "  Preferential Bayesian optimization (PBO) is a sample-efficient framework for\nlearning human preferences between candidate designs. PBO classically relies on\nhomoscedastic noise models to represent human aleatoric uncertainty. Yet, such\nnoise fails to accurately capture the varying levels of human aleatoric\nuncertainty, particularly when the user possesses partial knowledge among\ndifferent pairs of candidates. For instance, a chemist with solid expertise in\nglucose-related molecules may easily compare two compounds from that family\nwhile struggling to compare alcohol-related molecules. Currently, PBO overlooks\nthis uncertainty during the search for a new candidate through the maximization\nof the acquisition function, consequently underestimating the risk associated\nwith human uncertainty. To address this issue, we propose a heteroscedastic\nnoise model to capture human aleatoric uncertainty. This model adaptively\nassigns noise levels based on the distance of a specific input to a predefined\nset of reliable inputs known as anchors provided by the human. Anchors\nencapsulate partial knowledge and offer insight into the comparative difficulty\nof evaluating different candidate pairs. Such a model can be seamlessly\nintegrated into the acquisition function, thus leading to candidate design\npairs that elegantly trade informativeness and ease of comparison for the human\nexpert. We perform an extensive empirical evaluation of the proposed approach,\ndemonstrating a consistent improvement over homoscedastic PBO.\n", "link": "http://arxiv.org/abs/2405.14657v1", "date": "2024-05-23", "relevancy": 1.4946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heteroscedastic%20Preferential%20Bayesian%20Optimization%20with%20Informative%0A%20%20Noise%20Distributions&body=Title%3A%20Heteroscedastic%20Preferential%20Bayesian%20Optimization%20with%20Informative%0A%20%20Noise%20Distributions%0AAuthor%3A%20Marshal%20Arijona%20Sinaga%20and%20Julien%20Martinelli%20and%20Vikas%20Garg%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Preferential%20Bayesian%20optimization%20%28PBO%29%20is%20a%20sample-efficient%20framework%20for%0Alearning%20human%20preferences%20between%20candidate%20designs.%20PBO%20classically%20relies%20on%0Ahomoscedastic%20noise%20models%20to%20represent%20human%20aleatoric%20uncertainty.%20Yet%2C%20such%0Anoise%20fails%20to%20accurately%20capture%20the%20varying%20levels%20of%20human%20aleatoric%0Auncertainty%2C%20particularly%20when%20the%20user%20possesses%20partial%20knowledge%20among%0Adifferent%20pairs%20of%20candidates.%20For%20instance%2C%20a%20chemist%20with%20solid%20expertise%20in%0Aglucose-related%20molecules%20may%20easily%20compare%20two%20compounds%20from%20that%20family%0Awhile%20struggling%20to%20compare%20alcohol-related%20molecules.%20Currently%2C%20PBO%20overlooks%0Athis%20uncertainty%20during%20the%20search%20for%20a%20new%20candidate%20through%20the%20maximization%0Aof%20the%20acquisition%20function%2C%20consequently%20underestimating%20the%20risk%20associated%0Awith%20human%20uncertainty.%20To%20address%20this%20issue%2C%20we%20propose%20a%20heteroscedastic%0Anoise%20model%20to%20capture%20human%20aleatoric%20uncertainty.%20This%20model%20adaptively%0Aassigns%20noise%20levels%20based%20on%20the%20distance%20of%20a%20specific%20input%20to%20a%20predefined%0Aset%20of%20reliable%20inputs%20known%20as%20anchors%20provided%20by%20the%20human.%20Anchors%0Aencapsulate%20partial%20knowledge%20and%20offer%20insight%20into%20the%20comparative%20difficulty%0Aof%20evaluating%20different%20candidate%20pairs.%20Such%20a%20model%20can%20be%20seamlessly%0Aintegrated%20into%20the%20acquisition%20function%2C%20thus%20leading%20to%20candidate%20design%0Apairs%20that%20elegantly%20trade%20informativeness%20and%20ease%20of%20comparison%20for%20the%20human%0Aexpert.%20We%20perform%20an%20extensive%20empirical%20evaluation%20of%20the%20proposed%20approach%2C%0Ademonstrating%20a%20consistent%20improvement%20over%20homoscedastic%20PBO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeteroscedastic%2520Preferential%2520Bayesian%2520Optimization%2520with%2520Informative%250A%2520%2520Noise%2520Distributions%26entry.906535625%3DMarshal%2520Arijona%2520Sinaga%2520and%2520Julien%2520Martinelli%2520and%2520Vikas%2520Garg%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Preferential%2520Bayesian%2520optimization%2520%2528PBO%2529%2520is%2520a%2520sample-efficient%2520framework%2520for%250Alearning%2520human%2520preferences%2520between%2520candidate%2520designs.%2520PBO%2520classically%2520relies%2520on%250Ahomoscedastic%2520noise%2520models%2520to%2520represent%2520human%2520aleatoric%2520uncertainty.%2520Yet%252C%2520such%250Anoise%2520fails%2520to%2520accurately%2520capture%2520the%2520varying%2520levels%2520of%2520human%2520aleatoric%250Auncertainty%252C%2520particularly%2520when%2520the%2520user%2520possesses%2520partial%2520knowledge%2520among%250Adifferent%2520pairs%2520of%2520candidates.%2520For%2520instance%252C%2520a%2520chemist%2520with%2520solid%2520expertise%2520in%250Aglucose-related%2520molecules%2520may%2520easily%2520compare%2520two%2520compounds%2520from%2520that%2520family%250Awhile%2520struggling%2520to%2520compare%2520alcohol-related%2520molecules.%2520Currently%252C%2520PBO%2520overlooks%250Athis%2520uncertainty%2520during%2520the%2520search%2520for%2520a%2520new%2520candidate%2520through%2520the%2520maximization%250Aof%2520the%2520acquisition%2520function%252C%2520consequently%2520underestimating%2520the%2520risk%2520associated%250Awith%2520human%2520uncertainty.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520heteroscedastic%250Anoise%2520model%2520to%2520capture%2520human%2520aleatoric%2520uncertainty.%2520This%2520model%2520adaptively%250Aassigns%2520noise%2520levels%2520based%2520on%2520the%2520distance%2520of%2520a%2520specific%2520input%2520to%2520a%2520predefined%250Aset%2520of%2520reliable%2520inputs%2520known%2520as%2520anchors%2520provided%2520by%2520the%2520human.%2520Anchors%250Aencapsulate%2520partial%2520knowledge%2520and%2520offer%2520insight%2520into%2520the%2520comparative%2520difficulty%250Aof%2520evaluating%2520different%2520candidate%2520pairs.%2520Such%2520a%2520model%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520the%2520acquisition%2520function%252C%2520thus%2520leading%2520to%2520candidate%2520design%250Apairs%2520that%2520elegantly%2520trade%2520informativeness%2520and%2520ease%2520of%2520comparison%2520for%2520the%2520human%250Aexpert.%2520We%2520perform%2520an%2520extensive%2520empirical%2520evaluation%2520of%2520the%2520proposed%2520approach%252C%250Ademonstrating%2520a%2520consistent%2520improvement%2520over%2520homoscedastic%2520PBO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heteroscedastic%20Preferential%20Bayesian%20Optimization%20with%20Informative%0A%20%20Noise%20Distributions&entry.906535625=Marshal%20Arijona%20Sinaga%20and%20Julien%20Martinelli%20and%20Vikas%20Garg%20and%20Samuel%20Kaski&entry.1292438233=%20%20Preferential%20Bayesian%20optimization%20%28PBO%29%20is%20a%20sample-efficient%20framework%20for%0Alearning%20human%20preferences%20between%20candidate%20designs.%20PBO%20classically%20relies%20on%0Ahomoscedastic%20noise%20models%20to%20represent%20human%20aleatoric%20uncertainty.%20Yet%2C%20such%0Anoise%20fails%20to%20accurately%20capture%20the%20varying%20levels%20of%20human%20aleatoric%0Auncertainty%2C%20particularly%20when%20the%20user%20possesses%20partial%20knowledge%20among%0Adifferent%20pairs%20of%20candidates.%20For%20instance%2C%20a%20chemist%20with%20solid%20expertise%20in%0Aglucose-related%20molecules%20may%20easily%20compare%20two%20compounds%20from%20that%20family%0Awhile%20struggling%20to%20compare%20alcohol-related%20molecules.%20Currently%2C%20PBO%20overlooks%0Athis%20uncertainty%20during%20the%20search%20for%20a%20new%20candidate%20through%20the%20maximization%0Aof%20the%20acquisition%20function%2C%20consequently%20underestimating%20the%20risk%20associated%0Awith%20human%20uncertainty.%20To%20address%20this%20issue%2C%20we%20propose%20a%20heteroscedastic%0Anoise%20model%20to%20capture%20human%20aleatoric%20uncertainty.%20This%20model%20adaptively%0Aassigns%20noise%20levels%20based%20on%20the%20distance%20of%20a%20specific%20input%20to%20a%20predefined%0Aset%20of%20reliable%20inputs%20known%20as%20anchors%20provided%20by%20the%20human.%20Anchors%0Aencapsulate%20partial%20knowledge%20and%20offer%20insight%20into%20the%20comparative%20difficulty%0Aof%20evaluating%20different%20candidate%20pairs.%20Such%20a%20model%20can%20be%20seamlessly%0Aintegrated%20into%20the%20acquisition%20function%2C%20thus%20leading%20to%20candidate%20design%0Apairs%20that%20elegantly%20trade%20informativeness%20and%20ease%20of%20comparison%20for%20the%20human%0Aexpert.%20We%20perform%20an%20extensive%20empirical%20evaluation%20of%20the%20proposed%20approach%2C%0Ademonstrating%20a%20consistent%20improvement%20over%20homoscedastic%20PBO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14657v1&entry.124074799=Read"},
{"title": "Visuo-Tactile based Predictive Cross Modal Perception for Object\n  Exploration in Robotics", "author": "Anirvan Dutta and Etienne Burdet and Mohsen Kaboli", "abstract": "  Autonomously exploring the unknown physical properties of novel objects such\nas stiffness, mass, center of mass, friction coefficient, and shape is crucial\nfor autonomous robotic systems operating continuously in unstructured\nenvironments. We introduce a novel visuo-tactile based predictive cross-modal\nperception framework where initial visual observations (shape) aid in obtaining\nan initial prior over the object properties (mass). The initial prior improves\nthe efficiency of the object property estimation, which is autonomously\ninferred via interactive non-prehensile pushing and using a dual filtering\napproach. The inferred properties are then used to enhance the predictive\ncapability of the cross-modal function efficiently by using a human-inspired\n`surprise' formulation. We evaluated our proposed framework in the real-robotic\nscenario, demonstrating superior performance.\n", "link": "http://arxiv.org/abs/2405.12634v2", "date": "2024-05-23", "relevancy": 1.9164, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.7309}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6435}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visuo-Tactile%20based%20Predictive%20Cross%20Modal%20Perception%20for%20Object%0A%20%20Exploration%20in%20Robotics&body=Title%3A%20Visuo-Tactile%20based%20Predictive%20Cross%20Modal%20Perception%20for%20Object%0A%20%20Exploration%20in%20Robotics%0AAuthor%3A%20Anirvan%20Dutta%20and%20Etienne%20Burdet%20and%20Mohsen%20Kaboli%0AAbstract%3A%20%20%20Autonomously%20exploring%20the%20unknown%20physical%20properties%20of%20novel%20objects%20such%0Aas%20stiffness%2C%20mass%2C%20center%20of%20mass%2C%20friction%20coefficient%2C%20and%20shape%20is%20crucial%0Afor%20autonomous%20robotic%20systems%20operating%20continuously%20in%20unstructured%0Aenvironments.%20We%20introduce%20a%20novel%20visuo-tactile%20based%20predictive%20cross-modal%0Aperception%20framework%20where%20initial%20visual%20observations%20%28shape%29%20aid%20in%20obtaining%0Aan%20initial%20prior%20over%20the%20object%20properties%20%28mass%29.%20The%20initial%20prior%20improves%0Athe%20efficiency%20of%20the%20object%20property%20estimation%2C%20which%20is%20autonomously%0Ainferred%20via%20interactive%20non-prehensile%20pushing%20and%20using%20a%20dual%20filtering%0Aapproach.%20The%20inferred%20properties%20are%20then%20used%20to%20enhance%20the%20predictive%0Acapability%20of%20the%20cross-modal%20function%20efficiently%20by%20using%20a%20human-inspired%0A%60surprise%27%20formulation.%20We%20evaluated%20our%20proposed%20framework%20in%20the%20real-robotic%0Ascenario%2C%20demonstrating%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisuo-Tactile%2520based%2520Predictive%2520Cross%2520Modal%2520Perception%2520for%2520Object%250A%2520%2520Exploration%2520in%2520Robotics%26entry.906535625%3DAnirvan%2520Dutta%2520and%2520Etienne%2520Burdet%2520and%2520Mohsen%2520Kaboli%26entry.1292438233%3D%2520%2520Autonomously%2520exploring%2520the%2520unknown%2520physical%2520properties%2520of%2520novel%2520objects%2520such%250Aas%2520stiffness%252C%2520mass%252C%2520center%2520of%2520mass%252C%2520friction%2520coefficient%252C%2520and%2520shape%2520is%2520crucial%250Afor%2520autonomous%2520robotic%2520systems%2520operating%2520continuously%2520in%2520unstructured%250Aenvironments.%2520We%2520introduce%2520a%2520novel%2520visuo-tactile%2520based%2520predictive%2520cross-modal%250Aperception%2520framework%2520where%2520initial%2520visual%2520observations%2520%2528shape%2529%2520aid%2520in%2520obtaining%250Aan%2520initial%2520prior%2520over%2520the%2520object%2520properties%2520%2528mass%2529.%2520The%2520initial%2520prior%2520improves%250Athe%2520efficiency%2520of%2520the%2520object%2520property%2520estimation%252C%2520which%2520is%2520autonomously%250Ainferred%2520via%2520interactive%2520non-prehensile%2520pushing%2520and%2520using%2520a%2520dual%2520filtering%250Aapproach.%2520The%2520inferred%2520properties%2520are%2520then%2520used%2520to%2520enhance%2520the%2520predictive%250Acapability%2520of%2520the%2520cross-modal%2520function%2520efficiently%2520by%2520using%2520a%2520human-inspired%250A%2560surprise%2527%2520formulation.%2520We%2520evaluated%2520our%2520proposed%2520framework%2520in%2520the%2520real-robotic%250Ascenario%252C%2520demonstrating%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visuo-Tactile%20based%20Predictive%20Cross%20Modal%20Perception%20for%20Object%0A%20%20Exploration%20in%20Robotics&entry.906535625=Anirvan%20Dutta%20and%20Etienne%20Burdet%20and%20Mohsen%20Kaboli&entry.1292438233=%20%20Autonomously%20exploring%20the%20unknown%20physical%20properties%20of%20novel%20objects%20such%0Aas%20stiffness%2C%20mass%2C%20center%20of%20mass%2C%20friction%20coefficient%2C%20and%20shape%20is%20crucial%0Afor%20autonomous%20robotic%20systems%20operating%20continuously%20in%20unstructured%0Aenvironments.%20We%20introduce%20a%20novel%20visuo-tactile%20based%20predictive%20cross-modal%0Aperception%20framework%20where%20initial%20visual%20observations%20%28shape%29%20aid%20in%20obtaining%0Aan%20initial%20prior%20over%20the%20object%20properties%20%28mass%29.%20The%20initial%20prior%20improves%0Athe%20efficiency%20of%20the%20object%20property%20estimation%2C%20which%20is%20autonomously%0Ainferred%20via%20interactive%20non-prehensile%20pushing%20and%20using%20a%20dual%20filtering%0Aapproach.%20The%20inferred%20properties%20are%20then%20used%20to%20enhance%20the%20predictive%0Acapability%20of%20the%20cross-modal%20function%20efficiently%20by%20using%20a%20human-inspired%0A%60surprise%27%20formulation.%20We%20evaluated%20our%20proposed%20framework%20in%20the%20real-robotic%0Ascenario%2C%20demonstrating%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12634v2&entry.124074799=Read"},
{"title": "Towards Privacy-Aware and Personalised Assistive Robots: A User-Centred\n  Approach", "author": "Fernando E. Casado", "abstract": "  The global increase in the elderly population necessitates innovative\nlong-term care solutions to improve the quality of life for vulnerable\nindividuals while reducing caregiver burdens. Assistive robots, leveraging\nadvancements in Machine Learning, offer promising personalised support.\nHowever, their integration into daily life raises significant privacy concerns.\nWidely used frameworks like the Robot Operating System (ROS) historically lack\ninherent privacy mechanisms, complicating data-driven approaches in robotics.\nThis research pioneers user-centric, privacy-aware technologies such as\nFederated Learning (FL) to advance assistive robotics. FL enables collaborative\nlearning without sharing sensitive data, addressing privacy and scalability\nissues. This work includes developing solutions for smart wheelchair\nassistance, enhancing user independence and well-being. By tackling challenges\nrelated to non-stationary data and heterogeneous environments, the research\naims to improve personalisation and user experience. Ultimately, it seeks to\nlead the responsible integration of assistive robots into society, enhancing\nthe quality of life for elderly and care-dependent individuals.\n", "link": "http://arxiv.org/abs/2405.14528v1", "date": "2024-05-23", "relevancy": 1.7009, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6022}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5397}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Privacy-Aware%20and%20Personalised%20Assistive%20Robots%3A%20A%20User-Centred%0A%20%20Approach&body=Title%3A%20Towards%20Privacy-Aware%20and%20Personalised%20Assistive%20Robots%3A%20A%20User-Centred%0A%20%20Approach%0AAuthor%3A%20Fernando%20E.%20Casado%0AAbstract%3A%20%20%20The%20global%20increase%20in%20the%20elderly%20population%20necessitates%20innovative%0Along-term%20care%20solutions%20to%20improve%20the%20quality%20of%20life%20for%20vulnerable%0Aindividuals%20while%20reducing%20caregiver%20burdens.%20Assistive%20robots%2C%20leveraging%0Aadvancements%20in%20Machine%20Learning%2C%20offer%20promising%20personalised%20support.%0AHowever%2C%20their%20integration%20into%20daily%20life%20raises%20significant%20privacy%20concerns.%0AWidely%20used%20frameworks%20like%20the%20Robot%20Operating%20System%20%28ROS%29%20historically%20lack%0Ainherent%20privacy%20mechanisms%2C%20complicating%20data-driven%20approaches%20in%20robotics.%0AThis%20research%20pioneers%20user-centric%2C%20privacy-aware%20technologies%20such%20as%0AFederated%20Learning%20%28FL%29%20to%20advance%20assistive%20robotics.%20FL%20enables%20collaborative%0Alearning%20without%20sharing%20sensitive%20data%2C%20addressing%20privacy%20and%20scalability%0Aissues.%20This%20work%20includes%20developing%20solutions%20for%20smart%20wheelchair%0Aassistance%2C%20enhancing%20user%20independence%20and%20well-being.%20By%20tackling%20challenges%0Arelated%20to%20non-stationary%20data%20and%20heterogeneous%20environments%2C%20the%20research%0Aaims%20to%20improve%20personalisation%20and%20user%20experience.%20Ultimately%2C%20it%20seeks%20to%0Alead%20the%20responsible%20integration%20of%20assistive%20robots%20into%20society%2C%20enhancing%0Athe%20quality%20of%20life%20for%20elderly%20and%20care-dependent%20individuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Privacy-Aware%2520and%2520Personalised%2520Assistive%2520Robots%253A%2520A%2520User-Centred%250A%2520%2520Approach%26entry.906535625%3DFernando%2520E.%2520Casado%26entry.1292438233%3D%2520%2520The%2520global%2520increase%2520in%2520the%2520elderly%2520population%2520necessitates%2520innovative%250Along-term%2520care%2520solutions%2520to%2520improve%2520the%2520quality%2520of%2520life%2520for%2520vulnerable%250Aindividuals%2520while%2520reducing%2520caregiver%2520burdens.%2520Assistive%2520robots%252C%2520leveraging%250Aadvancements%2520in%2520Machine%2520Learning%252C%2520offer%2520promising%2520personalised%2520support.%250AHowever%252C%2520their%2520integration%2520into%2520daily%2520life%2520raises%2520significant%2520privacy%2520concerns.%250AWidely%2520used%2520frameworks%2520like%2520the%2520Robot%2520Operating%2520System%2520%2528ROS%2529%2520historically%2520lack%250Ainherent%2520privacy%2520mechanisms%252C%2520complicating%2520data-driven%2520approaches%2520in%2520robotics.%250AThis%2520research%2520pioneers%2520user-centric%252C%2520privacy-aware%2520technologies%2520such%2520as%250AFederated%2520Learning%2520%2528FL%2529%2520to%2520advance%2520assistive%2520robotics.%2520FL%2520enables%2520collaborative%250Alearning%2520without%2520sharing%2520sensitive%2520data%252C%2520addressing%2520privacy%2520and%2520scalability%250Aissues.%2520This%2520work%2520includes%2520developing%2520solutions%2520for%2520smart%2520wheelchair%250Aassistance%252C%2520enhancing%2520user%2520independence%2520and%2520well-being.%2520By%2520tackling%2520challenges%250Arelated%2520to%2520non-stationary%2520data%2520and%2520heterogeneous%2520environments%252C%2520the%2520research%250Aaims%2520to%2520improve%2520personalisation%2520and%2520user%2520experience.%2520Ultimately%252C%2520it%2520seeks%2520to%250Alead%2520the%2520responsible%2520integration%2520of%2520assistive%2520robots%2520into%2520society%252C%2520enhancing%250Athe%2520quality%2520of%2520life%2520for%2520elderly%2520and%2520care-dependent%2520individuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Privacy-Aware%20and%20Personalised%20Assistive%20Robots%3A%20A%20User-Centred%0A%20%20Approach&entry.906535625=Fernando%20E.%20Casado&entry.1292438233=%20%20The%20global%20increase%20in%20the%20elderly%20population%20necessitates%20innovative%0Along-term%20care%20solutions%20to%20improve%20the%20quality%20of%20life%20for%20vulnerable%0Aindividuals%20while%20reducing%20caregiver%20burdens.%20Assistive%20robots%2C%20leveraging%0Aadvancements%20in%20Machine%20Learning%2C%20offer%20promising%20personalised%20support.%0AHowever%2C%20their%20integration%20into%20daily%20life%20raises%20significant%20privacy%20concerns.%0AWidely%20used%20frameworks%20like%20the%20Robot%20Operating%20System%20%28ROS%29%20historically%20lack%0Ainherent%20privacy%20mechanisms%2C%20complicating%20data-driven%20approaches%20in%20robotics.%0AThis%20research%20pioneers%20user-centric%2C%20privacy-aware%20technologies%20such%20as%0AFederated%20Learning%20%28FL%29%20to%20advance%20assistive%20robotics.%20FL%20enables%20collaborative%0Alearning%20without%20sharing%20sensitive%20data%2C%20addressing%20privacy%20and%20scalability%0Aissues.%20This%20work%20includes%20developing%20solutions%20for%20smart%20wheelchair%0Aassistance%2C%20enhancing%20user%20independence%20and%20well-being.%20By%20tackling%20challenges%0Arelated%20to%20non-stationary%20data%20and%20heterogeneous%20environments%2C%20the%20research%0Aaims%20to%20improve%20personalisation%20and%20user%20experience.%20Ultimately%2C%20it%20seeks%20to%0Alead%20the%20responsible%20integration%20of%20assistive%20robots%20into%20society%2C%20enhancing%0Athe%20quality%20of%20life%20for%20elderly%20and%20care-dependent%20individuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14528v1&entry.124074799=Read"},
{"title": "Kinetics of orbital ordering in cooperative Jahn-Teller models:\n  Machine-learning enabled large-scale simulations", "author": "Supriyo Ghosh and Sheng Zhang and Chen Cheng and Gia-Wei Chern", "abstract": "  We present a scalable machine learning (ML) force-field model for the\nadiabatic dynamics of cooperative Jahn-Teller (JT) systems. Large scale\ndynamical simulations of the JT model also shed light on the orbital ordering\ndynamics in colossal magnetoresistance manganites. The JT effect in these\nmaterials describes the distortion of local oxygen octahedra driven by a\ncoupling to the orbital degrees of freedom of $e_g$ electrons. An effective\nelectron-mediated interaction between the local JT modes leads to a structural\ntransition and the emergence of long-range orbital order at low temperatures.\nAssuming the principle of locality, a deep-learning neural-network model is\ndeveloped to accurately and efficiently predict the electron-induced forces\nthat drive the dynamical evolution of JT phonons. A group-theoretical method is\nutilized to develop a descriptor that incorporates the combined orbital and\nlattice symmetry into the ML model. Large-scale Langevin dynamics simulations,\nenabled by the ML force-field models, are performed to investigate the\ncoarsening dynamics of the composite JT distortion and orbital order after a\nthermal quench. The late-stage coarsening of orbital domains exhibits\npronounced freezing behaviors which are likely related to the unusual\nmorphology of the domain structures. Our work highlights a promising avenue for\nmulti-scale dynamical modeling of correlated electron systems.\n", "link": "http://arxiv.org/abs/2405.14776v1", "date": "2024-05-23", "relevancy": 1.2683, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4088}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinetics%20of%20orbital%20ordering%20in%20cooperative%20Jahn-Teller%20models%3A%0A%20%20Machine-learning%20enabled%20large-scale%20simulations&body=Title%3A%20Kinetics%20of%20orbital%20ordering%20in%20cooperative%20Jahn-Teller%20models%3A%0A%20%20Machine-learning%20enabled%20large-scale%20simulations%0AAuthor%3A%20Supriyo%20Ghosh%20and%20Sheng%20Zhang%20and%20Chen%20Cheng%20and%20Gia-Wei%20Chern%0AAbstract%3A%20%20%20We%20present%20a%20scalable%20machine%20learning%20%28ML%29%20force-field%20model%20for%20the%0Aadiabatic%20dynamics%20of%20cooperative%20Jahn-Teller%20%28JT%29%20systems.%20Large%20scale%0Adynamical%20simulations%20of%20the%20JT%20model%20also%20shed%20light%20on%20the%20orbital%20ordering%0Adynamics%20in%20colossal%20magnetoresistance%20manganites.%20The%20JT%20effect%20in%20these%0Amaterials%20describes%20the%20distortion%20of%20local%20oxygen%20octahedra%20driven%20by%20a%0Acoupling%20to%20the%20orbital%20degrees%20of%20freedom%20of%20%24e_g%24%20electrons.%20An%20effective%0Aelectron-mediated%20interaction%20between%20the%20local%20JT%20modes%20leads%20to%20a%20structural%0Atransition%20and%20the%20emergence%20of%20long-range%20orbital%20order%20at%20low%20temperatures.%0AAssuming%20the%20principle%20of%20locality%2C%20a%20deep-learning%20neural-network%20model%20is%0Adeveloped%20to%20accurately%20and%20efficiently%20predict%20the%20electron-induced%20forces%0Athat%20drive%20the%20dynamical%20evolution%20of%20JT%20phonons.%20A%20group-theoretical%20method%20is%0Autilized%20to%20develop%20a%20descriptor%20that%20incorporates%20the%20combined%20orbital%20and%0Alattice%20symmetry%20into%20the%20ML%20model.%20Large-scale%20Langevin%20dynamics%20simulations%2C%0Aenabled%20by%20the%20ML%20force-field%20models%2C%20are%20performed%20to%20investigate%20the%0Acoarsening%20dynamics%20of%20the%20composite%20JT%20distortion%20and%20orbital%20order%20after%20a%0Athermal%20quench.%20The%20late-stage%20coarsening%20of%20orbital%20domains%20exhibits%0Apronounced%20freezing%20behaviors%20which%20are%20likely%20related%20to%20the%20unusual%0Amorphology%20of%20the%20domain%20structures.%20Our%20work%20highlights%20a%20promising%20avenue%20for%0Amulti-scale%20dynamical%20modeling%20of%20correlated%20electron%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinetics%2520of%2520orbital%2520ordering%2520in%2520cooperative%2520Jahn-Teller%2520models%253A%250A%2520%2520Machine-learning%2520enabled%2520large-scale%2520simulations%26entry.906535625%3DSupriyo%2520Ghosh%2520and%2520Sheng%2520Zhang%2520and%2520Chen%2520Cheng%2520and%2520Gia-Wei%2520Chern%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520scalable%2520machine%2520learning%2520%2528ML%2529%2520force-field%2520model%2520for%2520the%250Aadiabatic%2520dynamics%2520of%2520cooperative%2520Jahn-Teller%2520%2528JT%2529%2520systems.%2520Large%2520scale%250Adynamical%2520simulations%2520of%2520the%2520JT%2520model%2520also%2520shed%2520light%2520on%2520the%2520orbital%2520ordering%250Adynamics%2520in%2520colossal%2520magnetoresistance%2520manganites.%2520The%2520JT%2520effect%2520in%2520these%250Amaterials%2520describes%2520the%2520distortion%2520of%2520local%2520oxygen%2520octahedra%2520driven%2520by%2520a%250Acoupling%2520to%2520the%2520orbital%2520degrees%2520of%2520freedom%2520of%2520%2524e_g%2524%2520electrons.%2520An%2520effective%250Aelectron-mediated%2520interaction%2520between%2520the%2520local%2520JT%2520modes%2520leads%2520to%2520a%2520structural%250Atransition%2520and%2520the%2520emergence%2520of%2520long-range%2520orbital%2520order%2520at%2520low%2520temperatures.%250AAssuming%2520the%2520principle%2520of%2520locality%252C%2520a%2520deep-learning%2520neural-network%2520model%2520is%250Adeveloped%2520to%2520accurately%2520and%2520efficiently%2520predict%2520the%2520electron-induced%2520forces%250Athat%2520drive%2520the%2520dynamical%2520evolution%2520of%2520JT%2520phonons.%2520A%2520group-theoretical%2520method%2520is%250Autilized%2520to%2520develop%2520a%2520descriptor%2520that%2520incorporates%2520the%2520combined%2520orbital%2520and%250Alattice%2520symmetry%2520into%2520the%2520ML%2520model.%2520Large-scale%2520Langevin%2520dynamics%2520simulations%252C%250Aenabled%2520by%2520the%2520ML%2520force-field%2520models%252C%2520are%2520performed%2520to%2520investigate%2520the%250Acoarsening%2520dynamics%2520of%2520the%2520composite%2520JT%2520distortion%2520and%2520orbital%2520order%2520after%2520a%250Athermal%2520quench.%2520The%2520late-stage%2520coarsening%2520of%2520orbital%2520domains%2520exhibits%250Apronounced%2520freezing%2520behaviors%2520which%2520are%2520likely%2520related%2520to%2520the%2520unusual%250Amorphology%2520of%2520the%2520domain%2520structures.%2520Our%2520work%2520highlights%2520a%2520promising%2520avenue%2520for%250Amulti-scale%2520dynamical%2520modeling%2520of%2520correlated%2520electron%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinetics%20of%20orbital%20ordering%20in%20cooperative%20Jahn-Teller%20models%3A%0A%20%20Machine-learning%20enabled%20large-scale%20simulations&entry.906535625=Supriyo%20Ghosh%20and%20Sheng%20Zhang%20and%20Chen%20Cheng%20and%20Gia-Wei%20Chern&entry.1292438233=%20%20We%20present%20a%20scalable%20machine%20learning%20%28ML%29%20force-field%20model%20for%20the%0Aadiabatic%20dynamics%20of%20cooperative%20Jahn-Teller%20%28JT%29%20systems.%20Large%20scale%0Adynamical%20simulations%20of%20the%20JT%20model%20also%20shed%20light%20on%20the%20orbital%20ordering%0Adynamics%20in%20colossal%20magnetoresistance%20manganites.%20The%20JT%20effect%20in%20these%0Amaterials%20describes%20the%20distortion%20of%20local%20oxygen%20octahedra%20driven%20by%20a%0Acoupling%20to%20the%20orbital%20degrees%20of%20freedom%20of%20%24e_g%24%20electrons.%20An%20effective%0Aelectron-mediated%20interaction%20between%20the%20local%20JT%20modes%20leads%20to%20a%20structural%0Atransition%20and%20the%20emergence%20of%20long-range%20orbital%20order%20at%20low%20temperatures.%0AAssuming%20the%20principle%20of%20locality%2C%20a%20deep-learning%20neural-network%20model%20is%0Adeveloped%20to%20accurately%20and%20efficiently%20predict%20the%20electron-induced%20forces%0Athat%20drive%20the%20dynamical%20evolution%20of%20JT%20phonons.%20A%20group-theoretical%20method%20is%0Autilized%20to%20develop%20a%20descriptor%20that%20incorporates%20the%20combined%20orbital%20and%0Alattice%20symmetry%20into%20the%20ML%20model.%20Large-scale%20Langevin%20dynamics%20simulations%2C%0Aenabled%20by%20the%20ML%20force-field%20models%2C%20are%20performed%20to%20investigate%20the%0Acoarsening%20dynamics%20of%20the%20composite%20JT%20distortion%20and%20orbital%20order%20after%20a%0Athermal%20quench.%20The%20late-stage%20coarsening%20of%20orbital%20domains%20exhibits%0Apronounced%20freezing%20behaviors%20which%20are%20likely%20related%20to%20the%20unusual%0Amorphology%20of%20the%20domain%20structures.%20Our%20work%20highlights%20a%20promising%20avenue%20for%0Amulti-scale%20dynamical%20modeling%20of%20correlated%20electron%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14776v1&entry.124074799=Read"},
{"title": "Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement\n  Learning with Provable Convergence", "author": "Minheng Xiao and Xian Yu and Lei Ying", "abstract": "  Risk-sensitive reinforcement learning (RL) is crucial for maintaining\nreliable performance in many high-stakes applications. While most RL methods\naim to learn a point estimate of the random cumulative cost, distributional RL\n(DRL) seeks to estimate the entire distribution of it. The distribution\nprovides all necessary information about the cost and leads to a unified\nframework for handling various risk measures in a risk-sensitive setting.\nHowever, developing policy gradient methods for risk-sensitive DRL is\ninherently more complex as it pertains to finding the gradient of a probability\nmeasure. This paper introduces a policy gradient method for risk-sensitive DRL\nwith general coherent risk measures, where we provide an analytical form of the\nprobability measure's gradient. We further prove the local convergence of the\nproposed algorithm under mild smoothness assumptions. For practical use, we\nalso design a categorical distributional policy gradient algorithm (CDPG) based\non categorical distributional policy evaluation and trajectory-based gradient\nestimation. Through experiments on a stochastic cliff-walking environment, we\nillustrate the benefits of considering a risk-sensitive setting in DRL.\n", "link": "http://arxiv.org/abs/2405.14749v1", "date": "2024-05-23", "relevancy": 1.3913, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy%20Gradient%20Methods%20for%20Risk-Sensitive%20Distributional%20Reinforcement%0A%20%20Learning%20with%20Provable%20Convergence&body=Title%3A%20Policy%20Gradient%20Methods%20for%20Risk-Sensitive%20Distributional%20Reinforcement%0A%20%20Learning%20with%20Provable%20Convergence%0AAuthor%3A%20Minheng%20Xiao%20and%20Xian%20Yu%20and%20Lei%20Ying%0AAbstract%3A%20%20%20Risk-sensitive%20reinforcement%20learning%20%28RL%29%20is%20crucial%20for%20maintaining%0Areliable%20performance%20in%20many%20high-stakes%20applications.%20While%20most%20RL%20methods%0Aaim%20to%20learn%20a%20point%20estimate%20of%20the%20random%20cumulative%20cost%2C%20distributional%20RL%0A%28DRL%29%20seeks%20to%20estimate%20the%20entire%20distribution%20of%20it.%20The%20distribution%0Aprovides%20all%20necessary%20information%20about%20the%20cost%20and%20leads%20to%20a%20unified%0Aframework%20for%20handling%20various%20risk%20measures%20in%20a%20risk-sensitive%20setting.%0AHowever%2C%20developing%20policy%20gradient%20methods%20for%20risk-sensitive%20DRL%20is%0Ainherently%20more%20complex%20as%20it%20pertains%20to%20finding%20the%20gradient%20of%20a%20probability%0Ameasure.%20This%20paper%20introduces%20a%20policy%20gradient%20method%20for%20risk-sensitive%20DRL%0Awith%20general%20coherent%20risk%20measures%2C%20where%20we%20provide%20an%20analytical%20form%20of%20the%0Aprobability%20measure%27s%20gradient.%20We%20further%20prove%20the%20local%20convergence%20of%20the%0Aproposed%20algorithm%20under%20mild%20smoothness%20assumptions.%20For%20practical%20use%2C%20we%0Aalso%20design%20a%20categorical%20distributional%20policy%20gradient%20algorithm%20%28CDPG%29%20based%0Aon%20categorical%20distributional%20policy%20evaluation%20and%20trajectory-based%20gradient%0Aestimation.%20Through%20experiments%20on%20a%20stochastic%20cliff-walking%20environment%2C%20we%0Aillustrate%20the%20benefits%20of%20considering%20a%20risk-sensitive%20setting%20in%20DRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy%2520Gradient%2520Methods%2520for%2520Risk-Sensitive%2520Distributional%2520Reinforcement%250A%2520%2520Learning%2520with%2520Provable%2520Convergence%26entry.906535625%3DMinheng%2520Xiao%2520and%2520Xian%2520Yu%2520and%2520Lei%2520Ying%26entry.1292438233%3D%2520%2520Risk-sensitive%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520crucial%2520for%2520maintaining%250Areliable%2520performance%2520in%2520many%2520high-stakes%2520applications.%2520While%2520most%2520RL%2520methods%250Aaim%2520to%2520learn%2520a%2520point%2520estimate%2520of%2520the%2520random%2520cumulative%2520cost%252C%2520distributional%2520RL%250A%2528DRL%2529%2520seeks%2520to%2520estimate%2520the%2520entire%2520distribution%2520of%2520it.%2520The%2520distribution%250Aprovides%2520all%2520necessary%2520information%2520about%2520the%2520cost%2520and%2520leads%2520to%2520a%2520unified%250Aframework%2520for%2520handling%2520various%2520risk%2520measures%2520in%2520a%2520risk-sensitive%2520setting.%250AHowever%252C%2520developing%2520policy%2520gradient%2520methods%2520for%2520risk-sensitive%2520DRL%2520is%250Ainherently%2520more%2520complex%2520as%2520it%2520pertains%2520to%2520finding%2520the%2520gradient%2520of%2520a%2520probability%250Ameasure.%2520This%2520paper%2520introduces%2520a%2520policy%2520gradient%2520method%2520for%2520risk-sensitive%2520DRL%250Awith%2520general%2520coherent%2520risk%2520measures%252C%2520where%2520we%2520provide%2520an%2520analytical%2520form%2520of%2520the%250Aprobability%2520measure%2527s%2520gradient.%2520We%2520further%2520prove%2520the%2520local%2520convergence%2520of%2520the%250Aproposed%2520algorithm%2520under%2520mild%2520smoothness%2520assumptions.%2520For%2520practical%2520use%252C%2520we%250Aalso%2520design%2520a%2520categorical%2520distributional%2520policy%2520gradient%2520algorithm%2520%2528CDPG%2529%2520based%250Aon%2520categorical%2520distributional%2520policy%2520evaluation%2520and%2520trajectory-based%2520gradient%250Aestimation.%2520Through%2520experiments%2520on%2520a%2520stochastic%2520cliff-walking%2520environment%252C%2520we%250Aillustrate%2520the%2520benefits%2520of%2520considering%2520a%2520risk-sensitive%2520setting%2520in%2520DRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Gradient%20Methods%20for%20Risk-Sensitive%20Distributional%20Reinforcement%0A%20%20Learning%20with%20Provable%20Convergence&entry.906535625=Minheng%20Xiao%20and%20Xian%20Yu%20and%20Lei%20Ying&entry.1292438233=%20%20Risk-sensitive%20reinforcement%20learning%20%28RL%29%20is%20crucial%20for%20maintaining%0Areliable%20performance%20in%20many%20high-stakes%20applications.%20While%20most%20RL%20methods%0Aaim%20to%20learn%20a%20point%20estimate%20of%20the%20random%20cumulative%20cost%2C%20distributional%20RL%0A%28DRL%29%20seeks%20to%20estimate%20the%20entire%20distribution%20of%20it.%20The%20distribution%0Aprovides%20all%20necessary%20information%20about%20the%20cost%20and%20leads%20to%20a%20unified%0Aframework%20for%20handling%20various%20risk%20measures%20in%20a%20risk-sensitive%20setting.%0AHowever%2C%20developing%20policy%20gradient%20methods%20for%20risk-sensitive%20DRL%20is%0Ainherently%20more%20complex%20as%20it%20pertains%20to%20finding%20the%20gradient%20of%20a%20probability%0Ameasure.%20This%20paper%20introduces%20a%20policy%20gradient%20method%20for%20risk-sensitive%20DRL%0Awith%20general%20coherent%20risk%20measures%2C%20where%20we%20provide%20an%20analytical%20form%20of%20the%0Aprobability%20measure%27s%20gradient.%20We%20further%20prove%20the%20local%20convergence%20of%20the%0Aproposed%20algorithm%20under%20mild%20smoothness%20assumptions.%20For%20practical%20use%2C%20we%0Aalso%20design%20a%20categorical%20distributional%20policy%20gradient%20algorithm%20%28CDPG%29%20based%0Aon%20categorical%20distributional%20policy%20evaluation%20and%20trajectory-based%20gradient%0Aestimation.%20Through%20experiments%20on%20a%20stochastic%20cliff-walking%20environment%2C%20we%0Aillustrate%20the%20benefits%20of%20considering%20a%20risk-sensitive%20setting%20in%20DRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14749v1&entry.124074799=Read"},
{"title": "Leveraging feature communication in federated learning for remote\n  sensing image classification", "author": "Anh-Kiet Duong and Ho\u00e0ng-\u00c2n L\u00ea and Minh-Tan Pham", "abstract": "  In the realm of Federated Learning (FL) applied to remote sensing image\nclassification, this study introduces and assesses several innovative\ncommunication strategies. Our exploration includes feature-centric\ncommunication, pseudo-weight amalgamation, and a combined method utilizing both\nweights and features. Experiments conducted on two public scene classification\ndatasets unveil the effectiveness of these strategies, showcasing accelerated\nconvergence, heightened privacy, and reduced network information exchange. This\nresearch provides valuable insights into the implications of feature-centric\ncommunication in FL, offering potential applications tailored for remote\nsensing scenarios.\n", "link": "http://arxiv.org/abs/2403.13575v2", "date": "2024-05-23", "relevancy": 1.9452, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5078}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification&body=Title%3A%20Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification%0AAuthor%3A%20Anh-Kiet%20Duong%20and%20Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Minh-Tan%20Pham%0AAbstract%3A%20%20%20In%20the%20realm%20of%20Federated%20Learning%20%28FL%29%20applied%20to%20remote%20sensing%20image%0Aclassification%2C%20this%20study%20introduces%20and%20assesses%20several%20innovative%0Acommunication%20strategies.%20Our%20exploration%20includes%20feature-centric%0Acommunication%2C%20pseudo-weight%20amalgamation%2C%20and%20a%20combined%20method%20utilizing%20both%0Aweights%20and%20features.%20Experiments%20conducted%20on%20two%20public%20scene%20classification%0Adatasets%20unveil%20the%20effectiveness%20of%20these%20strategies%2C%20showcasing%20accelerated%0Aconvergence%2C%20heightened%20privacy%2C%20and%20reduced%20network%20information%20exchange.%20This%0Aresearch%20provides%20valuable%20insights%20into%20the%20implications%20of%20feature-centric%0Acommunication%20in%20FL%2C%20offering%20potential%20applications%20tailored%20for%20remote%0Asensing%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13575v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520feature%2520communication%2520in%2520federated%2520learning%2520for%2520remote%250A%2520%2520sensing%2520image%2520classification%26entry.906535625%3DAnh-Kiet%2520Duong%2520and%2520Ho%25C3%25A0ng-%25C3%2582n%2520L%25C3%25AA%2520and%2520Minh-Tan%2520Pham%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520applied%2520to%2520remote%2520sensing%2520image%250Aclassification%252C%2520this%2520study%2520introduces%2520and%2520assesses%2520several%2520innovative%250Acommunication%2520strategies.%2520Our%2520exploration%2520includes%2520feature-centric%250Acommunication%252C%2520pseudo-weight%2520amalgamation%252C%2520and%2520a%2520combined%2520method%2520utilizing%2520both%250Aweights%2520and%2520features.%2520Experiments%2520conducted%2520on%2520two%2520public%2520scene%2520classification%250Adatasets%2520unveil%2520the%2520effectiveness%2520of%2520these%2520strategies%252C%2520showcasing%2520accelerated%250Aconvergence%252C%2520heightened%2520privacy%252C%2520and%2520reduced%2520network%2520information%2520exchange.%2520This%250Aresearch%2520provides%2520valuable%2520insights%2520into%2520the%2520implications%2520of%2520feature-centric%250Acommunication%2520in%2520FL%252C%2520offering%2520potential%2520applications%2520tailored%2520for%2520remote%250Asensing%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13575v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20feature%20communication%20in%20federated%20learning%20for%20remote%0A%20%20sensing%20image%20classification&entry.906535625=Anh-Kiet%20Duong%20and%20Ho%C3%A0ng-%C3%82n%20L%C3%AA%20and%20Minh-Tan%20Pham&entry.1292438233=%20%20In%20the%20realm%20of%20Federated%20Learning%20%28FL%29%20applied%20to%20remote%20sensing%20image%0Aclassification%2C%20this%20study%20introduces%20and%20assesses%20several%20innovative%0Acommunication%20strategies.%20Our%20exploration%20includes%20feature-centric%0Acommunication%2C%20pseudo-weight%20amalgamation%2C%20and%20a%20combined%20method%20utilizing%20both%0Aweights%20and%20features.%20Experiments%20conducted%20on%20two%20public%20scene%20classification%0Adatasets%20unveil%20the%20effectiveness%20of%20these%20strategies%2C%20showcasing%20accelerated%0Aconvergence%2C%20heightened%20privacy%2C%20and%20reduced%20network%20information%20exchange.%20This%0Aresearch%20provides%20valuable%20insights%20into%20the%20implications%20of%20feature-centric%0Acommunication%20in%20FL%2C%20offering%20potential%20applications%20tailored%20for%20remote%0Asensing%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13575v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


