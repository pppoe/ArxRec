<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250930.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian\n  Splatting", "author": "Daniel Wang and Patrick Rim and Tian Tian and Dong Lao and Alex Wong and Ganesh Sundaramoorthi", "abstract": "  We introduce ODE-GS, a novel approach that integrates 3D Gaussian Splatting\nwith latent neural ordinary differential equations (ODEs) to enable future\nextrapolation of dynamic 3D scenes. Unlike existing dynamic scene\nreconstruction methods, which rely on time-conditioned deformation networks and\nare limited to interpolation within a fixed time window, ODE-GS eliminates\ntimestamp dependency by modeling Gaussian parameter trajectories as\ncontinuous-time latent dynamics. Our approach first learns an interpolation\nmodel to generate accurate Gaussian trajectories within the observed window,\nthen trains a Transformer encoder to aggregate past trajectories into a latent\nstate evolved via a neural ODE. Finally, numerical integration produces smooth,\nphysically plausible future Gaussian trajectories, enabling rendering at\narbitrary future timestamps. On the D-NeRF, NVFi, and HyperNeRF benchmarks,\nODE-GS achieves state-of-the-art extrapolation performance, improving metrics\nby 19.8% compared to leading baselines, demonstrating its ability to accurately\nrepresent and predict 3D scene dynamics.\n", "link": "http://arxiv.org/abs/2506.05480v3", "date": "2025-09-30", "relevancy": 3.2609, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6628}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6523}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODE-GS%3A%20Latent%20ODEs%20for%20Dynamic%20Scene%20Extrapolation%20with%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20ODE-GS%3A%20Latent%20ODEs%20for%20Dynamic%20Scene%20Extrapolation%20with%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Daniel%20Wang%20and%20Patrick%20Rim%20and%20Tian%20Tian%20and%20Dong%20Lao%20and%20Alex%20Wong%20and%20Ganesh%20Sundaramoorthi%0AAbstract%3A%20%20%20We%20introduce%20ODE-GS%2C%20a%20novel%20approach%20that%20integrates%203D%20Gaussian%20Splatting%0Awith%20latent%20neural%20ordinary%20differential%20equations%20%28ODEs%29%20to%20enable%20future%0Aextrapolation%20of%20dynamic%203D%20scenes.%20Unlike%20existing%20dynamic%20scene%0Areconstruction%20methods%2C%20which%20rely%20on%20time-conditioned%20deformation%20networks%20and%0Aare%20limited%20to%20interpolation%20within%20a%20fixed%20time%20window%2C%20ODE-GS%20eliminates%0Atimestamp%20dependency%20by%20modeling%20Gaussian%20parameter%20trajectories%20as%0Acontinuous-time%20latent%20dynamics.%20Our%20approach%20first%20learns%20an%20interpolation%0Amodel%20to%20generate%20accurate%20Gaussian%20trajectories%20within%20the%20observed%20window%2C%0Athen%20trains%20a%20Transformer%20encoder%20to%20aggregate%20past%20trajectories%20into%20a%20latent%0Astate%20evolved%20via%20a%20neural%20ODE.%20Finally%2C%20numerical%20integration%20produces%20smooth%2C%0Aphysically%20plausible%20future%20Gaussian%20trajectories%2C%20enabling%20rendering%20at%0Aarbitrary%20future%20timestamps.%20On%20the%20D-NeRF%2C%20NVFi%2C%20and%20HyperNeRF%20benchmarks%2C%0AODE-GS%20achieves%20state-of-the-art%20extrapolation%20performance%2C%20improving%20metrics%0Aby%2019.8%25%20compared%20to%20leading%20baselines%2C%20demonstrating%20its%20ability%20to%20accurately%0Arepresent%20and%20predict%203D%20scene%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05480v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODE-GS%253A%2520Latent%2520ODEs%2520for%2520Dynamic%2520Scene%2520Extrapolation%2520with%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DDaniel%2520Wang%2520and%2520Patrick%2520Rim%2520and%2520Tian%2520Tian%2520and%2520Dong%2520Lao%2520and%2520Alex%2520Wong%2520and%2520Ganesh%2520Sundaramoorthi%26entry.1292438233%3D%2520%2520We%2520introduce%2520ODE-GS%252C%2520a%2520novel%2520approach%2520that%2520integrates%25203D%2520Gaussian%2520Splatting%250Awith%2520latent%2520neural%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%2520to%2520enable%2520future%250Aextrapolation%2520of%2520dynamic%25203D%2520scenes.%2520Unlike%2520existing%2520dynamic%2520scene%250Areconstruction%2520methods%252C%2520which%2520rely%2520on%2520time-conditioned%2520deformation%2520networks%2520and%250Aare%2520limited%2520to%2520interpolation%2520within%2520a%2520fixed%2520time%2520window%252C%2520ODE-GS%2520eliminates%250Atimestamp%2520dependency%2520by%2520modeling%2520Gaussian%2520parameter%2520trajectories%2520as%250Acontinuous-time%2520latent%2520dynamics.%2520Our%2520approach%2520first%2520learns%2520an%2520interpolation%250Amodel%2520to%2520generate%2520accurate%2520Gaussian%2520trajectories%2520within%2520the%2520observed%2520window%252C%250Athen%2520trains%2520a%2520Transformer%2520encoder%2520to%2520aggregate%2520past%2520trajectories%2520into%2520a%2520latent%250Astate%2520evolved%2520via%2520a%2520neural%2520ODE.%2520Finally%252C%2520numerical%2520integration%2520produces%2520smooth%252C%250Aphysically%2520plausible%2520future%2520Gaussian%2520trajectories%252C%2520enabling%2520rendering%2520at%250Aarbitrary%2520future%2520timestamps.%2520On%2520the%2520D-NeRF%252C%2520NVFi%252C%2520and%2520HyperNeRF%2520benchmarks%252C%250AODE-GS%2520achieves%2520state-of-the-art%2520extrapolation%2520performance%252C%2520improving%2520metrics%250Aby%252019.8%2525%2520compared%2520to%2520leading%2520baselines%252C%2520demonstrating%2520its%2520ability%2520to%2520accurately%250Arepresent%2520and%2520predict%25203D%2520scene%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05480v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODE-GS%3A%20Latent%20ODEs%20for%20Dynamic%20Scene%20Extrapolation%20with%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Daniel%20Wang%20and%20Patrick%20Rim%20and%20Tian%20Tian%20and%20Dong%20Lao%20and%20Alex%20Wong%20and%20Ganesh%20Sundaramoorthi&entry.1292438233=%20%20We%20introduce%20ODE-GS%2C%20a%20novel%20approach%20that%20integrates%203D%20Gaussian%20Splatting%0Awith%20latent%20neural%20ordinary%20differential%20equations%20%28ODEs%29%20to%20enable%20future%0Aextrapolation%20of%20dynamic%203D%20scenes.%20Unlike%20existing%20dynamic%20scene%0Areconstruction%20methods%2C%20which%20rely%20on%20time-conditioned%20deformation%20networks%20and%0Aare%20limited%20to%20interpolation%20within%20a%20fixed%20time%20window%2C%20ODE-GS%20eliminates%0Atimestamp%20dependency%20by%20modeling%20Gaussian%20parameter%20trajectories%20as%0Acontinuous-time%20latent%20dynamics.%20Our%20approach%20first%20learns%20an%20interpolation%0Amodel%20to%20generate%20accurate%20Gaussian%20trajectories%20within%20the%20observed%20window%2C%0Athen%20trains%20a%20Transformer%20encoder%20to%20aggregate%20past%20trajectories%20into%20a%20latent%0Astate%20evolved%20via%20a%20neural%20ODE.%20Finally%2C%20numerical%20integration%20produces%20smooth%2C%0Aphysically%20plausible%20future%20Gaussian%20trajectories%2C%20enabling%20rendering%20at%0Aarbitrary%20future%20timestamps.%20On%20the%20D-NeRF%2C%20NVFi%2C%20and%20HyperNeRF%20benchmarks%2C%0AODE-GS%20achieves%20state-of-the-art%20extrapolation%20performance%2C%20improving%20metrics%0Aby%2019.8%25%20compared%20to%20leading%20baselines%2C%20demonstrating%20its%20ability%20to%20accurately%0Arepresent%20and%20predict%203D%20scene%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05480v3&entry.124074799=Read"},
{"title": "HART: Human Aligned Reconstruction Transformer", "author": "Xiyi Chen and Shaofei Wang and Marko Mihajlovic and Taewon Kang and Sergey Prokudin and Ming Lin", "abstract": "  We introduce HART, a unified framework for sparse-view human reconstruction.\nGiven a small set of uncalibrated RGB images of a person as input, it outputs a\nwatertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat\nrepresentation for photorealistic novel-view rendering. Prior methods for\nclothed human reconstruction either optimize parametric templates, which\noverlook loose garments and human-object interactions, or train implicit\nfunctions under simplified camera assumptions, limiting applicability in real\nscenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body\ncorrespondences, and employs an occlusion-aware Poisson reconstruction to\nrecover complete geometry, even in self-occluded regions. These predictions\nalso align with a parametric SMPL-X body model, ensuring that reconstructed\ngeometry remains consistent with human structure while capturing loose clothing\nand interactions. These human-aligned meshes initialize Gaussian splats to\nfurther enable sparse-view rendering. While trained on only 2.3K synthetic\nscans, HART achieves state-of-the-art results: Chamfer Distance improves by\n18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for\nSMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on\na wide range of datasets. These results suggest that feed-forward transformers\ncan serve as a scalable model for robust human reconstruction in real-world\nsettings. Code and models will be released.\n", "link": "http://arxiv.org/abs/2509.26621v1", "date": "2025-09-30", "relevancy": 3.2525, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7169}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.638}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HART%3A%20Human%20Aligned%20Reconstruction%20Transformer&body=Title%3A%20HART%3A%20Human%20Aligned%20Reconstruction%20Transformer%0AAuthor%3A%20Xiyi%20Chen%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Taewon%20Kang%20and%20Sergey%20Prokudin%20and%20Ming%20Lin%0AAbstract%3A%20%20%20We%20introduce%20HART%2C%20a%20unified%20framework%20for%20sparse-view%20human%20reconstruction.%0AGiven%20a%20small%20set%20of%20uncalibrated%20RGB%20images%20of%20a%20person%20as%20input%2C%20it%20outputs%20a%0Awatertight%20clothed%20mesh%2C%20the%20aligned%20SMPL-X%20body%20mesh%2C%20and%20a%20Gaussian-splat%0Arepresentation%20for%20photorealistic%20novel-view%20rendering.%20Prior%20methods%20for%0Aclothed%20human%20reconstruction%20either%20optimize%20parametric%20templates%2C%20which%0Aoverlook%20loose%20garments%20and%20human-object%20interactions%2C%20or%20train%20implicit%0Afunctions%20under%20simplified%20camera%20assumptions%2C%20limiting%20applicability%20in%20real%0Ascenes.%20In%20contrast%2C%20HART%20predicts%20per-pixel%203D%20point%20maps%2C%20normals%2C%20and%20body%0Acorrespondences%2C%20and%20employs%20an%20occlusion-aware%20Poisson%20reconstruction%20to%0Arecover%20complete%20geometry%2C%20even%20in%20self-occluded%20regions.%20These%20predictions%0Aalso%20align%20with%20a%20parametric%20SMPL-X%20body%20model%2C%20ensuring%20that%20reconstructed%0Ageometry%20remains%20consistent%20with%20human%20structure%20while%20capturing%20loose%20clothing%0Aand%20interactions.%20These%20human-aligned%20meshes%20initialize%20Gaussian%20splats%20to%0Afurther%20enable%20sparse-view%20rendering.%20While%20trained%20on%20only%202.3K%20synthetic%0Ascans%2C%20HART%20achieves%20state-of-the-art%20results%3A%20Chamfer%20Distance%20improves%20by%0A18-23%20percent%20for%20clothed-mesh%20reconstruction%2C%20PA-V2V%20drops%20by%206-27%20percent%20for%0ASMPL-X%20estimation%2C%20LPIPS%20decreases%20by%2015-27%20percent%20for%20novel-view%20synthesis%20on%0Aa%20wide%20range%20of%20datasets.%20These%20results%20suggest%20that%20feed-forward%20transformers%0Acan%20serve%20as%20a%20scalable%20model%20for%20robust%20human%20reconstruction%20in%20real-world%0Asettings.%20Code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHART%253A%2520Human%2520Aligned%2520Reconstruction%2520Transformer%26entry.906535625%3DXiyi%2520Chen%2520and%2520Shaofei%2520Wang%2520and%2520Marko%2520Mihajlovic%2520and%2520Taewon%2520Kang%2520and%2520Sergey%2520Prokudin%2520and%2520Ming%2520Lin%26entry.1292438233%3D%2520%2520We%2520introduce%2520HART%252C%2520a%2520unified%2520framework%2520for%2520sparse-view%2520human%2520reconstruction.%250AGiven%2520a%2520small%2520set%2520of%2520uncalibrated%2520RGB%2520images%2520of%2520a%2520person%2520as%2520input%252C%2520it%2520outputs%2520a%250Awatertight%2520clothed%2520mesh%252C%2520the%2520aligned%2520SMPL-X%2520body%2520mesh%252C%2520and%2520a%2520Gaussian-splat%250Arepresentation%2520for%2520photorealistic%2520novel-view%2520rendering.%2520Prior%2520methods%2520for%250Aclothed%2520human%2520reconstruction%2520either%2520optimize%2520parametric%2520templates%252C%2520which%250Aoverlook%2520loose%2520garments%2520and%2520human-object%2520interactions%252C%2520or%2520train%2520implicit%250Afunctions%2520under%2520simplified%2520camera%2520assumptions%252C%2520limiting%2520applicability%2520in%2520real%250Ascenes.%2520In%2520contrast%252C%2520HART%2520predicts%2520per-pixel%25203D%2520point%2520maps%252C%2520normals%252C%2520and%2520body%250Acorrespondences%252C%2520and%2520employs%2520an%2520occlusion-aware%2520Poisson%2520reconstruction%2520to%250Arecover%2520complete%2520geometry%252C%2520even%2520in%2520self-occluded%2520regions.%2520These%2520predictions%250Aalso%2520align%2520with%2520a%2520parametric%2520SMPL-X%2520body%2520model%252C%2520ensuring%2520that%2520reconstructed%250Ageometry%2520remains%2520consistent%2520with%2520human%2520structure%2520while%2520capturing%2520loose%2520clothing%250Aand%2520interactions.%2520These%2520human-aligned%2520meshes%2520initialize%2520Gaussian%2520splats%2520to%250Afurther%2520enable%2520sparse-view%2520rendering.%2520While%2520trained%2520on%2520only%25202.3K%2520synthetic%250Ascans%252C%2520HART%2520achieves%2520state-of-the-art%2520results%253A%2520Chamfer%2520Distance%2520improves%2520by%250A18-23%2520percent%2520for%2520clothed-mesh%2520reconstruction%252C%2520PA-V2V%2520drops%2520by%25206-27%2520percent%2520for%250ASMPL-X%2520estimation%252C%2520LPIPS%2520decreases%2520by%252015-27%2520percent%2520for%2520novel-view%2520synthesis%2520on%250Aa%2520wide%2520range%2520of%2520datasets.%2520These%2520results%2520suggest%2520that%2520feed-forward%2520transformers%250Acan%2520serve%2520as%2520a%2520scalable%2520model%2520for%2520robust%2520human%2520reconstruction%2520in%2520real-world%250Asettings.%2520Code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HART%3A%20Human%20Aligned%20Reconstruction%20Transformer&entry.906535625=Xiyi%20Chen%20and%20Shaofei%20Wang%20and%20Marko%20Mihajlovic%20and%20Taewon%20Kang%20and%20Sergey%20Prokudin%20and%20Ming%20Lin&entry.1292438233=%20%20We%20introduce%20HART%2C%20a%20unified%20framework%20for%20sparse-view%20human%20reconstruction.%0AGiven%20a%20small%20set%20of%20uncalibrated%20RGB%20images%20of%20a%20person%20as%20input%2C%20it%20outputs%20a%0Awatertight%20clothed%20mesh%2C%20the%20aligned%20SMPL-X%20body%20mesh%2C%20and%20a%20Gaussian-splat%0Arepresentation%20for%20photorealistic%20novel-view%20rendering.%20Prior%20methods%20for%0Aclothed%20human%20reconstruction%20either%20optimize%20parametric%20templates%2C%20which%0Aoverlook%20loose%20garments%20and%20human-object%20interactions%2C%20or%20train%20implicit%0Afunctions%20under%20simplified%20camera%20assumptions%2C%20limiting%20applicability%20in%20real%0Ascenes.%20In%20contrast%2C%20HART%20predicts%20per-pixel%203D%20point%20maps%2C%20normals%2C%20and%20body%0Acorrespondences%2C%20and%20employs%20an%20occlusion-aware%20Poisson%20reconstruction%20to%0Arecover%20complete%20geometry%2C%20even%20in%20self-occluded%20regions.%20These%20predictions%0Aalso%20align%20with%20a%20parametric%20SMPL-X%20body%20model%2C%20ensuring%20that%20reconstructed%0Ageometry%20remains%20consistent%20with%20human%20structure%20while%20capturing%20loose%20clothing%0Aand%20interactions.%20These%20human-aligned%20meshes%20initialize%20Gaussian%20splats%20to%0Afurther%20enable%20sparse-view%20rendering.%20While%20trained%20on%20only%202.3K%20synthetic%0Ascans%2C%20HART%20achieves%20state-of-the-art%20results%3A%20Chamfer%20Distance%20improves%20by%0A18-23%20percent%20for%20clothed-mesh%20reconstruction%2C%20PA-V2V%20drops%20by%206-27%20percent%20for%0ASMPL-X%20estimation%2C%20LPIPS%20decreases%20by%2015-27%20percent%20for%20novel-view%20synthesis%20on%0Aa%20wide%20range%20of%20datasets.%20These%20results%20suggest%20that%20feed-forward%20transformers%0Acan%20serve%20as%20a%20scalable%20model%20for%20robust%20human%20reconstruction%20in%20real-world%0Asettings.%20Code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26621v1&entry.124074799=Read"},
{"title": "TTT3R: 3D Reconstruction as Test-Time Training", "author": "Xingyu Chen and Yue Chen and Yuliang Xiu and Andreas Geiger and Anpei Chen", "abstract": "  Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R\n", "link": "http://arxiv.org/abs/2509.26645v1", "date": "2025-09-30", "relevancy": 3.0765, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6095}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&body=Title%3A%20TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training%0AAuthor%3A%20Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen%0AAbstract%3A%20%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTT3R%253A%25203D%2520Reconstruction%2520as%2520Test-Time%2520Training%26entry.906535625%3DXingyu%2520Chen%2520and%2520Yue%2520Chen%2520and%2520Yuliang%2520Xiu%2520and%2520Andreas%2520Geiger%2520and%2520Anpei%2520Chen%26entry.1292438233%3D%2520%2520Modern%2520Recurrent%2520Neural%2520Networks%2520have%2520become%2520a%2520competitive%2520architecture%2520for%250A3D%2520reconstruction%2520due%2520to%2520their%2520linear-time%2520complexity.%2520However%252C%2520their%250Aperformance%2520degrades%2520significantly%2520when%2520applied%2520beyond%2520the%2520training%2520context%250Alength%252C%2520revealing%2520limited%2520length%2520generalization.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%250A3D%2520reconstruction%2520foundation%2520models%2520from%2520a%2520Test-Time%2520Training%2520perspective%252C%250Aframing%2520their%2520designs%2520as%2520an%2520online%2520learning%2520problem.%2520Building%2520on%2520this%250Aperspective%252C%2520we%2520leverage%2520the%2520alignment%2520confidence%2520between%2520the%2520memory%2520state%2520and%250Aincoming%2520observations%2520to%2520derive%2520a%2520closed-form%2520learning%2520rate%2520for%2520memory%2520updates%252C%250Ato%2520balance%2520between%2520retaining%2520historical%2520information%2520and%2520adapting%2520to%2520new%250Aobservations.%2520This%2520training-free%2520intervention%252C%2520termed%2520TTT3R%252C%2520substantially%250Aimproves%2520length%2520generalization%252C%2520achieving%2520a%2520%25242%255Ctimes%2524%2520improvement%2520in%2520global%250Apose%2520estimation%2520over%2520baselines%252C%2520while%2520operating%2520at%252020%2520FPS%2520with%2520just%25206%2520GB%2520of%2520GPU%250Amemory%2520to%2520process%2520thousands%2520of%2520images.%2520Code%2520available%2520in%250Ahttps%253A//rover-xingyu.github.io/TTT3R%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT3R%3A%203D%20Reconstruction%20as%20Test-Time%20Training&entry.906535625=Xingyu%20Chen%20and%20Yue%20Chen%20and%20Yuliang%20Xiu%20and%20Andreas%20Geiger%20and%20Anpei%20Chen&entry.1292438233=%20%20Modern%20Recurrent%20Neural%20Networks%20have%20become%20a%20competitive%20architecture%20for%0A3D%20reconstruction%20due%20to%20their%20linear-time%20complexity.%20However%2C%20their%0Aperformance%20degrades%20significantly%20when%20applied%20beyond%20the%20training%20context%0Alength%2C%20revealing%20limited%20length%20generalization.%20In%20this%20work%2C%20we%20revisit%20the%0A3D%20reconstruction%20foundation%20models%20from%20a%20Test-Time%20Training%20perspective%2C%0Aframing%20their%20designs%20as%20an%20online%20learning%20problem.%20Building%20on%20this%0Aperspective%2C%20we%20leverage%20the%20alignment%20confidence%20between%20the%20memory%20state%20and%0Aincoming%20observations%20to%20derive%20a%20closed-form%20learning%20rate%20for%20memory%20updates%2C%0Ato%20balance%20between%20retaining%20historical%20information%20and%20adapting%20to%20new%0Aobservations.%20This%20training-free%20intervention%2C%20termed%20TTT3R%2C%20substantially%0Aimproves%20length%20generalization%2C%20achieving%20a%20%242%5Ctimes%24%20improvement%20in%20global%0Apose%20estimation%20over%20baselines%2C%20while%20operating%20at%2020%20FPS%20with%20just%206%20GB%20of%20GPU%0Amemory%20to%20process%20thousands%20of%20images.%20Code%20available%20in%0Ahttps%3A//rover-xingyu.github.io/TTT3R%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26645v1&entry.124074799=Read"},
{"title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training", "author": "Junlin Han and Shengbang Tong and David Fan and Yufan Ren and Koustuv Sinha and Philip Torr and Filippos Kokkinos", "abstract": "  Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.\n", "link": "http://arxiv.org/abs/2509.26625v1", "date": "2025-09-30", "relevancy": 3.0683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20See%20Before%20Seeing%3A%20Demystifying%20LLM%20Visual%20Priors%20from%0A%20%20Language%20Pre-training&body=Title%3A%20Learning%20to%20See%20Before%20Seeing%3A%20Demystifying%20LLM%20Visual%20Priors%20from%0A%20%20Language%20Pre-training%0AAuthor%3A%20Junlin%20Han%20and%20Shengbang%20Tong%20and%20David%20Fan%20and%20Yufan%20Ren%20and%20Koustuv%20Sinha%20and%20Philip%20Torr%20and%20Filippos%20Kokkinos%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20being%20trained%20on%20text%20alone%2C%0Asurprisingly%20develop%20rich%20visual%20priors.%20These%20priors%20allow%20latent%20visual%0Acapabilities%20to%20be%20unlocked%20for%20vision%20tasks%20with%20a%20relatively%20small%20amount%20of%0Amultimodal%20data%2C%20and%20in%20some%20cases%2C%20to%20perform%20visual%20tasks%20without%20ever%20having%0Aseen%20an%20image.%20Through%20systematic%20analysis%2C%20we%20reveal%20that%20visual%20priors-the%0Aimplicit%2C%20emergent%20knowledge%20about%20the%20visual%20world%20acquired%20during%20language%0Apre-training-are%20composed%20of%20separable%20perception%20and%20reasoning%20priors%20with%0Aunique%20scaling%20trends%20and%20origins.%20We%20show%20that%20an%20LLM%27s%20latent%20visual%0Areasoning%20ability%20is%20predominantly%20developed%20by%20pre-training%20on%0Areasoning-centric%20data%20%28e.g.%2C%20code%2C%20math%2C%20academia%29%20and%20scales%20progressively.%0AThis%20reasoning%20prior%20acquired%20from%20language%20pre-training%20is%20transferable%20and%0Auniversally%20applicable%20to%20visual%20reasoning.%20In%20contrast%2C%20a%20perception%20prior%0Aemerges%20more%20diffusely%20from%20broad%20corpora%2C%20and%20perception%20ability%20is%20more%0Asensitive%20to%20the%20vision%20encoder%20and%20visual%20instruction%20tuning%20data.%20In%0Aparallel%2C%20text%20describing%20the%20visual%20world%20proves%20crucial%2C%20though%20its%0Aperformance%20impact%20saturates%20rapidly.%20Leveraging%20these%20insights%2C%20we%20propose%20a%0Adata-centric%20recipe%20for%20pre-training%20vision-aware%20LLMs%20and%20verify%20it%20in%201T%0Atoken%20scale%20pre-training.%20Our%20findings%20are%20grounded%20in%20over%20100%20controlled%0Aexperiments%20consuming%20500%2C000%20GPU-hours%2C%20spanning%20the%20full%20MLLM%20construction%0Apipeline-from%20LLM%20pre-training%20to%20visual%20alignment%20and%20supervised%20multimodal%0Afine-tuning-across%20five%20model%20scales%2C%20a%20wide%20range%20of%20data%20categories%20and%0Amixtures%2C%20and%20multiple%20adaptation%20setups.%20Along%20with%20our%20main%20findings%2C%20we%0Apropose%20and%20investigate%20several%20hypotheses%2C%20and%20introduce%20the%20Multi-Level%0AExistence%20Bench%20%28MLE-Bench%29.%20Together%2C%20this%20work%20provides%20a%20new%20way%20of%0Adeliberately%20cultivating%20visual%20priors%20from%20language%20pre-training%2C%20paving%20the%0Away%20for%20the%20next%20generation%20of%20multimodal%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520See%2520Before%2520Seeing%253A%2520Demystifying%2520LLM%2520Visual%2520Priors%2520from%250A%2520%2520Language%2520Pre-training%26entry.906535625%3DJunlin%2520Han%2520and%2520Shengbang%2520Tong%2520and%2520David%2520Fan%2520and%2520Yufan%2520Ren%2520and%2520Koustuv%2520Sinha%2520and%2520Philip%2520Torr%2520and%2520Filippos%2520Kokkinos%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520despite%2520being%2520trained%2520on%2520text%2520alone%252C%250Asurprisingly%2520develop%2520rich%2520visual%2520priors.%2520These%2520priors%2520allow%2520latent%2520visual%250Acapabilities%2520to%2520be%2520unlocked%2520for%2520vision%2520tasks%2520with%2520a%2520relatively%2520small%2520amount%2520of%250Amultimodal%2520data%252C%2520and%2520in%2520some%2520cases%252C%2520to%2520perform%2520visual%2520tasks%2520without%2520ever%2520having%250Aseen%2520an%2520image.%2520Through%2520systematic%2520analysis%252C%2520we%2520reveal%2520that%2520visual%2520priors-the%250Aimplicit%252C%2520emergent%2520knowledge%2520about%2520the%2520visual%2520world%2520acquired%2520during%2520language%250Apre-training-are%2520composed%2520of%2520separable%2520perception%2520and%2520reasoning%2520priors%2520with%250Aunique%2520scaling%2520trends%2520and%2520origins.%2520We%2520show%2520that%2520an%2520LLM%2527s%2520latent%2520visual%250Areasoning%2520ability%2520is%2520predominantly%2520developed%2520by%2520pre-training%2520on%250Areasoning-centric%2520data%2520%2528e.g.%252C%2520code%252C%2520math%252C%2520academia%2529%2520and%2520scales%2520progressively.%250AThis%2520reasoning%2520prior%2520acquired%2520from%2520language%2520pre-training%2520is%2520transferable%2520and%250Auniversally%2520applicable%2520to%2520visual%2520reasoning.%2520In%2520contrast%252C%2520a%2520perception%2520prior%250Aemerges%2520more%2520diffusely%2520from%2520broad%2520corpora%252C%2520and%2520perception%2520ability%2520is%2520more%250Asensitive%2520to%2520the%2520vision%2520encoder%2520and%2520visual%2520instruction%2520tuning%2520data.%2520In%250Aparallel%252C%2520text%2520describing%2520the%2520visual%2520world%2520proves%2520crucial%252C%2520though%2520its%250Aperformance%2520impact%2520saturates%2520rapidly.%2520Leveraging%2520these%2520insights%252C%2520we%2520propose%2520a%250Adata-centric%2520recipe%2520for%2520pre-training%2520vision-aware%2520LLMs%2520and%2520verify%2520it%2520in%25201T%250Atoken%2520scale%2520pre-training.%2520Our%2520findings%2520are%2520grounded%2520in%2520over%2520100%2520controlled%250Aexperiments%2520consuming%2520500%252C000%2520GPU-hours%252C%2520spanning%2520the%2520full%2520MLLM%2520construction%250Apipeline-from%2520LLM%2520pre-training%2520to%2520visual%2520alignment%2520and%2520supervised%2520multimodal%250Afine-tuning-across%2520five%2520model%2520scales%252C%2520a%2520wide%2520range%2520of%2520data%2520categories%2520and%250Amixtures%252C%2520and%2520multiple%2520adaptation%2520setups.%2520Along%2520with%2520our%2520main%2520findings%252C%2520we%250Apropose%2520and%2520investigate%2520several%2520hypotheses%252C%2520and%2520introduce%2520the%2520Multi-Level%250AExistence%2520Bench%2520%2528MLE-Bench%2529.%2520Together%252C%2520this%2520work%2520provides%2520a%2520new%2520way%2520of%250Adeliberately%2520cultivating%2520visual%2520priors%2520from%2520language%2520pre-training%252C%2520paving%2520the%250Away%2520for%2520the%2520next%2520generation%2520of%2520multimodal%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20See%20Before%20Seeing%3A%20Demystifying%20LLM%20Visual%20Priors%20from%0A%20%20Language%20Pre-training&entry.906535625=Junlin%20Han%20and%20Shengbang%20Tong%20and%20David%20Fan%20and%20Yufan%20Ren%20and%20Koustuv%20Sinha%20and%20Philip%20Torr%20and%20Filippos%20Kokkinos&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20being%20trained%20on%20text%20alone%2C%0Asurprisingly%20develop%20rich%20visual%20priors.%20These%20priors%20allow%20latent%20visual%0Acapabilities%20to%20be%20unlocked%20for%20vision%20tasks%20with%20a%20relatively%20small%20amount%20of%0Amultimodal%20data%2C%20and%20in%20some%20cases%2C%20to%20perform%20visual%20tasks%20without%20ever%20having%0Aseen%20an%20image.%20Through%20systematic%20analysis%2C%20we%20reveal%20that%20visual%20priors-the%0Aimplicit%2C%20emergent%20knowledge%20about%20the%20visual%20world%20acquired%20during%20language%0Apre-training-are%20composed%20of%20separable%20perception%20and%20reasoning%20priors%20with%0Aunique%20scaling%20trends%20and%20origins.%20We%20show%20that%20an%20LLM%27s%20latent%20visual%0Areasoning%20ability%20is%20predominantly%20developed%20by%20pre-training%20on%0Areasoning-centric%20data%20%28e.g.%2C%20code%2C%20math%2C%20academia%29%20and%20scales%20progressively.%0AThis%20reasoning%20prior%20acquired%20from%20language%20pre-training%20is%20transferable%20and%0Auniversally%20applicable%20to%20visual%20reasoning.%20In%20contrast%2C%20a%20perception%20prior%0Aemerges%20more%20diffusely%20from%20broad%20corpora%2C%20and%20perception%20ability%20is%20more%0Asensitive%20to%20the%20vision%20encoder%20and%20visual%20instruction%20tuning%20data.%20In%0Aparallel%2C%20text%20describing%20the%20visual%20world%20proves%20crucial%2C%20though%20its%0Aperformance%20impact%20saturates%20rapidly.%20Leveraging%20these%20insights%2C%20we%20propose%20a%0Adata-centric%20recipe%20for%20pre-training%20vision-aware%20LLMs%20and%20verify%20it%20in%201T%0Atoken%20scale%20pre-training.%20Our%20findings%20are%20grounded%20in%20over%20100%20controlled%0Aexperiments%20consuming%20500%2C000%20GPU-hours%2C%20spanning%20the%20full%20MLLM%20construction%0Apipeline-from%20LLM%20pre-training%20to%20visual%20alignment%20and%20supervised%20multimodal%0Afine-tuning-across%20five%20model%20scales%2C%20a%20wide%20range%20of%20data%20categories%20and%0Amixtures%2C%20and%20multiple%20adaptation%20setups.%20Along%20with%20our%20main%20findings%2C%20we%0Apropose%20and%20investigate%20several%20hypotheses%2C%20and%20introduce%20the%20Multi-Level%0AExistence%20Bench%20%28MLE-Bench%29.%20Together%2C%20this%20work%20provides%20a%20new%20way%20of%0Adeliberately%20cultivating%20visual%20priors%20from%20language%20pre-training%2C%20paving%20the%0Away%20for%20the%20next%20generation%20of%20multimodal%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26625v1&entry.124074799=Read"},
{"title": "Clarification as Supervision: Reinforcement Learning for Vision-Language\n  Interfaces", "author": "John Gkountouras and Ivan Titov", "abstract": "  Recent text-only models demonstrate remarkable mathematical reasoning\ncapabilities. Extending these to visual domains requires vision-language models\nto translate images into text descriptions. However, current models, trained to\nproduce captions for human readers, often omit the precise details that\nreasoning systems require. This creates an interface mismatch: reasoners often\nfail not due to reasoning limitations but because they lack access to critical\nvisual information. We propose Adaptive-Clarification Reinforcement Learning\n(AC-RL), which teaches vision models what information reasoners need through\ninteraction. Our key insight is that clarification requests during training\nreveal information gaps; by penalizing success that requires clarification, we\ncreate pressure for comprehensive initial captions that enable the reasoner to\nsolve the problem in a single pass. AC-RL improves average accuracy by 4.4\npoints over pretrained baselines across seven visual mathematical reasoning\nbenchmarks, and analysis shows it would cut clarification requests by up to 39%\nif those were allowed. By treating clarification as a form of implicit\nsupervision, AC-RL demonstrates that vision-language interfaces can be\neffectively learned through interaction alone, without requiring explicit\nannotations.\n", "link": "http://arxiv.org/abs/2509.26594v1", "date": "2025-09-30", "relevancy": 2.9847, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6187}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clarification%20as%20Supervision%3A%20Reinforcement%20Learning%20for%20Vision-Language%0A%20%20Interfaces&body=Title%3A%20Clarification%20as%20Supervision%3A%20Reinforcement%20Learning%20for%20Vision-Language%0A%20%20Interfaces%0AAuthor%3A%20John%20Gkountouras%20and%20Ivan%20Titov%0AAbstract%3A%20%20%20Recent%20text-only%20models%20demonstrate%20remarkable%20mathematical%20reasoning%0Acapabilities.%20Extending%20these%20to%20visual%20domains%20requires%20vision-language%20models%0Ato%20translate%20images%20into%20text%20descriptions.%20However%2C%20current%20models%2C%20trained%20to%0Aproduce%20captions%20for%20human%20readers%2C%20often%20omit%20the%20precise%20details%20that%0Areasoning%20systems%20require.%20This%20creates%20an%20interface%20mismatch%3A%20reasoners%20often%0Afail%20not%20due%20to%20reasoning%20limitations%20but%20because%20they%20lack%20access%20to%20critical%0Avisual%20information.%20We%20propose%20Adaptive-Clarification%20Reinforcement%20Learning%0A%28AC-RL%29%2C%20which%20teaches%20vision%20models%20what%20information%20reasoners%20need%20through%0Ainteraction.%20Our%20key%20insight%20is%20that%20clarification%20requests%20during%20training%0Areveal%20information%20gaps%3B%20by%20penalizing%20success%20that%20requires%20clarification%2C%20we%0Acreate%20pressure%20for%20comprehensive%20initial%20captions%20that%20enable%20the%20reasoner%20to%0Asolve%20the%20problem%20in%20a%20single%20pass.%20AC-RL%20improves%20average%20accuracy%20by%204.4%0Apoints%20over%20pretrained%20baselines%20across%20seven%20visual%20mathematical%20reasoning%0Abenchmarks%2C%20and%20analysis%20shows%20it%20would%20cut%20clarification%20requests%20by%20up%20to%2039%25%0Aif%20those%20were%20allowed.%20By%20treating%20clarification%20as%20a%20form%20of%20implicit%0Asupervision%2C%20AC-RL%20demonstrates%20that%20vision-language%20interfaces%20can%20be%0Aeffectively%20learned%20through%20interaction%20alone%2C%20without%20requiring%20explicit%0Aannotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClarification%2520as%2520Supervision%253A%2520Reinforcement%2520Learning%2520for%2520Vision-Language%250A%2520%2520Interfaces%26entry.906535625%3DJohn%2520Gkountouras%2520and%2520Ivan%2520Titov%26entry.1292438233%3D%2520%2520Recent%2520text-only%2520models%2520demonstrate%2520remarkable%2520mathematical%2520reasoning%250Acapabilities.%2520Extending%2520these%2520to%2520visual%2520domains%2520requires%2520vision-language%2520models%250Ato%2520translate%2520images%2520into%2520text%2520descriptions.%2520However%252C%2520current%2520models%252C%2520trained%2520to%250Aproduce%2520captions%2520for%2520human%2520readers%252C%2520often%2520omit%2520the%2520precise%2520details%2520that%250Areasoning%2520systems%2520require.%2520This%2520creates%2520an%2520interface%2520mismatch%253A%2520reasoners%2520often%250Afail%2520not%2520due%2520to%2520reasoning%2520limitations%2520but%2520because%2520they%2520lack%2520access%2520to%2520critical%250Avisual%2520information.%2520We%2520propose%2520Adaptive-Clarification%2520Reinforcement%2520Learning%250A%2528AC-RL%2529%252C%2520which%2520teaches%2520vision%2520models%2520what%2520information%2520reasoners%2520need%2520through%250Ainteraction.%2520Our%2520key%2520insight%2520is%2520that%2520clarification%2520requests%2520during%2520training%250Areveal%2520information%2520gaps%253B%2520by%2520penalizing%2520success%2520that%2520requires%2520clarification%252C%2520we%250Acreate%2520pressure%2520for%2520comprehensive%2520initial%2520captions%2520that%2520enable%2520the%2520reasoner%2520to%250Asolve%2520the%2520problem%2520in%2520a%2520single%2520pass.%2520AC-RL%2520improves%2520average%2520accuracy%2520by%25204.4%250Apoints%2520over%2520pretrained%2520baselines%2520across%2520seven%2520visual%2520mathematical%2520reasoning%250Abenchmarks%252C%2520and%2520analysis%2520shows%2520it%2520would%2520cut%2520clarification%2520requests%2520by%2520up%2520to%252039%2525%250Aif%2520those%2520were%2520allowed.%2520By%2520treating%2520clarification%2520as%2520a%2520form%2520of%2520implicit%250Asupervision%252C%2520AC-RL%2520demonstrates%2520that%2520vision-language%2520interfaces%2520can%2520be%250Aeffectively%2520learned%2520through%2520interaction%2520alone%252C%2520without%2520requiring%2520explicit%250Aannotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clarification%20as%20Supervision%3A%20Reinforcement%20Learning%20for%20Vision-Language%0A%20%20Interfaces&entry.906535625=John%20Gkountouras%20and%20Ivan%20Titov&entry.1292438233=%20%20Recent%20text-only%20models%20demonstrate%20remarkable%20mathematical%20reasoning%0Acapabilities.%20Extending%20these%20to%20visual%20domains%20requires%20vision-language%20models%0Ato%20translate%20images%20into%20text%20descriptions.%20However%2C%20current%20models%2C%20trained%20to%0Aproduce%20captions%20for%20human%20readers%2C%20often%20omit%20the%20precise%20details%20that%0Areasoning%20systems%20require.%20This%20creates%20an%20interface%20mismatch%3A%20reasoners%20often%0Afail%20not%20due%20to%20reasoning%20limitations%20but%20because%20they%20lack%20access%20to%20critical%0Avisual%20information.%20We%20propose%20Adaptive-Clarification%20Reinforcement%20Learning%0A%28AC-RL%29%2C%20which%20teaches%20vision%20models%20what%20information%20reasoners%20need%20through%0Ainteraction.%20Our%20key%20insight%20is%20that%20clarification%20requests%20during%20training%0Areveal%20information%20gaps%3B%20by%20penalizing%20success%20that%20requires%20clarification%2C%20we%0Acreate%20pressure%20for%20comprehensive%20initial%20captions%20that%20enable%20the%20reasoner%20to%0Asolve%20the%20problem%20in%20a%20single%20pass.%20AC-RL%20improves%20average%20accuracy%20by%204.4%0Apoints%20over%20pretrained%20baselines%20across%20seven%20visual%20mathematical%20reasoning%0Abenchmarks%2C%20and%20analysis%20shows%20it%20would%20cut%20clarification%20requests%20by%20up%20to%2039%25%0Aif%20those%20were%20allowed.%20By%20treating%20clarification%20as%20a%20form%20of%20implicit%0Asupervision%2C%20AC-RL%20demonstrates%20that%20vision-language%20interfaces%20can%20be%0Aeffectively%20learned%20through%20interaction%20alone%2C%20without%20requiring%20explicit%0Aannotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26594v1&entry.124074799=Read"},
{"title": "Learning Generalizable Shape Completion with SIM(3) Equivariance", "author": "Yuqing Wang and Zhaiyu Chen and Xiao Xiang Zhu", "abstract": "  3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.\n", "link": "http://arxiv.org/abs/2509.26631v1", "date": "2025-09-30", "relevancy": 2.9698, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6358}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5833}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generalizable%20Shape%20Completion%20with%20SIM%283%29%20Equivariance&body=Title%3A%20Learning%20Generalizable%20Shape%20Completion%20with%20SIM%283%29%20Equivariance%0AAuthor%3A%20Yuqing%20Wang%20and%20Zhaiyu%20Chen%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%203D%20shape%20completion%20methods%20typically%20assume%20scans%20are%20pre-aligned%20to%20a%0Acanonical%20frame.%20This%20leaks%20pose%20and%20scale%20cues%20that%20networks%20may%20exploit%20to%0Amemorize%20absolute%20positions%20rather%20than%20inferring%20intrinsic%20geometry.%20When%20such%0Aalignment%20is%20absent%20in%20real%20data%2C%20performance%20collapses.%20We%20argue%20that%20robust%0Ageneralization%20demands%20architectural%20equivariance%20to%20the%20similarity%20group%2C%0ASIM%283%29%2C%20so%20the%20model%20remains%20agnostic%20to%20pose%20and%20scale.%20Following%20this%0Aprinciple%2C%20we%20introduce%20the%20first%20SIM%283%29-equivariant%20shape%20completion%20network%2C%0Awhose%20modular%20layers%20successively%20canonicalize%20features%2C%20reason%20over%0Asimilarity-invariant%20geometry%2C%20and%20restore%20the%20original%20frame.%20Under%20a%0Ade-biased%20evaluation%20protocol%20that%20removes%20the%20hidden%20cues%2C%20our%20model%0Aoutperforms%20both%20equivariant%20and%20augmentation%20baselines%20on%20the%20PCN%20benchmark.%0AIt%20also%20sets%20new%20cross-domain%20records%20on%20real%20driving%20and%20indoor%20scans%2C%0Alowering%20minimal%20matching%20distance%20on%20KITTI%20by%2017%25%20and%20Chamfer%20distance%20%24%5Cell1%24%0Aon%20OmniObject3D%20by%2014%25.%20Perhaps%20surprisingly%2C%20ours%20under%20the%20stricter%20protocol%0Astill%20outperforms%20competitors%20under%20their%20biased%20settings.%20These%20results%0Aestablish%20full%20SIM%283%29%20equivariance%20as%20an%20effective%20route%20to%20truly%20generalizable%0Ashape%20completion.%20Project%20page%3A%20https%3A//sime-completion.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generalizable%2520Shape%2520Completion%2520with%2520SIM%25283%2529%2520Equivariance%26entry.906535625%3DYuqing%2520Wang%2520and%2520Zhaiyu%2520Chen%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%25203D%2520shape%2520completion%2520methods%2520typically%2520assume%2520scans%2520are%2520pre-aligned%2520to%2520a%250Acanonical%2520frame.%2520This%2520leaks%2520pose%2520and%2520scale%2520cues%2520that%2520networks%2520may%2520exploit%2520to%250Amemorize%2520absolute%2520positions%2520rather%2520than%2520inferring%2520intrinsic%2520geometry.%2520When%2520such%250Aalignment%2520is%2520absent%2520in%2520real%2520data%252C%2520performance%2520collapses.%2520We%2520argue%2520that%2520robust%250Ageneralization%2520demands%2520architectural%2520equivariance%2520to%2520the%2520similarity%2520group%252C%250ASIM%25283%2529%252C%2520so%2520the%2520model%2520remains%2520agnostic%2520to%2520pose%2520and%2520scale.%2520Following%2520this%250Aprinciple%252C%2520we%2520introduce%2520the%2520first%2520SIM%25283%2529-equivariant%2520shape%2520completion%2520network%252C%250Awhose%2520modular%2520layers%2520successively%2520canonicalize%2520features%252C%2520reason%2520over%250Asimilarity-invariant%2520geometry%252C%2520and%2520restore%2520the%2520original%2520frame.%2520Under%2520a%250Ade-biased%2520evaluation%2520protocol%2520that%2520removes%2520the%2520hidden%2520cues%252C%2520our%2520model%250Aoutperforms%2520both%2520equivariant%2520and%2520augmentation%2520baselines%2520on%2520the%2520PCN%2520benchmark.%250AIt%2520also%2520sets%2520new%2520cross-domain%2520records%2520on%2520real%2520driving%2520and%2520indoor%2520scans%252C%250Alowering%2520minimal%2520matching%2520distance%2520on%2520KITTI%2520by%252017%2525%2520and%2520Chamfer%2520distance%2520%2524%255Cell1%2524%250Aon%2520OmniObject3D%2520by%252014%2525.%2520Perhaps%2520surprisingly%252C%2520ours%2520under%2520the%2520stricter%2520protocol%250Astill%2520outperforms%2520competitors%2520under%2520their%2520biased%2520settings.%2520These%2520results%250Aestablish%2520full%2520SIM%25283%2529%2520equivariance%2520as%2520an%2520effective%2520route%2520to%2520truly%2520generalizable%250Ashape%2520completion.%2520Project%2520page%253A%2520https%253A//sime-completion.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generalizable%20Shape%20Completion%20with%20SIM%283%29%20Equivariance&entry.906535625=Yuqing%20Wang%20and%20Zhaiyu%20Chen%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%203D%20shape%20completion%20methods%20typically%20assume%20scans%20are%20pre-aligned%20to%20a%0Acanonical%20frame.%20This%20leaks%20pose%20and%20scale%20cues%20that%20networks%20may%20exploit%20to%0Amemorize%20absolute%20positions%20rather%20than%20inferring%20intrinsic%20geometry.%20When%20such%0Aalignment%20is%20absent%20in%20real%20data%2C%20performance%20collapses.%20We%20argue%20that%20robust%0Ageneralization%20demands%20architectural%20equivariance%20to%20the%20similarity%20group%2C%0ASIM%283%29%2C%20so%20the%20model%20remains%20agnostic%20to%20pose%20and%20scale.%20Following%20this%0Aprinciple%2C%20we%20introduce%20the%20first%20SIM%283%29-equivariant%20shape%20completion%20network%2C%0Awhose%20modular%20layers%20successively%20canonicalize%20features%2C%20reason%20over%0Asimilarity-invariant%20geometry%2C%20and%20restore%20the%20original%20frame.%20Under%20a%0Ade-biased%20evaluation%20protocol%20that%20removes%20the%20hidden%20cues%2C%20our%20model%0Aoutperforms%20both%20equivariant%20and%20augmentation%20baselines%20on%20the%20PCN%20benchmark.%0AIt%20also%20sets%20new%20cross-domain%20records%20on%20real%20driving%20and%20indoor%20scans%2C%0Alowering%20minimal%20matching%20distance%20on%20KITTI%20by%2017%25%20and%20Chamfer%20distance%20%24%5Cell1%24%0Aon%20OmniObject3D%20by%2014%25.%20Perhaps%20surprisingly%2C%20ours%20under%20the%20stricter%20protocol%0Astill%20outperforms%20competitors%20under%20their%20biased%20settings.%20These%20results%0Aestablish%20full%20SIM%283%29%20equivariance%20as%20an%20effective%20route%20to%20truly%20generalizable%0Ashape%20completion.%20Project%20page%3A%20https%3A//sime-completion.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26631v1&entry.124074799=Read"},
{"title": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling\n  Native-like Quality Across 47 Languages", "author": "Chenxi Whitehouse and Sebastian Ruder and Tony Lin and Oksana Kurylo and Haruka Takagi and Janice Lam and Nicol\u00f2 Busetto and Denise Diaz", "abstract": "  Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation.\n", "link": "http://arxiv.org/abs/2509.26601v1", "date": "2025-09-30", "relevancy": 2.6524, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MENLO%3A%20From%20Preferences%20to%20Proficiency%20--%20Evaluating%20and%20Modeling%0A%20%20Native-like%20Quality%20Across%2047%20Languages&body=Title%3A%20MENLO%3A%20From%20Preferences%20to%20Proficiency%20--%20Evaluating%20and%20Modeling%0A%20%20Native-like%20Quality%20Across%2047%20Languages%0AAuthor%3A%20Chenxi%20Whitehouse%20and%20Sebastian%20Ruder%20and%20Tony%20Lin%20and%20Oksana%20Kurylo%20and%20Haruka%20Takagi%20and%20Janice%20Lam%20and%20Nicol%C3%B2%20Busetto%20and%20Denise%20Diaz%0AAbstract%3A%20%20%20Ensuring%20native-like%20quality%20of%20large%20language%20model%20%28LLM%29%20responses%20across%0Amany%20languages%20is%20challenging.%20To%20address%20this%2C%20we%20introduce%20MENLO%2C%20a%20framework%0Athat%20operationalizes%20the%20evaluation%20of%20native-like%20response%20quality%20based%20on%0Aaudience%20design-inspired%20mechanisms.%20Using%20MENLO%2C%20we%20create%20a%20dataset%20of%206%2C423%0Ahuman-annotated%20prompt-response%20preference%20pairs%20covering%20four%20quality%0Adimensions%20with%20high%20inter-annotator%20agreement%20in%2047%20language%20varieties.%20Our%0Aevaluation%20reveals%20that%20zero-shot%20LLM%20judges%20benefit%20significantly%20from%0Apairwise%20evaluation%20and%20our%20structured%20annotation%20rubrics%2C%20yet%20they%20still%0Aunderperform%20human%20annotators%20on%20our%20dataset.%20We%20demonstrate%20substantial%0Aimprovements%20through%20fine-tuning%20with%20reinforcement%20learning%2C%20reward%20shaping%2C%0Aand%20multi-task%20learning%20approaches.%20Additionally%2C%20we%20show%20that%20RL-trained%0Ajudges%20can%20serve%20as%20generative%20reward%20models%20to%20enhance%20LLMs%27%20multilingual%0Aproficiency%2C%20though%20discrepancies%20with%20human%20judgment%20remain.%20Our%20findings%0Asuggest%20promising%20directions%20for%20scalable%20multilingual%20evaluation%20and%0Apreference%20alignment.%20We%20release%20our%20dataset%20and%20evaluation%20framework%20to%0Asupport%20further%20research%20in%20multilingual%20LLM%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMENLO%253A%2520From%2520Preferences%2520to%2520Proficiency%2520--%2520Evaluating%2520and%2520Modeling%250A%2520%2520Native-like%2520Quality%2520Across%252047%2520Languages%26entry.906535625%3DChenxi%2520Whitehouse%2520and%2520Sebastian%2520Ruder%2520and%2520Tony%2520Lin%2520and%2520Oksana%2520Kurylo%2520and%2520Haruka%2520Takagi%2520and%2520Janice%2520Lam%2520and%2520Nicol%25C3%25B2%2520Busetto%2520and%2520Denise%2520Diaz%26entry.1292438233%3D%2520%2520Ensuring%2520native-like%2520quality%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520responses%2520across%250Amany%2520languages%2520is%2520challenging.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MENLO%252C%2520a%2520framework%250Athat%2520operationalizes%2520the%2520evaluation%2520of%2520native-like%2520response%2520quality%2520based%2520on%250Aaudience%2520design-inspired%2520mechanisms.%2520Using%2520MENLO%252C%2520we%2520create%2520a%2520dataset%2520of%25206%252C423%250Ahuman-annotated%2520prompt-response%2520preference%2520pairs%2520covering%2520four%2520quality%250Adimensions%2520with%2520high%2520inter-annotator%2520agreement%2520in%252047%2520language%2520varieties.%2520Our%250Aevaluation%2520reveals%2520that%2520zero-shot%2520LLM%2520judges%2520benefit%2520significantly%2520from%250Apairwise%2520evaluation%2520and%2520our%2520structured%2520annotation%2520rubrics%252C%2520yet%2520they%2520still%250Aunderperform%2520human%2520annotators%2520on%2520our%2520dataset.%2520We%2520demonstrate%2520substantial%250Aimprovements%2520through%2520fine-tuning%2520with%2520reinforcement%2520learning%252C%2520reward%2520shaping%252C%250Aand%2520multi-task%2520learning%2520approaches.%2520Additionally%252C%2520we%2520show%2520that%2520RL-trained%250Ajudges%2520can%2520serve%2520as%2520generative%2520reward%2520models%2520to%2520enhance%2520LLMs%2527%2520multilingual%250Aproficiency%252C%2520though%2520discrepancies%2520with%2520human%2520judgment%2520remain.%2520Our%2520findings%250Asuggest%2520promising%2520directions%2520for%2520scalable%2520multilingual%2520evaluation%2520and%250Apreference%2520alignment.%2520We%2520release%2520our%2520dataset%2520and%2520evaluation%2520framework%2520to%250Asupport%2520further%2520research%2520in%2520multilingual%2520LLM%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MENLO%3A%20From%20Preferences%20to%20Proficiency%20--%20Evaluating%20and%20Modeling%0A%20%20Native-like%20Quality%20Across%2047%20Languages&entry.906535625=Chenxi%20Whitehouse%20and%20Sebastian%20Ruder%20and%20Tony%20Lin%20and%20Oksana%20Kurylo%20and%20Haruka%20Takagi%20and%20Janice%20Lam%20and%20Nicol%C3%B2%20Busetto%20and%20Denise%20Diaz&entry.1292438233=%20%20Ensuring%20native-like%20quality%20of%20large%20language%20model%20%28LLM%29%20responses%20across%0Amany%20languages%20is%20challenging.%20To%20address%20this%2C%20we%20introduce%20MENLO%2C%20a%20framework%0Athat%20operationalizes%20the%20evaluation%20of%20native-like%20response%20quality%20based%20on%0Aaudience%20design-inspired%20mechanisms.%20Using%20MENLO%2C%20we%20create%20a%20dataset%20of%206%2C423%0Ahuman-annotated%20prompt-response%20preference%20pairs%20covering%20four%20quality%0Adimensions%20with%20high%20inter-annotator%20agreement%20in%2047%20language%20varieties.%20Our%0Aevaluation%20reveals%20that%20zero-shot%20LLM%20judges%20benefit%20significantly%20from%0Apairwise%20evaluation%20and%20our%20structured%20annotation%20rubrics%2C%20yet%20they%20still%0Aunderperform%20human%20annotators%20on%20our%20dataset.%20We%20demonstrate%20substantial%0Aimprovements%20through%20fine-tuning%20with%20reinforcement%20learning%2C%20reward%20shaping%2C%0Aand%20multi-task%20learning%20approaches.%20Additionally%2C%20we%20show%20that%20RL-trained%0Ajudges%20can%20serve%20as%20generative%20reward%20models%20to%20enhance%20LLMs%27%20multilingual%0Aproficiency%2C%20though%20discrepancies%20with%20human%20judgment%20remain.%20Our%20findings%0Asuggest%20promising%20directions%20for%20scalable%20multilingual%20evaluation%20and%0Apreference%20alignment.%20We%20release%20our%20dataset%20and%20evaluation%20framework%20to%0Asupport%20further%20research%20in%20multilingual%20LLM%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26601v1&entry.124074799=Read"},
{"title": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype\n  Discovery", "author": "Zekun Wang and Ethan Haarer and Tianyi Zhu and Zhiyi Dai and Christopher J. MacLellan", "abstract": "  Inspired by the human ability to learn and organize knowledge into\nhierarchical taxonomies with prototypes, this paper addresses key limitations\nin current deep hierarchical clustering methods. Existing methods often tie the\nstructure to the number of classes and underutilize the rich prototype\ninformation available at intermediate hierarchical levels. We introduce deep\ntaxonomic networks, a novel deep latent variable approach designed to bridge\nthese gaps. Our method optimizes a large latent taxonomic hierarchy,\nspecifically a complete binary tree structured mixture-of-Gaussian prior within\na variational inference framework, to automatically discover taxonomic\nstructures and associated prototype clusters directly from unlabeled data\nwithout assuming true label sizes. We analytically show that optimizing the\nELBO of our method encourages the discovery of hierarchical relationships among\nprototypes. Empirically, our learned models demonstrate strong hierarchical\nclustering performance, outperforming baselines across diverse image\nclassification datasets using our novel evaluation mechanism that leverages\nprototype clusters discovered at all hierarchical levels. Qualitative results\nfurther reveal that deep taxonomic networks discover rich and interpretable\nhierarchical taxonomies, capturing both coarse-grained semantic categories and\nfine-grained visual distinctions.\n", "link": "http://arxiv.org/abs/2509.23602v2", "date": "2025-09-30", "relevancy": 2.6486, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Taxonomic%20Networks%20for%20Unsupervised%20Hierarchical%20Prototype%0A%20%20Discovery&body=Title%3A%20Deep%20Taxonomic%20Networks%20for%20Unsupervised%20Hierarchical%20Prototype%0A%20%20Discovery%0AAuthor%3A%20Zekun%20Wang%20and%20Ethan%20Haarer%20and%20Tianyi%20Zhu%20and%20Zhiyi%20Dai%20and%20Christopher%20J.%20MacLellan%0AAbstract%3A%20%20%20Inspired%20by%20the%20human%20ability%20to%20learn%20and%20organize%20knowledge%20into%0Ahierarchical%20taxonomies%20with%20prototypes%2C%20this%20paper%20addresses%20key%20limitations%0Ain%20current%20deep%20hierarchical%20clustering%20methods.%20Existing%20methods%20often%20tie%20the%0Astructure%20to%20the%20number%20of%20classes%20and%20underutilize%20the%20rich%20prototype%0Ainformation%20available%20at%20intermediate%20hierarchical%20levels.%20We%20introduce%20deep%0Ataxonomic%20networks%2C%20a%20novel%20deep%20latent%20variable%20approach%20designed%20to%20bridge%0Athese%20gaps.%20Our%20method%20optimizes%20a%20large%20latent%20taxonomic%20hierarchy%2C%0Aspecifically%20a%20complete%20binary%20tree%20structured%20mixture-of-Gaussian%20prior%20within%0Aa%20variational%20inference%20framework%2C%20to%20automatically%20discover%20taxonomic%0Astructures%20and%20associated%20prototype%20clusters%20directly%20from%20unlabeled%20data%0Awithout%20assuming%20true%20label%20sizes.%20We%20analytically%20show%20that%20optimizing%20the%0AELBO%20of%20our%20method%20encourages%20the%20discovery%20of%20hierarchical%20relationships%20among%0Aprototypes.%20Empirically%2C%20our%20learned%20models%20demonstrate%20strong%20hierarchical%0Aclustering%20performance%2C%20outperforming%20baselines%20across%20diverse%20image%0Aclassification%20datasets%20using%20our%20novel%20evaluation%20mechanism%20that%20leverages%0Aprototype%20clusters%20discovered%20at%20all%20hierarchical%20levels.%20Qualitative%20results%0Afurther%20reveal%20that%20deep%20taxonomic%20networks%20discover%20rich%20and%20interpretable%0Ahierarchical%20taxonomies%2C%20capturing%20both%20coarse-grained%20semantic%20categories%20and%0Afine-grained%20visual%20distinctions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Taxonomic%2520Networks%2520for%2520Unsupervised%2520Hierarchical%2520Prototype%250A%2520%2520Discovery%26entry.906535625%3DZekun%2520Wang%2520and%2520Ethan%2520Haarer%2520and%2520Tianyi%2520Zhu%2520and%2520Zhiyi%2520Dai%2520and%2520Christopher%2520J.%2520MacLellan%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520human%2520ability%2520to%2520learn%2520and%2520organize%2520knowledge%2520into%250Ahierarchical%2520taxonomies%2520with%2520prototypes%252C%2520this%2520paper%2520addresses%2520key%2520limitations%250Ain%2520current%2520deep%2520hierarchical%2520clustering%2520methods.%2520Existing%2520methods%2520often%2520tie%2520the%250Astructure%2520to%2520the%2520number%2520of%2520classes%2520and%2520underutilize%2520the%2520rich%2520prototype%250Ainformation%2520available%2520at%2520intermediate%2520hierarchical%2520levels.%2520We%2520introduce%2520deep%250Ataxonomic%2520networks%252C%2520a%2520novel%2520deep%2520latent%2520variable%2520approach%2520designed%2520to%2520bridge%250Athese%2520gaps.%2520Our%2520method%2520optimizes%2520a%2520large%2520latent%2520taxonomic%2520hierarchy%252C%250Aspecifically%2520a%2520complete%2520binary%2520tree%2520structured%2520mixture-of-Gaussian%2520prior%2520within%250Aa%2520variational%2520inference%2520framework%252C%2520to%2520automatically%2520discover%2520taxonomic%250Astructures%2520and%2520associated%2520prototype%2520clusters%2520directly%2520from%2520unlabeled%2520data%250Awithout%2520assuming%2520true%2520label%2520sizes.%2520We%2520analytically%2520show%2520that%2520optimizing%2520the%250AELBO%2520of%2520our%2520method%2520encourages%2520the%2520discovery%2520of%2520hierarchical%2520relationships%2520among%250Aprototypes.%2520Empirically%252C%2520our%2520learned%2520models%2520demonstrate%2520strong%2520hierarchical%250Aclustering%2520performance%252C%2520outperforming%2520baselines%2520across%2520diverse%2520image%250Aclassification%2520datasets%2520using%2520our%2520novel%2520evaluation%2520mechanism%2520that%2520leverages%250Aprototype%2520clusters%2520discovered%2520at%2520all%2520hierarchical%2520levels.%2520Qualitative%2520results%250Afurther%2520reveal%2520that%2520deep%2520taxonomic%2520networks%2520discover%2520rich%2520and%2520interpretable%250Ahierarchical%2520taxonomies%252C%2520capturing%2520both%2520coarse-grained%2520semantic%2520categories%2520and%250Afine-grained%2520visual%2520distinctions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Taxonomic%20Networks%20for%20Unsupervised%20Hierarchical%20Prototype%0A%20%20Discovery&entry.906535625=Zekun%20Wang%20and%20Ethan%20Haarer%20and%20Tianyi%20Zhu%20and%20Zhiyi%20Dai%20and%20Christopher%20J.%20MacLellan&entry.1292438233=%20%20Inspired%20by%20the%20human%20ability%20to%20learn%20and%20organize%20knowledge%20into%0Ahierarchical%20taxonomies%20with%20prototypes%2C%20this%20paper%20addresses%20key%20limitations%0Ain%20current%20deep%20hierarchical%20clustering%20methods.%20Existing%20methods%20often%20tie%20the%0Astructure%20to%20the%20number%20of%20classes%20and%20underutilize%20the%20rich%20prototype%0Ainformation%20available%20at%20intermediate%20hierarchical%20levels.%20We%20introduce%20deep%0Ataxonomic%20networks%2C%20a%20novel%20deep%20latent%20variable%20approach%20designed%20to%20bridge%0Athese%20gaps.%20Our%20method%20optimizes%20a%20large%20latent%20taxonomic%20hierarchy%2C%0Aspecifically%20a%20complete%20binary%20tree%20structured%20mixture-of-Gaussian%20prior%20within%0Aa%20variational%20inference%20framework%2C%20to%20automatically%20discover%20taxonomic%0Astructures%20and%20associated%20prototype%20clusters%20directly%20from%20unlabeled%20data%0Awithout%20assuming%20true%20label%20sizes.%20We%20analytically%20show%20that%20optimizing%20the%0AELBO%20of%20our%20method%20encourages%20the%20discovery%20of%20hierarchical%20relationships%20among%0Aprototypes.%20Empirically%2C%20our%20learned%20models%20demonstrate%20strong%20hierarchical%0Aclustering%20performance%2C%20outperforming%20baselines%20across%20diverse%20image%0Aclassification%20datasets%20using%20our%20novel%20evaluation%20mechanism%20that%20leverages%0Aprototype%20clusters%20discovered%20at%20all%20hierarchical%20levels.%20Qualitative%20results%0Afurther%20reveal%20that%20deep%20taxonomic%20networks%20discover%20rich%20and%20interpretable%0Ahierarchical%20taxonomies%2C%20capturing%20both%20coarse-grained%20semantic%20categories%20and%0Afine-grained%20visual%20distinctions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23602v2&entry.124074799=Read"},
{"title": "Provable Scaling Laws of Feature Emergence from Learning Dynamics of\n  Grokking", "author": "Yuandong Tian", "abstract": "  While the phenomenon of grokking, i.e., delayed generalization, has been\nstudied extensively, it remains an open problem whether there is a mathematical\nframework that characterizes what kind of features will emerge, how and in\nwhich conditions it happens, and is closely related to the gradient dynamics of\nthe training, for complex structured inputs. We propose a novel framework,\nnamed $\\mathbf{Li_2}$, that captures three key stages for the grokking behavior\nof 2-layer nonlinear networks: (I) \\underline{\\textbf{L}}azy learning, (II)\n\\underline{\\textbf{i}}ndependent feature learning and (III)\n\\underline{\\textbf{i}}nteractive feature learning. At the lazy learning stage,\ntop layer overfits to random hidden representation and the model appears to\nmemorize. Thanks to lazy learning and weight decay, the \\emph{backpropagated\ngradient} $G_F$ from the top layer now carries information about the target\nlabel, with a specific structure that enables each hidden node to learn their\nrepresentation \\emph{independently}. Interestingly, the independent dynamics\nfollows exactly the \\emph{gradient ascent} of an energy function $E$, and its\nlocal maxima are precisely the emerging features. We study whether these\nlocal-optima induced features are generalizable, their representation power,\nand how they change on sample size, in group arithmetic tasks. When hidden\nnodes start to interact in the later stage of learning, we provably show how\n$G_F$ changes to focus on missing features that need to be learned. Our study\nsheds lights on roles played by key hyperparameters such as weight decay,\nlearning rate and sample sizes in grokking, leads to provable scaling laws of\nfeature emergence, memorization and generalization, and reveals the underlying\ncause why recent optimizers such as Muon can be effective, from the first\nprinciples of gradient dynamics. Our analysis can be extended to multi-layer\narchitectures.\n", "link": "http://arxiv.org/abs/2509.21519v3", "date": "2025-09-30", "relevancy": 2.6091, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%0A%20%20Grokking&body=Title%3A%20Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%0A%20%20Grokking%0AAuthor%3A%20Yuandong%20Tian%0AAbstract%3A%20%20%20While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%0Astudied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%0Aframework%20that%20characterizes%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%0Awhich%20conditions%20it%20happens%2C%20and%20is%20closely%20related%20to%20the%20gradient%20dynamics%20of%0Athe%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%20a%20novel%20framework%2C%0Anamed%20%24%5Cmathbf%7BLi_2%7D%24%2C%20that%20captures%20three%20key%20stages%20for%20the%20grokking%20behavior%0Aof%202-layer%20nonlinear%20networks%3A%20%28I%29%20%5Cunderline%7B%5Ctextbf%7BL%7D%7Dazy%20learning%2C%20%28II%29%0A%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dndependent%20feature%20learning%20and%20%28III%29%0A%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dnteractive%20feature%20learning.%20At%20the%20lazy%20learning%20stage%2C%0Atop%20layer%20overfits%20to%20random%20hidden%20representation%20and%20the%20model%20appears%20to%0Amemorize.%20Thanks%20to%20lazy%20learning%20and%20weight%20decay%2C%20the%20%5Cemph%7Bbackpropagated%0Agradient%7D%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%20information%20about%20the%20target%0Alabel%2C%20with%20a%20specific%20structure%20that%20enables%20each%20hidden%20node%20to%20learn%20their%0Arepresentation%20%5Cemph%7Bindependently%7D.%20Interestingly%2C%20the%20independent%20dynamics%0Afollows%20exactly%20the%20%5Cemph%7Bgradient%20ascent%7D%20of%20an%20energy%20function%20%24E%24%2C%20and%20its%0Alocal%20maxima%20are%20precisely%20the%20emerging%20features.%20We%20study%20whether%20these%0Alocal-optima%20induced%20features%20are%20generalizable%2C%20their%20representation%20power%2C%0Aand%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%20tasks.%20When%20hidden%0Anodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%20provably%20show%20how%0A%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%20learned.%20Our%20study%0Asheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%20weight%20decay%2C%0Alearning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%20scaling%20laws%20of%0Afeature%20emergence%2C%20memorization%20and%20generalization%2C%20and%20reveals%20the%20underlying%0Acause%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%0Aprinciples%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layer%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.21519v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Scaling%2520Laws%2520of%2520Feature%2520Emergence%2520from%2520Learning%2520Dynamics%2520of%250A%2520%2520Grokking%26entry.906535625%3DYuandong%2520Tian%26entry.1292438233%3D%2520%2520While%2520the%2520phenomenon%2520of%2520grokking%252C%2520i.e.%252C%2520delayed%2520generalization%252C%2520has%2520been%250Astudied%2520extensively%252C%2520it%2520remains%2520an%2520open%2520problem%2520whether%2520there%2520is%2520a%2520mathematical%250Aframework%2520that%2520characterizes%2520what%2520kind%2520of%2520features%2520will%2520emerge%252C%2520how%2520and%2520in%250Awhich%2520conditions%2520it%2520happens%252C%2520and%2520is%2520closely%2520related%2520to%2520the%2520gradient%2520dynamics%2520of%250Athe%2520training%252C%2520for%2520complex%2520structured%2520inputs.%2520We%2520propose%2520a%2520novel%2520framework%252C%250Anamed%2520%2524%255Cmathbf%257BLi_2%257D%2524%252C%2520that%2520captures%2520three%2520key%2520stages%2520for%2520the%2520grokking%2520behavior%250Aof%25202-layer%2520nonlinear%2520networks%253A%2520%2528I%2529%2520%255Cunderline%257B%255Ctextbf%257BL%257D%257Dazy%2520learning%252C%2520%2528II%2529%250A%255Cunderline%257B%255Ctextbf%257Bi%257D%257Dndependent%2520feature%2520learning%2520and%2520%2528III%2529%250A%255Cunderline%257B%255Ctextbf%257Bi%257D%257Dnteractive%2520feature%2520learning.%2520At%2520the%2520lazy%2520learning%2520stage%252C%250Atop%2520layer%2520overfits%2520to%2520random%2520hidden%2520representation%2520and%2520the%2520model%2520appears%2520to%250Amemorize.%2520Thanks%2520to%2520lazy%2520learning%2520and%2520weight%2520decay%252C%2520the%2520%255Cemph%257Bbackpropagated%250Agradient%257D%2520%2524G_F%2524%2520from%2520the%2520top%2520layer%2520now%2520carries%2520information%2520about%2520the%2520target%250Alabel%252C%2520with%2520a%2520specific%2520structure%2520that%2520enables%2520each%2520hidden%2520node%2520to%2520learn%2520their%250Arepresentation%2520%255Cemph%257Bindependently%257D.%2520Interestingly%252C%2520the%2520independent%2520dynamics%250Afollows%2520exactly%2520the%2520%255Cemph%257Bgradient%2520ascent%257D%2520of%2520an%2520energy%2520function%2520%2524E%2524%252C%2520and%2520its%250Alocal%2520maxima%2520are%2520precisely%2520the%2520emerging%2520features.%2520We%2520study%2520whether%2520these%250Alocal-optima%2520induced%2520features%2520are%2520generalizable%252C%2520their%2520representation%2520power%252C%250Aand%2520how%2520they%2520change%2520on%2520sample%2520size%252C%2520in%2520group%2520arithmetic%2520tasks.%2520When%2520hidden%250Anodes%2520start%2520to%2520interact%2520in%2520the%2520later%2520stage%2520of%2520learning%252C%2520we%2520provably%2520show%2520how%250A%2524G_F%2524%2520changes%2520to%2520focus%2520on%2520missing%2520features%2520that%2520need%2520to%2520be%2520learned.%2520Our%2520study%250Asheds%2520lights%2520on%2520roles%2520played%2520by%2520key%2520hyperparameters%2520such%2520as%2520weight%2520decay%252C%250Alearning%2520rate%2520and%2520sample%2520sizes%2520in%2520grokking%252C%2520leads%2520to%2520provable%2520scaling%2520laws%2520of%250Afeature%2520emergence%252C%2520memorization%2520and%2520generalization%252C%2520and%2520reveals%2520the%2520underlying%250Acause%2520why%2520recent%2520optimizers%2520such%2520as%2520Muon%2520can%2520be%2520effective%252C%2520from%2520the%2520first%250Aprinciples%2520of%2520gradient%2520dynamics.%2520Our%2520analysis%2520can%2520be%2520extended%2520to%2520multi-layer%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21519v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%0A%20%20Grokking&entry.906535625=Yuandong%20Tian&entry.1292438233=%20%20While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%0Astudied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%0Aframework%20that%20characterizes%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%0Awhich%20conditions%20it%20happens%2C%20and%20is%20closely%20related%20to%20the%20gradient%20dynamics%20of%0Athe%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%20a%20novel%20framework%2C%0Anamed%20%24%5Cmathbf%7BLi_2%7D%24%2C%20that%20captures%20three%20key%20stages%20for%20the%20grokking%20behavior%0Aof%202-layer%20nonlinear%20networks%3A%20%28I%29%20%5Cunderline%7B%5Ctextbf%7BL%7D%7Dazy%20learning%2C%20%28II%29%0A%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dndependent%20feature%20learning%20and%20%28III%29%0A%5Cunderline%7B%5Ctextbf%7Bi%7D%7Dnteractive%20feature%20learning.%20At%20the%20lazy%20learning%20stage%2C%0Atop%20layer%20overfits%20to%20random%20hidden%20representation%20and%20the%20model%20appears%20to%0Amemorize.%20Thanks%20to%20lazy%20learning%20and%20weight%20decay%2C%20the%20%5Cemph%7Bbackpropagated%0Agradient%7D%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%20information%20about%20the%20target%0Alabel%2C%20with%20a%20specific%20structure%20that%20enables%20each%20hidden%20node%20to%20learn%20their%0Arepresentation%20%5Cemph%7Bindependently%7D.%20Interestingly%2C%20the%20independent%20dynamics%0Afollows%20exactly%20the%20%5Cemph%7Bgradient%20ascent%7D%20of%20an%20energy%20function%20%24E%24%2C%20and%20its%0Alocal%20maxima%20are%20precisely%20the%20emerging%20features.%20We%20study%20whether%20these%0Alocal-optima%20induced%20features%20are%20generalizable%2C%20their%20representation%20power%2C%0Aand%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%20tasks.%20When%20hidden%0Anodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%20provably%20show%20how%0A%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%20learned.%20Our%20study%0Asheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%20weight%20decay%2C%0Alearning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%20scaling%20laws%20of%0Afeature%20emergence%2C%20memorization%20and%20generalization%2C%20and%20reveals%20the%20underlying%0Acause%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%0Aprinciples%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layer%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.21519v3&entry.124074799=Read"},
{"title": "LoLA: Low-Rank Linear Attention With Sparse Caching", "author": "Luke McDermott and Robert W. Heath Jr. and Rahul Parhi", "abstract": "  The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.\n", "link": "http://arxiv.org/abs/2505.23666v2", "date": "2025-09-30", "relevancy": 2.5658, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5373}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5058}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoLA%3A%20Low-Rank%20Linear%20Attention%20With%20Sparse%20Caching&body=Title%3A%20LoLA%3A%20Low-Rank%20Linear%20Attention%20With%20Sparse%20Caching%0AAuthor%3A%20Luke%20McDermott%20and%20Robert%20W.%20Heath%20Jr.%20and%20Rahul%20Parhi%0AAbstract%3A%20%20%20The%20per-token%20cost%20of%20transformer%20inference%20scales%20with%20context%20length%2C%0Apreventing%20its%20application%20to%20lifelong%20in-context%20learning.%20Linear%20attention%20is%0Aan%20efficient%20alternative%20that%20maintains%20a%20constant%20memory%20footprint%2C%20even%20on%0Ainfinite%20context%20lengths.%20While%20this%20is%20a%20potential%20candidate%20for%20lifelong%0Alearning%2C%20it%20falls%20short%20in%20memory%20capacity.%20In%20this%20paper%2C%20we%20propose%20LoLA%2C%20a%0Atraining-free%20augmentation%20to%20linear%20attention%20that%20boosts%20associative%20recall.%0ALoLA%20distributes%20past%20key-value%20pairs%20from%20context%20into%20three%20memory%20systems%3A%0A%28i%29%20recent%20pairs%20in%20a%20local%20sliding%20window%20cache%3B%20%28ii%29%20difficult-to-memorize%0Apairs%20in%20a%20sparse%2C%20global%20cache%3B%20and%20%28iii%29%20generic%20pairs%20in%20the%20recurrent%0Ahidden%20state%20of%20linear%20attention.%20We%20show%20through%20ablations%20that%20our%0Aself-recall%20error%20metric%20is%20crucial%20to%20efficiently%20manage%20long-term%20associative%0Amemories.%20On%20pass-key%20retrieval%20tasks%2C%20LoLA%20improves%20the%20base%20model%27s%0Aperformance%20from%200.6%25%20to%2097.4%25%20accuracy.%20This%20is%20achieved%20with%20a%204.6x%20smaller%0Acache%20than%20Llama-3.1%208B%20on%204K%20context%20length.%20LoLA%20also%20outperforms%20other%201B%0Aand%208B%20parameter%20subquadratic%20models%20on%20zero-shot%20commonsense%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoLA%253A%2520Low-Rank%2520Linear%2520Attention%2520With%2520Sparse%2520Caching%26entry.906535625%3DLuke%2520McDermott%2520and%2520Robert%2520W.%2520Heath%2520Jr.%2520and%2520Rahul%2520Parhi%26entry.1292438233%3D%2520%2520The%2520per-token%2520cost%2520of%2520transformer%2520inference%2520scales%2520with%2520context%2520length%252C%250Apreventing%2520its%2520application%2520to%2520lifelong%2520in-context%2520learning.%2520Linear%2520attention%2520is%250Aan%2520efficient%2520alternative%2520that%2520maintains%2520a%2520constant%2520memory%2520footprint%252C%2520even%2520on%250Ainfinite%2520context%2520lengths.%2520While%2520this%2520is%2520a%2520potential%2520candidate%2520for%2520lifelong%250Alearning%252C%2520it%2520falls%2520short%2520in%2520memory%2520capacity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LoLA%252C%2520a%250Atraining-free%2520augmentation%2520to%2520linear%2520attention%2520that%2520boosts%2520associative%2520recall.%250ALoLA%2520distributes%2520past%2520key-value%2520pairs%2520from%2520context%2520into%2520three%2520memory%2520systems%253A%250A%2528i%2529%2520recent%2520pairs%2520in%2520a%2520local%2520sliding%2520window%2520cache%253B%2520%2528ii%2529%2520difficult-to-memorize%250Apairs%2520in%2520a%2520sparse%252C%2520global%2520cache%253B%2520and%2520%2528iii%2529%2520generic%2520pairs%2520in%2520the%2520recurrent%250Ahidden%2520state%2520of%2520linear%2520attention.%2520We%2520show%2520through%2520ablations%2520that%2520our%250Aself-recall%2520error%2520metric%2520is%2520crucial%2520to%2520efficiently%2520manage%2520long-term%2520associative%250Amemories.%2520On%2520pass-key%2520retrieval%2520tasks%252C%2520LoLA%2520improves%2520the%2520base%2520model%2527s%250Aperformance%2520from%25200.6%2525%2520to%252097.4%2525%2520accuracy.%2520This%2520is%2520achieved%2520with%2520a%25204.6x%2520smaller%250Acache%2520than%2520Llama-3.1%25208B%2520on%25204K%2520context%2520length.%2520LoLA%2520also%2520outperforms%2520other%25201B%250Aand%25208B%2520parameter%2520subquadratic%2520models%2520on%2520zero-shot%2520commonsense%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoLA%3A%20Low-Rank%20Linear%20Attention%20With%20Sparse%20Caching&entry.906535625=Luke%20McDermott%20and%20Robert%20W.%20Heath%20Jr.%20and%20Rahul%20Parhi&entry.1292438233=%20%20The%20per-token%20cost%20of%20transformer%20inference%20scales%20with%20context%20length%2C%0Apreventing%20its%20application%20to%20lifelong%20in-context%20learning.%20Linear%20attention%20is%0Aan%20efficient%20alternative%20that%20maintains%20a%20constant%20memory%20footprint%2C%20even%20on%0Ainfinite%20context%20lengths.%20While%20this%20is%20a%20potential%20candidate%20for%20lifelong%0Alearning%2C%20it%20falls%20short%20in%20memory%20capacity.%20In%20this%20paper%2C%20we%20propose%20LoLA%2C%20a%0Atraining-free%20augmentation%20to%20linear%20attention%20that%20boosts%20associative%20recall.%0ALoLA%20distributes%20past%20key-value%20pairs%20from%20context%20into%20three%20memory%20systems%3A%0A%28i%29%20recent%20pairs%20in%20a%20local%20sliding%20window%20cache%3B%20%28ii%29%20difficult-to-memorize%0Apairs%20in%20a%20sparse%2C%20global%20cache%3B%20and%20%28iii%29%20generic%20pairs%20in%20the%20recurrent%0Ahidden%20state%20of%20linear%20attention.%20We%20show%20through%20ablations%20that%20our%0Aself-recall%20error%20metric%20is%20crucial%20to%20efficiently%20manage%20long-term%20associative%0Amemories.%20On%20pass-key%20retrieval%20tasks%2C%20LoLA%20improves%20the%20base%20model%27s%0Aperformance%20from%200.6%25%20to%2097.4%25%20accuracy.%20This%20is%20achieved%20with%20a%204.6x%20smaller%0Acache%20than%20Llama-3.1%208B%20on%204K%20context%20length.%20LoLA%20also%20outperforms%20other%201B%0Aand%208B%20parameter%20subquadratic%20models%20on%20zero-shot%20commonsense%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23666v2&entry.124074799=Read"},
{"title": "Unpicking Data at the Seams: Understanding Disentanglement in VAEs", "author": "Carl Allen", "abstract": "  A generative latent variable model is said to be disentangled when varying a\nsingle latent co-ordinate changes a single aspect of samples generated, e.g.\nobject position or facial expression in an image. Related phenomena are seen in\nseveral generative paradigms, including state-of-the-art diffusion models, but\ndisentanglement is most notably observed in Variational Autoencoders (VAEs),\nwhere oft-used diagonal posterior covariances are argued to be the cause. We\nmake this picture precise. From a known exact link between optimal Gaussian\nposteriors and decoder derivatives, we show how diagonal posteriors \"lock\" a\ndecoder's local axes so that density over the data manifold factorises along\nindependent one-dimensional seams that map to axis-aligned directions in latent\nspace. This gives a clear definition of disentanglement, explains why it\nemerges in VAEs and shows that, under stated assumptions, ground truth factors\nare identifiable even with a symmetric prior.\n", "link": "http://arxiv.org/abs/2410.22559v6", "date": "2025-09-30", "relevancy": 2.5131, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5208}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5047}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unpicking%20Data%20at%20the%20Seams%3A%20Understanding%20Disentanglement%20in%20VAEs&body=Title%3A%20Unpicking%20Data%20at%20the%20Seams%3A%20Understanding%20Disentanglement%20in%20VAEs%0AAuthor%3A%20Carl%20Allen%0AAbstract%3A%20%20%20A%20generative%20latent%20variable%20model%20is%20said%20to%20be%20disentangled%20when%20varying%20a%0Asingle%20latent%20co-ordinate%20changes%20a%20single%20aspect%20of%20samples%20generated%2C%20e.g.%0Aobject%20position%20or%20facial%20expression%20in%20an%20image.%20Related%20phenomena%20are%20seen%20in%0Aseveral%20generative%20paradigms%2C%20including%20state-of-the-art%20diffusion%20models%2C%20but%0Adisentanglement%20is%20most%20notably%20observed%20in%20Variational%20Autoencoders%20%28VAEs%29%2C%0Awhere%20oft-used%20diagonal%20posterior%20covariances%20are%20argued%20to%20be%20the%20cause.%20We%0Amake%20this%20picture%20precise.%20From%20a%20known%20exact%20link%20between%20optimal%20Gaussian%0Aposteriors%20and%20decoder%20derivatives%2C%20we%20show%20how%20diagonal%20posteriors%20%22lock%22%20a%0Adecoder%27s%20local%20axes%20so%20that%20density%20over%20the%20data%20manifold%20factorises%20along%0Aindependent%20one-dimensional%20seams%20that%20map%20to%20axis-aligned%20directions%20in%20latent%0Aspace.%20This%20gives%20a%20clear%20definition%20of%20disentanglement%2C%20explains%20why%20it%0Aemerges%20in%20VAEs%20and%20shows%20that%2C%20under%20stated%20assumptions%2C%20ground%20truth%20factors%0Aare%20identifiable%20even%20with%20a%20symmetric%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22559v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnpicking%2520Data%2520at%2520the%2520Seams%253A%2520Understanding%2520Disentanglement%2520in%2520VAEs%26entry.906535625%3DCarl%2520Allen%26entry.1292438233%3D%2520%2520A%2520generative%2520latent%2520variable%2520model%2520is%2520said%2520to%2520be%2520disentangled%2520when%2520varying%2520a%250Asingle%2520latent%2520co-ordinate%2520changes%2520a%2520single%2520aspect%2520of%2520samples%2520generated%252C%2520e.g.%250Aobject%2520position%2520or%2520facial%2520expression%2520in%2520an%2520image.%2520Related%2520phenomena%2520are%2520seen%2520in%250Aseveral%2520generative%2520paradigms%252C%2520including%2520state-of-the-art%2520diffusion%2520models%252C%2520but%250Adisentanglement%2520is%2520most%2520notably%2520observed%2520in%2520Variational%2520Autoencoders%2520%2528VAEs%2529%252C%250Awhere%2520oft-used%2520diagonal%2520posterior%2520covariances%2520are%2520argued%2520to%2520be%2520the%2520cause.%2520We%250Amake%2520this%2520picture%2520precise.%2520From%2520a%2520known%2520exact%2520link%2520between%2520optimal%2520Gaussian%250Aposteriors%2520and%2520decoder%2520derivatives%252C%2520we%2520show%2520how%2520diagonal%2520posteriors%2520%2522lock%2522%2520a%250Adecoder%2527s%2520local%2520axes%2520so%2520that%2520density%2520over%2520the%2520data%2520manifold%2520factorises%2520along%250Aindependent%2520one-dimensional%2520seams%2520that%2520map%2520to%2520axis-aligned%2520directions%2520in%2520latent%250Aspace.%2520This%2520gives%2520a%2520clear%2520definition%2520of%2520disentanglement%252C%2520explains%2520why%2520it%250Aemerges%2520in%2520VAEs%2520and%2520shows%2520that%252C%2520under%2520stated%2520assumptions%252C%2520ground%2520truth%2520factors%250Aare%2520identifiable%2520even%2520with%2520a%2520symmetric%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22559v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpicking%20Data%20at%20the%20Seams%3A%20Understanding%20Disentanglement%20in%20VAEs&entry.906535625=Carl%20Allen&entry.1292438233=%20%20A%20generative%20latent%20variable%20model%20is%20said%20to%20be%20disentangled%20when%20varying%20a%0Asingle%20latent%20co-ordinate%20changes%20a%20single%20aspect%20of%20samples%20generated%2C%20e.g.%0Aobject%20position%20or%20facial%20expression%20in%20an%20image.%20Related%20phenomena%20are%20seen%20in%0Aseveral%20generative%20paradigms%2C%20including%20state-of-the-art%20diffusion%20models%2C%20but%0Adisentanglement%20is%20most%20notably%20observed%20in%20Variational%20Autoencoders%20%28VAEs%29%2C%0Awhere%20oft-used%20diagonal%20posterior%20covariances%20are%20argued%20to%20be%20the%20cause.%20We%0Amake%20this%20picture%20precise.%20From%20a%20known%20exact%20link%20between%20optimal%20Gaussian%0Aposteriors%20and%20decoder%20derivatives%2C%20we%20show%20how%20diagonal%20posteriors%20%22lock%22%20a%0Adecoder%27s%20local%20axes%20so%20that%20density%20over%20the%20data%20manifold%20factorises%20along%0Aindependent%20one-dimensional%20seams%20that%20map%20to%20axis-aligned%20directions%20in%20latent%0Aspace.%20This%20gives%20a%20clear%20definition%20of%20disentanglement%2C%20explains%20why%20it%0Aemerges%20in%20VAEs%20and%20shows%20that%2C%20under%20stated%20assumptions%2C%20ground%20truth%20factors%0Aare%20identifiable%20even%20with%20a%20symmetric%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22559v6&entry.124074799=Read"},
{"title": "Unlocking Transfer Learning for Open-World Few-Shot Recognition", "author": "Byeonggeun Kim and Juntae Lee and Kyuhong Shim and Simyung Chang", "abstract": "  Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world\nchallenge, aiming to categorize inputs into known categories, termed closed-set\nclasses, while identifying open-set inputs that fall outside these classes.\nAlthough transfer learning where a model is tuned to a given few-shot task has\nbecome a prominent paradigm in closed-world, we observe that it fails to expand\nto open-world. To unlock this challenge, we propose a two-stage method which\nconsists of open-set aware meta-learning with open-set free transfer learning.\nIn the open-set aware meta-learning stage, a model is trained to establish a\nmetric space that serves as a beneficial starting point for the subsequent\nstage. During the open-set free transfer learning stage, the model is further\nadapted to a specific target task through transfer learning. Additionally, we\nintroduce a strategy to simulate open-set examples by modifying the training\ndataset or generating pseudo open-set examples. The proposed method achieves\nstate-of-the-art performance on two widely recognized benchmarks, miniImageNet\nand tieredImageNet, with only a 1.5\\% increase in training effort. Our work\ndemonstrates the effectiveness of transfer learning in FSOSR.\n", "link": "http://arxiv.org/abs/2411.09986v3", "date": "2025-09-30", "relevancy": 2.5041, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Transfer%20Learning%20for%20Open-World%20Few-Shot%20Recognition&body=Title%3A%20Unlocking%20Transfer%20Learning%20for%20Open-World%20Few-Shot%20Recognition%0AAuthor%3A%20Byeonggeun%20Kim%20and%20Juntae%20Lee%20and%20Kyuhong%20Shim%20and%20Simyung%20Chang%0AAbstract%3A%20%20%20Few-Shot%20Open-Set%20Recognition%20%28FSOSR%29%20targets%20a%20critical%20real-world%0Achallenge%2C%20aiming%20to%20categorize%20inputs%20into%20known%20categories%2C%20termed%20closed-set%0Aclasses%2C%20while%20identifying%20open-set%20inputs%20that%20fall%20outside%20these%20classes.%0AAlthough%20transfer%20learning%20where%20a%20model%20is%20tuned%20to%20a%20given%20few-shot%20task%20has%0Abecome%20a%20prominent%20paradigm%20in%20closed-world%2C%20we%20observe%20that%20it%20fails%20to%20expand%0Ato%20open-world.%20To%20unlock%20this%20challenge%2C%20we%20propose%20a%20two-stage%20method%20which%0Aconsists%20of%20open-set%20aware%20meta-learning%20with%20open-set%20free%20transfer%20learning.%0AIn%20the%20open-set%20aware%20meta-learning%20stage%2C%20a%20model%20is%20trained%20to%20establish%20a%0Ametric%20space%20that%20serves%20as%20a%20beneficial%20starting%20point%20for%20the%20subsequent%0Astage.%20During%20the%20open-set%20free%20transfer%20learning%20stage%2C%20the%20model%20is%20further%0Aadapted%20to%20a%20specific%20target%20task%20through%20transfer%20learning.%20Additionally%2C%20we%0Aintroduce%20a%20strategy%20to%20simulate%20open-set%20examples%20by%20modifying%20the%20training%0Adataset%20or%20generating%20pseudo%20open-set%20examples.%20The%20proposed%20method%20achieves%0Astate-of-the-art%20performance%20on%20two%20widely%20recognized%20benchmarks%2C%20miniImageNet%0Aand%20tieredImageNet%2C%20with%20only%20a%201.5%5C%25%20increase%20in%20training%20effort.%20Our%20work%0Ademonstrates%20the%20effectiveness%20of%20transfer%20learning%20in%20FSOSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09986v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Transfer%2520Learning%2520for%2520Open-World%2520Few-Shot%2520Recognition%26entry.906535625%3DByeonggeun%2520Kim%2520and%2520Juntae%2520Lee%2520and%2520Kyuhong%2520Shim%2520and%2520Simyung%2520Chang%26entry.1292438233%3D%2520%2520Few-Shot%2520Open-Set%2520Recognition%2520%2528FSOSR%2529%2520targets%2520a%2520critical%2520real-world%250Achallenge%252C%2520aiming%2520to%2520categorize%2520inputs%2520into%2520known%2520categories%252C%2520termed%2520closed-set%250Aclasses%252C%2520while%2520identifying%2520open-set%2520inputs%2520that%2520fall%2520outside%2520these%2520classes.%250AAlthough%2520transfer%2520learning%2520where%2520a%2520model%2520is%2520tuned%2520to%2520a%2520given%2520few-shot%2520task%2520has%250Abecome%2520a%2520prominent%2520paradigm%2520in%2520closed-world%252C%2520we%2520observe%2520that%2520it%2520fails%2520to%2520expand%250Ato%2520open-world.%2520To%2520unlock%2520this%2520challenge%252C%2520we%2520propose%2520a%2520two-stage%2520method%2520which%250Aconsists%2520of%2520open-set%2520aware%2520meta-learning%2520with%2520open-set%2520free%2520transfer%2520learning.%250AIn%2520the%2520open-set%2520aware%2520meta-learning%2520stage%252C%2520a%2520model%2520is%2520trained%2520to%2520establish%2520a%250Ametric%2520space%2520that%2520serves%2520as%2520a%2520beneficial%2520starting%2520point%2520for%2520the%2520subsequent%250Astage.%2520During%2520the%2520open-set%2520free%2520transfer%2520learning%2520stage%252C%2520the%2520model%2520is%2520further%250Aadapted%2520to%2520a%2520specific%2520target%2520task%2520through%2520transfer%2520learning.%2520Additionally%252C%2520we%250Aintroduce%2520a%2520strategy%2520to%2520simulate%2520open-set%2520examples%2520by%2520modifying%2520the%2520training%250Adataset%2520or%2520generating%2520pseudo%2520open-set%2520examples.%2520The%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520two%2520widely%2520recognized%2520benchmarks%252C%2520miniImageNet%250Aand%2520tieredImageNet%252C%2520with%2520only%2520a%25201.5%255C%2525%2520increase%2520in%2520training%2520effort.%2520Our%2520work%250Ademonstrates%2520the%2520effectiveness%2520of%2520transfer%2520learning%2520in%2520FSOSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09986v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Transfer%20Learning%20for%20Open-World%20Few-Shot%20Recognition&entry.906535625=Byeonggeun%20Kim%20and%20Juntae%20Lee%20and%20Kyuhong%20Shim%20and%20Simyung%20Chang&entry.1292438233=%20%20Few-Shot%20Open-Set%20Recognition%20%28FSOSR%29%20targets%20a%20critical%20real-world%0Achallenge%2C%20aiming%20to%20categorize%20inputs%20into%20known%20categories%2C%20termed%20closed-set%0Aclasses%2C%20while%20identifying%20open-set%20inputs%20that%20fall%20outside%20these%20classes.%0AAlthough%20transfer%20learning%20where%20a%20model%20is%20tuned%20to%20a%20given%20few-shot%20task%20has%0Abecome%20a%20prominent%20paradigm%20in%20closed-world%2C%20we%20observe%20that%20it%20fails%20to%20expand%0Ato%20open-world.%20To%20unlock%20this%20challenge%2C%20we%20propose%20a%20two-stage%20method%20which%0Aconsists%20of%20open-set%20aware%20meta-learning%20with%20open-set%20free%20transfer%20learning.%0AIn%20the%20open-set%20aware%20meta-learning%20stage%2C%20a%20model%20is%20trained%20to%20establish%20a%0Ametric%20space%20that%20serves%20as%20a%20beneficial%20starting%20point%20for%20the%20subsequent%0Astage.%20During%20the%20open-set%20free%20transfer%20learning%20stage%2C%20the%20model%20is%20further%0Aadapted%20to%20a%20specific%20target%20task%20through%20transfer%20learning.%20Additionally%2C%20we%0Aintroduce%20a%20strategy%20to%20simulate%20open-set%20examples%20by%20modifying%20the%20training%0Adataset%20or%20generating%20pseudo%20open-set%20examples.%20The%20proposed%20method%20achieves%0Astate-of-the-art%20performance%20on%20two%20widely%20recognized%20benchmarks%2C%20miniImageNet%0Aand%20tieredImageNet%2C%20with%20only%20a%201.5%5C%25%20increase%20in%20training%20effort.%20Our%20work%0Ademonstrates%20the%20effectiveness%20of%20transfer%20learning%20in%20FSOSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09986v3&entry.124074799=Read"},
{"title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation", "author": "Zhuoyang Liu and Jiaming Liu and Jiadong Xu and Nuowei Han and Chenyang Gu and Hao Chen and Kaichen Zhou and Renrui Zhang and Kai Chin Hsieh and Kun Wu and Zhengping Che and Jian Tang and Shanghang Zhang", "abstract": "  Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla\n", "link": "http://arxiv.org/abs/2509.26642v1", "date": "2025-09-30", "relevancy": 2.477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLA%3A%20A%20Multisensory%20Language-Action%20Model%20for%20Multimodal%20Understanding%0A%20%20and%20Forecasting%20in%20Robotic%20Manipulation&body=Title%3A%20MLA%3A%20A%20Multisensory%20Language-Action%20Model%20for%20Multimodal%20Understanding%0A%20%20and%20Forecasting%20in%20Robotic%20Manipulation%0AAuthor%3A%20Zhuoyang%20Liu%20and%20Jiaming%20Liu%20and%20Jiadong%20Xu%20and%20Nuowei%20Han%20and%20Chenyang%20Gu%20and%20Hao%20Chen%20and%20Kaichen%20Zhou%20and%20Renrui%20Zhang%20and%20Kai%20Chin%20Hsieh%20and%20Kun%20Wu%20and%20Zhengping%20Che%20and%20Jian%20Tang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Vision-language-action%20models%20%28VLAs%29%20have%20shown%20generalization%20capabilities%0Ain%20robotic%20manipulation%20tasks%20by%20inheriting%20from%20vision-language%20models%20%28VLMs%29%0Aand%20learning%20action%20generation.%20Most%20VLA%20models%20focus%20on%20interpreting%20vision%0Aand%20language%20to%20generate%20actions%2C%20whereas%20robots%20must%20perceive%20and%20interact%0Awithin%20the%20spatial-physical%20world.%20This%20gap%20highlights%20the%20need%20for%20a%0Acomprehensive%20understanding%20of%20robotic-specific%20multisensory%20information%2C%20which%0Ais%20crucial%20for%20achieving%20complex%20and%20contact-rich%20control.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multisensory%20language-action%20%28MLA%29%20model%20that%20collaboratively%0Aperceives%20heterogeneous%20sensory%20modalities%20and%20predicts%20future%20multisensory%0Aobjectives%20to%20facilitate%20physical%20world%20modeling.%20Specifically%2C%20to%20enhance%0Aperceptual%20representations%2C%20we%20propose%20an%20encoder-free%20multimodal%20alignment%0Ascheme%20that%20innovatively%20repurposes%20the%20large%20language%20model%20itself%20as%20a%0Aperception%20module%2C%20directly%20interpreting%20multimodal%20cues%20by%20aligning%202D%20images%2C%0A3D%20point%20clouds%2C%20and%20tactile%20tokens%20through%20positional%20correspondence.%20To%0Afurther%20enhance%20MLA%27s%20understanding%20of%20physical%20dynamics%2C%20we%20design%20a%20future%0Amultisensory%20generation%20post-training%20strategy%20that%20enables%20MLA%20to%20reason%20about%0Asemantic%2C%20geometric%2C%20and%20interaction%20information%2C%20providing%20more%20robust%0Aconditions%20for%20action%20generation.%20For%20evaluation%2C%20the%20MLA%20model%20outperforms%20the%0Aprevious%20state-of-the-art%202D%20and%203D%20VLA%20methods%20by%2012%25%20and%2024%25%20in%20complex%2C%0Acontact-rich%20real-world%20tasks%2C%20respectively%2C%20while%20also%20demonstrating%20improved%0Ageneralization%20to%20unseen%20configurations.%20Project%20website%3A%0Ahttps%3A//sites.google.com/view/open-mla%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLA%253A%2520A%2520Multisensory%2520Language-Action%2520Model%2520for%2520Multimodal%2520Understanding%250A%2520%2520and%2520Forecasting%2520in%2520Robotic%2520Manipulation%26entry.906535625%3DZhuoyang%2520Liu%2520and%2520Jiaming%2520Liu%2520and%2520Jiadong%2520Xu%2520and%2520Nuowei%2520Han%2520and%2520Chenyang%2520Gu%2520and%2520Hao%2520Chen%2520and%2520Kaichen%2520Zhou%2520and%2520Renrui%2520Zhang%2520and%2520Kai%2520Chin%2520Hsieh%2520and%2520Kun%2520Wu%2520and%2520Zhengping%2520Che%2520and%2520Jian%2520Tang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-language-action%2520models%2520%2528VLAs%2529%2520have%2520shown%2520generalization%2520capabilities%250Ain%2520robotic%2520manipulation%2520tasks%2520by%2520inheriting%2520from%2520vision-language%2520models%2520%2528VLMs%2529%250Aand%2520learning%2520action%2520generation.%2520Most%2520VLA%2520models%2520focus%2520on%2520interpreting%2520vision%250Aand%2520language%2520to%2520generate%2520actions%252C%2520whereas%2520robots%2520must%2520perceive%2520and%2520interact%250Awithin%2520the%2520spatial-physical%2520world.%2520This%2520gap%2520highlights%2520the%2520need%2520for%2520a%250Acomprehensive%2520understanding%2520of%2520robotic-specific%2520multisensory%2520information%252C%2520which%250Ais%2520crucial%2520for%2520achieving%2520complex%2520and%2520contact-rich%2520control.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520multisensory%2520language-action%2520%2528MLA%2529%2520model%2520that%2520collaboratively%250Aperceives%2520heterogeneous%2520sensory%2520modalities%2520and%2520predicts%2520future%2520multisensory%250Aobjectives%2520to%2520facilitate%2520physical%2520world%2520modeling.%2520Specifically%252C%2520to%2520enhance%250Aperceptual%2520representations%252C%2520we%2520propose%2520an%2520encoder-free%2520multimodal%2520alignment%250Ascheme%2520that%2520innovatively%2520repurposes%2520the%2520large%2520language%2520model%2520itself%2520as%2520a%250Aperception%2520module%252C%2520directly%2520interpreting%2520multimodal%2520cues%2520by%2520aligning%25202D%2520images%252C%250A3D%2520point%2520clouds%252C%2520and%2520tactile%2520tokens%2520through%2520positional%2520correspondence.%2520To%250Afurther%2520enhance%2520MLA%2527s%2520understanding%2520of%2520physical%2520dynamics%252C%2520we%2520design%2520a%2520future%250Amultisensory%2520generation%2520post-training%2520strategy%2520that%2520enables%2520MLA%2520to%2520reason%2520about%250Asemantic%252C%2520geometric%252C%2520and%2520interaction%2520information%252C%2520providing%2520more%2520robust%250Aconditions%2520for%2520action%2520generation.%2520For%2520evaluation%252C%2520the%2520MLA%2520model%2520outperforms%2520the%250Aprevious%2520state-of-the-art%25202D%2520and%25203D%2520VLA%2520methods%2520by%252012%2525%2520and%252024%2525%2520in%2520complex%252C%250Acontact-rich%2520real-world%2520tasks%252C%2520respectively%252C%2520while%2520also%2520demonstrating%2520improved%250Ageneralization%2520to%2520unseen%2520configurations.%2520Project%2520website%253A%250Ahttps%253A//sites.google.com/view/open-mla%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLA%3A%20A%20Multisensory%20Language-Action%20Model%20for%20Multimodal%20Understanding%0A%20%20and%20Forecasting%20in%20Robotic%20Manipulation&entry.906535625=Zhuoyang%20Liu%20and%20Jiaming%20Liu%20and%20Jiadong%20Xu%20and%20Nuowei%20Han%20and%20Chenyang%20Gu%20and%20Hao%20Chen%20and%20Kaichen%20Zhou%20and%20Renrui%20Zhang%20and%20Kai%20Chin%20Hsieh%20and%20Kun%20Wu%20and%20Zhengping%20Che%20and%20Jian%20Tang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Vision-language-action%20models%20%28VLAs%29%20have%20shown%20generalization%20capabilities%0Ain%20robotic%20manipulation%20tasks%20by%20inheriting%20from%20vision-language%20models%20%28VLMs%29%0Aand%20learning%20action%20generation.%20Most%20VLA%20models%20focus%20on%20interpreting%20vision%0Aand%20language%20to%20generate%20actions%2C%20whereas%20robots%20must%20perceive%20and%20interact%0Awithin%20the%20spatial-physical%20world.%20This%20gap%20highlights%20the%20need%20for%20a%0Acomprehensive%20understanding%20of%20robotic-specific%20multisensory%20information%2C%20which%0Ais%20crucial%20for%20achieving%20complex%20and%20contact-rich%20control.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multisensory%20language-action%20%28MLA%29%20model%20that%20collaboratively%0Aperceives%20heterogeneous%20sensory%20modalities%20and%20predicts%20future%20multisensory%0Aobjectives%20to%20facilitate%20physical%20world%20modeling.%20Specifically%2C%20to%20enhance%0Aperceptual%20representations%2C%20we%20propose%20an%20encoder-free%20multimodal%20alignment%0Ascheme%20that%20innovatively%20repurposes%20the%20large%20language%20model%20itself%20as%20a%0Aperception%20module%2C%20directly%20interpreting%20multimodal%20cues%20by%20aligning%202D%20images%2C%0A3D%20point%20clouds%2C%20and%20tactile%20tokens%20through%20positional%20correspondence.%20To%0Afurther%20enhance%20MLA%27s%20understanding%20of%20physical%20dynamics%2C%20we%20design%20a%20future%0Amultisensory%20generation%20post-training%20strategy%20that%20enables%20MLA%20to%20reason%20about%0Asemantic%2C%20geometric%2C%20and%20interaction%20information%2C%20providing%20more%20robust%0Aconditions%20for%20action%20generation.%20For%20evaluation%2C%20the%20MLA%20model%20outperforms%20the%0Aprevious%20state-of-the-art%202D%20and%203D%20VLA%20methods%20by%2012%25%20and%2024%25%20in%20complex%2C%0Acontact-rich%20real-world%20tasks%2C%20respectively%2C%20while%20also%20demonstrating%20improved%0Ageneralization%20to%20unseen%20configurations.%20Project%20website%3A%0Ahttps%3A//sites.google.com/view/open-mla%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26642v1&entry.124074799=Read"},
{"title": "Stitch: Training-Free Position Control in Multimodal Diffusion\n  Transformers", "author": "Jessica Bader and Mateusz Pach and Maria A. Bravo and Serge Belongie and Zeynep Akata", "abstract": "  Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.\n", "link": "http://arxiv.org/abs/2509.26644v1", "date": "2025-09-30", "relevancy": 2.4342, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6188}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6038}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stitch%3A%20Training-Free%20Position%20Control%20in%20Multimodal%20Diffusion%0A%20%20Transformers&body=Title%3A%20Stitch%3A%20Training-Free%20Position%20Control%20in%20Multimodal%20Diffusion%0A%20%20Transformers%0AAuthor%3A%20Jessica%20Bader%20and%20Mateusz%20Pach%20and%20Maria%20A.%20Bravo%20and%20Serge%20Belongie%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20Text-to-Image%20%28T2I%29%20generation%20models%20have%20advanced%20rapidly%20in%20recent%20years%2C%0Abut%20accurately%20capturing%20spatial%20relationships%20like%20%22above%22%20or%20%22to%20the%20right%0Aof%22%20poses%20a%20persistent%20challenge.%20Earlier%20methods%20improved%20spatial%20relationship%0Afollowing%20with%20external%20position%20control.%20However%2C%20as%20architectures%20evolved%20to%0Aenhance%20image%20quality%2C%20these%20techniques%20became%20incompatible%20with%20modern%20models.%0AWe%20propose%20Stitch%2C%20a%20training-free%20method%20for%20incorporating%20external%20position%0Acontrol%20into%20Multi-Modal%20Diffusion%20Transformers%20%28MMDiT%29%20via%0Aautomatically-generated%20bounding%20boxes.%20Stitch%20produces%20images%20that%20are%20both%0Aspatially%20accurate%20and%20visually%20appealing%20by%20generating%20individual%20objects%0Awithin%20designated%20bounding%20boxes%20and%20seamlessly%20stitching%20them%20together.%20We%0Afind%20that%20targeted%20attention%20heads%20capture%20the%20information%20necessary%20to%20isolate%0Aand%20cut%20out%20individual%20objects%20mid-generation%2C%20without%20needing%20to%20fully%0Acomplete%20the%20image.%20We%20evaluate%20Stitch%20on%20PosEval%2C%20our%20benchmark%20for%0Aposition-based%20T2I%20generation.%20Featuring%20five%20new%20tasks%20that%20extend%20the%20concept%0Aof%20Position%20beyond%20the%20basic%20GenEval%20task%2C%20PosEval%20demonstrates%20that%20even%20top%0Amodels%20still%20have%20significant%20room%20for%20improvement%20in%20position-based%0Ageneration.%20Tested%20on%20Qwen-Image%2C%20FLUX%2C%20and%20SD3.5%2C%20Stitch%20consistently%20enhances%0Abase%20models%2C%20even%20improving%20FLUX%20by%20218%25%20on%20GenEval%27s%20Position%20task%20and%20by%20206%25%0Aon%20PosEval.%20Stitch%20achieves%20state-of-the-art%20results%20with%20Qwen-Image%20on%0APosEval%2C%20improving%20over%20previous%20models%20by%2054%25%2C%20all%20accomplished%20while%0Aintegrating%20position%20control%20into%20leading%20models%20training-free.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ExplainableML/Stitch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStitch%253A%2520Training-Free%2520Position%2520Control%2520in%2520Multimodal%2520Diffusion%250A%2520%2520Transformers%26entry.906535625%3DJessica%2520Bader%2520and%2520Mateusz%2520Pach%2520and%2520Maria%2520A.%2520Bravo%2520and%2520Serge%2520Belongie%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520models%2520have%2520advanced%2520rapidly%2520in%2520recent%2520years%252C%250Abut%2520accurately%2520capturing%2520spatial%2520relationships%2520like%2520%2522above%2522%2520or%2520%2522to%2520the%2520right%250Aof%2522%2520poses%2520a%2520persistent%2520challenge.%2520Earlier%2520methods%2520improved%2520spatial%2520relationship%250Afollowing%2520with%2520external%2520position%2520control.%2520However%252C%2520as%2520architectures%2520evolved%2520to%250Aenhance%2520image%2520quality%252C%2520these%2520techniques%2520became%2520incompatible%2520with%2520modern%2520models.%250AWe%2520propose%2520Stitch%252C%2520a%2520training-free%2520method%2520for%2520incorporating%2520external%2520position%250Acontrol%2520into%2520Multi-Modal%2520Diffusion%2520Transformers%2520%2528MMDiT%2529%2520via%250Aautomatically-generated%2520bounding%2520boxes.%2520Stitch%2520produces%2520images%2520that%2520are%2520both%250Aspatially%2520accurate%2520and%2520visually%2520appealing%2520by%2520generating%2520individual%2520objects%250Awithin%2520designated%2520bounding%2520boxes%2520and%2520seamlessly%2520stitching%2520them%2520together.%2520We%250Afind%2520that%2520targeted%2520attention%2520heads%2520capture%2520the%2520information%2520necessary%2520to%2520isolate%250Aand%2520cut%2520out%2520individual%2520objects%2520mid-generation%252C%2520without%2520needing%2520to%2520fully%250Acomplete%2520the%2520image.%2520We%2520evaluate%2520Stitch%2520on%2520PosEval%252C%2520our%2520benchmark%2520for%250Aposition-based%2520T2I%2520generation.%2520Featuring%2520five%2520new%2520tasks%2520that%2520extend%2520the%2520concept%250Aof%2520Position%2520beyond%2520the%2520basic%2520GenEval%2520task%252C%2520PosEval%2520demonstrates%2520that%2520even%2520top%250Amodels%2520still%2520have%2520significant%2520room%2520for%2520improvement%2520in%2520position-based%250Ageneration.%2520Tested%2520on%2520Qwen-Image%252C%2520FLUX%252C%2520and%2520SD3.5%252C%2520Stitch%2520consistently%2520enhances%250Abase%2520models%252C%2520even%2520improving%2520FLUX%2520by%2520218%2525%2520on%2520GenEval%2527s%2520Position%2520task%2520and%2520by%2520206%2525%250Aon%2520PosEval.%2520Stitch%2520achieves%2520state-of-the-art%2520results%2520with%2520Qwen-Image%2520on%250APosEval%252C%2520improving%2520over%2520previous%2520models%2520by%252054%2525%252C%2520all%2520accomplished%2520while%250Aintegrating%2520position%2520control%2520into%2520leading%2520models%2520training-free.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ExplainableML/Stitch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stitch%3A%20Training-Free%20Position%20Control%20in%20Multimodal%20Diffusion%0A%20%20Transformers&entry.906535625=Jessica%20Bader%20and%20Mateusz%20Pach%20and%20Maria%20A.%20Bravo%20and%20Serge%20Belongie%20and%20Zeynep%20Akata&entry.1292438233=%20%20Text-to-Image%20%28T2I%29%20generation%20models%20have%20advanced%20rapidly%20in%20recent%20years%2C%0Abut%20accurately%20capturing%20spatial%20relationships%20like%20%22above%22%20or%20%22to%20the%20right%0Aof%22%20poses%20a%20persistent%20challenge.%20Earlier%20methods%20improved%20spatial%20relationship%0Afollowing%20with%20external%20position%20control.%20However%2C%20as%20architectures%20evolved%20to%0Aenhance%20image%20quality%2C%20these%20techniques%20became%20incompatible%20with%20modern%20models.%0AWe%20propose%20Stitch%2C%20a%20training-free%20method%20for%20incorporating%20external%20position%0Acontrol%20into%20Multi-Modal%20Diffusion%20Transformers%20%28MMDiT%29%20via%0Aautomatically-generated%20bounding%20boxes.%20Stitch%20produces%20images%20that%20are%20both%0Aspatially%20accurate%20and%20visually%20appealing%20by%20generating%20individual%20objects%0Awithin%20designated%20bounding%20boxes%20and%20seamlessly%20stitching%20them%20together.%20We%0Afind%20that%20targeted%20attention%20heads%20capture%20the%20information%20necessary%20to%20isolate%0Aand%20cut%20out%20individual%20objects%20mid-generation%2C%20without%20needing%20to%20fully%0Acomplete%20the%20image.%20We%20evaluate%20Stitch%20on%20PosEval%2C%20our%20benchmark%20for%0Aposition-based%20T2I%20generation.%20Featuring%20five%20new%20tasks%20that%20extend%20the%20concept%0Aof%20Position%20beyond%20the%20basic%20GenEval%20task%2C%20PosEval%20demonstrates%20that%20even%20top%0Amodels%20still%20have%20significant%20room%20for%20improvement%20in%20position-based%0Ageneration.%20Tested%20on%20Qwen-Image%2C%20FLUX%2C%20and%20SD3.5%2C%20Stitch%20consistently%20enhances%0Abase%20models%2C%20even%20improving%20FLUX%20by%20218%25%20on%20GenEval%27s%20Position%20task%20and%20by%20206%25%0Aon%20PosEval.%20Stitch%20achieves%20state-of-the-art%20results%20with%20Qwen-Image%20on%0APosEval%2C%20improving%20over%20previous%20models%20by%2054%25%2C%20all%20accomplished%20while%0Aintegrating%20position%20control%20into%20leading%20models%20training-free.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ExplainableML/Stitch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26644v1&entry.124074799=Read"},
{"title": "Pretrain-Test Task Alignment Governs Generalization in In-Context\n  Learning", "author": "Mary I. Letey and Jacob A. Zavatone-Veth and Yue M. Lu and Cengiz Pehlevan", "abstract": "  In-context learning (ICL) is a central capability of Transformer models, but\nthe structures in data that enable its emergence and govern its robustness\nremain poorly understood. In this work, we study how the structure of\npretraining tasks governs generalization in ICL. Using a solvable model for ICL\nof linear regression by linear attention, we derive an exact expression for ICL\ngeneralization error in high dimensions under arbitrary pretraining-testing\ntask covariance mismatch. This leads to a new alignment measure that quantifies\nhow much information about the pretraining task distribution is useful for\ninference at test time. We show that this measure directly predicts ICL\nperformance not only in the solvable model but also in nonlinear Transformers.\nOur analysis further reveals a tradeoff between specialization and\ngeneralization in ICL: depending on task distribution alignment, increasing\npretraining task diversity can either improve or harm test performance.\nTogether, these results identify train-test task alignment as a key determinant\nof generalization in ICL.\n", "link": "http://arxiv.org/abs/2509.26551v1", "date": "2025-09-30", "relevancy": 2.4323, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pretrain-Test%20Task%20Alignment%20Governs%20Generalization%20in%20In-Context%0A%20%20Learning&body=Title%3A%20Pretrain-Test%20Task%20Alignment%20Governs%20Generalization%20in%20In-Context%0A%20%20Learning%0AAuthor%3A%20Mary%20I.%20Letey%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Yue%20M.%20Lu%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20is%20a%20central%20capability%20of%20Transformer%20models%2C%20but%0Athe%20structures%20in%20data%20that%20enable%20its%20emergence%20and%20govern%20its%20robustness%0Aremain%20poorly%20understood.%20In%20this%20work%2C%20we%20study%20how%20the%20structure%20of%0Apretraining%20tasks%20governs%20generalization%20in%20ICL.%20Using%20a%20solvable%20model%20for%20ICL%0Aof%20linear%20regression%20by%20linear%20attention%2C%20we%20derive%20an%20exact%20expression%20for%20ICL%0Ageneralization%20error%20in%20high%20dimensions%20under%20arbitrary%20pretraining-testing%0Atask%20covariance%20mismatch.%20This%20leads%20to%20a%20new%20alignment%20measure%20that%20quantifies%0Ahow%20much%20information%20about%20the%20pretraining%20task%20distribution%20is%20useful%20for%0Ainference%20at%20test%20time.%20We%20show%20that%20this%20measure%20directly%20predicts%20ICL%0Aperformance%20not%20only%20in%20the%20solvable%20model%20but%20also%20in%20nonlinear%20Transformers.%0AOur%20analysis%20further%20reveals%20a%20tradeoff%20between%20specialization%20and%0Ageneralization%20in%20ICL%3A%20depending%20on%20task%20distribution%20alignment%2C%20increasing%0Apretraining%20task%20diversity%20can%20either%20improve%20or%20harm%20test%20performance.%0ATogether%2C%20these%20results%20identify%20train-test%20task%20alignment%20as%20a%20key%20determinant%0Aof%20generalization%20in%20ICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPretrain-Test%2520Task%2520Alignment%2520Governs%2520Generalization%2520in%2520In-Context%250A%2520%2520Learning%26entry.906535625%3DMary%2520I.%2520Letey%2520and%2520Jacob%2520A.%2520Zavatone-Veth%2520and%2520Yue%2520M.%2520Lu%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520is%2520a%2520central%2520capability%2520of%2520Transformer%2520models%252C%2520but%250Athe%2520structures%2520in%2520data%2520that%2520enable%2520its%2520emergence%2520and%2520govern%2520its%2520robustness%250Aremain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520study%2520how%2520the%2520structure%2520of%250Apretraining%2520tasks%2520governs%2520generalization%2520in%2520ICL.%2520Using%2520a%2520solvable%2520model%2520for%2520ICL%250Aof%2520linear%2520regression%2520by%2520linear%2520attention%252C%2520we%2520derive%2520an%2520exact%2520expression%2520for%2520ICL%250Ageneralization%2520error%2520in%2520high%2520dimensions%2520under%2520arbitrary%2520pretraining-testing%250Atask%2520covariance%2520mismatch.%2520This%2520leads%2520to%2520a%2520new%2520alignment%2520measure%2520that%2520quantifies%250Ahow%2520much%2520information%2520about%2520the%2520pretraining%2520task%2520distribution%2520is%2520useful%2520for%250Ainference%2520at%2520test%2520time.%2520We%2520show%2520that%2520this%2520measure%2520directly%2520predicts%2520ICL%250Aperformance%2520not%2520only%2520in%2520the%2520solvable%2520model%2520but%2520also%2520in%2520nonlinear%2520Transformers.%250AOur%2520analysis%2520further%2520reveals%2520a%2520tradeoff%2520between%2520specialization%2520and%250Ageneralization%2520in%2520ICL%253A%2520depending%2520on%2520task%2520distribution%2520alignment%252C%2520increasing%250Apretraining%2520task%2520diversity%2520can%2520either%2520improve%2520or%2520harm%2520test%2520performance.%250ATogether%252C%2520these%2520results%2520identify%2520train-test%2520task%2520alignment%2520as%2520a%2520key%2520determinant%250Aof%2520generalization%2520in%2520ICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pretrain-Test%20Task%20Alignment%20Governs%20Generalization%20in%20In-Context%0A%20%20Learning&entry.906535625=Mary%20I.%20Letey%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Yue%20M.%20Lu%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20is%20a%20central%20capability%20of%20Transformer%20models%2C%20but%0Athe%20structures%20in%20data%20that%20enable%20its%20emergence%20and%20govern%20its%20robustness%0Aremain%20poorly%20understood.%20In%20this%20work%2C%20we%20study%20how%20the%20structure%20of%0Apretraining%20tasks%20governs%20generalization%20in%20ICL.%20Using%20a%20solvable%20model%20for%20ICL%0Aof%20linear%20regression%20by%20linear%20attention%2C%20we%20derive%20an%20exact%20expression%20for%20ICL%0Ageneralization%20error%20in%20high%20dimensions%20under%20arbitrary%20pretraining-testing%0Atask%20covariance%20mismatch.%20This%20leads%20to%20a%20new%20alignment%20measure%20that%20quantifies%0Ahow%20much%20information%20about%20the%20pretraining%20task%20distribution%20is%20useful%20for%0Ainference%20at%20test%20time.%20We%20show%20that%20this%20measure%20directly%20predicts%20ICL%0Aperformance%20not%20only%20in%20the%20solvable%20model%20but%20also%20in%20nonlinear%20Transformers.%0AOur%20analysis%20further%20reveals%20a%20tradeoff%20between%20specialization%20and%0Ageneralization%20in%20ICL%3A%20depending%20on%20task%20distribution%20alignment%2C%20increasing%0Apretraining%20task%20diversity%20can%20either%20improve%20or%20harm%20test%20performance.%0ATogether%2C%20these%20results%20identify%20train-test%20task%20alignment%20as%20a%20key%20determinant%0Aof%20generalization%20in%20ICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26551v1&entry.124074799=Read"},
{"title": "Radio-based Multi-Robot Odometry and Relative Localization", "author": "Andr\u00e9s Mart\u00ednez-Silva and David Alejo and Luis Merino and Fernando Caballero", "abstract": "  Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And\nRanging (radar), which have traditionally seen limited adoption in robotics,\nare experiencing a boost in popularity thanks to their robustness to harsh\nenvironmental conditions and cluttered environments. This work proposes a\nmulti-robot UGV-UAV localization system that leverages the two technologies\nwith inexpensive and readily-available sensors, such as Inertial Measurement\nUnits (IMUs) and wheel encoders, to estimate the relative position of an aerial\nrobot with respect to a ground robot. The first stage of the system pipeline\nincludes a nonlinear optimization framework to trilaterate the location of the\naerial platform based on UWB range data, and a radar pre-processing module with\nloosely coupled ego-motion estimation which has been adapted for a multi-robot\nscenario. Then, the pre-processed radar data as well as the relative\ntransformation are fed to a pose-graph optimization framework with odometry and\ninter-robot constraints. The system, implemented for the Robotic Operating\nSystem (ROS 2) with the Ceres optimizer, has been validated in\nSoftware-in-the-Loop (SITL) simulations and in a real-world dataset. The\nproposed relative localization module outperforms state-of-the-art closed-form\nmethods which are less robust to noise. Our SITL environment includes a custom\nGazebo plugin for generating realistic UWB measurements modeled after real\ndata. Conveniently, the proposed factor graph formulation makes the system\nreadily extensible to full Simultaneous Localization And Mapping (SLAM).\nFinally, all the code and experimental data is publicly available to support\nreproducibility and to serve as a common open dataset for benchmarking.\n", "link": "http://arxiv.org/abs/2509.26558v1", "date": "2025-09-30", "relevancy": 2.4242, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6185}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radio-based%20Multi-Robot%20Odometry%20and%20Relative%20Localization&body=Title%3A%20Radio-based%20Multi-Robot%20Odometry%20and%20Relative%20Localization%0AAuthor%3A%20Andr%C3%A9s%20Mart%C3%ADnez-Silva%20and%20David%20Alejo%20and%20Luis%20Merino%20and%20Fernando%20Caballero%0AAbstract%3A%20%20%20Radio-based%20methods%20such%20as%20Ultra-Wideband%20%28UWB%29%20and%20RAdio%20Detection%20And%0ARanging%20%28radar%29%2C%20which%20have%20traditionally%20seen%20limited%20adoption%20in%20robotics%2C%0Aare%20experiencing%20a%20boost%20in%20popularity%20thanks%20to%20their%20robustness%20to%20harsh%0Aenvironmental%20conditions%20and%20cluttered%20environments.%20This%20work%20proposes%20a%0Amulti-robot%20UGV-UAV%20localization%20system%20that%20leverages%20the%20two%20technologies%0Awith%20inexpensive%20and%20readily-available%20sensors%2C%20such%20as%20Inertial%20Measurement%0AUnits%20%28IMUs%29%20and%20wheel%20encoders%2C%20to%20estimate%20the%20relative%20position%20of%20an%20aerial%0Arobot%20with%20respect%20to%20a%20ground%20robot.%20The%20first%20stage%20of%20the%20system%20pipeline%0Aincludes%20a%20nonlinear%20optimization%20framework%20to%20trilaterate%20the%20location%20of%20the%0Aaerial%20platform%20based%20on%20UWB%20range%20data%2C%20and%20a%20radar%20pre-processing%20module%20with%0Aloosely%20coupled%20ego-motion%20estimation%20which%20has%20been%20adapted%20for%20a%20multi-robot%0Ascenario.%20Then%2C%20the%20pre-processed%20radar%20data%20as%20well%20as%20the%20relative%0Atransformation%20are%20fed%20to%20a%20pose-graph%20optimization%20framework%20with%20odometry%20and%0Ainter-robot%20constraints.%20The%20system%2C%20implemented%20for%20the%20Robotic%20Operating%0ASystem%20%28ROS%202%29%20with%20the%20Ceres%20optimizer%2C%20has%20been%20validated%20in%0ASoftware-in-the-Loop%20%28SITL%29%20simulations%20and%20in%20a%20real-world%20dataset.%20The%0Aproposed%20relative%20localization%20module%20outperforms%20state-of-the-art%20closed-form%0Amethods%20which%20are%20less%20robust%20to%20noise.%20Our%20SITL%20environment%20includes%20a%20custom%0AGazebo%20plugin%20for%20generating%20realistic%20UWB%20measurements%20modeled%20after%20real%0Adata.%20Conveniently%2C%20the%20proposed%20factor%20graph%20formulation%20makes%20the%20system%0Areadily%20extensible%20to%20full%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29.%0AFinally%2C%20all%20the%20code%20and%20experimental%20data%20is%20publicly%20available%20to%20support%0Areproducibility%20and%20to%20serve%20as%20a%20common%20open%20dataset%20for%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadio-based%2520Multi-Robot%2520Odometry%2520and%2520Relative%2520Localization%26entry.906535625%3DAndr%25C3%25A9s%2520Mart%25C3%25ADnez-Silva%2520and%2520David%2520Alejo%2520and%2520Luis%2520Merino%2520and%2520Fernando%2520Caballero%26entry.1292438233%3D%2520%2520Radio-based%2520methods%2520such%2520as%2520Ultra-Wideband%2520%2528UWB%2529%2520and%2520RAdio%2520Detection%2520And%250ARanging%2520%2528radar%2529%252C%2520which%2520have%2520traditionally%2520seen%2520limited%2520adoption%2520in%2520robotics%252C%250Aare%2520experiencing%2520a%2520boost%2520in%2520popularity%2520thanks%2520to%2520their%2520robustness%2520to%2520harsh%250Aenvironmental%2520conditions%2520and%2520cluttered%2520environments.%2520This%2520work%2520proposes%2520a%250Amulti-robot%2520UGV-UAV%2520localization%2520system%2520that%2520leverages%2520the%2520two%2520technologies%250Awith%2520inexpensive%2520and%2520readily-available%2520sensors%252C%2520such%2520as%2520Inertial%2520Measurement%250AUnits%2520%2528IMUs%2529%2520and%2520wheel%2520encoders%252C%2520to%2520estimate%2520the%2520relative%2520position%2520of%2520an%2520aerial%250Arobot%2520with%2520respect%2520to%2520a%2520ground%2520robot.%2520The%2520first%2520stage%2520of%2520the%2520system%2520pipeline%250Aincludes%2520a%2520nonlinear%2520optimization%2520framework%2520to%2520trilaterate%2520the%2520location%2520of%2520the%250Aaerial%2520platform%2520based%2520on%2520UWB%2520range%2520data%252C%2520and%2520a%2520radar%2520pre-processing%2520module%2520with%250Aloosely%2520coupled%2520ego-motion%2520estimation%2520which%2520has%2520been%2520adapted%2520for%2520a%2520multi-robot%250Ascenario.%2520Then%252C%2520the%2520pre-processed%2520radar%2520data%2520as%2520well%2520as%2520the%2520relative%250Atransformation%2520are%2520fed%2520to%2520a%2520pose-graph%2520optimization%2520framework%2520with%2520odometry%2520and%250Ainter-robot%2520constraints.%2520The%2520system%252C%2520implemented%2520for%2520the%2520Robotic%2520Operating%250ASystem%2520%2528ROS%25202%2529%2520with%2520the%2520Ceres%2520optimizer%252C%2520has%2520been%2520validated%2520in%250ASoftware-in-the-Loop%2520%2528SITL%2529%2520simulations%2520and%2520in%2520a%2520real-world%2520dataset.%2520The%250Aproposed%2520relative%2520localization%2520module%2520outperforms%2520state-of-the-art%2520closed-form%250Amethods%2520which%2520are%2520less%2520robust%2520to%2520noise.%2520Our%2520SITL%2520environment%2520includes%2520a%2520custom%250AGazebo%2520plugin%2520for%2520generating%2520realistic%2520UWB%2520measurements%2520modeled%2520after%2520real%250Adata.%2520Conveniently%252C%2520the%2520proposed%2520factor%2520graph%2520formulation%2520makes%2520the%2520system%250Areadily%2520extensible%2520to%2520full%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529.%250AFinally%252C%2520all%2520the%2520code%2520and%2520experimental%2520data%2520is%2520publicly%2520available%2520to%2520support%250Areproducibility%2520and%2520to%2520serve%2520as%2520a%2520common%2520open%2520dataset%2520for%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radio-based%20Multi-Robot%20Odometry%20and%20Relative%20Localization&entry.906535625=Andr%C3%A9s%20Mart%C3%ADnez-Silva%20and%20David%20Alejo%20and%20Luis%20Merino%20and%20Fernando%20Caballero&entry.1292438233=%20%20Radio-based%20methods%20such%20as%20Ultra-Wideband%20%28UWB%29%20and%20RAdio%20Detection%20And%0ARanging%20%28radar%29%2C%20which%20have%20traditionally%20seen%20limited%20adoption%20in%20robotics%2C%0Aare%20experiencing%20a%20boost%20in%20popularity%20thanks%20to%20their%20robustness%20to%20harsh%0Aenvironmental%20conditions%20and%20cluttered%20environments.%20This%20work%20proposes%20a%0Amulti-robot%20UGV-UAV%20localization%20system%20that%20leverages%20the%20two%20technologies%0Awith%20inexpensive%20and%20readily-available%20sensors%2C%20such%20as%20Inertial%20Measurement%0AUnits%20%28IMUs%29%20and%20wheel%20encoders%2C%20to%20estimate%20the%20relative%20position%20of%20an%20aerial%0Arobot%20with%20respect%20to%20a%20ground%20robot.%20The%20first%20stage%20of%20the%20system%20pipeline%0Aincludes%20a%20nonlinear%20optimization%20framework%20to%20trilaterate%20the%20location%20of%20the%0Aaerial%20platform%20based%20on%20UWB%20range%20data%2C%20and%20a%20radar%20pre-processing%20module%20with%0Aloosely%20coupled%20ego-motion%20estimation%20which%20has%20been%20adapted%20for%20a%20multi-robot%0Ascenario.%20Then%2C%20the%20pre-processed%20radar%20data%20as%20well%20as%20the%20relative%0Atransformation%20are%20fed%20to%20a%20pose-graph%20optimization%20framework%20with%20odometry%20and%0Ainter-robot%20constraints.%20The%20system%2C%20implemented%20for%20the%20Robotic%20Operating%0ASystem%20%28ROS%202%29%20with%20the%20Ceres%20optimizer%2C%20has%20been%20validated%20in%0ASoftware-in-the-Loop%20%28SITL%29%20simulations%20and%20in%20a%20real-world%20dataset.%20The%0Aproposed%20relative%20localization%20module%20outperforms%20state-of-the-art%20closed-form%0Amethods%20which%20are%20less%20robust%20to%20noise.%20Our%20SITL%20environment%20includes%20a%20custom%0AGazebo%20plugin%20for%20generating%20realistic%20UWB%20measurements%20modeled%20after%20real%0Adata.%20Conveniently%2C%20the%20proposed%20factor%20graph%20formulation%20makes%20the%20system%0Areadily%20extensible%20to%20full%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29.%0AFinally%2C%20all%20the%20code%20and%20experimental%20data%20is%20publicly%20available%20to%20support%0Areproducibility%20and%20to%20serve%20as%20a%20common%20open%20dataset%20for%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26558v1&entry.124074799=Read"},
{"title": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale", "author": "Anusha Krishnan and Shaohui Liu and Paul-Edouard Sarlin and Oscar Gentilhomme and David Caruso and Maurizio Monge and Richard Newcombe and Jakob Engel and Marc Pollefeys", "abstract": "  Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard\nsensors is critical for wearable devices capturing egocentric data, which\nexhibits specific challenges, such as a wider diversity of motions and\nviewpoints, prevalent dynamic visual content, or long sessions affected by\ntime-varying sensor calibration. While recent progress on SLAM has been swift,\nacademic research is still driven by benchmarks that do not reflect these\nchallenges or do not offer sufficiently accurate ground truth poses. In this\npaper, we introduce a new dataset and benchmark for visual-inertial SLAM with\negocentric, multi-modal data. We record hours and kilometers of trajectories\nthrough a city center with glasses-like devices equipped with various sensors.\nWe leverage surveying tools to obtain control points as indirect pose\nannotations that are metric, centimeter-accurate, and available at city scale.\nThis makes it possible to evaluate extreme trajectories that involve walking at\nnight or traveling in a vehicle. We show that state-of-the-art systems\ndeveloped by academia are not robust to these challenges and we identify\ncomponents that are responsible for this. In addition, we design tracks with\ndifferent levels of difficulty to ease in-depth analysis and evaluation of less\nmature approaches. The dataset and benchmark are available at\nhttps://www.lamaria.ethz.ch.\n", "link": "http://arxiv.org/abs/2509.26639v1", "date": "2025-09-30", "relevancy": 2.4195, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6196}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Egocentric%20Visual-Inertial%20SLAM%20at%20City%20Scale&body=Title%3A%20Benchmarking%20Egocentric%20Visual-Inertial%20SLAM%20at%20City%20Scale%0AAuthor%3A%20Anusha%20Krishnan%20and%20Shaohui%20Liu%20and%20Paul-Edouard%20Sarlin%20and%20Oscar%20Gentilhomme%20and%20David%20Caruso%20and%20Maurizio%20Monge%20and%20Richard%20Newcombe%20and%20Jakob%20Engel%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Precise%206-DoF%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20from%20onboard%0Asensors%20is%20critical%20for%20wearable%20devices%20capturing%20egocentric%20data%2C%20which%0Aexhibits%20specific%20challenges%2C%20such%20as%20a%20wider%20diversity%20of%20motions%20and%0Aviewpoints%2C%20prevalent%20dynamic%20visual%20content%2C%20or%20long%20sessions%20affected%20by%0Atime-varying%20sensor%20calibration.%20While%20recent%20progress%20on%20SLAM%20has%20been%20swift%2C%0Aacademic%20research%20is%20still%20driven%20by%20benchmarks%20that%20do%20not%20reflect%20these%0Achallenges%20or%20do%20not%20offer%20sufficiently%20accurate%20ground%20truth%20poses.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20dataset%20and%20benchmark%20for%20visual-inertial%20SLAM%20with%0Aegocentric%2C%20multi-modal%20data.%20We%20record%20hours%20and%20kilometers%20of%20trajectories%0Athrough%20a%20city%20center%20with%20glasses-like%20devices%20equipped%20with%20various%20sensors.%0AWe%20leverage%20surveying%20tools%20to%20obtain%20control%20points%20as%20indirect%20pose%0Aannotations%20that%20are%20metric%2C%20centimeter-accurate%2C%20and%20available%20at%20city%20scale.%0AThis%20makes%20it%20possible%20to%20evaluate%20extreme%20trajectories%20that%20involve%20walking%20at%0Anight%20or%20traveling%20in%20a%20vehicle.%20We%20show%20that%20state-of-the-art%20systems%0Adeveloped%20by%20academia%20are%20not%20robust%20to%20these%20challenges%20and%20we%20identify%0Acomponents%20that%20are%20responsible%20for%20this.%20In%20addition%2C%20we%20design%20tracks%20with%0Adifferent%20levels%20of%20difficulty%20to%20ease%20in-depth%20analysis%20and%20evaluation%20of%20less%0Amature%20approaches.%20The%20dataset%20and%20benchmark%20are%20available%20at%0Ahttps%3A//www.lamaria.ethz.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Egocentric%2520Visual-Inertial%2520SLAM%2520at%2520City%2520Scale%26entry.906535625%3DAnusha%2520Krishnan%2520and%2520Shaohui%2520Liu%2520and%2520Paul-Edouard%2520Sarlin%2520and%2520Oscar%2520Gentilhomme%2520and%2520David%2520Caruso%2520and%2520Maurizio%2520Monge%2520and%2520Richard%2520Newcombe%2520and%2520Jakob%2520Engel%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Precise%25206-DoF%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520from%2520onboard%250Asensors%2520is%2520critical%2520for%2520wearable%2520devices%2520capturing%2520egocentric%2520data%252C%2520which%250Aexhibits%2520specific%2520challenges%252C%2520such%2520as%2520a%2520wider%2520diversity%2520of%2520motions%2520and%250Aviewpoints%252C%2520prevalent%2520dynamic%2520visual%2520content%252C%2520or%2520long%2520sessions%2520affected%2520by%250Atime-varying%2520sensor%2520calibration.%2520While%2520recent%2520progress%2520on%2520SLAM%2520has%2520been%2520swift%252C%250Aacademic%2520research%2520is%2520still%2520driven%2520by%2520benchmarks%2520that%2520do%2520not%2520reflect%2520these%250Achallenges%2520or%2520do%2520not%2520offer%2520sufficiently%2520accurate%2520ground%2520truth%2520poses.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520new%2520dataset%2520and%2520benchmark%2520for%2520visual-inertial%2520SLAM%2520with%250Aegocentric%252C%2520multi-modal%2520data.%2520We%2520record%2520hours%2520and%2520kilometers%2520of%2520trajectories%250Athrough%2520a%2520city%2520center%2520with%2520glasses-like%2520devices%2520equipped%2520with%2520various%2520sensors.%250AWe%2520leverage%2520surveying%2520tools%2520to%2520obtain%2520control%2520points%2520as%2520indirect%2520pose%250Aannotations%2520that%2520are%2520metric%252C%2520centimeter-accurate%252C%2520and%2520available%2520at%2520city%2520scale.%250AThis%2520makes%2520it%2520possible%2520to%2520evaluate%2520extreme%2520trajectories%2520that%2520involve%2520walking%2520at%250Anight%2520or%2520traveling%2520in%2520a%2520vehicle.%2520We%2520show%2520that%2520state-of-the-art%2520systems%250Adeveloped%2520by%2520academia%2520are%2520not%2520robust%2520to%2520these%2520challenges%2520and%2520we%2520identify%250Acomponents%2520that%2520are%2520responsible%2520for%2520this.%2520In%2520addition%252C%2520we%2520design%2520tracks%2520with%250Adifferent%2520levels%2520of%2520difficulty%2520to%2520ease%2520in-depth%2520analysis%2520and%2520evaluation%2520of%2520less%250Amature%2520approaches.%2520The%2520dataset%2520and%2520benchmark%2520are%2520available%2520at%250Ahttps%253A//www.lamaria.ethz.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Egocentric%20Visual-Inertial%20SLAM%20at%20City%20Scale&entry.906535625=Anusha%20Krishnan%20and%20Shaohui%20Liu%20and%20Paul-Edouard%20Sarlin%20and%20Oscar%20Gentilhomme%20and%20David%20Caruso%20and%20Maurizio%20Monge%20and%20Richard%20Newcombe%20and%20Jakob%20Engel%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Precise%206-DoF%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20from%20onboard%0Asensors%20is%20critical%20for%20wearable%20devices%20capturing%20egocentric%20data%2C%20which%0Aexhibits%20specific%20challenges%2C%20such%20as%20a%20wider%20diversity%20of%20motions%20and%0Aviewpoints%2C%20prevalent%20dynamic%20visual%20content%2C%20or%20long%20sessions%20affected%20by%0Atime-varying%20sensor%20calibration.%20While%20recent%20progress%20on%20SLAM%20has%20been%20swift%2C%0Aacademic%20research%20is%20still%20driven%20by%20benchmarks%20that%20do%20not%20reflect%20these%0Achallenges%20or%20do%20not%20offer%20sufficiently%20accurate%20ground%20truth%20poses.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20dataset%20and%20benchmark%20for%20visual-inertial%20SLAM%20with%0Aegocentric%2C%20multi-modal%20data.%20We%20record%20hours%20and%20kilometers%20of%20trajectories%0Athrough%20a%20city%20center%20with%20glasses-like%20devices%20equipped%20with%20various%20sensors.%0AWe%20leverage%20surveying%20tools%20to%20obtain%20control%20points%20as%20indirect%20pose%0Aannotations%20that%20are%20metric%2C%20centimeter-accurate%2C%20and%20available%20at%20city%20scale.%0AThis%20makes%20it%20possible%20to%20evaluate%20extreme%20trajectories%20that%20involve%20walking%20at%0Anight%20or%20traveling%20in%20a%20vehicle.%20We%20show%20that%20state-of-the-art%20systems%0Adeveloped%20by%20academia%20are%20not%20robust%20to%20these%20challenges%20and%20we%20identify%0Acomponents%20that%20are%20responsible%20for%20this.%20In%20addition%2C%20we%20design%20tracks%20with%0Adifferent%20levels%20of%20difficulty%20to%20ease%20in-depth%20analysis%20and%20evaluation%20of%20less%0Amature%20approaches.%20The%20dataset%20and%20benchmark%20are%20available%20at%0Ahttps%3A//www.lamaria.ethz.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26639v1&entry.124074799=Read"},
{"title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF\n  and RGB Guidance", "author": "Jijun Xiang and Longliang Liu and Xuan Zhu and Xianqi Wang and Min Lin and Xin Yang", "abstract": "  Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach.\n", "link": "http://arxiv.org/abs/2509.26498v1", "date": "2025-09-30", "relevancy": 2.3994, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6142}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEPTHOR%2B%2B%3A%20Robust%20Depth%20Enhancement%20from%20a%20Real-World%20Lightweight%20dToF%0A%20%20and%20RGB%20Guidance&body=Title%3A%20DEPTHOR%2B%2B%3A%20Robust%20Depth%20Enhancement%20from%20a%20Real-World%20Lightweight%20dToF%0A%20%20and%20RGB%20Guidance%0AAuthor%3A%20Jijun%20Xiang%20and%20Longliang%20Liu%20and%20Xuan%20Zhu%20and%20Xianqi%20Wang%20and%20Min%20Lin%20and%20Xin%20Yang%0AAbstract%3A%20%20%20Depth%20enhancement%2C%20which%20converts%20raw%20dToF%20signals%20into%20dense%20depth%20maps%0Ausing%20RGB%20guidance%2C%20is%20crucial%20for%20improving%20depth%20perception%20in%20high-precision%0Atasks%20such%20as%203D%20reconstruction%20and%20SLAM.%20However%2C%20existing%20methods%20often%0Aassume%20ideal%20dToF%20inputs%20and%20perfect%20dToF-RGB%20alignment%2C%20overlooking%0Acalibration%20errors%20and%20anomalies%2C%20thus%20limiting%20real-world%20applicability.%20This%0Awork%20systematically%20analyzes%20the%20noise%20characteristics%20of%20real-world%0Alightweight%20dToF%20sensors%20and%20proposes%20a%20practical%20and%20novel%20depth%20completion%0Aframework%2C%20DEPTHOR%2B%2B%2C%20which%20enhances%20robustness%20to%20noisy%20dToF%20inputs%20from%20three%0Akey%20aspects.%20First%2C%20we%20introduce%20a%20simulation%20method%20based%20on%20synthetic%0Adatasets%20to%20generate%20realistic%20training%20samples%20for%20robust%20model%20training.%0ASecond%2C%20we%20propose%20a%20learnable-parameter-free%20anomaly%20detection%20mechanism%20to%0Aidentify%20and%20remove%20erroneous%20dToF%20measurements%2C%20preventing%20misleading%0Apropagation%20during%20completion.%20Third%2C%20we%20design%20a%20depth%20completion%20network%0Atailored%20to%20noisy%20dToF%20inputs%2C%20which%20integrates%20RGB%20images%20and%20pre-trained%0Amonocular%20depth%20estimation%20priors%20to%20improve%20depth%20recovery%20in%20challenging%0Aregions.%20On%20the%20ZJU-L5%20dataset%20and%20real-world%20samples%2C%20our%20training%20strategy%0Asignificantly%20boosts%20existing%20depth%20completion%20models%2C%20with%20our%20model%20achieving%0Astate-of-the-art%20performance%2C%20improving%20RMSE%20and%20Rel%20by%2022%25%20and%2011%25%20on%20average.%0AOn%20the%20Mirror3D-NYU%20dataset%2C%20by%20incorporating%20the%20anomaly%20detection%20method%2C%20our%0Amodel%20improves%20upon%20the%20previous%20SOTA%20by%2037%25%20in%20mirror%20regions.%20On%20the%20Hammer%0Adataset%2C%20using%20simulated%20low-cost%20dToF%20data%20from%20RealSense%20L515%2C%20our%20method%0Asurpasses%20the%20L515%20measurements%20with%20an%20average%20gain%20of%2022%25%2C%20demonstrating%20its%0Apotential%20to%20enable%20low-cost%20sensors%20to%20outperform%20higher-end%20devices.%0AQualitative%20results%20across%20diverse%20real-world%20datasets%20further%20validate%20the%0Aeffectiveness%20and%20generalizability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEPTHOR%252B%252B%253A%2520Robust%2520Depth%2520Enhancement%2520from%2520a%2520Real-World%2520Lightweight%2520dToF%250A%2520%2520and%2520RGB%2520Guidance%26entry.906535625%3DJijun%2520Xiang%2520and%2520Longliang%2520Liu%2520and%2520Xuan%2520Zhu%2520and%2520Xianqi%2520Wang%2520and%2520Min%2520Lin%2520and%2520Xin%2520Yang%26entry.1292438233%3D%2520%2520Depth%2520enhancement%252C%2520which%2520converts%2520raw%2520dToF%2520signals%2520into%2520dense%2520depth%2520maps%250Ausing%2520RGB%2520guidance%252C%2520is%2520crucial%2520for%2520improving%2520depth%2520perception%2520in%2520high-precision%250Atasks%2520such%2520as%25203D%2520reconstruction%2520and%2520SLAM.%2520However%252C%2520existing%2520methods%2520often%250Aassume%2520ideal%2520dToF%2520inputs%2520and%2520perfect%2520dToF-RGB%2520alignment%252C%2520overlooking%250Acalibration%2520errors%2520and%2520anomalies%252C%2520thus%2520limiting%2520real-world%2520applicability.%2520This%250Awork%2520systematically%2520analyzes%2520the%2520noise%2520characteristics%2520of%2520real-world%250Alightweight%2520dToF%2520sensors%2520and%2520proposes%2520a%2520practical%2520and%2520novel%2520depth%2520completion%250Aframework%252C%2520DEPTHOR%252B%252B%252C%2520which%2520enhances%2520robustness%2520to%2520noisy%2520dToF%2520inputs%2520from%2520three%250Akey%2520aspects.%2520First%252C%2520we%2520introduce%2520a%2520simulation%2520method%2520based%2520on%2520synthetic%250Adatasets%2520to%2520generate%2520realistic%2520training%2520samples%2520for%2520robust%2520model%2520training.%250ASecond%252C%2520we%2520propose%2520a%2520learnable-parameter-free%2520anomaly%2520detection%2520mechanism%2520to%250Aidentify%2520and%2520remove%2520erroneous%2520dToF%2520measurements%252C%2520preventing%2520misleading%250Apropagation%2520during%2520completion.%2520Third%252C%2520we%2520design%2520a%2520depth%2520completion%2520network%250Atailored%2520to%2520noisy%2520dToF%2520inputs%252C%2520which%2520integrates%2520RGB%2520images%2520and%2520pre-trained%250Amonocular%2520depth%2520estimation%2520priors%2520to%2520improve%2520depth%2520recovery%2520in%2520challenging%250Aregions.%2520On%2520the%2520ZJU-L5%2520dataset%2520and%2520real-world%2520samples%252C%2520our%2520training%2520strategy%250Asignificantly%2520boosts%2520existing%2520depth%2520completion%2520models%252C%2520with%2520our%2520model%2520achieving%250Astate-of-the-art%2520performance%252C%2520improving%2520RMSE%2520and%2520Rel%2520by%252022%2525%2520and%252011%2525%2520on%2520average.%250AOn%2520the%2520Mirror3D-NYU%2520dataset%252C%2520by%2520incorporating%2520the%2520anomaly%2520detection%2520method%252C%2520our%250Amodel%2520improves%2520upon%2520the%2520previous%2520SOTA%2520by%252037%2525%2520in%2520mirror%2520regions.%2520On%2520the%2520Hammer%250Adataset%252C%2520using%2520simulated%2520low-cost%2520dToF%2520data%2520from%2520RealSense%2520L515%252C%2520our%2520method%250Asurpasses%2520the%2520L515%2520measurements%2520with%2520an%2520average%2520gain%2520of%252022%2525%252C%2520demonstrating%2520its%250Apotential%2520to%2520enable%2520low-cost%2520sensors%2520to%2520outperform%2520higher-end%2520devices.%250AQualitative%2520results%2520across%2520diverse%2520real-world%2520datasets%2520further%2520validate%2520the%250Aeffectiveness%2520and%2520generalizability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEPTHOR%2B%2B%3A%20Robust%20Depth%20Enhancement%20from%20a%20Real-World%20Lightweight%20dToF%0A%20%20and%20RGB%20Guidance&entry.906535625=Jijun%20Xiang%20and%20Longliang%20Liu%20and%20Xuan%20Zhu%20and%20Xianqi%20Wang%20and%20Min%20Lin%20and%20Xin%20Yang&entry.1292438233=%20%20Depth%20enhancement%2C%20which%20converts%20raw%20dToF%20signals%20into%20dense%20depth%20maps%0Ausing%20RGB%20guidance%2C%20is%20crucial%20for%20improving%20depth%20perception%20in%20high-precision%0Atasks%20such%20as%203D%20reconstruction%20and%20SLAM.%20However%2C%20existing%20methods%20often%0Aassume%20ideal%20dToF%20inputs%20and%20perfect%20dToF-RGB%20alignment%2C%20overlooking%0Acalibration%20errors%20and%20anomalies%2C%20thus%20limiting%20real-world%20applicability.%20This%0Awork%20systematically%20analyzes%20the%20noise%20characteristics%20of%20real-world%0Alightweight%20dToF%20sensors%20and%20proposes%20a%20practical%20and%20novel%20depth%20completion%0Aframework%2C%20DEPTHOR%2B%2B%2C%20which%20enhances%20robustness%20to%20noisy%20dToF%20inputs%20from%20three%0Akey%20aspects.%20First%2C%20we%20introduce%20a%20simulation%20method%20based%20on%20synthetic%0Adatasets%20to%20generate%20realistic%20training%20samples%20for%20robust%20model%20training.%0ASecond%2C%20we%20propose%20a%20learnable-parameter-free%20anomaly%20detection%20mechanism%20to%0Aidentify%20and%20remove%20erroneous%20dToF%20measurements%2C%20preventing%20misleading%0Apropagation%20during%20completion.%20Third%2C%20we%20design%20a%20depth%20completion%20network%0Atailored%20to%20noisy%20dToF%20inputs%2C%20which%20integrates%20RGB%20images%20and%20pre-trained%0Amonocular%20depth%20estimation%20priors%20to%20improve%20depth%20recovery%20in%20challenging%0Aregions.%20On%20the%20ZJU-L5%20dataset%20and%20real-world%20samples%2C%20our%20training%20strategy%0Asignificantly%20boosts%20existing%20depth%20completion%20models%2C%20with%20our%20model%20achieving%0Astate-of-the-art%20performance%2C%20improving%20RMSE%20and%20Rel%20by%2022%25%20and%2011%25%20on%20average.%0AOn%20the%20Mirror3D-NYU%20dataset%2C%20by%20incorporating%20the%20anomaly%20detection%20method%2C%20our%0Amodel%20improves%20upon%20the%20previous%20SOTA%20by%2037%25%20in%20mirror%20regions.%20On%20the%20Hammer%0Adataset%2C%20using%20simulated%20low-cost%20dToF%20data%20from%20RealSense%20L515%2C%20our%20method%0Asurpasses%20the%20L515%20measurements%20with%20an%20average%20gain%20of%2022%25%2C%20demonstrating%20its%0Apotential%20to%20enable%20low-cost%20sensors%20to%20outperform%20higher-end%20devices.%0AQualitative%20results%20across%20diverse%20real-world%20datasets%20further%20validate%20the%0Aeffectiveness%20and%20generalizability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26498v1&entry.124074799=Read"},
{"title": "DA$^2$: Depth Anything in Any Direction", "author": "Haodong Li and Wangguangdong Zheng and Jing He and Yuhao Liu and Xin Lin and Xin Yang and Ying-Cong Chen and Chunchao Guo", "abstract": "  Panorama has a full FoV (360$^\\circ\\times$180$^\\circ$), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\n$\\textbf{DA}$$^{\\textbf{2}}$: $\\textbf{D}$epth $\\textbf{A}$nything in\n$\\textbf{A}$ny $\\textbf{D}$irection, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create $\\sim$543K panoramic\nRGB-depth pairs, bringing the total to $\\sim$607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA$^{2}$'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA$^{2}$ even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.\n", "link": "http://arxiv.org/abs/2509.26618v1", "date": "2025-09-30", "relevancy": 2.3875, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5998}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5998}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA%24%5E2%24%3A%20Depth%20Anything%20in%20Any%20Direction&body=Title%3A%20DA%24%5E2%24%3A%20Depth%20Anything%20in%20Any%20Direction%0AAuthor%3A%20Haodong%20Li%20and%20Wangguangdong%20Zheng%20and%20Jing%20He%20and%20Yuhao%20Liu%20and%20Xin%20Lin%20and%20Xin%20Yang%20and%20Ying-Cong%20Chen%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20Panorama%20has%20a%20full%20FoV%20%28360%24%5E%5Ccirc%5Ctimes%24180%24%5E%5Ccirc%24%29%2C%20offering%20a%20more%0Acomplete%20visual%20description%20than%20perspective%20images.%20Thanks%20to%20this%0Acharacteristic%2C%20panoramic%20depth%20estimation%20is%20gaining%20increasing%20traction%20in%203D%0Avision.%20However%2C%20due%20to%20the%20scarcity%20of%20panoramic%20data%2C%20previous%20methods%20are%0Aoften%20restricted%20to%20in-domain%20settings%2C%20leading%20to%20poor%20zero-shot%0Ageneralization.%20Furthermore%2C%20due%20to%20the%20spherical%20distortions%20inherent%20in%0Apanoramas%2C%20many%20approaches%20rely%20on%20perspective%20splitting%20%28e.g.%2C%20cubemaps%29%2C%0Awhich%20leads%20to%20suboptimal%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%0A%24%5Ctextbf%7BDA%7D%24%24%5E%7B%5Ctextbf%7B2%7D%7D%24%3A%20%24%5Ctextbf%7BD%7D%24epth%20%24%5Ctextbf%7BA%7D%24nything%20in%0A%24%5Ctextbf%7BA%7D%24ny%20%24%5Ctextbf%7BD%7D%24irection%2C%20an%20accurate%2C%20zero-shot%20generalizable%2C%20and%0Afully%20end-to-end%20panoramic%20depth%20estimator.%20Specifically%2C%20for%20scaling%20up%0Apanoramic%20data%2C%20we%20introduce%20a%20data%20curation%20engine%20for%20generating%20high-quality%0Apanoramic%20depth%20data%20from%20perspective%2C%20and%20create%20%24%5Csim%24543K%20panoramic%0ARGB-depth%20pairs%2C%20bringing%20the%20total%20to%20%24%5Csim%24607K.%20To%20further%20mitigate%20the%0Aspherical%20distortions%2C%20we%20present%20SphereViT%2C%20which%20explicitly%20leverages%0Aspherical%20coordinates%20to%20enforce%20the%20spherical%20geometric%20consistency%20in%0Apanoramic%20image%20features%2C%20yielding%20improved%20performance.%20A%20comprehensive%0Abenchmark%20on%20multiple%20datasets%20clearly%20demonstrates%20DA%24%5E%7B2%7D%24%27s%20SoTA%0Aperformance%2C%20with%20an%20average%2038%25%20improvement%20on%20AbsRel%20over%20the%20strongest%0Azero-shot%20baseline.%20Surprisingly%2C%20DA%24%5E%7B2%7D%24%20even%20outperforms%20prior%20in-domain%0Amethods%2C%20highlighting%20its%20superior%20zero-shot%20generalization.%20Moreover%2C%20as%20an%0Aend-to-end%20solution%2C%20DA%24%5E%7B2%7D%24%20exhibits%20much%20higher%20efficiency%20over%20fusion-based%0Aapproaches.%20Both%20the%20code%20and%20the%20curated%20panoramic%20data%20will%20be%20released.%0AProject%20page%3A%20https%3A//depth-any-in-any-dir.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA%2524%255E2%2524%253A%2520Depth%2520Anything%2520in%2520Any%2520Direction%26entry.906535625%3DHaodong%2520Li%2520and%2520Wangguangdong%2520Zheng%2520and%2520Jing%2520He%2520and%2520Yuhao%2520Liu%2520and%2520Xin%2520Lin%2520and%2520Xin%2520Yang%2520and%2520Ying-Cong%2520Chen%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520Panorama%2520has%2520a%2520full%2520FoV%2520%2528360%2524%255E%255Ccirc%255Ctimes%2524180%2524%255E%255Ccirc%2524%2529%252C%2520offering%2520a%2520more%250Acomplete%2520visual%2520description%2520than%2520perspective%2520images.%2520Thanks%2520to%2520this%250Acharacteristic%252C%2520panoramic%2520depth%2520estimation%2520is%2520gaining%2520increasing%2520traction%2520in%25203D%250Avision.%2520However%252C%2520due%2520to%2520the%2520scarcity%2520of%2520panoramic%2520data%252C%2520previous%2520methods%2520are%250Aoften%2520restricted%2520to%2520in-domain%2520settings%252C%2520leading%2520to%2520poor%2520zero-shot%250Ageneralization.%2520Furthermore%252C%2520due%2520to%2520the%2520spherical%2520distortions%2520inherent%2520in%250Apanoramas%252C%2520many%2520approaches%2520rely%2520on%2520perspective%2520splitting%2520%2528e.g.%252C%2520cubemaps%2529%252C%250Awhich%2520leads%2520to%2520suboptimal%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250A%2524%255Ctextbf%257BDA%257D%2524%2524%255E%257B%255Ctextbf%257B2%257D%257D%2524%253A%2520%2524%255Ctextbf%257BD%257D%2524epth%2520%2524%255Ctextbf%257BA%257D%2524nything%2520in%250A%2524%255Ctextbf%257BA%257D%2524ny%2520%2524%255Ctextbf%257BD%257D%2524irection%252C%2520an%2520accurate%252C%2520zero-shot%2520generalizable%252C%2520and%250Afully%2520end-to-end%2520panoramic%2520depth%2520estimator.%2520Specifically%252C%2520for%2520scaling%2520up%250Apanoramic%2520data%252C%2520we%2520introduce%2520a%2520data%2520curation%2520engine%2520for%2520generating%2520high-quality%250Apanoramic%2520depth%2520data%2520from%2520perspective%252C%2520and%2520create%2520%2524%255Csim%2524543K%2520panoramic%250ARGB-depth%2520pairs%252C%2520bringing%2520the%2520total%2520to%2520%2524%255Csim%2524607K.%2520To%2520further%2520mitigate%2520the%250Aspherical%2520distortions%252C%2520we%2520present%2520SphereViT%252C%2520which%2520explicitly%2520leverages%250Aspherical%2520coordinates%2520to%2520enforce%2520the%2520spherical%2520geometric%2520consistency%2520in%250Apanoramic%2520image%2520features%252C%2520yielding%2520improved%2520performance.%2520A%2520comprehensive%250Abenchmark%2520on%2520multiple%2520datasets%2520clearly%2520demonstrates%2520DA%2524%255E%257B2%257D%2524%2527s%2520SoTA%250Aperformance%252C%2520with%2520an%2520average%252038%2525%2520improvement%2520on%2520AbsRel%2520over%2520the%2520strongest%250Azero-shot%2520baseline.%2520Surprisingly%252C%2520DA%2524%255E%257B2%257D%2524%2520even%2520outperforms%2520prior%2520in-domain%250Amethods%252C%2520highlighting%2520its%2520superior%2520zero-shot%2520generalization.%2520Moreover%252C%2520as%2520an%250Aend-to-end%2520solution%252C%2520DA%2524%255E%257B2%257D%2524%2520exhibits%2520much%2520higher%2520efficiency%2520over%2520fusion-based%250Aapproaches.%2520Both%2520the%2520code%2520and%2520the%2520curated%2520panoramic%2520data%2520will%2520be%2520released.%250AProject%2520page%253A%2520https%253A//depth-any-in-any-dir.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA%24%5E2%24%3A%20Depth%20Anything%20in%20Any%20Direction&entry.906535625=Haodong%20Li%20and%20Wangguangdong%20Zheng%20and%20Jing%20He%20and%20Yuhao%20Liu%20and%20Xin%20Lin%20and%20Xin%20Yang%20and%20Ying-Cong%20Chen%20and%20Chunchao%20Guo&entry.1292438233=%20%20Panorama%20has%20a%20full%20FoV%20%28360%24%5E%5Ccirc%5Ctimes%24180%24%5E%5Ccirc%24%29%2C%20offering%20a%20more%0Acomplete%20visual%20description%20than%20perspective%20images.%20Thanks%20to%20this%0Acharacteristic%2C%20panoramic%20depth%20estimation%20is%20gaining%20increasing%20traction%20in%203D%0Avision.%20However%2C%20due%20to%20the%20scarcity%20of%20panoramic%20data%2C%20previous%20methods%20are%0Aoften%20restricted%20to%20in-domain%20settings%2C%20leading%20to%20poor%20zero-shot%0Ageneralization.%20Furthermore%2C%20due%20to%20the%20spherical%20distortions%20inherent%20in%0Apanoramas%2C%20many%20approaches%20rely%20on%20perspective%20splitting%20%28e.g.%2C%20cubemaps%29%2C%0Awhich%20leads%20to%20suboptimal%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%0A%24%5Ctextbf%7BDA%7D%24%24%5E%7B%5Ctextbf%7B2%7D%7D%24%3A%20%24%5Ctextbf%7BD%7D%24epth%20%24%5Ctextbf%7BA%7D%24nything%20in%0A%24%5Ctextbf%7BA%7D%24ny%20%24%5Ctextbf%7BD%7D%24irection%2C%20an%20accurate%2C%20zero-shot%20generalizable%2C%20and%0Afully%20end-to-end%20panoramic%20depth%20estimator.%20Specifically%2C%20for%20scaling%20up%0Apanoramic%20data%2C%20we%20introduce%20a%20data%20curation%20engine%20for%20generating%20high-quality%0Apanoramic%20depth%20data%20from%20perspective%2C%20and%20create%20%24%5Csim%24543K%20panoramic%0ARGB-depth%20pairs%2C%20bringing%20the%20total%20to%20%24%5Csim%24607K.%20To%20further%20mitigate%20the%0Aspherical%20distortions%2C%20we%20present%20SphereViT%2C%20which%20explicitly%20leverages%0Aspherical%20coordinates%20to%20enforce%20the%20spherical%20geometric%20consistency%20in%0Apanoramic%20image%20features%2C%20yielding%20improved%20performance.%20A%20comprehensive%0Abenchmark%20on%20multiple%20datasets%20clearly%20demonstrates%20DA%24%5E%7B2%7D%24%27s%20SoTA%0Aperformance%2C%20with%20an%20average%2038%25%20improvement%20on%20AbsRel%20over%20the%20strongest%0Azero-shot%20baseline.%20Surprisingly%2C%20DA%24%5E%7B2%7D%24%20even%20outperforms%20prior%20in-domain%0Amethods%2C%20highlighting%20its%20superior%20zero-shot%20generalization.%20Moreover%2C%20as%20an%0Aend-to-end%20solution%2C%20DA%24%5E%7B2%7D%24%20exhibits%20much%20higher%20efficiency%20over%20fusion-based%0Aapproaches.%20Both%20the%20code%20and%20the%20curated%20panoramic%20data%20will%20be%20released.%0AProject%20page%3A%20https%3A//depth-any-in-any-dir.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26618v1&entry.124074799=Read"},
{"title": "SPATA: Systematic Pattern Analysis for Detailed and Transparent Data\n  Cards", "author": "Jo\u00e3o Vitorino and Eva Maia and Isabel Pra\u00e7a and Carlos Soares", "abstract": "  Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.\n", "link": "http://arxiv.org/abs/2509.26640v1", "date": "2025-09-30", "relevancy": 2.3679, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4773}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPATA%3A%20Systematic%20Pattern%20Analysis%20for%20Detailed%20and%20Transparent%20Data%0A%20%20Cards&body=Title%3A%20SPATA%3A%20Systematic%20Pattern%20Analysis%20for%20Detailed%20and%20Transparent%20Data%0A%20%20Cards%0AAuthor%3A%20Jo%C3%A3o%20Vitorino%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a%20and%20Carlos%20Soares%0AAbstract%3A%20%20%20Due%20to%20the%20susceptibility%20of%20Artificial%20Intelligence%20%28AI%29%20to%20data%0Aperturbations%20and%20adversarial%20examples%2C%20it%20is%20crucial%20to%20perform%20a%20thorough%0Arobustness%20evaluation%20before%20any%20Machine%20Learning%20%28ML%29%20model%20is%20deployed.%0AHowever%2C%20examining%20a%20model%27s%20decision%20boundaries%20and%20identifying%20potential%0Avulnerabilities%20typically%20requires%20access%20to%20the%20training%20and%20testing%20datasets%2C%0Awhich%20may%20pose%20risks%20to%20data%20privacy%20and%20confidentiality.%20To%20improve%0Atransparency%20in%20organizations%20that%20handle%20confidential%20data%20or%20manage%20critical%0Ainfrastructure%2C%20it%20is%20essential%20to%20allow%20external%20verification%20and%20validation%0Aof%20AI%20without%20the%20disclosure%20of%20private%20datasets.%20This%20paper%20presents%0ASystematic%20Pattern%20Analysis%20%28SPATA%29%2C%20a%20deterministic%20method%20that%20converts%20any%0Atabular%20dataset%20to%20a%20domain-independent%20representation%20of%20its%20statistical%0Apatterns%2C%20to%20provide%20more%20detailed%20and%20transparent%20data%20cards.%20SPATA%20computes%0Athe%20projection%20of%20each%20data%20instance%20into%20a%20discrete%20space%20where%20they%20can%20be%0Aanalyzed%20and%20compared%2C%20without%20risking%20data%20leakage.%20These%20projected%20datasets%0Acan%20be%20reliably%20used%20for%20the%20evaluation%20of%20how%20different%20features%20affect%20ML%0Amodel%20robustness%20and%20for%20the%20generation%20of%20interpretable%20explanations%20of%20their%0Abehavior%2C%20contributing%20to%20more%20trustworthy%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPATA%253A%2520Systematic%2520Pattern%2520Analysis%2520for%2520Detailed%2520and%2520Transparent%2520Data%250A%2520%2520Cards%26entry.906535625%3DJo%25C3%25A3o%2520Vitorino%2520and%2520Eva%2520Maia%2520and%2520Isabel%2520Pra%25C3%25A7a%2520and%2520Carlos%2520Soares%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520susceptibility%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520to%2520data%250Aperturbations%2520and%2520adversarial%2520examples%252C%2520it%2520is%2520crucial%2520to%2520perform%2520a%2520thorough%250Arobustness%2520evaluation%2520before%2520any%2520Machine%2520Learning%2520%2528ML%2529%2520model%2520is%2520deployed.%250AHowever%252C%2520examining%2520a%2520model%2527s%2520decision%2520boundaries%2520and%2520identifying%2520potential%250Avulnerabilities%2520typically%2520requires%2520access%2520to%2520the%2520training%2520and%2520testing%2520datasets%252C%250Awhich%2520may%2520pose%2520risks%2520to%2520data%2520privacy%2520and%2520confidentiality.%2520To%2520improve%250Atransparency%2520in%2520organizations%2520that%2520handle%2520confidential%2520data%2520or%2520manage%2520critical%250Ainfrastructure%252C%2520it%2520is%2520essential%2520to%2520allow%2520external%2520verification%2520and%2520validation%250Aof%2520AI%2520without%2520the%2520disclosure%2520of%2520private%2520datasets.%2520This%2520paper%2520presents%250ASystematic%2520Pattern%2520Analysis%2520%2528SPATA%2529%252C%2520a%2520deterministic%2520method%2520that%2520converts%2520any%250Atabular%2520dataset%2520to%2520a%2520domain-independent%2520representation%2520of%2520its%2520statistical%250Apatterns%252C%2520to%2520provide%2520more%2520detailed%2520and%2520transparent%2520data%2520cards.%2520SPATA%2520computes%250Athe%2520projection%2520of%2520each%2520data%2520instance%2520into%2520a%2520discrete%2520space%2520where%2520they%2520can%2520be%250Aanalyzed%2520and%2520compared%252C%2520without%2520risking%2520data%2520leakage.%2520These%2520projected%2520datasets%250Acan%2520be%2520reliably%2520used%2520for%2520the%2520evaluation%2520of%2520how%2520different%2520features%2520affect%2520ML%250Amodel%2520robustness%2520and%2520for%2520the%2520generation%2520of%2520interpretable%2520explanations%2520of%2520their%250Abehavior%252C%2520contributing%2520to%2520more%2520trustworthy%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPATA%3A%20Systematic%20Pattern%20Analysis%20for%20Detailed%20and%20Transparent%20Data%0A%20%20Cards&entry.906535625=Jo%C3%A3o%20Vitorino%20and%20Eva%20Maia%20and%20Isabel%20Pra%C3%A7a%20and%20Carlos%20Soares&entry.1292438233=%20%20Due%20to%20the%20susceptibility%20of%20Artificial%20Intelligence%20%28AI%29%20to%20data%0Aperturbations%20and%20adversarial%20examples%2C%20it%20is%20crucial%20to%20perform%20a%20thorough%0Arobustness%20evaluation%20before%20any%20Machine%20Learning%20%28ML%29%20model%20is%20deployed.%0AHowever%2C%20examining%20a%20model%27s%20decision%20boundaries%20and%20identifying%20potential%0Avulnerabilities%20typically%20requires%20access%20to%20the%20training%20and%20testing%20datasets%2C%0Awhich%20may%20pose%20risks%20to%20data%20privacy%20and%20confidentiality.%20To%20improve%0Atransparency%20in%20organizations%20that%20handle%20confidential%20data%20or%20manage%20critical%0Ainfrastructure%2C%20it%20is%20essential%20to%20allow%20external%20verification%20and%20validation%0Aof%20AI%20without%20the%20disclosure%20of%20private%20datasets.%20This%20paper%20presents%0ASystematic%20Pattern%20Analysis%20%28SPATA%29%2C%20a%20deterministic%20method%20that%20converts%20any%0Atabular%20dataset%20to%20a%20domain-independent%20representation%20of%20its%20statistical%0Apatterns%2C%20to%20provide%20more%20detailed%20and%20transparent%20data%20cards.%20SPATA%20computes%0Athe%20projection%20of%20each%20data%20instance%20into%20a%20discrete%20space%20where%20they%20can%20be%0Aanalyzed%20and%20compared%2C%20without%20risking%20data%20leakage.%20These%20projected%20datasets%0Acan%20be%20reliably%20used%20for%20the%20evaluation%20of%20how%20different%20features%20affect%20ML%0Amodel%20robustness%20and%20for%20the%20generation%20of%20interpretable%20explanations%20of%20their%0Abehavior%2C%20contributing%20to%20more%20trustworthy%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26640v1&entry.124074799=Read"},
{"title": "TASP: Topology-aware Sequence Parallelism", "author": "Yida Wang and Ke Hong and Xiuhong Li and Yuanchao Xu and Wenxun Wang and Guohao Dai and Yu Wang", "abstract": "  Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.\n", "link": "http://arxiv.org/abs/2509.26541v1", "date": "2025-09-30", "relevancy": 2.3615, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4681}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TASP%3A%20Topology-aware%20Sequence%20Parallelism&body=Title%3A%20TASP%3A%20Topology-aware%20Sequence%20Parallelism%0AAuthor%3A%20Yida%20Wang%20and%20Ke%20Hong%20and%20Xiuhong%20Li%20and%20Yuanchao%20Xu%20and%20Wenxun%20Wang%20and%20Guohao%20Dai%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Long-context%20large%20language%20models%20%28LLMs%29%20face%20constraints%20due%20to%20the%0Aquadratic%20complexity%20of%20the%20self-attention%20mechanism.%20The%20mainstream%20sequence%0Aparallelism%20%28SP%29%20method%2C%20Ring%20Attention%2C%20attempts%20to%20solve%20this%20by%20distributing%0Athe%20query%20into%20multiple%20query%20chunks%20across%20accelerators%20and%20enable%20each%20Q%0Atensor%20to%20access%20all%20KV%20tensors%20from%20other%20accelerators%20via%20the%20Ring%20AllGather%0Acommunication%20primitive.%20However%2C%20it%20exhibits%20low%20communication%20efficiency%2C%0Arestricting%20its%20practical%20applicability.%20This%20inefficiency%20stems%20from%20the%0Amismatch%20between%20the%20Ring%20AllGather%20communication%20primitive%20it%20adopts%20and%20the%0AAlltoAll%20topology%20of%20modern%20accelerators.%20A%20Ring%20AllGather%20primitive%20is%0Acomposed%20of%20iterations%20of%20ring-styled%20data%20transfer%2C%20which%20can%20only%20utilize%20a%0Avery%20limited%20fraction%20of%20an%20AlltoAll%20topology.%0A%20%20Inspired%20by%20the%20Hamiltonian%20decomposition%20of%20complete%20directed%20graphs%2C%20we%0Aidentify%20that%20modern%20accelerator%20topology%20can%20be%20decomposed%20into%20multiple%0Aorthogonal%20ring%20datapaths%20which%20can%20concurrently%20transfer%20data%20without%0Ainterference.%20Based%20on%20this%2C%20we%20further%20observe%20that%20the%20Ring%20AllGather%0Aprimitive%20can%20also%20be%20decomposed%20into%20the%20same%20number%20of%20concurrent%20ring-styled%0Adata%20transfer%20at%20every%20iteration.%20Based%20on%20these%20insights%2C%20we%20propose%20TASP%2C%20a%0Atopology-aware%20SP%20method%20for%20long-context%20LLMs%20that%20fully%20utilizes%20the%0Acommunication%20capacity%20of%20modern%20accelerators%20via%20topology%20decomposition%20and%0Aprimitive%20decomposition.%20Experimental%20results%20on%20both%20single-node%20and%0Amulti-node%20NVIDIA%20H100%20systems%20and%20a%20single-node%20AMD%20MI300X%20system%20demonstrate%0Athat%20TASP%20achieves%20higher%20communication%20efficiency%20than%20Ring%20Attention%20on%20these%0Amodern%20accelerator%20topologies%20and%20achieves%20up%20to%203.58%20speedup%20than%20Ring%0AAttention%20and%20its%20variant%20Zigzag-Ring%20Attention.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/infinigence/HamiltonAttention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTASP%253A%2520Topology-aware%2520Sequence%2520Parallelism%26entry.906535625%3DYida%2520Wang%2520and%2520Ke%2520Hong%2520and%2520Xiuhong%2520Li%2520and%2520Yuanchao%2520Xu%2520and%2520Wenxun%2520Wang%2520and%2520Guohao%2520Dai%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Long-context%2520large%2520language%2520models%2520%2528LLMs%2529%2520face%2520constraints%2520due%2520to%2520the%250Aquadratic%2520complexity%2520of%2520the%2520self-attention%2520mechanism.%2520The%2520mainstream%2520sequence%250Aparallelism%2520%2528SP%2529%2520method%252C%2520Ring%2520Attention%252C%2520attempts%2520to%2520solve%2520this%2520by%2520distributing%250Athe%2520query%2520into%2520multiple%2520query%2520chunks%2520across%2520accelerators%2520and%2520enable%2520each%2520Q%250Atensor%2520to%2520access%2520all%2520KV%2520tensors%2520from%2520other%2520accelerators%2520via%2520the%2520Ring%2520AllGather%250Acommunication%2520primitive.%2520However%252C%2520it%2520exhibits%2520low%2520communication%2520efficiency%252C%250Arestricting%2520its%2520practical%2520applicability.%2520This%2520inefficiency%2520stems%2520from%2520the%250Amismatch%2520between%2520the%2520Ring%2520AllGather%2520communication%2520primitive%2520it%2520adopts%2520and%2520the%250AAlltoAll%2520topology%2520of%2520modern%2520accelerators.%2520A%2520Ring%2520AllGather%2520primitive%2520is%250Acomposed%2520of%2520iterations%2520of%2520ring-styled%2520data%2520transfer%252C%2520which%2520can%2520only%2520utilize%2520a%250Avery%2520limited%2520fraction%2520of%2520an%2520AlltoAll%2520topology.%250A%2520%2520Inspired%2520by%2520the%2520Hamiltonian%2520decomposition%2520of%2520complete%2520directed%2520graphs%252C%2520we%250Aidentify%2520that%2520modern%2520accelerator%2520topology%2520can%2520be%2520decomposed%2520into%2520multiple%250Aorthogonal%2520ring%2520datapaths%2520which%2520can%2520concurrently%2520transfer%2520data%2520without%250Ainterference.%2520Based%2520on%2520this%252C%2520we%2520further%2520observe%2520that%2520the%2520Ring%2520AllGather%250Aprimitive%2520can%2520also%2520be%2520decomposed%2520into%2520the%2520same%2520number%2520of%2520concurrent%2520ring-styled%250Adata%2520transfer%2520at%2520every%2520iteration.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520TASP%252C%2520a%250Atopology-aware%2520SP%2520method%2520for%2520long-context%2520LLMs%2520that%2520fully%2520utilizes%2520the%250Acommunication%2520capacity%2520of%2520modern%2520accelerators%2520via%2520topology%2520decomposition%2520and%250Aprimitive%2520decomposition.%2520Experimental%2520results%2520on%2520both%2520single-node%2520and%250Amulti-node%2520NVIDIA%2520H100%2520systems%2520and%2520a%2520single-node%2520AMD%2520MI300X%2520system%2520demonstrate%250Athat%2520TASP%2520achieves%2520higher%2520communication%2520efficiency%2520than%2520Ring%2520Attention%2520on%2520these%250Amodern%2520accelerator%2520topologies%2520and%2520achieves%2520up%2520to%25203.58%2520speedup%2520than%2520Ring%250AAttention%2520and%2520its%2520variant%2520Zigzag-Ring%2520Attention.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/infinigence/HamiltonAttention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TASP%3A%20Topology-aware%20Sequence%20Parallelism&entry.906535625=Yida%20Wang%20and%20Ke%20Hong%20and%20Xiuhong%20Li%20and%20Yuanchao%20Xu%20and%20Wenxun%20Wang%20and%20Guohao%20Dai%20and%20Yu%20Wang&entry.1292438233=%20%20Long-context%20large%20language%20models%20%28LLMs%29%20face%20constraints%20due%20to%20the%0Aquadratic%20complexity%20of%20the%20self-attention%20mechanism.%20The%20mainstream%20sequence%0Aparallelism%20%28SP%29%20method%2C%20Ring%20Attention%2C%20attempts%20to%20solve%20this%20by%20distributing%0Athe%20query%20into%20multiple%20query%20chunks%20across%20accelerators%20and%20enable%20each%20Q%0Atensor%20to%20access%20all%20KV%20tensors%20from%20other%20accelerators%20via%20the%20Ring%20AllGather%0Acommunication%20primitive.%20However%2C%20it%20exhibits%20low%20communication%20efficiency%2C%0Arestricting%20its%20practical%20applicability.%20This%20inefficiency%20stems%20from%20the%0Amismatch%20between%20the%20Ring%20AllGather%20communication%20primitive%20it%20adopts%20and%20the%0AAlltoAll%20topology%20of%20modern%20accelerators.%20A%20Ring%20AllGather%20primitive%20is%0Acomposed%20of%20iterations%20of%20ring-styled%20data%20transfer%2C%20which%20can%20only%20utilize%20a%0Avery%20limited%20fraction%20of%20an%20AlltoAll%20topology.%0A%20%20Inspired%20by%20the%20Hamiltonian%20decomposition%20of%20complete%20directed%20graphs%2C%20we%0Aidentify%20that%20modern%20accelerator%20topology%20can%20be%20decomposed%20into%20multiple%0Aorthogonal%20ring%20datapaths%20which%20can%20concurrently%20transfer%20data%20without%0Ainterference.%20Based%20on%20this%2C%20we%20further%20observe%20that%20the%20Ring%20AllGather%0Aprimitive%20can%20also%20be%20decomposed%20into%20the%20same%20number%20of%20concurrent%20ring-styled%0Adata%20transfer%20at%20every%20iteration.%20Based%20on%20these%20insights%2C%20we%20propose%20TASP%2C%20a%0Atopology-aware%20SP%20method%20for%20long-context%20LLMs%20that%20fully%20utilizes%20the%0Acommunication%20capacity%20of%20modern%20accelerators%20via%20topology%20decomposition%20and%0Aprimitive%20decomposition.%20Experimental%20results%20on%20both%20single-node%20and%0Amulti-node%20NVIDIA%20H100%20systems%20and%20a%20single-node%20AMD%20MI300X%20system%20demonstrate%0Athat%20TASP%20achieves%20higher%20communication%20efficiency%20than%20Ring%20Attention%20on%20these%0Amodern%20accelerator%20topologies%20and%20achieves%20up%20to%203.58%20speedup%20than%20Ring%0AAttention%20and%20its%20variant%20Zigzag-Ring%20Attention.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/infinigence/HamiltonAttention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26541v1&entry.124074799=Read"},
{"title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation", "author": "Agneet Chatterjee and Rahim Entezari and Maksym Zhuravinskyi and Maksim Lapin and Reshinth Adithyan and Amit Raj and Chitta Baral and Yezhou Yang and Varun Jampani", "abstract": "  Recent advances in video generation have enabled high-fidelity video\nsynthesis from user provided prompts. However, existing models and benchmarks\nfail to capture the complexity and requirements of professional video\ngeneration. Towards that goal, we introduce Stable Cinemetrics, a structured\nevaluation framework that formalizes filmmaking controls into four\ndisentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.\nTogether, these taxonomies define 76 fine-grained control nodes grounded in\nindustry practices. Using these taxonomies, we construct a benchmark of prompts\naligned with professional use cases and develop an automated pipeline for\nprompt categorization and question generation, enabling independent evaluation\nof each control dimension. We conduct a large-scale human study spanning 10+\nmodels and 20K videos, annotated by a pool of 80+ film professionals. Our\nanalysis, both coarse and fine-grained reveal that even the strongest current\nmodels exhibit significant gaps, particularly in Events and Camera-related\ncontrols. To enable scalable evaluation, we train an automatic evaluator, a\nvision-language model aligned with expert annotations that outperforms existing\nzero-shot baselines. SCINE is the first approach to situate professional video\ngeneration within the landscape of video generative models, introducing\ntaxonomies centered around cinematic controls and supporting them with\nstructured evaluation pipelines and detailed analyses to guide future research.\n", "link": "http://arxiv.org/abs/2509.26555v1", "date": "2025-09-30", "relevancy": 2.3601, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5873}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Cinemetrics%20%3A%20Structured%20Taxonomy%20and%20Evaluation%20for%20Professional%0A%20%20Video%20Generation&body=Title%3A%20Stable%20Cinemetrics%20%3A%20Structured%20Taxonomy%20and%20Evaluation%20for%20Professional%0A%20%20Video%20Generation%0AAuthor%3A%20Agneet%20Chatterjee%20and%20Rahim%20Entezari%20and%20Maksym%20Zhuravinskyi%20and%20Maksim%20Lapin%20and%20Reshinth%20Adithyan%20and%20Amit%20Raj%20and%20Chitta%20Baral%20and%20Yezhou%20Yang%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20enabled%20high-fidelity%20video%0Asynthesis%20from%20user%20provided%20prompts.%20However%2C%20existing%20models%20and%20benchmarks%0Afail%20to%20capture%20the%20complexity%20and%20requirements%20of%20professional%20video%0Ageneration.%20Towards%20that%20goal%2C%20we%20introduce%20Stable%20Cinemetrics%2C%20a%20structured%0Aevaluation%20framework%20that%20formalizes%20filmmaking%20controls%20into%20four%0Adisentangled%2C%20hierarchical%20taxonomies%3A%20Setup%2C%20Event%2C%20Lighting%2C%20and%20Camera.%0ATogether%2C%20these%20taxonomies%20define%2076%20fine-grained%20control%20nodes%20grounded%20in%0Aindustry%20practices.%20Using%20these%20taxonomies%2C%20we%20construct%20a%20benchmark%20of%20prompts%0Aaligned%20with%20professional%20use%20cases%20and%20develop%20an%20automated%20pipeline%20for%0Aprompt%20categorization%20and%20question%20generation%2C%20enabling%20independent%20evaluation%0Aof%20each%20control%20dimension.%20We%20conduct%20a%20large-scale%20human%20study%20spanning%2010%2B%0Amodels%20and%2020K%20videos%2C%20annotated%20by%20a%20pool%20of%2080%2B%20film%20professionals.%20Our%0Aanalysis%2C%20both%20coarse%20and%20fine-grained%20reveal%20that%20even%20the%20strongest%20current%0Amodels%20exhibit%20significant%20gaps%2C%20particularly%20in%20Events%20and%20Camera-related%0Acontrols.%20To%20enable%20scalable%20evaluation%2C%20we%20train%20an%20automatic%20evaluator%2C%20a%0Avision-language%20model%20aligned%20with%20expert%20annotations%20that%20outperforms%20existing%0Azero-shot%20baselines.%20SCINE%20is%20the%20first%20approach%20to%20situate%20professional%20video%0Ageneration%20within%20the%20landscape%20of%20video%20generative%20models%2C%20introducing%0Ataxonomies%20centered%20around%20cinematic%20controls%20and%20supporting%20them%20with%0Astructured%20evaluation%20pipelines%20and%20detailed%20analyses%20to%20guide%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Cinemetrics%2520%253A%2520Structured%2520Taxonomy%2520and%2520Evaluation%2520for%2520Professional%250A%2520%2520Video%2520Generation%26entry.906535625%3DAgneet%2520Chatterjee%2520and%2520Rahim%2520Entezari%2520and%2520Maksym%2520Zhuravinskyi%2520and%2520Maksim%2520Lapin%2520and%2520Reshinth%2520Adithyan%2520and%2520Amit%2520Raj%2520and%2520Chitta%2520Baral%2520and%2520Yezhou%2520Yang%2520and%2520Varun%2520Jampani%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520enabled%2520high-fidelity%2520video%250Asynthesis%2520from%2520user%2520provided%2520prompts.%2520However%252C%2520existing%2520models%2520and%2520benchmarks%250Afail%2520to%2520capture%2520the%2520complexity%2520and%2520requirements%2520of%2520professional%2520video%250Ageneration.%2520Towards%2520that%2520goal%252C%2520we%2520introduce%2520Stable%2520Cinemetrics%252C%2520a%2520structured%250Aevaluation%2520framework%2520that%2520formalizes%2520filmmaking%2520controls%2520into%2520four%250Adisentangled%252C%2520hierarchical%2520taxonomies%253A%2520Setup%252C%2520Event%252C%2520Lighting%252C%2520and%2520Camera.%250ATogether%252C%2520these%2520taxonomies%2520define%252076%2520fine-grained%2520control%2520nodes%2520grounded%2520in%250Aindustry%2520practices.%2520Using%2520these%2520taxonomies%252C%2520we%2520construct%2520a%2520benchmark%2520of%2520prompts%250Aaligned%2520with%2520professional%2520use%2520cases%2520and%2520develop%2520an%2520automated%2520pipeline%2520for%250Aprompt%2520categorization%2520and%2520question%2520generation%252C%2520enabling%2520independent%2520evaluation%250Aof%2520each%2520control%2520dimension.%2520We%2520conduct%2520a%2520large-scale%2520human%2520study%2520spanning%252010%252B%250Amodels%2520and%252020K%2520videos%252C%2520annotated%2520by%2520a%2520pool%2520of%252080%252B%2520film%2520professionals.%2520Our%250Aanalysis%252C%2520both%2520coarse%2520and%2520fine-grained%2520reveal%2520that%2520even%2520the%2520strongest%2520current%250Amodels%2520exhibit%2520significant%2520gaps%252C%2520particularly%2520in%2520Events%2520and%2520Camera-related%250Acontrols.%2520To%2520enable%2520scalable%2520evaluation%252C%2520we%2520train%2520an%2520automatic%2520evaluator%252C%2520a%250Avision-language%2520model%2520aligned%2520with%2520expert%2520annotations%2520that%2520outperforms%2520existing%250Azero-shot%2520baselines.%2520SCINE%2520is%2520the%2520first%2520approach%2520to%2520situate%2520professional%2520video%250Ageneration%2520within%2520the%2520landscape%2520of%2520video%2520generative%2520models%252C%2520introducing%250Ataxonomies%2520centered%2520around%2520cinematic%2520controls%2520and%2520supporting%2520them%2520with%250Astructured%2520evaluation%2520pipelines%2520and%2520detailed%2520analyses%2520to%2520guide%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Cinemetrics%20%3A%20Structured%20Taxonomy%20and%20Evaluation%20for%20Professional%0A%20%20Video%20Generation&entry.906535625=Agneet%20Chatterjee%20and%20Rahim%20Entezari%20and%20Maksym%20Zhuravinskyi%20and%20Maksim%20Lapin%20and%20Reshinth%20Adithyan%20and%20Amit%20Raj%20and%20Chitta%20Baral%20and%20Yezhou%20Yang%20and%20Varun%20Jampani&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20enabled%20high-fidelity%20video%0Asynthesis%20from%20user%20provided%20prompts.%20However%2C%20existing%20models%20and%20benchmarks%0Afail%20to%20capture%20the%20complexity%20and%20requirements%20of%20professional%20video%0Ageneration.%20Towards%20that%20goal%2C%20we%20introduce%20Stable%20Cinemetrics%2C%20a%20structured%0Aevaluation%20framework%20that%20formalizes%20filmmaking%20controls%20into%20four%0Adisentangled%2C%20hierarchical%20taxonomies%3A%20Setup%2C%20Event%2C%20Lighting%2C%20and%20Camera.%0ATogether%2C%20these%20taxonomies%20define%2076%20fine-grained%20control%20nodes%20grounded%20in%0Aindustry%20practices.%20Using%20these%20taxonomies%2C%20we%20construct%20a%20benchmark%20of%20prompts%0Aaligned%20with%20professional%20use%20cases%20and%20develop%20an%20automated%20pipeline%20for%0Aprompt%20categorization%20and%20question%20generation%2C%20enabling%20independent%20evaluation%0Aof%20each%20control%20dimension.%20We%20conduct%20a%20large-scale%20human%20study%20spanning%2010%2B%0Amodels%20and%2020K%20videos%2C%20annotated%20by%20a%20pool%20of%2080%2B%20film%20professionals.%20Our%0Aanalysis%2C%20both%20coarse%20and%20fine-grained%20reveal%20that%20even%20the%20strongest%20current%0Amodels%20exhibit%20significant%20gaps%2C%20particularly%20in%20Events%20and%20Camera-related%0Acontrols.%20To%20enable%20scalable%20evaluation%2C%20we%20train%20an%20automatic%20evaluator%2C%20a%0Avision-language%20model%20aligned%20with%20expert%20annotations%20that%20outperforms%20existing%0Azero-shot%20baselines.%20SCINE%20is%20the%20first%20approach%20to%20situate%20professional%20video%0Ageneration%20within%20the%20landscape%20of%20video%20generative%20models%2C%20introducing%0Ataxonomies%20centered%20around%20cinematic%20controls%20and%20supporting%20them%20with%0Astructured%20evaluation%20pipelines%20and%20detailed%20analyses%20to%20guide%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26555v1&entry.124074799=Read"},
{"title": "Video Object Segmentation-Aware Audio Generation", "author": "Ilpo Viertola and Vladimir Iashin and Esa Rahtu", "abstract": "  Existing multimodal audio generation models often lack precise user control,\nwhich limits their applicability in professional Foley workflows. In\nparticular, these models focus on the entire video and do not provide precise\nmethods for prioritizing a specific object within a scene, generating\nunnecessary background sounds, or focusing on the wrong objects. To address\nthis gap, we introduce the novel task of video object segmentation-aware audio\ngeneration, which explicitly conditions sound synthesis on object-level\nsegmentation maps. We present SAGANet, a new multimodal generative model that\nenables controllable audio generation by leveraging visual segmentation masks\nalong with video and textual cues. Our model provides users with fine-grained\nand visually localized control over audio generation. To support this task and\nfurther research on segmentation-aware Foley, we propose Segmented Music Solos,\na benchmark dataset of musical instrument performance videos with segmentation\ninformation. Our method demonstrates substantial improvements over current\nstate-of-the-art methods and sets a new standard for controllable,\nhigh-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are\navailable at https://saganet.notion.site\n", "link": "http://arxiv.org/abs/2509.26604v1", "date": "2025-09-30", "relevancy": 2.2768, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5841}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5801}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Object%20Segmentation-Aware%20Audio%20Generation&body=Title%3A%20Video%20Object%20Segmentation-Aware%20Audio%20Generation%0AAuthor%3A%20Ilpo%20Viertola%20and%20Vladimir%20Iashin%20and%20Esa%20Rahtu%0AAbstract%3A%20%20%20Existing%20multimodal%20audio%20generation%20models%20often%20lack%20precise%20user%20control%2C%0Awhich%20limits%20their%20applicability%20in%20professional%20Foley%20workflows.%20In%0Aparticular%2C%20these%20models%20focus%20on%20the%20entire%20video%20and%20do%20not%20provide%20precise%0Amethods%20for%20prioritizing%20a%20specific%20object%20within%20a%20scene%2C%20generating%0Aunnecessary%20background%20sounds%2C%20or%20focusing%20on%20the%20wrong%20objects.%20To%20address%0Athis%20gap%2C%20we%20introduce%20the%20novel%20task%20of%20video%20object%20segmentation-aware%20audio%0Ageneration%2C%20which%20explicitly%20conditions%20sound%20synthesis%20on%20object-level%0Asegmentation%20maps.%20We%20present%20SAGANet%2C%20a%20new%20multimodal%20generative%20model%20that%0Aenables%20controllable%20audio%20generation%20by%20leveraging%20visual%20segmentation%20masks%0Aalong%20with%20video%20and%20textual%20cues.%20Our%20model%20provides%20users%20with%20fine-grained%0Aand%20visually%20localized%20control%20over%20audio%20generation.%20To%20support%20this%20task%20and%0Afurther%20research%20on%20segmentation-aware%20Foley%2C%20we%20propose%20Segmented%20Music%20Solos%2C%0Aa%20benchmark%20dataset%20of%20musical%20instrument%20performance%20videos%20with%20segmentation%0Ainformation.%20Our%20method%20demonstrates%20substantial%20improvements%20over%20current%0Astate-of-the-art%20methods%20and%20sets%20a%20new%20standard%20for%20controllable%2C%0Ahigh-fidelity%20Foley%20synthesis.%20Code%2C%20samples%2C%20and%20Segmented%20Music%20Solos%20are%0Aavailable%20at%20https%3A//saganet.notion.site%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Object%2520Segmentation-Aware%2520Audio%2520Generation%26entry.906535625%3DIlpo%2520Viertola%2520and%2520Vladimir%2520Iashin%2520and%2520Esa%2520Rahtu%26entry.1292438233%3D%2520%2520Existing%2520multimodal%2520audio%2520generation%2520models%2520often%2520lack%2520precise%2520user%2520control%252C%250Awhich%2520limits%2520their%2520applicability%2520in%2520professional%2520Foley%2520workflows.%2520In%250Aparticular%252C%2520these%2520models%2520focus%2520on%2520the%2520entire%2520video%2520and%2520do%2520not%2520provide%2520precise%250Amethods%2520for%2520prioritizing%2520a%2520specific%2520object%2520within%2520a%2520scene%252C%2520generating%250Aunnecessary%2520background%2520sounds%252C%2520or%2520focusing%2520on%2520the%2520wrong%2520objects.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%2520video%2520object%2520segmentation-aware%2520audio%250Ageneration%252C%2520which%2520explicitly%2520conditions%2520sound%2520synthesis%2520on%2520object-level%250Asegmentation%2520maps.%2520We%2520present%2520SAGANet%252C%2520a%2520new%2520multimodal%2520generative%2520model%2520that%250Aenables%2520controllable%2520audio%2520generation%2520by%2520leveraging%2520visual%2520segmentation%2520masks%250Aalong%2520with%2520video%2520and%2520textual%2520cues.%2520Our%2520model%2520provides%2520users%2520with%2520fine-grained%250Aand%2520visually%2520localized%2520control%2520over%2520audio%2520generation.%2520To%2520support%2520this%2520task%2520and%250Afurther%2520research%2520on%2520segmentation-aware%2520Foley%252C%2520we%2520propose%2520Segmented%2520Music%2520Solos%252C%250Aa%2520benchmark%2520dataset%2520of%2520musical%2520instrument%2520performance%2520videos%2520with%2520segmentation%250Ainformation.%2520Our%2520method%2520demonstrates%2520substantial%2520improvements%2520over%2520current%250Astate-of-the-art%2520methods%2520and%2520sets%2520a%2520new%2520standard%2520for%2520controllable%252C%250Ahigh-fidelity%2520Foley%2520synthesis.%2520Code%252C%2520samples%252C%2520and%2520Segmented%2520Music%2520Solos%2520are%250Aavailable%2520at%2520https%253A//saganet.notion.site%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Object%20Segmentation-Aware%20Audio%20Generation&entry.906535625=Ilpo%20Viertola%20and%20Vladimir%20Iashin%20and%20Esa%20Rahtu&entry.1292438233=%20%20Existing%20multimodal%20audio%20generation%20models%20often%20lack%20precise%20user%20control%2C%0Awhich%20limits%20their%20applicability%20in%20professional%20Foley%20workflows.%20In%0Aparticular%2C%20these%20models%20focus%20on%20the%20entire%20video%20and%20do%20not%20provide%20precise%0Amethods%20for%20prioritizing%20a%20specific%20object%20within%20a%20scene%2C%20generating%0Aunnecessary%20background%20sounds%2C%20or%20focusing%20on%20the%20wrong%20objects.%20To%20address%0Athis%20gap%2C%20we%20introduce%20the%20novel%20task%20of%20video%20object%20segmentation-aware%20audio%0Ageneration%2C%20which%20explicitly%20conditions%20sound%20synthesis%20on%20object-level%0Asegmentation%20maps.%20We%20present%20SAGANet%2C%20a%20new%20multimodal%20generative%20model%20that%0Aenables%20controllable%20audio%20generation%20by%20leveraging%20visual%20segmentation%20masks%0Aalong%20with%20video%20and%20textual%20cues.%20Our%20model%20provides%20users%20with%20fine-grained%0Aand%20visually%20localized%20control%20over%20audio%20generation.%20To%20support%20this%20task%20and%0Afurther%20research%20on%20segmentation-aware%20Foley%2C%20we%20propose%20Segmented%20Music%20Solos%2C%0Aa%20benchmark%20dataset%20of%20musical%20instrument%20performance%20videos%20with%20segmentation%0Ainformation.%20Our%20method%20demonstrates%20substantial%20improvements%20over%20current%0Astate-of-the-art%20methods%20and%20sets%20a%20new%20standard%20for%20controllable%2C%0Ahigh-fidelity%20Foley%20synthesis.%20Code%2C%20samples%2C%20and%20Segmented%20Music%20Solos%20are%0Aavailable%20at%20https%3A//saganet.notion.site%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26604v1&entry.124074799=Read"},
{"title": "BoxDreamer: Dreaming Box Corners for Generalizable Object Pose\n  Estimation", "author": "Yuanhong Yu and Xingyi He and Chen Zhao and Junhao Yu and Jiaqi Yang and Ruizhen Hu and Yujun Shen and Xing Zhu and Xiaowei Zhou and Sida Peng", "abstract": "  This paper presents a generalizable RGB-based approach for object pose\nestimation, specifically designed to address challenges in sparse-view\nsettings. While existing methods can estimate the poses of unseen objects,\ntheir generalization ability remains limited in scenarios involving occlusions\nand sparse reference views, restricting their real-world applicability. To\novercome these limitations, we introduce corner points of the object bounding\nbox as an intermediate representation of the object pose. The 3D object corners\ncan be reliably recovered from sparse input views, while the 2D corner points\nin the target view are estimated through a novel reference-based point\nsynthesizer, which works well even in scenarios involving occlusions. As object\nsemantic points, object corners naturally establish 2D-3D correspondences for\nobject pose estimation with a PnP algorithm. Extensive experiments on the\nYCB-Video and Occluded-LINEMOD datasets show that our approach outperforms\nstate-of-the-art methods, highlighting the effectiveness of the proposed\nrepresentation and significantly enhancing the generalization capabilities of\nobject pose estimation, which is crucial for real-world applications.\n", "link": "http://arxiv.org/abs/2504.07955v2", "date": "2025-09-30", "relevancy": 2.2509, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5615}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoxDreamer%3A%20Dreaming%20Box%20Corners%20for%20Generalizable%20Object%20Pose%0A%20%20Estimation&body=Title%3A%20BoxDreamer%3A%20Dreaming%20Box%20Corners%20for%20Generalizable%20Object%20Pose%0A%20%20Estimation%0AAuthor%3A%20Yuanhong%20Yu%20and%20Xingyi%20He%20and%20Chen%20Zhao%20and%20Junhao%20Yu%20and%20Jiaqi%20Yang%20and%20Ruizhen%20Hu%20and%20Yujun%20Shen%20and%20Xing%20Zhu%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20generalizable%20RGB-based%20approach%20for%20object%20pose%0Aestimation%2C%20specifically%20designed%20to%20address%20challenges%20in%20sparse-view%0Asettings.%20While%20existing%20methods%20can%20estimate%20the%20poses%20of%20unseen%20objects%2C%0Atheir%20generalization%20ability%20remains%20limited%20in%20scenarios%20involving%20occlusions%0Aand%20sparse%20reference%20views%2C%20restricting%20their%20real-world%20applicability.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20corner%20points%20of%20the%20object%20bounding%0Abox%20as%20an%20intermediate%20representation%20of%20the%20object%20pose.%20The%203D%20object%20corners%0Acan%20be%20reliably%20recovered%20from%20sparse%20input%20views%2C%20while%20the%202D%20corner%20points%0Ain%20the%20target%20view%20are%20estimated%20through%20a%20novel%20reference-based%20point%0Asynthesizer%2C%20which%20works%20well%20even%20in%20scenarios%20involving%20occlusions.%20As%20object%0Asemantic%20points%2C%20object%20corners%20naturally%20establish%202D-3D%20correspondences%20for%0Aobject%20pose%20estimation%20with%20a%20PnP%20algorithm.%20Extensive%20experiments%20on%20the%0AYCB-Video%20and%20Occluded-LINEMOD%20datasets%20show%20that%20our%20approach%20outperforms%0Astate-of-the-art%20methods%2C%20highlighting%20the%20effectiveness%20of%20the%20proposed%0Arepresentation%20and%20significantly%20enhancing%20the%20generalization%20capabilities%20of%0Aobject%20pose%20estimation%2C%20which%20is%20crucial%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxDreamer%253A%2520Dreaming%2520Box%2520Corners%2520for%2520Generalizable%2520Object%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DYuanhong%2520Yu%2520and%2520Xingyi%2520He%2520and%2520Chen%2520Zhao%2520and%2520Junhao%2520Yu%2520and%2520Jiaqi%2520Yang%2520and%2520Ruizhen%2520Hu%2520and%2520Yujun%2520Shen%2520and%2520Xing%2520Zhu%2520and%2520Xiaowei%2520Zhou%2520and%2520Sida%2520Peng%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520generalizable%2520RGB-based%2520approach%2520for%2520object%2520pose%250Aestimation%252C%2520specifically%2520designed%2520to%2520address%2520challenges%2520in%2520sparse-view%250Asettings.%2520While%2520existing%2520methods%2520can%2520estimate%2520the%2520poses%2520of%2520unseen%2520objects%252C%250Atheir%2520generalization%2520ability%2520remains%2520limited%2520in%2520scenarios%2520involving%2520occlusions%250Aand%2520sparse%2520reference%2520views%252C%2520restricting%2520their%2520real-world%2520applicability.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520introduce%2520corner%2520points%2520of%2520the%2520object%2520bounding%250Abox%2520as%2520an%2520intermediate%2520representation%2520of%2520the%2520object%2520pose.%2520The%25203D%2520object%2520corners%250Acan%2520be%2520reliably%2520recovered%2520from%2520sparse%2520input%2520views%252C%2520while%2520the%25202D%2520corner%2520points%250Ain%2520the%2520target%2520view%2520are%2520estimated%2520through%2520a%2520novel%2520reference-based%2520point%250Asynthesizer%252C%2520which%2520works%2520well%2520even%2520in%2520scenarios%2520involving%2520occlusions.%2520As%2520object%250Asemantic%2520points%252C%2520object%2520corners%2520naturally%2520establish%25202D-3D%2520correspondences%2520for%250Aobject%2520pose%2520estimation%2520with%2520a%2520PnP%2520algorithm.%2520Extensive%2520experiments%2520on%2520the%250AYCB-Video%2520and%2520Occluded-LINEMOD%2520datasets%2520show%2520that%2520our%2520approach%2520outperforms%250Astate-of-the-art%2520methods%252C%2520highlighting%2520the%2520effectiveness%2520of%2520the%2520proposed%250Arepresentation%2520and%2520significantly%2520enhancing%2520the%2520generalization%2520capabilities%2520of%250Aobject%2520pose%2520estimation%252C%2520which%2520is%2520crucial%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoxDreamer%3A%20Dreaming%20Box%20Corners%20for%20Generalizable%20Object%20Pose%0A%20%20Estimation&entry.906535625=Yuanhong%20Yu%20and%20Xingyi%20He%20and%20Chen%20Zhao%20and%20Junhao%20Yu%20and%20Jiaqi%20Yang%20and%20Ruizhen%20Hu%20and%20Yujun%20Shen%20and%20Xing%20Zhu%20and%20Xiaowei%20Zhou%20and%20Sida%20Peng&entry.1292438233=%20%20This%20paper%20presents%20a%20generalizable%20RGB-based%20approach%20for%20object%20pose%0Aestimation%2C%20specifically%20designed%20to%20address%20challenges%20in%20sparse-view%0Asettings.%20While%20existing%20methods%20can%20estimate%20the%20poses%20of%20unseen%20objects%2C%0Atheir%20generalization%20ability%20remains%20limited%20in%20scenarios%20involving%20occlusions%0Aand%20sparse%20reference%20views%2C%20restricting%20their%20real-world%20applicability.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20corner%20points%20of%20the%20object%20bounding%0Abox%20as%20an%20intermediate%20representation%20of%20the%20object%20pose.%20The%203D%20object%20corners%0Acan%20be%20reliably%20recovered%20from%20sparse%20input%20views%2C%20while%20the%202D%20corner%20points%0Ain%20the%20target%20view%20are%20estimated%20through%20a%20novel%20reference-based%20point%0Asynthesizer%2C%20which%20works%20well%20even%20in%20scenarios%20involving%20occlusions.%20As%20object%0Asemantic%20points%2C%20object%20corners%20naturally%20establish%202D-3D%20correspondences%20for%0Aobject%20pose%20estimation%20with%20a%20PnP%20algorithm.%20Extensive%20experiments%20on%20the%0AYCB-Video%20and%20Occluded-LINEMOD%20datasets%20show%20that%20our%20approach%20outperforms%0Astate-of-the-art%20methods%2C%20highlighting%20the%20effectiveness%20of%20the%20proposed%0Arepresentation%20and%20significantly%20enhancing%20the%20generalization%20capabilities%20of%0Aobject%20pose%20estimation%2C%20which%20is%20crucial%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07955v2&entry.124074799=Read"},
{"title": "Parametric Neural Amp Modeling with Active Learning", "author": "Florian Gr\u00f6tschla and Longxiang Jiao and Luca A. Lanzend\u00f6rfer and Roger Wattenhofer", "abstract": "  We introduce Panama, an active learning framework to train parametric guitar\namp models end-to-end using a combination of an LSTM model and a WaveNet-like\narchitecture. With \\model, one can create a virtual amp by recording samples\nthat are determined through an ensemble-based active learning strategy to\nminimize the amount of datapoints needed (i.e., amp knob settings). Our\nstrategy uses gradient-based optimization to maximize the disagreement among\nensemble models, in order to identify the most informative datapoints. MUSHRA\nlistening tests reveal that, with 75 datapoints, our models are able to match\nthe perceptual quality of NAM, the leading open-source non-parametric amp\nmodeler.\n", "link": "http://arxiv.org/abs/2509.26564v1", "date": "2025-09-30", "relevancy": 2.2508, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4427}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parametric%20Neural%20Amp%20Modeling%20with%20Active%20Learning&body=Title%3A%20Parametric%20Neural%20Amp%20Modeling%20with%20Active%20Learning%0AAuthor%3A%20Florian%20Gr%C3%B6tschla%20and%20Longxiang%20Jiao%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer%0AAbstract%3A%20%20%20We%20introduce%20Panama%2C%20an%20active%20learning%20framework%20to%20train%20parametric%20guitar%0Aamp%20models%20end-to-end%20using%20a%20combination%20of%20an%20LSTM%20model%20and%20a%20WaveNet-like%0Aarchitecture.%20With%20%5Cmodel%2C%20one%20can%20create%20a%20virtual%20amp%20by%20recording%20samples%0Athat%20are%20determined%20through%20an%20ensemble-based%20active%20learning%20strategy%20to%0Aminimize%20the%20amount%20of%20datapoints%20needed%20%28i.e.%2C%20amp%20knob%20settings%29.%20Our%0Astrategy%20uses%20gradient-based%20optimization%20to%20maximize%20the%20disagreement%20among%0Aensemble%20models%2C%20in%20order%20to%20identify%20the%20most%20informative%20datapoints.%20MUSHRA%0Alistening%20tests%20reveal%20that%2C%20with%2075%20datapoints%2C%20our%20models%20are%20able%20to%20match%0Athe%20perceptual%20quality%20of%20NAM%2C%20the%20leading%20open-source%20non-parametric%20amp%0Amodeler.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParametric%2520Neural%2520Amp%2520Modeling%2520with%2520Active%2520Learning%26entry.906535625%3DFlorian%2520Gr%25C3%25B6tschla%2520and%2520Longxiang%2520Jiao%2520and%2520Luca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Roger%2520Wattenhofer%26entry.1292438233%3D%2520%2520We%2520introduce%2520Panama%252C%2520an%2520active%2520learning%2520framework%2520to%2520train%2520parametric%2520guitar%250Aamp%2520models%2520end-to-end%2520using%2520a%2520combination%2520of%2520an%2520LSTM%2520model%2520and%2520a%2520WaveNet-like%250Aarchitecture.%2520With%2520%255Cmodel%252C%2520one%2520can%2520create%2520a%2520virtual%2520amp%2520by%2520recording%2520samples%250Athat%2520are%2520determined%2520through%2520an%2520ensemble-based%2520active%2520learning%2520strategy%2520to%250Aminimize%2520the%2520amount%2520of%2520datapoints%2520needed%2520%2528i.e.%252C%2520amp%2520knob%2520settings%2529.%2520Our%250Astrategy%2520uses%2520gradient-based%2520optimization%2520to%2520maximize%2520the%2520disagreement%2520among%250Aensemble%2520models%252C%2520in%2520order%2520to%2520identify%2520the%2520most%2520informative%2520datapoints.%2520MUSHRA%250Alistening%2520tests%2520reveal%2520that%252C%2520with%252075%2520datapoints%252C%2520our%2520models%2520are%2520able%2520to%2520match%250Athe%2520perceptual%2520quality%2520of%2520NAM%252C%2520the%2520leading%2520open-source%2520non-parametric%2520amp%250Amodeler.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parametric%20Neural%20Amp%20Modeling%20with%20Active%20Learning&entry.906535625=Florian%20Gr%C3%B6tschla%20and%20Longxiang%20Jiao%20and%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Roger%20Wattenhofer&entry.1292438233=%20%20We%20introduce%20Panama%2C%20an%20active%20learning%20framework%20to%20train%20parametric%20guitar%0Aamp%20models%20end-to-end%20using%20a%20combination%20of%20an%20LSTM%20model%20and%20a%20WaveNet-like%0Aarchitecture.%20With%20%5Cmodel%2C%20one%20can%20create%20a%20virtual%20amp%20by%20recording%20samples%0Athat%20are%20determined%20through%20an%20ensemble-based%20active%20learning%20strategy%20to%0Aminimize%20the%20amount%20of%20datapoints%20needed%20%28i.e.%2C%20amp%20knob%20settings%29.%20Our%0Astrategy%20uses%20gradient-based%20optimization%20to%20maximize%20the%20disagreement%20among%0Aensemble%20models%2C%20in%20order%20to%20identify%20the%20most%20informative%20datapoints.%20MUSHRA%0Alistening%20tests%20reveal%20that%2C%20with%2075%20datapoints%2C%20our%20models%20are%20able%20to%20match%0Athe%20perceptual%20quality%20of%20NAM%2C%20the%20leading%20open-source%20non-parametric%20amp%0Amodeler.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26564v1&entry.124074799=Read"},
{"title": "Estimating Dimensionality of Neural Representations from Finite Samples", "author": "Chanwoo Chun and Abdulkadir Canatar and SueYeon Chung and Daniel Lee", "abstract": "  The global dimensionality of a neural representation manifold provides rich\ninsight into the computational process underlying both artificial and\nbiological neural networks. However, all existing measures of global\ndimensionality are sensitive to the number of samples, i.e., the number of rows\nand columns of the sample matrix. We show that, in particular, the\nparticipation ratio of eigenvalues, a popular measure of global dimensionality,\nis highly biased with small sample sizes, and propose a bias-corrected\nestimator that is more accurate with finite samples and with noise. On\nsynthetic data examples, we demonstrate that our estimator can recover the true\nknown dimensionality. We apply our estimator to neural brain recordings,\nincluding calcium imaging, electrophysiological recordings, and fMRI data, and\nto the neural activations in a large language model and show our estimator is\ninvariant to the sample size. Finally, our estimators can additionally be used\nto measure the local dimensionalities of curved neural manifolds by weighting\nthe finite samples appropriately.\n", "link": "http://arxiv.org/abs/2509.26560v1", "date": "2025-09-30", "relevancy": 2.2277, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4465}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.446}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Dimensionality%20of%20Neural%20Representations%20from%20Finite%20Samples&body=Title%3A%20Estimating%20Dimensionality%20of%20Neural%20Representations%20from%20Finite%20Samples%0AAuthor%3A%20Chanwoo%20Chun%20and%20Abdulkadir%20Canatar%20and%20SueYeon%20Chung%20and%20Daniel%20Lee%0AAbstract%3A%20%20%20The%20global%20dimensionality%20of%20a%20neural%20representation%20manifold%20provides%20rich%0Ainsight%20into%20the%20computational%20process%20underlying%20both%20artificial%20and%0Abiological%20neural%20networks.%20However%2C%20all%20existing%20measures%20of%20global%0Adimensionality%20are%20sensitive%20to%20the%20number%20of%20samples%2C%20i.e.%2C%20the%20number%20of%20rows%0Aand%20columns%20of%20the%20sample%20matrix.%20We%20show%20that%2C%20in%20particular%2C%20the%0Aparticipation%20ratio%20of%20eigenvalues%2C%20a%20popular%20measure%20of%20global%20dimensionality%2C%0Ais%20highly%20biased%20with%20small%20sample%20sizes%2C%20and%20propose%20a%20bias-corrected%0Aestimator%20that%20is%20more%20accurate%20with%20finite%20samples%20and%20with%20noise.%20On%0Asynthetic%20data%20examples%2C%20we%20demonstrate%20that%20our%20estimator%20can%20recover%20the%20true%0Aknown%20dimensionality.%20We%20apply%20our%20estimator%20to%20neural%20brain%20recordings%2C%0Aincluding%20calcium%20imaging%2C%20electrophysiological%20recordings%2C%20and%20fMRI%20data%2C%20and%0Ato%20the%20neural%20activations%20in%20a%20large%20language%20model%20and%20show%20our%20estimator%20is%0Ainvariant%20to%20the%20sample%20size.%20Finally%2C%20our%20estimators%20can%20additionally%20be%20used%0Ato%20measure%20the%20local%20dimensionalities%20of%20curved%20neural%20manifolds%20by%20weighting%0Athe%20finite%20samples%20appropriately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Dimensionality%2520of%2520Neural%2520Representations%2520from%2520Finite%2520Samples%26entry.906535625%3DChanwoo%2520Chun%2520and%2520Abdulkadir%2520Canatar%2520and%2520SueYeon%2520Chung%2520and%2520Daniel%2520Lee%26entry.1292438233%3D%2520%2520The%2520global%2520dimensionality%2520of%2520a%2520neural%2520representation%2520manifold%2520provides%2520rich%250Ainsight%2520into%2520the%2520computational%2520process%2520underlying%2520both%2520artificial%2520and%250Abiological%2520neural%2520networks.%2520However%252C%2520all%2520existing%2520measures%2520of%2520global%250Adimensionality%2520are%2520sensitive%2520to%2520the%2520number%2520of%2520samples%252C%2520i.e.%252C%2520the%2520number%2520of%2520rows%250Aand%2520columns%2520of%2520the%2520sample%2520matrix.%2520We%2520show%2520that%252C%2520in%2520particular%252C%2520the%250Aparticipation%2520ratio%2520of%2520eigenvalues%252C%2520a%2520popular%2520measure%2520of%2520global%2520dimensionality%252C%250Ais%2520highly%2520biased%2520with%2520small%2520sample%2520sizes%252C%2520and%2520propose%2520a%2520bias-corrected%250Aestimator%2520that%2520is%2520more%2520accurate%2520with%2520finite%2520samples%2520and%2520with%2520noise.%2520On%250Asynthetic%2520data%2520examples%252C%2520we%2520demonstrate%2520that%2520our%2520estimator%2520can%2520recover%2520the%2520true%250Aknown%2520dimensionality.%2520We%2520apply%2520our%2520estimator%2520to%2520neural%2520brain%2520recordings%252C%250Aincluding%2520calcium%2520imaging%252C%2520electrophysiological%2520recordings%252C%2520and%2520fMRI%2520data%252C%2520and%250Ato%2520the%2520neural%2520activations%2520in%2520a%2520large%2520language%2520model%2520and%2520show%2520our%2520estimator%2520is%250Ainvariant%2520to%2520the%2520sample%2520size.%2520Finally%252C%2520our%2520estimators%2520can%2520additionally%2520be%2520used%250Ato%2520measure%2520the%2520local%2520dimensionalities%2520of%2520curved%2520neural%2520manifolds%2520by%2520weighting%250Athe%2520finite%2520samples%2520appropriately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Dimensionality%20of%20Neural%20Representations%20from%20Finite%20Samples&entry.906535625=Chanwoo%20Chun%20and%20Abdulkadir%20Canatar%20and%20SueYeon%20Chung%20and%20Daniel%20Lee&entry.1292438233=%20%20The%20global%20dimensionality%20of%20a%20neural%20representation%20manifold%20provides%20rich%0Ainsight%20into%20the%20computational%20process%20underlying%20both%20artificial%20and%0Abiological%20neural%20networks.%20However%2C%20all%20existing%20measures%20of%20global%0Adimensionality%20are%20sensitive%20to%20the%20number%20of%20samples%2C%20i.e.%2C%20the%20number%20of%20rows%0Aand%20columns%20of%20the%20sample%20matrix.%20We%20show%20that%2C%20in%20particular%2C%20the%0Aparticipation%20ratio%20of%20eigenvalues%2C%20a%20popular%20measure%20of%20global%20dimensionality%2C%0Ais%20highly%20biased%20with%20small%20sample%20sizes%2C%20and%20propose%20a%20bias-corrected%0Aestimator%20that%20is%20more%20accurate%20with%20finite%20samples%20and%20with%20noise.%20On%0Asynthetic%20data%20examples%2C%20we%20demonstrate%20that%20our%20estimator%20can%20recover%20the%20true%0Aknown%20dimensionality.%20We%20apply%20our%20estimator%20to%20neural%20brain%20recordings%2C%0Aincluding%20calcium%20imaging%2C%20electrophysiological%20recordings%2C%20and%20fMRI%20data%2C%20and%0Ato%20the%20neural%20activations%20in%20a%20large%20language%20model%20and%20show%20our%20estimator%20is%0Ainvariant%20to%20the%20sample%20size.%20Finally%2C%20our%20estimators%20can%20additionally%20be%20used%0Ato%20measure%20the%20local%20dimensionalities%20of%20curved%20neural%20manifolds%20by%20weighting%0Athe%20finite%20samples%20appropriately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26560v1&entry.124074799=Read"},
{"title": "Learning from Hallucinating Critical Points for Navigation in Dynamic\n  Environments", "author": "Saad Abdul Ghani and Kameron Lee and Xuesu Xiao", "abstract": "  Generating large and diverse obstacle datasets to learn motion planning in\nenvironments with dynamic obstacles is challenging due to the vast space of\npossible obstacle trajectories. Inspired by hallucination-based data synthesis\napproaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a\nself-supervised framework for creating rich dynamic obstacle datasets based on\nexisting optimal motion plans without requiring expensive expert demonstrations\nor trial-and-error exploration. LfH-CP factorizes hallucination into two\nstages: first identifying when and where obstacles must appear in order to\nresult in an optimal motion plan, i.e., the critical points, and then\nprocedurally generating diverse trajectories that pass through these points\nwhile avoiding collisions. This factorization avoids generative failures such\nas mode collapse and ensures coverage of diverse dynamic behaviors. We further\nintroduce a diversity metric to quantify dataset richness and show that LfH-CP\nproduces substantially more varied training data than existing baselines.\nExperiments in simulation demonstrate that planners trained on LfH-CP datasets\nachieves higher success rates compared to a prior hallucination method.\n", "link": "http://arxiv.org/abs/2509.26513v1", "date": "2025-09-30", "relevancy": 2.2258, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5639}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5581}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Hallucinating%20Critical%20Points%20for%20Navigation%20in%20Dynamic%0A%20%20Environments&body=Title%3A%20Learning%20from%20Hallucinating%20Critical%20Points%20for%20Navigation%20in%20Dynamic%0A%20%20Environments%0AAuthor%3A%20Saad%20Abdul%20Ghani%20and%20Kameron%20Lee%20and%20Xuesu%20Xiao%0AAbstract%3A%20%20%20Generating%20large%20and%20diverse%20obstacle%20datasets%20to%20learn%20motion%20planning%20in%0Aenvironments%20with%20dynamic%20obstacles%20is%20challenging%20due%20to%20the%20vast%20space%20of%0Apossible%20obstacle%20trajectories.%20Inspired%20by%20hallucination-based%20data%20synthesis%0Aapproaches%2C%20we%20propose%20Learning%20from%20Hallucinating%20Critical%20Points%20%28LfH-CP%29%2C%20a%0Aself-supervised%20framework%20for%20creating%20rich%20dynamic%20obstacle%20datasets%20based%20on%0Aexisting%20optimal%20motion%20plans%20without%20requiring%20expensive%20expert%20demonstrations%0Aor%20trial-and-error%20exploration.%20LfH-CP%20factorizes%20hallucination%20into%20two%0Astages%3A%20first%20identifying%20when%20and%20where%20obstacles%20must%20appear%20in%20order%20to%0Aresult%20in%20an%20optimal%20motion%20plan%2C%20i.e.%2C%20the%20critical%20points%2C%20and%20then%0Aprocedurally%20generating%20diverse%20trajectories%20that%20pass%20through%20these%20points%0Awhile%20avoiding%20collisions.%20This%20factorization%20avoids%20generative%20failures%20such%0Aas%20mode%20collapse%20and%20ensures%20coverage%20of%20diverse%20dynamic%20behaviors.%20We%20further%0Aintroduce%20a%20diversity%20metric%20to%20quantify%20dataset%20richness%20and%20show%20that%20LfH-CP%0Aproduces%20substantially%20more%20varied%20training%20data%20than%20existing%20baselines.%0AExperiments%20in%20simulation%20demonstrate%20that%20planners%20trained%20on%20LfH-CP%20datasets%0Aachieves%20higher%20success%20rates%20compared%20to%20a%20prior%20hallucination%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Hallucinating%2520Critical%2520Points%2520for%2520Navigation%2520in%2520Dynamic%250A%2520%2520Environments%26entry.906535625%3DSaad%2520Abdul%2520Ghani%2520and%2520Kameron%2520Lee%2520and%2520Xuesu%2520Xiao%26entry.1292438233%3D%2520%2520Generating%2520large%2520and%2520diverse%2520obstacle%2520datasets%2520to%2520learn%2520motion%2520planning%2520in%250Aenvironments%2520with%2520dynamic%2520obstacles%2520is%2520challenging%2520due%2520to%2520the%2520vast%2520space%2520of%250Apossible%2520obstacle%2520trajectories.%2520Inspired%2520by%2520hallucination-based%2520data%2520synthesis%250Aapproaches%252C%2520we%2520propose%2520Learning%2520from%2520Hallucinating%2520Critical%2520Points%2520%2528LfH-CP%2529%252C%2520a%250Aself-supervised%2520framework%2520for%2520creating%2520rich%2520dynamic%2520obstacle%2520datasets%2520based%2520on%250Aexisting%2520optimal%2520motion%2520plans%2520without%2520requiring%2520expensive%2520expert%2520demonstrations%250Aor%2520trial-and-error%2520exploration.%2520LfH-CP%2520factorizes%2520hallucination%2520into%2520two%250Astages%253A%2520first%2520identifying%2520when%2520and%2520where%2520obstacles%2520must%2520appear%2520in%2520order%2520to%250Aresult%2520in%2520an%2520optimal%2520motion%2520plan%252C%2520i.e.%252C%2520the%2520critical%2520points%252C%2520and%2520then%250Aprocedurally%2520generating%2520diverse%2520trajectories%2520that%2520pass%2520through%2520these%2520points%250Awhile%2520avoiding%2520collisions.%2520This%2520factorization%2520avoids%2520generative%2520failures%2520such%250Aas%2520mode%2520collapse%2520and%2520ensures%2520coverage%2520of%2520diverse%2520dynamic%2520behaviors.%2520We%2520further%250Aintroduce%2520a%2520diversity%2520metric%2520to%2520quantify%2520dataset%2520richness%2520and%2520show%2520that%2520LfH-CP%250Aproduces%2520substantially%2520more%2520varied%2520training%2520data%2520than%2520existing%2520baselines.%250AExperiments%2520in%2520simulation%2520demonstrate%2520that%2520planners%2520trained%2520on%2520LfH-CP%2520datasets%250Aachieves%2520higher%2520success%2520rates%2520compared%2520to%2520a%2520prior%2520hallucination%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Hallucinating%20Critical%20Points%20for%20Navigation%20in%20Dynamic%0A%20%20Environments&entry.906535625=Saad%20Abdul%20Ghani%20and%20Kameron%20Lee%20and%20Xuesu%20Xiao&entry.1292438233=%20%20Generating%20large%20and%20diverse%20obstacle%20datasets%20to%20learn%20motion%20planning%20in%0Aenvironments%20with%20dynamic%20obstacles%20is%20challenging%20due%20to%20the%20vast%20space%20of%0Apossible%20obstacle%20trajectories.%20Inspired%20by%20hallucination-based%20data%20synthesis%0Aapproaches%2C%20we%20propose%20Learning%20from%20Hallucinating%20Critical%20Points%20%28LfH-CP%29%2C%20a%0Aself-supervised%20framework%20for%20creating%20rich%20dynamic%20obstacle%20datasets%20based%20on%0Aexisting%20optimal%20motion%20plans%20without%20requiring%20expensive%20expert%20demonstrations%0Aor%20trial-and-error%20exploration.%20LfH-CP%20factorizes%20hallucination%20into%20two%0Astages%3A%20first%20identifying%20when%20and%20where%20obstacles%20must%20appear%20in%20order%20to%0Aresult%20in%20an%20optimal%20motion%20plan%2C%20i.e.%2C%20the%20critical%20points%2C%20and%20then%0Aprocedurally%20generating%20diverse%20trajectories%20that%20pass%20through%20these%20points%0Awhile%20avoiding%20collisions.%20This%20factorization%20avoids%20generative%20failures%20such%0Aas%20mode%20collapse%20and%20ensures%20coverage%20of%20diverse%20dynamic%20behaviors.%20We%20further%0Aintroduce%20a%20diversity%20metric%20to%20quantify%20dataset%20richness%20and%20show%20that%20LfH-CP%0Aproduces%20substantially%20more%20varied%20training%20data%20than%20existing%20baselines.%0AExperiments%20in%20simulation%20demonstrate%20that%20planners%20trained%20on%20LfH-CP%20datasets%0Aachieves%20higher%20success%20rates%20compared%20to%20a%20prior%20hallucination%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26513v1&entry.124074799=Read"},
{"title": "GastroViT: A Vision Transformer Based Ensemble Learning Approach for\n  Gastrointestinal Disease Classification with Grad CAM & SHAP Visualization", "author": "Sumaiya Tabassum and Md. Faysal Ahamed and Hafsa Binte Kibria and Md. Nahiduzzaman and Julfikar Haider and Muhammad E. H. Chowdhury and Mohammad Tariqul Islam", "abstract": "  The gastrointestinal (GI) tract of humans can have a wide variety of aberrant\nmucosal abnormality findings, ranging from mild irritations to extremely fatal\nillnesses. Prompt identification of gastrointestinal disorders greatly\ncontributes to arresting the progression of the illness and improving\ntherapeutic outcomes. This paper presents an ensemble of pre-trained vision\ntransformers (ViTs) for accurately classifying endoscopic images of the GI\ntract to categorize gastrointestinal problems and illnesses. ViTs,\nattention-based neural networks, have revolutionized image recognition by\nleveraging the transformative power of the transformer architecture, achieving\nstate-of-the-art (SOTA) performance across various visual tasks. The proposed\nmodel was evaluated on the publicly available HyperKvasir dataset with 10,662\nimages of 23 different GI diseases for the purpose of identifying GI tract\ndiseases. An ensemble method is proposed utilizing the predictions of two\npre-trained models, MobileViT_XS and MobileViT_V2_200, which achieved\naccuracies of 90.57% and 90.48%, respectively. All the individual models are\noutperformed by the ensemble model, GastroViT, with an average precision,\nrecall, F1 score, and accuracy of 69%, 63%, 64%, and 91.98%, respectively, in\nthe first testing that involves 23 classes. The model comprises only 20 million\n(M) parameters, even without data augmentation and despite the highly\nimbalanced dataset. For the second testing with 16 classes, the scores are even\nhigher, with average precision, recall, F1 score, and accuracy of 87%, 86%,\n87%, and 92.70%, respectively. Additionally, the incorporation of explainable\nAI (XAI) methods such as Grad-CAM (Gradient Weighted Class Activation Mapping)\nand SHAP (Shapley Additive Explanations) enhances model interpretability,\nproviding valuable insights for reliable GI diagnosis in real-world settings.\n", "link": "http://arxiv.org/abs/2509.26502v1", "date": "2025-09-30", "relevancy": 2.2168, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5705}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GastroViT%3A%20A%20Vision%20Transformer%20Based%20Ensemble%20Learning%20Approach%20for%0A%20%20Gastrointestinal%20Disease%20Classification%20with%20Grad%20CAM%20%26%20SHAP%20Visualization&body=Title%3A%20GastroViT%3A%20A%20Vision%20Transformer%20Based%20Ensemble%20Learning%20Approach%20for%0A%20%20Gastrointestinal%20Disease%20Classification%20with%20Grad%20CAM%20%26%20SHAP%20Visualization%0AAuthor%3A%20Sumaiya%20Tabassum%20and%20Md.%20Faysal%20Ahamed%20and%20Hafsa%20Binte%20Kibria%20and%20Md.%20Nahiduzzaman%20and%20Julfikar%20Haider%20and%20Muhammad%20E.%20H.%20Chowdhury%20and%20Mohammad%20Tariqul%20Islam%0AAbstract%3A%20%20%20The%20gastrointestinal%20%28GI%29%20tract%20of%20humans%20can%20have%20a%20wide%20variety%20of%20aberrant%0Amucosal%20abnormality%20findings%2C%20ranging%20from%20mild%20irritations%20to%20extremely%20fatal%0Aillnesses.%20Prompt%20identification%20of%20gastrointestinal%20disorders%20greatly%0Acontributes%20to%20arresting%20the%20progression%20of%20the%20illness%20and%20improving%0Atherapeutic%20outcomes.%20This%20paper%20presents%20an%20ensemble%20of%20pre-trained%20vision%0Atransformers%20%28ViTs%29%20for%20accurately%20classifying%20endoscopic%20images%20of%20the%20GI%0Atract%20to%20categorize%20gastrointestinal%20problems%20and%20illnesses.%20ViTs%2C%0Aattention-based%20neural%20networks%2C%20have%20revolutionized%20image%20recognition%20by%0Aleveraging%20the%20transformative%20power%20of%20the%20transformer%20architecture%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20various%20visual%20tasks.%20The%20proposed%0Amodel%20was%20evaluated%20on%20the%20publicly%20available%20HyperKvasir%20dataset%20with%2010%2C662%0Aimages%20of%2023%20different%20GI%20diseases%20for%20the%20purpose%20of%20identifying%20GI%20tract%0Adiseases.%20An%20ensemble%20method%20is%20proposed%20utilizing%20the%20predictions%20of%20two%0Apre-trained%20models%2C%20MobileViT_XS%20and%20MobileViT_V2_200%2C%20which%20achieved%0Aaccuracies%20of%2090.57%25%20and%2090.48%25%2C%20respectively.%20All%20the%20individual%20models%20are%0Aoutperformed%20by%20the%20ensemble%20model%2C%20GastroViT%2C%20with%20an%20average%20precision%2C%0Arecall%2C%20F1%20score%2C%20and%20accuracy%20of%2069%25%2C%2063%25%2C%2064%25%2C%20and%2091.98%25%2C%20respectively%2C%20in%0Athe%20first%20testing%20that%20involves%2023%20classes.%20The%20model%20comprises%20only%2020%20million%0A%28M%29%20parameters%2C%20even%20without%20data%20augmentation%20and%20despite%20the%20highly%0Aimbalanced%20dataset.%20For%20the%20second%20testing%20with%2016%20classes%2C%20the%20scores%20are%20even%0Ahigher%2C%20with%20average%20precision%2C%20recall%2C%20F1%20score%2C%20and%20accuracy%20of%2087%25%2C%2086%25%2C%0A87%25%2C%20and%2092.70%25%2C%20respectively.%20Additionally%2C%20the%20incorporation%20of%20explainable%0AAI%20%28XAI%29%20methods%20such%20as%20Grad-CAM%20%28Gradient%20Weighted%20Class%20Activation%20Mapping%29%0Aand%20SHAP%20%28Shapley%20Additive%20Explanations%29%20enhances%20model%20interpretability%2C%0Aproviding%20valuable%20insights%20for%20reliable%20GI%20diagnosis%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGastroViT%253A%2520A%2520Vision%2520Transformer%2520Based%2520Ensemble%2520Learning%2520Approach%2520for%250A%2520%2520Gastrointestinal%2520Disease%2520Classification%2520with%2520Grad%2520CAM%2520%2526%2520SHAP%2520Visualization%26entry.906535625%3DSumaiya%2520Tabassum%2520and%2520Md.%2520Faysal%2520Ahamed%2520and%2520Hafsa%2520Binte%2520Kibria%2520and%2520Md.%2520Nahiduzzaman%2520and%2520Julfikar%2520Haider%2520and%2520Muhammad%2520E.%2520H.%2520Chowdhury%2520and%2520Mohammad%2520Tariqul%2520Islam%26entry.1292438233%3D%2520%2520The%2520gastrointestinal%2520%2528GI%2529%2520tract%2520of%2520humans%2520can%2520have%2520a%2520wide%2520variety%2520of%2520aberrant%250Amucosal%2520abnormality%2520findings%252C%2520ranging%2520from%2520mild%2520irritations%2520to%2520extremely%2520fatal%250Aillnesses.%2520Prompt%2520identification%2520of%2520gastrointestinal%2520disorders%2520greatly%250Acontributes%2520to%2520arresting%2520the%2520progression%2520of%2520the%2520illness%2520and%2520improving%250Atherapeutic%2520outcomes.%2520This%2520paper%2520presents%2520an%2520ensemble%2520of%2520pre-trained%2520vision%250Atransformers%2520%2528ViTs%2529%2520for%2520accurately%2520classifying%2520endoscopic%2520images%2520of%2520the%2520GI%250Atract%2520to%2520categorize%2520gastrointestinal%2520problems%2520and%2520illnesses.%2520ViTs%252C%250Aattention-based%2520neural%2520networks%252C%2520have%2520revolutionized%2520image%2520recognition%2520by%250Aleveraging%2520the%2520transformative%2520power%2520of%2520the%2520transformer%2520architecture%252C%2520achieving%250Astate-of-the-art%2520%2528SOTA%2529%2520performance%2520across%2520various%2520visual%2520tasks.%2520The%2520proposed%250Amodel%2520was%2520evaluated%2520on%2520the%2520publicly%2520available%2520HyperKvasir%2520dataset%2520with%252010%252C662%250Aimages%2520of%252023%2520different%2520GI%2520diseases%2520for%2520the%2520purpose%2520of%2520identifying%2520GI%2520tract%250Adiseases.%2520An%2520ensemble%2520method%2520is%2520proposed%2520utilizing%2520the%2520predictions%2520of%2520two%250Apre-trained%2520models%252C%2520MobileViT_XS%2520and%2520MobileViT_V2_200%252C%2520which%2520achieved%250Aaccuracies%2520of%252090.57%2525%2520and%252090.48%2525%252C%2520respectively.%2520All%2520the%2520individual%2520models%2520are%250Aoutperformed%2520by%2520the%2520ensemble%2520model%252C%2520GastroViT%252C%2520with%2520an%2520average%2520precision%252C%250Arecall%252C%2520F1%2520score%252C%2520and%2520accuracy%2520of%252069%2525%252C%252063%2525%252C%252064%2525%252C%2520and%252091.98%2525%252C%2520respectively%252C%2520in%250Athe%2520first%2520testing%2520that%2520involves%252023%2520classes.%2520The%2520model%2520comprises%2520only%252020%2520million%250A%2528M%2529%2520parameters%252C%2520even%2520without%2520data%2520augmentation%2520and%2520despite%2520the%2520highly%250Aimbalanced%2520dataset.%2520For%2520the%2520second%2520testing%2520with%252016%2520classes%252C%2520the%2520scores%2520are%2520even%250Ahigher%252C%2520with%2520average%2520precision%252C%2520recall%252C%2520F1%2520score%252C%2520and%2520accuracy%2520of%252087%2525%252C%252086%2525%252C%250A87%2525%252C%2520and%252092.70%2525%252C%2520respectively.%2520Additionally%252C%2520the%2520incorporation%2520of%2520explainable%250AAI%2520%2528XAI%2529%2520methods%2520such%2520as%2520Grad-CAM%2520%2528Gradient%2520Weighted%2520Class%2520Activation%2520Mapping%2529%250Aand%2520SHAP%2520%2528Shapley%2520Additive%2520Explanations%2529%2520enhances%2520model%2520interpretability%252C%250Aproviding%2520valuable%2520insights%2520for%2520reliable%2520GI%2520diagnosis%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GastroViT%3A%20A%20Vision%20Transformer%20Based%20Ensemble%20Learning%20Approach%20for%0A%20%20Gastrointestinal%20Disease%20Classification%20with%20Grad%20CAM%20%26%20SHAP%20Visualization&entry.906535625=Sumaiya%20Tabassum%20and%20Md.%20Faysal%20Ahamed%20and%20Hafsa%20Binte%20Kibria%20and%20Md.%20Nahiduzzaman%20and%20Julfikar%20Haider%20and%20Muhammad%20E.%20H.%20Chowdhury%20and%20Mohammad%20Tariqul%20Islam&entry.1292438233=%20%20The%20gastrointestinal%20%28GI%29%20tract%20of%20humans%20can%20have%20a%20wide%20variety%20of%20aberrant%0Amucosal%20abnormality%20findings%2C%20ranging%20from%20mild%20irritations%20to%20extremely%20fatal%0Aillnesses.%20Prompt%20identification%20of%20gastrointestinal%20disorders%20greatly%0Acontributes%20to%20arresting%20the%20progression%20of%20the%20illness%20and%20improving%0Atherapeutic%20outcomes.%20This%20paper%20presents%20an%20ensemble%20of%20pre-trained%20vision%0Atransformers%20%28ViTs%29%20for%20accurately%20classifying%20endoscopic%20images%20of%20the%20GI%0Atract%20to%20categorize%20gastrointestinal%20problems%20and%20illnesses.%20ViTs%2C%0Aattention-based%20neural%20networks%2C%20have%20revolutionized%20image%20recognition%20by%0Aleveraging%20the%20transformative%20power%20of%20the%20transformer%20architecture%2C%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance%20across%20various%20visual%20tasks.%20The%20proposed%0Amodel%20was%20evaluated%20on%20the%20publicly%20available%20HyperKvasir%20dataset%20with%2010%2C662%0Aimages%20of%2023%20different%20GI%20diseases%20for%20the%20purpose%20of%20identifying%20GI%20tract%0Adiseases.%20An%20ensemble%20method%20is%20proposed%20utilizing%20the%20predictions%20of%20two%0Apre-trained%20models%2C%20MobileViT_XS%20and%20MobileViT_V2_200%2C%20which%20achieved%0Aaccuracies%20of%2090.57%25%20and%2090.48%25%2C%20respectively.%20All%20the%20individual%20models%20are%0Aoutperformed%20by%20the%20ensemble%20model%2C%20GastroViT%2C%20with%20an%20average%20precision%2C%0Arecall%2C%20F1%20score%2C%20and%20accuracy%20of%2069%25%2C%2063%25%2C%2064%25%2C%20and%2091.98%25%2C%20respectively%2C%20in%0Athe%20first%20testing%20that%20involves%2023%20classes.%20The%20model%20comprises%20only%2020%20million%0A%28M%29%20parameters%2C%20even%20without%20data%20augmentation%20and%20despite%20the%20highly%0Aimbalanced%20dataset.%20For%20the%20second%20testing%20with%2016%20classes%2C%20the%20scores%20are%20even%0Ahigher%2C%20with%20average%20precision%2C%20recall%2C%20F1%20score%2C%20and%20accuracy%20of%2087%25%2C%2086%25%2C%0A87%25%2C%20and%2092.70%25%2C%20respectively.%20Additionally%2C%20the%20incorporation%20of%20explainable%0AAI%20%28XAI%29%20methods%20such%20as%20Grad-CAM%20%28Gradient%20Weighted%20Class%20Activation%20Mapping%29%0Aand%20SHAP%20%28Shapley%20Additive%20Explanations%29%20enhances%20model%20interpretability%2C%0Aproviding%20valuable%20insights%20for%20reliable%20GI%20diagnosis%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26502v1&entry.124074799=Read"},
{"title": "Seeing Through Deception: Uncovering Misleading Creator Intent in\n  Multimodal News with Vision-Language Models", "author": "Jiaying Wu and Fanxiao Li and Zihang Fu and Min-Yen Kan and Bryan Hooi", "abstract": "  The impact of misinformation arises not only from factual inaccuracies but\nalso from the misleading narratives that creators deliberately embed.\nInterpreting such creator intent is therefore essential for multimodal\nmisinformation detection (MMD) and effective information governance. To this\nend, we introduce DeceptionDecoded, a large-scale benchmark of 12,000\nimage-caption pairs grounded in trustworthy reference articles, created using\nan intent-guided simulation framework that models both the desired influence\nand the execution plan of news creators. The dataset captures both misleading\nand non-misleading cases, spanning manipulations across visual and textual\nmodalities, and supports three intent-centric tasks: (1) misleading intent\ndetection, (2) misleading source attribution, and (3) creator desire inference.\nWe evaluate 14 state-of-the-art vision-language models (VLMs) and find that\nthey struggle with intent reasoning, often relying on shallow cues such as\nsurface-level alignment, stylistic polish, or heuristic authenticity signals.\nThese results highlight the limitations of current VLMs and position\nDeceptionDecoded as a foundation for developing intent-aware models that go\nbeyond shallow cues in MMD.\n", "link": "http://arxiv.org/abs/2505.15489v3", "date": "2025-09-30", "relevancy": 2.2108, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Through%20Deception%3A%20Uncovering%20Misleading%20Creator%20Intent%20in%0A%20%20Multimodal%20News%20with%20Vision-Language%20Models&body=Title%3A%20Seeing%20Through%20Deception%3A%20Uncovering%20Misleading%20Creator%20Intent%20in%0A%20%20Multimodal%20News%20with%20Vision-Language%20Models%0AAuthor%3A%20Jiaying%20Wu%20and%20Fanxiao%20Li%20and%20Zihang%20Fu%20and%20Min-Yen%20Kan%20and%20Bryan%20Hooi%0AAbstract%3A%20%20%20The%20impact%20of%20misinformation%20arises%20not%20only%20from%20factual%20inaccuracies%20but%0Aalso%20from%20the%20misleading%20narratives%20that%20creators%20deliberately%20embed.%0AInterpreting%20such%20creator%20intent%20is%20therefore%20essential%20for%20multimodal%0Amisinformation%20detection%20%28MMD%29%20and%20effective%20information%20governance.%20To%20this%0Aend%2C%20we%20introduce%20DeceptionDecoded%2C%20a%20large-scale%20benchmark%20of%2012%2C000%0Aimage-caption%20pairs%20grounded%20in%20trustworthy%20reference%20articles%2C%20created%20using%0Aan%20intent-guided%20simulation%20framework%20that%20models%20both%20the%20desired%20influence%0Aand%20the%20execution%20plan%20of%20news%20creators.%20The%20dataset%20captures%20both%20misleading%0Aand%20non-misleading%20cases%2C%20spanning%20manipulations%20across%20visual%20and%20textual%0Amodalities%2C%20and%20supports%20three%20intent-centric%20tasks%3A%20%281%29%20misleading%20intent%0Adetection%2C%20%282%29%20misleading%20source%20attribution%2C%20and%20%283%29%20creator%20desire%20inference.%0AWe%20evaluate%2014%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20and%20find%20that%0Athey%20struggle%20with%20intent%20reasoning%2C%20often%20relying%20on%20shallow%20cues%20such%20as%0Asurface-level%20alignment%2C%20stylistic%20polish%2C%20or%20heuristic%20authenticity%20signals.%0AThese%20results%20highlight%20the%20limitations%20of%20current%20VLMs%20and%20position%0ADeceptionDecoded%20as%20a%20foundation%20for%20developing%20intent-aware%20models%20that%20go%0Abeyond%20shallow%20cues%20in%20MMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15489v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Through%2520Deception%253A%2520Uncovering%2520Misleading%2520Creator%2520Intent%2520in%250A%2520%2520Multimodal%2520News%2520with%2520Vision-Language%2520Models%26entry.906535625%3DJiaying%2520Wu%2520and%2520Fanxiao%2520Li%2520and%2520Zihang%2520Fu%2520and%2520Min-Yen%2520Kan%2520and%2520Bryan%2520Hooi%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520misinformation%2520arises%2520not%2520only%2520from%2520factual%2520inaccuracies%2520but%250Aalso%2520from%2520the%2520misleading%2520narratives%2520that%2520creators%2520deliberately%2520embed.%250AInterpreting%2520such%2520creator%2520intent%2520is%2520therefore%2520essential%2520for%2520multimodal%250Amisinformation%2520detection%2520%2528MMD%2529%2520and%2520effective%2520information%2520governance.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520DeceptionDecoded%252C%2520a%2520large-scale%2520benchmark%2520of%252012%252C000%250Aimage-caption%2520pairs%2520grounded%2520in%2520trustworthy%2520reference%2520articles%252C%2520created%2520using%250Aan%2520intent-guided%2520simulation%2520framework%2520that%2520models%2520both%2520the%2520desired%2520influence%250Aand%2520the%2520execution%2520plan%2520of%2520news%2520creators.%2520The%2520dataset%2520captures%2520both%2520misleading%250Aand%2520non-misleading%2520cases%252C%2520spanning%2520manipulations%2520across%2520visual%2520and%2520textual%250Amodalities%252C%2520and%2520supports%2520three%2520intent-centric%2520tasks%253A%2520%25281%2529%2520misleading%2520intent%250Adetection%252C%2520%25282%2529%2520misleading%2520source%2520attribution%252C%2520and%2520%25283%2529%2520creator%2520desire%2520inference.%250AWe%2520evaluate%252014%2520state-of-the-art%2520vision-language%2520models%2520%2528VLMs%2529%2520and%2520find%2520that%250Athey%2520struggle%2520with%2520intent%2520reasoning%252C%2520often%2520relying%2520on%2520shallow%2520cues%2520such%2520as%250Asurface-level%2520alignment%252C%2520stylistic%2520polish%252C%2520or%2520heuristic%2520authenticity%2520signals.%250AThese%2520results%2520highlight%2520the%2520limitations%2520of%2520current%2520VLMs%2520and%2520position%250ADeceptionDecoded%2520as%2520a%2520foundation%2520for%2520developing%2520intent-aware%2520models%2520that%2520go%250Abeyond%2520shallow%2520cues%2520in%2520MMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15489v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Through%20Deception%3A%20Uncovering%20Misleading%20Creator%20Intent%20in%0A%20%20Multimodal%20News%20with%20Vision-Language%20Models&entry.906535625=Jiaying%20Wu%20and%20Fanxiao%20Li%20and%20Zihang%20Fu%20and%20Min-Yen%20Kan%20and%20Bryan%20Hooi&entry.1292438233=%20%20The%20impact%20of%20misinformation%20arises%20not%20only%20from%20factual%20inaccuracies%20but%0Aalso%20from%20the%20misleading%20narratives%20that%20creators%20deliberately%20embed.%0AInterpreting%20such%20creator%20intent%20is%20therefore%20essential%20for%20multimodal%0Amisinformation%20detection%20%28MMD%29%20and%20effective%20information%20governance.%20To%20this%0Aend%2C%20we%20introduce%20DeceptionDecoded%2C%20a%20large-scale%20benchmark%20of%2012%2C000%0Aimage-caption%20pairs%20grounded%20in%20trustworthy%20reference%20articles%2C%20created%20using%0Aan%20intent-guided%20simulation%20framework%20that%20models%20both%20the%20desired%20influence%0Aand%20the%20execution%20plan%20of%20news%20creators.%20The%20dataset%20captures%20both%20misleading%0Aand%20non-misleading%20cases%2C%20spanning%20manipulations%20across%20visual%20and%20textual%0Amodalities%2C%20and%20supports%20three%20intent-centric%20tasks%3A%20%281%29%20misleading%20intent%0Adetection%2C%20%282%29%20misleading%20source%20attribution%2C%20and%20%283%29%20creator%20desire%20inference.%0AWe%20evaluate%2014%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20and%20find%20that%0Athey%20struggle%20with%20intent%20reasoning%2C%20often%20relying%20on%20shallow%20cues%20such%20as%0Asurface-level%20alignment%2C%20stylistic%20polish%2C%20or%20heuristic%20authenticity%20signals.%0AThese%20results%20highlight%20the%20limitations%20of%20current%20VLMs%20and%20position%0ADeceptionDecoded%20as%20a%20foundation%20for%20developing%20intent-aware%20models%20that%20go%0Abeyond%20shallow%20cues%20in%20MMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15489v3&entry.124074799=Read"},
{"title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in\n  Vehicle Accidents and Beyond", "author": "Shangding Gu and Xiaohan Wang and Donghao Ying and Haoyu Zhao and Runing Yang and Ming Jin and Boyi Li and Marco Pavone and Serena Yeung-Levy and Jun Wang and Dawn Song and Costas Spanos", "abstract": "  Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench\n", "link": "http://arxiv.org/abs/2509.26636v1", "date": "2025-09-30", "relevancy": 2.2008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AccidentBench%3A%20Benchmarking%20Multimodal%20Understanding%20and%20Reasoning%20in%0A%20%20Vehicle%20Accidents%20and%20Beyond&body=Title%3A%20AccidentBench%3A%20Benchmarking%20Multimodal%20Understanding%20and%20Reasoning%20in%0A%20%20Vehicle%20Accidents%20and%20Beyond%0AAuthor%3A%20Shangding%20Gu%20and%20Xiaohan%20Wang%20and%20Donghao%20Ying%20and%20Haoyu%20Zhao%20and%20Runing%20Yang%20and%20Ming%20Jin%20and%20Boyi%20Li%20and%20Marco%20Pavone%20and%20Serena%20Yeung-Levy%20and%20Jun%20Wang%20and%20Dawn%20Song%20and%20Costas%20Spanos%0AAbstract%3A%20%20%20Rapid%20advances%20in%20multimodal%20models%20demand%20benchmarks%20that%20rigorously%0Aevaluate%20understanding%20and%20reasoning%20in%20safety-critical%2C%20dynamic%20real-world%0Asettings.%20We%20present%20AccidentBench%2C%20a%20large-scale%20benchmark%20that%20combines%0Avehicle%20accident%20scenarios%20with%20Beyond%20domains%2C%20safety-critical%20settings%20in%20air%0Aand%20water%20that%20emphasize%20spatial%20and%20temporal%20reasoning%20%28e.g.%2C%20navigation%2C%0Aorientation%2C%20multi-vehicle%20motion%29.%20The%20benchmark%20contains%20approximately%202000%0Avideos%20and%20over%2019000%20human-annotated%20question--answer%20pairs%20spanning%20multiple%0Avideo%20lengths%20%28short/medium/long%29%20and%20difficulty%20levels%20%28easy/medium/hard%29.%0ATasks%20systematically%20probe%20core%20capabilities%3A%20temporal%2C%20spatial%2C%20and%20intent%0Aunderstanding%20and%20reasoning.%20By%20unifying%20accident-centric%20traffic%20scenes%20with%0Abroader%20safety-critical%20scenarios%20in%20air%20and%20water%2C%20AccidentBench%20offers%20a%0Acomprehensive%2C%20physically%20grounded%20testbed%20for%20evaluating%20models%20under%0Areal-world%20variability.%20Evaluations%20of%20state-of-the-art%20models%20%28e.g.%2C%0AGemini-2.5%20Pro%20and%20GPT-5%29%20show%20that%20even%20the%20strongest%20models%20achieve%20only%0Aabout%2018%25%20accuracy%20on%20the%20hardest%20tasks%20and%20longest%20videos%2C%20revealing%0Asubstantial%20gaps%20in%20real-world%20temporal%2C%20spatial%2C%20and%20intent%20reasoning.%0AAccidentBench%20is%20designed%20to%20expose%20these%20critical%20gaps%20and%20drive%20the%0Adevelopment%20of%20multimodal%20models%20that%20are%20safer%2C%20more%20robust%2C%20and%20better%0Aaligned%20with%20real-world%20safety-critical%20challenges.%20The%20code%20and%20dataset%20are%0Aavailable%20at%3A%20https%3A//github.com/SafeRL-Lab/AccidentBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccidentBench%253A%2520Benchmarking%2520Multimodal%2520Understanding%2520and%2520Reasoning%2520in%250A%2520%2520Vehicle%2520Accidents%2520and%2520Beyond%26entry.906535625%3DShangding%2520Gu%2520and%2520Xiaohan%2520Wang%2520and%2520Donghao%2520Ying%2520and%2520Haoyu%2520Zhao%2520and%2520Runing%2520Yang%2520and%2520Ming%2520Jin%2520and%2520Boyi%2520Li%2520and%2520Marco%2520Pavone%2520and%2520Serena%2520Yeung-Levy%2520and%2520Jun%2520Wang%2520and%2520Dawn%2520Song%2520and%2520Costas%2520Spanos%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520multimodal%2520models%2520demand%2520benchmarks%2520that%2520rigorously%250Aevaluate%2520understanding%2520and%2520reasoning%2520in%2520safety-critical%252C%2520dynamic%2520real-world%250Asettings.%2520We%2520present%2520AccidentBench%252C%2520a%2520large-scale%2520benchmark%2520that%2520combines%250Avehicle%2520accident%2520scenarios%2520with%2520Beyond%2520domains%252C%2520safety-critical%2520settings%2520in%2520air%250Aand%2520water%2520that%2520emphasize%2520spatial%2520and%2520temporal%2520reasoning%2520%2528e.g.%252C%2520navigation%252C%250Aorientation%252C%2520multi-vehicle%2520motion%2529.%2520The%2520benchmark%2520contains%2520approximately%25202000%250Avideos%2520and%2520over%252019000%2520human-annotated%2520question--answer%2520pairs%2520spanning%2520multiple%250Avideo%2520lengths%2520%2528short/medium/long%2529%2520and%2520difficulty%2520levels%2520%2528easy/medium/hard%2529.%250ATasks%2520systematically%2520probe%2520core%2520capabilities%253A%2520temporal%252C%2520spatial%252C%2520and%2520intent%250Aunderstanding%2520and%2520reasoning.%2520By%2520unifying%2520accident-centric%2520traffic%2520scenes%2520with%250Abroader%2520safety-critical%2520scenarios%2520in%2520air%2520and%2520water%252C%2520AccidentBench%2520offers%2520a%250Acomprehensive%252C%2520physically%2520grounded%2520testbed%2520for%2520evaluating%2520models%2520under%250Areal-world%2520variability.%2520Evaluations%2520of%2520state-of-the-art%2520models%2520%2528e.g.%252C%250AGemini-2.5%2520Pro%2520and%2520GPT-5%2529%2520show%2520that%2520even%2520the%2520strongest%2520models%2520achieve%2520only%250Aabout%252018%2525%2520accuracy%2520on%2520the%2520hardest%2520tasks%2520and%2520longest%2520videos%252C%2520revealing%250Asubstantial%2520gaps%2520in%2520real-world%2520temporal%252C%2520spatial%252C%2520and%2520intent%2520reasoning.%250AAccidentBench%2520is%2520designed%2520to%2520expose%2520these%2520critical%2520gaps%2520and%2520drive%2520the%250Adevelopment%2520of%2520multimodal%2520models%2520that%2520are%2520safer%252C%2520more%2520robust%252C%2520and%2520better%250Aaligned%2520with%2520real-world%2520safety-critical%2520challenges.%2520The%2520code%2520and%2520dataset%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/SafeRL-Lab/AccidentBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AccidentBench%3A%20Benchmarking%20Multimodal%20Understanding%20and%20Reasoning%20in%0A%20%20Vehicle%20Accidents%20and%20Beyond&entry.906535625=Shangding%20Gu%20and%20Xiaohan%20Wang%20and%20Donghao%20Ying%20and%20Haoyu%20Zhao%20and%20Runing%20Yang%20and%20Ming%20Jin%20and%20Boyi%20Li%20and%20Marco%20Pavone%20and%20Serena%20Yeung-Levy%20and%20Jun%20Wang%20and%20Dawn%20Song%20and%20Costas%20Spanos&entry.1292438233=%20%20Rapid%20advances%20in%20multimodal%20models%20demand%20benchmarks%20that%20rigorously%0Aevaluate%20understanding%20and%20reasoning%20in%20safety-critical%2C%20dynamic%20real-world%0Asettings.%20We%20present%20AccidentBench%2C%20a%20large-scale%20benchmark%20that%20combines%0Avehicle%20accident%20scenarios%20with%20Beyond%20domains%2C%20safety-critical%20settings%20in%20air%0Aand%20water%20that%20emphasize%20spatial%20and%20temporal%20reasoning%20%28e.g.%2C%20navigation%2C%0Aorientation%2C%20multi-vehicle%20motion%29.%20The%20benchmark%20contains%20approximately%202000%0Avideos%20and%20over%2019000%20human-annotated%20question--answer%20pairs%20spanning%20multiple%0Avideo%20lengths%20%28short/medium/long%29%20and%20difficulty%20levels%20%28easy/medium/hard%29.%0ATasks%20systematically%20probe%20core%20capabilities%3A%20temporal%2C%20spatial%2C%20and%20intent%0Aunderstanding%20and%20reasoning.%20By%20unifying%20accident-centric%20traffic%20scenes%20with%0Abroader%20safety-critical%20scenarios%20in%20air%20and%20water%2C%20AccidentBench%20offers%20a%0Acomprehensive%2C%20physically%20grounded%20testbed%20for%20evaluating%20models%20under%0Areal-world%20variability.%20Evaluations%20of%20state-of-the-art%20models%20%28e.g.%2C%0AGemini-2.5%20Pro%20and%20GPT-5%29%20show%20that%20even%20the%20strongest%20models%20achieve%20only%0Aabout%2018%25%20accuracy%20on%20the%20hardest%20tasks%20and%20longest%20videos%2C%20revealing%0Asubstantial%20gaps%20in%20real-world%20temporal%2C%20spatial%2C%20and%20intent%20reasoning.%0AAccidentBench%20is%20designed%20to%20expose%20these%20critical%20gaps%20and%20drive%20the%0Adevelopment%20of%20multimodal%20models%20that%20are%20safer%2C%20more%20robust%2C%20and%20better%0Aaligned%20with%20real-world%20safety-critical%20challenges.%20The%20code%20and%20dataset%20are%0Aavailable%20at%3A%20https%3A//github.com/SafeRL-Lab/AccidentBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26636v1&entry.124074799=Read"},
{"title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise\n  Temporal Distance", "author": "Yuyang Liu and Chuan Wen and Yihang Hu and Dinesh Jayaraman and Yang Gao", "abstract": "  Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.\n", "link": "http://arxiv.org/abs/2509.26627v1", "date": "2025-09-30", "relevancy": 2.1426, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeRewarder%3A%20Learning%20Dense%20Reward%20from%20Passive%20Videos%20via%20Frame-wise%0A%20%20Temporal%20Distance&body=Title%3A%20TimeRewarder%3A%20Learning%20Dense%20Reward%20from%20Passive%20Videos%20via%20Frame-wise%0A%20%20Temporal%20Distance%0AAuthor%3A%20Yuyang%20Liu%20and%20Chuan%20Wen%20and%20Yihang%20Hu%20and%20Dinesh%20Jayaraman%20and%20Yang%20Gao%0AAbstract%3A%20%20%20Designing%20dense%20rewards%20is%20crucial%20for%20reinforcement%20learning%20%28RL%29%2C%20yet%20in%0Arobotics%20it%20often%20demands%20extensive%20manual%20effort%20and%20lacks%20scalability.%20One%0Apromising%20solution%20is%20to%20view%20task%20progress%20as%20a%20dense%20reward%20signal%2C%20as%20it%0Aquantifies%20the%20degree%20to%20which%20actions%20advance%20the%20system%20toward%20task%0Acompletion%20over%20time.%20We%20present%20TimeRewarder%2C%20a%20simple%20yet%20effective%20reward%0Alearning%20method%20that%20derives%20progress%20estimation%20signals%20from%20passive%20videos%2C%0Aincluding%20robot%20demonstrations%20and%20human%20videos%2C%20by%20modeling%20temporal%20distances%0Abetween%20frame%20pairs.%20We%20then%20demonstrate%20how%20TimeRewarder%20can%20supply%20step-wise%0Aproxy%20rewards%20to%20guide%20reinforcement%20learning.%20In%20our%20comprehensive%20experiments%0Aon%20ten%20challenging%20Meta-World%20tasks%2C%20we%20show%20that%20TimeRewarder%20dramatically%0Aimproves%20RL%20for%20sparse-reward%20tasks%2C%20achieving%20nearly%20perfect%20success%20in%209/10%0Atasks%20with%20only%20200%2C000%20interactions%20per%20task%20with%20the%20environment.%20This%0Aapproach%20outperformed%20previous%20methods%20and%20even%20the%20manually%20designed%0Aenvironment%20dense%20reward%20on%20both%20the%20final%20success%20rate%20and%20sample%20efficiency.%0AMoreover%2C%20we%20show%20that%20TimeRewarder%20pretraining%20can%20exploit%20real-world%20human%0Avideos%2C%20highlighting%20its%20potential%20as%20a%20scalable%20approach%20path%20to%20rich%20reward%0Asignals%20from%20diverse%20video%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeRewarder%253A%2520Learning%2520Dense%2520Reward%2520from%2520Passive%2520Videos%2520via%2520Frame-wise%250A%2520%2520Temporal%2520Distance%26entry.906535625%3DYuyang%2520Liu%2520and%2520Chuan%2520Wen%2520and%2520Yihang%2520Hu%2520and%2520Dinesh%2520Jayaraman%2520and%2520Yang%2520Gao%26entry.1292438233%3D%2520%2520Designing%2520dense%2520rewards%2520is%2520crucial%2520for%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520yet%2520in%250Arobotics%2520it%2520often%2520demands%2520extensive%2520manual%2520effort%2520and%2520lacks%2520scalability.%2520One%250Apromising%2520solution%2520is%2520to%2520view%2520task%2520progress%2520as%2520a%2520dense%2520reward%2520signal%252C%2520as%2520it%250Aquantifies%2520the%2520degree%2520to%2520which%2520actions%2520advance%2520the%2520system%2520toward%2520task%250Acompletion%2520over%2520time.%2520We%2520present%2520TimeRewarder%252C%2520a%2520simple%2520yet%2520effective%2520reward%250Alearning%2520method%2520that%2520derives%2520progress%2520estimation%2520signals%2520from%2520passive%2520videos%252C%250Aincluding%2520robot%2520demonstrations%2520and%2520human%2520videos%252C%2520by%2520modeling%2520temporal%2520distances%250Abetween%2520frame%2520pairs.%2520We%2520then%2520demonstrate%2520how%2520TimeRewarder%2520can%2520supply%2520step-wise%250Aproxy%2520rewards%2520to%2520guide%2520reinforcement%2520learning.%2520In%2520our%2520comprehensive%2520experiments%250Aon%2520ten%2520challenging%2520Meta-World%2520tasks%252C%2520we%2520show%2520that%2520TimeRewarder%2520dramatically%250Aimproves%2520RL%2520for%2520sparse-reward%2520tasks%252C%2520achieving%2520nearly%2520perfect%2520success%2520in%25209/10%250Atasks%2520with%2520only%2520200%252C000%2520interactions%2520per%2520task%2520with%2520the%2520environment.%2520This%250Aapproach%2520outperformed%2520previous%2520methods%2520and%2520even%2520the%2520manually%2520designed%250Aenvironment%2520dense%2520reward%2520on%2520both%2520the%2520final%2520success%2520rate%2520and%2520sample%2520efficiency.%250AMoreover%252C%2520we%2520show%2520that%2520TimeRewarder%2520pretraining%2520can%2520exploit%2520real-world%2520human%250Avideos%252C%2520highlighting%2520its%2520potential%2520as%2520a%2520scalable%2520approach%2520path%2520to%2520rich%2520reward%250Asignals%2520from%2520diverse%2520video%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeRewarder%3A%20Learning%20Dense%20Reward%20from%20Passive%20Videos%20via%20Frame-wise%0A%20%20Temporal%20Distance&entry.906535625=Yuyang%20Liu%20and%20Chuan%20Wen%20and%20Yihang%20Hu%20and%20Dinesh%20Jayaraman%20and%20Yang%20Gao&entry.1292438233=%20%20Designing%20dense%20rewards%20is%20crucial%20for%20reinforcement%20learning%20%28RL%29%2C%20yet%20in%0Arobotics%20it%20often%20demands%20extensive%20manual%20effort%20and%20lacks%20scalability.%20One%0Apromising%20solution%20is%20to%20view%20task%20progress%20as%20a%20dense%20reward%20signal%2C%20as%20it%0Aquantifies%20the%20degree%20to%20which%20actions%20advance%20the%20system%20toward%20task%0Acompletion%20over%20time.%20We%20present%20TimeRewarder%2C%20a%20simple%20yet%20effective%20reward%0Alearning%20method%20that%20derives%20progress%20estimation%20signals%20from%20passive%20videos%2C%0Aincluding%20robot%20demonstrations%20and%20human%20videos%2C%20by%20modeling%20temporal%20distances%0Abetween%20frame%20pairs.%20We%20then%20demonstrate%20how%20TimeRewarder%20can%20supply%20step-wise%0Aproxy%20rewards%20to%20guide%20reinforcement%20learning.%20In%20our%20comprehensive%20experiments%0Aon%20ten%20challenging%20Meta-World%20tasks%2C%20we%20show%20that%20TimeRewarder%20dramatically%0Aimproves%20RL%20for%20sparse-reward%20tasks%2C%20achieving%20nearly%20perfect%20success%20in%209/10%0Atasks%20with%20only%20200%2C000%20interactions%20per%20task%20with%20the%20environment.%20This%0Aapproach%20outperformed%20previous%20methods%20and%20even%20the%20manually%20designed%0Aenvironment%20dense%20reward%20on%20both%20the%20final%20success%20rate%20and%20sample%20efficiency.%0AMoreover%2C%20we%20show%20that%20TimeRewarder%20pretraining%20can%20exploit%20real-world%20human%0Avideos%2C%20highlighting%20its%20potential%20as%20a%20scalable%20approach%20path%20to%20rich%20reward%0Asignals%20from%20diverse%20video%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26627v1&entry.124074799=Read"},
{"title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework", "author": "Shishir Gopinath and Karthik Dantu and Steven Y. Ko", "abstract": "  We present Graphite, a GPU-accelerated nonlinear graph optimization\nframework. It provides a CUDA C++ interface to enable the sharing of code\nbetween a realtime application, such as a SLAM system, and its optimization\ntasks. The framework supports techniques to reduce memory usage, including\nin-place optimization, support for multiple floating point types and\nmixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite\non well-known bundle adjustment problems and find that it achieves similar\nperformance to MegBA, a solver specialized for bundle adjustment, while\nmaintaining generality and using less memory. We also apply Graphite to global\nvisual-inertial bundle adjustment on maps generated from stereo-inertial SLAM\ndatasets, and observe speed ups of up to 59x compared to a CPU baseline. Our\nresults indicate that our solver enables faster large-scale optimization on\nboth desktop and resource-constrained devices.\n", "link": "http://arxiv.org/abs/2509.26581v1", "date": "2025-09-30", "relevancy": 2.1419, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5564}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5421}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graphite%3A%20A%20GPU-Accelerated%20Mixed-Precision%20Graph%20Optimization%20Framework&body=Title%3A%20Graphite%3A%20A%20GPU-Accelerated%20Mixed-Precision%20Graph%20Optimization%20Framework%0AAuthor%3A%20Shishir%20Gopinath%20and%20Karthik%20Dantu%20and%20Steven%20Y.%20Ko%0AAbstract%3A%20%20%20We%20present%20Graphite%2C%20a%20GPU-accelerated%20nonlinear%20graph%20optimization%0Aframework.%20It%20provides%20a%20CUDA%20C%2B%2B%20interface%20to%20enable%20the%20sharing%20of%20code%0Abetween%20a%20realtime%20application%2C%20such%20as%20a%20SLAM%20system%2C%20and%20its%20optimization%0Atasks.%20The%20framework%20supports%20techniques%20to%20reduce%20memory%20usage%2C%20including%0Ain-place%20optimization%2C%20support%20for%20multiple%20floating%20point%20types%20and%0Amixed-precision%20modes%2C%20and%20dynamically%20computed%20Jacobians.%20We%20evaluate%20Graphite%0Aon%20well-known%20bundle%20adjustment%20problems%20and%20find%20that%20it%20achieves%20similar%0Aperformance%20to%20MegBA%2C%20a%20solver%20specialized%20for%20bundle%20adjustment%2C%20while%0Amaintaining%20generality%20and%20using%20less%20memory.%20We%20also%20apply%20Graphite%20to%20global%0Avisual-inertial%20bundle%20adjustment%20on%20maps%20generated%20from%20stereo-inertial%20SLAM%0Adatasets%2C%20and%20observe%20speed%20ups%20of%20up%20to%2059x%20compared%20to%20a%20CPU%20baseline.%20Our%0Aresults%20indicate%20that%20our%20solver%20enables%20faster%20large-scale%20optimization%20on%0Aboth%20desktop%20and%20resource-constrained%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphite%253A%2520A%2520GPU-Accelerated%2520Mixed-Precision%2520Graph%2520Optimization%2520Framework%26entry.906535625%3DShishir%2520Gopinath%2520and%2520Karthik%2520Dantu%2520and%2520Steven%2520Y.%2520Ko%26entry.1292438233%3D%2520%2520We%2520present%2520Graphite%252C%2520a%2520GPU-accelerated%2520nonlinear%2520graph%2520optimization%250Aframework.%2520It%2520provides%2520a%2520CUDA%2520C%252B%252B%2520interface%2520to%2520enable%2520the%2520sharing%2520of%2520code%250Abetween%2520a%2520realtime%2520application%252C%2520such%2520as%2520a%2520SLAM%2520system%252C%2520and%2520its%2520optimization%250Atasks.%2520The%2520framework%2520supports%2520techniques%2520to%2520reduce%2520memory%2520usage%252C%2520including%250Ain-place%2520optimization%252C%2520support%2520for%2520multiple%2520floating%2520point%2520types%2520and%250Amixed-precision%2520modes%252C%2520and%2520dynamically%2520computed%2520Jacobians.%2520We%2520evaluate%2520Graphite%250Aon%2520well-known%2520bundle%2520adjustment%2520problems%2520and%2520find%2520that%2520it%2520achieves%2520similar%250Aperformance%2520to%2520MegBA%252C%2520a%2520solver%2520specialized%2520for%2520bundle%2520adjustment%252C%2520while%250Amaintaining%2520generality%2520and%2520using%2520less%2520memory.%2520We%2520also%2520apply%2520Graphite%2520to%2520global%250Avisual-inertial%2520bundle%2520adjustment%2520on%2520maps%2520generated%2520from%2520stereo-inertial%2520SLAM%250Adatasets%252C%2520and%2520observe%2520speed%2520ups%2520of%2520up%2520to%252059x%2520compared%2520to%2520a%2520CPU%2520baseline.%2520Our%250Aresults%2520indicate%2520that%2520our%2520solver%2520enables%2520faster%2520large-scale%2520optimization%2520on%250Aboth%2520desktop%2520and%2520resource-constrained%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graphite%3A%20A%20GPU-Accelerated%20Mixed-Precision%20Graph%20Optimization%20Framework&entry.906535625=Shishir%20Gopinath%20and%20Karthik%20Dantu%20and%20Steven%20Y.%20Ko&entry.1292438233=%20%20We%20present%20Graphite%2C%20a%20GPU-accelerated%20nonlinear%20graph%20optimization%0Aframework.%20It%20provides%20a%20CUDA%20C%2B%2B%20interface%20to%20enable%20the%20sharing%20of%20code%0Abetween%20a%20realtime%20application%2C%20such%20as%20a%20SLAM%20system%2C%20and%20its%20optimization%0Atasks.%20The%20framework%20supports%20techniques%20to%20reduce%20memory%20usage%2C%20including%0Ain-place%20optimization%2C%20support%20for%20multiple%20floating%20point%20types%20and%0Amixed-precision%20modes%2C%20and%20dynamically%20computed%20Jacobians.%20We%20evaluate%20Graphite%0Aon%20well-known%20bundle%20adjustment%20problems%20and%20find%20that%20it%20achieves%20similar%0Aperformance%20to%20MegBA%2C%20a%20solver%20specialized%20for%20bundle%20adjustment%2C%20while%0Amaintaining%20generality%20and%20using%20less%20memory.%20We%20also%20apply%20Graphite%20to%20global%0Avisual-inertial%20bundle%20adjustment%20on%20maps%20generated%20from%20stereo-inertial%20SLAM%0Adatasets%2C%20and%20observe%20speed%20ups%20of%20up%20to%2059x%20compared%20to%20a%20CPU%20baseline.%20Our%0Aresults%20indicate%20that%20our%20solver%20enables%20faster%20large-scale%20optimization%20on%0Aboth%20desktop%20and%20resource-constrained%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26581v1&entry.124074799=Read"},
{"title": "Source Separation for A Cappella Music", "author": "Luca A. Lanzend\u00f6rfer and Constantin Pinkl and Florian Gr\u00f6tschla", "abstract": "  In this work, we study the task of multi-singer separation in a cappella\nmusic, where the number of active singers varies across mixtures. To address\nthis, we use a power set-based data augmentation strategy that expands limited\nmulti-singer datasets into exponentially more training samples. To separate\nsingers, we introduce SepACap, an adaptation of SepReformer, a state-of-the-art\nspeaker separation model architecture. We adapt the model with periodic\nactivations and a composite loss function that remains effective when stems are\nsilent, enabling robust detection and separation. Experiments on the JaCappella\ndataset demonstrate that our approach achieves state-of-the-art performance in\nboth full-ensemble and subset singer separation scenarios, outperforming\nspectrogram-based baselines while generalizing to realistic mixtures with\nvarying numbers of singers.\n", "link": "http://arxiv.org/abs/2509.26580v1", "date": "2025-09-30", "relevancy": 2.1278, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4377}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4231}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source%20Separation%20for%20A%20Cappella%20Music&body=Title%3A%20Source%20Separation%20for%20A%20Cappella%20Music%0AAuthor%3A%20Luca%20A.%20Lanzend%C3%B6rfer%20and%20Constantin%20Pinkl%20and%20Florian%20Gr%C3%B6tschla%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20task%20of%20multi-singer%20separation%20in%20a%20cappella%0Amusic%2C%20where%20the%20number%20of%20active%20singers%20varies%20across%20mixtures.%20To%20address%0Athis%2C%20we%20use%20a%20power%20set-based%20data%20augmentation%20strategy%20that%20expands%20limited%0Amulti-singer%20datasets%20into%20exponentially%20more%20training%20samples.%20To%20separate%0Asingers%2C%20we%20introduce%20SepACap%2C%20an%20adaptation%20of%20SepReformer%2C%20a%20state-of-the-art%0Aspeaker%20separation%20model%20architecture.%20We%20adapt%20the%20model%20with%20periodic%0Aactivations%20and%20a%20composite%20loss%20function%20that%20remains%20effective%20when%20stems%20are%0Asilent%2C%20enabling%20robust%20detection%20and%20separation.%20Experiments%20on%20the%20JaCappella%0Adataset%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%0Aboth%20full-ensemble%20and%20subset%20singer%20separation%20scenarios%2C%20outperforming%0Aspectrogram-based%20baselines%20while%20generalizing%20to%20realistic%20mixtures%20with%0Avarying%20numbers%20of%20singers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource%2520Separation%2520for%2520A%2520Cappella%2520Music%26entry.906535625%3DLuca%2520A.%2520Lanzend%25C3%25B6rfer%2520and%2520Constantin%2520Pinkl%2520and%2520Florian%2520Gr%25C3%25B6tschla%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520task%2520of%2520multi-singer%2520separation%2520in%2520a%2520cappella%250Amusic%252C%2520where%2520the%2520number%2520of%2520active%2520singers%2520varies%2520across%2520mixtures.%2520To%2520address%250Athis%252C%2520we%2520use%2520a%2520power%2520set-based%2520data%2520augmentation%2520strategy%2520that%2520expands%2520limited%250Amulti-singer%2520datasets%2520into%2520exponentially%2520more%2520training%2520samples.%2520To%2520separate%250Asingers%252C%2520we%2520introduce%2520SepACap%252C%2520an%2520adaptation%2520of%2520SepReformer%252C%2520a%2520state-of-the-art%250Aspeaker%2520separation%2520model%2520architecture.%2520We%2520adapt%2520the%2520model%2520with%2520periodic%250Aactivations%2520and%2520a%2520composite%2520loss%2520function%2520that%2520remains%2520effective%2520when%2520stems%2520are%250Asilent%252C%2520enabling%2520robust%2520detection%2520and%2520separation.%2520Experiments%2520on%2520the%2520JaCappella%250Adataset%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520in%250Aboth%2520full-ensemble%2520and%2520subset%2520singer%2520separation%2520scenarios%252C%2520outperforming%250Aspectrogram-based%2520baselines%2520while%2520generalizing%2520to%2520realistic%2520mixtures%2520with%250Avarying%2520numbers%2520of%2520singers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source%20Separation%20for%20A%20Cappella%20Music&entry.906535625=Luca%20A.%20Lanzend%C3%B6rfer%20and%20Constantin%20Pinkl%20and%20Florian%20Gr%C3%B6tschla&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20task%20of%20multi-singer%20separation%20in%20a%20cappella%0Amusic%2C%20where%20the%20number%20of%20active%20singers%20varies%20across%20mixtures.%20To%20address%0Athis%2C%20we%20use%20a%20power%20set-based%20data%20augmentation%20strategy%20that%20expands%20limited%0Amulti-singer%20datasets%20into%20exponentially%20more%20training%20samples.%20To%20separate%0Asingers%2C%20we%20introduce%20SepACap%2C%20an%20adaptation%20of%20SepReformer%2C%20a%20state-of-the-art%0Aspeaker%20separation%20model%20architecture.%20We%20adapt%20the%20model%20with%20periodic%0Aactivations%20and%20a%20composite%20loss%20function%20that%20remains%20effective%20when%20stems%20are%0Asilent%2C%20enabling%20robust%20detection%20and%20separation.%20Experiments%20on%20the%20JaCappella%0Adataset%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20in%0Aboth%20full-ensemble%20and%20subset%20singer%20separation%20scenarios%2C%20outperforming%0Aspectrogram-based%20baselines%20while%20generalizing%20to%20realistic%20mixtures%20with%0Avarying%20numbers%20of%20singers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26580v1&entry.124074799=Read"},
{"title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms", "author": "Shuoyu Yue and Pengpeng Li and Yang Xu and Kunrui Ze and Xingjian Long and Huazi Cao and Guibin Sun", "abstract": "  Mean-shift-based approaches have recently emerged as the most effective\nmethods for robot swarm shape assembly tasks. These methods rely on image-based\nrepresentations of target shapes to compute local density gradients and perform\nmean-shift exploration, which constitute their core mechanism. However, such\nimage representations incur substantial memory overhead, which can become\nprohibitive for high-resolution or 3D shapes. To overcome this limitation, we\npropose a memory-efficient tree map representation that hierarchically encodes\nuser-specified shapes and is applicable to both 2D and 3D scenarios. Building\non this representation, we design a behavior-based distributed controller that\nenables assignment-free shape assembly. Comparative 2D and 3D simulations\nagainst a state-of-the-art mean-shift algorithm demonstrate one to two orders\nof magnitude lower memory usage and two to three times faster shape entry while\nmaintaining comparable uniformity. Finally, we validate the framework through\nphysical experiments with 6 to 7 UAVs, confirming its real-world practicality.\n", "link": "http://arxiv.org/abs/2509.26518v1", "date": "2025-09-30", "relevancy": 2.1137, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5262}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%202D/3D%20Shape%20Assembly%20of%20Robot%20Swarms&body=Title%3A%20Memory-Efficient%202D/3D%20Shape%20Assembly%20of%20Robot%20Swarms%0AAuthor%3A%20Shuoyu%20Yue%20and%20Pengpeng%20Li%20and%20Yang%20Xu%20and%20Kunrui%20Ze%20and%20Xingjian%20Long%20and%20Huazi%20Cao%20and%20Guibin%20Sun%0AAbstract%3A%20%20%20Mean-shift-based%20approaches%20have%20recently%20emerged%20as%20the%20most%20effective%0Amethods%20for%20robot%20swarm%20shape%20assembly%20tasks.%20These%20methods%20rely%20on%20image-based%0Arepresentations%20of%20target%20shapes%20to%20compute%20local%20density%20gradients%20and%20perform%0Amean-shift%20exploration%2C%20which%20constitute%20their%20core%20mechanism.%20However%2C%20such%0Aimage%20representations%20incur%20substantial%20memory%20overhead%2C%20which%20can%20become%0Aprohibitive%20for%20high-resolution%20or%203D%20shapes.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20memory-efficient%20tree%20map%20representation%20that%20hierarchically%20encodes%0Auser-specified%20shapes%20and%20is%20applicable%20to%20both%202D%20and%203D%20scenarios.%20Building%0Aon%20this%20representation%2C%20we%20design%20a%20behavior-based%20distributed%20controller%20that%0Aenables%20assignment-free%20shape%20assembly.%20Comparative%202D%20and%203D%20simulations%0Aagainst%20a%20state-of-the-art%20mean-shift%20algorithm%20demonstrate%20one%20to%20two%20orders%0Aof%20magnitude%20lower%20memory%20usage%20and%20two%20to%20three%20times%20faster%20shape%20entry%20while%0Amaintaining%20comparable%20uniformity.%20Finally%2C%20we%20validate%20the%20framework%20through%0Aphysical%20experiments%20with%206%20to%207%20UAVs%2C%20confirming%20its%20real-world%20practicality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%25202D/3D%2520Shape%2520Assembly%2520of%2520Robot%2520Swarms%26entry.906535625%3DShuoyu%2520Yue%2520and%2520Pengpeng%2520Li%2520and%2520Yang%2520Xu%2520and%2520Kunrui%2520Ze%2520and%2520Xingjian%2520Long%2520and%2520Huazi%2520Cao%2520and%2520Guibin%2520Sun%26entry.1292438233%3D%2520%2520Mean-shift-based%2520approaches%2520have%2520recently%2520emerged%2520as%2520the%2520most%2520effective%250Amethods%2520for%2520robot%2520swarm%2520shape%2520assembly%2520tasks.%2520These%2520methods%2520rely%2520on%2520image-based%250Arepresentations%2520of%2520target%2520shapes%2520to%2520compute%2520local%2520density%2520gradients%2520and%2520perform%250Amean-shift%2520exploration%252C%2520which%2520constitute%2520their%2520core%2520mechanism.%2520However%252C%2520such%250Aimage%2520representations%2520incur%2520substantial%2520memory%2520overhead%252C%2520which%2520can%2520become%250Aprohibitive%2520for%2520high-resolution%2520or%25203D%2520shapes.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520a%2520memory-efficient%2520tree%2520map%2520representation%2520that%2520hierarchically%2520encodes%250Auser-specified%2520shapes%2520and%2520is%2520applicable%2520to%2520both%25202D%2520and%25203D%2520scenarios.%2520Building%250Aon%2520this%2520representation%252C%2520we%2520design%2520a%2520behavior-based%2520distributed%2520controller%2520that%250Aenables%2520assignment-free%2520shape%2520assembly.%2520Comparative%25202D%2520and%25203D%2520simulations%250Aagainst%2520a%2520state-of-the-art%2520mean-shift%2520algorithm%2520demonstrate%2520one%2520to%2520two%2520orders%250Aof%2520magnitude%2520lower%2520memory%2520usage%2520and%2520two%2520to%2520three%2520times%2520faster%2520shape%2520entry%2520while%250Amaintaining%2520comparable%2520uniformity.%2520Finally%252C%2520we%2520validate%2520the%2520framework%2520through%250Aphysical%2520experiments%2520with%25206%2520to%25207%2520UAVs%252C%2520confirming%2520its%2520real-world%2520practicality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%202D/3D%20Shape%20Assembly%20of%20Robot%20Swarms&entry.906535625=Shuoyu%20Yue%20and%20Pengpeng%20Li%20and%20Yang%20Xu%20and%20Kunrui%20Ze%20and%20Xingjian%20Long%20and%20Huazi%20Cao%20and%20Guibin%20Sun&entry.1292438233=%20%20Mean-shift-based%20approaches%20have%20recently%20emerged%20as%20the%20most%20effective%0Amethods%20for%20robot%20swarm%20shape%20assembly%20tasks.%20These%20methods%20rely%20on%20image-based%0Arepresentations%20of%20target%20shapes%20to%20compute%20local%20density%20gradients%20and%20perform%0Amean-shift%20exploration%2C%20which%20constitute%20their%20core%20mechanism.%20However%2C%20such%0Aimage%20representations%20incur%20substantial%20memory%20overhead%2C%20which%20can%20become%0Aprohibitive%20for%20high-resolution%20or%203D%20shapes.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20a%20memory-efficient%20tree%20map%20representation%20that%20hierarchically%20encodes%0Auser-specified%20shapes%20and%20is%20applicable%20to%20both%202D%20and%203D%20scenarios.%20Building%0Aon%20this%20representation%2C%20we%20design%20a%20behavior-based%20distributed%20controller%20that%0Aenables%20assignment-free%20shape%20assembly.%20Comparative%202D%20and%203D%20simulations%0Aagainst%20a%20state-of-the-art%20mean-shift%20algorithm%20demonstrate%20one%20to%20two%20orders%0Aof%20magnitude%20lower%20memory%20usage%20and%20two%20to%20three%20times%20faster%20shape%20entry%20while%0Amaintaining%20comparable%20uniformity.%20Finally%2C%20we%20validate%20the%20framework%20through%0Aphysical%20experiments%20with%206%20to%207%20UAVs%2C%20confirming%20its%20real-world%20practicality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26518v1&entry.124074799=Read"},
{"title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain", "author": "Adrian Kosowski and Przemys\u0142aw Uzna\u0144ski and Jan Chorowski and Zuzanna Stamirowska and Micha\u0142 Bartoszkiewicz", "abstract": "  The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\$n\\$\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.\n", "link": "http://arxiv.org/abs/2509.26507v1", "date": "2025-09-30", "relevancy": 2.1116, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dragon%20Hatchling%3A%20The%20Missing%20Link%20between%20the%20Transformer%20and%0A%20%20Models%20of%20the%20Brain&body=Title%3A%20The%20Dragon%20Hatchling%3A%20The%20Missing%20Link%20between%20the%20Transformer%20and%0A%20%20Models%20of%20the%20Brain%0AAuthor%3A%20Adrian%20Kosowski%20and%20Przemys%C5%82aw%20Uzna%C5%84ski%20and%20Jan%20Chorowski%20and%20Zuzanna%20Stamirowska%20and%20Micha%C5%82%20Bartoszkiewicz%0AAbstract%3A%20%20%20The%20relationship%20between%20computing%20systems%20and%20the%20brain%20has%20served%20as%0Amotivation%20for%20pioneering%20theoreticians%20since%20John%20von%20Neumann%20and%20Alan%20Turing.%0AUniform%2C%20scale-free%20biological%20networks%2C%20such%20as%20the%20brain%2C%20have%20powerful%0Aproperties%2C%20including%20generalizing%20over%20time%2C%20which%20is%20the%20main%20barrier%20for%0AMachine%20Learning%20on%20the%20path%20to%20Universal%20Reasoning%20Models.%0A%20%20We%20introduce%20%60Dragon%20Hatchling%27%20%28BDH%29%2C%20a%20new%20Large%20Language%20Model%0Aarchitecture%20based%20on%20a%20scale-free%20biologically%20inspired%20network%20of%20%5C%24n%5C%24%0Alocally-interacting%20neuron%20particles.%20BDH%20couples%20strong%20theoretical%0Afoundations%20and%20inherent%20interpretability%20without%20sacrificing%20Transformer-like%0Aperformance.%0A%20%20BDH%20is%20a%20practical%2C%20performant%20state-of-the-art%20attention-based%20state%20space%0Asequence%20learning%20architecture.%20In%20addition%20to%20being%20a%20graph%20model%2C%20BDH%20admits%0Aa%20GPU-friendly%20formulation.%20It%20exhibits%20Transformer-like%20scaling%20laws%3A%0Aempirically%20BDH%20rivals%20GPT2%20performance%20on%20language%20and%20translation%20tasks%2C%20at%0Athe%20same%20number%20of%20parameters%20%2810M%20to%201B%29%2C%20for%20the%20same%20training%20data.%0A%20%20BDH%20can%20be%20represented%20as%20a%20brain%20model.%20The%20working%20memory%20of%20BDH%20during%0Ainference%20entirely%20relies%20on%20synaptic%20plasticity%20with%20Hebbian%20learning%20using%0Aspiking%20neurons.%20We%20confirm%20empirically%20that%20specific%2C%20individual%20synapses%0Astrengthen%20connection%20whenever%20BDH%20hears%20or%20reasons%20about%20a%20specific%20concept%0Awhile%20processing%20language%20inputs.%20The%20neuron%20interaction%20network%20of%20BDH%20is%20a%0Agraph%20of%20high%20modularity%20with%20heavy-tailed%20degree%20distribution.%20The%20BDH%20model%0Ais%20biologically%20plausible%2C%20explaining%20one%20possible%20mechanism%20which%20human%0Aneurons%20could%20use%20to%20achieve%20speech.%0A%20%20BDH%20is%20designed%20for%20interpretability.%20Activation%20vectors%20of%20BDH%20are%20sparse%0Aand%20positive.%20We%20demonstrate%20monosemanticity%20in%20BDH%20on%20language%20tasks.%0AInterpretability%20of%20state%2C%20which%20goes%20beyond%20interpretability%20of%20neurons%20and%0Amodel%20parameters%2C%20is%20an%20inherent%20feature%20of%20the%20BDH%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dragon%2520Hatchling%253A%2520The%2520Missing%2520Link%2520between%2520the%2520Transformer%2520and%250A%2520%2520Models%2520of%2520the%2520Brain%26entry.906535625%3DAdrian%2520Kosowski%2520and%2520Przemys%25C5%2582aw%2520Uzna%25C5%2584ski%2520and%2520Jan%2520Chorowski%2520and%2520Zuzanna%2520Stamirowska%2520and%2520Micha%25C5%2582%2520Bartoszkiewicz%26entry.1292438233%3D%2520%2520The%2520relationship%2520between%2520computing%2520systems%2520and%2520the%2520brain%2520has%2520served%2520as%250Amotivation%2520for%2520pioneering%2520theoreticians%2520since%2520John%2520von%2520Neumann%2520and%2520Alan%2520Turing.%250AUniform%252C%2520scale-free%2520biological%2520networks%252C%2520such%2520as%2520the%2520brain%252C%2520have%2520powerful%250Aproperties%252C%2520including%2520generalizing%2520over%2520time%252C%2520which%2520is%2520the%2520main%2520barrier%2520for%250AMachine%2520Learning%2520on%2520the%2520path%2520to%2520Universal%2520Reasoning%2520Models.%250A%2520%2520We%2520introduce%2520%2560Dragon%2520Hatchling%2527%2520%2528BDH%2529%252C%2520a%2520new%2520Large%2520Language%2520Model%250Aarchitecture%2520based%2520on%2520a%2520scale-free%2520biologically%2520inspired%2520network%2520of%2520%255C%2524n%255C%2524%250Alocally-interacting%2520neuron%2520particles.%2520BDH%2520couples%2520strong%2520theoretical%250Afoundations%2520and%2520inherent%2520interpretability%2520without%2520sacrificing%2520Transformer-like%250Aperformance.%250A%2520%2520BDH%2520is%2520a%2520practical%252C%2520performant%2520state-of-the-art%2520attention-based%2520state%2520space%250Asequence%2520learning%2520architecture.%2520In%2520addition%2520to%2520being%2520a%2520graph%2520model%252C%2520BDH%2520admits%250Aa%2520GPU-friendly%2520formulation.%2520It%2520exhibits%2520Transformer-like%2520scaling%2520laws%253A%250Aempirically%2520BDH%2520rivals%2520GPT2%2520performance%2520on%2520language%2520and%2520translation%2520tasks%252C%2520at%250Athe%2520same%2520number%2520of%2520parameters%2520%252810M%2520to%25201B%2529%252C%2520for%2520the%2520same%2520training%2520data.%250A%2520%2520BDH%2520can%2520be%2520represented%2520as%2520a%2520brain%2520model.%2520The%2520working%2520memory%2520of%2520BDH%2520during%250Ainference%2520entirely%2520relies%2520on%2520synaptic%2520plasticity%2520with%2520Hebbian%2520learning%2520using%250Aspiking%2520neurons.%2520We%2520confirm%2520empirically%2520that%2520specific%252C%2520individual%2520synapses%250Astrengthen%2520connection%2520whenever%2520BDH%2520hears%2520or%2520reasons%2520about%2520a%2520specific%2520concept%250Awhile%2520processing%2520language%2520inputs.%2520The%2520neuron%2520interaction%2520network%2520of%2520BDH%2520is%2520a%250Agraph%2520of%2520high%2520modularity%2520with%2520heavy-tailed%2520degree%2520distribution.%2520The%2520BDH%2520model%250Ais%2520biologically%2520plausible%252C%2520explaining%2520one%2520possible%2520mechanism%2520which%2520human%250Aneurons%2520could%2520use%2520to%2520achieve%2520speech.%250A%2520%2520BDH%2520is%2520designed%2520for%2520interpretability.%2520Activation%2520vectors%2520of%2520BDH%2520are%2520sparse%250Aand%2520positive.%2520We%2520demonstrate%2520monosemanticity%2520in%2520BDH%2520on%2520language%2520tasks.%250AInterpretability%2520of%2520state%252C%2520which%2520goes%2520beyond%2520interpretability%2520of%2520neurons%2520and%250Amodel%2520parameters%252C%2520is%2520an%2520inherent%2520feature%2520of%2520the%2520BDH%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dragon%20Hatchling%3A%20The%20Missing%20Link%20between%20the%20Transformer%20and%0A%20%20Models%20of%20the%20Brain&entry.906535625=Adrian%20Kosowski%20and%20Przemys%C5%82aw%20Uzna%C5%84ski%20and%20Jan%20Chorowski%20and%20Zuzanna%20Stamirowska%20and%20Micha%C5%82%20Bartoszkiewicz&entry.1292438233=%20%20The%20relationship%20between%20computing%20systems%20and%20the%20brain%20has%20served%20as%0Amotivation%20for%20pioneering%20theoreticians%20since%20John%20von%20Neumann%20and%20Alan%20Turing.%0AUniform%2C%20scale-free%20biological%20networks%2C%20such%20as%20the%20brain%2C%20have%20powerful%0Aproperties%2C%20including%20generalizing%20over%20time%2C%20which%20is%20the%20main%20barrier%20for%0AMachine%20Learning%20on%20the%20path%20to%20Universal%20Reasoning%20Models.%0A%20%20We%20introduce%20%60Dragon%20Hatchling%27%20%28BDH%29%2C%20a%20new%20Large%20Language%20Model%0Aarchitecture%20based%20on%20a%20scale-free%20biologically%20inspired%20network%20of%20%5C%24n%5C%24%0Alocally-interacting%20neuron%20particles.%20BDH%20couples%20strong%20theoretical%0Afoundations%20and%20inherent%20interpretability%20without%20sacrificing%20Transformer-like%0Aperformance.%0A%20%20BDH%20is%20a%20practical%2C%20performant%20state-of-the-art%20attention-based%20state%20space%0Asequence%20learning%20architecture.%20In%20addition%20to%20being%20a%20graph%20model%2C%20BDH%20admits%0Aa%20GPU-friendly%20formulation.%20It%20exhibits%20Transformer-like%20scaling%20laws%3A%0Aempirically%20BDH%20rivals%20GPT2%20performance%20on%20language%20and%20translation%20tasks%2C%20at%0Athe%20same%20number%20of%20parameters%20%2810M%20to%201B%29%2C%20for%20the%20same%20training%20data.%0A%20%20BDH%20can%20be%20represented%20as%20a%20brain%20model.%20The%20working%20memory%20of%20BDH%20during%0Ainference%20entirely%20relies%20on%20synaptic%20plasticity%20with%20Hebbian%20learning%20using%0Aspiking%20neurons.%20We%20confirm%20empirically%20that%20specific%2C%20individual%20synapses%0Astrengthen%20connection%20whenever%20BDH%20hears%20or%20reasons%20about%20a%20specific%20concept%0Awhile%20processing%20language%20inputs.%20The%20neuron%20interaction%20network%20of%20BDH%20is%20a%0Agraph%20of%20high%20modularity%20with%20heavy-tailed%20degree%20distribution.%20The%20BDH%20model%0Ais%20biologically%20plausible%2C%20explaining%20one%20possible%20mechanism%20which%20human%0Aneurons%20could%20use%20to%20achieve%20speech.%0A%20%20BDH%20is%20designed%20for%20interpretability.%20Activation%20vectors%20of%20BDH%20are%20sparse%0Aand%20positive.%20We%20demonstrate%20monosemanticity%20in%20BDH%20on%20language%20tasks.%0AInterpretability%20of%20state%2C%20which%20goes%20beyond%20interpretability%20of%20neurons%20and%0Amodel%20parameters%2C%20is%20an%20inherent%20feature%20of%20the%20BDH%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26507v1&entry.124074799=Read"},
{"title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models", "author": "Siddarth Venkatraman and Vineet Jain and Sarthak Mittal and Vedant Shah and Johan Obando-Ceron and Yoshua Bengio and Brian R. Bartoldson and Bhavya Kailkhura and Guillaume Lajoie and Glen Berseth and Nikolay Malkin and Moksh Jain", "abstract": "  Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.\n", "link": "http://arxiv.org/abs/2509.26626v1", "date": "2025-09-30", "relevancy": 2.0928, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.524}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Self-Aggregation%20Unlocks%20Deep%20Thinking%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Recursive%20Self-Aggregation%20Unlocks%20Deep%20Thinking%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Siddarth%20Venkatraman%20and%20Vineet%20Jain%20and%20Sarthak%20Mittal%20and%20Vedant%20Shah%20and%20Johan%20Obando-Ceron%20and%20Yoshua%20Bengio%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Guillaume%20Lajoie%20and%20Glen%20Berseth%20and%20Nikolay%20Malkin%20and%20Moksh%20Jain%0AAbstract%3A%20%20%20Test-time%20scaling%20methods%20improve%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20by%20increasing%20the%20amount%20of%20compute%20used%20during%20inference%20to%20make%20a%0Aprediction.%20Inference-time%20compute%20can%20be%20scaled%20in%20parallel%20by%20choosing%20among%0Amultiple%20independent%20solutions%20or%20sequentially%20through%20self-refinement.%20We%0Apropose%20Recursive%20Self-Aggregation%20%28RSA%29%2C%20a%20test-time%20scaling%20method%20inspired%0Aby%20evolutionary%20methods%20that%20combines%20the%20benefits%20of%20both%20parallel%20and%0Asequential%20scaling.%20Each%20step%20of%20RSA%20refines%20a%20population%20of%20candidate%0Areasoning%20chains%20through%20aggregation%20of%20subsets%20to%20yield%20a%20population%20of%0Aimproved%20solutions%2C%20which%20are%20then%20used%20as%20the%20candidate%20pool%20for%20the%20next%0Aiteration.%20RSA%20exploits%20the%20rich%20information%20embedded%20in%20the%20reasoning%20chains%0A--%20not%20just%20the%20final%20answers%20--%20and%20enables%20bootstrapping%20from%20partially%0Acorrect%20intermediate%20steps%20within%20different%20chains%20of%20thought.%20Empirically%2C%20RSA%0Adelivers%20substantial%20performance%20gains%20with%20increasing%20compute%20budgets%20across%0Adiverse%20tasks%2C%20model%20families%20and%20sizes.%20Notably%2C%20RSA%20enables%0AQwen3-4B-Instruct-2507%20to%20achieve%20competitive%20performance%20with%20larger%20reasoning%0Amodels%2C%20including%20DeepSeek-R1%20and%20o3-mini%20%28high%29%2C%20while%20outperforming%20purely%0Aparallel%20and%20sequential%20scaling%20strategies%20across%20AIME-25%2C%20HMMT-25%2C%20Reasoning%0AGym%2C%20LiveCodeBench-v6%2C%20and%20SuperGPQA.%20We%20further%20demonstrate%20that%20training%20the%0Amodel%20to%20combine%20solutions%20via%20a%20novel%20aggregation-aware%20reinforcement%20learning%0Aapproach%20yields%20significant%20performance%20gains.%20Code%20available%20at%0Ahttps%3A//github.com/HyperPotatoNeo/RSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Self-Aggregation%2520Unlocks%2520Deep%2520Thinking%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DSiddarth%2520Venkatraman%2520and%2520Vineet%2520Jain%2520and%2520Sarthak%2520Mittal%2520and%2520Vedant%2520Shah%2520and%2520Johan%2520Obando-Ceron%2520and%2520Yoshua%2520Bengio%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%2520and%2520Guillaume%2520Lajoie%2520and%2520Glen%2520Berseth%2520and%2520Nikolay%2520Malkin%2520and%2520Moksh%2520Jain%26entry.1292438233%3D%2520%2520Test-time%2520scaling%2520methods%2520improve%2520the%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520by%2520increasing%2520the%2520amount%2520of%2520compute%2520used%2520during%2520inference%2520to%2520make%2520a%250Aprediction.%2520Inference-time%2520compute%2520can%2520be%2520scaled%2520in%2520parallel%2520by%2520choosing%2520among%250Amultiple%2520independent%2520solutions%2520or%2520sequentially%2520through%2520self-refinement.%2520We%250Apropose%2520Recursive%2520Self-Aggregation%2520%2528RSA%2529%252C%2520a%2520test-time%2520scaling%2520method%2520inspired%250Aby%2520evolutionary%2520methods%2520that%2520combines%2520the%2520benefits%2520of%2520both%2520parallel%2520and%250Asequential%2520scaling.%2520Each%2520step%2520of%2520RSA%2520refines%2520a%2520population%2520of%2520candidate%250Areasoning%2520chains%2520through%2520aggregation%2520of%2520subsets%2520to%2520yield%2520a%2520population%2520of%250Aimproved%2520solutions%252C%2520which%2520are%2520then%2520used%2520as%2520the%2520candidate%2520pool%2520for%2520the%2520next%250Aiteration.%2520RSA%2520exploits%2520the%2520rich%2520information%2520embedded%2520in%2520the%2520reasoning%2520chains%250A--%2520not%2520just%2520the%2520final%2520answers%2520--%2520and%2520enables%2520bootstrapping%2520from%2520partially%250Acorrect%2520intermediate%2520steps%2520within%2520different%2520chains%2520of%2520thought.%2520Empirically%252C%2520RSA%250Adelivers%2520substantial%2520performance%2520gains%2520with%2520increasing%2520compute%2520budgets%2520across%250Adiverse%2520tasks%252C%2520model%2520families%2520and%2520sizes.%2520Notably%252C%2520RSA%2520enables%250AQwen3-4B-Instruct-2507%2520to%2520achieve%2520competitive%2520performance%2520with%2520larger%2520reasoning%250Amodels%252C%2520including%2520DeepSeek-R1%2520and%2520o3-mini%2520%2528high%2529%252C%2520while%2520outperforming%2520purely%250Aparallel%2520and%2520sequential%2520scaling%2520strategies%2520across%2520AIME-25%252C%2520HMMT-25%252C%2520Reasoning%250AGym%252C%2520LiveCodeBench-v6%252C%2520and%2520SuperGPQA.%2520We%2520further%2520demonstrate%2520that%2520training%2520the%250Amodel%2520to%2520combine%2520solutions%2520via%2520a%2520novel%2520aggregation-aware%2520reinforcement%2520learning%250Aapproach%2520yields%2520significant%2520performance%2520gains.%2520Code%2520available%2520at%250Ahttps%253A//github.com/HyperPotatoNeo/RSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Self-Aggregation%20Unlocks%20Deep%20Thinking%20in%20Large%20Language%0A%20%20Models&entry.906535625=Siddarth%20Venkatraman%20and%20Vineet%20Jain%20and%20Sarthak%20Mittal%20and%20Vedant%20Shah%20and%20Johan%20Obando-Ceron%20and%20Yoshua%20Bengio%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Guillaume%20Lajoie%20and%20Glen%20Berseth%20and%20Nikolay%20Malkin%20and%20Moksh%20Jain&entry.1292438233=%20%20Test-time%20scaling%20methods%20improve%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20by%20increasing%20the%20amount%20of%20compute%20used%20during%20inference%20to%20make%20a%0Aprediction.%20Inference-time%20compute%20can%20be%20scaled%20in%20parallel%20by%20choosing%20among%0Amultiple%20independent%20solutions%20or%20sequentially%20through%20self-refinement.%20We%0Apropose%20Recursive%20Self-Aggregation%20%28RSA%29%2C%20a%20test-time%20scaling%20method%20inspired%0Aby%20evolutionary%20methods%20that%20combines%20the%20benefits%20of%20both%20parallel%20and%0Asequential%20scaling.%20Each%20step%20of%20RSA%20refines%20a%20population%20of%20candidate%0Areasoning%20chains%20through%20aggregation%20of%20subsets%20to%20yield%20a%20population%20of%0Aimproved%20solutions%2C%20which%20are%20then%20used%20as%20the%20candidate%20pool%20for%20the%20next%0Aiteration.%20RSA%20exploits%20the%20rich%20information%20embedded%20in%20the%20reasoning%20chains%0A--%20not%20just%20the%20final%20answers%20--%20and%20enables%20bootstrapping%20from%20partially%0Acorrect%20intermediate%20steps%20within%20different%20chains%20of%20thought.%20Empirically%2C%20RSA%0Adelivers%20substantial%20performance%20gains%20with%20increasing%20compute%20budgets%20across%0Adiverse%20tasks%2C%20model%20families%20and%20sizes.%20Notably%2C%20RSA%20enables%0AQwen3-4B-Instruct-2507%20to%20achieve%20competitive%20performance%20with%20larger%20reasoning%0Amodels%2C%20including%20DeepSeek-R1%20and%20o3-mini%20%28high%29%2C%20while%20outperforming%20purely%0Aparallel%20and%20sequential%20scaling%20strategies%20across%20AIME-25%2C%20HMMT-25%2C%20Reasoning%0AGym%2C%20LiveCodeBench-v6%2C%20and%20SuperGPQA.%20We%20further%20demonstrate%20that%20training%20the%0Amodel%20to%20combine%20solutions%20via%20a%20novel%20aggregation-aware%20reinforcement%20learning%0Aapproach%20yields%20significant%20performance%20gains.%20Code%20available%20at%0Ahttps%3A//github.com/HyperPotatoNeo/RSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26626v1&entry.124074799=Read"},
{"title": "EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model\n  Explanations", "author": "Emerald Zhang and Julian Weaver and Samantha R Santacruz and Edward Castillo", "abstract": "  Explainable AI (XAI) methods help identify which image regions influence a\nmodel's prediction, but often face a trade-off between detail and\ninterpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware\nalternative. However, LRP implementations commonly rely on heuristic rule sets\nthat are not optimized for clarity or alignment with model behavior. We\nintroduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES) to tune LRP hyperparameters based on quantitative\ninterpretability metrics, such as faithfulness or sparseness. EVO-LRP\noutperforms traditional XAI approaches in both interpretability metric\nperformance and visual coherence, with strong sensitivity to class-specific\nfeatures. These findings demonstrate that attribution quality can be\nsystematically improved through principled, task-specific optimization.\n", "link": "http://arxiv.org/abs/2509.23585v2", "date": "2025-09-30", "relevancy": 2.0881, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EVO-LRP%3A%20Evolutionary%20Optimization%20of%20LRP%20for%20Interpretable%20Model%0A%20%20Explanations&body=Title%3A%20EVO-LRP%3A%20Evolutionary%20Optimization%20of%20LRP%20for%20Interpretable%20Model%0A%20%20Explanations%0AAuthor%3A%20Emerald%20Zhang%20and%20Julian%20Weaver%20and%20Samantha%20R%20Santacruz%20and%20Edward%20Castillo%0AAbstract%3A%20%20%20Explainable%20AI%20%28XAI%29%20methods%20help%20identify%20which%20image%20regions%20influence%20a%0Amodel%27s%20prediction%2C%20but%20often%20face%20a%20trade-off%20between%20detail%20and%0Ainterpretability.%20Layer-wise%20Relevance%20Propagation%20%28LRP%29%20offers%20a%20model-aware%0Aalternative.%20However%2C%20LRP%20implementations%20commonly%20rely%20on%20heuristic%20rule%20sets%0Athat%20are%20not%20optimized%20for%20clarity%20or%20alignment%20with%20model%20behavior.%20We%0Aintroduce%20EVO-LRP%2C%20a%20method%20that%20applies%20Covariance%20Matrix%20Adaptation%20Evolution%0AStrategy%20%28CMA-ES%29%20to%20tune%20LRP%20hyperparameters%20based%20on%20quantitative%0Ainterpretability%20metrics%2C%20such%20as%20faithfulness%20or%20sparseness.%20EVO-LRP%0Aoutperforms%20traditional%20XAI%20approaches%20in%20both%20interpretability%20metric%0Aperformance%20and%20visual%20coherence%2C%20with%20strong%20sensitivity%20to%20class-specific%0Afeatures.%20These%20findings%20demonstrate%20that%20attribution%20quality%20can%20be%0Asystematically%20improved%20through%20principled%2C%20task-specific%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEVO-LRP%253A%2520Evolutionary%2520Optimization%2520of%2520LRP%2520for%2520Interpretable%2520Model%250A%2520%2520Explanations%26entry.906535625%3DEmerald%2520Zhang%2520and%2520Julian%2520Weaver%2520and%2520Samantha%2520R%2520Santacruz%2520and%2520Edward%2520Castillo%26entry.1292438233%3D%2520%2520Explainable%2520AI%2520%2528XAI%2529%2520methods%2520help%2520identify%2520which%2520image%2520regions%2520influence%2520a%250Amodel%2527s%2520prediction%252C%2520but%2520often%2520face%2520a%2520trade-off%2520between%2520detail%2520and%250Ainterpretability.%2520Layer-wise%2520Relevance%2520Propagation%2520%2528LRP%2529%2520offers%2520a%2520model-aware%250Aalternative.%2520However%252C%2520LRP%2520implementations%2520commonly%2520rely%2520on%2520heuristic%2520rule%2520sets%250Athat%2520are%2520not%2520optimized%2520for%2520clarity%2520or%2520alignment%2520with%2520model%2520behavior.%2520We%250Aintroduce%2520EVO-LRP%252C%2520a%2520method%2520that%2520applies%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%250AStrategy%2520%2528CMA-ES%2529%2520to%2520tune%2520LRP%2520hyperparameters%2520based%2520on%2520quantitative%250Ainterpretability%2520metrics%252C%2520such%2520as%2520faithfulness%2520or%2520sparseness.%2520EVO-LRP%250Aoutperforms%2520traditional%2520XAI%2520approaches%2520in%2520both%2520interpretability%2520metric%250Aperformance%2520and%2520visual%2520coherence%252C%2520with%2520strong%2520sensitivity%2520to%2520class-specific%250Afeatures.%2520These%2520findings%2520demonstrate%2520that%2520attribution%2520quality%2520can%2520be%250Asystematically%2520improved%2520through%2520principled%252C%2520task-specific%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EVO-LRP%3A%20Evolutionary%20Optimization%20of%20LRP%20for%20Interpretable%20Model%0A%20%20Explanations&entry.906535625=Emerald%20Zhang%20and%20Julian%20Weaver%20and%20Samantha%20R%20Santacruz%20and%20Edward%20Castillo&entry.1292438233=%20%20Explainable%20AI%20%28XAI%29%20methods%20help%20identify%20which%20image%20regions%20influence%20a%0Amodel%27s%20prediction%2C%20but%20often%20face%20a%20trade-off%20between%20detail%20and%0Ainterpretability.%20Layer-wise%20Relevance%20Propagation%20%28LRP%29%20offers%20a%20model-aware%0Aalternative.%20However%2C%20LRP%20implementations%20commonly%20rely%20on%20heuristic%20rule%20sets%0Athat%20are%20not%20optimized%20for%20clarity%20or%20alignment%20with%20model%20behavior.%20We%0Aintroduce%20EVO-LRP%2C%20a%20method%20that%20applies%20Covariance%20Matrix%20Adaptation%20Evolution%0AStrategy%20%28CMA-ES%29%20to%20tune%20LRP%20hyperparameters%20based%20on%20quantitative%0Ainterpretability%20metrics%2C%20such%20as%20faithfulness%20or%20sparseness.%20EVO-LRP%0Aoutperforms%20traditional%20XAI%20approaches%20in%20both%20interpretability%20metric%0Aperformance%20and%20visual%20coherence%2C%20with%20strong%20sensitivity%20to%20class-specific%0Afeatures.%20These%20findings%20demonstrate%20that%20attribution%20quality%20can%20be%0Asystematically%20improved%20through%20principled%2C%20task-specific%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23585v2&entry.124074799=Read"},
{"title": "DeepProv: Behavioral Characterization and Repair of Neural Networks via\n  Inference Provenance Graph Analysis", "author": "Firas Ben Hmida and Abderrahmen Amich and Ata Kaboudi and Birhanu Eshete", "abstract": "  Deep neural networks (DNNs) are increasingly being deployed in high-stakes\napplications, from self-driving cars to biometric authentication. However,\ntheir unpredictable and unreliable behaviors in real-world settings require new\napproaches to characterize and ensure their reliability.\n  This paper introduces DeepProv, a novel and customizable system designed to\ncapture and characterize the runtime behavior of DNNs during inference by using\ntheir underlying graph structure. Inspired by system audit provenance graphs,\nDeepProv models the computational information flow of a DNN's inference process\nthrough Inference Provenance Graphs (IPGs). These graphs provide a detailed\nstructural representation of the behavior of DNN, allowing both empirical and\nstructural analysis. DeepProv uses these insights to systematically repair DNNs\nfor specific objectives, such as improving robustness, privacy, or fairness.\n  We instantiate DeepProv with adversarial robustness as the goal of model\nrepair and conduct extensive case studies to evaluate its effectiveness. Our\nresults demonstrate its effectiveness and scalability across diverse\nclassification tasks, attack scenarios, and model complexities. DeepProv\nautomatically identifies repair actions at the node and edge-level within IPGs,\nsignificantly enhancing the robustness of the model. In particular, applying\nDeepProv repair strategies to just a single layer of a DNN yields an average\n55% improvement in adversarial accuracy. Moreover, DeepProv complements\nexisting defenses, achieving substantial gains in adversarial robustness.\nBeyond robustness, we demonstrate the broader potential of DeepProv as an\nadaptable system to characterize DNN behavior in other critical areas, such as\nprivacy auditing and fairness analysis.\n", "link": "http://arxiv.org/abs/2509.26562v1", "date": "2025-09-30", "relevancy": 2.0702, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepProv%3A%20Behavioral%20Characterization%20and%20Repair%20of%20Neural%20Networks%20via%0A%20%20Inference%20Provenance%20Graph%20Analysis&body=Title%3A%20DeepProv%3A%20Behavioral%20Characterization%20and%20Repair%20of%20Neural%20Networks%20via%0A%20%20Inference%20Provenance%20Graph%20Analysis%0AAuthor%3A%20Firas%20Ben%20Hmida%20and%20Abderrahmen%20Amich%20and%20Ata%20Kaboudi%20and%20Birhanu%20Eshete%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20increasingly%20being%20deployed%20in%20high-stakes%0Aapplications%2C%20from%20self-driving%20cars%20to%20biometric%20authentication.%20However%2C%0Atheir%20unpredictable%20and%20unreliable%20behaviors%20in%20real-world%20settings%20require%20new%0Aapproaches%20to%20characterize%20and%20ensure%20their%20reliability.%0A%20%20This%20paper%20introduces%20DeepProv%2C%20a%20novel%20and%20customizable%20system%20designed%20to%0Acapture%20and%20characterize%20the%20runtime%20behavior%20of%20DNNs%20during%20inference%20by%20using%0Atheir%20underlying%20graph%20structure.%20Inspired%20by%20system%20audit%20provenance%20graphs%2C%0ADeepProv%20models%20the%20computational%20information%20flow%20of%20a%20DNN%27s%20inference%20process%0Athrough%20Inference%20Provenance%20Graphs%20%28IPGs%29.%20These%20graphs%20provide%20a%20detailed%0Astructural%20representation%20of%20the%20behavior%20of%20DNN%2C%20allowing%20both%20empirical%20and%0Astructural%20analysis.%20DeepProv%20uses%20these%20insights%20to%20systematically%20repair%20DNNs%0Afor%20specific%20objectives%2C%20such%20as%20improving%20robustness%2C%20privacy%2C%20or%20fairness.%0A%20%20We%20instantiate%20DeepProv%20with%20adversarial%20robustness%20as%20the%20goal%20of%20model%0Arepair%20and%20conduct%20extensive%20case%20studies%20to%20evaluate%20its%20effectiveness.%20Our%0Aresults%20demonstrate%20its%20effectiveness%20and%20scalability%20across%20diverse%0Aclassification%20tasks%2C%20attack%20scenarios%2C%20and%20model%20complexities.%20DeepProv%0Aautomatically%20identifies%20repair%20actions%20at%20the%20node%20and%20edge-level%20within%20IPGs%2C%0Asignificantly%20enhancing%20the%20robustness%20of%20the%20model.%20In%20particular%2C%20applying%0ADeepProv%20repair%20strategies%20to%20just%20a%20single%20layer%20of%20a%20DNN%20yields%20an%20average%0A55%25%20improvement%20in%20adversarial%20accuracy.%20Moreover%2C%20DeepProv%20complements%0Aexisting%20defenses%2C%20achieving%20substantial%20gains%20in%20adversarial%20robustness.%0ABeyond%20robustness%2C%20we%20demonstrate%20the%20broader%20potential%20of%20DeepProv%20as%20an%0Aadaptable%20system%20to%20characterize%20DNN%20behavior%20in%20other%20critical%20areas%2C%20such%20as%0Aprivacy%20auditing%20and%20fairness%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepProv%253A%2520Behavioral%2520Characterization%2520and%2520Repair%2520of%2520Neural%2520Networks%2520via%250A%2520%2520Inference%2520Provenance%2520Graph%2520Analysis%26entry.906535625%3DFiras%2520Ben%2520Hmida%2520and%2520Abderrahmen%2520Amich%2520and%2520Ata%2520Kaboudi%2520and%2520Birhanu%2520Eshete%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520increasingly%2520being%2520deployed%2520in%2520high-stakes%250Aapplications%252C%2520from%2520self-driving%2520cars%2520to%2520biometric%2520authentication.%2520However%252C%250Atheir%2520unpredictable%2520and%2520unreliable%2520behaviors%2520in%2520real-world%2520settings%2520require%2520new%250Aapproaches%2520to%2520characterize%2520and%2520ensure%2520their%2520reliability.%250A%2520%2520This%2520paper%2520introduces%2520DeepProv%252C%2520a%2520novel%2520and%2520customizable%2520system%2520designed%2520to%250Acapture%2520and%2520characterize%2520the%2520runtime%2520behavior%2520of%2520DNNs%2520during%2520inference%2520by%2520using%250Atheir%2520underlying%2520graph%2520structure.%2520Inspired%2520by%2520system%2520audit%2520provenance%2520graphs%252C%250ADeepProv%2520models%2520the%2520computational%2520information%2520flow%2520of%2520a%2520DNN%2527s%2520inference%2520process%250Athrough%2520Inference%2520Provenance%2520Graphs%2520%2528IPGs%2529.%2520These%2520graphs%2520provide%2520a%2520detailed%250Astructural%2520representation%2520of%2520the%2520behavior%2520of%2520DNN%252C%2520allowing%2520both%2520empirical%2520and%250Astructural%2520analysis.%2520DeepProv%2520uses%2520these%2520insights%2520to%2520systematically%2520repair%2520DNNs%250Afor%2520specific%2520objectives%252C%2520such%2520as%2520improving%2520robustness%252C%2520privacy%252C%2520or%2520fairness.%250A%2520%2520We%2520instantiate%2520DeepProv%2520with%2520adversarial%2520robustness%2520as%2520the%2520goal%2520of%2520model%250Arepair%2520and%2520conduct%2520extensive%2520case%2520studies%2520to%2520evaluate%2520its%2520effectiveness.%2520Our%250Aresults%2520demonstrate%2520its%2520effectiveness%2520and%2520scalability%2520across%2520diverse%250Aclassification%2520tasks%252C%2520attack%2520scenarios%252C%2520and%2520model%2520complexities.%2520DeepProv%250Aautomatically%2520identifies%2520repair%2520actions%2520at%2520the%2520node%2520and%2520edge-level%2520within%2520IPGs%252C%250Asignificantly%2520enhancing%2520the%2520robustness%2520of%2520the%2520model.%2520In%2520particular%252C%2520applying%250ADeepProv%2520repair%2520strategies%2520to%2520just%2520a%2520single%2520layer%2520of%2520a%2520DNN%2520yields%2520an%2520average%250A55%2525%2520improvement%2520in%2520adversarial%2520accuracy.%2520Moreover%252C%2520DeepProv%2520complements%250Aexisting%2520defenses%252C%2520achieving%2520substantial%2520gains%2520in%2520adversarial%2520robustness.%250ABeyond%2520robustness%252C%2520we%2520demonstrate%2520the%2520broader%2520potential%2520of%2520DeepProv%2520as%2520an%250Aadaptable%2520system%2520to%2520characterize%2520DNN%2520behavior%2520in%2520other%2520critical%2520areas%252C%2520such%2520as%250Aprivacy%2520auditing%2520and%2520fairness%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepProv%3A%20Behavioral%20Characterization%20and%20Repair%20of%20Neural%20Networks%20via%0A%20%20Inference%20Provenance%20Graph%20Analysis&entry.906535625=Firas%20Ben%20Hmida%20and%20Abderrahmen%20Amich%20and%20Ata%20Kaboudi%20and%20Birhanu%20Eshete&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20increasingly%20being%20deployed%20in%20high-stakes%0Aapplications%2C%20from%20self-driving%20cars%20to%20biometric%20authentication.%20However%2C%0Atheir%20unpredictable%20and%20unreliable%20behaviors%20in%20real-world%20settings%20require%20new%0Aapproaches%20to%20characterize%20and%20ensure%20their%20reliability.%0A%20%20This%20paper%20introduces%20DeepProv%2C%20a%20novel%20and%20customizable%20system%20designed%20to%0Acapture%20and%20characterize%20the%20runtime%20behavior%20of%20DNNs%20during%20inference%20by%20using%0Atheir%20underlying%20graph%20structure.%20Inspired%20by%20system%20audit%20provenance%20graphs%2C%0ADeepProv%20models%20the%20computational%20information%20flow%20of%20a%20DNN%27s%20inference%20process%0Athrough%20Inference%20Provenance%20Graphs%20%28IPGs%29.%20These%20graphs%20provide%20a%20detailed%0Astructural%20representation%20of%20the%20behavior%20of%20DNN%2C%20allowing%20both%20empirical%20and%0Astructural%20analysis.%20DeepProv%20uses%20these%20insights%20to%20systematically%20repair%20DNNs%0Afor%20specific%20objectives%2C%20such%20as%20improving%20robustness%2C%20privacy%2C%20or%20fairness.%0A%20%20We%20instantiate%20DeepProv%20with%20adversarial%20robustness%20as%20the%20goal%20of%20model%0Arepair%20and%20conduct%20extensive%20case%20studies%20to%20evaluate%20its%20effectiveness.%20Our%0Aresults%20demonstrate%20its%20effectiveness%20and%20scalability%20across%20diverse%0Aclassification%20tasks%2C%20attack%20scenarios%2C%20and%20model%20complexities.%20DeepProv%0Aautomatically%20identifies%20repair%20actions%20at%20the%20node%20and%20edge-level%20within%20IPGs%2C%0Asignificantly%20enhancing%20the%20robustness%20of%20the%20model.%20In%20particular%2C%20applying%0ADeepProv%20repair%20strategies%20to%20just%20a%20single%20layer%20of%20a%20DNN%20yields%20an%20average%0A55%25%20improvement%20in%20adversarial%20accuracy.%20Moreover%2C%20DeepProv%20complements%0Aexisting%20defenses%2C%20achieving%20substantial%20gains%20in%20adversarial%20robustness.%0ABeyond%20robustness%2C%20we%20demonstrate%20the%20broader%20potential%20of%20DeepProv%20as%20an%0Aadaptable%20system%20to%20characterize%20DNN%20behavior%20in%20other%20critical%20areas%2C%20such%20as%0Aprivacy%20auditing%20and%20fairness%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26562v1&entry.124074799=Read"},
{"title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark", "author": "Minhui Zhu and Minyang Tian and Xiaocheng Yang and Tianci Zhou and Penghao Zhu and Eli Chertkov and Shengyan Liu and Yufeng Du and Lifan Yuan and Ziming Ji and Indranil Das and Junyi Cao and Yufeng Du and Jinchen He and Yifan Su and Jiabin Yu and Yikun Jiang and Yujie Zhang and Chang Liu and Ze-Min Huang and Weizhen Jia and Xinan Chen and Peixue Wu and Yunkai Wang and Juntai Zhou and Yong Zhao and Farshid Jafarpour and Jessie Shelton and Aaron Young and John Bartolotta and Wenchao Xu and Yue Sun and Anjun Chu and Victor Colussi and Chris Akers and Nathan Brooks and Wenbo Fu and Christopher Wilson and Jinchao Zhao and Marvin Qi and Anqi Mu and Yubo Yang and Allen Zang and Yang Lyu and Peizhi Mai and Xuefei Guo and Luyu Gao and Ze Yang and Chi Xue and Dmytro Bandak and Ya\u00efr Hein and Yonatan Kahn and Kevin Zhou and John Drew Wilson Jarrod T. Reilly and Di Luo and Daniel Inafuku and Hao Tong and Liang Yang and Ruixing Zhang and Xueying Wang and Ofir Press and Nicolas Chia and Eliu Huerta and Hao Peng", "abstract": "  While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.\n", "link": "http://arxiv.org/abs/2509.26574v1", "date": "2025-09-30", "relevancy": 2.04, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20the%20Critical%20Point%20%28CritPt%29%20of%20AI%20Reasoning%3A%20a%20Frontier%20Physics%0A%20%20Research%20Benchmark&body=Title%3A%20Probing%20the%20Critical%20Point%20%28CritPt%29%20of%20AI%20Reasoning%3A%20a%20Frontier%20Physics%0A%20%20Research%20Benchmark%0AAuthor%3A%20Minhui%20Zhu%20and%20Minyang%20Tian%20and%20Xiaocheng%20Yang%20and%20Tianci%20Zhou%20and%20Penghao%20Zhu%20and%20Eli%20Chertkov%20and%20Shengyan%20Liu%20and%20Yufeng%20Du%20and%20Lifan%20Yuan%20and%20Ziming%20Ji%20and%20Indranil%20Das%20and%20Junyi%20Cao%20and%20Yufeng%20Du%20and%20Jinchen%20He%20and%20Yifan%20Su%20and%20Jiabin%20Yu%20and%20Yikun%20Jiang%20and%20Yujie%20Zhang%20and%20Chang%20Liu%20and%20Ze-Min%20Huang%20and%20Weizhen%20Jia%20and%20Xinan%20Chen%20and%20Peixue%20Wu%20and%20Yunkai%20Wang%20and%20Juntai%20Zhou%20and%20Yong%20Zhao%20and%20Farshid%20Jafarpour%20and%20Jessie%20Shelton%20and%20Aaron%20Young%20and%20John%20Bartolotta%20and%20Wenchao%20Xu%20and%20Yue%20Sun%20and%20Anjun%20Chu%20and%20Victor%20Colussi%20and%20Chris%20Akers%20and%20Nathan%20Brooks%20and%20Wenbo%20Fu%20and%20Christopher%20Wilson%20and%20Jinchao%20Zhao%20and%20Marvin%20Qi%20and%20Anqi%20Mu%20and%20Yubo%20Yang%20and%20Allen%20Zang%20and%20Yang%20Lyu%20and%20Peizhi%20Mai%20and%20Xuefei%20Guo%20and%20Luyu%20Gao%20and%20Ze%20Yang%20and%20Chi%20Xue%20and%20Dmytro%20Bandak%20and%20Ya%C3%AFr%20Hein%20and%20Yonatan%20Kahn%20and%20Kevin%20Zhou%20and%20John%20Drew%20Wilson%20Jarrod%20T.%20Reilly%20and%20Di%20Luo%20and%20Daniel%20Inafuku%20and%20Hao%20Tong%20and%20Liang%20Yang%20and%20Ruixing%20Zhang%20and%20Xueying%20Wang%20and%20Ofir%20Press%20and%20Nicolas%20Chia%20and%20Eliu%20Huerta%20and%20Hao%20Peng%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20with%20reasoning%20capabilities%20are%0Aprogressing%20rapidly%20on%20high-school%20math%20competitions%20and%20coding%2C%20can%20they%0Areason%20effectively%20through%20complex%2C%20open-ended%20challenges%20found%20in%20frontier%0Aphysics%20research%3F%20And%20crucially%2C%20what%20kinds%20of%20reasoning%20tasks%20do%20physicists%0Awant%20LLMs%20to%20assist%20with%3F%20To%20address%20these%20questions%2C%20we%20present%20the%20CritPt%0A%28Complex%20Research%20using%20Integrated%20Thinking%20-%20Physics%20Test%2C%20pronounced%0A%22critical%20point%22%29%2C%20the%20first%20benchmark%20designed%20to%20test%20LLMs%20on%20unpublished%2C%0Aresearch-level%20reasoning%20tasks%20that%20broadly%20covers%20modern%20physics%20research%0Aareas%2C%20including%20condensed%20matter%2C%20quantum%20physics%2C%20atomic%2C%20molecular%20%26%20optical%0Aphysics%2C%20astrophysics%2C%20high%20energy%20physics%2C%20mathematical%20physics%2C%20statistical%0Aphysics%2C%20nuclear%20physics%2C%20nonlinear%20dynamics%2C%20fluid%20dynamics%20and%20biophysics.%0ACritPt%20consists%20of%2071%20composite%20research%20challenges%20designed%20to%20simulate%0Afull-scale%20research%20projects%20at%20the%20entry%20level%2C%20which%20are%20also%20decomposed%20to%0A190%20simpler%20checkpoint%20tasks%20for%20more%20fine-grained%20insights.%20All%20problems%20are%0Anewly%20created%20by%2050%2B%20active%20physics%20researchers%20based%20on%20their%20own%20research.%0AEvery%20problem%20is%20hand-curated%20to%20admit%20a%20guess-resistant%20and%20machine-verifiable%0Aanswer%20and%20is%20evaluated%20by%20an%20automated%20grading%20pipeline%20heavily%20customized%20for%0Aadvanced%20physics-specific%20output%20formats.%20We%20find%20that%20while%20current%0Astate-of-the-art%20LLMs%20show%20early%20promise%20on%20isolated%20checkpoints%2C%20they%20remain%0Afar%20from%20being%20able%20to%20reliably%20solve%20full%20research-scale%20challenges%3A%20the%20best%0Aaverage%20accuracy%20among%20base%20models%20is%20only%204.0%25%20%2C%20achieved%20by%20GPT-5%20%28high%29%2C%0Amoderately%20rising%20to%20around%2010%25%20when%20equipped%20with%20coding%20tools.%20Through%20the%0Arealistic%20yet%20standardized%20evaluation%20offered%20by%20CritPt%2C%20we%20highlight%20a%20large%0Adisconnect%20between%20current%20model%20capabilities%20and%20realistic%20physics%20research%0Ademands%2C%20offering%20a%20foundation%20to%20guide%20the%20development%20of%20scientifically%0Agrounded%20AI%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520the%2520Critical%2520Point%2520%2528CritPt%2529%2520of%2520AI%2520Reasoning%253A%2520a%2520Frontier%2520Physics%250A%2520%2520Research%2520Benchmark%26entry.906535625%3DMinhui%2520Zhu%2520and%2520Minyang%2520Tian%2520and%2520Xiaocheng%2520Yang%2520and%2520Tianci%2520Zhou%2520and%2520Penghao%2520Zhu%2520and%2520Eli%2520Chertkov%2520and%2520Shengyan%2520Liu%2520and%2520Yufeng%2520Du%2520and%2520Lifan%2520Yuan%2520and%2520Ziming%2520Ji%2520and%2520Indranil%2520Das%2520and%2520Junyi%2520Cao%2520and%2520Yufeng%2520Du%2520and%2520Jinchen%2520He%2520and%2520Yifan%2520Su%2520and%2520Jiabin%2520Yu%2520and%2520Yikun%2520Jiang%2520and%2520Yujie%2520Zhang%2520and%2520Chang%2520Liu%2520and%2520Ze-Min%2520Huang%2520and%2520Weizhen%2520Jia%2520and%2520Xinan%2520Chen%2520and%2520Peixue%2520Wu%2520and%2520Yunkai%2520Wang%2520and%2520Juntai%2520Zhou%2520and%2520Yong%2520Zhao%2520and%2520Farshid%2520Jafarpour%2520and%2520Jessie%2520Shelton%2520and%2520Aaron%2520Young%2520and%2520John%2520Bartolotta%2520and%2520Wenchao%2520Xu%2520and%2520Yue%2520Sun%2520and%2520Anjun%2520Chu%2520and%2520Victor%2520Colussi%2520and%2520Chris%2520Akers%2520and%2520Nathan%2520Brooks%2520and%2520Wenbo%2520Fu%2520and%2520Christopher%2520Wilson%2520and%2520Jinchao%2520Zhao%2520and%2520Marvin%2520Qi%2520and%2520Anqi%2520Mu%2520and%2520Yubo%2520Yang%2520and%2520Allen%2520Zang%2520and%2520Yang%2520Lyu%2520and%2520Peizhi%2520Mai%2520and%2520Xuefei%2520Guo%2520and%2520Luyu%2520Gao%2520and%2520Ze%2520Yang%2520and%2520Chi%2520Xue%2520and%2520Dmytro%2520Bandak%2520and%2520Ya%25C3%25AFr%2520Hein%2520and%2520Yonatan%2520Kahn%2520and%2520Kevin%2520Zhou%2520and%2520John%2520Drew%2520Wilson%2520Jarrod%2520T.%2520Reilly%2520and%2520Di%2520Luo%2520and%2520Daniel%2520Inafuku%2520and%2520Hao%2520Tong%2520and%2520Liang%2520Yang%2520and%2520Ruixing%2520Zhang%2520and%2520Xueying%2520Wang%2520and%2520Ofir%2520Press%2520and%2520Nicolas%2520Chia%2520and%2520Eliu%2520Huerta%2520and%2520Hao%2520Peng%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520reasoning%2520capabilities%2520are%250Aprogressing%2520rapidly%2520on%2520high-school%2520math%2520competitions%2520and%2520coding%252C%2520can%2520they%250Areason%2520effectively%2520through%2520complex%252C%2520open-ended%2520challenges%2520found%2520in%2520frontier%250Aphysics%2520research%253F%2520And%2520crucially%252C%2520what%2520kinds%2520of%2520reasoning%2520tasks%2520do%2520physicists%250Awant%2520LLMs%2520to%2520assist%2520with%253F%2520To%2520address%2520these%2520questions%252C%2520we%2520present%2520the%2520CritPt%250A%2528Complex%2520Research%2520using%2520Integrated%2520Thinking%2520-%2520Physics%2520Test%252C%2520pronounced%250A%2522critical%2520point%2522%2529%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520test%2520LLMs%2520on%2520unpublished%252C%250Aresearch-level%2520reasoning%2520tasks%2520that%2520broadly%2520covers%2520modern%2520physics%2520research%250Aareas%252C%2520including%2520condensed%2520matter%252C%2520quantum%2520physics%252C%2520atomic%252C%2520molecular%2520%2526%2520optical%250Aphysics%252C%2520astrophysics%252C%2520high%2520energy%2520physics%252C%2520mathematical%2520physics%252C%2520statistical%250Aphysics%252C%2520nuclear%2520physics%252C%2520nonlinear%2520dynamics%252C%2520fluid%2520dynamics%2520and%2520biophysics.%250ACritPt%2520consists%2520of%252071%2520composite%2520research%2520challenges%2520designed%2520to%2520simulate%250Afull-scale%2520research%2520projects%2520at%2520the%2520entry%2520level%252C%2520which%2520are%2520also%2520decomposed%2520to%250A190%2520simpler%2520checkpoint%2520tasks%2520for%2520more%2520fine-grained%2520insights.%2520All%2520problems%2520are%250Anewly%2520created%2520by%252050%252B%2520active%2520physics%2520researchers%2520based%2520on%2520their%2520own%2520research.%250AEvery%2520problem%2520is%2520hand-curated%2520to%2520admit%2520a%2520guess-resistant%2520and%2520machine-verifiable%250Aanswer%2520and%2520is%2520evaluated%2520by%2520an%2520automated%2520grading%2520pipeline%2520heavily%2520customized%2520for%250Aadvanced%2520physics-specific%2520output%2520formats.%2520We%2520find%2520that%2520while%2520current%250Astate-of-the-art%2520LLMs%2520show%2520early%2520promise%2520on%2520isolated%2520checkpoints%252C%2520they%2520remain%250Afar%2520from%2520being%2520able%2520to%2520reliably%2520solve%2520full%2520research-scale%2520challenges%253A%2520the%2520best%250Aaverage%2520accuracy%2520among%2520base%2520models%2520is%2520only%25204.0%2525%2520%252C%2520achieved%2520by%2520GPT-5%2520%2528high%2529%252C%250Amoderately%2520rising%2520to%2520around%252010%2525%2520when%2520equipped%2520with%2520coding%2520tools.%2520Through%2520the%250Arealistic%2520yet%2520standardized%2520evaluation%2520offered%2520by%2520CritPt%252C%2520we%2520highlight%2520a%2520large%250Adisconnect%2520between%2520current%2520model%2520capabilities%2520and%2520realistic%2520physics%2520research%250Ademands%252C%2520offering%2520a%2520foundation%2520to%2520guide%2520the%2520development%2520of%2520scientifically%250Agrounded%2520AI%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20the%20Critical%20Point%20%28CritPt%29%20of%20AI%20Reasoning%3A%20a%20Frontier%20Physics%0A%20%20Research%20Benchmark&entry.906535625=Minhui%20Zhu%20and%20Minyang%20Tian%20and%20Xiaocheng%20Yang%20and%20Tianci%20Zhou%20and%20Penghao%20Zhu%20and%20Eli%20Chertkov%20and%20Shengyan%20Liu%20and%20Yufeng%20Du%20and%20Lifan%20Yuan%20and%20Ziming%20Ji%20and%20Indranil%20Das%20and%20Junyi%20Cao%20and%20Yufeng%20Du%20and%20Jinchen%20He%20and%20Yifan%20Su%20and%20Jiabin%20Yu%20and%20Yikun%20Jiang%20and%20Yujie%20Zhang%20and%20Chang%20Liu%20and%20Ze-Min%20Huang%20and%20Weizhen%20Jia%20and%20Xinan%20Chen%20and%20Peixue%20Wu%20and%20Yunkai%20Wang%20and%20Juntai%20Zhou%20and%20Yong%20Zhao%20and%20Farshid%20Jafarpour%20and%20Jessie%20Shelton%20and%20Aaron%20Young%20and%20John%20Bartolotta%20and%20Wenchao%20Xu%20and%20Yue%20Sun%20and%20Anjun%20Chu%20and%20Victor%20Colussi%20and%20Chris%20Akers%20and%20Nathan%20Brooks%20and%20Wenbo%20Fu%20and%20Christopher%20Wilson%20and%20Jinchao%20Zhao%20and%20Marvin%20Qi%20and%20Anqi%20Mu%20and%20Yubo%20Yang%20and%20Allen%20Zang%20and%20Yang%20Lyu%20and%20Peizhi%20Mai%20and%20Xuefei%20Guo%20and%20Luyu%20Gao%20and%20Ze%20Yang%20and%20Chi%20Xue%20and%20Dmytro%20Bandak%20and%20Ya%C3%AFr%20Hein%20and%20Yonatan%20Kahn%20and%20Kevin%20Zhou%20and%20John%20Drew%20Wilson%20Jarrod%20T.%20Reilly%20and%20Di%20Luo%20and%20Daniel%20Inafuku%20and%20Hao%20Tong%20and%20Liang%20Yang%20and%20Ruixing%20Zhang%20and%20Xueying%20Wang%20and%20Ofir%20Press%20and%20Nicolas%20Chia%20and%20Eliu%20Huerta%20and%20Hao%20Peng&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20with%20reasoning%20capabilities%20are%0Aprogressing%20rapidly%20on%20high-school%20math%20competitions%20and%20coding%2C%20can%20they%0Areason%20effectively%20through%20complex%2C%20open-ended%20challenges%20found%20in%20frontier%0Aphysics%20research%3F%20And%20crucially%2C%20what%20kinds%20of%20reasoning%20tasks%20do%20physicists%0Awant%20LLMs%20to%20assist%20with%3F%20To%20address%20these%20questions%2C%20we%20present%20the%20CritPt%0A%28Complex%20Research%20using%20Integrated%20Thinking%20-%20Physics%20Test%2C%20pronounced%0A%22critical%20point%22%29%2C%20the%20first%20benchmark%20designed%20to%20test%20LLMs%20on%20unpublished%2C%0Aresearch-level%20reasoning%20tasks%20that%20broadly%20covers%20modern%20physics%20research%0Aareas%2C%20including%20condensed%20matter%2C%20quantum%20physics%2C%20atomic%2C%20molecular%20%26%20optical%0Aphysics%2C%20astrophysics%2C%20high%20energy%20physics%2C%20mathematical%20physics%2C%20statistical%0Aphysics%2C%20nuclear%20physics%2C%20nonlinear%20dynamics%2C%20fluid%20dynamics%20and%20biophysics.%0ACritPt%20consists%20of%2071%20composite%20research%20challenges%20designed%20to%20simulate%0Afull-scale%20research%20projects%20at%20the%20entry%20level%2C%20which%20are%20also%20decomposed%20to%0A190%20simpler%20checkpoint%20tasks%20for%20more%20fine-grained%20insights.%20All%20problems%20are%0Anewly%20created%20by%2050%2B%20active%20physics%20researchers%20based%20on%20their%20own%20research.%0AEvery%20problem%20is%20hand-curated%20to%20admit%20a%20guess-resistant%20and%20machine-verifiable%0Aanswer%20and%20is%20evaluated%20by%20an%20automated%20grading%20pipeline%20heavily%20customized%20for%0Aadvanced%20physics-specific%20output%20formats.%20We%20find%20that%20while%20current%0Astate-of-the-art%20LLMs%20show%20early%20promise%20on%20isolated%20checkpoints%2C%20they%20remain%0Afar%20from%20being%20able%20to%20reliably%20solve%20full%20research-scale%20challenges%3A%20the%20best%0Aaverage%20accuracy%20among%20base%20models%20is%20only%204.0%25%20%2C%20achieved%20by%20GPT-5%20%28high%29%2C%0Amoderately%20rising%20to%20around%2010%25%20when%20equipped%20with%20coding%20tools.%20Through%20the%0Arealistic%20yet%20standardized%20evaluation%20offered%20by%20CritPt%2C%20we%20highlight%20a%20large%0Adisconnect%20between%20current%20model%20capabilities%20and%20realistic%20physics%20research%0Ademands%2C%20offering%20a%20foundation%20to%20guide%20the%20development%20of%20scientifically%0Agrounded%20AI%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26574v1&entry.124074799=Read"},
{"title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models", "author": "Shaoyi Zheng and Wenbo Lu and Yuxuan Xia and Haomin Liu and Shengjie Wang", "abstract": "  Designing sparse attention for diffusion transformers requires reconciling\ntwo-dimensional spatial locality with GPU efficiency, a trade-off that current\nmethods struggle to achieve. Existing approaches enforce two-dimensional\nspatial locality but often incur uncoalesced memory access. We present\nHilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA\nreorders image tokens along Hilbert curves to achieve a contiguous memory\nlayout while preserving spatial neighborhoods, and employs a sliding schedule\nacross layers to enable long-range information propagation without repeated or\nuncoalesced memory access. To further enhance cross-tile communication and\npositional awareness, HilbertA introduces a small central shared region.\nImplemented in Triton, HilbertA delivers comparable image quality with\nsignificant acceleration over prior methods on Flux.1-dev, demonstrating the\nfeasibility of hardware-aligned two-dimensional sparse attention for\nhigh-resolution image generation. HilbertA delivers attention speedups of\n$2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at\n$2048\\times 2048$, while achieving image quality comparable to or surpassing\nbaselines.\n", "link": "http://arxiv.org/abs/2509.26538v1", "date": "2025-09-30", "relevancy": 2.0203, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.7011}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HilbertA%3A%20Hilbert%20Attention%20for%20Image%20Generation%20with%20Diffusion%20Models&body=Title%3A%20HilbertA%3A%20Hilbert%20Attention%20for%20Image%20Generation%20with%20Diffusion%20Models%0AAuthor%3A%20Shaoyi%20Zheng%20and%20Wenbo%20Lu%20and%20Yuxuan%20Xia%20and%20Haomin%20Liu%20and%20Shengjie%20Wang%0AAbstract%3A%20%20%20Designing%20sparse%20attention%20for%20diffusion%20transformers%20requires%20reconciling%0Atwo-dimensional%20spatial%20locality%20with%20GPU%20efficiency%2C%20a%20trade-off%20that%20current%0Amethods%20struggle%20to%20achieve.%20Existing%20approaches%20enforce%20two-dimensional%0Aspatial%20locality%20but%20often%20incur%20uncoalesced%20memory%20access.%20We%20present%0AHilbertA%2C%20a%202D-aware%20and%20GPU-efficient%20sparse%20attention%20mechanism.%20HilbertA%0Areorders%20image%20tokens%20along%20Hilbert%20curves%20to%20achieve%20a%20contiguous%20memory%0Alayout%20while%20preserving%20spatial%20neighborhoods%2C%20and%20employs%20a%20sliding%20schedule%0Aacross%20layers%20to%20enable%20long-range%20information%20propagation%20without%20repeated%20or%0Auncoalesced%20memory%20access.%20To%20further%20enhance%20cross-tile%20communication%20and%0Apositional%20awareness%2C%20HilbertA%20introduces%20a%20small%20central%20shared%20region.%0AImplemented%20in%20Triton%2C%20HilbertA%20delivers%20comparable%20image%20quality%20with%0Asignificant%20acceleration%20over%20prior%20methods%20on%20Flux.1-dev%2C%20demonstrating%20the%0Afeasibility%20of%20hardware-aligned%20two-dimensional%20sparse%20attention%20for%0Ahigh-resolution%20image%20generation.%20HilbertA%20delivers%20attention%20speedups%20of%0A%242.3%5Ctimes%24%20when%20generating%20%241024%5Ctimes%201024%24%20images%2C%20and%20up%20to%20%244.17%5Ctimes%24%20at%0A%242048%5Ctimes%202048%24%2C%20while%20achieving%20image%20quality%20comparable%20to%20or%20surpassing%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHilbertA%253A%2520Hilbert%2520Attention%2520for%2520Image%2520Generation%2520with%2520Diffusion%2520Models%26entry.906535625%3DShaoyi%2520Zheng%2520and%2520Wenbo%2520Lu%2520and%2520Yuxuan%2520Xia%2520and%2520Haomin%2520Liu%2520and%2520Shengjie%2520Wang%26entry.1292438233%3D%2520%2520Designing%2520sparse%2520attention%2520for%2520diffusion%2520transformers%2520requires%2520reconciling%250Atwo-dimensional%2520spatial%2520locality%2520with%2520GPU%2520efficiency%252C%2520a%2520trade-off%2520that%2520current%250Amethods%2520struggle%2520to%2520achieve.%2520Existing%2520approaches%2520enforce%2520two-dimensional%250Aspatial%2520locality%2520but%2520often%2520incur%2520uncoalesced%2520memory%2520access.%2520We%2520present%250AHilbertA%252C%2520a%25202D-aware%2520and%2520GPU-efficient%2520sparse%2520attention%2520mechanism.%2520HilbertA%250Areorders%2520image%2520tokens%2520along%2520Hilbert%2520curves%2520to%2520achieve%2520a%2520contiguous%2520memory%250Alayout%2520while%2520preserving%2520spatial%2520neighborhoods%252C%2520and%2520employs%2520a%2520sliding%2520schedule%250Aacross%2520layers%2520to%2520enable%2520long-range%2520information%2520propagation%2520without%2520repeated%2520or%250Auncoalesced%2520memory%2520access.%2520To%2520further%2520enhance%2520cross-tile%2520communication%2520and%250Apositional%2520awareness%252C%2520HilbertA%2520introduces%2520a%2520small%2520central%2520shared%2520region.%250AImplemented%2520in%2520Triton%252C%2520HilbertA%2520delivers%2520comparable%2520image%2520quality%2520with%250Asignificant%2520acceleration%2520over%2520prior%2520methods%2520on%2520Flux.1-dev%252C%2520demonstrating%2520the%250Afeasibility%2520of%2520hardware-aligned%2520two-dimensional%2520sparse%2520attention%2520for%250Ahigh-resolution%2520image%2520generation.%2520HilbertA%2520delivers%2520attention%2520speedups%2520of%250A%25242.3%255Ctimes%2524%2520when%2520generating%2520%25241024%255Ctimes%25201024%2524%2520images%252C%2520and%2520up%2520to%2520%25244.17%255Ctimes%2524%2520at%250A%25242048%255Ctimes%25202048%2524%252C%2520while%2520achieving%2520image%2520quality%2520comparable%2520to%2520or%2520surpassing%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HilbertA%3A%20Hilbert%20Attention%20for%20Image%20Generation%20with%20Diffusion%20Models&entry.906535625=Shaoyi%20Zheng%20and%20Wenbo%20Lu%20and%20Yuxuan%20Xia%20and%20Haomin%20Liu%20and%20Shengjie%20Wang&entry.1292438233=%20%20Designing%20sparse%20attention%20for%20diffusion%20transformers%20requires%20reconciling%0Atwo-dimensional%20spatial%20locality%20with%20GPU%20efficiency%2C%20a%20trade-off%20that%20current%0Amethods%20struggle%20to%20achieve.%20Existing%20approaches%20enforce%20two-dimensional%0Aspatial%20locality%20but%20often%20incur%20uncoalesced%20memory%20access.%20We%20present%0AHilbertA%2C%20a%202D-aware%20and%20GPU-efficient%20sparse%20attention%20mechanism.%20HilbertA%0Areorders%20image%20tokens%20along%20Hilbert%20curves%20to%20achieve%20a%20contiguous%20memory%0Alayout%20while%20preserving%20spatial%20neighborhoods%2C%20and%20employs%20a%20sliding%20schedule%0Aacross%20layers%20to%20enable%20long-range%20information%20propagation%20without%20repeated%20or%0Auncoalesced%20memory%20access.%20To%20further%20enhance%20cross-tile%20communication%20and%0Apositional%20awareness%2C%20HilbertA%20introduces%20a%20small%20central%20shared%20region.%0AImplemented%20in%20Triton%2C%20HilbertA%20delivers%20comparable%20image%20quality%20with%0Asignificant%20acceleration%20over%20prior%20methods%20on%20Flux.1-dev%2C%20demonstrating%20the%0Afeasibility%20of%20hardware-aligned%20two-dimensional%20sparse%20attention%20for%0Ahigh-resolution%20image%20generation.%20HilbertA%20delivers%20attention%20speedups%20of%0A%242.3%5Ctimes%24%20when%20generating%20%241024%5Ctimes%201024%24%20images%2C%20and%20up%20to%20%244.17%5Ctimes%24%20at%0A%242048%5Ctimes%202048%24%2C%20while%20achieving%20image%20quality%20comparable%20to%20or%20surpassing%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26538v1&entry.124074799=Read"},
{"title": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text\n  Models", "author": "Lina Conti and Dennis Fucci and Marco Gaido and Matteo Negri and Guillaume Wisniewski and Luisa Bentivogli", "abstract": "  Contrastive explanations, which indicate why an AI system produced one output\n(the target) instead of another (the foil), are widely regarded in explainable\nAI as more informative and interpretable than standard explanations. However,\nobtaining such explanations for speech-to-text (S2T) generative models remains\nan open challenge. Drawing from feature attribution techniques, we propose the\nfirst method to obtain contrastive explanations in S2T by analyzing how parts\nof the input spectrogram influence the choice between alternative outputs.\nThrough a case study on gender assignment in speech translation, we show that\nour method accurately identifies the audio features that drive the selection of\none gender over another. By extending the scope of contrastive explanations to\nS2T, our work provides a foundation for better understanding S2T models.\n", "link": "http://arxiv.org/abs/2509.26543v1", "date": "2025-09-30", "relevancy": 2.016, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Unheard%20Alternative%3A%20Contrastive%20Explanations%20for%20Speech-to-Text%0A%20%20Models&body=Title%3A%20The%20Unheard%20Alternative%3A%20Contrastive%20Explanations%20for%20Speech-to-Text%0A%20%20Models%0AAuthor%3A%20Lina%20Conti%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Guillaume%20Wisniewski%20and%20Luisa%20Bentivogli%0AAbstract%3A%20%20%20Contrastive%20explanations%2C%20which%20indicate%20why%20an%20AI%20system%20produced%20one%20output%0A%28the%20target%29%20instead%20of%20another%20%28the%20foil%29%2C%20are%20widely%20regarded%20in%20explainable%0AAI%20as%20more%20informative%20and%20interpretable%20than%20standard%20explanations.%20However%2C%0Aobtaining%20such%20explanations%20for%20speech-to-text%20%28S2T%29%20generative%20models%20remains%0Aan%20open%20challenge.%20Drawing%20from%20feature%20attribution%20techniques%2C%20we%20propose%20the%0Afirst%20method%20to%20obtain%20contrastive%20explanations%20in%20S2T%20by%20analyzing%20how%20parts%0Aof%20the%20input%20spectrogram%20influence%20the%20choice%20between%20alternative%20outputs.%0AThrough%20a%20case%20study%20on%20gender%20assignment%20in%20speech%20translation%2C%20we%20show%20that%0Aour%20method%20accurately%20identifies%20the%20audio%20features%20that%20drive%20the%20selection%20of%0Aone%20gender%20over%20another.%20By%20extending%20the%20scope%20of%20contrastive%20explanations%20to%0AS2T%2C%20our%20work%20provides%20a%20foundation%20for%20better%20understanding%20S2T%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Unheard%2520Alternative%253A%2520Contrastive%2520Explanations%2520for%2520Speech-to-Text%250A%2520%2520Models%26entry.906535625%3DLina%2520Conti%2520and%2520Dennis%2520Fucci%2520and%2520Marco%2520Gaido%2520and%2520Matteo%2520Negri%2520and%2520Guillaume%2520Wisniewski%2520and%2520Luisa%2520Bentivogli%26entry.1292438233%3D%2520%2520Contrastive%2520explanations%252C%2520which%2520indicate%2520why%2520an%2520AI%2520system%2520produced%2520one%2520output%250A%2528the%2520target%2529%2520instead%2520of%2520another%2520%2528the%2520foil%2529%252C%2520are%2520widely%2520regarded%2520in%2520explainable%250AAI%2520as%2520more%2520informative%2520and%2520interpretable%2520than%2520standard%2520explanations.%2520However%252C%250Aobtaining%2520such%2520explanations%2520for%2520speech-to-text%2520%2528S2T%2529%2520generative%2520models%2520remains%250Aan%2520open%2520challenge.%2520Drawing%2520from%2520feature%2520attribution%2520techniques%252C%2520we%2520propose%2520the%250Afirst%2520method%2520to%2520obtain%2520contrastive%2520explanations%2520in%2520S2T%2520by%2520analyzing%2520how%2520parts%250Aof%2520the%2520input%2520spectrogram%2520influence%2520the%2520choice%2520between%2520alternative%2520outputs.%250AThrough%2520a%2520case%2520study%2520on%2520gender%2520assignment%2520in%2520speech%2520translation%252C%2520we%2520show%2520that%250Aour%2520method%2520accurately%2520identifies%2520the%2520audio%2520features%2520that%2520drive%2520the%2520selection%2520of%250Aone%2520gender%2520over%2520another.%2520By%2520extending%2520the%2520scope%2520of%2520contrastive%2520explanations%2520to%250AS2T%252C%2520our%2520work%2520provides%2520a%2520foundation%2520for%2520better%2520understanding%2520S2T%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unheard%20Alternative%3A%20Contrastive%20Explanations%20for%20Speech-to-Text%0A%20%20Models&entry.906535625=Lina%20Conti%20and%20Dennis%20Fucci%20and%20Marco%20Gaido%20and%20Matteo%20Negri%20and%20Guillaume%20Wisniewski%20and%20Luisa%20Bentivogli&entry.1292438233=%20%20Contrastive%20explanations%2C%20which%20indicate%20why%20an%20AI%20system%20produced%20one%20output%0A%28the%20target%29%20instead%20of%20another%20%28the%20foil%29%2C%20are%20widely%20regarded%20in%20explainable%0AAI%20as%20more%20informative%20and%20interpretable%20than%20standard%20explanations.%20However%2C%0Aobtaining%20such%20explanations%20for%20speech-to-text%20%28S2T%29%20generative%20models%20remains%0Aan%20open%20challenge.%20Drawing%20from%20feature%20attribution%20techniques%2C%20we%20propose%20the%0Afirst%20method%20to%20obtain%20contrastive%20explanations%20in%20S2T%20by%20analyzing%20how%20parts%0Aof%20the%20input%20spectrogram%20influence%20the%20choice%20between%20alternative%20outputs.%0AThrough%20a%20case%20study%20on%20gender%20assignment%20in%20speech%20translation%2C%20we%20show%20that%0Aour%20method%20accurately%20identifies%20the%20audio%20features%20that%20drive%20the%20selection%20of%0Aone%20gender%20over%20another.%20By%20extending%20the%20scope%20of%20contrastive%20explanations%20to%0AS2T%2C%20our%20work%20provides%20a%20foundation%20for%20better%20understanding%20S2T%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26543v1&entry.124074799=Read"},
{"title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "author": "Yihao Li and Jiayi Xin and Miranda Muqing Miao and Qi Long and Lyle Ungar", "abstract": "  Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing-alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We show that language mixing can enhance\nreasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage\npoints on MATH500. Additionally, a lightweight probe can be trained to predict\nwhether a potential language switch would benefit or harm reasoning, and when\nused to guide decoding, increases accuracy by 2.92 percentage points. Our\nfindings suggest that language mixing is not merely a byproduct of multilingual\ntraining, but is a strategic reasoning behavior.\n", "link": "http://arxiv.org/abs/2507.15849v2", "date": "2025-09-30", "relevancy": 2.0104, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Language%20Mixing%20on%20Bilingual%20LLM%20Reasoning&body=Title%3A%20The%20Impact%20of%20Language%20Mixing%20on%20Bilingual%20LLM%20Reasoning%0AAuthor%3A%20Yihao%20Li%20and%20Jiayi%20Xin%20and%20Miranda%20Muqing%20Miao%20and%20Qi%20Long%20and%20Lyle%20Ungar%0AAbstract%3A%20%20%20Proficient%20multilingual%20speakers%20often%20intentionally%20switch%20languages%20in%20the%0Amiddle%20of%20a%20conversation.%20Similarly%2C%20recent%20reasoning-focused%20bilingual%20large%0Alanguage%20models%20%28LLMs%29%20with%20strong%20capabilities%20in%20both%20languages%20exhibit%0Alanguage%20mixing-alternating%20languages%20within%20their%20chain%20of%20thought.%0ADiscouraging%20this%20behavior%20in%20DeepSeek-R1%20was%20found%20to%20degrade%20accuracy%2C%0Asuggesting%20that%20language%20mixing%20may%20benefit%20reasoning.%20In%20this%20work%2C%20we%20study%0Alanguage%20switching%20in%20Chinese-English%20bilingual%20reasoning%20models.%20We%20identify%0Areinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20as%20the%20critical%20training%0Astage%20that%20leads%20to%20language%20mixing.%20We%20show%20that%20language%20mixing%20can%20enhance%0Areasoning%3A%20enforcing%20monolingual%20decoding%20reduces%20accuracy%20by%205.6%20percentage%0Apoints%20on%20MATH500.%20Additionally%2C%20a%20lightweight%20probe%20can%20be%20trained%20to%20predict%0Awhether%20a%20potential%20language%20switch%20would%20benefit%20or%20harm%20reasoning%2C%20and%20when%0Aused%20to%20guide%20decoding%2C%20increases%20accuracy%20by%202.92%20percentage%20points.%20Our%0Afindings%20suggest%20that%20language%20mixing%20is%20not%20merely%20a%20byproduct%20of%20multilingual%0Atraining%2C%20but%20is%20a%20strategic%20reasoning%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Language%2520Mixing%2520on%2520Bilingual%2520LLM%2520Reasoning%26entry.906535625%3DYihao%2520Li%2520and%2520Jiayi%2520Xin%2520and%2520Miranda%2520Muqing%2520Miao%2520and%2520Qi%2520Long%2520and%2520Lyle%2520Ungar%26entry.1292438233%3D%2520%2520Proficient%2520multilingual%2520speakers%2520often%2520intentionally%2520switch%2520languages%2520in%2520the%250Amiddle%2520of%2520a%2520conversation.%2520Similarly%252C%2520recent%2520reasoning-focused%2520bilingual%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520with%2520strong%2520capabilities%2520in%2520both%2520languages%2520exhibit%250Alanguage%2520mixing-alternating%2520languages%2520within%2520their%2520chain%2520of%2520thought.%250ADiscouraging%2520this%2520behavior%2520in%2520DeepSeek-R1%2520was%2520found%2520to%2520degrade%2520accuracy%252C%250Asuggesting%2520that%2520language%2520mixing%2520may%2520benefit%2520reasoning.%2520In%2520this%2520work%252C%2520we%2520study%250Alanguage%2520switching%2520in%2520Chinese-English%2520bilingual%2520reasoning%2520models.%2520We%2520identify%250Areinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520as%2520the%2520critical%2520training%250Astage%2520that%2520leads%2520to%2520language%2520mixing.%2520We%2520show%2520that%2520language%2520mixing%2520can%2520enhance%250Areasoning%253A%2520enforcing%2520monolingual%2520decoding%2520reduces%2520accuracy%2520by%25205.6%2520percentage%250Apoints%2520on%2520MATH500.%2520Additionally%252C%2520a%2520lightweight%2520probe%2520can%2520be%2520trained%2520to%2520predict%250Awhether%2520a%2520potential%2520language%2520switch%2520would%2520benefit%2520or%2520harm%2520reasoning%252C%2520and%2520when%250Aused%2520to%2520guide%2520decoding%252C%2520increases%2520accuracy%2520by%25202.92%2520percentage%2520points.%2520Our%250Afindings%2520suggest%2520that%2520language%2520mixing%2520is%2520not%2520merely%2520a%2520byproduct%2520of%2520multilingual%250Atraining%252C%2520but%2520is%2520a%2520strategic%2520reasoning%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Language%20Mixing%20on%20Bilingual%20LLM%20Reasoning&entry.906535625=Yihao%20Li%20and%20Jiayi%20Xin%20and%20Miranda%20Muqing%20Miao%20and%20Qi%20Long%20and%20Lyle%20Ungar&entry.1292438233=%20%20Proficient%20multilingual%20speakers%20often%20intentionally%20switch%20languages%20in%20the%0Amiddle%20of%20a%20conversation.%20Similarly%2C%20recent%20reasoning-focused%20bilingual%20large%0Alanguage%20models%20%28LLMs%29%20with%20strong%20capabilities%20in%20both%20languages%20exhibit%0Alanguage%20mixing-alternating%20languages%20within%20their%20chain%20of%20thought.%0ADiscouraging%20this%20behavior%20in%20DeepSeek-R1%20was%20found%20to%20degrade%20accuracy%2C%0Asuggesting%20that%20language%20mixing%20may%20benefit%20reasoning.%20In%20this%20work%2C%20we%20study%0Alanguage%20switching%20in%20Chinese-English%20bilingual%20reasoning%20models.%20We%20identify%0Areinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20as%20the%20critical%20training%0Astage%20that%20leads%20to%20language%20mixing.%20We%20show%20that%20language%20mixing%20can%20enhance%0Areasoning%3A%20enforcing%20monolingual%20decoding%20reduces%20accuracy%20by%205.6%20percentage%0Apoints%20on%20MATH500.%20Additionally%2C%20a%20lightweight%20probe%20can%20be%20trained%20to%20predict%0Awhether%20a%20potential%20language%20switch%20would%20benefit%20or%20harm%20reasoning%2C%20and%20when%0Aused%20to%20guide%20decoding%2C%20increases%20accuracy%20by%202.92%20percentage%20points.%20Our%0Afindings%20suggest%20that%20language%20mixing%20is%20not%20merely%20a%20byproduct%20of%20multilingual%0Atraining%2C%20but%20is%20a%20strategic%20reasoning%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15849v2&entry.124074799=Read"},
{"title": "Information-Geometric Barycenters for Bayesian Federated Learning", "author": "Nour Jamoussi and Giuseppe Serra and Photios A. Stavrou and Marios Kountouris", "abstract": "  Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration.\n", "link": "http://arxiv.org/abs/2412.11646v3", "date": "2025-09-30", "relevancy": 2.0031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5313}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5124}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning&body=Title%3A%20Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning%0AAuthor%3A%20Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20widely%20used%20and%20impactful%20distributed%0Aoptimization%20framework%20that%20achieves%20consensus%20through%20averaging%20locally%0Atrained%20models.%20While%20effective%2C%20this%20approach%20may%20not%20align%20well%20with%20Bayesian%0Ainference%2C%20where%20the%20model%20space%20has%20the%20structure%20of%20a%20distribution%20space.%0ATaking%20an%20information-geometric%20perspective%2C%20we%20reinterpret%20FL%20aggregation%20as%0Athe%20problem%20of%20finding%20the%20barycenter%20of%20local%20posteriors%20using%20a%20prespecified%0Adivergence%20metric%2C%20minimizing%20the%20average%20discrepancy%20across%20clients.%20This%0Aperspective%20provides%20a%20unifying%20framework%20that%20generalizes%20many%20existing%0Amethods%20and%20offers%20crisp%20insights%20into%20their%20theoretical%20underpinnings.%20We%20then%0Apropose%20BA-BFL%2C%20an%20algorithm%20that%20retains%20the%20convergence%20properties%20of%0AFederated%20Averaging%20in%20non-convex%20settings.%20In%20non-independent%20and%20identically%0Adistributed%20scenarios%2C%20we%20conduct%20extensive%20comparisons%20with%20statistical%0Aaggregation%20techniques%2C%20showing%20that%20BA-BFL%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%20while%20offering%20a%20geometric%20interpretation%20of%20the%0Aaggregation%20phase.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20Hybrid%20Bayesian%20Deep%0ALearning%2C%20exploring%20the%20impact%20of%20Bayesian%20layers%20on%20uncertainty%20quantification%0Aand%20model%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11646v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Geometric%2520Barycenters%2520for%2520Bayesian%2520Federated%2520Learning%26entry.906535625%3DNour%2520Jamoussi%2520and%2520Giuseppe%2520Serra%2520and%2520Photios%2520A.%2520Stavrou%2520and%2520Marios%2520Kountouris%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520widely%2520used%2520and%2520impactful%2520distributed%250Aoptimization%2520framework%2520that%2520achieves%2520consensus%2520through%2520averaging%2520locally%250Atrained%2520models.%2520While%2520effective%252C%2520this%2520approach%2520may%2520not%2520align%2520well%2520with%2520Bayesian%250Ainference%252C%2520where%2520the%2520model%2520space%2520has%2520the%2520structure%2520of%2520a%2520distribution%2520space.%250ATaking%2520an%2520information-geometric%2520perspective%252C%2520we%2520reinterpret%2520FL%2520aggregation%2520as%250Athe%2520problem%2520of%2520finding%2520the%2520barycenter%2520of%2520local%2520posteriors%2520using%2520a%2520prespecified%250Adivergence%2520metric%252C%2520minimizing%2520the%2520average%2520discrepancy%2520across%2520clients.%2520This%250Aperspective%2520provides%2520a%2520unifying%2520framework%2520that%2520generalizes%2520many%2520existing%250Amethods%2520and%2520offers%2520crisp%2520insights%2520into%2520their%2520theoretical%2520underpinnings.%2520We%2520then%250Apropose%2520BA-BFL%252C%2520an%2520algorithm%2520that%2520retains%2520the%2520convergence%2520properties%2520of%250AFederated%2520Averaging%2520in%2520non-convex%2520settings.%2520In%2520non-independent%2520and%2520identically%250Adistributed%2520scenarios%252C%2520we%2520conduct%2520extensive%2520comparisons%2520with%2520statistical%250Aaggregation%2520techniques%252C%2520showing%2520that%2520BA-BFL%2520achieves%2520performance%2520comparable%2520to%250Astate-of-the-art%2520methods%2520while%2520offering%2520a%2520geometric%2520interpretation%2520of%2520the%250Aaggregation%2520phase.%2520Additionally%252C%2520we%2520extend%2520our%2520analysis%2520to%2520Hybrid%2520Bayesian%2520Deep%250ALearning%252C%2520exploring%2520the%2520impact%2520of%2520Bayesian%2520layers%2520on%2520uncertainty%2520quantification%250Aand%2520model%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11646v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning&entry.906535625=Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20widely%20used%20and%20impactful%20distributed%0Aoptimization%20framework%20that%20achieves%20consensus%20through%20averaging%20locally%0Atrained%20models.%20While%20effective%2C%20this%20approach%20may%20not%20align%20well%20with%20Bayesian%0Ainference%2C%20where%20the%20model%20space%20has%20the%20structure%20of%20a%20distribution%20space.%0ATaking%20an%20information-geometric%20perspective%2C%20we%20reinterpret%20FL%20aggregation%20as%0Athe%20problem%20of%20finding%20the%20barycenter%20of%20local%20posteriors%20using%20a%20prespecified%0Adivergence%20metric%2C%20minimizing%20the%20average%20discrepancy%20across%20clients.%20This%0Aperspective%20provides%20a%20unifying%20framework%20that%20generalizes%20many%20existing%0Amethods%20and%20offers%20crisp%20insights%20into%20their%20theoretical%20underpinnings.%20We%20then%0Apropose%20BA-BFL%2C%20an%20algorithm%20that%20retains%20the%20convergence%20properties%20of%0AFederated%20Averaging%20in%20non-convex%20settings.%20In%20non-independent%20and%20identically%0Adistributed%20scenarios%2C%20we%20conduct%20extensive%20comparisons%20with%20statistical%0Aaggregation%20techniques%2C%20showing%20that%20BA-BFL%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%20while%20offering%20a%20geometric%20interpretation%20of%20the%0Aaggregation%20phase.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20Hybrid%20Bayesian%20Deep%0ALearning%2C%20exploring%20the%20impact%20of%20Bayesian%20layers%20on%20uncertainty%20quantification%0Aand%20model%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11646v3&entry.124074799=Read"},
{"title": "Linking Process to Outcome: Conditional Reward Modeling for LLM\n  Reasoning", "author": "Zheng Zhang and Ziwei Shan and Kaitao Song and Yexin Li and Kan Ren", "abstract": "  Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.\n", "link": "http://arxiv.org/abs/2509.26578v1", "date": "2025-09-30", "relevancy": 1.9983, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linking%20Process%20to%20Outcome%3A%20Conditional%20Reward%20Modeling%20for%20LLM%0A%20%20Reasoning&body=Title%3A%20Linking%20Process%20to%20Outcome%3A%20Conditional%20Reward%20Modeling%20for%20LLM%0A%20%20Reasoning%0AAuthor%3A%20Zheng%20Zhang%20and%20Ziwei%20Shan%20and%20Kaitao%20Song%20and%20Yexin%20Li%20and%20Kan%20Ren%0AAbstract%3A%20%20%20Process%20Reward%20Models%20%28PRMs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20enhance%0Athe%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20guiding%20their%0Astep-by-step%20reasoning%20toward%20a%20final%20answer.%20However%2C%20existing%20PRMs%20either%0Atreat%20each%20reasoning%20step%20in%20isolation%2C%20failing%20to%20capture%20inter-step%0Adependencies%2C%20or%20struggle%20to%20align%20process%20rewards%20with%20the%20final%20outcome.%0AConsequently%2C%20the%20reward%20signal%20fails%20to%20respect%20temporal%20causality%20in%0Asequential%20reasoning%20and%20faces%20ambiguous%20credit%20assignment.%20These%20limitations%0Amake%20downstream%20models%20vulnerable%20to%20reward%20hacking%20and%20lead%20to%20suboptimal%0Aperformance.%20In%20this%20work%2C%20we%20propose%20Conditional%20Reward%20Modeling%20%28CRM%29%20that%0Aframes%20LLM%20reasoning%20as%20a%20temporal%20process%20leading%20to%20a%20correct%20answer.%20The%0Areward%20of%20each%20reasoning%20step%20is%20not%20only%20conditioned%20on%20the%20preceding%20steps%0Abut%20also%20explicitly%20linked%20to%20the%20final%20outcome%20of%20the%20reasoning%20trajectory.%20By%0Aenforcing%20conditional%20probability%20rules%2C%20our%20design%20captures%20the%20causal%0Arelationships%20among%20reasoning%20steps%2C%20with%20the%20link%20to%20the%20outcome%20allowing%0Aprecise%20attribution%20of%20each%20intermediate%20step%2C%20thereby%20resolving%20credit%0Aassignment%20ambiguity.%20Further%2C%20through%20this%20consistent%20probabilistic%20modeling%2C%0Athe%20rewards%20produced%20by%20CRM%20enable%20more%20reliable%20cross-sample%20comparison.%0AExperiments%20across%20Best-of-N%20sampling%2C%20beam%20search%20and%20reinforcement%20learning%0Ademonstrate%20that%20CRM%20consistently%20outperforms%20existing%20reward%20models%2C%20offering%0Aa%20principled%20framework%20for%20enhancing%20LLM%20reasoning.%20In%20particular%2C%20CRM%20is%20more%0Arobust%20to%20reward%20hacking%20and%20delivers%20stable%20downstream%20improvements%20without%0Arelying%20on%20verifiable%20rewards%20derived%20from%20ground%20truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinking%2520Process%2520to%2520Outcome%253A%2520Conditional%2520Reward%2520Modeling%2520for%2520LLM%250A%2520%2520Reasoning%26entry.906535625%3DZheng%2520Zhang%2520and%2520Ziwei%2520Shan%2520and%2520Kaitao%2520Song%2520and%2520Yexin%2520Li%2520and%2520Kan%2520Ren%26entry.1292438233%3D%2520%2520Process%2520Reward%2520Models%2520%2528PRMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520enhance%250Athe%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520guiding%2520their%250Astep-by-step%2520reasoning%2520toward%2520a%2520final%2520answer.%2520However%252C%2520existing%2520PRMs%2520either%250Atreat%2520each%2520reasoning%2520step%2520in%2520isolation%252C%2520failing%2520to%2520capture%2520inter-step%250Adependencies%252C%2520or%2520struggle%2520to%2520align%2520process%2520rewards%2520with%2520the%2520final%2520outcome.%250AConsequently%252C%2520the%2520reward%2520signal%2520fails%2520to%2520respect%2520temporal%2520causality%2520in%250Asequential%2520reasoning%2520and%2520faces%2520ambiguous%2520credit%2520assignment.%2520These%2520limitations%250Amake%2520downstream%2520models%2520vulnerable%2520to%2520reward%2520hacking%2520and%2520lead%2520to%2520suboptimal%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520propose%2520Conditional%2520Reward%2520Modeling%2520%2528CRM%2529%2520that%250Aframes%2520LLM%2520reasoning%2520as%2520a%2520temporal%2520process%2520leading%2520to%2520a%2520correct%2520answer.%2520The%250Areward%2520of%2520each%2520reasoning%2520step%2520is%2520not%2520only%2520conditioned%2520on%2520the%2520preceding%2520steps%250Abut%2520also%2520explicitly%2520linked%2520to%2520the%2520final%2520outcome%2520of%2520the%2520reasoning%2520trajectory.%2520By%250Aenforcing%2520conditional%2520probability%2520rules%252C%2520our%2520design%2520captures%2520the%2520causal%250Arelationships%2520among%2520reasoning%2520steps%252C%2520with%2520the%2520link%2520to%2520the%2520outcome%2520allowing%250Aprecise%2520attribution%2520of%2520each%2520intermediate%2520step%252C%2520thereby%2520resolving%2520credit%250Aassignment%2520ambiguity.%2520Further%252C%2520through%2520this%2520consistent%2520probabilistic%2520modeling%252C%250Athe%2520rewards%2520produced%2520by%2520CRM%2520enable%2520more%2520reliable%2520cross-sample%2520comparison.%250AExperiments%2520across%2520Best-of-N%2520sampling%252C%2520beam%2520search%2520and%2520reinforcement%2520learning%250Ademonstrate%2520that%2520CRM%2520consistently%2520outperforms%2520existing%2520reward%2520models%252C%2520offering%250Aa%2520principled%2520framework%2520for%2520enhancing%2520LLM%2520reasoning.%2520In%2520particular%252C%2520CRM%2520is%2520more%250Arobust%2520to%2520reward%2520hacking%2520and%2520delivers%2520stable%2520downstream%2520improvements%2520without%250Arelying%2520on%2520verifiable%2520rewards%2520derived%2520from%2520ground%2520truth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linking%20Process%20to%20Outcome%3A%20Conditional%20Reward%20Modeling%20for%20LLM%0A%20%20Reasoning&entry.906535625=Zheng%20Zhang%20and%20Ziwei%20Shan%20and%20Kaitao%20Song%20and%20Yexin%20Li%20and%20Kan%20Ren&entry.1292438233=%20%20Process%20Reward%20Models%20%28PRMs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20enhance%0Athe%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20guiding%20their%0Astep-by-step%20reasoning%20toward%20a%20final%20answer.%20However%2C%20existing%20PRMs%20either%0Atreat%20each%20reasoning%20step%20in%20isolation%2C%20failing%20to%20capture%20inter-step%0Adependencies%2C%20or%20struggle%20to%20align%20process%20rewards%20with%20the%20final%20outcome.%0AConsequently%2C%20the%20reward%20signal%20fails%20to%20respect%20temporal%20causality%20in%0Asequential%20reasoning%20and%20faces%20ambiguous%20credit%20assignment.%20These%20limitations%0Amake%20downstream%20models%20vulnerable%20to%20reward%20hacking%20and%20lead%20to%20suboptimal%0Aperformance.%20In%20this%20work%2C%20we%20propose%20Conditional%20Reward%20Modeling%20%28CRM%29%20that%0Aframes%20LLM%20reasoning%20as%20a%20temporal%20process%20leading%20to%20a%20correct%20answer.%20The%0Areward%20of%20each%20reasoning%20step%20is%20not%20only%20conditioned%20on%20the%20preceding%20steps%0Abut%20also%20explicitly%20linked%20to%20the%20final%20outcome%20of%20the%20reasoning%20trajectory.%20By%0Aenforcing%20conditional%20probability%20rules%2C%20our%20design%20captures%20the%20causal%0Arelationships%20among%20reasoning%20steps%2C%20with%20the%20link%20to%20the%20outcome%20allowing%0Aprecise%20attribution%20of%20each%20intermediate%20step%2C%20thereby%20resolving%20credit%0Aassignment%20ambiguity.%20Further%2C%20through%20this%20consistent%20probabilistic%20modeling%2C%0Athe%20rewards%20produced%20by%20CRM%20enable%20more%20reliable%20cross-sample%20comparison.%0AExperiments%20across%20Best-of-N%20sampling%2C%20beam%20search%20and%20reinforcement%20learning%0Ademonstrate%20that%20CRM%20consistently%20outperforms%20existing%20reward%20models%2C%20offering%0Aa%20principled%20framework%20for%20enhancing%20LLM%20reasoning.%20In%20particular%2C%20CRM%20is%20more%0Arobust%20to%20reward%20hacking%20and%20delivers%20stable%20downstream%20improvements%20without%0Arelying%20on%20verifiable%20rewards%20derived%20from%20ground%20truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26578v1&entry.124074799=Read"},
{"title": "Convergence and Divergence of Language Models under Different Random\n  Seeds", "author": "Finlay Fehlauer and Kyle Mahowald and Tiago Pimentel", "abstract": "  In this paper, we investigate the convergence of language models (LMs)\ntrained under different random seeds, measuring convergence as the expected\nper-token Kullback--Leibler (KL) divergence across seeds. By comparing LM\nconvergence as a function of model size and training checkpoint, we identify a\nfour-phase convergence pattern: (i) an initial uniform phase, (ii) a\nsharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a\nslow-reconvergence phase. Further, we observe that larger models reconverge\nfaster in later training stages, while smaller models never actually\nreconverge; these results suggest that a certain model size may be necessary to\nlearn stable distributions. Restricting our analysis to specific token\nfrequencies or part-of-speech (PoS) tags further reveals that convergence is\nuneven across linguistic categories: frequent tokens and function words\nconverge faster and more reliably than their counterparts (infrequent tokens\nand content words). Overall, our findings highlight factors that influence the\nstability of the learned distributions in model training.\n", "link": "http://arxiv.org/abs/2509.26643v1", "date": "2025-09-30", "relevancy": 1.9921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5067}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20and%20Divergence%20of%20Language%20Models%20under%20Different%20Random%0A%20%20Seeds&body=Title%3A%20Convergence%20and%20Divergence%20of%20Language%20Models%20under%20Different%20Random%0A%20%20Seeds%0AAuthor%3A%20Finlay%20Fehlauer%20and%20Kyle%20Mahowald%20and%20Tiago%20Pimentel%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20convergence%20of%20language%20models%20%28LMs%29%0Atrained%20under%20different%20random%20seeds%2C%20measuring%20convergence%20as%20the%20expected%0Aper-token%20Kullback--Leibler%20%28KL%29%20divergence%20across%20seeds.%20By%20comparing%20LM%0Aconvergence%20as%20a%20function%20of%20model%20size%20and%20training%20checkpoint%2C%20we%20identify%20a%0Afour-phase%20convergence%20pattern%3A%20%28i%29%20an%20initial%20uniform%20phase%2C%20%28ii%29%20a%0Asharp-convergence%20phase%2C%20%28iii%29%20a%20sharp-divergence%20phase%2C%20and%20%28iv%29%20a%0Aslow-reconvergence%20phase.%20Further%2C%20we%20observe%20that%20larger%20models%20reconverge%0Afaster%20in%20later%20training%20stages%2C%20while%20smaller%20models%20never%20actually%0Areconverge%3B%20these%20results%20suggest%20that%20a%20certain%20model%20size%20may%20be%20necessary%20to%0Alearn%20stable%20distributions.%20Restricting%20our%20analysis%20to%20specific%20token%0Afrequencies%20or%20part-of-speech%20%28PoS%29%20tags%20further%20reveals%20that%20convergence%20is%0Auneven%20across%20linguistic%20categories%3A%20frequent%20tokens%20and%20function%20words%0Aconverge%20faster%20and%20more%20reliably%20than%20their%20counterparts%20%28infrequent%20tokens%0Aand%20content%20words%29.%20Overall%2C%20our%20findings%20highlight%20factors%20that%20influence%20the%0Astability%20of%20the%20learned%20distributions%20in%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520and%2520Divergence%2520of%2520Language%2520Models%2520under%2520Different%2520Random%250A%2520%2520Seeds%26entry.906535625%3DFinlay%2520Fehlauer%2520and%2520Kyle%2520Mahowald%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520convergence%2520of%2520language%2520models%2520%2528LMs%2529%250Atrained%2520under%2520different%2520random%2520seeds%252C%2520measuring%2520convergence%2520as%2520the%2520expected%250Aper-token%2520Kullback--Leibler%2520%2528KL%2529%2520divergence%2520across%2520seeds.%2520By%2520comparing%2520LM%250Aconvergence%2520as%2520a%2520function%2520of%2520model%2520size%2520and%2520training%2520checkpoint%252C%2520we%2520identify%2520a%250Afour-phase%2520convergence%2520pattern%253A%2520%2528i%2529%2520an%2520initial%2520uniform%2520phase%252C%2520%2528ii%2529%2520a%250Asharp-convergence%2520phase%252C%2520%2528iii%2529%2520a%2520sharp-divergence%2520phase%252C%2520and%2520%2528iv%2529%2520a%250Aslow-reconvergence%2520phase.%2520Further%252C%2520we%2520observe%2520that%2520larger%2520models%2520reconverge%250Afaster%2520in%2520later%2520training%2520stages%252C%2520while%2520smaller%2520models%2520never%2520actually%250Areconverge%253B%2520these%2520results%2520suggest%2520that%2520a%2520certain%2520model%2520size%2520may%2520be%2520necessary%2520to%250Alearn%2520stable%2520distributions.%2520Restricting%2520our%2520analysis%2520to%2520specific%2520token%250Afrequencies%2520or%2520part-of-speech%2520%2528PoS%2529%2520tags%2520further%2520reveals%2520that%2520convergence%2520is%250Auneven%2520across%2520linguistic%2520categories%253A%2520frequent%2520tokens%2520and%2520function%2520words%250Aconverge%2520faster%2520and%2520more%2520reliably%2520than%2520their%2520counterparts%2520%2528infrequent%2520tokens%250Aand%2520content%2520words%2529.%2520Overall%252C%2520our%2520findings%2520highlight%2520factors%2520that%2520influence%2520the%250Astability%2520of%2520the%2520learned%2520distributions%2520in%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20and%20Divergence%20of%20Language%20Models%20under%20Different%20Random%0A%20%20Seeds&entry.906535625=Finlay%20Fehlauer%20and%20Kyle%20Mahowald%20and%20Tiago%20Pimentel&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20convergence%20of%20language%20models%20%28LMs%29%0Atrained%20under%20different%20random%20seeds%2C%20measuring%20convergence%20as%20the%20expected%0Aper-token%20Kullback--Leibler%20%28KL%29%20divergence%20across%20seeds.%20By%20comparing%20LM%0Aconvergence%20as%20a%20function%20of%20model%20size%20and%20training%20checkpoint%2C%20we%20identify%20a%0Afour-phase%20convergence%20pattern%3A%20%28i%29%20an%20initial%20uniform%20phase%2C%20%28ii%29%20a%0Asharp-convergence%20phase%2C%20%28iii%29%20a%20sharp-divergence%20phase%2C%20and%20%28iv%29%20a%0Aslow-reconvergence%20phase.%20Further%2C%20we%20observe%20that%20larger%20models%20reconverge%0Afaster%20in%20later%20training%20stages%2C%20while%20smaller%20models%20never%20actually%0Areconverge%3B%20these%20results%20suggest%20that%20a%20certain%20model%20size%20may%20be%20necessary%20to%0Alearn%20stable%20distributions.%20Restricting%20our%20analysis%20to%20specific%20token%0Afrequencies%20or%20part-of-speech%20%28PoS%29%20tags%20further%20reveals%20that%20convergence%20is%0Auneven%20across%20linguistic%20categories%3A%20frequent%20tokens%20and%20function%20words%0Aconverge%20faster%20and%20more%20reliably%20than%20their%20counterparts%20%28infrequent%20tokens%0Aand%20content%20words%29.%20Overall%2C%20our%20findings%20highlight%20factors%20that%20influence%20the%0Astability%20of%20the%20learned%20distributions%20in%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26643v1&entry.124074799=Read"},
{"title": "FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit\n  Neural Representation with Periodic Activation", "author": "Mohammed Alsakabi and Wael Mobeirek and John M. Dolan and Ozan K. Tonguz", "abstract": "  Existing periodic activation-based implicit neural representation (INR)\nnetworks, such as SIREN and FINER, suffer from hidden feature redundancy, where\nneurons within a layer capture overlapping frequency components due to the use\nof a fixed frequency multiplier. This redundancy limits the expressive capacity\nof multilayer perceptrons (MLPs). Drawing inspiration from classical signal\nprocessing methods such as the Discrete Sine Transform (DST), we propose\nFM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency\nmultipliers to periodic activations. Unlike existing approaches, our design\nintroduces frequency diversity without requiring hyperparameter tuning or\nadditional network depth. This simple yet principled modification reduces the\nredundancy of features by nearly 50% and consistently improves signal\nreconstruction across diverse INR tasks, including fitting 1D audio, 2D image\nand 3D shape, and synthesis of neural radiance fields (NeRF), outperforming\ntheir baseline counterparts while maintaining efficiency.\n", "link": "http://arxiv.org/abs/2509.23438v2", "date": "2025-09-30", "relevancy": 1.9744, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5046}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4896}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FM-SIREN%20%26%20FM-FINER%3A%20Nyquist-Informed%20Frequency%20Multiplier%20for%20Implicit%0A%20%20Neural%20Representation%20with%20Periodic%20Activation&body=Title%3A%20FM-SIREN%20%26%20FM-FINER%3A%20Nyquist-Informed%20Frequency%20Multiplier%20for%20Implicit%0A%20%20Neural%20Representation%20with%20Periodic%20Activation%0AAuthor%3A%20Mohammed%20Alsakabi%20and%20Wael%20Mobeirek%20and%20John%20M.%20Dolan%20and%20Ozan%20K.%20Tonguz%0AAbstract%3A%20%20%20Existing%20periodic%20activation-based%20implicit%20neural%20representation%20%28INR%29%0Anetworks%2C%20such%20as%20SIREN%20and%20FINER%2C%20suffer%20from%20hidden%20feature%20redundancy%2C%20where%0Aneurons%20within%20a%20layer%20capture%20overlapping%20frequency%20components%20due%20to%20the%20use%0Aof%20a%20fixed%20frequency%20multiplier.%20This%20redundancy%20limits%20the%20expressive%20capacity%0Aof%20multilayer%20perceptrons%20%28MLPs%29.%20Drawing%20inspiration%20from%20classical%20signal%0Aprocessing%20methods%20such%20as%20the%20Discrete%20Sine%20Transform%20%28DST%29%2C%20we%20propose%0AFM-SIREN%20and%20FM-FINER%2C%20which%20assign%20Nyquist-informed%2C%20neuron-specific%20frequency%0Amultipliers%20to%20periodic%20activations.%20Unlike%20existing%20approaches%2C%20our%20design%0Aintroduces%20frequency%20diversity%20without%20requiring%20hyperparameter%20tuning%20or%0Aadditional%20network%20depth.%20This%20simple%20yet%20principled%20modification%20reduces%20the%0Aredundancy%20of%20features%20by%20nearly%2050%25%20and%20consistently%20improves%20signal%0Areconstruction%20across%20diverse%20INR%20tasks%2C%20including%20fitting%201D%20audio%2C%202D%20image%0Aand%203D%20shape%2C%20and%20synthesis%20of%20neural%20radiance%20fields%20%28NeRF%29%2C%20outperforming%0Atheir%20baseline%20counterparts%20while%20maintaining%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.23438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFM-SIREN%2520%2526%2520FM-FINER%253A%2520Nyquist-Informed%2520Frequency%2520Multiplier%2520for%2520Implicit%250A%2520%2520Neural%2520Representation%2520with%2520Periodic%2520Activation%26entry.906535625%3DMohammed%2520Alsakabi%2520and%2520Wael%2520Mobeirek%2520and%2520John%2520M.%2520Dolan%2520and%2520Ozan%2520K.%2520Tonguz%26entry.1292438233%3D%2520%2520Existing%2520periodic%2520activation-based%2520implicit%2520neural%2520representation%2520%2528INR%2529%250Anetworks%252C%2520such%2520as%2520SIREN%2520and%2520FINER%252C%2520suffer%2520from%2520hidden%2520feature%2520redundancy%252C%2520where%250Aneurons%2520within%2520a%2520layer%2520capture%2520overlapping%2520frequency%2520components%2520due%2520to%2520the%2520use%250Aof%2520a%2520fixed%2520frequency%2520multiplier.%2520This%2520redundancy%2520limits%2520the%2520expressive%2520capacity%250Aof%2520multilayer%2520perceptrons%2520%2528MLPs%2529.%2520Drawing%2520inspiration%2520from%2520classical%2520signal%250Aprocessing%2520methods%2520such%2520as%2520the%2520Discrete%2520Sine%2520Transform%2520%2528DST%2529%252C%2520we%2520propose%250AFM-SIREN%2520and%2520FM-FINER%252C%2520which%2520assign%2520Nyquist-informed%252C%2520neuron-specific%2520frequency%250Amultipliers%2520to%2520periodic%2520activations.%2520Unlike%2520existing%2520approaches%252C%2520our%2520design%250Aintroduces%2520frequency%2520diversity%2520without%2520requiring%2520hyperparameter%2520tuning%2520or%250Aadditional%2520network%2520depth.%2520This%2520simple%2520yet%2520principled%2520modification%2520reduces%2520the%250Aredundancy%2520of%2520features%2520by%2520nearly%252050%2525%2520and%2520consistently%2520improves%2520signal%250Areconstruction%2520across%2520diverse%2520INR%2520tasks%252C%2520including%2520fitting%25201D%2520audio%252C%25202D%2520image%250Aand%25203D%2520shape%252C%2520and%2520synthesis%2520of%2520neural%2520radiance%2520fields%2520%2528NeRF%2529%252C%2520outperforming%250Atheir%2520baseline%2520counterparts%2520while%2520maintaining%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FM-SIREN%20%26%20FM-FINER%3A%20Nyquist-Informed%20Frequency%20Multiplier%20for%20Implicit%0A%20%20Neural%20Representation%20with%20Periodic%20Activation&entry.906535625=Mohammed%20Alsakabi%20and%20Wael%20Mobeirek%20and%20John%20M.%20Dolan%20and%20Ozan%20K.%20Tonguz&entry.1292438233=%20%20Existing%20periodic%20activation-based%20implicit%20neural%20representation%20%28INR%29%0Anetworks%2C%20such%20as%20SIREN%20and%20FINER%2C%20suffer%20from%20hidden%20feature%20redundancy%2C%20where%0Aneurons%20within%20a%20layer%20capture%20overlapping%20frequency%20components%20due%20to%20the%20use%0Aof%20a%20fixed%20frequency%20multiplier.%20This%20redundancy%20limits%20the%20expressive%20capacity%0Aof%20multilayer%20perceptrons%20%28MLPs%29.%20Drawing%20inspiration%20from%20classical%20signal%0Aprocessing%20methods%20such%20as%20the%20Discrete%20Sine%20Transform%20%28DST%29%2C%20we%20propose%0AFM-SIREN%20and%20FM-FINER%2C%20which%20assign%20Nyquist-informed%2C%20neuron-specific%20frequency%0Amultipliers%20to%20periodic%20activations.%20Unlike%20existing%20approaches%2C%20our%20design%0Aintroduces%20frequency%20diversity%20without%20requiring%20hyperparameter%20tuning%20or%0Aadditional%20network%20depth.%20This%20simple%20yet%20principled%20modification%20reduces%20the%0Aredundancy%20of%20features%20by%20nearly%2050%25%20and%20consistently%20improves%20signal%0Areconstruction%20across%20diverse%20INR%20tasks%2C%20including%20fitting%201D%20audio%2C%202D%20image%0Aand%203D%20shape%2C%20and%20synthesis%20of%20neural%20radiance%20fields%20%28NeRF%29%2C%20outperforming%0Atheir%20baseline%20counterparts%20while%20maintaining%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.23438v2&entry.124074799=Read"},
{"title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively", "author": "Yixuan Weng and Minjun Zhu and Qiujie Xie and Qiyao Sun and Zhen Lin and Sifan Liu and Yue Zhang", "abstract": "  While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.\n", "link": "http://arxiv.org/abs/2509.26603v1", "date": "2025-09-30", "relevancy": 1.9122, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5163}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4797}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepScientist%3A%20Advancing%20Frontier-Pushing%20Scientific%20Findings%0A%20%20Progressively&body=Title%3A%20DeepScientist%3A%20Advancing%20Frontier-Pushing%20Scientific%20Findings%0A%20%20Progressively%0AAuthor%3A%20Yixuan%20Weng%20and%20Minjun%20Zhu%20and%20Qiujie%20Xie%20and%20Qiyao%20Sun%20and%20Zhen%20Lin%20and%20Sifan%20Liu%20and%20Yue%20Zhang%0AAbstract%3A%20%20%20While%20previous%20AI%20Scientist%20systems%20can%20generate%20novel%20findings%2C%20they%20often%0Alack%20the%20focus%20to%20produce%20scientifically%20valuable%20contributions%20that%20address%0Apressing%20human-defined%20challenges.%20We%20introduce%20DeepScientist%2C%20a%20system%0Adesigned%20to%20overcome%20this%20by%20conducting%20goal-oriented%2C%20fully%20autonomous%0Ascientific%20discovery%20over%20month-long%20timelines.%20It%20formalizes%20discovery%20as%20a%0ABayesian%20Optimization%20problem%2C%20operationalized%20through%20a%20hierarchical%0Aevaluation%20process%20consisting%20of%20%22hypothesize%2C%20verify%2C%20and%20analyze%22.%20Leveraging%0Aa%20cumulative%20Findings%20Memory%2C%20this%20loop%20intelligently%20balances%20the%20exploration%0Aof%20novel%20hypotheses%20with%20exploitation%2C%20selectively%20promoting%20the%20most%20promising%0Afindings%20to%20higher-fidelity%20levels%20of%20validation.%20Consuming%20over%2020%2C000%20GPU%0Ahours%2C%20the%20system%20generated%20about%205%2C000%20unique%20scientific%20ideas%20and%0Aexperimentally%20validated%20approximately%201100%20of%20them%2C%20ultimately%20surpassing%0Ahuman-designed%20state-of-the-art%20%28SOTA%29%20methods%20on%20three%20frontier%20AI%20tasks%20by%0A183.7%5C%25%2C%201.9%5C%25%2C%20and%207.9%5C%25.%20This%20work%20provides%20the%20first%20large-scale%20evidence%20of%0Aan%20AI%20achieving%20discoveries%20that%20progressively%20surpass%20human%20SOTA%20on%20scientific%0Atasks%2C%20producing%20valuable%20findings%20that%20genuinely%20push%20the%20frontier%20of%0Ascientific%20discovery.%20To%20facilitate%20further%20research%20into%20this%20process%2C%20we%20will%0Aopen-source%20all%20experimental%20logs%20and%20system%20code%20at%0Ahttps%3A//github.com/ResearAI/DeepScientist/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepScientist%253A%2520Advancing%2520Frontier-Pushing%2520Scientific%2520Findings%250A%2520%2520Progressively%26entry.906535625%3DYixuan%2520Weng%2520and%2520Minjun%2520Zhu%2520and%2520Qiujie%2520Xie%2520and%2520Qiyao%2520Sun%2520and%2520Zhen%2520Lin%2520and%2520Sifan%2520Liu%2520and%2520Yue%2520Zhang%26entry.1292438233%3D%2520%2520While%2520previous%2520AI%2520Scientist%2520systems%2520can%2520generate%2520novel%2520findings%252C%2520they%2520often%250Alack%2520the%2520focus%2520to%2520produce%2520scientifically%2520valuable%2520contributions%2520that%2520address%250Apressing%2520human-defined%2520challenges.%2520We%2520introduce%2520DeepScientist%252C%2520a%2520system%250Adesigned%2520to%2520overcome%2520this%2520by%2520conducting%2520goal-oriented%252C%2520fully%2520autonomous%250Ascientific%2520discovery%2520over%2520month-long%2520timelines.%2520It%2520formalizes%2520discovery%2520as%2520a%250ABayesian%2520Optimization%2520problem%252C%2520operationalized%2520through%2520a%2520hierarchical%250Aevaluation%2520process%2520consisting%2520of%2520%2522hypothesize%252C%2520verify%252C%2520and%2520analyze%2522.%2520Leveraging%250Aa%2520cumulative%2520Findings%2520Memory%252C%2520this%2520loop%2520intelligently%2520balances%2520the%2520exploration%250Aof%2520novel%2520hypotheses%2520with%2520exploitation%252C%2520selectively%2520promoting%2520the%2520most%2520promising%250Afindings%2520to%2520higher-fidelity%2520levels%2520of%2520validation.%2520Consuming%2520over%252020%252C000%2520GPU%250Ahours%252C%2520the%2520system%2520generated%2520about%25205%252C000%2520unique%2520scientific%2520ideas%2520and%250Aexperimentally%2520validated%2520approximately%25201100%2520of%2520them%252C%2520ultimately%2520surpassing%250Ahuman-designed%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520three%2520frontier%2520AI%2520tasks%2520by%250A183.7%255C%2525%252C%25201.9%255C%2525%252C%2520and%25207.9%255C%2525.%2520This%2520work%2520provides%2520the%2520first%2520large-scale%2520evidence%2520of%250Aan%2520AI%2520achieving%2520discoveries%2520that%2520progressively%2520surpass%2520human%2520SOTA%2520on%2520scientific%250Atasks%252C%2520producing%2520valuable%2520findings%2520that%2520genuinely%2520push%2520the%2520frontier%2520of%250Ascientific%2520discovery.%2520To%2520facilitate%2520further%2520research%2520into%2520this%2520process%252C%2520we%2520will%250Aopen-source%2520all%2520experimental%2520logs%2520and%2520system%2520code%2520at%250Ahttps%253A//github.com/ResearAI/DeepScientist/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepScientist%3A%20Advancing%20Frontier-Pushing%20Scientific%20Findings%0A%20%20Progressively&entry.906535625=Yixuan%20Weng%20and%20Minjun%20Zhu%20and%20Qiujie%20Xie%20and%20Qiyao%20Sun%20and%20Zhen%20Lin%20and%20Sifan%20Liu%20and%20Yue%20Zhang&entry.1292438233=%20%20While%20previous%20AI%20Scientist%20systems%20can%20generate%20novel%20findings%2C%20they%20often%0Alack%20the%20focus%20to%20produce%20scientifically%20valuable%20contributions%20that%20address%0Apressing%20human-defined%20challenges.%20We%20introduce%20DeepScientist%2C%20a%20system%0Adesigned%20to%20overcome%20this%20by%20conducting%20goal-oriented%2C%20fully%20autonomous%0Ascientific%20discovery%20over%20month-long%20timelines.%20It%20formalizes%20discovery%20as%20a%0ABayesian%20Optimization%20problem%2C%20operationalized%20through%20a%20hierarchical%0Aevaluation%20process%20consisting%20of%20%22hypothesize%2C%20verify%2C%20and%20analyze%22.%20Leveraging%0Aa%20cumulative%20Findings%20Memory%2C%20this%20loop%20intelligently%20balances%20the%20exploration%0Aof%20novel%20hypotheses%20with%20exploitation%2C%20selectively%20promoting%20the%20most%20promising%0Afindings%20to%20higher-fidelity%20levels%20of%20validation.%20Consuming%20over%2020%2C000%20GPU%0Ahours%2C%20the%20system%20generated%20about%205%2C000%20unique%20scientific%20ideas%20and%0Aexperimentally%20validated%20approximately%201100%20of%20them%2C%20ultimately%20surpassing%0Ahuman-designed%20state-of-the-art%20%28SOTA%29%20methods%20on%20three%20frontier%20AI%20tasks%20by%0A183.7%5C%25%2C%201.9%5C%25%2C%20and%207.9%5C%25.%20This%20work%20provides%20the%20first%20large-scale%20evidence%20of%0Aan%20AI%20achieving%20discoveries%20that%20progressively%20surpass%20human%20SOTA%20on%20scientific%0Atasks%2C%20producing%20valuable%20findings%20that%20genuinely%20push%20the%20frontier%20of%0Ascientific%20discovery.%20To%20facilitate%20further%20research%20into%20this%20process%2C%20we%20will%0Aopen-source%20all%20experimental%20logs%20and%20system%20code%20at%0Ahttps%3A//github.com/ResearAI/DeepScientist/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26603v1&entry.124074799=Read"},
{"title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT\n  Dataset", "author": "Ruixu Zhang and Yuran Wang and Xinyi Hu and Chaoyu Mai and Wenxuan Liu and Danni Xu and Xian Zhong and Zheng Wang", "abstract": "  Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.\n", "link": "http://arxiv.org/abs/2509.20715v2", "date": "2025-09-30", "relevancy": 1.9108, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4842}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4813}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset&body=Title%3A%20Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset%0AAuthor%3A%20Ruixu%20Zhang%20and%20Yuran%20Wang%20and%20Xinyi%20Hu%20and%20Chaoyu%20Mai%20and%20Wenxuan%20Liu%20and%20Danni%20Xu%20and%20Xian%20Zhong%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20Intention%20recognition%20has%20traditionally%20focused%20on%20individual%20intentions%2C%0Aoverlooking%20the%20complexities%20of%20collective%20intentions%20in%20group%20settings.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20concept%20of%20group%20intention%2C%20which%0Arepresents%20shared%20goals%20emerging%20through%20the%20actions%20of%20multiple%20individuals%2C%0Aand%20Group%20Intention%20Forecasting%20%28GIF%29%2C%20a%20novel%20task%20that%20forecasts%20when%20group%0Aintentions%20will%20occur%20by%20analyzing%20individual%20actions%20and%20interactions%20before%0Athe%20collective%20goal%20becomes%20apparent.%20To%20investigate%20GIF%20in%20a%20specific%0Ascenario%2C%20we%20propose%20SHOT%2C%20the%20first%20large-scale%20dataset%20for%20GIF%2C%20consisting%20of%0A1%2C979%20basketball%20video%20clips%20captured%20from%205%20camera%20views%20and%20annotated%20with%206%0Atypes%20of%20individual%20attributes.%20SHOT%20is%20designed%20with%203%20key%20characteristics%3A%0Amulti-individual%20information%2C%20multi-view%20adaptability%2C%20and%20multi-level%0Aintention%2C%20making%20it%20well-suited%20for%20studying%20emerging%20group%20intentions.%0AFurthermore%2C%20we%20introduce%20GIFT%20%28Group%20Intention%20ForecasTer%29%2C%20a%20framework%20that%0Aextracts%20fine-grained%20individual%20features%20and%20models%20evolving%20group%20dynamics%20to%0Aforecast%20intention%20emergence.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0ASHOT%20and%20GIFT%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20group%0Aintention%20forecasting.%20The%20dataset%20is%20available%20at%0Ahttps%3A//xinyi-hu.github.io/SHOT_DATASET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Individual%253A%2520Introducing%2520Group%2520Intention%2520Forecasting%2520with%2520SHOT%250A%2520%2520Dataset%26entry.906535625%3DRuixu%2520Zhang%2520and%2520Yuran%2520Wang%2520and%2520Xinyi%2520Hu%2520and%2520Chaoyu%2520Mai%2520and%2520Wenxuan%2520Liu%2520and%2520Danni%2520Xu%2520and%2520Xian%2520Zhong%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520Intention%2520recognition%2520has%2520traditionally%2520focused%2520on%2520individual%2520intentions%252C%250Aoverlooking%2520the%2520complexities%2520of%2520collective%2520intentions%2520in%2520group%2520settings.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520concept%2520of%2520group%2520intention%252C%2520which%250Arepresents%2520shared%2520goals%2520emerging%2520through%2520the%2520actions%2520of%2520multiple%2520individuals%252C%250Aand%2520Group%2520Intention%2520Forecasting%2520%2528GIF%2529%252C%2520a%2520novel%2520task%2520that%2520forecasts%2520when%2520group%250Aintentions%2520will%2520occur%2520by%2520analyzing%2520individual%2520actions%2520and%2520interactions%2520before%250Athe%2520collective%2520goal%2520becomes%2520apparent.%2520To%2520investigate%2520GIF%2520in%2520a%2520specific%250Ascenario%252C%2520we%2520propose%2520SHOT%252C%2520the%2520first%2520large-scale%2520dataset%2520for%2520GIF%252C%2520consisting%2520of%250A1%252C979%2520basketball%2520video%2520clips%2520captured%2520from%25205%2520camera%2520views%2520and%2520annotated%2520with%25206%250Atypes%2520of%2520individual%2520attributes.%2520SHOT%2520is%2520designed%2520with%25203%2520key%2520characteristics%253A%250Amulti-individual%2520information%252C%2520multi-view%2520adaptability%252C%2520and%2520multi-level%250Aintention%252C%2520making%2520it%2520well-suited%2520for%2520studying%2520emerging%2520group%2520intentions.%250AFurthermore%252C%2520we%2520introduce%2520GIFT%2520%2528Group%2520Intention%2520ForecasTer%2529%252C%2520a%2520framework%2520that%250Aextracts%2520fine-grained%2520individual%2520features%2520and%2520models%2520evolving%2520group%2520dynamics%2520to%250Aforecast%2520intention%2520emergence.%2520Experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%250ASHOT%2520and%2520GIFT%252C%2520establishing%2520a%2520strong%2520foundation%2520for%2520future%2520research%2520in%2520group%250Aintention%2520forecasting.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//xinyi-hu.github.io/SHOT_DATASET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Individual%3A%20Introducing%20Group%20Intention%20Forecasting%20with%20SHOT%0A%20%20Dataset&entry.906535625=Ruixu%20Zhang%20and%20Yuran%20Wang%20and%20Xinyi%20Hu%20and%20Chaoyu%20Mai%20and%20Wenxuan%20Liu%20and%20Danni%20Xu%20and%20Xian%20Zhong%20and%20Zheng%20Wang&entry.1292438233=%20%20Intention%20recognition%20has%20traditionally%20focused%20on%20individual%20intentions%2C%0Aoverlooking%20the%20complexities%20of%20collective%20intentions%20in%20group%20settings.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20concept%20of%20group%20intention%2C%20which%0Arepresents%20shared%20goals%20emerging%20through%20the%20actions%20of%20multiple%20individuals%2C%0Aand%20Group%20Intention%20Forecasting%20%28GIF%29%2C%20a%20novel%20task%20that%20forecasts%20when%20group%0Aintentions%20will%20occur%20by%20analyzing%20individual%20actions%20and%20interactions%20before%0Athe%20collective%20goal%20becomes%20apparent.%20To%20investigate%20GIF%20in%20a%20specific%0Ascenario%2C%20we%20propose%20SHOT%2C%20the%20first%20large-scale%20dataset%20for%20GIF%2C%20consisting%20of%0A1%2C979%20basketball%20video%20clips%20captured%20from%205%20camera%20views%20and%20annotated%20with%206%0Atypes%20of%20individual%20attributes.%20SHOT%20is%20designed%20with%203%20key%20characteristics%3A%0Amulti-individual%20information%2C%20multi-view%20adaptability%2C%20and%20multi-level%0Aintention%2C%20making%20it%20well-suited%20for%20studying%20emerging%20group%20intentions.%0AFurthermore%2C%20we%20introduce%20GIFT%20%28Group%20Intention%20ForecasTer%29%2C%20a%20framework%20that%0Aextracts%20fine-grained%20individual%20features%20and%20models%20evolving%20group%20dynamics%20to%0Aforecast%20intention%20emergence.%20Experimental%20results%20confirm%20the%20effectiveness%20of%0ASHOT%20and%20GIFT%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20group%0Aintention%20forecasting.%20The%20dataset%20is%20available%20at%0Ahttps%3A//xinyi-hu.github.io/SHOT_DATASET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20715v2&entry.124074799=Read"},
{"title": "Bayesian Influence Functions for Hessian-Free Data Attribution", "author": "Philipp Alexander Kreer and Wilson Wu and Maxwell Adam and Zach Furman and Jesse Hoogland", "abstract": "  Classical influence functions face significant challenges when applied to\ndeep neural networks, primarily due to non-invertible Hessians and\nhigh-dimensional parameter spaces. We propose the local Bayesian influence\nfunction (BIF), an extension of classical influence functions that replaces\nHessian inversion with loss landscape statistics that can be estimated via\nstochastic-gradient MCMC sampling. This Hessian-free approach captures\nhigher-order interactions among parameters and scales efficiently to neural\nnetworks with billions of parameters. We demonstrate state-of-the-art results\non predicting retraining experiments.\n", "link": "http://arxiv.org/abs/2509.26544v1", "date": "2025-09-30", "relevancy": 1.9067, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.492}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Influence%20Functions%20for%20Hessian-Free%20Data%20Attribution&body=Title%3A%20Bayesian%20Influence%20Functions%20for%20Hessian-Free%20Data%20Attribution%0AAuthor%3A%20Philipp%20Alexander%20Kreer%20and%20Wilson%20Wu%20and%20Maxwell%20Adam%20and%20Zach%20Furman%20and%20Jesse%20Hoogland%0AAbstract%3A%20%20%20Classical%20influence%20functions%20face%20significant%20challenges%20when%20applied%20to%0Adeep%20neural%20networks%2C%20primarily%20due%20to%20non-invertible%20Hessians%20and%0Ahigh-dimensional%20parameter%20spaces.%20We%20propose%20the%20local%20Bayesian%20influence%0Afunction%20%28BIF%29%2C%20an%20extension%20of%20classical%20influence%20functions%20that%20replaces%0AHessian%20inversion%20with%20loss%20landscape%20statistics%20that%20can%20be%20estimated%20via%0Astochastic-gradient%20MCMC%20sampling.%20This%20Hessian-free%20approach%20captures%0Ahigher-order%20interactions%20among%20parameters%20and%20scales%20efficiently%20to%20neural%0Anetworks%20with%20billions%20of%20parameters.%20We%20demonstrate%20state-of-the-art%20results%0Aon%20predicting%20retraining%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Influence%2520Functions%2520for%2520Hessian-Free%2520Data%2520Attribution%26entry.906535625%3DPhilipp%2520Alexander%2520Kreer%2520and%2520Wilson%2520Wu%2520and%2520Maxwell%2520Adam%2520and%2520Zach%2520Furman%2520and%2520Jesse%2520Hoogland%26entry.1292438233%3D%2520%2520Classical%2520influence%2520functions%2520face%2520significant%2520challenges%2520when%2520applied%2520to%250Adeep%2520neural%2520networks%252C%2520primarily%2520due%2520to%2520non-invertible%2520Hessians%2520and%250Ahigh-dimensional%2520parameter%2520spaces.%2520We%2520propose%2520the%2520local%2520Bayesian%2520influence%250Afunction%2520%2528BIF%2529%252C%2520an%2520extension%2520of%2520classical%2520influence%2520functions%2520that%2520replaces%250AHessian%2520inversion%2520with%2520loss%2520landscape%2520statistics%2520that%2520can%2520be%2520estimated%2520via%250Astochastic-gradient%2520MCMC%2520sampling.%2520This%2520Hessian-free%2520approach%2520captures%250Ahigher-order%2520interactions%2520among%2520parameters%2520and%2520scales%2520efficiently%2520to%2520neural%250Anetworks%2520with%2520billions%2520of%2520parameters.%2520We%2520demonstrate%2520state-of-the-art%2520results%250Aon%2520predicting%2520retraining%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Influence%20Functions%20for%20Hessian-Free%20Data%20Attribution&entry.906535625=Philipp%20Alexander%20Kreer%20and%20Wilson%20Wu%20and%20Maxwell%20Adam%20and%20Zach%20Furman%20and%20Jesse%20Hoogland&entry.1292438233=%20%20Classical%20influence%20functions%20face%20significant%20challenges%20when%20applied%20to%0Adeep%20neural%20networks%2C%20primarily%20due%20to%20non-invertible%20Hessians%20and%0Ahigh-dimensional%20parameter%20spaces.%20We%20propose%20the%20local%20Bayesian%20influence%0Afunction%20%28BIF%29%2C%20an%20extension%20of%20classical%20influence%20functions%20that%20replaces%0AHessian%20inversion%20with%20loss%20landscape%20statistics%20that%20can%20be%20estimated%20via%0Astochastic-gradient%20MCMC%20sampling.%20This%20Hessian-free%20approach%20captures%0Ahigher-order%20interactions%20among%20parameters%20and%20scales%20efficiently%20to%20neural%0Anetworks%20with%20billions%20of%20parameters.%20We%20demonstrate%20state-of-the-art%20results%0Aon%20predicting%20retraining%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26544v1&entry.124074799=Read"},
{"title": "Autoproof: Automated Segmentation Proofreading for Connectomics", "author": "Gary B Huang and William M Katz and Stuart Berg and Louis Scheffer", "abstract": "  Producing connectomes from electron microscopy (EM) images has historically\nrequired a great deal of human proofreading effort. This manual annotation cost\nis the current bottleneck in scaling EM connectomics, for example, in making\nlarger connectome reconstructions feasible, or in enabling comparative\nconnectomics where multiple related reconstructions are produced. In this work,\nwe propose using the available ground-truth data generated by this manual\nannotation effort to learn a machine learning model to automate or optimize\nparts of the required proofreading workflows. We validate our approach on a\nrecent complete reconstruction of the \\emph{Drosophila} male central nervous\nsystem. We first show our method would allow for obtaining 90\\% of the value of\na guided proofreading workflow while reducing required cost by 80\\%. We then\ndemonstrate a second application for automatically merging many segmentation\nfragments to proofread neurons. Our system is able to automatically attach 200\nthousand fragments, equivalent to four proofreader years of manual work, and\nincreasing the connectivity completion rate of the connectome by 1.3\\% points.\n", "link": "http://arxiv.org/abs/2509.26585v1", "date": "2025-09-30", "relevancy": 1.9054, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4821}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoproof%3A%20Automated%20Segmentation%20Proofreading%20for%20Connectomics&body=Title%3A%20Autoproof%3A%20Automated%20Segmentation%20Proofreading%20for%20Connectomics%0AAuthor%3A%20Gary%20B%20Huang%20and%20William%20M%20Katz%20and%20Stuart%20Berg%20and%20Louis%20Scheffer%0AAbstract%3A%20%20%20Producing%20connectomes%20from%20electron%20microscopy%20%28EM%29%20images%20has%20historically%0Arequired%20a%20great%20deal%20of%20human%20proofreading%20effort.%20This%20manual%20annotation%20cost%0Ais%20the%20current%20bottleneck%20in%20scaling%20EM%20connectomics%2C%20for%20example%2C%20in%20making%0Alarger%20connectome%20reconstructions%20feasible%2C%20or%20in%20enabling%20comparative%0Aconnectomics%20where%20multiple%20related%20reconstructions%20are%20produced.%20In%20this%20work%2C%0Awe%20propose%20using%20the%20available%20ground-truth%20data%20generated%20by%20this%20manual%0Aannotation%20effort%20to%20learn%20a%20machine%20learning%20model%20to%20automate%20or%20optimize%0Aparts%20of%20the%20required%20proofreading%20workflows.%20We%20validate%20our%20approach%20on%20a%0Arecent%20complete%20reconstruction%20of%20the%20%5Cemph%7BDrosophila%7D%20male%20central%20nervous%0Asystem.%20We%20first%20show%20our%20method%20would%20allow%20for%20obtaining%2090%5C%25%20of%20the%20value%20of%0Aa%20guided%20proofreading%20workflow%20while%20reducing%20required%20cost%20by%2080%5C%25.%20We%20then%0Ademonstrate%20a%20second%20application%20for%20automatically%20merging%20many%20segmentation%0Afragments%20to%20proofread%20neurons.%20Our%20system%20is%20able%20to%20automatically%20attach%20200%0Athousand%20fragments%2C%20equivalent%20to%20four%20proofreader%20years%20of%20manual%20work%2C%20and%0Aincreasing%20the%20connectivity%20completion%20rate%20of%20the%20connectome%20by%201.3%5C%25%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoproof%253A%2520Automated%2520Segmentation%2520Proofreading%2520for%2520Connectomics%26entry.906535625%3DGary%2520B%2520Huang%2520and%2520William%2520M%2520Katz%2520and%2520Stuart%2520Berg%2520and%2520Louis%2520Scheffer%26entry.1292438233%3D%2520%2520Producing%2520connectomes%2520from%2520electron%2520microscopy%2520%2528EM%2529%2520images%2520has%2520historically%250Arequired%2520a%2520great%2520deal%2520of%2520human%2520proofreading%2520effort.%2520This%2520manual%2520annotation%2520cost%250Ais%2520the%2520current%2520bottleneck%2520in%2520scaling%2520EM%2520connectomics%252C%2520for%2520example%252C%2520in%2520making%250Alarger%2520connectome%2520reconstructions%2520feasible%252C%2520or%2520in%2520enabling%2520comparative%250Aconnectomics%2520where%2520multiple%2520related%2520reconstructions%2520are%2520produced.%2520In%2520this%2520work%252C%250Awe%2520propose%2520using%2520the%2520available%2520ground-truth%2520data%2520generated%2520by%2520this%2520manual%250Aannotation%2520effort%2520to%2520learn%2520a%2520machine%2520learning%2520model%2520to%2520automate%2520or%2520optimize%250Aparts%2520of%2520the%2520required%2520proofreading%2520workflows.%2520We%2520validate%2520our%2520approach%2520on%2520a%250Arecent%2520complete%2520reconstruction%2520of%2520the%2520%255Cemph%257BDrosophila%257D%2520male%2520central%2520nervous%250Asystem.%2520We%2520first%2520show%2520our%2520method%2520would%2520allow%2520for%2520obtaining%252090%255C%2525%2520of%2520the%2520value%2520of%250Aa%2520guided%2520proofreading%2520workflow%2520while%2520reducing%2520required%2520cost%2520by%252080%255C%2525.%2520We%2520then%250Ademonstrate%2520a%2520second%2520application%2520for%2520automatically%2520merging%2520many%2520segmentation%250Afragments%2520to%2520proofread%2520neurons.%2520Our%2520system%2520is%2520able%2520to%2520automatically%2520attach%2520200%250Athousand%2520fragments%252C%2520equivalent%2520to%2520four%2520proofreader%2520years%2520of%2520manual%2520work%252C%2520and%250Aincreasing%2520the%2520connectivity%2520completion%2520rate%2520of%2520the%2520connectome%2520by%25201.3%255C%2525%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoproof%3A%20Automated%20Segmentation%20Proofreading%20for%20Connectomics&entry.906535625=Gary%20B%20Huang%20and%20William%20M%20Katz%20and%20Stuart%20Berg%20and%20Louis%20Scheffer&entry.1292438233=%20%20Producing%20connectomes%20from%20electron%20microscopy%20%28EM%29%20images%20has%20historically%0Arequired%20a%20great%20deal%20of%20human%20proofreading%20effort.%20This%20manual%20annotation%20cost%0Ais%20the%20current%20bottleneck%20in%20scaling%20EM%20connectomics%2C%20for%20example%2C%20in%20making%0Alarger%20connectome%20reconstructions%20feasible%2C%20or%20in%20enabling%20comparative%0Aconnectomics%20where%20multiple%20related%20reconstructions%20are%20produced.%20In%20this%20work%2C%0Awe%20propose%20using%20the%20available%20ground-truth%20data%20generated%20by%20this%20manual%0Aannotation%20effort%20to%20learn%20a%20machine%20learning%20model%20to%20automate%20or%20optimize%0Aparts%20of%20the%20required%20proofreading%20workflows.%20We%20validate%20our%20approach%20on%20a%0Arecent%20complete%20reconstruction%20of%20the%20%5Cemph%7BDrosophila%7D%20male%20central%20nervous%0Asystem.%20We%20first%20show%20our%20method%20would%20allow%20for%20obtaining%2090%5C%25%20of%20the%20value%20of%0Aa%20guided%20proofreading%20workflow%20while%20reducing%20required%20cost%20by%2080%5C%25.%20We%20then%0Ademonstrate%20a%20second%20application%20for%20automatically%20merging%20many%20segmentation%0Afragments%20to%20proofread%20neurons.%20Our%20system%20is%20able%20to%20automatically%20attach%20200%0Athousand%20fragments%2C%20equivalent%20to%20four%20proofreader%20years%20of%20manual%20work%2C%20and%0Aincreasing%20the%20connectivity%20completion%20rate%20of%20the%20connectome%20by%201.3%5C%25%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26585v1&entry.124074799=Read"},
{"title": "Query-Kontext: An Unified Multimodal Model for Image Generation and\n  Editing", "author": "Yuxin Song and Wenkai Dong and Shizun Wang and Qi Zhang and Song Xue and Tao Yuan and Hu Yang and Haocheng Feng and Hang Zhou and Xinyan Xiao and Jingdong Wang", "abstract": "  Unified Multimodal Models (UMMs) have demonstrated remarkable performance in\ntext-to-image generation (T2I) and editing (TI2I), whether instantiated as\nassembled unified frameworks which couple powerful vision-language model (VLM)\nwith diffusion-based generator, or as naive Unified Multimodal Models with an\nearly fusion of understanding and generation modalities. We contend that in\ncurrent unified frameworks, the crucial capability of multimodal generative\nreasoning which encompasses instruction understanding, grounding, and image\nreferring for identity preservation and faithful reconstruction, is\nintrinsically entangled with high-fidelity synthesis. In this work, we\nintroduce Query-Kontext, a novel approach that bridges the VLM and diffusion\nmodel via a multimodal ``kontext'' composed of semantic cues and coarse-grained\nimage conditions encoded from multimodal inputs. This design delegates the\ncomplex ability of multimodal generative reasoning to powerful VLM while\nreserving diffusion model's role for high-quality visual synthesis. To achieve\nthis, we propose a three-stage progressive training strategy. First, we connect\nthe VLM to a lightweight diffusion head via multimodal kontext tokens to\nunleash the VLM's generative reasoning ability. Second, we scale this head to a\nlarge, pre-trained diffusion model to enhance visual detail and realism.\nFinally, we introduce a low-level image encoder to improve image fidelity and\nperform instruction tuning on downstream tasks. Furthermore, we build a\ncomprehensive data pipeline integrating real, synthetic, and open-source\ndatasets, covering diverse multimodal reference-to-image scenarios, including\nimage generation, instruction-driven editing, customized generation, and\nmulti-subject composition. Experiments show that our approach matches strong\nunified baselines and even outperforms task-specific state-of-the-art methods\nin several cases.\n", "link": "http://arxiv.org/abs/2509.26641v1", "date": "2025-09-30", "relevancy": 1.9001, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6533}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Query-Kontext%3A%20An%20Unified%20Multimodal%20Model%20for%20Image%20Generation%20and%0A%20%20Editing&body=Title%3A%20Query-Kontext%3A%20An%20Unified%20Multimodal%20Model%20for%20Image%20Generation%20and%0A%20%20Editing%0AAuthor%3A%20Yuxin%20Song%20and%20Wenkai%20Dong%20and%20Shizun%20Wang%20and%20Qi%20Zhang%20and%20Song%20Xue%20and%20Tao%20Yuan%20and%20Hu%20Yang%20and%20Haocheng%20Feng%20and%20Hang%20Zhou%20and%20Xinyan%20Xiao%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Unified%20Multimodal%20Models%20%28UMMs%29%20have%20demonstrated%20remarkable%20performance%20in%0Atext-to-image%20generation%20%28T2I%29%20and%20editing%20%28TI2I%29%2C%20whether%20instantiated%20as%0Aassembled%20unified%20frameworks%20which%20couple%20powerful%20vision-language%20model%20%28VLM%29%0Awith%20diffusion-based%20generator%2C%20or%20as%20naive%20Unified%20Multimodal%20Models%20with%20an%0Aearly%20fusion%20of%20understanding%20and%20generation%20modalities.%20We%20contend%20that%20in%0Acurrent%20unified%20frameworks%2C%20the%20crucial%20capability%20of%20multimodal%20generative%0Areasoning%20which%20encompasses%20instruction%20understanding%2C%20grounding%2C%20and%20image%0Areferring%20for%20identity%20preservation%20and%20faithful%20reconstruction%2C%20is%0Aintrinsically%20entangled%20with%20high-fidelity%20synthesis.%20In%20this%20work%2C%20we%0Aintroduce%20Query-Kontext%2C%20a%20novel%20approach%20that%20bridges%20the%20VLM%20and%20diffusion%0Amodel%20via%20a%20multimodal%20%60%60kontext%27%27%20composed%20of%20semantic%20cues%20and%20coarse-grained%0Aimage%20conditions%20encoded%20from%20multimodal%20inputs.%20This%20design%20delegates%20the%0Acomplex%20ability%20of%20multimodal%20generative%20reasoning%20to%20powerful%20VLM%20while%0Areserving%20diffusion%20model%27s%20role%20for%20high-quality%20visual%20synthesis.%20To%20achieve%0Athis%2C%20we%20propose%20a%20three-stage%20progressive%20training%20strategy.%20First%2C%20we%20connect%0Athe%20VLM%20to%20a%20lightweight%20diffusion%20head%20via%20multimodal%20kontext%20tokens%20to%0Aunleash%20the%20VLM%27s%20generative%20reasoning%20ability.%20Second%2C%20we%20scale%20this%20head%20to%20a%0Alarge%2C%20pre-trained%20diffusion%20model%20to%20enhance%20visual%20detail%20and%20realism.%0AFinally%2C%20we%20introduce%20a%20low-level%20image%20encoder%20to%20improve%20image%20fidelity%20and%0Aperform%20instruction%20tuning%20on%20downstream%20tasks.%20Furthermore%2C%20we%20build%20a%0Acomprehensive%20data%20pipeline%20integrating%20real%2C%20synthetic%2C%20and%20open-source%0Adatasets%2C%20covering%20diverse%20multimodal%20reference-to-image%20scenarios%2C%20including%0Aimage%20generation%2C%20instruction-driven%20editing%2C%20customized%20generation%2C%20and%0Amulti-subject%20composition.%20Experiments%20show%20that%20our%20approach%20matches%20strong%0Aunified%20baselines%20and%20even%20outperforms%20task-specific%20state-of-the-art%20methods%0Ain%20several%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuery-Kontext%253A%2520An%2520Unified%2520Multimodal%2520Model%2520for%2520Image%2520Generation%2520and%250A%2520%2520Editing%26entry.906535625%3DYuxin%2520Song%2520and%2520Wenkai%2520Dong%2520and%2520Shizun%2520Wang%2520and%2520Qi%2520Zhang%2520and%2520Song%2520Xue%2520and%2520Tao%2520Yuan%2520and%2520Hu%2520Yang%2520and%2520Haocheng%2520Feng%2520and%2520Hang%2520Zhou%2520and%2520Xinyan%2520Xiao%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Unified%2520Multimodal%2520Models%2520%2528UMMs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%250Atext-to-image%2520generation%2520%2528T2I%2529%2520and%2520editing%2520%2528TI2I%2529%252C%2520whether%2520instantiated%2520as%250Aassembled%2520unified%2520frameworks%2520which%2520couple%2520powerful%2520vision-language%2520model%2520%2528VLM%2529%250Awith%2520diffusion-based%2520generator%252C%2520or%2520as%2520naive%2520Unified%2520Multimodal%2520Models%2520with%2520an%250Aearly%2520fusion%2520of%2520understanding%2520and%2520generation%2520modalities.%2520We%2520contend%2520that%2520in%250Acurrent%2520unified%2520frameworks%252C%2520the%2520crucial%2520capability%2520of%2520multimodal%2520generative%250Areasoning%2520which%2520encompasses%2520instruction%2520understanding%252C%2520grounding%252C%2520and%2520image%250Areferring%2520for%2520identity%2520preservation%2520and%2520faithful%2520reconstruction%252C%2520is%250Aintrinsically%2520entangled%2520with%2520high-fidelity%2520synthesis.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Query-Kontext%252C%2520a%2520novel%2520approach%2520that%2520bridges%2520the%2520VLM%2520and%2520diffusion%250Amodel%2520via%2520a%2520multimodal%2520%2560%2560kontext%2527%2527%2520composed%2520of%2520semantic%2520cues%2520and%2520coarse-grained%250Aimage%2520conditions%2520encoded%2520from%2520multimodal%2520inputs.%2520This%2520design%2520delegates%2520the%250Acomplex%2520ability%2520of%2520multimodal%2520generative%2520reasoning%2520to%2520powerful%2520VLM%2520while%250Areserving%2520diffusion%2520model%2527s%2520role%2520for%2520high-quality%2520visual%2520synthesis.%2520To%2520achieve%250Athis%252C%2520we%2520propose%2520a%2520three-stage%2520progressive%2520training%2520strategy.%2520First%252C%2520we%2520connect%250Athe%2520VLM%2520to%2520a%2520lightweight%2520diffusion%2520head%2520via%2520multimodal%2520kontext%2520tokens%2520to%250Aunleash%2520the%2520VLM%2527s%2520generative%2520reasoning%2520ability.%2520Second%252C%2520we%2520scale%2520this%2520head%2520to%2520a%250Alarge%252C%2520pre-trained%2520diffusion%2520model%2520to%2520enhance%2520visual%2520detail%2520and%2520realism.%250AFinally%252C%2520we%2520introduce%2520a%2520low-level%2520image%2520encoder%2520to%2520improve%2520image%2520fidelity%2520and%250Aperform%2520instruction%2520tuning%2520on%2520downstream%2520tasks.%2520Furthermore%252C%2520we%2520build%2520a%250Acomprehensive%2520data%2520pipeline%2520integrating%2520real%252C%2520synthetic%252C%2520and%2520open-source%250Adatasets%252C%2520covering%2520diverse%2520multimodal%2520reference-to-image%2520scenarios%252C%2520including%250Aimage%2520generation%252C%2520instruction-driven%2520editing%252C%2520customized%2520generation%252C%2520and%250Amulti-subject%2520composition.%2520Experiments%2520show%2520that%2520our%2520approach%2520matches%2520strong%250Aunified%2520baselines%2520and%2520even%2520outperforms%2520task-specific%2520state-of-the-art%2520methods%250Ain%2520several%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Query-Kontext%3A%20An%20Unified%20Multimodal%20Model%20for%20Image%20Generation%20and%0A%20%20Editing&entry.906535625=Yuxin%20Song%20and%20Wenkai%20Dong%20and%20Shizun%20Wang%20and%20Qi%20Zhang%20and%20Song%20Xue%20and%20Tao%20Yuan%20and%20Hu%20Yang%20and%20Haocheng%20Feng%20and%20Hang%20Zhou%20and%20Xinyan%20Xiao%20and%20Jingdong%20Wang&entry.1292438233=%20%20Unified%20Multimodal%20Models%20%28UMMs%29%20have%20demonstrated%20remarkable%20performance%20in%0Atext-to-image%20generation%20%28T2I%29%20and%20editing%20%28TI2I%29%2C%20whether%20instantiated%20as%0Aassembled%20unified%20frameworks%20which%20couple%20powerful%20vision-language%20model%20%28VLM%29%0Awith%20diffusion-based%20generator%2C%20or%20as%20naive%20Unified%20Multimodal%20Models%20with%20an%0Aearly%20fusion%20of%20understanding%20and%20generation%20modalities.%20We%20contend%20that%20in%0Acurrent%20unified%20frameworks%2C%20the%20crucial%20capability%20of%20multimodal%20generative%0Areasoning%20which%20encompasses%20instruction%20understanding%2C%20grounding%2C%20and%20image%0Areferring%20for%20identity%20preservation%20and%20faithful%20reconstruction%2C%20is%0Aintrinsically%20entangled%20with%20high-fidelity%20synthesis.%20In%20this%20work%2C%20we%0Aintroduce%20Query-Kontext%2C%20a%20novel%20approach%20that%20bridges%20the%20VLM%20and%20diffusion%0Amodel%20via%20a%20multimodal%20%60%60kontext%27%27%20composed%20of%20semantic%20cues%20and%20coarse-grained%0Aimage%20conditions%20encoded%20from%20multimodal%20inputs.%20This%20design%20delegates%20the%0Acomplex%20ability%20of%20multimodal%20generative%20reasoning%20to%20powerful%20VLM%20while%0Areserving%20diffusion%20model%27s%20role%20for%20high-quality%20visual%20synthesis.%20To%20achieve%0Athis%2C%20we%20propose%20a%20three-stage%20progressive%20training%20strategy.%20First%2C%20we%20connect%0Athe%20VLM%20to%20a%20lightweight%20diffusion%20head%20via%20multimodal%20kontext%20tokens%20to%0Aunleash%20the%20VLM%27s%20generative%20reasoning%20ability.%20Second%2C%20we%20scale%20this%20head%20to%20a%0Alarge%2C%20pre-trained%20diffusion%20model%20to%20enhance%20visual%20detail%20and%20realism.%0AFinally%2C%20we%20introduce%20a%20low-level%20image%20encoder%20to%20improve%20image%20fidelity%20and%0Aperform%20instruction%20tuning%20on%20downstream%20tasks.%20Furthermore%2C%20we%20build%20a%0Acomprehensive%20data%20pipeline%20integrating%20real%2C%20synthetic%2C%20and%20open-source%0Adatasets%2C%20covering%20diverse%20multimodal%20reference-to-image%20scenarios%2C%20including%0Aimage%20generation%2C%20instruction-driven%20editing%2C%20customized%20generation%2C%20and%0Amulti-subject%20composition.%20Experiments%20show%20that%20our%20approach%20matches%20strong%0Aunified%20baselines%20and%20even%20outperforms%20task-specific%20state-of-the-art%20methods%0Ain%20several%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26641v1&entry.124074799=Read"},
{"title": "CBAM Integrated Attention Driven Model For Betel Leaf Diseases\n  Classification With Explainable AI", "author": "Sumaiya Tabassum and Md. Faysal Ahamed", "abstract": "  Betel leaf is an important crop because of its economic advantages and\nwidespread use. Its betel vines are susceptible to a number of illnesses that\nare commonly referred to as betel leaf disease. Plant diseases are the largest\nthreat to the food supply's security, and they are challenging to identify in\ntime to stop possible financial damage. Interestingly, artificial intelligence\ncan leave a big mark on the betel leaf industry since it helps with output\ngrowth by forecasting sickness. This paper presents a lightweight CBAM-CNN\nmodel with just 2.13 million parameters (8.13 MB), incorporating CBAM\n(Convolutional Block Attention Module) to improve feature emphasis without\ndepending on heavy pre-trained networks. The model's capacity to discern minute\nvariations among leaf disease classes is improved by the integrated attention\nmechanism, which allows it to adaptively focus on significant spatial and\nchannel-wise information. In order to ensure class balance and diversity for\nefficient model training and validation, this work makes use of an enriched\ndataset of 10,185 images divided into three categories: Healthy Leaf, Leaf Rot,\nand Leaf Spot. The proposed model achieved a precision of 97%, recall of 94%,\nand F1 score of 95%, and 95.58% accuracy on the test set demonstrating strong\nand balanced classification performance outperforming traditional pre trained\nCNN models. The model's focus regions were visualized and interpreted using\nGrad-CAM (Gradient-weighted Class Activation Mapping), an explainable AI\ntechnique.\n", "link": "http://arxiv.org/abs/2509.26484v1", "date": "2025-09-30", "relevancy": 1.8919, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBAM%20Integrated%20Attention%20Driven%20Model%20For%20Betel%20Leaf%20Diseases%0A%20%20Classification%20With%20Explainable%20AI&body=Title%3A%20CBAM%20Integrated%20Attention%20Driven%20Model%20For%20Betel%20Leaf%20Diseases%0A%20%20Classification%20With%20Explainable%20AI%0AAuthor%3A%20Sumaiya%20Tabassum%20and%20Md.%20Faysal%20Ahamed%0AAbstract%3A%20%20%20Betel%20leaf%20is%20an%20important%20crop%20because%20of%20its%20economic%20advantages%20and%0Awidespread%20use.%20Its%20betel%20vines%20are%20susceptible%20to%20a%20number%20of%20illnesses%20that%0Aare%20commonly%20referred%20to%20as%20betel%20leaf%20disease.%20Plant%20diseases%20are%20the%20largest%0Athreat%20to%20the%20food%20supply%27s%20security%2C%20and%20they%20are%20challenging%20to%20identify%20in%0Atime%20to%20stop%20possible%20financial%20damage.%20Interestingly%2C%20artificial%20intelligence%0Acan%20leave%20a%20big%20mark%20on%20the%20betel%20leaf%20industry%20since%20it%20helps%20with%20output%0Agrowth%20by%20forecasting%20sickness.%20This%20paper%20presents%20a%20lightweight%20CBAM-CNN%0Amodel%20with%20just%202.13%20million%20parameters%20%288.13%20MB%29%2C%20incorporating%20CBAM%0A%28Convolutional%20Block%20Attention%20Module%29%20to%20improve%20feature%20emphasis%20without%0Adepending%20on%20heavy%20pre-trained%20networks.%20The%20model%27s%20capacity%20to%20discern%20minute%0Avariations%20among%20leaf%20disease%20classes%20is%20improved%20by%20the%20integrated%20attention%0Amechanism%2C%20which%20allows%20it%20to%20adaptively%20focus%20on%20significant%20spatial%20and%0Achannel-wise%20information.%20In%20order%20to%20ensure%20class%20balance%20and%20diversity%20for%0Aefficient%20model%20training%20and%20validation%2C%20this%20work%20makes%20use%20of%20an%20enriched%0Adataset%20of%2010%2C185%20images%20divided%20into%20three%20categories%3A%20Healthy%20Leaf%2C%20Leaf%20Rot%2C%0Aand%20Leaf%20Spot.%20The%20proposed%20model%20achieved%20a%20precision%20of%2097%25%2C%20recall%20of%2094%25%2C%0Aand%20F1%20score%20of%2095%25%2C%20and%2095.58%25%20accuracy%20on%20the%20test%20set%20demonstrating%20strong%0Aand%20balanced%20classification%20performance%20outperforming%20traditional%20pre%20trained%0ACNN%20models.%20The%20model%27s%20focus%20regions%20were%20visualized%20and%20interpreted%20using%0AGrad-CAM%20%28Gradient-weighted%20Class%20Activation%20Mapping%29%2C%20an%20explainable%20AI%0Atechnique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBAM%2520Integrated%2520Attention%2520Driven%2520Model%2520For%2520Betel%2520Leaf%2520Diseases%250A%2520%2520Classification%2520With%2520Explainable%2520AI%26entry.906535625%3DSumaiya%2520Tabassum%2520and%2520Md.%2520Faysal%2520Ahamed%26entry.1292438233%3D%2520%2520Betel%2520leaf%2520is%2520an%2520important%2520crop%2520because%2520of%2520its%2520economic%2520advantages%2520and%250Awidespread%2520use.%2520Its%2520betel%2520vines%2520are%2520susceptible%2520to%2520a%2520number%2520of%2520illnesses%2520that%250Aare%2520commonly%2520referred%2520to%2520as%2520betel%2520leaf%2520disease.%2520Plant%2520diseases%2520are%2520the%2520largest%250Athreat%2520to%2520the%2520food%2520supply%2527s%2520security%252C%2520and%2520they%2520are%2520challenging%2520to%2520identify%2520in%250Atime%2520to%2520stop%2520possible%2520financial%2520damage.%2520Interestingly%252C%2520artificial%2520intelligence%250Acan%2520leave%2520a%2520big%2520mark%2520on%2520the%2520betel%2520leaf%2520industry%2520since%2520it%2520helps%2520with%2520output%250Agrowth%2520by%2520forecasting%2520sickness.%2520This%2520paper%2520presents%2520a%2520lightweight%2520CBAM-CNN%250Amodel%2520with%2520just%25202.13%2520million%2520parameters%2520%25288.13%2520MB%2529%252C%2520incorporating%2520CBAM%250A%2528Convolutional%2520Block%2520Attention%2520Module%2529%2520to%2520improve%2520feature%2520emphasis%2520without%250Adepending%2520on%2520heavy%2520pre-trained%2520networks.%2520The%2520model%2527s%2520capacity%2520to%2520discern%2520minute%250Avariations%2520among%2520leaf%2520disease%2520classes%2520is%2520improved%2520by%2520the%2520integrated%2520attention%250Amechanism%252C%2520which%2520allows%2520it%2520to%2520adaptively%2520focus%2520on%2520significant%2520spatial%2520and%250Achannel-wise%2520information.%2520In%2520order%2520to%2520ensure%2520class%2520balance%2520and%2520diversity%2520for%250Aefficient%2520model%2520training%2520and%2520validation%252C%2520this%2520work%2520makes%2520use%2520of%2520an%2520enriched%250Adataset%2520of%252010%252C185%2520images%2520divided%2520into%2520three%2520categories%253A%2520Healthy%2520Leaf%252C%2520Leaf%2520Rot%252C%250Aand%2520Leaf%2520Spot.%2520The%2520proposed%2520model%2520achieved%2520a%2520precision%2520of%252097%2525%252C%2520recall%2520of%252094%2525%252C%250Aand%2520F1%2520score%2520of%252095%2525%252C%2520and%252095.58%2525%2520accuracy%2520on%2520the%2520test%2520set%2520demonstrating%2520strong%250Aand%2520balanced%2520classification%2520performance%2520outperforming%2520traditional%2520pre%2520trained%250ACNN%2520models.%2520The%2520model%2527s%2520focus%2520regions%2520were%2520visualized%2520and%2520interpreted%2520using%250AGrad-CAM%2520%2528Gradient-weighted%2520Class%2520Activation%2520Mapping%2529%252C%2520an%2520explainable%2520AI%250Atechnique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBAM%20Integrated%20Attention%20Driven%20Model%20For%20Betel%20Leaf%20Diseases%0A%20%20Classification%20With%20Explainable%20AI&entry.906535625=Sumaiya%20Tabassum%20and%20Md.%20Faysal%20Ahamed&entry.1292438233=%20%20Betel%20leaf%20is%20an%20important%20crop%20because%20of%20its%20economic%20advantages%20and%0Awidespread%20use.%20Its%20betel%20vines%20are%20susceptible%20to%20a%20number%20of%20illnesses%20that%0Aare%20commonly%20referred%20to%20as%20betel%20leaf%20disease.%20Plant%20diseases%20are%20the%20largest%0Athreat%20to%20the%20food%20supply%27s%20security%2C%20and%20they%20are%20challenging%20to%20identify%20in%0Atime%20to%20stop%20possible%20financial%20damage.%20Interestingly%2C%20artificial%20intelligence%0Acan%20leave%20a%20big%20mark%20on%20the%20betel%20leaf%20industry%20since%20it%20helps%20with%20output%0Agrowth%20by%20forecasting%20sickness.%20This%20paper%20presents%20a%20lightweight%20CBAM-CNN%0Amodel%20with%20just%202.13%20million%20parameters%20%288.13%20MB%29%2C%20incorporating%20CBAM%0A%28Convolutional%20Block%20Attention%20Module%29%20to%20improve%20feature%20emphasis%20without%0Adepending%20on%20heavy%20pre-trained%20networks.%20The%20model%27s%20capacity%20to%20discern%20minute%0Avariations%20among%20leaf%20disease%20classes%20is%20improved%20by%20the%20integrated%20attention%0Amechanism%2C%20which%20allows%20it%20to%20adaptively%20focus%20on%20significant%20spatial%20and%0Achannel-wise%20information.%20In%20order%20to%20ensure%20class%20balance%20and%20diversity%20for%0Aefficient%20model%20training%20and%20validation%2C%20this%20work%20makes%20use%20of%20an%20enriched%0Adataset%20of%2010%2C185%20images%20divided%20into%20three%20categories%3A%20Healthy%20Leaf%2C%20Leaf%20Rot%2C%0Aand%20Leaf%20Spot.%20The%20proposed%20model%20achieved%20a%20precision%20of%2097%25%2C%20recall%20of%2094%25%2C%0Aand%20F1%20score%20of%2095%25%2C%20and%2095.58%25%20accuracy%20on%20the%20test%20set%20demonstrating%20strong%0Aand%20balanced%20classification%20performance%20outperforming%20traditional%20pre%20trained%0ACNN%20models.%20The%20model%27s%20focus%20regions%20were%20visualized%20and%20interpreted%20using%0AGrad-CAM%20%28Gradient-weighted%20Class%20Activation%20Mapping%29%2C%20an%20explainable%20AI%0Atechnique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26484v1&entry.124074799=Read"},
{"title": "The Loss Kernel: A Geometric Probe for Deep Learning Interpretability", "author": "Maxwell Adam and Zach Furman and Jesse Hoogland", "abstract": "  We introduce the loss kernel, an interpretability method for measuring\nsimilarity between data points according to a trained neural network. The\nkernel is the covariance matrix of per-sample losses computed under a\ndistribution of low-loss-preserving parameter perturbations. We first validate\nour method on a synthetic multitask problem, showing it separates inputs by\ntask as predicted by theory. We then apply this kernel to Inception-v1 to\nvisualize the structure of ImageNet, and we show that the kernel's structure\naligns with the WordNet semantic hierarchy. This establishes the loss kernel as\na practical tool for interpretability and data attribution.\n", "link": "http://arxiv.org/abs/2509.26537v1", "date": "2025-09-30", "relevancy": 1.8873, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4712}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Loss%20Kernel%3A%20A%20Geometric%20Probe%20for%20Deep%20Learning%20Interpretability&body=Title%3A%20The%20Loss%20Kernel%3A%20A%20Geometric%20Probe%20for%20Deep%20Learning%20Interpretability%0AAuthor%3A%20Maxwell%20Adam%20and%20Zach%20Furman%20and%20Jesse%20Hoogland%0AAbstract%3A%20%20%20We%20introduce%20the%20loss%20kernel%2C%20an%20interpretability%20method%20for%20measuring%0Asimilarity%20between%20data%20points%20according%20to%20a%20trained%20neural%20network.%20The%0Akernel%20is%20the%20covariance%20matrix%20of%20per-sample%20losses%20computed%20under%20a%0Adistribution%20of%20low-loss-preserving%20parameter%20perturbations.%20We%20first%20validate%0Aour%20method%20on%20a%20synthetic%20multitask%20problem%2C%20showing%20it%20separates%20inputs%20by%0Atask%20as%20predicted%20by%20theory.%20We%20then%20apply%20this%20kernel%20to%20Inception-v1%20to%0Avisualize%20the%20structure%20of%20ImageNet%2C%20and%20we%20show%20that%20the%20kernel%27s%20structure%0Aaligns%20with%20the%20WordNet%20semantic%20hierarchy.%20This%20establishes%20the%20loss%20kernel%20as%0Aa%20practical%20tool%20for%20interpretability%20and%20data%20attribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Loss%2520Kernel%253A%2520A%2520Geometric%2520Probe%2520for%2520Deep%2520Learning%2520Interpretability%26entry.906535625%3DMaxwell%2520Adam%2520and%2520Zach%2520Furman%2520and%2520Jesse%2520Hoogland%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520loss%2520kernel%252C%2520an%2520interpretability%2520method%2520for%2520measuring%250Asimilarity%2520between%2520data%2520points%2520according%2520to%2520a%2520trained%2520neural%2520network.%2520The%250Akernel%2520is%2520the%2520covariance%2520matrix%2520of%2520per-sample%2520losses%2520computed%2520under%2520a%250Adistribution%2520of%2520low-loss-preserving%2520parameter%2520perturbations.%2520We%2520first%2520validate%250Aour%2520method%2520on%2520a%2520synthetic%2520multitask%2520problem%252C%2520showing%2520it%2520separates%2520inputs%2520by%250Atask%2520as%2520predicted%2520by%2520theory.%2520We%2520then%2520apply%2520this%2520kernel%2520to%2520Inception-v1%2520to%250Avisualize%2520the%2520structure%2520of%2520ImageNet%252C%2520and%2520we%2520show%2520that%2520the%2520kernel%2527s%2520structure%250Aaligns%2520with%2520the%2520WordNet%2520semantic%2520hierarchy.%2520This%2520establishes%2520the%2520loss%2520kernel%2520as%250Aa%2520practical%2520tool%2520for%2520interpretability%2520and%2520data%2520attribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Loss%20Kernel%3A%20A%20Geometric%20Probe%20for%20Deep%20Learning%20Interpretability&entry.906535625=Maxwell%20Adam%20and%20Zach%20Furman%20and%20Jesse%20Hoogland&entry.1292438233=%20%20We%20introduce%20the%20loss%20kernel%2C%20an%20interpretability%20method%20for%20measuring%0Asimilarity%20between%20data%20points%20according%20to%20a%20trained%20neural%20network.%20The%0Akernel%20is%20the%20covariance%20matrix%20of%20per-sample%20losses%20computed%20under%20a%0Adistribution%20of%20low-loss-preserving%20parameter%20perturbations.%20We%20first%20validate%0Aour%20method%20on%20a%20synthetic%20multitask%20problem%2C%20showing%20it%20separates%20inputs%20by%0Atask%20as%20predicted%20by%20theory.%20We%20then%20apply%20this%20kernel%20to%20Inception-v1%20to%0Avisualize%20the%20structure%20of%20ImageNet%2C%20and%20we%20show%20that%20the%20kernel%27s%20structure%0Aaligns%20with%20the%20WordNet%20semantic%20hierarchy.%20This%20establishes%20the%20loss%20kernel%20as%0Aa%20practical%20tool%20for%20interpretability%20and%20data%20attribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26537v1&entry.124074799=Read"},
{"title": "CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image\n  Diffusion Models via a Semantic-Driven Word Vocabulary", "author": "Jiahang Tu and Qian Feng and Jiahua Dong and Hanbin Zhao and Chao Zhang and Nicu Sebe and Hui Qian", "abstract": "  Large-scale text-to-image (T2I) diffusion models have achieved remarkable\ngenerative performance about various concepts. With the limitation of privacy\nand safety in practice, the generative capability concerning NSFW (Not Safe For\nWork) concepts is undesirable, e.g., producing sexually explicit photos, and\nlicensed images. The concept erasure task for T2I diffusion models has\nattracted considerable attention and requires an effective and efficient\nmethod. To achieve this goal, we propose a CE-SDWV framework, which removes the\ntarget concepts (e.g., NSFW concepts) of T2I diffusion models in the text\nsemantic space by only adjusting the text condition tokens and does not need to\nre-train the original T2I diffusion model's weights. Specifically, our\nframework first builds a target concept-related word vocabulary to enhance the\nrepresentation of the target concepts within the text semantic space, and then\nutilizes an adaptive semantic component suppression strategy to ablate the\ntarget concept-related semantic information in the text condition tokens. To\nfurther adapt the above text condition tokens to the original image semantic\nspace, we propose an end-to-end gradient-orthogonal token optimization\nstrategy. Extensive experiments on I2P and UnlearnCanvas benchmarks demonstrate\nthe effectiveness and efficiency of our method. Code is available at\nhttps://github.com/TtuHamg/CE-SDWV.\n", "link": "http://arxiv.org/abs/2501.15562v2", "date": "2025-09-30", "relevancy": 1.8873, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6323}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6286}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CE-SDWV%3A%20Effective%20and%20Efficient%20Concept%20Erasure%20for%20Text-to-Image%0A%20%20Diffusion%20Models%20via%20a%20Semantic-Driven%20Word%20Vocabulary&body=Title%3A%20CE-SDWV%3A%20Effective%20and%20Efficient%20Concept%20Erasure%20for%20Text-to-Image%0A%20%20Diffusion%20Models%20via%20a%20Semantic-Driven%20Word%20Vocabulary%0AAuthor%3A%20Jiahang%20Tu%20and%20Qian%20Feng%20and%20Jiahua%20Dong%20and%20Hanbin%20Zhao%20and%20Chao%20Zhang%20and%20Nicu%20Sebe%20and%20Hui%20Qian%0AAbstract%3A%20%20%20Large-scale%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20achieved%20remarkable%0Agenerative%20performance%20about%20various%20concepts.%20With%20the%20limitation%20of%20privacy%0Aand%20safety%20in%20practice%2C%20the%20generative%20capability%20concerning%20NSFW%20%28Not%20Safe%20For%0AWork%29%20concepts%20is%20undesirable%2C%20e.g.%2C%20producing%20sexually%20explicit%20photos%2C%20and%0Alicensed%20images.%20The%20concept%20erasure%20task%20for%20T2I%20diffusion%20models%20has%0Aattracted%20considerable%20attention%20and%20requires%20an%20effective%20and%20efficient%0Amethod.%20To%20achieve%20this%20goal%2C%20we%20propose%20a%20CE-SDWV%20framework%2C%20which%20removes%20the%0Atarget%20concepts%20%28e.g.%2C%20NSFW%20concepts%29%20of%20T2I%20diffusion%20models%20in%20the%20text%0Asemantic%20space%20by%20only%20adjusting%20the%20text%20condition%20tokens%20and%20does%20not%20need%20to%0Are-train%20the%20original%20T2I%20diffusion%20model%27s%20weights.%20Specifically%2C%20our%0Aframework%20first%20builds%20a%20target%20concept-related%20word%20vocabulary%20to%20enhance%20the%0Arepresentation%20of%20the%20target%20concepts%20within%20the%20text%20semantic%20space%2C%20and%20then%0Autilizes%20an%20adaptive%20semantic%20component%20suppression%20strategy%20to%20ablate%20the%0Atarget%20concept-related%20semantic%20information%20in%20the%20text%20condition%20tokens.%20To%0Afurther%20adapt%20the%20above%20text%20condition%20tokens%20to%20the%20original%20image%20semantic%0Aspace%2C%20we%20propose%20an%20end-to-end%20gradient-orthogonal%20token%20optimization%0Astrategy.%20Extensive%20experiments%20on%20I2P%20and%20UnlearnCanvas%20benchmarks%20demonstrate%0Athe%20effectiveness%20and%20efficiency%20of%20our%20method.%20Code%20is%20available%20at%0Ahttps%3A//github.com/TtuHamg/CE-SDWV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCE-SDWV%253A%2520Effective%2520and%2520Efficient%2520Concept%2520Erasure%2520for%2520Text-to-Image%250A%2520%2520Diffusion%2520Models%2520via%2520a%2520Semantic-Driven%2520Word%2520Vocabulary%26entry.906535625%3DJiahang%2520Tu%2520and%2520Qian%2520Feng%2520and%2520Jiahua%2520Dong%2520and%2520Hanbin%2520Zhao%2520and%2520Chao%2520Zhang%2520and%2520Nicu%2520Sebe%2520and%2520Hui%2520Qian%26entry.1292438233%3D%2520%2520Large-scale%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520achieved%2520remarkable%250Agenerative%2520performance%2520about%2520various%2520concepts.%2520With%2520the%2520limitation%2520of%2520privacy%250Aand%2520safety%2520in%2520practice%252C%2520the%2520generative%2520capability%2520concerning%2520NSFW%2520%2528Not%2520Safe%2520For%250AWork%2529%2520concepts%2520is%2520undesirable%252C%2520e.g.%252C%2520producing%2520sexually%2520explicit%2520photos%252C%2520and%250Alicensed%2520images.%2520The%2520concept%2520erasure%2520task%2520for%2520T2I%2520diffusion%2520models%2520has%250Aattracted%2520considerable%2520attention%2520and%2520requires%2520an%2520effective%2520and%2520efficient%250Amethod.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520propose%2520a%2520CE-SDWV%2520framework%252C%2520which%2520removes%2520the%250Atarget%2520concepts%2520%2528e.g.%252C%2520NSFW%2520concepts%2529%2520of%2520T2I%2520diffusion%2520models%2520in%2520the%2520text%250Asemantic%2520space%2520by%2520only%2520adjusting%2520the%2520text%2520condition%2520tokens%2520and%2520does%2520not%2520need%2520to%250Are-train%2520the%2520original%2520T2I%2520diffusion%2520model%2527s%2520weights.%2520Specifically%252C%2520our%250Aframework%2520first%2520builds%2520a%2520target%2520concept-related%2520word%2520vocabulary%2520to%2520enhance%2520the%250Arepresentation%2520of%2520the%2520target%2520concepts%2520within%2520the%2520text%2520semantic%2520space%252C%2520and%2520then%250Autilizes%2520an%2520adaptive%2520semantic%2520component%2520suppression%2520strategy%2520to%2520ablate%2520the%250Atarget%2520concept-related%2520semantic%2520information%2520in%2520the%2520text%2520condition%2520tokens.%2520To%250Afurther%2520adapt%2520the%2520above%2520text%2520condition%2520tokens%2520to%2520the%2520original%2520image%2520semantic%250Aspace%252C%2520we%2520propose%2520an%2520end-to-end%2520gradient-orthogonal%2520token%2520optimization%250Astrategy.%2520Extensive%2520experiments%2520on%2520I2P%2520and%2520UnlearnCanvas%2520benchmarks%2520demonstrate%250Athe%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520method.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/TtuHamg/CE-SDWV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CE-SDWV%3A%20Effective%20and%20Efficient%20Concept%20Erasure%20for%20Text-to-Image%0A%20%20Diffusion%20Models%20via%20a%20Semantic-Driven%20Word%20Vocabulary&entry.906535625=Jiahang%20Tu%20and%20Qian%20Feng%20and%20Jiahua%20Dong%20and%20Hanbin%20Zhao%20and%20Chao%20Zhang%20and%20Nicu%20Sebe%20and%20Hui%20Qian&entry.1292438233=%20%20Large-scale%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20achieved%20remarkable%0Agenerative%20performance%20about%20various%20concepts.%20With%20the%20limitation%20of%20privacy%0Aand%20safety%20in%20practice%2C%20the%20generative%20capability%20concerning%20NSFW%20%28Not%20Safe%20For%0AWork%29%20concepts%20is%20undesirable%2C%20e.g.%2C%20producing%20sexually%20explicit%20photos%2C%20and%0Alicensed%20images.%20The%20concept%20erasure%20task%20for%20T2I%20diffusion%20models%20has%0Aattracted%20considerable%20attention%20and%20requires%20an%20effective%20and%20efficient%0Amethod.%20To%20achieve%20this%20goal%2C%20we%20propose%20a%20CE-SDWV%20framework%2C%20which%20removes%20the%0Atarget%20concepts%20%28e.g.%2C%20NSFW%20concepts%29%20of%20T2I%20diffusion%20models%20in%20the%20text%0Asemantic%20space%20by%20only%20adjusting%20the%20text%20condition%20tokens%20and%20does%20not%20need%20to%0Are-train%20the%20original%20T2I%20diffusion%20model%27s%20weights.%20Specifically%2C%20our%0Aframework%20first%20builds%20a%20target%20concept-related%20word%20vocabulary%20to%20enhance%20the%0Arepresentation%20of%20the%20target%20concepts%20within%20the%20text%20semantic%20space%2C%20and%20then%0Autilizes%20an%20adaptive%20semantic%20component%20suppression%20strategy%20to%20ablate%20the%0Atarget%20concept-related%20semantic%20information%20in%20the%20text%20condition%20tokens.%20To%0Afurther%20adapt%20the%20above%20text%20condition%20tokens%20to%20the%20original%20image%20semantic%0Aspace%2C%20we%20propose%20an%20end-to-end%20gradient-orthogonal%20token%20optimization%0Astrategy.%20Extensive%20experiments%20on%20I2P%20and%20UnlearnCanvas%20benchmarks%20demonstrate%0Athe%20effectiveness%20and%20efficiency%20of%20our%20method.%20Code%20is%20available%20at%0Ahttps%3A//github.com/TtuHamg/CE-SDWV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15562v2&entry.124074799=Read"},
{"title": "Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\n  Materials via a Deep Segmentation Framework", "author": "Jian Guo Pan and Lin Wang and Xia Cai", "abstract": "  Scanning Electron Microscopy (SEM) is indispensable for characterizing the\nmicrostructure of thin films during perovskite solar cell fabrication. Accurate\nidentification and quantification of lead iodide and perovskite phases are\ncritical because residual lead iodide strongly influences crystallization\npathways and defect formation, while the morphology of perovskite grains\ngoverns carrier transport and device stability. Yet current SEM image analysis\nis still largely manual, limiting throughput and consistency. Here, we present\nan automated deep learning-based framework for SEM image segmentation that\nenables precise and efficient identification of lead iodide, perovskite and\ndefect domains across diverse morphologies. Built upon an improved YOLOv8x\narchitecture, our model named PerovSegNet incorporates two novel modules: (i)\nAdaptive Shuffle Dilated Convolution Block, which enhances multi-scale and\nfine-grained feature extraction through group convolutions and channel mixing;\nand (ii) Separable Adaptive Downsampling module, which jointly preserves\nfine-scale textures and large-scale structures for more robust boundary\nrecognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet\nachieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point\nOperations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing\nmodel size and computational load by 24.43% and 25.22%, respectively. Beyond\nsegmentation, the framework provides quantitative grain-level metrics, such as\nlead iodide/perovskite area and count, which can serve as reliable indicators\nof crystallization efficiency and microstructural quality. These capabilities\nestablish PerovSegNet as a scalable tool for real-time process monitoring and\ndata-driven optimization of perovskite thin-film fabrication.The source code is\navailable at:https://github.com/wlyyj/PerovSegNet/tree/master.\n", "link": "http://arxiv.org/abs/2509.26548v1", "date": "2025-09-30", "relevancy": 1.8776, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20and%20Scalable%20SEM%20Image%20Analysis%20of%20Perovskite%20Solar%20Cell%0A%20%20Materials%20via%20a%20Deep%20Segmentation%20Framework&body=Title%3A%20Automated%20and%20Scalable%20SEM%20Image%20Analysis%20of%20Perovskite%20Solar%20Cell%0A%20%20Materials%20via%20a%20Deep%20Segmentation%20Framework%0AAuthor%3A%20Jian%20Guo%20Pan%20and%20Lin%20Wang%20and%20Xia%20Cai%0AAbstract%3A%20%20%20Scanning%20Electron%20Microscopy%20%28SEM%29%20is%20indispensable%20for%20characterizing%20the%0Amicrostructure%20of%20thin%20films%20during%20perovskite%20solar%20cell%20fabrication.%20Accurate%0Aidentification%20and%20quantification%20of%20lead%20iodide%20and%20perovskite%20phases%20are%0Acritical%20because%20residual%20lead%20iodide%20strongly%20influences%20crystallization%0Apathways%20and%20defect%20formation%2C%20while%20the%20morphology%20of%20perovskite%20grains%0Agoverns%20carrier%20transport%20and%20device%20stability.%20Yet%20current%20SEM%20image%20analysis%0Ais%20still%20largely%20manual%2C%20limiting%20throughput%20and%20consistency.%20Here%2C%20we%20present%0Aan%20automated%20deep%20learning-based%20framework%20for%20SEM%20image%20segmentation%20that%0Aenables%20precise%20and%20efficient%20identification%20of%20lead%20iodide%2C%20perovskite%20and%0Adefect%20domains%20across%20diverse%20morphologies.%20Built%20upon%20an%20improved%20YOLOv8x%0Aarchitecture%2C%20our%20model%20named%20PerovSegNet%20incorporates%20two%20novel%20modules%3A%20%28i%29%0AAdaptive%20Shuffle%20Dilated%20Convolution%20Block%2C%20which%20enhances%20multi-scale%20and%0Afine-grained%20feature%20extraction%20through%20group%20convolutions%20and%20channel%20mixing%3B%0Aand%20%28ii%29%20Separable%20Adaptive%20Downsampling%20module%2C%20which%20jointly%20preserves%0Afine-scale%20textures%20and%20large-scale%20structures%20for%20more%20robust%20boundary%0Arecognition.%20Trained%20on%20an%20augmented%20dataset%20of%2010%2C994%20SEM%20images%2C%20PerovSegNet%0Aachieves%20a%20mean%20Average%20Precision%20of%2087.25%25%20with%20265.4%20Giga%20Floating%20Point%0AOperations%2C%20outperforming%20the%20baseline%20YOLOv8x-seg%20by%204.08%25%2C%20while%20reducing%0Amodel%20size%20and%20computational%20load%20by%2024.43%25%20and%2025.22%25%2C%20respectively.%20Beyond%0Asegmentation%2C%20the%20framework%20provides%20quantitative%20grain-level%20metrics%2C%20such%20as%0Alead%20iodide/perovskite%20area%20and%20count%2C%20which%20can%20serve%20as%20reliable%20indicators%0Aof%20crystallization%20efficiency%20and%20microstructural%20quality.%20These%20capabilities%0Aestablish%20PerovSegNet%20as%20a%20scalable%20tool%20for%20real-time%20process%20monitoring%20and%0Adata-driven%20optimization%20of%20perovskite%20thin-film%20fabrication.The%20source%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/wlyyj/PerovSegNet/tree/master.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520and%2520Scalable%2520SEM%2520Image%2520Analysis%2520of%2520Perovskite%2520Solar%2520Cell%250A%2520%2520Materials%2520via%2520a%2520Deep%2520Segmentation%2520Framework%26entry.906535625%3DJian%2520Guo%2520Pan%2520and%2520Lin%2520Wang%2520and%2520Xia%2520Cai%26entry.1292438233%3D%2520%2520Scanning%2520Electron%2520Microscopy%2520%2528SEM%2529%2520is%2520indispensable%2520for%2520characterizing%2520the%250Amicrostructure%2520of%2520thin%2520films%2520during%2520perovskite%2520solar%2520cell%2520fabrication.%2520Accurate%250Aidentification%2520and%2520quantification%2520of%2520lead%2520iodide%2520and%2520perovskite%2520phases%2520are%250Acritical%2520because%2520residual%2520lead%2520iodide%2520strongly%2520influences%2520crystallization%250Apathways%2520and%2520defect%2520formation%252C%2520while%2520the%2520morphology%2520of%2520perovskite%2520grains%250Agoverns%2520carrier%2520transport%2520and%2520device%2520stability.%2520Yet%2520current%2520SEM%2520image%2520analysis%250Ais%2520still%2520largely%2520manual%252C%2520limiting%2520throughput%2520and%2520consistency.%2520Here%252C%2520we%2520present%250Aan%2520automated%2520deep%2520learning-based%2520framework%2520for%2520SEM%2520image%2520segmentation%2520that%250Aenables%2520precise%2520and%2520efficient%2520identification%2520of%2520lead%2520iodide%252C%2520perovskite%2520and%250Adefect%2520domains%2520across%2520diverse%2520morphologies.%2520Built%2520upon%2520an%2520improved%2520YOLOv8x%250Aarchitecture%252C%2520our%2520model%2520named%2520PerovSegNet%2520incorporates%2520two%2520novel%2520modules%253A%2520%2528i%2529%250AAdaptive%2520Shuffle%2520Dilated%2520Convolution%2520Block%252C%2520which%2520enhances%2520multi-scale%2520and%250Afine-grained%2520feature%2520extraction%2520through%2520group%2520convolutions%2520and%2520channel%2520mixing%253B%250Aand%2520%2528ii%2529%2520Separable%2520Adaptive%2520Downsampling%2520module%252C%2520which%2520jointly%2520preserves%250Afine-scale%2520textures%2520and%2520large-scale%2520structures%2520for%2520more%2520robust%2520boundary%250Arecognition.%2520Trained%2520on%2520an%2520augmented%2520dataset%2520of%252010%252C994%2520SEM%2520images%252C%2520PerovSegNet%250Aachieves%2520a%2520mean%2520Average%2520Precision%2520of%252087.25%2525%2520with%2520265.4%2520Giga%2520Floating%2520Point%250AOperations%252C%2520outperforming%2520the%2520baseline%2520YOLOv8x-seg%2520by%25204.08%2525%252C%2520while%2520reducing%250Amodel%2520size%2520and%2520computational%2520load%2520by%252024.43%2525%2520and%252025.22%2525%252C%2520respectively.%2520Beyond%250Asegmentation%252C%2520the%2520framework%2520provides%2520quantitative%2520grain-level%2520metrics%252C%2520such%2520as%250Alead%2520iodide/perovskite%2520area%2520and%2520count%252C%2520which%2520can%2520serve%2520as%2520reliable%2520indicators%250Aof%2520crystallization%2520efficiency%2520and%2520microstructural%2520quality.%2520These%2520capabilities%250Aestablish%2520PerovSegNet%2520as%2520a%2520scalable%2520tool%2520for%2520real-time%2520process%2520monitoring%2520and%250Adata-driven%2520optimization%2520of%2520perovskite%2520thin-film%2520fabrication.The%2520source%2520code%2520is%250Aavailable%2520at%253Ahttps%253A//github.com/wlyyj/PerovSegNet/tree/master.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20and%20Scalable%20SEM%20Image%20Analysis%20of%20Perovskite%20Solar%20Cell%0A%20%20Materials%20via%20a%20Deep%20Segmentation%20Framework&entry.906535625=Jian%20Guo%20Pan%20and%20Lin%20Wang%20and%20Xia%20Cai&entry.1292438233=%20%20Scanning%20Electron%20Microscopy%20%28SEM%29%20is%20indispensable%20for%20characterizing%20the%0Amicrostructure%20of%20thin%20films%20during%20perovskite%20solar%20cell%20fabrication.%20Accurate%0Aidentification%20and%20quantification%20of%20lead%20iodide%20and%20perovskite%20phases%20are%0Acritical%20because%20residual%20lead%20iodide%20strongly%20influences%20crystallization%0Apathways%20and%20defect%20formation%2C%20while%20the%20morphology%20of%20perovskite%20grains%0Agoverns%20carrier%20transport%20and%20device%20stability.%20Yet%20current%20SEM%20image%20analysis%0Ais%20still%20largely%20manual%2C%20limiting%20throughput%20and%20consistency.%20Here%2C%20we%20present%0Aan%20automated%20deep%20learning-based%20framework%20for%20SEM%20image%20segmentation%20that%0Aenables%20precise%20and%20efficient%20identification%20of%20lead%20iodide%2C%20perovskite%20and%0Adefect%20domains%20across%20diverse%20morphologies.%20Built%20upon%20an%20improved%20YOLOv8x%0Aarchitecture%2C%20our%20model%20named%20PerovSegNet%20incorporates%20two%20novel%20modules%3A%20%28i%29%0AAdaptive%20Shuffle%20Dilated%20Convolution%20Block%2C%20which%20enhances%20multi-scale%20and%0Afine-grained%20feature%20extraction%20through%20group%20convolutions%20and%20channel%20mixing%3B%0Aand%20%28ii%29%20Separable%20Adaptive%20Downsampling%20module%2C%20which%20jointly%20preserves%0Afine-scale%20textures%20and%20large-scale%20structures%20for%20more%20robust%20boundary%0Arecognition.%20Trained%20on%20an%20augmented%20dataset%20of%2010%2C994%20SEM%20images%2C%20PerovSegNet%0Aachieves%20a%20mean%20Average%20Precision%20of%2087.25%25%20with%20265.4%20Giga%20Floating%20Point%0AOperations%2C%20outperforming%20the%20baseline%20YOLOv8x-seg%20by%204.08%25%2C%20while%20reducing%0Amodel%20size%20and%20computational%20load%20by%2024.43%25%20and%2025.22%25%2C%20respectively.%20Beyond%0Asegmentation%2C%20the%20framework%20provides%20quantitative%20grain-level%20metrics%2C%20such%20as%0Alead%20iodide/perovskite%20area%20and%20count%2C%20which%20can%20serve%20as%20reliable%20indicators%0Aof%20crystallization%20efficiency%20and%20microstructural%20quality.%20These%20capabilities%0Aestablish%20PerovSegNet%20as%20a%20scalable%20tool%20for%20real-time%20process%20monitoring%20and%0Adata-driven%20optimization%20of%20perovskite%20thin-film%20fabrication.The%20source%20code%20is%0Aavailable%20at%3Ahttps%3A//github.com/wlyyj/PerovSegNet/tree/master.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26548v1&entry.124074799=Read"},
{"title": "Searching for Difficult-to-Translate Test Examples at Scale", "author": "Wenda Xu and Vil\u00e9m Zouhar and Parker Riley and Mara Finkelstein and Markus Freitag and Daniel Deutsch", "abstract": "  NLP models require test data that are sufficiently challenging. The\ndifficulty of an example is linked to the topic it originates from (''seed\ntopic''). The relationship between the topic and the difficulty of its\ninstances is stochastic in nature: an example about a difficult topic can\nhappen to be easy, and vice versa. At the scale of the Internet, there are tens\nof thousands of potential topics, and finding the most difficult one by drawing\nand evaluating a large number of examples across all topics is computationally\ninfeasible. We formalize this task and treat it as a multi-armed bandit\nproblem. In this framework, each topic is an ''arm,'' and pulling an arm (at a\ncost) involves drawing a single example, evaluating it, and measuring its\ndifficulty. The goal is to efficiently identify the most difficult topics\nwithin a fixed computational budget. We illustrate the bandit problem setup of\nfinding difficult examples for the task of machine translation. We find that\nvarious bandit strategies vastly outperform baseline methods like brute-force\nsearching the most challenging topics.\n", "link": "http://arxiv.org/abs/2509.26619v1", "date": "2025-09-30", "relevancy": 1.8583, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4668}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Searching%20for%20Difficult-to-Translate%20Test%20Examples%20at%20Scale&body=Title%3A%20Searching%20for%20Difficult-to-Translate%20Test%20Examples%20at%20Scale%0AAuthor%3A%20Wenda%20Xu%20and%20Vil%C3%A9m%20Zouhar%20and%20Parker%20Riley%20and%20Mara%20Finkelstein%20and%20Markus%20Freitag%20and%20Daniel%20Deutsch%0AAbstract%3A%20%20%20NLP%20models%20require%20test%20data%20that%20are%20sufficiently%20challenging.%20The%0Adifficulty%20of%20an%20example%20is%20linked%20to%20the%20topic%20it%20originates%20from%20%28%27%27seed%0Atopic%27%27%29.%20The%20relationship%20between%20the%20topic%20and%20the%20difficulty%20of%20its%0Ainstances%20is%20stochastic%20in%20nature%3A%20an%20example%20about%20a%20difficult%20topic%20can%0Ahappen%20to%20be%20easy%2C%20and%20vice%20versa.%20At%20the%20scale%20of%20the%20Internet%2C%20there%20are%20tens%0Aof%20thousands%20of%20potential%20topics%2C%20and%20finding%20the%20most%20difficult%20one%20by%20drawing%0Aand%20evaluating%20a%20large%20number%20of%20examples%20across%20all%20topics%20is%20computationally%0Ainfeasible.%20We%20formalize%20this%20task%20and%20treat%20it%20as%20a%20multi-armed%20bandit%0Aproblem.%20In%20this%20framework%2C%20each%20topic%20is%20an%20%27%27arm%2C%27%27%20and%20pulling%20an%20arm%20%28at%20a%0Acost%29%20involves%20drawing%20a%20single%20example%2C%20evaluating%20it%2C%20and%20measuring%20its%0Adifficulty.%20The%20goal%20is%20to%20efficiently%20identify%20the%20most%20difficult%20topics%0Awithin%20a%20fixed%20computational%20budget.%20We%20illustrate%20the%20bandit%20problem%20setup%20of%0Afinding%20difficult%20examples%20for%20the%20task%20of%20machine%20translation.%20We%20find%20that%0Avarious%20bandit%20strategies%20vastly%20outperform%20baseline%20methods%20like%20brute-force%0Asearching%20the%20most%20challenging%20topics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearching%2520for%2520Difficult-to-Translate%2520Test%2520Examples%2520at%2520Scale%26entry.906535625%3DWenda%2520Xu%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Parker%2520Riley%2520and%2520Mara%2520Finkelstein%2520and%2520Markus%2520Freitag%2520and%2520Daniel%2520Deutsch%26entry.1292438233%3D%2520%2520NLP%2520models%2520require%2520test%2520data%2520that%2520are%2520sufficiently%2520challenging.%2520The%250Adifficulty%2520of%2520an%2520example%2520is%2520linked%2520to%2520the%2520topic%2520it%2520originates%2520from%2520%2528%2527%2527seed%250Atopic%2527%2527%2529.%2520The%2520relationship%2520between%2520the%2520topic%2520and%2520the%2520difficulty%2520of%2520its%250Ainstances%2520is%2520stochastic%2520in%2520nature%253A%2520an%2520example%2520about%2520a%2520difficult%2520topic%2520can%250Ahappen%2520to%2520be%2520easy%252C%2520and%2520vice%2520versa.%2520At%2520the%2520scale%2520of%2520the%2520Internet%252C%2520there%2520are%2520tens%250Aof%2520thousands%2520of%2520potential%2520topics%252C%2520and%2520finding%2520the%2520most%2520difficult%2520one%2520by%2520drawing%250Aand%2520evaluating%2520a%2520large%2520number%2520of%2520examples%2520across%2520all%2520topics%2520is%2520computationally%250Ainfeasible.%2520We%2520formalize%2520this%2520task%2520and%2520treat%2520it%2520as%2520a%2520multi-armed%2520bandit%250Aproblem.%2520In%2520this%2520framework%252C%2520each%2520topic%2520is%2520an%2520%2527%2527arm%252C%2527%2527%2520and%2520pulling%2520an%2520arm%2520%2528at%2520a%250Acost%2529%2520involves%2520drawing%2520a%2520single%2520example%252C%2520evaluating%2520it%252C%2520and%2520measuring%2520its%250Adifficulty.%2520The%2520goal%2520is%2520to%2520efficiently%2520identify%2520the%2520most%2520difficult%2520topics%250Awithin%2520a%2520fixed%2520computational%2520budget.%2520We%2520illustrate%2520the%2520bandit%2520problem%2520setup%2520of%250Afinding%2520difficult%2520examples%2520for%2520the%2520task%2520of%2520machine%2520translation.%2520We%2520find%2520that%250Avarious%2520bandit%2520strategies%2520vastly%2520outperform%2520baseline%2520methods%2520like%2520brute-force%250Asearching%2520the%2520most%2520challenging%2520topics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Searching%20for%20Difficult-to-Translate%20Test%20Examples%20at%20Scale&entry.906535625=Wenda%20Xu%20and%20Vil%C3%A9m%20Zouhar%20and%20Parker%20Riley%20and%20Mara%20Finkelstein%20and%20Markus%20Freitag%20and%20Daniel%20Deutsch&entry.1292438233=%20%20NLP%20models%20require%20test%20data%20that%20are%20sufficiently%20challenging.%20The%0Adifficulty%20of%20an%20example%20is%20linked%20to%20the%20topic%20it%20originates%20from%20%28%27%27seed%0Atopic%27%27%29.%20The%20relationship%20between%20the%20topic%20and%20the%20difficulty%20of%20its%0Ainstances%20is%20stochastic%20in%20nature%3A%20an%20example%20about%20a%20difficult%20topic%20can%0Ahappen%20to%20be%20easy%2C%20and%20vice%20versa.%20At%20the%20scale%20of%20the%20Internet%2C%20there%20are%20tens%0Aof%20thousands%20of%20potential%20topics%2C%20and%20finding%20the%20most%20difficult%20one%20by%20drawing%0Aand%20evaluating%20a%20large%20number%20of%20examples%20across%20all%20topics%20is%20computationally%0Ainfeasible.%20We%20formalize%20this%20task%20and%20treat%20it%20as%20a%20multi-armed%20bandit%0Aproblem.%20In%20this%20framework%2C%20each%20topic%20is%20an%20%27%27arm%2C%27%27%20and%20pulling%20an%20arm%20%28at%20a%0Acost%29%20involves%20drawing%20a%20single%20example%2C%20evaluating%20it%2C%20and%20measuring%20its%0Adifficulty.%20The%20goal%20is%20to%20efficiently%20identify%20the%20most%20difficult%20topics%0Awithin%20a%20fixed%20computational%20budget.%20We%20illustrate%20the%20bandit%20problem%20setup%20of%0Afinding%20difficult%20examples%20for%20the%20task%20of%20machine%20translation.%20We%20find%20that%0Avarious%20bandit%20strategies%20vastly%20outperform%20baseline%20methods%20like%20brute-force%0Asearching%20the%20most%20challenging%20topics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26619v1&entry.124074799=Read"},
{"title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction", "author": "Lujie Yang and Xiaoyu Huang and Zhen Wu and Angjoo Kanazawa and Pieter Abbeel and Carmelo Sferrazza and C. Karen Liu and Rocky Duan and Guanya Shi", "abstract": "  A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.\n", "link": "http://arxiv.org/abs/2509.26633v1", "date": "2025-09-30", "relevancy": 1.8456, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6281}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniRetarget%3A%20Interaction-Preserving%20Data%20Generation%20for%20Humanoid%0A%20%20Whole-Body%20Loco-Manipulation%20and%20Scene%20Interaction&body=Title%3A%20OmniRetarget%3A%20Interaction-Preserving%20Data%20Generation%20for%20Humanoid%0A%20%20Whole-Body%20Loco-Manipulation%20and%20Scene%20Interaction%0AAuthor%3A%20Lujie%20Yang%20and%20Xiaoyu%20Huang%20and%20Zhen%20Wu%20and%20Angjoo%20Kanazawa%20and%20Pieter%20Abbeel%20and%20Carmelo%20Sferrazza%20and%20C.%20Karen%20Liu%20and%20Rocky%20Duan%20and%20Guanya%20Shi%0AAbstract%3A%20%20%20A%20dominant%20paradigm%20for%20teaching%20humanoid%20robots%20complex%20skills%20is%20to%0Aretarget%20human%20motions%20as%20kinematic%20references%20to%20train%20reinforcement%20learning%0A%28RL%29%20policies.%20However%2C%20existing%20retargeting%20pipelines%20often%20struggle%20with%20the%0Asignificant%20embodiment%20gap%20between%20humans%20and%20robots%2C%20producing%20physically%0Aimplausible%20artifacts%20like%20foot-skating%20and%20penetration.%20More%20importantly%2C%0Acommon%20retargeting%20methods%20neglect%20the%20rich%20human-object%20and%20human-environment%0Ainteractions%20essential%20for%20expressive%20locomotion%20and%20loco-manipulation.%20To%0Aaddress%20this%2C%20we%20introduce%20OmniRetarget%2C%20an%20interaction-preserving%20data%0Ageneration%20engine%20based%20on%20an%20interaction%20mesh%20that%20explicitly%20models%20and%0Apreserves%20the%20crucial%20spatial%20and%20contact%20relationships%20between%20an%20agent%2C%20the%0Aterrain%2C%20and%20manipulated%20objects.%20By%20minimizing%20the%20Laplacian%20deformation%0Abetween%20the%20human%20and%20robot%20meshes%20while%20enforcing%20kinematic%20constraints%2C%0AOmniRetarget%20generates%20kinematically%20feasible%20trajectories.%20Moreover%2C%0Apreserving%20task-relevant%20interactions%20enables%20efficient%20data%20augmentation%2C%20from%0Aa%20single%20demonstration%20to%20different%20robot%20embodiments%2C%20terrains%2C%20and%20object%0Aconfigurations.%20We%20comprehensively%20evaluate%20OmniRetarget%20by%20retargeting%20motions%0Afrom%20OMOMO%2C%20LAFAN1%2C%20and%20our%20in-house%20MoCap%20datasets%2C%20generating%20over%208-hour%0Atrajectories%20that%20achieve%20better%20kinematic%20constraint%20satisfaction%20and%20contact%0Apreservation%20than%20widely%20used%20baselines.%20Such%20high-quality%20data%20enables%0Aproprioceptive%20RL%20policies%20to%20successfully%20execute%20long-horizon%20%28up%20to%2030%0Aseconds%29%20parkour%20and%20loco-manipulation%20skills%20on%20a%20Unitree%20G1%20humanoid%2C%20trained%0Awith%20only%205%20reward%20terms%20and%20simple%20domain%20randomization%20shared%20by%20all%20tasks%2C%0Awithout%20any%20learning%20curriculum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniRetarget%253A%2520Interaction-Preserving%2520Data%2520Generation%2520for%2520Humanoid%250A%2520%2520Whole-Body%2520Loco-Manipulation%2520and%2520Scene%2520Interaction%26entry.906535625%3DLujie%2520Yang%2520and%2520Xiaoyu%2520Huang%2520and%2520Zhen%2520Wu%2520and%2520Angjoo%2520Kanazawa%2520and%2520Pieter%2520Abbeel%2520and%2520Carmelo%2520Sferrazza%2520and%2520C.%2520Karen%2520Liu%2520and%2520Rocky%2520Duan%2520and%2520Guanya%2520Shi%26entry.1292438233%3D%2520%2520A%2520dominant%2520paradigm%2520for%2520teaching%2520humanoid%2520robots%2520complex%2520skills%2520is%2520to%250Aretarget%2520human%2520motions%2520as%2520kinematic%2520references%2520to%2520train%2520reinforcement%2520learning%250A%2528RL%2529%2520policies.%2520However%252C%2520existing%2520retargeting%2520pipelines%2520often%2520struggle%2520with%2520the%250Asignificant%2520embodiment%2520gap%2520between%2520humans%2520and%2520robots%252C%2520producing%2520physically%250Aimplausible%2520artifacts%2520like%2520foot-skating%2520and%2520penetration.%2520More%2520importantly%252C%250Acommon%2520retargeting%2520methods%2520neglect%2520the%2520rich%2520human-object%2520and%2520human-environment%250Ainteractions%2520essential%2520for%2520expressive%2520locomotion%2520and%2520loco-manipulation.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520OmniRetarget%252C%2520an%2520interaction-preserving%2520data%250Ageneration%2520engine%2520based%2520on%2520an%2520interaction%2520mesh%2520that%2520explicitly%2520models%2520and%250Apreserves%2520the%2520crucial%2520spatial%2520and%2520contact%2520relationships%2520between%2520an%2520agent%252C%2520the%250Aterrain%252C%2520and%2520manipulated%2520objects.%2520By%2520minimizing%2520the%2520Laplacian%2520deformation%250Abetween%2520the%2520human%2520and%2520robot%2520meshes%2520while%2520enforcing%2520kinematic%2520constraints%252C%250AOmniRetarget%2520generates%2520kinematically%2520feasible%2520trajectories.%2520Moreover%252C%250Apreserving%2520task-relevant%2520interactions%2520enables%2520efficient%2520data%2520augmentation%252C%2520from%250Aa%2520single%2520demonstration%2520to%2520different%2520robot%2520embodiments%252C%2520terrains%252C%2520and%2520object%250Aconfigurations.%2520We%2520comprehensively%2520evaluate%2520OmniRetarget%2520by%2520retargeting%2520motions%250Afrom%2520OMOMO%252C%2520LAFAN1%252C%2520and%2520our%2520in-house%2520MoCap%2520datasets%252C%2520generating%2520over%25208-hour%250Atrajectories%2520that%2520achieve%2520better%2520kinematic%2520constraint%2520satisfaction%2520and%2520contact%250Apreservation%2520than%2520widely%2520used%2520baselines.%2520Such%2520high-quality%2520data%2520enables%250Aproprioceptive%2520RL%2520policies%2520to%2520successfully%2520execute%2520long-horizon%2520%2528up%2520to%252030%250Aseconds%2529%2520parkour%2520and%2520loco-manipulation%2520skills%2520on%2520a%2520Unitree%2520G1%2520humanoid%252C%2520trained%250Awith%2520only%25205%2520reward%2520terms%2520and%2520simple%2520domain%2520randomization%2520shared%2520by%2520all%2520tasks%252C%250Awithout%2520any%2520learning%2520curriculum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniRetarget%3A%20Interaction-Preserving%20Data%20Generation%20for%20Humanoid%0A%20%20Whole-Body%20Loco-Manipulation%20and%20Scene%20Interaction&entry.906535625=Lujie%20Yang%20and%20Xiaoyu%20Huang%20and%20Zhen%20Wu%20and%20Angjoo%20Kanazawa%20and%20Pieter%20Abbeel%20and%20Carmelo%20Sferrazza%20and%20C.%20Karen%20Liu%20and%20Rocky%20Duan%20and%20Guanya%20Shi&entry.1292438233=%20%20A%20dominant%20paradigm%20for%20teaching%20humanoid%20robots%20complex%20skills%20is%20to%0Aretarget%20human%20motions%20as%20kinematic%20references%20to%20train%20reinforcement%20learning%0A%28RL%29%20policies.%20However%2C%20existing%20retargeting%20pipelines%20often%20struggle%20with%20the%0Asignificant%20embodiment%20gap%20between%20humans%20and%20robots%2C%20producing%20physically%0Aimplausible%20artifacts%20like%20foot-skating%20and%20penetration.%20More%20importantly%2C%0Acommon%20retargeting%20methods%20neglect%20the%20rich%20human-object%20and%20human-environment%0Ainteractions%20essential%20for%20expressive%20locomotion%20and%20loco-manipulation.%20To%0Aaddress%20this%2C%20we%20introduce%20OmniRetarget%2C%20an%20interaction-preserving%20data%0Ageneration%20engine%20based%20on%20an%20interaction%20mesh%20that%20explicitly%20models%20and%0Apreserves%20the%20crucial%20spatial%20and%20contact%20relationships%20between%20an%20agent%2C%20the%0Aterrain%2C%20and%20manipulated%20objects.%20By%20minimizing%20the%20Laplacian%20deformation%0Abetween%20the%20human%20and%20robot%20meshes%20while%20enforcing%20kinematic%20constraints%2C%0AOmniRetarget%20generates%20kinematically%20feasible%20trajectories.%20Moreover%2C%0Apreserving%20task-relevant%20interactions%20enables%20efficient%20data%20augmentation%2C%20from%0Aa%20single%20demonstration%20to%20different%20robot%20embodiments%2C%20terrains%2C%20and%20object%0Aconfigurations.%20We%20comprehensively%20evaluate%20OmniRetarget%20by%20retargeting%20motions%0Afrom%20OMOMO%2C%20LAFAN1%2C%20and%20our%20in-house%20MoCap%20datasets%2C%20generating%20over%208-hour%0Atrajectories%20that%20achieve%20better%20kinematic%20constraint%20satisfaction%20and%20contact%0Apreservation%20than%20widely%20used%20baselines.%20Such%20high-quality%20data%20enables%0Aproprioceptive%20RL%20policies%20to%20successfully%20execute%20long-horizon%20%28up%20to%2030%0Aseconds%29%20parkour%20and%20loco-manipulation%20skills%20on%20a%20Unitree%20G1%20humanoid%2C%20trained%0Awith%20only%205%20reward%20terms%20and%20simple%20domain%20randomization%20shared%20by%20all%20tasks%2C%0Awithout%20any%20learning%20curriculum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26633v1&entry.124074799=Read"},
{"title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents", "author": "Zhen Yang and Zi-Yi Dou and Di Feng and Forrest Huang and Anh Nguyen and Keen You and Omar Attia and Yuhao Yang and Michael Feng and Haotian Zhang and Ram Ramrakhya and Chao Jia and Jeffrey Nichols and Alexander Toshev and Yinfei Yang and Zhe Gan", "abstract": "  Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of $91.6\\%$, $53.3\\%$, and $61.2\\%$ on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of $28.0\\%$ on AndroidWorld\nand $19.8\\%$ on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.\n", "link": "http://arxiv.org/abs/2509.26539v1", "date": "2025-09-30", "relevancy": 1.8398, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4727}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4532}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ferret-UI%20Lite%3A%20Lessons%20from%20Building%20Small%20On-Device%20GUI%20Agents&body=Title%3A%20Ferret-UI%20Lite%3A%20Lessons%20from%20Building%20Small%20On-Device%20GUI%20Agents%0AAuthor%3A%20Zhen%20Yang%20and%20Zi-Yi%20Dou%20and%20Di%20Feng%20and%20Forrest%20Huang%20and%20Anh%20Nguyen%20and%20Keen%20You%20and%20Omar%20Attia%20and%20Yuhao%20Yang%20and%20Michael%20Feng%20and%20Haotian%20Zhang%20and%20Ram%20Ramrakhya%20and%20Chao%20Jia%20and%20Jeffrey%20Nichols%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang%20and%20Zhe%20Gan%0AAbstract%3A%20%20%20Developing%20autonomous%20agents%20that%20effectively%20interact%20with%20Graphic%20User%0AInterfaces%20%28GUIs%29%20remains%20a%20challenging%20open%20problem%2C%20especially%20for%20small%0Aon-device%20models.%20In%20this%20paper%2C%20we%20present%20Ferret-UI%20Lite%2C%20a%20compact%2C%0Aend-to-end%20GUI%20agent%20that%20operates%20across%20diverse%20platforms%2C%20including%20mobile%2C%0Aweb%2C%20and%20desktop.%20Utilizing%20techniques%20optimized%20for%20developing%20small%20models%2C%0Awe%20build%20our%203B%20Ferret-UI%20Lite%20agent%20through%20curating%20a%20diverse%20GUI%20data%0Amixture%20from%20real%20and%20synthetic%20sources%2C%20strengthening%20inference-time%0Aperformance%20through%20chain-of-thought%20reasoning%20and%20visual%20tool-use%2C%20and%0Areinforcement%20learning%20with%20designed%20rewards.%20Ferret-UI%20Lite%20achieves%0Acompetitive%20performance%20with%20other%20small-scale%20GUI%20agents.%20In%20GUI%20grounding%2C%0AFerret-UI%20Lite%20attains%20scores%20of%20%2491.6%5C%25%24%2C%20%2453.3%5C%25%24%2C%20and%20%2461.2%5C%25%24%20on%20the%0AScreenSpot-V2%2C%20ScreenSpot-Pro%2C%20and%20OSWorld-G%20benchmarks%2C%20respectively.%20For%20GUI%0Anavigation%2C%20Ferret-UI%20Lite%20achieves%20success%20rates%20of%20%2428.0%5C%25%24%20on%20AndroidWorld%0Aand%20%2419.8%5C%25%24%20on%20OSWorld.%20We%20share%20our%20methods%20and%20lessons%20learned%20from%0Adeveloping%20compact%2C%20on-device%20GUI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFerret-UI%2520Lite%253A%2520Lessons%2520from%2520Building%2520Small%2520On-Device%2520GUI%2520Agents%26entry.906535625%3DZhen%2520Yang%2520and%2520Zi-Yi%2520Dou%2520and%2520Di%2520Feng%2520and%2520Forrest%2520Huang%2520and%2520Anh%2520Nguyen%2520and%2520Keen%2520You%2520and%2520Omar%2520Attia%2520and%2520Yuhao%2520Yang%2520and%2520Michael%2520Feng%2520and%2520Haotian%2520Zhang%2520and%2520Ram%2520Ramrakhya%2520and%2520Chao%2520Jia%2520and%2520Jeffrey%2520Nichols%2520and%2520Alexander%2520Toshev%2520and%2520Yinfei%2520Yang%2520and%2520Zhe%2520Gan%26entry.1292438233%3D%2520%2520Developing%2520autonomous%2520agents%2520that%2520effectively%2520interact%2520with%2520Graphic%2520User%250AInterfaces%2520%2528GUIs%2529%2520remains%2520a%2520challenging%2520open%2520problem%252C%2520especially%2520for%2520small%250Aon-device%2520models.%2520In%2520this%2520paper%252C%2520we%2520present%2520Ferret-UI%2520Lite%252C%2520a%2520compact%252C%250Aend-to-end%2520GUI%2520agent%2520that%2520operates%2520across%2520diverse%2520platforms%252C%2520including%2520mobile%252C%250Aweb%252C%2520and%2520desktop.%2520Utilizing%2520techniques%2520optimized%2520for%2520developing%2520small%2520models%252C%250Awe%2520build%2520our%25203B%2520Ferret-UI%2520Lite%2520agent%2520through%2520curating%2520a%2520diverse%2520GUI%2520data%250Amixture%2520from%2520real%2520and%2520synthetic%2520sources%252C%2520strengthening%2520inference-time%250Aperformance%2520through%2520chain-of-thought%2520reasoning%2520and%2520visual%2520tool-use%252C%2520and%250Areinforcement%2520learning%2520with%2520designed%2520rewards.%2520Ferret-UI%2520Lite%2520achieves%250Acompetitive%2520performance%2520with%2520other%2520small-scale%2520GUI%2520agents.%2520In%2520GUI%2520grounding%252C%250AFerret-UI%2520Lite%2520attains%2520scores%2520of%2520%252491.6%255C%2525%2524%252C%2520%252453.3%255C%2525%2524%252C%2520and%2520%252461.2%255C%2525%2524%2520on%2520the%250AScreenSpot-V2%252C%2520ScreenSpot-Pro%252C%2520and%2520OSWorld-G%2520benchmarks%252C%2520respectively.%2520For%2520GUI%250Anavigation%252C%2520Ferret-UI%2520Lite%2520achieves%2520success%2520rates%2520of%2520%252428.0%255C%2525%2524%2520on%2520AndroidWorld%250Aand%2520%252419.8%255C%2525%2524%2520on%2520OSWorld.%2520We%2520share%2520our%2520methods%2520and%2520lessons%2520learned%2520from%250Adeveloping%2520compact%252C%2520on-device%2520GUI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ferret-UI%20Lite%3A%20Lessons%20from%20Building%20Small%20On-Device%20GUI%20Agents&entry.906535625=Zhen%20Yang%20and%20Zi-Yi%20Dou%20and%20Di%20Feng%20and%20Forrest%20Huang%20and%20Anh%20Nguyen%20and%20Keen%20You%20and%20Omar%20Attia%20and%20Yuhao%20Yang%20and%20Michael%20Feng%20and%20Haotian%20Zhang%20and%20Ram%20Ramrakhya%20and%20Chao%20Jia%20and%20Jeffrey%20Nichols%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang%20and%20Zhe%20Gan&entry.1292438233=%20%20Developing%20autonomous%20agents%20that%20effectively%20interact%20with%20Graphic%20User%0AInterfaces%20%28GUIs%29%20remains%20a%20challenging%20open%20problem%2C%20especially%20for%20small%0Aon-device%20models.%20In%20this%20paper%2C%20we%20present%20Ferret-UI%20Lite%2C%20a%20compact%2C%0Aend-to-end%20GUI%20agent%20that%20operates%20across%20diverse%20platforms%2C%20including%20mobile%2C%0Aweb%2C%20and%20desktop.%20Utilizing%20techniques%20optimized%20for%20developing%20small%20models%2C%0Awe%20build%20our%203B%20Ferret-UI%20Lite%20agent%20through%20curating%20a%20diverse%20GUI%20data%0Amixture%20from%20real%20and%20synthetic%20sources%2C%20strengthening%20inference-time%0Aperformance%20through%20chain-of-thought%20reasoning%20and%20visual%20tool-use%2C%20and%0Areinforcement%20learning%20with%20designed%20rewards.%20Ferret-UI%20Lite%20achieves%0Acompetitive%20performance%20with%20other%20small-scale%20GUI%20agents.%20In%20GUI%20grounding%2C%0AFerret-UI%20Lite%20attains%20scores%20of%20%2491.6%5C%25%24%2C%20%2453.3%5C%25%24%2C%20and%20%2461.2%5C%25%24%20on%20the%0AScreenSpot-V2%2C%20ScreenSpot-Pro%2C%20and%20OSWorld-G%20benchmarks%2C%20respectively.%20For%20GUI%0Anavigation%2C%20Ferret-UI%20Lite%20achieves%20success%20rates%20of%20%2428.0%5C%25%24%20on%20AndroidWorld%0Aand%20%2419.8%5C%25%24%20on%20OSWorld.%20We%20share%20our%20methods%20and%20lessons%20learned%20from%0Adeveloping%20compact%2C%20on-device%20GUI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26539v1&entry.124074799=Read"},
{"title": "Winning Gold at IMO 2025 with a Model-Agnostic\n  Verification-and-Refinement Pipeline", "author": "Yichen Huang and Lin F. Yang", "abstract": "  The International Mathematical Olympiad (IMO) is widely regarded as the world\nchampionship of high-school mathematics. IMO problems are renowned for their\ndifficulty and novelty, demanding deep insight, creativity, and rigor. Although\nlarge language models perform well on many mathematical benchmarks, they often\nstruggle with Olympiad-level problems. Using carefully designed prompts, we\nconstruct a model-agnostic, verification-and-refinement pipeline. We\ndemonstrate its effectiveness on the recent IMO 2025, avoiding data\ncontamination for models released before the competition. Equipped with any of\nthe three leading models -- Gemini 2.5 Pro, Grok-4, or GPT-5 -- our pipeline\ncorrectly solved 5 out of the 6 problems ($\\approx$85.7% accuracy). This is in\nsharp contrast to their baseline accuracies: 31.6% (Gemini 2.5 Pro), 21.4%\n(Grok-4), and 38.1% (GPT-5), obtained by selecting the best of 32 candidate\nsolutions. The substantial improvement underscores that the path to advanced AI\nreasoning requires not only developing more powerful base models but also\ndesigning effective methodologies to harness their full potential for complex\ntasks.\n", "link": "http://arxiv.org/abs/2507.15855v4", "date": "2025-09-30", "relevancy": 1.818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Winning%20Gold%20at%20IMO%202025%20with%20a%20Model-Agnostic%0A%20%20Verification-and-Refinement%20Pipeline&body=Title%3A%20Winning%20Gold%20at%20IMO%202025%20with%20a%20Model-Agnostic%0A%20%20Verification-and-Refinement%20Pipeline%0AAuthor%3A%20Yichen%20Huang%20and%20Lin%20F.%20Yang%0AAbstract%3A%20%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20is%20widely%20regarded%20as%20the%20world%0Achampionship%20of%20high-school%20mathematics.%20IMO%20problems%20are%20renowned%20for%20their%0Adifficulty%20and%20novelty%2C%20demanding%20deep%20insight%2C%20creativity%2C%20and%20rigor.%20Although%0Alarge%20language%20models%20perform%20well%20on%20many%20mathematical%20benchmarks%2C%20they%20often%0Astruggle%20with%20Olympiad-level%20problems.%20Using%20carefully%20designed%20prompts%2C%20we%0Aconstruct%20a%20model-agnostic%2C%20verification-and-refinement%20pipeline.%20We%0Ademonstrate%20its%20effectiveness%20on%20the%20recent%20IMO%202025%2C%20avoiding%20data%0Acontamination%20for%20models%20released%20before%20the%20competition.%20Equipped%20with%20any%20of%0Athe%20three%20leading%20models%20--%20Gemini%202.5%20Pro%2C%20Grok-4%2C%20or%20GPT-5%20--%20our%20pipeline%0Acorrectly%20solved%205%20out%20of%20the%206%20problems%20%28%24%5Capprox%2485.7%25%20accuracy%29.%20This%20is%20in%0Asharp%20contrast%20to%20their%20baseline%20accuracies%3A%2031.6%25%20%28Gemini%202.5%20Pro%29%2C%2021.4%25%0A%28Grok-4%29%2C%20and%2038.1%25%20%28GPT-5%29%2C%20obtained%20by%20selecting%20the%20best%20of%2032%20candidate%0Asolutions.%20The%20substantial%20improvement%20underscores%20that%20the%20path%20to%20advanced%20AI%0Areasoning%20requires%20not%20only%20developing%20more%20powerful%20base%20models%20but%20also%0Adesigning%20effective%20methodologies%20to%20harness%20their%20full%20potential%20for%20complex%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15855v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWinning%2520Gold%2520at%2520IMO%25202025%2520with%2520a%2520Model-Agnostic%250A%2520%2520Verification-and-Refinement%2520Pipeline%26entry.906535625%3DYichen%2520Huang%2520and%2520Lin%2520F.%2520Yang%26entry.1292438233%3D%2520%2520The%2520International%2520Mathematical%2520Olympiad%2520%2528IMO%2529%2520is%2520widely%2520regarded%2520as%2520the%2520world%250Achampionship%2520of%2520high-school%2520mathematics.%2520IMO%2520problems%2520are%2520renowned%2520for%2520their%250Adifficulty%2520and%2520novelty%252C%2520demanding%2520deep%2520insight%252C%2520creativity%252C%2520and%2520rigor.%2520Although%250Alarge%2520language%2520models%2520perform%2520well%2520on%2520many%2520mathematical%2520benchmarks%252C%2520they%2520often%250Astruggle%2520with%2520Olympiad-level%2520problems.%2520Using%2520carefully%2520designed%2520prompts%252C%2520we%250Aconstruct%2520a%2520model-agnostic%252C%2520verification-and-refinement%2520pipeline.%2520We%250Ademonstrate%2520its%2520effectiveness%2520on%2520the%2520recent%2520IMO%25202025%252C%2520avoiding%2520data%250Acontamination%2520for%2520models%2520released%2520before%2520the%2520competition.%2520Equipped%2520with%2520any%2520of%250Athe%2520three%2520leading%2520models%2520--%2520Gemini%25202.5%2520Pro%252C%2520Grok-4%252C%2520or%2520GPT-5%2520--%2520our%2520pipeline%250Acorrectly%2520solved%25205%2520out%2520of%2520the%25206%2520problems%2520%2528%2524%255Capprox%252485.7%2525%2520accuracy%2529.%2520This%2520is%2520in%250Asharp%2520contrast%2520to%2520their%2520baseline%2520accuracies%253A%252031.6%2525%2520%2528Gemini%25202.5%2520Pro%2529%252C%252021.4%2525%250A%2528Grok-4%2529%252C%2520and%252038.1%2525%2520%2528GPT-5%2529%252C%2520obtained%2520by%2520selecting%2520the%2520best%2520of%252032%2520candidate%250Asolutions.%2520The%2520substantial%2520improvement%2520underscores%2520that%2520the%2520path%2520to%2520advanced%2520AI%250Areasoning%2520requires%2520not%2520only%2520developing%2520more%2520powerful%2520base%2520models%2520but%2520also%250Adesigning%2520effective%2520methodologies%2520to%2520harness%2520their%2520full%2520potential%2520for%2520complex%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15855v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Winning%20Gold%20at%20IMO%202025%20with%20a%20Model-Agnostic%0A%20%20Verification-and-Refinement%20Pipeline&entry.906535625=Yichen%20Huang%20and%20Lin%20F.%20Yang&entry.1292438233=%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20is%20widely%20regarded%20as%20the%20world%0Achampionship%20of%20high-school%20mathematics.%20IMO%20problems%20are%20renowned%20for%20their%0Adifficulty%20and%20novelty%2C%20demanding%20deep%20insight%2C%20creativity%2C%20and%20rigor.%20Although%0Alarge%20language%20models%20perform%20well%20on%20many%20mathematical%20benchmarks%2C%20they%20often%0Astruggle%20with%20Olympiad-level%20problems.%20Using%20carefully%20designed%20prompts%2C%20we%0Aconstruct%20a%20model-agnostic%2C%20verification-and-refinement%20pipeline.%20We%0Ademonstrate%20its%20effectiveness%20on%20the%20recent%20IMO%202025%2C%20avoiding%20data%0Acontamination%20for%20models%20released%20before%20the%20competition.%20Equipped%20with%20any%20of%0Athe%20three%20leading%20models%20--%20Gemini%202.5%20Pro%2C%20Grok-4%2C%20or%20GPT-5%20--%20our%20pipeline%0Acorrectly%20solved%205%20out%20of%20the%206%20problems%20%28%24%5Capprox%2485.7%25%20accuracy%29.%20This%20is%20in%0Asharp%20contrast%20to%20their%20baseline%20accuracies%3A%2031.6%25%20%28Gemini%202.5%20Pro%29%2C%2021.4%25%0A%28Grok-4%29%2C%20and%2038.1%25%20%28GPT-5%29%2C%20obtained%20by%20selecting%20the%20best%20of%2032%20candidate%0Asolutions.%20The%20substantial%20improvement%20underscores%20that%20the%20path%20to%20advanced%20AI%0Areasoning%20requires%20not%20only%20developing%20more%20powerful%20base%20models%20but%20also%0Adesigning%20effective%20methodologies%20to%20harness%20their%20full%20potential%20for%20complex%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15855v4&entry.124074799=Read"},
{"title": "TVS Sidekick: Challenges and Practical Insights from Deploying Large\n  Language Models in the Enterprise", "author": "Paula Reyero Lobo and Kevin Johnson and Bill Buchanan and Matthew Shardlow and Ashley Williams and Samuel Attwood", "abstract": "  Many enterprises are increasingly adopting Artificial Intelligence (AI) to\nmake internal processes more competitive and efficient. In response to public\nconcern and new regulations for the ethical and responsible use of AI,\nimplementing AI governance frameworks could help to integrate AI within\norganisations and mitigate associated risks. However, the rapid technological\nadvances and lack of shared ethical AI infrastructures creates barriers to\ntheir practical adoption in businesses. This paper presents a real-world AI\napplication at TVS Supply Chain Solutions, reporting on the experience\ndeveloping an AI assistant underpinned by large language models and the\nethical, regulatory, and sociotechnical challenges in deployment for enterprise\nuse.\n", "link": "http://arxiv.org/abs/2509.26482v1", "date": "2025-09-30", "relevancy": 1.813, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TVS%20Sidekick%3A%20Challenges%20and%20Practical%20Insights%20from%20Deploying%20Large%0A%20%20Language%20Models%20in%20the%20Enterprise&body=Title%3A%20TVS%20Sidekick%3A%20Challenges%20and%20Practical%20Insights%20from%20Deploying%20Large%0A%20%20Language%20Models%20in%20the%20Enterprise%0AAuthor%3A%20Paula%20Reyero%20Lobo%20and%20Kevin%20Johnson%20and%20Bill%20Buchanan%20and%20Matthew%20Shardlow%20and%20Ashley%20Williams%20and%20Samuel%20Attwood%0AAbstract%3A%20%20%20Many%20enterprises%20are%20increasingly%20adopting%20Artificial%20Intelligence%20%28AI%29%20to%0Amake%20internal%20processes%20more%20competitive%20and%20efficient.%20In%20response%20to%20public%0Aconcern%20and%20new%20regulations%20for%20the%20ethical%20and%20responsible%20use%20of%20AI%2C%0Aimplementing%20AI%20governance%20frameworks%20could%20help%20to%20integrate%20AI%20within%0Aorganisations%20and%20mitigate%20associated%20risks.%20However%2C%20the%20rapid%20technological%0Aadvances%20and%20lack%20of%20shared%20ethical%20AI%20infrastructures%20creates%20barriers%20to%0Atheir%20practical%20adoption%20in%20businesses.%20This%20paper%20presents%20a%20real-world%20AI%0Aapplication%20at%20TVS%20Supply%20Chain%20Solutions%2C%20reporting%20on%20the%20experience%0Adeveloping%20an%20AI%20assistant%20underpinned%20by%20large%20language%20models%20and%20the%0Aethical%2C%20regulatory%2C%20and%20sociotechnical%20challenges%20in%20deployment%20for%20enterprise%0Ause.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTVS%2520Sidekick%253A%2520Challenges%2520and%2520Practical%2520Insights%2520from%2520Deploying%2520Large%250A%2520%2520Language%2520Models%2520in%2520the%2520Enterprise%26entry.906535625%3DPaula%2520Reyero%2520Lobo%2520and%2520Kevin%2520Johnson%2520and%2520Bill%2520Buchanan%2520and%2520Matthew%2520Shardlow%2520and%2520Ashley%2520Williams%2520and%2520Samuel%2520Attwood%26entry.1292438233%3D%2520%2520Many%2520enterprises%2520are%2520increasingly%2520adopting%2520Artificial%2520Intelligence%2520%2528AI%2529%2520to%250Amake%2520internal%2520processes%2520more%2520competitive%2520and%2520efficient.%2520In%2520response%2520to%2520public%250Aconcern%2520and%2520new%2520regulations%2520for%2520the%2520ethical%2520and%2520responsible%2520use%2520of%2520AI%252C%250Aimplementing%2520AI%2520governance%2520frameworks%2520could%2520help%2520to%2520integrate%2520AI%2520within%250Aorganisations%2520and%2520mitigate%2520associated%2520risks.%2520However%252C%2520the%2520rapid%2520technological%250Aadvances%2520and%2520lack%2520of%2520shared%2520ethical%2520AI%2520infrastructures%2520creates%2520barriers%2520to%250Atheir%2520practical%2520adoption%2520in%2520businesses.%2520This%2520paper%2520presents%2520a%2520real-world%2520AI%250Aapplication%2520at%2520TVS%2520Supply%2520Chain%2520Solutions%252C%2520reporting%2520on%2520the%2520experience%250Adeveloping%2520an%2520AI%2520assistant%2520underpinned%2520by%2520large%2520language%2520models%2520and%2520the%250Aethical%252C%2520regulatory%252C%2520and%2520sociotechnical%2520challenges%2520in%2520deployment%2520for%2520enterprise%250Ause.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TVS%20Sidekick%3A%20Challenges%20and%20Practical%20Insights%20from%20Deploying%20Large%0A%20%20Language%20Models%20in%20the%20Enterprise&entry.906535625=Paula%20Reyero%20Lobo%20and%20Kevin%20Johnson%20and%20Bill%20Buchanan%20and%20Matthew%20Shardlow%20and%20Ashley%20Williams%20and%20Samuel%20Attwood&entry.1292438233=%20%20Many%20enterprises%20are%20increasingly%20adopting%20Artificial%20Intelligence%20%28AI%29%20to%0Amake%20internal%20processes%20more%20competitive%20and%20efficient.%20In%20response%20to%20public%0Aconcern%20and%20new%20regulations%20for%20the%20ethical%20and%20responsible%20use%20of%20AI%2C%0Aimplementing%20AI%20governance%20frameworks%20could%20help%20to%20integrate%20AI%20within%0Aorganisations%20and%20mitigate%20associated%20risks.%20However%2C%20the%20rapid%20technological%0Aadvances%20and%20lack%20of%20shared%20ethical%20AI%20infrastructures%20creates%20barriers%20to%0Atheir%20practical%20adoption%20in%20businesses.%20This%20paper%20presents%20a%20real-world%20AI%0Aapplication%20at%20TVS%20Supply%20Chain%20Solutions%2C%20reporting%20on%20the%20experience%0Adeveloping%20an%20AI%20assistant%20underpinned%20by%20large%20language%20models%20and%20the%0Aethical%2C%20regulatory%2C%20and%20sociotechnical%20challenges%20in%20deployment%20for%20enterprise%0Ause.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26482v1&entry.124074799=Read"},
{"title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks", "author": "Wenda Xu and Sweta Agrawal and Vil\u00e9m Zouhar and Markus Freitag and Daniel Deutsch", "abstract": "  As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.\n", "link": "http://arxiv.org/abs/2509.26600v1", "date": "2025-09-30", "relevancy": 1.8066, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconstructing%20Self-Bias%20in%20LLM-generated%20Translation%20Benchmarks&body=Title%3A%20Deconstructing%20Self-Bias%20in%20LLM-generated%20Translation%20Benchmarks%0AAuthor%3A%20Wenda%20Xu%20and%20Sweta%20Agrawal%20and%20Vil%C3%A9m%20Zouhar%20and%20Markus%20Freitag%20and%20Daniel%20Deutsch%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20begin%20to%20saturate%20existing%20benchmarks%2C%0Aautomated%20benchmark%20creation%20using%20LLMs%20%28LLM%20as%20a%20benchmark%29%20has%20emerged%20as%20a%0Ascalable%20alternative%20to%20slow%20and%20costly%20human%20curation.%20While%20these%20generated%0Atest%20sets%20have%20to%20potential%20to%20cheaply%20rank%20models%2C%20we%20demonstrate%20a%20critical%0Aflaw.%20LLM%20generated%20benchmarks%20systematically%20favor%20the%20model%20that%20created%20the%0Abenchmark%2C%20they%20exhibit%20self%20bias%20on%20low%20resource%20languages%20to%20English%0Atranslation%20tasks.%20We%20show%20three%20key%20findings%20on%20automatic%20benchmarking%20of%20LLMs%0Afor%20translation%3A%20First%2C%20this%20bias%20originates%20from%20two%20sources%3A%20the%20generated%0Atest%20data%20%28LLM%20as%20a%20testset%29%20and%20the%20evaluation%20method%20%28LLM%20as%20an%20evaluator%29%2C%0Awith%20their%20combination%20amplifying%20the%20effect.%20Second%2C%20self%20bias%20in%20LLM%20as%20a%0Abenchmark%20is%20heavily%20influenced%20by%20the%20model%27s%20generation%20capabilities%20in%20the%0Asource%20language.%20For%20instance%2C%20we%20observe%20more%20pronounced%20bias%20in%20into%20English%0Atranslation%2C%20where%20the%20model%27s%20generation%20system%20is%20developed%2C%20than%20in%20out%20of%0AEnglish%20translation%20tasks.%20Third%2C%20we%20observe%20that%20low%20diversity%20in%20source%20text%0Ais%20one%20attribution%20to%20self%20bias.%20Our%20results%20suggest%20that%20improving%20the%0Adiversity%20of%20these%20generated%20source%20texts%20can%20mitigate%20some%20of%20the%20observed%0Aself%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconstructing%2520Self-Bias%2520in%2520LLM-generated%2520Translation%2520Benchmarks%26entry.906535625%3DWenda%2520Xu%2520and%2520Sweta%2520Agrawal%2520and%2520Vil%25C3%25A9m%2520Zouhar%2520and%2520Markus%2520Freitag%2520and%2520Daniel%2520Deutsch%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520begin%2520to%2520saturate%2520existing%2520benchmarks%252C%250Aautomated%2520benchmark%2520creation%2520using%2520LLMs%2520%2528LLM%2520as%2520a%2520benchmark%2529%2520has%2520emerged%2520as%2520a%250Ascalable%2520alternative%2520to%2520slow%2520and%2520costly%2520human%2520curation.%2520While%2520these%2520generated%250Atest%2520sets%2520have%2520to%2520potential%2520to%2520cheaply%2520rank%2520models%252C%2520we%2520demonstrate%2520a%2520critical%250Aflaw.%2520LLM%2520generated%2520benchmarks%2520systematically%2520favor%2520the%2520model%2520that%2520created%2520the%250Abenchmark%252C%2520they%2520exhibit%2520self%2520bias%2520on%2520low%2520resource%2520languages%2520to%2520English%250Atranslation%2520tasks.%2520We%2520show%2520three%2520key%2520findings%2520on%2520automatic%2520benchmarking%2520of%2520LLMs%250Afor%2520translation%253A%2520First%252C%2520this%2520bias%2520originates%2520from%2520two%2520sources%253A%2520the%2520generated%250Atest%2520data%2520%2528LLM%2520as%2520a%2520testset%2529%2520and%2520the%2520evaluation%2520method%2520%2528LLM%2520as%2520an%2520evaluator%2529%252C%250Awith%2520their%2520combination%2520amplifying%2520the%2520effect.%2520Second%252C%2520self%2520bias%2520in%2520LLM%2520as%2520a%250Abenchmark%2520is%2520heavily%2520influenced%2520by%2520the%2520model%2527s%2520generation%2520capabilities%2520in%2520the%250Asource%2520language.%2520For%2520instance%252C%2520we%2520observe%2520more%2520pronounced%2520bias%2520in%2520into%2520English%250Atranslation%252C%2520where%2520the%2520model%2527s%2520generation%2520system%2520is%2520developed%252C%2520than%2520in%2520out%2520of%250AEnglish%2520translation%2520tasks.%2520Third%252C%2520we%2520observe%2520that%2520low%2520diversity%2520in%2520source%2520text%250Ais%2520one%2520attribution%2520to%2520self%2520bias.%2520Our%2520results%2520suggest%2520that%2520improving%2520the%250Adiversity%2520of%2520these%2520generated%2520source%2520texts%2520can%2520mitigate%2520some%2520of%2520the%2520observed%250Aself%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconstructing%20Self-Bias%20in%20LLM-generated%20Translation%20Benchmarks&entry.906535625=Wenda%20Xu%20and%20Sweta%20Agrawal%20and%20Vil%C3%A9m%20Zouhar%20and%20Markus%20Freitag%20and%20Daniel%20Deutsch&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20begin%20to%20saturate%20existing%20benchmarks%2C%0Aautomated%20benchmark%20creation%20using%20LLMs%20%28LLM%20as%20a%20benchmark%29%20has%20emerged%20as%20a%0Ascalable%20alternative%20to%20slow%20and%20costly%20human%20curation.%20While%20these%20generated%0Atest%20sets%20have%20to%20potential%20to%20cheaply%20rank%20models%2C%20we%20demonstrate%20a%20critical%0Aflaw.%20LLM%20generated%20benchmarks%20systematically%20favor%20the%20model%20that%20created%20the%0Abenchmark%2C%20they%20exhibit%20self%20bias%20on%20low%20resource%20languages%20to%20English%0Atranslation%20tasks.%20We%20show%20three%20key%20findings%20on%20automatic%20benchmarking%20of%20LLMs%0Afor%20translation%3A%20First%2C%20this%20bias%20originates%20from%20two%20sources%3A%20the%20generated%0Atest%20data%20%28LLM%20as%20a%20testset%29%20and%20the%20evaluation%20method%20%28LLM%20as%20an%20evaluator%29%2C%0Awith%20their%20combination%20amplifying%20the%20effect.%20Second%2C%20self%20bias%20in%20LLM%20as%20a%0Abenchmark%20is%20heavily%20influenced%20by%20the%20model%27s%20generation%20capabilities%20in%20the%0Asource%20language.%20For%20instance%2C%20we%20observe%20more%20pronounced%20bias%20in%20into%20English%0Atranslation%2C%20where%20the%20model%27s%20generation%20system%20is%20developed%2C%20than%20in%20out%20of%0AEnglish%20translation%20tasks.%20Third%2C%20we%20observe%20that%20low%20diversity%20in%20source%20text%0Ais%20one%20attribution%20to%20self%20bias.%20Our%20results%20suggest%20that%20improving%20the%0Adiversity%20of%20these%20generated%20source%20texts%20can%20mitigate%20some%20of%20the%20observed%0Aself%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26600v1&entry.124074799=Read"},
{"title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model\n  early exiting", "author": "Xi Wang and James McInerney and Lequn Wang and Nathan Kallus", "abstract": "  Large reasoning models show improved performance with longer chains of\nthought. However, recent work has highlighted (qualitatively) their tendency to\noverthink, continuing to revise answers even after reaching the correct\nsolution. We quantitatively confirm this inefficiency by tracking Pass@1 for\nanswers averaged over a large number of rollouts and find that the model often\nbegins to always produce the correct answer early in the reasoning, making\nextra reasoning a waste of tokens. To detect and prevent overthinking, we\npropose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)\n-- for monitoring and deciding whether to exit reasoning early. By appending a\nstop thinking token (</think>) and monitoring the entropy of the following\ntoken as the model reasons, we obtain a trajectory that decreases and\nstabilizes when Pass@1 plateaus; thresholding its variance under an exponential\nmoving average yields a practical stopping rule. Importantly, our approach\nenables adaptively allocating compute based on the EAT trajectory, allowing us\nto spend compute in a more efficient way compared with fixing the token budget\nfor all questions. Empirically, on MATH500 and AIME2025, EAT reduces token\nusage by 13 - 21% without harming accuracy, and it remains effective in black\nbox settings where logits from the reasoning model are not accessible, and EAT\nis computed with proxy models.\n", "link": "http://arxiv.org/abs/2509.26522v1", "date": "2025-09-30", "relevancy": 1.7867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4319}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy%20After%20%24%5Clangle%20%5Ctexttt%7B/Think%7D%20%5Crangle%24%20for%20reasoning%20model%0A%20%20early%20exiting&body=Title%3A%20Entropy%20After%20%24%5Clangle%20%5Ctexttt%7B/Think%7D%20%5Crangle%24%20for%20reasoning%20model%0A%20%20early%20exiting%0AAuthor%3A%20Xi%20Wang%20and%20James%20McInerney%20and%20Lequn%20Wang%20and%20Nathan%20Kallus%0AAbstract%3A%20%20%20Large%20reasoning%20models%20show%20improved%20performance%20with%20longer%20chains%20of%0Athought.%20However%2C%20recent%20work%20has%20highlighted%20%28qualitatively%29%20their%20tendency%20to%0Aoverthink%2C%20continuing%20to%20revise%20answers%20even%20after%20reaching%20the%20correct%0Asolution.%20We%20quantitatively%20confirm%20this%20inefficiency%20by%20tracking%20Pass%401%20for%0Aanswers%20averaged%20over%20a%20large%20number%20of%20rollouts%20and%20find%20that%20the%20model%20often%0Abegins%20to%20always%20produce%20the%20correct%20answer%20early%20in%20the%20reasoning%2C%20making%0Aextra%20reasoning%20a%20waste%20of%20tokens.%20To%20detect%20and%20prevent%20overthinking%2C%20we%0Apropose%20a%20simple%20and%20inexpensive%20novel%20signal%20--%20Entropy%20After%20%3C/Think%3E%20%28EAT%29%0A--%20for%20monitoring%20and%20deciding%20whether%20to%20exit%20reasoning%20early.%20By%20appending%20a%0Astop%20thinking%20token%20%28%3C/think%3E%29%20and%20monitoring%20the%20entropy%20of%20the%20following%0Atoken%20as%20the%20model%20reasons%2C%20we%20obtain%20a%20trajectory%20that%20decreases%20and%0Astabilizes%20when%20Pass%401%20plateaus%3B%20thresholding%20its%20variance%20under%20an%20exponential%0Amoving%20average%20yields%20a%20practical%20stopping%20rule.%20Importantly%2C%20our%20approach%0Aenables%20adaptively%20allocating%20compute%20based%20on%20the%20EAT%20trajectory%2C%20allowing%20us%0Ato%20spend%20compute%20in%20a%20more%20efficient%20way%20compared%20with%20fixing%20the%20token%20budget%0Afor%20all%20questions.%20Empirically%2C%20on%20MATH500%20and%20AIME2025%2C%20EAT%20reduces%20token%0Ausage%20by%2013%20-%2021%25%20without%20harming%20accuracy%2C%20and%20it%20remains%20effective%20in%20black%0Abox%20settings%20where%20logits%20from%20the%20reasoning%20model%20are%20not%20accessible%2C%20and%20EAT%0Ais%20computed%20with%20proxy%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy%2520After%2520%2524%255Clangle%2520%255Ctexttt%257B/Think%257D%2520%255Crangle%2524%2520for%2520reasoning%2520model%250A%2520%2520early%2520exiting%26entry.906535625%3DXi%2520Wang%2520and%2520James%2520McInerney%2520and%2520Lequn%2520Wang%2520and%2520Nathan%2520Kallus%26entry.1292438233%3D%2520%2520Large%2520reasoning%2520models%2520show%2520improved%2520performance%2520with%2520longer%2520chains%2520of%250Athought.%2520However%252C%2520recent%2520work%2520has%2520highlighted%2520%2528qualitatively%2529%2520their%2520tendency%2520to%250Aoverthink%252C%2520continuing%2520to%2520revise%2520answers%2520even%2520after%2520reaching%2520the%2520correct%250Asolution.%2520We%2520quantitatively%2520confirm%2520this%2520inefficiency%2520by%2520tracking%2520Pass%25401%2520for%250Aanswers%2520averaged%2520over%2520a%2520large%2520number%2520of%2520rollouts%2520and%2520find%2520that%2520the%2520model%2520often%250Abegins%2520to%2520always%2520produce%2520the%2520correct%2520answer%2520early%2520in%2520the%2520reasoning%252C%2520making%250Aextra%2520reasoning%2520a%2520waste%2520of%2520tokens.%2520To%2520detect%2520and%2520prevent%2520overthinking%252C%2520we%250Apropose%2520a%2520simple%2520and%2520inexpensive%2520novel%2520signal%2520--%2520Entropy%2520After%2520%253C/Think%253E%2520%2528EAT%2529%250A--%2520for%2520monitoring%2520and%2520deciding%2520whether%2520to%2520exit%2520reasoning%2520early.%2520By%2520appending%2520a%250Astop%2520thinking%2520token%2520%2528%253C/think%253E%2529%2520and%2520monitoring%2520the%2520entropy%2520of%2520the%2520following%250Atoken%2520as%2520the%2520model%2520reasons%252C%2520we%2520obtain%2520a%2520trajectory%2520that%2520decreases%2520and%250Astabilizes%2520when%2520Pass%25401%2520plateaus%253B%2520thresholding%2520its%2520variance%2520under%2520an%2520exponential%250Amoving%2520average%2520yields%2520a%2520practical%2520stopping%2520rule.%2520Importantly%252C%2520our%2520approach%250Aenables%2520adaptively%2520allocating%2520compute%2520based%2520on%2520the%2520EAT%2520trajectory%252C%2520allowing%2520us%250Ato%2520spend%2520compute%2520in%2520a%2520more%2520efficient%2520way%2520compared%2520with%2520fixing%2520the%2520token%2520budget%250Afor%2520all%2520questions.%2520Empirically%252C%2520on%2520MATH500%2520and%2520AIME2025%252C%2520EAT%2520reduces%2520token%250Ausage%2520by%252013%2520-%252021%2525%2520without%2520harming%2520accuracy%252C%2520and%2520it%2520remains%2520effective%2520in%2520black%250Abox%2520settings%2520where%2520logits%2520from%2520the%2520reasoning%2520model%2520are%2520not%2520accessible%252C%2520and%2520EAT%250Ais%2520computed%2520with%2520proxy%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy%20After%20%24%5Clangle%20%5Ctexttt%7B/Think%7D%20%5Crangle%24%20for%20reasoning%20model%0A%20%20early%20exiting&entry.906535625=Xi%20Wang%20and%20James%20McInerney%20and%20Lequn%20Wang%20and%20Nathan%20Kallus&entry.1292438233=%20%20Large%20reasoning%20models%20show%20improved%20performance%20with%20longer%20chains%20of%0Athought.%20However%2C%20recent%20work%20has%20highlighted%20%28qualitatively%29%20their%20tendency%20to%0Aoverthink%2C%20continuing%20to%20revise%20answers%20even%20after%20reaching%20the%20correct%0Asolution.%20We%20quantitatively%20confirm%20this%20inefficiency%20by%20tracking%20Pass%401%20for%0Aanswers%20averaged%20over%20a%20large%20number%20of%20rollouts%20and%20find%20that%20the%20model%20often%0Abegins%20to%20always%20produce%20the%20correct%20answer%20early%20in%20the%20reasoning%2C%20making%0Aextra%20reasoning%20a%20waste%20of%20tokens.%20To%20detect%20and%20prevent%20overthinking%2C%20we%0Apropose%20a%20simple%20and%20inexpensive%20novel%20signal%20--%20Entropy%20After%20%3C/Think%3E%20%28EAT%29%0A--%20for%20monitoring%20and%20deciding%20whether%20to%20exit%20reasoning%20early.%20By%20appending%20a%0Astop%20thinking%20token%20%28%3C/think%3E%29%20and%20monitoring%20the%20entropy%20of%20the%20following%0Atoken%20as%20the%20model%20reasons%2C%20we%20obtain%20a%20trajectory%20that%20decreases%20and%0Astabilizes%20when%20Pass%401%20plateaus%3B%20thresholding%20its%20variance%20under%20an%20exponential%0Amoving%20average%20yields%20a%20practical%20stopping%20rule.%20Importantly%2C%20our%20approach%0Aenables%20adaptively%20allocating%20compute%20based%20on%20the%20EAT%20trajectory%2C%20allowing%20us%0Ato%20spend%20compute%20in%20a%20more%20efficient%20way%20compared%20with%20fixing%20the%20token%20budget%0Afor%20all%20questions.%20Empirically%2C%20on%20MATH500%20and%20AIME2025%2C%20EAT%20reduces%20token%0Ausage%20by%2013%20-%2021%25%20without%20harming%20accuracy%2C%20and%20it%20remains%20effective%20in%20black%0Abox%20settings%20where%20logits%20from%20the%20reasoning%20model%20are%20not%20accessible%2C%20and%20EAT%0Ais%20computed%20with%20proxy%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26522v1&entry.124074799=Read"},
{"title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal\n  Foundation Models in Federated Learning", "author": "Seohyun Lee and Wenzhi Fang and Dong-Jun Han and Seyyedali Hosseinalipour and Christopher G. Brinton", "abstract": "  Federated Learning (FL), despite demonstrating impressive capabilities in the\ntraining of multiple models in a decentralized manner, has been shown to\nproduce a final model not necessarily well-suited to the needs of each client.\nWhile extensive work has been conducted on how to create tailored personalized\nmodels, called Personalized Federated Learning (PFL), less attention has been\ngiven to personalization via fine-tuning of foundation models with multi-task\nand multi-modal properties. Moreover, there exists a lack of understanding in\nthe literature on how to fine-tune and personalize such models in a setting\nthat is heterogeneous across clients not only in data, but also in tasks and\nmodalities. To address this gap in the literature, we propose TAP (Two-Stage\nAdaptive Personalization), which (i) leverages mismatched model architectures\nbetween the clients and server to selectively conduct replacement operations\nwhen it benefits a client's local tasks and (ii) engages in post-FL knowledge\ndistillation for capturing beneficial general knowledge without compromising\npersonalization. We also introduce the first convergence analysis of the server\nmodel under its modality-task pair architecture, and demonstrate that as the\nnumber of modality-task pairs increases, its ability to cater to all tasks\nsuffers. Through extensive experiments, we demonstrate the effectiveness of our\nproposed algorithm across a variety of datasets and tasks in comparison to a\nmultitude of baselines. Implementation code is publicly available at\nhttps://github.com/lee3296/TAP.\n", "link": "http://arxiv.org/abs/2509.26524v1", "date": "2025-09-30", "relevancy": 1.7611, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.625}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAP%3A%20Two-Stage%20Adaptive%20Personalization%20of%20Multi-task%20and%20Multi-Modal%0A%20%20Foundation%20Models%20in%20Federated%20Learning&body=Title%3A%20TAP%3A%20Two-Stage%20Adaptive%20Personalization%20of%20Multi-task%20and%20Multi-Modal%0A%20%20Foundation%20Models%20in%20Federated%20Learning%0AAuthor%3A%20Seohyun%20Lee%20and%20Wenzhi%20Fang%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20G.%20Brinton%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%2C%20despite%20demonstrating%20impressive%20capabilities%20in%20the%0Atraining%20of%20multiple%20models%20in%20a%20decentralized%20manner%2C%20has%20been%20shown%20to%0Aproduce%20a%20final%20model%20not%20necessarily%20well-suited%20to%20the%20needs%20of%20each%20client.%0AWhile%20extensive%20work%20has%20been%20conducted%20on%20how%20to%20create%20tailored%20personalized%0Amodels%2C%20called%20Personalized%20Federated%20Learning%20%28PFL%29%2C%20less%20attention%20has%20been%0Agiven%20to%20personalization%20via%20fine-tuning%20of%20foundation%20models%20with%20multi-task%0Aand%20multi-modal%20properties.%20Moreover%2C%20there%20exists%20a%20lack%20of%20understanding%20in%0Athe%20literature%20on%20how%20to%20fine-tune%20and%20personalize%20such%20models%20in%20a%20setting%0Athat%20is%20heterogeneous%20across%20clients%20not%20only%20in%20data%2C%20but%20also%20in%20tasks%20and%0Amodalities.%20To%20address%20this%20gap%20in%20the%20literature%2C%20we%20propose%20TAP%20%28Two-Stage%0AAdaptive%20Personalization%29%2C%20which%20%28i%29%20leverages%20mismatched%20model%20architectures%0Abetween%20the%20clients%20and%20server%20to%20selectively%20conduct%20replacement%20operations%0Awhen%20it%20benefits%20a%20client%27s%20local%20tasks%20and%20%28ii%29%20engages%20in%20post-FL%20knowledge%0Adistillation%20for%20capturing%20beneficial%20general%20knowledge%20without%20compromising%0Apersonalization.%20We%20also%20introduce%20the%20first%20convergence%20analysis%20of%20the%20server%0Amodel%20under%20its%20modality-task%20pair%20architecture%2C%20and%20demonstrate%20that%20as%20the%0Anumber%20of%20modality-task%20pairs%20increases%2C%20its%20ability%20to%20cater%20to%20all%20tasks%0Asuffers.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20algorithm%20across%20a%20variety%20of%20datasets%20and%20tasks%20in%20comparison%20to%20a%0Amultitude%20of%20baselines.%20Implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/lee3296/TAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAP%253A%2520Two-Stage%2520Adaptive%2520Personalization%2520of%2520Multi-task%2520and%2520Multi-Modal%250A%2520%2520Foundation%2520Models%2520in%2520Federated%2520Learning%26entry.906535625%3DSeohyun%2520Lee%2520and%2520Wenzhi%2520Fang%2520and%2520Dong-Jun%2520Han%2520and%2520Seyyedali%2520Hosseinalipour%2520and%2520Christopher%2520G.%2520Brinton%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%252C%2520despite%2520demonstrating%2520impressive%2520capabilities%2520in%2520the%250Atraining%2520of%2520multiple%2520models%2520in%2520a%2520decentralized%2520manner%252C%2520has%2520been%2520shown%2520to%250Aproduce%2520a%2520final%2520model%2520not%2520necessarily%2520well-suited%2520to%2520the%2520needs%2520of%2520each%2520client.%250AWhile%2520extensive%2520work%2520has%2520been%2520conducted%2520on%2520how%2520to%2520create%2520tailored%2520personalized%250Amodels%252C%2520called%2520Personalized%2520Federated%2520Learning%2520%2528PFL%2529%252C%2520less%2520attention%2520has%2520been%250Agiven%2520to%2520personalization%2520via%2520fine-tuning%2520of%2520foundation%2520models%2520with%2520multi-task%250Aand%2520multi-modal%2520properties.%2520Moreover%252C%2520there%2520exists%2520a%2520lack%2520of%2520understanding%2520in%250Athe%2520literature%2520on%2520how%2520to%2520fine-tune%2520and%2520personalize%2520such%2520models%2520in%2520a%2520setting%250Athat%2520is%2520heterogeneous%2520across%2520clients%2520not%2520only%2520in%2520data%252C%2520but%2520also%2520in%2520tasks%2520and%250Amodalities.%2520To%2520address%2520this%2520gap%2520in%2520the%2520literature%252C%2520we%2520propose%2520TAP%2520%2528Two-Stage%250AAdaptive%2520Personalization%2529%252C%2520which%2520%2528i%2529%2520leverages%2520mismatched%2520model%2520architectures%250Abetween%2520the%2520clients%2520and%2520server%2520to%2520selectively%2520conduct%2520replacement%2520operations%250Awhen%2520it%2520benefits%2520a%2520client%2527s%2520local%2520tasks%2520and%2520%2528ii%2529%2520engages%2520in%2520post-FL%2520knowledge%250Adistillation%2520for%2520capturing%2520beneficial%2520general%2520knowledge%2520without%2520compromising%250Apersonalization.%2520We%2520also%2520introduce%2520the%2520first%2520convergence%2520analysis%2520of%2520the%2520server%250Amodel%2520under%2520its%2520modality-task%2520pair%2520architecture%252C%2520and%2520demonstrate%2520that%2520as%2520the%250Anumber%2520of%2520modality-task%2520pairs%2520increases%252C%2520its%2520ability%2520to%2520cater%2520to%2520all%2520tasks%250Asuffers.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520algorithm%2520across%2520a%2520variety%2520of%2520datasets%2520and%2520tasks%2520in%2520comparison%2520to%2520a%250Amultitude%2520of%2520baselines.%2520Implementation%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lee3296/TAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAP%3A%20Two-Stage%20Adaptive%20Personalization%20of%20Multi-task%20and%20Multi-Modal%0A%20%20Foundation%20Models%20in%20Federated%20Learning&entry.906535625=Seohyun%20Lee%20and%20Wenzhi%20Fang%20and%20Dong-Jun%20Han%20and%20Seyyedali%20Hosseinalipour%20and%20Christopher%20G.%20Brinton&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%2C%20despite%20demonstrating%20impressive%20capabilities%20in%20the%0Atraining%20of%20multiple%20models%20in%20a%20decentralized%20manner%2C%20has%20been%20shown%20to%0Aproduce%20a%20final%20model%20not%20necessarily%20well-suited%20to%20the%20needs%20of%20each%20client.%0AWhile%20extensive%20work%20has%20been%20conducted%20on%20how%20to%20create%20tailored%20personalized%0Amodels%2C%20called%20Personalized%20Federated%20Learning%20%28PFL%29%2C%20less%20attention%20has%20been%0Agiven%20to%20personalization%20via%20fine-tuning%20of%20foundation%20models%20with%20multi-task%0Aand%20multi-modal%20properties.%20Moreover%2C%20there%20exists%20a%20lack%20of%20understanding%20in%0Athe%20literature%20on%20how%20to%20fine-tune%20and%20personalize%20such%20models%20in%20a%20setting%0Athat%20is%20heterogeneous%20across%20clients%20not%20only%20in%20data%2C%20but%20also%20in%20tasks%20and%0Amodalities.%20To%20address%20this%20gap%20in%20the%20literature%2C%20we%20propose%20TAP%20%28Two-Stage%0AAdaptive%20Personalization%29%2C%20which%20%28i%29%20leverages%20mismatched%20model%20architectures%0Abetween%20the%20clients%20and%20server%20to%20selectively%20conduct%20replacement%20operations%0Awhen%20it%20benefits%20a%20client%27s%20local%20tasks%20and%20%28ii%29%20engages%20in%20post-FL%20knowledge%0Adistillation%20for%20capturing%20beneficial%20general%20knowledge%20without%20compromising%0Apersonalization.%20We%20also%20introduce%20the%20first%20convergence%20analysis%20of%20the%20server%0Amodel%20under%20its%20modality-task%20pair%20architecture%2C%20and%20demonstrate%20that%20as%20the%0Anumber%20of%20modality-task%20pairs%20increases%2C%20its%20ability%20to%20cater%20to%20all%20tasks%0Asuffers.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20algorithm%20across%20a%20variety%20of%20datasets%20and%20tasks%20in%20comparison%20to%20a%0Amultitude%20of%20baselines.%20Implementation%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/lee3296/TAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26524v1&entry.124074799=Read"},
{"title": "Branching Out: Broadening AI Measurement and Evaluation with Measurement\n  Trees", "author": "Craig Greenberg and Patrick Hall and Theodore Jensen and Kristen Greene and Razvan Amironesei", "abstract": "  This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.\n", "link": "http://arxiv.org/abs/2509.26632v1", "date": "2025-09-30", "relevancy": 1.7599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Branching%20Out%3A%20Broadening%20AI%20Measurement%20and%20Evaluation%20with%20Measurement%0A%20%20Trees&body=Title%3A%20Branching%20Out%3A%20Broadening%20AI%20Measurement%20and%20Evaluation%20with%20Measurement%0A%20%20Trees%0AAuthor%3A%20Craig%20Greenberg%20and%20Patrick%20Hall%20and%20Theodore%20Jensen%20and%20Kristen%20Greene%20and%20Razvan%20Amironesei%0AAbstract%3A%20%20%20This%20paper%20introduces%20%5Ctextit%7Bmeasurement%20trees%7D%2C%20a%20novel%20class%20of%20metrics%0Adesigned%20to%20combine%20various%20constructs%20into%20an%20interpretable%20multi-level%0Arepresentation%20of%20a%20measurand.%20Unlike%20conventional%20metrics%20that%20yield%20single%0Avalues%2C%20vectors%2C%20surfaces%2C%20or%20categories%2C%20measurement%20trees%20produce%20a%0Ahierarchical%20directed%20graph%20in%20which%20each%20node%20summarizes%20its%20children%20through%0Auser-defined%20aggregation%20methods.%20In%20response%20to%20recent%20calls%20to%20expand%20the%0Ascope%20of%20AI%20system%20evaluation%2C%20measurement%20trees%20enhance%20metric%20transparency%0Aand%20facilitate%20the%20integration%20of%20heterogeneous%20evidence%2C%20including%2C%20e.g.%2C%0Aagentic%2C%20business%2C%20energy-efficiency%2C%20sociotechnical%2C%20or%20security%20signals.%20We%0Apresent%20definitions%20and%20examples%2C%20demonstrate%20practical%20utility%20through%20a%0Alarge-scale%20measurement%20exercise%2C%20and%20provide%20accompanying%20open-source%20Python%0Acode.%20By%20operationalizing%20a%20transparent%20approach%20to%20measurement%20of%20complex%0Aconstructs%2C%20this%20work%20offers%20a%20principled%20foundation%20for%20broader%20and%20more%0Ainterpretable%20AI%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranching%2520Out%253A%2520Broadening%2520AI%2520Measurement%2520and%2520Evaluation%2520with%2520Measurement%250A%2520%2520Trees%26entry.906535625%3DCraig%2520Greenberg%2520and%2520Patrick%2520Hall%2520and%2520Theodore%2520Jensen%2520and%2520Kristen%2520Greene%2520and%2520Razvan%2520Amironesei%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520%255Ctextit%257Bmeasurement%2520trees%257D%252C%2520a%2520novel%2520class%2520of%2520metrics%250Adesigned%2520to%2520combine%2520various%2520constructs%2520into%2520an%2520interpretable%2520multi-level%250Arepresentation%2520of%2520a%2520measurand.%2520Unlike%2520conventional%2520metrics%2520that%2520yield%2520single%250Avalues%252C%2520vectors%252C%2520surfaces%252C%2520or%2520categories%252C%2520measurement%2520trees%2520produce%2520a%250Ahierarchical%2520directed%2520graph%2520in%2520which%2520each%2520node%2520summarizes%2520its%2520children%2520through%250Auser-defined%2520aggregation%2520methods.%2520In%2520response%2520to%2520recent%2520calls%2520to%2520expand%2520the%250Ascope%2520of%2520AI%2520system%2520evaluation%252C%2520measurement%2520trees%2520enhance%2520metric%2520transparency%250Aand%2520facilitate%2520the%2520integration%2520of%2520heterogeneous%2520evidence%252C%2520including%252C%2520e.g.%252C%250Aagentic%252C%2520business%252C%2520energy-efficiency%252C%2520sociotechnical%252C%2520or%2520security%2520signals.%2520We%250Apresent%2520definitions%2520and%2520examples%252C%2520demonstrate%2520practical%2520utility%2520through%2520a%250Alarge-scale%2520measurement%2520exercise%252C%2520and%2520provide%2520accompanying%2520open-source%2520Python%250Acode.%2520By%2520operationalizing%2520a%2520transparent%2520approach%2520to%2520measurement%2520of%2520complex%250Aconstructs%252C%2520this%2520work%2520offers%2520a%2520principled%2520foundation%2520for%2520broader%2520and%2520more%250Ainterpretable%2520AI%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branching%20Out%3A%20Broadening%20AI%20Measurement%20and%20Evaluation%20with%20Measurement%0A%20%20Trees&entry.906535625=Craig%20Greenberg%20and%20Patrick%20Hall%20and%20Theodore%20Jensen%20and%20Kristen%20Greene%20and%20Razvan%20Amironesei&entry.1292438233=%20%20This%20paper%20introduces%20%5Ctextit%7Bmeasurement%20trees%7D%2C%20a%20novel%20class%20of%20metrics%0Adesigned%20to%20combine%20various%20constructs%20into%20an%20interpretable%20multi-level%0Arepresentation%20of%20a%20measurand.%20Unlike%20conventional%20metrics%20that%20yield%20single%0Avalues%2C%20vectors%2C%20surfaces%2C%20or%20categories%2C%20measurement%20trees%20produce%20a%0Ahierarchical%20directed%20graph%20in%20which%20each%20node%20summarizes%20its%20children%20through%0Auser-defined%20aggregation%20methods.%20In%20response%20to%20recent%20calls%20to%20expand%20the%0Ascope%20of%20AI%20system%20evaluation%2C%20measurement%20trees%20enhance%20metric%20transparency%0Aand%20facilitate%20the%20integration%20of%20heterogeneous%20evidence%2C%20including%2C%20e.g.%2C%0Aagentic%2C%20business%2C%20energy-efficiency%2C%20sociotechnical%2C%20or%20security%20signals.%20We%0Apresent%20definitions%20and%20examples%2C%20demonstrate%20practical%20utility%20through%20a%0Alarge-scale%20measurement%20exercise%2C%20and%20provide%20accompanying%20open-source%20Python%0Acode.%20By%20operationalizing%20a%20transparent%20approach%20to%20measurement%20of%20complex%0Aconstructs%2C%20this%20work%20offers%20a%20principled%20foundation%20for%20broader%20and%20more%0Ainterpretable%20AI%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26632v1&entry.124074799=Read"},
{"title": "Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in\n  Criminal Investigations", "author": "Riccardo Pozzi and Valentina Barbera and Renzo Alva Principe and Davide Giardini and Riccardo Rubini and Matteo Palmonari", "abstract": "  Criminal investigations often involve the analysis of messages exchanged\nthrough instant messaging apps such as WhatsApp, which can be an extremely\neffort-consuming task. Our approach integrates knowledge graphs and NLP models\nto support this analysis by semantically enriching data collected from\nsuspects' mobile phones, and help prosecutors and investigators search into the\ndata and get valuable insights. Our semantic enrichment process involves\nextracting message data and modeling it using a knowledge graph, generating\ntranscriptions of voice messages, and annotating the data using an end-to-end\nentity extraction approach. We adopt two different solutions to help users get\ninsights into the data, one based on querying and visualizing the graph, and\none based on semantic search. The proposed approach ensures that users can\nverify the information by accessing the original data. While we report about\nearly results and prototypes developed in the context of an ongoing project,\nour proposal has undergone practical applications with real investigation data.\nAs a consequence, we had the chance to interact closely with prosecutors,\ncollecting positive feedback but also identifying interesting opportunities as\nwell as promising research directions to share with the research community.\n", "link": "http://arxiv.org/abs/2509.26487v1", "date": "2025-09-30", "relevancy": 1.7576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Knowledge%20Graphs%20and%20NLP%20to%20Analyze%20Instant%20Messaging%20Data%20in%0A%20%20Criminal%20Investigations&body=Title%3A%20Combining%20Knowledge%20Graphs%20and%20NLP%20to%20Analyze%20Instant%20Messaging%20Data%20in%0A%20%20Criminal%20Investigations%0AAuthor%3A%20Riccardo%20Pozzi%20and%20Valentina%20Barbera%20and%20Renzo%20Alva%20Principe%20and%20Davide%20Giardini%20and%20Riccardo%20Rubini%20and%20Matteo%20Palmonari%0AAbstract%3A%20%20%20Criminal%20investigations%20often%20involve%20the%20analysis%20of%20messages%20exchanged%0Athrough%20instant%20messaging%20apps%20such%20as%20WhatsApp%2C%20which%20can%20be%20an%20extremely%0Aeffort-consuming%20task.%20Our%20approach%20integrates%20knowledge%20graphs%20and%20NLP%20models%0Ato%20support%20this%20analysis%20by%20semantically%20enriching%20data%20collected%20from%0Asuspects%27%20mobile%20phones%2C%20and%20help%20prosecutors%20and%20investigators%20search%20into%20the%0Adata%20and%20get%20valuable%20insights.%20Our%20semantic%20enrichment%20process%20involves%0Aextracting%20message%20data%20and%20modeling%20it%20using%20a%20knowledge%20graph%2C%20generating%0Atranscriptions%20of%20voice%20messages%2C%20and%20annotating%20the%20data%20using%20an%20end-to-end%0Aentity%20extraction%20approach.%20We%20adopt%20two%20different%20solutions%20to%20help%20users%20get%0Ainsights%20into%20the%20data%2C%20one%20based%20on%20querying%20and%20visualizing%20the%20graph%2C%20and%0Aone%20based%20on%20semantic%20search.%20The%20proposed%20approach%20ensures%20that%20users%20can%0Averify%20the%20information%20by%20accessing%20the%20original%20data.%20While%20we%20report%20about%0Aearly%20results%20and%20prototypes%20developed%20in%20the%20context%20of%20an%20ongoing%20project%2C%0Aour%20proposal%20has%20undergone%20practical%20applications%20with%20real%20investigation%20data.%0AAs%20a%20consequence%2C%20we%20had%20the%20chance%20to%20interact%20closely%20with%20prosecutors%2C%0Acollecting%20positive%20feedback%20but%20also%20identifying%20interesting%20opportunities%20as%0Awell%20as%20promising%20research%20directions%20to%20share%20with%20the%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Knowledge%2520Graphs%2520and%2520NLP%2520to%2520Analyze%2520Instant%2520Messaging%2520Data%2520in%250A%2520%2520Criminal%2520Investigations%26entry.906535625%3DRiccardo%2520Pozzi%2520and%2520Valentina%2520Barbera%2520and%2520Renzo%2520Alva%2520Principe%2520and%2520Davide%2520Giardini%2520and%2520Riccardo%2520Rubini%2520and%2520Matteo%2520Palmonari%26entry.1292438233%3D%2520%2520Criminal%2520investigations%2520often%2520involve%2520the%2520analysis%2520of%2520messages%2520exchanged%250Athrough%2520instant%2520messaging%2520apps%2520such%2520as%2520WhatsApp%252C%2520which%2520can%2520be%2520an%2520extremely%250Aeffort-consuming%2520task.%2520Our%2520approach%2520integrates%2520knowledge%2520graphs%2520and%2520NLP%2520models%250Ato%2520support%2520this%2520analysis%2520by%2520semantically%2520enriching%2520data%2520collected%2520from%250Asuspects%2527%2520mobile%2520phones%252C%2520and%2520help%2520prosecutors%2520and%2520investigators%2520search%2520into%2520the%250Adata%2520and%2520get%2520valuable%2520insights.%2520Our%2520semantic%2520enrichment%2520process%2520involves%250Aextracting%2520message%2520data%2520and%2520modeling%2520it%2520using%2520a%2520knowledge%2520graph%252C%2520generating%250Atranscriptions%2520of%2520voice%2520messages%252C%2520and%2520annotating%2520the%2520data%2520using%2520an%2520end-to-end%250Aentity%2520extraction%2520approach.%2520We%2520adopt%2520two%2520different%2520solutions%2520to%2520help%2520users%2520get%250Ainsights%2520into%2520the%2520data%252C%2520one%2520based%2520on%2520querying%2520and%2520visualizing%2520the%2520graph%252C%2520and%250Aone%2520based%2520on%2520semantic%2520search.%2520The%2520proposed%2520approach%2520ensures%2520that%2520users%2520can%250Averify%2520the%2520information%2520by%2520accessing%2520the%2520original%2520data.%2520While%2520we%2520report%2520about%250Aearly%2520results%2520and%2520prototypes%2520developed%2520in%2520the%2520context%2520of%2520an%2520ongoing%2520project%252C%250Aour%2520proposal%2520has%2520undergone%2520practical%2520applications%2520with%2520real%2520investigation%2520data.%250AAs%2520a%2520consequence%252C%2520we%2520had%2520the%2520chance%2520to%2520interact%2520closely%2520with%2520prosecutors%252C%250Acollecting%2520positive%2520feedback%2520but%2520also%2520identifying%2520interesting%2520opportunities%2520as%250Awell%2520as%2520promising%2520research%2520directions%2520to%2520share%2520with%2520the%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Knowledge%20Graphs%20and%20NLP%20to%20Analyze%20Instant%20Messaging%20Data%20in%0A%20%20Criminal%20Investigations&entry.906535625=Riccardo%20Pozzi%20and%20Valentina%20Barbera%20and%20Renzo%20Alva%20Principe%20and%20Davide%20Giardini%20and%20Riccardo%20Rubini%20and%20Matteo%20Palmonari&entry.1292438233=%20%20Criminal%20investigations%20often%20involve%20the%20analysis%20of%20messages%20exchanged%0Athrough%20instant%20messaging%20apps%20such%20as%20WhatsApp%2C%20which%20can%20be%20an%20extremely%0Aeffort-consuming%20task.%20Our%20approach%20integrates%20knowledge%20graphs%20and%20NLP%20models%0Ato%20support%20this%20analysis%20by%20semantically%20enriching%20data%20collected%20from%0Asuspects%27%20mobile%20phones%2C%20and%20help%20prosecutors%20and%20investigators%20search%20into%20the%0Adata%20and%20get%20valuable%20insights.%20Our%20semantic%20enrichment%20process%20involves%0Aextracting%20message%20data%20and%20modeling%20it%20using%20a%20knowledge%20graph%2C%20generating%0Atranscriptions%20of%20voice%20messages%2C%20and%20annotating%20the%20data%20using%20an%20end-to-end%0Aentity%20extraction%20approach.%20We%20adopt%20two%20different%20solutions%20to%20help%20users%20get%0Ainsights%20into%20the%20data%2C%20one%20based%20on%20querying%20and%20visualizing%20the%20graph%2C%20and%0Aone%20based%20on%20semantic%20search.%20The%20proposed%20approach%20ensures%20that%20users%20can%0Averify%20the%20information%20by%20accessing%20the%20original%20data.%20While%20we%20report%20about%0Aearly%20results%20and%20prototypes%20developed%20in%20the%20context%20of%20an%20ongoing%20project%2C%0Aour%20proposal%20has%20undergone%20practical%20applications%20with%20real%20investigation%20data.%0AAs%20a%20consequence%2C%20we%20had%20the%20chance%20to%20interact%20closely%20with%20prosecutors%2C%0Acollecting%20positive%20feedback%20but%20also%20identifying%20interesting%20opportunities%20as%0Awell%20as%20promising%20research%20directions%20to%20share%20with%20the%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26487v1&entry.124074799=Read"},
{"title": "MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph\n  Classification Models", "author": "Baptiste Hilaire and Emmanouil Karystinaios and Gerhard Widmer", "abstract": "  Interpretability is essential for deploying deep learning models in symbolic\nmusic analysis, yet most research emphasizes model performance over\nexplanation. To address this, we introduce MUSE-Explainer, a new method that\nhelps reveal how music Graph Neural Network models make decisions by providing\nclear, human-friendly explanations. Our approach generates counterfactual\nexplanations by making small, meaningful changes to musical score graphs that\nalter a model's prediction while ensuring the results remain musically\ncoherent. Unlike existing methods, MUSE-Explainer tailors its explanations to\nthe structure of musical data and avoids unrealistic or confusing outputs. We\nevaluate our method on a music analysis task and show it offers intuitive\ninsights that can be visualized with standard music tools such as Verovio.\n", "link": "http://arxiv.org/abs/2509.26521v1", "date": "2025-09-30", "relevancy": 1.7564, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4391}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUSE-Explainer%3A%20Counterfactual%20Explanations%20for%20Symbolic%20Music%20Graph%0A%20%20Classification%20Models&body=Title%3A%20MUSE-Explainer%3A%20Counterfactual%20Explanations%20for%20Symbolic%20Music%20Graph%0A%20%20Classification%20Models%0AAuthor%3A%20Baptiste%20Hilaire%20and%20Emmanouil%20Karystinaios%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20Interpretability%20is%20essential%20for%20deploying%20deep%20learning%20models%20in%20symbolic%0Amusic%20analysis%2C%20yet%20most%20research%20emphasizes%20model%20performance%20over%0Aexplanation.%20To%20address%20this%2C%20we%20introduce%20MUSE-Explainer%2C%20a%20new%20method%20that%0Ahelps%20reveal%20how%20music%20Graph%20Neural%20Network%20models%20make%20decisions%20by%20providing%0Aclear%2C%20human-friendly%20explanations.%20Our%20approach%20generates%20counterfactual%0Aexplanations%20by%20making%20small%2C%20meaningful%20changes%20to%20musical%20score%20graphs%20that%0Aalter%20a%20model%27s%20prediction%20while%20ensuring%20the%20results%20remain%20musically%0Acoherent.%20Unlike%20existing%20methods%2C%20MUSE-Explainer%20tailors%20its%20explanations%20to%0Athe%20structure%20of%20musical%20data%20and%20avoids%20unrealistic%20or%20confusing%20outputs.%20We%0Aevaluate%20our%20method%20on%20a%20music%20analysis%20task%20and%20show%20it%20offers%20intuitive%0Ainsights%20that%20can%20be%20visualized%20with%20standard%20music%20tools%20such%20as%20Verovio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUSE-Explainer%253A%2520Counterfactual%2520Explanations%2520for%2520Symbolic%2520Music%2520Graph%250A%2520%2520Classification%2520Models%26entry.906535625%3DBaptiste%2520Hilaire%2520and%2520Emmanouil%2520Karystinaios%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520Interpretability%2520is%2520essential%2520for%2520deploying%2520deep%2520learning%2520models%2520in%2520symbolic%250Amusic%2520analysis%252C%2520yet%2520most%2520research%2520emphasizes%2520model%2520performance%2520over%250Aexplanation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520MUSE-Explainer%252C%2520a%2520new%2520method%2520that%250Ahelps%2520reveal%2520how%2520music%2520Graph%2520Neural%2520Network%2520models%2520make%2520decisions%2520by%2520providing%250Aclear%252C%2520human-friendly%2520explanations.%2520Our%2520approach%2520generates%2520counterfactual%250Aexplanations%2520by%2520making%2520small%252C%2520meaningful%2520changes%2520to%2520musical%2520score%2520graphs%2520that%250Aalter%2520a%2520model%2527s%2520prediction%2520while%2520ensuring%2520the%2520results%2520remain%2520musically%250Acoherent.%2520Unlike%2520existing%2520methods%252C%2520MUSE-Explainer%2520tailors%2520its%2520explanations%2520to%250Athe%2520structure%2520of%2520musical%2520data%2520and%2520avoids%2520unrealistic%2520or%2520confusing%2520outputs.%2520We%250Aevaluate%2520our%2520method%2520on%2520a%2520music%2520analysis%2520task%2520and%2520show%2520it%2520offers%2520intuitive%250Ainsights%2520that%2520can%2520be%2520visualized%2520with%2520standard%2520music%2520tools%2520such%2520as%2520Verovio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUSE-Explainer%3A%20Counterfactual%20Explanations%20for%20Symbolic%20Music%20Graph%0A%20%20Classification%20Models&entry.906535625=Baptiste%20Hilaire%20and%20Emmanouil%20Karystinaios%20and%20Gerhard%20Widmer&entry.1292438233=%20%20Interpretability%20is%20essential%20for%20deploying%20deep%20learning%20models%20in%20symbolic%0Amusic%20analysis%2C%20yet%20most%20research%20emphasizes%20model%20performance%20over%0Aexplanation.%20To%20address%20this%2C%20we%20introduce%20MUSE-Explainer%2C%20a%20new%20method%20that%0Ahelps%20reveal%20how%20music%20Graph%20Neural%20Network%20models%20make%20decisions%20by%20providing%0Aclear%2C%20human-friendly%20explanations.%20Our%20approach%20generates%20counterfactual%0Aexplanations%20by%20making%20small%2C%20meaningful%20changes%20to%20musical%20score%20graphs%20that%0Aalter%20a%20model%27s%20prediction%20while%20ensuring%20the%20results%20remain%20musically%0Acoherent.%20Unlike%20existing%20methods%2C%20MUSE-Explainer%20tailors%20its%20explanations%20to%0Athe%20structure%20of%20musical%20data%20and%20avoids%20unrealistic%20or%20confusing%20outputs.%20We%0Aevaluate%20our%20method%20on%20a%20music%20analysis%20task%20and%20show%20it%20offers%20intuitive%0Ainsights%20that%20can%20be%20visualized%20with%20standard%20music%20tools%20such%20as%20Verovio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26521v1&entry.124074799=Read"},
{"title": "Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers", "author": "Hossein Nasiri and Muhammad Iqbal Rochman and Monisha Ghosh", "abstract": "  The desirability of the mid-band frequency range (1 - 10 GHz) for federal and\ncommercial applications, combined with the growing applications for commercial\nindoor use-cases, such as factory automation, opens up a new approach to\nspectrum sharing: the same frequency bands used outdoors by federal incumbents\ncan be reused by commercial indoor users. A recent example of such sharing,\nbetween commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where\nunlicensed, low-power-indoor (LPI) users share the band with outdoor\nincumbents, primarily fixed microwave links. However, to date, there exist no\nreliable, automatic means of determining whether a device is indoors or\noutdoors, necessitating the use of other mechanisms such as mandating indoor\naccess points (APs) to have integrated antennas and not be battery powered, and\nreducing transmit power of client devices which may be outdoors. An accurate\nindoor/outdoor (I/O) classification addresses these challenges, enabling\nautomatic transmit power adjustments without interfering with incumbents. To\nthis end, we leverage the Global Navigation Satellite System (GNSS) signals for\nI/O classification. GNSS signals, designed inherently for outdoor reception and\nhighly susceptible to indoor attenuation and blocking, provide a robust and\ndistinguishing feature for environmental sensing. We develop various\nmethodologies, including threshold-based techniques and machine learning\napproaches and evaluate them using an expanded dataset gathered from diverse\ngeographical locations. Our results demonstrate that GNSS-based methods alone\ncan achieve greater accuracy than approaches relying solely on wireless (Wi-Fi)\ndata, particularly in unfamiliar locations. Furthermore, the integration of\nGNSS data with Wi-Fi information leads to improved classification accuracy,\nshowcasing the significant benefits of multi-modal data fusion.\n", "link": "http://arxiv.org/abs/2509.26500v1", "date": "2025-09-30", "relevancy": 1.7493, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4456}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.44}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Indoor/Outdoor%20Spectrum%20Sharing%20Enabled%20by%20GNSS-based%20Classifiers&body=Title%3A%20Indoor/Outdoor%20Spectrum%20Sharing%20Enabled%20by%20GNSS-based%20Classifiers%0AAuthor%3A%20Hossein%20Nasiri%20and%20Muhammad%20Iqbal%20Rochman%20and%20Monisha%20Ghosh%0AAbstract%3A%20%20%20The%20desirability%20of%20the%20mid-band%20frequency%20range%20%281%20-%2010%20GHz%29%20for%20federal%20and%0Acommercial%20applications%2C%20combined%20with%20the%20growing%20applications%20for%20commercial%0Aindoor%20use-cases%2C%20such%20as%20factory%20automation%2C%20opens%20up%20a%20new%20approach%20to%0Aspectrum%20sharing%3A%20the%20same%20frequency%20bands%20used%20outdoors%20by%20federal%20incumbents%0Acan%20be%20reused%20by%20commercial%20indoor%20users.%20A%20recent%20example%20of%20such%20sharing%2C%0Abetween%20commercial%20systems%2C%20is%20the%206%20GHz%20band%20%285.925%20-%207.125%20GHz%29%20where%0Aunlicensed%2C%20low-power-indoor%20%28LPI%29%20users%20share%20the%20band%20with%20outdoor%0Aincumbents%2C%20primarily%20fixed%20microwave%20links.%20However%2C%20to%20date%2C%20there%20exist%20no%0Areliable%2C%20automatic%20means%20of%20determining%20whether%20a%20device%20is%20indoors%20or%0Aoutdoors%2C%20necessitating%20the%20use%20of%20other%20mechanisms%20such%20as%20mandating%20indoor%0Aaccess%20points%20%28APs%29%20to%20have%20integrated%20antennas%20and%20not%20be%20battery%20powered%2C%20and%0Areducing%20transmit%20power%20of%20client%20devices%20which%20may%20be%20outdoors.%20An%20accurate%0Aindoor/outdoor%20%28I/O%29%20classification%20addresses%20these%20challenges%2C%20enabling%0Aautomatic%20transmit%20power%20adjustments%20without%20interfering%20with%20incumbents.%20To%0Athis%20end%2C%20we%20leverage%20the%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20signals%20for%0AI/O%20classification.%20GNSS%20signals%2C%20designed%20inherently%20for%20outdoor%20reception%20and%0Ahighly%20susceptible%20to%20indoor%20attenuation%20and%20blocking%2C%20provide%20a%20robust%20and%0Adistinguishing%20feature%20for%20environmental%20sensing.%20We%20develop%20various%0Amethodologies%2C%20including%20threshold-based%20techniques%20and%20machine%20learning%0Aapproaches%20and%20evaluate%20them%20using%20an%20expanded%20dataset%20gathered%20from%20diverse%0Ageographical%20locations.%20Our%20results%20demonstrate%20that%20GNSS-based%20methods%20alone%0Acan%20achieve%20greater%20accuracy%20than%20approaches%20relying%20solely%20on%20wireless%20%28Wi-Fi%29%0Adata%2C%20particularly%20in%20unfamiliar%20locations.%20Furthermore%2C%20the%20integration%20of%0AGNSS%20data%20with%20Wi-Fi%20information%20leads%20to%20improved%20classification%20accuracy%2C%0Ashowcasing%20the%20significant%20benefits%20of%20multi-modal%20data%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndoor/Outdoor%2520Spectrum%2520Sharing%2520Enabled%2520by%2520GNSS-based%2520Classifiers%26entry.906535625%3DHossein%2520Nasiri%2520and%2520Muhammad%2520Iqbal%2520Rochman%2520and%2520Monisha%2520Ghosh%26entry.1292438233%3D%2520%2520The%2520desirability%2520of%2520the%2520mid-band%2520frequency%2520range%2520%25281%2520-%252010%2520GHz%2529%2520for%2520federal%2520and%250Acommercial%2520applications%252C%2520combined%2520with%2520the%2520growing%2520applications%2520for%2520commercial%250Aindoor%2520use-cases%252C%2520such%2520as%2520factory%2520automation%252C%2520opens%2520up%2520a%2520new%2520approach%2520to%250Aspectrum%2520sharing%253A%2520the%2520same%2520frequency%2520bands%2520used%2520outdoors%2520by%2520federal%2520incumbents%250Acan%2520be%2520reused%2520by%2520commercial%2520indoor%2520users.%2520A%2520recent%2520example%2520of%2520such%2520sharing%252C%250Abetween%2520commercial%2520systems%252C%2520is%2520the%25206%2520GHz%2520band%2520%25285.925%2520-%25207.125%2520GHz%2529%2520where%250Aunlicensed%252C%2520low-power-indoor%2520%2528LPI%2529%2520users%2520share%2520the%2520band%2520with%2520outdoor%250Aincumbents%252C%2520primarily%2520fixed%2520microwave%2520links.%2520However%252C%2520to%2520date%252C%2520there%2520exist%2520no%250Areliable%252C%2520automatic%2520means%2520of%2520determining%2520whether%2520a%2520device%2520is%2520indoors%2520or%250Aoutdoors%252C%2520necessitating%2520the%2520use%2520of%2520other%2520mechanisms%2520such%2520as%2520mandating%2520indoor%250Aaccess%2520points%2520%2528APs%2529%2520to%2520have%2520integrated%2520antennas%2520and%2520not%2520be%2520battery%2520powered%252C%2520and%250Areducing%2520transmit%2520power%2520of%2520client%2520devices%2520which%2520may%2520be%2520outdoors.%2520An%2520accurate%250Aindoor/outdoor%2520%2528I/O%2529%2520classification%2520addresses%2520these%2520challenges%252C%2520enabling%250Aautomatic%2520transmit%2520power%2520adjustments%2520without%2520interfering%2520with%2520incumbents.%2520To%250Athis%2520end%252C%2520we%2520leverage%2520the%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%2520signals%2520for%250AI/O%2520classification.%2520GNSS%2520signals%252C%2520designed%2520inherently%2520for%2520outdoor%2520reception%2520and%250Ahighly%2520susceptible%2520to%2520indoor%2520attenuation%2520and%2520blocking%252C%2520provide%2520a%2520robust%2520and%250Adistinguishing%2520feature%2520for%2520environmental%2520sensing.%2520We%2520develop%2520various%250Amethodologies%252C%2520including%2520threshold-based%2520techniques%2520and%2520machine%2520learning%250Aapproaches%2520and%2520evaluate%2520them%2520using%2520an%2520expanded%2520dataset%2520gathered%2520from%2520diverse%250Ageographical%2520locations.%2520Our%2520results%2520demonstrate%2520that%2520GNSS-based%2520methods%2520alone%250Acan%2520achieve%2520greater%2520accuracy%2520than%2520approaches%2520relying%2520solely%2520on%2520wireless%2520%2528Wi-Fi%2529%250Adata%252C%2520particularly%2520in%2520unfamiliar%2520locations.%2520Furthermore%252C%2520the%2520integration%2520of%250AGNSS%2520data%2520with%2520Wi-Fi%2520information%2520leads%2520to%2520improved%2520classification%2520accuracy%252C%250Ashowcasing%2520the%2520significant%2520benefits%2520of%2520multi-modal%2520data%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Indoor/Outdoor%20Spectrum%20Sharing%20Enabled%20by%20GNSS-based%20Classifiers&entry.906535625=Hossein%20Nasiri%20and%20Muhammad%20Iqbal%20Rochman%20and%20Monisha%20Ghosh&entry.1292438233=%20%20The%20desirability%20of%20the%20mid-band%20frequency%20range%20%281%20-%2010%20GHz%29%20for%20federal%20and%0Acommercial%20applications%2C%20combined%20with%20the%20growing%20applications%20for%20commercial%0Aindoor%20use-cases%2C%20such%20as%20factory%20automation%2C%20opens%20up%20a%20new%20approach%20to%0Aspectrum%20sharing%3A%20the%20same%20frequency%20bands%20used%20outdoors%20by%20federal%20incumbents%0Acan%20be%20reused%20by%20commercial%20indoor%20users.%20A%20recent%20example%20of%20such%20sharing%2C%0Abetween%20commercial%20systems%2C%20is%20the%206%20GHz%20band%20%285.925%20-%207.125%20GHz%29%20where%0Aunlicensed%2C%20low-power-indoor%20%28LPI%29%20users%20share%20the%20band%20with%20outdoor%0Aincumbents%2C%20primarily%20fixed%20microwave%20links.%20However%2C%20to%20date%2C%20there%20exist%20no%0Areliable%2C%20automatic%20means%20of%20determining%20whether%20a%20device%20is%20indoors%20or%0Aoutdoors%2C%20necessitating%20the%20use%20of%20other%20mechanisms%20such%20as%20mandating%20indoor%0Aaccess%20points%20%28APs%29%20to%20have%20integrated%20antennas%20and%20not%20be%20battery%20powered%2C%20and%0Areducing%20transmit%20power%20of%20client%20devices%20which%20may%20be%20outdoors.%20An%20accurate%0Aindoor/outdoor%20%28I/O%29%20classification%20addresses%20these%20challenges%2C%20enabling%0Aautomatic%20transmit%20power%20adjustments%20without%20interfering%20with%20incumbents.%20To%0Athis%20end%2C%20we%20leverage%20the%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20signals%20for%0AI/O%20classification.%20GNSS%20signals%2C%20designed%20inherently%20for%20outdoor%20reception%20and%0Ahighly%20susceptible%20to%20indoor%20attenuation%20and%20blocking%2C%20provide%20a%20robust%20and%0Adistinguishing%20feature%20for%20environmental%20sensing.%20We%20develop%20various%0Amethodologies%2C%20including%20threshold-based%20techniques%20and%20machine%20learning%0Aapproaches%20and%20evaluate%20them%20using%20an%20expanded%20dataset%20gathered%20from%20diverse%0Ageographical%20locations.%20Our%20results%20demonstrate%20that%20GNSS-based%20methods%20alone%0Acan%20achieve%20greater%20accuracy%20than%20approaches%20relying%20solely%20on%20wireless%20%28Wi-Fi%29%0Adata%2C%20particularly%20in%20unfamiliar%20locations.%20Furthermore%2C%20the%20integration%20of%0AGNSS%20data%20with%20Wi-Fi%20information%20leads%20to%20improved%20classification%20accuracy%2C%0Ashowcasing%20the%20significant%20benefits%20of%20multi-modal%20data%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26500v1&entry.124074799=Read"},
{"title": "Visual-auditory Extrinsic Contact Estimation", "author": "Xili Yi and Jayjun Lee and Nima Fazeli", "abstract": "  Robust manipulation often hinges on a robot's ability to perceive extrinsic\ncontacts-contacts between a grasped object and its surrounding environment.\nHowever, these contacts are difficult to observe through vision alone due to\nocclusions, limited resolution, and ambiguous near-contact states. In this\npaper, we propose a visual-auditory method for extrinsic contact estimation\nthat integrates global scene information from vision with local contact cues\nobtained through active audio sensing. Our approach equips a robotic gripper\nwith contact microphones and conduction speakers, enabling the system to emit\nand receive acoustic signals through the grasped object to detect external\ncontacts. We train our perception pipeline entirely in simulation and zero-shot\ntransfer to the real world. To bridge the sim-to-real gap, we introduce a\nreal-to-sim audio hallucination technique, injecting real-world audio samples\ninto simulated scenes with ground-truth contact labels. The resulting\nmultimodal model accurately estimates both the location and size of extrinsic\ncontacts across a range of cluttered and occluded scenarios. We further\ndemonstrate that explicit contact prediction significantly improves policy\nlearning for downstream contact-rich manipulation tasks.\n", "link": "http://arxiv.org/abs/2409.14608v3", "date": "2025-09-30", "relevancy": 1.7024, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6053}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5948}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-auditory%20Extrinsic%20Contact%20Estimation&body=Title%3A%20Visual-auditory%20Extrinsic%20Contact%20Estimation%0AAuthor%3A%20Xili%20Yi%20and%20Jayjun%20Lee%20and%20Nima%20Fazeli%0AAbstract%3A%20%20%20Robust%20manipulation%20often%20hinges%20on%20a%20robot%27s%20ability%20to%20perceive%20extrinsic%0Acontacts-contacts%20between%20a%20grasped%20object%20and%20its%20surrounding%20environment.%0AHowever%2C%20these%20contacts%20are%20difficult%20to%20observe%20through%20vision%20alone%20due%20to%0Aocclusions%2C%20limited%20resolution%2C%20and%20ambiguous%20near-contact%20states.%20In%20this%0Apaper%2C%20we%20propose%20a%20visual-auditory%20method%20for%20extrinsic%20contact%20estimation%0Athat%20integrates%20global%20scene%20information%20from%20vision%20with%20local%20contact%20cues%0Aobtained%20through%20active%20audio%20sensing.%20Our%20approach%20equips%20a%20robotic%20gripper%0Awith%20contact%20microphones%20and%20conduction%20speakers%2C%20enabling%20the%20system%20to%20emit%0Aand%20receive%20acoustic%20signals%20through%20the%20grasped%20object%20to%20detect%20external%0Acontacts.%20We%20train%20our%20perception%20pipeline%20entirely%20in%20simulation%20and%20zero-shot%0Atransfer%20to%20the%20real%20world.%20To%20bridge%20the%20sim-to-real%20gap%2C%20we%20introduce%20a%0Areal-to-sim%20audio%20hallucination%20technique%2C%20injecting%20real-world%20audio%20samples%0Ainto%20simulated%20scenes%20with%20ground-truth%20contact%20labels.%20The%20resulting%0Amultimodal%20model%20accurately%20estimates%20both%20the%20location%20and%20size%20of%20extrinsic%0Acontacts%20across%20a%20range%20of%20cluttered%20and%20occluded%20scenarios.%20We%20further%0Ademonstrate%20that%20explicit%20contact%20prediction%20significantly%20improves%20policy%0Alearning%20for%20downstream%20contact-rich%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14608v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-auditory%2520Extrinsic%2520Contact%2520Estimation%26entry.906535625%3DXili%2520Yi%2520and%2520Jayjun%2520Lee%2520and%2520Nima%2520Fazeli%26entry.1292438233%3D%2520%2520Robust%2520manipulation%2520often%2520hinges%2520on%2520a%2520robot%2527s%2520ability%2520to%2520perceive%2520extrinsic%250Acontacts-contacts%2520between%2520a%2520grasped%2520object%2520and%2520its%2520surrounding%2520environment.%250AHowever%252C%2520these%2520contacts%2520are%2520difficult%2520to%2520observe%2520through%2520vision%2520alone%2520due%2520to%250Aocclusions%252C%2520limited%2520resolution%252C%2520and%2520ambiguous%2520near-contact%2520states.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520visual-auditory%2520method%2520for%2520extrinsic%2520contact%2520estimation%250Athat%2520integrates%2520global%2520scene%2520information%2520from%2520vision%2520with%2520local%2520contact%2520cues%250Aobtained%2520through%2520active%2520audio%2520sensing.%2520Our%2520approach%2520equips%2520a%2520robotic%2520gripper%250Awith%2520contact%2520microphones%2520and%2520conduction%2520speakers%252C%2520enabling%2520the%2520system%2520to%2520emit%250Aand%2520receive%2520acoustic%2520signals%2520through%2520the%2520grasped%2520object%2520to%2520detect%2520external%250Acontacts.%2520We%2520train%2520our%2520perception%2520pipeline%2520entirely%2520in%2520simulation%2520and%2520zero-shot%250Atransfer%2520to%2520the%2520real%2520world.%2520To%2520bridge%2520the%2520sim-to-real%2520gap%252C%2520we%2520introduce%2520a%250Areal-to-sim%2520audio%2520hallucination%2520technique%252C%2520injecting%2520real-world%2520audio%2520samples%250Ainto%2520simulated%2520scenes%2520with%2520ground-truth%2520contact%2520labels.%2520The%2520resulting%250Amultimodal%2520model%2520accurately%2520estimates%2520both%2520the%2520location%2520and%2520size%2520of%2520extrinsic%250Acontacts%2520across%2520a%2520range%2520of%2520cluttered%2520and%2520occluded%2520scenarios.%2520We%2520further%250Ademonstrate%2520that%2520explicit%2520contact%2520prediction%2520significantly%2520improves%2520policy%250Alearning%2520for%2520downstream%2520contact-rich%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14608v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-auditory%20Extrinsic%20Contact%20Estimation&entry.906535625=Xili%20Yi%20and%20Jayjun%20Lee%20and%20Nima%20Fazeli&entry.1292438233=%20%20Robust%20manipulation%20often%20hinges%20on%20a%20robot%27s%20ability%20to%20perceive%20extrinsic%0Acontacts-contacts%20between%20a%20grasped%20object%20and%20its%20surrounding%20environment.%0AHowever%2C%20these%20contacts%20are%20difficult%20to%20observe%20through%20vision%20alone%20due%20to%0Aocclusions%2C%20limited%20resolution%2C%20and%20ambiguous%20near-contact%20states.%20In%20this%0Apaper%2C%20we%20propose%20a%20visual-auditory%20method%20for%20extrinsic%20contact%20estimation%0Athat%20integrates%20global%20scene%20information%20from%20vision%20with%20local%20contact%20cues%0Aobtained%20through%20active%20audio%20sensing.%20Our%20approach%20equips%20a%20robotic%20gripper%0Awith%20contact%20microphones%20and%20conduction%20speakers%2C%20enabling%20the%20system%20to%20emit%0Aand%20receive%20acoustic%20signals%20through%20the%20grasped%20object%20to%20detect%20external%0Acontacts.%20We%20train%20our%20perception%20pipeline%20entirely%20in%20simulation%20and%20zero-shot%0Atransfer%20to%20the%20real%20world.%20To%20bridge%20the%20sim-to-real%20gap%2C%20we%20introduce%20a%0Areal-to-sim%20audio%20hallucination%20technique%2C%20injecting%20real-world%20audio%20samples%0Ainto%20simulated%20scenes%20with%20ground-truth%20contact%20labels.%20The%20resulting%0Amultimodal%20model%20accurately%20estimates%20both%20the%20location%20and%20size%20of%20extrinsic%0Acontacts%20across%20a%20range%20of%20cluttered%20and%20occluded%20scenarios.%20We%20further%0Ademonstrate%20that%20explicit%20contact%20prediction%20significantly%20improves%20policy%0Alearning%20for%20downstream%20contact-rich%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14608v3&entry.124074799=Read"},
{"title": "Are Robust LLM Fingerprints Adversarially Robust?", "author": "Anshul Nasery and Edoardo Contente and Alkin Kaz and Pramod Viswanath and Sewoong Oh", "abstract": "  Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods.\n", "link": "http://arxiv.org/abs/2509.26598v1", "date": "2025-09-30", "relevancy": 1.7013, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4289}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4248}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Robust%20LLM%20Fingerprints%20Adversarially%20Robust%3F&body=Title%3A%20Are%20Robust%20LLM%20Fingerprints%20Adversarially%20Robust%3F%0AAuthor%3A%20Anshul%20Nasery%20and%20Edoardo%20Contente%20and%20Alkin%20Kaz%20and%20Pramod%20Viswanath%20and%20Sewoong%20Oh%0AAbstract%3A%20%20%20Model%20fingerprinting%20has%20emerged%20as%20a%20promising%20paradigm%20for%20claiming%20model%0Aownership.%20However%2C%20robustness%20evaluations%20of%20these%20schemes%20have%20mostly%20focused%0Aon%20benign%20perturbations%20such%20as%20incremental%20fine-tuning%2C%20model%20merging%2C%20and%0Aprompting.%20Lack%20of%20systematic%20investigations%20into%20%7B%5Cem%20adversarial%20robustness%7D%0Aagainst%20a%20malicious%20model%20host%20leaves%20current%20systems%20vulnerable.%20To%20bridge%0Athis%20gap%2C%20we%20first%20define%20a%20concrete%2C%20practical%20threat%20model%20against%20model%0Afingerprinting.%20We%20then%20take%20a%20critical%20look%20at%20existing%20model%20fingerprinting%0Aschemes%20to%20identify%20their%20fundamental%20vulnerabilities.%20Based%20on%20these%2C%20we%0Adevelop%20adaptive%20adversarial%20attacks%20tailored%20for%20each%20vulnerability%2C%20and%0Ademonstrate%20that%20these%20can%20bypass%20model%20authentication%20completely%20for%20ten%0Arecently%20proposed%20fingerprinting%20schemes%20while%20maintaining%20high%20utility%20of%20the%0Amodel%20for%20the%20end%20users.%20Our%20work%20encourages%20fingerprint%20designers%20to%20adopt%0Aadversarial%20robustness%20by%20design.%20We%20end%20with%20recommendations%20for%20future%0Afingerprinting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Robust%2520LLM%2520Fingerprints%2520Adversarially%2520Robust%253F%26entry.906535625%3DAnshul%2520Nasery%2520and%2520Edoardo%2520Contente%2520and%2520Alkin%2520Kaz%2520and%2520Pramod%2520Viswanath%2520and%2520Sewoong%2520Oh%26entry.1292438233%3D%2520%2520Model%2520fingerprinting%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520claiming%2520model%250Aownership.%2520However%252C%2520robustness%2520evaluations%2520of%2520these%2520schemes%2520have%2520mostly%2520focused%250Aon%2520benign%2520perturbations%2520such%2520as%2520incremental%2520fine-tuning%252C%2520model%2520merging%252C%2520and%250Aprompting.%2520Lack%2520of%2520systematic%2520investigations%2520into%2520%257B%255Cem%2520adversarial%2520robustness%257D%250Aagainst%2520a%2520malicious%2520model%2520host%2520leaves%2520current%2520systems%2520vulnerable.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520first%2520define%2520a%2520concrete%252C%2520practical%2520threat%2520model%2520against%2520model%250Afingerprinting.%2520We%2520then%2520take%2520a%2520critical%2520look%2520at%2520existing%2520model%2520fingerprinting%250Aschemes%2520to%2520identify%2520their%2520fundamental%2520vulnerabilities.%2520Based%2520on%2520these%252C%2520we%250Adevelop%2520adaptive%2520adversarial%2520attacks%2520tailored%2520for%2520each%2520vulnerability%252C%2520and%250Ademonstrate%2520that%2520these%2520can%2520bypass%2520model%2520authentication%2520completely%2520for%2520ten%250Arecently%2520proposed%2520fingerprinting%2520schemes%2520while%2520maintaining%2520high%2520utility%2520of%2520the%250Amodel%2520for%2520the%2520end%2520users.%2520Our%2520work%2520encourages%2520fingerprint%2520designers%2520to%2520adopt%250Aadversarial%2520robustness%2520by%2520design.%2520We%2520end%2520with%2520recommendations%2520for%2520future%250Afingerprinting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Robust%20LLM%20Fingerprints%20Adversarially%20Robust%3F&entry.906535625=Anshul%20Nasery%20and%20Edoardo%20Contente%20and%20Alkin%20Kaz%20and%20Pramod%20Viswanath%20and%20Sewoong%20Oh&entry.1292438233=%20%20Model%20fingerprinting%20has%20emerged%20as%20a%20promising%20paradigm%20for%20claiming%20model%0Aownership.%20However%2C%20robustness%20evaluations%20of%20these%20schemes%20have%20mostly%20focused%0Aon%20benign%20perturbations%20such%20as%20incremental%20fine-tuning%2C%20model%20merging%2C%20and%0Aprompting.%20Lack%20of%20systematic%20investigations%20into%20%7B%5Cem%20adversarial%20robustness%7D%0Aagainst%20a%20malicious%20model%20host%20leaves%20current%20systems%20vulnerable.%20To%20bridge%0Athis%20gap%2C%20we%20first%20define%20a%20concrete%2C%20practical%20threat%20model%20against%20model%0Afingerprinting.%20We%20then%20take%20a%20critical%20look%20at%20existing%20model%20fingerprinting%0Aschemes%20to%20identify%20their%20fundamental%20vulnerabilities.%20Based%20on%20these%2C%20we%0Adevelop%20adaptive%20adversarial%20attacks%20tailored%20for%20each%20vulnerability%2C%20and%0Ademonstrate%20that%20these%20can%20bypass%20model%20authentication%20completely%20for%20ten%0Arecently%20proposed%20fingerprinting%20schemes%20while%20maintaining%20high%20utility%20of%20the%0Amodel%20for%20the%20end%20users.%20Our%20work%20encourages%20fingerprint%20designers%20to%20adopt%0Aadversarial%20robustness%20by%20design.%20We%20end%20with%20recommendations%20for%20future%0Afingerprinting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26598v1&entry.124074799=Read"},
{"title": "SCUBA: Salesforce Computer Use Benchmark", "author": "Yutong Dai and Krithika Ramakrishnan and Jing Gu and Matthew Fernandez and Yanqi Luo and Viraj Prabhu and Zhenyu Hu and Silvio Savarese and Caiming Xiong and Zeyuan Chen and Ran Xu", "abstract": "  We introduce SCUBA, a benchmark designed to evaluate computer-use agents on\ncustomer relationship management (CRM) workflows within the Salesforce\nplatform. SCUBA contains 300 task instances derived from real user interviews,\nspanning three primary personas, platform administrators, sales\nrepresentatives, and service agents. The tasks test a range of\nenterprise-critical abilities, including Enterprise Software UI navigation,\ndata manipulation, workflow automation, information retrieval, and\ntroubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox\nenvironments with support for parallel execution and fine-grained evaluation\nmetrics to capture milestone progress. We benchmark a diverse set of agents\nunder both zero-shot and demonstration-augmented settings. We observed huge\nperformance gaps in different agent design paradigms and gaps between the\nopen-source model and the closed-source model. In the zero-shot setting,\nopen-source model powered computer-use agents that have strong performance on\nrelated benchmarks like OSWorld only have less than 5\\% success rate on SCUBA,\nwhile methods built on closed-source models can still have up to 39% task\nsuccess rate. In the demonstration-augmented settings, task success rates can\nbe improved to 50\\% while simultaneously reducing time and costs by 13% and\n16%, respectively. These findings highlight both the challenges of enterprise\ntasks automation and the promise of agentic solutions. By offering a realistic\nbenchmark with interpretable evaluation, SCUBA aims to accelerate progress in\nbuilding reliable computer-use agents for complex business software ecosystems.\n", "link": "http://arxiv.org/abs/2509.26506v1", "date": "2025-09-30", "relevancy": 1.6883, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCUBA%3A%20Salesforce%20Computer%20Use%20Benchmark&body=Title%3A%20SCUBA%3A%20Salesforce%20Computer%20Use%20Benchmark%0AAuthor%3A%20Yutong%20Dai%20and%20Krithika%20Ramakrishnan%20and%20Jing%20Gu%20and%20Matthew%20Fernandez%20and%20Yanqi%20Luo%20and%20Viraj%20Prabhu%20and%20Zhenyu%20Hu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Zeyuan%20Chen%20and%20Ran%20Xu%0AAbstract%3A%20%20%20We%20introduce%20SCUBA%2C%20a%20benchmark%20designed%20to%20evaluate%20computer-use%20agents%20on%0Acustomer%20relationship%20management%20%28CRM%29%20workflows%20within%20the%20Salesforce%0Aplatform.%20SCUBA%20contains%20300%20task%20instances%20derived%20from%20real%20user%20interviews%2C%0Aspanning%20three%20primary%20personas%2C%20platform%20administrators%2C%20sales%0Arepresentatives%2C%20and%20service%20agents.%20The%20tasks%20test%20a%20range%20of%0Aenterprise-critical%20abilities%2C%20including%20Enterprise%20Software%20UI%20navigation%2C%0Adata%20manipulation%2C%20workflow%20automation%2C%20information%20retrieval%2C%20and%0Atroubleshooting.%20To%20ensure%20realism%2C%20SCUBA%20operates%20in%20Salesforce%20sandbox%0Aenvironments%20with%20support%20for%20parallel%20execution%20and%20fine-grained%20evaluation%0Ametrics%20to%20capture%20milestone%20progress.%20We%20benchmark%20a%20diverse%20set%20of%20agents%0Aunder%20both%20zero-shot%20and%20demonstration-augmented%20settings.%20We%20observed%20huge%0Aperformance%20gaps%20in%20different%20agent%20design%20paradigms%20and%20gaps%20between%20the%0Aopen-source%20model%20and%20the%20closed-source%20model.%20In%20the%20zero-shot%20setting%2C%0Aopen-source%20model%20powered%20computer-use%20agents%20that%20have%20strong%20performance%20on%0Arelated%20benchmarks%20like%20OSWorld%20only%20have%20less%20than%205%5C%25%20success%20rate%20on%20SCUBA%2C%0Awhile%20methods%20built%20on%20closed-source%20models%20can%20still%20have%20up%20to%2039%25%20task%0Asuccess%20rate.%20In%20the%20demonstration-augmented%20settings%2C%20task%20success%20rates%20can%0Abe%20improved%20to%2050%5C%25%20while%20simultaneously%20reducing%20time%20and%20costs%20by%2013%25%20and%0A16%25%2C%20respectively.%20These%20findings%20highlight%20both%20the%20challenges%20of%20enterprise%0Atasks%20automation%20and%20the%20promise%20of%20agentic%20solutions.%20By%20offering%20a%20realistic%0Abenchmark%20with%20interpretable%20evaluation%2C%20SCUBA%20aims%20to%20accelerate%20progress%20in%0Abuilding%20reliable%20computer-use%20agents%20for%20complex%20business%20software%20ecosystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCUBA%253A%2520Salesforce%2520Computer%2520Use%2520Benchmark%26entry.906535625%3DYutong%2520Dai%2520and%2520Krithika%2520Ramakrishnan%2520and%2520Jing%2520Gu%2520and%2520Matthew%2520Fernandez%2520and%2520Yanqi%2520Luo%2520and%2520Viraj%2520Prabhu%2520and%2520Zhenyu%2520Hu%2520and%2520Silvio%2520Savarese%2520and%2520Caiming%2520Xiong%2520and%2520Zeyuan%2520Chen%2520and%2520Ran%2520Xu%26entry.1292438233%3D%2520%2520We%2520introduce%2520SCUBA%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520computer-use%2520agents%2520on%250Acustomer%2520relationship%2520management%2520%2528CRM%2529%2520workflows%2520within%2520the%2520Salesforce%250Aplatform.%2520SCUBA%2520contains%2520300%2520task%2520instances%2520derived%2520from%2520real%2520user%2520interviews%252C%250Aspanning%2520three%2520primary%2520personas%252C%2520platform%2520administrators%252C%2520sales%250Arepresentatives%252C%2520and%2520service%2520agents.%2520The%2520tasks%2520test%2520a%2520range%2520of%250Aenterprise-critical%2520abilities%252C%2520including%2520Enterprise%2520Software%2520UI%2520navigation%252C%250Adata%2520manipulation%252C%2520workflow%2520automation%252C%2520information%2520retrieval%252C%2520and%250Atroubleshooting.%2520To%2520ensure%2520realism%252C%2520SCUBA%2520operates%2520in%2520Salesforce%2520sandbox%250Aenvironments%2520with%2520support%2520for%2520parallel%2520execution%2520and%2520fine-grained%2520evaluation%250Ametrics%2520to%2520capture%2520milestone%2520progress.%2520We%2520benchmark%2520a%2520diverse%2520set%2520of%2520agents%250Aunder%2520both%2520zero-shot%2520and%2520demonstration-augmented%2520settings.%2520We%2520observed%2520huge%250Aperformance%2520gaps%2520in%2520different%2520agent%2520design%2520paradigms%2520and%2520gaps%2520between%2520the%250Aopen-source%2520model%2520and%2520the%2520closed-source%2520model.%2520In%2520the%2520zero-shot%2520setting%252C%250Aopen-source%2520model%2520powered%2520computer-use%2520agents%2520that%2520have%2520strong%2520performance%2520on%250Arelated%2520benchmarks%2520like%2520OSWorld%2520only%2520have%2520less%2520than%25205%255C%2525%2520success%2520rate%2520on%2520SCUBA%252C%250Awhile%2520methods%2520built%2520on%2520closed-source%2520models%2520can%2520still%2520have%2520up%2520to%252039%2525%2520task%250Asuccess%2520rate.%2520In%2520the%2520demonstration-augmented%2520settings%252C%2520task%2520success%2520rates%2520can%250Abe%2520improved%2520to%252050%255C%2525%2520while%2520simultaneously%2520reducing%2520time%2520and%2520costs%2520by%252013%2525%2520and%250A16%2525%252C%2520respectively.%2520These%2520findings%2520highlight%2520both%2520the%2520challenges%2520of%2520enterprise%250Atasks%2520automation%2520and%2520the%2520promise%2520of%2520agentic%2520solutions.%2520By%2520offering%2520a%2520realistic%250Abenchmark%2520with%2520interpretable%2520evaluation%252C%2520SCUBA%2520aims%2520to%2520accelerate%2520progress%2520in%250Abuilding%2520reliable%2520computer-use%2520agents%2520for%2520complex%2520business%2520software%2520ecosystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCUBA%3A%20Salesforce%20Computer%20Use%20Benchmark&entry.906535625=Yutong%20Dai%20and%20Krithika%20Ramakrishnan%20and%20Jing%20Gu%20and%20Matthew%20Fernandez%20and%20Yanqi%20Luo%20and%20Viraj%20Prabhu%20and%20Zhenyu%20Hu%20and%20Silvio%20Savarese%20and%20Caiming%20Xiong%20and%20Zeyuan%20Chen%20and%20Ran%20Xu&entry.1292438233=%20%20We%20introduce%20SCUBA%2C%20a%20benchmark%20designed%20to%20evaluate%20computer-use%20agents%20on%0Acustomer%20relationship%20management%20%28CRM%29%20workflows%20within%20the%20Salesforce%0Aplatform.%20SCUBA%20contains%20300%20task%20instances%20derived%20from%20real%20user%20interviews%2C%0Aspanning%20three%20primary%20personas%2C%20platform%20administrators%2C%20sales%0Arepresentatives%2C%20and%20service%20agents.%20The%20tasks%20test%20a%20range%20of%0Aenterprise-critical%20abilities%2C%20including%20Enterprise%20Software%20UI%20navigation%2C%0Adata%20manipulation%2C%20workflow%20automation%2C%20information%20retrieval%2C%20and%0Atroubleshooting.%20To%20ensure%20realism%2C%20SCUBA%20operates%20in%20Salesforce%20sandbox%0Aenvironments%20with%20support%20for%20parallel%20execution%20and%20fine-grained%20evaluation%0Ametrics%20to%20capture%20milestone%20progress.%20We%20benchmark%20a%20diverse%20set%20of%20agents%0Aunder%20both%20zero-shot%20and%20demonstration-augmented%20settings.%20We%20observed%20huge%0Aperformance%20gaps%20in%20different%20agent%20design%20paradigms%20and%20gaps%20between%20the%0Aopen-source%20model%20and%20the%20closed-source%20model.%20In%20the%20zero-shot%20setting%2C%0Aopen-source%20model%20powered%20computer-use%20agents%20that%20have%20strong%20performance%20on%0Arelated%20benchmarks%20like%20OSWorld%20only%20have%20less%20than%205%5C%25%20success%20rate%20on%20SCUBA%2C%0Awhile%20methods%20built%20on%20closed-source%20models%20can%20still%20have%20up%20to%2039%25%20task%0Asuccess%20rate.%20In%20the%20demonstration-augmented%20settings%2C%20task%20success%20rates%20can%0Abe%20improved%20to%2050%5C%25%20while%20simultaneously%20reducing%20time%20and%20costs%20by%2013%25%20and%0A16%25%2C%20respectively.%20These%20findings%20highlight%20both%20the%20challenges%20of%20enterprise%0Atasks%20automation%20and%20the%20promise%20of%20agentic%20solutions.%20By%20offering%20a%20realistic%0Abenchmark%20with%20interpretable%20evaluation%2C%20SCUBA%20aims%20to%20accelerate%20progress%20in%0Abuilding%20reliable%20computer-use%20agents%20for%20complex%20business%20software%20ecosystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26506v1&entry.124074799=Read"},
{"title": "Photography Perspective Composition: Towards Aesthetic Perspective\n  Recommendation", "author": "Lujian Yao and Siming Zheng and Xinbin Yuan and Zhuoxuan Cai and Pu Wu and Jinwei Chen and Bo Li and Peng-Tao Jiang", "abstract": "  Traditional photography composition approaches are dominated by 2D\ncropping-based methods. However, these methods fall short when scenes contain\npoorly arranged subjects. Professional photographers often employ perspective\nadjustment as a form of 3D recomposition, modifying the projected 2D\nrelationships between subjects while maintaining their actual spatial positions\nto achieve better compositional balance. Inspired by this artistic practice, we\npropose photography perspective composition (PPC), extending beyond traditional\ncropping-based methods. However, implementing the PPC faces significant\nchallenges: the scarcity of perspective transformation datasets and undefined\nassessment criteria for perspective quality. To address these challenges, we\npresent three key contributions: (1) An automated framework for building PPC\ndatasets through expert photographs. (2) A video generation approach that\ndemonstrates the transformation process from less favorable to aesthetically\nenhanced perspectives. (3) A perspective quality assessment (PQA) model\nconstructed based on human performance. Our approach is concise and requires no\nadditional prompt instructions or camera trajectories, helping and guiding\nordinary users to enhance their composition skills.\n", "link": "http://arxiv.org/abs/2505.20655v3", "date": "2025-09-30", "relevancy": 1.6762, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5712}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5576}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photography%20Perspective%20Composition%3A%20Towards%20Aesthetic%20Perspective%0A%20%20Recommendation&body=Title%3A%20Photography%20Perspective%20Composition%3A%20Towards%20Aesthetic%20Perspective%0A%20%20Recommendation%0AAuthor%3A%20Lujian%20Yao%20and%20Siming%20Zheng%20and%20Xinbin%20Yuan%20and%20Zhuoxuan%20Cai%20and%20Pu%20Wu%20and%20Jinwei%20Chen%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang%0AAbstract%3A%20%20%20Traditional%20photography%20composition%20approaches%20are%20dominated%20by%202D%0Acropping-based%20methods.%20However%2C%20these%20methods%20fall%20short%20when%20scenes%20contain%0Apoorly%20arranged%20subjects.%20Professional%20photographers%20often%20employ%20perspective%0Aadjustment%20as%20a%20form%20of%203D%20recomposition%2C%20modifying%20the%20projected%202D%0Arelationships%20between%20subjects%20while%20maintaining%20their%20actual%20spatial%20positions%0Ato%20achieve%20better%20compositional%20balance.%20Inspired%20by%20this%20artistic%20practice%2C%20we%0Apropose%20photography%20perspective%20composition%20%28PPC%29%2C%20extending%20beyond%20traditional%0Acropping-based%20methods.%20However%2C%20implementing%20the%20PPC%20faces%20significant%0Achallenges%3A%20the%20scarcity%20of%20perspective%20transformation%20datasets%20and%20undefined%0Aassessment%20criteria%20for%20perspective%20quality.%20To%20address%20these%20challenges%2C%20we%0Apresent%20three%20key%20contributions%3A%20%281%29%20An%20automated%20framework%20for%20building%20PPC%0Adatasets%20through%20expert%20photographs.%20%282%29%20A%20video%20generation%20approach%20that%0Ademonstrates%20the%20transformation%20process%20from%20less%20favorable%20to%20aesthetically%0Aenhanced%20perspectives.%20%283%29%20A%20perspective%20quality%20assessment%20%28PQA%29%20model%0Aconstructed%20based%20on%20human%20performance.%20Our%20approach%20is%20concise%20and%20requires%20no%0Aadditional%20prompt%20instructions%20or%20camera%20trajectories%2C%20helping%20and%20guiding%0Aordinary%20users%20to%20enhance%20their%20composition%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20655v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhotography%2520Perspective%2520Composition%253A%2520Towards%2520Aesthetic%2520Perspective%250A%2520%2520Recommendation%26entry.906535625%3DLujian%2520Yao%2520and%2520Siming%2520Zheng%2520and%2520Xinbin%2520Yuan%2520and%2520Zhuoxuan%2520Cai%2520and%2520Pu%2520Wu%2520and%2520Jinwei%2520Chen%2520and%2520Bo%2520Li%2520and%2520Peng-Tao%2520Jiang%26entry.1292438233%3D%2520%2520Traditional%2520photography%2520composition%2520approaches%2520are%2520dominated%2520by%25202D%250Acropping-based%2520methods.%2520However%252C%2520these%2520methods%2520fall%2520short%2520when%2520scenes%2520contain%250Apoorly%2520arranged%2520subjects.%2520Professional%2520photographers%2520often%2520employ%2520perspective%250Aadjustment%2520as%2520a%2520form%2520of%25203D%2520recomposition%252C%2520modifying%2520the%2520projected%25202D%250Arelationships%2520between%2520subjects%2520while%2520maintaining%2520their%2520actual%2520spatial%2520positions%250Ato%2520achieve%2520better%2520compositional%2520balance.%2520Inspired%2520by%2520this%2520artistic%2520practice%252C%2520we%250Apropose%2520photography%2520perspective%2520composition%2520%2528PPC%2529%252C%2520extending%2520beyond%2520traditional%250Acropping-based%2520methods.%2520However%252C%2520implementing%2520the%2520PPC%2520faces%2520significant%250Achallenges%253A%2520the%2520scarcity%2520of%2520perspective%2520transformation%2520datasets%2520and%2520undefined%250Aassessment%2520criteria%2520for%2520perspective%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apresent%2520three%2520key%2520contributions%253A%2520%25281%2529%2520An%2520automated%2520framework%2520for%2520building%2520PPC%250Adatasets%2520through%2520expert%2520photographs.%2520%25282%2529%2520A%2520video%2520generation%2520approach%2520that%250Ademonstrates%2520the%2520transformation%2520process%2520from%2520less%2520favorable%2520to%2520aesthetically%250Aenhanced%2520perspectives.%2520%25283%2529%2520A%2520perspective%2520quality%2520assessment%2520%2528PQA%2529%2520model%250Aconstructed%2520based%2520on%2520human%2520performance.%2520Our%2520approach%2520is%2520concise%2520and%2520requires%2520no%250Aadditional%2520prompt%2520instructions%2520or%2520camera%2520trajectories%252C%2520helping%2520and%2520guiding%250Aordinary%2520users%2520to%2520enhance%2520their%2520composition%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20655v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photography%20Perspective%20Composition%3A%20Towards%20Aesthetic%20Perspective%0A%20%20Recommendation&entry.906535625=Lujian%20Yao%20and%20Siming%20Zheng%20and%20Xinbin%20Yuan%20and%20Zhuoxuan%20Cai%20and%20Pu%20Wu%20and%20Jinwei%20Chen%20and%20Bo%20Li%20and%20Peng-Tao%20Jiang&entry.1292438233=%20%20Traditional%20photography%20composition%20approaches%20are%20dominated%20by%202D%0Acropping-based%20methods.%20However%2C%20these%20methods%20fall%20short%20when%20scenes%20contain%0Apoorly%20arranged%20subjects.%20Professional%20photographers%20often%20employ%20perspective%0Aadjustment%20as%20a%20form%20of%203D%20recomposition%2C%20modifying%20the%20projected%202D%0Arelationships%20between%20subjects%20while%20maintaining%20their%20actual%20spatial%20positions%0Ato%20achieve%20better%20compositional%20balance.%20Inspired%20by%20this%20artistic%20practice%2C%20we%0Apropose%20photography%20perspective%20composition%20%28PPC%29%2C%20extending%20beyond%20traditional%0Acropping-based%20methods.%20However%2C%20implementing%20the%20PPC%20faces%20significant%0Achallenges%3A%20the%20scarcity%20of%20perspective%20transformation%20datasets%20and%20undefined%0Aassessment%20criteria%20for%20perspective%20quality.%20To%20address%20these%20challenges%2C%20we%0Apresent%20three%20key%20contributions%3A%20%281%29%20An%20automated%20framework%20for%20building%20PPC%0Adatasets%20through%20expert%20photographs.%20%282%29%20A%20video%20generation%20approach%20that%0Ademonstrates%20the%20transformation%20process%20from%20less%20favorable%20to%20aesthetically%0Aenhanced%20perspectives.%20%283%29%20A%20perspective%20quality%20assessment%20%28PQA%29%20model%0Aconstructed%20based%20on%20human%20performance.%20Our%20approach%20is%20concise%20and%20requires%20no%0Aadditional%20prompt%20instructions%20or%20camera%20trajectories%2C%20helping%20and%20guiding%0Aordinary%20users%20to%20enhance%20their%20composition%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20655v3&entry.124074799=Read"},
{"title": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs", "author": "Yue Meng and Fei Chen and Yongchao Chen and Chuchu Fan", "abstract": "  Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration.\n", "link": "http://arxiv.org/abs/2504.03015v2", "date": "2025-09-30", "relevancy": 1.6641, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5585}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuDeRe%3A%20Automated%20Strategy%20Decision%20and%20Realization%20in%20Robot%20Planning%0A%20%20and%20Control%20via%20LLMs&body=Title%3A%20AuDeRe%3A%20Automated%20Strategy%20Decision%20and%20Realization%20in%20Robot%20Planning%0A%20%20and%20Control%20via%20LLMs%0AAuthor%3A%20Yue%20Meng%20and%20Fei%20Chen%20and%20Yongchao%20Chen%20and%20Chuchu%20Fan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Apromise%20in%20various%20domains%2C%20especially%20robotics.%20However%2C%20most%20prior%20LLM-based%0Awork%20in%20robotic%20applications%20either%20directly%20predicts%20waypoints%20or%20applies%20LLMs%0Awithin%20fixed%20tool%20integration%20frameworks%2C%20offering%20limited%20flexibility%20in%0Aexploring%20and%20configuring%20solutions%20best%20suited%20to%20different%20tasks.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20that%20leverages%20LLMs%20to%20select%20appropriate%20planning%0Aand%20control%20strategies%20based%20on%20task%20descriptions%2C%20environmental%20constraints%2C%0Aand%20system%20dynamics.%20These%20strategies%20are%20then%20executed%20by%20calling%20the%0Aavailable%20comprehensive%20planning%20and%20control%20APIs.%20Our%20approach%20employs%0Aiterative%20LLM-based%20reasoning%20with%20performance%20feedback%20to%20refine%20the%20algorithm%0Aselection.%20We%20validate%20our%20approach%20through%20extensive%20experiments%20across%20tasks%0Aof%20varying%20complexity%2C%20from%20simple%20tracking%20to%20complex%20planning%20scenarios%0Ainvolving%20spatiotemporal%20constraints.%20The%20results%20demonstrate%20that%20using%20LLMs%0Ato%20determine%20planning%20and%20control%20strategies%20from%20natural%20language%20descriptions%0Asignificantly%20enhances%20robotic%20autonomy%20while%20reducing%20the%20need%20for%20extensive%0Amanual%20tuning%20and%20expert%20knowledge.%20Furthermore%2C%20our%20framework%20maintains%0Ageneralizability%20across%20different%20tasks%20and%20notably%20outperforms%20baseline%0Amethods%20that%20rely%20on%20LLMs%20for%20direct%20trajectory%2C%20control%20sequence%2C%20or%20code%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03015v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuDeRe%253A%2520Automated%2520Strategy%2520Decision%2520and%2520Realization%2520in%2520Robot%2520Planning%250A%2520%2520and%2520Control%2520via%2520LLMs%26entry.906535625%3DYue%2520Meng%2520and%2520Fei%2520Chen%2520and%2520Yongchao%2520Chen%2520and%2520Chuchu%2520Fan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%250Apromise%2520in%2520various%2520domains%252C%2520especially%2520robotics.%2520However%252C%2520most%2520prior%2520LLM-based%250Awork%2520in%2520robotic%2520applications%2520either%2520directly%2520predicts%2520waypoints%2520or%2520applies%2520LLMs%250Awithin%2520fixed%2520tool%2520integration%2520frameworks%252C%2520offering%2520limited%2520flexibility%2520in%250Aexploring%2520and%2520configuring%2520solutions%2520best%2520suited%2520to%2520different%2520tasks.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520framework%2520that%2520leverages%2520LLMs%2520to%2520select%2520appropriate%2520planning%250Aand%2520control%2520strategies%2520based%2520on%2520task%2520descriptions%252C%2520environmental%2520constraints%252C%250Aand%2520system%2520dynamics.%2520These%2520strategies%2520are%2520then%2520executed%2520by%2520calling%2520the%250Aavailable%2520comprehensive%2520planning%2520and%2520control%2520APIs.%2520Our%2520approach%2520employs%250Aiterative%2520LLM-based%2520reasoning%2520with%2520performance%2520feedback%2520to%2520refine%2520the%2520algorithm%250Aselection.%2520We%2520validate%2520our%2520approach%2520through%2520extensive%2520experiments%2520across%2520tasks%250Aof%2520varying%2520complexity%252C%2520from%2520simple%2520tracking%2520to%2520complex%2520planning%2520scenarios%250Ainvolving%2520spatiotemporal%2520constraints.%2520The%2520results%2520demonstrate%2520that%2520using%2520LLMs%250Ato%2520determine%2520planning%2520and%2520control%2520strategies%2520from%2520natural%2520language%2520descriptions%250Asignificantly%2520enhances%2520robotic%2520autonomy%2520while%2520reducing%2520the%2520need%2520for%2520extensive%250Amanual%2520tuning%2520and%2520expert%2520knowledge.%2520Furthermore%252C%2520our%2520framework%2520maintains%250Ageneralizability%2520across%2520different%2520tasks%2520and%2520notably%2520outperforms%2520baseline%250Amethods%2520that%2520rely%2520on%2520LLMs%2520for%2520direct%2520trajectory%252C%2520control%2520sequence%252C%2520or%2520code%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03015v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuDeRe%3A%20Automated%20Strategy%20Decision%20and%20Realization%20in%20Robot%20Planning%0A%20%20and%20Control%20via%20LLMs&entry.906535625=Yue%20Meng%20and%20Fei%20Chen%20and%20Yongchao%20Chen%20and%20Chuchu%20Fan&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20significant%0Apromise%20in%20various%20domains%2C%20especially%20robotics.%20However%2C%20most%20prior%20LLM-based%0Awork%20in%20robotic%20applications%20either%20directly%20predicts%20waypoints%20or%20applies%20LLMs%0Awithin%20fixed%20tool%20integration%20frameworks%2C%20offering%20limited%20flexibility%20in%0Aexploring%20and%20configuring%20solutions%20best%20suited%20to%20different%20tasks.%20In%20this%0Awork%2C%20we%20propose%20a%20framework%20that%20leverages%20LLMs%20to%20select%20appropriate%20planning%0Aand%20control%20strategies%20based%20on%20task%20descriptions%2C%20environmental%20constraints%2C%0Aand%20system%20dynamics.%20These%20strategies%20are%20then%20executed%20by%20calling%20the%0Aavailable%20comprehensive%20planning%20and%20control%20APIs.%20Our%20approach%20employs%0Aiterative%20LLM-based%20reasoning%20with%20performance%20feedback%20to%20refine%20the%20algorithm%0Aselection.%20We%20validate%20our%20approach%20through%20extensive%20experiments%20across%20tasks%0Aof%20varying%20complexity%2C%20from%20simple%20tracking%20to%20complex%20planning%20scenarios%0Ainvolving%20spatiotemporal%20constraints.%20The%20results%20demonstrate%20that%20using%20LLMs%0Ato%20determine%20planning%20and%20control%20strategies%20from%20natural%20language%20descriptions%0Asignificantly%20enhances%20robotic%20autonomy%20while%20reducing%20the%20need%20for%20extensive%0Amanual%20tuning%20and%20expert%20knowledge.%20Furthermore%2C%20our%20framework%20maintains%0Ageneralizability%20across%20different%20tasks%20and%20notably%20outperforms%20baseline%0Amethods%20that%20rely%20on%20LLMs%20for%20direct%20trajectory%2C%20control%20sequence%2C%20or%20code%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03015v2&entry.124074799=Read"},
{"title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in\n  Power Grids", "author": "Justin Tackett and Benjamin Francis and Luis Garcia and David Grimsman and Sean Warnick", "abstract": "  Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms.\n", "link": "http://arxiv.org/abs/2509.26532v1", "date": "2025-09-30", "relevancy": 1.664, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4244}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4145}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine-Learning%20Driven%20Load%20Shedding%20to%20Mitigate%20Instability%20Attacks%20in%0A%20%20Power%20Grids&body=Title%3A%20Machine-Learning%20Driven%20Load%20Shedding%20to%20Mitigate%20Instability%20Attacks%20in%0A%20%20Power%20Grids%0AAuthor%3A%20Justin%20Tackett%20and%20Benjamin%20Francis%20and%20Luis%20Garcia%20and%20David%20Grimsman%20and%20Sean%20Warnick%0AAbstract%3A%20%20%20Every%20year%20critical%20infrastructure%20becomes%20more%20complex%20and%20we%20grow%20to%20rely%0Aon%20it%20more%20and%20more.%20With%20this%20reliance%2C%20it%20becomes%20an%20attractive%20target%20for%0Acyberattacks%20from%20sophisticated%20actors%2C%20with%20one%20of%20the%20most%20attractive%20targets%0Abeing%20the%20power%20grid.%20One%20class%20of%20attacks%2C%20instability%20attacks%2C%20is%20a%20newer%0Atype%20of%20attack%20that%20has%20relatively%20few%20protections%20developed.%20We%20present%20a%20cost%0Aeffective%2C%20data-driven%20approach%20to%20training%20a%20supervised%20machine%20learning%20model%0Ato%20retrofit%20load%20shedding%20decision%20systems%20in%20power%20grids%20with%20the%20capacity%20to%0Adefend%20against%20instability%20attacks.%20We%20show%20a%20proof%20of%20concept%20on%20the%20IEEE%2014%0ABus%20System%20using%20the%20Achilles%20Heel%20Technologies%20Power%20Grid%20Analyzer%2C%20and%20show%0Athrough%20an%20implementation%20of%20modified%20Prony%20analysis%20%28MPA%29%20that%20MPA%20is%20a%20viable%0Amethod%20for%20detecting%20instability%20attacks%20and%20triggering%20defense%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine-Learning%2520Driven%2520Load%2520Shedding%2520to%2520Mitigate%2520Instability%2520Attacks%2520in%250A%2520%2520Power%2520Grids%26entry.906535625%3DJustin%2520Tackett%2520and%2520Benjamin%2520Francis%2520and%2520Luis%2520Garcia%2520and%2520David%2520Grimsman%2520and%2520Sean%2520Warnick%26entry.1292438233%3D%2520%2520Every%2520year%2520critical%2520infrastructure%2520becomes%2520more%2520complex%2520and%2520we%2520grow%2520to%2520rely%250Aon%2520it%2520more%2520and%2520more.%2520With%2520this%2520reliance%252C%2520it%2520becomes%2520an%2520attractive%2520target%2520for%250Acyberattacks%2520from%2520sophisticated%2520actors%252C%2520with%2520one%2520of%2520the%2520most%2520attractive%2520targets%250Abeing%2520the%2520power%2520grid.%2520One%2520class%2520of%2520attacks%252C%2520instability%2520attacks%252C%2520is%2520a%2520newer%250Atype%2520of%2520attack%2520that%2520has%2520relatively%2520few%2520protections%2520developed.%2520We%2520present%2520a%2520cost%250Aeffective%252C%2520data-driven%2520approach%2520to%2520training%2520a%2520supervised%2520machine%2520learning%2520model%250Ato%2520retrofit%2520load%2520shedding%2520decision%2520systems%2520in%2520power%2520grids%2520with%2520the%2520capacity%2520to%250Adefend%2520against%2520instability%2520attacks.%2520We%2520show%2520a%2520proof%2520of%2520concept%2520on%2520the%2520IEEE%252014%250ABus%2520System%2520using%2520the%2520Achilles%2520Heel%2520Technologies%2520Power%2520Grid%2520Analyzer%252C%2520and%2520show%250Athrough%2520an%2520implementation%2520of%2520modified%2520Prony%2520analysis%2520%2528MPA%2529%2520that%2520MPA%2520is%2520a%2520viable%250Amethod%2520for%2520detecting%2520instability%2520attacks%2520and%2520triggering%2520defense%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine-Learning%20Driven%20Load%20Shedding%20to%20Mitigate%20Instability%20Attacks%20in%0A%20%20Power%20Grids&entry.906535625=Justin%20Tackett%20and%20Benjamin%20Francis%20and%20Luis%20Garcia%20and%20David%20Grimsman%20and%20Sean%20Warnick&entry.1292438233=%20%20Every%20year%20critical%20infrastructure%20becomes%20more%20complex%20and%20we%20grow%20to%20rely%0Aon%20it%20more%20and%20more.%20With%20this%20reliance%2C%20it%20becomes%20an%20attractive%20target%20for%0Acyberattacks%20from%20sophisticated%20actors%2C%20with%20one%20of%20the%20most%20attractive%20targets%0Abeing%20the%20power%20grid.%20One%20class%20of%20attacks%2C%20instability%20attacks%2C%20is%20a%20newer%0Atype%20of%20attack%20that%20has%20relatively%20few%20protections%20developed.%20We%20present%20a%20cost%0Aeffective%2C%20data-driven%20approach%20to%20training%20a%20supervised%20machine%20learning%20model%0Ato%20retrofit%20load%20shedding%20decision%20systems%20in%20power%20grids%20with%20the%20capacity%20to%0Adefend%20against%20instability%20attacks.%20We%20show%20a%20proof%20of%20concept%20on%20the%20IEEE%2014%0ABus%20System%20using%20the%20Achilles%20Heel%20Technologies%20Power%20Grid%20Analyzer%2C%20and%20show%0Athrough%20an%20implementation%20of%20modified%20Prony%20analysis%20%28MPA%29%20that%20MPA%20is%20a%20viable%0Amethod%20for%20detecting%20instability%20attacks%20and%20triggering%20defense%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26532v1&entry.124074799=Read"},
{"title": "Contrastive Diffusion Guidance for Spatial Inverse Problems", "author": "Sattwik Basu and Chaitanya Amballa and Zhongweiyang Xu and Jorge Van\u010do Sampedro and Srihari Nelakuditi and Romit Roy Choudhury", "abstract": "  We consider the inverse problem of reconstructing the spatial layout of a\nplace, a home floorplan for example, from a user`s movements inside that\nlayout. Direct inversion is ill-posed since many floorplans can explain the\nsame movement trajectories. We adopt a diffusion-based posterior sampler to\ngenerate layouts consistent with the measurements. While active research is in\nprogress on generative inverse solvers, we find that the forward operator in\nour problem poses new challenges. The path-planning process inside a floorplan\nis a non-invertible, non-differentiable function, and causes instability while\noptimizing using the likelihood score. We break-away from existing approaches\nand reformulate the likelihood score in a smoother embedding space. The\nembedding space is trained with a contrastive loss which brings compatible\nfloorplans and trajectories close to each other, while pushing mismatched pairs\nfar apart. We show that a surrogate form of the likelihood score in this\nembedding space is a valid approximation of the true likelihood score, making\nit possible to steer the denoising process towards the posterior. Across\nextensive experiments, our model CoGuide produces more consistent floorplans\nfrom trajectories, and is more robust than differentiable-planner baselines and\nguided-diffusion methods.\n", "link": "http://arxiv.org/abs/2509.26489v1", "date": "2025-09-30", "relevancy": 1.6425, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5577}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5387}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Diffusion%20Guidance%20for%20Spatial%20Inverse%20Problems&body=Title%3A%20Contrastive%20Diffusion%20Guidance%20for%20Spatial%20Inverse%20Problems%0AAuthor%3A%20Sattwik%20Basu%20and%20Chaitanya%20Amballa%20and%20Zhongweiyang%20Xu%20and%20Jorge%20Van%C4%8Do%20Sampedro%20and%20Srihari%20Nelakuditi%20and%20Romit%20Roy%20Choudhury%0AAbstract%3A%20%20%20We%20consider%20the%20inverse%20problem%20of%20reconstructing%20the%20spatial%20layout%20of%20a%0Aplace%2C%20a%20home%20floorplan%20for%20example%2C%20from%20a%20user%60s%20movements%20inside%20that%0Alayout.%20Direct%20inversion%20is%20ill-posed%20since%20many%20floorplans%20can%20explain%20the%0Asame%20movement%20trajectories.%20We%20adopt%20a%20diffusion-based%20posterior%20sampler%20to%0Agenerate%20layouts%20consistent%20with%20the%20measurements.%20While%20active%20research%20is%20in%0Aprogress%20on%20generative%20inverse%20solvers%2C%20we%20find%20that%20the%20forward%20operator%20in%0Aour%20problem%20poses%20new%20challenges.%20The%20path-planning%20process%20inside%20a%20floorplan%0Ais%20a%20non-invertible%2C%20non-differentiable%20function%2C%20and%20causes%20instability%20while%0Aoptimizing%20using%20the%20likelihood%20score.%20We%20break-away%20from%20existing%20approaches%0Aand%20reformulate%20the%20likelihood%20score%20in%20a%20smoother%20embedding%20space.%20The%0Aembedding%20space%20is%20trained%20with%20a%20contrastive%20loss%20which%20brings%20compatible%0Afloorplans%20and%20trajectories%20close%20to%20each%20other%2C%20while%20pushing%20mismatched%20pairs%0Afar%20apart.%20We%20show%20that%20a%20surrogate%20form%20of%20the%20likelihood%20score%20in%20this%0Aembedding%20space%20is%20a%20valid%20approximation%20of%20the%20true%20likelihood%20score%2C%20making%0Ait%20possible%20to%20steer%20the%20denoising%20process%20towards%20the%20posterior.%20Across%0Aextensive%20experiments%2C%20our%20model%20CoGuide%20produces%20more%20consistent%20floorplans%0Afrom%20trajectories%2C%20and%20is%20more%20robust%20than%20differentiable-planner%20baselines%20and%0Aguided-diffusion%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Diffusion%2520Guidance%2520for%2520Spatial%2520Inverse%2520Problems%26entry.906535625%3DSattwik%2520Basu%2520and%2520Chaitanya%2520Amballa%2520and%2520Zhongweiyang%2520Xu%2520and%2520Jorge%2520Van%25C4%258Do%2520Sampedro%2520and%2520Srihari%2520Nelakuditi%2520and%2520Romit%2520Roy%2520Choudhury%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520inverse%2520problem%2520of%2520reconstructing%2520the%2520spatial%2520layout%2520of%2520a%250Aplace%252C%2520a%2520home%2520floorplan%2520for%2520example%252C%2520from%2520a%2520user%2560s%2520movements%2520inside%2520that%250Alayout.%2520Direct%2520inversion%2520is%2520ill-posed%2520since%2520many%2520floorplans%2520can%2520explain%2520the%250Asame%2520movement%2520trajectories.%2520We%2520adopt%2520a%2520diffusion-based%2520posterior%2520sampler%2520to%250Agenerate%2520layouts%2520consistent%2520with%2520the%2520measurements.%2520While%2520active%2520research%2520is%2520in%250Aprogress%2520on%2520generative%2520inverse%2520solvers%252C%2520we%2520find%2520that%2520the%2520forward%2520operator%2520in%250Aour%2520problem%2520poses%2520new%2520challenges.%2520The%2520path-planning%2520process%2520inside%2520a%2520floorplan%250Ais%2520a%2520non-invertible%252C%2520non-differentiable%2520function%252C%2520and%2520causes%2520instability%2520while%250Aoptimizing%2520using%2520the%2520likelihood%2520score.%2520We%2520break-away%2520from%2520existing%2520approaches%250Aand%2520reformulate%2520the%2520likelihood%2520score%2520in%2520a%2520smoother%2520embedding%2520space.%2520The%250Aembedding%2520space%2520is%2520trained%2520with%2520a%2520contrastive%2520loss%2520which%2520brings%2520compatible%250Afloorplans%2520and%2520trajectories%2520close%2520to%2520each%2520other%252C%2520while%2520pushing%2520mismatched%2520pairs%250Afar%2520apart.%2520We%2520show%2520that%2520a%2520surrogate%2520form%2520of%2520the%2520likelihood%2520score%2520in%2520this%250Aembedding%2520space%2520is%2520a%2520valid%2520approximation%2520of%2520the%2520true%2520likelihood%2520score%252C%2520making%250Ait%2520possible%2520to%2520steer%2520the%2520denoising%2520process%2520towards%2520the%2520posterior.%2520Across%250Aextensive%2520experiments%252C%2520our%2520model%2520CoGuide%2520produces%2520more%2520consistent%2520floorplans%250Afrom%2520trajectories%252C%2520and%2520is%2520more%2520robust%2520than%2520differentiable-planner%2520baselines%2520and%250Aguided-diffusion%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Diffusion%20Guidance%20for%20Spatial%20Inverse%20Problems&entry.906535625=Sattwik%20Basu%20and%20Chaitanya%20Amballa%20and%20Zhongweiyang%20Xu%20and%20Jorge%20Van%C4%8Do%20Sampedro%20and%20Srihari%20Nelakuditi%20and%20Romit%20Roy%20Choudhury&entry.1292438233=%20%20We%20consider%20the%20inverse%20problem%20of%20reconstructing%20the%20spatial%20layout%20of%20a%0Aplace%2C%20a%20home%20floorplan%20for%20example%2C%20from%20a%20user%60s%20movements%20inside%20that%0Alayout.%20Direct%20inversion%20is%20ill-posed%20since%20many%20floorplans%20can%20explain%20the%0Asame%20movement%20trajectories.%20We%20adopt%20a%20diffusion-based%20posterior%20sampler%20to%0Agenerate%20layouts%20consistent%20with%20the%20measurements.%20While%20active%20research%20is%20in%0Aprogress%20on%20generative%20inverse%20solvers%2C%20we%20find%20that%20the%20forward%20operator%20in%0Aour%20problem%20poses%20new%20challenges.%20The%20path-planning%20process%20inside%20a%20floorplan%0Ais%20a%20non-invertible%2C%20non-differentiable%20function%2C%20and%20causes%20instability%20while%0Aoptimizing%20using%20the%20likelihood%20score.%20We%20break-away%20from%20existing%20approaches%0Aand%20reformulate%20the%20likelihood%20score%20in%20a%20smoother%20embedding%20space.%20The%0Aembedding%20space%20is%20trained%20with%20a%20contrastive%20loss%20which%20brings%20compatible%0Afloorplans%20and%20trajectories%20close%20to%20each%20other%2C%20while%20pushing%20mismatched%20pairs%0Afar%20apart.%20We%20show%20that%20a%20surrogate%20form%20of%20the%20likelihood%20score%20in%20this%0Aembedding%20space%20is%20a%20valid%20approximation%20of%20the%20true%20likelihood%20score%2C%20making%0Ait%20possible%20to%20steer%20the%20denoising%20process%20towards%20the%20posterior.%20Across%0Aextensive%20experiments%2C%20our%20model%20CoGuide%20produces%20more%20consistent%20floorplans%0Afrom%20trajectories%2C%20and%20is%20more%20robust%20than%20differentiable-planner%20baselines%20and%0Aguided-diffusion%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26489v1&entry.124074799=Read"},
{"title": "Robot Conga: A Leader-Follower Walking Approach to Sequential Path\n  Following in Multi-Agent Systems", "author": "Pranav Tiwari and Soumyodipta Nath", "abstract": "  Coordinated path following in multi-agent systems is a key challenge in\nrobotics, with applications in automated logistics, surveillance, and\ncollaborative exploration. Traditional formation control techniques often rely\non time-parameterized trajectories and path integrals, which can result in\nsynchronization issues and rigid behavior. In this work, we address the problem\nof sequential path following, where agents maintain fixed spatial separation\nalong a common trajectory, guided by a leader under centralized control. We\nintroduce Robot Conga, a leader-follower control strategy that updates each\nagent's desired state based on the leader's spatial displacement rather than\ntime, assuming access to a global position reference, an assumption valid in\nindoor environments equipped with motion capture, vision-based tracking, or UWB\nlocalization systems. The algorithm was validated in simulation using both\nTurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate\ntrajectory tracking, stable inter-agent spacing, and fast convergence, with all\nagents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped\ncase, and almost instantaneously in the TurtleBot3 implementation.\n", "link": "http://arxiv.org/abs/2509.16482v2", "date": "2025-09-30", "relevancy": 1.639, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5597}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20Conga%3A%20A%20Leader-Follower%20Walking%20Approach%20to%20Sequential%20Path%0A%20%20Following%20in%20Multi-Agent%20Systems&body=Title%3A%20Robot%20Conga%3A%20A%20Leader-Follower%20Walking%20Approach%20to%20Sequential%20Path%0A%20%20Following%20in%20Multi-Agent%20Systems%0AAuthor%3A%20Pranav%20Tiwari%20and%20Soumyodipta%20Nath%0AAbstract%3A%20%20%20Coordinated%20path%20following%20in%20multi-agent%20systems%20is%20a%20key%20challenge%20in%0Arobotics%2C%20with%20applications%20in%20automated%20logistics%2C%20surveillance%2C%20and%0Acollaborative%20exploration.%20Traditional%20formation%20control%20techniques%20often%20rely%0Aon%20time-parameterized%20trajectories%20and%20path%20integrals%2C%20which%20can%20result%20in%0Asynchronization%20issues%20and%20rigid%20behavior.%20In%20this%20work%2C%20we%20address%20the%20problem%0Aof%20sequential%20path%20following%2C%20where%20agents%20maintain%20fixed%20spatial%20separation%0Aalong%20a%20common%20trajectory%2C%20guided%20by%20a%20leader%20under%20centralized%20control.%20We%0Aintroduce%20Robot%20Conga%2C%20a%20leader-follower%20control%20strategy%20that%20updates%20each%0Aagent%27s%20desired%20state%20based%20on%20the%20leader%27s%20spatial%20displacement%20rather%20than%0Atime%2C%20assuming%20access%20to%20a%20global%20position%20reference%2C%20an%20assumption%20valid%20in%0Aindoor%20environments%20equipped%20with%20motion%20capture%2C%20vision-based%20tracking%2C%20or%20UWB%0Alocalization%20systems.%20The%20algorithm%20was%20validated%20in%20simulation%20using%20both%0ATurtleBot3%20and%20quadruped%20%28Laikago%29%20robots.%20Results%20demonstrate%20accurate%0Atrajectory%20tracking%2C%20stable%20inter-agent%20spacing%2C%20and%20fast%20convergence%2C%20with%20all%0Aagents%20aligning%20within%20250%20time%20steps%20%28approx.%200.25%20seconds%29%20in%20the%20quadruped%0Acase%2C%20and%20almost%20instantaneously%20in%20the%20TurtleBot3%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520Conga%253A%2520A%2520Leader-Follower%2520Walking%2520Approach%2520to%2520Sequential%2520Path%250A%2520%2520Following%2520in%2520Multi-Agent%2520Systems%26entry.906535625%3DPranav%2520Tiwari%2520and%2520Soumyodipta%2520Nath%26entry.1292438233%3D%2520%2520Coordinated%2520path%2520following%2520in%2520multi-agent%2520systems%2520is%2520a%2520key%2520challenge%2520in%250Arobotics%252C%2520with%2520applications%2520in%2520automated%2520logistics%252C%2520surveillance%252C%2520and%250Acollaborative%2520exploration.%2520Traditional%2520formation%2520control%2520techniques%2520often%2520rely%250Aon%2520time-parameterized%2520trajectories%2520and%2520path%2520integrals%252C%2520which%2520can%2520result%2520in%250Asynchronization%2520issues%2520and%2520rigid%2520behavior.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%250Aof%2520sequential%2520path%2520following%252C%2520where%2520agents%2520maintain%2520fixed%2520spatial%2520separation%250Aalong%2520a%2520common%2520trajectory%252C%2520guided%2520by%2520a%2520leader%2520under%2520centralized%2520control.%2520We%250Aintroduce%2520Robot%2520Conga%252C%2520a%2520leader-follower%2520control%2520strategy%2520that%2520updates%2520each%250Aagent%2527s%2520desired%2520state%2520based%2520on%2520the%2520leader%2527s%2520spatial%2520displacement%2520rather%2520than%250Atime%252C%2520assuming%2520access%2520to%2520a%2520global%2520position%2520reference%252C%2520an%2520assumption%2520valid%2520in%250Aindoor%2520environments%2520equipped%2520with%2520motion%2520capture%252C%2520vision-based%2520tracking%252C%2520or%2520UWB%250Alocalization%2520systems.%2520The%2520algorithm%2520was%2520validated%2520in%2520simulation%2520using%2520both%250ATurtleBot3%2520and%2520quadruped%2520%2528Laikago%2529%2520robots.%2520Results%2520demonstrate%2520accurate%250Atrajectory%2520tracking%252C%2520stable%2520inter-agent%2520spacing%252C%2520and%2520fast%2520convergence%252C%2520with%2520all%250Aagents%2520aligning%2520within%2520250%2520time%2520steps%2520%2528approx.%25200.25%2520seconds%2529%2520in%2520the%2520quadruped%250Acase%252C%2520and%2520almost%2520instantaneously%2520in%2520the%2520TurtleBot3%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Conga%3A%20A%20Leader-Follower%20Walking%20Approach%20to%20Sequential%20Path%0A%20%20Following%20in%20Multi-Agent%20Systems&entry.906535625=Pranav%20Tiwari%20and%20Soumyodipta%20Nath&entry.1292438233=%20%20Coordinated%20path%20following%20in%20multi-agent%20systems%20is%20a%20key%20challenge%20in%0Arobotics%2C%20with%20applications%20in%20automated%20logistics%2C%20surveillance%2C%20and%0Acollaborative%20exploration.%20Traditional%20formation%20control%20techniques%20often%20rely%0Aon%20time-parameterized%20trajectories%20and%20path%20integrals%2C%20which%20can%20result%20in%0Asynchronization%20issues%20and%20rigid%20behavior.%20In%20this%20work%2C%20we%20address%20the%20problem%0Aof%20sequential%20path%20following%2C%20where%20agents%20maintain%20fixed%20spatial%20separation%0Aalong%20a%20common%20trajectory%2C%20guided%20by%20a%20leader%20under%20centralized%20control.%20We%0Aintroduce%20Robot%20Conga%2C%20a%20leader-follower%20control%20strategy%20that%20updates%20each%0Aagent%27s%20desired%20state%20based%20on%20the%20leader%27s%20spatial%20displacement%20rather%20than%0Atime%2C%20assuming%20access%20to%20a%20global%20position%20reference%2C%20an%20assumption%20valid%20in%0Aindoor%20environments%20equipped%20with%20motion%20capture%2C%20vision-based%20tracking%2C%20or%20UWB%0Alocalization%20systems.%20The%20algorithm%20was%20validated%20in%20simulation%20using%20both%0ATurtleBot3%20and%20quadruped%20%28Laikago%29%20robots.%20Results%20demonstrate%20accurate%0Atrajectory%20tracking%2C%20stable%20inter-agent%20spacing%2C%20and%20fast%20convergence%2C%20with%20all%0Aagents%20aligning%20within%20250%20time%20steps%20%28approx.%200.25%20seconds%29%20in%20the%20quadruped%0Acase%2C%20and%20almost%20instantaneously%20in%20the%20TurtleBot3%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16482v2&entry.124074799=Read"},
{"title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents", "author": "Yida Xue and Mingjun Mao and Xiangyuan Ru and Yuqi Zhu and Baochang Ren and Shuofei Qiao and Mengru Wang and Shumin Deng and Xinyu An and Ningyu Zhang and Ying Chen and Huajun Chen", "abstract": "  We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.\n", "link": "http://arxiv.org/abs/2509.26536v1", "date": "2025-09-30", "relevancy": 1.6363, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5433}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OceanGym%3A%20A%20Benchmark%20Environment%20for%20Underwater%20Embodied%20Agents&body=Title%3A%20OceanGym%3A%20A%20Benchmark%20Environment%20for%20Underwater%20Embodied%20Agents%0AAuthor%3A%20Yida%20Xue%20and%20Mingjun%20Mao%20and%20Xiangyuan%20Ru%20and%20Yuqi%20Zhu%20and%20Baochang%20Ren%20and%20Shuofei%20Qiao%20and%20Mengru%20Wang%20and%20Shumin%20Deng%20and%20Xinyu%20An%20and%20Ningyu%20Zhang%20and%20Ying%20Chen%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20We%20introduce%20OceanGym%2C%20the%20first%20comprehensive%20benchmark%20for%20ocean%20underwater%0Aembodied%20agents%2C%20designed%20to%20advance%20AI%20in%20one%20of%20the%20most%20demanding%20real-world%0Aenvironments.%20Unlike%20terrestrial%20or%20aerial%20domains%2C%20underwater%20settings%20present%0Aextreme%20perceptual%20and%20decision-making%20challenges%2C%20including%20low%20visibility%2C%0Adynamic%20ocean%20currents%2C%20making%20effective%20agent%20deployment%20exceptionally%0Adifficult.%20OceanGym%20encompasses%20eight%20realistic%20task%20domains%20and%20a%20unified%0Aagent%20framework%20driven%20by%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%0Aintegrates%20perception%2C%20memory%2C%20and%20sequential%20decision-making.%20Agents%20are%0Arequired%20to%20comprehend%20optical%20and%20sonar%20data%2C%20autonomously%20explore%20complex%0Aenvironments%2C%20and%20accomplish%20long-horizon%20objectives%20under%20these%20harsh%0Aconditions.%20Extensive%20experiments%20reveal%20substantial%20gaps%20between%0Astate-of-the-art%20MLLM-driven%20agents%20and%20human%20experts%2C%20highlighting%20the%0Apersistent%20difficulty%20of%20perception%2C%20planning%2C%20and%20adaptability%20in%20ocean%0Aunderwater%20environments.%20By%20providing%20a%20high-fidelity%2C%20rigorously%20designed%0Aplatform%2C%20OceanGym%20establishes%20a%20testbed%20for%20developing%20robust%20embodied%20AI%20and%0Atransferring%20these%20capabilities%20to%20real-world%20autonomous%20ocean%20underwater%0Avehicles%2C%20marking%20a%20decisive%20step%20toward%20intelligent%20agents%20capable%20of%0Aoperating%20in%20one%20of%20Earth%27s%20last%20unexplored%20frontiers.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/OceanGPT/OceanGym.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOceanGym%253A%2520A%2520Benchmark%2520Environment%2520for%2520Underwater%2520Embodied%2520Agents%26entry.906535625%3DYida%2520Xue%2520and%2520Mingjun%2520Mao%2520and%2520Xiangyuan%2520Ru%2520and%2520Yuqi%2520Zhu%2520and%2520Baochang%2520Ren%2520and%2520Shuofei%2520Qiao%2520and%2520Mengru%2520Wang%2520and%2520Shumin%2520Deng%2520and%2520Xinyu%2520An%2520and%2520Ningyu%2520Zhang%2520and%2520Ying%2520Chen%2520and%2520Huajun%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520OceanGym%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%2520ocean%2520underwater%250Aembodied%2520agents%252C%2520designed%2520to%2520advance%2520AI%2520in%2520one%2520of%2520the%2520most%2520demanding%2520real-world%250Aenvironments.%2520Unlike%2520terrestrial%2520or%2520aerial%2520domains%252C%2520underwater%2520settings%2520present%250Aextreme%2520perceptual%2520and%2520decision-making%2520challenges%252C%2520including%2520low%2520visibility%252C%250Adynamic%2520ocean%2520currents%252C%2520making%2520effective%2520agent%2520deployment%2520exceptionally%250Adifficult.%2520OceanGym%2520encompasses%2520eight%2520realistic%2520task%2520domains%2520and%2520a%2520unified%250Aagent%2520framework%2520driven%2520by%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520which%250Aintegrates%2520perception%252C%2520memory%252C%2520and%2520sequential%2520decision-making.%2520Agents%2520are%250Arequired%2520to%2520comprehend%2520optical%2520and%2520sonar%2520data%252C%2520autonomously%2520explore%2520complex%250Aenvironments%252C%2520and%2520accomplish%2520long-horizon%2520objectives%2520under%2520these%2520harsh%250Aconditions.%2520Extensive%2520experiments%2520reveal%2520substantial%2520gaps%2520between%250Astate-of-the-art%2520MLLM-driven%2520agents%2520and%2520human%2520experts%252C%2520highlighting%2520the%250Apersistent%2520difficulty%2520of%2520perception%252C%2520planning%252C%2520and%2520adaptability%2520in%2520ocean%250Aunderwater%2520environments.%2520By%2520providing%2520a%2520high-fidelity%252C%2520rigorously%2520designed%250Aplatform%252C%2520OceanGym%2520establishes%2520a%2520testbed%2520for%2520developing%2520robust%2520embodied%2520AI%2520and%250Atransferring%2520these%2520capabilities%2520to%2520real-world%2520autonomous%2520ocean%2520underwater%250Avehicles%252C%2520marking%2520a%2520decisive%2520step%2520toward%2520intelligent%2520agents%2520capable%2520of%250Aoperating%2520in%2520one%2520of%2520Earth%2527s%2520last%2520unexplored%2520frontiers.%2520The%2520code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//github.com/OceanGPT/OceanGym.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OceanGym%3A%20A%20Benchmark%20Environment%20for%20Underwater%20Embodied%20Agents&entry.906535625=Yida%20Xue%20and%20Mingjun%20Mao%20and%20Xiangyuan%20Ru%20and%20Yuqi%20Zhu%20and%20Baochang%20Ren%20and%20Shuofei%20Qiao%20and%20Mengru%20Wang%20and%20Shumin%20Deng%20and%20Xinyu%20An%20and%20Ningyu%20Zhang%20and%20Ying%20Chen%20and%20Huajun%20Chen&entry.1292438233=%20%20We%20introduce%20OceanGym%2C%20the%20first%20comprehensive%20benchmark%20for%20ocean%20underwater%0Aembodied%20agents%2C%20designed%20to%20advance%20AI%20in%20one%20of%20the%20most%20demanding%20real-world%0Aenvironments.%20Unlike%20terrestrial%20or%20aerial%20domains%2C%20underwater%20settings%20present%0Aextreme%20perceptual%20and%20decision-making%20challenges%2C%20including%20low%20visibility%2C%0Adynamic%20ocean%20currents%2C%20making%20effective%20agent%20deployment%20exceptionally%0Adifficult.%20OceanGym%20encompasses%20eight%20realistic%20task%20domains%20and%20a%20unified%0Aagent%20framework%20driven%20by%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%0Aintegrates%20perception%2C%20memory%2C%20and%20sequential%20decision-making.%20Agents%20are%0Arequired%20to%20comprehend%20optical%20and%20sonar%20data%2C%20autonomously%20explore%20complex%0Aenvironments%2C%20and%20accomplish%20long-horizon%20objectives%20under%20these%20harsh%0Aconditions.%20Extensive%20experiments%20reveal%20substantial%20gaps%20between%0Astate-of-the-art%20MLLM-driven%20agents%20and%20human%20experts%2C%20highlighting%20the%0Apersistent%20difficulty%20of%20perception%2C%20planning%2C%20and%20adaptability%20in%20ocean%0Aunderwater%20environments.%20By%20providing%20a%20high-fidelity%2C%20rigorously%20designed%0Aplatform%2C%20OceanGym%20establishes%20a%20testbed%20for%20developing%20robust%20embodied%20AI%20and%0Atransferring%20these%20capabilities%20to%20real-world%20autonomous%20ocean%20underwater%0Avehicles%2C%20marking%20a%20decisive%20step%20toward%20intelligent%20agents%20capable%20of%0Aoperating%20in%20one%20of%20Earth%27s%20last%20unexplored%20frontiers.%20The%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/OceanGPT/OceanGym.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26536v1&entry.124074799=Read"},
{"title": "Fine-tuning Behavioral Cloning Policies with Preference-Based\n  Reinforcement Learning", "author": "Ma\u00ebl Macuglia and Paul Friedrich and Giorgia Ramponi", "abstract": "  Deploying reinforcement learning (RL) in robotics, industry, and health care\nis blocked by two obstacles: the difficulty of specifying accurate rewards and\nthe risk of unsafe, data-hungry exploration. We address this by proposing a\ntwo-stage framework that first learns a safe initial policy from a reward-free\ndataset of expert demonstrations, then fine-tunes it online using\npreference-based human feedback. We provide the first principled analysis of\nthis offline-to-online approach and introduce BRIDGE, a unified algorithm that\nintegrates both signals via an uncertainty-weighted objective. We derive regret\nbounds that shrink with the number of offline demonstrations, explicitly\nconnecting the quantity of offline data to online sample efficiency. We\nvalidate BRIDGE in discrete and continuous control MuJoCo environments, showing\nit achieves lower regret than both standalone behavioral cloning and online\npreference-based RL. Our work establishes a theoretical foundation for\ndesigning more sample-efficient interactive agents.\n", "link": "http://arxiv.org/abs/2509.26605v1", "date": "2025-09-30", "relevancy": 1.635, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5666}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5479}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20Behavioral%20Cloning%20Policies%20with%20Preference-Based%0A%20%20Reinforcement%20Learning&body=Title%3A%20Fine-tuning%20Behavioral%20Cloning%20Policies%20with%20Preference-Based%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ma%C3%ABl%20Macuglia%20and%20Paul%20Friedrich%20and%20Giorgia%20Ramponi%0AAbstract%3A%20%20%20Deploying%20reinforcement%20learning%20%28RL%29%20in%20robotics%2C%20industry%2C%20and%20health%20care%0Ais%20blocked%20by%20two%20obstacles%3A%20the%20difficulty%20of%20specifying%20accurate%20rewards%20and%0Athe%20risk%20of%20unsafe%2C%20data-hungry%20exploration.%20We%20address%20this%20by%20proposing%20a%0Atwo-stage%20framework%20that%20first%20learns%20a%20safe%20initial%20policy%20from%20a%20reward-free%0Adataset%20of%20expert%20demonstrations%2C%20then%20fine-tunes%20it%20online%20using%0Apreference-based%20human%20feedback.%20We%20provide%20the%20first%20principled%20analysis%20of%0Athis%20offline-to-online%20approach%20and%20introduce%20BRIDGE%2C%20a%20unified%20algorithm%20that%0Aintegrates%20both%20signals%20via%20an%20uncertainty-weighted%20objective.%20We%20derive%20regret%0Abounds%20that%20shrink%20with%20the%20number%20of%20offline%20demonstrations%2C%20explicitly%0Aconnecting%20the%20quantity%20of%20offline%20data%20to%20online%20sample%20efficiency.%20We%0Avalidate%20BRIDGE%20in%20discrete%20and%20continuous%20control%20MuJoCo%20environments%2C%20showing%0Ait%20achieves%20lower%20regret%20than%20both%20standalone%20behavioral%20cloning%20and%20online%0Apreference-based%20RL.%20Our%20work%20establishes%20a%20theoretical%20foundation%20for%0Adesigning%20more%20sample-efficient%20interactive%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520Behavioral%2520Cloning%2520Policies%2520with%2520Preference-Based%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DMa%25C3%25ABl%2520Macuglia%2520and%2520Paul%2520Friedrich%2520and%2520Giorgia%2520Ramponi%26entry.1292438233%3D%2520%2520Deploying%2520reinforcement%2520learning%2520%2528RL%2529%2520in%2520robotics%252C%2520industry%252C%2520and%2520health%2520care%250Ais%2520blocked%2520by%2520two%2520obstacles%253A%2520the%2520difficulty%2520of%2520specifying%2520accurate%2520rewards%2520and%250Athe%2520risk%2520of%2520unsafe%252C%2520data-hungry%2520exploration.%2520We%2520address%2520this%2520by%2520proposing%2520a%250Atwo-stage%2520framework%2520that%2520first%2520learns%2520a%2520safe%2520initial%2520policy%2520from%2520a%2520reward-free%250Adataset%2520of%2520expert%2520demonstrations%252C%2520then%2520fine-tunes%2520it%2520online%2520using%250Apreference-based%2520human%2520feedback.%2520We%2520provide%2520the%2520first%2520principled%2520analysis%2520of%250Athis%2520offline-to-online%2520approach%2520and%2520introduce%2520BRIDGE%252C%2520a%2520unified%2520algorithm%2520that%250Aintegrates%2520both%2520signals%2520via%2520an%2520uncertainty-weighted%2520objective.%2520We%2520derive%2520regret%250Abounds%2520that%2520shrink%2520with%2520the%2520number%2520of%2520offline%2520demonstrations%252C%2520explicitly%250Aconnecting%2520the%2520quantity%2520of%2520offline%2520data%2520to%2520online%2520sample%2520efficiency.%2520We%250Avalidate%2520BRIDGE%2520in%2520discrete%2520and%2520continuous%2520control%2520MuJoCo%2520environments%252C%2520showing%250Ait%2520achieves%2520lower%2520regret%2520than%2520both%2520standalone%2520behavioral%2520cloning%2520and%2520online%250Apreference-based%2520RL.%2520Our%2520work%2520establishes%2520a%2520theoretical%2520foundation%2520for%250Adesigning%2520more%2520sample-efficient%2520interactive%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20Behavioral%20Cloning%20Policies%20with%20Preference-Based%0A%20%20Reinforcement%20Learning&entry.906535625=Ma%C3%ABl%20Macuglia%20and%20Paul%20Friedrich%20and%20Giorgia%20Ramponi&entry.1292438233=%20%20Deploying%20reinforcement%20learning%20%28RL%29%20in%20robotics%2C%20industry%2C%20and%20health%20care%0Ais%20blocked%20by%20two%20obstacles%3A%20the%20difficulty%20of%20specifying%20accurate%20rewards%20and%0Athe%20risk%20of%20unsafe%2C%20data-hungry%20exploration.%20We%20address%20this%20by%20proposing%20a%0Atwo-stage%20framework%20that%20first%20learns%20a%20safe%20initial%20policy%20from%20a%20reward-free%0Adataset%20of%20expert%20demonstrations%2C%20then%20fine-tunes%20it%20online%20using%0Apreference-based%20human%20feedback.%20We%20provide%20the%20first%20principled%20analysis%20of%0Athis%20offline-to-online%20approach%20and%20introduce%20BRIDGE%2C%20a%20unified%20algorithm%20that%0Aintegrates%20both%20signals%20via%20an%20uncertainty-weighted%20objective.%20We%20derive%20regret%0Abounds%20that%20shrink%20with%20the%20number%20of%20offline%20demonstrations%2C%20explicitly%0Aconnecting%20the%20quantity%20of%20offline%20data%20to%20online%20sample%20efficiency.%20We%0Avalidate%20BRIDGE%20in%20discrete%20and%20continuous%20control%20MuJoCo%20environments%2C%20showing%0Ait%20achieves%20lower%20regret%20than%20both%20standalone%20behavioral%20cloning%20and%20online%0Apreference-based%20RL.%20Our%20work%20establishes%20a%20theoretical%20foundation%20for%0Adesigning%20more%20sample-efficient%20interactive%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26605v1&entry.124074799=Read"},
{"title": "AI-assisted Advanced Propellant Development for Electric Propulsion", "author": "Angel Pan Du and Miguel Arana-Catania and Enric Grustan Guti\u00e9rrez", "abstract": "  Artificial Intelligence algorithms are introduced in this work as a tool to\npredict the performance of new chemical compounds as alternative propellants\nfor electric propulsion, focusing on predicting their ionisation\ncharacteristics and fragmentation patterns. The chemical properties and\nstructure of the compounds are encoded using a chemical fingerprint, and the\ntraining datasets are extracted from the NIST WebBook. The AI-predicted\nionisation energy and minimum appearance energy have a mean relative error of\n6.87% and 7.99%, respectively, and a predicted ion mass with a 23.89% relative\nerror. In the cases of full mass spectra due to electron ionisation, the\npredictions have a cosine similarity of 0.6395 and align with the top 10 most\nsimilar mass spectra in 78% of instances within a 30 Da range.\n", "link": "http://arxiv.org/abs/2509.26567v1", "date": "2025-09-30", "relevancy": 1.6291, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4597}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3987}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.3949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-assisted%20Advanced%20Propellant%20Development%20for%20Electric%20Propulsion&body=Title%3A%20AI-assisted%20Advanced%20Propellant%20Development%20for%20Electric%20Propulsion%0AAuthor%3A%20Angel%20Pan%20Du%20and%20Miguel%20Arana-Catania%20and%20Enric%20Grustan%20Guti%C3%A9rrez%0AAbstract%3A%20%20%20Artificial%20Intelligence%20algorithms%20are%20introduced%20in%20this%20work%20as%20a%20tool%20to%0Apredict%20the%20performance%20of%20new%20chemical%20compounds%20as%20alternative%20propellants%0Afor%20electric%20propulsion%2C%20focusing%20on%20predicting%20their%20ionisation%0Acharacteristics%20and%20fragmentation%20patterns.%20The%20chemical%20properties%20and%0Astructure%20of%20the%20compounds%20are%20encoded%20using%20a%20chemical%20fingerprint%2C%20and%20the%0Atraining%20datasets%20are%20extracted%20from%20the%20NIST%20WebBook.%20The%20AI-predicted%0Aionisation%20energy%20and%20minimum%20appearance%20energy%20have%20a%20mean%20relative%20error%20of%0A6.87%25%20and%207.99%25%2C%20respectively%2C%20and%20a%20predicted%20ion%20mass%20with%20a%2023.89%25%20relative%0Aerror.%20In%20the%20cases%20of%20full%20mass%20spectra%20due%20to%20electron%20ionisation%2C%20the%0Apredictions%20have%20a%20cosine%20similarity%20of%200.6395%20and%20align%20with%20the%20top%2010%20most%0Asimilar%20mass%20spectra%20in%2078%25%20of%20instances%20within%20a%2030%20Da%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-assisted%2520Advanced%2520Propellant%2520Development%2520for%2520Electric%2520Propulsion%26entry.906535625%3DAngel%2520Pan%2520Du%2520and%2520Miguel%2520Arana-Catania%2520and%2520Enric%2520Grustan%2520Guti%25C3%25A9rrez%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520algorithms%2520are%2520introduced%2520in%2520this%2520work%2520as%2520a%2520tool%2520to%250Apredict%2520the%2520performance%2520of%2520new%2520chemical%2520compounds%2520as%2520alternative%2520propellants%250Afor%2520electric%2520propulsion%252C%2520focusing%2520on%2520predicting%2520their%2520ionisation%250Acharacteristics%2520and%2520fragmentation%2520patterns.%2520The%2520chemical%2520properties%2520and%250Astructure%2520of%2520the%2520compounds%2520are%2520encoded%2520using%2520a%2520chemical%2520fingerprint%252C%2520and%2520the%250Atraining%2520datasets%2520are%2520extracted%2520from%2520the%2520NIST%2520WebBook.%2520The%2520AI-predicted%250Aionisation%2520energy%2520and%2520minimum%2520appearance%2520energy%2520have%2520a%2520mean%2520relative%2520error%2520of%250A6.87%2525%2520and%25207.99%2525%252C%2520respectively%252C%2520and%2520a%2520predicted%2520ion%2520mass%2520with%2520a%252023.89%2525%2520relative%250Aerror.%2520In%2520the%2520cases%2520of%2520full%2520mass%2520spectra%2520due%2520to%2520electron%2520ionisation%252C%2520the%250Apredictions%2520have%2520a%2520cosine%2520similarity%2520of%25200.6395%2520and%2520align%2520with%2520the%2520top%252010%2520most%250Asimilar%2520mass%2520spectra%2520in%252078%2525%2520of%2520instances%2520within%2520a%252030%2520Da%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-assisted%20Advanced%20Propellant%20Development%20for%20Electric%20Propulsion&entry.906535625=Angel%20Pan%20Du%20and%20Miguel%20Arana-Catania%20and%20Enric%20Grustan%20Guti%C3%A9rrez&entry.1292438233=%20%20Artificial%20Intelligence%20algorithms%20are%20introduced%20in%20this%20work%20as%20a%20tool%20to%0Apredict%20the%20performance%20of%20new%20chemical%20compounds%20as%20alternative%20propellants%0Afor%20electric%20propulsion%2C%20focusing%20on%20predicting%20their%20ionisation%0Acharacteristics%20and%20fragmentation%20patterns.%20The%20chemical%20properties%20and%0Astructure%20of%20the%20compounds%20are%20encoded%20using%20a%20chemical%20fingerprint%2C%20and%20the%0Atraining%20datasets%20are%20extracted%20from%20the%20NIST%20WebBook.%20The%20AI-predicted%0Aionisation%20energy%20and%20minimum%20appearance%20energy%20have%20a%20mean%20relative%20error%20of%0A6.87%25%20and%207.99%25%2C%20respectively%2C%20and%20a%20predicted%20ion%20mass%20with%20a%2023.89%25%20relative%0Aerror.%20In%20the%20cases%20of%20full%20mass%20spectra%20due%20to%20electron%20ionisation%2C%20the%0Apredictions%20have%20a%20cosine%20similarity%20of%200.6395%20and%20align%20with%20the%20top%2010%20most%0Asimilar%20mass%20spectra%20in%2078%25%20of%20instances%20within%20a%2030%20Da%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26567v1&entry.124074799=Read"},
{"title": "On Fitting Flow Models with Large Sinkhorn Couplings", "author": "Stephen Zhang and Alireza Mousavi-Hosseini and Michal Klein and Marco Cuturi", "abstract": "  Flow models transform data gradually from one modality (e.g. noise) onto\nanother (e.g. images). Such models are parameterized by a time-dependent\nvelocity field, trained to fit segments connecting pairs of source and target\npoints. When the pairing between source and target points is given, training\nflow models boils down to a supervised regression problem. When no such pairing\nexists, as is the case when generating data from noise, training flows is much\nharder. A popular approach lies in picking source and target points\nindependently. This can, however, lead to velocity fields that are slow to\ntrain, but also costly to integrate at inference time. In theory, one would\ngreatly benefit from training flow models by sampling pairs from an optimal\ntransport (OT) measure coupling source and target, since this would lead to a\nhighly efficient flow solving the Benamou and Brenier dynamical OT problem. In\npractice, recent works have proposed to sample mini-batches of $n$ source and\n$n$ target points and reorder them using an OT solver to form better pairs.\nThese works have advocated using batches of size $n\\approx 256$, and considered\nOT solvers that return couplings that are either sharp (using e.g. the\nHungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.\nSinkhorn). We follow in the footsteps of these works by exploring the benefits\nof increasing $n$ by three to four orders of magnitude, and look more carefully\non the effect of the entropic regularization $\\varepsilon$ used in the Sinkhorn\nalgorithm. Our analysis is facilitated by new scale invariant quantities to\nreport the sharpness of a coupling, while our sharded computations across\nmultiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic\nand image generation tasks, flow models greatly benefit when fitted with large\nSinkhorn couplings, with a low entropic regularization $\\varepsilon$.\n", "link": "http://arxiv.org/abs/2506.05526v3", "date": "2025-09-30", "relevancy": 1.5845, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6358}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5127}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Fitting%20Flow%20Models%20with%20Large%20Sinkhorn%20Couplings&body=Title%3A%20On%20Fitting%20Flow%20Models%20with%20Large%20Sinkhorn%20Couplings%0AAuthor%3A%20Stephen%20Zhang%20and%20Alireza%20Mousavi-Hosseini%20and%20Michal%20Klein%20and%20Marco%20Cuturi%0AAbstract%3A%20%20%20Flow%20models%20transform%20data%20gradually%20from%20one%20modality%20%28e.g.%20noise%29%20onto%0Aanother%20%28e.g.%20images%29.%20Such%20models%20are%20parameterized%20by%20a%20time-dependent%0Avelocity%20field%2C%20trained%20to%20fit%20segments%20connecting%20pairs%20of%20source%20and%20target%0Apoints.%20When%20the%20pairing%20between%20source%20and%20target%20points%20is%20given%2C%20training%0Aflow%20models%20boils%20down%20to%20a%20supervised%20regression%20problem.%20When%20no%20such%20pairing%0Aexists%2C%20as%20is%20the%20case%20when%20generating%20data%20from%20noise%2C%20training%20flows%20is%20much%0Aharder.%20A%20popular%20approach%20lies%20in%20picking%20source%20and%20target%20points%0Aindependently.%20This%20can%2C%20however%2C%20lead%20to%20velocity%20fields%20that%20are%20slow%20to%0Atrain%2C%20but%20also%20costly%20to%20integrate%20at%20inference%20time.%20In%20theory%2C%20one%20would%0Agreatly%20benefit%20from%20training%20flow%20models%20by%20sampling%20pairs%20from%20an%20optimal%0Atransport%20%28OT%29%20measure%20coupling%20source%20and%20target%2C%20since%20this%20would%20lead%20to%20a%0Ahighly%20efficient%20flow%20solving%20the%20Benamou%20and%20Brenier%20dynamical%20OT%20problem.%20In%0Apractice%2C%20recent%20works%20have%20proposed%20to%20sample%20mini-batches%20of%20%24n%24%20source%20and%0A%24n%24%20target%20points%20and%20reorder%20them%20using%20an%20OT%20solver%20to%20form%20better%20pairs.%0AThese%20works%20have%20advocated%20using%20batches%20of%20size%20%24n%5Capprox%20256%24%2C%20and%20considered%0AOT%20solvers%20that%20return%20couplings%20that%20are%20either%20sharp%20%28using%20e.g.%20the%0AHungarian%20algorithm%29%20or%20blurred%20%28using%20e.g.%20entropic%20regularization%2C%20a.k.a.%0ASinkhorn%29.%20We%20follow%20in%20the%20footsteps%20of%20these%20works%20by%20exploring%20the%20benefits%0Aof%20increasing%20%24n%24%20by%20three%20to%20four%20orders%20of%20magnitude%2C%20and%20look%20more%20carefully%0Aon%20the%20effect%20of%20the%20entropic%20regularization%20%24%5Cvarepsilon%24%20used%20in%20the%20Sinkhorn%0Aalgorithm.%20Our%20analysis%20is%20facilitated%20by%20new%20scale%20invariant%20quantities%20to%0Areport%20the%20sharpness%20of%20a%20coupling%2C%20while%20our%20sharded%20computations%20across%0Amultiple%20GPU%20or%20GPU%20nodes%20allow%20scaling%20up%20%24n%24.%20We%20show%20that%20in%20both%20synthetic%0Aand%20image%20generation%20tasks%2C%20flow%20models%20greatly%20benefit%20when%20fitted%20with%20large%0ASinkhorn%20couplings%2C%20with%20a%20low%20entropic%20regularization%20%24%5Cvarepsilon%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05526v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Fitting%2520Flow%2520Models%2520with%2520Large%2520Sinkhorn%2520Couplings%26entry.906535625%3DStephen%2520Zhang%2520and%2520Alireza%2520Mousavi-Hosseini%2520and%2520Michal%2520Klein%2520and%2520Marco%2520Cuturi%26entry.1292438233%3D%2520%2520Flow%2520models%2520transform%2520data%2520gradually%2520from%2520one%2520modality%2520%2528e.g.%2520noise%2529%2520onto%250Aanother%2520%2528e.g.%2520images%2529.%2520Such%2520models%2520are%2520parameterized%2520by%2520a%2520time-dependent%250Avelocity%2520field%252C%2520trained%2520to%2520fit%2520segments%2520connecting%2520pairs%2520of%2520source%2520and%2520target%250Apoints.%2520When%2520the%2520pairing%2520between%2520source%2520and%2520target%2520points%2520is%2520given%252C%2520training%250Aflow%2520models%2520boils%2520down%2520to%2520a%2520supervised%2520regression%2520problem.%2520When%2520no%2520such%2520pairing%250Aexists%252C%2520as%2520is%2520the%2520case%2520when%2520generating%2520data%2520from%2520noise%252C%2520training%2520flows%2520is%2520much%250Aharder.%2520A%2520popular%2520approach%2520lies%2520in%2520picking%2520source%2520and%2520target%2520points%250Aindependently.%2520This%2520can%252C%2520however%252C%2520lead%2520to%2520velocity%2520fields%2520that%2520are%2520slow%2520to%250Atrain%252C%2520but%2520also%2520costly%2520to%2520integrate%2520at%2520inference%2520time.%2520In%2520theory%252C%2520one%2520would%250Agreatly%2520benefit%2520from%2520training%2520flow%2520models%2520by%2520sampling%2520pairs%2520from%2520an%2520optimal%250Atransport%2520%2528OT%2529%2520measure%2520coupling%2520source%2520and%2520target%252C%2520since%2520this%2520would%2520lead%2520to%2520a%250Ahighly%2520efficient%2520flow%2520solving%2520the%2520Benamou%2520and%2520Brenier%2520dynamical%2520OT%2520problem.%2520In%250Apractice%252C%2520recent%2520works%2520have%2520proposed%2520to%2520sample%2520mini-batches%2520of%2520%2524n%2524%2520source%2520and%250A%2524n%2524%2520target%2520points%2520and%2520reorder%2520them%2520using%2520an%2520OT%2520solver%2520to%2520form%2520better%2520pairs.%250AThese%2520works%2520have%2520advocated%2520using%2520batches%2520of%2520size%2520%2524n%255Capprox%2520256%2524%252C%2520and%2520considered%250AOT%2520solvers%2520that%2520return%2520couplings%2520that%2520are%2520either%2520sharp%2520%2528using%2520e.g.%2520the%250AHungarian%2520algorithm%2529%2520or%2520blurred%2520%2528using%2520e.g.%2520entropic%2520regularization%252C%2520a.k.a.%250ASinkhorn%2529.%2520We%2520follow%2520in%2520the%2520footsteps%2520of%2520these%2520works%2520by%2520exploring%2520the%2520benefits%250Aof%2520increasing%2520%2524n%2524%2520by%2520three%2520to%2520four%2520orders%2520of%2520magnitude%252C%2520and%2520look%2520more%2520carefully%250Aon%2520the%2520effect%2520of%2520the%2520entropic%2520regularization%2520%2524%255Cvarepsilon%2524%2520used%2520in%2520the%2520Sinkhorn%250Aalgorithm.%2520Our%2520analysis%2520is%2520facilitated%2520by%2520new%2520scale%2520invariant%2520quantities%2520to%250Areport%2520the%2520sharpness%2520of%2520a%2520coupling%252C%2520while%2520our%2520sharded%2520computations%2520across%250Amultiple%2520GPU%2520or%2520GPU%2520nodes%2520allow%2520scaling%2520up%2520%2524n%2524.%2520We%2520show%2520that%2520in%2520both%2520synthetic%250Aand%2520image%2520generation%2520tasks%252C%2520flow%2520models%2520greatly%2520benefit%2520when%2520fitted%2520with%2520large%250ASinkhorn%2520couplings%252C%2520with%2520a%2520low%2520entropic%2520regularization%2520%2524%255Cvarepsilon%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05526v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Fitting%20Flow%20Models%20with%20Large%20Sinkhorn%20Couplings&entry.906535625=Stephen%20Zhang%20and%20Alireza%20Mousavi-Hosseini%20and%20Michal%20Klein%20and%20Marco%20Cuturi&entry.1292438233=%20%20Flow%20models%20transform%20data%20gradually%20from%20one%20modality%20%28e.g.%20noise%29%20onto%0Aanother%20%28e.g.%20images%29.%20Such%20models%20are%20parameterized%20by%20a%20time-dependent%0Avelocity%20field%2C%20trained%20to%20fit%20segments%20connecting%20pairs%20of%20source%20and%20target%0Apoints.%20When%20the%20pairing%20between%20source%20and%20target%20points%20is%20given%2C%20training%0Aflow%20models%20boils%20down%20to%20a%20supervised%20regression%20problem.%20When%20no%20such%20pairing%0Aexists%2C%20as%20is%20the%20case%20when%20generating%20data%20from%20noise%2C%20training%20flows%20is%20much%0Aharder.%20A%20popular%20approach%20lies%20in%20picking%20source%20and%20target%20points%0Aindependently.%20This%20can%2C%20however%2C%20lead%20to%20velocity%20fields%20that%20are%20slow%20to%0Atrain%2C%20but%20also%20costly%20to%20integrate%20at%20inference%20time.%20In%20theory%2C%20one%20would%0Agreatly%20benefit%20from%20training%20flow%20models%20by%20sampling%20pairs%20from%20an%20optimal%0Atransport%20%28OT%29%20measure%20coupling%20source%20and%20target%2C%20since%20this%20would%20lead%20to%20a%0Ahighly%20efficient%20flow%20solving%20the%20Benamou%20and%20Brenier%20dynamical%20OT%20problem.%20In%0Apractice%2C%20recent%20works%20have%20proposed%20to%20sample%20mini-batches%20of%20%24n%24%20source%20and%0A%24n%24%20target%20points%20and%20reorder%20them%20using%20an%20OT%20solver%20to%20form%20better%20pairs.%0AThese%20works%20have%20advocated%20using%20batches%20of%20size%20%24n%5Capprox%20256%24%2C%20and%20considered%0AOT%20solvers%20that%20return%20couplings%20that%20are%20either%20sharp%20%28using%20e.g.%20the%0AHungarian%20algorithm%29%20or%20blurred%20%28using%20e.g.%20entropic%20regularization%2C%20a.k.a.%0ASinkhorn%29.%20We%20follow%20in%20the%20footsteps%20of%20these%20works%20by%20exploring%20the%20benefits%0Aof%20increasing%20%24n%24%20by%20three%20to%20four%20orders%20of%20magnitude%2C%20and%20look%20more%20carefully%0Aon%20the%20effect%20of%20the%20entropic%20regularization%20%24%5Cvarepsilon%24%20used%20in%20the%20Sinkhorn%0Aalgorithm.%20Our%20analysis%20is%20facilitated%20by%20new%20scale%20invariant%20quantities%20to%0Areport%20the%20sharpness%20of%20a%20coupling%2C%20while%20our%20sharded%20computations%20across%0Amultiple%20GPU%20or%20GPU%20nodes%20allow%20scaling%20up%20%24n%24.%20We%20show%20that%20in%20both%20synthetic%0Aand%20image%20generation%20tasks%2C%20flow%20models%20greatly%20benefit%20when%20fitted%20with%20large%0ASinkhorn%20couplings%2C%20with%20a%20low%20entropic%20regularization%20%24%5Cvarepsilon%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05526v3&entry.124074799=Read"},
{"title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models", "author": "Runze Liu and Jiakang Wang and Yuling Shi and Zhihui Xie and Chenxin An and Kaiyan Zhang and Jian Zhao and Xiaodong Gu and Lei Lin and Wenping Hu and Xiu Li and Fuzheng Zhang and Guorui Zhou and Kun Gai", "abstract": "  Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.\n", "link": "http://arxiv.org/abs/2509.26628v1", "date": "2025-09-30", "relevancy": 1.5801, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20as%20a%20Compass%3A%20Efficient%20Exploration%20for%20Process-Supervised%20RL%0A%20%20in%20Reasoning%20Models&body=Title%3A%20Attention%20as%20a%20Compass%3A%20Efficient%20Exploration%20for%20Process-Supervised%20RL%0A%20%20in%20Reasoning%20Models%0AAuthor%3A%20Runze%20Liu%20and%20Jiakang%20Wang%20and%20Yuling%20Shi%20and%20Zhihui%20Xie%20and%20Chenxin%20An%20and%20Kaiyan%20Zhang%20and%20Jian%20Zhao%20and%20Xiaodong%20Gu%20and%20Lei%20Lin%20and%20Wenping%20Hu%20and%20Xiu%20Li%20and%20Fuzheng%20Zhang%20and%20Guorui%20Zhou%20and%20Kun%20Gai%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20shown%20remarkable%20success%20in%20enhancing%20the%0Areasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20Process-Supervised%20RL%0A%28PSRL%29%20has%20emerged%20as%20a%20more%20effective%20paradigm%20compared%20to%20outcome-based%20RL.%0AHowever%2C%20existing%20PSRL%20approaches%20suffer%20from%20limited%20exploration%20efficiency%2C%0Aboth%20in%20terms%20of%20branching%20positions%20and%20sampling.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20PSRL%20framework%20%28AttnRL%29%2C%20which%20enables%20efficient%20exploration%20for%0Areasoning%20models.%20Motivated%20by%20preliminary%20observations%20that%20steps%20exhibiting%0Ahigh%20attention%20scores%20correlate%20with%20reasoning%20behaviors%2C%20we%20propose%20to%20branch%0Afrom%20positions%20with%20high%20values.%20Furthermore%2C%20we%20develop%20an%20adaptive%20sampling%0Astrategy%20that%20accounts%20for%20problem%20difficulty%20and%20historical%20batch%20size%2C%0Aensuring%20that%20the%20whole%20training%20batch%20maintains%20non-zero%20advantage%20values.%20To%0Afurther%20improve%20sampling%20efficiency%2C%20we%20design%20a%20one-step%20off-policy%20training%0Apipeline%20for%20PSRL.%20Extensive%20experiments%20on%20multiple%20challenging%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20our%20method%20consistently%20outperforms%20prior%0Aapproaches%20in%20terms%20of%20performance%20and%20sampling%20and%20training%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520as%2520a%2520Compass%253A%2520Efficient%2520Exploration%2520for%2520Process-Supervised%2520RL%250A%2520%2520in%2520Reasoning%2520Models%26entry.906535625%3DRunze%2520Liu%2520and%2520Jiakang%2520Wang%2520and%2520Yuling%2520Shi%2520and%2520Zhihui%2520Xie%2520and%2520Chenxin%2520An%2520and%2520Kaiyan%2520Zhang%2520and%2520Jian%2520Zhao%2520and%2520Xiaodong%2520Gu%2520and%2520Lei%2520Lin%2520and%2520Wenping%2520Hu%2520and%2520Xiu%2520Li%2520and%2520Fuzheng%2520Zhang%2520and%2520Guorui%2520Zhou%2520and%2520Kun%2520Gai%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520shown%2520remarkable%2520success%2520in%2520enhancing%2520the%250Areasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Process-Supervised%2520RL%250A%2528PSRL%2529%2520has%2520emerged%2520as%2520a%2520more%2520effective%2520paradigm%2520compared%2520to%2520outcome-based%2520RL.%250AHowever%252C%2520existing%2520PSRL%2520approaches%2520suffer%2520from%2520limited%2520exploration%2520efficiency%252C%250Aboth%2520in%2520terms%2520of%2520branching%2520positions%2520and%2520sampling.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Aa%2520novel%2520PSRL%2520framework%2520%2528AttnRL%2529%252C%2520which%2520enables%2520efficient%2520exploration%2520for%250Areasoning%2520models.%2520Motivated%2520by%2520preliminary%2520observations%2520that%2520steps%2520exhibiting%250Ahigh%2520attention%2520scores%2520correlate%2520with%2520reasoning%2520behaviors%252C%2520we%2520propose%2520to%2520branch%250Afrom%2520positions%2520with%2520high%2520values.%2520Furthermore%252C%2520we%2520develop%2520an%2520adaptive%2520sampling%250Astrategy%2520that%2520accounts%2520for%2520problem%2520difficulty%2520and%2520historical%2520batch%2520size%252C%250Aensuring%2520that%2520the%2520whole%2520training%2520batch%2520maintains%2520non-zero%2520advantage%2520values.%2520To%250Afurther%2520improve%2520sampling%2520efficiency%252C%2520we%2520design%2520a%2520one-step%2520off-policy%2520training%250Apipeline%2520for%2520PSRL.%2520Extensive%2520experiments%2520on%2520multiple%2520challenging%2520mathematical%250Areasoning%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520prior%250Aapproaches%2520in%2520terms%2520of%2520performance%2520and%2520sampling%2520and%2520training%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20as%20a%20Compass%3A%20Efficient%20Exploration%20for%20Process-Supervised%20RL%0A%20%20in%20Reasoning%20Models&entry.906535625=Runze%20Liu%20and%20Jiakang%20Wang%20and%20Yuling%20Shi%20and%20Zhihui%20Xie%20and%20Chenxin%20An%20and%20Kaiyan%20Zhang%20and%20Jian%20Zhao%20and%20Xiaodong%20Gu%20and%20Lei%20Lin%20and%20Wenping%20Hu%20and%20Xiu%20Li%20and%20Fuzheng%20Zhang%20and%20Guorui%20Zhou%20and%20Kun%20Gai&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20shown%20remarkable%20success%20in%20enhancing%20the%0Areasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%20Process-Supervised%20RL%0A%28PSRL%29%20has%20emerged%20as%20a%20more%20effective%20paradigm%20compared%20to%20outcome-based%20RL.%0AHowever%2C%20existing%20PSRL%20approaches%20suffer%20from%20limited%20exploration%20efficiency%2C%0Aboth%20in%20terms%20of%20branching%20positions%20and%20sampling.%20In%20this%20paper%2C%20we%20introduce%0Aa%20novel%20PSRL%20framework%20%28AttnRL%29%2C%20which%20enables%20efficient%20exploration%20for%0Areasoning%20models.%20Motivated%20by%20preliminary%20observations%20that%20steps%20exhibiting%0Ahigh%20attention%20scores%20correlate%20with%20reasoning%20behaviors%2C%20we%20propose%20to%20branch%0Afrom%20positions%20with%20high%20values.%20Furthermore%2C%20we%20develop%20an%20adaptive%20sampling%0Astrategy%20that%20accounts%20for%20problem%20difficulty%20and%20historical%20batch%20size%2C%0Aensuring%20that%20the%20whole%20training%20batch%20maintains%20non-zero%20advantage%20values.%20To%0Afurther%20improve%20sampling%20efficiency%2C%20we%20design%20a%20one-step%20off-policy%20training%0Apipeline%20for%20PSRL.%20Extensive%20experiments%20on%20multiple%20challenging%20mathematical%0Areasoning%20benchmarks%20demonstrate%20that%20our%20method%20consistently%20outperforms%20prior%0Aapproaches%20in%20terms%20of%20performance%20and%20sampling%20and%20training%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26628v1&entry.124074799=Read"},
{"title": "The Trajectory Bundle Method: Unifying Sequential-Convex Programming and\n  Sampling-Based Trajectory Optimization", "author": "Kevin Tracy and John Z. Zhang and Jon Arrizabalaga and Stefan Schaal and Yuval Tassa and Tom Erez and Zachary Manchester", "abstract": "  We present a unified framework for solving trajectory optimization problems\nin a derivative-free manner through the use of sequential convex programming.\nTraditionally, nonconvex optimization problems are solved by forming and\nsolving a sequence of convex optimization problems, where the cost and\nconstraint functions are approximated locally through Taylor series expansions.\nThis presents a challenge for functions where differentiation is expensive or\nunavailable. In this work, we present a derivative-free approach to form these\nconvex approximations by computing samples of the dynamics, cost, and\nconstraint functions and letting the solver interpolate between them. Our\nframework includes sample-based trajectory optimization techniques like\nmodel-predictive path integral (MPPI) control as a special case and generalizes\nthem to enable features like multiple shooting and general equality and\ninequality constraints that are traditionally associated with derivative-based\nsequential convex programming methods. The resulting framework is simple,\nflexible, and capable of solving a wide variety of practical motion planning\nand control problems.\n", "link": "http://arxiv.org/abs/2509.26575v1", "date": "2025-09-30", "relevancy": 1.5471, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5309}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Trajectory%20Bundle%20Method%3A%20Unifying%20Sequential-Convex%20Programming%20and%0A%20%20Sampling-Based%20Trajectory%20Optimization&body=Title%3A%20The%20Trajectory%20Bundle%20Method%3A%20Unifying%20Sequential-Convex%20Programming%20and%0A%20%20Sampling-Based%20Trajectory%20Optimization%0AAuthor%3A%20Kevin%20Tracy%20and%20John%20Z.%20Zhang%20and%20Jon%20Arrizabalaga%20and%20Stefan%20Schaal%20and%20Yuval%20Tassa%20and%20Tom%20Erez%20and%20Zachary%20Manchester%0AAbstract%3A%20%20%20We%20present%20a%20unified%20framework%20for%20solving%20trajectory%20optimization%20problems%0Ain%20a%20derivative-free%20manner%20through%20the%20use%20of%20sequential%20convex%20programming.%0ATraditionally%2C%20nonconvex%20optimization%20problems%20are%20solved%20by%20forming%20and%0Asolving%20a%20sequence%20of%20convex%20optimization%20problems%2C%20where%20the%20cost%20and%0Aconstraint%20functions%20are%20approximated%20locally%20through%20Taylor%20series%20expansions.%0AThis%20presents%20a%20challenge%20for%20functions%20where%20differentiation%20is%20expensive%20or%0Aunavailable.%20In%20this%20work%2C%20we%20present%20a%20derivative-free%20approach%20to%20form%20these%0Aconvex%20approximations%20by%20computing%20samples%20of%20the%20dynamics%2C%20cost%2C%20and%0Aconstraint%20functions%20and%20letting%20the%20solver%20interpolate%20between%20them.%20Our%0Aframework%20includes%20sample-based%20trajectory%20optimization%20techniques%20like%0Amodel-predictive%20path%20integral%20%28MPPI%29%20control%20as%20a%20special%20case%20and%20generalizes%0Athem%20to%20enable%20features%20like%20multiple%20shooting%20and%20general%20equality%20and%0Ainequality%20constraints%20that%20are%20traditionally%20associated%20with%20derivative-based%0Asequential%20convex%20programming%20methods.%20The%20resulting%20framework%20is%20simple%2C%0Aflexible%2C%20and%20capable%20of%20solving%20a%20wide%20variety%20of%20practical%20motion%20planning%0Aand%20control%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Trajectory%2520Bundle%2520Method%253A%2520Unifying%2520Sequential-Convex%2520Programming%2520and%250A%2520%2520Sampling-Based%2520Trajectory%2520Optimization%26entry.906535625%3DKevin%2520Tracy%2520and%2520John%2520Z.%2520Zhang%2520and%2520Jon%2520Arrizabalaga%2520and%2520Stefan%2520Schaal%2520and%2520Yuval%2520Tassa%2520and%2520Tom%2520Erez%2520and%2520Zachary%2520Manchester%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520unified%2520framework%2520for%2520solving%2520trajectory%2520optimization%2520problems%250Ain%2520a%2520derivative-free%2520manner%2520through%2520the%2520use%2520of%2520sequential%2520convex%2520programming.%250ATraditionally%252C%2520nonconvex%2520optimization%2520problems%2520are%2520solved%2520by%2520forming%2520and%250Asolving%2520a%2520sequence%2520of%2520convex%2520optimization%2520problems%252C%2520where%2520the%2520cost%2520and%250Aconstraint%2520functions%2520are%2520approximated%2520locally%2520through%2520Taylor%2520series%2520expansions.%250AThis%2520presents%2520a%2520challenge%2520for%2520functions%2520where%2520differentiation%2520is%2520expensive%2520or%250Aunavailable.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520derivative-free%2520approach%2520to%2520form%2520these%250Aconvex%2520approximations%2520by%2520computing%2520samples%2520of%2520the%2520dynamics%252C%2520cost%252C%2520and%250Aconstraint%2520functions%2520and%2520letting%2520the%2520solver%2520interpolate%2520between%2520them.%2520Our%250Aframework%2520includes%2520sample-based%2520trajectory%2520optimization%2520techniques%2520like%250Amodel-predictive%2520path%2520integral%2520%2528MPPI%2529%2520control%2520as%2520a%2520special%2520case%2520and%2520generalizes%250Athem%2520to%2520enable%2520features%2520like%2520multiple%2520shooting%2520and%2520general%2520equality%2520and%250Ainequality%2520constraints%2520that%2520are%2520traditionally%2520associated%2520with%2520derivative-based%250Asequential%2520convex%2520programming%2520methods.%2520The%2520resulting%2520framework%2520is%2520simple%252C%250Aflexible%252C%2520and%2520capable%2520of%2520solving%2520a%2520wide%2520variety%2520of%2520practical%2520motion%2520planning%250Aand%2520control%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Trajectory%20Bundle%20Method%3A%20Unifying%20Sequential-Convex%20Programming%20and%0A%20%20Sampling-Based%20Trajectory%20Optimization&entry.906535625=Kevin%20Tracy%20and%20John%20Z.%20Zhang%20and%20Jon%20Arrizabalaga%20and%20Stefan%20Schaal%20and%20Yuval%20Tassa%20and%20Tom%20Erez%20and%20Zachary%20Manchester&entry.1292438233=%20%20We%20present%20a%20unified%20framework%20for%20solving%20trajectory%20optimization%20problems%0Ain%20a%20derivative-free%20manner%20through%20the%20use%20of%20sequential%20convex%20programming.%0ATraditionally%2C%20nonconvex%20optimization%20problems%20are%20solved%20by%20forming%20and%0Asolving%20a%20sequence%20of%20convex%20optimization%20problems%2C%20where%20the%20cost%20and%0Aconstraint%20functions%20are%20approximated%20locally%20through%20Taylor%20series%20expansions.%0AThis%20presents%20a%20challenge%20for%20functions%20where%20differentiation%20is%20expensive%20or%0Aunavailable.%20In%20this%20work%2C%20we%20present%20a%20derivative-free%20approach%20to%20form%20these%0Aconvex%20approximations%20by%20computing%20samples%20of%20the%20dynamics%2C%20cost%2C%20and%0Aconstraint%20functions%20and%20letting%20the%20solver%20interpolate%20between%20them.%20Our%0Aframework%20includes%20sample-based%20trajectory%20optimization%20techniques%20like%0Amodel-predictive%20path%20integral%20%28MPPI%29%20control%20as%20a%20special%20case%20and%20generalizes%0Athem%20to%20enable%20features%20like%20multiple%20shooting%20and%20general%20equality%20and%0Ainequality%20constraints%20that%20are%20traditionally%20associated%20with%20derivative-based%0Asequential%20convex%20programming%20methods.%20The%20resulting%20framework%20is%20simple%2C%0Aflexible%2C%20and%20capable%20of%20solving%20a%20wide%20variety%20of%20practical%20motion%20planning%0Aand%20control%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26575v1&entry.124074799=Read"},
{"title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications", "author": "Wei He and Yueqing Sun and Hongyan Hao and Xueyuan Hao and Zhikang Xia and Qi Gu and Chengcheng Han and Dengchang Zhao and Hui Su and Kefeng Zhang and Man Gao and Xi Su and Xiaodong Cai and Xunliang Cai and Yu Yang and Yunke Zhao", "abstract": "  As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/\n", "link": "http://arxiv.org/abs/2509.26490v1", "date": "2025-09-30", "relevancy": 1.5243, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5271}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5262}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VitaBench%3A%20Benchmarking%20LLM%20Agents%20with%20Versatile%20Interactive%20Tasks%20in%0A%20%20Real-world%20Applications&body=Title%3A%20VitaBench%3A%20Benchmarking%20LLM%20Agents%20with%20Versatile%20Interactive%20Tasks%20in%0A%20%20Real-world%20Applications%0AAuthor%3A%20Wei%20He%20and%20Yueqing%20Sun%20and%20Hongyan%20Hao%20and%20Xueyuan%20Hao%20and%20Zhikang%20Xia%20and%20Qi%20Gu%20and%20Chengcheng%20Han%20and%20Dengchang%20Zhao%20and%20Hui%20Su%20and%20Kefeng%20Zhang%20and%20Man%20Gao%20and%20Xi%20Su%20and%20Xiaodong%20Cai%20and%20Xunliang%20Cai%20and%20Yu%20Yang%20and%20Yunke%20Zhao%0AAbstract%3A%20%20%20As%20LLM-based%20agents%20are%20increasingly%20deployed%20in%20real-life%20scenarios%2C%0Aexisting%20benchmarks%20fail%20to%20capture%20their%20inherent%20complexity%20of%20handling%0Aextensive%20information%2C%20leveraging%20diverse%20resources%2C%20and%20managing%20dynamic%20user%0Ainteractions.%20To%20address%20this%20gap%2C%20we%20introduce%20VitaBench%2C%20a%20challenging%0Abenchmark%20that%20evaluates%20agents%20on%20versatile%20interactive%20tasks%20grounded%20in%0Areal-world%20settings.%20Drawing%20from%20daily%20applications%20in%20food%20delivery%2C%20in-store%0Aconsumption%2C%20and%20online%20travel%20services%2C%20VitaBench%20presents%20agents%20with%20the%0Amost%20complex%20life-serving%20simulation%20environment%20to%20date%2C%20comprising%2066%20tools.%0AThrough%20a%20framework%20that%20eliminates%20domain-specific%20policies%2C%20we%20enable%0Aflexible%20composition%20of%20these%20scenarios%20and%20tools%2C%20yielding%20100%20cross-scenario%0Atasks%20%28main%20results%29%20and%20300%20single-scenario%20tasks.%20Each%20task%20is%20derived%20from%0Amultiple%20real%20user%20requests%20and%20requires%20agents%20to%20reason%20across%20temporal%20and%0Aspatial%20dimensions%2C%20utilize%20complex%20tool%20sets%2C%20proactively%20clarify%20ambiguous%0Ainstructions%2C%20and%20track%20shifting%20user%20intent%20throughout%20multi-turn%0Aconversations.%20Moreover%2C%20we%20propose%20a%20rubric-based%20sliding%20window%20evaluator%2C%0Aenabling%20robust%20assessment%20of%20diverse%20solution%20pathways%20in%20complex%20environments%0Aand%20stochastic%20interactions.%20Our%20comprehensive%20evaluation%20reveals%20that%20even%20the%0Amost%20advanced%20models%20achieve%20only%2030%25%20success%20rate%20on%20cross-scenario%20tasks%2C%20and%0Aless%20than%2050%25%20success%20rate%20on%20others.%20Overall%2C%20we%20believe%20VitaBench%20will%20serve%0Aas%20a%20valuable%20resource%20for%20advancing%20the%20development%20of%20AI%20agents%20in%20practical%0Areal-world%20applications.%20The%20code%2C%20dataset%2C%20and%20leaderboard%20are%20available%20at%0Ahttps%3A//vitabench.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVitaBench%253A%2520Benchmarking%2520LLM%2520Agents%2520with%2520Versatile%2520Interactive%2520Tasks%2520in%250A%2520%2520Real-world%2520Applications%26entry.906535625%3DWei%2520He%2520and%2520Yueqing%2520Sun%2520and%2520Hongyan%2520Hao%2520and%2520Xueyuan%2520Hao%2520and%2520Zhikang%2520Xia%2520and%2520Qi%2520Gu%2520and%2520Chengcheng%2520Han%2520and%2520Dengchang%2520Zhao%2520and%2520Hui%2520Su%2520and%2520Kefeng%2520Zhang%2520and%2520Man%2520Gao%2520and%2520Xi%2520Su%2520and%2520Xiaodong%2520Cai%2520and%2520Xunliang%2520Cai%2520and%2520Yu%2520Yang%2520and%2520Yunke%2520Zhao%26entry.1292438233%3D%2520%2520As%2520LLM-based%2520agents%2520are%2520increasingly%2520deployed%2520in%2520real-life%2520scenarios%252C%250Aexisting%2520benchmarks%2520fail%2520to%2520capture%2520their%2520inherent%2520complexity%2520of%2520handling%250Aextensive%2520information%252C%2520leveraging%2520diverse%2520resources%252C%2520and%2520managing%2520dynamic%2520user%250Ainteractions.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520VitaBench%252C%2520a%2520challenging%250Abenchmark%2520that%2520evaluates%2520agents%2520on%2520versatile%2520interactive%2520tasks%2520grounded%2520in%250Areal-world%2520settings.%2520Drawing%2520from%2520daily%2520applications%2520in%2520food%2520delivery%252C%2520in-store%250Aconsumption%252C%2520and%2520online%2520travel%2520services%252C%2520VitaBench%2520presents%2520agents%2520with%2520the%250Amost%2520complex%2520life-serving%2520simulation%2520environment%2520to%2520date%252C%2520comprising%252066%2520tools.%250AThrough%2520a%2520framework%2520that%2520eliminates%2520domain-specific%2520policies%252C%2520we%2520enable%250Aflexible%2520composition%2520of%2520these%2520scenarios%2520and%2520tools%252C%2520yielding%2520100%2520cross-scenario%250Atasks%2520%2528main%2520results%2529%2520and%2520300%2520single-scenario%2520tasks.%2520Each%2520task%2520is%2520derived%2520from%250Amultiple%2520real%2520user%2520requests%2520and%2520requires%2520agents%2520to%2520reason%2520across%2520temporal%2520and%250Aspatial%2520dimensions%252C%2520utilize%2520complex%2520tool%2520sets%252C%2520proactively%2520clarify%2520ambiguous%250Ainstructions%252C%2520and%2520track%2520shifting%2520user%2520intent%2520throughout%2520multi-turn%250Aconversations.%2520Moreover%252C%2520we%2520propose%2520a%2520rubric-based%2520sliding%2520window%2520evaluator%252C%250Aenabling%2520robust%2520assessment%2520of%2520diverse%2520solution%2520pathways%2520in%2520complex%2520environments%250Aand%2520stochastic%2520interactions.%2520Our%2520comprehensive%2520evaluation%2520reveals%2520that%2520even%2520the%250Amost%2520advanced%2520models%2520achieve%2520only%252030%2525%2520success%2520rate%2520on%2520cross-scenario%2520tasks%252C%2520and%250Aless%2520than%252050%2525%2520success%2520rate%2520on%2520others.%2520Overall%252C%2520we%2520believe%2520VitaBench%2520will%2520serve%250Aas%2520a%2520valuable%2520resource%2520for%2520advancing%2520the%2520development%2520of%2520AI%2520agents%2520in%2520practical%250Areal-world%2520applications.%2520The%2520code%252C%2520dataset%252C%2520and%2520leaderboard%2520are%2520available%2520at%250Ahttps%253A//vitabench.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VitaBench%3A%20Benchmarking%20LLM%20Agents%20with%20Versatile%20Interactive%20Tasks%20in%0A%20%20Real-world%20Applications&entry.906535625=Wei%20He%20and%20Yueqing%20Sun%20and%20Hongyan%20Hao%20and%20Xueyuan%20Hao%20and%20Zhikang%20Xia%20and%20Qi%20Gu%20and%20Chengcheng%20Han%20and%20Dengchang%20Zhao%20and%20Hui%20Su%20and%20Kefeng%20Zhang%20and%20Man%20Gao%20and%20Xi%20Su%20and%20Xiaodong%20Cai%20and%20Xunliang%20Cai%20and%20Yu%20Yang%20and%20Yunke%20Zhao&entry.1292438233=%20%20As%20LLM-based%20agents%20are%20increasingly%20deployed%20in%20real-life%20scenarios%2C%0Aexisting%20benchmarks%20fail%20to%20capture%20their%20inherent%20complexity%20of%20handling%0Aextensive%20information%2C%20leveraging%20diverse%20resources%2C%20and%20managing%20dynamic%20user%0Ainteractions.%20To%20address%20this%20gap%2C%20we%20introduce%20VitaBench%2C%20a%20challenging%0Abenchmark%20that%20evaluates%20agents%20on%20versatile%20interactive%20tasks%20grounded%20in%0Areal-world%20settings.%20Drawing%20from%20daily%20applications%20in%20food%20delivery%2C%20in-store%0Aconsumption%2C%20and%20online%20travel%20services%2C%20VitaBench%20presents%20agents%20with%20the%0Amost%20complex%20life-serving%20simulation%20environment%20to%20date%2C%20comprising%2066%20tools.%0AThrough%20a%20framework%20that%20eliminates%20domain-specific%20policies%2C%20we%20enable%0Aflexible%20composition%20of%20these%20scenarios%20and%20tools%2C%20yielding%20100%20cross-scenario%0Atasks%20%28main%20results%29%20and%20300%20single-scenario%20tasks.%20Each%20task%20is%20derived%20from%0Amultiple%20real%20user%20requests%20and%20requires%20agents%20to%20reason%20across%20temporal%20and%0Aspatial%20dimensions%2C%20utilize%20complex%20tool%20sets%2C%20proactively%20clarify%20ambiguous%0Ainstructions%2C%20and%20track%20shifting%20user%20intent%20throughout%20multi-turn%0Aconversations.%20Moreover%2C%20we%20propose%20a%20rubric-based%20sliding%20window%20evaluator%2C%0Aenabling%20robust%20assessment%20of%20diverse%20solution%20pathways%20in%20complex%20environments%0Aand%20stochastic%20interactions.%20Our%20comprehensive%20evaluation%20reveals%20that%20even%20the%0Amost%20advanced%20models%20achieve%20only%2030%25%20success%20rate%20on%20cross-scenario%20tasks%2C%20and%0Aless%20than%2050%25%20success%20rate%20on%20others.%20Overall%2C%20we%20believe%20VitaBench%20will%20serve%0Aas%20a%20valuable%20resource%20for%20advancing%20the%20development%20of%20AI%20agents%20in%20practical%0Areal-world%20applications.%20The%20code%2C%20dataset%2C%20and%20leaderboard%20are%20available%20at%0Ahttps%3A//vitabench.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26490v1&entry.124074799=Read"},
{"title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use", "author": "Dongfu Jiang and Yi Lu and Zhuofeng Li and Zhiheng Lyu and Ping Nie and Haozhe Wang and Alex Su and Hui Chen and Kai Zou and Chao Du and Tianyu Pang and Wenhu Chen", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.\n", "link": "http://arxiv.org/abs/2509.01055v2", "date": "2025-09-30", "relevancy": 1.5116, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5189}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VerlTool%3A%20Towards%20Holistic%20Agentic%20Reinforcement%20Learning%20with%20Tool%20Use&body=Title%3A%20VerlTool%3A%20Towards%20Holistic%20Agentic%20Reinforcement%20Learning%20with%20Tool%20Use%0AAuthor%3A%20Dongfu%20Jiang%20and%20Yi%20Lu%20and%20Zhuofeng%20Li%20and%20Zhiheng%20Lyu%20and%20Ping%20Nie%20and%20Haozhe%20Wang%20and%20Alex%20Su%20and%20Hui%20Chen%20and%20Kai%20Zou%20and%20Chao%20Du%20and%20Tianyu%20Pang%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20demonstrated%0Asuccess%20in%20enhancing%20LLM%20reasoning%20capabilities%2C%20but%20remains%20limited%20to%0Asingle-turn%20interactions%20without%20tool%20integration.%20While%20recent%20Agentic%0AReinforcement%20Learning%20with%20Tool%20use%20%28ARLT%29%20approaches%20have%20emerged%20to%20address%0Amulti-turn%20tool%20interactions%2C%20existing%20works%20develop%20task-specific%20codebases%0Athat%20suffer%20from%20fragmentation%2C%20synchronous%20execution%20bottlenecks%2C%20and%20limited%0Aextensibility%20across%20domains.%20These%20inefficiencies%20hinder%20broader%20community%0Aadoption%20and%20algorithmic%20innovation.%20We%20introduce%20VerlTool%2C%20a%20unified%20and%0Amodular%20framework%20that%20addresses%20these%20limitations%20through%20systematic%20design%0Aprinciples.%20VerlTool%20provides%20four%20key%20contributions%3A%20%281%29%20upstream%20alignment%0Awith%20VeRL%20ensuring%20compatibility%20and%20simplified%20maintenance%2C%20%282%29%20unified%20tool%0Amanagement%20via%20standardized%20APIs%20supporting%20diverse%20modalities%20including%20code%0Aexecution%2C%20search%2C%20SQL%20databases%2C%20and%20vision%20processing%2C%20%283%29%20asynchronous%0Arollout%20execution%20achieving%20near%202%24%5Ctimes%24%20speedup%20by%20eliminating%0Asynchronization%20bottlenecks%2C%20and%20%284%29%20comprehensive%20evaluation%20demonstrating%0Acompetitive%20performance%20across%206%20ARLT%20domains.%20Our%20framework%20formalizes%20ARLT%20as%0Amulti-turn%20trajectories%20with%20multi-modal%20observation%20tokens%20%28text/image/video%29%2C%0Aextending%20beyond%20single-turn%20RLVR%20paradigms.%20We%20train%20and%20evaluate%20models%20on%0Amathematical%20reasoning%2C%20knowledge%20QA%2C%20SQL%20generation%2C%20visual%20reasoning%2C%20web%0Asearch%2C%20and%20software%20engineering%20tasks%2C%20achieving%20results%20comparable%20to%0Aspecialized%20systems%20while%20providing%20unified%20training%20infrastructure.%20The%0Amodular%20plugin%20architecture%20enables%20rapid%20tool%20integration%20requiring%20only%0Alightweight%20Python%20definitions%2C%20significantly%20reducing%20development%20overhead%20and%0Aproviding%20a%20scalable%20foundation%20for%20tool-augmented%20RL%20research.%20Our%20code%20is%0Aopen-sourced%20at%20https%3A//github.com/TIGER-AI-Lab/verl-tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.01055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerlTool%253A%2520Towards%2520Holistic%2520Agentic%2520Reinforcement%2520Learning%2520with%2520Tool%2520Use%26entry.906535625%3DDongfu%2520Jiang%2520and%2520Yi%2520Lu%2520and%2520Zhuofeng%2520Li%2520and%2520Zhiheng%2520Lyu%2520and%2520Ping%2520Nie%2520and%2520Haozhe%2520Wang%2520and%2520Alex%2520Su%2520and%2520Hui%2520Chen%2520and%2520Kai%2520Zou%2520and%2520Chao%2520Du%2520and%2520Tianyu%2520Pang%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520demonstrated%250Asuccess%2520in%2520enhancing%2520LLM%2520reasoning%2520capabilities%252C%2520but%2520remains%2520limited%2520to%250Asingle-turn%2520interactions%2520without%2520tool%2520integration.%2520While%2520recent%2520Agentic%250AReinforcement%2520Learning%2520with%2520Tool%2520use%2520%2528ARLT%2529%2520approaches%2520have%2520emerged%2520to%2520address%250Amulti-turn%2520tool%2520interactions%252C%2520existing%2520works%2520develop%2520task-specific%2520codebases%250Athat%2520suffer%2520from%2520fragmentation%252C%2520synchronous%2520execution%2520bottlenecks%252C%2520and%2520limited%250Aextensibility%2520across%2520domains.%2520These%2520inefficiencies%2520hinder%2520broader%2520community%250Aadoption%2520and%2520algorithmic%2520innovation.%2520We%2520introduce%2520VerlTool%252C%2520a%2520unified%2520and%250Amodular%2520framework%2520that%2520addresses%2520these%2520limitations%2520through%2520systematic%2520design%250Aprinciples.%2520VerlTool%2520provides%2520four%2520key%2520contributions%253A%2520%25281%2529%2520upstream%2520alignment%250Awith%2520VeRL%2520ensuring%2520compatibility%2520and%2520simplified%2520maintenance%252C%2520%25282%2529%2520unified%2520tool%250Amanagement%2520via%2520standardized%2520APIs%2520supporting%2520diverse%2520modalities%2520including%2520code%250Aexecution%252C%2520search%252C%2520SQL%2520databases%252C%2520and%2520vision%2520processing%252C%2520%25283%2529%2520asynchronous%250Arollout%2520execution%2520achieving%2520near%25202%2524%255Ctimes%2524%2520speedup%2520by%2520eliminating%250Asynchronization%2520bottlenecks%252C%2520and%2520%25284%2529%2520comprehensive%2520evaluation%2520demonstrating%250Acompetitive%2520performance%2520across%25206%2520ARLT%2520domains.%2520Our%2520framework%2520formalizes%2520ARLT%2520as%250Amulti-turn%2520trajectories%2520with%2520multi-modal%2520observation%2520tokens%2520%2528text/image/video%2529%252C%250Aextending%2520beyond%2520single-turn%2520RLVR%2520paradigms.%2520We%2520train%2520and%2520evaluate%2520models%2520on%250Amathematical%2520reasoning%252C%2520knowledge%2520QA%252C%2520SQL%2520generation%252C%2520visual%2520reasoning%252C%2520web%250Asearch%252C%2520and%2520software%2520engineering%2520tasks%252C%2520achieving%2520results%2520comparable%2520to%250Aspecialized%2520systems%2520while%2520providing%2520unified%2520training%2520infrastructure.%2520The%250Amodular%2520plugin%2520architecture%2520enables%2520rapid%2520tool%2520integration%2520requiring%2520only%250Alightweight%2520Python%2520definitions%252C%2520significantly%2520reducing%2520development%2520overhead%2520and%250Aproviding%2520a%2520scalable%2520foundation%2520for%2520tool-augmented%2520RL%2520research.%2520Our%2520code%2520is%250Aopen-sourced%2520at%2520https%253A//github.com/TIGER-AI-Lab/verl-tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VerlTool%3A%20Towards%20Holistic%20Agentic%20Reinforcement%20Learning%20with%20Tool%20Use&entry.906535625=Dongfu%20Jiang%20and%20Yi%20Lu%20and%20Zhuofeng%20Li%20and%20Zhiheng%20Lyu%20and%20Ping%20Nie%20and%20Haozhe%20Wang%20and%20Alex%20Su%20and%20Hui%20Chen%20and%20Kai%20Zou%20and%20Chao%20Du%20and%20Tianyu%20Pang%20and%20Wenhu%20Chen&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20demonstrated%0Asuccess%20in%20enhancing%20LLM%20reasoning%20capabilities%2C%20but%20remains%20limited%20to%0Asingle-turn%20interactions%20without%20tool%20integration.%20While%20recent%20Agentic%0AReinforcement%20Learning%20with%20Tool%20use%20%28ARLT%29%20approaches%20have%20emerged%20to%20address%0Amulti-turn%20tool%20interactions%2C%20existing%20works%20develop%20task-specific%20codebases%0Athat%20suffer%20from%20fragmentation%2C%20synchronous%20execution%20bottlenecks%2C%20and%20limited%0Aextensibility%20across%20domains.%20These%20inefficiencies%20hinder%20broader%20community%0Aadoption%20and%20algorithmic%20innovation.%20We%20introduce%20VerlTool%2C%20a%20unified%20and%0Amodular%20framework%20that%20addresses%20these%20limitations%20through%20systematic%20design%0Aprinciples.%20VerlTool%20provides%20four%20key%20contributions%3A%20%281%29%20upstream%20alignment%0Awith%20VeRL%20ensuring%20compatibility%20and%20simplified%20maintenance%2C%20%282%29%20unified%20tool%0Amanagement%20via%20standardized%20APIs%20supporting%20diverse%20modalities%20including%20code%0Aexecution%2C%20search%2C%20SQL%20databases%2C%20and%20vision%20processing%2C%20%283%29%20asynchronous%0Arollout%20execution%20achieving%20near%202%24%5Ctimes%24%20speedup%20by%20eliminating%0Asynchronization%20bottlenecks%2C%20and%20%284%29%20comprehensive%20evaluation%20demonstrating%0Acompetitive%20performance%20across%206%20ARLT%20domains.%20Our%20framework%20formalizes%20ARLT%20as%0Amulti-turn%20trajectories%20with%20multi-modal%20observation%20tokens%20%28text/image/video%29%2C%0Aextending%20beyond%20single-turn%20RLVR%20paradigms.%20We%20train%20and%20evaluate%20models%20on%0Amathematical%20reasoning%2C%20knowledge%20QA%2C%20SQL%20generation%2C%20visual%20reasoning%2C%20web%0Asearch%2C%20and%20software%20engineering%20tasks%2C%20achieving%20results%20comparable%20to%0Aspecialized%20systems%20while%20providing%20unified%20training%20infrastructure.%20The%0Amodular%20plugin%20architecture%20enables%20rapid%20tool%20integration%20requiring%20only%0Alightweight%20Python%20definitions%2C%20significantly%20reducing%20development%20overhead%20and%0Aproviding%20a%20scalable%20foundation%20for%20tool-augmented%20RL%20research.%20Our%20code%20is%0Aopen-sourced%20at%20https%3A//github.com/TIGER-AI-Lab/verl-tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.01055v2&entry.124074799=Read"},
{"title": "Revealing the Power of Post-Training for Small Language Models via\n  Knowledge Distillation", "author": "Miao Rang and Zhenni Bi and Hang Zhou and Hanting Chen and An Xiao and Tianyu Guo and Kai Han and Xinghao Chen and Yunhe Wang", "abstract": "  The rapid advancement of large language models (LLMs) has significantly\nadvanced the capabilities of artificial intelligence across various domains.\nHowever, their massive scale and high computational costs render them\nunsuitable for direct deployment in resource-constrained edge environments.\nThis creates a critical need for high-performance small models that can operate\nefficiently at the edge. Yet, after pre-training alone, these smaller models\noften fail to meet the performance requirements of complex tasks. To bridge\nthis gap, we introduce a systematic post-training pipeline that efficiently\nenhances small model accuracy. Our post training pipeline consists of\ncurriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge\ndistillation. The resulting instruction-tuned model achieves state-of-the-art\nperformance among billion-parameter models, demonstrating strong generalization\nunder strict hardware constraints while maintaining competitive accuracy across\na variety of tasks. This work provides a practical and efficient solution for\ndeveloping high-performance language models on Ascend edge devices.\n", "link": "http://arxiv.org/abs/2509.26497v1", "date": "2025-09-30", "relevancy": 1.5096, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5203}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20the%20Power%20of%20Post-Training%20for%20Small%20Language%20Models%20via%0A%20%20Knowledge%20Distillation&body=Title%3A%20Revealing%20the%20Power%20of%20Post-Training%20for%20Small%20Language%20Models%20via%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Miao%20Rang%20and%20Zhenni%20Bi%20and%20Hang%20Zhou%20and%20Hanting%20Chen%20and%20An%20Xiao%20and%20Tianyu%20Guo%20and%20Kai%20Han%20and%20Xinghao%20Chen%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aadvanced%20the%20capabilities%20of%20artificial%20intelligence%20across%20various%20domains.%0AHowever%2C%20their%20massive%20scale%20and%20high%20computational%20costs%20render%20them%0Aunsuitable%20for%20direct%20deployment%20in%20resource-constrained%20edge%20environments.%0AThis%20creates%20a%20critical%20need%20for%20high-performance%20small%20models%20that%20can%20operate%0Aefficiently%20at%20the%20edge.%20Yet%2C%20after%20pre-training%20alone%2C%20these%20smaller%20models%0Aoften%20fail%20to%20meet%20the%20performance%20requirements%20of%20complex%20tasks.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20systematic%20post-training%20pipeline%20that%20efficiently%0Aenhances%20small%20model%20accuracy.%20Our%20post%20training%20pipeline%20consists%20of%0Acurriculum-based%20supervised%20fine-tuning%20%28SFT%29%20and%20offline%20on-policy%20knowledge%0Adistillation.%20The%20resulting%20instruction-tuned%20model%20achieves%20state-of-the-art%0Aperformance%20among%20billion-parameter%20models%2C%20demonstrating%20strong%20generalization%0Aunder%20strict%20hardware%20constraints%20while%20maintaining%20competitive%20accuracy%20across%0Aa%20variety%20of%20tasks.%20This%20work%20provides%20a%20practical%20and%20efficient%20solution%20for%0Adeveloping%20high-performance%20language%20models%20on%20Ascend%20edge%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520the%2520Power%2520of%2520Post-Training%2520for%2520Small%2520Language%2520Models%2520via%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DMiao%2520Rang%2520and%2520Zhenni%2520Bi%2520and%2520Hang%2520Zhou%2520and%2520Hanting%2520Chen%2520and%2520An%2520Xiao%2520and%2520Tianyu%2520Guo%2520and%2520Kai%2520Han%2520and%2520Xinghao%2520Chen%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520significantly%250Aadvanced%2520the%2520capabilities%2520of%2520artificial%2520intelligence%2520across%2520various%2520domains.%250AHowever%252C%2520their%2520massive%2520scale%2520and%2520high%2520computational%2520costs%2520render%2520them%250Aunsuitable%2520for%2520direct%2520deployment%2520in%2520resource-constrained%2520edge%2520environments.%250AThis%2520creates%2520a%2520critical%2520need%2520for%2520high-performance%2520small%2520models%2520that%2520can%2520operate%250Aefficiently%2520at%2520the%2520edge.%2520Yet%252C%2520after%2520pre-training%2520alone%252C%2520these%2520smaller%2520models%250Aoften%2520fail%2520to%2520meet%2520the%2520performance%2520requirements%2520of%2520complex%2520tasks.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520a%2520systematic%2520post-training%2520pipeline%2520that%2520efficiently%250Aenhances%2520small%2520model%2520accuracy.%2520Our%2520post%2520training%2520pipeline%2520consists%2520of%250Acurriculum-based%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520offline%2520on-policy%2520knowledge%250Adistillation.%2520The%2520resulting%2520instruction-tuned%2520model%2520achieves%2520state-of-the-art%250Aperformance%2520among%2520billion-parameter%2520models%252C%2520demonstrating%2520strong%2520generalization%250Aunder%2520strict%2520hardware%2520constraints%2520while%2520maintaining%2520competitive%2520accuracy%2520across%250Aa%2520variety%2520of%2520tasks.%2520This%2520work%2520provides%2520a%2520practical%2520and%2520efficient%2520solution%2520for%250Adeveloping%2520high-performance%2520language%2520models%2520on%2520Ascend%2520edge%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20the%20Power%20of%20Post-Training%20for%20Small%20Language%20Models%20via%0A%20%20Knowledge%20Distillation&entry.906535625=Miao%20Rang%20and%20Zhenni%20Bi%20and%20Hang%20Zhou%20and%20Hanting%20Chen%20and%20An%20Xiao%20and%20Tianyu%20Guo%20and%20Kai%20Han%20and%20Xinghao%20Chen%20and%20Yunhe%20Wang&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aadvanced%20the%20capabilities%20of%20artificial%20intelligence%20across%20various%20domains.%0AHowever%2C%20their%20massive%20scale%20and%20high%20computational%20costs%20render%20them%0Aunsuitable%20for%20direct%20deployment%20in%20resource-constrained%20edge%20environments.%0AThis%20creates%20a%20critical%20need%20for%20high-performance%20small%20models%20that%20can%20operate%0Aefficiently%20at%20the%20edge.%20Yet%2C%20after%20pre-training%20alone%2C%20these%20smaller%20models%0Aoften%20fail%20to%20meet%20the%20performance%20requirements%20of%20complex%20tasks.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20a%20systematic%20post-training%20pipeline%20that%20efficiently%0Aenhances%20small%20model%20accuracy.%20Our%20post%20training%20pipeline%20consists%20of%0Acurriculum-based%20supervised%20fine-tuning%20%28SFT%29%20and%20offline%20on-policy%20knowledge%0Adistillation.%20The%20resulting%20instruction-tuned%20model%20achieves%20state-of-the-art%0Aperformance%20among%20billion-parameter%20models%2C%20demonstrating%20strong%20generalization%0Aunder%20strict%20hardware%20constraints%20while%20maintaining%20competitive%20accuracy%20across%0Aa%20variety%20of%20tasks.%20This%20work%20provides%20a%20practical%20and%20efficient%20solution%20for%0Adeveloping%20high-performance%20language%20models%20on%20Ascend%20edge%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26497v1&entry.124074799=Read"},
{"title": "GIM: Improved Interpretability for Large Language Models", "author": "Joakim Edin and R\u00f3bert Csord\u00e1s and Tuukka Ruotsalo and Zhengxuan Wu and Maria Maistro and Jing Huang and Lars Maal\u00f8e", "abstract": "  Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.\n", "link": "http://arxiv.org/abs/2505.17630v2", "date": "2025-09-30", "relevancy": 1.5095, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4987}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIM%3A%20Improved%20Interpretability%20for%20Large%20Language%20Models&body=Title%3A%20GIM%3A%20Improved%20Interpretability%20for%20Large%20Language%20Models%0AAuthor%3A%20Joakim%20Edin%20and%20R%C3%B3bert%20Csord%C3%A1s%20and%20Tuukka%20Ruotsalo%20and%20Zhengxuan%20Wu%20and%20Maria%20Maistro%20and%20Jing%20Huang%20and%20Lars%20Maal%C3%B8e%0AAbstract%3A%20%20%20Ensuring%20faithful%20interpretability%20in%20large%20language%20models%20is%20imperative%20for%0Atrustworthy%20and%20reliable%20AI.%20A%20key%20obstacle%20is%20self-repair%2C%20a%20phenomenon%20where%0Anetworks%20compensate%20for%20reduced%20signal%20in%20one%20component%20by%20amplifying%20others%2C%0Amasking%20the%20true%20importance%20of%20the%20ablated%20component.%20While%20prior%20work%0Aattributes%20self-repair%20to%20layer%20normalization%20and%20back-up%20components%20that%0Acompensate%20for%20ablated%20components%2C%20we%20identify%20a%20novel%20form%20occurring%20within%0Athe%20attention%20mechanism%2C%20where%20softmax%20redistribution%20conceals%20the%20influence%20of%0Aimportant%20attention%20scores.%20This%20leads%20traditional%20ablation%20and%20gradient-based%0Amethods%20to%20underestimate%20the%20significance%20of%20all%20components%20contributing%20to%0Athese%20attention%20scores.%20We%20introduce%20Gradient%20Interaction%20Modifications%20%28GIM%29%2C%0Aa%20technique%20that%20accounts%20for%20self-repair%20during%20backpropagation.%20Extensive%0Aexperiments%20across%20multiple%20large%20language%20models%20%28Gemma%202B/9B%2C%20LLAMA%201B/3B/8B%2C%0AQwen%201.5B/3B%29%20and%20diverse%20tasks%20demonstrate%20that%20GIM%20significantly%20improves%0Afaithfulness%20over%20existing%20circuit%20identification%20and%20feature%20attribution%0Amethods.%20Our%20work%20is%20a%20significant%20step%20toward%20better%20understanding%20the%20inner%0Amechanisms%20of%20LLMs%2C%20which%20is%20crucial%20for%20improving%20them%20and%20ensuring%20their%0Asafety.%20Our%20code%20is%20available%20at%20https%3A//github.com/JoakimEdin/gim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIM%253A%2520Improved%2520Interpretability%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJoakim%2520Edin%2520and%2520R%25C3%25B3bert%2520Csord%25C3%25A1s%2520and%2520Tuukka%2520Ruotsalo%2520and%2520Zhengxuan%2520Wu%2520and%2520Maria%2520Maistro%2520and%2520Jing%2520Huang%2520and%2520Lars%2520Maal%25C3%25B8e%26entry.1292438233%3D%2520%2520Ensuring%2520faithful%2520interpretability%2520in%2520large%2520language%2520models%2520is%2520imperative%2520for%250Atrustworthy%2520and%2520reliable%2520AI.%2520A%2520key%2520obstacle%2520is%2520self-repair%252C%2520a%2520phenomenon%2520where%250Anetworks%2520compensate%2520for%2520reduced%2520signal%2520in%2520one%2520component%2520by%2520amplifying%2520others%252C%250Amasking%2520the%2520true%2520importance%2520of%2520the%2520ablated%2520component.%2520While%2520prior%2520work%250Aattributes%2520self-repair%2520to%2520layer%2520normalization%2520and%2520back-up%2520components%2520that%250Acompensate%2520for%2520ablated%2520components%252C%2520we%2520identify%2520a%2520novel%2520form%2520occurring%2520within%250Athe%2520attention%2520mechanism%252C%2520where%2520softmax%2520redistribution%2520conceals%2520the%2520influence%2520of%250Aimportant%2520attention%2520scores.%2520This%2520leads%2520traditional%2520ablation%2520and%2520gradient-based%250Amethods%2520to%2520underestimate%2520the%2520significance%2520of%2520all%2520components%2520contributing%2520to%250Athese%2520attention%2520scores.%2520We%2520introduce%2520Gradient%2520Interaction%2520Modifications%2520%2528GIM%2529%252C%250Aa%2520technique%2520that%2520accounts%2520for%2520self-repair%2520during%2520backpropagation.%2520Extensive%250Aexperiments%2520across%2520multiple%2520large%2520language%2520models%2520%2528Gemma%25202B/9B%252C%2520LLAMA%25201B/3B/8B%252C%250AQwen%25201.5B/3B%2529%2520and%2520diverse%2520tasks%2520demonstrate%2520that%2520GIM%2520significantly%2520improves%250Afaithfulness%2520over%2520existing%2520circuit%2520identification%2520and%2520feature%2520attribution%250Amethods.%2520Our%2520work%2520is%2520a%2520significant%2520step%2520toward%2520better%2520understanding%2520the%2520inner%250Amechanisms%2520of%2520LLMs%252C%2520which%2520is%2520crucial%2520for%2520improving%2520them%2520and%2520ensuring%2520their%250Asafety.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/JoakimEdin/gim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIM%3A%20Improved%20Interpretability%20for%20Large%20Language%20Models&entry.906535625=Joakim%20Edin%20and%20R%C3%B3bert%20Csord%C3%A1s%20and%20Tuukka%20Ruotsalo%20and%20Zhengxuan%20Wu%20and%20Maria%20Maistro%20and%20Jing%20Huang%20and%20Lars%20Maal%C3%B8e&entry.1292438233=%20%20Ensuring%20faithful%20interpretability%20in%20large%20language%20models%20is%20imperative%20for%0Atrustworthy%20and%20reliable%20AI.%20A%20key%20obstacle%20is%20self-repair%2C%20a%20phenomenon%20where%0Anetworks%20compensate%20for%20reduced%20signal%20in%20one%20component%20by%20amplifying%20others%2C%0Amasking%20the%20true%20importance%20of%20the%20ablated%20component.%20While%20prior%20work%0Aattributes%20self-repair%20to%20layer%20normalization%20and%20back-up%20components%20that%0Acompensate%20for%20ablated%20components%2C%20we%20identify%20a%20novel%20form%20occurring%20within%0Athe%20attention%20mechanism%2C%20where%20softmax%20redistribution%20conceals%20the%20influence%20of%0Aimportant%20attention%20scores.%20This%20leads%20traditional%20ablation%20and%20gradient-based%0Amethods%20to%20underestimate%20the%20significance%20of%20all%20components%20contributing%20to%0Athese%20attention%20scores.%20We%20introduce%20Gradient%20Interaction%20Modifications%20%28GIM%29%2C%0Aa%20technique%20that%20accounts%20for%20self-repair%20during%20backpropagation.%20Extensive%0Aexperiments%20across%20multiple%20large%20language%20models%20%28Gemma%202B/9B%2C%20LLAMA%201B/3B/8B%2C%0AQwen%201.5B/3B%29%20and%20diverse%20tasks%20demonstrate%20that%20GIM%20significantly%20improves%0Afaithfulness%20over%20existing%20circuit%20identification%20and%20feature%20attribution%0Amethods.%20Our%20work%20is%20a%20significant%20step%20toward%20better%20understanding%20the%20inner%0Amechanisms%20of%20LLMs%2C%20which%20is%20crucial%20for%20improving%20them%20and%20ensuring%20their%0Asafety.%20Our%20code%20is%20available%20at%20https%3A//github.com/JoakimEdin/gim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17630v2&entry.124074799=Read"},
{"title": "DisCO: Reinforcing Large Reasoning Models with Discriminative\n  Constrained Optimization", "author": "Gang Li and Ming Lin and Tomer Galanti and Zhengzhong Tu and Tianbao Yang", "abstract": "  The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint. As a result,\nDisCO offers notable advantages over GRPO and its variants: (i) it completely\neliminates difficulty bias by adopting discriminative objectives; (ii) it\naddresses the entropy instability in GRPO and its variants through the use of\nnon-clipping scoring functions and a constrained optimization approach,\nyielding long and stable training dynamics; (iii) it allows the incorporation\nof advanced discriminative learning techniques to address data imbalance, where\na significant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel.\n", "link": "http://arxiv.org/abs/2505.12366v3", "date": "2025-09-30", "relevancy": 1.5044, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5234}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisCO%3A%20Reinforcing%20Large%20Reasoning%20Models%20with%20Discriminative%0A%20%20Constrained%20Optimization&body=Title%3A%20DisCO%3A%20Reinforcing%20Large%20Reasoning%20Models%20with%20Discriminative%0A%20%20Constrained%20Optimization%0AAuthor%3A%20Gang%20Li%20and%20Ming%20Lin%20and%20Tomer%20Galanti%20and%20Zhengzhong%20Tu%20and%20Tianbao%20Yang%0AAbstract%3A%20%20%20The%20recent%20success%20and%20openness%20of%20DeepSeek-R1%20have%20brought%20widespread%0Aattention%20to%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20as%20a%20reinforcement%0Alearning%20method%20for%20large%20reasoning%20models%20%28LRMs%29.%20In%20this%20work%2C%20we%20analyze%20the%0AGRPO%20objective%20under%20a%20binary%20reward%20setting%20and%20reveal%20an%20inherent%20limitation%0Aof%20question-level%20difficulty%20bias.%20We%20also%20identify%20a%20connection%20between%20GRPO%0Aand%20traditional%20discriminative%20methods%20in%20supervised%20learning.%20Motivated%20by%0Athese%20insights%2C%20we%20introduce%20a%20new%20Discriminative%20Constrained%20Optimization%0A%28DisCO%29%20framework%20for%20reinforcing%20LRMs%2C%20grounded%20in%20the%20principle%20of%0Adiscriminative%20learning.%20The%20main%20differences%20between%20DisCO%20and%20GRPO%20and%20its%0Arecent%20variants%20are%3A%20%281%29%20it%20replaces%20the%20group%20relative%20objective%20with%20a%0Adiscriminative%20objective%20defined%20by%20a%20scoring%20function%3B%20%282%29%20it%20abandons%0Aclipping-based%20surrogates%20in%20favor%20of%20non-clipping%20RL%20surrogate%20objectives%20used%0Aas%20scoring%20functions%3B%20%283%29%20it%20employs%20a%20simple%20yet%20effective%20constrained%0Aoptimization%20approach%20to%20enforce%20the%20KL%20divergence%20constraint.%20As%20a%20result%2C%0ADisCO%20offers%20notable%20advantages%20over%20GRPO%20and%20its%20variants%3A%20%28i%29%20it%20completely%0Aeliminates%20difficulty%20bias%20by%20adopting%20discriminative%20objectives%3B%20%28ii%29%20it%0Aaddresses%20the%20entropy%20instability%20in%20GRPO%20and%20its%20variants%20through%20the%20use%20of%0Anon-clipping%20scoring%20functions%20and%20a%20constrained%20optimization%20approach%2C%0Ayielding%20long%20and%20stable%20training%20dynamics%3B%20%28iii%29%20it%20allows%20the%20incorporation%0Aof%20advanced%20discriminative%20learning%20techniques%20to%20address%20data%20imbalance%2C%20where%0Aa%20significant%20number%20of%20questions%20have%20more%20negative%20than%20positive%20generated%0Aanswers%20during%20training.%20Our%20experiments%20on%20enhancing%20the%20mathematical%0Areasoning%20capabilities%20of%20SFT-finetuned%20models%20show%20that%20DisCO%20significantly%0Aoutperforms%20GRPO%20and%20its%20improved%20variants%20such%20as%20DAPO%2C%20achieving%20average%0Agains%20of%207%5C%25%20over%20GRPO%20and%206%5C%25%20over%20DAPO%20across%20six%20benchmark%20tasks%20for%20an%201.5B%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisCO%253A%2520Reinforcing%2520Large%2520Reasoning%2520Models%2520with%2520Discriminative%250A%2520%2520Constrained%2520Optimization%26entry.906535625%3DGang%2520Li%2520and%2520Ming%2520Lin%2520and%2520Tomer%2520Galanti%2520and%2520Zhengzhong%2520Tu%2520and%2520Tianbao%2520Yang%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520and%2520openness%2520of%2520DeepSeek-R1%2520have%2520brought%2520widespread%250Aattention%2520to%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520as%2520a%2520reinforcement%250Alearning%2520method%2520for%2520large%2520reasoning%2520models%2520%2528LRMs%2529.%2520In%2520this%2520work%252C%2520we%2520analyze%2520the%250AGRPO%2520objective%2520under%2520a%2520binary%2520reward%2520setting%2520and%2520reveal%2520an%2520inherent%2520limitation%250Aof%2520question-level%2520difficulty%2520bias.%2520We%2520also%2520identify%2520a%2520connection%2520between%2520GRPO%250Aand%2520traditional%2520discriminative%2520methods%2520in%2520supervised%2520learning.%2520Motivated%2520by%250Athese%2520insights%252C%2520we%2520introduce%2520a%2520new%2520Discriminative%2520Constrained%2520Optimization%250A%2528DisCO%2529%2520framework%2520for%2520reinforcing%2520LRMs%252C%2520grounded%2520in%2520the%2520principle%2520of%250Adiscriminative%2520learning.%2520The%2520main%2520differences%2520between%2520DisCO%2520and%2520GRPO%2520and%2520its%250Arecent%2520variants%2520are%253A%2520%25281%2529%2520it%2520replaces%2520the%2520group%2520relative%2520objective%2520with%2520a%250Adiscriminative%2520objective%2520defined%2520by%2520a%2520scoring%2520function%253B%2520%25282%2529%2520it%2520abandons%250Aclipping-based%2520surrogates%2520in%2520favor%2520of%2520non-clipping%2520RL%2520surrogate%2520objectives%2520used%250Aas%2520scoring%2520functions%253B%2520%25283%2529%2520it%2520employs%2520a%2520simple%2520yet%2520effective%2520constrained%250Aoptimization%2520approach%2520to%2520enforce%2520the%2520KL%2520divergence%2520constraint.%2520As%2520a%2520result%252C%250ADisCO%2520offers%2520notable%2520advantages%2520over%2520GRPO%2520and%2520its%2520variants%253A%2520%2528i%2529%2520it%2520completely%250Aeliminates%2520difficulty%2520bias%2520by%2520adopting%2520discriminative%2520objectives%253B%2520%2528ii%2529%2520it%250Aaddresses%2520the%2520entropy%2520instability%2520in%2520GRPO%2520and%2520its%2520variants%2520through%2520the%2520use%2520of%250Anon-clipping%2520scoring%2520functions%2520and%2520a%2520constrained%2520optimization%2520approach%252C%250Ayielding%2520long%2520and%2520stable%2520training%2520dynamics%253B%2520%2528iii%2529%2520it%2520allows%2520the%2520incorporation%250Aof%2520advanced%2520discriminative%2520learning%2520techniques%2520to%2520address%2520data%2520imbalance%252C%2520where%250Aa%2520significant%2520number%2520of%2520questions%2520have%2520more%2520negative%2520than%2520positive%2520generated%250Aanswers%2520during%2520training.%2520Our%2520experiments%2520on%2520enhancing%2520the%2520mathematical%250Areasoning%2520capabilities%2520of%2520SFT-finetuned%2520models%2520show%2520that%2520DisCO%2520significantly%250Aoutperforms%2520GRPO%2520and%2520its%2520improved%2520variants%2520such%2520as%2520DAPO%252C%2520achieving%2520average%250Agains%2520of%25207%255C%2525%2520over%2520GRPO%2520and%25206%255C%2525%2520over%2520DAPO%2520across%2520six%2520benchmark%2520tasks%2520for%2520an%25201.5B%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisCO%3A%20Reinforcing%20Large%20Reasoning%20Models%20with%20Discriminative%0A%20%20Constrained%20Optimization&entry.906535625=Gang%20Li%20and%20Ming%20Lin%20and%20Tomer%20Galanti%20and%20Zhengzhong%20Tu%20and%20Tianbao%20Yang&entry.1292438233=%20%20The%20recent%20success%20and%20openness%20of%20DeepSeek-R1%20have%20brought%20widespread%0Aattention%20to%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20as%20a%20reinforcement%0Alearning%20method%20for%20large%20reasoning%20models%20%28LRMs%29.%20In%20this%20work%2C%20we%20analyze%20the%0AGRPO%20objective%20under%20a%20binary%20reward%20setting%20and%20reveal%20an%20inherent%20limitation%0Aof%20question-level%20difficulty%20bias.%20We%20also%20identify%20a%20connection%20between%20GRPO%0Aand%20traditional%20discriminative%20methods%20in%20supervised%20learning.%20Motivated%20by%0Athese%20insights%2C%20we%20introduce%20a%20new%20Discriminative%20Constrained%20Optimization%0A%28DisCO%29%20framework%20for%20reinforcing%20LRMs%2C%20grounded%20in%20the%20principle%20of%0Adiscriminative%20learning.%20The%20main%20differences%20between%20DisCO%20and%20GRPO%20and%20its%0Arecent%20variants%20are%3A%20%281%29%20it%20replaces%20the%20group%20relative%20objective%20with%20a%0Adiscriminative%20objective%20defined%20by%20a%20scoring%20function%3B%20%282%29%20it%20abandons%0Aclipping-based%20surrogates%20in%20favor%20of%20non-clipping%20RL%20surrogate%20objectives%20used%0Aas%20scoring%20functions%3B%20%283%29%20it%20employs%20a%20simple%20yet%20effective%20constrained%0Aoptimization%20approach%20to%20enforce%20the%20KL%20divergence%20constraint.%20As%20a%20result%2C%0ADisCO%20offers%20notable%20advantages%20over%20GRPO%20and%20its%20variants%3A%20%28i%29%20it%20completely%0Aeliminates%20difficulty%20bias%20by%20adopting%20discriminative%20objectives%3B%20%28ii%29%20it%0Aaddresses%20the%20entropy%20instability%20in%20GRPO%20and%20its%20variants%20through%20the%20use%20of%0Anon-clipping%20scoring%20functions%20and%20a%20constrained%20optimization%20approach%2C%0Ayielding%20long%20and%20stable%20training%20dynamics%3B%20%28iii%29%20it%20allows%20the%20incorporation%0Aof%20advanced%20discriminative%20learning%20techniques%20to%20address%20data%20imbalance%2C%20where%0Aa%20significant%20number%20of%20questions%20have%20more%20negative%20than%20positive%20generated%0Aanswers%20during%20training.%20Our%20experiments%20on%20enhancing%20the%20mathematical%0Areasoning%20capabilities%20of%20SFT-finetuned%20models%20show%20that%20DisCO%20significantly%0Aoutperforms%20GRPO%20and%20its%20improved%20variants%20such%20as%20DAPO%2C%20achieving%20average%0Agains%20of%207%5C%25%20over%20GRPO%20and%206%5C%25%20over%20DAPO%20across%20six%20benchmark%20tasks%20for%20an%201.5B%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12366v3&entry.124074799=Read"},
{"title": "Equivariance by Local Canonicalization: A Matter of Representation", "author": "Gerrit Gerhartz and Peter Lippmann and Fred A. Hamprecht", "abstract": "  Equivariant neural networks offer strong inductive biases for learning from\nmolecular and geometric data but often rely on specialized, computationally\nexpensive tensor operations. We present a framework to transfers existing\ntensor field networks into the more efficient local canonicalization paradigm,\npreserving equivariance while significantly improving the runtime. Within this\nframework, we systematically compare different equivariant representations in\nterms of theoretical complexity, empirical runtime, and predictive accuracy. We\npublish the tensor_frames package, a PyTorchGeometric based implementation for\nlocal canonicalization, that enables straightforward integration of\nequivariance into any standard message passing neural network.\n", "link": "http://arxiv.org/abs/2509.26499v1", "date": "2025-09-30", "relevancy": 1.4936, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5086}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariance%20by%20Local%20Canonicalization%3A%20A%20Matter%20of%20Representation&body=Title%3A%20Equivariance%20by%20Local%20Canonicalization%3A%20A%20Matter%20of%20Representation%0AAuthor%3A%20Gerrit%20Gerhartz%20and%20Peter%20Lippmann%20and%20Fred%20A.%20Hamprecht%0AAbstract%3A%20%20%20Equivariant%20neural%20networks%20offer%20strong%20inductive%20biases%20for%20learning%20from%0Amolecular%20and%20geometric%20data%20but%20often%20rely%20on%20specialized%2C%20computationally%0Aexpensive%20tensor%20operations.%20We%20present%20a%20framework%20to%20transfers%20existing%0Atensor%20field%20networks%20into%20the%20more%20efficient%20local%20canonicalization%20paradigm%2C%0Apreserving%20equivariance%20while%20significantly%20improving%20the%20runtime.%20Within%20this%0Aframework%2C%20we%20systematically%20compare%20different%20equivariant%20representations%20in%0Aterms%20of%20theoretical%20complexity%2C%20empirical%20runtime%2C%20and%20predictive%20accuracy.%20We%0Apublish%20the%20tensor_frames%20package%2C%20a%20PyTorchGeometric%20based%20implementation%20for%0Alocal%20canonicalization%2C%20that%20enables%20straightforward%20integration%20of%0Aequivariance%20into%20any%20standard%20message%20passing%20neural%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariance%2520by%2520Local%2520Canonicalization%253A%2520A%2520Matter%2520of%2520Representation%26entry.906535625%3DGerrit%2520Gerhartz%2520and%2520Peter%2520Lippmann%2520and%2520Fred%2520A.%2520Hamprecht%26entry.1292438233%3D%2520%2520Equivariant%2520neural%2520networks%2520offer%2520strong%2520inductive%2520biases%2520for%2520learning%2520from%250Amolecular%2520and%2520geometric%2520data%2520but%2520often%2520rely%2520on%2520specialized%252C%2520computationally%250Aexpensive%2520tensor%2520operations.%2520We%2520present%2520a%2520framework%2520to%2520transfers%2520existing%250Atensor%2520field%2520networks%2520into%2520the%2520more%2520efficient%2520local%2520canonicalization%2520paradigm%252C%250Apreserving%2520equivariance%2520while%2520significantly%2520improving%2520the%2520runtime.%2520Within%2520this%250Aframework%252C%2520we%2520systematically%2520compare%2520different%2520equivariant%2520representations%2520in%250Aterms%2520of%2520theoretical%2520complexity%252C%2520empirical%2520runtime%252C%2520and%2520predictive%2520accuracy.%2520We%250Apublish%2520the%2520tensor_frames%2520package%252C%2520a%2520PyTorchGeometric%2520based%2520implementation%2520for%250Alocal%2520canonicalization%252C%2520that%2520enables%2520straightforward%2520integration%2520of%250Aequivariance%2520into%2520any%2520standard%2520message%2520passing%2520neural%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariance%20by%20Local%20Canonicalization%3A%20A%20Matter%20of%20Representation&entry.906535625=Gerrit%20Gerhartz%20and%20Peter%20Lippmann%20and%20Fred%20A.%20Hamprecht&entry.1292438233=%20%20Equivariant%20neural%20networks%20offer%20strong%20inductive%20biases%20for%20learning%20from%0Amolecular%20and%20geometric%20data%20but%20often%20rely%20on%20specialized%2C%20computationally%0Aexpensive%20tensor%20operations.%20We%20present%20a%20framework%20to%20transfers%20existing%0Atensor%20field%20networks%20into%20the%20more%20efficient%20local%20canonicalization%20paradigm%2C%0Apreserving%20equivariance%20while%20significantly%20improving%20the%20runtime.%20Within%20this%0Aframework%2C%20we%20systematically%20compare%20different%20equivariant%20representations%20in%0Aterms%20of%20theoretical%20complexity%2C%20empirical%20runtime%2C%20and%20predictive%20accuracy.%20We%0Apublish%20the%20tensor_frames%20package%2C%20a%20PyTorchGeometric%20based%20implementation%20for%0Alocal%20canonicalization%2C%20that%20enables%20straightforward%20integration%20of%0Aequivariance%20into%20any%20standard%20message%20passing%20neural%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26499v1&entry.124074799=Read"},
{"title": "Hy-Facial: Hybrid Feature Extraction by Dimensionality Reduction Methods\n  for Enhanced Facial Expression Classification", "author": "Xinjin Li and Yu Ma and Kaisen Ye and Jinghan Cao and Minghao Zhou and Yeyang Zhou", "abstract": "  Facial expression classification remains a challenging task due to the high\ndimensionality and inherent complexity of facial image data. This paper\npresents Hy-Facial, a hybrid feature extraction framework that integrates both\ndeep learning and traditional image processing techniques, complemented by a\nsystematic investigation of dimensionality reduction strategies. The proposed\nmethod fuses deep features extracted from the Visual Geometry Group 19-layer\nnetwork (VGG19) with handcrafted local descriptors and the scale-invariant\nfeature transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB) algorithms,\nto obtain rich and diverse image representations. To mitigate feature\nredundancy and reduce computational complexity, we conduct a comprehensive\nevaluation of dimensionality reduction techniques and feature extraction. Among\nthese, UMAP is identified as the most effective, preserving both local and\nglobal structures of the high-dimensional feature space. The Hy-Facial pipeline\nintegrated VGG19, SIFT, and ORB for feature extraction, followed by K-means\nclustering and UMAP for dimensionality reduction, resulting in a classification\naccuracy of 83. 3\\% in the facial expression recognition (FER) dataset. These\nfindings underscore the pivotal role of dimensionality reduction not only as a\npre-processing step but as an essential component in improving feature quality\nand overall classification performance.\n", "link": "http://arxiv.org/abs/2509.26614v1", "date": "2025-09-30", "relevancy": 1.4933, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5119}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4947}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hy-Facial%3A%20Hybrid%20Feature%20Extraction%20by%20Dimensionality%20Reduction%20Methods%0A%20%20for%20Enhanced%20Facial%20Expression%20Classification&body=Title%3A%20Hy-Facial%3A%20Hybrid%20Feature%20Extraction%20by%20Dimensionality%20Reduction%20Methods%0A%20%20for%20Enhanced%20Facial%20Expression%20Classification%0AAuthor%3A%20Xinjin%20Li%20and%20Yu%20Ma%20and%20Kaisen%20Ye%20and%20Jinghan%20Cao%20and%20Minghao%20Zhou%20and%20Yeyang%20Zhou%0AAbstract%3A%20%20%20Facial%20expression%20classification%20remains%20a%20challenging%20task%20due%20to%20the%20high%0Adimensionality%20and%20inherent%20complexity%20of%20facial%20image%20data.%20This%20paper%0Apresents%20Hy-Facial%2C%20a%20hybrid%20feature%20extraction%20framework%20that%20integrates%20both%0Adeep%20learning%20and%20traditional%20image%20processing%20techniques%2C%20complemented%20by%20a%0Asystematic%20investigation%20of%20dimensionality%20reduction%20strategies.%20The%20proposed%0Amethod%20fuses%20deep%20features%20extracted%20from%20the%20Visual%20Geometry%20Group%2019-layer%0Anetwork%20%28VGG19%29%20with%20handcrafted%20local%20descriptors%20and%20the%20scale-invariant%0Afeature%20transform%20%28SIFT%29%20and%20Oriented%20FAST%20and%20Rotated%20BRIEF%20%28ORB%29%20algorithms%2C%0Ato%20obtain%20rich%20and%20diverse%20image%20representations.%20To%20mitigate%20feature%0Aredundancy%20and%20reduce%20computational%20complexity%2C%20we%20conduct%20a%20comprehensive%0Aevaluation%20of%20dimensionality%20reduction%20techniques%20and%20feature%20extraction.%20Among%0Athese%2C%20UMAP%20is%20identified%20as%20the%20most%20effective%2C%20preserving%20both%20local%20and%0Aglobal%20structures%20of%20the%20high-dimensional%20feature%20space.%20The%20Hy-Facial%20pipeline%0Aintegrated%20VGG19%2C%20SIFT%2C%20and%20ORB%20for%20feature%20extraction%2C%20followed%20by%20K-means%0Aclustering%20and%20UMAP%20for%20dimensionality%20reduction%2C%20resulting%20in%20a%20classification%0Aaccuracy%20of%2083.%203%5C%25%20in%20the%20facial%20expression%20recognition%20%28FER%29%20dataset.%20These%0Afindings%20underscore%20the%20pivotal%20role%20of%20dimensionality%20reduction%20not%20only%20as%20a%0Apre-processing%20step%20but%20as%20an%20essential%20component%20in%20improving%20feature%20quality%0Aand%20overall%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHy-Facial%253A%2520Hybrid%2520Feature%2520Extraction%2520by%2520Dimensionality%2520Reduction%2520Methods%250A%2520%2520for%2520Enhanced%2520Facial%2520Expression%2520Classification%26entry.906535625%3DXinjin%2520Li%2520and%2520Yu%2520Ma%2520and%2520Kaisen%2520Ye%2520and%2520Jinghan%2520Cao%2520and%2520Minghao%2520Zhou%2520and%2520Yeyang%2520Zhou%26entry.1292438233%3D%2520%2520Facial%2520expression%2520classification%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520high%250Adimensionality%2520and%2520inherent%2520complexity%2520of%2520facial%2520image%2520data.%2520This%2520paper%250Apresents%2520Hy-Facial%252C%2520a%2520hybrid%2520feature%2520extraction%2520framework%2520that%2520integrates%2520both%250Adeep%2520learning%2520and%2520traditional%2520image%2520processing%2520techniques%252C%2520complemented%2520by%2520a%250Asystematic%2520investigation%2520of%2520dimensionality%2520reduction%2520strategies.%2520The%2520proposed%250Amethod%2520fuses%2520deep%2520features%2520extracted%2520from%2520the%2520Visual%2520Geometry%2520Group%252019-layer%250Anetwork%2520%2528VGG19%2529%2520with%2520handcrafted%2520local%2520descriptors%2520and%2520the%2520scale-invariant%250Afeature%2520transform%2520%2528SIFT%2529%2520and%2520Oriented%2520FAST%2520and%2520Rotated%2520BRIEF%2520%2528ORB%2529%2520algorithms%252C%250Ato%2520obtain%2520rich%2520and%2520diverse%2520image%2520representations.%2520To%2520mitigate%2520feature%250Aredundancy%2520and%2520reduce%2520computational%2520complexity%252C%2520we%2520conduct%2520a%2520comprehensive%250Aevaluation%2520of%2520dimensionality%2520reduction%2520techniques%2520and%2520feature%2520extraction.%2520Among%250Athese%252C%2520UMAP%2520is%2520identified%2520as%2520the%2520most%2520effective%252C%2520preserving%2520both%2520local%2520and%250Aglobal%2520structures%2520of%2520the%2520high-dimensional%2520feature%2520space.%2520The%2520Hy-Facial%2520pipeline%250Aintegrated%2520VGG19%252C%2520SIFT%252C%2520and%2520ORB%2520for%2520feature%2520extraction%252C%2520followed%2520by%2520K-means%250Aclustering%2520and%2520UMAP%2520for%2520dimensionality%2520reduction%252C%2520resulting%2520in%2520a%2520classification%250Aaccuracy%2520of%252083.%25203%255C%2525%2520in%2520the%2520facial%2520expression%2520recognition%2520%2528FER%2529%2520dataset.%2520These%250Afindings%2520underscore%2520the%2520pivotal%2520role%2520of%2520dimensionality%2520reduction%2520not%2520only%2520as%2520a%250Apre-processing%2520step%2520but%2520as%2520an%2520essential%2520component%2520in%2520improving%2520feature%2520quality%250Aand%2520overall%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hy-Facial%3A%20Hybrid%20Feature%20Extraction%20by%20Dimensionality%20Reduction%20Methods%0A%20%20for%20Enhanced%20Facial%20Expression%20Classification&entry.906535625=Xinjin%20Li%20and%20Yu%20Ma%20and%20Kaisen%20Ye%20and%20Jinghan%20Cao%20and%20Minghao%20Zhou%20and%20Yeyang%20Zhou&entry.1292438233=%20%20Facial%20expression%20classification%20remains%20a%20challenging%20task%20due%20to%20the%20high%0Adimensionality%20and%20inherent%20complexity%20of%20facial%20image%20data.%20This%20paper%0Apresents%20Hy-Facial%2C%20a%20hybrid%20feature%20extraction%20framework%20that%20integrates%20both%0Adeep%20learning%20and%20traditional%20image%20processing%20techniques%2C%20complemented%20by%20a%0Asystematic%20investigation%20of%20dimensionality%20reduction%20strategies.%20The%20proposed%0Amethod%20fuses%20deep%20features%20extracted%20from%20the%20Visual%20Geometry%20Group%2019-layer%0Anetwork%20%28VGG19%29%20with%20handcrafted%20local%20descriptors%20and%20the%20scale-invariant%0Afeature%20transform%20%28SIFT%29%20and%20Oriented%20FAST%20and%20Rotated%20BRIEF%20%28ORB%29%20algorithms%2C%0Ato%20obtain%20rich%20and%20diverse%20image%20representations.%20To%20mitigate%20feature%0Aredundancy%20and%20reduce%20computational%20complexity%2C%20we%20conduct%20a%20comprehensive%0Aevaluation%20of%20dimensionality%20reduction%20techniques%20and%20feature%20extraction.%20Among%0Athese%2C%20UMAP%20is%20identified%20as%20the%20most%20effective%2C%20preserving%20both%20local%20and%0Aglobal%20structures%20of%20the%20high-dimensional%20feature%20space.%20The%20Hy-Facial%20pipeline%0Aintegrated%20VGG19%2C%20SIFT%2C%20and%20ORB%20for%20feature%20extraction%2C%20followed%20by%20K-means%0Aclustering%20and%20UMAP%20for%20dimensionality%20reduction%2C%20resulting%20in%20a%20classification%0Aaccuracy%20of%2083.%203%5C%25%20in%20the%20facial%20expression%20recognition%20%28FER%29%20dataset.%20These%0Afindings%20underscore%20the%20pivotal%20role%20of%20dimensionality%20reduction%20not%20only%20as%20a%0Apre-processing%20step%20but%20as%20an%20essential%20component%20in%20improving%20feature%20quality%0Aand%20overall%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26614v1&entry.124074799=Read"},
{"title": "Signal-Aware Workload Shifting Algorithms with Uncertainty-Quantified\n  Predictors", "author": "Ezra Johnson and Adam Lechowicz and Mohammad Hajiesmaili", "abstract": "  A wide range of sustainability and grid-integration strategies depend on\nworkload shifting, which aligns the timing of energy consumption with external\nsignals such as grid curtailment events, carbon intensity, or time-of-use\nelectricity prices. The main challenge lies in the online nature of the\nproblem: operators must make real-time decisions (e.g., whether to consume\nenergy now) without knowledge of the future. While forecasts of signal values\nare typically available, prior work on learning-augmented online algorithms has\nrelied almost exclusively on simple point forecasts. In parallel, the\nforecasting research has made significant progress in uncertainty\nquantification (UQ), which provides richer and more fine-grained predictive\ninformation. In this paper, we study how online workload shifting can leverage\nUQ predictors to improve decision-making. We introduce $\\texttt{UQ-Advice}$, a\nlearning-augmented algorithm that systematically integrates UQ forecasts\nthrough a $\\textit{decision uncertainty score}$ that measures how forecast\nuncertainty affects optimal future decisions. By introducing\n$\\textit{UQ-robustness}$, a new metric that characterizes how performance\ndegrades with forecast uncertainty, we establish theoretical performance\nguarantees for $\\texttt{UQ-Advice}$. Finally, using trace-driven experiments on\ncarbon intensity and electricity price data, we demonstrate that\n$\\texttt{UQ-Advice}$ consistently outperforms robust baselines and existing\nlearning-augmented methods that ignore uncertainty.\n", "link": "http://arxiv.org/abs/2509.26511v1", "date": "2025-09-30", "relevancy": 1.4698, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal-Aware%20Workload%20Shifting%20Algorithms%20with%20Uncertainty-Quantified%0A%20%20Predictors&body=Title%3A%20Signal-Aware%20Workload%20Shifting%20Algorithms%20with%20Uncertainty-Quantified%0A%20%20Predictors%0AAuthor%3A%20Ezra%20Johnson%20and%20Adam%20Lechowicz%20and%20Mohammad%20Hajiesmaili%0AAbstract%3A%20%20%20A%20wide%20range%20of%20sustainability%20and%20grid-integration%20strategies%20depend%20on%0Aworkload%20shifting%2C%20which%20aligns%20the%20timing%20of%20energy%20consumption%20with%20external%0Asignals%20such%20as%20grid%20curtailment%20events%2C%20carbon%20intensity%2C%20or%20time-of-use%0Aelectricity%20prices.%20The%20main%20challenge%20lies%20in%20the%20online%20nature%20of%20the%0Aproblem%3A%20operators%20must%20make%20real-time%20decisions%20%28e.g.%2C%20whether%20to%20consume%0Aenergy%20now%29%20without%20knowledge%20of%20the%20future.%20While%20forecasts%20of%20signal%20values%0Aare%20typically%20available%2C%20prior%20work%20on%20learning-augmented%20online%20algorithms%20has%0Arelied%20almost%20exclusively%20on%20simple%20point%20forecasts.%20In%20parallel%2C%20the%0Aforecasting%20research%20has%20made%20significant%20progress%20in%20uncertainty%0Aquantification%20%28UQ%29%2C%20which%20provides%20richer%20and%20more%20fine-grained%20predictive%0Ainformation.%20In%20this%20paper%2C%20we%20study%20how%20online%20workload%20shifting%20can%20leverage%0AUQ%20predictors%20to%20improve%20decision-making.%20We%20introduce%20%24%5Ctexttt%7BUQ-Advice%7D%24%2C%20a%0Alearning-augmented%20algorithm%20that%20systematically%20integrates%20UQ%20forecasts%0Athrough%20a%20%24%5Ctextit%7Bdecision%20uncertainty%20score%7D%24%20that%20measures%20how%20forecast%0Auncertainty%20affects%20optimal%20future%20decisions.%20By%20introducing%0A%24%5Ctextit%7BUQ-robustness%7D%24%2C%20a%20new%20metric%20that%20characterizes%20how%20performance%0Adegrades%20with%20forecast%20uncertainty%2C%20we%20establish%20theoretical%20performance%0Aguarantees%20for%20%24%5Ctexttt%7BUQ-Advice%7D%24.%20Finally%2C%20using%20trace-driven%20experiments%20on%0Acarbon%20intensity%20and%20electricity%20price%20data%2C%20we%20demonstrate%20that%0A%24%5Ctexttt%7BUQ-Advice%7D%24%20consistently%20outperforms%20robust%20baselines%20and%20existing%0Alearning-augmented%20methods%20that%20ignore%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal-Aware%2520Workload%2520Shifting%2520Algorithms%2520with%2520Uncertainty-Quantified%250A%2520%2520Predictors%26entry.906535625%3DEzra%2520Johnson%2520and%2520Adam%2520Lechowicz%2520and%2520Mohammad%2520Hajiesmaili%26entry.1292438233%3D%2520%2520A%2520wide%2520range%2520of%2520sustainability%2520and%2520grid-integration%2520strategies%2520depend%2520on%250Aworkload%2520shifting%252C%2520which%2520aligns%2520the%2520timing%2520of%2520energy%2520consumption%2520with%2520external%250Asignals%2520such%2520as%2520grid%2520curtailment%2520events%252C%2520carbon%2520intensity%252C%2520or%2520time-of-use%250Aelectricity%2520prices.%2520The%2520main%2520challenge%2520lies%2520in%2520the%2520online%2520nature%2520of%2520the%250Aproblem%253A%2520operators%2520must%2520make%2520real-time%2520decisions%2520%2528e.g.%252C%2520whether%2520to%2520consume%250Aenergy%2520now%2529%2520without%2520knowledge%2520of%2520the%2520future.%2520While%2520forecasts%2520of%2520signal%2520values%250Aare%2520typically%2520available%252C%2520prior%2520work%2520on%2520learning-augmented%2520online%2520algorithms%2520has%250Arelied%2520almost%2520exclusively%2520on%2520simple%2520point%2520forecasts.%2520In%2520parallel%252C%2520the%250Aforecasting%2520research%2520has%2520made%2520significant%2520progress%2520in%2520uncertainty%250Aquantification%2520%2528UQ%2529%252C%2520which%2520provides%2520richer%2520and%2520more%2520fine-grained%2520predictive%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520study%2520how%2520online%2520workload%2520shifting%2520can%2520leverage%250AUQ%2520predictors%2520to%2520improve%2520decision-making.%2520We%2520introduce%2520%2524%255Ctexttt%257BUQ-Advice%257D%2524%252C%2520a%250Alearning-augmented%2520algorithm%2520that%2520systematically%2520integrates%2520UQ%2520forecasts%250Athrough%2520a%2520%2524%255Ctextit%257Bdecision%2520uncertainty%2520score%257D%2524%2520that%2520measures%2520how%2520forecast%250Auncertainty%2520affects%2520optimal%2520future%2520decisions.%2520By%2520introducing%250A%2524%255Ctextit%257BUQ-robustness%257D%2524%252C%2520a%2520new%2520metric%2520that%2520characterizes%2520how%2520performance%250Adegrades%2520with%2520forecast%2520uncertainty%252C%2520we%2520establish%2520theoretical%2520performance%250Aguarantees%2520for%2520%2524%255Ctexttt%257BUQ-Advice%257D%2524.%2520Finally%252C%2520using%2520trace-driven%2520experiments%2520on%250Acarbon%2520intensity%2520and%2520electricity%2520price%2520data%252C%2520we%2520demonstrate%2520that%250A%2524%255Ctexttt%257BUQ-Advice%257D%2524%2520consistently%2520outperforms%2520robust%2520baselines%2520and%2520existing%250Alearning-augmented%2520methods%2520that%2520ignore%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal-Aware%20Workload%20Shifting%20Algorithms%20with%20Uncertainty-Quantified%0A%20%20Predictors&entry.906535625=Ezra%20Johnson%20and%20Adam%20Lechowicz%20and%20Mohammad%20Hajiesmaili&entry.1292438233=%20%20A%20wide%20range%20of%20sustainability%20and%20grid-integration%20strategies%20depend%20on%0Aworkload%20shifting%2C%20which%20aligns%20the%20timing%20of%20energy%20consumption%20with%20external%0Asignals%20such%20as%20grid%20curtailment%20events%2C%20carbon%20intensity%2C%20or%20time-of-use%0Aelectricity%20prices.%20The%20main%20challenge%20lies%20in%20the%20online%20nature%20of%20the%0Aproblem%3A%20operators%20must%20make%20real-time%20decisions%20%28e.g.%2C%20whether%20to%20consume%0Aenergy%20now%29%20without%20knowledge%20of%20the%20future.%20While%20forecasts%20of%20signal%20values%0Aare%20typically%20available%2C%20prior%20work%20on%20learning-augmented%20online%20algorithms%20has%0Arelied%20almost%20exclusively%20on%20simple%20point%20forecasts.%20In%20parallel%2C%20the%0Aforecasting%20research%20has%20made%20significant%20progress%20in%20uncertainty%0Aquantification%20%28UQ%29%2C%20which%20provides%20richer%20and%20more%20fine-grained%20predictive%0Ainformation.%20In%20this%20paper%2C%20we%20study%20how%20online%20workload%20shifting%20can%20leverage%0AUQ%20predictors%20to%20improve%20decision-making.%20We%20introduce%20%24%5Ctexttt%7BUQ-Advice%7D%24%2C%20a%0Alearning-augmented%20algorithm%20that%20systematically%20integrates%20UQ%20forecasts%0Athrough%20a%20%24%5Ctextit%7Bdecision%20uncertainty%20score%7D%24%20that%20measures%20how%20forecast%0Auncertainty%20affects%20optimal%20future%20decisions.%20By%20introducing%0A%24%5Ctextit%7BUQ-robustness%7D%24%2C%20a%20new%20metric%20that%20characterizes%20how%20performance%0Adegrades%20with%20forecast%20uncertainty%2C%20we%20establish%20theoretical%20performance%0Aguarantees%20for%20%24%5Ctexttt%7BUQ-Advice%7D%24.%20Finally%2C%20using%20trace-driven%20experiments%20on%0Acarbon%20intensity%20and%20electricity%20price%20data%2C%20we%20demonstrate%20that%0A%24%5Ctexttt%7BUQ-Advice%7D%24%20consistently%20outperforms%20robust%20baselines%20and%20existing%0Alearning-augmented%20methods%20that%20ignore%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26511v1&entry.124074799=Read"},
{"title": "Scalable Fingerprinting of Large Language Models", "author": "Anshul Nasery and Jonathan Hayase and Creston Brooks and Peiyao Sheng and Himanshu Tyagi and Pramod Viswanath and Sewoong Oh", "abstract": "  Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks. Our code is available\nat https://github.com/SewoongLab/scalable-fingerprinting-of-llms\n", "link": "http://arxiv.org/abs/2502.07760v2", "date": "2025-09-30", "relevancy": 1.4581, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4952}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4844}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Fingerprinting%20of%20Large%20Language%20Models&body=Title%3A%20Scalable%20Fingerprinting%20of%20Large%20Language%20Models%0AAuthor%3A%20Anshul%20Nasery%20and%20Jonathan%20Hayase%20and%20Creston%20Brooks%20and%20Peiyao%20Sheng%20and%20Himanshu%20Tyagi%20and%20Pramod%20Viswanath%20and%20Sewoong%20Oh%0AAbstract%3A%20%20%20Model%20fingerprinting%20has%20emerged%20as%20a%20powerful%20tool%20for%20model%20owners%20to%0Aidentify%20their%20shared%20model%20given%20API%20access.%20However%2C%20to%20lower%20false%20discovery%0Arate%2C%20fight%20fingerprint%20leakage%2C%20and%20defend%20against%20coalitions%20of%20model%20users%0Aattempting%20to%20bypass%20detection%2C%20we%20argue%20that%20%7B%5Cem%20scalability%7D%20is%20critical%2C%0Ai.e.%2C%20scaling%20up%20the%20number%20of%20fingerprints%20one%20can%20embed%20into%20a%20model.%20Hence%2C%0Awe%20pose%20scalability%20as%20a%20crucial%20requirement%20for%20fingerprinting%20schemes.%20We%0Aexperiment%20with%20fingerprint%20design%20at%20a%20scale%20significantly%20larger%20than%0Apreviously%20considered%2C%20and%20introduce%20a%20new%20method%2C%20dubbed%20Perinucleus%20sampling%2C%0Ato%20generate%20scalable%2C%20persistent%2C%20and%20harmless%20fingerprints.%20We%20demonstrate%0Athat%20this%20scheme%20can%20add%2024%2C576%20fingerprints%20to%20a%20Llama-3.1-8B%20model%20--%20two%0Aorders%20of%20magnitude%20more%20than%20existing%20schemes%20--%20without%20degrading%20the%20model%27s%0Autility.%20Our%20inserted%20fingerprints%20persist%20even%20after%20supervised%20fine-tuning%20on%0Astandard%20post-training%20data.%20We%20further%20address%20security%20risks%20for%0Afingerprinting%2C%20and%20theoretically%20and%20empirically%20show%20how%20a%20scalable%0Afingerprinting%20scheme%20like%20ours%20can%20mitigate%20these%20risks.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/SewoongLab/scalable-fingerprinting-of-llms%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Fingerprinting%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DAnshul%2520Nasery%2520and%2520Jonathan%2520Hayase%2520and%2520Creston%2520Brooks%2520and%2520Peiyao%2520Sheng%2520and%2520Himanshu%2520Tyagi%2520and%2520Pramod%2520Viswanath%2520and%2520Sewoong%2520Oh%26entry.1292438233%3D%2520%2520Model%2520fingerprinting%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520model%2520owners%2520to%250Aidentify%2520their%2520shared%2520model%2520given%2520API%2520access.%2520However%252C%2520to%2520lower%2520false%2520discovery%250Arate%252C%2520fight%2520fingerprint%2520leakage%252C%2520and%2520defend%2520against%2520coalitions%2520of%2520model%2520users%250Aattempting%2520to%2520bypass%2520detection%252C%2520we%2520argue%2520that%2520%257B%255Cem%2520scalability%257D%2520is%2520critical%252C%250Ai.e.%252C%2520scaling%2520up%2520the%2520number%2520of%2520fingerprints%2520one%2520can%2520embed%2520into%2520a%2520model.%2520Hence%252C%250Awe%2520pose%2520scalability%2520as%2520a%2520crucial%2520requirement%2520for%2520fingerprinting%2520schemes.%2520We%250Aexperiment%2520with%2520fingerprint%2520design%2520at%2520a%2520scale%2520significantly%2520larger%2520than%250Apreviously%2520considered%252C%2520and%2520introduce%2520a%2520new%2520method%252C%2520dubbed%2520Perinucleus%2520sampling%252C%250Ato%2520generate%2520scalable%252C%2520persistent%252C%2520and%2520harmless%2520fingerprints.%2520We%2520demonstrate%250Athat%2520this%2520scheme%2520can%2520add%252024%252C576%2520fingerprints%2520to%2520a%2520Llama-3.1-8B%2520model%2520--%2520two%250Aorders%2520of%2520magnitude%2520more%2520than%2520existing%2520schemes%2520--%2520without%2520degrading%2520the%2520model%2527s%250Autility.%2520Our%2520inserted%2520fingerprints%2520persist%2520even%2520after%2520supervised%2520fine-tuning%2520on%250Astandard%2520post-training%2520data.%2520We%2520further%2520address%2520security%2520risks%2520for%250Afingerprinting%252C%2520and%2520theoretically%2520and%2520empirically%2520show%2520how%2520a%2520scalable%250Afingerprinting%2520scheme%2520like%2520ours%2520can%2520mitigate%2520these%2520risks.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/SewoongLab/scalable-fingerprinting-of-llms%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Fingerprinting%20of%20Large%20Language%20Models&entry.906535625=Anshul%20Nasery%20and%20Jonathan%20Hayase%20and%20Creston%20Brooks%20and%20Peiyao%20Sheng%20and%20Himanshu%20Tyagi%20and%20Pramod%20Viswanath%20and%20Sewoong%20Oh&entry.1292438233=%20%20Model%20fingerprinting%20has%20emerged%20as%20a%20powerful%20tool%20for%20model%20owners%20to%0Aidentify%20their%20shared%20model%20given%20API%20access.%20However%2C%20to%20lower%20false%20discovery%0Arate%2C%20fight%20fingerprint%20leakage%2C%20and%20defend%20against%20coalitions%20of%20model%20users%0Aattempting%20to%20bypass%20detection%2C%20we%20argue%20that%20%7B%5Cem%20scalability%7D%20is%20critical%2C%0Ai.e.%2C%20scaling%20up%20the%20number%20of%20fingerprints%20one%20can%20embed%20into%20a%20model.%20Hence%2C%0Awe%20pose%20scalability%20as%20a%20crucial%20requirement%20for%20fingerprinting%20schemes.%20We%0Aexperiment%20with%20fingerprint%20design%20at%20a%20scale%20significantly%20larger%20than%0Apreviously%20considered%2C%20and%20introduce%20a%20new%20method%2C%20dubbed%20Perinucleus%20sampling%2C%0Ato%20generate%20scalable%2C%20persistent%2C%20and%20harmless%20fingerprints.%20We%20demonstrate%0Athat%20this%20scheme%20can%20add%2024%2C576%20fingerprints%20to%20a%20Llama-3.1-8B%20model%20--%20two%0Aorders%20of%20magnitude%20more%20than%20existing%20schemes%20--%20without%20degrading%20the%20model%27s%0Autility.%20Our%20inserted%20fingerprints%20persist%20even%20after%20supervised%20fine-tuning%20on%0Astandard%20post-training%20data.%20We%20further%20address%20security%20risks%20for%0Afingerprinting%2C%20and%20theoretically%20and%20empirically%20show%20how%20a%20scalable%0Afingerprinting%20scheme%20like%20ours%20can%20mitigate%20these%20risks.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/SewoongLab/scalable-fingerprinting-of-llms%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07760v2&entry.124074799=Read"},
{"title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!", "author": "Jingdi Lei and Varun Gumma and Rishabh Bhardwaj and Seok Min Lim and Chuan Li and Amir Zadeh and Soujanya Poria", "abstract": "  Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.\n", "link": "http://arxiv.org/abs/2509.26495v1", "date": "2025-09-30", "relevancy": 0.9738, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OffTopicEval%3A%20When%20Large%20Language%20Models%20Enter%20the%20Wrong%20Chat%2C%20Almost%0A%20%20Always%21&body=Title%3A%20OffTopicEval%3A%20When%20Large%20Language%20Models%20Enter%20the%20Wrong%20Chat%2C%20Almost%0A%20%20Always%21%0AAuthor%3A%20Jingdi%20Lei%20and%20Varun%20Gumma%20and%20Rishabh%20Bhardwaj%20and%20Seok%20Min%20Lim%20and%20Chuan%20Li%20and%20Amir%20Zadeh%20and%20Soujanya%20Poria%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20safety%20is%20one%20of%20the%20most%20pressing%20challenges%20for%0Aenabling%20wide-scale%20deployment.%20While%20most%20studies%20and%20global%20discussions%20focus%0Aon%20generic%20harms%2C%20such%20as%20models%20assisting%20users%20in%20harming%20themselves%20or%0Aothers%2C%20enterprises%20face%20a%20more%20fundamental%20concern%3A%20whether%20LLM-based%20agents%0Aare%20safe%20for%20their%20intended%20use%20case.%20To%20address%20this%2C%20we%20introduce%20operational%0Asafety%2C%20defined%20as%20an%20LLM%27s%20ability%20to%20appropriately%20accept%20or%20refuse%20user%0Aqueries%20when%20tasked%20with%20a%20specific%20purpose.%20We%20further%20propose%20OffTopicEval%2C%0Aan%20evaluation%20suite%20and%20benchmark%20for%20measuring%20operational%20safety%20both%20in%0Ageneral%20and%20within%20specific%20agentic%20use%20cases.%20Our%20evaluations%20on%20six%20model%0Afamilies%20comprising%2020%20open-weight%20LLMs%20reveal%20that%20while%20performance%20varies%0Aacross%20models%2C%20all%20of%20them%20remain%20highly%20operationally%20unsafe.%20Even%20the%0Astrongest%20models%20--%20Qwen-3%20%28235B%29%20with%2077.77%5C%25%20and%20Mistral%20%2824B%29%20with%2079.96%5C%25%0A--%20fall%20far%20short%20of%20reliable%20operational%20safety%2C%20while%20GPT%20models%20plateau%20in%0Athe%2062--73%5C%25%20range%2C%20Phi%20achieves%20only%20mid-level%20scores%20%2848--70%5C%25%29%2C%20and%20Gemma%0Aand%20Llama-3%20collapse%20to%2039.53%5C%25%20and%2023.84%5C%25%2C%20respectively.%20While%20operational%0Asafety%20is%20a%20core%20model%20alignment%20issue%2C%20to%20suppress%20these%20failures%2C%20we%20propose%0Aprompt-based%20steering%20methods%3A%20query%20grounding%20%28Q-ground%29%20and%20system-prompt%0Agrounding%20%28P-ground%29%2C%20which%20substantially%20improve%20OOD%20refusal.%20Q-ground%0Aprovides%20consistent%20gains%20of%20up%20to%2023%5C%25%2C%20while%20P-ground%20delivers%20even%20larger%0Aboosts%2C%20raising%20Llama-3.3%20%2870B%29%20by%2041%5C%25%20and%20Qwen-3%20%2830B%29%20by%2027%5C%25.%20These%20results%0Ahighlight%20both%20the%20urgent%20need%20for%20operational%20safety%20interventions%20and%20the%0Apromise%20of%20prompt-based%20steering%20as%20a%20first%20step%20toward%20more%20reliable%20LLM-based%0Aagents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffTopicEval%253A%2520When%2520Large%2520Language%2520Models%2520Enter%2520the%2520Wrong%2520Chat%252C%2520Almost%250A%2520%2520Always%2521%26entry.906535625%3DJingdi%2520Lei%2520and%2520Varun%2520Gumma%2520and%2520Rishabh%2520Bhardwaj%2520and%2520Seok%2520Min%2520Lim%2520and%2520Chuan%2520Li%2520and%2520Amir%2520Zadeh%2520and%2520Soujanya%2520Poria%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520safety%2520is%2520one%2520of%2520the%2520most%2520pressing%2520challenges%2520for%250Aenabling%2520wide-scale%2520deployment.%2520While%2520most%2520studies%2520and%2520global%2520discussions%2520focus%250Aon%2520generic%2520harms%252C%2520such%2520as%2520models%2520assisting%2520users%2520in%2520harming%2520themselves%2520or%250Aothers%252C%2520enterprises%2520face%2520a%2520more%2520fundamental%2520concern%253A%2520whether%2520LLM-based%2520agents%250Aare%2520safe%2520for%2520their%2520intended%2520use%2520case.%2520To%2520address%2520this%252C%2520we%2520introduce%2520operational%250Asafety%252C%2520defined%2520as%2520an%2520LLM%2527s%2520ability%2520to%2520appropriately%2520accept%2520or%2520refuse%2520user%250Aqueries%2520when%2520tasked%2520with%2520a%2520specific%2520purpose.%2520We%2520further%2520propose%2520OffTopicEval%252C%250Aan%2520evaluation%2520suite%2520and%2520benchmark%2520for%2520measuring%2520operational%2520safety%2520both%2520in%250Ageneral%2520and%2520within%2520specific%2520agentic%2520use%2520cases.%2520Our%2520evaluations%2520on%2520six%2520model%250Afamilies%2520comprising%252020%2520open-weight%2520LLMs%2520reveal%2520that%2520while%2520performance%2520varies%250Aacross%2520models%252C%2520all%2520of%2520them%2520remain%2520highly%2520operationally%2520unsafe.%2520Even%2520the%250Astrongest%2520models%2520--%2520Qwen-3%2520%2528235B%2529%2520with%252077.77%255C%2525%2520and%2520Mistral%2520%252824B%2529%2520with%252079.96%255C%2525%250A--%2520fall%2520far%2520short%2520of%2520reliable%2520operational%2520safety%252C%2520while%2520GPT%2520models%2520plateau%2520in%250Athe%252062--73%255C%2525%2520range%252C%2520Phi%2520achieves%2520only%2520mid-level%2520scores%2520%252848--70%255C%2525%2529%252C%2520and%2520Gemma%250Aand%2520Llama-3%2520collapse%2520to%252039.53%255C%2525%2520and%252023.84%255C%2525%252C%2520respectively.%2520While%2520operational%250Asafety%2520is%2520a%2520core%2520model%2520alignment%2520issue%252C%2520to%2520suppress%2520these%2520failures%252C%2520we%2520propose%250Aprompt-based%2520steering%2520methods%253A%2520query%2520grounding%2520%2528Q-ground%2529%2520and%2520system-prompt%250Agrounding%2520%2528P-ground%2529%252C%2520which%2520substantially%2520improve%2520OOD%2520refusal.%2520Q-ground%250Aprovides%2520consistent%2520gains%2520of%2520up%2520to%252023%255C%2525%252C%2520while%2520P-ground%2520delivers%2520even%2520larger%250Aboosts%252C%2520raising%2520Llama-3.3%2520%252870B%2529%2520by%252041%255C%2525%2520and%2520Qwen-3%2520%252830B%2529%2520by%252027%255C%2525.%2520These%2520results%250Ahighlight%2520both%2520the%2520urgent%2520need%2520for%2520operational%2520safety%2520interventions%2520and%2520the%250Apromise%2520of%2520prompt-based%2520steering%2520as%2520a%2520first%2520step%2520toward%2520more%2520reliable%2520LLM-based%250Aagents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OffTopicEval%3A%20When%20Large%20Language%20Models%20Enter%20the%20Wrong%20Chat%2C%20Almost%0A%20%20Always%21&entry.906535625=Jingdi%20Lei%20and%20Varun%20Gumma%20and%20Rishabh%20Bhardwaj%20and%20Seok%20Min%20Lim%20and%20Chuan%20Li%20and%20Amir%20Zadeh%20and%20Soujanya%20Poria&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20safety%20is%20one%20of%20the%20most%20pressing%20challenges%20for%0Aenabling%20wide-scale%20deployment.%20While%20most%20studies%20and%20global%20discussions%20focus%0Aon%20generic%20harms%2C%20such%20as%20models%20assisting%20users%20in%20harming%20themselves%20or%0Aothers%2C%20enterprises%20face%20a%20more%20fundamental%20concern%3A%20whether%20LLM-based%20agents%0Aare%20safe%20for%20their%20intended%20use%20case.%20To%20address%20this%2C%20we%20introduce%20operational%0Asafety%2C%20defined%20as%20an%20LLM%27s%20ability%20to%20appropriately%20accept%20or%20refuse%20user%0Aqueries%20when%20tasked%20with%20a%20specific%20purpose.%20We%20further%20propose%20OffTopicEval%2C%0Aan%20evaluation%20suite%20and%20benchmark%20for%20measuring%20operational%20safety%20both%20in%0Ageneral%20and%20within%20specific%20agentic%20use%20cases.%20Our%20evaluations%20on%20six%20model%0Afamilies%20comprising%2020%20open-weight%20LLMs%20reveal%20that%20while%20performance%20varies%0Aacross%20models%2C%20all%20of%20them%20remain%20highly%20operationally%20unsafe.%20Even%20the%0Astrongest%20models%20--%20Qwen-3%20%28235B%29%20with%2077.77%5C%25%20and%20Mistral%20%2824B%29%20with%2079.96%5C%25%0A--%20fall%20far%20short%20of%20reliable%20operational%20safety%2C%20while%20GPT%20models%20plateau%20in%0Athe%2062--73%5C%25%20range%2C%20Phi%20achieves%20only%20mid-level%20scores%20%2848--70%5C%25%29%2C%20and%20Gemma%0Aand%20Llama-3%20collapse%20to%2039.53%5C%25%20and%2023.84%5C%25%2C%20respectively.%20While%20operational%0Asafety%20is%20a%20core%20model%20alignment%20issue%2C%20to%20suppress%20these%20failures%2C%20we%20propose%0Aprompt-based%20steering%20methods%3A%20query%20grounding%20%28Q-ground%29%20and%20system-prompt%0Agrounding%20%28P-ground%29%2C%20which%20substantially%20improve%20OOD%20refusal.%20Q-ground%0Aprovides%20consistent%20gains%20of%20up%20to%2023%5C%25%2C%20while%20P-ground%20delivers%20even%20larger%0Aboosts%2C%20raising%20Llama-3.3%20%2870B%29%20by%2041%5C%25%20and%20Qwen-3%20%2830B%29%20by%2027%5C%25.%20These%20results%0Ahighlight%20both%20the%20urgent%20need%20for%20operational%20safety%20interventions%20and%20the%0Apromise%20of%20prompt-based%20steering%20as%20a%20first%20step%20toward%20more%20reliable%20LLM-based%0Aagents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26495v1&entry.124074799=Read"},
{"title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework", "author": "Jovan Stojkovic and Chaojie Zhang and \u00cd\u00f1igo Goiri and Ricardo Bianchini", "abstract": "  The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future.\n", "link": "http://arxiv.org/abs/2509.26534v1", "date": "2025-09-30", "relevancy": 1.3475, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4757}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4515}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rearchitecting%20Datacenter%20Lifecycle%20for%20AI%3A%20A%20TCO-Driven%20Framework&body=Title%3A%20Rearchitecting%20Datacenter%20Lifecycle%20for%20AI%3A%20A%20TCO-Driven%20Framework%0AAuthor%3A%20Jovan%20Stojkovic%20and%20Chaojie%20Zhang%20and%20%C3%8D%C3%B1igo%20Goiri%20and%20Ricardo%20Bianchini%0AAbstract%3A%20%20%20The%20rapid%20rise%20of%20large%20language%20models%20%28LLMs%29%20has%20been%20driving%20an%20enormous%0Ademand%20for%20AI%20inference%20infrastructure%2C%20mainly%20powered%20by%20high-end%20GPUs.%20While%0Athese%20accelerators%20offer%20immense%20computational%20power%2C%20they%20incur%20high%20capital%0Aand%20operational%20costs%20due%20to%20frequent%20upgrades%2C%20dense%20power%20consumption%2C%20and%0Acooling%20demands%2C%20making%20total%20cost%20of%20ownership%20%28TCO%29%20for%20AI%20datacenters%20a%0Acritical%20concern%20for%20cloud%20providers.%20Unfortunately%2C%20traditional%20datacenter%0Alifecycle%20management%20%28designed%20for%20general-purpose%20workloads%29%20struggles%20to%20keep%0Apace%20with%20AI%27s%20fast-evolving%20models%2C%20rising%20resource%20needs%2C%20and%20diverse%0Ahardware%20profiles.%20In%20this%20paper%2C%20we%20rethink%20the%20AI%20datacenter%20lifecycle%20scheme%0Aacross%20three%20stages%3A%20building%2C%20hardware%20refresh%2C%20and%20operation.%20We%20show%20how%0Adesign%20choices%20in%20power%2C%20cooling%2C%20and%20networking%20provisioning%20impact%20long-term%0ATCO.%20We%20also%20explore%20refresh%20strategies%20aligned%20with%20hardware%20trends.%20Finally%2C%0Awe%20use%20operation%20software%20optimizations%20to%20reduce%20cost.%20While%20these%0Aoptimizations%20at%20each%20stage%20yield%20benefits%2C%20unlocking%20the%20full%20potential%0Arequires%20rethinking%20the%20entire%20lifecycle.%20Thus%2C%20we%20present%20a%20holistic%20lifecycle%0Amanagement%20framework%20that%20coordinates%20and%20co-optimizes%20decisions%20across%20all%0Athree%20stages%2C%20accounting%20for%20workload%20dynamics%2C%20hardware%20evolution%2C%20and%20system%0Aaging.%20Our%20system%20reduces%20the%20TCO%20by%20up%20to%2040%5C%25%20over%20traditional%20approaches.%0AUsing%20our%20framework%20we%20provide%20guidelines%20on%20how%20to%20manage%20AI%20datacenter%0Alifecycle%20for%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRearchitecting%2520Datacenter%2520Lifecycle%2520for%2520AI%253A%2520A%2520TCO-Driven%2520Framework%26entry.906535625%3DJovan%2520Stojkovic%2520and%2520Chaojie%2520Zhang%2520and%2520%25C3%258D%25C3%25B1igo%2520Goiri%2520and%2520Ricardo%2520Bianchini%26entry.1292438233%3D%2520%2520The%2520rapid%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520been%2520driving%2520an%2520enormous%250Ademand%2520for%2520AI%2520inference%2520infrastructure%252C%2520mainly%2520powered%2520by%2520high-end%2520GPUs.%2520While%250Athese%2520accelerators%2520offer%2520immense%2520computational%2520power%252C%2520they%2520incur%2520high%2520capital%250Aand%2520operational%2520costs%2520due%2520to%2520frequent%2520upgrades%252C%2520dense%2520power%2520consumption%252C%2520and%250Acooling%2520demands%252C%2520making%2520total%2520cost%2520of%2520ownership%2520%2528TCO%2529%2520for%2520AI%2520datacenters%2520a%250Acritical%2520concern%2520for%2520cloud%2520providers.%2520Unfortunately%252C%2520traditional%2520datacenter%250Alifecycle%2520management%2520%2528designed%2520for%2520general-purpose%2520workloads%2529%2520struggles%2520to%2520keep%250Apace%2520with%2520AI%2527s%2520fast-evolving%2520models%252C%2520rising%2520resource%2520needs%252C%2520and%2520diverse%250Ahardware%2520profiles.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520the%2520AI%2520datacenter%2520lifecycle%2520scheme%250Aacross%2520three%2520stages%253A%2520building%252C%2520hardware%2520refresh%252C%2520and%2520operation.%2520We%2520show%2520how%250Adesign%2520choices%2520in%2520power%252C%2520cooling%252C%2520and%2520networking%2520provisioning%2520impact%2520long-term%250ATCO.%2520We%2520also%2520explore%2520refresh%2520strategies%2520aligned%2520with%2520hardware%2520trends.%2520Finally%252C%250Awe%2520use%2520operation%2520software%2520optimizations%2520to%2520reduce%2520cost.%2520While%2520these%250Aoptimizations%2520at%2520each%2520stage%2520yield%2520benefits%252C%2520unlocking%2520the%2520full%2520potential%250Arequires%2520rethinking%2520the%2520entire%2520lifecycle.%2520Thus%252C%2520we%2520present%2520a%2520holistic%2520lifecycle%250Amanagement%2520framework%2520that%2520coordinates%2520and%2520co-optimizes%2520decisions%2520across%2520all%250Athree%2520stages%252C%2520accounting%2520for%2520workload%2520dynamics%252C%2520hardware%2520evolution%252C%2520and%2520system%250Aaging.%2520Our%2520system%2520reduces%2520the%2520TCO%2520by%2520up%2520to%252040%255C%2525%2520over%2520traditional%2520approaches.%250AUsing%2520our%2520framework%2520we%2520provide%2520guidelines%2520on%2520how%2520to%2520manage%2520AI%2520datacenter%250Alifecycle%2520for%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rearchitecting%20Datacenter%20Lifecycle%20for%20AI%3A%20A%20TCO-Driven%20Framework&entry.906535625=Jovan%20Stojkovic%20and%20Chaojie%20Zhang%20and%20%C3%8D%C3%B1igo%20Goiri%20and%20Ricardo%20Bianchini&entry.1292438233=%20%20The%20rapid%20rise%20of%20large%20language%20models%20%28LLMs%29%20has%20been%20driving%20an%20enormous%0Ademand%20for%20AI%20inference%20infrastructure%2C%20mainly%20powered%20by%20high-end%20GPUs.%20While%0Athese%20accelerators%20offer%20immense%20computational%20power%2C%20they%20incur%20high%20capital%0Aand%20operational%20costs%20due%20to%20frequent%20upgrades%2C%20dense%20power%20consumption%2C%20and%0Acooling%20demands%2C%20making%20total%20cost%20of%20ownership%20%28TCO%29%20for%20AI%20datacenters%20a%0Acritical%20concern%20for%20cloud%20providers.%20Unfortunately%2C%20traditional%20datacenter%0Alifecycle%20management%20%28designed%20for%20general-purpose%20workloads%29%20struggles%20to%20keep%0Apace%20with%20AI%27s%20fast-evolving%20models%2C%20rising%20resource%20needs%2C%20and%20diverse%0Ahardware%20profiles.%20In%20this%20paper%2C%20we%20rethink%20the%20AI%20datacenter%20lifecycle%20scheme%0Aacross%20three%20stages%3A%20building%2C%20hardware%20refresh%2C%20and%20operation.%20We%20show%20how%0Adesign%20choices%20in%20power%2C%20cooling%2C%20and%20networking%20provisioning%20impact%20long-term%0ATCO.%20We%20also%20explore%20refresh%20strategies%20aligned%20with%20hardware%20trends.%20Finally%2C%0Awe%20use%20operation%20software%20optimizations%20to%20reduce%20cost.%20While%20these%0Aoptimizations%20at%20each%20stage%20yield%20benefits%2C%20unlocking%20the%20full%20potential%0Arequires%20rethinking%20the%20entire%20lifecycle.%20Thus%2C%20we%20present%20a%20holistic%20lifecycle%0Amanagement%20framework%20that%20coordinates%20and%20co-optimizes%20decisions%20across%20all%0Athree%20stages%2C%20accounting%20for%20workload%20dynamics%2C%20hardware%20evolution%2C%20and%20system%0Aaging.%20Our%20system%20reduces%20the%20TCO%20by%20up%20to%2040%5C%25%20over%20traditional%20approaches.%0AUsing%20our%20framework%20we%20provide%20guidelines%20on%20how%20to%20manage%20AI%20datacenter%0Alifecycle%20for%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26534v1&entry.124074799=Read"},
{"title": "FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation", "author": "Xenia Heilmann and Luca Corbucci and Mattia Cerrato and Anna Monreale", "abstract": "  Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing clients' private data. However, the diverse and often\nconflicting biases present across clients pose significant challenges to model\nfairness. Current fairness-enhancing FL solutions often fall short, as they\ntypically mitigate biases for a single, usually binary, sensitive attribute,\nwhile ignoring the heterogeneous fairness needs that exist in real-world\nsettings. Moreover, these solutions often evaluate unfairness reduction only on\nthe server side, hiding persistent unfairness at the individual client level.\nTo support more robust and reproducible fairness research in FL, we introduce a\ncomprehensive benchmarking framework for fairness-aware FL at both the global\nand client levels. Our contributions are three-fold: (1) We introduce\n\\fairdataset, a library to create tabular datasets tailored to evaluating fair\nFL methods under heterogeneous client bias; (2) we release four\nbias-heterogeneous datasets and corresponding benchmarks to compare fairness\nmitigation methods in a controlled environment; (3) we provide ready-to-use\nfunctions for evaluating fairness outcomes for these datasets.\n", "link": "http://arxiv.org/abs/2506.21095v3", "date": "2025-09-30", "relevancy": 1.3039, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4364}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4348}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeDa4Fair%3A%20Client-Level%20Federated%20Datasets%20for%20Fairness%20Evaluation&body=Title%3A%20FeDa4Fair%3A%20Client-Level%20Federated%20Datasets%20for%20Fairness%20Evaluation%0AAuthor%3A%20Xenia%20Heilmann%20and%20Luca%20Corbucci%20and%20Mattia%20Cerrato%20and%20Anna%20Monreale%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20multiple%0Aclients%20without%20sharing%20clients%27%20private%20data.%20However%2C%20the%20diverse%20and%20often%0Aconflicting%20biases%20present%20across%20clients%20pose%20significant%20challenges%20to%20model%0Afairness.%20Current%20fairness-enhancing%20FL%20solutions%20often%20fall%20short%2C%20as%20they%0Atypically%20mitigate%20biases%20for%20a%20single%2C%20usually%20binary%2C%20sensitive%20attribute%2C%0Awhile%20ignoring%20the%20heterogeneous%20fairness%20needs%20that%20exist%20in%20real-world%0Asettings.%20Moreover%2C%20these%20solutions%20often%20evaluate%20unfairness%20reduction%20only%20on%0Athe%20server%20side%2C%20hiding%20persistent%20unfairness%20at%20the%20individual%20client%20level.%0ATo%20support%20more%20robust%20and%20reproducible%20fairness%20research%20in%20FL%2C%20we%20introduce%20a%0Acomprehensive%20benchmarking%20framework%20for%20fairness-aware%20FL%20at%20both%20the%20global%0Aand%20client%20levels.%20Our%20contributions%20are%20three-fold%3A%20%281%29%20We%20introduce%0A%5Cfairdataset%2C%20a%20library%20to%20create%20tabular%20datasets%20tailored%20to%20evaluating%20fair%0AFL%20methods%20under%20heterogeneous%20client%20bias%3B%20%282%29%20we%20release%20four%0Abias-heterogeneous%20datasets%20and%20corresponding%20benchmarks%20to%20compare%20fairness%0Amitigation%20methods%20in%20a%20controlled%20environment%3B%20%283%29%20we%20provide%20ready-to-use%0Afunctions%20for%20evaluating%20fairness%20outcomes%20for%20these%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21095v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeDa4Fair%253A%2520Client-Level%2520Federated%2520Datasets%2520for%2520Fairness%2520Evaluation%26entry.906535625%3DXenia%2520Heilmann%2520and%2520Luca%2520Corbucci%2520and%2520Mattia%2520Cerrato%2520and%2520Anna%2520Monreale%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%2520multiple%250Aclients%2520without%2520sharing%2520clients%2527%2520private%2520data.%2520However%252C%2520the%2520diverse%2520and%2520often%250Aconflicting%2520biases%2520present%2520across%2520clients%2520pose%2520significant%2520challenges%2520to%2520model%250Afairness.%2520Current%2520fairness-enhancing%2520FL%2520solutions%2520often%2520fall%2520short%252C%2520as%2520they%250Atypically%2520mitigate%2520biases%2520for%2520a%2520single%252C%2520usually%2520binary%252C%2520sensitive%2520attribute%252C%250Awhile%2520ignoring%2520the%2520heterogeneous%2520fairness%2520needs%2520that%2520exist%2520in%2520real-world%250Asettings.%2520Moreover%252C%2520these%2520solutions%2520often%2520evaluate%2520unfairness%2520reduction%2520only%2520on%250Athe%2520server%2520side%252C%2520hiding%2520persistent%2520unfairness%2520at%2520the%2520individual%2520client%2520level.%250ATo%2520support%2520more%2520robust%2520and%2520reproducible%2520fairness%2520research%2520in%2520FL%252C%2520we%2520introduce%2520a%250Acomprehensive%2520benchmarking%2520framework%2520for%2520fairness-aware%2520FL%2520at%2520both%2520the%2520global%250Aand%2520client%2520levels.%2520Our%2520contributions%2520are%2520three-fold%253A%2520%25281%2529%2520We%2520introduce%250A%255Cfairdataset%252C%2520a%2520library%2520to%2520create%2520tabular%2520datasets%2520tailored%2520to%2520evaluating%2520fair%250AFL%2520methods%2520under%2520heterogeneous%2520client%2520bias%253B%2520%25282%2529%2520we%2520release%2520four%250Abias-heterogeneous%2520datasets%2520and%2520corresponding%2520benchmarks%2520to%2520compare%2520fairness%250Amitigation%2520methods%2520in%2520a%2520controlled%2520environment%253B%2520%25283%2529%2520we%2520provide%2520ready-to-use%250Afunctions%2520for%2520evaluating%2520fairness%2520outcomes%2520for%2520these%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21095v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeDa4Fair%3A%20Client-Level%20Federated%20Datasets%20for%20Fairness%20Evaluation&entry.906535625=Xenia%20Heilmann%20and%20Luca%20Corbucci%20and%20Mattia%20Cerrato%20and%20Anna%20Monreale&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%20multiple%0Aclients%20without%20sharing%20clients%27%20private%20data.%20However%2C%20the%20diverse%20and%20often%0Aconflicting%20biases%20present%20across%20clients%20pose%20significant%20challenges%20to%20model%0Afairness.%20Current%20fairness-enhancing%20FL%20solutions%20often%20fall%20short%2C%20as%20they%0Atypically%20mitigate%20biases%20for%20a%20single%2C%20usually%20binary%2C%20sensitive%20attribute%2C%0Awhile%20ignoring%20the%20heterogeneous%20fairness%20needs%20that%20exist%20in%20real-world%0Asettings.%20Moreover%2C%20these%20solutions%20often%20evaluate%20unfairness%20reduction%20only%20on%0Athe%20server%20side%2C%20hiding%20persistent%20unfairness%20at%20the%20individual%20client%20level.%0ATo%20support%20more%20robust%20and%20reproducible%20fairness%20research%20in%20FL%2C%20we%20introduce%20a%0Acomprehensive%20benchmarking%20framework%20for%20fairness-aware%20FL%20at%20both%20the%20global%0Aand%20client%20levels.%20Our%20contributions%20are%20three-fold%3A%20%281%29%20We%20introduce%0A%5Cfairdataset%2C%20a%20library%20to%20create%20tabular%20datasets%20tailored%20to%20evaluating%20fair%0AFL%20methods%20under%20heterogeneous%20client%20bias%3B%20%282%29%20we%20release%20four%0Abias-heterogeneous%20datasets%20and%20corresponding%20benchmarks%20to%20compare%20fairness%0Amitigation%20methods%20in%20a%20controlled%20environment%3B%20%283%29%20we%20provide%20ready-to-use%0Afunctions%20for%20evaluating%20fairness%20outcomes%20for%20these%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21095v3&entry.124074799=Read"},
{"title": "Towards Verified Code Reasoning by LLMs", "author": "Meghana Sistla and Gogul Balakrishnan and Pat Rondon and Jos\u00e9 Cambronero and Michele Tufano and Satish Chandra", "abstract": "  While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent.\n", "link": "http://arxiv.org/abs/2509.26546v1", "date": "2025-09-30", "relevancy": 1.3756, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4657}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4516}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Verified%20Code%20Reasoning%20by%20LLMs&body=Title%3A%20Towards%20Verified%20Code%20Reasoning%20by%20LLMs%0AAuthor%3A%20Meghana%20Sistla%20and%20Gogul%20Balakrishnan%20and%20Pat%20Rondon%20and%20Jos%C3%A9%20Cambronero%20and%20Michele%20Tufano%20and%20Satish%20Chandra%0AAbstract%3A%20%20%20While%20LLM-based%20agents%20are%20able%20to%20tackle%20a%20wide%20variety%20of%20code%20reasoning%0Aquestions%2C%20the%20answers%20are%20not%20always%20correct.%20This%20prevents%20the%20agent%20from%0Abeing%20useful%20in%20situations%20where%20high%20precision%20is%20desired%3A%20%281%29%20helping%20a%0Asoftware%20engineer%20understand%20a%20new%20code%20base%2C%20%282%29%20helping%20a%20software%20engineer%0Aduring%20code%20review%20sessions%2C%20and%20%283%29%20ensuring%20that%20the%20code%20generated%20by%20an%0Aautomated%20code%20generation%20system%20meets%20certain%20requirements%20%28e.g.%20fixes%20a%20bug%2C%0Aimproves%20readability%2C%20implements%20a%20feature%29.%0A%20%20As%20a%20result%20of%20this%20lack%20of%20trustworthiness%2C%20the%20agent%27s%20answers%20need%20to%20be%0Amanually%20verified%20before%20they%20can%20be%20trusted.%20Manually%20confirming%20responses%0Afrom%20a%20code%20reasoning%20agent%20requires%20human%20effort%20and%20can%20result%20in%20slower%0Adeveloper%20productivity%2C%20which%20weakens%20the%20assistance%20benefits%20of%20the%20agent.%20In%0Athis%20paper%2C%20we%20describe%20a%20method%20to%20automatically%20validate%20the%20answers%20provided%0Aby%20a%20code%20reasoning%20agent%20by%20verifying%20its%20reasoning%20steps.%20At%20a%20very%20high%0Alevel%2C%20the%20method%20consists%20of%20extracting%20a%20formal%20representation%20of%20the%20agent%27s%0Aresponse%20and%2C%20subsequently%2C%20using%20formal%20verification%20and%20program%20analysis%0Atools%20to%20verify%20the%20agent%27s%20reasoning%20steps.%0A%20%20We%20applied%20this%20approach%20to%20a%20benchmark%20set%20of%2020%20uninitialized%20variable%0Aerrors%20detected%20by%20sanitizers%20and%2020%20program%20equivalence%20queries.%20For%20the%0Auninitialized%20variable%20errors%2C%20the%20formal%20verification%20step%20was%20able%20to%0Avalidate%20the%20agent%27s%20reasoning%20on%2013/20%20examples%2C%20and%20for%20the%20program%0Aequivalence%20queries%2C%20the%20formal%20verification%20step%20successfully%20caught%206/8%0Aincorrect%20judgments%20made%20by%20the%20agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Verified%2520Code%2520Reasoning%2520by%2520LLMs%26entry.906535625%3DMeghana%2520Sistla%2520and%2520Gogul%2520Balakrishnan%2520and%2520Pat%2520Rondon%2520and%2520Jos%25C3%25A9%2520Cambronero%2520and%2520Michele%2520Tufano%2520and%2520Satish%2520Chandra%26entry.1292438233%3D%2520%2520While%2520LLM-based%2520agents%2520are%2520able%2520to%2520tackle%2520a%2520wide%2520variety%2520of%2520code%2520reasoning%250Aquestions%252C%2520the%2520answers%2520are%2520not%2520always%2520correct.%2520This%2520prevents%2520the%2520agent%2520from%250Abeing%2520useful%2520in%2520situations%2520where%2520high%2520precision%2520is%2520desired%253A%2520%25281%2529%2520helping%2520a%250Asoftware%2520engineer%2520understand%2520a%2520new%2520code%2520base%252C%2520%25282%2529%2520helping%2520a%2520software%2520engineer%250Aduring%2520code%2520review%2520sessions%252C%2520and%2520%25283%2529%2520ensuring%2520that%2520the%2520code%2520generated%2520by%2520an%250Aautomated%2520code%2520generation%2520system%2520meets%2520certain%2520requirements%2520%2528e.g.%2520fixes%2520a%2520bug%252C%250Aimproves%2520readability%252C%2520implements%2520a%2520feature%2529.%250A%2520%2520As%2520a%2520result%2520of%2520this%2520lack%2520of%2520trustworthiness%252C%2520the%2520agent%2527s%2520answers%2520need%2520to%2520be%250Amanually%2520verified%2520before%2520they%2520can%2520be%2520trusted.%2520Manually%2520confirming%2520responses%250Afrom%2520a%2520code%2520reasoning%2520agent%2520requires%2520human%2520effort%2520and%2520can%2520result%2520in%2520slower%250Adeveloper%2520productivity%252C%2520which%2520weakens%2520the%2520assistance%2520benefits%2520of%2520the%2520agent.%2520In%250Athis%2520paper%252C%2520we%2520describe%2520a%2520method%2520to%2520automatically%2520validate%2520the%2520answers%2520provided%250Aby%2520a%2520code%2520reasoning%2520agent%2520by%2520verifying%2520its%2520reasoning%2520steps.%2520At%2520a%2520very%2520high%250Alevel%252C%2520the%2520method%2520consists%2520of%2520extracting%2520a%2520formal%2520representation%2520of%2520the%2520agent%2527s%250Aresponse%2520and%252C%2520subsequently%252C%2520using%2520formal%2520verification%2520and%2520program%2520analysis%250Atools%2520to%2520verify%2520the%2520agent%2527s%2520reasoning%2520steps.%250A%2520%2520We%2520applied%2520this%2520approach%2520to%2520a%2520benchmark%2520set%2520of%252020%2520uninitialized%2520variable%250Aerrors%2520detected%2520by%2520sanitizers%2520and%252020%2520program%2520equivalence%2520queries.%2520For%2520the%250Auninitialized%2520variable%2520errors%252C%2520the%2520formal%2520verification%2520step%2520was%2520able%2520to%250Avalidate%2520the%2520agent%2527s%2520reasoning%2520on%252013/20%2520examples%252C%2520and%2520for%2520the%2520program%250Aequivalence%2520queries%252C%2520the%2520formal%2520verification%2520step%2520successfully%2520caught%25206/8%250Aincorrect%2520judgments%2520made%2520by%2520the%2520agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Verified%20Code%20Reasoning%20by%20LLMs&entry.906535625=Meghana%20Sistla%20and%20Gogul%20Balakrishnan%20and%20Pat%20Rondon%20and%20Jos%C3%A9%20Cambronero%20and%20Michele%20Tufano%20and%20Satish%20Chandra&entry.1292438233=%20%20While%20LLM-based%20agents%20are%20able%20to%20tackle%20a%20wide%20variety%20of%20code%20reasoning%0Aquestions%2C%20the%20answers%20are%20not%20always%20correct.%20This%20prevents%20the%20agent%20from%0Abeing%20useful%20in%20situations%20where%20high%20precision%20is%20desired%3A%20%281%29%20helping%20a%0Asoftware%20engineer%20understand%20a%20new%20code%20base%2C%20%282%29%20helping%20a%20software%20engineer%0Aduring%20code%20review%20sessions%2C%20and%20%283%29%20ensuring%20that%20the%20code%20generated%20by%20an%0Aautomated%20code%20generation%20system%20meets%20certain%20requirements%20%28e.g.%20fixes%20a%20bug%2C%0Aimproves%20readability%2C%20implements%20a%20feature%29.%0A%20%20As%20a%20result%20of%20this%20lack%20of%20trustworthiness%2C%20the%20agent%27s%20answers%20need%20to%20be%0Amanually%20verified%20before%20they%20can%20be%20trusted.%20Manually%20confirming%20responses%0Afrom%20a%20code%20reasoning%20agent%20requires%20human%20effort%20and%20can%20result%20in%20slower%0Adeveloper%20productivity%2C%20which%20weakens%20the%20assistance%20benefits%20of%20the%20agent.%20In%0Athis%20paper%2C%20we%20describe%20a%20method%20to%20automatically%20validate%20the%20answers%20provided%0Aby%20a%20code%20reasoning%20agent%20by%20verifying%20its%20reasoning%20steps.%20At%20a%20very%20high%0Alevel%2C%20the%20method%20consists%20of%20extracting%20a%20formal%20representation%20of%20the%20agent%27s%0Aresponse%20and%2C%20subsequently%2C%20using%20formal%20verification%20and%20program%20analysis%0Atools%20to%20verify%20the%20agent%27s%20reasoning%20steps.%0A%20%20We%20applied%20this%20approach%20to%20a%20benchmark%20set%20of%2020%20uninitialized%20variable%0Aerrors%20detected%20by%20sanitizers%20and%2020%20program%20equivalence%20queries.%20For%20the%0Auninitialized%20variable%20errors%2C%20the%20formal%20verification%20step%20was%20able%20to%0Avalidate%20the%20agent%27s%20reasoning%20on%2013/20%20examples%2C%20and%20for%20the%20program%0Aequivalence%20queries%2C%20the%20formal%20verification%20step%20successfully%20caught%206/8%0Aincorrect%20judgments%20made%20by%20the%20agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26546v1&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Regression using Proper Scoring Rules", "author": "Alexander Fishkov and Kajetan Schweighofer and Mykyta Ielanskyi and Nikita Kotelevskii and Mohsen Guizani and Maxim Panov", "abstract": "  Quantifying uncertainty of machine learning model predictions is essential\nfor reliable decision-making, especially in safety-critical applications.\nRecently, uncertainty quantification (UQ) theory has advanced significantly,\nbuilding on a firm basis of learning with proper scoring rules. However, these\nadvances were focused on classification, while extending these ideas to\nregression remains challenging. In this work, we introduce a unified UQ\nframework for regression based on proper scoring rules, such as CRPS,\nlogarithmic, squared error, and quadratic scores. We derive closed-form\nexpressions for the resulting uncertainty measures under practical parametric\nassumptions and show how to estimate them using ensembles of models. In\nparticular, the derived uncertainty measures naturally decompose into aleatoric\nand epistemic components. The framework recovers popular regression UQ measures\nbased on predictive variance and differential entropy. Our broad evaluation on\nsynthetic and real-world regression datasets provides guidance for selecting\nreliable UQ measures.\n", "link": "http://arxiv.org/abs/2509.26610v1", "date": "2025-09-30", "relevancy": 1.4422, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Regression%20using%20Proper%20Scoring%20Rules&body=Title%3A%20Uncertainty%20Quantification%20for%20Regression%20using%20Proper%20Scoring%20Rules%0AAuthor%3A%20Alexander%20Fishkov%20and%20Kajetan%20Schweighofer%20and%20Mykyta%20Ielanskyi%20and%20Nikita%20Kotelevskii%20and%20Mohsen%20Guizani%20and%20Maxim%20Panov%0AAbstract%3A%20%20%20Quantifying%20uncertainty%20of%20machine%20learning%20model%20predictions%20is%20essential%0Afor%20reliable%20decision-making%2C%20especially%20in%20safety-critical%20applications.%0ARecently%2C%20uncertainty%20quantification%20%28UQ%29%20theory%20has%20advanced%20significantly%2C%0Abuilding%20on%20a%20firm%20basis%20of%20learning%20with%20proper%20scoring%20rules.%20However%2C%20these%0Aadvances%20were%20focused%20on%20classification%2C%20while%20extending%20these%20ideas%20to%0Aregression%20remains%20challenging.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20UQ%0Aframework%20for%20regression%20based%20on%20proper%20scoring%20rules%2C%20such%20as%20CRPS%2C%0Alogarithmic%2C%20squared%20error%2C%20and%20quadratic%20scores.%20We%20derive%20closed-form%0Aexpressions%20for%20the%20resulting%20uncertainty%20measures%20under%20practical%20parametric%0Aassumptions%20and%20show%20how%20to%20estimate%20them%20using%20ensembles%20of%20models.%20In%0Aparticular%2C%20the%20derived%20uncertainty%20measures%20naturally%20decompose%20into%20aleatoric%0Aand%20epistemic%20components.%20The%20framework%20recovers%20popular%20regression%20UQ%20measures%0Abased%20on%20predictive%20variance%20and%20differential%20entropy.%20Our%20broad%20evaluation%20on%0Asynthetic%20and%20real-world%20regression%20datasets%20provides%20guidance%20for%20selecting%0Areliable%20UQ%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Regression%2520using%2520Proper%2520Scoring%2520Rules%26entry.906535625%3DAlexander%2520Fishkov%2520and%2520Kajetan%2520Schweighofer%2520and%2520Mykyta%2520Ielanskyi%2520and%2520Nikita%2520Kotelevskii%2520and%2520Mohsen%2520Guizani%2520and%2520Maxim%2520Panov%26entry.1292438233%3D%2520%2520Quantifying%2520uncertainty%2520of%2520machine%2520learning%2520model%2520predictions%2520is%2520essential%250Afor%2520reliable%2520decision-making%252C%2520especially%2520in%2520safety-critical%2520applications.%250ARecently%252C%2520uncertainty%2520quantification%2520%2528UQ%2529%2520theory%2520has%2520advanced%2520significantly%252C%250Abuilding%2520on%2520a%2520firm%2520basis%2520of%2520learning%2520with%2520proper%2520scoring%2520rules.%2520However%252C%2520these%250Aadvances%2520were%2520focused%2520on%2520classification%252C%2520while%2520extending%2520these%2520ideas%2520to%250Aregression%2520remains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520unified%2520UQ%250Aframework%2520for%2520regression%2520based%2520on%2520proper%2520scoring%2520rules%252C%2520such%2520as%2520CRPS%252C%250Alogarithmic%252C%2520squared%2520error%252C%2520and%2520quadratic%2520scores.%2520We%2520derive%2520closed-form%250Aexpressions%2520for%2520the%2520resulting%2520uncertainty%2520measures%2520under%2520practical%2520parametric%250Aassumptions%2520and%2520show%2520how%2520to%2520estimate%2520them%2520using%2520ensembles%2520of%2520models.%2520In%250Aparticular%252C%2520the%2520derived%2520uncertainty%2520measures%2520naturally%2520decompose%2520into%2520aleatoric%250Aand%2520epistemic%2520components.%2520The%2520framework%2520recovers%2520popular%2520regression%2520UQ%2520measures%250Abased%2520on%2520predictive%2520variance%2520and%2520differential%2520entropy.%2520Our%2520broad%2520evaluation%2520on%250Asynthetic%2520and%2520real-world%2520regression%2520datasets%2520provides%2520guidance%2520for%2520selecting%250Areliable%2520UQ%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Regression%20using%20Proper%20Scoring%20Rules&entry.906535625=Alexander%20Fishkov%20and%20Kajetan%20Schweighofer%20and%20Mykyta%20Ielanskyi%20and%20Nikita%20Kotelevskii%20and%20Mohsen%20Guizani%20and%20Maxim%20Panov&entry.1292438233=%20%20Quantifying%20uncertainty%20of%20machine%20learning%20model%20predictions%20is%20essential%0Afor%20reliable%20decision-making%2C%20especially%20in%20safety-critical%20applications.%0ARecently%2C%20uncertainty%20quantification%20%28UQ%29%20theory%20has%20advanced%20significantly%2C%0Abuilding%20on%20a%20firm%20basis%20of%20learning%20with%20proper%20scoring%20rules.%20However%2C%20these%0Aadvances%20were%20focused%20on%20classification%2C%20while%20extending%20these%20ideas%20to%0Aregression%20remains%20challenging.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20UQ%0Aframework%20for%20regression%20based%20on%20proper%20scoring%20rules%2C%20such%20as%20CRPS%2C%0Alogarithmic%2C%20squared%20error%2C%20and%20quadratic%20scores.%20We%20derive%20closed-form%0Aexpressions%20for%20the%20resulting%20uncertainty%20measures%20under%20practical%20parametric%0Aassumptions%20and%20show%20how%20to%20estimate%20them%20using%20ensembles%20of%20models.%20In%0Aparticular%2C%20the%20derived%20uncertainty%20measures%20naturally%20decompose%20into%20aleatoric%0Aand%20epistemic%20components.%20The%20framework%20recovers%20popular%20regression%20UQ%20measures%0Abased%20on%20predictive%20variance%20and%20differential%20entropy.%20Our%20broad%20evaluation%20on%0Asynthetic%20and%20real-world%20regression%20datasets%20provides%20guidance%20for%20selecting%0Areliable%20UQ%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26610v1&entry.124074799=Read"},
{"title": "Importance of localized dilatation and distensibility in identifying\n  determinants of thoracic aortic aneurysm with neural operators", "author": "David S. Li and Somdatta Goswami and Qianying Cao and Vivek Oommen and Roland Assi and Jay D. Humphrey and George E. Karniadakis", "abstract": "  Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and\nmechanobiological disruptions to the aortic wall that increase the risk of\ndissection or rupture. Evidence links TAA development to dysfunctions in the\naortic mechanotransduction axis, including loss of elastic fiber integrity and\ncell-matrix connections. Because distinct insults create different mechanical\nvulnerabilities, there is a critical need to identify interacting factors that\ndrive progression. Here, we use a finite element framework to generate\nsynthetic TAAs from hundreds of heterogeneous insults spanning varying degrees\nof elastic fiber damage and impaired mechanosensing. From these simulations, we\nconstruct spatial maps of localized dilatation and distensibility to train\nneural networks that predict the initiating combined insult. We compare several\narchitectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and\nmultiple input data formats to define a standard for future subject-specific\nmodeling. We also quantify predictive performance when networks are trained\nusing only geometric data (dilatation) versus both geometric and mechanical\ndata (dilatation plus distensibility). Across all networks, prediction errors\nare significantly higher when trained on dilatation alone, underscoring the\nadded value of distensibility information. Among the tested models, UNet\nconsistently provides the highest accuracy across all data formats. These\nfindings highlight the importance of acquiring full-field measurements of both\ndilatation and distensibility in TAA assessment to reveal the mechanobiological\ndrivers of disease and support the development of personalized treatment\nstrategies.\n", "link": "http://arxiv.org/abs/2509.26576v1", "date": "2025-09-30", "relevancy": 1.3412, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Importance%20of%20localized%20dilatation%20and%20distensibility%20in%20identifying%0A%20%20determinants%20of%20thoracic%20aortic%20aneurysm%20with%20neural%20operators&body=Title%3A%20Importance%20of%20localized%20dilatation%20and%20distensibility%20in%20identifying%0A%20%20determinants%20of%20thoracic%20aortic%20aneurysm%20with%20neural%20operators%0AAuthor%3A%20David%20S.%20Li%20and%20Somdatta%20Goswami%20and%20Qianying%20Cao%20and%20Vivek%20Oommen%20and%20Roland%20Assi%20and%20Jay%20D.%20Humphrey%20and%20George%20E.%20Karniadakis%0AAbstract%3A%20%20%20Thoracic%20aortic%20aneurysms%20%28TAAs%29%20arise%20from%20diverse%20mechanical%20and%0Amechanobiological%20disruptions%20to%20the%20aortic%20wall%20that%20increase%20the%20risk%20of%0Adissection%20or%20rupture.%20Evidence%20links%20TAA%20development%20to%20dysfunctions%20in%20the%0Aaortic%20mechanotransduction%20axis%2C%20including%20loss%20of%20elastic%20fiber%20integrity%20and%0Acell-matrix%20connections.%20Because%20distinct%20insults%20create%20different%20mechanical%0Avulnerabilities%2C%20there%20is%20a%20critical%20need%20to%20identify%20interacting%20factors%20that%0Adrive%20progression.%20Here%2C%20we%20use%20a%20finite%20element%20framework%20to%20generate%0Asynthetic%20TAAs%20from%20hundreds%20of%20heterogeneous%20insults%20spanning%20varying%20degrees%0Aof%20elastic%20fiber%20damage%20and%20impaired%20mechanosensing.%20From%20these%20simulations%2C%20we%0Aconstruct%20spatial%20maps%20of%20localized%20dilatation%20and%20distensibility%20to%20train%0Aneural%20networks%20that%20predict%20the%20initiating%20combined%20insult.%20We%20compare%20several%0Aarchitectures%20%28Deep%20Operator%20Networks%2C%20UNets%2C%20and%20Laplace%20Neural%20Operators%29%20and%0Amultiple%20input%20data%20formats%20to%20define%20a%20standard%20for%20future%20subject-specific%0Amodeling.%20We%20also%20quantify%20predictive%20performance%20when%20networks%20are%20trained%0Ausing%20only%20geometric%20data%20%28dilatation%29%20versus%20both%20geometric%20and%20mechanical%0Adata%20%28dilatation%20plus%20distensibility%29.%20Across%20all%20networks%2C%20prediction%20errors%0Aare%20significantly%20higher%20when%20trained%20on%20dilatation%20alone%2C%20underscoring%20the%0Aadded%20value%20of%20distensibility%20information.%20Among%20the%20tested%20models%2C%20UNet%0Aconsistently%20provides%20the%20highest%20accuracy%20across%20all%20data%20formats.%20These%0Afindings%20highlight%20the%20importance%20of%20acquiring%20full-field%20measurements%20of%20both%0Adilatation%20and%20distensibility%20in%20TAA%20assessment%20to%20reveal%20the%20mechanobiological%0Adrivers%20of%20disease%20and%20support%20the%20development%20of%20personalized%20treatment%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImportance%2520of%2520localized%2520dilatation%2520and%2520distensibility%2520in%2520identifying%250A%2520%2520determinants%2520of%2520thoracic%2520aortic%2520aneurysm%2520with%2520neural%2520operators%26entry.906535625%3DDavid%2520S.%2520Li%2520and%2520Somdatta%2520Goswami%2520and%2520Qianying%2520Cao%2520and%2520Vivek%2520Oommen%2520and%2520Roland%2520Assi%2520and%2520Jay%2520D.%2520Humphrey%2520and%2520George%2520E.%2520Karniadakis%26entry.1292438233%3D%2520%2520Thoracic%2520aortic%2520aneurysms%2520%2528TAAs%2529%2520arise%2520from%2520diverse%2520mechanical%2520and%250Amechanobiological%2520disruptions%2520to%2520the%2520aortic%2520wall%2520that%2520increase%2520the%2520risk%2520of%250Adissection%2520or%2520rupture.%2520Evidence%2520links%2520TAA%2520development%2520to%2520dysfunctions%2520in%2520the%250Aaortic%2520mechanotransduction%2520axis%252C%2520including%2520loss%2520of%2520elastic%2520fiber%2520integrity%2520and%250Acell-matrix%2520connections.%2520Because%2520distinct%2520insults%2520create%2520different%2520mechanical%250Avulnerabilities%252C%2520there%2520is%2520a%2520critical%2520need%2520to%2520identify%2520interacting%2520factors%2520that%250Adrive%2520progression.%2520Here%252C%2520we%2520use%2520a%2520finite%2520element%2520framework%2520to%2520generate%250Asynthetic%2520TAAs%2520from%2520hundreds%2520of%2520heterogeneous%2520insults%2520spanning%2520varying%2520degrees%250Aof%2520elastic%2520fiber%2520damage%2520and%2520impaired%2520mechanosensing.%2520From%2520these%2520simulations%252C%2520we%250Aconstruct%2520spatial%2520maps%2520of%2520localized%2520dilatation%2520and%2520distensibility%2520to%2520train%250Aneural%2520networks%2520that%2520predict%2520the%2520initiating%2520combined%2520insult.%2520We%2520compare%2520several%250Aarchitectures%2520%2528Deep%2520Operator%2520Networks%252C%2520UNets%252C%2520and%2520Laplace%2520Neural%2520Operators%2529%2520and%250Amultiple%2520input%2520data%2520formats%2520to%2520define%2520a%2520standard%2520for%2520future%2520subject-specific%250Amodeling.%2520We%2520also%2520quantify%2520predictive%2520performance%2520when%2520networks%2520are%2520trained%250Ausing%2520only%2520geometric%2520data%2520%2528dilatation%2529%2520versus%2520both%2520geometric%2520and%2520mechanical%250Adata%2520%2528dilatation%2520plus%2520distensibility%2529.%2520Across%2520all%2520networks%252C%2520prediction%2520errors%250Aare%2520significantly%2520higher%2520when%2520trained%2520on%2520dilatation%2520alone%252C%2520underscoring%2520the%250Aadded%2520value%2520of%2520distensibility%2520information.%2520Among%2520the%2520tested%2520models%252C%2520UNet%250Aconsistently%2520provides%2520the%2520highest%2520accuracy%2520across%2520all%2520data%2520formats.%2520These%250Afindings%2520highlight%2520the%2520importance%2520of%2520acquiring%2520full-field%2520measurements%2520of%2520both%250Adilatation%2520and%2520distensibility%2520in%2520TAA%2520assessment%2520to%2520reveal%2520the%2520mechanobiological%250Adrivers%2520of%2520disease%2520and%2520support%2520the%2520development%2520of%2520personalized%2520treatment%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Importance%20of%20localized%20dilatation%20and%20distensibility%20in%20identifying%0A%20%20determinants%20of%20thoracic%20aortic%20aneurysm%20with%20neural%20operators&entry.906535625=David%20S.%20Li%20and%20Somdatta%20Goswami%20and%20Qianying%20Cao%20and%20Vivek%20Oommen%20and%20Roland%20Assi%20and%20Jay%20D.%20Humphrey%20and%20George%20E.%20Karniadakis&entry.1292438233=%20%20Thoracic%20aortic%20aneurysms%20%28TAAs%29%20arise%20from%20diverse%20mechanical%20and%0Amechanobiological%20disruptions%20to%20the%20aortic%20wall%20that%20increase%20the%20risk%20of%0Adissection%20or%20rupture.%20Evidence%20links%20TAA%20development%20to%20dysfunctions%20in%20the%0Aaortic%20mechanotransduction%20axis%2C%20including%20loss%20of%20elastic%20fiber%20integrity%20and%0Acell-matrix%20connections.%20Because%20distinct%20insults%20create%20different%20mechanical%0Avulnerabilities%2C%20there%20is%20a%20critical%20need%20to%20identify%20interacting%20factors%20that%0Adrive%20progression.%20Here%2C%20we%20use%20a%20finite%20element%20framework%20to%20generate%0Asynthetic%20TAAs%20from%20hundreds%20of%20heterogeneous%20insults%20spanning%20varying%20degrees%0Aof%20elastic%20fiber%20damage%20and%20impaired%20mechanosensing.%20From%20these%20simulations%2C%20we%0Aconstruct%20spatial%20maps%20of%20localized%20dilatation%20and%20distensibility%20to%20train%0Aneural%20networks%20that%20predict%20the%20initiating%20combined%20insult.%20We%20compare%20several%0Aarchitectures%20%28Deep%20Operator%20Networks%2C%20UNets%2C%20and%20Laplace%20Neural%20Operators%29%20and%0Amultiple%20input%20data%20formats%20to%20define%20a%20standard%20for%20future%20subject-specific%0Amodeling.%20We%20also%20quantify%20predictive%20performance%20when%20networks%20are%20trained%0Ausing%20only%20geometric%20data%20%28dilatation%29%20versus%20both%20geometric%20and%20mechanical%0Adata%20%28dilatation%20plus%20distensibility%29.%20Across%20all%20networks%2C%20prediction%20errors%0Aare%20significantly%20higher%20when%20trained%20on%20dilatation%20alone%2C%20underscoring%20the%0Aadded%20value%20of%20distensibility%20information.%20Among%20the%20tested%20models%2C%20UNet%0Aconsistently%20provides%20the%20highest%20accuracy%20across%20all%20data%20formats.%20These%0Afindings%20highlight%20the%20importance%20of%20acquiring%20full-field%20measurements%20of%20both%0Adilatation%20and%20distensibility%20in%20TAA%20assessment%20to%20reveal%20the%20mechanobiological%0Adrivers%20of%20disease%20and%20support%20the%20development%20of%20personalized%20treatment%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26576v1&entry.124074799=Read"},
{"title": "AutoJudge: Judge Decoding Without Manual Annotation", "author": "Roman Garipov and Fedor Velikonivtsev and Ivan Ermakov and Ruslan Svirschevski and Vage Egiazarian and Max Ryabinin", "abstract": "  We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.\n", "link": "http://arxiv.org/abs/2504.20039v2", "date": "2025-09-30", "relevancy": 1.4438, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4718}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation&body=Title%3A%20AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation%0AAuthor%3A%20Roman%20Garipov%20and%20Fedor%20Velikonivtsev%20and%20Ivan%20Ermakov%20and%20Ruslan%20Svirschevski%20and%20Vage%20Egiazarian%20and%20Max%20Ryabinin%0AAbstract%3A%20%20%20We%20introduce%20AutoJudge%2C%20a%20method%20that%20accelerates%20large%20language%20model%20%28LLM%29%0Ainference%20with%20task-specific%20lossy%20speculative%20decoding.%20Instead%20of%20matching%0Athe%20original%20model%20output%20distribution%20token-by-token%2C%20we%20identify%20which%20of%20the%0Agenerated%20tokens%20affect%20the%20downstream%20quality%20of%20the%20response%2C%20relaxing%20the%0Adistribution%20match%20guarantee%20so%20that%20the%20%22unimportant%22%20tokens%20can%20be%20generated%0Afaster.%20Our%20approach%20relies%20on%20a%20semi-greedy%20search%20algorithm%20to%20test%20which%20of%0Athe%20mismatches%20between%20target%20and%20draft%20models%20should%20be%20corrected%20to%20preserve%0Aquality%20and%20which%20ones%20may%20be%20skipped.%20We%20then%20train%20a%20lightweight%20classifier%0Abased%20on%20existing%20LLM%20embeddings%20to%20predict%2C%20at%20inference%20time%2C%20which%0Amismatching%20tokens%20can%20be%20safely%20accepted%20without%20compromising%20the%20final%20answer%0Aquality.%20We%20evaluate%20the%20effectiveness%20of%20AutoJudge%20with%20multiple%20draft/target%0Amodel%20pairs%20on%20mathematical%20reasoning%20and%20programming%20benchmarks%2C%20achieving%0Asignificant%20speedups%20at%20the%20cost%20of%20a%20minor%20accuracy%20reduction.%20Notably%2C%20on%0AGSM8k%20with%20the%20Llama%203.1%2070B%20target%20model%2C%20our%20approach%20achieves%20up%20to%0A%24%5Capprox2%5Ctimes%24%20speedup%20over%20speculative%20decoding%20at%20the%20cost%20of%20%24%5Cle%201%5C%25%24%0Adrop%20in%20accuracy.%20When%20applied%20to%20the%20LiveCodeBench%20benchmark%2C%20AutoJudge%0Aautomatically%20detects%20programming-specific%20important%20tokens%2C%20accepting%20%24%5Cge%2025%24%0Atokens%20per%20speculation%20cycle%20at%20%242%5C%25%24%20drop%20in%20Pass%401.%20Our%20approach%20requires%20no%0Ahuman%20annotation%20and%20is%20easy%20to%20integrate%20with%20modern%20LLM%20inference%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoJudge%253A%2520Judge%2520Decoding%2520Without%2520Manual%2520Annotation%26entry.906535625%3DRoman%2520Garipov%2520and%2520Fedor%2520Velikonivtsev%2520and%2520Ivan%2520Ermakov%2520and%2520Ruslan%2520Svirschevski%2520and%2520Vage%2520Egiazarian%2520and%2520Max%2520Ryabinin%26entry.1292438233%3D%2520%2520We%2520introduce%2520AutoJudge%252C%2520a%2520method%2520that%2520accelerates%2520large%2520language%2520model%2520%2528LLM%2529%250Ainference%2520with%2520task-specific%2520lossy%2520speculative%2520decoding.%2520Instead%2520of%2520matching%250Athe%2520original%2520model%2520output%2520distribution%2520token-by-token%252C%2520we%2520identify%2520which%2520of%2520the%250Agenerated%2520tokens%2520affect%2520the%2520downstream%2520quality%2520of%2520the%2520response%252C%2520relaxing%2520the%250Adistribution%2520match%2520guarantee%2520so%2520that%2520the%2520%2522unimportant%2522%2520tokens%2520can%2520be%2520generated%250Afaster.%2520Our%2520approach%2520relies%2520on%2520a%2520semi-greedy%2520search%2520algorithm%2520to%2520test%2520which%2520of%250Athe%2520mismatches%2520between%2520target%2520and%2520draft%2520models%2520should%2520be%2520corrected%2520to%2520preserve%250Aquality%2520and%2520which%2520ones%2520may%2520be%2520skipped.%2520We%2520then%2520train%2520a%2520lightweight%2520classifier%250Abased%2520on%2520existing%2520LLM%2520embeddings%2520to%2520predict%252C%2520at%2520inference%2520time%252C%2520which%250Amismatching%2520tokens%2520can%2520be%2520safely%2520accepted%2520without%2520compromising%2520the%2520final%2520answer%250Aquality.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520AutoJudge%2520with%2520multiple%2520draft/target%250Amodel%2520pairs%2520on%2520mathematical%2520reasoning%2520and%2520programming%2520benchmarks%252C%2520achieving%250Asignificant%2520speedups%2520at%2520the%2520cost%2520of%2520a%2520minor%2520accuracy%2520reduction.%2520Notably%252C%2520on%250AGSM8k%2520with%2520the%2520Llama%25203.1%252070B%2520target%2520model%252C%2520our%2520approach%2520achieves%2520up%2520to%250A%2524%255Capprox2%255Ctimes%2524%2520speedup%2520over%2520speculative%2520decoding%2520at%2520the%2520cost%2520of%2520%2524%255Cle%25201%255C%2525%2524%250Adrop%2520in%2520accuracy.%2520When%2520applied%2520to%2520the%2520LiveCodeBench%2520benchmark%252C%2520AutoJudge%250Aautomatically%2520detects%2520programming-specific%2520important%2520tokens%252C%2520accepting%2520%2524%255Cge%252025%2524%250Atokens%2520per%2520speculation%2520cycle%2520at%2520%25242%255C%2525%2524%2520drop%2520in%2520Pass%25401.%2520Our%2520approach%2520requires%2520no%250Ahuman%2520annotation%2520and%2520is%2520easy%2520to%2520integrate%2520with%2520modern%2520LLM%2520inference%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoJudge%3A%20Judge%20Decoding%20Without%20Manual%20Annotation&entry.906535625=Roman%20Garipov%20and%20Fedor%20Velikonivtsev%20and%20Ivan%20Ermakov%20and%20Ruslan%20Svirschevski%20and%20Vage%20Egiazarian%20and%20Max%20Ryabinin&entry.1292438233=%20%20We%20introduce%20AutoJudge%2C%20a%20method%20that%20accelerates%20large%20language%20model%20%28LLM%29%0Ainference%20with%20task-specific%20lossy%20speculative%20decoding.%20Instead%20of%20matching%0Athe%20original%20model%20output%20distribution%20token-by-token%2C%20we%20identify%20which%20of%20the%0Agenerated%20tokens%20affect%20the%20downstream%20quality%20of%20the%20response%2C%20relaxing%20the%0Adistribution%20match%20guarantee%20so%20that%20the%20%22unimportant%22%20tokens%20can%20be%20generated%0Afaster.%20Our%20approach%20relies%20on%20a%20semi-greedy%20search%20algorithm%20to%20test%20which%20of%0Athe%20mismatches%20between%20target%20and%20draft%20models%20should%20be%20corrected%20to%20preserve%0Aquality%20and%20which%20ones%20may%20be%20skipped.%20We%20then%20train%20a%20lightweight%20classifier%0Abased%20on%20existing%20LLM%20embeddings%20to%20predict%2C%20at%20inference%20time%2C%20which%0Amismatching%20tokens%20can%20be%20safely%20accepted%20without%20compromising%20the%20final%20answer%0Aquality.%20We%20evaluate%20the%20effectiveness%20of%20AutoJudge%20with%20multiple%20draft/target%0Amodel%20pairs%20on%20mathematical%20reasoning%20and%20programming%20benchmarks%2C%20achieving%0Asignificant%20speedups%20at%20the%20cost%20of%20a%20minor%20accuracy%20reduction.%20Notably%2C%20on%0AGSM8k%20with%20the%20Llama%203.1%2070B%20target%20model%2C%20our%20approach%20achieves%20up%20to%0A%24%5Capprox2%5Ctimes%24%20speedup%20over%20speculative%20decoding%20at%20the%20cost%20of%20%24%5Cle%201%5C%25%24%0Adrop%20in%20accuracy.%20When%20applied%20to%20the%20LiveCodeBench%20benchmark%2C%20AutoJudge%0Aautomatically%20detects%20programming-specific%20important%20tokens%2C%20accepting%20%24%5Cge%2025%24%0Atokens%20per%20speculation%20cycle%20at%20%242%5C%25%24%20drop%20in%20Pass%401.%20Our%20approach%20requires%20no%0Ahuman%20annotation%20and%20is%20easy%20to%20integrate%20with%20modern%20LLM%20inference%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20039v2&entry.124074799=Read"},
{"title": "DiffCamera: Arbitrary Refocusing on Images", "author": "Yiyang Wang and Xi Chen and Xiaogang Xu and Yu Liu and Hengshuang Zhao", "abstract": "  The depth-of-field (DoF) effect, which introduces aesthetically pleasing\nblur, enhances photographic quality but is fixed and difficult to modify once\nthe image has been created. This becomes problematic when the applied blur is\nundesirable~(e.g., the subject is out of focus). To address this, we propose\nDiffCamera, a model that enables flexible refocusing of a created image\nconditioned on an arbitrary new focus point and a blur level. Specifically, we\ndesign a diffusion transformer framework for refocusing learning. However, the\ntraining requires pairs of data with different focus planes and bokeh levels in\nthe same scene, which are hard to acquire. To overcome this limitation, we\ndevelop a simulation-based pipeline to generate large-scale image pairs with\nvarying focus planes and bokeh levels. With the simulated data, we find that\ntraining with only a vanilla diffusion objective often leads to incorrect DoF\nbehaviors due to the complexity of the task. This requires a stronger\nconstraint during training. Inspired by the photographic principle that photos\nof different focus planes can be linearly blended into a multi-focus image, we\npropose a stacking constraint during training to enforce precise DoF\nmanipulation. This constraint enhances model training by imposing physically\ngrounded refocusing behavior that the focusing results should be faithfully\naligned with the scene structure and the camera conditions so that they can be\ncombined into the correct multi-focus image. We also construct a benchmark to\nevaluate the effectiveness of our refocusing model. Extensive experiments\ndemonstrate that DiffCamera supports stable refocusing across a wide range of\nscenes, providing unprecedented control over DoF adjustments for photography\nand generative AI applications.\n", "link": "http://arxiv.org/abs/2509.26599v1", "date": "2025-09-30", "relevancy": 1.1915, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6098}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5919}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffCamera%3A%20Arbitrary%20Refocusing%20on%20Images&body=Title%3A%20DiffCamera%3A%20Arbitrary%20Refocusing%20on%20Images%0AAuthor%3A%20Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Yu%20Liu%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20The%20depth-of-field%20%28DoF%29%20effect%2C%20which%20introduces%20aesthetically%20pleasing%0Ablur%2C%20enhances%20photographic%20quality%20but%20is%20fixed%20and%20difficult%20to%20modify%20once%0Athe%20image%20has%20been%20created.%20This%20becomes%20problematic%20when%20the%20applied%20blur%20is%0Aundesirable~%28e.g.%2C%20the%20subject%20is%20out%20of%20focus%29.%20To%20address%20this%2C%20we%20propose%0ADiffCamera%2C%20a%20model%20that%20enables%20flexible%20refocusing%20of%20a%20created%20image%0Aconditioned%20on%20an%20arbitrary%20new%20focus%20point%20and%20a%20blur%20level.%20Specifically%2C%20we%0Adesign%20a%20diffusion%20transformer%20framework%20for%20refocusing%20learning.%20However%2C%20the%0Atraining%20requires%20pairs%20of%20data%20with%20different%20focus%20planes%20and%20bokeh%20levels%20in%0Athe%20same%20scene%2C%20which%20are%20hard%20to%20acquire.%20To%20overcome%20this%20limitation%2C%20we%0Adevelop%20a%20simulation-based%20pipeline%20to%20generate%20large-scale%20image%20pairs%20with%0Avarying%20focus%20planes%20and%20bokeh%20levels.%20With%20the%20simulated%20data%2C%20we%20find%20that%0Atraining%20with%20only%20a%20vanilla%20diffusion%20objective%20often%20leads%20to%20incorrect%20DoF%0Abehaviors%20due%20to%20the%20complexity%20of%20the%20task.%20This%20requires%20a%20stronger%0Aconstraint%20during%20training.%20Inspired%20by%20the%20photographic%20principle%20that%20photos%0Aof%20different%20focus%20planes%20can%20be%20linearly%20blended%20into%20a%20multi-focus%20image%2C%20we%0Apropose%20a%20stacking%20constraint%20during%20training%20to%20enforce%20precise%20DoF%0Amanipulation.%20This%20constraint%20enhances%20model%20training%20by%20imposing%20physically%0Agrounded%20refocusing%20behavior%20that%20the%20focusing%20results%20should%20be%20faithfully%0Aaligned%20with%20the%20scene%20structure%20and%20the%20camera%20conditions%20so%20that%20they%20can%20be%0Acombined%20into%20the%20correct%20multi-focus%20image.%20We%20also%20construct%20a%20benchmark%20to%0Aevaluate%20the%20effectiveness%20of%20our%20refocusing%20model.%20Extensive%20experiments%0Ademonstrate%20that%20DiffCamera%20supports%20stable%20refocusing%20across%20a%20wide%20range%20of%0Ascenes%2C%20providing%20unprecedented%20control%20over%20DoF%20adjustments%20for%20photography%0Aand%20generative%20AI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffCamera%253A%2520Arbitrary%2520Refocusing%2520on%2520Images%26entry.906535625%3DYiyang%2520Wang%2520and%2520Xi%2520Chen%2520and%2520Xiaogang%2520Xu%2520and%2520Yu%2520Liu%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520The%2520depth-of-field%2520%2528DoF%2529%2520effect%252C%2520which%2520introduces%2520aesthetically%2520pleasing%250Ablur%252C%2520enhances%2520photographic%2520quality%2520but%2520is%2520fixed%2520and%2520difficult%2520to%2520modify%2520once%250Athe%2520image%2520has%2520been%2520created.%2520This%2520becomes%2520problematic%2520when%2520the%2520applied%2520blur%2520is%250Aundesirable~%2528e.g.%252C%2520the%2520subject%2520is%2520out%2520of%2520focus%2529.%2520To%2520address%2520this%252C%2520we%2520propose%250ADiffCamera%252C%2520a%2520model%2520that%2520enables%2520flexible%2520refocusing%2520of%2520a%2520created%2520image%250Aconditioned%2520on%2520an%2520arbitrary%2520new%2520focus%2520point%2520and%2520a%2520blur%2520level.%2520Specifically%252C%2520we%250Adesign%2520a%2520diffusion%2520transformer%2520framework%2520for%2520refocusing%2520learning.%2520However%252C%2520the%250Atraining%2520requires%2520pairs%2520of%2520data%2520with%2520different%2520focus%2520planes%2520and%2520bokeh%2520levels%2520in%250Athe%2520same%2520scene%252C%2520which%2520are%2520hard%2520to%2520acquire.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Adevelop%2520a%2520simulation-based%2520pipeline%2520to%2520generate%2520large-scale%2520image%2520pairs%2520with%250Avarying%2520focus%2520planes%2520and%2520bokeh%2520levels.%2520With%2520the%2520simulated%2520data%252C%2520we%2520find%2520that%250Atraining%2520with%2520only%2520a%2520vanilla%2520diffusion%2520objective%2520often%2520leads%2520to%2520incorrect%2520DoF%250Abehaviors%2520due%2520to%2520the%2520complexity%2520of%2520the%2520task.%2520This%2520requires%2520a%2520stronger%250Aconstraint%2520during%2520training.%2520Inspired%2520by%2520the%2520photographic%2520principle%2520that%2520photos%250Aof%2520different%2520focus%2520planes%2520can%2520be%2520linearly%2520blended%2520into%2520a%2520multi-focus%2520image%252C%2520we%250Apropose%2520a%2520stacking%2520constraint%2520during%2520training%2520to%2520enforce%2520precise%2520DoF%250Amanipulation.%2520This%2520constraint%2520enhances%2520model%2520training%2520by%2520imposing%2520physically%250Agrounded%2520refocusing%2520behavior%2520that%2520the%2520focusing%2520results%2520should%2520be%2520faithfully%250Aaligned%2520with%2520the%2520scene%2520structure%2520and%2520the%2520camera%2520conditions%2520so%2520that%2520they%2520can%2520be%250Acombined%2520into%2520the%2520correct%2520multi-focus%2520image.%2520We%2520also%2520construct%2520a%2520benchmark%2520to%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520refocusing%2520model.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520DiffCamera%2520supports%2520stable%2520refocusing%2520across%2520a%2520wide%2520range%2520of%250Ascenes%252C%2520providing%2520unprecedented%2520control%2520over%2520DoF%2520adjustments%2520for%2520photography%250Aand%2520generative%2520AI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffCamera%3A%20Arbitrary%20Refocusing%20on%20Images&entry.906535625=Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Yu%20Liu%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20The%20depth-of-field%20%28DoF%29%20effect%2C%20which%20introduces%20aesthetically%20pleasing%0Ablur%2C%20enhances%20photographic%20quality%20but%20is%20fixed%20and%20difficult%20to%20modify%20once%0Athe%20image%20has%20been%20created.%20This%20becomes%20problematic%20when%20the%20applied%20blur%20is%0Aundesirable~%28e.g.%2C%20the%20subject%20is%20out%20of%20focus%29.%20To%20address%20this%2C%20we%20propose%0ADiffCamera%2C%20a%20model%20that%20enables%20flexible%20refocusing%20of%20a%20created%20image%0Aconditioned%20on%20an%20arbitrary%20new%20focus%20point%20and%20a%20blur%20level.%20Specifically%2C%20we%0Adesign%20a%20diffusion%20transformer%20framework%20for%20refocusing%20learning.%20However%2C%20the%0Atraining%20requires%20pairs%20of%20data%20with%20different%20focus%20planes%20and%20bokeh%20levels%20in%0Athe%20same%20scene%2C%20which%20are%20hard%20to%20acquire.%20To%20overcome%20this%20limitation%2C%20we%0Adevelop%20a%20simulation-based%20pipeline%20to%20generate%20large-scale%20image%20pairs%20with%0Avarying%20focus%20planes%20and%20bokeh%20levels.%20With%20the%20simulated%20data%2C%20we%20find%20that%0Atraining%20with%20only%20a%20vanilla%20diffusion%20objective%20often%20leads%20to%20incorrect%20DoF%0Abehaviors%20due%20to%20the%20complexity%20of%20the%20task.%20This%20requires%20a%20stronger%0Aconstraint%20during%20training.%20Inspired%20by%20the%20photographic%20principle%20that%20photos%0Aof%20different%20focus%20planes%20can%20be%20linearly%20blended%20into%20a%20multi-focus%20image%2C%20we%0Apropose%20a%20stacking%20constraint%20during%20training%20to%20enforce%20precise%20DoF%0Amanipulation.%20This%20constraint%20enhances%20model%20training%20by%20imposing%20physically%0Agrounded%20refocusing%20behavior%20that%20the%20focusing%20results%20should%20be%20faithfully%0Aaligned%20with%20the%20scene%20structure%20and%20the%20camera%20conditions%20so%20that%20they%20can%20be%0Acombined%20into%20the%20correct%20multi-focus%20image.%20We%20also%20construct%20a%20benchmark%20to%0Aevaluate%20the%20effectiveness%20of%20our%20refocusing%20model.%20Extensive%20experiments%0Ademonstrate%20that%20DiffCamera%20supports%20stable%20refocusing%20across%20a%20wide%20range%20of%0Ascenes%2C%20providing%20unprecedented%20control%20over%20DoF%20adjustments%20for%20photography%0Aand%20generative%20AI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26599v1&entry.124074799=Read"},
{"title": "Fairness Testing in Retrieval-Augmented Generation: How Small\n  Perturbations Reveal Bias in Small Language Models", "author": "Matheus Vinicius da Silva de Oliveira and Jonathan de Andrade Silva and Awdren de Lima Fontao", "abstract": "  Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.\n", "link": "http://arxiv.org/abs/2509.26584v1", "date": "2025-09-30", "relevancy": 1.4311, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4881}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Testing%20in%20Retrieval-Augmented%20Generation%3A%20How%20Small%0A%20%20Perturbations%20Reveal%20Bias%20in%20Small%20Language%20Models&body=Title%3A%20Fairness%20Testing%20in%20Retrieval-Augmented%20Generation%3A%20How%20Small%0A%20%20Perturbations%20Reveal%20Bias%20in%20Small%20Language%20Models%0AAuthor%3A%20Matheus%20Vinicius%20da%20Silva%20de%20Oliveira%20and%20Jonathan%20de%20Andrade%20Silva%20and%20Awdren%20de%20Lima%20Fontao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20across%20multiple%20domains%20but%0Acontinue%20to%20raise%20concerns%20regarding%20security%20and%20fairness.%20Beyond%20known%20attack%0Avectors%20such%20as%20data%20poisoning%20and%20prompt%20injection%2C%20LLMs%20are%20also%20vulnerable%0Ato%20fairness%20bugs.%20These%20refer%20to%20unintended%20behaviors%20influenced%20by%20sensitive%0Ademographic%20cues%20%28e.g.%2C%20race%20or%20sexual%20orientation%29%20that%20should%20not%20affect%0Aoutcomes.%20Another%20key%20issue%20is%20hallucination%2C%20where%20models%20generate%20plausible%0Ayet%20false%20information.%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%0Astrategy%20to%20mitigate%20hallucinations%20by%20combining%20external%20retrieval%20with%20text%0Ageneration.%20However%2C%20its%20adoption%20raises%20new%20fairness%20concerns%2C%20as%20the%0Aretrieved%20content%20itself%20may%20surface%20or%20amplify%20bias.%20This%20study%20conducts%0Afairness%20testing%20through%20metamorphic%20testing%20%28MT%29%2C%20introducing%20controlled%0Ademographic%20perturbations%20in%20prompts%20to%20assess%20fairness%20in%20sentiment%20analysis%0Aperformed%20by%20three%20Small%20Language%20Models%20%28SLMs%29%20hosted%20on%20HuggingFace%0A%28Llama-3.2-3B-Instruct%2C%20Mistral-7B-Instruct-v0.3%2C%20and%20Llama-3.1-Nemotron-8B%29%2C%0Aeach%20integrated%20into%20a%20RAG%20pipeline.%20Results%20show%20that%20minor%20demographic%0Avariations%20can%20break%20up%20to%20one%20third%20of%20metamorphic%20relations%20%28MRs%29.%20A%20detailed%0Aanalysis%20of%20these%20failures%20reveals%20a%20consistent%20bias%20hierarchy%2C%20with%0Aperturbations%20involving%20racial%20cues%20being%20the%20predominant%20cause%20of%20the%0Aviolations.%20In%20addition%20to%20offering%20a%20comparative%20evaluation%2C%20this%20work%0Areinforces%20that%20the%20retrieval%20component%20in%20RAG%20must%20be%20carefully%20curated%20to%0Aprevent%20bias%20amplification.%20The%20findings%20serve%20as%20a%20practical%20alert%20for%0Adevelopers%2C%20testers%20and%20small%20organizations%20aiming%20to%20adopt%20accessible%20SLMs%0Awithout%20compromising%20fairness%20or%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.26584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Testing%2520in%2520Retrieval-Augmented%2520Generation%253A%2520How%2520Small%250A%2520%2520Perturbations%2520Reveal%2520Bias%2520in%2520Small%2520Language%2520Models%26entry.906535625%3DMatheus%2520Vinicius%2520da%2520Silva%2520de%2520Oliveira%2520and%2520Jonathan%2520de%2520Andrade%2520Silva%2520and%2520Awdren%2520de%2520Lima%2520Fontao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520across%2520multiple%2520domains%2520but%250Acontinue%2520to%2520raise%2520concerns%2520regarding%2520security%2520and%2520fairness.%2520Beyond%2520known%2520attack%250Avectors%2520such%2520as%2520data%2520poisoning%2520and%2520prompt%2520injection%252C%2520LLMs%2520are%2520also%2520vulnerable%250Ato%2520fairness%2520bugs.%2520These%2520refer%2520to%2520unintended%2520behaviors%2520influenced%2520by%2520sensitive%250Ademographic%2520cues%2520%2528e.g.%252C%2520race%2520or%2520sexual%2520orientation%2529%2520that%2520should%2520not%2520affect%250Aoutcomes.%2520Another%2520key%2520issue%2520is%2520hallucination%252C%2520where%2520models%2520generate%2520plausible%250Ayet%2520false%2520information.%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520emerged%2520as%2520a%250Astrategy%2520to%2520mitigate%2520hallucinations%2520by%2520combining%2520external%2520retrieval%2520with%2520text%250Ageneration.%2520However%252C%2520its%2520adoption%2520raises%2520new%2520fairness%2520concerns%252C%2520as%2520the%250Aretrieved%2520content%2520itself%2520may%2520surface%2520or%2520amplify%2520bias.%2520This%2520study%2520conducts%250Afairness%2520testing%2520through%2520metamorphic%2520testing%2520%2528MT%2529%252C%2520introducing%2520controlled%250Ademographic%2520perturbations%2520in%2520prompts%2520to%2520assess%2520fairness%2520in%2520sentiment%2520analysis%250Aperformed%2520by%2520three%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520hosted%2520on%2520HuggingFace%250A%2528Llama-3.2-3B-Instruct%252C%2520Mistral-7B-Instruct-v0.3%252C%2520and%2520Llama-3.1-Nemotron-8B%2529%252C%250Aeach%2520integrated%2520into%2520a%2520RAG%2520pipeline.%2520Results%2520show%2520that%2520minor%2520demographic%250Avariations%2520can%2520break%2520up%2520to%2520one%2520third%2520of%2520metamorphic%2520relations%2520%2528MRs%2529.%2520A%2520detailed%250Aanalysis%2520of%2520these%2520failures%2520reveals%2520a%2520consistent%2520bias%2520hierarchy%252C%2520with%250Aperturbations%2520involving%2520racial%2520cues%2520being%2520the%2520predominant%2520cause%2520of%2520the%250Aviolations.%2520In%2520addition%2520to%2520offering%2520a%2520comparative%2520evaluation%252C%2520this%2520work%250Areinforces%2520that%2520the%2520retrieval%2520component%2520in%2520RAG%2520must%2520be%2520carefully%2520curated%2520to%250Aprevent%2520bias%2520amplification.%2520The%2520findings%2520serve%2520as%2520a%2520practical%2520alert%2520for%250Adevelopers%252C%2520testers%2520and%2520small%2520organizations%2520aiming%2520to%2520adopt%2520accessible%2520SLMs%250Awithout%2520compromising%2520fairness%2520or%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Testing%20in%20Retrieval-Augmented%20Generation%3A%20How%20Small%0A%20%20Perturbations%20Reveal%20Bias%20in%20Small%20Language%20Models&entry.906535625=Matheus%20Vinicius%20da%20Silva%20de%20Oliveira%20and%20Jonathan%20de%20Andrade%20Silva%20and%20Awdren%20de%20Lima%20Fontao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20across%20multiple%20domains%20but%0Acontinue%20to%20raise%20concerns%20regarding%20security%20and%20fairness.%20Beyond%20known%20attack%0Avectors%20such%20as%20data%20poisoning%20and%20prompt%20injection%2C%20LLMs%20are%20also%20vulnerable%0Ato%20fairness%20bugs.%20These%20refer%20to%20unintended%20behaviors%20influenced%20by%20sensitive%0Ademographic%20cues%20%28e.g.%2C%20race%20or%20sexual%20orientation%29%20that%20should%20not%20affect%0Aoutcomes.%20Another%20key%20issue%20is%20hallucination%2C%20where%20models%20generate%20plausible%0Ayet%20false%20information.%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%0Astrategy%20to%20mitigate%20hallucinations%20by%20combining%20external%20retrieval%20with%20text%0Ageneration.%20However%2C%20its%20adoption%20raises%20new%20fairness%20concerns%2C%20as%20the%0Aretrieved%20content%20itself%20may%20surface%20or%20amplify%20bias.%20This%20study%20conducts%0Afairness%20testing%20through%20metamorphic%20testing%20%28MT%29%2C%20introducing%20controlled%0Ademographic%20perturbations%20in%20prompts%20to%20assess%20fairness%20in%20sentiment%20analysis%0Aperformed%20by%20three%20Small%20Language%20Models%20%28SLMs%29%20hosted%20on%20HuggingFace%0A%28Llama-3.2-3B-Instruct%2C%20Mistral-7B-Instruct-v0.3%2C%20and%20Llama-3.1-Nemotron-8B%29%2C%0Aeach%20integrated%20into%20a%20RAG%20pipeline.%20Results%20show%20that%20minor%20demographic%0Avariations%20can%20break%20up%20to%20one%20third%20of%20metamorphic%20relations%20%28MRs%29.%20A%20detailed%0Aanalysis%20of%20these%20failures%20reveals%20a%20consistent%20bias%20hierarchy%2C%20with%0Aperturbations%20involving%20racial%20cues%20being%20the%20predominant%20cause%20of%20the%0Aviolations.%20In%20addition%20to%20offering%20a%20comparative%20evaluation%2C%20this%20work%0Areinforces%20that%20the%20retrieval%20component%20in%20RAG%20must%20be%20carefully%20curated%20to%0Aprevent%20bias%20amplification.%20The%20findings%20serve%20as%20a%20practical%20alert%20for%0Adevelopers%2C%20testers%20and%20small%20organizations%20aiming%20to%20adopt%20accessible%20SLMs%0Awithout%20compromising%20fairness%20or%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.26584v1&entry.124074799=Read"},
{"title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning", "author": "Huihao Jing and Wenbin Hu and Hongyu Luo and Jianhui Yang and Wei Fan and Haoran Li and Yangqiu Song", "abstract": "  Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.\n", "link": "http://arxiv.org/abs/2509.24922v2", "date": "2025-09-30", "relevancy": 1.4541, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4984}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASLegalBench%3A%20Benchmarking%20Multi-Agent%20Systems%20in%20Deductive%20Legal%0A%20%20Reasoning&body=Title%3A%20MASLegalBench%3A%20Benchmarking%20Multi-Agent%20Systems%20in%20Deductive%20Legal%0A%20%20Reasoning%0AAuthor%3A%20Huihao%20Jing%20and%20Wenbin%20Hu%20and%20Hongyu%20Luo%20and%20Jianhui%20Yang%20and%20Wei%20Fan%20and%20Haoran%20Li%20and%20Yangqiu%20Song%0AAbstract%3A%20%20%20Multi-agent%20systems%20%28MAS%29%2C%20leveraging%20the%20remarkable%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20show%20great%20potential%20in%20addressing%20complex%20tasks.%20In%0Athis%20context%2C%20integrating%20MAS%20with%20legal%20tasks%20is%20a%20crucial%20step.%20While%0Aprevious%20studies%20have%20developed%20legal%20benchmarks%20for%20LLM%20agents%2C%20none%20are%0Aspecifically%20designed%20to%20consider%20the%20unique%20advantages%20of%20MAS%2C%20such%20as%20task%0Adecomposition%2C%20agent%20specialization%2C%20and%20flexible%20training.%20In%20fact%2C%20the%20lack%0Aof%20evaluation%20methods%20limits%20the%20potential%20of%20MAS%20in%20the%20legal%20domain.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20MASLegalBench%2C%20a%20legal%20benchmark%20tailored%20for%20MAS%0Aand%20designed%20with%20a%20deductive%20reasoning%20approach.%20Our%20benchmark%20uses%20GDPR%20as%0Athe%20application%20scenario%2C%20encompassing%20extensive%20background%20knowledge%20and%0Acovering%20complex%20reasoning%20processes%20that%20effectively%20reflect%20the%20intricacies%0Aof%20real-world%20legal%20situations.%20Furthermore%2C%20we%20manually%20design%20various%0Arole-based%20MAS%20and%20conduct%20extensive%20experiments%20using%20different%0Astate-of-the-art%20LLMs.%20Our%20results%20highlight%20the%20strengths%2C%20limitations%2C%20and%0Apotential%20areas%20for%20improvement%20of%20existing%20models%20and%20MAS%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.24922v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASLegalBench%253A%2520Benchmarking%2520Multi-Agent%2520Systems%2520in%2520Deductive%2520Legal%250A%2520%2520Reasoning%26entry.906535625%3DHuihao%2520Jing%2520and%2520Wenbin%2520Hu%2520and%2520Hongyu%2520Luo%2520and%2520Jianhui%2520Yang%2520and%2520Wei%2520Fan%2520and%2520Haoran%2520Li%2520and%2520Yangqiu%2520Song%26entry.1292438233%3D%2520%2520Multi-agent%2520systems%2520%2528MAS%2529%252C%2520leveraging%2520the%2520remarkable%2520capabilities%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520show%2520great%2520potential%2520in%2520addressing%2520complex%2520tasks.%2520In%250Athis%2520context%252C%2520integrating%2520MAS%2520with%2520legal%2520tasks%2520is%2520a%2520crucial%2520step.%2520While%250Aprevious%2520studies%2520have%2520developed%2520legal%2520benchmarks%2520for%2520LLM%2520agents%252C%2520none%2520are%250Aspecifically%2520designed%2520to%2520consider%2520the%2520unique%2520advantages%2520of%2520MAS%252C%2520such%2520as%2520task%250Adecomposition%252C%2520agent%2520specialization%252C%2520and%2520flexible%2520training.%2520In%2520fact%252C%2520the%2520lack%250Aof%2520evaluation%2520methods%2520limits%2520the%2520potential%2520of%2520MAS%2520in%2520the%2520legal%2520domain.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520MASLegalBench%252C%2520a%2520legal%2520benchmark%2520tailored%2520for%2520MAS%250Aand%2520designed%2520with%2520a%2520deductive%2520reasoning%2520approach.%2520Our%2520benchmark%2520uses%2520GDPR%2520as%250Athe%2520application%2520scenario%252C%2520encompassing%2520extensive%2520background%2520knowledge%2520and%250Acovering%2520complex%2520reasoning%2520processes%2520that%2520effectively%2520reflect%2520the%2520intricacies%250Aof%2520real-world%2520legal%2520situations.%2520Furthermore%252C%2520we%2520manually%2520design%2520various%250Arole-based%2520MAS%2520and%2520conduct%2520extensive%2520experiments%2520using%2520different%250Astate-of-the-art%2520LLMs.%2520Our%2520results%2520highlight%2520the%2520strengths%252C%2520limitations%252C%2520and%250Apotential%2520areas%2520for%2520improvement%2520of%2520existing%2520models%2520and%2520MAS%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24922v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASLegalBench%3A%20Benchmarking%20Multi-Agent%20Systems%20in%20Deductive%20Legal%0A%20%20Reasoning&entry.906535625=Huihao%20Jing%20and%20Wenbin%20Hu%20and%20Hongyu%20Luo%20and%20Jianhui%20Yang%20and%20Wei%20Fan%20and%20Haoran%20Li%20and%20Yangqiu%20Song&entry.1292438233=%20%20Multi-agent%20systems%20%28MAS%29%2C%20leveraging%20the%20remarkable%20capabilities%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20show%20great%20potential%20in%20addressing%20complex%20tasks.%20In%0Athis%20context%2C%20integrating%20MAS%20with%20legal%20tasks%20is%20a%20crucial%20step.%20While%0Aprevious%20studies%20have%20developed%20legal%20benchmarks%20for%20LLM%20agents%2C%20none%20are%0Aspecifically%20designed%20to%20consider%20the%20unique%20advantages%20of%20MAS%2C%20such%20as%20task%0Adecomposition%2C%20agent%20specialization%2C%20and%20flexible%20training.%20In%20fact%2C%20the%20lack%0Aof%20evaluation%20methods%20limits%20the%20potential%20of%20MAS%20in%20the%20legal%20domain.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20MASLegalBench%2C%20a%20legal%20benchmark%20tailored%20for%20MAS%0Aand%20designed%20with%20a%20deductive%20reasoning%20approach.%20Our%20benchmark%20uses%20GDPR%20as%0Athe%20application%20scenario%2C%20encompassing%20extensive%20background%20knowledge%20and%0Acovering%20complex%20reasoning%20processes%20that%20effectively%20reflect%20the%20intricacies%0Aof%20real-world%20legal%20situations.%20Furthermore%2C%20we%20manually%20design%20various%0Arole-based%20MAS%20and%20conduct%20extensive%20experiments%20using%20different%0Astate-of-the-art%20LLMs.%20Our%20results%20highlight%20the%20strengths%2C%20limitations%2C%20and%0Apotential%20areas%20for%20improvement%20of%20existing%20models%20and%20MAS%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.24922v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


