<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Learning Neural Volumetric Pose Features for Camera Localization", "author": "Jingyu Lin and Jiaqi Gu and Bojian Wu and Lubin Fan and Renjie Chen and Ligang Liu and Jieping Ye", "abstract": "  We introduce a novel neural volumetric pose feature, termed PoseMap, designed\nto enhance camera localization by encapsulating the information between images\nand the associated camera poses. Our framework leverages an Absolute Pose\nRegression (APR) architecture, together with an augmented NeRF module. This\nintegration not only facilitates the generation of novel views to enrich the\ntraining dataset but also enables the learning of effective pose features.\nAdditionally, we extend our architecture for self-supervised online alignment,\nallowing our method to be used and fine-tuned for unlabelled images within a\nunified framework. Experiments demonstrate that our method achieves 14.28% and\n20.51% performance gain on average in indoor and outdoor benchmark scenes,\noutperforming existing APR methods with state-of-the-art accuracy.\n", "link": "http://arxiv.org/abs/2403.12800v1", "date": "2024-03-19", "relevancy": 3.036, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6263}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5637}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Neural%20Volumetric%20Pose%20Features%20for%20Camera%20Localization&body=Title%3A%20Learning%20Neural%20Volumetric%20Pose%20Features%20for%20Camera%20Localization%0AAuthor%3A%20Jingyu%20Lin%20and%20Jiaqi%20Gu%20and%20Bojian%20Wu%20and%20Lubin%20Fan%20and%20Renjie%20Chen%20and%20Ligang%20Liu%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20neural%20volumetric%20pose%20feature%2C%20termed%20PoseMap%2C%20designed%0Ato%20enhance%20camera%20localization%20by%20encapsulating%20the%20information%20between%20images%0Aand%20the%20associated%20camera%20poses.%20Our%20framework%20leverages%20an%20Absolute%20Pose%0ARegression%20%28APR%29%20architecture%2C%20together%20with%20an%20augmented%20NeRF%20module.%20This%0Aintegration%20not%20only%20facilitates%20the%20generation%20of%20novel%20views%20to%20enrich%20the%0Atraining%20dataset%20but%20also%20enables%20the%20learning%20of%20effective%20pose%20features.%0AAdditionally%2C%20we%20extend%20our%20architecture%20for%20self-supervised%20online%20alignment%2C%0Aallowing%20our%20method%20to%20be%20used%20and%20fine-tuned%20for%20unlabelled%20images%20within%20a%0Aunified%20framework.%20Experiments%20demonstrate%20that%20our%20method%20achieves%2014.28%25%20and%0A20.51%25%20performance%20gain%20on%20average%20in%20indoor%20and%20outdoor%20benchmark%20scenes%2C%0Aoutperforming%20existing%20APR%20methods%20with%20state-of-the-art%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12800v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Neural%20Volumetric%20Pose%20Features%20for%20Camera%20Localization&entry.906535625=Jingyu%20Lin%20and%20Jiaqi%20Gu%20and%20Bojian%20Wu%20and%20Lubin%20Fan%20and%20Renjie%20Chen%20and%20Ligang%20Liu%20and%20Jieping%20Ye&entry.1292438233=%20%20We%20introduce%20a%20novel%20neural%20volumetric%20pose%20feature%2C%20termed%20PoseMap%2C%20designed%0Ato%20enhance%20camera%20localization%20by%20encapsulating%20the%20information%20between%20images%0Aand%20the%20associated%20camera%20poses.%20Our%20framework%20leverages%20an%20Absolute%20Pose%0ARegression%20%28APR%29%20architecture%2C%20together%20with%20an%20augmented%20NeRF%20module.%20This%0Aintegration%20not%20only%20facilitates%20the%20generation%20of%20novel%20views%20to%20enrich%20the%0Atraining%20dataset%20but%20also%20enables%20the%20learning%20of%20effective%20pose%20features.%0AAdditionally%2C%20we%20extend%20our%20architecture%20for%20self-supervised%20online%20alignment%2C%0Aallowing%20our%20method%20to%20be%20used%20and%20fine-tuned%20for%20unlabelled%20images%20within%20a%0Aunified%20framework.%20Experiments%20demonstrate%20that%20our%20method%20achieves%2014.28%25%20and%0A20.51%25%20performance%20gain%20on%20average%20in%20indoor%20and%20outdoor%20benchmark%20scenes%2C%0Aoutperforming%20existing%20APR%20methods%20with%20state-of-the-art%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12800v1&entry.124074799=Read"},
{"title": "Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and\n  Pose Estimation", "author": "Jingtao Sun and Yaonan Wang and Mingtao Feng and Chao Ding and Mike Zheng Shou and Ajmal Saeed Mian", "abstract": "  Fully-supervised category-level pose estimation aims to determine the 6-DoF\nposes of unseen instances from known categories, requiring expensive mannual\nlabeling costs. Recently, various self-supervised category-level pose\nestimation methods have been proposed to reduce the requirement of the\nannotated datasets. However, most methods rely on synthetic data or 3D CAD\nmodel for self-supervised training, and they are typically limited to\naddressing single-object pose problems without considering multi-objective\ntasks or shape reconstruction. To overcome these challenges and limitations, we\nintroduce a diffusion-driven self-supervised network for multi-object shape\nreconstruction and categorical pose estimation, only leveraging the shape\npriors. Specifically, to capture the SE(3)-equivariant pose features and 3D\nscale-invariant shape information, we present a Prior-Aware Pyramid 3D Point\nTransformer in our network. This module adopts a point convolutional layer with\nradial-kernels for pose-aware learning and a 3D scale-invariant graph\nconvolution layer for object-level shape representation, respectively.\nFurthermore, we introduce a pretrain-to-refine self-supervised training\nparadigm to train our network. It enables proposed network to capture the\nassociations between shape priors and observations, addressing the challenge of\nintra-class shape variations by utilising the diffusion mechanism. Extensive\nexperiments conducted on four public datasets and a self-built dataset\ndemonstrate that our method significantly outperforms state-of-the-art\nself-supervised category-level baselines and even surpasses some\nfully-supervised instance-level and category-level methods.\n", "link": "http://arxiv.org/abs/2403.12728v1", "date": "2024-03-19", "relevancy": 2.9485, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6149}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.581}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5733}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diffusion-Driven%20Self-Supervised%20Learning%20for%20Shape%20Reconstruction%20and%0A%20%20Pose%20Estimation&body=Title%3A%20Diffusion-Driven%20Self-Supervised%20Learning%20for%20Shape%20Reconstruction%20and%0A%20%20Pose%20Estimation%0AAuthor%3A%20Jingtao%20Sun%20and%20Yaonan%20Wang%20and%20Mingtao%20Feng%20and%20Chao%20Ding%20and%20Mike%20Zheng%20Shou%20and%20Ajmal%20Saeed%20Mian%0AAbstract%3A%20%20%20Fully-supervised%20category-level%20pose%20estimation%20aims%20to%20determine%20the%206-DoF%0Aposes%20of%20unseen%20instances%20from%20known%20categories%2C%20requiring%20expensive%20mannual%0Alabeling%20costs.%20Recently%2C%20various%20self-supervised%20category-level%20pose%0Aestimation%20methods%20have%20been%20proposed%20to%20reduce%20the%20requirement%20of%20the%0Aannotated%20datasets.%20However%2C%20most%20methods%20rely%20on%20synthetic%20data%20or%203D%20CAD%0Amodel%20for%20self-supervised%20training%2C%20and%20they%20are%20typically%20limited%20to%0Aaddressing%20single-object%20pose%20problems%20without%20considering%20multi-objective%0Atasks%20or%20shape%20reconstruction.%20To%20overcome%20these%20challenges%20and%20limitations%2C%20we%0Aintroduce%20a%20diffusion-driven%20self-supervised%20network%20for%20multi-object%20shape%0Areconstruction%20and%20categorical%20pose%20estimation%2C%20only%20leveraging%20the%20shape%0Apriors.%20Specifically%2C%20to%20capture%20the%20SE%283%29-equivariant%20pose%20features%20and%203D%0Ascale-invariant%20shape%20information%2C%20we%20present%20a%20Prior-Aware%20Pyramid%203D%20Point%0ATransformer%20in%20our%20network.%20This%20module%20adopts%20a%20point%20convolutional%20layer%20with%0Aradial-kernels%20for%20pose-aware%20learning%20and%20a%203D%20scale-invariant%20graph%0Aconvolution%20layer%20for%20object-level%20shape%20representation%2C%20respectively.%0AFurthermore%2C%20we%20introduce%20a%20pretrain-to-refine%20self-supervised%20training%0Aparadigm%20to%20train%20our%20network.%20It%20enables%20proposed%20network%20to%20capture%20the%0Aassociations%20between%20shape%20priors%20and%20observations%2C%20addressing%20the%20challenge%20of%0Aintra-class%20shape%20variations%20by%20utilising%20the%20diffusion%20mechanism.%20Extensive%0Aexperiments%20conducted%20on%20four%20public%20datasets%20and%20a%20self-built%20dataset%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aself-supervised%20category-level%20baselines%20and%20even%20surpasses%20some%0Afully-supervised%20instance-level%20and%20category-level%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12728v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Driven%20Self-Supervised%20Learning%20for%20Shape%20Reconstruction%20and%0A%20%20Pose%20Estimation&entry.906535625=Jingtao%20Sun%20and%20Yaonan%20Wang%20and%20Mingtao%20Feng%20and%20Chao%20Ding%20and%20Mike%20Zheng%20Shou%20and%20Ajmal%20Saeed%20Mian&entry.1292438233=%20%20Fully-supervised%20category-level%20pose%20estimation%20aims%20to%20determine%20the%206-DoF%0Aposes%20of%20unseen%20instances%20from%20known%20categories%2C%20requiring%20expensive%20mannual%0Alabeling%20costs.%20Recently%2C%20various%20self-supervised%20category-level%20pose%0Aestimation%20methods%20have%20been%20proposed%20to%20reduce%20the%20requirement%20of%20the%0Aannotated%20datasets.%20However%2C%20most%20methods%20rely%20on%20synthetic%20data%20or%203D%20CAD%0Amodel%20for%20self-supervised%20training%2C%20and%20they%20are%20typically%20limited%20to%0Aaddressing%20single-object%20pose%20problems%20without%20considering%20multi-objective%0Atasks%20or%20shape%20reconstruction.%20To%20overcome%20these%20challenges%20and%20limitations%2C%20we%0Aintroduce%20a%20diffusion-driven%20self-supervised%20network%20for%20multi-object%20shape%0Areconstruction%20and%20categorical%20pose%20estimation%2C%20only%20leveraging%20the%20shape%0Apriors.%20Specifically%2C%20to%20capture%20the%20SE%283%29-equivariant%20pose%20features%20and%203D%0Ascale-invariant%20shape%20information%2C%20we%20present%20a%20Prior-Aware%20Pyramid%203D%20Point%0ATransformer%20in%20our%20network.%20This%20module%20adopts%20a%20point%20convolutional%20layer%20with%0Aradial-kernels%20for%20pose-aware%20learning%20and%20a%203D%20scale-invariant%20graph%0Aconvolution%20layer%20for%20object-level%20shape%20representation%2C%20respectively.%0AFurthermore%2C%20we%20introduce%20a%20pretrain-to-refine%20self-supervised%20training%0Aparadigm%20to%20train%20our%20network.%20It%20enables%20proposed%20network%20to%20capture%20the%0Aassociations%20between%20shape%20priors%20and%20observations%2C%20addressing%20the%20challenge%20of%0Aintra-class%20shape%20variations%20by%20utilising%20the%20diffusion%20mechanism.%20Extensive%0Aexperiments%20conducted%20on%20four%20public%20datasets%20and%20a%20self-built%20dataset%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Aself-supervised%20category-level%20baselines%20and%20even%20surpasses%20some%0Afully-supervised%20instance-level%20and%20category-level%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12728v1&entry.124074799=Read"},
{"title": "Learning Cross-view Visual Geo-localization without Ground Truth", "author": "Haoyuan Li and Chang Xu and Wen Yang and Huai Yu and Gui-Song Xia", "abstract": "  Cross-View Geo-Localization (CVGL) involves determining the geographical\nlocation of a query image by matching it with a corresponding GPS-tagged\nreference image. Current state-of-the-art methods predominantly rely on\ntraining models with labeled paired images, incurring substantial annotation\ncosts and training burdens. In this study, we investigate the adaptation of\nfrozen models for CVGL without requiring ground truth pair labels. We observe\nthat training on unlabeled cross-view images presents significant challenges,\nincluding the need to establish relationships within unlabeled data and\nreconcile view discrepancies between uncertain queries and references. To\naddress these challenges, we propose a self-supervised learning framework to\ntrain a learnable adapter for a frozen Foundation Model (FM). This adapter is\ndesigned to map feature distributions from diverse views into a uniform space\nusing unlabeled data exclusively. To establish relationships within unlabeled\ndata, we introduce an Expectation-Maximization-based Pseudo-labeling module,\nwhich iteratively estimates associations between cross-view features and\noptimizes the adapter. To maintain the robustness of the FM's representation,\nwe incorporate an information consistency module with a reconstruction loss,\nensuring that adapted features retain strong discriminative ability across\nviews. Experimental results demonstrate that our proposed method achieves\nsignificant improvements over vanilla FMs and competitive accuracy compared to\nsupervised methods, while necessitating fewer training parameters and relying\nsolely on unlabeled data. Evaluation of our adaptation for task-specific models\nfurther highlights its broad applicability.\n", "link": "http://arxiv.org/abs/2403.12702v1", "date": "2024-03-19", "relevancy": 2.9257, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6053}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.575}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Cross-view%20Visual%20Geo-localization%20without%20Ground%20Truth&body=Title%3A%20Learning%20Cross-view%20Visual%20Geo-localization%20without%20Ground%20Truth%0AAuthor%3A%20Haoyuan%20Li%20and%20Chang%20Xu%20and%20Wen%20Yang%20and%20Huai%20Yu%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Cross-View%20Geo-Localization%20%28CVGL%29%20involves%20determining%20the%20geographical%0Alocation%20of%20a%20query%20image%20by%20matching%20it%20with%20a%20corresponding%20GPS-tagged%0Areference%20image.%20Current%20state-of-the-art%20methods%20predominantly%20rely%20on%0Atraining%20models%20with%20labeled%20paired%20images%2C%20incurring%20substantial%20annotation%0Acosts%20and%20training%20burdens.%20In%20this%20study%2C%20we%20investigate%20the%20adaptation%20of%0Afrozen%20models%20for%20CVGL%20without%20requiring%20ground%20truth%20pair%20labels.%20We%20observe%0Athat%20training%20on%20unlabeled%20cross-view%20images%20presents%20significant%20challenges%2C%0Aincluding%20the%20need%20to%20establish%20relationships%20within%20unlabeled%20data%20and%0Areconcile%20view%20discrepancies%20between%20uncertain%20queries%20and%20references.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20self-supervised%20learning%20framework%20to%0Atrain%20a%20learnable%20adapter%20for%20a%20frozen%20Foundation%20Model%20%28FM%29.%20This%20adapter%20is%0Adesigned%20to%20map%20feature%20distributions%20from%20diverse%20views%20into%20a%20uniform%20space%0Ausing%20unlabeled%20data%20exclusively.%20To%20establish%20relationships%20within%20unlabeled%0Adata%2C%20we%20introduce%20an%20Expectation-Maximization-based%20Pseudo-labeling%20module%2C%0Awhich%20iteratively%20estimates%20associations%20between%20cross-view%20features%20and%0Aoptimizes%20the%20adapter.%20To%20maintain%20the%20robustness%20of%20the%20FM%27s%20representation%2C%0Awe%20incorporate%20an%20information%20consistency%20module%20with%20a%20reconstruction%20loss%2C%0Aensuring%20that%20adapted%20features%20retain%20strong%20discriminative%20ability%20across%0Aviews.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%0Asignificant%20improvements%20over%20vanilla%20FMs%20and%20competitive%20accuracy%20compared%20to%0Asupervised%20methods%2C%20while%20necessitating%20fewer%20training%20parameters%20and%20relying%0Asolely%20on%20unlabeled%20data.%20Evaluation%20of%20our%20adaptation%20for%20task-specific%20models%0Afurther%20highlights%20its%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12702v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Cross-view%20Visual%20Geo-localization%20without%20Ground%20Truth&entry.906535625=Haoyuan%20Li%20and%20Chang%20Xu%20and%20Wen%20Yang%20and%20Huai%20Yu%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Cross-View%20Geo-Localization%20%28CVGL%29%20involves%20determining%20the%20geographical%0Alocation%20of%20a%20query%20image%20by%20matching%20it%20with%20a%20corresponding%20GPS-tagged%0Areference%20image.%20Current%20state-of-the-art%20methods%20predominantly%20rely%20on%0Atraining%20models%20with%20labeled%20paired%20images%2C%20incurring%20substantial%20annotation%0Acosts%20and%20training%20burdens.%20In%20this%20study%2C%20we%20investigate%20the%20adaptation%20of%0Afrozen%20models%20for%20CVGL%20without%20requiring%20ground%20truth%20pair%20labels.%20We%20observe%0Athat%20training%20on%20unlabeled%20cross-view%20images%20presents%20significant%20challenges%2C%0Aincluding%20the%20need%20to%20establish%20relationships%20within%20unlabeled%20data%20and%0Areconcile%20view%20discrepancies%20between%20uncertain%20queries%20and%20references.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20self-supervised%20learning%20framework%20to%0Atrain%20a%20learnable%20adapter%20for%20a%20frozen%20Foundation%20Model%20%28FM%29.%20This%20adapter%20is%0Adesigned%20to%20map%20feature%20distributions%20from%20diverse%20views%20into%20a%20uniform%20space%0Ausing%20unlabeled%20data%20exclusively.%20To%20establish%20relationships%20within%20unlabeled%0Adata%2C%20we%20introduce%20an%20Expectation-Maximization-based%20Pseudo-labeling%20module%2C%0Awhich%20iteratively%20estimates%20associations%20between%20cross-view%20features%20and%0Aoptimizes%20the%20adapter.%20To%20maintain%20the%20robustness%20of%20the%20FM%27s%20representation%2C%0Awe%20incorporate%20an%20information%20consistency%20module%20with%20a%20reconstruction%20loss%2C%0Aensuring%20that%20adapted%20features%20retain%20strong%20discriminative%20ability%20across%0Aviews.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%0Asignificant%20improvements%20over%20vanilla%20FMs%20and%20competitive%20accuracy%20compared%20to%0Asupervised%20methods%2C%20while%20necessitating%20fewer%20training%20parameters%20and%20relying%0Asolely%20on%20unlabeled%20data.%20Evaluation%20of%20our%20adaptation%20for%20task-specific%20models%0Afurther%20highlights%20its%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12702v1&entry.124074799=Read"},
{"title": "BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model", "author": "Yiran Song and Qianyu Zhou and Xiangtai Li and Deng-Ping Fan and Xuequan Lu and Lizhuang Ma", "abstract": "  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.\n", "link": "http://arxiv.org/abs/2401.02317v3", "date": "2024-03-19", "relevancy": 2.9237, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5977}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5844}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5722}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BA-SAM%3A%20Scalable%20Bias-Mode%20Attention%20Mask%20for%20Segment%20Anything%20Model&body=Title%3A%20BA-SAM%3A%20Scalable%20Bias-Mode%20Attention%20Mask%20for%20Segment%20Anything%20Model%0AAuthor%3A%20Yiran%20Song%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Deng-Ping%20Fan%20and%20Xuequan%20Lu%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20image%20resolution%20variation%20for%20the%0ASegment%20Anything%20Model%20%28SAM%29.%20SAM%2C%20known%20for%20its%20zero-shot%20generalizability%2C%0Aexhibits%20a%20performance%20degradation%20when%20faced%20with%20datasets%20with%20varying%20image%0Asizes.%20Previous%20approaches%20tend%20to%20resize%20the%20image%20to%20a%20fixed%20size%20or%20adopt%0Astructure%20modifications%2C%20hindering%20the%20preservation%20of%20SAM%27s%20rich%20prior%0Aknowledge.%20Besides%2C%20such%20task-specific%20tuning%20necessitates%20a%20complete%0Aretraining%20of%20the%20model%2C%20which%20is%20cost-expensive%20and%20unacceptable%20for%0Adeployment%20in%20the%20downstream%20tasks.%20In%20this%20paper%2C%20we%20reformulate%20this%20issue%20as%0Aa%20length%20extrapolation%20problem%2C%20where%20token%20sequence%20length%20varies%20while%0Amaintaining%20a%20consistent%20patch%20size%20for%20images%20of%20different%20sizes.%20To%20this%20end%2C%0Awe%20propose%20Scalable%20Bias-Mode%20Attention%20Mask%20%28BA-SAM%29%20to%20enhance%20SAM%27s%0Aadaptability%20to%20varying%20image%20resolutions%20while%20eliminating%20the%20need%20for%0Astructure%20modifications.%20Firstly%2C%20we%20introduce%20a%20new%20scaling%20factor%20to%20ensure%0Aconsistent%20magnitude%20in%20the%20attention%20layer%27s%20dot%20product%20values%20when%20the%20token%0Asequence%20length%20changes.%20Secondly%2C%20we%20present%20a%20bias-mode%20attention%20mask%20that%0Aallows%20each%20token%20to%20prioritize%20neighboring%20information%2C%20mitigating%20the%20impact%0Aof%20untrained%20distant%20information.%20Our%20BA-SAM%20demonstrates%20efficacy%20in%20two%0Ascenarios%3A%20zero-shot%20and%20fine-tuning.%20Extensive%20evaluation%20on%20diverse%20datasets%2C%0Aincluding%20DIS5K%2C%20DUTS%2C%20ISIC%2C%20COD10K%2C%20and%20COCO%2C%20reveals%20its%20ability%20to%0Asignificantly%20mitigate%20performance%20degradation%20in%20the%20zero-shot%20setting%20and%0Aachieve%20state-of-the-art%20performance%20with%20minimal%20fine-tuning.%20Furthermore%2C%20we%0Apropose%20a%20generalized%20model%20and%20benchmark%2C%20showcasing%20BA-SAM%27s%20generalizability%0Aacross%20all%20four%20datasets%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02317v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BA-SAM%3A%20Scalable%20Bias-Mode%20Attention%20Mask%20for%20Segment%20Anything%20Model&entry.906535625=Yiran%20Song%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Deng-Ping%20Fan%20and%20Xuequan%20Lu%20and%20Lizhuang%20Ma&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20image%20resolution%20variation%20for%20the%0ASegment%20Anything%20Model%20%28SAM%29.%20SAM%2C%20known%20for%20its%20zero-shot%20generalizability%2C%0Aexhibits%20a%20performance%20degradation%20when%20faced%20with%20datasets%20with%20varying%20image%0Asizes.%20Previous%20approaches%20tend%20to%20resize%20the%20image%20to%20a%20fixed%20size%20or%20adopt%0Astructure%20modifications%2C%20hindering%20the%20preservation%20of%20SAM%27s%20rich%20prior%0Aknowledge.%20Besides%2C%20such%20task-specific%20tuning%20necessitates%20a%20complete%0Aretraining%20of%20the%20model%2C%20which%20is%20cost-expensive%20and%20unacceptable%20for%0Adeployment%20in%20the%20downstream%20tasks.%20In%20this%20paper%2C%20we%20reformulate%20this%20issue%20as%0Aa%20length%20extrapolation%20problem%2C%20where%20token%20sequence%20length%20varies%20while%0Amaintaining%20a%20consistent%20patch%20size%20for%20images%20of%20different%20sizes.%20To%20this%20end%2C%0Awe%20propose%20Scalable%20Bias-Mode%20Attention%20Mask%20%28BA-SAM%29%20to%20enhance%20SAM%27s%0Aadaptability%20to%20varying%20image%20resolutions%20while%20eliminating%20the%20need%20for%0Astructure%20modifications.%20Firstly%2C%20we%20introduce%20a%20new%20scaling%20factor%20to%20ensure%0Aconsistent%20magnitude%20in%20the%20attention%20layer%27s%20dot%20product%20values%20when%20the%20token%0Asequence%20length%20changes.%20Secondly%2C%20we%20present%20a%20bias-mode%20attention%20mask%20that%0Aallows%20each%20token%20to%20prioritize%20neighboring%20information%2C%20mitigating%20the%20impact%0Aof%20untrained%20distant%20information.%20Our%20BA-SAM%20demonstrates%20efficacy%20in%20two%0Ascenarios%3A%20zero-shot%20and%20fine-tuning.%20Extensive%20evaluation%20on%20diverse%20datasets%2C%0Aincluding%20DIS5K%2C%20DUTS%2C%20ISIC%2C%20COD10K%2C%20and%20COCO%2C%20reveals%20its%20ability%20to%0Asignificantly%20mitigate%20performance%20degradation%20in%20the%20zero-shot%20setting%20and%0Aachieve%20state-of-the-art%20performance%20with%20minimal%20fine-tuning.%20Furthermore%2C%20we%0Apropose%20a%20generalized%20model%20and%20benchmark%2C%20showcasing%20BA-SAM%27s%20generalizability%0Aacross%20all%20four%20datasets%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02317v3&entry.124074799=Read"},
{"title": "Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language\n  Models", "author": "Ce Zhang and Simon Stepputtis and Katia Sycara and Yaqi Xie", "abstract": "  Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.\n", "link": "http://arxiv.org/abs/2403.12964v1", "date": "2024-03-19", "relevancy": 2.8957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Negative%20Yields%20Positive%3A%20Unified%20Dual-Path%20Adapter%20for%20Vision-Language%0A%20%20Models&body=Title%3A%20Negative%20Yields%20Positive%3A%20Unified%20Dual-Path%20Adapter%20for%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%0Ademonstrated%20great%20potential%20in%20learning%20open-world%20visual%20representations%2C%20and%0Aexhibit%20remarkable%20performance%20across%20a%20wide%20range%20of%20downstream%20tasks%20through%0Aefficient%20fine-tuning.%20In%20this%20work%2C%20we%20innovatively%20introduce%20the%20concept%20of%0Adual%20learning%20into%20fine-tuning%20VLMs%2C%20i.e.%2C%20we%20not%20only%20learn%20what%20an%20image%20is%2C%0Abut%20also%20what%20an%20image%20isn%27t.%20Building%20on%20this%20concept%2C%20we%20introduce%20a%20novel%0ADualAdapter%20approach%20to%20enable%20dual-path%20adaptation%20of%20VLMs%20from%20both%20positive%0Aand%20negative%20perspectives%20with%20only%20limited%20annotated%20samples.%20In%20the%20inference%0Astage%2C%20our%20DualAdapter%20performs%20unified%20predictions%20by%20simultaneously%0Aconducting%20complementary%20positive%20selection%20and%20negative%20exclusion%20across%0Atarget%20classes%2C%20thereby%20enhancing%20the%20overall%20recognition%20accuracy%20of%20VLMs%20in%0Adownstream%20tasks.%20Our%20extensive%20experimental%20results%20across%2015%20datasets%0Avalidate%20that%20the%20proposed%20DualAdapter%20outperforms%20existing%20state-of-the-art%0Amethods%20on%20both%20few-shot%20learning%20and%20domain%20generalization%20tasks%20while%0Aachieving%20competitive%20computational%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangce01/DualAdapter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12964v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Negative%20Yields%20Positive%3A%20Unified%20Dual-Path%20Adapter%20for%20Vision-Language%0A%20%20Models&entry.906535625=Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie&entry.1292438233=%20%20Recently%2C%20large-scale%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%0Ademonstrated%20great%20potential%20in%20learning%20open-world%20visual%20representations%2C%20and%0Aexhibit%20remarkable%20performance%20across%20a%20wide%20range%20of%20downstream%20tasks%20through%0Aefficient%20fine-tuning.%20In%20this%20work%2C%20we%20innovatively%20introduce%20the%20concept%20of%0Adual%20learning%20into%20fine-tuning%20VLMs%2C%20i.e.%2C%20we%20not%20only%20learn%20what%20an%20image%20is%2C%0Abut%20also%20what%20an%20image%20isn%27t.%20Building%20on%20this%20concept%2C%20we%20introduce%20a%20novel%0ADualAdapter%20approach%20to%20enable%20dual-path%20adaptation%20of%20VLMs%20from%20both%20positive%0Aand%20negative%20perspectives%20with%20only%20limited%20annotated%20samples.%20In%20the%20inference%0Astage%2C%20our%20DualAdapter%20performs%20unified%20predictions%20by%20simultaneously%0Aconducting%20complementary%20positive%20selection%20and%20negative%20exclusion%20across%0Atarget%20classes%2C%20thereby%20enhancing%20the%20overall%20recognition%20accuracy%20of%20VLMs%20in%0Adownstream%20tasks.%20Our%20extensive%20experimental%20results%20across%2015%20datasets%0Avalidate%20that%20the%20proposed%20DualAdapter%20outperforms%20existing%20state-of-the-art%0Amethods%20on%20both%20few-shot%20learning%20and%20domain%20generalization%20tasks%20while%0Aachieving%20competitive%20computational%20efficiency.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zhangce01/DualAdapter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12964v1&entry.124074799=Read"},
{"title": "Exploring Facial Expression Recognition through Semi-Supervised\n  Pretraining and Temporal Modeling", "author": "Jun Yu and Zhihong Wei and Zhongpeng Cai and Gongpeng Zhao and Zerui Zhang and Yongqi Wang and Guochen Xie and Jichao Zhu and Wangyuan Zhu", "abstract": "  Facial Expression Recognition (FER) plays a crucial role in computer vision\nand finds extensive applications across various fields. This paper aims to\npresent our approach for the upcoming 6th Affective Behavior Analysis\nin-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial\nexpression recognition task, The limited size of the FER dataset poses a\nchallenge to the expression recognition model's generalization ability,\nresulting in subpar recognition performance. To address this problem, we employ\na semi-supervised learning technique to generate expression category\npseudo-labels for unlabeled face data. At the same time, we uniformly sampled\nthe labeled facial expression samples and implemented a debiased feedback\nlearning strategy to address the problem of category imbalance in the dataset\nand the possible data bias in semi-supervised learning. Moreover, to further\ncompensate for the limitation and bias of features obtained only from static\nimages, we introduced a Temporal Encoder to learn and capture temporal\nrelationships between neighbouring expression image features. In the 6th ABAW\ncompetition, our method achieved outstanding results on the official validation\nset, a result that fully confirms the effectiveness and competitiveness of our\nproposed method.\n", "link": "http://arxiv.org/abs/2403.11942v2", "date": "2024-03-19", "relevancy": 2.7945, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Facial%20Expression%20Recognition%20through%20Semi-Supervised%0A%20%20Pretraining%20and%20Temporal%20Modeling&body=Title%3A%20Exploring%20Facial%20Expression%20Recognition%20through%20Semi-Supervised%0A%20%20Pretraining%20and%20Temporal%20Modeling%0AAuthor%3A%20Jun%20Yu%20and%20Zhihong%20Wei%20and%20Zhongpeng%20Cai%20and%20Gongpeng%20Zhao%20and%20Zerui%20Zhang%20and%20Yongqi%20Wang%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu%0AAbstract%3A%20%20%20Facial%20Expression%20Recognition%20%28FER%29%20plays%20a%20crucial%20role%20in%20computer%20vision%0Aand%20finds%20extensive%20applications%20across%20various%20fields.%20This%20paper%20aims%20to%0Apresent%20our%20approach%20for%20the%20upcoming%206th%20Affective%20Behavior%20Analysis%0Ain-the-Wild%20%28ABAW%29%20competition%2C%20scheduled%20to%20be%20held%20at%20CVPR2024.%20In%20the%20facial%0Aexpression%20recognition%20task%2C%20The%20limited%20size%20of%20the%20FER%20dataset%20poses%20a%0Achallenge%20to%20the%20expression%20recognition%20model%27s%20generalization%20ability%2C%0Aresulting%20in%20subpar%20recognition%20performance.%20To%20address%20this%20problem%2C%20we%20employ%0Aa%20semi-supervised%20learning%20technique%20to%20generate%20expression%20category%0Apseudo-labels%20for%20unlabeled%20face%20data.%20At%20the%20same%20time%2C%20we%20uniformly%20sampled%0Athe%20labeled%20facial%20expression%20samples%20and%20implemented%20a%20debiased%20feedback%0Alearning%20strategy%20to%20address%20the%20problem%20of%20category%20imbalance%20in%20the%20dataset%0Aand%20the%20possible%20data%20bias%20in%20semi-supervised%20learning.%20Moreover%2C%20to%20further%0Acompensate%20for%20the%20limitation%20and%20bias%20of%20features%20obtained%20only%20from%20static%0Aimages%2C%20we%20introduced%20a%20Temporal%20Encoder%20to%20learn%20and%20capture%20temporal%0Arelationships%20between%20neighbouring%20expression%20image%20features.%20In%20the%206th%20ABAW%0Acompetition%2C%20our%20method%20achieved%20outstanding%20results%20on%20the%20official%20validation%0Aset%2C%20a%20result%20that%20fully%20confirms%20the%20effectiveness%20and%20competitiveness%20of%20our%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11942v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Facial%20Expression%20Recognition%20through%20Semi-Supervised%0A%20%20Pretraining%20and%20Temporal%20Modeling&entry.906535625=Jun%20Yu%20and%20Zhihong%20Wei%20and%20Zhongpeng%20Cai%20and%20Gongpeng%20Zhao%20and%20Zerui%20Zhang%20and%20Yongqi%20Wang%20and%20Guochen%20Xie%20and%20Jichao%20Zhu%20and%20Wangyuan%20Zhu&entry.1292438233=%20%20Facial%20Expression%20Recognition%20%28FER%29%20plays%20a%20crucial%20role%20in%20computer%20vision%0Aand%20finds%20extensive%20applications%20across%20various%20fields.%20This%20paper%20aims%20to%0Apresent%20our%20approach%20for%20the%20upcoming%206th%20Affective%20Behavior%20Analysis%0Ain-the-Wild%20%28ABAW%29%20competition%2C%20scheduled%20to%20be%20held%20at%20CVPR2024.%20In%20the%20facial%0Aexpression%20recognition%20task%2C%20The%20limited%20size%20of%20the%20FER%20dataset%20poses%20a%0Achallenge%20to%20the%20expression%20recognition%20model%27s%20generalization%20ability%2C%0Aresulting%20in%20subpar%20recognition%20performance.%20To%20address%20this%20problem%2C%20we%20employ%0Aa%20semi-supervised%20learning%20technique%20to%20generate%20expression%20category%0Apseudo-labels%20for%20unlabeled%20face%20data.%20At%20the%20same%20time%2C%20we%20uniformly%20sampled%0Athe%20labeled%20facial%20expression%20samples%20and%20implemented%20a%20debiased%20feedback%0Alearning%20strategy%20to%20address%20the%20problem%20of%20category%20imbalance%20in%20the%20dataset%0Aand%20the%20possible%20data%20bias%20in%20semi-supervised%20learning.%20Moreover%2C%20to%20further%0Acompensate%20for%20the%20limitation%20and%20bias%20of%20features%20obtained%20only%20from%20static%0Aimages%2C%20we%20introduced%20a%20Temporal%20Encoder%20to%20learn%20and%20capture%20temporal%0Arelationships%20between%20neighbouring%20expression%20image%20features.%20In%20the%206th%20ABAW%0Acompetition%2C%20our%20method%20achieved%20outstanding%20results%20on%20the%20official%20validation%0Aset%2C%20a%20result%20that%20fully%20confirms%20the%20effectiveness%20and%20competitiveness%20of%20our%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11942v2&entry.124074799=Read"},
{"title": "Zero-Reference Low-Light Enhancement via Physical Quadruple Priors", "author": "Wenjing Wang and Huan Yang and Jianlong Fu and Jiaying Liu", "abstract": "  Understanding illumination and reducing the need for supervision pose a\nsignificant challenge in low-light enhancement. Current approaches are highly\nsensitive to data usage during training and illumination-specific\nhyper-parameters, limiting their ability to handle unseen scenarios. In this\npaper, we propose a new zero-reference low-light enhancement framework\ntrainable solely with normal light images. To accomplish this, we devise an\nillumination-invariant prior inspired by the theory of physical light transfer.\nThis prior serves as the bridge between normal and low-light images. Then, we\ndevelop a prior-to-image framework trained without low-light data. During\ntesting, this framework is able to restore our illumination-invariant prior\nback to images, automatically achieving low-light enhancement. Within this\nframework, we leverage a pretrained generative diffusion model for model\nability, introduce a bypass decoder to handle detail distortion, as well as\noffer a lightweight version for practicality. Extensive experiments demonstrate\nour framework's superiority in various scenarios as well as good\ninterpretability, robustness, and efficiency. Code is available on our project\nhomepage: http://daooshee.github.io/QuadPrior-Website/\n", "link": "http://arxiv.org/abs/2403.12933v1", "date": "2024-03-19", "relevancy": 2.7011, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5448}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-Reference%20Low-Light%20Enhancement%20via%20Physical%20Quadruple%20Priors&body=Title%3A%20Zero-Reference%20Low-Light%20Enhancement%20via%20Physical%20Quadruple%20Priors%0AAuthor%3A%20Wenjing%20Wang%20and%20Huan%20Yang%20and%20Jianlong%20Fu%20and%20Jiaying%20Liu%0AAbstract%3A%20%20%20Understanding%20illumination%20and%20reducing%20the%20need%20for%20supervision%20pose%20a%0Asignificant%20challenge%20in%20low-light%20enhancement.%20Current%20approaches%20are%20highly%0Asensitive%20to%20data%20usage%20during%20training%20and%20illumination-specific%0Ahyper-parameters%2C%20limiting%20their%20ability%20to%20handle%20unseen%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20zero-reference%20low-light%20enhancement%20framework%0Atrainable%20solely%20with%20normal%20light%20images.%20To%20accomplish%20this%2C%20we%20devise%20an%0Aillumination-invariant%20prior%20inspired%20by%20the%20theory%20of%20physical%20light%20transfer.%0AThis%20prior%20serves%20as%20the%20bridge%20between%20normal%20and%20low-light%20images.%20Then%2C%20we%0Adevelop%20a%20prior-to-image%20framework%20trained%20without%20low-light%20data.%20During%0Atesting%2C%20this%20framework%20is%20able%20to%20restore%20our%20illumination-invariant%20prior%0Aback%20to%20images%2C%20automatically%20achieving%20low-light%20enhancement.%20Within%20this%0Aframework%2C%20we%20leverage%20a%20pretrained%20generative%20diffusion%20model%20for%20model%0Aability%2C%20introduce%20a%20bypass%20decoder%20to%20handle%20detail%20distortion%2C%20as%20well%20as%0Aoffer%20a%20lightweight%20version%20for%20practicality.%20Extensive%20experiments%20demonstrate%0Aour%20framework%27s%20superiority%20in%20various%20scenarios%20as%20well%20as%20good%0Ainterpretability%2C%20robustness%2C%20and%20efficiency.%20Code%20is%20available%20on%20our%20project%0Ahomepage%3A%20http%3A//daooshee.github.io/QuadPrior-Website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Reference%20Low-Light%20Enhancement%20via%20Physical%20Quadruple%20Priors&entry.906535625=Wenjing%20Wang%20and%20Huan%20Yang%20and%20Jianlong%20Fu%20and%20Jiaying%20Liu&entry.1292438233=%20%20Understanding%20illumination%20and%20reducing%20the%20need%20for%20supervision%20pose%20a%0Asignificant%20challenge%20in%20low-light%20enhancement.%20Current%20approaches%20are%20highly%0Asensitive%20to%20data%20usage%20during%20training%20and%20illumination-specific%0Ahyper-parameters%2C%20limiting%20their%20ability%20to%20handle%20unseen%20scenarios.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20zero-reference%20low-light%20enhancement%20framework%0Atrainable%20solely%20with%20normal%20light%20images.%20To%20accomplish%20this%2C%20we%20devise%20an%0Aillumination-invariant%20prior%20inspired%20by%20the%20theory%20of%20physical%20light%20transfer.%0AThis%20prior%20serves%20as%20the%20bridge%20between%20normal%20and%20low-light%20images.%20Then%2C%20we%0Adevelop%20a%20prior-to-image%20framework%20trained%20without%20low-light%20data.%20During%0Atesting%2C%20this%20framework%20is%20able%20to%20restore%20our%20illumination-invariant%20prior%0Aback%20to%20images%2C%20automatically%20achieving%20low-light%20enhancement.%20Within%20this%0Aframework%2C%20we%20leverage%20a%20pretrained%20generative%20diffusion%20model%20for%20model%0Aability%2C%20introduce%20a%20bypass%20decoder%20to%20handle%20detail%20distortion%2C%20as%20well%20as%0Aoffer%20a%20lightweight%20version%20for%20practicality.%20Extensive%20experiments%20demonstrate%0Aour%20framework%27s%20superiority%20in%20various%20scenarios%20as%20well%20as%20good%0Ainterpretability%2C%20robustness%2C%20and%20efficiency.%20Code%20is%20available%20on%20our%20project%0Ahomepage%3A%20http%3A//daooshee.github.io/QuadPrior-Website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12933v1&entry.124074799=Read"},
{"title": "Vertical Federated Image Segmentation", "author": "Paul K. Mandal and Cole Leo", "abstract": "  With the popularization of AI solutions for image based problems, there has\nbeen a growing concern for both data privacy and acquisition. In a large number\nof cases, information is located on separate data silos and it can be difficult\nfor a developer to consolidate all of it in a fashion that is appropriate for\nmachine learning model development. Alongside this, a portion of these\nlocalized data regions may not have access to a labelled ground truth. This\nindicates that they have the capacity to reach conclusions numerically, but are\nnot able to assign classifications amid a lack of pertinent information. Such a\ndetermination is often negligible, especially when attempting to develop image\nbased solutions that often necessitate this capability. With this being the\ncase, we propose an innovative vertical federated learning (VFL) model\narchitecture that can operate under this common set of conditions. This is the\nfirst (and currently the only) implementation of a system that can work under\nthe constraints of a VFL environment and perform image segmentation while\nmaintaining nominal accuracies. We achieved this by utilizing an FCN that\nboasts the ability to operate on federates that lack labelled data and\nprivately share the respective weights with a central server, that of which\nhosts the necessary features for classification. Tests were conducted on the\nCamVid dataset in order to determine the impact of heavy feature compression\nrequired for the transfer of information between federates, as well as to reach\nnominal conclusions about the overall performance metrics when working under\nsuch constraints.\n", "link": "http://arxiv.org/abs/2401.07931v2", "date": "2024-03-19", "relevancy": 2.6482, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5278}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Vertical%20Federated%20Image%20Segmentation&body=Title%3A%20Vertical%20Federated%20Image%20Segmentation%0AAuthor%3A%20Paul%20K.%20Mandal%20and%20Cole%20Leo%0AAbstract%3A%20%20%20With%20the%20popularization%20of%20AI%20solutions%20for%20image%20based%20problems%2C%20there%20has%0Abeen%20a%20growing%20concern%20for%20both%20data%20privacy%20and%20acquisition.%20In%20a%20large%20number%0Aof%20cases%2C%20information%20is%20located%20on%20separate%20data%20silos%20and%20it%20can%20be%20difficult%0Afor%20a%20developer%20to%20consolidate%20all%20of%20it%20in%20a%20fashion%20that%20is%20appropriate%20for%0Amachine%20learning%20model%20development.%20Alongside%20this%2C%20a%20portion%20of%20these%0Alocalized%20data%20regions%20may%20not%20have%20access%20to%20a%20labelled%20ground%20truth.%20This%0Aindicates%20that%20they%20have%20the%20capacity%20to%20reach%20conclusions%20numerically%2C%20but%20are%0Anot%20able%20to%20assign%20classifications%20amid%20a%20lack%20of%20pertinent%20information.%20Such%20a%0Adetermination%20is%20often%20negligible%2C%20especially%20when%20attempting%20to%20develop%20image%0Abased%20solutions%20that%20often%20necessitate%20this%20capability.%20With%20this%20being%20the%0Acase%2C%20we%20propose%20an%20innovative%20vertical%20federated%20learning%20%28VFL%29%20model%0Aarchitecture%20that%20can%20operate%20under%20this%20common%20set%20of%20conditions.%20This%20is%20the%0Afirst%20%28and%20currently%20the%20only%29%20implementation%20of%20a%20system%20that%20can%20work%20under%0Athe%20constraints%20of%20a%20VFL%20environment%20and%20perform%20image%20segmentation%20while%0Amaintaining%20nominal%20accuracies.%20We%20achieved%20this%20by%20utilizing%20an%20FCN%20that%0Aboasts%20the%20ability%20to%20operate%20on%20federates%20that%20lack%20labelled%20data%20and%0Aprivately%20share%20the%20respective%20weights%20with%20a%20central%20server%2C%20that%20of%20which%0Ahosts%20the%20necessary%20features%20for%20classification.%20Tests%20were%20conducted%20on%20the%0ACamVid%20dataset%20in%20order%20to%20determine%20the%20impact%20of%20heavy%20feature%20compression%0Arequired%20for%20the%20transfer%20of%20information%20between%20federates%2C%20as%20well%20as%20to%20reach%0Anominal%20conclusions%20about%20the%20overall%20performance%20metrics%20when%20working%20under%0Asuch%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07931v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vertical%20Federated%20Image%20Segmentation&entry.906535625=Paul%20K.%20Mandal%20and%20Cole%20Leo&entry.1292438233=%20%20With%20the%20popularization%20of%20AI%20solutions%20for%20image%20based%20problems%2C%20there%20has%0Abeen%20a%20growing%20concern%20for%20both%20data%20privacy%20and%20acquisition.%20In%20a%20large%20number%0Aof%20cases%2C%20information%20is%20located%20on%20separate%20data%20silos%20and%20it%20can%20be%20difficult%0Afor%20a%20developer%20to%20consolidate%20all%20of%20it%20in%20a%20fashion%20that%20is%20appropriate%20for%0Amachine%20learning%20model%20development.%20Alongside%20this%2C%20a%20portion%20of%20these%0Alocalized%20data%20regions%20may%20not%20have%20access%20to%20a%20labelled%20ground%20truth.%20This%0Aindicates%20that%20they%20have%20the%20capacity%20to%20reach%20conclusions%20numerically%2C%20but%20are%0Anot%20able%20to%20assign%20classifications%20amid%20a%20lack%20of%20pertinent%20information.%20Such%20a%0Adetermination%20is%20often%20negligible%2C%20especially%20when%20attempting%20to%20develop%20image%0Abased%20solutions%20that%20often%20necessitate%20this%20capability.%20With%20this%20being%20the%0Acase%2C%20we%20propose%20an%20innovative%20vertical%20federated%20learning%20%28VFL%29%20model%0Aarchitecture%20that%20can%20operate%20under%20this%20common%20set%20of%20conditions.%20This%20is%20the%0Afirst%20%28and%20currently%20the%20only%29%20implementation%20of%20a%20system%20that%20can%20work%20under%0Athe%20constraints%20of%20a%20VFL%20environment%20and%20perform%20image%20segmentation%20while%0Amaintaining%20nominal%20accuracies.%20We%20achieved%20this%20by%20utilizing%20an%20FCN%20that%0Aboasts%20the%20ability%20to%20operate%20on%20federates%20that%20lack%20labelled%20data%20and%0Aprivately%20share%20the%20respective%20weights%20with%20a%20central%20server%2C%20that%20of%20which%0Ahosts%20the%20necessary%20features%20for%20classification.%20Tests%20were%20conducted%20on%20the%0ACamVid%20dataset%20in%20order%20to%20determine%20the%20impact%20of%20heavy%20feature%20compression%0Arequired%20for%20the%20transfer%20of%20information%20between%20federates%2C%20as%20well%20as%20to%20reach%0Anominal%20conclusions%20about%20the%20overall%20performance%20metrics%20when%20working%20under%0Asuch%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07931v2&entry.124074799=Read"},
{"title": "WHAC: World-grounded Humans and Cameras", "author": "Wanqi Yin and Zhongang Cai and Ruisi Wang and Fanzhou Wang and Chen Wei and Haiyi Mei and Weiye Xiao and Zhitao Yang and Qingping Sun and Atsushi Yamashita and Ziwei Liu and Lei Yang", "abstract": "  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n", "link": "http://arxiv.org/abs/2403.12959v1", "date": "2024-03-19", "relevancy": 2.6453, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5266}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5256}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WHAC%3A%20World-grounded%20Humans%20and%20Cameras&body=Title%3A%20WHAC%3A%20World-grounded%20Humans%20and%20Cameras%0AAuthor%3A%20Wanqi%20Yin%20and%20Zhongang%20Cai%20and%20Ruisi%20Wang%20and%20Fanzhou%20Wang%20and%20Chen%20Wei%20and%20Haiyi%20Mei%20and%20Weiye%20Xiao%20and%20Zhitao%20Yang%20and%20Qingping%20Sun%20and%20Atsushi%20Yamashita%20and%20Ziwei%20Liu%20and%20Lei%20Yang%0AAbstract%3A%20%20%20Estimating%20human%20and%20camera%20trajectories%20with%20accurate%20scale%20in%20the%20world%0Acoordinate%20system%20from%20a%20monocular%20video%20is%20a%20highly%20desirable%20yet%20challenging%0Aand%20ill-posed%20problem.%20In%20this%20study%2C%20we%20aim%20to%20recover%20expressive%20parametric%0Ahuman%20models%20%28i.e.%2C%20SMPL-X%29%20and%20corresponding%20camera%20poses%20jointly%2C%20by%0Aleveraging%20the%20synergy%20between%20three%20critical%20players%3A%20the%20world%2C%20the%20human%2C%0Aand%20the%20camera.%20Our%20approach%20is%20founded%20on%20two%20key%20observations.%20Firstly%2C%0Acamera-frame%20SMPL-X%20estimation%20methods%20readily%20recover%20absolute%20human%20depth.%0ASecondly%2C%20human%20motions%20inherently%20provide%20absolute%20spatial%20cues.%20By%0Aintegrating%20these%20insights%2C%20we%20introduce%20a%20novel%20framework%2C%20referred%20to%20as%0AWHAC%2C%20to%20facilitate%20world-grounded%20expressive%20human%20pose%20and%20shape%20estimation%0A%28EHPS%29%20alongside%20camera%20pose%20estimation%2C%20without%20relying%20on%20traditional%0Aoptimization%20techniques.%20Additionally%2C%20we%20present%20a%20new%20synthetic%20dataset%2C%0AWHAC-A-Mole%2C%20which%20includes%20accurately%20annotated%20humans%20and%20cameras%2C%20and%0Afeatures%20diverse%20interactive%20human%20motions%20as%20well%20as%20realistic%20camera%0Atrajectories.%20Extensive%20experiments%20on%20both%20standard%20and%20newly%20established%0Abenchmarks%20highlight%20the%20superiority%20and%20efficacy%20of%20our%20framework.%20We%20will%0Amake%20the%20code%20and%20dataset%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12959v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WHAC%3A%20World-grounded%20Humans%20and%20Cameras&entry.906535625=Wanqi%20Yin%20and%20Zhongang%20Cai%20and%20Ruisi%20Wang%20and%20Fanzhou%20Wang%20and%20Chen%20Wei%20and%20Haiyi%20Mei%20and%20Weiye%20Xiao%20and%20Zhitao%20Yang%20and%20Qingping%20Sun%20and%20Atsushi%20Yamashita%20and%20Ziwei%20Liu%20and%20Lei%20Yang&entry.1292438233=%20%20Estimating%20human%20and%20camera%20trajectories%20with%20accurate%20scale%20in%20the%20world%0Acoordinate%20system%20from%20a%20monocular%20video%20is%20a%20highly%20desirable%20yet%20challenging%0Aand%20ill-posed%20problem.%20In%20this%20study%2C%20we%20aim%20to%20recover%20expressive%20parametric%0Ahuman%20models%20%28i.e.%2C%20SMPL-X%29%20and%20corresponding%20camera%20poses%20jointly%2C%20by%0Aleveraging%20the%20synergy%20between%20three%20critical%20players%3A%20the%20world%2C%20the%20human%2C%0Aand%20the%20camera.%20Our%20approach%20is%20founded%20on%20two%20key%20observations.%20Firstly%2C%0Acamera-frame%20SMPL-X%20estimation%20methods%20readily%20recover%20absolute%20human%20depth.%0ASecondly%2C%20human%20motions%20inherently%20provide%20absolute%20spatial%20cues.%20By%0Aintegrating%20these%20insights%2C%20we%20introduce%20a%20novel%20framework%2C%20referred%20to%20as%0AWHAC%2C%20to%20facilitate%20world-grounded%20expressive%20human%20pose%20and%20shape%20estimation%0A%28EHPS%29%20alongside%20camera%20pose%20estimation%2C%20without%20relying%20on%20traditional%0Aoptimization%20techniques.%20Additionally%2C%20we%20present%20a%20new%20synthetic%20dataset%2C%0AWHAC-A-Mole%2C%20which%20includes%20accurately%20annotated%20humans%20and%20cameras%2C%20and%0Afeatures%20diverse%20interactive%20human%20motions%20as%20well%20as%20realistic%20camera%0Atrajectories.%20Extensive%20experiments%20on%20both%20standard%20and%20newly%20established%0Abenchmarks%20highlight%20the%20superiority%20and%20efficacy%20of%20our%20framework.%20We%20will%0Amake%20the%20code%20and%20dataset%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12959v1&entry.124074799=Read"},
{"title": "Discover and Mitigate Multiple Biased Subgroups in Image Classifiers", "author": "Zeliang Zhang and Mingqian Feng and Zhiheng Li and Chenliang Xu", "abstract": "  Machine learning models can perform well on in-distribution data but often\nfail on biased subgroups that are underrepresented in the training data,\nhindering the robustness of models for reliable applications. Such subgroups\nare typically unknown due to the absence of subgroup labels. Discovering biased\nsubgroups is the key to understanding models' failure modes and further\nimproving models' robustness. Most previous works of subgroup discovery make an\nimplicit assumption that models only underperform on a single biased subgroup,\nwhich does not hold on in-the-wild data where multiple biased subgroups exist.\n  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),\na novel method to address a more challenging but also more practical problem of\ndiscovering multiple biased subgroups in image classifiers. Our approach\ndecomposes the image features into multiple components that represent multiple\nsubgroups. This decomposition is achieved via a bilinear dimension reduction\nmethod, Partial Least Square (PLS), guided by useful supervision from the image\nclassifier. We further interpret the semantic meaning of each subgroup\ncomponent by generating natural language descriptions using vision-language\nfoundation models. Finally, DIM mitigates multiple biased subgroups\nsimultaneously via two strategies, including the data- and model-centric\nstrategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate\nthe effectiveness of DIM in discovering and mitigating multiple biased\nsubgroups. Furthermore, DIM uncovers the failure modes of the classifier on\nHard ImageNet, showcasing its broader applicability to understanding model bias\nin image classifiers. The code is available at\nhttps://github.com/ZhangAIPI/DIM.\n", "link": "http://arxiv.org/abs/2403.12777v1", "date": "2024-03-19", "relevancy": 2.6439, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5133}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5103}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Discover%20and%20Mitigate%20Multiple%20Biased%20Subgroups%20in%20Image%20Classifiers&body=Title%3A%20Discover%20and%20Mitigate%20Multiple%20Biased%20Subgroups%20in%20Image%20Classifiers%0AAuthor%3A%20Zeliang%20Zhang%20and%20Mingqian%20Feng%20and%20Zhiheng%20Li%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Machine%20learning%20models%20can%20perform%20well%20on%20in-distribution%20data%20but%20often%0Afail%20on%20biased%20subgroups%20that%20are%20underrepresented%20in%20the%20training%20data%2C%0Ahindering%20the%20robustness%20of%20models%20for%20reliable%20applications.%20Such%20subgroups%0Aare%20typically%20unknown%20due%20to%20the%20absence%20of%20subgroup%20labels.%20Discovering%20biased%0Asubgroups%20is%20the%20key%20to%20understanding%20models%27%20failure%20modes%20and%20further%0Aimproving%20models%27%20robustness.%20Most%20previous%20works%20of%20subgroup%20discovery%20make%20an%0Aimplicit%20assumption%20that%20models%20only%20underperform%20on%20a%20single%20biased%20subgroup%2C%0Awhich%20does%20not%20hold%20on%20in-the-wild%20data%20where%20multiple%20biased%20subgroups%20exist.%0A%20%20In%20this%20work%2C%20we%20propose%20Decomposition%2C%20Interpretation%2C%20and%20Mitigation%20%28DIM%29%2C%0Aa%20novel%20method%20to%20address%20a%20more%20challenging%20but%20also%20more%20practical%20problem%20of%0Adiscovering%20multiple%20biased%20subgroups%20in%20image%20classifiers.%20Our%20approach%0Adecomposes%20the%20image%20features%20into%20multiple%20components%20that%20represent%20multiple%0Asubgroups.%20This%20decomposition%20is%20achieved%20via%20a%20bilinear%20dimension%20reduction%0Amethod%2C%20Partial%20Least%20Square%20%28PLS%29%2C%20guided%20by%20useful%20supervision%20from%20the%20image%0Aclassifier.%20We%20further%20interpret%20the%20semantic%20meaning%20of%20each%20subgroup%0Acomponent%20by%20generating%20natural%20language%20descriptions%20using%20vision-language%0Afoundation%20models.%20Finally%2C%20DIM%20mitigates%20multiple%20biased%20subgroups%0Asimultaneously%20via%20two%20strategies%2C%20including%20the%20data-%20and%20model-centric%0Astrategies.%20Extensive%20experiments%20on%20CIFAR-100%20and%20Breeds%20datasets%20demonstrate%0Athe%20effectiveness%20of%20DIM%20in%20discovering%20and%20mitigating%20multiple%20biased%0Asubgroups.%20Furthermore%2C%20DIM%20uncovers%20the%20failure%20modes%20of%20the%20classifier%20on%0AHard%20ImageNet%2C%20showcasing%20its%20broader%20applicability%20to%20understanding%20model%20bias%0Ain%20image%20classifiers.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhangAIPI/DIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12777v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discover%20and%20Mitigate%20Multiple%20Biased%20Subgroups%20in%20Image%20Classifiers&entry.906535625=Zeliang%20Zhang%20and%20Mingqian%20Feng%20and%20Zhiheng%20Li%20and%20Chenliang%20Xu&entry.1292438233=%20%20Machine%20learning%20models%20can%20perform%20well%20on%20in-distribution%20data%20but%20often%0Afail%20on%20biased%20subgroups%20that%20are%20underrepresented%20in%20the%20training%20data%2C%0Ahindering%20the%20robustness%20of%20models%20for%20reliable%20applications.%20Such%20subgroups%0Aare%20typically%20unknown%20due%20to%20the%20absence%20of%20subgroup%20labels.%20Discovering%20biased%0Asubgroups%20is%20the%20key%20to%20understanding%20models%27%20failure%20modes%20and%20further%0Aimproving%20models%27%20robustness.%20Most%20previous%20works%20of%20subgroup%20discovery%20make%20an%0Aimplicit%20assumption%20that%20models%20only%20underperform%20on%20a%20single%20biased%20subgroup%2C%0Awhich%20does%20not%20hold%20on%20in-the-wild%20data%20where%20multiple%20biased%20subgroups%20exist.%0A%20%20In%20this%20work%2C%20we%20propose%20Decomposition%2C%20Interpretation%2C%20and%20Mitigation%20%28DIM%29%2C%0Aa%20novel%20method%20to%20address%20a%20more%20challenging%20but%20also%20more%20practical%20problem%20of%0Adiscovering%20multiple%20biased%20subgroups%20in%20image%20classifiers.%20Our%20approach%0Adecomposes%20the%20image%20features%20into%20multiple%20components%20that%20represent%20multiple%0Asubgroups.%20This%20decomposition%20is%20achieved%20via%20a%20bilinear%20dimension%20reduction%0Amethod%2C%20Partial%20Least%20Square%20%28PLS%29%2C%20guided%20by%20useful%20supervision%20from%20the%20image%0Aclassifier.%20We%20further%20interpret%20the%20semantic%20meaning%20of%20each%20subgroup%0Acomponent%20by%20generating%20natural%20language%20descriptions%20using%20vision-language%0Afoundation%20models.%20Finally%2C%20DIM%20mitigates%20multiple%20biased%20subgroups%0Asimultaneously%20via%20two%20strategies%2C%20including%20the%20data-%20and%20model-centric%0Astrategies.%20Extensive%20experiments%20on%20CIFAR-100%20and%20Breeds%20datasets%20demonstrate%0Athe%20effectiveness%20of%20DIM%20in%20discovering%20and%20mitigating%20multiple%20biased%0Asubgroups.%20Furthermore%2C%20DIM%20uncovers%20the%20failure%20modes%20of%20the%20classifier%20on%0AHard%20ImageNet%2C%20showcasing%20its%20broader%20applicability%20to%20understanding%20model%20bias%0Ain%20image%20classifiers.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ZhangAIPI/DIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12777v1&entry.124074799=Read"},
{"title": "Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with\n  Dual-Phase Optimization", "author": "Ziming Wang and Shuang Lian and Yuhao Zhang and Xiaoxin Cui and Rui Yan and Huajin Tang", "abstract": "  Spiking neural networks (SNNs) operating with asynchronous discrete events\nshow higher energy efficiency with sparse computation. A popular approach for\nimplementing deep SNNs is ANN-SNN conversion combining both efficient training\nof ANNs and efficient inference of SNNs. However, the accuracy loss is usually\nnon-negligible, especially under a few time steps, which restricts the\napplications of SNN on latency-sensitive edge devices greatly. In this paper,\nwe first identify that such performance degradation stems from the\nmisrepresentation of the negative or overflow residual membrane potential in\nSNNs. Inspired by this, we decompose the conversion error into three parts:\nquantization error, clipping error, and residual membrane potential\nrepresentation error. With such insights, we propose a two-stage conversion\nalgorithm to minimize those errors respectively. Besides, We show each stage\nachieves significant performance gains in a complementary manner. By evaluating\non challenging datasets including CIFAR-10, CIFAR- 100 and ImageNet, the\nproposed method demonstrates the state-of-the-art performance in terms of\naccuracy, latency and energy preservation. Furthermore, our method is evaluated\nusing a more challenging object detection task, revealing notable gains in\nregression performance under ultra-low latency when compared to existing\nspike-based detection algorithms. Codes are available at\nhttps://github.com/Windere/snn-cvt-dual-phase.\n", "link": "http://arxiv.org/abs/2205.07473v3", "date": "2024-03-19", "relevancy": 2.6223, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.558}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5094}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Lossless%20ANN-SNN%20Conversion%20under%20Ultra-Low%20Latency%20with%0A%20%20Dual-Phase%20Optimization&body=Title%3A%20Towards%20Lossless%20ANN-SNN%20Conversion%20under%20Ultra-Low%20Latency%20with%0A%20%20Dual-Phase%20Optimization%0AAuthor%3A%20Ziming%20Wang%20and%20Shuang%20Lian%20and%20Yuhao%20Zhang%20and%20Xiaoxin%20Cui%20and%20Rui%20Yan%20and%20Huajin%20Tang%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20operating%20with%20asynchronous%20discrete%20events%0Ashow%20higher%20energy%20efficiency%20with%20sparse%20computation.%20A%20popular%20approach%20for%0Aimplementing%20deep%20SNNs%20is%20ANN-SNN%20conversion%20combining%20both%20efficient%20training%0Aof%20ANNs%20and%20efficient%20inference%20of%20SNNs.%20However%2C%20the%20accuracy%20loss%20is%20usually%0Anon-negligible%2C%20especially%20under%20a%20few%20time%20steps%2C%20which%20restricts%20the%0Aapplications%20of%20SNN%20on%20latency-sensitive%20edge%20devices%20greatly.%20In%20this%20paper%2C%0Awe%20first%20identify%20that%20such%20performance%20degradation%20stems%20from%20the%0Amisrepresentation%20of%20the%20negative%20or%20overflow%20residual%20membrane%20potential%20in%0ASNNs.%20Inspired%20by%20this%2C%20we%20decompose%20the%20conversion%20error%20into%20three%20parts%3A%0Aquantization%20error%2C%20clipping%20error%2C%20and%20residual%20membrane%20potential%0Arepresentation%20error.%20With%20such%20insights%2C%20we%20propose%20a%20two-stage%20conversion%0Aalgorithm%20to%20minimize%20those%20errors%20respectively.%20Besides%2C%20We%20show%20each%20stage%0Aachieves%20significant%20performance%20gains%20in%20a%20complementary%20manner.%20By%20evaluating%0Aon%20challenging%20datasets%20including%20CIFAR-10%2C%20CIFAR-%20100%20and%20ImageNet%2C%20the%0Aproposed%20method%20demonstrates%20the%20state-of-the-art%20performance%20in%20terms%20of%0Aaccuracy%2C%20latency%20and%20energy%20preservation.%20Furthermore%2C%20our%20method%20is%20evaluated%0Ausing%20a%20more%20challenging%20object%20detection%20task%2C%20revealing%20notable%20gains%20in%0Aregression%20performance%20under%20ultra-low%20latency%20when%20compared%20to%20existing%0Aspike-based%20detection%20algorithms.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Windere/snn-cvt-dual-phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.07473v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Lossless%20ANN-SNN%20Conversion%20under%20Ultra-Low%20Latency%20with%0A%20%20Dual-Phase%20Optimization&entry.906535625=Ziming%20Wang%20and%20Shuang%20Lian%20and%20Yuhao%20Zhang%20and%20Xiaoxin%20Cui%20and%20Rui%20Yan%20and%20Huajin%20Tang&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20operating%20with%20asynchronous%20discrete%20events%0Ashow%20higher%20energy%20efficiency%20with%20sparse%20computation.%20A%20popular%20approach%20for%0Aimplementing%20deep%20SNNs%20is%20ANN-SNN%20conversion%20combining%20both%20efficient%20training%0Aof%20ANNs%20and%20efficient%20inference%20of%20SNNs.%20However%2C%20the%20accuracy%20loss%20is%20usually%0Anon-negligible%2C%20especially%20under%20a%20few%20time%20steps%2C%20which%20restricts%20the%0Aapplications%20of%20SNN%20on%20latency-sensitive%20edge%20devices%20greatly.%20In%20this%20paper%2C%0Awe%20first%20identify%20that%20such%20performance%20degradation%20stems%20from%20the%0Amisrepresentation%20of%20the%20negative%20or%20overflow%20residual%20membrane%20potential%20in%0ASNNs.%20Inspired%20by%20this%2C%20we%20decompose%20the%20conversion%20error%20into%20three%20parts%3A%0Aquantization%20error%2C%20clipping%20error%2C%20and%20residual%20membrane%20potential%0Arepresentation%20error.%20With%20such%20insights%2C%20we%20propose%20a%20two-stage%20conversion%0Aalgorithm%20to%20minimize%20those%20errors%20respectively.%20Besides%2C%20We%20show%20each%20stage%0Aachieves%20significant%20performance%20gains%20in%20a%20complementary%20manner.%20By%20evaluating%0Aon%20challenging%20datasets%20including%20CIFAR-10%2C%20CIFAR-%20100%20and%20ImageNet%2C%20the%0Aproposed%20method%20demonstrates%20the%20state-of-the-art%20performance%20in%20terms%20of%0Aaccuracy%2C%20latency%20and%20energy%20preservation.%20Furthermore%2C%20our%20method%20is%20evaluated%0Ausing%20a%20more%20challenging%20object%20detection%20task%2C%20revealing%20notable%20gains%20in%0Aregression%20performance%20under%20ultra-low%20latency%20when%20compared%20to%20existing%0Aspike-based%20detection%20algorithms.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/Windere/snn-cvt-dual-phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.07473v3&entry.124074799=Read"},
{"title": "Selective Domain-Invariant Feature for Generalizable Deepfake Detection", "author": "Yingxin Lai and Guoqing Yang Yifan He and Zhiming Luo and Shaozi Li", "abstract": "  With diverse presentation forgery methods emerging continually, detecting the\nauthenticity of images has drawn growing attention. Although existing methods\nhave achieved impressive accuracy in training dataset detection, they still\nperform poorly in the unseen domain and suffer from forgery of irrelevant\ninformation such as background and identity, affecting generalizability. To\nsolve this problem, we proposed a novel framework Selective Domain-Invariant\nFeature (SDIF), which reduces the sensitivity to face forgery by fusing content\nfeatures and styles. Specifically, we first use a Farthest-Point Sampling (FPS)\ntraining strategy to construct a task-relevant style sample representation\nspace for fusing with content features. Then, we propose a dynamic feature\nextraction module to generate features with diverse styles to improve the\nperformance and effectiveness of the feature extractor. Finally, a domain\nseparation strategy is used to retain domain-related features to help\ndistinguish between real and fake faces. Both qualitative and quantitative\nresults in existing benchmarks and proposals demonstrate the effectiveness of\nour approach.\n", "link": "http://arxiv.org/abs/2403.12707v1", "date": "2024-03-19", "relevancy": 2.6187, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.551}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5237}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Selective%20Domain-Invariant%20Feature%20for%20Generalizable%20Deepfake%20Detection&body=Title%3A%20Selective%20Domain-Invariant%20Feature%20for%20Generalizable%20Deepfake%20Detection%0AAuthor%3A%20Yingxin%20Lai%20and%20Guoqing%20Yang%20Yifan%20He%20and%20Zhiming%20Luo%20and%20Shaozi%20Li%0AAbstract%3A%20%20%20With%20diverse%20presentation%20forgery%20methods%20emerging%20continually%2C%20detecting%20the%0Aauthenticity%20of%20images%20has%20drawn%20growing%20attention.%20Although%20existing%20methods%0Ahave%20achieved%20impressive%20accuracy%20in%20training%20dataset%20detection%2C%20they%20still%0Aperform%20poorly%20in%20the%20unseen%20domain%20and%20suffer%20from%20forgery%20of%20irrelevant%0Ainformation%20such%20as%20background%20and%20identity%2C%20affecting%20generalizability.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20framework%20Selective%20Domain-Invariant%0AFeature%20%28SDIF%29%2C%20which%20reduces%20the%20sensitivity%20to%20face%20forgery%20by%20fusing%20content%0Afeatures%20and%20styles.%20Specifically%2C%20we%20first%20use%20a%20Farthest-Point%20Sampling%20%28FPS%29%0Atraining%20strategy%20to%20construct%20a%20task-relevant%20style%20sample%20representation%0Aspace%20for%20fusing%20with%20content%20features.%20Then%2C%20we%20propose%20a%20dynamic%20feature%0Aextraction%20module%20to%20generate%20features%20with%20diverse%20styles%20to%20improve%20the%0Aperformance%20and%20effectiveness%20of%20the%20feature%20extractor.%20Finally%2C%20a%20domain%0Aseparation%20strategy%20is%20used%20to%20retain%20domain-related%20features%20to%20help%0Adistinguish%20between%20real%20and%20fake%20faces.%20Both%20qualitative%20and%20quantitative%0Aresults%20in%20existing%20benchmarks%20and%20proposals%20demonstrate%20the%20effectiveness%20of%0Aour%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12707v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Domain-Invariant%20Feature%20for%20Generalizable%20Deepfake%20Detection&entry.906535625=Yingxin%20Lai%20and%20Guoqing%20Yang%20Yifan%20He%20and%20Zhiming%20Luo%20and%20Shaozi%20Li&entry.1292438233=%20%20With%20diverse%20presentation%20forgery%20methods%20emerging%20continually%2C%20detecting%20the%0Aauthenticity%20of%20images%20has%20drawn%20growing%20attention.%20Although%20existing%20methods%0Ahave%20achieved%20impressive%20accuracy%20in%20training%20dataset%20detection%2C%20they%20still%0Aperform%20poorly%20in%20the%20unseen%20domain%20and%20suffer%20from%20forgery%20of%20irrelevant%0Ainformation%20such%20as%20background%20and%20identity%2C%20affecting%20generalizability.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20framework%20Selective%20Domain-Invariant%0AFeature%20%28SDIF%29%2C%20which%20reduces%20the%20sensitivity%20to%20face%20forgery%20by%20fusing%20content%0Afeatures%20and%20styles.%20Specifically%2C%20we%20first%20use%20a%20Farthest-Point%20Sampling%20%28FPS%29%0Atraining%20strategy%20to%20construct%20a%20task-relevant%20style%20sample%20representation%0Aspace%20for%20fusing%20with%20content%20features.%20Then%2C%20we%20propose%20a%20dynamic%20feature%0Aextraction%20module%20to%20generate%20features%20with%20diverse%20styles%20to%20improve%20the%0Aperformance%20and%20effectiveness%20of%20the%20feature%20extractor.%20Finally%2C%20a%20domain%0Aseparation%20strategy%20is%20used%20to%20retain%20domain-related%20features%20to%20help%0Adistinguish%20between%20real%20and%20fake%20faces.%20Both%20qualitative%20and%20quantitative%0Aresults%20in%20existing%20benchmarks%20and%20proposals%20demonstrate%20the%20effectiveness%20of%0Aour%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12707v1&entry.124074799=Read"},
{"title": "GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning", "author": "Rui Xu and Lei Xing and Shuai Shao and Lifei Zhao and Baodi Liu and Weifeng Liu and Yicong Zhou", "abstract": "  Few-shot learning (FSL), purposing to resolve the problem of data-scarce, has\nattracted considerable attention in recent years. A popular FSL framework\ncontains two phases: (i) the pre-train phase employs the base data to train a\nCNN-based feature extractor. (ii) the meta-test phase applies the frozen\nfeature extractor to novel data (novel data has different categories from base\ndata) and designs a classifier for recognition. To correct few-shot data\ndistribution, researchers propose Semi-Supervised Few-Shot Learning (SSFSL) by\nintroducing unlabeled data. Although SSFSL has been proved to achieve\noutstanding performances in the FSL community, there still exists a fundamental\nproblem: the pre-trained feature extractor can not adapt to the novel data\nflawlessly due to the cross-category setting. Usually, large amounts of noises\nare introduced to the novel feature. We dub it as Feature-Extractor-Maladaptive\n(FEM) problem. To tackle FEM, we make two efforts in this paper. First, we\npropose a novel label prediction method, Isolated Graph Learning (IGL). IGL\nintroduces the Laplacian operator to encode the raw data to graph space, which\nhelps reduce the dependence on features when classifying, and then project\ngraph representation to label space for prediction. The key point is that: IGL\ncan weaken the negative influence of noise from the feature representation\nperspective, and is also flexible to independently complete training and\ntesting procedures, which is suitable for SSFSL. Second, we propose Graph\nCo-Training (GCT) to tackle this challenge from a multi-modal fusion\nperspective by extending the proposed IGL to the co-training framework. GCT is\na semi-supervised method that exploits the unlabeled samples with two modal\nfeatures to crossly strengthen the IGL classifier.\n", "link": "http://arxiv.org/abs/2203.07738v4", "date": "2024-03-19", "relevancy": 2.5939, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4985}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GCT%3A%20Graph%20Co-Training%20for%20Semi-Supervised%20Few-Shot%20Learning&body=Title%3A%20GCT%3A%20Graph%20Co-Training%20for%20Semi-Supervised%20Few-Shot%20Learning%0AAuthor%3A%20Rui%20Xu%20and%20Lei%20Xing%20and%20Shuai%20Shao%20and%20Lifei%20Zhao%20and%20Baodi%20Liu%20and%20Weifeng%20Liu%20and%20Yicong%20Zhou%0AAbstract%3A%20%20%20Few-shot%20learning%20%28FSL%29%2C%20purposing%20to%20resolve%20the%20problem%20of%20data-scarce%2C%20has%0Aattracted%20considerable%20attention%20in%20recent%20years.%20A%20popular%20FSL%20framework%0Acontains%20two%20phases%3A%20%28i%29%20the%20pre-train%20phase%20employs%20the%20base%20data%20to%20train%20a%0ACNN-based%20feature%20extractor.%20%28ii%29%20the%20meta-test%20phase%20applies%20the%20frozen%0Afeature%20extractor%20to%20novel%20data%20%28novel%20data%20has%20different%20categories%20from%20base%0Adata%29%20and%20designs%20a%20classifier%20for%20recognition.%20To%20correct%20few-shot%20data%0Adistribution%2C%20researchers%20propose%20Semi-Supervised%20Few-Shot%20Learning%20%28SSFSL%29%20by%0Aintroducing%20unlabeled%20data.%20Although%20SSFSL%20has%20been%20proved%20to%20achieve%0Aoutstanding%20performances%20in%20the%20FSL%20community%2C%20there%20still%20exists%20a%20fundamental%0Aproblem%3A%20the%20pre-trained%20feature%20extractor%20can%20not%20adapt%20to%20the%20novel%20data%0Aflawlessly%20due%20to%20the%20cross-category%20setting.%20Usually%2C%20large%20amounts%20of%20noises%0Aare%20introduced%20to%20the%20novel%20feature.%20We%20dub%20it%20as%20Feature-Extractor-Maladaptive%0A%28FEM%29%20problem.%20To%20tackle%20FEM%2C%20we%20make%20two%20efforts%20in%20this%20paper.%20First%2C%20we%0Apropose%20a%20novel%20label%20prediction%20method%2C%20Isolated%20Graph%20Learning%20%28IGL%29.%20IGL%0Aintroduces%20the%20Laplacian%20operator%20to%20encode%20the%20raw%20data%20to%20graph%20space%2C%20which%0Ahelps%20reduce%20the%20dependence%20on%20features%20when%20classifying%2C%20and%20then%20project%0Agraph%20representation%20to%20label%20space%20for%20prediction.%20The%20key%20point%20is%20that%3A%20IGL%0Acan%20weaken%20the%20negative%20influence%20of%20noise%20from%20the%20feature%20representation%0Aperspective%2C%20and%20is%20also%20flexible%20to%20independently%20complete%20training%20and%0Atesting%20procedures%2C%20which%20is%20suitable%20for%20SSFSL.%20Second%2C%20we%20propose%20Graph%0ACo-Training%20%28GCT%29%20to%20tackle%20this%20challenge%20from%20a%20multi-modal%20fusion%0Aperspective%20by%20extending%20the%20proposed%20IGL%20to%20the%20co-training%20framework.%20GCT%20is%0Aa%20semi-supervised%20method%20that%20exploits%20the%20unlabeled%20samples%20with%20two%20modal%0Afeatures%20to%20crossly%20strengthen%20the%20IGL%20classifier.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.07738v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCT%3A%20Graph%20Co-Training%20for%20Semi-Supervised%20Few-Shot%20Learning&entry.906535625=Rui%20Xu%20and%20Lei%20Xing%20and%20Shuai%20Shao%20and%20Lifei%20Zhao%20and%20Baodi%20Liu%20and%20Weifeng%20Liu%20and%20Yicong%20Zhou&entry.1292438233=%20%20Few-shot%20learning%20%28FSL%29%2C%20purposing%20to%20resolve%20the%20problem%20of%20data-scarce%2C%20has%0Aattracted%20considerable%20attention%20in%20recent%20years.%20A%20popular%20FSL%20framework%0Acontains%20two%20phases%3A%20%28i%29%20the%20pre-train%20phase%20employs%20the%20base%20data%20to%20train%20a%0ACNN-based%20feature%20extractor.%20%28ii%29%20the%20meta-test%20phase%20applies%20the%20frozen%0Afeature%20extractor%20to%20novel%20data%20%28novel%20data%20has%20different%20categories%20from%20base%0Adata%29%20and%20designs%20a%20classifier%20for%20recognition.%20To%20correct%20few-shot%20data%0Adistribution%2C%20researchers%20propose%20Semi-Supervised%20Few-Shot%20Learning%20%28SSFSL%29%20by%0Aintroducing%20unlabeled%20data.%20Although%20SSFSL%20has%20been%20proved%20to%20achieve%0Aoutstanding%20performances%20in%20the%20FSL%20community%2C%20there%20still%20exists%20a%20fundamental%0Aproblem%3A%20the%20pre-trained%20feature%20extractor%20can%20not%20adapt%20to%20the%20novel%20data%0Aflawlessly%20due%20to%20the%20cross-category%20setting.%20Usually%2C%20large%20amounts%20of%20noises%0Aare%20introduced%20to%20the%20novel%20feature.%20We%20dub%20it%20as%20Feature-Extractor-Maladaptive%0A%28FEM%29%20problem.%20To%20tackle%20FEM%2C%20we%20make%20two%20efforts%20in%20this%20paper.%20First%2C%20we%0Apropose%20a%20novel%20label%20prediction%20method%2C%20Isolated%20Graph%20Learning%20%28IGL%29.%20IGL%0Aintroduces%20the%20Laplacian%20operator%20to%20encode%20the%20raw%20data%20to%20graph%20space%2C%20which%0Ahelps%20reduce%20the%20dependence%20on%20features%20when%20classifying%2C%20and%20then%20project%0Agraph%20representation%20to%20label%20space%20for%20prediction.%20The%20key%20point%20is%20that%3A%20IGL%0Acan%20weaken%20the%20negative%20influence%20of%20noise%20from%20the%20feature%20representation%0Aperspective%2C%20and%20is%20also%20flexible%20to%20independently%20complete%20training%20and%0Atesting%20procedures%2C%20which%20is%20suitable%20for%20SSFSL.%20Second%2C%20we%20propose%20Graph%0ACo-Training%20%28GCT%29%20to%20tackle%20this%20challenge%20from%20a%20multi-modal%20fusion%0Aperspective%20by%20extending%20the%20proposed%20IGL%20to%20the%20co-training%20framework.%20GCT%20is%0Aa%20semi-supervised%20method%20that%20exploits%20the%20unlabeled%20samples%20with%20two%20modal%0Afeatures%20to%20crossly%20strengthen%20the%20IGL%20classifier.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.07738v4&entry.124074799=Read"},
{"title": "Guiding Masked Representation Learning to Capture Spatio-Temporal\n  Relationship of Electrocardiogram", "author": "Yeongyeon Na and Minje Park and Yunwon Tae and Sunghoon Joo", "abstract": "  Electrocardiograms (ECG) are widely employed as a diagnostic tool for\nmonitoring electrical signals originating from a heart. Recent machine learning\nresearch efforts have focused on the application of screening various diseases\nusing ECG signals. However, adapting to the application of screening disease is\nchallenging in that labeled ECG data are limited. Achieving general\nrepresentation through self-supervised learning (SSL) is a well-known approach\nto overcome the scarcity of labeled data; however, a naive application of SSL\nto ECG data, without considering the spatial-temporal relationships inherent in\nECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\nspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\noutperforms other SSL baseline methods in various experimental settings for\narrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\nadaptable to various lead combinations. Through quantitative and qualitative\nanalysis, we show a spatio-temporal relationship within ECG data. Our code is\navailable at https://github.com/bakqui/ST-MEM.\n", "link": "http://arxiv.org/abs/2402.09450v3", "date": "2024-03-19", "relevancy": 2.5912, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5346}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.53}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram&body=Title%3A%20Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram%0AAuthor%3A%20Yeongyeon%20Na%20and%20Minje%20Park%20and%20Yunwon%20Tae%20and%20Sunghoon%20Joo%0AAbstract%3A%20%20%20Electrocardiograms%20%28ECG%29%20are%20widely%20employed%20as%20a%20diagnostic%20tool%20for%0Amonitoring%20electrical%20signals%20originating%20from%20a%20heart.%20Recent%20machine%20learning%0Aresearch%20efforts%20have%20focused%20on%20the%20application%20of%20screening%20various%20diseases%0Ausing%20ECG%20signals.%20However%2C%20adapting%20to%20the%20application%20of%20screening%20disease%20is%0Achallenging%20in%20that%20labeled%20ECG%20data%20are%20limited.%20Achieving%20general%0Arepresentation%20through%20self-supervised%20learning%20%28SSL%29%20is%20a%20well-known%20approach%0Ato%20overcome%20the%20scarcity%20of%20labeled%20data%3B%20however%2C%20a%20naive%20application%20of%20SSL%0Ato%20ECG%20data%2C%20without%20considering%20the%20spatial-temporal%20relationships%20inherent%20in%0AECG%20signals%2C%20may%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20ST-MEM%0A%28Spatio-Temporal%20Masked%20Electrocardiogram%20Modeling%29%2C%20designed%20to%20learn%0Aspatio-temporal%20features%20by%20reconstructing%20masked%2012-lead%20ECG%20data.%20ST-MEM%0Aoutperforms%20other%20SSL%20baseline%20methods%20in%20various%20experimental%20settings%20for%0Aarrhythmia%20classification%20tasks.%20Moreover%2C%20we%20demonstrate%20that%20ST-MEM%20is%0Aadaptable%20to%20various%20lead%20combinations.%20Through%20quantitative%20and%20qualitative%0Aanalysis%2C%20we%20show%20a%20spatio-temporal%20relationship%20within%20ECG%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bakqui/ST-MEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09450v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Masked%20Representation%20Learning%20to%20Capture%20Spatio-Temporal%0A%20%20Relationship%20of%20Electrocardiogram&entry.906535625=Yeongyeon%20Na%20and%20Minje%20Park%20and%20Yunwon%20Tae%20and%20Sunghoon%20Joo&entry.1292438233=%20%20Electrocardiograms%20%28ECG%29%20are%20widely%20employed%20as%20a%20diagnostic%20tool%20for%0Amonitoring%20electrical%20signals%20originating%20from%20a%20heart.%20Recent%20machine%20learning%0Aresearch%20efforts%20have%20focused%20on%20the%20application%20of%20screening%20various%20diseases%0Ausing%20ECG%20signals.%20However%2C%20adapting%20to%20the%20application%20of%20screening%20disease%20is%0Achallenging%20in%20that%20labeled%20ECG%20data%20are%20limited.%20Achieving%20general%0Arepresentation%20through%20self-supervised%20learning%20%28SSL%29%20is%20a%20well-known%20approach%0Ato%20overcome%20the%20scarcity%20of%20labeled%20data%3B%20however%2C%20a%20naive%20application%20of%20SSL%0Ato%20ECG%20data%2C%20without%20considering%20the%20spatial-temporal%20relationships%20inherent%20in%0AECG%20signals%2C%20may%20yield%20suboptimal%20results.%20In%20this%20paper%2C%20we%20introduce%20ST-MEM%0A%28Spatio-Temporal%20Masked%20Electrocardiogram%20Modeling%29%2C%20designed%20to%20learn%0Aspatio-temporal%20features%20by%20reconstructing%20masked%2012-lead%20ECG%20data.%20ST-MEM%0Aoutperforms%20other%20SSL%20baseline%20methods%20in%20various%20experimental%20settings%20for%0Aarrhythmia%20classification%20tasks.%20Moreover%2C%20we%20demonstrate%20that%20ST-MEM%20is%0Aadaptable%20to%20various%20lead%20combinations.%20Through%20quantitative%20and%20qualitative%0Aanalysis%2C%20we%20show%20a%20spatio-temporal%20relationship%20within%20ECG%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/bakqui/ST-MEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09450v3&entry.124074799=Read"},
{"title": "Rethinking of Encoder-based Warm-start Methods in Hyperparameter\n  Optimization", "author": "Dawid P\u0142udowski and Antoni Zajko and Anna Kozak and Katarzyna Wo\u017anica", "abstract": "  Effectively representing heterogeneous tabular datasets for meta-learning\nremains an open problem. Previous approaches rely on predefined meta-features,\nfor example, statistical measures or landmarkers. Encoder-based models, such as\nDataset2Vec, allow us to extract significant meta-features automatically\nwithout human intervention. This research introduces a novel encoder-based\nrepresentation of tabular datasets implemented within the liltab package\navailable on GitHub https://github.com/azoz01/liltab. Our package is based on\nan established model for heterogeneous tabular data proposed in [Tomoharu Iwata\nand Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute\nSpaces. In Advances in Neural Information Processing Systems, 2020]. The\nproposed approach employs a different model for encoding feature relationships,\ngenerating alternative representations compared to existing methods like\nDataset2Vec. Both of them leverage the fundamental assumption of dataset\nsimilarity learning. In this work, we evaluate Dataset2Vec and liltab on two\ncommon meta-tasks - representing entire datasets and hyperparameter\noptimization warm-start. However, validation on an independent metaMIMIC\ndataset highlights the nuanced challenges in representation learning. We show\nthat general representations may not suffice for some meta-tasks where\nrequirements are not explicitly considered during extraction.\n", "link": "http://arxiv.org/abs/2403.04720v2", "date": "2024-03-19", "relevancy": 2.5832, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5096}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&body=Title%3A%20Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization%0AAuthor%3A%20Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica%0AAbstract%3A%20%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Aremains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%20meta-features%2C%0Afor%20example%2C%20statistical%20measures%20or%20landmarkers.%20Encoder-based%20models%2C%20such%20as%0ADataset2Vec%2C%20allow%20us%20to%20extract%20significant%20meta-features%20automatically%0Awithout%20human%20intervention.%20This%20research%20introduces%20a%20novel%20encoder-based%0Arepresentation%20of%20tabular%20datasets%20implemented%20within%20the%20liltab%20package%0Aavailable%20on%20GitHub%20https%3A//github.com/azoz01/liltab.%20Our%20package%20is%20based%20on%0Aan%20established%20model%20for%20heterogeneous%20tabular%20data%20proposed%20in%20%5BTomoharu%20Iwata%0Aand%20Atsutoshi%20Kumagai.%20Meta-learning%20from%20Tasks%20with%20Heterogeneous%20Attribute%0ASpaces.%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%202020%5D.%20The%0Aproposed%20approach%20employs%20a%20different%20model%20for%20encoding%20feature%20relationships%2C%0Agenerating%20alternative%20representations%20compared%20to%20existing%20methods%20like%0ADataset2Vec.%20Both%20of%20them%20leverage%20the%20fundamental%20assumption%20of%20dataset%0Asimilarity%20learning.%20In%20this%20work%2C%20we%20evaluate%20Dataset2Vec%20and%20liltab%20on%20two%0Acommon%20meta-tasks%20-%20representing%20entire%20datasets%20and%20hyperparameter%0Aoptimization%20warm-start.%20However%2C%20validation%20on%20an%20independent%20metaMIMIC%0Adataset%20highlights%20the%20nuanced%20challenges%20in%20representation%20learning.%20We%20show%0Athat%20general%20representations%20may%20not%20suffice%20for%20some%20meta-tasks%20where%0Arequirements%20are%20not%20explicitly%20considered%20during%20extraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04720v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20of%20Encoder-based%20Warm-start%20Methods%20in%20Hyperparameter%0A%20%20Optimization&entry.906535625=Dawid%20P%C5%82udowski%20and%20Antoni%20Zajko%20and%20Anna%20Kozak%20and%20Katarzyna%20Wo%C5%BAnica&entry.1292438233=%20%20Effectively%20representing%20heterogeneous%20tabular%20datasets%20for%20meta-learning%0Aremains%20an%20open%20problem.%20Previous%20approaches%20rely%20on%20predefined%20meta-features%2C%0Afor%20example%2C%20statistical%20measures%20or%20landmarkers.%20Encoder-based%20models%2C%20such%20as%0ADataset2Vec%2C%20allow%20us%20to%20extract%20significant%20meta-features%20automatically%0Awithout%20human%20intervention.%20This%20research%20introduces%20a%20novel%20encoder-based%0Arepresentation%20of%20tabular%20datasets%20implemented%20within%20the%20liltab%20package%0Aavailable%20on%20GitHub%20https%3A//github.com/azoz01/liltab.%20Our%20package%20is%20based%20on%0Aan%20established%20model%20for%20heterogeneous%20tabular%20data%20proposed%20in%20%5BTomoharu%20Iwata%0Aand%20Atsutoshi%20Kumagai.%20Meta-learning%20from%20Tasks%20with%20Heterogeneous%20Attribute%0ASpaces.%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%2C%202020%5D.%20The%0Aproposed%20approach%20employs%20a%20different%20model%20for%20encoding%20feature%20relationships%2C%0Agenerating%20alternative%20representations%20compared%20to%20existing%20methods%20like%0ADataset2Vec.%20Both%20of%20them%20leverage%20the%20fundamental%20assumption%20of%20dataset%0Asimilarity%20learning.%20In%20this%20work%2C%20we%20evaluate%20Dataset2Vec%20and%20liltab%20on%20two%0Acommon%20meta-tasks%20-%20representing%20entire%20datasets%20and%20hyperparameter%0Aoptimization%20warm-start.%20However%2C%20validation%20on%20an%20independent%20metaMIMIC%0Adataset%20highlights%20the%20nuanced%20challenges%20in%20representation%20learning.%20We%20show%0Athat%20general%20representations%20may%20not%20suffice%20for%20some%20meta-tasks%20where%0Arequirements%20are%20not%20explicitly%20considered%20during%20extraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04720v2&entry.124074799=Read"},
{"title": "Adaptive Multilevel Neural Networks for Parametric PDEs with Error\n  Estimation", "author": "Janina E. Sch\u00fctte and Martin Eigel", "abstract": "  To solve high-dimensional parameter-dependent partial differential equations\n(pPDEs), a neural network architecture is presented. It is constructed to map\nparameters of the model data to corresponding finite element solutions. To\nimprove training efficiency and to enable control of the approximation error,\nthe network mimics an adaptive finite element method (AFEM). It outputs a\ncoarse grid solution and a series of corrections as produced in an AFEM,\nallowing a tracking of the error decay over successive layers of the network.\nThe observed errors are measured by a reliable residual based a posteriori\nerror estimator, enabling the reduction to only few parameters for the\napproximation in the output of the network. This leads to a problem adapted\nrepresentation of the solution on locally refined grids. Furthermore, each\nsolution of the AFEM is discretized in a hierarchical basis. For the\narchitecture, convolutional neural networks (CNNs) are chosen. The hierarchical\nbasis then allows to handle sparse images for finely discretized meshes.\nAdditionally, as corrections on finer levels decrease in amplitude, i.e.,\nimportance for the overall approximation, the accuracy of the network\napproximation is allowed to decrease successively. This can either be\nincorporated in the number of generated high fidelity samples used for training\nor the size of the network components responsible for the fine grid outputs.\nThe architecture is described and preliminary numerical examples are presented.\n", "link": "http://arxiv.org/abs/2403.12650v1", "date": "2024-03-19", "relevancy": 2.5694, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5328}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5208}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Multilevel%20Neural%20Networks%20for%20Parametric%20PDEs%20with%20Error%0A%20%20Estimation&body=Title%3A%20Adaptive%20Multilevel%20Neural%20Networks%20for%20Parametric%20PDEs%20with%20Error%0A%20%20Estimation%0AAuthor%3A%20Janina%20E.%20Sch%C3%BCtte%20and%20Martin%20Eigel%0AAbstract%3A%20%20%20To%20solve%20high-dimensional%20parameter-dependent%20partial%20differential%20equations%0A%28pPDEs%29%2C%20a%20neural%20network%20architecture%20is%20presented.%20It%20is%20constructed%20to%20map%0Aparameters%20of%20the%20model%20data%20to%20corresponding%20finite%20element%20solutions.%20To%0Aimprove%20training%20efficiency%20and%20to%20enable%20control%20of%20the%20approximation%20error%2C%0Athe%20network%20mimics%20an%20adaptive%20finite%20element%20method%20%28AFEM%29.%20It%20outputs%20a%0Acoarse%20grid%20solution%20and%20a%20series%20of%20corrections%20as%20produced%20in%20an%20AFEM%2C%0Aallowing%20a%20tracking%20of%20the%20error%20decay%20over%20successive%20layers%20of%20the%20network.%0AThe%20observed%20errors%20are%20measured%20by%20a%20reliable%20residual%20based%20a%20posteriori%0Aerror%20estimator%2C%20enabling%20the%20reduction%20to%20only%20few%20parameters%20for%20the%0Aapproximation%20in%20the%20output%20of%20the%20network.%20This%20leads%20to%20a%20problem%20adapted%0Arepresentation%20of%20the%20solution%20on%20locally%20refined%20grids.%20Furthermore%2C%20each%0Asolution%20of%20the%20AFEM%20is%20discretized%20in%20a%20hierarchical%20basis.%20For%20the%0Aarchitecture%2C%20convolutional%20neural%20networks%20%28CNNs%29%20are%20chosen.%20The%20hierarchical%0Abasis%20then%20allows%20to%20handle%20sparse%20images%20for%20finely%20discretized%20meshes.%0AAdditionally%2C%20as%20corrections%20on%20finer%20levels%20decrease%20in%20amplitude%2C%20i.e.%2C%0Aimportance%20for%20the%20overall%20approximation%2C%20the%20accuracy%20of%20the%20network%0Aapproximation%20is%20allowed%20to%20decrease%20successively.%20This%20can%20either%20be%0Aincorporated%20in%20the%20number%20of%20generated%20high%20fidelity%20samples%20used%20for%20training%0Aor%20the%20size%20of%20the%20network%20components%20responsible%20for%20the%20fine%20grid%20outputs.%0AThe%20architecture%20is%20described%20and%20preliminary%20numerical%20examples%20are%20presented.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Multilevel%20Neural%20Networks%20for%20Parametric%20PDEs%20with%20Error%0A%20%20Estimation&entry.906535625=Janina%20E.%20Sch%C3%BCtte%20and%20Martin%20Eigel&entry.1292438233=%20%20To%20solve%20high-dimensional%20parameter-dependent%20partial%20differential%20equations%0A%28pPDEs%29%2C%20a%20neural%20network%20architecture%20is%20presented.%20It%20is%20constructed%20to%20map%0Aparameters%20of%20the%20model%20data%20to%20corresponding%20finite%20element%20solutions.%20To%0Aimprove%20training%20efficiency%20and%20to%20enable%20control%20of%20the%20approximation%20error%2C%0Athe%20network%20mimics%20an%20adaptive%20finite%20element%20method%20%28AFEM%29.%20It%20outputs%20a%0Acoarse%20grid%20solution%20and%20a%20series%20of%20corrections%20as%20produced%20in%20an%20AFEM%2C%0Aallowing%20a%20tracking%20of%20the%20error%20decay%20over%20successive%20layers%20of%20the%20network.%0AThe%20observed%20errors%20are%20measured%20by%20a%20reliable%20residual%20based%20a%20posteriori%0Aerror%20estimator%2C%20enabling%20the%20reduction%20to%20only%20few%20parameters%20for%20the%0Aapproximation%20in%20the%20output%20of%20the%20network.%20This%20leads%20to%20a%20problem%20adapted%0Arepresentation%20of%20the%20solution%20on%20locally%20refined%20grids.%20Furthermore%2C%20each%0Asolution%20of%20the%20AFEM%20is%20discretized%20in%20a%20hierarchical%20basis.%20For%20the%0Aarchitecture%2C%20convolutional%20neural%20networks%20%28CNNs%29%20are%20chosen.%20The%20hierarchical%0Abasis%20then%20allows%20to%20handle%20sparse%20images%20for%20finely%20discretized%20meshes.%0AAdditionally%2C%20as%20corrections%20on%20finer%20levels%20decrease%20in%20amplitude%2C%20i.e.%2C%0Aimportance%20for%20the%20overall%20approximation%2C%20the%20accuracy%20of%20the%20network%0Aapproximation%20is%20allowed%20to%20decrease%20successively.%20This%20can%20either%20be%0Aincorporated%20in%20the%20number%20of%20generated%20high%20fidelity%20samples%20used%20for%20training%0Aor%20the%20size%20of%20the%20network%20components%20responsible%20for%20the%20fine%20grid%20outputs.%0AThe%20architecture%20is%20described%20and%20preliminary%20numerical%20examples%20are%20presented.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12650v1&entry.124074799=Read"},
{"title": "Tuning-Free Image Customization with Image and Text Guidance", "author": "Pengzhi Li and Qiang Nie and Ying Chen and Xi Jiang and Kai Wu and Yuhuan Lin and Yong Liu and Jinlong Peng and Chengjie Wang and Feng Zheng", "abstract": "  Despite significant advancements in image customization with diffusion\nmodels, current methods still have several limitations: 1) unintended changes\nin non-target areas when regenerating the entire image; 2) guidance solely by a\nreference image or text descriptions; and 3) time-consuming fine-tuning, which\nlimits their practical application. In response, we introduce a tuning-free\nframework for simultaneous text-image-guided image customization, enabling\nprecise editing of specific image regions within seconds. Our approach\npreserves the semantic features of the reference image subject while allowing\nmodification of detailed attributes based on text descriptions. To achieve\nthis, we propose an innovative attention blending strategy that blends\nself-attention features in the UNet decoder during the denoising process. To\nour knowledge, this is the first tuning-free method that concurrently utilizes\ntext and image guidance for image customization in specific regions. Our\napproach outperforms previous methods in both human and quantitative\nevaluations, providing an efficient solution for various practical\napplications, such as image synthesis, design, and creative photography.\n", "link": "http://arxiv.org/abs/2403.12658v1", "date": "2024-03-19", "relevancy": 2.565, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6672}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6567}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Image%20Customization%20with%20Image%20and%20Text%20Guidance&body=Title%3A%20Tuning-Free%20Image%20Customization%20with%20Image%20and%20Text%20Guidance%0AAuthor%3A%20Pengzhi%20Li%20and%20Qiang%20Nie%20and%20Ying%20Chen%20and%20Xi%20Jiang%20and%20Kai%20Wu%20and%20Yuhuan%20Lin%20and%20Yong%20Liu%20and%20Jinlong%20Peng%20and%20Chengjie%20Wang%20and%20Feng%20Zheng%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20image%20customization%20with%20diffusion%0Amodels%2C%20current%20methods%20still%20have%20several%20limitations%3A%201%29%20unintended%20changes%0Ain%20non-target%20areas%20when%20regenerating%20the%20entire%20image%3B%202%29%20guidance%20solely%20by%20a%0Areference%20image%20or%20text%20descriptions%3B%20and%203%29%20time-consuming%20fine-tuning%2C%20which%0Alimits%20their%20practical%20application.%20In%20response%2C%20we%20introduce%20a%20tuning-free%0Aframework%20for%20simultaneous%20text-image-guided%20image%20customization%2C%20enabling%0Aprecise%20editing%20of%20specific%20image%20regions%20within%20seconds.%20Our%20approach%0Apreserves%20the%20semantic%20features%20of%20the%20reference%20image%20subject%20while%20allowing%0Amodification%20of%20detailed%20attributes%20based%20on%20text%20descriptions.%20To%20achieve%0Athis%2C%20we%20propose%20an%20innovative%20attention%20blending%20strategy%20that%20blends%0Aself-attention%20features%20in%20the%20UNet%20decoder%20during%20the%20denoising%20process.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20tuning-free%20method%20that%20concurrently%20utilizes%0Atext%20and%20image%20guidance%20for%20image%20customization%20in%20specific%20regions.%20Our%0Aapproach%20outperforms%20previous%20methods%20in%20both%20human%20and%20quantitative%0Aevaluations%2C%20providing%20an%20efficient%20solution%20for%20various%20practical%0Aapplications%2C%20such%20as%20image%20synthesis%2C%20design%2C%20and%20creative%20photography.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12658v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Image%20Customization%20with%20Image%20and%20Text%20Guidance&entry.906535625=Pengzhi%20Li%20and%20Qiang%20Nie%20and%20Ying%20Chen%20and%20Xi%20Jiang%20and%20Kai%20Wu%20and%20Yuhuan%20Lin%20and%20Yong%20Liu%20and%20Jinlong%20Peng%20and%20Chengjie%20Wang%20and%20Feng%20Zheng&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20image%20customization%20with%20diffusion%0Amodels%2C%20current%20methods%20still%20have%20several%20limitations%3A%201%29%20unintended%20changes%0Ain%20non-target%20areas%20when%20regenerating%20the%20entire%20image%3B%202%29%20guidance%20solely%20by%20a%0Areference%20image%20or%20text%20descriptions%3B%20and%203%29%20time-consuming%20fine-tuning%2C%20which%0Alimits%20their%20practical%20application.%20In%20response%2C%20we%20introduce%20a%20tuning-free%0Aframework%20for%20simultaneous%20text-image-guided%20image%20customization%2C%20enabling%0Aprecise%20editing%20of%20specific%20image%20regions%20within%20seconds.%20Our%20approach%0Apreserves%20the%20semantic%20features%20of%20the%20reference%20image%20subject%20while%20allowing%0Amodification%20of%20detailed%20attributes%20based%20on%20text%20descriptions.%20To%20achieve%0Athis%2C%20we%20propose%20an%20innovative%20attention%20blending%20strategy%20that%20blends%0Aself-attention%20features%20in%20the%20UNet%20decoder%20during%20the%20denoising%20process.%20To%0Aour%20knowledge%2C%20this%20is%20the%20first%20tuning-free%20method%20that%20concurrently%20utilizes%0Atext%20and%20image%20guidance%20for%20image%20customization%20in%20specific%20regions.%20Our%0Aapproach%20outperforms%20previous%20methods%20in%20both%20human%20and%20quantitative%0Aevaluations%2C%20providing%20an%20efficient%20solution%20for%20various%20practical%0Aapplications%2C%20such%20as%20image%20synthesis%2C%20design%2C%20and%20creative%20photography.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12658v1&entry.124074799=Read"},
{"title": "The Training Process of Many Deep Networks Explores the Same\n  Low-Dimensional Manifold", "author": "Jialin Mao and Itay Griniasty and Han Kheng Teoh and Rahul Ramesh and Rubing Yang and Mark K. Transtrum and James P. Sethna and Pratik Chaudhari", "abstract": "  We develop information-geometric techniques to analyze the trajectories of\nthe predictions of deep networks during training. By examining the underlying\nhigh-dimensional probabilistic models, we reveal that the training process\nexplores an effectively low-dimensional manifold. Networks with a wide range of\narchitectures, sizes, trained using different optimization methods,\nregularization techniques, data augmentation techniques, and weight\ninitializations lie on the same manifold in the prediction space. We study the\ndetails of this manifold to find that networks with different architectures\nfollow distinguishable trajectories but other factors have a minimal influence;\nlarger networks train along a similar manifold as that of smaller networks,\njust faster; and networks initialized at very different parts of the prediction\nspace converge to the solution along a similar manifold.\n", "link": "http://arxiv.org/abs/2305.01604v3", "date": "2024-03-19", "relevancy": 2.5266, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5225}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5074}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.486}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Training%20Process%20of%20Many%20Deep%20Networks%20Explores%20the%20Same%0A%20%20Low-Dimensional%20Manifold&body=Title%3A%20The%20Training%20Process%20of%20Many%20Deep%20Networks%20Explores%20the%20Same%0A%20%20Low-Dimensional%20Manifold%0AAuthor%3A%20Jialin%20Mao%20and%20Itay%20Griniasty%20and%20Han%20Kheng%20Teoh%20and%20Rahul%20Ramesh%20and%20Rubing%20Yang%20and%20Mark%20K.%20Transtrum%20and%20James%20P.%20Sethna%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20We%20develop%20information-geometric%20techniques%20to%20analyze%20the%20trajectories%20of%0Athe%20predictions%20of%20deep%20networks%20during%20training.%20By%20examining%20the%20underlying%0Ahigh-dimensional%20probabilistic%20models%2C%20we%20reveal%20that%20the%20training%20process%0Aexplores%20an%20effectively%20low-dimensional%20manifold.%20Networks%20with%20a%20wide%20range%20of%0Aarchitectures%2C%20sizes%2C%20trained%20using%20different%20optimization%20methods%2C%0Aregularization%20techniques%2C%20data%20augmentation%20techniques%2C%20and%20weight%0Ainitializations%20lie%20on%20the%20same%20manifold%20in%20the%20prediction%20space.%20We%20study%20the%0Adetails%20of%20this%20manifold%20to%20find%20that%20networks%20with%20different%20architectures%0Afollow%20distinguishable%20trajectories%20but%20other%20factors%20have%20a%20minimal%20influence%3B%0Alarger%20networks%20train%20along%20a%20similar%20manifold%20as%20that%20of%20smaller%20networks%2C%0Ajust%20faster%3B%20and%20networks%20initialized%20at%20very%20different%20parts%20of%20the%20prediction%0Aspace%20converge%20to%20the%20solution%20along%20a%20similar%20manifold.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.01604v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Training%20Process%20of%20Many%20Deep%20Networks%20Explores%20the%20Same%0A%20%20Low-Dimensional%20Manifold&entry.906535625=Jialin%20Mao%20and%20Itay%20Griniasty%20and%20Han%20Kheng%20Teoh%20and%20Rahul%20Ramesh%20and%20Rubing%20Yang%20and%20Mark%20K.%20Transtrum%20and%20James%20P.%20Sethna%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20We%20develop%20information-geometric%20techniques%20to%20analyze%20the%20trajectories%20of%0Athe%20predictions%20of%20deep%20networks%20during%20training.%20By%20examining%20the%20underlying%0Ahigh-dimensional%20probabilistic%20models%2C%20we%20reveal%20that%20the%20training%20process%0Aexplores%20an%20effectively%20low-dimensional%20manifold.%20Networks%20with%20a%20wide%20range%20of%0Aarchitectures%2C%20sizes%2C%20trained%20using%20different%20optimization%20methods%2C%0Aregularization%20techniques%2C%20data%20augmentation%20techniques%2C%20and%20weight%0Ainitializations%20lie%20on%20the%20same%20manifold%20in%20the%20prediction%20space.%20We%20study%20the%0Adetails%20of%20this%20manifold%20to%20find%20that%20networks%20with%20different%20architectures%0Afollow%20distinguishable%20trajectories%20but%20other%20factors%20have%20a%20minimal%20influence%3B%0Alarger%20networks%20train%20along%20a%20similar%20manifold%20as%20that%20of%20smaller%20networks%2C%0Ajust%20faster%3B%20and%20networks%20initialized%20at%20very%20different%20parts%20of%20the%20prediction%0Aspace%20converge%20to%20the%20solution%20along%20a%20similar%20manifold.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.01604v3&entry.124074799=Read"},
{"title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution\n  Image Synthesis", "author": "Linjiang Huang and Rongyao Fang and Aiping Zhang and Guanglu Song and Si Liu and Yu Liu and Hongsheng Li", "abstract": "  In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale.\n", "link": "http://arxiv.org/abs/2403.12963v1", "date": "2024-03-19", "relevancy": 2.5052, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6626}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6232}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6149}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FouriScale%3A%20A%20Frequency%20Perspective%20on%20Training-Free%20High-Resolution%0A%20%20Image%20Synthesis&body=Title%3A%20FouriScale%3A%20A%20Frequency%20Perspective%20on%20Training-Free%20High-Resolution%0A%20%20Image%20Synthesis%0AAuthor%3A%20Linjiang%20Huang%20and%20Rongyao%20Fang%20and%20Aiping%20Zhang%20and%20Guanglu%20Song%20and%20Si%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20delve%20into%20the%20generation%20of%20high-resolution%20images%20from%0Apre-trained%20diffusion%20models%2C%20addressing%20persistent%20challenges%2C%20such%20as%0Arepetitive%20patterns%20and%20structural%20distortions%2C%20that%20emerge%20when%20models%20are%0Aapplied%20beyond%20their%20trained%20resolutions.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20innovative%2C%20training-free%20approach%20FouriScale%20from%20the%20perspective%20of%0Afrequency%20domain%20analysis.%20We%20replace%20the%20original%20convolutional%20layers%20in%0Apre-trained%20diffusion%20models%20by%20incorporating%20a%20dilation%20technique%20along%20with%20a%0Alow-pass%20operation%2C%20intending%20to%20achieve%20structural%20consistency%20and%20scale%0Aconsistency%20across%20resolutions%2C%20respectively.%20Further%20enhanced%20by%20a%0Apadding-then-crop%20strategy%2C%20our%20method%20can%20flexibly%20handle%20text-to-image%0Ageneration%20of%20various%20aspect%20ratios.%20By%20using%20the%20FouriScale%20as%20guidance%2C%20our%0Amethod%20successfully%20balances%20the%20structural%20integrity%20and%20fidelity%20of%20generated%0Aimages%2C%20achieving%20an%20astonishing%20capacity%20of%20arbitrary-size%2C%20high-resolution%2C%0Aand%20high-quality%20generation.%20With%20its%20simplicity%20and%20compatibility%2C%20our%20method%0Acan%20provide%20valuable%20insights%20for%20future%20explorations%20into%20the%20synthesis%20of%0Aultra-high-resolution%20images.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/LeonHLJ/FouriScale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12963v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FouriScale%3A%20A%20Frequency%20Perspective%20on%20Training-Free%20High-Resolution%0A%20%20Image%20Synthesis&entry.906535625=Linjiang%20Huang%20and%20Rongyao%20Fang%20and%20Aiping%20Zhang%20and%20Guanglu%20Song%20and%20Si%20Liu%20and%20Yu%20Liu%20and%20Hongsheng%20Li&entry.1292438233=%20%20In%20this%20study%2C%20we%20delve%20into%20the%20generation%20of%20high-resolution%20images%20from%0Apre-trained%20diffusion%20models%2C%20addressing%20persistent%20challenges%2C%20such%20as%0Arepetitive%20patterns%20and%20structural%20distortions%2C%20that%20emerge%20when%20models%20are%0Aapplied%20beyond%20their%20trained%20resolutions.%20To%20address%20this%20issue%2C%20we%20introduce%0Aan%20innovative%2C%20training-free%20approach%20FouriScale%20from%20the%20perspective%20of%0Afrequency%20domain%20analysis.%20We%20replace%20the%20original%20convolutional%20layers%20in%0Apre-trained%20diffusion%20models%20by%20incorporating%20a%20dilation%20technique%20along%20with%20a%0Alow-pass%20operation%2C%20intending%20to%20achieve%20structural%20consistency%20and%20scale%0Aconsistency%20across%20resolutions%2C%20respectively.%20Further%20enhanced%20by%20a%0Apadding-then-crop%20strategy%2C%20our%20method%20can%20flexibly%20handle%20text-to-image%0Ageneration%20of%20various%20aspect%20ratios.%20By%20using%20the%20FouriScale%20as%20guidance%2C%20our%0Amethod%20successfully%20balances%20the%20structural%20integrity%20and%20fidelity%20of%20generated%0Aimages%2C%20achieving%20an%20astonishing%20capacity%20of%20arbitrary-size%2C%20high-resolution%2C%0Aand%20high-quality%20generation.%20With%20its%20simplicity%20and%20compatibility%2C%20our%20method%0Acan%20provide%20valuable%20insights%20for%20future%20explorations%20into%20the%20synthesis%20of%0Aultra-high-resolution%20images.%20The%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/LeonHLJ/FouriScale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12963v1&entry.124074799=Read"},
{"title": "PoNQ: a Neural QEM-based Mesh Representation", "author": "Nissim Maruani and Maks Ovsjanikov and Pierre Alliez and Mathieu Desbrun", "abstract": "  Although polygon meshes have been a standard representation in geometry\nprocessing, their irregular and combinatorial nature hinders their suitability\nfor learning-based applications. In this work, we introduce a novel learnable\nmesh representation through a set of local 3D sample Points and their\nassociated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,\nwhich we denote PoNQ. A global mesh is directly derived from PoNQ by\nefficiently leveraging the knowledge of the local quadric errors. Besides\nmarking the first use of QEM within a neural shape representation, our\ncontribution guarantees both topological and geometrical properties by ensuring\nthat a PoNQ mesh does not self-intersect and is always the boundary of a\nvolume. Notably, our representation does not rely on a regular grid, is\nsupervised directly by the target surface alone, and also handles open surfaces\nwith boundaries and/or sharp features. We demonstrate the efficacy of PoNQ\nthrough a learning-based mesh prediction from SDF grids and show that our\nmethod surpasses recent state-of-the-art techniques in terms of both surface\nand edge-based metrics.\n", "link": "http://arxiv.org/abs/2403.12870v1", "date": "2024-03-19", "relevancy": 2.4979, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4857}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PoNQ%3A%20a%20Neural%20QEM-based%20Mesh%20Representation&body=Title%3A%20PoNQ%3A%20a%20Neural%20QEM-based%20Mesh%20Representation%0AAuthor%3A%20Nissim%20Maruani%20and%20Maks%20Ovsjanikov%20and%20Pierre%20Alliez%20and%20Mathieu%20Desbrun%0AAbstract%3A%20%20%20Although%20polygon%20meshes%20have%20been%20a%20standard%20representation%20in%20geometry%0Aprocessing%2C%20their%20irregular%20and%20combinatorial%20nature%20hinders%20their%20suitability%0Afor%20learning-based%20applications.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20learnable%0Amesh%20representation%20through%20a%20set%20of%20local%203D%20sample%20Points%20and%20their%0Aassociated%20Normals%20and%20Quadric%20error%20metrics%20%28QEM%29%20w.r.t.%20the%20underlying%20shape%2C%0Awhich%20we%20denote%20PoNQ.%20A%20global%20mesh%20is%20directly%20derived%20from%20PoNQ%20by%0Aefficiently%20leveraging%20the%20knowledge%20of%20the%20local%20quadric%20errors.%20Besides%0Amarking%20the%20first%20use%20of%20QEM%20within%20a%20neural%20shape%20representation%2C%20our%0Acontribution%20guarantees%20both%20topological%20and%20geometrical%20properties%20by%20ensuring%0Athat%20a%20PoNQ%20mesh%20does%20not%20self-intersect%20and%20is%20always%20the%20boundary%20of%20a%0Avolume.%20Notably%2C%20our%20representation%20does%20not%20rely%20on%20a%20regular%20grid%2C%20is%0Asupervised%20directly%20by%20the%20target%20surface%20alone%2C%20and%20also%20handles%20open%20surfaces%0Awith%20boundaries%20and/or%20sharp%20features.%20We%20demonstrate%20the%20efficacy%20of%20PoNQ%0Athrough%20a%20learning-based%20mesh%20prediction%20from%20SDF%20grids%20and%20show%20that%20our%0Amethod%20surpasses%20recent%20state-of-the-art%20techniques%20in%20terms%20of%20both%20surface%0Aand%20edge-based%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12870v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoNQ%3A%20a%20Neural%20QEM-based%20Mesh%20Representation&entry.906535625=Nissim%20Maruani%20and%20Maks%20Ovsjanikov%20and%20Pierre%20Alliez%20and%20Mathieu%20Desbrun&entry.1292438233=%20%20Although%20polygon%20meshes%20have%20been%20a%20standard%20representation%20in%20geometry%0Aprocessing%2C%20their%20irregular%20and%20combinatorial%20nature%20hinders%20their%20suitability%0Afor%20learning-based%20applications.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20learnable%0Amesh%20representation%20through%20a%20set%20of%20local%203D%20sample%20Points%20and%20their%0Aassociated%20Normals%20and%20Quadric%20error%20metrics%20%28QEM%29%20w.r.t.%20the%20underlying%20shape%2C%0Awhich%20we%20denote%20PoNQ.%20A%20global%20mesh%20is%20directly%20derived%20from%20PoNQ%20by%0Aefficiently%20leveraging%20the%20knowledge%20of%20the%20local%20quadric%20errors.%20Besides%0Amarking%20the%20first%20use%20of%20QEM%20within%20a%20neural%20shape%20representation%2C%20our%0Acontribution%20guarantees%20both%20topological%20and%20geometrical%20properties%20by%20ensuring%0Athat%20a%20PoNQ%20mesh%20does%20not%20self-intersect%20and%20is%20always%20the%20boundary%20of%20a%0Avolume.%20Notably%2C%20our%20representation%20does%20not%20rely%20on%20a%20regular%20grid%2C%20is%0Asupervised%20directly%20by%20the%20target%20surface%20alone%2C%20and%20also%20handles%20open%20surfaces%0Awith%20boundaries%20and/or%20sharp%20features.%20We%20demonstrate%20the%20efficacy%20of%20PoNQ%0Athrough%20a%20learning-based%20mesh%20prediction%20from%20SDF%20grids%20and%20show%20that%20our%0Amethod%20surpasses%20recent%20state-of-the-art%20techniques%20in%20terms%20of%20both%20surface%0Aand%20edge-based%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12870v1&entry.124074799=Read"},
{"title": "Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs", "author": "Hao Fei and Shengqiong Wu and Wei Ji and Hanwang Zhang and Tat-Seng Chua", "abstract": "  Text-to-video (T2V) synthesis has gained increasing attention in the\ncommunity, in which the recently emerged diffusion models (DMs) have\npromisingly shown stronger performance than the past approaches. While existing\nstate-of-the-art DMs are competent to achieve high-resolution video generation,\nthey may largely suffer from key limitations (e.g., action occurrence\ndisorders, crude video motions) with respect to the intricate temporal dynamics\nmodeling, one of the crux of video synthesis. In this work, we investigate\nstrengthening the awareness of video dynamics for DMs, for high-quality T2V\ngeneration. Inspired by human intuition, we design an innovative dynamic scene\nmanager (dubbed as Dysen) module, which includes (step-1) extracting from input\ntext the key actions with proper time-order arrangement, (step-2) transforming\nthe action schedules into the dynamic scene graph (DSG) representations, and\n(step-3) enriching the scenes in the DSG with sufficient and reasonable\ndetails. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via\nin-context learning, Dysen realizes (nearly) human-level temporal dynamics\nunderstanding. Finally, the resulting video DSG with rich action scene details\nis encoded as fine-grained spatio-temporal features, integrated into the\nbackbone T2V DM for video generating. Experiments on popular T2V datasets\nsuggest that our Dysen-VDM consistently outperforms prior arts with significant\nmargins, especially in scenarios with complex actions. Codes at\nhttps://haofei.vip/Dysen-VDM\n", "link": "http://arxiv.org/abs/2308.13812v2", "date": "2024-03-19", "relevancy": 2.4876, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6533}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6439}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5817}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dysen-VDM%3A%20Empowering%20Dynamics-aware%20Text-to-Video%20Diffusion%20with%20LLMs&body=Title%3A%20Dysen-VDM%3A%20Empowering%20Dynamics-aware%20Text-to-Video%20Diffusion%20with%20LLMs%0AAuthor%3A%20Hao%20Fei%20and%20Shengqiong%20Wu%20and%20Wei%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Text-to-video%20%28T2V%29%20synthesis%20has%20gained%20increasing%20attention%20in%20the%0Acommunity%2C%20in%20which%20the%20recently%20emerged%20diffusion%20models%20%28DMs%29%20have%0Apromisingly%20shown%20stronger%20performance%20than%20the%20past%20approaches.%20While%20existing%0Astate-of-the-art%20DMs%20are%20competent%20to%20achieve%20high-resolution%20video%20generation%2C%0Athey%20may%20largely%20suffer%20from%20key%20limitations%20%28e.g.%2C%20action%20occurrence%0Adisorders%2C%20crude%20video%20motions%29%20with%20respect%20to%20the%20intricate%20temporal%20dynamics%0Amodeling%2C%20one%20of%20the%20crux%20of%20video%20synthesis.%20In%20this%20work%2C%20we%20investigate%0Astrengthening%20the%20awareness%20of%20video%20dynamics%20for%20DMs%2C%20for%20high-quality%20T2V%0Ageneration.%20Inspired%20by%20human%20intuition%2C%20we%20design%20an%20innovative%20dynamic%20scene%0Amanager%20%28dubbed%20as%20Dysen%29%20module%2C%20which%20includes%20%28step-1%29%20extracting%20from%20input%0Atext%20the%20key%20actions%20with%20proper%20time-order%20arrangement%2C%20%28step-2%29%20transforming%0Athe%20action%20schedules%20into%20the%20dynamic%20scene%20graph%20%28DSG%29%20representations%2C%20and%0A%28step-3%29%20enriching%20the%20scenes%20in%20the%20DSG%20with%20sufficient%20and%20reasonable%0Adetails.%20Taking%20advantage%20of%20the%20existing%20powerful%20LLMs%20%28e.g.%2C%20ChatGPT%29%20via%0Ain-context%20learning%2C%20Dysen%20realizes%20%28nearly%29%20human-level%20temporal%20dynamics%0Aunderstanding.%20Finally%2C%20the%20resulting%20video%20DSG%20with%20rich%20action%20scene%20details%0Ais%20encoded%20as%20fine-grained%20spatio-temporal%20features%2C%20integrated%20into%20the%0Abackbone%20T2V%20DM%20for%20video%20generating.%20Experiments%20on%20popular%20T2V%20datasets%0Asuggest%20that%20our%20Dysen-VDM%20consistently%20outperforms%20prior%20arts%20with%20significant%0Amargins%2C%20especially%20in%20scenarios%20with%20complex%20actions.%20Codes%20at%0Ahttps%3A//haofei.vip/Dysen-VDM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13812v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dysen-VDM%3A%20Empowering%20Dynamics-aware%20Text-to-Video%20Diffusion%20with%20LLMs&entry.906535625=Hao%20Fei%20and%20Shengqiong%20Wu%20and%20Wei%20Ji%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Text-to-video%20%28T2V%29%20synthesis%20has%20gained%20increasing%20attention%20in%20the%0Acommunity%2C%20in%20which%20the%20recently%20emerged%20diffusion%20models%20%28DMs%29%20have%0Apromisingly%20shown%20stronger%20performance%20than%20the%20past%20approaches.%20While%20existing%0Astate-of-the-art%20DMs%20are%20competent%20to%20achieve%20high-resolution%20video%20generation%2C%0Athey%20may%20largely%20suffer%20from%20key%20limitations%20%28e.g.%2C%20action%20occurrence%0Adisorders%2C%20crude%20video%20motions%29%20with%20respect%20to%20the%20intricate%20temporal%20dynamics%0Amodeling%2C%20one%20of%20the%20crux%20of%20video%20synthesis.%20In%20this%20work%2C%20we%20investigate%0Astrengthening%20the%20awareness%20of%20video%20dynamics%20for%20DMs%2C%20for%20high-quality%20T2V%0Ageneration.%20Inspired%20by%20human%20intuition%2C%20we%20design%20an%20innovative%20dynamic%20scene%0Amanager%20%28dubbed%20as%20Dysen%29%20module%2C%20which%20includes%20%28step-1%29%20extracting%20from%20input%0Atext%20the%20key%20actions%20with%20proper%20time-order%20arrangement%2C%20%28step-2%29%20transforming%0Athe%20action%20schedules%20into%20the%20dynamic%20scene%20graph%20%28DSG%29%20representations%2C%20and%0A%28step-3%29%20enriching%20the%20scenes%20in%20the%20DSG%20with%20sufficient%20and%20reasonable%0Adetails.%20Taking%20advantage%20of%20the%20existing%20powerful%20LLMs%20%28e.g.%2C%20ChatGPT%29%20via%0Ain-context%20learning%2C%20Dysen%20realizes%20%28nearly%29%20human-level%20temporal%20dynamics%0Aunderstanding.%20Finally%2C%20the%20resulting%20video%20DSG%20with%20rich%20action%20scene%20details%0Ais%20encoded%20as%20fine-grained%20spatio-temporal%20features%2C%20integrated%20into%20the%0Abackbone%20T2V%20DM%20for%20video%20generating.%20Experiments%20on%20popular%20T2V%20datasets%0Asuggest%20that%20our%20Dysen-VDM%20consistently%20outperforms%20prior%20arts%20with%20significant%0Amargins%2C%20especially%20in%20scenarios%20with%20complex%20actions.%20Codes%20at%0Ahttps%3A//haofei.vip/Dysen-VDM%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13812v2&entry.124074799=Read"},
{"title": "Bidirectional Temporal Diffusion Model for Temporally Consistent Human\n  Animation", "author": "Tserendorj Adiya and Jae Shin Yoon and Jungeun Lee and Sanghun Kim and Hwasup Lim", "abstract": "  We introduce a method to generate temporally coherent human animation from a\nsingle image, a video, or a random noise. This problem has been formulated as\nmodeling of an auto-regressive generation, i.e., to regress past frames to\ndecode future frames. However, such unidirectional generation is highly prone\nto motion drifting over time, generating unrealistic human animation with\nsignificant artifacts such as appearance distortion. We claim that\nbidirectional temporal modeling enforces temporal coherence on a generative\nnetwork by largely suppressing the motion ambiguity of human appearance. To\nprove our claim, we design a novel human animation framework using a denoising\ndiffusion model: a neural network learns to generate the image of a person by\ndenoising temporal Gaussian noises whose intermediate results are\ncross-conditioned bidirectionally between consecutive frames. In the\nexperiments, our method demonstrates strong performance compared to existing\nunidirectional approaches with realistic temporal coherence\n", "link": "http://arxiv.org/abs/2307.00574v4", "date": "2024-03-19", "relevancy": 2.4841, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6454}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6223}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5961}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Temporal%20Diffusion%20Model%20for%20Temporally%20Consistent%20Human%0A%20%20Animation&body=Title%3A%20Bidirectional%20Temporal%20Diffusion%20Model%20for%20Temporally%20Consistent%20Human%0A%20%20Animation%0AAuthor%3A%20Tserendorj%20Adiya%20and%20Jae%20Shin%20Yoon%20and%20Jungeun%20Lee%20and%20Sanghun%20Kim%20and%20Hwasup%20Lim%0AAbstract%3A%20%20%20We%20introduce%20a%20method%20to%20generate%20temporally%20coherent%20human%20animation%20from%20a%0Asingle%20image%2C%20a%20video%2C%20or%20a%20random%20noise.%20This%20problem%20has%20been%20formulated%20as%0Amodeling%20of%20an%20auto-regressive%20generation%2C%20i.e.%2C%20to%20regress%20past%20frames%20to%0Adecode%20future%20frames.%20However%2C%20such%20unidirectional%20generation%20is%20highly%20prone%0Ato%20motion%20drifting%20over%20time%2C%20generating%20unrealistic%20human%20animation%20with%0Asignificant%20artifacts%20such%20as%20appearance%20distortion.%20We%20claim%20that%0Abidirectional%20temporal%20modeling%20enforces%20temporal%20coherence%20on%20a%20generative%0Anetwork%20by%20largely%20suppressing%20the%20motion%20ambiguity%20of%20human%20appearance.%20To%0Aprove%20our%20claim%2C%20we%20design%20a%20novel%20human%20animation%20framework%20using%20a%20denoising%0Adiffusion%20model%3A%20a%20neural%20network%20learns%20to%20generate%20the%20image%20of%20a%20person%20by%0Adenoising%20temporal%20Gaussian%20noises%20whose%20intermediate%20results%20are%0Across-conditioned%20bidirectionally%20between%20consecutive%20frames.%20In%20the%0Aexperiments%2C%20our%20method%20demonstrates%20strong%20performance%20compared%20to%20existing%0Aunidirectional%20approaches%20with%20realistic%20temporal%20coherence%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.00574v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Temporal%20Diffusion%20Model%20for%20Temporally%20Consistent%20Human%0A%20%20Animation&entry.906535625=Tserendorj%20Adiya%20and%20Jae%20Shin%20Yoon%20and%20Jungeun%20Lee%20and%20Sanghun%20Kim%20and%20Hwasup%20Lim&entry.1292438233=%20%20We%20introduce%20a%20method%20to%20generate%20temporally%20coherent%20human%20animation%20from%20a%0Asingle%20image%2C%20a%20video%2C%20or%20a%20random%20noise.%20This%20problem%20has%20been%20formulated%20as%0Amodeling%20of%20an%20auto-regressive%20generation%2C%20i.e.%2C%20to%20regress%20past%20frames%20to%0Adecode%20future%20frames.%20However%2C%20such%20unidirectional%20generation%20is%20highly%20prone%0Ato%20motion%20drifting%20over%20time%2C%20generating%20unrealistic%20human%20animation%20with%0Asignificant%20artifacts%20such%20as%20appearance%20distortion.%20We%20claim%20that%0Abidirectional%20temporal%20modeling%20enforces%20temporal%20coherence%20on%20a%20generative%0Anetwork%20by%20largely%20suppressing%20the%20motion%20ambiguity%20of%20human%20appearance.%20To%0Aprove%20our%20claim%2C%20we%20design%20a%20novel%20human%20animation%20framework%20using%20a%20denoising%0Adiffusion%20model%3A%20a%20neural%20network%20learns%20to%20generate%20the%20image%20of%20a%20person%20by%0Adenoising%20temporal%20Gaussian%20noises%20whose%20intermediate%20results%20are%0Across-conditioned%20bidirectionally%20between%20consecutive%20frames.%20In%20the%0Aexperiments%2C%20our%20method%20demonstrates%20strong%20performance%20compared%20to%20existing%0Aunidirectional%20approaches%20with%20realistic%20temporal%20coherence%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.00574v4&entry.124074799=Read"},
{"title": "IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single\n  image and a NeRF model", "author": "Matteo Bortolon and Theodore Tsesmelis and Stuart James and Fabio Poiesi and Alessio Del Bue", "abstract": "  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera\npose of a given image, building on the Neural Radiance Fields (NeRF)\nformulation. IFFNeRF is specifically designed to operate in real-time and\neliminates the need for an initial pose guess that is proximate to the sought\nsolution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface\npoints from within the NeRF model. From these sampled points, we cast rays and\ndeduce the color for each ray through pixel-level view synthesis. The camera\npose can then be estimated as the solution to a Least Squares problem by\nselecting correspondences between the query image and the resulting bundle. We\nfacilitate this process through a learned attention mechanism, bridging the\nquery image embedding with the embedding of parameterized rays, thereby\nmatching rays pertinent to the image. Through synthetic and real evaluation\nsettings, we show that our method can improve the angular and translation error\naccuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing\nat 34fps on consumer hardware and not requiring the initial pose guess.\n", "link": "http://arxiv.org/abs/2403.12682v1", "date": "2024-03-19", "relevancy": 2.4702, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5334}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4877}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.461}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IFFNeRF%3A%20Initialisation%20Free%20and%20Fast%206DoF%20pose%20estimation%20from%20a%20single%0A%20%20image%20and%20a%20NeRF%20model&body=Title%3A%20IFFNeRF%3A%20Initialisation%20Free%20and%20Fast%206DoF%20pose%20estimation%20from%20a%20single%0A%20%20image%20and%20a%20NeRF%20model%0AAuthor%3A%20Matteo%20Bortolon%20and%20Theodore%20Tsesmelis%20and%20Stuart%20James%20and%20Fabio%20Poiesi%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20introduce%20IFFNeRF%20to%20estimate%20the%20six%20degrees-of-freedom%20%286DoF%29%20camera%0Apose%20of%20a%20given%20image%2C%20building%20on%20the%20Neural%20Radiance%20Fields%20%28NeRF%29%0Aformulation.%20IFFNeRF%20is%20specifically%20designed%20to%20operate%20in%20real-time%20and%0Aeliminates%20the%20need%20for%20an%20initial%20pose%20guess%20that%20is%20proximate%20to%20the%20sought%0Asolution.%20IFFNeRF%20utilizes%20the%20Metropolis-Hasting%20algorithm%20to%20sample%20surface%0Apoints%20from%20within%20the%20NeRF%20model.%20From%20these%20sampled%20points%2C%20we%20cast%20rays%20and%0Adeduce%20the%20color%20for%20each%20ray%20through%20pixel-level%20view%20synthesis.%20The%20camera%0Apose%20can%20then%20be%20estimated%20as%20the%20solution%20to%20a%20Least%20Squares%20problem%20by%0Aselecting%20correspondences%20between%20the%20query%20image%20and%20the%20resulting%20bundle.%20We%0Afacilitate%20this%20process%20through%20a%20learned%20attention%20mechanism%2C%20bridging%20the%0Aquery%20image%20embedding%20with%20the%20embedding%20of%20parameterized%20rays%2C%20thereby%0Amatching%20rays%20pertinent%20to%20the%20image.%20Through%20synthetic%20and%20real%20evaluation%0Asettings%2C%20we%20show%20that%20our%20method%20can%20improve%20the%20angular%20and%20translation%20error%0Aaccuracy%20by%2080.1%25%20and%2067.3%25%2C%20respectively%2C%20compared%20to%20iNeRF%20while%20performing%0Aat%2034fps%20on%20consumer%20hardware%20and%20not%20requiring%20the%20initial%20pose%20guess.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12682v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IFFNeRF%3A%20Initialisation%20Free%20and%20Fast%206DoF%20pose%20estimation%20from%20a%20single%0A%20%20image%20and%20a%20NeRF%20model&entry.906535625=Matteo%20Bortolon%20and%20Theodore%20Tsesmelis%20and%20Stuart%20James%20and%20Fabio%20Poiesi%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20introduce%20IFFNeRF%20to%20estimate%20the%20six%20degrees-of-freedom%20%286DoF%29%20camera%0Apose%20of%20a%20given%20image%2C%20building%20on%20the%20Neural%20Radiance%20Fields%20%28NeRF%29%0Aformulation.%20IFFNeRF%20is%20specifically%20designed%20to%20operate%20in%20real-time%20and%0Aeliminates%20the%20need%20for%20an%20initial%20pose%20guess%20that%20is%20proximate%20to%20the%20sought%0Asolution.%20IFFNeRF%20utilizes%20the%20Metropolis-Hasting%20algorithm%20to%20sample%20surface%0Apoints%20from%20within%20the%20NeRF%20model.%20From%20these%20sampled%20points%2C%20we%20cast%20rays%20and%0Adeduce%20the%20color%20for%20each%20ray%20through%20pixel-level%20view%20synthesis.%20The%20camera%0Apose%20can%20then%20be%20estimated%20as%20the%20solution%20to%20a%20Least%20Squares%20problem%20by%0Aselecting%20correspondences%20between%20the%20query%20image%20and%20the%20resulting%20bundle.%20We%0Afacilitate%20this%20process%20through%20a%20learned%20attention%20mechanism%2C%20bridging%20the%0Aquery%20image%20embedding%20with%20the%20embedding%20of%20parameterized%20rays%2C%20thereby%0Amatching%20rays%20pertinent%20to%20the%20image.%20Through%20synthetic%20and%20real%20evaluation%0Asettings%2C%20we%20show%20that%20our%20method%20can%20improve%20the%20angular%20and%20translation%20error%0Aaccuracy%20by%2080.1%25%20and%2067.3%25%2C%20respectively%2C%20compared%20to%20iNeRF%20while%20performing%0Aat%2034fps%20on%20consumer%20hardware%20and%20not%20requiring%20the%20initial%20pose%20guess.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12682v1&entry.124074799=Read"},
{"title": "LNPT: Label-free Network Pruning and Training", "author": "Jinying Xiao and Ping Li and Zhe Tang and Jie Nie", "abstract": "  Pruning before training enables the deployment of neural networks on smart\ndevices. By retaining weights conducive to generalization, pruned networks can\nbe accommodated on resource-constrained smart devices. It is commonly held that\nthe distance on weight norms between the initialized and the fully-trained\nnetworks correlates with generalization performance. However, as we have\nuncovered, inconsistency between this metric and generalization during training\nprocesses, which poses an obstacle to determine the pruned structures on smart\ndevices in advance. In this paper, we introduce the concept of the learning\ngap, emphasizing its accurate correlation with generalization. Experiments show\nthat the learning gap, in the form of feature maps from the penultimate layer\nof networks, aligns with variations of generalization performance. We propose a\nnovel learning framework, LNPT, which enables mature networks on the cloud to\nprovide online guidance for network pruning and learning on smart devices with\nunlabeled data. Our results demonstrate the superiority of this approach over\nsupervised training.\n", "link": "http://arxiv.org/abs/2403.12690v1", "date": "2024-03-19", "relevancy": 2.4353, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training&body=Title%3A%20LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training%0AAuthor%3A%20Jinying%20Xiao%20and%20Ping%20Li%20and%20Zhe%20Tang%20and%20Jie%20Nie%0AAbstract%3A%20%20%20Pruning%20before%20training%20enables%20the%20deployment%20of%20neural%20networks%20on%20smart%0Adevices.%20By%20retaining%20weights%20conducive%20to%20generalization%2C%20pruned%20networks%20can%0Abe%20accommodated%20on%20resource-constrained%20smart%20devices.%20It%20is%20commonly%20held%20that%0Athe%20distance%20on%20weight%20norms%20between%20the%20initialized%20and%20the%20fully-trained%0Anetworks%20correlates%20with%20generalization%20performance.%20However%2C%20as%20we%20have%0Auncovered%2C%20inconsistency%20between%20this%20metric%20and%20generalization%20during%20training%0Aprocesses%2C%20which%20poses%20an%20obstacle%20to%20determine%20the%20pruned%20structures%20on%20smart%0Adevices%20in%20advance.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20learning%0Agap%2C%20emphasizing%20its%20accurate%20correlation%20with%20generalization.%20Experiments%20show%0Athat%20the%20learning%20gap%2C%20in%20the%20form%20of%20feature%20maps%20from%20the%20penultimate%20layer%0Aof%20networks%2C%20aligns%20with%20variations%20of%20generalization%20performance.%20We%20propose%20a%0Anovel%20learning%20framework%2C%20LNPT%2C%20which%20enables%20mature%20networks%20on%20the%20cloud%20to%0Aprovide%20online%20guidance%20for%20network%20pruning%20and%20learning%20on%20smart%20devices%20with%0Aunlabeled%20data.%20Our%20results%20demonstrate%20the%20superiority%20of%20this%20approach%20over%0Asupervised%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12690v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LNPT%3A%20Label-free%20Network%20Pruning%20and%20Training&entry.906535625=Jinying%20Xiao%20and%20Ping%20Li%20and%20Zhe%20Tang%20and%20Jie%20Nie&entry.1292438233=%20%20Pruning%20before%20training%20enables%20the%20deployment%20of%20neural%20networks%20on%20smart%0Adevices.%20By%20retaining%20weights%20conducive%20to%20generalization%2C%20pruned%20networks%20can%0Abe%20accommodated%20on%20resource-constrained%20smart%20devices.%20It%20is%20commonly%20held%20that%0Athe%20distance%20on%20weight%20norms%20between%20the%20initialized%20and%20the%20fully-trained%0Anetworks%20correlates%20with%20generalization%20performance.%20However%2C%20as%20we%20have%0Auncovered%2C%20inconsistency%20between%20this%20metric%20and%20generalization%20during%20training%0Aprocesses%2C%20which%20poses%20an%20obstacle%20to%20determine%20the%20pruned%20structures%20on%20smart%0Adevices%20in%20advance.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20learning%0Agap%2C%20emphasizing%20its%20accurate%20correlation%20with%20generalization.%20Experiments%20show%0Athat%20the%20learning%20gap%2C%20in%20the%20form%20of%20feature%20maps%20from%20the%20penultimate%20layer%0Aof%20networks%2C%20aligns%20with%20variations%20of%20generalization%20performance.%20We%20propose%20a%0Anovel%20learning%20framework%2C%20LNPT%2C%20which%20enables%20mature%20networks%20on%20the%20cloud%20to%0Aprovide%20online%20guidance%20for%20network%20pruning%20and%20learning%20on%20smart%20devices%20with%0Aunlabeled%20data.%20Our%20results%20demonstrate%20the%20superiority%20of%20this%20approach%20over%0Asupervised%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12690v1&entry.124074799=Read"},
{"title": "Subjective-Aligned Dateset and Metric for Text-to-Video Quality\n  Assessment", "author": "Tengchuan Kou and Xiaohong Liu and Zicheng Zhang and Chunyi Li and Haoning Wu and Xiongkuo Min and Guangtao Zhai and Ning Liu", "abstract": "  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n", "link": "http://arxiv.org/abs/2403.11956v2", "date": "2024-03-19", "relevancy": 2.4311, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.63}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6233}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5794}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Subjective-Aligned%20Dateset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment&body=Title%3A%20Subjective-Aligned%20Dateset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment%0AAuthor%3A%20Tengchuan%20Kou%20and%20Xiaohong%20Liu%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Ning%20Liu%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20generative%20models%2C%20Artificial%0AIntelligence-Generated%20Contents%20%28AIGC%29%20have%20exponentially%20increased%20in%20daily%0Alives.%20Among%20them%2C%20Text-to-Video%20%28T2V%29%20generation%20has%20received%20widespread%0Aattention.%20Though%20many%20T2V%20models%20have%20been%20released%20for%20generating%20high%0Aperceptual%20quality%20videos%2C%20there%20is%20still%20lack%20of%20a%20method%20to%20evaluate%20the%0Aquality%20of%20these%20videos%20quantitatively.%20To%20solve%20this%20issue%2C%20we%20establish%20the%0Alargest-scale%20Text-to-Video%20Quality%20Assessment%20DataBase%20%28T2VQA-DB%29%20to%20date.%20The%0Adataset%20is%20composed%20of%2010%2C000%20videos%20generated%20by%209%20different%20T2V%20models.%20We%0Aalso%20conduct%20a%20subjective%20study%20to%20obtain%20each%20video%27s%20corresponding%20mean%0Aopinion%20score.%20Based%20on%20T2VQA-DB%2C%20we%20propose%20a%20novel%20transformer-based%20model%0Afor%20subjective-aligned%20Text-to-Video%20Quality%20Assessment%20%28T2VQA%29.%20The%20model%0Aextracts%20features%20from%20text-video%20alignment%20and%20video%20fidelity%20perspectives%2C%0Athen%20it%20leverages%20the%20ability%20of%20a%20large%20language%20model%20to%20give%20the%20prediction%0Ascore.%20Experimental%20results%20show%20that%20T2VQA%20outperforms%20existing%20T2V%20metrics%0Aand%20SOTA%20video%20quality%20assessment%20models.%20Quantitative%20analysis%20indicates%20that%0AT2VQA%20is%20capable%20of%20giving%20subjective-align%20predictions%2C%20validating%20its%0Aeffectiveness.%20The%20dataset%20and%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/QMME/T2VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11956v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective-Aligned%20Dateset%20and%20Metric%20for%20Text-to-Video%20Quality%0A%20%20Assessment&entry.906535625=Tengchuan%20Kou%20and%20Xiaohong%20Liu%20and%20Zicheng%20Zhang%20and%20Chunyi%20Li%20and%20Haoning%20Wu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Ning%20Liu&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20generative%20models%2C%20Artificial%0AIntelligence-Generated%20Contents%20%28AIGC%29%20have%20exponentially%20increased%20in%20daily%0Alives.%20Among%20them%2C%20Text-to-Video%20%28T2V%29%20generation%20has%20received%20widespread%0Aattention.%20Though%20many%20T2V%20models%20have%20been%20released%20for%20generating%20high%0Aperceptual%20quality%20videos%2C%20there%20is%20still%20lack%20of%20a%20method%20to%20evaluate%20the%0Aquality%20of%20these%20videos%20quantitatively.%20To%20solve%20this%20issue%2C%20we%20establish%20the%0Alargest-scale%20Text-to-Video%20Quality%20Assessment%20DataBase%20%28T2VQA-DB%29%20to%20date.%20The%0Adataset%20is%20composed%20of%2010%2C000%20videos%20generated%20by%209%20different%20T2V%20models.%20We%0Aalso%20conduct%20a%20subjective%20study%20to%20obtain%20each%20video%27s%20corresponding%20mean%0Aopinion%20score.%20Based%20on%20T2VQA-DB%2C%20we%20propose%20a%20novel%20transformer-based%20model%0Afor%20subjective-aligned%20Text-to-Video%20Quality%20Assessment%20%28T2VQA%29.%20The%20model%0Aextracts%20features%20from%20text-video%20alignment%20and%20video%20fidelity%20perspectives%2C%0Athen%20it%20leverages%20the%20ability%20of%20a%20large%20language%20model%20to%20give%20the%20prediction%0Ascore.%20Experimental%20results%20show%20that%20T2VQA%20outperforms%20existing%20T2V%20metrics%0Aand%20SOTA%20video%20quality%20assessment%20models.%20Quantitative%20analysis%20indicates%20that%0AT2VQA%20is%20capable%20of%20giving%20subjective-align%20predictions%2C%20validating%20its%0Aeffectiveness.%20The%20dataset%20and%20code%20will%20be%20released%20at%0Ahttps%3A//github.com/QMME/T2VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11956v2&entry.124074799=Read"},
{"title": "Generic 3D Diffusion Adapter Using Controlled Multi-View Editing", "author": "Hansheng Chen and Ruoxi Shi and Yulin Liu and Bokui Shen and Jiayuan Gu and Gordon Wetzstein and Hao Su and Leonidas Guibas", "abstract": "  Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.\n", "link": "http://arxiv.org/abs/2403.12032v2", "date": "2024-03-19", "relevancy": 2.4137, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6109}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6072}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generic%203D%20Diffusion%20Adapter%20Using%20Controlled%20Multi-View%20Editing&body=Title%3A%20Generic%203D%20Diffusion%20Adapter%20Using%20Controlled%20Multi-View%20Editing%0AAuthor%3A%20Hansheng%20Chen%20and%20Ruoxi%20Shi%20and%20Yulin%20Liu%20and%20Bokui%20Shen%20and%20Jiayuan%20Gu%20and%20Gordon%20Wetzstein%20and%20Hao%20Su%20and%20Leonidas%20Guibas%0AAbstract%3A%20%20%20Open-domain%203D%20object%20synthesis%20has%20been%20lagging%20behind%20image%20synthesis%20due%0Ato%20limited%20data%20and%20higher%20computational%20complexity.%20To%20bridge%20this%20gap%2C%20recent%0Aworks%20have%20investigated%20multi-view%20diffusion%20but%20often%20fall%20short%20in%20either%203D%0Aconsistency%2C%20visual%20quality%2C%20or%20efficiency.%20This%20paper%20proposes%20MVEdit%2C%20which%0Afunctions%20as%20a%203D%20counterpart%20of%20SDEdit%2C%20employing%20ancestral%20sampling%20to%0Ajointly%20denoise%20multi-view%20images%20and%20output%20high-quality%20textured%20meshes.%0ABuilt%20on%20off-the-shelf%202D%20diffusion%20models%2C%20MVEdit%20achieves%203D%20consistency%0Athrough%20a%20training-free%203D%20Adapter%2C%20which%20lifts%20the%202D%20views%20of%20the%20last%0Atimestep%20into%20a%20coherent%203D%20representation%2C%20then%20conditions%20the%202D%20views%20of%20the%0Anext%20timestep%20using%20rendered%20views%2C%20without%20uncompromising%20visual%20quality.%20With%0Aan%20inference%20time%20of%20only%202-5%20minutes%2C%20this%20framework%20achieves%20better%20trade-off%0Abetween%20quality%20and%20speed%20than%20score%20distillation.%20MVEdit%20is%20highly%20versatile%0Aand%20extendable%2C%20with%20a%20wide%20range%20of%20applications%20including%20text/image-to-3D%0Ageneration%2C%203D-to-3D%20editing%2C%20and%20high-quality%20texture%20synthesis.%20In%0Aparticular%2C%20evaluations%20demonstrate%20state-of-the-art%20performance%20in%20both%0Aimage-to-3D%20and%20text-guided%20texture%20generation%20tasks.%20Additionally%2C%20we%0Aintroduce%20a%20method%20for%20fine-tuning%202D%20latent%20diffusion%20models%20on%20small%203D%0Adatasets%20with%20limited%20resources%2C%20enabling%20fast%20low-resolution%20text-to-3D%0Ainitialization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12032v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generic%203D%20Diffusion%20Adapter%20Using%20Controlled%20Multi-View%20Editing&entry.906535625=Hansheng%20Chen%20and%20Ruoxi%20Shi%20and%20Yulin%20Liu%20and%20Bokui%20Shen%20and%20Jiayuan%20Gu%20and%20Gordon%20Wetzstein%20and%20Hao%20Su%20and%20Leonidas%20Guibas&entry.1292438233=%20%20Open-domain%203D%20object%20synthesis%20has%20been%20lagging%20behind%20image%20synthesis%20due%0Ato%20limited%20data%20and%20higher%20computational%20complexity.%20To%20bridge%20this%20gap%2C%20recent%0Aworks%20have%20investigated%20multi-view%20diffusion%20but%20often%20fall%20short%20in%20either%203D%0Aconsistency%2C%20visual%20quality%2C%20or%20efficiency.%20This%20paper%20proposes%20MVEdit%2C%20which%0Afunctions%20as%20a%203D%20counterpart%20of%20SDEdit%2C%20employing%20ancestral%20sampling%20to%0Ajointly%20denoise%20multi-view%20images%20and%20output%20high-quality%20textured%20meshes.%0ABuilt%20on%20off-the-shelf%202D%20diffusion%20models%2C%20MVEdit%20achieves%203D%20consistency%0Athrough%20a%20training-free%203D%20Adapter%2C%20which%20lifts%20the%202D%20views%20of%20the%20last%0Atimestep%20into%20a%20coherent%203D%20representation%2C%20then%20conditions%20the%202D%20views%20of%20the%0Anext%20timestep%20using%20rendered%20views%2C%20without%20uncompromising%20visual%20quality.%20With%0Aan%20inference%20time%20of%20only%202-5%20minutes%2C%20this%20framework%20achieves%20better%20trade-off%0Abetween%20quality%20and%20speed%20than%20score%20distillation.%20MVEdit%20is%20highly%20versatile%0Aand%20extendable%2C%20with%20a%20wide%20range%20of%20applications%20including%20text/image-to-3D%0Ageneration%2C%203D-to-3D%20editing%2C%20and%20high-quality%20texture%20synthesis.%20In%0Aparticular%2C%20evaluations%20demonstrate%20state-of-the-art%20performance%20in%20both%0Aimage-to-3D%20and%20text-guided%20texture%20generation%20tasks.%20Additionally%2C%20we%0Aintroduce%20a%20method%20for%20fine-tuning%202D%20latent%20diffusion%20models%20on%20small%203D%0Adatasets%20with%20limited%20resources%2C%20enabling%20fast%20low-resolution%20text-to-3D%0Ainitialization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12032v2&entry.124074799=Read"},
{"title": "RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based\n  Recommendation", "author": "Xiaohan Yu and Li Zhang and Xin Zhao and Yue Wang and Zhongrui Ma", "abstract": "  Large language models (LLM) have recently emerged as a powerful tool for a\nvariety of natural language processing tasks, bringing a new surge of combining\nLLM with recommendation systems, termed as LLM-based RS. Current approaches\ngenerally fall into two main paradigms, the ID direct usage paradigm and the ID\ntranslation paradigm, noting their core weakness stems from lacking\nrecommendation knowledge and uniqueness. To address this limitation, we propose\na new paradigm, ID representation, which incorporates pre-trained ID embeddings\ninto LLMs in a complementary manner. In this work, we present RA-Rec, an\nefficient ID representation alignment framework for LLM-based recommendation,\nwhich is compatible with multiple ID-based methods and LLM architectures.\nSpecifically, we treat ID embeddings as soft prompts and design an innovative\nalignment module and an efficient tuning method with tailored data construction\nfor alignment. Extensive experiments demonstrate RA-Rec substantially\noutperforms current state-of-the-art methods, achieving up to 3.0% absolute\nHitRate@100 improvements while utilizing less than 10x training data.\n", "link": "http://arxiv.org/abs/2402.04527v2", "date": "2024-03-19", "relevancy": 2.3911, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4788}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4715}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RA-Rec%3A%20An%20Efficient%20ID%20Representation%20Alignment%20Framework%20for%20LLM-based%0A%20%20Recommendation&body=Title%3A%20RA-Rec%3A%20An%20Efficient%20ID%20Representation%20Alignment%20Framework%20for%20LLM-based%0A%20%20Recommendation%0AAuthor%3A%20Xiaohan%20Yu%20and%20Li%20Zhang%20and%20Xin%20Zhao%20and%20Yue%20Wang%20and%20Zhongrui%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLM%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20a%0Avariety%20of%20natural%20language%20processing%20tasks%2C%20bringing%20a%20new%20surge%20of%20combining%0ALLM%20with%20recommendation%20systems%2C%20termed%20as%20LLM-based%20RS.%20Current%20approaches%0Agenerally%20fall%20into%20two%20main%20paradigms%2C%20the%20ID%20direct%20usage%20paradigm%20and%20the%20ID%0Atranslation%20paradigm%2C%20noting%20their%20core%20weakness%20stems%20from%20lacking%0Arecommendation%20knowledge%20and%20uniqueness.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20new%20paradigm%2C%20ID%20representation%2C%20which%20incorporates%20pre-trained%20ID%20embeddings%0Ainto%20LLMs%20in%20a%20complementary%20manner.%20In%20this%20work%2C%20we%20present%20RA-Rec%2C%20an%0Aefficient%20ID%20representation%20alignment%20framework%20for%20LLM-based%20recommendation%2C%0Awhich%20is%20compatible%20with%20multiple%20ID-based%20methods%20and%20LLM%20architectures.%0ASpecifically%2C%20we%20treat%20ID%20embeddings%20as%20soft%20prompts%20and%20design%20an%20innovative%0Aalignment%20module%20and%20an%20efficient%20tuning%20method%20with%20tailored%20data%20construction%0Afor%20alignment.%20Extensive%20experiments%20demonstrate%20RA-Rec%20substantially%0Aoutperforms%20current%20state-of-the-art%20methods%2C%20achieving%20up%20to%203.0%25%20absolute%0AHitRate%40100%20improvements%20while%20utilizing%20less%20than%2010x%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04527v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RA-Rec%3A%20An%20Efficient%20ID%20Representation%20Alignment%20Framework%20for%20LLM-based%0A%20%20Recommendation&entry.906535625=Xiaohan%20Yu%20and%20Li%20Zhang%20and%20Xin%20Zhao%20and%20Yue%20Wang%20and%20Zhongrui%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLM%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20a%0Avariety%20of%20natural%20language%20processing%20tasks%2C%20bringing%20a%20new%20surge%20of%20combining%0ALLM%20with%20recommendation%20systems%2C%20termed%20as%20LLM-based%20RS.%20Current%20approaches%0Agenerally%20fall%20into%20two%20main%20paradigms%2C%20the%20ID%20direct%20usage%20paradigm%20and%20the%20ID%0Atranslation%20paradigm%2C%20noting%20their%20core%20weakness%20stems%20from%20lacking%0Arecommendation%20knowledge%20and%20uniqueness.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20new%20paradigm%2C%20ID%20representation%2C%20which%20incorporates%20pre-trained%20ID%20embeddings%0Ainto%20LLMs%20in%20a%20complementary%20manner.%20In%20this%20work%2C%20we%20present%20RA-Rec%2C%20an%0Aefficient%20ID%20representation%20alignment%20framework%20for%20LLM-based%20recommendation%2C%0Awhich%20is%20compatible%20with%20multiple%20ID-based%20methods%20and%20LLM%20architectures.%0ASpecifically%2C%20we%20treat%20ID%20embeddings%20as%20soft%20prompts%20and%20design%20an%20innovative%0Aalignment%20module%20and%20an%20efficient%20tuning%20method%20with%20tailored%20data%20construction%0Afor%20alignment.%20Extensive%20experiments%20demonstrate%20RA-Rec%20substantially%0Aoutperforms%20current%20state-of-the-art%20methods%2C%20achieving%20up%20to%203.0%25%20absolute%0AHitRate%40100%20improvements%20while%20utilizing%20less%20than%2010x%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04527v2&entry.124074799=Read"},
{"title": "On the Effectiveness of Heterogeneous Ensemble Methods for\n  Re-identification", "author": "Simon Kl\u00fcttermann and J\u00e9r\u00f4me Rutinowski and Anh Nguyen and Britta Grimme and Moritz Roidl and Emmanuel M\u00fcller", "abstract": "  In this contribution, we introduce a novel ensemble method for the\nre-identification of industrial entities, using images of chipwood pallets and\ngalvanized metal plates as dataset examples. Our algorithms replace commonly\nused, complex siamese neural networks with an ensemble of simplified,\nrudimentary models, providing wider applicability, especially in\nhardware-restricted scenarios. Each ensemble sub-model uses different types of\nextracted features of the given data as its input, allowing for the creation of\neffective ensembles in a fraction of the training duration needed for more\ncomplex state-of-the-art models. We reach state-of-the-art performance at our\ntask, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%,\nand introduce five distinct feature extraction approaches, and study their\ncombination using different ensemble methods.\n", "link": "http://arxiv.org/abs/2403.12606v1", "date": "2024-03-19", "relevancy": 2.3794, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4902}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4693}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4681}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effectiveness%20of%20Heterogeneous%20Ensemble%20Methods%20for%0A%20%20Re-identification&body=Title%3A%20On%20the%20Effectiveness%20of%20Heterogeneous%20Ensemble%20Methods%20for%0A%20%20Re-identification%0AAuthor%3A%20Simon%20Kl%C3%BCttermann%20and%20J%C3%A9r%C3%B4me%20Rutinowski%20and%20Anh%20Nguyen%20and%20Britta%20Grimme%20and%20Moritz%20Roidl%20and%20Emmanuel%20M%C3%BCller%0AAbstract%3A%20%20%20In%20this%20contribution%2C%20we%20introduce%20a%20novel%20ensemble%20method%20for%20the%0Are-identification%20of%20industrial%20entities%2C%20using%20images%20of%20chipwood%20pallets%20and%0Agalvanized%20metal%20plates%20as%20dataset%20examples.%20Our%20algorithms%20replace%20commonly%0Aused%2C%20complex%20siamese%20neural%20networks%20with%20an%20ensemble%20of%20simplified%2C%0Arudimentary%20models%2C%20providing%20wider%20applicability%2C%20especially%20in%0Ahardware-restricted%20scenarios.%20Each%20ensemble%20sub-model%20uses%20different%20types%20of%0Aextracted%20features%20of%20the%20given%20data%20as%20its%20input%2C%20allowing%20for%20the%20creation%20of%0Aeffective%20ensembles%20in%20a%20fraction%20of%20the%20training%20duration%20needed%20for%20more%0Acomplex%20state-of-the-art%20models.%20We%20reach%20state-of-the-art%20performance%20at%20our%0Atask%2C%20with%20a%20Rank-1%20accuracy%20of%20over%2077%25%20and%20a%20Rank-10%20accuracy%20of%20over%2099%25%2C%0Aand%20introduce%20five%20distinct%20feature%20extraction%20approaches%2C%20and%20study%20their%0Acombination%20using%20different%20ensemble%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12606v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effectiveness%20of%20Heterogeneous%20Ensemble%20Methods%20for%0A%20%20Re-identification&entry.906535625=Simon%20Kl%C3%BCttermann%20and%20J%C3%A9r%C3%B4me%20Rutinowski%20and%20Anh%20Nguyen%20and%20Britta%20Grimme%20and%20Moritz%20Roidl%20and%20Emmanuel%20M%C3%BCller&entry.1292438233=%20%20In%20this%20contribution%2C%20we%20introduce%20a%20novel%20ensemble%20method%20for%20the%0Are-identification%20of%20industrial%20entities%2C%20using%20images%20of%20chipwood%20pallets%20and%0Agalvanized%20metal%20plates%20as%20dataset%20examples.%20Our%20algorithms%20replace%20commonly%0Aused%2C%20complex%20siamese%20neural%20networks%20with%20an%20ensemble%20of%20simplified%2C%0Arudimentary%20models%2C%20providing%20wider%20applicability%2C%20especially%20in%0Ahardware-restricted%20scenarios.%20Each%20ensemble%20sub-model%20uses%20different%20types%20of%0Aextracted%20features%20of%20the%20given%20data%20as%20its%20input%2C%20allowing%20for%20the%20creation%20of%0Aeffective%20ensembles%20in%20a%20fraction%20of%20the%20training%20duration%20needed%20for%20more%0Acomplex%20state-of-the-art%20models.%20We%20reach%20state-of-the-art%20performance%20at%20our%0Atask%2C%20with%20a%20Rank-1%20accuracy%20of%20over%2077%25%20and%20a%20Rank-10%20accuracy%20of%20over%2099%25%2C%0Aand%20introduce%20five%20distinct%20feature%20extraction%20approaches%2C%20and%20study%20their%0Acombination%20using%20different%20ensemble%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12606v1&entry.124074799=Read"},
{"title": "SAMAug: Point Prompt Augmentation for Segment Anything Model", "author": "Haixing Dai and Chong Ma and Zhiling Yan and Zhengliang Liu and Enze Shi and Yiwei Li and Peng Shu and Xiaozheng Wei and Lin Zhao and Zihao Wu and Fang Zeng and Dajiang Zhu and Wei Liu and Quanzheng Li and Lichao Sun and Shu Zhang Tianming Liu and Xiang Li", "abstract": "  This paper introduces SAMAug, a novel visual point augmentation method for\nthe Segment Anything Model (SAM) that enhances interactive image segmentation\nperformance. SAMAug generates augmented point prompts to provide more\ninformation about the user's intention to SAM. Starting with an initial point\nprompt, SAM produces an initial mask, which is then fed into our proposed\nSAMAug to generate augmented point prompts. By incorporating these extra\npoints, SAM can generate augmented segmentation masks based on both the\naugmented point prompts and the initial prompt, resulting in improved\nsegmentation performance. We conducted evaluations using four different point\naugmentation strategies: random sampling, sampling based on maximum difference\nentropy, maximum distance, and saliency. Experiment results on the COCO,\nFundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's\nsegmentation results, especially using the maximum distance and saliency.\nSAMAug demonstrates the potential of visual prompt augmentation for computer\nvision. Codes of SAMAug are available at github.com/yhydhx/SAMAug\n", "link": "http://arxiv.org/abs/2307.01187v4", "date": "2024-03-19", "relevancy": 2.3663, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.475}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4557}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SAMAug%3A%20Point%20Prompt%20Augmentation%20for%20Segment%20Anything%20Model&body=Title%3A%20SAMAug%3A%20Point%20Prompt%20Augmentation%20for%20Segment%20Anything%20Model%0AAuthor%3A%20Haixing%20Dai%20and%20Chong%20Ma%20and%20Zhiling%20Yan%20and%20Zhengliang%20Liu%20and%20Enze%20Shi%20and%20Yiwei%20Li%20and%20Peng%20Shu%20and%20Xiaozheng%20Wei%20and%20Lin%20Zhao%20and%20Zihao%20Wu%20and%20Fang%20Zeng%20and%20Dajiang%20Zhu%20and%20Wei%20Liu%20and%20Quanzheng%20Li%20and%20Lichao%20Sun%20and%20Shu%20Zhang%20Tianming%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20This%20paper%20introduces%20SAMAug%2C%20a%20novel%20visual%20point%20augmentation%20method%20for%0Athe%20Segment%20Anything%20Model%20%28SAM%29%20that%20enhances%20interactive%20image%20segmentation%0Aperformance.%20SAMAug%20generates%20augmented%20point%20prompts%20to%20provide%20more%0Ainformation%20about%20the%20user%27s%20intention%20to%20SAM.%20Starting%20with%20an%20initial%20point%0Aprompt%2C%20SAM%20produces%20an%20initial%20mask%2C%20which%20is%20then%20fed%20into%20our%20proposed%0ASAMAug%20to%20generate%20augmented%20point%20prompts.%20By%20incorporating%20these%20extra%0Apoints%2C%20SAM%20can%20generate%20augmented%20segmentation%20masks%20based%20on%20both%20the%0Aaugmented%20point%20prompts%20and%20the%20initial%20prompt%2C%20resulting%20in%20improved%0Asegmentation%20performance.%20We%20conducted%20evaluations%20using%20four%20different%20point%0Aaugmentation%20strategies%3A%20random%20sampling%2C%20sampling%20based%20on%20maximum%20difference%0Aentropy%2C%20maximum%20distance%2C%20and%20saliency.%20Experiment%20results%20on%20the%20COCO%2C%0AFundus%2C%20COVID%20QUEx%2C%20and%20ISIC2018%20datasets%20show%20that%20SAMAug%20can%20boost%20SAM%27s%0Asegmentation%20results%2C%20especially%20using%20the%20maximum%20distance%20and%20saliency.%0ASAMAug%20demonstrates%20the%20potential%20of%20visual%20prompt%20augmentation%20for%20computer%0Avision.%20Codes%20of%20SAMAug%20are%20available%20at%20github.com/yhydhx/SAMAug%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01187v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMAug%3A%20Point%20Prompt%20Augmentation%20for%20Segment%20Anything%20Model&entry.906535625=Haixing%20Dai%20and%20Chong%20Ma%20and%20Zhiling%20Yan%20and%20Zhengliang%20Liu%20and%20Enze%20Shi%20and%20Yiwei%20Li%20and%20Peng%20Shu%20and%20Xiaozheng%20Wei%20and%20Lin%20Zhao%20and%20Zihao%20Wu%20and%20Fang%20Zeng%20and%20Dajiang%20Zhu%20and%20Wei%20Liu%20and%20Quanzheng%20Li%20and%20Lichao%20Sun%20and%20Shu%20Zhang%20Tianming%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20This%20paper%20introduces%20SAMAug%2C%20a%20novel%20visual%20point%20augmentation%20method%20for%0Athe%20Segment%20Anything%20Model%20%28SAM%29%20that%20enhances%20interactive%20image%20segmentation%0Aperformance.%20SAMAug%20generates%20augmented%20point%20prompts%20to%20provide%20more%0Ainformation%20about%20the%20user%27s%20intention%20to%20SAM.%20Starting%20with%20an%20initial%20point%0Aprompt%2C%20SAM%20produces%20an%20initial%20mask%2C%20which%20is%20then%20fed%20into%20our%20proposed%0ASAMAug%20to%20generate%20augmented%20point%20prompts.%20By%20incorporating%20these%20extra%0Apoints%2C%20SAM%20can%20generate%20augmented%20segmentation%20masks%20based%20on%20both%20the%0Aaugmented%20point%20prompts%20and%20the%20initial%20prompt%2C%20resulting%20in%20improved%0Asegmentation%20performance.%20We%20conducted%20evaluations%20using%20four%20different%20point%0Aaugmentation%20strategies%3A%20random%20sampling%2C%20sampling%20based%20on%20maximum%20difference%0Aentropy%2C%20maximum%20distance%2C%20and%20saliency.%20Experiment%20results%20on%20the%20COCO%2C%0AFundus%2C%20COVID%20QUEx%2C%20and%20ISIC2018%20datasets%20show%20that%20SAMAug%20can%20boost%20SAM%27s%0Asegmentation%20results%2C%20especially%20using%20the%20maximum%20distance%20and%20saliency.%0ASAMAug%20demonstrates%20the%20potential%20of%20visual%20prompt%20augmentation%20for%20computer%0Avision.%20Codes%20of%20SAMAug%20are%20available%20at%20github.com/yhydhx/SAMAug%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01187v4&entry.124074799=Read"},
{"title": "PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic\n  Glove Applications", "author": "Chen Hu and Shirui Lyu and Eojin Rho and Daekyum Kim and Shan Luo and Letizia Gionfrida", "abstract": "  Controlling hand exoskeletons to assist individuals with grasping tasks poses\na challenge due to the difficulty in understanding user intentions. We propose\nthat most daily grasping tasks during activities of daily living (ADL) can be\ndeduced by analyzing object geometries (simple and complex) from 3D point\nclouds. The study introduces PointGrasp, a real-time system designed for\nidentifying household scenes semantically, aiming to support and enhance\nassistance during ADL for tailored end-to-end grasping tasks. The system\ncomprises an RGB-D camera with an inertial measurement unit and a\nmicroprocessor integrated into a tendon-driven soft robotic glove. The RGB-D\ncamera processes 3D scenes at a rate exceeding 30 frames per second. The\nproposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple\nand 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies\nand pinpoints reachable objects. This system shows promise in end-to-end\nvision-driven robotic-assisted rehabilitation manual tasks.\n", "link": "http://arxiv.org/abs/2403.12631v1", "date": "2024-03-19", "relevancy": 2.3644, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6184}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5719}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5708}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PointGrasp%3A%20Point%20Cloud-based%20Grasping%20for%20Tendon-driven%20Soft%20Robotic%0A%20%20Glove%20Applications&body=Title%3A%20PointGrasp%3A%20Point%20Cloud-based%20Grasping%20for%20Tendon-driven%20Soft%20Robotic%0A%20%20Glove%20Applications%0AAuthor%3A%20Chen%20Hu%20and%20Shirui%20Lyu%20and%20Eojin%20Rho%20and%20Daekyum%20Kim%20and%20Shan%20Luo%20and%20Letizia%20Gionfrida%0AAbstract%3A%20%20%20Controlling%20hand%20exoskeletons%20to%20assist%20individuals%20with%20grasping%20tasks%20poses%0Aa%20challenge%20due%20to%20the%20difficulty%20in%20understanding%20user%20intentions.%20We%20propose%0Athat%20most%20daily%20grasping%20tasks%20during%20activities%20of%20daily%20living%20%28ADL%29%20can%20be%0Adeduced%20by%20analyzing%20object%20geometries%20%28simple%20and%20complex%29%20from%203D%20point%0Aclouds.%20The%20study%20introduces%20PointGrasp%2C%20a%20real-time%20system%20designed%20for%0Aidentifying%20household%20scenes%20semantically%2C%20aiming%20to%20support%20and%20enhance%0Aassistance%20during%20ADL%20for%20tailored%20end-to-end%20grasping%20tasks.%20The%20system%0Acomprises%20an%20RGB-D%20camera%20with%20an%20inertial%20measurement%20unit%20and%20a%0Amicroprocessor%20integrated%20into%20a%20tendon-driven%20soft%20robotic%20glove.%20The%20RGB-D%0Acamera%20processes%203D%20scenes%20at%20a%20rate%20exceeding%2030%20frames%20per%20second.%20The%0Aproposed%20pipeline%20demonstrates%20an%20average%20RMSE%20of%200.8%20%24%5Cpm%24%200.39%20cm%20for%20simple%0Aand%200.11%20%24%5Cpm%24%200.06%20cm%20for%20complex%20geometries.%20Within%20each%20mode%2C%20it%20identifies%0Aand%20pinpoints%20reachable%20objects.%20This%20system%20shows%20promise%20in%20end-to-end%0Avision-driven%20robotic-assisted%20rehabilitation%20manual%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12631v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointGrasp%3A%20Point%20Cloud-based%20Grasping%20for%20Tendon-driven%20Soft%20Robotic%0A%20%20Glove%20Applications&entry.906535625=Chen%20Hu%20and%20Shirui%20Lyu%20and%20Eojin%20Rho%20and%20Daekyum%20Kim%20and%20Shan%20Luo%20and%20Letizia%20Gionfrida&entry.1292438233=%20%20Controlling%20hand%20exoskeletons%20to%20assist%20individuals%20with%20grasping%20tasks%20poses%0Aa%20challenge%20due%20to%20the%20difficulty%20in%20understanding%20user%20intentions.%20We%20propose%0Athat%20most%20daily%20grasping%20tasks%20during%20activities%20of%20daily%20living%20%28ADL%29%20can%20be%0Adeduced%20by%20analyzing%20object%20geometries%20%28simple%20and%20complex%29%20from%203D%20point%0Aclouds.%20The%20study%20introduces%20PointGrasp%2C%20a%20real-time%20system%20designed%20for%0Aidentifying%20household%20scenes%20semantically%2C%20aiming%20to%20support%20and%20enhance%0Aassistance%20during%20ADL%20for%20tailored%20end-to-end%20grasping%20tasks.%20The%20system%0Acomprises%20an%20RGB-D%20camera%20with%20an%20inertial%20measurement%20unit%20and%20a%0Amicroprocessor%20integrated%20into%20a%20tendon-driven%20soft%20robotic%20glove.%20The%20RGB-D%0Acamera%20processes%203D%20scenes%20at%20a%20rate%20exceeding%2030%20frames%20per%20second.%20The%0Aproposed%20pipeline%20demonstrates%20an%20average%20RMSE%20of%200.8%20%24%5Cpm%24%200.39%20cm%20for%20simple%0Aand%200.11%20%24%5Cpm%24%200.06%20cm%20for%20complex%20geometries.%20Within%20each%20mode%2C%20it%20identifies%0Aand%20pinpoints%20reachable%20objects.%20This%20system%20shows%20promise%20in%20end-to-end%0Avision-driven%20robotic-assisted%20rehabilitation%20manual%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12631v1&entry.124074799=Read"},
{"title": "Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model", "author": "Jiajie Yang", "abstract": "  We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed\nfor ultra-high-resolution image synthesis. PDM utilizes a pyramid latent\nrepresentation, providing a broader design space that enables more flexible,\nstructured, and efficient perceptual compression which enable AutoEncoder and\nNetwork of Diffusion to equip branches and deeper layers. To enhance PDM's\ncapabilities for generative tasks, we propose the integration of\nSpatial-Channel Attention and Res-Skip Connection, along with the utilization\nof Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and\nAutoEncoder. In summary, PDM achieves the synthesis of images with a 2K\nresolution for the first time, demonstrated on two new datasets comprising\nimages of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believe\nthat this work offers an alternative approach to designing scalable image\ngenerative models, while also providing incremental reinforcement for existing\nframeworks.\n", "link": "http://arxiv.org/abs/2403.12915v1", "date": "2024-03-19", "relevancy": 2.3622, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6303}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6293}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Ultra-High-Resolution%20Image%20Synthesis%20with%20Pyramid%20Diffusion%20Model&body=Title%3A%20Ultra-High-Resolution%20Image%20Synthesis%20with%20Pyramid%20Diffusion%20Model%0AAuthor%3A%20Jiajie%20Yang%0AAbstract%3A%20%20%20We%20introduce%20the%20Pyramid%20Diffusion%20Model%20%28PDM%29%2C%20a%20novel%20architecture%20designed%0Afor%20ultra-high-resolution%20image%20synthesis.%20PDM%20utilizes%20a%20pyramid%20latent%0Arepresentation%2C%20providing%20a%20broader%20design%20space%20that%20enables%20more%20flexible%2C%0Astructured%2C%20and%20efficient%20perceptual%20compression%20which%20enable%20AutoEncoder%20and%0ANetwork%20of%20Diffusion%20to%20equip%20branches%20and%20deeper%20layers.%20To%20enhance%20PDM%27s%0Acapabilities%20for%20generative%20tasks%2C%20we%20propose%20the%20integration%20of%0ASpatial-Channel%20Attention%20and%20Res-Skip%20Connection%2C%20along%20with%20the%20utilization%0Aof%20Spectral%20Norm%20and%20Decreasing%20Dropout%20Strategy%20for%20the%20Diffusion%20Network%20and%0AAutoEncoder.%20In%20summary%2C%20PDM%20achieves%20the%20synthesis%20of%20images%20with%20a%202K%0Aresolution%20for%20the%20first%20time%2C%20demonstrated%20on%20two%20new%20datasets%20comprising%0Aimages%20of%20sizes%202048x2048%20pixels%20and%202048x1024%20pixels%20respectively.%20We%20believe%0Athat%20this%20work%20offers%20an%20alternative%20approach%20to%20designing%20scalable%20image%0Agenerative%20models%2C%20while%20also%20providing%20incremental%20reinforcement%20for%20existing%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12915v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-High-Resolution%20Image%20Synthesis%20with%20Pyramid%20Diffusion%20Model&entry.906535625=Jiajie%20Yang&entry.1292438233=%20%20We%20introduce%20the%20Pyramid%20Diffusion%20Model%20%28PDM%29%2C%20a%20novel%20architecture%20designed%0Afor%20ultra-high-resolution%20image%20synthesis.%20PDM%20utilizes%20a%20pyramid%20latent%0Arepresentation%2C%20providing%20a%20broader%20design%20space%20that%20enables%20more%20flexible%2C%0Astructured%2C%20and%20efficient%20perceptual%20compression%20which%20enable%20AutoEncoder%20and%0ANetwork%20of%20Diffusion%20to%20equip%20branches%20and%20deeper%20layers.%20To%20enhance%20PDM%27s%0Acapabilities%20for%20generative%20tasks%2C%20we%20propose%20the%20integration%20of%0ASpatial-Channel%20Attention%20and%20Res-Skip%20Connection%2C%20along%20with%20the%20utilization%0Aof%20Spectral%20Norm%20and%20Decreasing%20Dropout%20Strategy%20for%20the%20Diffusion%20Network%20and%0AAutoEncoder.%20In%20summary%2C%20PDM%20achieves%20the%20synthesis%20of%20images%20with%20a%202K%0Aresolution%20for%20the%20first%20time%2C%20demonstrated%20on%20two%20new%20datasets%20comprising%0Aimages%20of%20sizes%202048x2048%20pixels%20and%202048x1024%20pixels%20respectively.%20We%20believe%0Athat%20this%20work%20offers%20an%20alternative%20approach%20to%20designing%20scalable%20image%0Agenerative%20models%2C%20while%20also%20providing%20incremental%20reinforcement%20for%20existing%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12915v1&entry.124074799=Read"},
{"title": "A Comparison of Deep Learning Architectures for Spacecraft Anomaly\n  Detection", "author": "Daniel Lakey and Tim Schlippe", "abstract": "  Spacecraft operations are highly critical, demanding impeccable reliability\nand safety. Ensuring the optimal performance of a spacecraft requires the early\ndetection and mitigation of anomalies, which could otherwise result in unit or\nmission failures. With the advent of deep learning, a surge of interest has\nbeen seen in leveraging these sophisticated algorithms for anomaly detection in\nspace operations. This study aims to compare the efficacy of various deep\nlearning architectures in detecting anomalies in spacecraft data. The deep\nlearning models under investigation include Convolutional Neural Networks\n(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)\nnetworks, and Transformer-based architectures. Each of these models was trained\nand validated using a comprehensive dataset sourced from multiple spacecraft\nmissions, encompassing diverse operational scenarios and anomaly types. Initial\nresults indicate that while CNNs excel in identifying spatial patterns and may\nbe effective for some classes of spacecraft data, LSTMs and RNNs show a marked\nproficiency in capturing temporal anomalies seen in time-series spacecraft\ntelemetry. The Transformer-based architectures, given their ability to focus on\nboth local and global contexts, have showcased promising results, especially in\nscenarios where anomalies are subtle and span over longer durations.\nAdditionally, considerations such as computational efficiency, ease of\ndeployment, and real-time processing capabilities were evaluated. While CNNs\nand LSTMs demonstrated a balance between accuracy and computational demands,\nTransformer architectures, though highly accurate, require significant\ncomputational resources. In conclusion, the choice of deep learning\narchitecture for spacecraft anomaly detection is highly contingent on the\nnature of the data, the type of anomalies, and operational constraints.\n", "link": "http://arxiv.org/abs/2403.12864v1", "date": "2024-03-19", "relevancy": 2.3611, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4709}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4583}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comparison%20of%20Deep%20Learning%20Architectures%20for%20Spacecraft%20Anomaly%0A%20%20Detection&body=Title%3A%20A%20Comparison%20of%20Deep%20Learning%20Architectures%20for%20Spacecraft%20Anomaly%0A%20%20Detection%0AAuthor%3A%20Daniel%20Lakey%20and%20Tim%20Schlippe%0AAbstract%3A%20%20%20Spacecraft%20operations%20are%20highly%20critical%2C%20demanding%20impeccable%20reliability%0Aand%20safety.%20Ensuring%20the%20optimal%20performance%20of%20a%20spacecraft%20requires%20the%20early%0Adetection%20and%20mitigation%20of%20anomalies%2C%20which%20could%20otherwise%20result%20in%20unit%20or%0Amission%20failures.%20With%20the%20advent%20of%20deep%20learning%2C%20a%20surge%20of%20interest%20has%0Abeen%20seen%20in%20leveraging%20these%20sophisticated%20algorithms%20for%20anomaly%20detection%20in%0Aspace%20operations.%20This%20study%20aims%20to%20compare%20the%20efficacy%20of%20various%20deep%0Alearning%20architectures%20in%20detecting%20anomalies%20in%20spacecraft%20data.%20The%20deep%0Alearning%20models%20under%20investigation%20include%20Convolutional%20Neural%20Networks%0A%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetworks%2C%20and%20Transformer-based%20architectures.%20Each%20of%20these%20models%20was%20trained%0Aand%20validated%20using%20a%20comprehensive%20dataset%20sourced%20from%20multiple%20spacecraft%0Amissions%2C%20encompassing%20diverse%20operational%20scenarios%20and%20anomaly%20types.%20Initial%0Aresults%20indicate%20that%20while%20CNNs%20excel%20in%20identifying%20spatial%20patterns%20and%20may%0Abe%20effective%20for%20some%20classes%20of%20spacecraft%20data%2C%20LSTMs%20and%20RNNs%20show%20a%20marked%0Aproficiency%20in%20capturing%20temporal%20anomalies%20seen%20in%20time-series%20spacecraft%0Atelemetry.%20The%20Transformer-based%20architectures%2C%20given%20their%20ability%20to%20focus%20on%0Aboth%20local%20and%20global%20contexts%2C%20have%20showcased%20promising%20results%2C%20especially%20in%0Ascenarios%20where%20anomalies%20are%20subtle%20and%20span%20over%20longer%20durations.%0AAdditionally%2C%20considerations%20such%20as%20computational%20efficiency%2C%20ease%20of%0Adeployment%2C%20and%20real-time%20processing%20capabilities%20were%20evaluated.%20While%20CNNs%0Aand%20LSTMs%20demonstrated%20a%20balance%20between%20accuracy%20and%20computational%20demands%2C%0ATransformer%20architectures%2C%20though%20highly%20accurate%2C%20require%20significant%0Acomputational%20resources.%20In%20conclusion%2C%20the%20choice%20of%20deep%20learning%0Aarchitecture%20for%20spacecraft%20anomaly%20detection%20is%20highly%20contingent%20on%20the%0Anature%20of%20the%20data%2C%20the%20type%20of%20anomalies%2C%20and%20operational%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12864v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparison%20of%20Deep%20Learning%20Architectures%20for%20Spacecraft%20Anomaly%0A%20%20Detection&entry.906535625=Daniel%20Lakey%20and%20Tim%20Schlippe&entry.1292438233=%20%20Spacecraft%20operations%20are%20highly%20critical%2C%20demanding%20impeccable%20reliability%0Aand%20safety.%20Ensuring%20the%20optimal%20performance%20of%20a%20spacecraft%20requires%20the%20early%0Adetection%20and%20mitigation%20of%20anomalies%2C%20which%20could%20otherwise%20result%20in%20unit%20or%0Amission%20failures.%20With%20the%20advent%20of%20deep%20learning%2C%20a%20surge%20of%20interest%20has%0Abeen%20seen%20in%20leveraging%20these%20sophisticated%20algorithms%20for%20anomaly%20detection%20in%0Aspace%20operations.%20This%20study%20aims%20to%20compare%20the%20efficacy%20of%20various%20deep%0Alearning%20architectures%20in%20detecting%20anomalies%20in%20spacecraft%20data.%20The%20deep%0Alearning%20models%20under%20investigation%20include%20Convolutional%20Neural%20Networks%0A%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%0Anetworks%2C%20and%20Transformer-based%20architectures.%20Each%20of%20these%20models%20was%20trained%0Aand%20validated%20using%20a%20comprehensive%20dataset%20sourced%20from%20multiple%20spacecraft%0Amissions%2C%20encompassing%20diverse%20operational%20scenarios%20and%20anomaly%20types.%20Initial%0Aresults%20indicate%20that%20while%20CNNs%20excel%20in%20identifying%20spatial%20patterns%20and%20may%0Abe%20effective%20for%20some%20classes%20of%20spacecraft%20data%2C%20LSTMs%20and%20RNNs%20show%20a%20marked%0Aproficiency%20in%20capturing%20temporal%20anomalies%20seen%20in%20time-series%20spacecraft%0Atelemetry.%20The%20Transformer-based%20architectures%2C%20given%20their%20ability%20to%20focus%20on%0Aboth%20local%20and%20global%20contexts%2C%20have%20showcased%20promising%20results%2C%20especially%20in%0Ascenarios%20where%20anomalies%20are%20subtle%20and%20span%20over%20longer%20durations.%0AAdditionally%2C%20considerations%20such%20as%20computational%20efficiency%2C%20ease%20of%0Adeployment%2C%20and%20real-time%20processing%20capabilities%20were%20evaluated.%20While%20CNNs%0Aand%20LSTMs%20demonstrated%20a%20balance%20between%20accuracy%20and%20computational%20demands%2C%0ATransformer%20architectures%2C%20though%20highly%20accurate%2C%20require%20significant%0Acomputational%20resources.%20In%20conclusion%2C%20the%20choice%20of%20deep%20learning%0Aarchitecture%20for%20spacecraft%20anomaly%20detection%20is%20highly%20contingent%20on%20the%0Anature%20of%20the%20data%2C%20the%20type%20of%20anomalies%2C%20and%20operational%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12864v1&entry.124074799=Read"},
{"title": "When Layers Play the Lottery, all Tickets Win at Initialization", "author": "Artur Jordao and George Correa de Araujo and Helena de Almeida Maia and Helio Pedrini", "abstract": "  Pruning is a standard technique for reducing the computational cost of deep\nnetworks. Many advances in pruning leverage concepts from the Lottery Ticket\nHypothesis (LTH). LTH reveals that inside a trained dense network exists sparse\nsubnetworks (tickets) able to achieve similar accuracy (i.e., win the lottery -\nwinning tickets). Pruning at initialization focuses on finding winning tickets\nwithout training a dense network. Studies on these concepts share the trend\nthat subnetworks come from weight or filter pruning. In this work, we\ninvestigate LTH and pruning at initialization from the lens of layer pruning.\nFirst, we confirm the existence of winning tickets when the pruning process\nremoves layers. Leveraged by this observation, we propose to discover these\nwinning tickets at initialization, eliminating the requirement of heavy\ncomputational resources for training the initial (over-parameterized) dense\nnetwork. Extensive experiments show that our winning tickets notably speed up\nthe training phase and reduce up to 51% of carbon emission, an important step\ntowards democratization and green Artificial Intelligence. Beyond computational\nbenefits, our winning tickets exhibit robustness against adversarial and\nout-of-distribution examples. Finally, we show that our subnetworks easily win\nthe lottery at initialization while tickets from filter removal (the standard\nstructured LTH) hardly become winning tickets.\n", "link": "http://arxiv.org/abs/2301.10835v2", "date": "2024-03-19", "relevancy": 2.3516, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4777}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4407}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20Layers%20Play%20the%20Lottery%2C%20all%20Tickets%20Win%20at%20Initialization&body=Title%3A%20When%20Layers%20Play%20the%20Lottery%2C%20all%20Tickets%20Win%20at%20Initialization%0AAuthor%3A%20Artur%20Jordao%20and%20George%20Correa%20de%20Araujo%20and%20Helena%20de%20Almeida%20Maia%20and%20Helio%20Pedrini%0AAbstract%3A%20%20%20Pruning%20is%20a%20standard%20technique%20for%20reducing%20the%20computational%20cost%20of%20deep%0Anetworks.%20Many%20advances%20in%20pruning%20leverage%20concepts%20from%20the%20Lottery%20Ticket%0AHypothesis%20%28LTH%29.%20LTH%20reveals%20that%20inside%20a%20trained%20dense%20network%20exists%20sparse%0Asubnetworks%20%28tickets%29%20able%20to%20achieve%20similar%20accuracy%20%28i.e.%2C%20win%20the%20lottery%20-%0Awinning%20tickets%29.%20Pruning%20at%20initialization%20focuses%20on%20finding%20winning%20tickets%0Awithout%20training%20a%20dense%20network.%20Studies%20on%20these%20concepts%20share%20the%20trend%0Athat%20subnetworks%20come%20from%20weight%20or%20filter%20pruning.%20In%20this%20work%2C%20we%0Ainvestigate%20LTH%20and%20pruning%20at%20initialization%20from%20the%20lens%20of%20layer%20pruning.%0AFirst%2C%20we%20confirm%20the%20existence%20of%20winning%20tickets%20when%20the%20pruning%20process%0Aremoves%20layers.%20Leveraged%20by%20this%20observation%2C%20we%20propose%20to%20discover%20these%0Awinning%20tickets%20at%20initialization%2C%20eliminating%20the%20requirement%20of%20heavy%0Acomputational%20resources%20for%20training%20the%20initial%20%28over-parameterized%29%20dense%0Anetwork.%20Extensive%20experiments%20show%20that%20our%20winning%20tickets%20notably%20speed%20up%0Athe%20training%20phase%20and%20reduce%20up%20to%2051%25%20of%20carbon%20emission%2C%20an%20important%20step%0Atowards%20democratization%20and%20green%20Artificial%20Intelligence.%20Beyond%20computational%0Abenefits%2C%20our%20winning%20tickets%20exhibit%20robustness%20against%20adversarial%20and%0Aout-of-distribution%20examples.%20Finally%2C%20we%20show%20that%20our%20subnetworks%20easily%20win%0Athe%20lottery%20at%20initialization%20while%20tickets%20from%20filter%20removal%20%28the%20standard%0Astructured%20LTH%29%20hardly%20become%20winning%20tickets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.10835v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Layers%20Play%20the%20Lottery%2C%20all%20Tickets%20Win%20at%20Initialization&entry.906535625=Artur%20Jordao%20and%20George%20Correa%20de%20Araujo%20and%20Helena%20de%20Almeida%20Maia%20and%20Helio%20Pedrini&entry.1292438233=%20%20Pruning%20is%20a%20standard%20technique%20for%20reducing%20the%20computational%20cost%20of%20deep%0Anetworks.%20Many%20advances%20in%20pruning%20leverage%20concepts%20from%20the%20Lottery%20Ticket%0AHypothesis%20%28LTH%29.%20LTH%20reveals%20that%20inside%20a%20trained%20dense%20network%20exists%20sparse%0Asubnetworks%20%28tickets%29%20able%20to%20achieve%20similar%20accuracy%20%28i.e.%2C%20win%20the%20lottery%20-%0Awinning%20tickets%29.%20Pruning%20at%20initialization%20focuses%20on%20finding%20winning%20tickets%0Awithout%20training%20a%20dense%20network.%20Studies%20on%20these%20concepts%20share%20the%20trend%0Athat%20subnetworks%20come%20from%20weight%20or%20filter%20pruning.%20In%20this%20work%2C%20we%0Ainvestigate%20LTH%20and%20pruning%20at%20initialization%20from%20the%20lens%20of%20layer%20pruning.%0AFirst%2C%20we%20confirm%20the%20existence%20of%20winning%20tickets%20when%20the%20pruning%20process%0Aremoves%20layers.%20Leveraged%20by%20this%20observation%2C%20we%20propose%20to%20discover%20these%0Awinning%20tickets%20at%20initialization%2C%20eliminating%20the%20requirement%20of%20heavy%0Acomputational%20resources%20for%20training%20the%20initial%20%28over-parameterized%29%20dense%0Anetwork.%20Extensive%20experiments%20show%20that%20our%20winning%20tickets%20notably%20speed%20up%0Athe%20training%20phase%20and%20reduce%20up%20to%2051%25%20of%20carbon%20emission%2C%20an%20important%20step%0Atowards%20democratization%20and%20green%20Artificial%20Intelligence.%20Beyond%20computational%0Abenefits%2C%20our%20winning%20tickets%20exhibit%20robustness%20against%20adversarial%20and%0Aout-of-distribution%20examples.%20Finally%2C%20we%20show%20that%20our%20subnetworks%20easily%20win%0Athe%20lottery%20at%20initialization%20while%20tickets%20from%20filter%20removal%20%28the%20standard%0Astructured%20LTH%29%20hardly%20become%20winning%20tickets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.10835v2&entry.124074799=Read"},
{"title": "DreamDA: Generative Data Augmentation with Diffusion Models", "author": "Yunxiang Fu and Chaoqi Chen and Yu Qiao and Yizhou Yu", "abstract": "  The acquisition of large-scale, high-quality data is a resource-intensive and\ntime-consuming endeavor. Compared to conventional Data Augmentation (DA)\ntechniques (e.g. cropping and rotation), exploiting prevailing diffusion models\nfor data generation has received scant attention in classification tasks.\nExisting generative DA methods either inadequately bridge the domain gap\nbetween real-world and synthesized images, or inherently suffer from a lack of\ndiversity. To solve these issues, this paper proposes a new\nclassification-oriented framework DreamDA, which enables data synthesis and\nlabel generation by way of diffusion models. DreamDA generates diverse samples\nthat adhere to the original data distribution by considering training images in\nthe original data as seeds and perturbing their reverse diffusion process. In\naddition, since the labels of the generated data may not align with the labels\nof their corresponding seed images, we introduce a self-training paradigm for\ngenerating pseudo labels and training classifiers using the synthesized data.\nExtensive experiments across four tasks and five datasets demonstrate\nconsistent improvements over strong baselines, revealing the efficacy of\nDreamDA in synthesizing high-quality and diverse images with accurate labels.\nOur code will be available at https://github.com/yunxiangfu2001/DreamDA.\n", "link": "http://arxiv.org/abs/2403.12803v1", "date": "2024-03-19", "relevancy": 2.3505, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5895}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5778}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DreamDA%3A%20Generative%20Data%20Augmentation%20with%20Diffusion%20Models&body=Title%3A%20DreamDA%3A%20Generative%20Data%20Augmentation%20with%20Diffusion%20Models%0AAuthor%3A%20Yunxiang%20Fu%20and%20Chaoqi%20Chen%20and%20Yu%20Qiao%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20The%20acquisition%20of%20large-scale%2C%20high-quality%20data%20is%20a%20resource-intensive%20and%0Atime-consuming%20endeavor.%20Compared%20to%20conventional%20Data%20Augmentation%20%28DA%29%0Atechniques%20%28e.g.%20cropping%20and%20rotation%29%2C%20exploiting%20prevailing%20diffusion%20models%0Afor%20data%20generation%20has%20received%20scant%20attention%20in%20classification%20tasks.%0AExisting%20generative%20DA%20methods%20either%20inadequately%20bridge%20the%20domain%20gap%0Abetween%20real-world%20and%20synthesized%20images%2C%20or%20inherently%20suffer%20from%20a%20lack%20of%0Adiversity.%20To%20solve%20these%20issues%2C%20this%20paper%20proposes%20a%20new%0Aclassification-oriented%20framework%20DreamDA%2C%20which%20enables%20data%20synthesis%20and%0Alabel%20generation%20by%20way%20of%20diffusion%20models.%20DreamDA%20generates%20diverse%20samples%0Athat%20adhere%20to%20the%20original%20data%20distribution%20by%20considering%20training%20images%20in%0Athe%20original%20data%20as%20seeds%20and%20perturbing%20their%20reverse%20diffusion%20process.%20In%0Aaddition%2C%20since%20the%20labels%20of%20the%20generated%20data%20may%20not%20align%20with%20the%20labels%0Aof%20their%20corresponding%20seed%20images%2C%20we%20introduce%20a%20self-training%20paradigm%20for%0Agenerating%20pseudo%20labels%20and%20training%20classifiers%20using%20the%20synthesized%20data.%0AExtensive%20experiments%20across%20four%20tasks%20and%20five%20datasets%20demonstrate%0Aconsistent%20improvements%20over%20strong%20baselines%2C%20revealing%20the%20efficacy%20of%0ADreamDA%20in%20synthesizing%20high-quality%20and%20diverse%20images%20with%20accurate%20labels.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/yunxiangfu2001/DreamDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12803v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamDA%3A%20Generative%20Data%20Augmentation%20with%20Diffusion%20Models&entry.906535625=Yunxiang%20Fu%20and%20Chaoqi%20Chen%20and%20Yu%20Qiao%20and%20Yizhou%20Yu&entry.1292438233=%20%20The%20acquisition%20of%20large-scale%2C%20high-quality%20data%20is%20a%20resource-intensive%20and%0Atime-consuming%20endeavor.%20Compared%20to%20conventional%20Data%20Augmentation%20%28DA%29%0Atechniques%20%28e.g.%20cropping%20and%20rotation%29%2C%20exploiting%20prevailing%20diffusion%20models%0Afor%20data%20generation%20has%20received%20scant%20attention%20in%20classification%20tasks.%0AExisting%20generative%20DA%20methods%20either%20inadequately%20bridge%20the%20domain%20gap%0Abetween%20real-world%20and%20synthesized%20images%2C%20or%20inherently%20suffer%20from%20a%20lack%20of%0Adiversity.%20To%20solve%20these%20issues%2C%20this%20paper%20proposes%20a%20new%0Aclassification-oriented%20framework%20DreamDA%2C%20which%20enables%20data%20synthesis%20and%0Alabel%20generation%20by%20way%20of%20diffusion%20models.%20DreamDA%20generates%20diverse%20samples%0Athat%20adhere%20to%20the%20original%20data%20distribution%20by%20considering%20training%20images%20in%0Athe%20original%20data%20as%20seeds%20and%20perturbing%20their%20reverse%20diffusion%20process.%20In%0Aaddition%2C%20since%20the%20labels%20of%20the%20generated%20data%20may%20not%20align%20with%20the%20labels%0Aof%20their%20corresponding%20seed%20images%2C%20we%20introduce%20a%20self-training%20paradigm%20for%0Agenerating%20pseudo%20labels%20and%20training%20classifiers%20using%20the%20synthesized%20data.%0AExtensive%20experiments%20across%20four%20tasks%20and%20five%20datasets%20demonstrate%0Aconsistent%20improvements%20over%20strong%20baselines%2C%20revealing%20the%20efficacy%20of%0ADreamDA%20in%20synthesizing%20high-quality%20and%20diverse%20images%20with%20accurate%20labels.%0AOur%20code%20will%20be%20available%20at%20https%3A//github.com/yunxiangfu2001/DreamDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12803v1&entry.124074799=Read"},
{"title": "Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater\n  Environments", "author": "Kurran Singh and Jungseok Hong and Nicholas R. Rypkema and John J. Leonard", "abstract": "  Despite recent advances in semantic Simultaneous Localization and Mapping\n(SLAM) for terrestrial and aerial applications, underwater semantic SLAM\nremains an open and largely unaddressed research problem due to the unique\nsensing modalities and the object classes found underwater. This paper presents\nan object-based semantic SLAM method for underwater environments that can\nidentify, localize, classify, and map a wide variety of marine objects without\na priori knowledge of the object classes present in the scene. The method\nperforms unsupervised object segmentation and object-level feature aggregation,\nand then uses opti-acoustic sensor fusion for object localization.\nProbabilistic data association is used to determine observation to landmark\ncorrespondences. Given such correspondences, the method then jointly optimizes\nlandmark and vehicle position estimates. Indoor and outdoor underwater datasets\nwith a wide variety of objects and challenging acoustic and lighting conditions\nare collected for evaluation and made publicly available. Quantitative and\nqualitative results show the proposed method achieves reduced trajectory error\ncompared to baseline methods, and is able to obtain comparable map accuracy to\na baseline closed-set method that requires hand-labeled data of all objects in\nthe scene.\n", "link": "http://arxiv.org/abs/2403.12837v1", "date": "2024-03-19", "relevancy": 2.3457, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6378}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Opti-Acoustic%20Semantic%20SLAM%20with%20Unknown%20Objects%20in%20Underwater%0A%20%20Environments&body=Title%3A%20Opti-Acoustic%20Semantic%20SLAM%20with%20Unknown%20Objects%20in%20Underwater%0A%20%20Environments%0AAuthor%3A%20Kurran%20Singh%20and%20Jungseok%20Hong%20and%20Nicholas%20R.%20Rypkema%20and%20John%20J.%20Leonard%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20semantic%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20for%20terrestrial%20and%20aerial%20applications%2C%20underwater%20semantic%20SLAM%0Aremains%20an%20open%20and%20largely%20unaddressed%20research%20problem%20due%20to%20the%20unique%0Asensing%20modalities%20and%20the%20object%20classes%20found%20underwater.%20This%20paper%20presents%0Aan%20object-based%20semantic%20SLAM%20method%20for%20underwater%20environments%20that%20can%0Aidentify%2C%20localize%2C%20classify%2C%20and%20map%20a%20wide%20variety%20of%20marine%20objects%20without%0Aa%20priori%20knowledge%20of%20the%20object%20classes%20present%20in%20the%20scene.%20The%20method%0Aperforms%20unsupervised%20object%20segmentation%20and%20object-level%20feature%20aggregation%2C%0Aand%20then%20uses%20opti-acoustic%20sensor%20fusion%20for%20object%20localization.%0AProbabilistic%20data%20association%20is%20used%20to%20determine%20observation%20to%20landmark%0Acorrespondences.%20Given%20such%20correspondences%2C%20the%20method%20then%20jointly%20optimizes%0Alandmark%20and%20vehicle%20position%20estimates.%20Indoor%20and%20outdoor%20underwater%20datasets%0Awith%20a%20wide%20variety%20of%20objects%20and%20challenging%20acoustic%20and%20lighting%20conditions%0Aare%20collected%20for%20evaluation%20and%20made%20publicly%20available.%20Quantitative%20and%0Aqualitative%20results%20show%20the%20proposed%20method%20achieves%20reduced%20trajectory%20error%0Acompared%20to%20baseline%20methods%2C%20and%20is%20able%20to%20obtain%20comparable%20map%20accuracy%20to%0Aa%20baseline%20closed-set%20method%20that%20requires%20hand-labeled%20data%20of%20all%20objects%20in%0Athe%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12837v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opti-Acoustic%20Semantic%20SLAM%20with%20Unknown%20Objects%20in%20Underwater%0A%20%20Environments&entry.906535625=Kurran%20Singh%20and%20Jungseok%20Hong%20and%20Nicholas%20R.%20Rypkema%20and%20John%20J.%20Leonard&entry.1292438233=%20%20Despite%20recent%20advances%20in%20semantic%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20for%20terrestrial%20and%20aerial%20applications%2C%20underwater%20semantic%20SLAM%0Aremains%20an%20open%20and%20largely%20unaddressed%20research%20problem%20due%20to%20the%20unique%0Asensing%20modalities%20and%20the%20object%20classes%20found%20underwater.%20This%20paper%20presents%0Aan%20object-based%20semantic%20SLAM%20method%20for%20underwater%20environments%20that%20can%0Aidentify%2C%20localize%2C%20classify%2C%20and%20map%20a%20wide%20variety%20of%20marine%20objects%20without%0Aa%20priori%20knowledge%20of%20the%20object%20classes%20present%20in%20the%20scene.%20The%20method%0Aperforms%20unsupervised%20object%20segmentation%20and%20object-level%20feature%20aggregation%2C%0Aand%20then%20uses%20opti-acoustic%20sensor%20fusion%20for%20object%20localization.%0AProbabilistic%20data%20association%20is%20used%20to%20determine%20observation%20to%20landmark%0Acorrespondences.%20Given%20such%20correspondences%2C%20the%20method%20then%20jointly%20optimizes%0Alandmark%20and%20vehicle%20position%20estimates.%20Indoor%20and%20outdoor%20underwater%20datasets%0Awith%20a%20wide%20variety%20of%20objects%20and%20challenging%20acoustic%20and%20lighting%20conditions%0Aare%20collected%20for%20evaluation%20and%20made%20publicly%20available.%20Quantitative%20and%0Aqualitative%20results%20show%20the%20proposed%20method%20achieves%20reduced%20trajectory%20error%0Acompared%20to%20baseline%20methods%2C%20and%20is%20able%20to%20obtain%20comparable%20map%20accuracy%20to%0Aa%20baseline%20closed-set%20method%20that%20requires%20hand-labeled%20data%20of%20all%20objects%20in%0Athe%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12837v1&entry.124074799=Read"},
{"title": "Towards Controllable Face Generation with Semantic Latent Diffusion\n  Models", "author": "Alex Ergasti and Claudio Ferrari and Tomaso Fontanini and Massimo Bertozzi and Andrea Prati", "abstract": "  Semantic Image Synthesis (SIS) is among the most popular and effective\ntechniques in the field of face generation and editing, thanks to its good\ngeneration quality and the versatility is brings along. Recent works attempted\nto go beyond the standard GAN-based framework, and started to explore Diffusion\nModels (DMs) for this task as these stand out with respect to GANs in terms of\nboth quality and diversity. On the other hand, DMs lack in fine-grained\ncontrollability and reproducibility. To address that, in this paper we propose\na SIS framework based on a novel Latent Diffusion Model architecture for human\nface generation and editing that is both able to reproduce and manipulate a\nreal reference image and generate diversity-driven results. The proposed system\nutilizes both SPADE normalization and cross-attention layers to merge shape and\nstyle information and, by doing so, allows for a precise control over each of\nthe semantic parts of the human face. This was not possible with previous\nmethods in the state of the art. Finally, we performed an extensive set of\nexperiments to prove that our model surpasses current state of the art, both\nqualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2403.12743v1", "date": "2024-03-19", "relevancy": 2.342, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6178}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5694}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5596}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Controllable%20Face%20Generation%20with%20Semantic%20Latent%20Diffusion%0A%20%20Models&body=Title%3A%20Towards%20Controllable%20Face%20Generation%20with%20Semantic%20Latent%20Diffusion%0A%20%20Models%0AAuthor%3A%20Alex%20Ergasti%20and%20Claudio%20Ferrari%20and%20Tomaso%20Fontanini%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati%0AAbstract%3A%20%20%20Semantic%20Image%20Synthesis%20%28SIS%29%20is%20among%20the%20most%20popular%20and%20effective%0Atechniques%20in%20the%20field%20of%20face%20generation%20and%20editing%2C%20thanks%20to%20its%20good%0Ageneration%20quality%20and%20the%20versatility%20is%20brings%20along.%20Recent%20works%20attempted%0Ato%20go%20beyond%20the%20standard%20GAN-based%20framework%2C%20and%20started%20to%20explore%20Diffusion%0AModels%20%28DMs%29%20for%20this%20task%20as%20these%20stand%20out%20with%20respect%20to%20GANs%20in%20terms%20of%0Aboth%20quality%20and%20diversity.%20On%20the%20other%20hand%2C%20DMs%20lack%20in%20fine-grained%0Acontrollability%20and%20reproducibility.%20To%20address%20that%2C%20in%20this%20paper%20we%20propose%0Aa%20SIS%20framework%20based%20on%20a%20novel%20Latent%20Diffusion%20Model%20architecture%20for%20human%0Aface%20generation%20and%20editing%20that%20is%20both%20able%20to%20reproduce%20and%20manipulate%20a%0Areal%20reference%20image%20and%20generate%20diversity-driven%20results.%20The%20proposed%20system%0Autilizes%20both%20SPADE%20normalization%20and%20cross-attention%20layers%20to%20merge%20shape%20and%0Astyle%20information%20and%2C%20by%20doing%20so%2C%20allows%20for%20a%20precise%20control%20over%20each%20of%0Athe%20semantic%20parts%20of%20the%20human%20face.%20This%20was%20not%20possible%20with%20previous%0Amethods%20in%20the%20state%20of%20the%20art.%20Finally%2C%20we%20performed%20an%20extensive%20set%20of%0Aexperiments%20to%20prove%20that%20our%20model%20surpasses%20current%20state%20of%20the%20art%2C%20both%0Aqualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12743v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Controllable%20Face%20Generation%20with%20Semantic%20Latent%20Diffusion%0A%20%20Models&entry.906535625=Alex%20Ergasti%20and%20Claudio%20Ferrari%20and%20Tomaso%20Fontanini%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati&entry.1292438233=%20%20Semantic%20Image%20Synthesis%20%28SIS%29%20is%20among%20the%20most%20popular%20and%20effective%0Atechniques%20in%20the%20field%20of%20face%20generation%20and%20editing%2C%20thanks%20to%20its%20good%0Ageneration%20quality%20and%20the%20versatility%20is%20brings%20along.%20Recent%20works%20attempted%0Ato%20go%20beyond%20the%20standard%20GAN-based%20framework%2C%20and%20started%20to%20explore%20Diffusion%0AModels%20%28DMs%29%20for%20this%20task%20as%20these%20stand%20out%20with%20respect%20to%20GANs%20in%20terms%20of%0Aboth%20quality%20and%20diversity.%20On%20the%20other%20hand%2C%20DMs%20lack%20in%20fine-grained%0Acontrollability%20and%20reproducibility.%20To%20address%20that%2C%20in%20this%20paper%20we%20propose%0Aa%20SIS%20framework%20based%20on%20a%20novel%20Latent%20Diffusion%20Model%20architecture%20for%20human%0Aface%20generation%20and%20editing%20that%20is%20both%20able%20to%20reproduce%20and%20manipulate%20a%0Areal%20reference%20image%20and%20generate%20diversity-driven%20results.%20The%20proposed%20system%0Autilizes%20both%20SPADE%20normalization%20and%20cross-attention%20layers%20to%20merge%20shape%20and%0Astyle%20information%20and%2C%20by%20doing%20so%2C%20allows%20for%20a%20precise%20control%20over%20each%20of%0Athe%20semantic%20parts%20of%20the%20human%20face.%20This%20was%20not%20possible%20with%20previous%0Amethods%20in%20the%20state%20of%20the%20art.%20Finally%2C%20we%20performed%20an%20extensive%20set%20of%0Aexperiments%20to%20prove%20that%20our%20model%20surpasses%20current%20state%20of%20the%20art%2C%20both%0Aqualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12743v1&entry.124074799=Read"},
{"title": "DePT: Decoupled Prompt Tuning", "author": "Ji Zhang and Shihan Wu and Lianli Gao and Heng Tao Shen and Jingkuan Song", "abstract": "  This work breaks through the Base-New Tradeoff (BNT)dilemma in prompt tuning,\ni.e., the better the tuned model generalizes to the base (or target) task, the\nworse it generalizes to new tasks, and vice versa. Specifically, through an\nin-depth analysis of the learned features of the base and new tasks, we observe\nthat the BNT stems from a channel bias issue, i.e., the vast majority of\nfeature channels are occupied by base-specific knowledge, resulting in the\ncollapse of taskshared knowledge important to new tasks. To address this, we\npropose the Decoupled Prompt Tuning (DePT) framework, which decouples\nbase-specific knowledge from feature channels into an isolated feature space\nduring prompt tuning, so as to maximally preserve task-shared knowledge in the\noriginal feature space for achieving better zero-shot generalization on new\ntasks. Importantly, our DePT is orthogonal to existing prompt tuning methods,\nhence it can improve all of them. Extensive experiments on 11 datasets show the\nstrong flexibility and effectiveness of DePT. Our code and pretrained models\nare available at https://github.com/Koorye/DePT.\n", "link": "http://arxiv.org/abs/2309.07439v2", "date": "2024-03-19", "relevancy": 2.3311, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4508}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4452}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DePT%3A%20Decoupled%20Prompt%20Tuning&body=Title%3A%20DePT%3A%20Decoupled%20Prompt%20Tuning%0AAuthor%3A%20Ji%20Zhang%20and%20Shihan%20Wu%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen%20and%20Jingkuan%20Song%0AAbstract%3A%20%20%20This%20work%20breaks%20through%20the%20Base-New%20Tradeoff%20%28BNT%29dilemma%20in%20prompt%20tuning%2C%0Ai.e.%2C%20the%20better%20the%20tuned%20model%20generalizes%20to%20the%20base%20%28or%20target%29%20task%2C%20the%0Aworse%20it%20generalizes%20to%20new%20tasks%2C%20and%20vice%20versa.%20Specifically%2C%20through%20an%0Ain-depth%20analysis%20of%20the%20learned%20features%20of%20the%20base%20and%20new%20tasks%2C%20we%20observe%0Athat%20the%20BNT%20stems%20from%20a%20channel%20bias%20issue%2C%20i.e.%2C%20the%20vast%20majority%20of%0Afeature%20channels%20are%20occupied%20by%20base-specific%20knowledge%2C%20resulting%20in%20the%0Acollapse%20of%20taskshared%20knowledge%20important%20to%20new%20tasks.%20To%20address%20this%2C%20we%0Apropose%20the%20Decoupled%20Prompt%20Tuning%20%28DePT%29%20framework%2C%20which%20decouples%0Abase-specific%20knowledge%20from%20feature%20channels%20into%20an%20isolated%20feature%20space%0Aduring%20prompt%20tuning%2C%20so%20as%20to%20maximally%20preserve%20task-shared%20knowledge%20in%20the%0Aoriginal%20feature%20space%20for%20achieving%20better%20zero-shot%20generalization%20on%20new%0Atasks.%20Importantly%2C%20our%20DePT%20is%20orthogonal%20to%20existing%20prompt%20tuning%20methods%2C%0Ahence%20it%20can%20improve%20all%20of%20them.%20Extensive%20experiments%20on%2011%20datasets%20show%20the%0Astrong%20flexibility%20and%20effectiveness%20of%20DePT.%20Our%20code%20and%20pretrained%20models%0Aare%20available%20at%20https%3A//github.com/Koorye/DePT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07439v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DePT%3A%20Decoupled%20Prompt%20Tuning&entry.906535625=Ji%20Zhang%20and%20Shihan%20Wu%20and%20Lianli%20Gao%20and%20Heng%20Tao%20Shen%20and%20Jingkuan%20Song&entry.1292438233=%20%20This%20work%20breaks%20through%20the%20Base-New%20Tradeoff%20%28BNT%29dilemma%20in%20prompt%20tuning%2C%0Ai.e.%2C%20the%20better%20the%20tuned%20model%20generalizes%20to%20the%20base%20%28or%20target%29%20task%2C%20the%0Aworse%20it%20generalizes%20to%20new%20tasks%2C%20and%20vice%20versa.%20Specifically%2C%20through%20an%0Ain-depth%20analysis%20of%20the%20learned%20features%20of%20the%20base%20and%20new%20tasks%2C%20we%20observe%0Athat%20the%20BNT%20stems%20from%20a%20channel%20bias%20issue%2C%20i.e.%2C%20the%20vast%20majority%20of%0Afeature%20channels%20are%20occupied%20by%20base-specific%20knowledge%2C%20resulting%20in%20the%0Acollapse%20of%20taskshared%20knowledge%20important%20to%20new%20tasks.%20To%20address%20this%2C%20we%0Apropose%20the%20Decoupled%20Prompt%20Tuning%20%28DePT%29%20framework%2C%20which%20decouples%0Abase-specific%20knowledge%20from%20feature%20channels%20into%20an%20isolated%20feature%20space%0Aduring%20prompt%20tuning%2C%20so%20as%20to%20maximally%20preserve%20task-shared%20knowledge%20in%20the%0Aoriginal%20feature%20space%20for%20achieving%20better%20zero-shot%20generalization%20on%20new%0Atasks.%20Importantly%2C%20our%20DePT%20is%20orthogonal%20to%20existing%20prompt%20tuning%20methods%2C%0Ahence%20it%20can%20improve%20all%20of%20them.%20Extensive%20experiments%20on%2011%20datasets%20show%20the%0Astrong%20flexibility%20and%20effectiveness%20of%20DePT.%20Our%20code%20and%20pretrained%20models%0Aare%20available%20at%20https%3A//github.com/Koorye/DePT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07439v2&entry.124074799=Read"},
{"title": "P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap\n  Priors", "author": "Zhou Jiang and Zhenxin Zhu and Pengfei Li and Huan-ang Gao and Tianyuan Yuan and Yongliang Shi and Hang Zhao and Hao Zhao", "abstract": "  Autonomous vehicles are gradually entering city roads today, with the help of\nhigh-definition maps (HDMaps). However, the reliance on HDMaps prevents\nautonomous vehicles from stepping into regions without this expensive digital\ninfrastructure. This fact drives many researchers to study online HDMap\ngeneration algorithms, but the performance of these algorithms at far regions\nis still unsatisfying. We present P-MapNet, in which the letter P highlights\nthe fact that we focus on incorporating map priors to improve model\nperformance. Specifically, we exploit priors in both SDMap and HDMap. On one\nhand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as an\nadditional conditioning branch. Despite the misalignment challenge, our\nattention-based architecture adaptively attends to relevant SDMap skeletons and\nsignificantly improves performance. On the other hand, we exploit a masked\nautoencoder to capture the prior distribution of HDMap, which can serve as a\nrefinement module to mitigate occlusions and artifacts. We benchmark on the\nnuScenes and Argoverse2 datasets. Through comprehensive experiments, we show\nthat: (1) our SDMap prior can improve online map generation performance, using\nboth rasterized (by up to $+18.73$ $\\rm mIoU$) and vectorized (by up to $+8.50$\n$\\rm mAP$) output representations. (2) our HDMap prior can improve map\nperceptual metrics by up to $6.34\\%$. (3) P-MapNet can be switched into\ndifferent inference modes that covers different regions of the\naccuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solution\nthat brings larger improvements on longer ranges. Codes and models are publicly\navailable at https://jike5.github.io/P-MapNet.\n", "link": "http://arxiv.org/abs/2403.10521v2", "date": "2024-03-19", "relevancy": 2.3012, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20P-MapNet%3A%20Far-seeing%20Map%20Generator%20Enhanced%20by%20both%20SDMap%20and%20HDMap%0A%20%20Priors&body=Title%3A%20P-MapNet%3A%20Far-seeing%20Map%20Generator%20Enhanced%20by%20both%20SDMap%20and%20HDMap%0A%20%20Priors%0AAuthor%3A%20Zhou%20Jiang%20and%20Zhenxin%20Zhu%20and%20Pengfei%20Li%20and%20Huan-ang%20Gao%20and%20Tianyuan%20Yuan%20and%20Yongliang%20Shi%20and%20Hang%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Autonomous%20vehicles%20are%20gradually%20entering%20city%20roads%20today%2C%20with%20the%20help%20of%0Ahigh-definition%20maps%20%28HDMaps%29.%20However%2C%20the%20reliance%20on%20HDMaps%20prevents%0Aautonomous%20vehicles%20from%20stepping%20into%20regions%20without%20this%20expensive%20digital%0Ainfrastructure.%20This%20fact%20drives%20many%20researchers%20to%20study%20online%20HDMap%0Ageneration%20algorithms%2C%20but%20the%20performance%20of%20these%20algorithms%20at%20far%20regions%0Ais%20still%20unsatisfying.%20We%20present%20P-MapNet%2C%20in%20which%20the%20letter%20P%20highlights%0Athe%20fact%20that%20we%20focus%20on%20incorporating%20map%20priors%20to%20improve%20model%0Aperformance.%20Specifically%2C%20we%20exploit%20priors%20in%20both%20SDMap%20and%20HDMap.%20On%20one%0Ahand%2C%20we%20extract%20weakly%20aligned%20SDMap%20from%20OpenStreetMap%2C%20and%20encode%20it%20as%20an%0Aadditional%20conditioning%20branch.%20Despite%20the%20misalignment%20challenge%2C%20our%0Aattention-based%20architecture%20adaptively%20attends%20to%20relevant%20SDMap%20skeletons%20and%0Asignificantly%20improves%20performance.%20On%20the%20other%20hand%2C%20we%20exploit%20a%20masked%0Aautoencoder%20to%20capture%20the%20prior%20distribution%20of%20HDMap%2C%20which%20can%20serve%20as%20a%0Arefinement%20module%20to%20mitigate%20occlusions%20and%20artifacts.%20We%20benchmark%20on%20the%0AnuScenes%20and%20Argoverse2%20datasets.%20Through%20comprehensive%20experiments%2C%20we%20show%0Athat%3A%20%281%29%20our%20SDMap%20prior%20can%20improve%20online%20map%20generation%20performance%2C%20using%0Aboth%20rasterized%20%28by%20up%20to%20%24%2B18.73%24%20%24%5Crm%20mIoU%24%29%20and%20vectorized%20%28by%20up%20to%20%24%2B8.50%24%0A%24%5Crm%20mAP%24%29%20output%20representations.%20%282%29%20our%20HDMap%20prior%20can%20improve%20map%0Aperceptual%20metrics%20by%20up%20to%20%246.34%5C%25%24.%20%283%29%20P-MapNet%20can%20be%20switched%20into%0Adifferent%20inference%20modes%20that%20covers%20different%20regions%20of%20the%0Aaccuracy-efficiency%20trade-off%20landscape.%20%284%29%20P-MapNet%20is%20a%20far-seeing%20solution%0Athat%20brings%20larger%20improvements%20on%20longer%20ranges.%20Codes%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//jike5.github.io/P-MapNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10521v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P-MapNet%3A%20Far-seeing%20Map%20Generator%20Enhanced%20by%20both%20SDMap%20and%20HDMap%0A%20%20Priors&entry.906535625=Zhou%20Jiang%20and%20Zhenxin%20Zhu%20and%20Pengfei%20Li%20and%20Huan-ang%20Gao%20and%20Tianyuan%20Yuan%20and%20Yongliang%20Shi%20and%20Hang%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Autonomous%20vehicles%20are%20gradually%20entering%20city%20roads%20today%2C%20with%20the%20help%20of%0Ahigh-definition%20maps%20%28HDMaps%29.%20However%2C%20the%20reliance%20on%20HDMaps%20prevents%0Aautonomous%20vehicles%20from%20stepping%20into%20regions%20without%20this%20expensive%20digital%0Ainfrastructure.%20This%20fact%20drives%20many%20researchers%20to%20study%20online%20HDMap%0Ageneration%20algorithms%2C%20but%20the%20performance%20of%20these%20algorithms%20at%20far%20regions%0Ais%20still%20unsatisfying.%20We%20present%20P-MapNet%2C%20in%20which%20the%20letter%20P%20highlights%0Athe%20fact%20that%20we%20focus%20on%20incorporating%20map%20priors%20to%20improve%20model%0Aperformance.%20Specifically%2C%20we%20exploit%20priors%20in%20both%20SDMap%20and%20HDMap.%20On%20one%0Ahand%2C%20we%20extract%20weakly%20aligned%20SDMap%20from%20OpenStreetMap%2C%20and%20encode%20it%20as%20an%0Aadditional%20conditioning%20branch.%20Despite%20the%20misalignment%20challenge%2C%20our%0Aattention-based%20architecture%20adaptively%20attends%20to%20relevant%20SDMap%20skeletons%20and%0Asignificantly%20improves%20performance.%20On%20the%20other%20hand%2C%20we%20exploit%20a%20masked%0Aautoencoder%20to%20capture%20the%20prior%20distribution%20of%20HDMap%2C%20which%20can%20serve%20as%20a%0Arefinement%20module%20to%20mitigate%20occlusions%20and%20artifacts.%20We%20benchmark%20on%20the%0AnuScenes%20and%20Argoverse2%20datasets.%20Through%20comprehensive%20experiments%2C%20we%20show%0Athat%3A%20%281%29%20our%20SDMap%20prior%20can%20improve%20online%20map%20generation%20performance%2C%20using%0Aboth%20rasterized%20%28by%20up%20to%20%24%2B18.73%24%20%24%5Crm%20mIoU%24%29%20and%20vectorized%20%28by%20up%20to%20%24%2B8.50%24%0A%24%5Crm%20mAP%24%29%20output%20representations.%20%282%29%20our%20HDMap%20prior%20can%20improve%20map%0Aperceptual%20metrics%20by%20up%20to%20%246.34%5C%25%24.%20%283%29%20P-MapNet%20can%20be%20switched%20into%0Adifferent%20inference%20modes%20that%20covers%20different%20regions%20of%20the%0Aaccuracy-efficiency%20trade-off%20landscape.%20%284%29%20P-MapNet%20is%20a%20far-seeing%20solution%0Athat%20brings%20larger%20improvements%20on%20longer%20ranges.%20Codes%20and%20models%20are%20publicly%0Aavailable%20at%20https%3A//jike5.github.io/P-MapNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10521v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Image Super-Resolution and Deblurring", "author": "J\u00e9r\u00e9my Scanvic and Mike Davies and Patrice Abry and Juli\u00e1n Tachella", "abstract": "  Self-supervised methods have recently proved to be nearly as effective as\nsupervised methods in various imaging inverse problems, paving the way for\nlearning-based methods in scientific and medical imaging applications where\nground truth data is hard or expensive to obtain. This is the case in magnetic\nresonance imaging and computed tomography. These methods critically rely on\ninvariance to translations and/or rotations of the image distribution to learn\nfrom incomplete measurement data alone. However, existing approaches fail to\nobtain competitive performances in the problems of image super-resolution and\ndeblurring, which play a key role in most imaging systems. In this work, we\nshow that invariance to translations and rotations is insufficient to learn\nfrom measurements that only contain low-frequency information. Instead, we\npropose a new self-supervised approach that leverages the fact that many image\ndistributions are approximately scale-invariant, and that enables recovering\nhigh-frequency information lost in the measurement process. We demonstrate\nthroughout a series of experiments on real datasets that the proposed method\noutperforms other self-supervised approaches, and obtains performances on par\nwith fully supervised learning.\n", "link": "http://arxiv.org/abs/2312.11232v2", "date": "2024-03-19", "relevancy": 2.298, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6276}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5374}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Image%20Super-Resolution%20and%20Deblurring&body=Title%3A%20Self-Supervised%20Learning%20for%20Image%20Super-Resolution%20and%20Deblurring%0AAuthor%3A%20J%C3%A9r%C3%A9my%20Scanvic%20and%20Mike%20Davies%20and%20Patrice%20Abry%20and%20Juli%C3%A1n%20Tachella%0AAbstract%3A%20%20%20Self-supervised%20methods%20have%20recently%20proved%20to%20be%20nearly%20as%20effective%20as%0Asupervised%20methods%20in%20various%20imaging%20inverse%20problems%2C%20paving%20the%20way%20for%0Alearning-based%20methods%20in%20scientific%20and%20medical%20imaging%20applications%20where%0Aground%20truth%20data%20is%20hard%20or%20expensive%20to%20obtain.%20This%20is%20the%20case%20in%20magnetic%0Aresonance%20imaging%20and%20computed%20tomography.%20These%20methods%20critically%20rely%20on%0Ainvariance%20to%20translations%20and/or%20rotations%20of%20the%20image%20distribution%20to%20learn%0Afrom%20incomplete%20measurement%20data%20alone.%20However%2C%20existing%20approaches%20fail%20to%0Aobtain%20competitive%20performances%20in%20the%20problems%20of%20image%20super-resolution%20and%0Adeblurring%2C%20which%20play%20a%20key%20role%20in%20most%20imaging%20systems.%20In%20this%20work%2C%20we%0Ashow%20that%20invariance%20to%20translations%20and%20rotations%20is%20insufficient%20to%20learn%0Afrom%20measurements%20that%20only%20contain%20low-frequency%20information.%20Instead%2C%20we%0Apropose%20a%20new%20self-supervised%20approach%20that%20leverages%20the%20fact%20that%20many%20image%0Adistributions%20are%20approximately%20scale-invariant%2C%20and%20that%20enables%20recovering%0Ahigh-frequency%20information%20lost%20in%20the%20measurement%20process.%20We%20demonstrate%0Athroughout%20a%20series%20of%20experiments%20on%20real%20datasets%20that%20the%20proposed%20method%0Aoutperforms%20other%20self-supervised%20approaches%2C%20and%20obtains%20performances%20on%20par%0Awith%20fully%20supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11232v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Image%20Super-Resolution%20and%20Deblurring&entry.906535625=J%C3%A9r%C3%A9my%20Scanvic%20and%20Mike%20Davies%20and%20Patrice%20Abry%20and%20Juli%C3%A1n%20Tachella&entry.1292438233=%20%20Self-supervised%20methods%20have%20recently%20proved%20to%20be%20nearly%20as%20effective%20as%0Asupervised%20methods%20in%20various%20imaging%20inverse%20problems%2C%20paving%20the%20way%20for%0Alearning-based%20methods%20in%20scientific%20and%20medical%20imaging%20applications%20where%0Aground%20truth%20data%20is%20hard%20or%20expensive%20to%20obtain.%20This%20is%20the%20case%20in%20magnetic%0Aresonance%20imaging%20and%20computed%20tomography.%20These%20methods%20critically%20rely%20on%0Ainvariance%20to%20translations%20and/or%20rotations%20of%20the%20image%20distribution%20to%20learn%0Afrom%20incomplete%20measurement%20data%20alone.%20However%2C%20existing%20approaches%20fail%20to%0Aobtain%20competitive%20performances%20in%20the%20problems%20of%20image%20super-resolution%20and%0Adeblurring%2C%20which%20play%20a%20key%20role%20in%20most%20imaging%20systems.%20In%20this%20work%2C%20we%0Ashow%20that%20invariance%20to%20translations%20and%20rotations%20is%20insufficient%20to%20learn%0Afrom%20measurements%20that%20only%20contain%20low-frequency%20information.%20Instead%2C%20we%0Apropose%20a%20new%20self-supervised%20approach%20that%20leverages%20the%20fact%20that%20many%20image%0Adistributions%20are%20approximately%20scale-invariant%2C%20and%20that%20enables%20recovering%0Ahigh-frequency%20information%20lost%20in%20the%20measurement%20process.%20We%20demonstrate%0Athroughout%20a%20series%20of%20experiments%20on%20real%20datasets%20that%20the%20proposed%20method%0Aoutperforms%20other%20self-supervised%20approaches%2C%20and%20obtains%20performances%20on%20par%0Awith%20fully%20supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11232v2&entry.124074799=Read"},
{"title": "In-Hand Following of Deformable Linear Objects Using Dexterous Fingers\n  with Tactile Sensing", "author": "Mingrui Yu and Boyuan Liang and Xiang Zhang and Xinghao Zhu and Xiang Li and Masayoshi Tomizuka", "abstract": "  Most research on deformable linear object (DLO) manipulation assumes rigid\ngrasping. However, beyond rigid grasping and re-grasping, in-hand following is\nalso an essential skill that humans use to dexterously manipulate DLOs, which\nrequires continuously changing the grasp point by in-hand sliding while holding\nthe DLO to prevent it from falling. Achieving such a skill is very challenging\nfor robots without using specially designed but not versatile end-effectors.\nPrevious works have attempted using generic parallel grippers, but their\nrobustness is unsatisfactory owing to the conflict between following and\nholding, which is hard to balance with a one-degree-of-freedom gripper. In this\nwork, inspired by how humans use fingers to follow DLOs, we explore the usage\nof a generic dexterous hand with tactile sensing to imitate human skills and\nachieve robust in-hand DLO following. To enable the hardware system to function\nin the real world, we develop a framework that includes Cartesian-space\narm-hand control, tactile-based in-hand 3-D DLO pose estimation, and\ntask-specific motion design. Experimental results demonstrate the significant\nsuperiority of our method over using parallel grippers, as well as its great\nrobustness, generalizability, and efficiency.\n", "link": "http://arxiv.org/abs/2403.12676v1", "date": "2024-03-19", "relevancy": 2.2958, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6209}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.545}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing&body=Title%3A%20In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing%0AAuthor%3A%20Mingrui%20Yu%20and%20Boyuan%20Liang%20and%20Xiang%20Zhang%20and%20Xinghao%20Zhu%20and%20Xiang%20Li%20and%20Masayoshi%20Tomizuka%0AAbstract%3A%20%20%20Most%20research%20on%20deformable%20linear%20object%20%28DLO%29%20manipulation%20assumes%20rigid%0Agrasping.%20However%2C%20beyond%20rigid%20grasping%20and%20re-grasping%2C%20in-hand%20following%20is%0Aalso%20an%20essential%20skill%20that%20humans%20use%20to%20dexterously%20manipulate%20DLOs%2C%20which%0Arequires%20continuously%20changing%20the%20grasp%20point%20by%20in-hand%20sliding%20while%20holding%0Athe%20DLO%20to%20prevent%20it%20from%20falling.%20Achieving%20such%20a%20skill%20is%20very%20challenging%0Afor%20robots%20without%20using%20specially%20designed%20but%20not%20versatile%20end-effectors.%0APrevious%20works%20have%20attempted%20using%20generic%20parallel%20grippers%2C%20but%20their%0Arobustness%20is%20unsatisfactory%20owing%20to%20the%20conflict%20between%20following%20and%0Aholding%2C%20which%20is%20hard%20to%20balance%20with%20a%20one-degree-of-freedom%20gripper.%20In%20this%0Awork%2C%20inspired%20by%20how%20humans%20use%20fingers%20to%20follow%20DLOs%2C%20we%20explore%20the%20usage%0Aof%20a%20generic%20dexterous%20hand%20with%20tactile%20sensing%20to%20imitate%20human%20skills%20and%0Aachieve%20robust%20in-hand%20DLO%20following.%20To%20enable%20the%20hardware%20system%20to%20function%0Ain%20the%20real%20world%2C%20we%20develop%20a%20framework%20that%20includes%20Cartesian-space%0Aarm-hand%20control%2C%20tactile-based%20in-hand%203-D%20DLO%20pose%20estimation%2C%20and%0Atask-specific%20motion%20design.%20Experimental%20results%20demonstrate%20the%20significant%0Asuperiority%20of%20our%20method%20over%20using%20parallel%20grippers%2C%20as%20well%20as%20its%20great%0Arobustness%2C%20generalizability%2C%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12676v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Hand%20Following%20of%20Deformable%20Linear%20Objects%20Using%20Dexterous%20Fingers%0A%20%20with%20Tactile%20Sensing&entry.906535625=Mingrui%20Yu%20and%20Boyuan%20Liang%20and%20Xiang%20Zhang%20and%20Xinghao%20Zhu%20and%20Xiang%20Li%20and%20Masayoshi%20Tomizuka&entry.1292438233=%20%20Most%20research%20on%20deformable%20linear%20object%20%28DLO%29%20manipulation%20assumes%20rigid%0Agrasping.%20However%2C%20beyond%20rigid%20grasping%20and%20re-grasping%2C%20in-hand%20following%20is%0Aalso%20an%20essential%20skill%20that%20humans%20use%20to%20dexterously%20manipulate%20DLOs%2C%20which%0Arequires%20continuously%20changing%20the%20grasp%20point%20by%20in-hand%20sliding%20while%20holding%0Athe%20DLO%20to%20prevent%20it%20from%20falling.%20Achieving%20such%20a%20skill%20is%20very%20challenging%0Afor%20robots%20without%20using%20specially%20designed%20but%20not%20versatile%20end-effectors.%0APrevious%20works%20have%20attempted%20using%20generic%20parallel%20grippers%2C%20but%20their%0Arobustness%20is%20unsatisfactory%20owing%20to%20the%20conflict%20between%20following%20and%0Aholding%2C%20which%20is%20hard%20to%20balance%20with%20a%20one-degree-of-freedom%20gripper.%20In%20this%0Awork%2C%20inspired%20by%20how%20humans%20use%20fingers%20to%20follow%20DLOs%2C%20we%20explore%20the%20usage%0Aof%20a%20generic%20dexterous%20hand%20with%20tactile%20sensing%20to%20imitate%20human%20skills%20and%0Aachieve%20robust%20in-hand%20DLO%20following.%20To%20enable%20the%20hardware%20system%20to%20function%0Ain%20the%20real%20world%2C%20we%20develop%20a%20framework%20that%20includes%20Cartesian-space%0Aarm-hand%20control%2C%20tactile-based%20in-hand%203-D%20DLO%20pose%20estimation%2C%20and%0Atask-specific%20motion%20design.%20Experimental%20results%20demonstrate%20the%20significant%0Asuperiority%20of%20our%20method%20over%20using%20parallel%20grippers%2C%20as%20well%20as%20its%20great%0Arobustness%2C%20generalizability%2C%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12676v1&entry.124074799=Read"},
{"title": "Incorporating Higher-order Structural Information for Graph Clustering", "author": "Qiankun Li and Haobing Liu and Ruobing Jiang and Tingting Wang", "abstract": "  Clustering holds profound significance in data mining. In recent years, graph\nconvolutional network (GCN) has emerged as a powerful tool for deep clustering,\nintegrating both graph structural information and node attributes. However,\nmost existing methods ignore the higher-order structural information of the\ngraph. Evidently, nodes within the same cluster can establish distant\nconnections. Besides, recent deep clustering methods usually apply a\nself-supervised module to monitor the training process of their model, focusing\nsolely on node attributes without paying attention to graph structure. In this\npaper, we propose a novel graph clustering network to make full use of graph\nstructural information. To capture the higher-order structural information, we\ndesign a graph mutual infomax module, effectively maximizing mutual information\nbetween graph-level and node-level representations, and employ a trinary\nself-supervised module that includes modularity as a structural constraint. Our\nproposed model outperforms many state-of-the-art methods on various datasets,\ndemonstrating its superiority.\n", "link": "http://arxiv.org/abs/2403.11087v2", "date": "2024-03-19", "relevancy": 2.2902, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4989}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4386}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4366}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Higher-order%20Structural%20Information%20for%20Graph%20Clustering&body=Title%3A%20Incorporating%20Higher-order%20Structural%20Information%20for%20Graph%20Clustering%0AAuthor%3A%20Qiankun%20Li%20and%20Haobing%20Liu%20and%20Ruobing%20Jiang%20and%20Tingting%20Wang%0AAbstract%3A%20%20%20Clustering%20holds%20profound%20significance%20in%20data%20mining.%20In%20recent%20years%2C%20graph%0Aconvolutional%20network%20%28GCN%29%20has%20emerged%20as%20a%20powerful%20tool%20for%20deep%20clustering%2C%0Aintegrating%20both%20graph%20structural%20information%20and%20node%20attributes.%20However%2C%0Amost%20existing%20methods%20ignore%20the%20higher-order%20structural%20information%20of%20the%0Agraph.%20Evidently%2C%20nodes%20within%20the%20same%20cluster%20can%20establish%20distant%0Aconnections.%20Besides%2C%20recent%20deep%20clustering%20methods%20usually%20apply%20a%0Aself-supervised%20module%20to%20monitor%20the%20training%20process%20of%20their%20model%2C%20focusing%0Asolely%20on%20node%20attributes%20without%20paying%20attention%20to%20graph%20structure.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20graph%20clustering%20network%20to%20make%20full%20use%20of%20graph%0Astructural%20information.%20To%20capture%20the%20higher-order%20structural%20information%2C%20we%0Adesign%20a%20graph%20mutual%20infomax%20module%2C%20effectively%20maximizing%20mutual%20information%0Abetween%20graph-level%20and%20node-level%20representations%2C%20and%20employ%20a%20trinary%0Aself-supervised%20module%20that%20includes%20modularity%20as%20a%20structural%20constraint.%20Our%0Aproposed%20model%20outperforms%20many%20state-of-the-art%20methods%20on%20various%20datasets%2C%0Ademonstrating%20its%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11087v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Higher-order%20Structural%20Information%20for%20Graph%20Clustering&entry.906535625=Qiankun%20Li%20and%20Haobing%20Liu%20and%20Ruobing%20Jiang%20and%20Tingting%20Wang&entry.1292438233=%20%20Clustering%20holds%20profound%20significance%20in%20data%20mining.%20In%20recent%20years%2C%20graph%0Aconvolutional%20network%20%28GCN%29%20has%20emerged%20as%20a%20powerful%20tool%20for%20deep%20clustering%2C%0Aintegrating%20both%20graph%20structural%20information%20and%20node%20attributes.%20However%2C%0Amost%20existing%20methods%20ignore%20the%20higher-order%20structural%20information%20of%20the%0Agraph.%20Evidently%2C%20nodes%20within%20the%20same%20cluster%20can%20establish%20distant%0Aconnections.%20Besides%2C%20recent%20deep%20clustering%20methods%20usually%20apply%20a%0Aself-supervised%20module%20to%20monitor%20the%20training%20process%20of%20their%20model%2C%20focusing%0Asolely%20on%20node%20attributes%20without%20paying%20attention%20to%20graph%20structure.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20graph%20clustering%20network%20to%20make%20full%20use%20of%20graph%0Astructural%20information.%20To%20capture%20the%20higher-order%20structural%20information%2C%20we%0Adesign%20a%20graph%20mutual%20infomax%20module%2C%20effectively%20maximizing%20mutual%20information%0Abetween%20graph-level%20and%20node-level%20representations%2C%20and%20employ%20a%20trinary%0Aself-supervised%20module%20that%20includes%20modularity%20as%20a%20structural%20constraint.%20Our%0Aproposed%20model%20outperforms%20many%20state-of-the-art%20methods%20on%20various%20datasets%2C%0Ademonstrating%20its%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11087v2&entry.124074799=Read"},
{"title": "Addressing Source Scale Bias via Image Warping for Domain Adaptation", "author": "Shen Zheng and Anurag Ghosh and Srinivasa G. Narasimhan", "abstract": "  In visual recognition, scale bias is a key challenge due to the imbalance of\nobject and image size distribution inherent in real scene datasets.\nConventional solutions involve injecting scale invariance priors, oversampling\nthe dataset at different scales during training, or adjusting scale at\ninference. While these strategies mitigate scale bias to some extent, their\nability to adapt across diverse datasets is limited. Besides, they increase\ncomputational load during training and latency during inference. In this work,\nwe use adaptive attentional processing -- oversampling salient object regions\nby warping images in-place during training. Discovering that shifting the\nsource scale distribution improves backbone features, we developed a\ninstance-level warping guidance aimed at object region sampling to mitigate\nsource scale bias in domain adaptation. Our approach improves adaptation across\ngeographies, lighting and weather conditions, is agnostic to the task, domain\nadaptation algorithm, saliency guidance, and underlying model architecture.\nHighlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7\nmAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear\n$\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC. Our\napproach adds minimal memory during training and has no additional latency at\ninference time. Please see Appendix for more results and analysis.\n", "link": "http://arxiv.org/abs/2403.12712v1", "date": "2024-03-19", "relevancy": 2.2898, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.59}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5483}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Addressing%20Source%20Scale%20Bias%20via%20Image%20Warping%20for%20Domain%20Adaptation&body=Title%3A%20Addressing%20Source%20Scale%20Bias%20via%20Image%20Warping%20for%20Domain%20Adaptation%0AAuthor%3A%20Shen%20Zheng%20and%20Anurag%20Ghosh%20and%20Srinivasa%20G.%20Narasimhan%0AAbstract%3A%20%20%20In%20visual%20recognition%2C%20scale%20bias%20is%20a%20key%20challenge%20due%20to%20the%20imbalance%20of%0Aobject%20and%20image%20size%20distribution%20inherent%20in%20real%20scene%20datasets.%0AConventional%20solutions%20involve%20injecting%20scale%20invariance%20priors%2C%20oversampling%0Athe%20dataset%20at%20different%20scales%20during%20training%2C%20or%20adjusting%20scale%20at%0Ainference.%20While%20these%20strategies%20mitigate%20scale%20bias%20to%20some%20extent%2C%20their%0Aability%20to%20adapt%20across%20diverse%20datasets%20is%20limited.%20Besides%2C%20they%20increase%0Acomputational%20load%20during%20training%20and%20latency%20during%20inference.%20In%20this%20work%2C%0Awe%20use%20adaptive%20attentional%20processing%20--%20oversampling%20salient%20object%20regions%0Aby%20warping%20images%20in-place%20during%20training.%20Discovering%20that%20shifting%20the%0Asource%20scale%20distribution%20improves%20backbone%20features%2C%20we%20developed%20a%0Ainstance-level%20warping%20guidance%20aimed%20at%20object%20region%20sampling%20to%20mitigate%0Asource%20scale%20bias%20in%20domain%20adaptation.%20Our%20approach%20improves%20adaptation%20across%0Ageographies%2C%20lighting%20and%20weather%20conditions%2C%20is%20agnostic%20to%20the%20task%2C%20domain%0Aadaptation%20algorithm%2C%20saliency%20guidance%2C%20and%20underlying%20model%20architecture.%0AHighlights%20include%20%2B6.1%20mAP50%20for%20BDD100K%20Clear%20%24%5Crightarrow%24%20DENSE%20Foggy%2C%20%2B3.7%0AmAP50%20for%20BDD100K%20Day%20%24%5Crightarrow%24%20Night%2C%20%2B3.0%20mAP50%20for%20BDD100K%20Clear%0A%24%5Crightarrow%24%20Rainy%2C%20and%20%2B6.3%20mIoU%20for%20Cityscapes%20%24%5Crightarrow%24%20ACDC.%20Our%0Aapproach%20adds%20minimal%20memory%20during%20training%20and%20has%20no%20additional%20latency%20at%0Ainference%20time.%20Please%20see%20Appendix%20for%20more%20results%20and%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12712v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Source%20Scale%20Bias%20via%20Image%20Warping%20for%20Domain%20Adaptation&entry.906535625=Shen%20Zheng%20and%20Anurag%20Ghosh%20and%20Srinivasa%20G.%20Narasimhan&entry.1292438233=%20%20In%20visual%20recognition%2C%20scale%20bias%20is%20a%20key%20challenge%20due%20to%20the%20imbalance%20of%0Aobject%20and%20image%20size%20distribution%20inherent%20in%20real%20scene%20datasets.%0AConventional%20solutions%20involve%20injecting%20scale%20invariance%20priors%2C%20oversampling%0Athe%20dataset%20at%20different%20scales%20during%20training%2C%20or%20adjusting%20scale%20at%0Ainference.%20While%20these%20strategies%20mitigate%20scale%20bias%20to%20some%20extent%2C%20their%0Aability%20to%20adapt%20across%20diverse%20datasets%20is%20limited.%20Besides%2C%20they%20increase%0Acomputational%20load%20during%20training%20and%20latency%20during%20inference.%20In%20this%20work%2C%0Awe%20use%20adaptive%20attentional%20processing%20--%20oversampling%20salient%20object%20regions%0Aby%20warping%20images%20in-place%20during%20training.%20Discovering%20that%20shifting%20the%0Asource%20scale%20distribution%20improves%20backbone%20features%2C%20we%20developed%20a%0Ainstance-level%20warping%20guidance%20aimed%20at%20object%20region%20sampling%20to%20mitigate%0Asource%20scale%20bias%20in%20domain%20adaptation.%20Our%20approach%20improves%20adaptation%20across%0Ageographies%2C%20lighting%20and%20weather%20conditions%2C%20is%20agnostic%20to%20the%20task%2C%20domain%0Aadaptation%20algorithm%2C%20saliency%20guidance%2C%20and%20underlying%20model%20architecture.%0AHighlights%20include%20%2B6.1%20mAP50%20for%20BDD100K%20Clear%20%24%5Crightarrow%24%20DENSE%20Foggy%2C%20%2B3.7%0AmAP50%20for%20BDD100K%20Day%20%24%5Crightarrow%24%20Night%2C%20%2B3.0%20mAP50%20for%20BDD100K%20Clear%0A%24%5Crightarrow%24%20Rainy%2C%20and%20%2B6.3%20mIoU%20for%20Cityscapes%20%24%5Crightarrow%24%20ACDC.%20Our%0Aapproach%20adds%20minimal%20memory%20during%20training%20and%20has%20no%20additional%20latency%20at%0Ainference%20time.%20Please%20see%20Appendix%20for%20more%20results%20and%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12712v1&entry.124074799=Read"},
{"title": "TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation", "author": "Yufei Liu and Junwei Zhu and Junshu Tang and Shijie Zhang and Jiangning Zhang and Weijian Cao and Chengjie Wang and Yunsheng Wu and Dongjin Huang", "abstract": "  Texturing 3D humans with semantic UV maps remains a challenge due to the\ndifficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D\nadvancements in supervising multi-view renderings using large text-to-image\n(T2I) models, issues persist with generation speed, text consistency, and\ntexture quality, resulting in data scarcity among existing datasets. We present\nTexDreamer, the first zero-shot multimodal high-fidelity 3D human texture\ngeneration model. Utilizing an efficient texture adaptation finetuning\nstrategy, we adapt large T2I model to a semantic UV structure while preserving\nits original generalization capability. Leveraging a novel feature translator\nmodule, the trained model is capable of generating high-fidelity 3D human\ntextures from either text or image within seconds. Furthermore, we introduce\nArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024)\n3D human texture dataset which contains 50k high-fidelity textures with text\ndescriptions.\n", "link": "http://arxiv.org/abs/2403.12906v1", "date": "2024-03-19", "relevancy": 2.2894, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6064}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5576}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TexDreamer%3A%20Towards%20Zero-Shot%20High-Fidelity%203D%20Human%20Texture%20Generation&body=Title%3A%20TexDreamer%3A%20Towards%20Zero-Shot%20High-Fidelity%203D%20Human%20Texture%20Generation%0AAuthor%3A%20Yufei%20Liu%20and%20Junwei%20Zhu%20and%20Junshu%20Tang%20and%20Shijie%20Zhang%20and%20Jiangning%20Zhang%20and%20Weijian%20Cao%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Dongjin%20Huang%0AAbstract%3A%20%20%20Texturing%203D%20humans%20with%20semantic%20UV%20maps%20remains%20a%20challenge%20due%20to%20the%0Adifficulty%20of%20acquiring%20reasonably%20unfolded%20UV.%20Despite%20recent%20text-to-3D%0Aadvancements%20in%20supervising%20multi-view%20renderings%20using%20large%20text-to-image%0A%28T2I%29%20models%2C%20issues%20persist%20with%20generation%20speed%2C%20text%20consistency%2C%20and%0Atexture%20quality%2C%20resulting%20in%20data%20scarcity%20among%20existing%20datasets.%20We%20present%0ATexDreamer%2C%20the%20first%20zero-shot%20multimodal%20high-fidelity%203D%20human%20texture%0Ageneration%20model.%20Utilizing%20an%20efficient%20texture%20adaptation%20finetuning%0Astrategy%2C%20we%20adapt%20large%20T2I%20model%20to%20a%20semantic%20UV%20structure%20while%20preserving%0Aits%20original%20generalization%20capability.%20Leveraging%20a%20novel%20feature%20translator%0Amodule%2C%20the%20trained%20model%20is%20capable%20of%20generating%20high-fidelity%203D%20human%0Atextures%20from%20either%20text%20or%20image%20within%20seconds.%20Furthermore%2C%20we%20introduce%0AArTicuLated%20humAn%20textureS%20%28ATLAS%29%2C%20the%20largest%20high-resolution%20%281024%20X%201024%29%0A3D%20human%20texture%20dataset%20which%20contains%2050k%20high-fidelity%20textures%20with%20text%0Adescriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12906v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexDreamer%3A%20Towards%20Zero-Shot%20High-Fidelity%203D%20Human%20Texture%20Generation&entry.906535625=Yufei%20Liu%20and%20Junwei%20Zhu%20and%20Junshu%20Tang%20and%20Shijie%20Zhang%20and%20Jiangning%20Zhang%20and%20Weijian%20Cao%20and%20Chengjie%20Wang%20and%20Yunsheng%20Wu%20and%20Dongjin%20Huang&entry.1292438233=%20%20Texturing%203D%20humans%20with%20semantic%20UV%20maps%20remains%20a%20challenge%20due%20to%20the%0Adifficulty%20of%20acquiring%20reasonably%20unfolded%20UV.%20Despite%20recent%20text-to-3D%0Aadvancements%20in%20supervising%20multi-view%20renderings%20using%20large%20text-to-image%0A%28T2I%29%20models%2C%20issues%20persist%20with%20generation%20speed%2C%20text%20consistency%2C%20and%0Atexture%20quality%2C%20resulting%20in%20data%20scarcity%20among%20existing%20datasets.%20We%20present%0ATexDreamer%2C%20the%20first%20zero-shot%20multimodal%20high-fidelity%203D%20human%20texture%0Ageneration%20model.%20Utilizing%20an%20efficient%20texture%20adaptation%20finetuning%0Astrategy%2C%20we%20adapt%20large%20T2I%20model%20to%20a%20semantic%20UV%20structure%20while%20preserving%0Aits%20original%20generalization%20capability.%20Leveraging%20a%20novel%20feature%20translator%0Amodule%2C%20the%20trained%20model%20is%20capable%20of%20generating%20high-fidelity%203D%20human%0Atextures%20from%20either%20text%20or%20image%20within%20seconds.%20Furthermore%2C%20we%20introduce%0AArTicuLated%20humAn%20textureS%20%28ATLAS%29%2C%20the%20largest%20high-resolution%20%281024%20X%201024%29%0A3D%20human%20texture%20dataset%20which%20contains%2050k%20high-fidelity%20textures%20with%20text%0Adescriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12906v1&entry.124074799=Read"},
{"title": "WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera\n  Driving Scene Generation", "author": "Jiachen Lu and Ze Huang and Zeyu Yang and Jiahui Zhang and Li Zhang", "abstract": "  Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.\n", "link": "http://arxiv.org/abs/2312.02934v3", "date": "2024-03-19", "relevancy": 2.2855, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5863}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5613}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5594}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WoVoGen%3A%20World%20Volume-aware%20Diffusion%20for%20Controllable%20Multi-camera%0A%20%20Driving%20Scene%20Generation&body=Title%3A%20WoVoGen%3A%20World%20Volume-aware%20Diffusion%20for%20Controllable%20Multi-camera%0A%20%20Driving%20Scene%20Generation%0AAuthor%3A%20Jiachen%20Lu%20and%20Ze%20Huang%20and%20Zeyu%20Yang%20and%20Jiahui%20Zhang%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Generating%20multi-camera%20street-view%20videos%20is%20critical%20for%20augmenting%0Aautonomous%20driving%20datasets%2C%20addressing%20the%20urgent%20demand%20for%20extensive%20and%0Avaried%20data.%20Due%20to%20the%20limitations%20in%20diversity%20and%20challenges%20in%20handling%0Alighting%20conditions%2C%20traditional%20rendering-based%20methods%20are%20increasingly%20being%0Asupplanted%20by%20diffusion-based%20methods.%20However%2C%20a%20significant%20challenge%20in%0Adiffusion-based%20methods%20is%20ensuring%20that%20the%20generated%20sensor%20data%20preserve%0Aboth%20intra-world%20consistency%20and%20inter-sensor%20coherence.%20To%20address%20these%0Achallenges%2C%20we%20combine%20an%20additional%20explicit%20world%20volume%20and%20propose%20the%0AWorld%20Volume-aware%20Multi-camera%20Driving%20Scene%20Generator%20%28WoVoGen%29.%20This%20system%0Ais%20specifically%20designed%20to%20leverage%204D%20world%20volume%20as%20a%20foundational%20element%0Afor%20video%20generation.%20Our%20model%20operates%20in%20two%20distinct%20phases%3A%20%28i%29%0Aenvisioning%20the%20future%204D%20temporal%20world%20volume%20based%20on%20vehicle%20control%0Asequences%2C%20and%20%28ii%29%20generating%20multi-camera%20videos%2C%20informed%20by%20this%20envisioned%0A4D%20temporal%20world%20volume%20and%20sensor%20interconnectivity.%20The%20incorporation%20of%20the%0A4D%20world%20volume%20empowers%20WoVoGen%20not%20only%20to%20generate%20high-quality%20street-view%0Avideos%20in%20response%20to%20vehicle%20control%20inputs%20but%20also%20to%20facilitate%20scene%0Aediting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02934v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WoVoGen%3A%20World%20Volume-aware%20Diffusion%20for%20Controllable%20Multi-camera%0A%20%20Driving%20Scene%20Generation&entry.906535625=Jiachen%20Lu%20and%20Ze%20Huang%20and%20Zeyu%20Yang%20and%20Jiahui%20Zhang%20and%20Li%20Zhang&entry.1292438233=%20%20Generating%20multi-camera%20street-view%20videos%20is%20critical%20for%20augmenting%0Aautonomous%20driving%20datasets%2C%20addressing%20the%20urgent%20demand%20for%20extensive%20and%0Avaried%20data.%20Due%20to%20the%20limitations%20in%20diversity%20and%20challenges%20in%20handling%0Alighting%20conditions%2C%20traditional%20rendering-based%20methods%20are%20increasingly%20being%0Asupplanted%20by%20diffusion-based%20methods.%20However%2C%20a%20significant%20challenge%20in%0Adiffusion-based%20methods%20is%20ensuring%20that%20the%20generated%20sensor%20data%20preserve%0Aboth%20intra-world%20consistency%20and%20inter-sensor%20coherence.%20To%20address%20these%0Achallenges%2C%20we%20combine%20an%20additional%20explicit%20world%20volume%20and%20propose%20the%0AWorld%20Volume-aware%20Multi-camera%20Driving%20Scene%20Generator%20%28WoVoGen%29.%20This%20system%0Ais%20specifically%20designed%20to%20leverage%204D%20world%20volume%20as%20a%20foundational%20element%0Afor%20video%20generation.%20Our%20model%20operates%20in%20two%20distinct%20phases%3A%20%28i%29%0Aenvisioning%20the%20future%204D%20temporal%20world%20volume%20based%20on%20vehicle%20control%0Asequences%2C%20and%20%28ii%29%20generating%20multi-camera%20videos%2C%20informed%20by%20this%20envisioned%0A4D%20temporal%20world%20volume%20and%20sensor%20interconnectivity.%20The%20incorporation%20of%20the%0A4D%20world%20volume%20empowers%20WoVoGen%20not%20only%20to%20generate%20high-quality%20street-view%0Avideos%20in%20response%20to%20vehicle%20control%20inputs%20but%20also%20to%20facilitate%20scene%0Aediting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02934v3&entry.124074799=Read"},
{"title": "EscherNet: A Generative Model for Scalable View Synthesis", "author": "Xin Kong and Shikun Liu and Xiaoyang Lyu and Marwan Taher and Xiaojuan Qi and Andrew J. Davison", "abstract": "  We introduce EscherNet, a multi-view conditioned diffusion model for view\nsynthesis. EscherNet learns implicit and generative 3D representations coupled\nwith a specialised camera positional encoding, allowing precise and continuous\nrelative control of the camera transformation between an arbitrary number of\nreference and target views. EscherNet offers exceptional generality,\nflexibility, and scalability in view synthesis -- it can generate more than 100\nconsistent target views simultaneously on a single consumer-grade GPU, despite\nbeing trained with a fixed number of 3 reference views to 3 target views. As a\nresult, EscherNet not only addresses zero-shot novel view synthesis, but also\nnaturally unifies single- and multi-image 3D reconstruction, combining these\ndiverse tasks into a single, cohesive framework. Our extensive experiments\ndemonstrate that EscherNet achieves state-of-the-art performance in multiple\nbenchmarks, even when compared to methods specifically tailored for each\nindividual problem. This remarkable versatility opens up new directions for\ndesigning scalable neural architectures for 3D vision. Project page:\nhttps://kxhit.github.io/EscherNet.\n", "link": "http://arxiv.org/abs/2402.03908v2", "date": "2024-03-19", "relevancy": 2.2789, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5856}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5784}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EscherNet%3A%20A%20Generative%20Model%20for%20Scalable%20View%20Synthesis&body=Title%3A%20EscherNet%3A%20A%20Generative%20Model%20for%20Scalable%20View%20Synthesis%0AAuthor%3A%20Xin%20Kong%20and%20Shikun%20Liu%20and%20Xiaoyang%20Lyu%20and%20Marwan%20Taher%20and%20Xiaojuan%20Qi%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20%20%20We%20introduce%20EscherNet%2C%20a%20multi-view%20conditioned%20diffusion%20model%20for%20view%0Asynthesis.%20EscherNet%20learns%20implicit%20and%20generative%203D%20representations%20coupled%0Awith%20a%20specialised%20camera%20positional%20encoding%2C%20allowing%20precise%20and%20continuous%0Arelative%20control%20of%20the%20camera%20transformation%20between%20an%20arbitrary%20number%20of%0Areference%20and%20target%20views.%20EscherNet%20offers%20exceptional%20generality%2C%0Aflexibility%2C%20and%20scalability%20in%20view%20synthesis%20--%20it%20can%20generate%20more%20than%20100%0Aconsistent%20target%20views%20simultaneously%20on%20a%20single%20consumer-grade%20GPU%2C%20despite%0Abeing%20trained%20with%20a%20fixed%20number%20of%203%20reference%20views%20to%203%20target%20views.%20As%20a%0Aresult%2C%20EscherNet%20not%20only%20addresses%20zero-shot%20novel%20view%20synthesis%2C%20but%20also%0Anaturally%20unifies%20single-%20and%20multi-image%203D%20reconstruction%2C%20combining%20these%0Adiverse%20tasks%20into%20a%20single%2C%20cohesive%20framework.%20Our%20extensive%20experiments%0Ademonstrate%20that%20EscherNet%20achieves%20state-of-the-art%20performance%20in%20multiple%0Abenchmarks%2C%20even%20when%20compared%20to%20methods%20specifically%20tailored%20for%20each%0Aindividual%20problem.%20This%20remarkable%20versatility%20opens%20up%20new%20directions%20for%0Adesigning%20scalable%20neural%20architectures%20for%203D%20vision.%20Project%20page%3A%0Ahttps%3A//kxhit.github.io/EscherNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03908v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EscherNet%3A%20A%20Generative%20Model%20for%20Scalable%20View%20Synthesis&entry.906535625=Xin%20Kong%20and%20Shikun%20Liu%20and%20Xiaoyang%20Lyu%20and%20Marwan%20Taher%20and%20Xiaojuan%20Qi%20and%20Andrew%20J.%20Davison&entry.1292438233=%20%20We%20introduce%20EscherNet%2C%20a%20multi-view%20conditioned%20diffusion%20model%20for%20view%0Asynthesis.%20EscherNet%20learns%20implicit%20and%20generative%203D%20representations%20coupled%0Awith%20a%20specialised%20camera%20positional%20encoding%2C%20allowing%20precise%20and%20continuous%0Arelative%20control%20of%20the%20camera%20transformation%20between%20an%20arbitrary%20number%20of%0Areference%20and%20target%20views.%20EscherNet%20offers%20exceptional%20generality%2C%0Aflexibility%2C%20and%20scalability%20in%20view%20synthesis%20--%20it%20can%20generate%20more%20than%20100%0Aconsistent%20target%20views%20simultaneously%20on%20a%20single%20consumer-grade%20GPU%2C%20despite%0Abeing%20trained%20with%20a%20fixed%20number%20of%203%20reference%20views%20to%203%20target%20views.%20As%20a%0Aresult%2C%20EscherNet%20not%20only%20addresses%20zero-shot%20novel%20view%20synthesis%2C%20but%20also%0Anaturally%20unifies%20single-%20and%20multi-image%203D%20reconstruction%2C%20combining%20these%0Adiverse%20tasks%20into%20a%20single%2C%20cohesive%20framework.%20Our%20extensive%20experiments%0Ademonstrate%20that%20EscherNet%20achieves%20state-of-the-art%20performance%20in%20multiple%0Abenchmarks%2C%20even%20when%20compared%20to%20methods%20specifically%20tailored%20for%20each%0Aindividual%20problem.%20This%20remarkable%20versatility%20opens%20up%20new%20directions%20for%0Adesigning%20scalable%20neural%20architectures%20for%203D%20vision.%20Project%20page%3A%0Ahttps%3A//kxhit.github.io/EscherNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03908v2&entry.124074799=Read"},
{"title": "WaveFace: Authentic Face Restoration with Efficient Frequency Recovery", "author": "Yunqi Miao and Jiankang Deng and Jungong Han", "abstract": "  Although diffusion models are rising as a powerful solution for blind face\nrestoration, they are criticized for two problems: 1) slow training and\ninference speed, and 2) failure in preserving identity and recovering\nfine-grained facial details. In this work, we propose WaveFace to solve the\nproblems in the frequency domain, where low- and high-frequency components\ndecomposed by wavelet transformation are considered individually to maximize\nauthenticity as well as efficiency. The diffusion model is applied to recover\nthe low-frequency component only, which presents general information of the\noriginal image but 1/16 in size. To preserve the original identity, the\ngeneration is conditioned on the low-frequency component of low-quality images\nat each denoising step. Meanwhile, high-frequency components at multiple\ndecomposition levels are handled by a unified network, which recovers complex\nfacial details in a single step. Evaluations on four benchmark datasets show\nthat: 1) WaveFace outperforms state-of-the-art methods in authenticity,\nespecially in terms of identity preservation, and 2) authentic images are\nrestored with the efficiency 10x faster than existing diffusion model-based BFR\nmethods.\n", "link": "http://arxiv.org/abs/2403.12760v1", "date": "2024-03-19", "relevancy": 2.2784, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5952}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5675}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WaveFace%3A%20Authentic%20Face%20Restoration%20with%20Efficient%20Frequency%20Recovery&body=Title%3A%20WaveFace%3A%20Authentic%20Face%20Restoration%20with%20Efficient%20Frequency%20Recovery%0AAuthor%3A%20Yunqi%20Miao%20and%20Jiankang%20Deng%20and%20Jungong%20Han%0AAbstract%3A%20%20%20Although%20diffusion%20models%20are%20rising%20as%20a%20powerful%20solution%20for%20blind%20face%0Arestoration%2C%20they%20are%20criticized%20for%20two%20problems%3A%201%29%20slow%20training%20and%0Ainference%20speed%2C%20and%202%29%20failure%20in%20preserving%20identity%20and%20recovering%0Afine-grained%20facial%20details.%20In%20this%20work%2C%20we%20propose%20WaveFace%20to%20solve%20the%0Aproblems%20in%20the%20frequency%20domain%2C%20where%20low-%20and%20high-frequency%20components%0Adecomposed%20by%20wavelet%20transformation%20are%20considered%20individually%20to%20maximize%0Aauthenticity%20as%20well%20as%20efficiency.%20The%20diffusion%20model%20is%20applied%20to%20recover%0Athe%20low-frequency%20component%20only%2C%20which%20presents%20general%20information%20of%20the%0Aoriginal%20image%20but%201/16%20in%20size.%20To%20preserve%20the%20original%20identity%2C%20the%0Ageneration%20is%20conditioned%20on%20the%20low-frequency%20component%20of%20low-quality%20images%0Aat%20each%20denoising%20step.%20Meanwhile%2C%20high-frequency%20components%20at%20multiple%0Adecomposition%20levels%20are%20handled%20by%20a%20unified%20network%2C%20which%20recovers%20complex%0Afacial%20details%20in%20a%20single%20step.%20Evaluations%20on%20four%20benchmark%20datasets%20show%0Athat%3A%201%29%20WaveFace%20outperforms%20state-of-the-art%20methods%20in%20authenticity%2C%0Aespecially%20in%20terms%20of%20identity%20preservation%2C%20and%202%29%20authentic%20images%20are%0Arestored%20with%20the%20efficiency%2010x%20faster%20than%20existing%20diffusion%20model-based%20BFR%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveFace%3A%20Authentic%20Face%20Restoration%20with%20Efficient%20Frequency%20Recovery&entry.906535625=Yunqi%20Miao%20and%20Jiankang%20Deng%20and%20Jungong%20Han&entry.1292438233=%20%20Although%20diffusion%20models%20are%20rising%20as%20a%20powerful%20solution%20for%20blind%20face%0Arestoration%2C%20they%20are%20criticized%20for%20two%20problems%3A%201%29%20slow%20training%20and%0Ainference%20speed%2C%20and%202%29%20failure%20in%20preserving%20identity%20and%20recovering%0Afine-grained%20facial%20details.%20In%20this%20work%2C%20we%20propose%20WaveFace%20to%20solve%20the%0Aproblems%20in%20the%20frequency%20domain%2C%20where%20low-%20and%20high-frequency%20components%0Adecomposed%20by%20wavelet%20transformation%20are%20considered%20individually%20to%20maximize%0Aauthenticity%20as%20well%20as%20efficiency.%20The%20diffusion%20model%20is%20applied%20to%20recover%0Athe%20low-frequency%20component%20only%2C%20which%20presents%20general%20information%20of%20the%0Aoriginal%20image%20but%201/16%20in%20size.%20To%20preserve%20the%20original%20identity%2C%20the%0Ageneration%20is%20conditioned%20on%20the%20low-frequency%20component%20of%20low-quality%20images%0Aat%20each%20denoising%20step.%20Meanwhile%2C%20high-frequency%20components%20at%20multiple%0Adecomposition%20levels%20are%20handled%20by%20a%20unified%20network%2C%20which%20recovers%20complex%0Afacial%20details%20in%20a%20single%20step.%20Evaluations%20on%20four%20benchmark%20datasets%20show%0Athat%3A%201%29%20WaveFace%20outperforms%20state-of-the-art%20methods%20in%20authenticity%2C%0Aespecially%20in%20terms%20of%20identity%20preservation%2C%20and%202%29%20authentic%20images%20are%0Arestored%20with%20the%20efficiency%2010x%20faster%20than%20existing%20diffusion%20model-based%20BFR%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12760v1&entry.124074799=Read"},
{"title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document\n  Understanding", "author": "Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo Zhang and Chen Li and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou", "abstract": "  Structure information is critical for understanding the semantics of\ntext-rich images, such as documents, tables, and charts. Existing Multimodal\nLarge Language Models (MLLMs) for Visual Document Understanding are equipped\nwith text recognition ability but lack general structure understanding\nabilities for text-rich document images. In this work, we emphasize the\nimportance of structure information in Visual Document Understanding and\npropose the Unified Structure Learning to boost the performance of MLLMs. Our\nUnified Structure Learning comprises structure-aware parsing tasks and\nmulti-grained text localization tasks across 5 domains: document, webpage,\ntable, chart, and natural image. To better encode structure information, we\ndesign a simple and effective vision-to-text module H-Reducer, which can not\nonly maintain the layout information but also reduce the length of visual\nfeatures by merging horizontal adjacent patches through convolution, enabling\nthe LLM to understand high-resolution images more efficiently. Furthermore, by\nconstructing structure-aware text sequences and multi-grained pairs of texts\nand bounding boxes for publicly available text-rich images, we build a\ncomprehensive training set DocStruct4M to support structure learning. Finally,\nwe construct a small but high-quality reasoning tuning dataset DocReason25K to\ntrigger the detailed explanation ability in the document domain. Our model\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document\nunderstanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM\nby more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are\npublicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.\n", "link": "http://arxiv.org/abs/2403.12895v1", "date": "2024-03-19", "relevancy": 2.2689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5428}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20mPLUG-DocOwl%201.5%3A%20Unified%20Structure%20Learning%20for%20OCR-free%20Document%0A%20%20Understanding&body=Title%3A%20mPLUG-DocOwl%201.5%3A%20Unified%20Structure%20Learning%20for%20OCR-free%20Document%0A%20%20Understanding%0AAuthor%3A%20Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Jiabo%20Ye%20and%20Ming%20Yan%20and%20Liang%20Zhang%20and%20Bo%20Zhang%20and%20Chen%20Li%20and%20Ji%20Zhang%20and%20Qin%20Jin%20and%20Fei%20Huang%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Structure%20information%20is%20critical%20for%20understanding%20the%20semantics%20of%0Atext-rich%20images%2C%20such%20as%20documents%2C%20tables%2C%20and%20charts.%20Existing%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20for%20Visual%20Document%20Understanding%20are%20equipped%0Awith%20text%20recognition%20ability%20but%20lack%20general%20structure%20understanding%0Aabilities%20for%20text-rich%20document%20images.%20In%20this%20work%2C%20we%20emphasize%20the%0Aimportance%20of%20structure%20information%20in%20Visual%20Document%20Understanding%20and%0Apropose%20the%20Unified%20Structure%20Learning%20to%20boost%20the%20performance%20of%20MLLMs.%20Our%0AUnified%20Structure%20Learning%20comprises%20structure-aware%20parsing%20tasks%20and%0Amulti-grained%20text%20localization%20tasks%20across%205%20domains%3A%20document%2C%20webpage%2C%0Atable%2C%20chart%2C%20and%20natural%20image.%20To%20better%20encode%20structure%20information%2C%20we%0Adesign%20a%20simple%20and%20effective%20vision-to-text%20module%20H-Reducer%2C%20which%20can%20not%0Aonly%20maintain%20the%20layout%20information%20but%20also%20reduce%20the%20length%20of%20visual%0Afeatures%20by%20merging%20horizontal%20adjacent%20patches%20through%20convolution%2C%20enabling%0Athe%20LLM%20to%20understand%20high-resolution%20images%20more%20efficiently.%20Furthermore%2C%20by%0Aconstructing%20structure-aware%20text%20sequences%20and%20multi-grained%20pairs%20of%20texts%0Aand%20bounding%20boxes%20for%20publicly%20available%20text-rich%20images%2C%20we%20build%20a%0Acomprehensive%20training%20set%20DocStruct4M%20to%20support%20structure%20learning.%20Finally%2C%0Awe%20construct%20a%20small%20but%20high-quality%20reasoning%20tuning%20dataset%20DocReason25K%20to%0Atrigger%20the%20detailed%20explanation%20ability%20in%20the%20document%20domain.%20Our%20model%0ADocOwl%201.5%20achieves%20state-of-the-art%20performance%20on%2010%20visual%20document%0Aunderstanding%20benchmarks%2C%20improving%20the%20SOTA%20performance%20of%20MLLMs%20with%20a%207B%20LLM%0Aby%20more%20than%2010%20points%20in%205/10%20benchmarks.%20Our%20codes%2C%20models%2C%20and%20datasets%20are%0Apublicly%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12895v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mPLUG-DocOwl%201.5%3A%20Unified%20Structure%20Learning%20for%20OCR-free%20Document%0A%20%20Understanding&entry.906535625=Anwen%20Hu%20and%20Haiyang%20Xu%20and%20Jiabo%20Ye%20and%20Ming%20Yan%20and%20Liang%20Zhang%20and%20Bo%20Zhang%20and%20Chen%20Li%20and%20Ji%20Zhang%20and%20Qin%20Jin%20and%20Fei%20Huang%20and%20Jingren%20Zhou&entry.1292438233=%20%20Structure%20information%20is%20critical%20for%20understanding%20the%20semantics%20of%0Atext-rich%20images%2C%20such%20as%20documents%2C%20tables%2C%20and%20charts.%20Existing%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20for%20Visual%20Document%20Understanding%20are%20equipped%0Awith%20text%20recognition%20ability%20but%20lack%20general%20structure%20understanding%0Aabilities%20for%20text-rich%20document%20images.%20In%20this%20work%2C%20we%20emphasize%20the%0Aimportance%20of%20structure%20information%20in%20Visual%20Document%20Understanding%20and%0Apropose%20the%20Unified%20Structure%20Learning%20to%20boost%20the%20performance%20of%20MLLMs.%20Our%0AUnified%20Structure%20Learning%20comprises%20structure-aware%20parsing%20tasks%20and%0Amulti-grained%20text%20localization%20tasks%20across%205%20domains%3A%20document%2C%20webpage%2C%0Atable%2C%20chart%2C%20and%20natural%20image.%20To%20better%20encode%20structure%20information%2C%20we%0Adesign%20a%20simple%20and%20effective%20vision-to-text%20module%20H-Reducer%2C%20which%20can%20not%0Aonly%20maintain%20the%20layout%20information%20but%20also%20reduce%20the%20length%20of%20visual%0Afeatures%20by%20merging%20horizontal%20adjacent%20patches%20through%20convolution%2C%20enabling%0Athe%20LLM%20to%20understand%20high-resolution%20images%20more%20efficiently.%20Furthermore%2C%20by%0Aconstructing%20structure-aware%20text%20sequences%20and%20multi-grained%20pairs%20of%20texts%0Aand%20bounding%20boxes%20for%20publicly%20available%20text-rich%20images%2C%20we%20build%20a%0Acomprehensive%20training%20set%20DocStruct4M%20to%20support%20structure%20learning.%20Finally%2C%0Awe%20construct%20a%20small%20but%20high-quality%20reasoning%20tuning%20dataset%20DocReason25K%20to%0Atrigger%20the%20detailed%20explanation%20ability%20in%20the%20document%20domain.%20Our%20model%0ADocOwl%201.5%20achieves%20state-of-the-art%20performance%20on%2010%20visual%20document%0Aunderstanding%20benchmarks%2C%20improving%20the%20SOTA%20performance%20of%20MLLMs%20with%20a%207B%20LLM%0Aby%20more%20than%2010%20points%20in%205/10%20benchmarks.%20Our%20codes%2C%20models%2C%20and%20datasets%20are%0Apublicly%20available%20at%0Ahttps%3A//github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12895v1&entry.124074799=Read"},
{"title": "Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape\n  Generation", "author": "Yao Wei and Martin Renqiang Min and George Vosselman and Li Erran Li and Michael Ying Yang", "abstract": "  Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Early works typically\nemploy shape retrieval based frameworks which naturally suffer from limited\nshape diversity. Recent progresses have been made in shape generation with\npowerful generative models, such as diffusion models, which increases the shape\nfidelity. However, these approaches separately treat 3D shape generation and\nlayout generation. The synthesized scenes are usually hampered by layout\ncollision, which implies that the scene-level fidelity is still under-explored.\nIn this paper, we aim at generating realistic and reasonable 3D scenes from\nscene graph. To enrich the representation capability of the given scene graph\ninputs, large language model is utilized to explicitly aggregate the global\ngraph features with local relationship features. With a unified graph\nconvolution network (GCN), graph features are extracted from scene graphs\nupdated via joint layout-shape distribution. During scene generation, an\nIoU-based regularization loss is introduced to constrain the predicted 3D\nlayouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D\nscene synthesis, especially in terms of scene-level fidelity. The source code\nwill be released after publication.\n", "link": "http://arxiv.org/abs/2403.12848v1", "date": "2024-03-19", "relevancy": 2.2648, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5714}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5274}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Compositional%203D%20Scene%20Synthesis%20with%20Scene%20Graph%20Guided%20Layout-Shape%0A%20%20Generation&body=Title%3A%20Compositional%203D%20Scene%20Synthesis%20with%20Scene%20Graph%20Guided%20Layout-Shape%0A%20%20Generation%0AAuthor%3A%20Yao%20Wei%20and%20Martin%20Renqiang%20Min%20and%20George%20Vosselman%20and%20Li%20Erran%20Li%20and%20Michael%20Ying%20Yang%0AAbstract%3A%20%20%20Compositional%203D%20scene%20synthesis%20has%20diverse%20applications%20across%20a%20spectrum%0Aof%20industries%20such%20as%20robotics%2C%20films%2C%20and%20video%20games%2C%20as%20it%20closely%20mirrors%0Athe%20complexity%20of%20real-world%20multi-object%20environments.%20Early%20works%20typically%0Aemploy%20shape%20retrieval%20based%20frameworks%20which%20naturally%20suffer%20from%20limited%0Ashape%20diversity.%20Recent%20progresses%20have%20been%20made%20in%20shape%20generation%20with%0Apowerful%20generative%20models%2C%20such%20as%20diffusion%20models%2C%20which%20increases%20the%20shape%0Afidelity.%20However%2C%20these%20approaches%20separately%20treat%203D%20shape%20generation%20and%0Alayout%20generation.%20The%20synthesized%20scenes%20are%20usually%20hampered%20by%20layout%0Acollision%2C%20which%20implies%20that%20the%20scene-level%20fidelity%20is%20still%20under-explored.%0AIn%20this%20paper%2C%20we%20aim%20at%20generating%20realistic%20and%20reasonable%203D%20scenes%20from%0Ascene%20graph.%20To%20enrich%20the%20representation%20capability%20of%20the%20given%20scene%20graph%0Ainputs%2C%20large%20language%20model%20is%20utilized%20to%20explicitly%20aggregate%20the%20global%0Agraph%20features%20with%20local%20relationship%20features.%20With%20a%20unified%20graph%0Aconvolution%20network%20%28GCN%29%2C%20graph%20features%20are%20extracted%20from%20scene%20graphs%0Aupdated%20via%20joint%20layout-shape%20distribution.%20During%20scene%20generation%2C%20an%0AIoU-based%20regularization%20loss%20is%20introduced%20to%20constrain%20the%20predicted%203D%0Alayouts.%20Benchmarked%20on%20the%20SG-FRONT%20dataset%2C%20our%20method%20achieves%20better%203D%0Ascene%20synthesis%2C%20especially%20in%20terms%20of%20scene-level%20fidelity.%20The%20source%20code%0Awill%20be%20released%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12848v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%203D%20Scene%20Synthesis%20with%20Scene%20Graph%20Guided%20Layout-Shape%0A%20%20Generation&entry.906535625=Yao%20Wei%20and%20Martin%20Renqiang%20Min%20and%20George%20Vosselman%20and%20Li%20Erran%20Li%20and%20Michael%20Ying%20Yang&entry.1292438233=%20%20Compositional%203D%20scene%20synthesis%20has%20diverse%20applications%20across%20a%20spectrum%0Aof%20industries%20such%20as%20robotics%2C%20films%2C%20and%20video%20games%2C%20as%20it%20closely%20mirrors%0Athe%20complexity%20of%20real-world%20multi-object%20environments.%20Early%20works%20typically%0Aemploy%20shape%20retrieval%20based%20frameworks%20which%20naturally%20suffer%20from%20limited%0Ashape%20diversity.%20Recent%20progresses%20have%20been%20made%20in%20shape%20generation%20with%0Apowerful%20generative%20models%2C%20such%20as%20diffusion%20models%2C%20which%20increases%20the%20shape%0Afidelity.%20However%2C%20these%20approaches%20separately%20treat%203D%20shape%20generation%20and%0Alayout%20generation.%20The%20synthesized%20scenes%20are%20usually%20hampered%20by%20layout%0Acollision%2C%20which%20implies%20that%20the%20scene-level%20fidelity%20is%20still%20under-explored.%0AIn%20this%20paper%2C%20we%20aim%20at%20generating%20realistic%20and%20reasonable%203D%20scenes%20from%0Ascene%20graph.%20To%20enrich%20the%20representation%20capability%20of%20the%20given%20scene%20graph%0Ainputs%2C%20large%20language%20model%20is%20utilized%20to%20explicitly%20aggregate%20the%20global%0Agraph%20features%20with%20local%20relationship%20features.%20With%20a%20unified%20graph%0Aconvolution%20network%20%28GCN%29%2C%20graph%20features%20are%20extracted%20from%20scene%20graphs%0Aupdated%20via%20joint%20layout-shape%20distribution.%20During%20scene%20generation%2C%20an%0AIoU-based%20regularization%20loss%20is%20introduced%20to%20constrain%20the%20predicted%203D%0Alayouts.%20Benchmarked%20on%20the%20SG-FRONT%20dataset%2C%20our%20method%20achieves%20better%203D%0Ascene%20synthesis%2C%20especially%20in%20terms%20of%20scene-level%20fidelity.%20The%20source%20code%0Awill%20be%20released%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12848v1&entry.124074799=Read"},
{"title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", "author": "Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu H\u00e8 and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang", "abstract": "  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n", "link": "http://arxiv.org/abs/2403.09611v2", "date": "2024-03-19", "relevancy": 2.2627, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5233}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&body=Title%3A%20MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training%0AAuthor%3A%20Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Ankur%20Jain%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20including%20both%0Adense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09611v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM1%3A%20Methods%2C%20Analysis%20%26%20Insights%20from%20Multimodal%20LLM%20Pre-training&entry.906535625=Brandon%20McKinzie%20and%20Zhe%20Gan%20and%20Jean-Philippe%20Fauconnier%20and%20Sam%20Dodge%20and%20Bowen%20Zhang%20and%20Philipp%20Dufter%20and%20Dhruti%20Shah%20and%20Xianzhi%20Du%20and%20Futang%20Peng%20and%20Floris%20Weers%20and%20Anton%20Belyi%20and%20Haotian%20Zhang%20and%20Karanjeet%20Singh%20and%20Doug%20Kang%20and%20Ankur%20Jain%20and%20Hongyu%20H%C3%A8%20and%20Max%20Schwarzer%20and%20Tom%20Gunter%20and%20Xiang%20Kong%20and%20Aonan%20Zhang%20and%20Jianyu%20Wang%20and%20Chong%20Wang%20and%20Nan%20Du%20and%20Tao%20Lei%20and%20Sam%20Wiseman%20and%20Mark%20Lee%20and%20Zirui%20Wang%20and%20Ruoming%20Pang%20and%20Peter%20Grasch%20and%20Alexander%20Toshev%20and%20Yinfei%20Yang&entry.1292438233=%20%20In%20this%20work%2C%20we%20discuss%20building%20performant%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29.%20In%20particular%2C%20we%20study%20the%20importance%20of%20various%20architecture%0Acomponents%20and%20data%20choices.%20Through%20careful%20and%20comprehensive%20ablations%20of%20the%0Aimage%20encoder%2C%20the%20vision%20language%20connector%2C%20and%20various%20pre-training%20data%0Achoices%2C%20we%20identified%20several%20crucial%20design%20lessons.%20For%20example%2C%20we%0Ademonstrate%20that%20for%20large-scale%20multimodal%20pre-training%20using%20a%20careful%20mix%20of%0Aimage-caption%2C%20interleaved%20image-text%2C%20and%20text-only%20data%20is%20crucial%20for%0Aachieving%20state-of-the-art%20%28SOTA%29%20few-shot%20results%20across%20multiple%20benchmarks%2C%0Acompared%20to%20other%20published%20pre-training%20results.%20Further%2C%20we%20show%20that%20the%0Aimage%20encoder%20together%20with%20image%20resolution%20and%20the%20image%20token%20count%20has%0Asubstantial%20impact%2C%20while%20the%20vision-language%20connector%20design%20is%20of%0Acomparatively%20negligible%20importance.%20By%20scaling%20up%20the%20presented%20recipe%2C%20we%0Abuild%20MM1%2C%20a%20family%20of%20multimodal%20models%20up%20to%2030B%20parameters%2C%20including%20both%0Adense%20models%20and%20mixture-of-experts%20%28MoE%29%20variants%2C%20that%20are%20SOTA%20in%0Apre-training%20metrics%20and%20achieve%20competitive%20performance%20after%20supervised%0Afine-tuning%20on%20a%20range%20of%20established%20multimodal%20benchmarks.%20Thanks%20to%0Alarge-scale%20pre-training%2C%20MM1%20enjoys%20appealing%20properties%20such%20as%20enhanced%0Ain-context%20learning%2C%20and%20multi-image%20reasoning%2C%20enabling%20few-shot%0Achain-of-thought%20prompting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09611v2&entry.124074799=Read"},
{"title": "Multispectral Image Restoration by Generalized Opponent Transformation\n  Total Variation", "author": "Zhantao Ma and Michael K. Ng", "abstract": "  Multispectral images (MSI) contain light information in different wavelengths\nof objects, which convey spectral-spatial information and help improve the\nperformance of various image processing tasks. Numerous techniques have been\ncreated to extend the application of total variation regularization in\nrestoring multispectral images, for example, based on channel coupling and\nadaptive total variation regularization. The primary contribution of this paper\nis to propose and develop a new multispectral total variation regularization in\na generalized opponent transformation domain instead of the original\nmultispectral image domain. Here opponent transformations for multispectral\nimages are generalized from a well-known opponent transformation for color\nimages. We will explore the properties of generalized opponent transformation\ntotal variation (GOTTV) regularization and the corresponding optimization\nformula for multispectral image restoration. To evaluate the effectiveness of\nthe new GOTTV method, we provide numerical examples that showcase its superior\nperformance compared to existing multispectral image total variation methods,\nusing criteria such as MPSNR and MSSIM.\n", "link": "http://arxiv.org/abs/2403.12770v1", "date": "2024-03-19", "relevancy": 2.2416, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4675}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4414}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.436}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multispectral%20Image%20Restoration%20by%20Generalized%20Opponent%20Transformation%0A%20%20Total%20Variation&body=Title%3A%20Multispectral%20Image%20Restoration%20by%20Generalized%20Opponent%20Transformation%0A%20%20Total%20Variation%0AAuthor%3A%20Zhantao%20Ma%20and%20Michael%20K.%20Ng%0AAbstract%3A%20%20%20Multispectral%20images%20%28MSI%29%20contain%20light%20information%20in%20different%20wavelengths%0Aof%20objects%2C%20which%20convey%20spectral-spatial%20information%20and%20help%20improve%20the%0Aperformance%20of%20various%20image%20processing%20tasks.%20Numerous%20techniques%20have%20been%0Acreated%20to%20extend%20the%20application%20of%20total%20variation%20regularization%20in%0Arestoring%20multispectral%20images%2C%20for%20example%2C%20based%20on%20channel%20coupling%20and%0Aadaptive%20total%20variation%20regularization.%20The%20primary%20contribution%20of%20this%20paper%0Ais%20to%20propose%20and%20develop%20a%20new%20multispectral%20total%20variation%20regularization%20in%0Aa%20generalized%20opponent%20transformation%20domain%20instead%20of%20the%20original%0Amultispectral%20image%20domain.%20Here%20opponent%20transformations%20for%20multispectral%0Aimages%20are%20generalized%20from%20a%20well-known%20opponent%20transformation%20for%20color%0Aimages.%20We%20will%20explore%20the%20properties%20of%20generalized%20opponent%20transformation%0Atotal%20variation%20%28GOTTV%29%20regularization%20and%20the%20corresponding%20optimization%0Aformula%20for%20multispectral%20image%20restoration.%20To%20evaluate%20the%20effectiveness%20of%0Athe%20new%20GOTTV%20method%2C%20we%20provide%20numerical%20examples%20that%20showcase%20its%20superior%0Aperformance%20compared%20to%20existing%20multispectral%20image%20total%20variation%20methods%2C%0Ausing%20criteria%20such%20as%20MPSNR%20and%20MSSIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12770v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multispectral%20Image%20Restoration%20by%20Generalized%20Opponent%20Transformation%0A%20%20Total%20Variation&entry.906535625=Zhantao%20Ma%20and%20Michael%20K.%20Ng&entry.1292438233=%20%20Multispectral%20images%20%28MSI%29%20contain%20light%20information%20in%20different%20wavelengths%0Aof%20objects%2C%20which%20convey%20spectral-spatial%20information%20and%20help%20improve%20the%0Aperformance%20of%20various%20image%20processing%20tasks.%20Numerous%20techniques%20have%20been%0Acreated%20to%20extend%20the%20application%20of%20total%20variation%20regularization%20in%0Arestoring%20multispectral%20images%2C%20for%20example%2C%20based%20on%20channel%20coupling%20and%0Aadaptive%20total%20variation%20regularization.%20The%20primary%20contribution%20of%20this%20paper%0Ais%20to%20propose%20and%20develop%20a%20new%20multispectral%20total%20variation%20regularization%20in%0Aa%20generalized%20opponent%20transformation%20domain%20instead%20of%20the%20original%0Amultispectral%20image%20domain.%20Here%20opponent%20transformations%20for%20multispectral%0Aimages%20are%20generalized%20from%20a%20well-known%20opponent%20transformation%20for%20color%0Aimages.%20We%20will%20explore%20the%20properties%20of%20generalized%20opponent%20transformation%0Atotal%20variation%20%28GOTTV%29%20regularization%20and%20the%20corresponding%20optimization%0Aformula%20for%20multispectral%20image%20restoration.%20To%20evaluate%20the%20effectiveness%20of%0Athe%20new%20GOTTV%20method%2C%20we%20provide%20numerical%20examples%20that%20showcase%20its%20superior%0Aperformance%20compared%20to%20existing%20multispectral%20image%20total%20variation%20methods%2C%0Ausing%20criteria%20such%20as%20MPSNR%20and%20MSSIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12770v1&entry.124074799=Read"},
{"title": "MineDreamer: Learning to Follow Instructions via Chain-of-Imagination\n  for Simulated-World Control", "author": "Enshen Zhou and Yiran Qin and Zhenfei Yin and Yuzhou Huang and Ruimao Zhang and Lu Sheng and Yu Qiao and Jing Shao", "abstract": "  It is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.\n", "link": "http://arxiv.org/abs/2403.12037v2", "date": "2024-03-19", "relevancy": 2.2367, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5689}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5584}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.556}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MineDreamer%3A%20Learning%20to%20Follow%20Instructions%20via%20Chain-of-Imagination%0A%20%20for%20Simulated-World%20Control&body=Title%3A%20MineDreamer%3A%20Learning%20to%20Follow%20Instructions%20via%20Chain-of-Imagination%0A%20%20for%20Simulated-World%20Control%0AAuthor%3A%20Enshen%20Zhou%20and%20Yiran%20Qin%20and%20Zhenfei%20Yin%20and%20Yuzhou%20Huang%20and%20Ruimao%20Zhang%20and%20Lu%20Sheng%20and%20Yu%20Qiao%20and%20Jing%20Shao%0AAbstract%3A%20%20%20It%20is%20a%20long-lasting%20goal%20to%20design%20a%20generalist-embodied%20agent%20that%20can%0Afollow%20diverse%20instructions%20in%20human-like%20ways.%20However%2C%20existing%20approaches%0Aoften%20fail%20to%20steadily%20follow%20instructions%20due%20to%20difficulties%20in%20understanding%0Aabstract%20and%20sequential%20natural%20language%20instructions.%20To%20this%20end%2C%20we%0Aintroduce%20MineDreamer%2C%20an%20open-ended%20embodied%20agent%20built%20upon%20the%20challenging%0AMinecraft%20simulator%20with%20an%20innovative%20paradigm%20that%20enhances%0Ainstruction-following%20ability%20in%20low-level%20control%20signal%20generation.%0ASpecifically%2C%20MineDreamer%20is%20developed%20on%20top%20of%20recent%20advances%20in%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20and%20diffusion%20models%2C%20and%20we%20employ%20a%0AChain-of-Imagination%20%28CoI%29%20mechanism%20to%20envision%20the%20step-by-step%20process%20of%0Aexecuting%20instructions%20and%20translating%20imaginations%20into%20more%20precise%20visual%0Aprompts%20tailored%20to%20the%20current%20state%3B%20subsequently%2C%20the%20agent%20generates%0Akeyboard-and-mouse%20actions%20to%20efficiently%20achieve%20these%20imaginations%2C%20steadily%0Afollowing%20the%20instructions%20at%20each%20step.%20Extensive%20experiments%20demonstrate%20that%0AMineDreamer%20follows%20single%20and%20multi-step%20instructions%20steadily%2C%20significantly%0Aoutperforming%20the%20best%20generalist%20agent%20baseline%20and%20nearly%20doubling%20its%0Aperformance.%20Moreover%2C%20qualitative%20analysis%20of%20the%20agent%27s%20imaginative%20ability%0Areveals%20its%20generalization%20and%20comprehension%20of%20the%20open%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12037v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MineDreamer%3A%20Learning%20to%20Follow%20Instructions%20via%20Chain-of-Imagination%0A%20%20for%20Simulated-World%20Control&entry.906535625=Enshen%20Zhou%20and%20Yiran%20Qin%20and%20Zhenfei%20Yin%20and%20Yuzhou%20Huang%20and%20Ruimao%20Zhang%20and%20Lu%20Sheng%20and%20Yu%20Qiao%20and%20Jing%20Shao&entry.1292438233=%20%20It%20is%20a%20long-lasting%20goal%20to%20design%20a%20generalist-embodied%20agent%20that%20can%0Afollow%20diverse%20instructions%20in%20human-like%20ways.%20However%2C%20existing%20approaches%0Aoften%20fail%20to%20steadily%20follow%20instructions%20due%20to%20difficulties%20in%20understanding%0Aabstract%20and%20sequential%20natural%20language%20instructions.%20To%20this%20end%2C%20we%0Aintroduce%20MineDreamer%2C%20an%20open-ended%20embodied%20agent%20built%20upon%20the%20challenging%0AMinecraft%20simulator%20with%20an%20innovative%20paradigm%20that%20enhances%0Ainstruction-following%20ability%20in%20low-level%20control%20signal%20generation.%0ASpecifically%2C%20MineDreamer%20is%20developed%20on%20top%20of%20recent%20advances%20in%20Multimodal%0ALarge%20Language%20Models%20%28MLLMs%29%20and%20diffusion%20models%2C%20and%20we%20employ%20a%0AChain-of-Imagination%20%28CoI%29%20mechanism%20to%20envision%20the%20step-by-step%20process%20of%0Aexecuting%20instructions%20and%20translating%20imaginations%20into%20more%20precise%20visual%0Aprompts%20tailored%20to%20the%20current%20state%3B%20subsequently%2C%20the%20agent%20generates%0Akeyboard-and-mouse%20actions%20to%20efficiently%20achieve%20these%20imaginations%2C%20steadily%0Afollowing%20the%20instructions%20at%20each%20step.%20Extensive%20experiments%20demonstrate%20that%0AMineDreamer%20follows%20single%20and%20multi-step%20instructions%20steadily%2C%20significantly%0Aoutperforming%20the%20best%20generalist%20agent%20baseline%20and%20nearly%20doubling%20its%0Aperformance.%20Moreover%2C%20qualitative%20analysis%20of%20the%20agent%27s%20imaginative%20ability%0Areveals%20its%20generalization%20and%20comprehension%20of%20the%20open%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12037v2&entry.124074799=Read"},
{"title": "Towards Multimodal In-Context Learning for Vision & Language Models", "author": "Sivan Doveh and Shaked Perek and M. Jehanzeb Mirza and Amit Alfassy and Assaf Arbelle and Shimon Ullman and Leonid Karlinsky", "abstract": "  Inspired by the emergence of Large Language Models (LLMs) that can truly\nunderstand human language, significant progress has been made in aligning\nother, non-language, modalities to be `understandable' by an LLM, primarily via\nconverting their samples into a sequence of embedded language-like tokens\ndirectly fed into the LLM (decoder) input stream. However, so far limited\nattention has been given to transferring (and evaluating) one of the core LLM\ncapabilities to the emerging VLMs, namely the In-Context Learning (ICL)\nability, or in other words to guide VLMs to desired target downstream tasks or\noutput structure using in-context image+text demonstrations. In this work, we\ndive deeper into analyzing the capabilities of some of the state-of-the-art\nVLMs to follow ICL instructions, discovering them to be somewhat lacking. We\ndiscover that even models that underwent large-scale mixed modality\npre-training and were implicitly guided to make use of interleaved image and\ntext information (intended to consume helpful context from multiple images)\nunder-perform when prompted with few-shot (ICL) demonstrations, likely due to\ntheir lack of `direct' ICL instruction tuning. To test this conjecture, we\npropose a simple, yet surprisingly effective, strategy of extending a common\nVLM alignment framework with ICL support, methodology, and curriculum. We\nexplore, analyze, and provide insights into effective data mixes, leading up to\na significant 21.03% (and 11.3% on average) ICL performance boost over the\nstrongest VLM baselines and a variety of ICL benchmarks. We also contribute new\nbenchmarks for ICL evaluation in VLMs and discuss their advantages over the\nprior art.\n", "link": "http://arxiv.org/abs/2403.12736v1", "date": "2024-03-19", "relevancy": 2.2345, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6181}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Multimodal%20In-Context%20Learning%20for%20Vision%20%26%20Language%20Models&body=Title%3A%20Towards%20Multimodal%20In-Context%20Learning%20for%20Vision%20%26%20Language%20Models%0AAuthor%3A%20Sivan%20Doveh%20and%20Shaked%20Perek%20and%20M.%20Jehanzeb%20Mirza%20and%20Amit%20Alfassy%20and%20Assaf%20Arbelle%20and%20Shimon%20Ullman%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Inspired%20by%20the%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20truly%0Aunderstand%20human%20language%2C%20significant%20progress%20has%20been%20made%20in%20aligning%0Aother%2C%20non-language%2C%20modalities%20to%20be%20%60understandable%27%20by%20an%20LLM%2C%20primarily%20via%0Aconverting%20their%20samples%20into%20a%20sequence%20of%20embedded%20language-like%20tokens%0Adirectly%20fed%20into%20the%20LLM%20%28decoder%29%20input%20stream.%20However%2C%20so%20far%20limited%0Aattention%20has%20been%20given%20to%20transferring%20%28and%20evaluating%29%20one%20of%20the%20core%20LLM%0Acapabilities%20to%20the%20emerging%20VLMs%2C%20namely%20the%20In-Context%20Learning%20%28ICL%29%0Aability%2C%20or%20in%20other%20words%20to%20guide%20VLMs%20to%20desired%20target%20downstream%20tasks%20or%0Aoutput%20structure%20using%20in-context%20image%2Btext%20demonstrations.%20In%20this%20work%2C%20we%0Adive%20deeper%20into%20analyzing%20the%20capabilities%20of%20some%20of%20the%20state-of-the-art%0AVLMs%20to%20follow%20ICL%20instructions%2C%20discovering%20them%20to%20be%20somewhat%20lacking.%20We%0Adiscover%20that%20even%20models%20that%20underwent%20large-scale%20mixed%20modality%0Apre-training%20and%20were%20implicitly%20guided%20to%20make%20use%20of%20interleaved%20image%20and%0Atext%20information%20%28intended%20to%20consume%20helpful%20context%20from%20multiple%20images%29%0Aunder-perform%20when%20prompted%20with%20few-shot%20%28ICL%29%20demonstrations%2C%20likely%20due%20to%0Atheir%20lack%20of%20%60direct%27%20ICL%20instruction%20tuning.%20To%20test%20this%20conjecture%2C%20we%0Apropose%20a%20simple%2C%20yet%20surprisingly%20effective%2C%20strategy%20of%20extending%20a%20common%0AVLM%20alignment%20framework%20with%20ICL%20support%2C%20methodology%2C%20and%20curriculum.%20We%0Aexplore%2C%20analyze%2C%20and%20provide%20insights%20into%20effective%20data%20mixes%2C%20leading%20up%20to%0Aa%20significant%2021.03%25%20%28and%2011.3%25%20on%20average%29%20ICL%20performance%20boost%20over%20the%0Astrongest%20VLM%20baselines%20and%20a%20variety%20of%20ICL%20benchmarks.%20We%20also%20contribute%20new%0Abenchmarks%20for%20ICL%20evaluation%20in%20VLMs%20and%20discuss%20their%20advantages%20over%20the%0Aprior%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12736v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Multimodal%20In-Context%20Learning%20for%20Vision%20%26%20Language%20Models&entry.906535625=Sivan%20Doveh%20and%20Shaked%20Perek%20and%20M.%20Jehanzeb%20Mirza%20and%20Amit%20Alfassy%20and%20Assaf%20Arbelle%20and%20Shimon%20Ullman%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Inspired%20by%20the%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20truly%0Aunderstand%20human%20language%2C%20significant%20progress%20has%20been%20made%20in%20aligning%0Aother%2C%20non-language%2C%20modalities%20to%20be%20%60understandable%27%20by%20an%20LLM%2C%20primarily%20via%0Aconverting%20their%20samples%20into%20a%20sequence%20of%20embedded%20language-like%20tokens%0Adirectly%20fed%20into%20the%20LLM%20%28decoder%29%20input%20stream.%20However%2C%20so%20far%20limited%0Aattention%20has%20been%20given%20to%20transferring%20%28and%20evaluating%29%20one%20of%20the%20core%20LLM%0Acapabilities%20to%20the%20emerging%20VLMs%2C%20namely%20the%20In-Context%20Learning%20%28ICL%29%0Aability%2C%20or%20in%20other%20words%20to%20guide%20VLMs%20to%20desired%20target%20downstream%20tasks%20or%0Aoutput%20structure%20using%20in-context%20image%2Btext%20demonstrations.%20In%20this%20work%2C%20we%0Adive%20deeper%20into%20analyzing%20the%20capabilities%20of%20some%20of%20the%20state-of-the-art%0AVLMs%20to%20follow%20ICL%20instructions%2C%20discovering%20them%20to%20be%20somewhat%20lacking.%20We%0Adiscover%20that%20even%20models%20that%20underwent%20large-scale%20mixed%20modality%0Apre-training%20and%20were%20implicitly%20guided%20to%20make%20use%20of%20interleaved%20image%20and%0Atext%20information%20%28intended%20to%20consume%20helpful%20context%20from%20multiple%20images%29%0Aunder-perform%20when%20prompted%20with%20few-shot%20%28ICL%29%20demonstrations%2C%20likely%20due%20to%0Atheir%20lack%20of%20%60direct%27%20ICL%20instruction%20tuning.%20To%20test%20this%20conjecture%2C%20we%0Apropose%20a%20simple%2C%20yet%20surprisingly%20effective%2C%20strategy%20of%20extending%20a%20common%0AVLM%20alignment%20framework%20with%20ICL%20support%2C%20methodology%2C%20and%20curriculum.%20We%0Aexplore%2C%20analyze%2C%20and%20provide%20insights%20into%20effective%20data%20mixes%2C%20leading%20up%20to%0Aa%20significant%2021.03%25%20%28and%2011.3%25%20on%20average%29%20ICL%20performance%20boost%20over%20the%0Astrongest%20VLM%20baselines%20and%20a%20variety%20of%20ICL%20benchmarks.%20We%20also%20contribute%20new%0Abenchmarks%20for%20ICL%20evaluation%20in%20VLMs%20and%20discuss%20their%20advantages%20over%20the%0Aprior%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12736v1&entry.124074799=Read"},
{"title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation", "author": "Shuai Yang and Yifan Zhou and Ziwei Liu and Chen Change Loy", "abstract": "  The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods.\n", "link": "http://arxiv.org/abs/2403.12962v1", "date": "2024-03-19", "relevancy": 2.2338, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5891}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5588}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5458}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FRESCO%3A%20Spatial-Temporal%20Correspondence%20for%20Zero-Shot%20Video%20Translation&body=Title%3A%20FRESCO%3A%20Spatial-Temporal%20Correspondence%20for%20Zero-Shot%20Video%20Translation%0AAuthor%3A%20Shuai%20Yang%20and%20Yifan%20Zhou%20and%20Ziwei%20Liu%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20The%20remarkable%20efficacy%20of%20text-to-image%20diffusion%20models%20has%20motivated%0Aextensive%20exploration%20of%20their%20potential%20application%20in%20video%20domains.%0AZero-shot%20methods%20seek%20to%20extend%20image%20diffusion%20models%20to%20videos%20without%0Anecessitating%20model%20training.%20Recent%20methods%20mainly%20focus%20on%20incorporating%0Ainter-frame%20correspondence%20into%20attention%20mechanisms.%20However%2C%20the%20soft%0Aconstraint%20imposed%20on%20determining%20where%20to%20attend%20to%20valid%20features%20can%0Asometimes%20be%20insufficient%2C%20resulting%20in%20temporal%20inconsistency.%20In%20this%20paper%2C%0Awe%20introduce%20FRESCO%2C%20intra-frame%20correspondence%20alongside%20inter-frame%0Acorrespondence%20to%20establish%20a%20more%20robust%20spatial-temporal%20constraint.%20This%0Aenhancement%20ensures%20a%20more%20consistent%20transformation%20of%20semantically%20similar%0Acontent%20across%20frames.%20Beyond%20mere%20attention%20guidance%2C%20our%20approach%20involves%20an%0Aexplicit%20update%20of%20features%20to%20achieve%20high%20spatial-temporal%20consistency%20with%0Athe%20input%20video%2C%20significantly%20improving%20the%20visual%20coherence%20of%20the%20resulting%0Atranslated%20videos.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20framework%20in%20producing%20high-quality%2C%20coherent%20videos%2C%20marking%20a%0Anotable%20improvement%20over%20existing%20zero-shot%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12962v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRESCO%3A%20Spatial-Temporal%20Correspondence%20for%20Zero-Shot%20Video%20Translation&entry.906535625=Shuai%20Yang%20and%20Yifan%20Zhou%20and%20Ziwei%20Liu%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20The%20remarkable%20efficacy%20of%20text-to-image%20diffusion%20models%20has%20motivated%0Aextensive%20exploration%20of%20their%20potential%20application%20in%20video%20domains.%0AZero-shot%20methods%20seek%20to%20extend%20image%20diffusion%20models%20to%20videos%20without%0Anecessitating%20model%20training.%20Recent%20methods%20mainly%20focus%20on%20incorporating%0Ainter-frame%20correspondence%20into%20attention%20mechanisms.%20However%2C%20the%20soft%0Aconstraint%20imposed%20on%20determining%20where%20to%20attend%20to%20valid%20features%20can%0Asometimes%20be%20insufficient%2C%20resulting%20in%20temporal%20inconsistency.%20In%20this%20paper%2C%0Awe%20introduce%20FRESCO%2C%20intra-frame%20correspondence%20alongside%20inter-frame%0Acorrespondence%20to%20establish%20a%20more%20robust%20spatial-temporal%20constraint.%20This%0Aenhancement%20ensures%20a%20more%20consistent%20transformation%20of%20semantically%20similar%0Acontent%20across%20frames.%20Beyond%20mere%20attention%20guidance%2C%20our%20approach%20involves%20an%0Aexplicit%20update%20of%20features%20to%20achieve%20high%20spatial-temporal%20consistency%20with%0Athe%20input%20video%2C%20significantly%20improving%20the%20visual%20coherence%20of%20the%20resulting%0Atranslated%20videos.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20framework%20in%20producing%20high-quality%2C%20coherent%20videos%2C%20marking%20a%0Anotable%20improvement%20over%20existing%20zero-shot%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12962v1&entry.124074799=Read"},
{"title": "ViTGaze: Gaze Following with Interaction Features in Vision Transformers", "author": "Yuehao Song and Xinggang Wang and Jingfeng Yao and Wenyu Liu and Jinglin Zhang and Xiangmin Xu", "abstract": "  Gaze following aims to interpret human-scene interactions by predicting the\nperson's focal point of gaze. Prevailing approaches often use multi-modality\ninputs, most of which adopt a two-stage framework. Hence their performance\nhighly depends on the previous prediction accuracy. Others use a\nsingle-modality approach with complex decoders, increasing network\ncomputational load. Inspired by the remarkable success of pre-trained plain\nVision Transformers (ViTs), we introduce a novel single-modality gaze following\nframework, ViTGaze. In contrast to previous methods, ViTGaze creates a brand\nnew gaze following framework based mainly on powerful encoders (dec. param.\nless than 1%). Our principal insight lies in that the inter-token interactions\nwithin self-attention can be transferred to interactions between humans and\nscenes. Leveraging this presumption, we formulate a framework consisting of a\n4D interaction encoder and a 2D spatial guidance module to extract human-scene\ninteraction information from self-attention maps. Furthermore, our\ninvestigation reveals that ViT with self-supervised pre-training exhibits an\nenhanced ability to extract correlated information. A large number of\nexperiments have been conducted to demonstrate the performance of the proposed\nmethod. Our method achieves state-of-the-art (SOTA) performance among all\nsingle-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and\nvery comparable performance against multi-modality methods with 59% number of\nparameters less.\n", "link": "http://arxiv.org/abs/2403.12778v1", "date": "2024-03-19", "relevancy": 2.2285, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5667}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5338}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers&body=Title%3A%20ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers%0AAuthor%3A%20Yuehao%20Song%20and%20Xinggang%20Wang%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Jinglin%20Zhang%20and%20Xiangmin%20Xu%0AAbstract%3A%20%20%20Gaze%20following%20aims%20to%20interpret%20human-scene%20interactions%20by%20predicting%20the%0Aperson%27s%20focal%20point%20of%20gaze.%20Prevailing%20approaches%20often%20use%20multi-modality%0Ainputs%2C%20most%20of%20which%20adopt%20a%20two-stage%20framework.%20Hence%20their%20performance%0Ahighly%20depends%20on%20the%20previous%20prediction%20accuracy.%20Others%20use%20a%0Asingle-modality%20approach%20with%20complex%20decoders%2C%20increasing%20network%0Acomputational%20load.%20Inspired%20by%20the%20remarkable%20success%20of%20pre-trained%20plain%0AVision%20Transformers%20%28ViTs%29%2C%20we%20introduce%20a%20novel%20single-modality%20gaze%20following%0Aframework%2C%20ViTGaze.%20In%20contrast%20to%20previous%20methods%2C%20ViTGaze%20creates%20a%20brand%0Anew%20gaze%20following%20framework%20based%20mainly%20on%20powerful%20encoders%20%28dec.%20param.%0Aless%20than%201%25%29.%20Our%20principal%20insight%20lies%20in%20that%20the%20inter-token%20interactions%0Awithin%20self-attention%20can%20be%20transferred%20to%20interactions%20between%20humans%20and%0Ascenes.%20Leveraging%20this%20presumption%2C%20we%20formulate%20a%20framework%20consisting%20of%20a%0A4D%20interaction%20encoder%20and%20a%202D%20spatial%20guidance%20module%20to%20extract%20human-scene%0Ainteraction%20information%20from%20self-attention%20maps.%20Furthermore%2C%20our%0Ainvestigation%20reveals%20that%20ViT%20with%20self-supervised%20pre-training%20exhibits%20an%0Aenhanced%20ability%20to%20extract%20correlated%20information.%20A%20large%20number%20of%0Aexperiments%20have%20been%20conducted%20to%20demonstrate%20the%20performance%20of%20the%20proposed%0Amethod.%20Our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20all%0Asingle-modality%20methods%20%283.4%25%20improvement%20on%20AUC%2C%205.1%25%20improvement%20on%20AP%29%20and%0Avery%20comparable%20performance%20against%20multi-modality%20methods%20with%2059%25%20number%20of%0Aparameters%20less.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12778v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTGaze%3A%20Gaze%20Following%20with%20Interaction%20Features%20in%20Vision%20Transformers&entry.906535625=Yuehao%20Song%20and%20Xinggang%20Wang%20and%20Jingfeng%20Yao%20and%20Wenyu%20Liu%20and%20Jinglin%20Zhang%20and%20Xiangmin%20Xu&entry.1292438233=%20%20Gaze%20following%20aims%20to%20interpret%20human-scene%20interactions%20by%20predicting%20the%0Aperson%27s%20focal%20point%20of%20gaze.%20Prevailing%20approaches%20often%20use%20multi-modality%0Ainputs%2C%20most%20of%20which%20adopt%20a%20two-stage%20framework.%20Hence%20their%20performance%0Ahighly%20depends%20on%20the%20previous%20prediction%20accuracy.%20Others%20use%20a%0Asingle-modality%20approach%20with%20complex%20decoders%2C%20increasing%20network%0Acomputational%20load.%20Inspired%20by%20the%20remarkable%20success%20of%20pre-trained%20plain%0AVision%20Transformers%20%28ViTs%29%2C%20we%20introduce%20a%20novel%20single-modality%20gaze%20following%0Aframework%2C%20ViTGaze.%20In%20contrast%20to%20previous%20methods%2C%20ViTGaze%20creates%20a%20brand%0Anew%20gaze%20following%20framework%20based%20mainly%20on%20powerful%20encoders%20%28dec.%20param.%0Aless%20than%201%25%29.%20Our%20principal%20insight%20lies%20in%20that%20the%20inter-token%20interactions%0Awithin%20self-attention%20can%20be%20transferred%20to%20interactions%20between%20humans%20and%0Ascenes.%20Leveraging%20this%20presumption%2C%20we%20formulate%20a%20framework%20consisting%20of%20a%0A4D%20interaction%20encoder%20and%20a%202D%20spatial%20guidance%20module%20to%20extract%20human-scene%0Ainteraction%20information%20from%20self-attention%20maps.%20Furthermore%2C%20our%0Ainvestigation%20reveals%20that%20ViT%20with%20self-supervised%20pre-training%20exhibits%20an%0Aenhanced%20ability%20to%20extract%20correlated%20information.%20A%20large%20number%20of%0Aexperiments%20have%20been%20conducted%20to%20demonstrate%20the%20performance%20of%20the%20proposed%0Amethod.%20Our%20method%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20all%0Asingle-modality%20methods%20%283.4%25%20improvement%20on%20AUC%2C%205.1%25%20improvement%20on%20AP%29%20and%0Avery%20comparable%20performance%20against%20multi-modality%20methods%20with%2059%25%20number%20of%0Aparameters%20less.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12778v1&entry.124074799=Read"},
{"title": "Align before Adapt: Leveraging Entity-to-Region Alignments for\n  Generalizable Video Action Recognition", "author": "Yifei Chen and Dapeng Chen and Ruijin Liu and Sai Zhou and Wenyuan Xue and Wei Peng", "abstract": "  Large-scale visual-language pre-trained models have achieved significant\nsuccess in various video tasks. However, most existing methods follow an \"adapt\nthen align\" paradigm, which adapts pre-trained image encoders to model\nvideo-level representations and utilizes one-hot or text embedding of the\naction labels for supervision. This paradigm overlooks the challenge of mapping\nfrom static images to complicated activity concepts. In this paper, we propose\na novel \"Align before Adapt\" (ALT) paradigm. Prior to adapting to video\nrepresentation learning, we exploit the entity-to-region alignments for each\nframe. The alignments are fulfilled by matching the region-aware image\nembeddings to an offline-constructed text corpus. With the aligned entities, we\nfeed their text embeddings to a transformer-based video adapter as the queries,\nwhich can help extract the semantics of the most important entities from a\nvideo to a vector. This paradigm reuses the visual-language alignment of VLP\nduring adaptation and tries to explain an action by the underlying entities.\nThis helps understand actions by bridging the gap with complex activity\nsemantics, particularly when facing unfamiliar or unseen categories. ALT\ndemonstrates competitive performance while maintaining remarkably low\ncomputational costs. In fully supervised experiments, it achieves 88.1% top-1\naccuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the\nprevious state-of-the-art methods in both zero-shot and few-shot experiments,\nemphasizing its superior generalizability across various learning scenarios.\n", "link": "http://arxiv.org/abs/2311.15619v2", "date": "2024-03-19", "relevancy": 2.2195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5769}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5187}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Align%20before%20Adapt%3A%20Leveraging%20Entity-to-Region%20Alignments%20for%0A%20%20Generalizable%20Video%20Action%20Recognition&body=Title%3A%20Align%20before%20Adapt%3A%20Leveraging%20Entity-to-Region%20Alignments%20for%0A%20%20Generalizable%20Video%20Action%20Recognition%0AAuthor%3A%20Yifei%20Chen%20and%20Dapeng%20Chen%20and%20Ruijin%20Liu%20and%20Sai%20Zhou%20and%20Wenyuan%20Xue%20and%20Wei%20Peng%0AAbstract%3A%20%20%20Large-scale%20visual-language%20pre-trained%20models%20have%20achieved%20significant%0Asuccess%20in%20various%20video%20tasks.%20However%2C%20most%20existing%20methods%20follow%20an%20%22adapt%0Athen%20align%22%20paradigm%2C%20which%20adapts%20pre-trained%20image%20encoders%20to%20model%0Avideo-level%20representations%20and%20utilizes%20one-hot%20or%20text%20embedding%20of%20the%0Aaction%20labels%20for%20supervision.%20This%20paradigm%20overlooks%20the%20challenge%20of%20mapping%0Afrom%20static%20images%20to%20complicated%20activity%20concepts.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20%22Align%20before%20Adapt%22%20%28ALT%29%20paradigm.%20Prior%20to%20adapting%20to%20video%0Arepresentation%20learning%2C%20we%20exploit%20the%20entity-to-region%20alignments%20for%20each%0Aframe.%20The%20alignments%20are%20fulfilled%20by%20matching%20the%20region-aware%20image%0Aembeddings%20to%20an%20offline-constructed%20text%20corpus.%20With%20the%20aligned%20entities%2C%20we%0Afeed%20their%20text%20embeddings%20to%20a%20transformer-based%20video%20adapter%20as%20the%20queries%2C%0Awhich%20can%20help%20extract%20the%20semantics%20of%20the%20most%20important%20entities%20from%20a%0Avideo%20to%20a%20vector.%20This%20paradigm%20reuses%20the%20visual-language%20alignment%20of%20VLP%0Aduring%20adaptation%20and%20tries%20to%20explain%20an%20action%20by%20the%20underlying%20entities.%0AThis%20helps%20understand%20actions%20by%20bridging%20the%20gap%20with%20complex%20activity%0Asemantics%2C%20particularly%20when%20facing%20unfamiliar%20or%20unseen%20categories.%20ALT%0Ademonstrates%20competitive%20performance%20while%20maintaining%20remarkably%20low%0Acomputational%20costs.%20In%20fully%20supervised%20experiments%2C%20it%20achieves%2088.1%25%20top-1%0Aaccuracy%20on%20Kinetics-400%20with%20only%204947%20GFLOPs.%20Moreover%2C%20ALT%20outperforms%20the%0Aprevious%20state-of-the-art%20methods%20in%20both%20zero-shot%20and%20few-shot%20experiments%2C%0Aemphasizing%20its%20superior%20generalizability%20across%20various%20learning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15619v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20before%20Adapt%3A%20Leveraging%20Entity-to-Region%20Alignments%20for%0A%20%20Generalizable%20Video%20Action%20Recognition&entry.906535625=Yifei%20Chen%20and%20Dapeng%20Chen%20and%20Ruijin%20Liu%20and%20Sai%20Zhou%20and%20Wenyuan%20Xue%20and%20Wei%20Peng&entry.1292438233=%20%20Large-scale%20visual-language%20pre-trained%20models%20have%20achieved%20significant%0Asuccess%20in%20various%20video%20tasks.%20However%2C%20most%20existing%20methods%20follow%20an%20%22adapt%0Athen%20align%22%20paradigm%2C%20which%20adapts%20pre-trained%20image%20encoders%20to%20model%0Avideo-level%20representations%20and%20utilizes%20one-hot%20or%20text%20embedding%20of%20the%0Aaction%20labels%20for%20supervision.%20This%20paradigm%20overlooks%20the%20challenge%20of%20mapping%0Afrom%20static%20images%20to%20complicated%20activity%20concepts.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20%22Align%20before%20Adapt%22%20%28ALT%29%20paradigm.%20Prior%20to%20adapting%20to%20video%0Arepresentation%20learning%2C%20we%20exploit%20the%20entity-to-region%20alignments%20for%20each%0Aframe.%20The%20alignments%20are%20fulfilled%20by%20matching%20the%20region-aware%20image%0Aembeddings%20to%20an%20offline-constructed%20text%20corpus.%20With%20the%20aligned%20entities%2C%20we%0Afeed%20their%20text%20embeddings%20to%20a%20transformer-based%20video%20adapter%20as%20the%20queries%2C%0Awhich%20can%20help%20extract%20the%20semantics%20of%20the%20most%20important%20entities%20from%20a%0Avideo%20to%20a%20vector.%20This%20paradigm%20reuses%20the%20visual-language%20alignment%20of%20VLP%0Aduring%20adaptation%20and%20tries%20to%20explain%20an%20action%20by%20the%20underlying%20entities.%0AThis%20helps%20understand%20actions%20by%20bridging%20the%20gap%20with%20complex%20activity%0Asemantics%2C%20particularly%20when%20facing%20unfamiliar%20or%20unseen%20categories.%20ALT%0Ademonstrates%20competitive%20performance%20while%20maintaining%20remarkably%20low%0Acomputational%20costs.%20In%20fully%20supervised%20experiments%2C%20it%20achieves%2088.1%25%20top-1%0Aaccuracy%20on%20Kinetics-400%20with%20only%204947%20GFLOPs.%20Moreover%2C%20ALT%20outperforms%20the%0Aprevious%20state-of-the-art%20methods%20in%20both%20zero-shot%20and%20few-shot%20experiments%2C%0Aemphasizing%20its%20superior%20generalizability%20across%20various%20learning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15619v2&entry.124074799=Read"},
{"title": "Contextual AD Narration with Interleaved Multimodal Sequence", "author": "Hanlin Wang and Zhan Tong and Kecheng Zheng and Yujun Shen and Limin Wang", "abstract": "  The Audio Description (AD) task aims to generate descriptions of visual\nelements for visually impaired individuals to help them access long-form video\ncontents, like movie. With video feature, text, character bank and context\ninformation as inputs, the generated ADs are able to correspond to the\ncharacters by name and provide reasonable, contextual descriptions to help\naudience understand the storyline of movie. To achieve this goal, we propose to\nleverage pre-trained foundation models through a simple and unified framework\nto generate ADs with interleaved multimodal sequence as input, termed as\nUni-AD. To enhance the alignment of features across various modalities with\nfiner granularity, we introduce a simple and lightweight module that maps video\nfeatures into the textual feature space. Moreover, we also propose a\ncharacter-refinement module to provide more precise information by identifying\nthe main characters who play more significant role in the video context. With\nthese unique designs, we further incorporate contextual information and a\ncontrastive loss into our architecture to generate more smooth and contextual\nADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve\nstate-of-the-art performance on AD generation, which demonstrates the\neffectiveness of our approach. Code will be available at\nhttps://github.com/MCG-NJU/Uni-AD.\n", "link": "http://arxiv.org/abs/2403.12922v1", "date": "2024-03-19", "relevancy": 2.218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5717}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5443}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5414}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Contextual%20AD%20Narration%20with%20Interleaved%20Multimodal%20Sequence&body=Title%3A%20Contextual%20AD%20Narration%20with%20Interleaved%20Multimodal%20Sequence%0AAuthor%3A%20Hanlin%20Wang%20and%20Zhan%20Tong%20and%20Kecheng%20Zheng%20and%20Yujun%20Shen%20and%20Limin%20Wang%0AAbstract%3A%20%20%20The%20Audio%20Description%20%28AD%29%20task%20aims%20to%20generate%20descriptions%20of%20visual%0Aelements%20for%20visually%20impaired%20individuals%20to%20help%20them%20access%20long-form%20video%0Acontents%2C%20like%20movie.%20With%20video%20feature%2C%20text%2C%20character%20bank%20and%20context%0Ainformation%20as%20inputs%2C%20the%20generated%20ADs%20are%20able%20to%20correspond%20to%20the%0Acharacters%20by%20name%20and%20provide%20reasonable%2C%20contextual%20descriptions%20to%20help%0Aaudience%20understand%20the%20storyline%20of%20movie.%20To%20achieve%20this%20goal%2C%20we%20propose%20to%0Aleverage%20pre-trained%20foundation%20models%20through%20a%20simple%20and%20unified%20framework%0Ato%20generate%20ADs%20with%20interleaved%20multimodal%20sequence%20as%20input%2C%20termed%20as%0AUni-AD.%20To%20enhance%20the%20alignment%20of%20features%20across%20various%20modalities%20with%0Afiner%20granularity%2C%20we%20introduce%20a%20simple%20and%20lightweight%20module%20that%20maps%20video%0Afeatures%20into%20the%20textual%20feature%20space.%20Moreover%2C%20we%20also%20propose%20a%0Acharacter-refinement%20module%20to%20provide%20more%20precise%20information%20by%20identifying%0Athe%20main%20characters%20who%20play%20more%20significant%20role%20in%20the%20video%20context.%20With%0Athese%20unique%20designs%2C%20we%20further%20incorporate%20contextual%20information%20and%20a%0Acontrastive%20loss%20into%20our%20architecture%20to%20generate%20more%20smooth%20and%20contextual%0AADs.%20Experiments%20on%20the%20MAD-eval%20dataset%20show%20that%20Uni-AD%20can%20achieve%0Astate-of-the-art%20performance%20on%20AD%20generation%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20our%20approach.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/MCG-NJU/Uni-AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12922v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20AD%20Narration%20with%20Interleaved%20Multimodal%20Sequence&entry.906535625=Hanlin%20Wang%20and%20Zhan%20Tong%20and%20Kecheng%20Zheng%20and%20Yujun%20Shen%20and%20Limin%20Wang&entry.1292438233=%20%20The%20Audio%20Description%20%28AD%29%20task%20aims%20to%20generate%20descriptions%20of%20visual%0Aelements%20for%20visually%20impaired%20individuals%20to%20help%20them%20access%20long-form%20video%0Acontents%2C%20like%20movie.%20With%20video%20feature%2C%20text%2C%20character%20bank%20and%20context%0Ainformation%20as%20inputs%2C%20the%20generated%20ADs%20are%20able%20to%20correspond%20to%20the%0Acharacters%20by%20name%20and%20provide%20reasonable%2C%20contextual%20descriptions%20to%20help%0Aaudience%20understand%20the%20storyline%20of%20movie.%20To%20achieve%20this%20goal%2C%20we%20propose%20to%0Aleverage%20pre-trained%20foundation%20models%20through%20a%20simple%20and%20unified%20framework%0Ato%20generate%20ADs%20with%20interleaved%20multimodal%20sequence%20as%20input%2C%20termed%20as%0AUni-AD.%20To%20enhance%20the%20alignment%20of%20features%20across%20various%20modalities%20with%0Afiner%20granularity%2C%20we%20introduce%20a%20simple%20and%20lightweight%20module%20that%20maps%20video%0Afeatures%20into%20the%20textual%20feature%20space.%20Moreover%2C%20we%20also%20propose%20a%0Acharacter-refinement%20module%20to%20provide%20more%20precise%20information%20by%20identifying%0Athe%20main%20characters%20who%20play%20more%20significant%20role%20in%20the%20video%20context.%20With%0Athese%20unique%20designs%2C%20we%20further%20incorporate%20contextual%20information%20and%20a%0Acontrastive%20loss%20into%20our%20architecture%20to%20generate%20more%20smooth%20and%20contextual%0AADs.%20Experiments%20on%20the%20MAD-eval%20dataset%20show%20that%20Uni-AD%20can%20achieve%0Astate-of-the-art%20performance%20on%20AD%20generation%2C%20which%20demonstrates%20the%0Aeffectiveness%20of%20our%20approach.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/MCG-NJU/Uni-AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12922v1&entry.124074799=Read"},
{"title": "HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting", "author": "Hongyu Zhou and Jiahao Shao and Lu Xu and Dongfeng Bai and Weichao Qiu and Bingbing Liu and Yue Wang and Andreas Geiger and Yiyi Liao", "abstract": "  Holistic understanding of urban scenes based on RGB images is a challenging\nyet important problem. It encompasses understanding both the geometry and\nappearance to enable novel view synthesis, parsing semantic labels, and\ntracking moving objects. Despite considerable progress, existing approaches\noften focus on specific aspects of this task and require additional inputs such\nas LiDAR scans or manually annotated 3D bounding boxes. In this paper, we\nintroduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic\nurban scene understanding. Our main idea involves the joint optimization of\ngeometry, appearance, semantics, and motion using a combination of static and\ndynamic 3D Gaussians, where moving object poses are regularized via physical\nconstraints. Our approach offers the ability to render new viewpoints in\nreal-time, yielding 2D and 3D semantic information with high accuracy, and\nreconstruct dynamic scenes, even in scenarios where 3D bounding box detection\nare highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2\ndemonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2403.12722v1", "date": "2024-03-19", "relevancy": 2.2058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5453}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5394}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HUGS%3A%20Holistic%20Urban%203D%20Scene%20Understanding%20via%20Gaussian%20Splatting&body=Title%3A%20HUGS%3A%20Holistic%20Urban%203D%20Scene%20Understanding%20via%20Gaussian%20Splatting%0AAuthor%3A%20Hongyu%20Zhou%20and%20Jiahao%20Shao%20and%20Lu%20Xu%20and%20Dongfeng%20Bai%20and%20Weichao%20Qiu%20and%20Bingbing%20Liu%20and%20Yue%20Wang%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20Holistic%20understanding%20of%20urban%20scenes%20based%20on%20RGB%20images%20is%20a%20challenging%0Ayet%20important%20problem.%20It%20encompasses%20understanding%20both%20the%20geometry%20and%0Aappearance%20to%20enable%20novel%20view%20synthesis%2C%20parsing%20semantic%20labels%2C%20and%0Atracking%20moving%20objects.%20Despite%20considerable%20progress%2C%20existing%20approaches%0Aoften%20focus%20on%20specific%20aspects%20of%20this%20task%20and%20require%20additional%20inputs%20such%0Aas%20LiDAR%20scans%20or%20manually%20annotated%203D%20bounding%20boxes.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20pipeline%20that%20utilizes%203D%20Gaussian%20Splatting%20for%20holistic%0Aurban%20scene%20understanding.%20Our%20main%20idea%20involves%20the%20joint%20optimization%20of%0Ageometry%2C%20appearance%2C%20semantics%2C%20and%20motion%20using%20a%20combination%20of%20static%20and%0Adynamic%203D%20Gaussians%2C%20where%20moving%20object%20poses%20are%20regularized%20via%20physical%0Aconstraints.%20Our%20approach%20offers%20the%20ability%20to%20render%20new%20viewpoints%20in%0Areal-time%2C%20yielding%202D%20and%203D%20semantic%20information%20with%20high%20accuracy%2C%20and%0Areconstruct%20dynamic%20scenes%2C%20even%20in%20scenarios%20where%203D%20bounding%20box%20detection%0Aare%20highly%20noisy.%20Experimental%20results%20on%20KITTI%2C%20KITTI-360%2C%20and%20Virtual%20KITTI%202%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUGS%3A%20Holistic%20Urban%203D%20Scene%20Understanding%20via%20Gaussian%20Splatting&entry.906535625=Hongyu%20Zhou%20and%20Jiahao%20Shao%20and%20Lu%20Xu%20and%20Dongfeng%20Bai%20and%20Weichao%20Qiu%20and%20Bingbing%20Liu%20and%20Yue%20Wang%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao&entry.1292438233=%20%20Holistic%20understanding%20of%20urban%20scenes%20based%20on%20RGB%20images%20is%20a%20challenging%0Ayet%20important%20problem.%20It%20encompasses%20understanding%20both%20the%20geometry%20and%0Aappearance%20to%20enable%20novel%20view%20synthesis%2C%20parsing%20semantic%20labels%2C%20and%0Atracking%20moving%20objects.%20Despite%20considerable%20progress%2C%20existing%20approaches%0Aoften%20focus%20on%20specific%20aspects%20of%20this%20task%20and%20require%20additional%20inputs%20such%0Aas%20LiDAR%20scans%20or%20manually%20annotated%203D%20bounding%20boxes.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20pipeline%20that%20utilizes%203D%20Gaussian%20Splatting%20for%20holistic%0Aurban%20scene%20understanding.%20Our%20main%20idea%20involves%20the%20joint%20optimization%20of%0Ageometry%2C%20appearance%2C%20semantics%2C%20and%20motion%20using%20a%20combination%20of%20static%20and%0Adynamic%203D%20Gaussians%2C%20where%20moving%20object%20poses%20are%20regularized%20via%20physical%0Aconstraints.%20Our%20approach%20offers%20the%20ability%20to%20render%20new%20viewpoints%20in%0Areal-time%2C%20yielding%202D%20and%203D%20semantic%20information%20with%20high%20accuracy%2C%20and%0Areconstruct%20dynamic%20scenes%2C%20even%20in%20scenarios%20where%203D%20bounding%20box%20detection%0Aare%20highly%20noisy.%20Experimental%20results%20on%20KITTI%2C%20KITTI-360%2C%20and%20Virtual%20KITTI%202%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12722v1&entry.124074799=Read"},
{"title": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks", "author": "Andreas Happe and Aaron Kaplan and J\u00fcrgen Cito", "abstract": "  Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.\n", "link": "http://arxiv.org/abs/2310.11409v3", "date": "2024-03-19", "relevancy": 2.2013, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4448}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4443}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4317}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks&body=Title%3A%20LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks%0AAuthor%3A%20Andreas%20Happe%20and%20Aaron%20Kaplan%20and%20J%C3%BCrgen%20Cito%0AAbstract%3A%20%20%20Penetration%20testing%2C%20an%20essential%20component%20of%20software%20security%20testing%2C%0Aallows%20organizations%20to%20proactively%20identify%20and%20remediate%20vulnerabilities%20in%0Atheir%20systems%2C%20thus%20bolstering%20their%20defense%20mechanisms%20against%20potential%0Acyberattacks.%20One%20recent%20advancement%20in%20the%20realm%20of%20penetration%20testing%20is%20the%0Autilization%20of%20Language%20Models%20%28LLMs%29.%0A%20%20We%20explore%20the%20intersection%20of%20LLMs%20and%20penetration%20testing%20to%20gain%20insight%0Ainto%20their%20capabilities%20and%20challenges%20in%20the%20context%20of%20privilege%20escalation.%0AWe%20create%20an%20automated%20Linux%20privilege-escalation%20benchmark%20utilizing%20local%0Avirtual%20machines.%20We%20introduce%20an%20LLM-guided%20privilege-escalation%20tool%20designed%0Afor%20evaluating%20different%20LLMs%20and%20prompt%20strategies%20against%20our%20benchmark.%0A%20%20Our%20results%20show%20that%20GPT-4%20is%20well%20suited%20for%20detecting%20file-based%20exploits%0Aas%20it%20can%20typically%20solve%2075-100%5C%25%20of%20test-cases%20of%20that%20vulnerability%20class.%0AGPT-3.5-turbo%20was%20only%20able%20to%20solve%2025-50%25%20of%20those%2C%20while%20local%20models%2C%20such%0Aas%20Llama2%20were%20not%20able%20to%20detect%20any%20exploits.%20We%20analyze%20the%20impact%20of%0Adifferent%20prompt%20designs%2C%20the%20benefits%20of%20in-context%20learning%2C%20and%20the%0Aadvantages%20of%20offering%20high-level%20guidance%20to%20LLMs.%20We%20discuss%20challenging%0Aareas%20for%20LLMs%2C%20including%20maintaining%20focus%20during%20testing%2C%20coping%20with%20errors%2C%0Aand%20finally%20comparing%20them%20with%20both%20stochastic%20parrots%20as%20well%20as%20with%20human%0Ahackers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11409v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Hackers%3A%20Autonomous%20Linux%20Privilege%20Escalation%20Attacks&entry.906535625=Andreas%20Happe%20and%20Aaron%20Kaplan%20and%20J%C3%BCrgen%20Cito&entry.1292438233=%20%20Penetration%20testing%2C%20an%20essential%20component%20of%20software%20security%20testing%2C%0Aallows%20organizations%20to%20proactively%20identify%20and%20remediate%20vulnerabilities%20in%0Atheir%20systems%2C%20thus%20bolstering%20their%20defense%20mechanisms%20against%20potential%0Acyberattacks.%20One%20recent%20advancement%20in%20the%20realm%20of%20penetration%20testing%20is%20the%0Autilization%20of%20Language%20Models%20%28LLMs%29.%0A%20%20We%20explore%20the%20intersection%20of%20LLMs%20and%20penetration%20testing%20to%20gain%20insight%0Ainto%20their%20capabilities%20and%20challenges%20in%20the%20context%20of%20privilege%20escalation.%0AWe%20create%20an%20automated%20Linux%20privilege-escalation%20benchmark%20utilizing%20local%0Avirtual%20machines.%20We%20introduce%20an%20LLM-guided%20privilege-escalation%20tool%20designed%0Afor%20evaluating%20different%20LLMs%20and%20prompt%20strategies%20against%20our%20benchmark.%0A%20%20Our%20results%20show%20that%20GPT-4%20is%20well%20suited%20for%20detecting%20file-based%20exploits%0Aas%20it%20can%20typically%20solve%2075-100%5C%25%20of%20test-cases%20of%20that%20vulnerability%20class.%0AGPT-3.5-turbo%20was%20only%20able%20to%20solve%2025-50%25%20of%20those%2C%20while%20local%20models%2C%20such%0Aas%20Llama2%20were%20not%20able%20to%20detect%20any%20exploits.%20We%20analyze%20the%20impact%20of%0Adifferent%20prompt%20designs%2C%20the%20benefits%20of%20in-context%20learning%2C%20and%20the%0Aadvantages%20of%20offering%20high-level%20guidance%20to%20LLMs.%20We%20discuss%20challenging%0Aareas%20for%20LLMs%2C%20including%20maintaining%20focus%20during%20testing%2C%20coping%20with%20errors%2C%0Aand%20finally%20comparing%20them%20with%20both%20stochastic%20parrots%20as%20well%20as%20with%20human%0Ahackers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11409v3&entry.124074799=Read"},
{"title": "Towards image compression with perfect realism at ultra-low bitrates", "author": "Marl\u00e8ne Careil and Matthew J. Muckley and Jakob Verbeek and St\u00e9phane Lathuili\u00e8re", "abstract": "  Image codecs are typically optimized to trade-off bitrate \\vs distortion\nmetrics. At low bitrates, this leads to compression artefacts which are easily\nperceptible, even when training with perceptual or adversarial losses. To\nimprove image quality and remove dependency on the bitrate, we propose to\ndecode with iterative diffusion models. We condition the decoding process on a\nvector-quantized image representation, as well as a global image description to\nprovide additional context. We dub our model PerCo for 'perceptual\ncompression', and compare it to state-of-the-art codecs at rates from 0.1 down\nto 0.003 bits per pixel. The latter rate is more than an order of magnitude\nsmaller than those considered in most prior work, compressing a 512x768 Kodak\nimage with less than 153 bytes. Despite this ultra-low bitrate, our approach\nmaintains the ability to reconstruct realistic images. We find that our model\nleads to reconstructions with state-of-the-art visual quality as measured by\nFID and KID. As predicted by rate-distortion-perception theory, visual quality\nis less dependent on the bitrate than previous methods.\n", "link": "http://arxiv.org/abs/2310.10325v2", "date": "2024-03-19", "relevancy": 2.1964, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5592}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5291}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20image%20compression%20with%20perfect%20realism%20at%20ultra-low%20bitrates&body=Title%3A%20Towards%20image%20compression%20with%20perfect%20realism%20at%20ultra-low%20bitrates%0AAuthor%3A%20Marl%C3%A8ne%20Careil%20and%20Matthew%20J.%20Muckley%20and%20Jakob%20Verbeek%20and%20St%C3%A9phane%20Lathuili%C3%A8re%0AAbstract%3A%20%20%20Image%20codecs%20are%20typically%20optimized%20to%20trade-off%20bitrate%20%5Cvs%20distortion%0Ametrics.%20At%20low%20bitrates%2C%20this%20leads%20to%20compression%20artefacts%20which%20are%20easily%0Aperceptible%2C%20even%20when%20training%20with%20perceptual%20or%20adversarial%20losses.%20To%0Aimprove%20image%20quality%20and%20remove%20dependency%20on%20the%20bitrate%2C%20we%20propose%20to%0Adecode%20with%20iterative%20diffusion%20models.%20We%20condition%20the%20decoding%20process%20on%20a%0Avector-quantized%20image%20representation%2C%20as%20well%20as%20a%20global%20image%20description%20to%0Aprovide%20additional%20context.%20We%20dub%20our%20model%20PerCo%20for%20%27perceptual%0Acompression%27%2C%20and%20compare%20it%20to%20state-of-the-art%20codecs%20at%20rates%20from%200.1%20down%0Ato%200.003%20bits%20per%20pixel.%20The%20latter%20rate%20is%20more%20than%20an%20order%20of%20magnitude%0Asmaller%20than%20those%20considered%20in%20most%20prior%20work%2C%20compressing%20a%20512x768%20Kodak%0Aimage%20with%20less%20than%20153%20bytes.%20Despite%20this%20ultra-low%20bitrate%2C%20our%20approach%0Amaintains%20the%20ability%20to%20reconstruct%20realistic%20images.%20We%20find%20that%20our%20model%0Aleads%20to%20reconstructions%20with%20state-of-the-art%20visual%20quality%20as%20measured%20by%0AFID%20and%20KID.%20As%20predicted%20by%20rate-distortion-perception%20theory%2C%20visual%20quality%0Ais%20less%20dependent%20on%20the%20bitrate%20than%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10325v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20image%20compression%20with%20perfect%20realism%20at%20ultra-low%20bitrates&entry.906535625=Marl%C3%A8ne%20Careil%20and%20Matthew%20J.%20Muckley%20and%20Jakob%20Verbeek%20and%20St%C3%A9phane%20Lathuili%C3%A8re&entry.1292438233=%20%20Image%20codecs%20are%20typically%20optimized%20to%20trade-off%20bitrate%20%5Cvs%20distortion%0Ametrics.%20At%20low%20bitrates%2C%20this%20leads%20to%20compression%20artefacts%20which%20are%20easily%0Aperceptible%2C%20even%20when%20training%20with%20perceptual%20or%20adversarial%20losses.%20To%0Aimprove%20image%20quality%20and%20remove%20dependency%20on%20the%20bitrate%2C%20we%20propose%20to%0Adecode%20with%20iterative%20diffusion%20models.%20We%20condition%20the%20decoding%20process%20on%20a%0Avector-quantized%20image%20representation%2C%20as%20well%20as%20a%20global%20image%20description%20to%0Aprovide%20additional%20context.%20We%20dub%20our%20model%20PerCo%20for%20%27perceptual%0Acompression%27%2C%20and%20compare%20it%20to%20state-of-the-art%20codecs%20at%20rates%20from%200.1%20down%0Ato%200.003%20bits%20per%20pixel.%20The%20latter%20rate%20is%20more%20than%20an%20order%20of%20magnitude%0Asmaller%20than%20those%20considered%20in%20most%20prior%20work%2C%20compressing%20a%20512x768%20Kodak%0Aimage%20with%20less%20than%20153%20bytes.%20Despite%20this%20ultra-low%20bitrate%2C%20our%20approach%0Amaintains%20the%20ability%20to%20reconstruct%20realistic%20images.%20We%20find%20that%20our%20model%0Aleads%20to%20reconstructions%20with%20state-of-the-art%20visual%20quality%20as%20measured%20by%0AFID%20and%20KID.%20As%20predicted%20by%20rate-distortion-perception%20theory%2C%20visual%20quality%0Ais%20less%20dependent%20on%20the%20bitrate%20than%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10325v2&entry.124074799=Read"},
{"title": "Inter- and intra-uncertainty based feature aggregation model for\n  semi-supervised histopathology image segmentation", "author": "Qiangguo Jin and Hui Cui and Changming Sun and Yang Song and Jiangbin Zheng and Leilei Cao and Leyi Wei and Ran Su", "abstract": "  Acquiring pixel-level annotations is often limited in applications such as\nhistology studies that require domain expertise. Various semi-supervised\nlearning approaches have been developed to work with limited ground truth\nannotations, such as the popular teacher-student models. However, hierarchical\nprediction uncertainty within the student model (intra-uncertainty) and image\nprediction uncertainty (inter-uncertainty) have not been fully utilized by\nexisting methods. To address these issues, we first propose a novel inter- and\nintra-uncertainty regularization method to measure and constrain both inter-\nand intra-inconsistencies in the teacher-student architecture. We also propose\na new two-stage network with pseudo-mask guided feature aggregation (PG-FANet)\nas the segmentation model. The two-stage structure complements with the\nuncertainty regularization strategy to avoid introducing extra modules in\nsolving uncertainties and the aggregation mechanisms enable multi-scale and\nmulti-stage feature integration. Comprehensive experimental results over the\nMoNuSeg and CRAG datasets show that our PG-FANet outperforms other\nstate-of-the-art methods and our semi-supervised learning framework yields\ncompetitive performance with a limited amount of labeled data.\n", "link": "http://arxiv.org/abs/2403.12767v1", "date": "2024-03-19", "relevancy": 2.1957, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5684}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5639}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5235}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inter-%20and%20intra-uncertainty%20based%20feature%20aggregation%20model%20for%0A%20%20semi-supervised%20histopathology%20image%20segmentation&body=Title%3A%20Inter-%20and%20intra-uncertainty%20based%20feature%20aggregation%20model%20for%0A%20%20semi-supervised%20histopathology%20image%20segmentation%0AAuthor%3A%20Qiangguo%20Jin%20and%20Hui%20Cui%20and%20Changming%20Sun%20and%20Yang%20Song%20and%20Jiangbin%20Zheng%20and%20Leilei%20Cao%20and%20Leyi%20Wei%20and%20Ran%20Su%0AAbstract%3A%20%20%20Acquiring%20pixel-level%20annotations%20is%20often%20limited%20in%20applications%20such%20as%0Ahistology%20studies%20that%20require%20domain%20expertise.%20Various%20semi-supervised%0Alearning%20approaches%20have%20been%20developed%20to%20work%20with%20limited%20ground%20truth%0Aannotations%2C%20such%20as%20the%20popular%20teacher-student%20models.%20However%2C%20hierarchical%0Aprediction%20uncertainty%20within%20the%20student%20model%20%28intra-uncertainty%29%20and%20image%0Aprediction%20uncertainty%20%28inter-uncertainty%29%20have%20not%20been%20fully%20utilized%20by%0Aexisting%20methods.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%20inter-%20and%0Aintra-uncertainty%20regularization%20method%20to%20measure%20and%20constrain%20both%20inter-%0Aand%20intra-inconsistencies%20in%20the%20teacher-student%20architecture.%20We%20also%20propose%0Aa%20new%20two-stage%20network%20with%20pseudo-mask%20guided%20feature%20aggregation%20%28PG-FANet%29%0Aas%20the%20segmentation%20model.%20The%20two-stage%20structure%20complements%20with%20the%0Auncertainty%20regularization%20strategy%20to%20avoid%20introducing%20extra%20modules%20in%0Asolving%20uncertainties%20and%20the%20aggregation%20mechanisms%20enable%20multi-scale%20and%0Amulti-stage%20feature%20integration.%20Comprehensive%20experimental%20results%20over%20the%0AMoNuSeg%20and%20CRAG%20datasets%20show%20that%20our%20PG-FANet%20outperforms%20other%0Astate-of-the-art%20methods%20and%20our%20semi-supervised%20learning%20framework%20yields%0Acompetitive%20performance%20with%20a%20limited%20amount%20of%20labeled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12767v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inter-%20and%20intra-uncertainty%20based%20feature%20aggregation%20model%20for%0A%20%20semi-supervised%20histopathology%20image%20segmentation&entry.906535625=Qiangguo%20Jin%20and%20Hui%20Cui%20and%20Changming%20Sun%20and%20Yang%20Song%20and%20Jiangbin%20Zheng%20and%20Leilei%20Cao%20and%20Leyi%20Wei%20and%20Ran%20Su&entry.1292438233=%20%20Acquiring%20pixel-level%20annotations%20is%20often%20limited%20in%20applications%20such%20as%0Ahistology%20studies%20that%20require%20domain%20expertise.%20Various%20semi-supervised%0Alearning%20approaches%20have%20been%20developed%20to%20work%20with%20limited%20ground%20truth%0Aannotations%2C%20such%20as%20the%20popular%20teacher-student%20models.%20However%2C%20hierarchical%0Aprediction%20uncertainty%20within%20the%20student%20model%20%28intra-uncertainty%29%20and%20image%0Aprediction%20uncertainty%20%28inter-uncertainty%29%20have%20not%20been%20fully%20utilized%20by%0Aexisting%20methods.%20To%20address%20these%20issues%2C%20we%20first%20propose%20a%20novel%20inter-%20and%0Aintra-uncertainty%20regularization%20method%20to%20measure%20and%20constrain%20both%20inter-%0Aand%20intra-inconsistencies%20in%20the%20teacher-student%20architecture.%20We%20also%20propose%0Aa%20new%20two-stage%20network%20with%20pseudo-mask%20guided%20feature%20aggregation%20%28PG-FANet%29%0Aas%20the%20segmentation%20model.%20The%20two-stage%20structure%20complements%20with%20the%0Auncertainty%20regularization%20strategy%20to%20avoid%20introducing%20extra%20modules%20in%0Asolving%20uncertainties%20and%20the%20aggregation%20mechanisms%20enable%20multi-scale%20and%0Amulti-stage%20feature%20integration.%20Comprehensive%20experimental%20results%20over%20the%0AMoNuSeg%20and%20CRAG%20datasets%20show%20that%20our%20PG-FANet%20outperforms%20other%0Astate-of-the-art%20methods%20and%20our%20semi-supervised%20learning%20framework%20yields%0Acompetitive%20performance%20with%20a%20limited%20amount%20of%20labeled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12767v1&entry.124074799=Read"},
{"title": "ScanTalk: 3D Talking Heads from Unregistered Scans", "author": "Federico Nocentini and Thomas Besnier and Claudio Ferrari and Sylvain Arguillere and Stefano Berretti and Mohamed Daoudi", "abstract": "  Speech-driven 3D talking heads generation has emerged as a significant area\nof interest among researchers, presenting numerous challenges. Existing methods\nare constrained by animating faces with fixed topologies, wherein point-wise\ncorrespondence is established, and the number and order of points remains\nconsistent across all identities the model can animate. In this work, we\npresent ScanTalk, a novel framework capable of animating 3D faces in arbitrary\ntopologies including scanned data. Our approach relies on the DiffusionNet\narchitecture to overcome the fixed topology constraint, offering promising\navenues for more flexible and realistic 3D animations. By leveraging the power\nof DiffusionNet, ScanTalk not only adapts to diverse facial structures but also\nmaintains fidelity when dealing with scanned data, thereby enhancing the\nauthenticity and versatility of generated 3D talking heads. Through\ncomprehensive comparisons with state-of-the-art methods, we validate the\nefficacy of our approach, demonstrating its capacity to generate realistic\ntalking heads comparable to existing techniques. While our primary objective is\nto develop a generic method free from topological constraints, all\nstate-of-the-art methodologies are bound by such limitations. Code for\nreproducing our results, and the pre-trained model will be made available.\n", "link": "http://arxiv.org/abs/2403.10942v2", "date": "2024-03-19", "relevancy": 2.1851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5443}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5441}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ScanTalk%3A%203D%20Talking%20Heads%20from%20Unregistered%20Scans&body=Title%3A%20ScanTalk%3A%203D%20Talking%20Heads%20from%20Unregistered%20Scans%0AAuthor%3A%20Federico%20Nocentini%20and%20Thomas%20Besnier%20and%20Claudio%20Ferrari%20and%20Sylvain%20Arguillere%20and%20Stefano%20Berretti%20and%20Mohamed%20Daoudi%0AAbstract%3A%20%20%20Speech-driven%203D%20talking%20heads%20generation%20has%20emerged%20as%20a%20significant%20area%0Aof%20interest%20among%20researchers%2C%20presenting%20numerous%20challenges.%20Existing%20methods%0Aare%20constrained%20by%20animating%20faces%20with%20fixed%20topologies%2C%20wherein%20point-wise%0Acorrespondence%20is%20established%2C%20and%20the%20number%20and%20order%20of%20points%20remains%0Aconsistent%20across%20all%20identities%20the%20model%20can%20animate.%20In%20this%20work%2C%20we%0Apresent%20ScanTalk%2C%20a%20novel%20framework%20capable%20of%20animating%203D%20faces%20in%20arbitrary%0Atopologies%20including%20scanned%20data.%20Our%20approach%20relies%20on%20the%20DiffusionNet%0Aarchitecture%20to%20overcome%20the%20fixed%20topology%20constraint%2C%20offering%20promising%0Aavenues%20for%20more%20flexible%20and%20realistic%203D%20animations.%20By%20leveraging%20the%20power%0Aof%20DiffusionNet%2C%20ScanTalk%20not%20only%20adapts%20to%20diverse%20facial%20structures%20but%20also%0Amaintains%20fidelity%20when%20dealing%20with%20scanned%20data%2C%20thereby%20enhancing%20the%0Aauthenticity%20and%20versatility%20of%20generated%203D%20talking%20heads.%20Through%0Acomprehensive%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20validate%20the%0Aefficacy%20of%20our%20approach%2C%20demonstrating%20its%20capacity%20to%20generate%20realistic%0Atalking%20heads%20comparable%20to%20existing%20techniques.%20While%20our%20primary%20objective%20is%0Ato%20develop%20a%20generic%20method%20free%20from%20topological%20constraints%2C%20all%0Astate-of-the-art%20methodologies%20are%20bound%20by%20such%20limitations.%20Code%20for%0Areproducing%20our%20results%2C%20and%20the%20pre-trained%20model%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10942v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScanTalk%3A%203D%20Talking%20Heads%20from%20Unregistered%20Scans&entry.906535625=Federico%20Nocentini%20and%20Thomas%20Besnier%20and%20Claudio%20Ferrari%20and%20Sylvain%20Arguillere%20and%20Stefano%20Berretti%20and%20Mohamed%20Daoudi&entry.1292438233=%20%20Speech-driven%203D%20talking%20heads%20generation%20has%20emerged%20as%20a%20significant%20area%0Aof%20interest%20among%20researchers%2C%20presenting%20numerous%20challenges.%20Existing%20methods%0Aare%20constrained%20by%20animating%20faces%20with%20fixed%20topologies%2C%20wherein%20point-wise%0Acorrespondence%20is%20established%2C%20and%20the%20number%20and%20order%20of%20points%20remains%0Aconsistent%20across%20all%20identities%20the%20model%20can%20animate.%20In%20this%20work%2C%20we%0Apresent%20ScanTalk%2C%20a%20novel%20framework%20capable%20of%20animating%203D%20faces%20in%20arbitrary%0Atopologies%20including%20scanned%20data.%20Our%20approach%20relies%20on%20the%20DiffusionNet%0Aarchitecture%20to%20overcome%20the%20fixed%20topology%20constraint%2C%20offering%20promising%0Aavenues%20for%20more%20flexible%20and%20realistic%203D%20animations.%20By%20leveraging%20the%20power%0Aof%20DiffusionNet%2C%20ScanTalk%20not%20only%20adapts%20to%20diverse%20facial%20structures%20but%20also%0Amaintains%20fidelity%20when%20dealing%20with%20scanned%20data%2C%20thereby%20enhancing%20the%0Aauthenticity%20and%20versatility%20of%20generated%203D%20talking%20heads.%20Through%0Acomprehensive%20comparisons%20with%20state-of-the-art%20methods%2C%20we%20validate%20the%0Aefficacy%20of%20our%20approach%2C%20demonstrating%20its%20capacity%20to%20generate%20realistic%0Atalking%20heads%20comparable%20to%20existing%20techniques.%20While%20our%20primary%20objective%20is%0Ato%20develop%20a%20generic%20method%20free%20from%20topological%20constraints%2C%20all%0Astate-of-the-art%20methodologies%20are%20bound%20by%20such%20limitations.%20Code%20for%0Areproducing%20our%20results%2C%20and%20the%20pre-trained%20model%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10942v2&entry.124074799=Read"},
{"title": "FaceXFormer: A Unified Transformer for Facial Analysis", "author": "Kartik Narayan and Vibashan VS and Rama Chellappa and Vishal M. Patel", "abstract": "  In this work, we introduce FaceXformer, an end-to-end unified transformer\nmodel for a comprehensive range of facial analysis tasks such as face parsing,\nlandmark detection, head pose estimation, attributes recognition, and\nestimation of age, gender, race, and landmarks visibility. Conventional methods\nin face analysis have often relied on task-specific designs and preprocessing\ntechniques, which limit their approach to a unified architecture. Unlike these\nconventional methods, our FaceXformer leverages a transformer-based\nencoder-decoder architecture where each task is treated as a learnable token,\nenabling the integration of multiple tasks within a single framework. Moreover,\nwe propose a parameter-efficient decoder, FaceX, which jointly processes face\nand task tokens, thereby learning generalized and robust face representations\nacross different tasks. To the best of our knowledge, this is the first work to\npropose a single model capable of handling all these facial analysis tasks\nusing transformers. We conducted a comprehensive analysis of effective\nbackbones for unified face task processing and evaluated different task queries\nand the synergy between them. We conduct experiments against state-of-the-art\nspecialized models and previous multi-task models in both intra-dataset and\ncross-dataset evaluations across multiple benchmarks. Additionally, our model\neffectively handles images \"in-the-wild,\" demonstrating its robustness and\ngeneralizability across eight different tasks, all while maintaining the\nreal-time performance of 37 FPS.\n", "link": "http://arxiv.org/abs/2403.12960v1", "date": "2024-03-19", "relevancy": 2.1834, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FaceXFormer%3A%20A%20Unified%20Transformer%20for%20Facial%20Analysis&body=Title%3A%20FaceXFormer%3A%20A%20Unified%20Transformer%20for%20Facial%20Analysis%0AAuthor%3A%20Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Rama%20Chellappa%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20FaceXformer%2C%20an%20end-to-end%20unified%20transformer%0Amodel%20for%20a%20comprehensive%20range%20of%20facial%20analysis%20tasks%20such%20as%20face%20parsing%2C%0Alandmark%20detection%2C%20head%20pose%20estimation%2C%20attributes%20recognition%2C%20and%0Aestimation%20of%20age%2C%20gender%2C%20race%2C%20and%20landmarks%20visibility.%20Conventional%20methods%0Ain%20face%20analysis%20have%20often%20relied%20on%20task-specific%20designs%20and%20preprocessing%0Atechniques%2C%20which%20limit%20their%20approach%20to%20a%20unified%20architecture.%20Unlike%20these%0Aconventional%20methods%2C%20our%20FaceXformer%20leverages%20a%20transformer-based%0Aencoder-decoder%20architecture%20where%20each%20task%20is%20treated%20as%20a%20learnable%20token%2C%0Aenabling%20the%20integration%20of%20multiple%20tasks%20within%20a%20single%20framework.%20Moreover%2C%0Awe%20propose%20a%20parameter-efficient%20decoder%2C%20FaceX%2C%20which%20jointly%20processes%20face%0Aand%20task%20tokens%2C%20thereby%20learning%20generalized%20and%20robust%20face%20representations%0Aacross%20different%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%0Apropose%20a%20single%20model%20capable%20of%20handling%20all%20these%20facial%20analysis%20tasks%0Ausing%20transformers.%20We%20conducted%20a%20comprehensive%20analysis%20of%20effective%0Abackbones%20for%20unified%20face%20task%20processing%20and%20evaluated%20different%20task%20queries%0Aand%20the%20synergy%20between%20them.%20We%20conduct%20experiments%20against%20state-of-the-art%0Aspecialized%20models%20and%20previous%20multi-task%20models%20in%20both%20intra-dataset%20and%0Across-dataset%20evaluations%20across%20multiple%20benchmarks.%20Additionally%2C%20our%20model%0Aeffectively%20handles%20images%20%22in-the-wild%2C%22%20demonstrating%20its%20robustness%20and%0Ageneralizability%20across%20eight%20different%20tasks%2C%20all%20while%20maintaining%20the%0Areal-time%20performance%20of%2037%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12960v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceXFormer%3A%20A%20Unified%20Transformer%20for%20Facial%20Analysis&entry.906535625=Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Rama%20Chellappa%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20FaceXformer%2C%20an%20end-to-end%20unified%20transformer%0Amodel%20for%20a%20comprehensive%20range%20of%20facial%20analysis%20tasks%20such%20as%20face%20parsing%2C%0Alandmark%20detection%2C%20head%20pose%20estimation%2C%20attributes%20recognition%2C%20and%0Aestimation%20of%20age%2C%20gender%2C%20race%2C%20and%20landmarks%20visibility.%20Conventional%20methods%0Ain%20face%20analysis%20have%20often%20relied%20on%20task-specific%20designs%20and%20preprocessing%0Atechniques%2C%20which%20limit%20their%20approach%20to%20a%20unified%20architecture.%20Unlike%20these%0Aconventional%20methods%2C%20our%20FaceXformer%20leverages%20a%20transformer-based%0Aencoder-decoder%20architecture%20where%20each%20task%20is%20treated%20as%20a%20learnable%20token%2C%0Aenabling%20the%20integration%20of%20multiple%20tasks%20within%20a%20single%20framework.%20Moreover%2C%0Awe%20propose%20a%20parameter-efficient%20decoder%2C%20FaceX%2C%20which%20jointly%20processes%20face%0Aand%20task%20tokens%2C%20thereby%20learning%20generalized%20and%20robust%20face%20representations%0Aacross%20different%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%0Apropose%20a%20single%20model%20capable%20of%20handling%20all%20these%20facial%20analysis%20tasks%0Ausing%20transformers.%20We%20conducted%20a%20comprehensive%20analysis%20of%20effective%0Abackbones%20for%20unified%20face%20task%20processing%20and%20evaluated%20different%20task%20queries%0Aand%20the%20synergy%20between%20them.%20We%20conduct%20experiments%20against%20state-of-the-art%0Aspecialized%20models%20and%20previous%20multi-task%20models%20in%20both%20intra-dataset%20and%0Across-dataset%20evaluations%20across%20multiple%20benchmarks.%20Additionally%2C%20our%20model%0Aeffectively%20handles%20images%20%22in-the-wild%2C%22%20demonstrating%20its%20robustness%20and%0Ageneralizability%20across%20eight%20different%20tasks%2C%20all%20while%20maintaining%20the%0Areal-time%20performance%20of%2037%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12960v1&entry.124074799=Read"},
{"title": "Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical\n  Flow Estimation", "author": "Shubham Negi and Deepika Sharma and Adarsh Kumar Kosta and Kaushik Roy", "abstract": "  In the field of robotics, event-based cameras are emerging as a promising\nlow-power alternative to traditional frame-based cameras for capturing\nhigh-speed motion and high dynamic range scenes. This is due to their sparse\nand asynchronous event outputs. Spiking Neural Networks (SNNs) with their\nasynchronous event-driven compute, show great potential for extracting the\nspatio-temporal features from these event streams. In contrast, the standard\nAnalog Neural Networks (ANNs) fail to process event data effectively. However,\ntraining SNNs is difficult due to additional trainable parameters (thresholds\nand leaks), vanishing spikes at deeper layers, and a non-differentiable binary\nactivation function. Furthermore, an additional data structure, membrane\npotential, responsible for keeping track of temporal information, must be\nfetched and updated at every timestep in SNNs. To overcome these challenges, we\npropose a novel SNN-ANN hybrid architecture that combines the strengths of\nboth. Specifically, we leverage the asynchronous compute capabilities of SNN\nlayers to effectively extract the input temporal information. Concurrently, the\nANN layers facilitate training and efficient hardware deployment on traditional\nmachine learning hardware such as GPUs. We provide extensive experimental\nanalysis for assigning each layer to be spiking or analog, leading to a network\nconfiguration optimized for performance and ease of training. We evaluate our\nhybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle\nStereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid\nSNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)\nwith 22% lower energy consumption compared to Full-SNN, and 48% lower AEE\ncompared to Full-ANN, while maintaining comparable energy usage.\n", "link": "http://arxiv.org/abs/2306.02960v2", "date": "2024-03-19", "relevancy": 2.1797, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5679}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5557}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Best%20of%20Both%20Worlds%3A%20Hybrid%20SNN-ANN%20Architecture%20for%20Event-based%20Optical%0A%20%20Flow%20Estimation&body=Title%3A%20Best%20of%20Both%20Worlds%3A%20Hybrid%20SNN-ANN%20Architecture%20for%20Event-based%20Optical%0A%20%20Flow%20Estimation%0AAuthor%3A%20Shubham%20Negi%20and%20Deepika%20Sharma%20and%20Adarsh%20Kumar%20Kosta%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20In%20the%20field%20of%20robotics%2C%20event-based%20cameras%20are%20emerging%20as%20a%20promising%0Alow-power%20alternative%20to%20traditional%20frame-based%20cameras%20for%20capturing%0Ahigh-speed%20motion%20and%20high%20dynamic%20range%20scenes.%20This%20is%20due%20to%20their%20sparse%0Aand%20asynchronous%20event%20outputs.%20Spiking%20Neural%20Networks%20%28SNNs%29%20with%20their%0Aasynchronous%20event-driven%20compute%2C%20show%20great%20potential%20for%20extracting%20the%0Aspatio-temporal%20features%20from%20these%20event%20streams.%20In%20contrast%2C%20the%20standard%0AAnalog%20Neural%20Networks%20%28ANNs%29%20fail%20to%20process%20event%20data%20effectively.%20However%2C%0Atraining%20SNNs%20is%20difficult%20due%20to%20additional%20trainable%20parameters%20%28thresholds%0Aand%20leaks%29%2C%20vanishing%20spikes%20at%20deeper%20layers%2C%20and%20a%20non-differentiable%20binary%0Aactivation%20function.%20Furthermore%2C%20an%20additional%20data%20structure%2C%20membrane%0Apotential%2C%20responsible%20for%20keeping%20track%20of%20temporal%20information%2C%20must%20be%0Afetched%20and%20updated%20at%20every%20timestep%20in%20SNNs.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20a%20novel%20SNN-ANN%20hybrid%20architecture%20that%20combines%20the%20strengths%20of%0Aboth.%20Specifically%2C%20we%20leverage%20the%20asynchronous%20compute%20capabilities%20of%20SNN%0Alayers%20to%20effectively%20extract%20the%20input%20temporal%20information.%20Concurrently%2C%20the%0AANN%20layers%20facilitate%20training%20and%20efficient%20hardware%20deployment%20on%20traditional%0Amachine%20learning%20hardware%20such%20as%20GPUs.%20We%20provide%20extensive%20experimental%0Aanalysis%20for%20assigning%20each%20layer%20to%20be%20spiking%20or%20analog%2C%20leading%20to%20a%20network%0Aconfiguration%20optimized%20for%20performance%20and%20ease%20of%20training.%20We%20evaluate%20our%0Ahybrid%20architecture%20for%20optical%20flow%20estimation%20on%20DSEC-flow%20and%20Multi-Vehicle%0AStereo%20Event-Camera%20%28MVSEC%29%20datasets.%20On%20the%20DSEC-flow%20dataset%2C%20the%20hybrid%0ASNN-ANN%20architecture%20achieves%20a%2040%25%20reduction%20in%20average%20endpoint%20error%20%28AEE%29%0Awith%2022%25%20lower%20energy%20consumption%20compared%20to%20Full-SNN%2C%20and%2048%25%20lower%20AEE%0Acompared%20to%20Full-ANN%2C%20while%20maintaining%20comparable%20energy%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02960v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best%20of%20Both%20Worlds%3A%20Hybrid%20SNN-ANN%20Architecture%20for%20Event-based%20Optical%0A%20%20Flow%20Estimation&entry.906535625=Shubham%20Negi%20and%20Deepika%20Sharma%20and%20Adarsh%20Kumar%20Kosta%20and%20Kaushik%20Roy&entry.1292438233=%20%20In%20the%20field%20of%20robotics%2C%20event-based%20cameras%20are%20emerging%20as%20a%20promising%0Alow-power%20alternative%20to%20traditional%20frame-based%20cameras%20for%20capturing%0Ahigh-speed%20motion%20and%20high%20dynamic%20range%20scenes.%20This%20is%20due%20to%20their%20sparse%0Aand%20asynchronous%20event%20outputs.%20Spiking%20Neural%20Networks%20%28SNNs%29%20with%20their%0Aasynchronous%20event-driven%20compute%2C%20show%20great%20potential%20for%20extracting%20the%0Aspatio-temporal%20features%20from%20these%20event%20streams.%20In%20contrast%2C%20the%20standard%0AAnalog%20Neural%20Networks%20%28ANNs%29%20fail%20to%20process%20event%20data%20effectively.%20However%2C%0Atraining%20SNNs%20is%20difficult%20due%20to%20additional%20trainable%20parameters%20%28thresholds%0Aand%20leaks%29%2C%20vanishing%20spikes%20at%20deeper%20layers%2C%20and%20a%20non-differentiable%20binary%0Aactivation%20function.%20Furthermore%2C%20an%20additional%20data%20structure%2C%20membrane%0Apotential%2C%20responsible%20for%20keeping%20track%20of%20temporal%20information%2C%20must%20be%0Afetched%20and%20updated%20at%20every%20timestep%20in%20SNNs.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20a%20novel%20SNN-ANN%20hybrid%20architecture%20that%20combines%20the%20strengths%20of%0Aboth.%20Specifically%2C%20we%20leverage%20the%20asynchronous%20compute%20capabilities%20of%20SNN%0Alayers%20to%20effectively%20extract%20the%20input%20temporal%20information.%20Concurrently%2C%20the%0AANN%20layers%20facilitate%20training%20and%20efficient%20hardware%20deployment%20on%20traditional%0Amachine%20learning%20hardware%20such%20as%20GPUs.%20We%20provide%20extensive%20experimental%0Aanalysis%20for%20assigning%20each%20layer%20to%20be%20spiking%20or%20analog%2C%20leading%20to%20a%20network%0Aconfiguration%20optimized%20for%20performance%20and%20ease%20of%20training.%20We%20evaluate%20our%0Ahybrid%20architecture%20for%20optical%20flow%20estimation%20on%20DSEC-flow%20and%20Multi-Vehicle%0AStereo%20Event-Camera%20%28MVSEC%29%20datasets.%20On%20the%20DSEC-flow%20dataset%2C%20the%20hybrid%0ASNN-ANN%20architecture%20achieves%20a%2040%25%20reduction%20in%20average%20endpoint%20error%20%28AEE%29%0Awith%2022%25%20lower%20energy%20consumption%20compared%20to%20Full-SNN%2C%20and%2048%25%20lower%20AEE%0Acompared%20to%20Full-ANN%2C%20while%20maintaining%20comparable%20energy%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02960v2&entry.124074799=Read"},
{"title": "MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis", "author": "Xuqian Ren and Wenjia Wang and Dingding Cai and Tuuli Tuominen and Juho Kannala and Esa Rahtu", "abstract": "  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n", "link": "http://arxiv.org/abs/2311.02778v2", "date": "2024-03-19", "relevancy": 2.1779, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5605}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5337}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5327}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MuSHRoom%3A%20Multi-Sensor%20Hybrid%20Room%20Dataset%20for%20Joint%203D%20Reconstruction%0A%20%20and%20Novel%20View%20Synthesis&body=Title%3A%20MuSHRoom%3A%20Multi-Sensor%20Hybrid%20Room%20Dataset%20for%20Joint%203D%20Reconstruction%0A%20%20and%20Novel%20View%20Synthesis%0AAuthor%3A%20Xuqian%20Ren%20and%20Wenjia%20Wang%20and%20Dingding%20Cai%20and%20Tuuli%20Tuominen%20and%20Juho%20Kannala%20and%20Esa%20Rahtu%0AAbstract%3A%20%20%20Metaverse%20technologies%20demand%20accurate%2C%20real-time%2C%20and%20immersive%20modeling%20on%0Aconsumer-grade%20hardware%20for%20both%20non-human%20perception%20%28e.g.%2C%0Adrone/robot/autonomous%20car%20navigation%29%20and%20immersive%20technologies%20like%20AR/VR%2C%0Arequiring%20both%20structural%20accuracy%20and%20photorealism.%20However%2C%20there%20exists%20a%0Aknowledge%20gap%20in%20how%20to%20apply%20geometric%20reconstruction%20and%20photorealism%0Amodeling%20%28novel%20view%20synthesis%29%20in%20a%20unified%20framework.%20To%20address%20this%20gap%20and%0Apromote%20the%20development%20of%20robust%20and%20immersive%20modeling%20and%20rendering%20with%0Aconsumer-grade%20devices%2C%20we%20propose%20a%20real-world%20Multi-Sensor%20Hybrid%20Room%0ADataset%20%28MuSHRoom%29.%20Our%20dataset%20presents%20exciting%20challenges%20and%20requires%0Astate-of-the-art%20methods%20to%20be%20cost-effective%2C%20robust%20to%20noisy%20data%20and%0Adevices%2C%20and%20can%20jointly%20learn%203D%20reconstruction%20and%20novel%20view%20synthesis%0Ainstead%20of%20treating%20them%20as%20separate%20tasks%2C%20making%20them%20ideal%20for%20real-world%0Aapplications.%20We%20benchmark%20several%20famous%20pipelines%20on%20our%20dataset%20for%20joint%203D%0Amesh%20reconstruction%20and%20novel%20view%20synthesis.%20Our%20dataset%20and%20benchmark%20show%0Agreat%20potential%20in%20promoting%20the%20improvements%20for%20fusing%203D%20reconstruction%20and%0Ahigh-quality%20rendering%20in%20a%20robust%20and%20computationally%20efficient%20end-to-end%0Afashion.%20The%20dataset%20and%20code%20are%20available%20at%20the%20project%20website%3A%0Ahttps%3A//xuqianren.github.io/publications/MuSHRoom/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02778v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuSHRoom%3A%20Multi-Sensor%20Hybrid%20Room%20Dataset%20for%20Joint%203D%20Reconstruction%0A%20%20and%20Novel%20View%20Synthesis&entry.906535625=Xuqian%20Ren%20and%20Wenjia%20Wang%20and%20Dingding%20Cai%20and%20Tuuli%20Tuominen%20and%20Juho%20Kannala%20and%20Esa%20Rahtu&entry.1292438233=%20%20Metaverse%20technologies%20demand%20accurate%2C%20real-time%2C%20and%20immersive%20modeling%20on%0Aconsumer-grade%20hardware%20for%20both%20non-human%20perception%20%28e.g.%2C%0Adrone/robot/autonomous%20car%20navigation%29%20and%20immersive%20technologies%20like%20AR/VR%2C%0Arequiring%20both%20structural%20accuracy%20and%20photorealism.%20However%2C%20there%20exists%20a%0Aknowledge%20gap%20in%20how%20to%20apply%20geometric%20reconstruction%20and%20photorealism%0Amodeling%20%28novel%20view%20synthesis%29%20in%20a%20unified%20framework.%20To%20address%20this%20gap%20and%0Apromote%20the%20development%20of%20robust%20and%20immersive%20modeling%20and%20rendering%20with%0Aconsumer-grade%20devices%2C%20we%20propose%20a%20real-world%20Multi-Sensor%20Hybrid%20Room%0ADataset%20%28MuSHRoom%29.%20Our%20dataset%20presents%20exciting%20challenges%20and%20requires%0Astate-of-the-art%20methods%20to%20be%20cost-effective%2C%20robust%20to%20noisy%20data%20and%0Adevices%2C%20and%20can%20jointly%20learn%203D%20reconstruction%20and%20novel%20view%20synthesis%0Ainstead%20of%20treating%20them%20as%20separate%20tasks%2C%20making%20them%20ideal%20for%20real-world%0Aapplications.%20We%20benchmark%20several%20famous%20pipelines%20on%20our%20dataset%20for%20joint%203D%0Amesh%20reconstruction%20and%20novel%20view%20synthesis.%20Our%20dataset%20and%20benchmark%20show%0Agreat%20potential%20in%20promoting%20the%20improvements%20for%20fusing%203D%20reconstruction%20and%0Ahigh-quality%20rendering%20in%20a%20robust%20and%20computationally%20efficient%20end-to-end%0Afashion.%20The%20dataset%20and%20code%20are%20available%20at%20the%20project%20website%3A%0Ahttps%3A//xuqianren.github.io/publications/MuSHRoom/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02778v2&entry.124074799=Read"},
{"title": "DDSB: An Unsupervised and Training-free Method for Phase Detection in\n  Echocardiography", "author": "Zhenyu Bu and Yang Liu and Jiayu Huo and Jingjing Peng and Kaini Wang and Guangquan Zhou and Rachel Sparks and Prokar Dasgupta and Alejandro Granados and Sebastien Ourselin", "abstract": "  Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is\nkey for cardiac function assessment through echocardiography. However,\ntraditional methods face several limitations: they require extensive amounts of\ndata, extensive annotations by medical experts, significant training resources,\nand often lack robustness. Addressing these challenges, we proposed an\nunsupervised and training-free method, our novel approach leverages\nunsupervised segmentation to enhance fault tolerance against segmentation\ninaccuracies. By identifying anchor points and analyzing directional\ndeformation, we effectively reduce dependence on the accuracy of initial\nsegmentation images and enhance fault tolerance, all while improving\nrobustness. Tested on Echo-dynamic and CAMUS datasets, our method achieves\ncomparable accuracy to learning-based models without their associated\ndrawbacks. The code is available at https://github.com/MRUIL/DDSB\n", "link": "http://arxiv.org/abs/2403.12787v1", "date": "2024-03-19", "relevancy": 2.175, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DDSB%3A%20An%20Unsupervised%20and%20Training-free%20Method%20for%20Phase%20Detection%20in%0A%20%20Echocardiography&body=Title%3A%20DDSB%3A%20An%20Unsupervised%20and%20Training-free%20Method%20for%20Phase%20Detection%20in%0A%20%20Echocardiography%0AAuthor%3A%20Zhenyu%20Bu%20and%20Yang%20Liu%20and%20Jiayu%20Huo%20and%20Jingjing%20Peng%20and%20Kaini%20Wang%20and%20Guangquan%20Zhou%20and%20Rachel%20Sparks%20and%20Prokar%20Dasgupta%20and%20Alejandro%20Granados%20and%20Sebastien%20Ourselin%0AAbstract%3A%20%20%20Accurate%20identification%20of%20End-Diastolic%20%28ED%29%20and%20End-Systolic%20%28ES%29%20frames%20is%0Akey%20for%20cardiac%20function%20assessment%20through%20echocardiography.%20However%2C%0Atraditional%20methods%20face%20several%20limitations%3A%20they%20require%20extensive%20amounts%20of%0Adata%2C%20extensive%20annotations%20by%20medical%20experts%2C%20significant%20training%20resources%2C%0Aand%20often%20lack%20robustness.%20Addressing%20these%20challenges%2C%20we%20proposed%20an%0Aunsupervised%20and%20training-free%20method%2C%20our%20novel%20approach%20leverages%0Aunsupervised%20segmentation%20to%20enhance%20fault%20tolerance%20against%20segmentation%0Ainaccuracies.%20By%20identifying%20anchor%20points%20and%20analyzing%20directional%0Adeformation%2C%20we%20effectively%20reduce%20dependence%20on%20the%20accuracy%20of%20initial%0Asegmentation%20images%20and%20enhance%20fault%20tolerance%2C%20all%20while%20improving%0Arobustness.%20Tested%20on%20Echo-dynamic%20and%20CAMUS%20datasets%2C%20our%20method%20achieves%0Acomparable%20accuracy%20to%20learning-based%20models%20without%20their%20associated%0Adrawbacks.%20The%20code%20is%20available%20at%20https%3A//github.com/MRUIL/DDSB%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12787v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDSB%3A%20An%20Unsupervised%20and%20Training-free%20Method%20for%20Phase%20Detection%20in%0A%20%20Echocardiography&entry.906535625=Zhenyu%20Bu%20and%20Yang%20Liu%20and%20Jiayu%20Huo%20and%20Jingjing%20Peng%20and%20Kaini%20Wang%20and%20Guangquan%20Zhou%20and%20Rachel%20Sparks%20and%20Prokar%20Dasgupta%20and%20Alejandro%20Granados%20and%20Sebastien%20Ourselin&entry.1292438233=%20%20Accurate%20identification%20of%20End-Diastolic%20%28ED%29%20and%20End-Systolic%20%28ES%29%20frames%20is%0Akey%20for%20cardiac%20function%20assessment%20through%20echocardiography.%20However%2C%0Atraditional%20methods%20face%20several%20limitations%3A%20they%20require%20extensive%20amounts%20of%0Adata%2C%20extensive%20annotations%20by%20medical%20experts%2C%20significant%20training%20resources%2C%0Aand%20often%20lack%20robustness.%20Addressing%20these%20challenges%2C%20we%20proposed%20an%0Aunsupervised%20and%20training-free%20method%2C%20our%20novel%20approach%20leverages%0Aunsupervised%20segmentation%20to%20enhance%20fault%20tolerance%20against%20segmentation%0Ainaccuracies.%20By%20identifying%20anchor%20points%20and%20analyzing%20directional%0Adeformation%2C%20we%20effectively%20reduce%20dependence%20on%20the%20accuracy%20of%20initial%0Asegmentation%20images%20and%20enhance%20fault%20tolerance%2C%20all%20while%20improving%0Arobustness.%20Tested%20on%20Echo-dynamic%20and%20CAMUS%20datasets%2C%20our%20method%20achieves%0Acomparable%20accuracy%20to%20learning-based%20models%20without%20their%20associated%0Adrawbacks.%20The%20code%20is%20available%20at%20https%3A//github.com/MRUIL/DDSB%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12787v1&entry.124074799=Read"},
{"title": "Generative Enhancement for 3D Medical Images", "author": "Lingting Zhu and Noel Codella and Dongdong Chen and Zhenchao Jin and Lu Yuan and Lequan Yu", "abstract": "  The limited availability of 3D medical image datasets, due to privacy\nconcerns and high collection or annotation costs, poses significant challenges\nin the field of medical imaging. While a promising alternative is the use of\nsynthesized medical data, there are few solutions for realistic 3D medical\nimage synthesis due to difficulties in backbone design and fewer 3D training\nsamples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel\ngenerative approach to the synthesis of 3D medical images and the enhancement\nof existing datasets using conditional diffusion models. Our method begins with\na 2D slice, noted as the informed slice to serve the patient prior, and\npropagates the generation process using a 3D segmentation mask. By decomposing\nthe 3D medical images into masks and patient prior information, GEM-3D offers a\nflexible yet effective solution for generating versatile 3D images from\nexisting datasets. GEM-3D can enable dataset enhancement by combining informed\nslice selection and generation at random positions, along with editable mask\nvolumes to introduce large variations in diffusion sampling. Moreover, as the\ninformed slice contains patient-wise information, GEM-3D can also facilitate\ncounterfactual image synthesis and dataset-level de-enhancement with desired\ncontrol. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D\nis capable of synthesizing high-quality 3D medical images with volumetric\nconsistency, offering a straightforward solution for dataset enhancement during\ninference. The code is available at https://github.com/HKU-MedAI/GEM-3D.\n", "link": "http://arxiv.org/abs/2403.12852v1", "date": "2024-03-19", "relevancy": 2.1692, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5736}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Enhancement%20for%203D%20Medical%20Images&body=Title%3A%20Generative%20Enhancement%20for%203D%20Medical%20Images%0AAuthor%3A%20Lingting%20Zhu%20and%20Noel%20Codella%20and%20Dongdong%20Chen%20and%20Zhenchao%20Jin%20and%20Lu%20Yuan%20and%20Lequan%20Yu%0AAbstract%3A%20%20%20The%20limited%20availability%20of%203D%20medical%20image%20datasets%2C%20due%20to%20privacy%0Aconcerns%20and%20high%20collection%20or%20annotation%20costs%2C%20poses%20significant%20challenges%0Ain%20the%20field%20of%20medical%20imaging.%20While%20a%20promising%20alternative%20is%20the%20use%20of%0Asynthesized%20medical%20data%2C%20there%20are%20few%20solutions%20for%20realistic%203D%20medical%0Aimage%20synthesis%20due%20to%20difficulties%20in%20backbone%20design%20and%20fewer%203D%20training%0Asamples%20compared%20to%202D%20counterparts.%20In%20this%20paper%2C%20we%20propose%20GEM-3D%2C%20a%20novel%0Agenerative%20approach%20to%20the%20synthesis%20of%203D%20medical%20images%20and%20the%20enhancement%0Aof%20existing%20datasets%20using%20conditional%20diffusion%20models.%20Our%20method%20begins%20with%0Aa%202D%20slice%2C%20noted%20as%20the%20informed%20slice%20to%20serve%20the%20patient%20prior%2C%20and%0Apropagates%20the%20generation%20process%20using%20a%203D%20segmentation%20mask.%20By%20decomposing%0Athe%203D%20medical%20images%20into%20masks%20and%20patient%20prior%20information%2C%20GEM-3D%20offers%20a%0Aflexible%20yet%20effective%20solution%20for%20generating%20versatile%203D%20images%20from%0Aexisting%20datasets.%20GEM-3D%20can%20enable%20dataset%20enhancement%20by%20combining%20informed%0Aslice%20selection%20and%20generation%20at%20random%20positions%2C%20along%20with%20editable%20mask%0Avolumes%20to%20introduce%20large%20variations%20in%20diffusion%20sampling.%20Moreover%2C%20as%20the%0Ainformed%20slice%20contains%20patient-wise%20information%2C%20GEM-3D%20can%20also%20facilitate%0Acounterfactual%20image%20synthesis%20and%20dataset-level%20de-enhancement%20with%20desired%0Acontrol.%20Experiments%20on%20brain%20MRI%20and%20abdomen%20CT%20images%20demonstrate%20that%20GEM-3D%0Ais%20capable%20of%20synthesizing%20high-quality%203D%20medical%20images%20with%20volumetric%0Aconsistency%2C%20offering%20a%20straightforward%20solution%20for%20dataset%20enhancement%20during%0Ainference.%20The%20code%20is%20available%20at%20https%3A//github.com/HKU-MedAI/GEM-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12852v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Enhancement%20for%203D%20Medical%20Images&entry.906535625=Lingting%20Zhu%20and%20Noel%20Codella%20and%20Dongdong%20Chen%20and%20Zhenchao%20Jin%20and%20Lu%20Yuan%20and%20Lequan%20Yu&entry.1292438233=%20%20The%20limited%20availability%20of%203D%20medical%20image%20datasets%2C%20due%20to%20privacy%0Aconcerns%20and%20high%20collection%20or%20annotation%20costs%2C%20poses%20significant%20challenges%0Ain%20the%20field%20of%20medical%20imaging.%20While%20a%20promising%20alternative%20is%20the%20use%20of%0Asynthesized%20medical%20data%2C%20there%20are%20few%20solutions%20for%20realistic%203D%20medical%0Aimage%20synthesis%20due%20to%20difficulties%20in%20backbone%20design%20and%20fewer%203D%20training%0Asamples%20compared%20to%202D%20counterparts.%20In%20this%20paper%2C%20we%20propose%20GEM-3D%2C%20a%20novel%0Agenerative%20approach%20to%20the%20synthesis%20of%203D%20medical%20images%20and%20the%20enhancement%0Aof%20existing%20datasets%20using%20conditional%20diffusion%20models.%20Our%20method%20begins%20with%0Aa%202D%20slice%2C%20noted%20as%20the%20informed%20slice%20to%20serve%20the%20patient%20prior%2C%20and%0Apropagates%20the%20generation%20process%20using%20a%203D%20segmentation%20mask.%20By%20decomposing%0Athe%203D%20medical%20images%20into%20masks%20and%20patient%20prior%20information%2C%20GEM-3D%20offers%20a%0Aflexible%20yet%20effective%20solution%20for%20generating%20versatile%203D%20images%20from%0Aexisting%20datasets.%20GEM-3D%20can%20enable%20dataset%20enhancement%20by%20combining%20informed%0Aslice%20selection%20and%20generation%20at%20random%20positions%2C%20along%20with%20editable%20mask%0Avolumes%20to%20introduce%20large%20variations%20in%20diffusion%20sampling.%20Moreover%2C%20as%20the%0Ainformed%20slice%20contains%20patient-wise%20information%2C%20GEM-3D%20can%20also%20facilitate%0Acounterfactual%20image%20synthesis%20and%20dataset-level%20de-enhancement%20with%20desired%0Acontrol.%20Experiments%20on%20brain%20MRI%20and%20abdomen%20CT%20images%20demonstrate%20that%20GEM-3D%0Ais%20capable%20of%20synthesizing%20high-quality%203D%20medical%20images%20with%20volumetric%0Aconsistency%2C%20offering%20a%20straightforward%20solution%20for%20dataset%20enhancement%20during%0Ainference.%20The%20code%20is%20available%20at%20https%3A//github.com/HKU-MedAI/GEM-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12852v1&entry.124074799=Read"},
{"title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models", "author": "Elaine Sui and Xiaohan Wang and Serena Yeung-Levy", "abstract": "  Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 datasets involving natural distribution shifts and cross-dataset\ngeneralization demonstrate TPS's superior performance, achieving\nstate-of-the-art results while reducing resource requirements.\n", "link": "http://arxiv.org/abs/2403.12952v1", "date": "2024-03-19", "relevancy": 2.1645, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5101}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Just%20Shift%20It%3A%20Test-Time%20Prototype%20Shifting%20for%20Zero-Shot%20Generalization%0A%20%20with%20Vision-Language%20Models&body=Title%3A%20Just%20Shift%20It%3A%20Test-Time%20Prototype%20Shifting%20for%20Zero-Shot%20Generalization%0A%20%20with%20Vision-Language%20Models%0AAuthor%3A%20Elaine%20Sui%20and%20Xiaohan%20Wang%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20Advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20propelled%20the%20field%20of%0Acomputer%20vision%2C%20particularly%20in%20the%20zero-shot%20learning%20setting.%20Despite%20their%0Apromise%2C%20the%20effectiveness%20of%20these%20models%20often%20diminishes%20due%20to%20domain%0Ashifts%20in%20test%20environments.%20To%20address%20this%2C%20we%20introduce%20the%20Test-Time%0APrototype%20Shifting%20%28TPS%29%20framework%2C%20a%20pioneering%20approach%20designed%20to%20adapt%0AVLMs%20to%20test%20datasets%20using%20unlabeled%20test%20inputs.%20Our%20method%20is%20based%20on%20the%0Anotion%20of%20modulating%20per-class%20prototypes%20in%20the%20shared%20embedding%20space.%20By%0Apre-computing%20and%20caching%20prototypes%20generated%20with%20the%20pre-trained%20text%0Aencoder%2C%20TPS%20not%20only%20facilitates%20optimization-free%20prototype%20reuse%20for%0Asubsequent%20predictions%20but%20also%20enables%20seamless%20integration%20with%20current%0Aadvancements%20in%20prompt%20engineering.%20At%20test-time%2C%20TPS%20dynamically%20learns%20shift%0Avectors%20for%20each%20prototype%20based%20solely%20on%20the%20given%20test%20sample%2C%20effectively%0Abridging%20the%20domain%20gap%20and%20enhancing%20classification%20accuracy.%20A%20notable%20aspect%0Aof%20our%20framework%20is%20its%20significantly%20reduced%20memory%20and%20computational%20demands%0Awhen%20compared%20to%20conventional%20text-prompt%20tuning%20methods.%20Extensive%20evaluations%0Aacross%2015%20datasets%20involving%20natural%20distribution%20shifts%20and%20cross-dataset%0Ageneralization%20demonstrate%20TPS%27s%20superior%20performance%2C%20achieving%0Astate-of-the-art%20results%20while%20reducing%20resource%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12952v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Just%20Shift%20It%3A%20Test-Time%20Prototype%20Shifting%20for%20Zero-Shot%20Generalization%0A%20%20with%20Vision-Language%20Models&entry.906535625=Elaine%20Sui%20and%20Xiaohan%20Wang%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20Advancements%20in%20vision-language%20models%20%28VLMs%29%20have%20propelled%20the%20field%20of%0Acomputer%20vision%2C%20particularly%20in%20the%20zero-shot%20learning%20setting.%20Despite%20their%0Apromise%2C%20the%20effectiveness%20of%20these%20models%20often%20diminishes%20due%20to%20domain%0Ashifts%20in%20test%20environments.%20To%20address%20this%2C%20we%20introduce%20the%20Test-Time%0APrototype%20Shifting%20%28TPS%29%20framework%2C%20a%20pioneering%20approach%20designed%20to%20adapt%0AVLMs%20to%20test%20datasets%20using%20unlabeled%20test%20inputs.%20Our%20method%20is%20based%20on%20the%0Anotion%20of%20modulating%20per-class%20prototypes%20in%20the%20shared%20embedding%20space.%20By%0Apre-computing%20and%20caching%20prototypes%20generated%20with%20the%20pre-trained%20text%0Aencoder%2C%20TPS%20not%20only%20facilitates%20optimization-free%20prototype%20reuse%20for%0Asubsequent%20predictions%20but%20also%20enables%20seamless%20integration%20with%20current%0Aadvancements%20in%20prompt%20engineering.%20At%20test-time%2C%20TPS%20dynamically%20learns%20shift%0Avectors%20for%20each%20prototype%20based%20solely%20on%20the%20given%20test%20sample%2C%20effectively%0Abridging%20the%20domain%20gap%20and%20enhancing%20classification%20accuracy.%20A%20notable%20aspect%0Aof%20our%20framework%20is%20its%20significantly%20reduced%20memory%20and%20computational%20demands%0Awhen%20compared%20to%20conventional%20text-prompt%20tuning%20methods.%20Extensive%20evaluations%0Aacross%2015%20datasets%20involving%20natural%20distribution%20shifts%20and%20cross-dataset%0Ageneralization%20demonstrate%20TPS%27s%20superior%20performance%2C%20achieving%0Astate-of-the-art%20results%20while%20reducing%20resource%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12952v1&entry.124074799=Read"},
{"title": "MEDBind: Unifying Language and Multimodal Medical Data Embeddings", "author": "Yuan Gao and Sangwook Kim and David E Austin and Chris McIntosh", "abstract": "  Medical vision-language pretraining models (VLPM) have achieved remarkable\nprogress in fusing chest X-rays (CXR) with clinical texts, introducing\nimage-text data binding approaches that enable zero-shot learning and\ndownstream clinical tasks. However, the current landscape lacks the holistic\nintegration of additional medical modalities, such as electrocardiograms (ECG).\nWe present MEDBind (Medical Electronic patient recorD), which learns joint\nembeddings across CXR, ECG, and medical text. Using text data as the central\nanchor, MEDBind features tri-modality binding, delivering competitive\nperformance in top-K retrieval, zero-shot, and few-shot benchmarks against\nestablished VLPM, and the ability for CXR-to-ECG zero-shot classification and\nretrieval. This seamless integration is achieved through combination of\ncontrastive loss on modality-text pairs with our proposed contrastive loss\nfunction, Edge-Modality Contrastive Loss, fostering a cohesive embedding space\nfor CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve\ndownstream tasks by directly integrating CXR and ECG embeddings into a\nlarge-language model for multimodal prompt tuning.\n", "link": "http://arxiv.org/abs/2403.12894v1", "date": "2024-03-19", "relevancy": 2.1598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5156}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MEDBind%3A%20Unifying%20Language%20and%20Multimodal%20Medical%20Data%20Embeddings&body=Title%3A%20MEDBind%3A%20Unifying%20Language%20and%20Multimodal%20Medical%20Data%20Embeddings%0AAuthor%3A%20Yuan%20Gao%20and%20Sangwook%20Kim%20and%20David%20E%20Austin%20and%20Chris%20McIntosh%0AAbstract%3A%20%20%20Medical%20vision-language%20pretraining%20models%20%28VLPM%29%20have%20achieved%20remarkable%0Aprogress%20in%20fusing%20chest%20X-rays%20%28CXR%29%20with%20clinical%20texts%2C%20introducing%0Aimage-text%20data%20binding%20approaches%20that%20enable%20zero-shot%20learning%20and%0Adownstream%20clinical%20tasks.%20However%2C%20the%20current%20landscape%20lacks%20the%20holistic%0Aintegration%20of%20additional%20medical%20modalities%2C%20such%20as%20electrocardiograms%20%28ECG%29.%0AWe%20present%20MEDBind%20%28Medical%20Electronic%20patient%20recorD%29%2C%20which%20learns%20joint%0Aembeddings%20across%20CXR%2C%20ECG%2C%20and%20medical%20text.%20Using%20text%20data%20as%20the%20central%0Aanchor%2C%20MEDBind%20features%20tri-modality%20binding%2C%20delivering%20competitive%0Aperformance%20in%20top-K%20retrieval%2C%20zero-shot%2C%20and%20few-shot%20benchmarks%20against%0Aestablished%20VLPM%2C%20and%20the%20ability%20for%20CXR-to-ECG%20zero-shot%20classification%20and%0Aretrieval.%20This%20seamless%20integration%20is%20achieved%20through%20combination%20of%0Acontrastive%20loss%20on%20modality-text%20pairs%20with%20our%20proposed%20contrastive%20loss%0Afunction%2C%20Edge-Modality%20Contrastive%20Loss%2C%20fostering%20a%20cohesive%20embedding%20space%0Afor%20CXR%2C%20ECG%2C%20and%20text.%20Finally%2C%20we%20demonstrate%20that%20MEDBind%20can%20improve%0Adownstream%20tasks%20by%20directly%20integrating%20CXR%20and%20ECG%20embeddings%20into%20a%0Alarge-language%20model%20for%20multimodal%20prompt%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12894v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDBind%3A%20Unifying%20Language%20and%20Multimodal%20Medical%20Data%20Embeddings&entry.906535625=Yuan%20Gao%20and%20Sangwook%20Kim%20and%20David%20E%20Austin%20and%20Chris%20McIntosh&entry.1292438233=%20%20Medical%20vision-language%20pretraining%20models%20%28VLPM%29%20have%20achieved%20remarkable%0Aprogress%20in%20fusing%20chest%20X-rays%20%28CXR%29%20with%20clinical%20texts%2C%20introducing%0Aimage-text%20data%20binding%20approaches%20that%20enable%20zero-shot%20learning%20and%0Adownstream%20clinical%20tasks.%20However%2C%20the%20current%20landscape%20lacks%20the%20holistic%0Aintegration%20of%20additional%20medical%20modalities%2C%20such%20as%20electrocardiograms%20%28ECG%29.%0AWe%20present%20MEDBind%20%28Medical%20Electronic%20patient%20recorD%29%2C%20which%20learns%20joint%0Aembeddings%20across%20CXR%2C%20ECG%2C%20and%20medical%20text.%20Using%20text%20data%20as%20the%20central%0Aanchor%2C%20MEDBind%20features%20tri-modality%20binding%2C%20delivering%20competitive%0Aperformance%20in%20top-K%20retrieval%2C%20zero-shot%2C%20and%20few-shot%20benchmarks%20against%0Aestablished%20VLPM%2C%20and%20the%20ability%20for%20CXR-to-ECG%20zero-shot%20classification%20and%0Aretrieval.%20This%20seamless%20integration%20is%20achieved%20through%20combination%20of%0Acontrastive%20loss%20on%20modality-text%20pairs%20with%20our%20proposed%20contrastive%20loss%0Afunction%2C%20Edge-Modality%20Contrastive%20Loss%2C%20fostering%20a%20cohesive%20embedding%20space%0Afor%20CXR%2C%20ECG%2C%20and%20text.%20Finally%2C%20we%20demonstrate%20that%20MEDBind%20can%20improve%0Adownstream%20tasks%20by%20directly%20integrating%20CXR%20and%20ECG%20embeddings%20into%20a%0Alarge-language%20model%20for%20multimodal%20prompt%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12894v1&entry.124074799=Read"},
{"title": "Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation", "author": "Karol Gotkowski and Carsten L\u00fcth and Paul F. J\u00e4ger and Sebastian Ziegler and Lars Kr\u00e4mer and Stefan Denner and Shuhan Xiao and Nico Disch and Klaus H. Maier-Hein and Fabian Isensee", "abstract": "  Traditionally, segmentation algorithms require dense annotations for\ntraining, demanding significant annotation efforts, particularly within the 3D\nmedical imaging field. Scribble-supervised learning emerges as a possible\nsolution to this challenge, promising a reduction in annotation efforts when\ncreating large-scale datasets. Recently, a plethora of methods for optimized\nlearning from scribbles have been proposed, but have so far failed to position\nscribble annotation as a beneficial alternative. We relate this shortcoming to\ntwo major issues: 1) the complex nature of many methods which deeply ties them\nto the underlying segmentation model, thus preventing a migration to more\npowerful state-of-the-art models as the field progresses and 2) the lack of a\nsystematic evaluation to validate consistent performance across the broader\nmedical domain, resulting in a lack of trust when applying these methods to new\nsegmentation problems. To address these issues, we propose a comprehensive\nscribble supervision benchmark consisting of seven datasets covering a diverse\nset of anatomies and pathologies imaged with varying modalities. We furthermore\npropose the systematic use of partial losses, i.e. losses that are only\ncomputed on annotated voxels. Contrary to most existing methods, these losses\ncan be seamlessly integrated into state-of-the-art segmentation methods,\nenabling them to learn from scribble annotations while preserving their\noriginal loss formulations. Our evaluation using nnU-Net reveals that while\nmost existing methods suffer from a lack of generalization, the proposed\napproach consistently delivers state-of-the-art performance. Thanks to its\nsimplicity, our approach presents an embarrassingly simple yet effective\nsolution to the challenges of scribble supervision. Source code as well as our\nextensive scribble benchmarking suite will be made publicly available upon\npublication.\n", "link": "http://arxiv.org/abs/2403.12834v1", "date": "2024-03-19", "relevancy": 2.1561, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embarrassingly%20Simple%20Scribble%20Supervision%20for%203D%20Medical%20Segmentation&body=Title%3A%20Embarrassingly%20Simple%20Scribble%20Supervision%20for%203D%20Medical%20Segmentation%0AAuthor%3A%20Karol%20Gotkowski%20and%20Carsten%20L%C3%BCth%20and%20Paul%20F.%20J%C3%A4ger%20and%20Sebastian%20Ziegler%20and%20Lars%20Kr%C3%A4mer%20and%20Stefan%20Denner%20and%20Shuhan%20Xiao%20and%20Nico%20Disch%20and%20Klaus%20H.%20Maier-Hein%20and%20Fabian%20Isensee%0AAbstract%3A%20%20%20Traditionally%2C%20segmentation%20algorithms%20require%20dense%20annotations%20for%0Atraining%2C%20demanding%20significant%20annotation%20efforts%2C%20particularly%20within%20the%203D%0Amedical%20imaging%20field.%20Scribble-supervised%20learning%20emerges%20as%20a%20possible%0Asolution%20to%20this%20challenge%2C%20promising%20a%20reduction%20in%20annotation%20efforts%20when%0Acreating%20large-scale%20datasets.%20Recently%2C%20a%20plethora%20of%20methods%20for%20optimized%0Alearning%20from%20scribbles%20have%20been%20proposed%2C%20but%20have%20so%20far%20failed%20to%20position%0Ascribble%20annotation%20as%20a%20beneficial%20alternative.%20We%20relate%20this%20shortcoming%20to%0Atwo%20major%20issues%3A%201%29%20the%20complex%20nature%20of%20many%20methods%20which%20deeply%20ties%20them%0Ato%20the%20underlying%20segmentation%20model%2C%20thus%20preventing%20a%20migration%20to%20more%0Apowerful%20state-of-the-art%20models%20as%20the%20field%20progresses%20and%202%29%20the%20lack%20of%20a%0Asystematic%20evaluation%20to%20validate%20consistent%20performance%20across%20the%20broader%0Amedical%20domain%2C%20resulting%20in%20a%20lack%20of%20trust%20when%20applying%20these%20methods%20to%20new%0Asegmentation%20problems.%20To%20address%20these%20issues%2C%20we%20propose%20a%20comprehensive%0Ascribble%20supervision%20benchmark%20consisting%20of%20seven%20datasets%20covering%20a%20diverse%0Aset%20of%20anatomies%20and%20pathologies%20imaged%20with%20varying%20modalities.%20We%20furthermore%0Apropose%20the%20systematic%20use%20of%20partial%20losses%2C%20i.e.%20losses%20that%20are%20only%0Acomputed%20on%20annotated%20voxels.%20Contrary%20to%20most%20existing%20methods%2C%20these%20losses%0Acan%20be%20seamlessly%20integrated%20into%20state-of-the-art%20segmentation%20methods%2C%0Aenabling%20them%20to%20learn%20from%20scribble%20annotations%20while%20preserving%20their%0Aoriginal%20loss%20formulations.%20Our%20evaluation%20using%20nnU-Net%20reveals%20that%20while%0Amost%20existing%20methods%20suffer%20from%20a%20lack%20of%20generalization%2C%20the%20proposed%0Aapproach%20consistently%20delivers%20state-of-the-art%20performance.%20Thanks%20to%20its%0Asimplicity%2C%20our%20approach%20presents%20an%20embarrassingly%20simple%20yet%20effective%0Asolution%20to%20the%20challenges%20of%20scribble%20supervision.%20Source%20code%20as%20well%20as%20our%0Aextensive%20scribble%20benchmarking%20suite%20will%20be%20made%20publicly%20available%20upon%0Apublication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12834v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embarrassingly%20Simple%20Scribble%20Supervision%20for%203D%20Medical%20Segmentation&entry.906535625=Karol%20Gotkowski%20and%20Carsten%20L%C3%BCth%20and%20Paul%20F.%20J%C3%A4ger%20and%20Sebastian%20Ziegler%20and%20Lars%20Kr%C3%A4mer%20and%20Stefan%20Denner%20and%20Shuhan%20Xiao%20and%20Nico%20Disch%20and%20Klaus%20H.%20Maier-Hein%20and%20Fabian%20Isensee&entry.1292438233=%20%20Traditionally%2C%20segmentation%20algorithms%20require%20dense%20annotations%20for%0Atraining%2C%20demanding%20significant%20annotation%20efforts%2C%20particularly%20within%20the%203D%0Amedical%20imaging%20field.%20Scribble-supervised%20learning%20emerges%20as%20a%20possible%0Asolution%20to%20this%20challenge%2C%20promising%20a%20reduction%20in%20annotation%20efforts%20when%0Acreating%20large-scale%20datasets.%20Recently%2C%20a%20plethora%20of%20methods%20for%20optimized%0Alearning%20from%20scribbles%20have%20been%20proposed%2C%20but%20have%20so%20far%20failed%20to%20position%0Ascribble%20annotation%20as%20a%20beneficial%20alternative.%20We%20relate%20this%20shortcoming%20to%0Atwo%20major%20issues%3A%201%29%20the%20complex%20nature%20of%20many%20methods%20which%20deeply%20ties%20them%0Ato%20the%20underlying%20segmentation%20model%2C%20thus%20preventing%20a%20migration%20to%20more%0Apowerful%20state-of-the-art%20models%20as%20the%20field%20progresses%20and%202%29%20the%20lack%20of%20a%0Asystematic%20evaluation%20to%20validate%20consistent%20performance%20across%20the%20broader%0Amedical%20domain%2C%20resulting%20in%20a%20lack%20of%20trust%20when%20applying%20these%20methods%20to%20new%0Asegmentation%20problems.%20To%20address%20these%20issues%2C%20we%20propose%20a%20comprehensive%0Ascribble%20supervision%20benchmark%20consisting%20of%20seven%20datasets%20covering%20a%20diverse%0Aset%20of%20anatomies%20and%20pathologies%20imaged%20with%20varying%20modalities.%20We%20furthermore%0Apropose%20the%20systematic%20use%20of%20partial%20losses%2C%20i.e.%20losses%20that%20are%20only%0Acomputed%20on%20annotated%20voxels.%20Contrary%20to%20most%20existing%20methods%2C%20these%20losses%0Acan%20be%20seamlessly%20integrated%20into%20state-of-the-art%20segmentation%20methods%2C%0Aenabling%20them%20to%20learn%20from%20scribble%20annotations%20while%20preserving%20their%0Aoriginal%20loss%20formulations.%20Our%20evaluation%20using%20nnU-Net%20reveals%20that%20while%0Amost%20existing%20methods%20suffer%20from%20a%20lack%20of%20generalization%2C%20the%20proposed%0Aapproach%20consistently%20delivers%20state-of-the-art%20performance.%20Thanks%20to%20its%0Asimplicity%2C%20our%20approach%20presents%20an%20embarrassingly%20simple%20yet%20effective%0Asolution%20to%20the%20challenges%20of%20scribble%20supervision.%20Source%20code%20as%20well%20as%20our%0Aextensive%20scribble%20benchmarking%20suite%20will%20be%20made%20publicly%20available%20upon%0Apublication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12834v1&entry.124074799=Read"},
{"title": "SynCDR : Training Cross Domain Retrieval Models with Synthetic Data", "author": "Samarth Mishra and Carlos D. Castillo and Hongcheng Wang and Kate Saenko and Venkatesh Saligrama", "abstract": "  In cross-domain retrieval, a model is required to identify images from the\nsame semantic category across two visual domains. For instance, given a sketch\nof an object, a model needs to retrieve a real image of it from an online\nstore's catalog. A standard approach for such a problem is learning a feature\nspace of images where Euclidean distances reflect similarity. Even without\nhuman annotations, which may be expensive to acquire, prior methods function\nreasonably well using unlabeled images for training. Our problem constraint\ntakes this further to scenarios where the two domains do not necessarily share\nany common categories in training data. This can occur when the two domains in\nquestion come from different versions of some biometric sensor recording\nidentities of different people. We posit a simple solution, which is to\ngenerate synthetic data to fill in these missing category examples across\ndomains. This, we do via category preserving translation of images from one\nvisual domain to another. We compare approaches specifically trained for this\ntranslation for a pair of domains, as well as those that can use large-scale\npre-trained text-to-image diffusion models via prompts, and find that the\nlatter can generate better replacement synthetic data, leading to more accurate\ncross-domain retrieval models. Our best SynCDR model can outperform prior art\nby up to 15\\%. Code for our work is available at\nhttps://github.com/samarth4149/SynCDR .\n", "link": "http://arxiv.org/abs/2401.00420v2", "date": "2024-03-19", "relevancy": 2.1547, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5484}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SynCDR%20%3A%20Training%20Cross%20Domain%20Retrieval%20Models%20with%20Synthetic%20Data&body=Title%3A%20SynCDR%20%3A%20Training%20Cross%20Domain%20Retrieval%20Models%20with%20Synthetic%20Data%0AAuthor%3A%20Samarth%20Mishra%20and%20Carlos%20D.%20Castillo%20and%20Hongcheng%20Wang%20and%20Kate%20Saenko%20and%20Venkatesh%20Saligrama%0AAbstract%3A%20%20%20In%20cross-domain%20retrieval%2C%20a%20model%20is%20required%20to%20identify%20images%20from%20the%0Asame%20semantic%20category%20across%20two%20visual%20domains.%20For%20instance%2C%20given%20a%20sketch%0Aof%20an%20object%2C%20a%20model%20needs%20to%20retrieve%20a%20real%20image%20of%20it%20from%20an%20online%0Astore%27s%20catalog.%20A%20standard%20approach%20for%20such%20a%20problem%20is%20learning%20a%20feature%0Aspace%20of%20images%20where%20Euclidean%20distances%20reflect%20similarity.%20Even%20without%0Ahuman%20annotations%2C%20which%20may%20be%20expensive%20to%20acquire%2C%20prior%20methods%20function%0Areasonably%20well%20using%20unlabeled%20images%20for%20training.%20Our%20problem%20constraint%0Atakes%20this%20further%20to%20scenarios%20where%20the%20two%20domains%20do%20not%20necessarily%20share%0Aany%20common%20categories%20in%20training%20data.%20This%20can%20occur%20when%20the%20two%20domains%20in%0Aquestion%20come%20from%20different%20versions%20of%20some%20biometric%20sensor%20recording%0Aidentities%20of%20different%20people.%20We%20posit%20a%20simple%20solution%2C%20which%20is%20to%0Agenerate%20synthetic%20data%20to%20fill%20in%20these%20missing%20category%20examples%20across%0Adomains.%20This%2C%20we%20do%20via%20category%20preserving%20translation%20of%20images%20from%20one%0Avisual%20domain%20to%20another.%20We%20compare%20approaches%20specifically%20trained%20for%20this%0Atranslation%20for%20a%20pair%20of%20domains%2C%20as%20well%20as%20those%20that%20can%20use%20large-scale%0Apre-trained%20text-to-image%20diffusion%20models%20via%20prompts%2C%20and%20find%20that%20the%0Alatter%20can%20generate%20better%20replacement%20synthetic%20data%2C%20leading%20to%20more%20accurate%0Across-domain%20retrieval%20models.%20Our%20best%20SynCDR%20model%20can%20outperform%20prior%20art%0Aby%20up%20to%2015%5C%25.%20Code%20for%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/samarth4149/SynCDR%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00420v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynCDR%20%3A%20Training%20Cross%20Domain%20Retrieval%20Models%20with%20Synthetic%20Data&entry.906535625=Samarth%20Mishra%20and%20Carlos%20D.%20Castillo%20and%20Hongcheng%20Wang%20and%20Kate%20Saenko%20and%20Venkatesh%20Saligrama&entry.1292438233=%20%20In%20cross-domain%20retrieval%2C%20a%20model%20is%20required%20to%20identify%20images%20from%20the%0Asame%20semantic%20category%20across%20two%20visual%20domains.%20For%20instance%2C%20given%20a%20sketch%0Aof%20an%20object%2C%20a%20model%20needs%20to%20retrieve%20a%20real%20image%20of%20it%20from%20an%20online%0Astore%27s%20catalog.%20A%20standard%20approach%20for%20such%20a%20problem%20is%20learning%20a%20feature%0Aspace%20of%20images%20where%20Euclidean%20distances%20reflect%20similarity.%20Even%20without%0Ahuman%20annotations%2C%20which%20may%20be%20expensive%20to%20acquire%2C%20prior%20methods%20function%0Areasonably%20well%20using%20unlabeled%20images%20for%20training.%20Our%20problem%20constraint%0Atakes%20this%20further%20to%20scenarios%20where%20the%20two%20domains%20do%20not%20necessarily%20share%0Aany%20common%20categories%20in%20training%20data.%20This%20can%20occur%20when%20the%20two%20domains%20in%0Aquestion%20come%20from%20different%20versions%20of%20some%20biometric%20sensor%20recording%0Aidentities%20of%20different%20people.%20We%20posit%20a%20simple%20solution%2C%20which%20is%20to%0Agenerate%20synthetic%20data%20to%20fill%20in%20these%20missing%20category%20examples%20across%0Adomains.%20This%2C%20we%20do%20via%20category%20preserving%20translation%20of%20images%20from%20one%0Avisual%20domain%20to%20another.%20We%20compare%20approaches%20specifically%20trained%20for%20this%0Atranslation%20for%20a%20pair%20of%20domains%2C%20as%20well%20as%20those%20that%20can%20use%20large-scale%0Apre-trained%20text-to-image%20diffusion%20models%20via%20prompts%2C%20and%20find%20that%20the%0Alatter%20can%20generate%20better%20replacement%20synthetic%20data%2C%20leading%20to%20more%20accurate%0Across-domain%20retrieval%20models.%20Our%20best%20SynCDR%20model%20can%20outperform%20prior%20art%0Aby%20up%20to%2015%5C%25.%20Code%20for%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/samarth4149/SynCDR%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00420v2&entry.124074799=Read"},
{"title": "GVGEN: Text-to-3D Generation with Volumetric Representation", "author": "Xianglong He and Junyi Chen and Sida Peng and Di Huang and Yangguang Li and Xiaoshui Huang and Chun Yuan and Wanli Ouyang and Tong He", "abstract": "  In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed ($\\sim$7\nseconds), effectively striking a balance between quality and efficiency.\n", "link": "http://arxiv.org/abs/2403.12957v1", "date": "2024-03-19", "relevancy": 2.1515, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5727}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5402}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5216}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation&body=Title%3A%20GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation%0AAuthor%3A%20Xianglong%20He%20and%20Junyi%20Chen%20and%20Sida%20Peng%20and%20Di%20Huang%20and%20Yangguang%20Li%20and%20Xiaoshui%20Huang%20and%20Chun%20Yuan%20and%20Wanli%20Ouyang%20and%20Tong%20He%0AAbstract%3A%20%20%20In%20recent%20years%2C%203D%20Gaussian%20splatting%20has%20emerged%20as%20a%20powerful%20technique%0Afor%203D%20reconstruction%20and%20generation%2C%20known%20for%20its%20fast%20and%20high-quality%0Arendering%20capabilities.%20To%20address%20these%20shortcomings%2C%20this%20paper%20introduces%20a%0Anovel%20diffusion-based%20framework%2C%20GVGEN%2C%20designed%20to%20efficiently%20generate%203D%0AGaussian%20representations%20from%20text%20input.%20We%20propose%20two%20innovative%0Atechniques%3A%281%29%20Structured%20Volumetric%20Representation.%20We%20first%20arrange%0Adisorganized%203D%20Gaussian%20points%20as%20a%20structured%20form%20GaussianVolume.%20This%0Atransformation%20allows%20the%20capture%20of%20intricate%20texture%20details%20within%20a%20volume%0Acomposed%20of%20a%20fixed%20number%20of%20Gaussians.%20To%20better%20optimize%20the%20representation%0Aof%20these%20details%2C%20we%20propose%20a%20unique%20pruning%20and%20densifying%20method%20named%20the%0ACandidate%20Pool%20Strategy%2C%20enhancing%20detail%20fidelity%20through%20selective%0Aoptimization.%20%282%29%20Coarse-to-fine%20Generation%20Pipeline.%20To%20simplify%20the%0Ageneration%20of%20GaussianVolume%20and%20empower%20the%20model%20to%20generate%20instances%20with%0Adetailed%203D%20geometry%2C%20we%20propose%20a%20coarse-to-fine%20pipeline.%20It%20initially%0Aconstructs%20a%20basic%20geometric%20structure%2C%20followed%20by%20the%20prediction%20of%20complete%0AGaussian%20attributes.%20Our%20framework%2C%20GVGEN%2C%20demonstrates%20superior%20performance%20in%0Aqualitative%20and%20quantitative%20assessments%20compared%20to%20existing%203D%20generation%0Amethods.%20Simultaneously%2C%20it%20maintains%20a%20fast%20generation%20speed%20%28%24%5Csim%247%0Aseconds%29%2C%20effectively%20striking%20a%20balance%20between%20quality%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12957v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GVGEN%3A%20Text-to-3D%20Generation%20with%20Volumetric%20Representation&entry.906535625=Xianglong%20He%20and%20Junyi%20Chen%20and%20Sida%20Peng%20and%20Di%20Huang%20and%20Yangguang%20Li%20and%20Xiaoshui%20Huang%20and%20Chun%20Yuan%20and%20Wanli%20Ouyang%20and%20Tong%20He&entry.1292438233=%20%20In%20recent%20years%2C%203D%20Gaussian%20splatting%20has%20emerged%20as%20a%20powerful%20technique%0Afor%203D%20reconstruction%20and%20generation%2C%20known%20for%20its%20fast%20and%20high-quality%0Arendering%20capabilities.%20To%20address%20these%20shortcomings%2C%20this%20paper%20introduces%20a%0Anovel%20diffusion-based%20framework%2C%20GVGEN%2C%20designed%20to%20efficiently%20generate%203D%0AGaussian%20representations%20from%20text%20input.%20We%20propose%20two%20innovative%0Atechniques%3A%281%29%20Structured%20Volumetric%20Representation.%20We%20first%20arrange%0Adisorganized%203D%20Gaussian%20points%20as%20a%20structured%20form%20GaussianVolume.%20This%0Atransformation%20allows%20the%20capture%20of%20intricate%20texture%20details%20within%20a%20volume%0Acomposed%20of%20a%20fixed%20number%20of%20Gaussians.%20To%20better%20optimize%20the%20representation%0Aof%20these%20details%2C%20we%20propose%20a%20unique%20pruning%20and%20densifying%20method%20named%20the%0ACandidate%20Pool%20Strategy%2C%20enhancing%20detail%20fidelity%20through%20selective%0Aoptimization.%20%282%29%20Coarse-to-fine%20Generation%20Pipeline.%20To%20simplify%20the%0Ageneration%20of%20GaussianVolume%20and%20empower%20the%20model%20to%20generate%20instances%20with%0Adetailed%203D%20geometry%2C%20we%20propose%20a%20coarse-to-fine%20pipeline.%20It%20initially%0Aconstructs%20a%20basic%20geometric%20structure%2C%20followed%20by%20the%20prediction%20of%20complete%0AGaussian%20attributes.%20Our%20framework%2C%20GVGEN%2C%20demonstrates%20superior%20performance%20in%0Aqualitative%20and%20quantitative%20assessments%20compared%20to%20existing%203D%20generation%0Amethods.%20Simultaneously%2C%20it%20maintains%20a%20fast%20generation%20speed%20%28%24%5Csim%247%0Aseconds%29%2C%20effectively%20striking%20a%20balance%20between%20quality%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12957v1&entry.124074799=Read"},
{"title": "Semantic Layering in Room Segmentation via LLMs", "author": "Taehyeon Kim and Byung-Cheol Min", "abstract": "  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs\n(SeLRoS), an advanced method for semantic room segmentation by integrating\nLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlike\nprevious approaches that solely focus on the geometric segmentation of indoor\nenvironments, our work enriches segmented maps with semantic data, including\nobject identification and spatial relationships, to enhance robotic navigation.\nBy leveraging LLMs, we provide a novel framework that interprets and organizes\ncomplex information about each segmented area, thereby improving the accuracy\nand contextual relevance of room segmentation. Furthermore, SeLRoS overcomes\nthe limitations of existing algorithms by using a semantic evaluation method to\naccurately distinguish true room divisions from those erroneously generated by\nfurniture and segmentation inaccuracies. The effectiveness of SeLRoS is\nverified through its application across 30 different 3D environments. Source\ncode and experiment videos for this work are available at:\nhttps://sites.google.com/view/selros.\n", "link": "http://arxiv.org/abs/2403.12920v1", "date": "2024-03-19", "relevancy": 2.1483, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5305}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic%20Layering%20in%20Room%20Segmentation%20via%20LLMs&body=Title%3A%20Semantic%20Layering%20in%20Room%20Segmentation%20via%20LLMs%0AAuthor%3A%20Taehyeon%20Kim%20and%20Byung-Cheol%20Min%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Semantic%20Layering%20in%20Room%20Segmentation%20via%20LLMs%0A%28SeLRoS%29%2C%20an%20advanced%20method%20for%20semantic%20room%20segmentation%20by%20integrating%0ALarge%20Language%20Models%20%28LLMs%29%20with%20traditional%202D%20map-based%20segmentation.%20Unlike%0Aprevious%20approaches%20that%20solely%20focus%20on%20the%20geometric%20segmentation%20of%20indoor%0Aenvironments%2C%20our%20work%20enriches%20segmented%20maps%20with%20semantic%20data%2C%20including%0Aobject%20identification%20and%20spatial%20relationships%2C%20to%20enhance%20robotic%20navigation.%0ABy%20leveraging%20LLMs%2C%20we%20provide%20a%20novel%20framework%20that%20interprets%20and%20organizes%0Acomplex%20information%20about%20each%20segmented%20area%2C%20thereby%20improving%20the%20accuracy%0Aand%20contextual%20relevance%20of%20room%20segmentation.%20Furthermore%2C%20SeLRoS%20overcomes%0Athe%20limitations%20of%20existing%20algorithms%20by%20using%20a%20semantic%20evaluation%20method%20to%0Aaccurately%20distinguish%20true%20room%20divisions%20from%20those%20erroneously%20generated%20by%0Afurniture%20and%20segmentation%20inaccuracies.%20The%20effectiveness%20of%20SeLRoS%20is%0Averified%20through%20its%20application%20across%2030%20different%203D%20environments.%20Source%0Acode%20and%20experiment%20videos%20for%20this%20work%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/selros.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12920v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Layering%20in%20Room%20Segmentation%20via%20LLMs&entry.906535625=Taehyeon%20Kim%20and%20Byung-Cheol%20Min&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Semantic%20Layering%20in%20Room%20Segmentation%20via%20LLMs%0A%28SeLRoS%29%2C%20an%20advanced%20method%20for%20semantic%20room%20segmentation%20by%20integrating%0ALarge%20Language%20Models%20%28LLMs%29%20with%20traditional%202D%20map-based%20segmentation.%20Unlike%0Aprevious%20approaches%20that%20solely%20focus%20on%20the%20geometric%20segmentation%20of%20indoor%0Aenvironments%2C%20our%20work%20enriches%20segmented%20maps%20with%20semantic%20data%2C%20including%0Aobject%20identification%20and%20spatial%20relationships%2C%20to%20enhance%20robotic%20navigation.%0ABy%20leveraging%20LLMs%2C%20we%20provide%20a%20novel%20framework%20that%20interprets%20and%20organizes%0Acomplex%20information%20about%20each%20segmented%20area%2C%20thereby%20improving%20the%20accuracy%0Aand%20contextual%20relevance%20of%20room%20segmentation.%20Furthermore%2C%20SeLRoS%20overcomes%0Athe%20limitations%20of%20existing%20algorithms%20by%20using%20a%20semantic%20evaluation%20method%20to%0Aaccurately%20distinguish%20true%20room%20divisions%20from%20those%20erroneously%20generated%20by%0Afurniture%20and%20segmentation%20inaccuracies.%20The%20effectiveness%20of%20SeLRoS%20is%0Averified%20through%20its%20application%20across%2030%20different%203D%20environments.%20Source%0Acode%20and%20experiment%20videos%20for%20this%20work%20are%20available%20at%3A%0Ahttps%3A//sites.google.com/view/selros.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12920v1&entry.124074799=Read"},
{"title": "EmoVOCA: Speech-Driven Emotional 3D Talking Heads", "author": "Federico Nocentini and Claudio Ferrari and Stefano Berretti", "abstract": "  The domain of 3D talking head generation has witnessed significant progress\nin recent years. A notable challenge in this field consists in blending\nspeech-related motions with expression dynamics, which is primarily caused by\nthe lack of comprehensive 3D datasets that combine diversity in spoken\nsentences with a variety of facial expressions. Whereas literature works\nattempted to exploit 2D video data and parametric 3D models as a workaround,\nthese still show limitations when jointly modeling the two motions. In this\nwork, we address this problem from a different perspective, and propose an\ninnovative data-driven technique that we used for creating a synthetic dataset,\ncalled EmoVOCA, obtained by combining a collection of inexpressive 3D talking\nheads and a set of 3D expressive sequences. To demonstrate the advantages of\nthis approach, and the quality of the dataset, we then designed and trained an\nemotional 3D talking head generator that accepts a 3D face, an audio file, an\nemotion label, and an intensity value as inputs, and learns to animate the\naudio-synchronized lip movements with expressive traits of the face.\nComprehensive experiments, both quantitative and qualitative, using our data\nand generator evidence superior ability in synthesizing convincing animations,\nwhen compared with the best performing methods in the literature. Our code and\npre-trained model will be made available.\n", "link": "http://arxiv.org/abs/2403.12886v1", "date": "2024-03-19", "relevancy": 2.1461, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5582}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.536}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&body=Title%3A%20EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads%0AAuthor%3A%20Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti%0AAbstract%3A%20%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12886v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoVOCA%3A%20Speech-Driven%20Emotional%203D%20Talking%20Heads&entry.906535625=Federico%20Nocentini%20and%20Claudio%20Ferrari%20and%20Stefano%20Berretti&entry.1292438233=%20%20The%20domain%20of%203D%20talking%20head%20generation%20has%20witnessed%20significant%20progress%0Ain%20recent%20years.%20A%20notable%20challenge%20in%20this%20field%20consists%20in%20blending%0Aspeech-related%20motions%20with%20expression%20dynamics%2C%20which%20is%20primarily%20caused%20by%0Athe%20lack%20of%20comprehensive%203D%20datasets%20that%20combine%20diversity%20in%20spoken%0Asentences%20with%20a%20variety%20of%20facial%20expressions.%20Whereas%20literature%20works%0Aattempted%20to%20exploit%202D%20video%20data%20and%20parametric%203D%20models%20as%20a%20workaround%2C%0Athese%20still%20show%20limitations%20when%20jointly%20modeling%20the%20two%20motions.%20In%20this%0Awork%2C%20we%20address%20this%20problem%20from%20a%20different%20perspective%2C%20and%20propose%20an%0Ainnovative%20data-driven%20technique%20that%20we%20used%20for%20creating%20a%20synthetic%20dataset%2C%0Acalled%20EmoVOCA%2C%20obtained%20by%20combining%20a%20collection%20of%20inexpressive%203D%20talking%0Aheads%20and%20a%20set%20of%203D%20expressive%20sequences.%20To%20demonstrate%20the%20advantages%20of%0Athis%20approach%2C%20and%20the%20quality%20of%20the%20dataset%2C%20we%20then%20designed%20and%20trained%20an%0Aemotional%203D%20talking%20head%20generator%20that%20accepts%20a%203D%20face%2C%20an%20audio%20file%2C%20an%0Aemotion%20label%2C%20and%20an%20intensity%20value%20as%20inputs%2C%20and%20learns%20to%20animate%20the%0Aaudio-synchronized%20lip%20movements%20with%20expressive%20traits%20of%20the%20face.%0AComprehensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20using%20our%20data%0Aand%20generator%20evidence%20superior%20ability%20in%20synthesizing%20convincing%20animations%2C%0Awhen%20compared%20with%20the%20best%20performing%20methods%20in%20the%20literature.%20Our%20code%20and%0Apre-trained%20model%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12886v1&entry.124074799=Read"},
{"title": "Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold\n  Functions with Nasty Noise", "author": "Shiwei Zeng and Jie Shen", "abstract": "  The concept class of low-degree polynomial threshold functions (PTFs) plays a\nfundamental role in machine learning. In this paper, we study PAC learning of\n$K$-sparse degree-$d$ PTFs on $\\mathbb{R}^n$, where any such concept depends\nonly on $K$ out of $n$ attributes of the input. Our main contribution is a new\nalgorithm that runs in time $({nd}/{\\epsilon})^{O(d)}$ and under the Gaussian\nmarginal distribution, PAC learns the class up to error rate $\\epsilon$ with\n$O(\\frac{K^{4d}}{\\epsilon^{2d}} \\cdot \\log^{5d} n)$ samples even when an $\\eta\n\\leq O(\\epsilon^d)$ fraction of them are corrupted by the nasty noise of\nBshouty et al. (2002), possibly the strongest corruption model. Prior to this\nwork, attribute-efficient robust algorithms are established only for the\nspecial case of sparse homogeneous halfspaces. Our key ingredients are: 1) a\nstructural result that translates the attribute sparsity to a sparsity pattern\nof the Chow vector under the basis of Hermite polynomials, and 2) a novel\nattribute-efficient robust Chow vector estimation algorithm which uses\nexclusively a restricted Frobenius norm to either certify a good approximation\nor to validate a sparsity-induced degree-$2d$ polynomial as a filter to detect\ncorrupted samples.\n", "link": "http://arxiv.org/abs/2306.00673v2", "date": "2024-03-19", "relevancy": 2.1345, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4378}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4257}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4172}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attribute-Efficient%20PAC%20Learning%20of%20Low-Degree%20Polynomial%20Threshold%0A%20%20Functions%20with%20Nasty%20Noise&body=Title%3A%20Attribute-Efficient%20PAC%20Learning%20of%20Low-Degree%20Polynomial%20Threshold%0A%20%20Functions%20with%20Nasty%20Noise%0AAuthor%3A%20Shiwei%20Zeng%20and%20Jie%20Shen%0AAbstract%3A%20%20%20The%20concept%20class%20of%20low-degree%20polynomial%20threshold%20functions%20%28PTFs%29%20plays%20a%0Afundamental%20role%20in%20machine%20learning.%20In%20this%20paper%2C%20we%20study%20PAC%20learning%20of%0A%24K%24-sparse%20degree-%24d%24%20PTFs%20on%20%24%5Cmathbb%7BR%7D%5En%24%2C%20where%20any%20such%20concept%20depends%0Aonly%20on%20%24K%24%20out%20of%20%24n%24%20attributes%20of%20the%20input.%20Our%20main%20contribution%20is%20a%20new%0Aalgorithm%20that%20runs%20in%20time%20%24%28%7Bnd%7D/%7B%5Cepsilon%7D%29%5E%7BO%28d%29%7D%24%20and%20under%20the%20Gaussian%0Amarginal%20distribution%2C%20PAC%20learns%20the%20class%20up%20to%20error%20rate%20%24%5Cepsilon%24%20with%0A%24O%28%5Cfrac%7BK%5E%7B4d%7D%7D%7B%5Cepsilon%5E%7B2d%7D%7D%20%5Ccdot%20%5Clog%5E%7B5d%7D%20n%29%24%20samples%20even%20when%20an%20%24%5Ceta%0A%5Cleq%20O%28%5Cepsilon%5Ed%29%24%20fraction%20of%20them%20are%20corrupted%20by%20the%20nasty%20noise%20of%0ABshouty%20et%20al.%20%282002%29%2C%20possibly%20the%20strongest%20corruption%20model.%20Prior%20to%20this%0Awork%2C%20attribute-efficient%20robust%20algorithms%20are%20established%20only%20for%20the%0Aspecial%20case%20of%20sparse%20homogeneous%20halfspaces.%20Our%20key%20ingredients%20are%3A%201%29%20a%0Astructural%20result%20that%20translates%20the%20attribute%20sparsity%20to%20a%20sparsity%20pattern%0Aof%20the%20Chow%20vector%20under%20the%20basis%20of%20Hermite%20polynomials%2C%20and%202%29%20a%20novel%0Aattribute-efficient%20robust%20Chow%20vector%20estimation%20algorithm%20which%20uses%0Aexclusively%20a%20restricted%20Frobenius%20norm%20to%20either%20certify%20a%20good%20approximation%0Aor%20to%20validate%20a%20sparsity-induced%20degree-%242d%24%20polynomial%20as%20a%20filter%20to%20detect%0Acorrupted%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00673v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribute-Efficient%20PAC%20Learning%20of%20Low-Degree%20Polynomial%20Threshold%0A%20%20Functions%20with%20Nasty%20Noise&entry.906535625=Shiwei%20Zeng%20and%20Jie%20Shen&entry.1292438233=%20%20The%20concept%20class%20of%20low-degree%20polynomial%20threshold%20functions%20%28PTFs%29%20plays%20a%0Afundamental%20role%20in%20machine%20learning.%20In%20this%20paper%2C%20we%20study%20PAC%20learning%20of%0A%24K%24-sparse%20degree-%24d%24%20PTFs%20on%20%24%5Cmathbb%7BR%7D%5En%24%2C%20where%20any%20such%20concept%20depends%0Aonly%20on%20%24K%24%20out%20of%20%24n%24%20attributes%20of%20the%20input.%20Our%20main%20contribution%20is%20a%20new%0Aalgorithm%20that%20runs%20in%20time%20%24%28%7Bnd%7D/%7B%5Cepsilon%7D%29%5E%7BO%28d%29%7D%24%20and%20under%20the%20Gaussian%0Amarginal%20distribution%2C%20PAC%20learns%20the%20class%20up%20to%20error%20rate%20%24%5Cepsilon%24%20with%0A%24O%28%5Cfrac%7BK%5E%7B4d%7D%7D%7B%5Cepsilon%5E%7B2d%7D%7D%20%5Ccdot%20%5Clog%5E%7B5d%7D%20n%29%24%20samples%20even%20when%20an%20%24%5Ceta%0A%5Cleq%20O%28%5Cepsilon%5Ed%29%24%20fraction%20of%20them%20are%20corrupted%20by%20the%20nasty%20noise%20of%0ABshouty%20et%20al.%20%282002%29%2C%20possibly%20the%20strongest%20corruption%20model.%20Prior%20to%20this%0Awork%2C%20attribute-efficient%20robust%20algorithms%20are%20established%20only%20for%20the%0Aspecial%20case%20of%20sparse%20homogeneous%20halfspaces.%20Our%20key%20ingredients%20are%3A%201%29%20a%0Astructural%20result%20that%20translates%20the%20attribute%20sparsity%20to%20a%20sparsity%20pattern%0Aof%20the%20Chow%20vector%20under%20the%20basis%20of%20Hermite%20polynomials%2C%20and%202%29%20a%20novel%0Aattribute-efficient%20robust%20Chow%20vector%20estimation%20algorithm%20which%20uses%0Aexclusively%20a%20restricted%20Frobenius%20norm%20to%20either%20certify%20a%20good%20approximation%0Aor%20to%20validate%20a%20sparsity-induced%20degree-%242d%24%20polynomial%20as%20a%20filter%20to%20detect%0Acorrupted%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00673v2&entry.124074799=Read"},
{"title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese\n  Address Entity Recognition Dataset for UAV Delivery", "author": "Yuxuan Yao and Sichun Luo and Haohan Zhao and Guanzhi Deng and Linqi Song", "abstract": "  We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.\n", "link": "http://arxiv.org/abs/2403.06097v2", "date": "2024-03-19", "relevancy": 2.1287, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5642}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5129}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20LLM%20Substitute%20Human%20Labeling%3F%20A%20Case%20Study%20of%20Fine-grained%20Chinese%0A%20%20Address%20Entity%20Recognition%20Dataset%20for%20UAV%20Delivery&body=Title%3A%20Can%20LLM%20Substitute%20Human%20Labeling%3F%20A%20Case%20Study%20of%20Fine-grained%20Chinese%0A%20%20Address%20Entity%20Recognition%20Dataset%20for%20UAV%20Delivery%0AAuthor%3A%20Yuxuan%20Yao%20and%20Sichun%20Luo%20and%20Haohan%20Zhao%20and%20Guanzhi%20Deng%20and%20Linqi%20Song%0AAbstract%3A%20%20%20We%20present%20CNER-UAV%2C%20a%20fine-grained%20%5Ctextbf%7BC%7Dhinese%20%5Ctextbf%7BN%7Dame%0A%5Ctextbf%7BE%7Dntity%20%5Ctextbf%7BR%7Decognition%20dataset%20specifically%20designed%20for%20the%20task%0Aof%20address%20resolution%20in%20%5Ctextbf%7BU%7Dnmanned%20%5Ctextbf%7BA%7Derial%20%5Ctextbf%7BV%7Dehicle%0Adelivery%20systems.%20The%20dataset%20encompasses%20a%20diverse%20range%20of%20five%20categories%2C%0Aenabling%20comprehensive%20training%20and%20evaluation%20of%20NER%20models.%20To%20construct%20this%0Adataset%2C%20we%20sourced%20the%20data%20from%20a%20real-world%20UAV%20delivery%20system%20and%0Aconducted%20a%20rigorous%20data%20cleaning%20and%20desensitization%20process%20to%20ensure%0Aprivacy%20and%20data%20integrity.%20The%20resulting%20dataset%2C%20consisting%20of%20around%2012%2C000%0Aannotated%20samples%2C%20underwent%20human%20experts%20and%20%5Ctextbf%7BL%7Darge%20%5Ctextbf%7BL%7Danguage%0A%5Ctextbf%7BM%7Dodel%20annotation.%20We%20evaluated%20classical%20NER%20models%20on%20our%20dataset%20and%0Aprovided%20in-depth%20analysis.%20The%20dataset%20and%20models%20are%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zhhvvv/CNER-UAV%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06097v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLM%20Substitute%20Human%20Labeling%3F%20A%20Case%20Study%20of%20Fine-grained%20Chinese%0A%20%20Address%20Entity%20Recognition%20Dataset%20for%20UAV%20Delivery&entry.906535625=Yuxuan%20Yao%20and%20Sichun%20Luo%20and%20Haohan%20Zhao%20and%20Guanzhi%20Deng%20and%20Linqi%20Song&entry.1292438233=%20%20We%20present%20CNER-UAV%2C%20a%20fine-grained%20%5Ctextbf%7BC%7Dhinese%20%5Ctextbf%7BN%7Dame%0A%5Ctextbf%7BE%7Dntity%20%5Ctextbf%7BR%7Decognition%20dataset%20specifically%20designed%20for%20the%20task%0Aof%20address%20resolution%20in%20%5Ctextbf%7BU%7Dnmanned%20%5Ctextbf%7BA%7Derial%20%5Ctextbf%7BV%7Dehicle%0Adelivery%20systems.%20The%20dataset%20encompasses%20a%20diverse%20range%20of%20five%20categories%2C%0Aenabling%20comprehensive%20training%20and%20evaluation%20of%20NER%20models.%20To%20construct%20this%0Adataset%2C%20we%20sourced%20the%20data%20from%20a%20real-world%20UAV%20delivery%20system%20and%0Aconducted%20a%20rigorous%20data%20cleaning%20and%20desensitization%20process%20to%20ensure%0Aprivacy%20and%20data%20integrity.%20The%20resulting%20dataset%2C%20consisting%20of%20around%2012%2C000%0Aannotated%20samples%2C%20underwent%20human%20experts%20and%20%5Ctextbf%7BL%7Darge%20%5Ctextbf%7BL%7Danguage%0A%5Ctextbf%7BM%7Dodel%20annotation.%20We%20evaluated%20classical%20NER%20models%20on%20our%20dataset%20and%0Aprovided%20in-depth%20analysis.%20The%20dataset%20and%20models%20are%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zhhvvv/CNER-UAV%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06097v2&entry.124074799=Read"},
{"title": "Global-guided Focal Neural Radiance Field for Large-scale Scene\n  Rendering", "author": "Mingqi Shao and Feng Xiong and Hang Zhang and Shuang Yang and Mu Xu and Wei Bian and Xueqian Wang", "abstract": "  Neural radiance fields~(NeRF) have recently been applied to render\nlarge-scale scenes. However, their limited model capacity typically results in\nblurred rendering results. Existing large-scale NeRFs primarily address this\nlimitation by partitioning the scene into blocks, which are subsequently\nhandled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and\nprocessed independently, lead to inconsistencies in geometry and appearance\nacross the scene. Consequently, the rendering quality fails to exhibit\nsignificant improvement despite the expansion of model capacity. In this work,\nwe present global-guided focal neural radiance field (GF-NeRF) that achieves\nhigh-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a\ntwo-stage (Global and Focal) architecture and a global-guided training\nstrategy. The global stage obtains a continuous representation of the entire\nscene while the focal stage decomposes the scene into multiple blocks and\nfurther processes them with distinct sub-encoders. Leveraging this two-stage\narchitecture, sub-encoders only need fine-tuning based on the global encoder,\nthus reducing training complexity in the focal stage while maintaining\nscene-wide consistency. Spatial information and error information from the\nglobal stage also benefit the sub-encoders to focus on crucial areas and\neffectively capture more details of large-scale scenes. Notably, our approach\ndoes not rely on any prior knowledge about the target scene, attributing\nGF-NeRF adaptable to various large-scale scene types, including street-view and\naerial-view scenes. We demonstrate that our method achieves high-fidelity,\nnatural rendering results on various types of large-scale datasets. Our project\npage: https://shaomq2187.github.io/GF-NeRF/\n", "link": "http://arxiv.org/abs/2403.12839v1", "date": "2024-03-19", "relevancy": 2.1225, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5475}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5448}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5097}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Global-guided%20Focal%20Neural%20Radiance%20Field%20for%20Large-scale%20Scene%0A%20%20Rendering&body=Title%3A%20Global-guided%20Focal%20Neural%20Radiance%20Field%20for%20Large-scale%20Scene%0A%20%20Rendering%0AAuthor%3A%20Mingqi%20Shao%20and%20Feng%20Xiong%20and%20Hang%20Zhang%20and%20Shuang%20Yang%20and%20Mu%20Xu%20and%20Wei%20Bian%20and%20Xueqian%20Wang%0AAbstract%3A%20%20%20Neural%20radiance%20fields~%28NeRF%29%20have%20recently%20been%20applied%20to%20render%0Alarge-scale%20scenes.%20However%2C%20their%20limited%20model%20capacity%20typically%20results%20in%0Ablurred%20rendering%20results.%20Existing%20large-scale%20NeRFs%20primarily%20address%20this%0Alimitation%20by%20partitioning%20the%20scene%20into%20blocks%2C%20which%20are%20subsequently%0Ahandled%20by%20separate%20sub-NeRFs.%20These%20sub-NeRFs%2C%20trained%20from%20scratch%20and%0Aprocessed%20independently%2C%20lead%20to%20inconsistencies%20in%20geometry%20and%20appearance%0Aacross%20the%20scene.%20Consequently%2C%20the%20rendering%20quality%20fails%20to%20exhibit%0Asignificant%20improvement%20despite%20the%20expansion%20of%20model%20capacity.%20In%20this%20work%2C%0Awe%20present%20global-guided%20focal%20neural%20radiance%20field%20%28GF-NeRF%29%20that%20achieves%0Ahigh-fidelity%20rendering%20of%20large-scale%20scenes.%20Our%20proposed%20GF-NeRF%20utilizes%20a%0Atwo-stage%20%28Global%20and%20Focal%29%20architecture%20and%20a%20global-guided%20training%0Astrategy.%20The%20global%20stage%20obtains%20a%20continuous%20representation%20of%20the%20entire%0Ascene%20while%20the%20focal%20stage%20decomposes%20the%20scene%20into%20multiple%20blocks%20and%0Afurther%20processes%20them%20with%20distinct%20sub-encoders.%20Leveraging%20this%20two-stage%0Aarchitecture%2C%20sub-encoders%20only%20need%20fine-tuning%20based%20on%20the%20global%20encoder%2C%0Athus%20reducing%20training%20complexity%20in%20the%20focal%20stage%20while%20maintaining%0Ascene-wide%20consistency.%20Spatial%20information%20and%20error%20information%20from%20the%0Aglobal%20stage%20also%20benefit%20the%20sub-encoders%20to%20focus%20on%20crucial%20areas%20and%0Aeffectively%20capture%20more%20details%20of%20large-scale%20scenes.%20Notably%2C%20our%20approach%0Adoes%20not%20rely%20on%20any%20prior%20knowledge%20about%20the%20target%20scene%2C%20attributing%0AGF-NeRF%20adaptable%20to%20various%20large-scale%20scene%20types%2C%20including%20street-view%20and%0Aaerial-view%20scenes.%20We%20demonstrate%20that%20our%20method%20achieves%20high-fidelity%2C%0Anatural%20rendering%20results%20on%20various%20types%20of%20large-scale%20datasets.%20Our%20project%0Apage%3A%20https%3A//shaomq2187.github.io/GF-NeRF/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12839v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-guided%20Focal%20Neural%20Radiance%20Field%20for%20Large-scale%20Scene%0A%20%20Rendering&entry.906535625=Mingqi%20Shao%20and%20Feng%20Xiong%20and%20Hang%20Zhang%20and%20Shuang%20Yang%20and%20Mu%20Xu%20and%20Wei%20Bian%20and%20Xueqian%20Wang&entry.1292438233=%20%20Neural%20radiance%20fields~%28NeRF%29%20have%20recently%20been%20applied%20to%20render%0Alarge-scale%20scenes.%20However%2C%20their%20limited%20model%20capacity%20typically%20results%20in%0Ablurred%20rendering%20results.%20Existing%20large-scale%20NeRFs%20primarily%20address%20this%0Alimitation%20by%20partitioning%20the%20scene%20into%20blocks%2C%20which%20are%20subsequently%0Ahandled%20by%20separate%20sub-NeRFs.%20These%20sub-NeRFs%2C%20trained%20from%20scratch%20and%0Aprocessed%20independently%2C%20lead%20to%20inconsistencies%20in%20geometry%20and%20appearance%0Aacross%20the%20scene.%20Consequently%2C%20the%20rendering%20quality%20fails%20to%20exhibit%0Asignificant%20improvement%20despite%20the%20expansion%20of%20model%20capacity.%20In%20this%20work%2C%0Awe%20present%20global-guided%20focal%20neural%20radiance%20field%20%28GF-NeRF%29%20that%20achieves%0Ahigh-fidelity%20rendering%20of%20large-scale%20scenes.%20Our%20proposed%20GF-NeRF%20utilizes%20a%0Atwo-stage%20%28Global%20and%20Focal%29%20architecture%20and%20a%20global-guided%20training%0Astrategy.%20The%20global%20stage%20obtains%20a%20continuous%20representation%20of%20the%20entire%0Ascene%20while%20the%20focal%20stage%20decomposes%20the%20scene%20into%20multiple%20blocks%20and%0Afurther%20processes%20them%20with%20distinct%20sub-encoders.%20Leveraging%20this%20two-stage%0Aarchitecture%2C%20sub-encoders%20only%20need%20fine-tuning%20based%20on%20the%20global%20encoder%2C%0Athus%20reducing%20training%20complexity%20in%20the%20focal%20stage%20while%20maintaining%0Ascene-wide%20consistency.%20Spatial%20information%20and%20error%20information%20from%20the%0Aglobal%20stage%20also%20benefit%20the%20sub-encoders%20to%20focus%20on%20crucial%20areas%20and%0Aeffectively%20capture%20more%20details%20of%20large-scale%20scenes.%20Notably%2C%20our%20approach%0Adoes%20not%20rely%20on%20any%20prior%20knowledge%20about%20the%20target%20scene%2C%20attributing%0AGF-NeRF%20adaptable%20to%20various%20large-scale%20scene%20types%2C%20including%20street-view%20and%0Aaerial-view%20scenes.%20We%20demonstrate%20that%20our%20method%20achieves%20high-fidelity%2C%0Anatural%20rendering%20results%20on%20various%20types%20of%20large-scale%20datasets.%20Our%20project%0Apage%3A%20https%3A//shaomq2187.github.io/GF-NeRF/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12839v1&entry.124074799=Read"},
{"title": "BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting", "author": "Lingzhe Zhao and Peng Wang and Peidong Liu", "abstract": "  While neural rendering has demonstrated impressive capabilities in 3D scene\nreconstruction and novel view synthesis, it heavily relies on high-quality\nsharp images and accurate camera poses. Numerous approaches have been proposed\nto train Neural Radiance Fields (NeRF) with motion-blurred images, commonly\nencountered in real-world scenarios such as low-light or long-exposure\nconditions. However, the implicit representation of NeRF struggles to\naccurately recover intricate details from severely motion-blurred images and\ncannot achieve real-time rendering. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction and real-time\nrendering by explicitly optimizing point clouds as Gaussian spheres.\n  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle\nAdjusted Deblur Gaussian Splatting), which leverages explicit Gaussian\nrepresentation and handles severe motion-blurred images with inaccurate camera\nposes to achieve high-quality scene reconstruction. Our method models the\nphysical image formation process of motion-blurred images and jointly learns\nthe parameters of Gaussians while recovering camera motion trajectories during\nexposure time.\n  In our experiments, we demonstrate that BAD-Gaussians not only achieves\nsuperior rendering quality compared to previous state-of-the-art deblur neural\nrendering methods on both synthetic and real datasets but also enables\nreal-time rendering capabilities.\n  Our project page and source code is available at\nhttps://lingzhezhao.github.io/BAD-Gaussians/\n", "link": "http://arxiv.org/abs/2403.11831v2", "date": "2024-03-19", "relevancy": 2.117, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5228}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5097}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BAD-Gaussians%3A%20Bundle%20Adjusted%20Deblur%20Gaussian%20Splatting&body=Title%3A%20BAD-Gaussians%3A%20Bundle%20Adjusted%20Deblur%20Gaussian%20Splatting%0AAuthor%3A%20Lingzhe%20Zhao%20and%20Peng%20Wang%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20While%20neural%20rendering%20has%20demonstrated%20impressive%20capabilities%20in%203D%20scene%0Areconstruction%20and%20novel%20view%20synthesis%2C%20it%20heavily%20relies%20on%20high-quality%0Asharp%20images%20and%20accurate%20camera%20poses.%20Numerous%20approaches%20have%20been%20proposed%0Ato%20train%20Neural%20Radiance%20Fields%20%28NeRF%29%20with%20motion-blurred%20images%2C%20commonly%0Aencountered%20in%20real-world%20scenarios%20such%20as%20low-light%20or%20long-exposure%0Aconditions.%20However%2C%20the%20implicit%20representation%20of%20NeRF%20struggles%20to%0Aaccurately%20recover%20intricate%20details%20from%20severely%20motion-blurred%20images%20and%0Acannot%20achieve%20real-time%20rendering.%20In%20contrast%2C%20recent%20advancements%20in%203D%0AGaussian%20Splatting%20achieve%20high-quality%203D%20scene%20reconstruction%20and%20real-time%0Arendering%20by%20explicitly%20optimizing%20point%20clouds%20as%20Gaussian%20spheres.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20named%20BAD-Gaussians%20%28Bundle%0AAdjusted%20Deblur%20Gaussian%20Splatting%29%2C%20which%20leverages%20explicit%20Gaussian%0Arepresentation%20and%20handles%20severe%20motion-blurred%20images%20with%20inaccurate%20camera%0Aposes%20to%20achieve%20high-quality%20scene%20reconstruction.%20Our%20method%20models%20the%0Aphysical%20image%20formation%20process%20of%20motion-blurred%20images%20and%20jointly%20learns%0Athe%20parameters%20of%20Gaussians%20while%20recovering%20camera%20motion%20trajectories%20during%0Aexposure%20time.%0A%20%20In%20our%20experiments%2C%20we%20demonstrate%20that%20BAD-Gaussians%20not%20only%20achieves%0Asuperior%20rendering%20quality%20compared%20to%20previous%20state-of-the-art%20deblur%20neural%0Arendering%20methods%20on%20both%20synthetic%20and%20real%20datasets%20but%20also%20enables%0Areal-time%20rendering%20capabilities.%0A%20%20Our%20project%20page%20and%20source%20code%20is%20available%20at%0Ahttps%3A//lingzhezhao.github.io/BAD-Gaussians/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11831v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAD-Gaussians%3A%20Bundle%20Adjusted%20Deblur%20Gaussian%20Splatting&entry.906535625=Lingzhe%20Zhao%20and%20Peng%20Wang%20and%20Peidong%20Liu&entry.1292438233=%20%20While%20neural%20rendering%20has%20demonstrated%20impressive%20capabilities%20in%203D%20scene%0Areconstruction%20and%20novel%20view%20synthesis%2C%20it%20heavily%20relies%20on%20high-quality%0Asharp%20images%20and%20accurate%20camera%20poses.%20Numerous%20approaches%20have%20been%20proposed%0Ato%20train%20Neural%20Radiance%20Fields%20%28NeRF%29%20with%20motion-blurred%20images%2C%20commonly%0Aencountered%20in%20real-world%20scenarios%20such%20as%20low-light%20or%20long-exposure%0Aconditions.%20However%2C%20the%20implicit%20representation%20of%20NeRF%20struggles%20to%0Aaccurately%20recover%20intricate%20details%20from%20severely%20motion-blurred%20images%20and%0Acannot%20achieve%20real-time%20rendering.%20In%20contrast%2C%20recent%20advancements%20in%203D%0AGaussian%20Splatting%20achieve%20high-quality%203D%20scene%20reconstruction%20and%20real-time%0Arendering%20by%20explicitly%20optimizing%20point%20clouds%20as%20Gaussian%20spheres.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20named%20BAD-Gaussians%20%28Bundle%0AAdjusted%20Deblur%20Gaussian%20Splatting%29%2C%20which%20leverages%20explicit%20Gaussian%0Arepresentation%20and%20handles%20severe%20motion-blurred%20images%20with%20inaccurate%20camera%0Aposes%20to%20achieve%20high-quality%20scene%20reconstruction.%20Our%20method%20models%20the%0Aphysical%20image%20formation%20process%20of%20motion-blurred%20images%20and%20jointly%20learns%0Athe%20parameters%20of%20Gaussians%20while%20recovering%20camera%20motion%20trajectories%20during%0Aexposure%20time.%0A%20%20In%20our%20experiments%2C%20we%20demonstrate%20that%20BAD-Gaussians%20not%20only%20achieves%0Asuperior%20rendering%20quality%20compared%20to%20previous%20state-of-the-art%20deblur%20neural%0Arendering%20methods%20on%20both%20synthetic%20and%20real%20datasets%20but%20also%20enables%0Areal-time%20rendering%20capabilities.%0A%20%20Our%20project%20page%20and%20source%20code%20is%20available%20at%0Ahttps%3A//lingzhezhao.github.io/BAD-Gaussians/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11831v2&entry.124074799=Read"},
{"title": "ECAMP: Entity-centered Context-aware Medical Vision Language\n  Pre-training", "author": "Rongsheng Wang and Qingsong Yao and Haoran Lai and Zhiyang He and Xiaodong Tao and Zihang Jiang and S. Kevin Zhou", "abstract": "  Despite significant advancements in medical vision-language pre-training,\nexisting methods have largely overlooked the inherent entity-specific context\nwithin radiology reports and the complex cross-modality contextual\nrelationships between text and images. To close this gap, we propose a novel\nEntity-centered Context-aware Medical Vision-language Pre-training (ECAMP)\nframework, which is designed to enable a more entity-centered and\ncontext-sensitive interpretation of medical data. Utilizing the recent powerful\nlarge language model, we distill entity-centered context from medical reports,\nwhich enables ECAMP to gain more effective supervision from the text modality.\nBy further pre-training our model with carefully designed entity-aware,\ncontext-enhanced masked language modeling and context-guided super-resolution\ntasks, ECAMP significantly refines the interplay between text and image\nmodalities, leading to an enhanced ability to extract entity-centered\ncontextual features. Besides, our proposed multi-scale context fusion design\nalso improves the semantic integration of both coarse and fine-level image\nrepresentations, prompting better performance for multi-scale downstream\napplications. Combining these components leads to significant performance leaps\nover current state-of-the-art methods and establishes a new standard for\ncross-modality learning in medical imaging, whose effectiveness is demonstrated\nby our extensive experiments on various tasks including classification,\nsegmentation, and detection across several public datasets. Code and models are\navailable at https://github.com/ToniChopp/ECAMP.\n", "link": "http://arxiv.org/abs/2312.13316v3", "date": "2024-03-19", "relevancy": 2.1163, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4964}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training&body=Title%3A%20ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training%0AAuthor%3A%20Rongsheng%20Wang%20and%20Qingsong%20Yao%20and%20Haoran%20Lai%20and%20Zhiyang%20He%20and%20Xiaodong%20Tao%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20medical%20vision-language%20pre-training%2C%0Aexisting%20methods%20have%20largely%20overlooked%20the%20inherent%20entity-specific%20context%0Awithin%20radiology%20reports%20and%20the%20complex%20cross-modality%20contextual%0Arelationships%20between%20text%20and%20images.%20To%20close%20this%20gap%2C%20we%20propose%20a%20novel%0AEntity-centered%20Context-aware%20Medical%20Vision-language%20Pre-training%20%28ECAMP%29%0Aframework%2C%20which%20is%20designed%20to%20enable%20a%20more%20entity-centered%20and%0Acontext-sensitive%20interpretation%20of%20medical%20data.%20Utilizing%20the%20recent%20powerful%0Alarge%20language%20model%2C%20we%20distill%20entity-centered%20context%20from%20medical%20reports%2C%0Awhich%20enables%20ECAMP%20to%20gain%20more%20effective%20supervision%20from%20the%20text%20modality.%0ABy%20further%20pre-training%20our%20model%20with%20carefully%20designed%20entity-aware%2C%0Acontext-enhanced%20masked%20language%20modeling%20and%20context-guided%20super-resolution%0Atasks%2C%20ECAMP%20significantly%20refines%20the%20interplay%20between%20text%20and%20image%0Amodalities%2C%20leading%20to%20an%20enhanced%20ability%20to%20extract%20entity-centered%0Acontextual%20features.%20Besides%2C%20our%20proposed%20multi-scale%20context%20fusion%20design%0Aalso%20improves%20the%20semantic%20integration%20of%20both%20coarse%20and%20fine-level%20image%0Arepresentations%2C%20prompting%20better%20performance%20for%20multi-scale%20downstream%0Aapplications.%20Combining%20these%20components%20leads%20to%20significant%20performance%20leaps%0Aover%20current%20state-of-the-art%20methods%20and%20establishes%20a%20new%20standard%20for%0Across-modality%20learning%20in%20medical%20imaging%2C%20whose%20effectiveness%20is%20demonstrated%0Aby%20our%20extensive%20experiments%20on%20various%20tasks%20including%20classification%2C%0Asegmentation%2C%20and%20detection%20across%20several%20public%20datasets.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/ToniChopp/ECAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13316v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training&entry.906535625=Rongsheng%20Wang%20and%20Qingsong%20Yao%20and%20Haoran%20Lai%20and%20Zhiyang%20He%20and%20Xiaodong%20Tao%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20medical%20vision-language%20pre-training%2C%0Aexisting%20methods%20have%20largely%20overlooked%20the%20inherent%20entity-specific%20context%0Awithin%20radiology%20reports%20and%20the%20complex%20cross-modality%20contextual%0Arelationships%20between%20text%20and%20images.%20To%20close%20this%20gap%2C%20we%20propose%20a%20novel%0AEntity-centered%20Context-aware%20Medical%20Vision-language%20Pre-training%20%28ECAMP%29%0Aframework%2C%20which%20is%20designed%20to%20enable%20a%20more%20entity-centered%20and%0Acontext-sensitive%20interpretation%20of%20medical%20data.%20Utilizing%20the%20recent%20powerful%0Alarge%20language%20model%2C%20we%20distill%20entity-centered%20context%20from%20medical%20reports%2C%0Awhich%20enables%20ECAMP%20to%20gain%20more%20effective%20supervision%20from%20the%20text%20modality.%0ABy%20further%20pre-training%20our%20model%20with%20carefully%20designed%20entity-aware%2C%0Acontext-enhanced%20masked%20language%20modeling%20and%20context-guided%20super-resolution%0Atasks%2C%20ECAMP%20significantly%20refines%20the%20interplay%20between%20text%20and%20image%0Amodalities%2C%20leading%20to%20an%20enhanced%20ability%20to%20extract%20entity-centered%0Acontextual%20features.%20Besides%2C%20our%20proposed%20multi-scale%20context%20fusion%20design%0Aalso%20improves%20the%20semantic%20integration%20of%20both%20coarse%20and%20fine-level%20image%0Arepresentations%2C%20prompting%20better%20performance%20for%20multi-scale%20downstream%0Aapplications.%20Combining%20these%20components%20leads%20to%20significant%20performance%20leaps%0Aover%20current%20state-of-the-art%20methods%20and%20establishes%20a%20new%20standard%20for%0Across-modality%20learning%20in%20medical%20imaging%2C%20whose%20effectiveness%20is%20demonstrated%0Aby%20our%20extensive%20experiments%20on%20various%20tasks%20including%20classification%2C%0Asegmentation%2C%20and%20detection%20across%20several%20public%20datasets.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/ToniChopp/ECAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13316v3&entry.124074799=Read"},
{"title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs", "author": "M. Jehanzeb Mirza and Leonid Karlinsky and Wei Lin and Sivan Doveh and Jakub Micorek and Mateusz Kozinski and Hilde Kuhene and Horst Possegger", "abstract": "  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n", "link": "http://arxiv.org/abs/2403.11755v2", "date": "2024-03-19", "relevancy": 2.1026, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5258}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5019}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Meta-Prompting%20for%20Automating%20Zero-shot%20Visual%20Recognition%20with%20LLMs&body=Title%3A%20Meta-Prompting%20for%20Automating%20Zero-shot%20Visual%20Recognition%20with%20LLMs%0AAuthor%3A%20M.%20Jehanzeb%20Mirza%20and%20Leonid%20Karlinsky%20and%20Wei%20Lin%20and%20Sivan%20Doveh%20and%20Jakub%20Micorek%20and%20Mateusz%20Kozinski%20and%20Hilde%20Kuhene%20and%20Horst%20Possegger%0AAbstract%3A%20%20%20Prompt%20ensembling%20of%20Large%20Language%20Model%20%28LLM%29%20generated%20category-specific%0Aprompts%20has%20emerged%20as%20an%20effective%20method%20to%20enhance%20zero-shot%20recognition%0Aability%20of%20Vision-Language%20Models%20%28VLMs%29.%20To%20obtain%20these%20category-specific%0Aprompts%2C%20the%20present%20methods%20rely%20on%20hand-crafting%20the%20prompts%20to%20the%20LLMs%20for%0Agenerating%20VLM%20prompts%20for%20the%20downstream%20tasks.%20However%2C%20this%20requires%0Amanually%20composing%20these%20task-specific%20prompts%20and%20still%2C%20they%20might%20not%20cover%0Athe%20diverse%20set%20of%20visual%20concepts%20and%20task-specific%20styles%20associated%20with%20the%0Acategories%20of%20interest.%20To%20effectively%20take%20humans%20out%20of%20the%20loop%20and%0Acompletely%20automate%20the%20prompt%20generation%20process%20for%20zero-shot%20recognition%2C%20we%0Apropose%20Meta-Prompting%20for%20Visual%20Recognition%20%28MPVR%29.%20Taking%20as%20input%20only%0Aminimal%20information%20about%20the%20target%20task%2C%20in%20the%20form%20of%20its%20short%20natural%0Alanguage%20description%2C%20and%20a%20list%20of%20associated%20class%20labels%2C%20MPVR%20automatically%0Aproduces%20a%20diverse%20set%20of%20category-specific%20prompts%20resulting%20in%20a%20strong%0Azero-shot%20classifier.%20MPVR%20generalizes%20effectively%20across%20various%20popular%0Azero-shot%20image%20recognition%20benchmarks%20belonging%20to%20widely%20different%20domains%0Awhen%20tested%20with%20multiple%20LLMs%20and%20VLMs.%20For%20example%2C%20MPVR%20obtains%20a%20zero-shot%0Arecognition%20improvement%20over%20CLIP%20by%20up%20to%2019.8%25%20and%2018.2%25%20%285.0%25%20and%204.5%25%20on%0Aaverage%20over%2020%20datasets%29%20leveraging%20GPT%20and%20Mixtral%20LLMs%2C%20respectively%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11755v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Prompting%20for%20Automating%20Zero-shot%20Visual%20Recognition%20with%20LLMs&entry.906535625=M.%20Jehanzeb%20Mirza%20and%20Leonid%20Karlinsky%20and%20Wei%20Lin%20and%20Sivan%20Doveh%20and%20Jakub%20Micorek%20and%20Mateusz%20Kozinski%20and%20Hilde%20Kuhene%20and%20Horst%20Possegger&entry.1292438233=%20%20Prompt%20ensembling%20of%20Large%20Language%20Model%20%28LLM%29%20generated%20category-specific%0Aprompts%20has%20emerged%20as%20an%20effective%20method%20to%20enhance%20zero-shot%20recognition%0Aability%20of%20Vision-Language%20Models%20%28VLMs%29.%20To%20obtain%20these%20category-specific%0Aprompts%2C%20the%20present%20methods%20rely%20on%20hand-crafting%20the%20prompts%20to%20the%20LLMs%20for%0Agenerating%20VLM%20prompts%20for%20the%20downstream%20tasks.%20However%2C%20this%20requires%0Amanually%20composing%20these%20task-specific%20prompts%20and%20still%2C%20they%20might%20not%20cover%0Athe%20diverse%20set%20of%20visual%20concepts%20and%20task-specific%20styles%20associated%20with%20the%0Acategories%20of%20interest.%20To%20effectively%20take%20humans%20out%20of%20the%20loop%20and%0Acompletely%20automate%20the%20prompt%20generation%20process%20for%20zero-shot%20recognition%2C%20we%0Apropose%20Meta-Prompting%20for%20Visual%20Recognition%20%28MPVR%29.%20Taking%20as%20input%20only%0Aminimal%20information%20about%20the%20target%20task%2C%20in%20the%20form%20of%20its%20short%20natural%0Alanguage%20description%2C%20and%20a%20list%20of%20associated%20class%20labels%2C%20MPVR%20automatically%0Aproduces%20a%20diverse%20set%20of%20category-specific%20prompts%20resulting%20in%20a%20strong%0Azero-shot%20classifier.%20MPVR%20generalizes%20effectively%20across%20various%20popular%0Azero-shot%20image%20recognition%20benchmarks%20belonging%20to%20widely%20different%20domains%0Awhen%20tested%20with%20multiple%20LLMs%20and%20VLMs.%20For%20example%2C%20MPVR%20obtains%20a%20zero-shot%0Arecognition%20improvement%20over%20CLIP%20by%20up%20to%2019.8%25%20and%2018.2%25%20%285.0%25%20and%204.5%25%20on%0Aaverage%20over%2020%20datasets%29%20leveraging%20GPT%20and%20Mixtral%20LLMs%2C%20respectively%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11755v2&entry.124074799=Read"},
{"title": "VisualCritic: Making LMMs Perceive Visual Quality Like Humans", "author": "Zhipeng Huang and Zhizheng Zhang and Yiting Lu and Zheng-Jun Zha and Zhibo Chen and Baining Guo", "abstract": "  At present, large multimodal models (LMMs) have exhibited impressive\ngeneralization capabilities in understanding and generating visual signals.\nHowever, they currently still lack sufficient capability to perceive low-level\nvisual quality akin to human perception. Can LMMs achieve this and show the\nsame degree of generalization in this regard? If so, not only could the\nversatility of LMMs be further enhanced, but also the challenge of poor\ncross-dataset performance in the field of visual quality assessment could be\naddressed. In this paper, we explore this question and provide the answer\n\"Yes!\". As the result of this initial exploration, we present VisualCritic, the\nfirst LMM for broad-spectrum image subjective quality assessment. VisualCritic\ncan be used across diverse data right out of box, without any requirements of\ndataset-specific adaptation operations like conventional specialist models. As\nan instruction-following LMM, VisualCritic enables new capabilities of (1)\nquantitatively measuring the perceptual quality of given images in terms of\ntheir Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other\nnumerical indicators, (2) qualitatively evaluating visual quality and providing\nexplainable descriptions, (3) discerning whether a given image is AI-generated\nor photographic. Extensive experiments demonstrate the efficacy of VisualCritic\nby comparing it with other open-source LMMs and conventional specialist models\nover both AI-generated and photographic images.\n", "link": "http://arxiv.org/abs/2403.12806v1", "date": "2024-03-19", "relevancy": 2.0954, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5217}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5112}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisualCritic%3A%20Making%20LMMs%20Perceive%20Visual%20Quality%20Like%20Humans&body=Title%3A%20VisualCritic%3A%20Making%20LMMs%20Perceive%20Visual%20Quality%20Like%20Humans%0AAuthor%3A%20Zhipeng%20Huang%20and%20Zhizheng%20Zhang%20and%20Yiting%20Lu%20and%20Zheng-Jun%20Zha%20and%20Zhibo%20Chen%20and%20Baining%20Guo%0AAbstract%3A%20%20%20At%20present%2C%20large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20impressive%0Ageneralization%20capabilities%20in%20understanding%20and%20generating%20visual%20signals.%0AHowever%2C%20they%20currently%20still%20lack%20sufficient%20capability%20to%20perceive%20low-level%0Avisual%20quality%20akin%20to%20human%20perception.%20Can%20LMMs%20achieve%20this%20and%20show%20the%0Asame%20degree%20of%20generalization%20in%20this%20regard%3F%20If%20so%2C%20not%20only%20could%20the%0Aversatility%20of%20LMMs%20be%20further%20enhanced%2C%20but%20also%20the%20challenge%20of%20poor%0Across-dataset%20performance%20in%20the%20field%20of%20visual%20quality%20assessment%20could%20be%0Aaddressed.%20In%20this%20paper%2C%20we%20explore%20this%20question%20and%20provide%20the%20answer%0A%22Yes%21%22.%20As%20the%20result%20of%20this%20initial%20exploration%2C%20we%20present%20VisualCritic%2C%20the%0Afirst%20LMM%20for%20broad-spectrum%20image%20subjective%20quality%20assessment.%20VisualCritic%0Acan%20be%20used%20across%20diverse%20data%20right%20out%20of%20box%2C%20without%20any%20requirements%20of%0Adataset-specific%20adaptation%20operations%20like%20conventional%20specialist%20models.%20As%0Aan%20instruction-following%20LMM%2C%20VisualCritic%20enables%20new%20capabilities%20of%20%281%29%0Aquantitatively%20measuring%20the%20perceptual%20quality%20of%20given%20images%20in%20terms%20of%0Atheir%20Mean%20Opinion%20Score%20%28MOS%29%2C%20noisiness%2C%20colorfulness%2C%20sharpness%2C%20and%20other%0Anumerical%20indicators%2C%20%282%29%20qualitatively%20evaluating%20visual%20quality%20and%20providing%0Aexplainable%20descriptions%2C%20%283%29%20discerning%20whether%20a%20given%20image%20is%20AI-generated%0Aor%20photographic.%20Extensive%20experiments%20demonstrate%20the%20efficacy%20of%20VisualCritic%0Aby%20comparing%20it%20with%20other%20open-source%20LMMs%20and%20conventional%20specialist%20models%0Aover%20both%20AI-generated%20and%20photographic%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12806v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualCritic%3A%20Making%20LMMs%20Perceive%20Visual%20Quality%20Like%20Humans&entry.906535625=Zhipeng%20Huang%20and%20Zhizheng%20Zhang%20and%20Yiting%20Lu%20and%20Zheng-Jun%20Zha%20and%20Zhibo%20Chen%20and%20Baining%20Guo&entry.1292438233=%20%20At%20present%2C%20large%20multimodal%20models%20%28LMMs%29%20have%20exhibited%20impressive%0Ageneralization%20capabilities%20in%20understanding%20and%20generating%20visual%20signals.%0AHowever%2C%20they%20currently%20still%20lack%20sufficient%20capability%20to%20perceive%20low-level%0Avisual%20quality%20akin%20to%20human%20perception.%20Can%20LMMs%20achieve%20this%20and%20show%20the%0Asame%20degree%20of%20generalization%20in%20this%20regard%3F%20If%20so%2C%20not%20only%20could%20the%0Aversatility%20of%20LMMs%20be%20further%20enhanced%2C%20but%20also%20the%20challenge%20of%20poor%0Across-dataset%20performance%20in%20the%20field%20of%20visual%20quality%20assessment%20could%20be%0Aaddressed.%20In%20this%20paper%2C%20we%20explore%20this%20question%20and%20provide%20the%20answer%0A%22Yes%21%22.%20As%20the%20result%20of%20this%20initial%20exploration%2C%20we%20present%20VisualCritic%2C%20the%0Afirst%20LMM%20for%20broad-spectrum%20image%20subjective%20quality%20assessment.%20VisualCritic%0Acan%20be%20used%20across%20diverse%20data%20right%20out%20of%20box%2C%20without%20any%20requirements%20of%0Adataset-specific%20adaptation%20operations%20like%20conventional%20specialist%20models.%20As%0Aan%20instruction-following%20LMM%2C%20VisualCritic%20enables%20new%20capabilities%20of%20%281%29%0Aquantitatively%20measuring%20the%20perceptual%20quality%20of%20given%20images%20in%20terms%20of%0Atheir%20Mean%20Opinion%20Score%20%28MOS%29%2C%20noisiness%2C%20colorfulness%2C%20sharpness%2C%20and%20other%0Anumerical%20indicators%2C%20%282%29%20qualitatively%20evaluating%20visual%20quality%20and%20providing%0Aexplainable%20descriptions%2C%20%283%29%20discerning%20whether%20a%20given%20image%20is%20AI-generated%0Aor%20photographic.%20Extensive%20experiments%20demonstrate%20the%20efficacy%20of%20VisualCritic%0Aby%20comparing%20it%20with%20other%20open-source%20LMMs%20and%20conventional%20specialist%20models%0Aover%20both%20AI-generated%20and%20photographic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12806v1&entry.124074799=Read"},
{"title": "PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for\n  Autonomous Flight in Complex and Dynamic Environments", "author": "Jiaxin Qiu and Qingchen Liu and Jiahu Qin and Dewang Cheng and Yawei Tian and Qichao Ma", "abstract": "  The role of a motion planner is pivotal in quadrotor applications, yet\nexisting methods often struggle to adapt to complex environments, limiting\ntheir ability to achieve fast, safe, and robust flight. In this letter, we\nintroduce a performance-enhanced quadrotor motion planner designed for\nautonomous flight in complex environments including dense obstacles, dynamic\nobstacles, and unknown disturbances. The global planner generates an initial\ntrajectory through kinodynamic path searching and refines it using B-spline\ntrajectory optimization. Subsequently, the local planner takes into account the\nquadrotor dynamics, estimated disturbance, global reference trajectory, control\ncost, time cost, and safety constraints to generate real-time control inputs,\nutilizing the framework of model predictive contouring control. Both\nsimulations and real-world experiments corroborate the heightened robustness,\nsafety, and speed of the proposed motion planner. Additionally, our motion\nplanner achieves flights at more than 6.8 m/s in a challenging and complex\nracing scenario.\n", "link": "http://arxiv.org/abs/2403.12865v1", "date": "2024-03-19", "relevancy": 2.0894, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PE-Planner%3A%20A%20Performance-Enhanced%20Quadrotor%20Motion%20Planner%20for%0A%20%20Autonomous%20Flight%20in%20Complex%20and%20Dynamic%20Environments&body=Title%3A%20PE-Planner%3A%20A%20Performance-Enhanced%20Quadrotor%20Motion%20Planner%20for%0A%20%20Autonomous%20Flight%20in%20Complex%20and%20Dynamic%20Environments%0AAuthor%3A%20Jiaxin%20Qiu%20and%20Qingchen%20Liu%20and%20Jiahu%20Qin%20and%20Dewang%20Cheng%20and%20Yawei%20Tian%20and%20Qichao%20Ma%0AAbstract%3A%20%20%20The%20role%20of%20a%20motion%20planner%20is%20pivotal%20in%20quadrotor%20applications%2C%20yet%0Aexisting%20methods%20often%20struggle%20to%20adapt%20to%20complex%20environments%2C%20limiting%0Atheir%20ability%20to%20achieve%20fast%2C%20safe%2C%20and%20robust%20flight.%20In%20this%20letter%2C%20we%0Aintroduce%20a%20performance-enhanced%20quadrotor%20motion%20planner%20designed%20for%0Aautonomous%20flight%20in%20complex%20environments%20including%20dense%20obstacles%2C%20dynamic%0Aobstacles%2C%20and%20unknown%20disturbances.%20The%20global%20planner%20generates%20an%20initial%0Atrajectory%20through%20kinodynamic%20path%20searching%20and%20refines%20it%20using%20B-spline%0Atrajectory%20optimization.%20Subsequently%2C%20the%20local%20planner%20takes%20into%20account%20the%0Aquadrotor%20dynamics%2C%20estimated%20disturbance%2C%20global%20reference%20trajectory%2C%20control%0Acost%2C%20time%20cost%2C%20and%20safety%20constraints%20to%20generate%20real-time%20control%20inputs%2C%0Autilizing%20the%20framework%20of%20model%20predictive%20contouring%20control.%20Both%0Asimulations%20and%20real-world%20experiments%20corroborate%20the%20heightened%20robustness%2C%0Asafety%2C%20and%20speed%20of%20the%20proposed%20motion%20planner.%20Additionally%2C%20our%20motion%0Aplanner%20achieves%20flights%20at%20more%20than%206.8%20m/s%20in%20a%20challenging%20and%20complex%0Aracing%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12865v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PE-Planner%3A%20A%20Performance-Enhanced%20Quadrotor%20Motion%20Planner%20for%0A%20%20Autonomous%20Flight%20in%20Complex%20and%20Dynamic%20Environments&entry.906535625=Jiaxin%20Qiu%20and%20Qingchen%20Liu%20and%20Jiahu%20Qin%20and%20Dewang%20Cheng%20and%20Yawei%20Tian%20and%20Qichao%20Ma&entry.1292438233=%20%20The%20role%20of%20a%20motion%20planner%20is%20pivotal%20in%20quadrotor%20applications%2C%20yet%0Aexisting%20methods%20often%20struggle%20to%20adapt%20to%20complex%20environments%2C%20limiting%0Atheir%20ability%20to%20achieve%20fast%2C%20safe%2C%20and%20robust%20flight.%20In%20this%20letter%2C%20we%0Aintroduce%20a%20performance-enhanced%20quadrotor%20motion%20planner%20designed%20for%0Aautonomous%20flight%20in%20complex%20environments%20including%20dense%20obstacles%2C%20dynamic%0Aobstacles%2C%20and%20unknown%20disturbances.%20The%20global%20planner%20generates%20an%20initial%0Atrajectory%20through%20kinodynamic%20path%20searching%20and%20refines%20it%20using%20B-spline%0Atrajectory%20optimization.%20Subsequently%2C%20the%20local%20planner%20takes%20into%20account%20the%0Aquadrotor%20dynamics%2C%20estimated%20disturbance%2C%20global%20reference%20trajectory%2C%20control%0Acost%2C%20time%20cost%2C%20and%20safety%20constraints%20to%20generate%20real-time%20control%20inputs%2C%0Autilizing%20the%20framework%20of%20model%20predictive%20contouring%20control.%20Both%0Asimulations%20and%20real-world%20experiments%20corroborate%20the%20heightened%20robustness%2C%0Asafety%2C%20and%20speed%20of%20the%20proposed%20motion%20planner.%20Additionally%2C%20our%20motion%0Aplanner%20achieves%20flights%20at%20more%20than%206.8%20m/s%20in%20a%20challenging%20and%20complex%0Aracing%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12865v1&entry.124074799=Read"},
{"title": "RelationVLM: Making Large Vision-Language Models Understand Visual\n  Relations", "author": "Zhipeng Huang and Zhizheng Zhang and Zheng-Jun Zha and Yan Lu and Baining Guo", "abstract": "  The development of Large Vision-Language Models (LVLMs) is striving to catch\nup with the success of Large Language Models (LLMs), yet it faces more\nchallenges to be resolved. Very recent works enable LVLMs to localize\nobject-level visual contents and ground text to them. Nonetheless, current\nLVLMs still struggle to precisely understand visual relations due to the lack\nof relevant data. In this work, we present RelationVLM, a large vision-language\nmodel capable of comprehending various levels and types of relations whether\nacross multiple images or within a video. Specifically, we devise a multi-stage\nrelation-aware training scheme and a series of corresponding data configuration\nstrategies to bestow RelationVLM with the capabilities of understanding\nsemantic relations, temporal associations and geometric transforms. Extensive\ncase studies and quantitative evaluations show RelationVLM has strong\ncapability in understanding such relations and emerges impressive in-context\ncapability of reasoning from few-shot examples by comparison. This work fosters\nthe advancements of LVLMs by enabling them to support a wider range of\ndownstream applications toward artificial general intelligence.\n", "link": "http://arxiv.org/abs/2403.12801v1", "date": "2024-03-19", "relevancy": 2.0863, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5471}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RelationVLM%3A%20Making%20Large%20Vision-Language%20Models%20Understand%20Visual%0A%20%20Relations&body=Title%3A%20RelationVLM%3A%20Making%20Large%20Vision-Language%20Models%20Understand%20Visual%0A%20%20Relations%0AAuthor%3A%20Zhipeng%20Huang%20and%20Zhizheng%20Zhang%20and%20Zheng-Jun%20Zha%20and%20Yan%20Lu%20and%20Baining%20Guo%0AAbstract%3A%20%20%20The%20development%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20is%20striving%20to%20catch%0Aup%20with%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20it%20faces%20more%0Achallenges%20to%20be%20resolved.%20Very%20recent%20works%20enable%20LVLMs%20to%20localize%0Aobject-level%20visual%20contents%20and%20ground%20text%20to%20them.%20Nonetheless%2C%20current%0ALVLMs%20still%20struggle%20to%20precisely%20understand%20visual%20relations%20due%20to%20the%20lack%0Aof%20relevant%20data.%20In%20this%20work%2C%20we%20present%20RelationVLM%2C%20a%20large%20vision-language%0Amodel%20capable%20of%20comprehending%20various%20levels%20and%20types%20of%20relations%20whether%0Aacross%20multiple%20images%20or%20within%20a%20video.%20Specifically%2C%20we%20devise%20a%20multi-stage%0Arelation-aware%20training%20scheme%20and%20a%20series%20of%20corresponding%20data%20configuration%0Astrategies%20to%20bestow%20RelationVLM%20with%20the%20capabilities%20of%20understanding%0Asemantic%20relations%2C%20temporal%20associations%20and%20geometric%20transforms.%20Extensive%0Acase%20studies%20and%20quantitative%20evaluations%20show%20RelationVLM%20has%20strong%0Acapability%20in%20understanding%20such%20relations%20and%20emerges%20impressive%20in-context%0Acapability%20of%20reasoning%20from%20few-shot%20examples%20by%20comparison.%20This%20work%20fosters%0Athe%20advancements%20of%20LVLMs%20by%20enabling%20them%20to%20support%20a%20wider%20range%20of%0Adownstream%20applications%20toward%20artificial%20general%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12801v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RelationVLM%3A%20Making%20Large%20Vision-Language%20Models%20Understand%20Visual%0A%20%20Relations&entry.906535625=Zhipeng%20Huang%20and%20Zhizheng%20Zhang%20and%20Zheng-Jun%20Zha%20and%20Yan%20Lu%20and%20Baining%20Guo&entry.1292438233=%20%20The%20development%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%20is%20striving%20to%20catch%0Aup%20with%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20yet%20it%20faces%20more%0Achallenges%20to%20be%20resolved.%20Very%20recent%20works%20enable%20LVLMs%20to%20localize%0Aobject-level%20visual%20contents%20and%20ground%20text%20to%20them.%20Nonetheless%2C%20current%0ALVLMs%20still%20struggle%20to%20precisely%20understand%20visual%20relations%20due%20to%20the%20lack%0Aof%20relevant%20data.%20In%20this%20work%2C%20we%20present%20RelationVLM%2C%20a%20large%20vision-language%0Amodel%20capable%20of%20comprehending%20various%20levels%20and%20types%20of%20relations%20whether%0Aacross%20multiple%20images%20or%20within%20a%20video.%20Specifically%2C%20we%20devise%20a%20multi-stage%0Arelation-aware%20training%20scheme%20and%20a%20series%20of%20corresponding%20data%20configuration%0Astrategies%20to%20bestow%20RelationVLM%20with%20the%20capabilities%20of%20understanding%0Asemantic%20relations%2C%20temporal%20associations%20and%20geometric%20transforms.%20Extensive%0Acase%20studies%20and%20quantitative%20evaluations%20show%20RelationVLM%20has%20strong%0Acapability%20in%20understanding%20such%20relations%20and%20emerges%20impressive%20in-context%0Acapability%20of%20reasoning%20from%20few-shot%20examples%20by%20comparison.%20This%20work%20fosters%0Athe%20advancements%20of%20LVLMs%20by%20enabling%20them%20to%20support%20a%20wider%20range%20of%0Adownstream%20applications%20toward%20artificial%20general%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12801v1&entry.124074799=Read"},
{"title": "Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object\n  Detector", "author": "Yuqian Fu and Yu Wang and Yixuan Pan and Lian Huai and Xingyu Qiu and Zeyu Shangguan and Tong Liu and Yanwei Fu and Luc Van Gool and Xingqun Jiang", "abstract": "  This paper studies the challenging cross-domain few-shot object detection\n(CD-FSOD), aiming to develop an accurate object detector for novel domains with\nminimal labeled examples. While transformer-based open-set detectors, such as\nDE-ViT, show promise in traditional few-shot object detection, their\ngeneralization to CD-FSOD remains unclear: 1) can such open-set detection\nmethods easily generalize to CD-FSOD? 2) If not, how can models be enhanced\nwhen facing huge domain gaps? To answer the first question, we employ measures\nincluding style, inter-class variance (ICV), and indefinable boundaries (IB) to\nunderstand the domain gap. Based on these measures, we establish a new\nbenchmark named CD-FSOD to evaluate object detection methods, revealing that\nmost of the current approaches fail to generalize across domains. Technically,\nwe observe that the performance decline is associated with our proposed\nmeasures: style, ICV, and IB. Consequently, we propose several novel modules to\naddress these issues. First, the learnable instance features align initial\nfixed instances with target categories, enhancing feature distinctiveness.\nSecond, the instance reweighting module assigns higher importance to\nhigh-quality instances with slight IB. Third, the domain prompter encourages\nfeatures resilient to different styles by synthesizing imaginary domains\nwithout altering semantic contents. These techniques collectively contribute to\nthe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),\nsignificantly improving upon the base DE-ViT. Experimental results validate the\nefficacy of our model. All datasets, codes, and models will be released to the\ncommunity.\n", "link": "http://arxiv.org/abs/2402.03094v2", "date": "2024-03-19", "relevancy": 2.0836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5297}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5135}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector&body=Title%3A%20Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector%0AAuthor%3A%20Yuqian%20Fu%20and%20Yu%20Wang%20and%20Yixuan%20Pan%20and%20Lian%20Huai%20and%20Xingyu%20Qiu%20and%20Zeyu%20Shangguan%20and%20Tong%20Liu%20and%20Yanwei%20Fu%20and%20Luc%20Van%20Gool%20and%20Xingqun%20Jiang%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20challenging%20cross-domain%20few-shot%20object%20detection%0A%28CD-FSOD%29%2C%20aiming%20to%20develop%20an%20accurate%20object%20detector%20for%20novel%20domains%20with%0Aminimal%20labeled%20examples.%20While%20transformer-based%20open-set%20detectors%2C%20such%20as%0ADE-ViT%2C%20show%20promise%20in%20traditional%20few-shot%20object%20detection%2C%20their%0Ageneralization%20to%20CD-FSOD%20remains%20unclear%3A%201%29%20can%20such%20open-set%20detection%0Amethods%20easily%20generalize%20to%20CD-FSOD%3F%202%29%20If%20not%2C%20how%20can%20models%20be%20enhanced%0Awhen%20facing%20huge%20domain%20gaps%3F%20To%20answer%20the%20first%20question%2C%20we%20employ%20measures%0Aincluding%20style%2C%20inter-class%20variance%20%28ICV%29%2C%20and%20indefinable%20boundaries%20%28IB%29%20to%0Aunderstand%20the%20domain%20gap.%20Based%20on%20these%20measures%2C%20we%20establish%20a%20new%0Abenchmark%20named%20CD-FSOD%20to%20evaluate%20object%20detection%20methods%2C%20revealing%20that%0Amost%20of%20the%20current%20approaches%20fail%20to%20generalize%20across%20domains.%20Technically%2C%0Awe%20observe%20that%20the%20performance%20decline%20is%20associated%20with%20our%20proposed%0Ameasures%3A%20style%2C%20ICV%2C%20and%20IB.%20Consequently%2C%20we%20propose%20several%20novel%20modules%20to%0Aaddress%20these%20issues.%20First%2C%20the%20learnable%20instance%20features%20align%20initial%0Afixed%20instances%20with%20target%20categories%2C%20enhancing%20feature%20distinctiveness.%0ASecond%2C%20the%20instance%20reweighting%20module%20assigns%20higher%20importance%20to%0Ahigh-quality%20instances%20with%20slight%20IB.%20Third%2C%20the%20domain%20prompter%20encourages%0Afeatures%20resilient%20to%20different%20styles%20by%20synthesizing%20imaginary%20domains%0Awithout%20altering%20semantic%20contents.%20These%20techniques%20collectively%20contribute%20to%0Athe%20development%20of%20the%20Cross-Domain%20Vision%20Transformer%20for%20CD-FSOD%20%28CD-ViTO%29%2C%0Asignificantly%20improving%20upon%20the%20base%20DE-ViT.%20Experimental%20results%20validate%20the%0Aefficacy%20of%20our%20model.%20All%20datasets%2C%20codes%2C%20and%20models%20will%20be%20released%20to%20the%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03094v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Few-Shot%20Object%20Detection%20via%20Enhanced%20Open-Set%20Object%0A%20%20Detector&entry.906535625=Yuqian%20Fu%20and%20Yu%20Wang%20and%20Yixuan%20Pan%20and%20Lian%20Huai%20and%20Xingyu%20Qiu%20and%20Zeyu%20Shangguan%20and%20Tong%20Liu%20and%20Yanwei%20Fu%20and%20Luc%20Van%20Gool%20and%20Xingqun%20Jiang&entry.1292438233=%20%20This%20paper%20studies%20the%20challenging%20cross-domain%20few-shot%20object%20detection%0A%28CD-FSOD%29%2C%20aiming%20to%20develop%20an%20accurate%20object%20detector%20for%20novel%20domains%20with%0Aminimal%20labeled%20examples.%20While%20transformer-based%20open-set%20detectors%2C%20such%20as%0ADE-ViT%2C%20show%20promise%20in%20traditional%20few-shot%20object%20detection%2C%20their%0Ageneralization%20to%20CD-FSOD%20remains%20unclear%3A%201%29%20can%20such%20open-set%20detection%0Amethods%20easily%20generalize%20to%20CD-FSOD%3F%202%29%20If%20not%2C%20how%20can%20models%20be%20enhanced%0Awhen%20facing%20huge%20domain%20gaps%3F%20To%20answer%20the%20first%20question%2C%20we%20employ%20measures%0Aincluding%20style%2C%20inter-class%20variance%20%28ICV%29%2C%20and%20indefinable%20boundaries%20%28IB%29%20to%0Aunderstand%20the%20domain%20gap.%20Based%20on%20these%20measures%2C%20we%20establish%20a%20new%0Abenchmark%20named%20CD-FSOD%20to%20evaluate%20object%20detection%20methods%2C%20revealing%20that%0Amost%20of%20the%20current%20approaches%20fail%20to%20generalize%20across%20domains.%20Technically%2C%0Awe%20observe%20that%20the%20performance%20decline%20is%20associated%20with%20our%20proposed%0Ameasures%3A%20style%2C%20ICV%2C%20and%20IB.%20Consequently%2C%20we%20propose%20several%20novel%20modules%20to%0Aaddress%20these%20issues.%20First%2C%20the%20learnable%20instance%20features%20align%20initial%0Afixed%20instances%20with%20target%20categories%2C%20enhancing%20feature%20distinctiveness.%0ASecond%2C%20the%20instance%20reweighting%20module%20assigns%20higher%20importance%20to%0Ahigh-quality%20instances%20with%20slight%20IB.%20Third%2C%20the%20domain%20prompter%20encourages%0Afeatures%20resilient%20to%20different%20styles%20by%20synthesizing%20imaginary%20domains%0Awithout%20altering%20semantic%20contents.%20These%20techniques%20collectively%20contribute%20to%0Athe%20development%20of%20the%20Cross-Domain%20Vision%20Transformer%20for%20CD-FSOD%20%28CD-ViTO%29%2C%0Asignificantly%20improving%20upon%20the%20base%20DE-ViT.%20Experimental%20results%20validate%20the%0Aefficacy%20of%20our%20model.%20All%20datasets%2C%20codes%2C%20and%20models%20will%20be%20released%20to%20the%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03094v2&entry.124074799=Read"},
{"title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware\n  Graph Transformer", "author": "Dongyeong Hwang and Hyunju Kim and Sunwoo Kim and Kijung Shin", "abstract": "  The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.\n", "link": "http://arxiv.org/abs/2403.12821v1", "date": "2024-03-19", "relevancy": 2.0797, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6211}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5145}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer&body=Title%3A%20FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer%0AAuthor%3A%20Dongyeong%20Hwang%20and%20Hyunju%20Kim%20and%20Sunwoo%20Kim%20and%20Kijung%20Shin%0AAbstract%3A%20%20%20The%20success%20of%20a%20specific%20neural%20network%20architecture%20is%20closely%20tied%20to%20the%0Adataset%20and%20task%20it%20tackles%3B%20there%20is%20no%20one-size-fits-all%20solution.%20Thus%2C%0Aconsiderable%20efforts%20have%20been%20made%20to%20quickly%20and%20accurately%20estimate%20the%0Aperformances%20of%20neural%20architectures%2C%20without%20full%20training%20or%20evaluation%2C%20for%0Agiven%20tasks%20and%20datasets.%20Neural%20architecture%20encoding%20has%20played%20a%20crucial%0Arole%20in%20the%20estimation%2C%20and%20graphbased%20methods%2C%20which%20treat%20an%20architecture%20as%0Aa%20graph%2C%20have%20shown%20prominent%20performance.%20For%20enhanced%20representation%20learning%0Aof%20neural%20architectures%2C%20we%20introduce%20FlowerFormer%2C%20a%20powerful%20graph%0Atransformer%20that%20incorporates%20the%20information%20flows%20within%20a%20neural%0Aarchitecture.%20FlowerFormer%20consists%20of%20two%20key%20components%3A%20%28a%29%20bidirectional%0Aasynchronous%20message%20passing%2C%20inspired%20by%20the%20flows%3B%20%28b%29%20global%20attention%20built%0Aon%20flow-based%20masking.%20Our%20extensive%20experiments%20demonstrate%20the%20superiority%20of%0AFlowerFormer%20over%20existing%20neural%20encoding%20methods%2C%20and%20its%20effectiveness%0Aextends%20beyond%20computer%20vision%20models%20to%20include%20graph%20neural%20networks%20and%20auto%0Aspeech%20recognition%20models.%20Our%20code%20is%20available%20at%0Ahttp%3A//github.com/y0ngjaenius/CVPR2024_FLOWERFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12821v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowerFormer%3A%20Empowering%20Neural%20Architecture%20Encoding%20using%20a%20Flow-aware%0A%20%20Graph%20Transformer&entry.906535625=Dongyeong%20Hwang%20and%20Hyunju%20Kim%20and%20Sunwoo%20Kim%20and%20Kijung%20Shin&entry.1292438233=%20%20The%20success%20of%20a%20specific%20neural%20network%20architecture%20is%20closely%20tied%20to%20the%0Adataset%20and%20task%20it%20tackles%3B%20there%20is%20no%20one-size-fits-all%20solution.%20Thus%2C%0Aconsiderable%20efforts%20have%20been%20made%20to%20quickly%20and%20accurately%20estimate%20the%0Aperformances%20of%20neural%20architectures%2C%20without%20full%20training%20or%20evaluation%2C%20for%0Agiven%20tasks%20and%20datasets.%20Neural%20architecture%20encoding%20has%20played%20a%20crucial%0Arole%20in%20the%20estimation%2C%20and%20graphbased%20methods%2C%20which%20treat%20an%20architecture%20as%0Aa%20graph%2C%20have%20shown%20prominent%20performance.%20For%20enhanced%20representation%20learning%0Aof%20neural%20architectures%2C%20we%20introduce%20FlowerFormer%2C%20a%20powerful%20graph%0Atransformer%20that%20incorporates%20the%20information%20flows%20within%20a%20neural%0Aarchitecture.%20FlowerFormer%20consists%20of%20two%20key%20components%3A%20%28a%29%20bidirectional%0Aasynchronous%20message%20passing%2C%20inspired%20by%20the%20flows%3B%20%28b%29%20global%20attention%20built%0Aon%20flow-based%20masking.%20Our%20extensive%20experiments%20demonstrate%20the%20superiority%20of%0AFlowerFormer%20over%20existing%20neural%20encoding%20methods%2C%20and%20its%20effectiveness%0Aextends%20beyond%20computer%20vision%20models%20to%20include%20graph%20neural%20networks%20and%20auto%0Aspeech%20recognition%20models.%20Our%20code%20is%20available%20at%0Ahttp%3A//github.com/y0ngjaenius/CVPR2024_FLOWERFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12821v1&entry.124074799=Read"},
{"title": "Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis", "author": "Angelica I. Aviles-Rivero and Chun-Wun Cheng and Zhongying Deng and Zoe Kourtzi and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Early detection of Alzheimer's disease's precursor stages is imperative for\nsignificantly enhancing patient outcomes and quality of life. This challenge is\ntackled through a semi-supervised multi-modal diagnosis framework. In\nparticular, we introduce a new hypergraph framework that enables higher-order\nrelations between multi-modal data, while utilising minimal labels. We first\nintroduce a bilevel hypergraph optimisation framework that jointly learns a\ngraph augmentation policy and a semi-supervised classifier. This dual learning\nstrategy is hypothesised to enhance the robustness and generalisation\ncapabilities of the model by fostering new pathways for information\npropagation. Secondly, we introduce a novel strategy for generating\npseudo-labels more effectively via a gradient-driven flow. Our experimental\nresults demonstrate the superior performance of our framework over current\ntechniques in diagnosing Alzheimer's disease.\n", "link": "http://arxiv.org/abs/2403.12719v1", "date": "2024-03-19", "relevancy": 2.0746, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5104}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bilevel%20Hypergraph%20Networks%20for%20Multi-Modal%20Alzheimer%27s%20Diagnosis&body=Title%3A%20Bilevel%20Hypergraph%20Networks%20for%20Multi-Modal%20Alzheimer%27s%20Diagnosis%0AAuthor%3A%20Angelica%20I.%20Aviles-Rivero%20and%20Chun-Wun%20Cheng%20and%20Zhongying%20Deng%20and%20Zoe%20Kourtzi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Early%20detection%20of%20Alzheimer%27s%20disease%27s%20precursor%20stages%20is%20imperative%20for%0Asignificantly%20enhancing%20patient%20outcomes%20and%20quality%20of%20life.%20This%20challenge%20is%0Atackled%20through%20a%20semi-supervised%20multi-modal%20diagnosis%20framework.%20In%0Aparticular%2C%20we%20introduce%20a%20new%20hypergraph%20framework%20that%20enables%20higher-order%0Arelations%20between%20multi-modal%20data%2C%20while%20utilising%20minimal%20labels.%20We%20first%0Aintroduce%20a%20bilevel%20hypergraph%20optimisation%20framework%20that%20jointly%20learns%20a%0Agraph%20augmentation%20policy%20and%20a%20semi-supervised%20classifier.%20This%20dual%20learning%0Astrategy%20is%20hypothesised%20to%20enhance%20the%20robustness%20and%20generalisation%0Acapabilities%20of%20the%20model%20by%20fostering%20new%20pathways%20for%20information%0Apropagation.%20Secondly%2C%20we%20introduce%20a%20novel%20strategy%20for%20generating%0Apseudo-labels%20more%20effectively%20via%20a%20gradient-driven%20flow.%20Our%20experimental%0Aresults%20demonstrate%20the%20superior%20performance%20of%20our%20framework%20over%20current%0Atechniques%20in%20diagnosing%20Alzheimer%27s%20disease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12719v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilevel%20Hypergraph%20Networks%20for%20Multi-Modal%20Alzheimer%27s%20Diagnosis&entry.906535625=Angelica%20I.%20Aviles-Rivero%20and%20Chun-Wun%20Cheng%20and%20Zhongying%20Deng%20and%20Zoe%20Kourtzi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Early%20detection%20of%20Alzheimer%27s%20disease%27s%20precursor%20stages%20is%20imperative%20for%0Asignificantly%20enhancing%20patient%20outcomes%20and%20quality%20of%20life.%20This%20challenge%20is%0Atackled%20through%20a%20semi-supervised%20multi-modal%20diagnosis%20framework.%20In%0Aparticular%2C%20we%20introduce%20a%20new%20hypergraph%20framework%20that%20enables%20higher-order%0Arelations%20between%20multi-modal%20data%2C%20while%20utilising%20minimal%20labels.%20We%20first%0Aintroduce%20a%20bilevel%20hypergraph%20optimisation%20framework%20that%20jointly%20learns%20a%0Agraph%20augmentation%20policy%20and%20a%20semi-supervised%20classifier.%20This%20dual%20learning%0Astrategy%20is%20hypothesised%20to%20enhance%20the%20robustness%20and%20generalisation%0Acapabilities%20of%20the%20model%20by%20fostering%20new%20pathways%20for%20information%0Apropagation.%20Secondly%2C%20we%20introduce%20a%20novel%20strategy%20for%20generating%0Apseudo-labels%20more%20effectively%20via%20a%20gradient-driven%20flow.%20Our%20experimental%0Aresults%20demonstrate%20the%20superior%20performance%20of%20our%20framework%20over%20current%0Atechniques%20in%20diagnosing%20Alzheimer%27s%20disease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12719v1&entry.124074799=Read"},
{"title": "DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback", "author": "Yangyi Chen and Karan Sikka and Michael Cogswell and Heng Ji and Ajay Divakaran", "abstract": "  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n", "link": "http://arxiv.org/abs/2311.10081v2", "date": "2024-03-19", "relevancy": 2.0721, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DRESS%3A%20Instructing%20Large%20Vision-Language%20Models%20to%20Align%20and%20Interact%0A%20%20with%20Humans%20via%20Natural%20Language%20Feedback&body=Title%3A%20DRESS%3A%20Instructing%20Large%20Vision-Language%20Models%20to%20Align%20and%20Interact%0A%20%20with%20Humans%20via%20Natural%20Language%20Feedback%0AAuthor%3A%20Yangyi%20Chen%20and%20Karan%20Sikka%20and%20Michael%20Cogswell%20and%20Heng%20Ji%20and%20Ajay%20Divakaran%0AAbstract%3A%20%20%20We%20present%20DRESS%2C%20a%20large%20vision%20language%20model%20%28LVLM%29%20that%20innovatively%0Aexploits%20Natural%20Language%20feedback%20%28NLF%29%20from%20Large%20Language%20Models%20to%20enhance%0Aits%20alignment%20and%20interactions%20by%20addressing%20two%20key%20limitations%20in%20the%0Astate-of-the-art%20LVLMs.%20First%2C%20prior%20LVLMs%20generally%20rely%20only%20on%20the%0Ainstruction%20finetuning%20stage%20to%20enhance%20alignment%20with%20human%20preferences.%0AWithout%20incorporating%20extra%20feedback%2C%20they%20are%20still%20prone%20to%20generate%0Aunhelpful%2C%20hallucinated%2C%20or%20harmful%20responses.%20Second%2C%20while%20the%20visual%0Ainstruction%20tuning%20data%20is%20generally%20structured%20in%20a%20multi-turn%20dialogue%0Aformat%2C%20the%20connections%20and%20dependencies%20among%20consecutive%20conversational%20turns%0Aare%20weak.%20This%20reduces%20the%20capacity%20for%20effective%20multi-turn%20interactions.%20To%0Atackle%20these%2C%20we%20propose%20a%20novel%20categorization%20of%20the%20NLF%20into%20two%20key%20types%3A%0Acritique%20and%20refinement.%20The%20critique%20NLF%20identifies%20the%20strengths%20and%0Aweaknesses%20of%20the%20responses%20and%20is%20used%20to%20align%20the%20LVLMs%20with%20human%0Apreferences.%20The%20refinement%20NLF%20offers%20concrete%20suggestions%20for%20improvement%20and%0Ais%20adopted%20to%20improve%20the%20interaction%20ability%20of%20the%20LVLMs--%20which%20focuses%20on%0ALVLMs%27%20ability%20to%20refine%20responses%20by%20incorporating%20feedback%20in%20multi-turn%0Ainteractions.%20To%20address%20the%20non-differentiable%20nature%20of%20NLF%2C%20we%20generalize%0Aconditional%20reinforcement%20learning%20for%20training.%20Our%20experimental%20results%0Ademonstrate%20that%20DRESS%20can%20generate%20more%20helpful%20%289.76%25%29%2C%20honest%20%2811.52%25%29%2C%20and%0Aharmless%20%2821.03%25%29%20responses%2C%20and%20more%20effectively%20learn%20from%20feedback%20during%0Amulti-turn%20interactions%20compared%20to%20SOTA%20LVMLs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10081v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRESS%3A%20Instructing%20Large%20Vision-Language%20Models%20to%20Align%20and%20Interact%0A%20%20with%20Humans%20via%20Natural%20Language%20Feedback&entry.906535625=Yangyi%20Chen%20and%20Karan%20Sikka%20and%20Michael%20Cogswell%20and%20Heng%20Ji%20and%20Ajay%20Divakaran&entry.1292438233=%20%20We%20present%20DRESS%2C%20a%20large%20vision%20language%20model%20%28LVLM%29%20that%20innovatively%0Aexploits%20Natural%20Language%20feedback%20%28NLF%29%20from%20Large%20Language%20Models%20to%20enhance%0Aits%20alignment%20and%20interactions%20by%20addressing%20two%20key%20limitations%20in%20the%0Astate-of-the-art%20LVLMs.%20First%2C%20prior%20LVLMs%20generally%20rely%20only%20on%20the%0Ainstruction%20finetuning%20stage%20to%20enhance%20alignment%20with%20human%20preferences.%0AWithout%20incorporating%20extra%20feedback%2C%20they%20are%20still%20prone%20to%20generate%0Aunhelpful%2C%20hallucinated%2C%20or%20harmful%20responses.%20Second%2C%20while%20the%20visual%0Ainstruction%20tuning%20data%20is%20generally%20structured%20in%20a%20multi-turn%20dialogue%0Aformat%2C%20the%20connections%20and%20dependencies%20among%20consecutive%20conversational%20turns%0Aare%20weak.%20This%20reduces%20the%20capacity%20for%20effective%20multi-turn%20interactions.%20To%0Atackle%20these%2C%20we%20propose%20a%20novel%20categorization%20of%20the%20NLF%20into%20two%20key%20types%3A%0Acritique%20and%20refinement.%20The%20critique%20NLF%20identifies%20the%20strengths%20and%0Aweaknesses%20of%20the%20responses%20and%20is%20used%20to%20align%20the%20LVLMs%20with%20human%0Apreferences.%20The%20refinement%20NLF%20offers%20concrete%20suggestions%20for%20improvement%20and%0Ais%20adopted%20to%20improve%20the%20interaction%20ability%20of%20the%20LVLMs--%20which%20focuses%20on%0ALVLMs%27%20ability%20to%20refine%20responses%20by%20incorporating%20feedback%20in%20multi-turn%0Ainteractions.%20To%20address%20the%20non-differentiable%20nature%20of%20NLF%2C%20we%20generalize%0Aconditional%20reinforcement%20learning%20for%20training.%20Our%20experimental%20results%0Ademonstrate%20that%20DRESS%20can%20generate%20more%20helpful%20%289.76%25%29%2C%20honest%20%2811.52%25%29%2C%20and%0Aharmless%20%2821.03%25%29%20responses%2C%20and%20more%20effectively%20learn%20from%20feedback%20during%0Amulti-turn%20interactions%20compared%20to%20SOTA%20LVMLs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10081v2&entry.124074799=Read"},
{"title": "Resolution- and Stimulus-agnostic Super-Resolution of Ultra-High-Field\n  Functional MRI: Application to Visual Studies", "author": "Hongwei Bran Li and Matthew S. Rosen and Shahin Nasr and Juan Eugenio Iglesias", "abstract": "  High-resolution fMRI provides a window into the brain's mesoscale\norganization. Yet, higher spatial resolution increases scan times, to\ncompensate for the low signal and contrast-to-noise ratio. This work introduces\na deep learning-based 3D super-resolution (SR) method for fMRI. By\nincorporating a resolution-agnostic image augmentation framework, our method\nadapts to varying voxel sizes without retraining. We apply this innovative\ntechnique to localize fine-scale motion-selective sites in the early visual\nareas. Detection of these sites typically requires a resolution higher than 1\nmm isotropic, whereas here, we visualize them based on lower resolution (2-3mm\nisotropic) fMRI data. Remarkably, the super-resolved fMRI is able to recover\nhigh-frequency detail of the interdigitated organization of these sites\n(relative to the color-selective sites), even with training data sourced from\ndifferent subjects and experimental paradigms -- including non-visual\nresting-state fMRI, underscoring its robustness and versatility. Quantitative\nand qualitative results indicate that our method has the potential to enhance\nthe spatial resolution of fMRI, leading to a drastic reduction in acquisition\ntime.\n", "link": "http://arxiv.org/abs/2311.14918v2", "date": "2024-03-19", "relevancy": 2.0704, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5335}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5064}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5061}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Resolution-%20and%20Stimulus-agnostic%20Super-Resolution%20of%20Ultra-High-Field%0A%20%20Functional%20MRI%3A%20Application%20to%20Visual%20Studies&body=Title%3A%20Resolution-%20and%20Stimulus-agnostic%20Super-Resolution%20of%20Ultra-High-Field%0A%20%20Functional%20MRI%3A%20Application%20to%20Visual%20Studies%0AAuthor%3A%20Hongwei%20Bran%20Li%20and%20Matthew%20S.%20Rosen%20and%20Shahin%20Nasr%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20High-resolution%20fMRI%20provides%20a%20window%20into%20the%20brain%27s%20mesoscale%0Aorganization.%20Yet%2C%20higher%20spatial%20resolution%20increases%20scan%20times%2C%20to%0Acompensate%20for%20the%20low%20signal%20and%20contrast-to-noise%20ratio.%20This%20work%20introduces%0Aa%20deep%20learning-based%203D%20super-resolution%20%28SR%29%20method%20for%20fMRI.%20By%0Aincorporating%20a%20resolution-agnostic%20image%20augmentation%20framework%2C%20our%20method%0Aadapts%20to%20varying%20voxel%20sizes%20without%20retraining.%20We%20apply%20this%20innovative%0Atechnique%20to%20localize%20fine-scale%20motion-selective%20sites%20in%20the%20early%20visual%0Aareas.%20Detection%20of%20these%20sites%20typically%20requires%20a%20resolution%20higher%20than%201%0Amm%20isotropic%2C%20whereas%20here%2C%20we%20visualize%20them%20based%20on%20lower%20resolution%20%282-3mm%0Aisotropic%29%20fMRI%20data.%20Remarkably%2C%20the%20super-resolved%20fMRI%20is%20able%20to%20recover%0Ahigh-frequency%20detail%20of%20the%20interdigitated%20organization%20of%20these%20sites%0A%28relative%20to%20the%20color-selective%20sites%29%2C%20even%20with%20training%20data%20sourced%20from%0Adifferent%20subjects%20and%20experimental%20paradigms%20--%20including%20non-visual%0Aresting-state%20fMRI%2C%20underscoring%20its%20robustness%20and%20versatility.%20Quantitative%0Aand%20qualitative%20results%20indicate%20that%20our%20method%20has%20the%20potential%20to%20enhance%0Athe%20spatial%20resolution%20of%20fMRI%2C%20leading%20to%20a%20drastic%20reduction%20in%20acquisition%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14918v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolution-%20and%20Stimulus-agnostic%20Super-Resolution%20of%20Ultra-High-Field%0A%20%20Functional%20MRI%3A%20Application%20to%20Visual%20Studies&entry.906535625=Hongwei%20Bran%20Li%20and%20Matthew%20S.%20Rosen%20and%20Shahin%20Nasr%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20High-resolution%20fMRI%20provides%20a%20window%20into%20the%20brain%27s%20mesoscale%0Aorganization.%20Yet%2C%20higher%20spatial%20resolution%20increases%20scan%20times%2C%20to%0Acompensate%20for%20the%20low%20signal%20and%20contrast-to-noise%20ratio.%20This%20work%20introduces%0Aa%20deep%20learning-based%203D%20super-resolution%20%28SR%29%20method%20for%20fMRI.%20By%0Aincorporating%20a%20resolution-agnostic%20image%20augmentation%20framework%2C%20our%20method%0Aadapts%20to%20varying%20voxel%20sizes%20without%20retraining.%20We%20apply%20this%20innovative%0Atechnique%20to%20localize%20fine-scale%20motion-selective%20sites%20in%20the%20early%20visual%0Aareas.%20Detection%20of%20these%20sites%20typically%20requires%20a%20resolution%20higher%20than%201%0Amm%20isotropic%2C%20whereas%20here%2C%20we%20visualize%20them%20based%20on%20lower%20resolution%20%282-3mm%0Aisotropic%29%20fMRI%20data.%20Remarkably%2C%20the%20super-resolved%20fMRI%20is%20able%20to%20recover%0Ahigh-frequency%20detail%20of%20the%20interdigitated%20organization%20of%20these%20sites%0A%28relative%20to%20the%20color-selective%20sites%29%2C%20even%20with%20training%20data%20sourced%20from%0Adifferent%20subjects%20and%20experimental%20paradigms%20--%20including%20non-visual%0Aresting-state%20fMRI%2C%20underscoring%20its%20robustness%20and%20versatility.%20Quantitative%0Aand%20qualitative%20results%20indicate%20that%20our%20method%20has%20the%20potential%20to%20enhance%0Athe%20spatial%20resolution%20of%20fMRI%2C%20leading%20to%20a%20drastic%20reduction%20in%20acquisition%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14918v2&entry.124074799=Read"},
{"title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly\n  Large Language Model Inference", "author": "Baolin Li and Yankai Jiang and Vijay Gadepally and Devesh Tiwari", "abstract": "  The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.\n", "link": "http://arxiv.org/abs/2403.12900v1", "date": "2024-03-19", "relevancy": 2.0623, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.557}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20Sustainable%20GenAI%20using%20Generation%20Directives%20for%20Carbon-Friendly%0A%20%20Large%20Language%20Model%20Inference&body=Title%3A%20Toward%20Sustainable%20GenAI%20using%20Generation%20Directives%20for%20Carbon-Friendly%0A%20%20Large%20Language%20Model%20Inference%0AAuthor%3A%20Baolin%20Li%20and%20Yankai%20Jiang%20and%20Vijay%20Gadepally%20and%20Devesh%20Tiwari%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20across%0Adiverse%20sectors%20raises%20significant%20environmental%20concerns%2C%20notably%20the%20carbon%0Aemissions%20from%20their%20cloud%20and%20high%20performance%20computing%20%28HPC%29%20infrastructure.%0AThis%20paper%20presents%20Sprout%2C%20an%20innovative%20framework%20designed%20to%20address%20these%0Aconcerns%20by%20reducing%20the%20carbon%20footprint%20of%20generative%20Large%20Language%20Model%0A%28LLM%29%20inference%20services.%20Sprout%20leverages%20the%20innovative%20concept%20of%0A%22generation%20directives%22%20to%20guide%20the%20autoregressive%20generation%20process%2C%20thereby%0Aenhancing%20carbon%20efficiency.%20Our%20proposed%20method%20meticulously%20balances%20the%20need%0Afor%20ecological%20sustainability%20with%20the%20demand%20for%20high-quality%20generation%0Aoutcomes.%20Employing%20a%20directive%20optimizer%20for%20the%20strategic%20assignment%20of%0Ageneration%20directives%20to%20user%20prompts%20and%20an%20original%20offline%20quality%0Aevaluator%2C%20Sprout%20demonstrates%20a%20significant%20reduction%20in%20carbon%20emissions%20by%0Aover%2040%25%20in%20real-world%20evaluations%20using%20the%20Llama2%20LLM%20and%20global%20electricity%0Agrid%20data.%20This%20research%20marks%20a%20critical%20step%20toward%20aligning%20AI%20technology%0Awith%20sustainable%20practices%2C%20highlighting%20the%20potential%20for%20mitigating%0Aenvironmental%20impacts%20in%20the%20rapidly%20expanding%20domain%20of%20generative%20artificial%0Aintelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12900v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Sustainable%20GenAI%20using%20Generation%20Directives%20for%20Carbon-Friendly%0A%20%20Large%20Language%20Model%20Inference&entry.906535625=Baolin%20Li%20and%20Yankai%20Jiang%20and%20Vijay%20Gadepally%20and%20Devesh%20Tiwari&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20across%0Adiverse%20sectors%20raises%20significant%20environmental%20concerns%2C%20notably%20the%20carbon%0Aemissions%20from%20their%20cloud%20and%20high%20performance%20computing%20%28HPC%29%20infrastructure.%0AThis%20paper%20presents%20Sprout%2C%20an%20innovative%20framework%20designed%20to%20address%20these%0Aconcerns%20by%20reducing%20the%20carbon%20footprint%20of%20generative%20Large%20Language%20Model%0A%28LLM%29%20inference%20services.%20Sprout%20leverages%20the%20innovative%20concept%20of%0A%22generation%20directives%22%20to%20guide%20the%20autoregressive%20generation%20process%2C%20thereby%0Aenhancing%20carbon%20efficiency.%20Our%20proposed%20method%20meticulously%20balances%20the%20need%0Afor%20ecological%20sustainability%20with%20the%20demand%20for%20high-quality%20generation%0Aoutcomes.%20Employing%20a%20directive%20optimizer%20for%20the%20strategic%20assignment%20of%0Ageneration%20directives%20to%20user%20prompts%20and%20an%20original%20offline%20quality%0Aevaluator%2C%20Sprout%20demonstrates%20a%20significant%20reduction%20in%20carbon%20emissions%20by%0Aover%2040%25%20in%20real-world%20evaluations%20using%20the%20Llama2%20LLM%20and%20global%20electricity%0Agrid%20data.%20This%20research%20marks%20a%20critical%20step%20toward%20aligning%20AI%20technology%0Awith%20sustainable%20practices%2C%20highlighting%20the%20potential%20for%20mitigating%0Aenvironmental%20impacts%20in%20the%20rapidly%20expanding%20domain%20of%20generative%20artificial%0Aintelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12900v1&entry.124074799=Read"},
{"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language\n  Models", "author": "Zuyan Liu and Yuhao Dong and Yongming Rao and Jie Zhou and Jiwen Lu", "abstract": "  In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot\n", "link": "http://arxiv.org/abs/2403.12966v1", "date": "2024-03-19", "relevancy": 2.0555, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5107}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chain-of-Spot%3A%20Interactive%20Reasoning%20Improves%20Large%20Vision-Language%0A%20%20Models&body=Title%3A%20Chain-of-Spot%3A%20Interactive%20Reasoning%20Improves%20Large%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Yongming%20Rao%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20vision-language%20understanding%2C%20the%20proficiency%20of%20models%20in%0Ainterpreting%20and%20reasoning%20over%20visual%20content%20has%20become%20a%20cornerstone%20for%0Anumerous%20applications.%20However%2C%20it%20is%20challenging%20for%20the%20visual%20encoder%20in%0ALarge%20Vision-Language%20Models%20%28LVLMs%29%20to%20extract%20useful%20features%20tailored%20to%0Aquestions%20that%20aid%20the%20language%20model%27s%20response.%20Furthermore%2C%20a%20common%0Apractice%20among%20existing%20LVLMs%20is%20to%20utilize%20lower-resolution%20images%2C%20which%0Arestricts%20the%20ability%20for%20visual%20recognition.%20Our%20work%20introduces%20the%0AChain-of-Spot%20%28CoS%29%20method%2C%20which%20we%20describe%20as%20Interactive%20Reasoning%2C%20a%20novel%0Aapproach%20that%20enhances%20feature%20extraction%20by%20focusing%20on%20key%20regions%20of%0Ainterest%20%28ROI%29%20within%20the%20image%2C%20corresponding%20to%20the%20posed%20questions%20or%0Ainstructions.%20This%20technique%20allows%20LVLMs%20to%20access%20more%20detailed%20visual%0Ainformation%20without%20altering%20the%20original%20image%20resolution%2C%20thereby%20offering%0Amulti-granularity%20image%20features.%20By%20integrating%20Chain-of-Spot%20with%0Ainstruct-following%20LLaVA-1.5%20models%2C%20the%20process%20of%20image%20reasoning%0Aconsistently%20improves%20performance%20across%20a%20wide%20range%20of%20multimodal%20datasets%0Aand%20benchmarks%20without%20bells%20and%20whistles%20and%20achieves%20new%20state-of-the-art%0Aresults.%20Our%20empirical%20findings%20demonstrate%20a%20significant%20improvement%20in%20LVLMs%27%0Aability%20to%20understand%20and%20reason%20about%20visual%20content%2C%20paving%20the%20way%20for%20more%0Asophisticated%20visual%20instruction-following%20applications.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/dongyh20/Chain-of-Spot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12966v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Spot%3A%20Interactive%20Reasoning%20Improves%20Large%20Vision-Language%0A%20%20Models&entry.906535625=Zuyan%20Liu%20and%20Yuhao%20Dong%20and%20Yongming%20Rao%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20the%20realm%20of%20vision-language%20understanding%2C%20the%20proficiency%20of%20models%20in%0Ainterpreting%20and%20reasoning%20over%20visual%20content%20has%20become%20a%20cornerstone%20for%0Anumerous%20applications.%20However%2C%20it%20is%20challenging%20for%20the%20visual%20encoder%20in%0ALarge%20Vision-Language%20Models%20%28LVLMs%29%20to%20extract%20useful%20features%20tailored%20to%0Aquestions%20that%20aid%20the%20language%20model%27s%20response.%20Furthermore%2C%20a%20common%0Apractice%20among%20existing%20LVLMs%20is%20to%20utilize%20lower-resolution%20images%2C%20which%0Arestricts%20the%20ability%20for%20visual%20recognition.%20Our%20work%20introduces%20the%0AChain-of-Spot%20%28CoS%29%20method%2C%20which%20we%20describe%20as%20Interactive%20Reasoning%2C%20a%20novel%0Aapproach%20that%20enhances%20feature%20extraction%20by%20focusing%20on%20key%20regions%20of%0Ainterest%20%28ROI%29%20within%20the%20image%2C%20corresponding%20to%20the%20posed%20questions%20or%0Ainstructions.%20This%20technique%20allows%20LVLMs%20to%20access%20more%20detailed%20visual%0Ainformation%20without%20altering%20the%20original%20image%20resolution%2C%20thereby%20offering%0Amulti-granularity%20image%20features.%20By%20integrating%20Chain-of-Spot%20with%0Ainstruct-following%20LLaVA-1.5%20models%2C%20the%20process%20of%20image%20reasoning%0Aconsistently%20improves%20performance%20across%20a%20wide%20range%20of%20multimodal%20datasets%0Aand%20benchmarks%20without%20bells%20and%20whistles%20and%20achieves%20new%20state-of-the-art%0Aresults.%20Our%20empirical%20findings%20demonstrate%20a%20significant%20improvement%20in%20LVLMs%27%0Aability%20to%20understand%20and%20reason%20about%20visual%20content%2C%20paving%20the%20way%20for%20more%0Asophisticated%20visual%20instruction-following%20applications.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/dongyh20/Chain-of-Spot%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12966v1&entry.124074799=Read"},
{"title": "Federated Semi-supervised Learning for Medical Image Segmentation with\n  intra-client and inter-client Consistency", "author": "Yubin Zheng and Peng Tang and Tianjie Ju and Weidong Qiu and Bo Yan", "abstract": "  Medical image segmentation plays a vital role in clinic disease diagnosis and\nmedical image analysis. However, labeling medical images for segmentation task\nis tough due to the indispensable domain expertise of radiologists.\nFurthermore, considering the privacy and sensitivity of medical images, it is\nimpractical to build a centralized segmentation dataset from different medical\ninstitutions. Federated learning aims to train a shared model of isolated\nclients without local data exchange which aligns well with the scarcity and\nprivacy characteristics of medical data. To solve the problem of labeling hard,\nmany advanced semi-supervised methods have been proposed in a centralized data\nsetting. As for federated learning, how to conduct semi-supervised learning\nunder this distributed scenario is worth investigating. In this work, we\npropose a novel federated semi-supervised learning framework for medical image\nsegmentation. The intra-client and inter-client consistency learning are\nintroduced to smooth predictions at the data level and avoid confirmation bias\nof local models. They are achieved with the assistance of a Variational\nAutoencoder (VAE) trained collaboratively by clients. The added VAE model plays\nthree roles: 1) extracting latent low-dimensional features of all labeled and\nunlabeled data; 2) performing a novel type of data augmentation in calculating\nintra-client consistency loss; 3) utilizing the generative ability of itself to\nconduct inter-client consistency distillation. The proposed framework is\ncompared with other federated semi-supervised or self-supervised learning\nmethods. The experimental results illustrate that our method outperforms the\nstate-of-the-art method while avoiding a lot of computation and communication\noverhead.\n", "link": "http://arxiv.org/abs/2403.12695v1", "date": "2024-03-19", "relevancy": 2.0523, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.516}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5034}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Semi-supervised%20Learning%20for%20Medical%20Image%20Segmentation%20with%0A%20%20intra-client%20and%20inter-client%20Consistency&body=Title%3A%20Federated%20Semi-supervised%20Learning%20for%20Medical%20Image%20Segmentation%20with%0A%20%20intra-client%20and%20inter-client%20Consistency%0AAuthor%3A%20Yubin%20Zheng%20and%20Peng%20Tang%20and%20Tianjie%20Ju%20and%20Weidong%20Qiu%20and%20Bo%20Yan%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20plays%20a%20vital%20role%20in%20clinic%20disease%20diagnosis%20and%0Amedical%20image%20analysis.%20However%2C%20labeling%20medical%20images%20for%20segmentation%20task%0Ais%20tough%20due%20to%20the%20indispensable%20domain%20expertise%20of%20radiologists.%0AFurthermore%2C%20considering%20the%20privacy%20and%20sensitivity%20of%20medical%20images%2C%20it%20is%0Aimpractical%20to%20build%20a%20centralized%20segmentation%20dataset%20from%20different%20medical%0Ainstitutions.%20Federated%20learning%20aims%20to%20train%20a%20shared%20model%20of%20isolated%0Aclients%20without%20local%20data%20exchange%20which%20aligns%20well%20with%20the%20scarcity%20and%0Aprivacy%20characteristics%20of%20medical%20data.%20To%20solve%20the%20problem%20of%20labeling%20hard%2C%0Amany%20advanced%20semi-supervised%20methods%20have%20been%20proposed%20in%20a%20centralized%20data%0Asetting.%20As%20for%20federated%20learning%2C%20how%20to%20conduct%20semi-supervised%20learning%0Aunder%20this%20distributed%20scenario%20is%20worth%20investigating.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20federated%20semi-supervised%20learning%20framework%20for%20medical%20image%0Asegmentation.%20The%20intra-client%20and%20inter-client%20consistency%20learning%20are%0Aintroduced%20to%20smooth%20predictions%20at%20the%20data%20level%20and%20avoid%20confirmation%20bias%0Aof%20local%20models.%20They%20are%20achieved%20with%20the%20assistance%20of%20a%20Variational%0AAutoencoder%20%28VAE%29%20trained%20collaboratively%20by%20clients.%20The%20added%20VAE%20model%20plays%0Athree%20roles%3A%201%29%20extracting%20latent%20low-dimensional%20features%20of%20all%20labeled%20and%0Aunlabeled%20data%3B%202%29%20performing%20a%20novel%20type%20of%20data%20augmentation%20in%20calculating%0Aintra-client%20consistency%20loss%3B%203%29%20utilizing%20the%20generative%20ability%20of%20itself%20to%0Aconduct%20inter-client%20consistency%20distillation.%20The%20proposed%20framework%20is%0Acompared%20with%20other%20federated%20semi-supervised%20or%20self-supervised%20learning%0Amethods.%20The%20experimental%20results%20illustrate%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20method%20while%20avoiding%20a%20lot%20of%20computation%20and%20communication%0Aoverhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Semi-supervised%20Learning%20for%20Medical%20Image%20Segmentation%20with%0A%20%20intra-client%20and%20inter-client%20Consistency&entry.906535625=Yubin%20Zheng%20and%20Peng%20Tang%20and%20Tianjie%20Ju%20and%20Weidong%20Qiu%20and%20Bo%20Yan&entry.1292438233=%20%20Medical%20image%20segmentation%20plays%20a%20vital%20role%20in%20clinic%20disease%20diagnosis%20and%0Amedical%20image%20analysis.%20However%2C%20labeling%20medical%20images%20for%20segmentation%20task%0Ais%20tough%20due%20to%20the%20indispensable%20domain%20expertise%20of%20radiologists.%0AFurthermore%2C%20considering%20the%20privacy%20and%20sensitivity%20of%20medical%20images%2C%20it%20is%0Aimpractical%20to%20build%20a%20centralized%20segmentation%20dataset%20from%20different%20medical%0Ainstitutions.%20Federated%20learning%20aims%20to%20train%20a%20shared%20model%20of%20isolated%0Aclients%20without%20local%20data%20exchange%20which%20aligns%20well%20with%20the%20scarcity%20and%0Aprivacy%20characteristics%20of%20medical%20data.%20To%20solve%20the%20problem%20of%20labeling%20hard%2C%0Amany%20advanced%20semi-supervised%20methods%20have%20been%20proposed%20in%20a%20centralized%20data%0Asetting.%20As%20for%20federated%20learning%2C%20how%20to%20conduct%20semi-supervised%20learning%0Aunder%20this%20distributed%20scenario%20is%20worth%20investigating.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20federated%20semi-supervised%20learning%20framework%20for%20medical%20image%0Asegmentation.%20The%20intra-client%20and%20inter-client%20consistency%20learning%20are%0Aintroduced%20to%20smooth%20predictions%20at%20the%20data%20level%20and%20avoid%20confirmation%20bias%0Aof%20local%20models.%20They%20are%20achieved%20with%20the%20assistance%20of%20a%20Variational%0AAutoencoder%20%28VAE%29%20trained%20collaboratively%20by%20clients.%20The%20added%20VAE%20model%20plays%0Athree%20roles%3A%201%29%20extracting%20latent%20low-dimensional%20features%20of%20all%20labeled%20and%0Aunlabeled%20data%3B%202%29%20performing%20a%20novel%20type%20of%20data%20augmentation%20in%20calculating%0Aintra-client%20consistency%20loss%3B%203%29%20utilizing%20the%20generative%20ability%20of%20itself%20to%0Aconduct%20inter-client%20consistency%20distillation.%20The%20proposed%20framework%20is%0Acompared%20with%20other%20federated%20semi-supervised%20or%20self-supervised%20learning%0Amethods.%20The%20experimental%20results%20illustrate%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20method%20while%20avoiding%20a%20lot%20of%20computation%20and%20communication%0Aoverhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12695v1&entry.124074799=Read"},
{"title": "MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation", "author": "Jiahao Huang and Liutao Yang and Fanwen Wang and Yinzhe Wu and Yang Nan and Angelica I. Aviles-Rivero and Carola-Bibiane Sch\u00f6nlieb and Daoqiang Zhang and Guang Yang", "abstract": "  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n", "link": "http://arxiv.org/abs/2402.18451v2", "date": "2024-03-19", "relevancy": 1.6456, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5727}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5579}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MambaMIR%3A%20An%20Arbitrary-Masked%20Mamba%20for%20Joint%20Medical%20Image%0A%20%20Reconstruction%20and%20Uncertainty%20Estimation&body=Title%3A%20MambaMIR%3A%20An%20Arbitrary-Masked%20Mamba%20for%20Joint%20Medical%20Image%0A%20%20Reconstruction%20and%20Uncertainty%20Estimation%0AAuthor%3A%20Jiahao%20Huang%20and%20Liutao%20Yang%20and%20Fanwen%20Wang%20and%20Yinzhe%20Wu%20and%20Yang%20Nan%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Daoqiang%20Zhang%20and%20Guang%20Yang%0AAbstract%3A%20%20%20The%20recent%20Mamba%20model%20has%20shown%20remarkable%20adaptability%20for%20visual%0Arepresentation%20learning%2C%20including%20in%20medical%20imaging%20tasks.%20This%20study%0Aintroduces%20MambaMIR%2C%20a%20Mamba-based%20model%20for%20medical%20image%20reconstruction%2C%20as%0Awell%20as%20its%20Generative%20Adversarial%20Network-based%20variant%2C%20MambaMIR-GAN.%20Our%0Aproposed%20MambaMIR%20inherits%20several%20advantages%2C%20such%20as%20linear%20complexity%2C%0Aglobal%20receptive%20fields%2C%20and%20dynamic%20weights%2C%20from%20the%20original%20Mamba%20model.%0AThe%20innovated%20arbitrary-mask%20mechanism%20effectively%20adapt%20Mamba%20to%20our%20image%0Areconstruction%20task%2C%20providing%20randomness%20for%20subsequent%20Monte%20Carlo-based%0Auncertainty%20estimation.%20Experiments%20conducted%20on%20various%20medical%20image%0Areconstruction%20tasks%2C%20including%20fast%20MRI%20and%20SVCT%2C%20which%20cover%20anatomical%0Aregions%20such%20as%20the%20knee%2C%20chest%2C%20and%20abdomen%2C%20have%20demonstrated%20that%20MambaMIR%0Aand%20MambaMIR-GAN%20achieve%20comparable%20or%20superior%20reconstruction%20results%20relative%0Ato%20state-of-the-art%20methods.%20Additionally%2C%20the%20estimated%20uncertainty%20maps%20offer%0Afurther%20insights%20into%20the%20reliability%20of%20the%20reconstruction%20quality.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/ayanglab/MambaMIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18451v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMIR%3A%20An%20Arbitrary-Masked%20Mamba%20for%20Joint%20Medical%20Image%0A%20%20Reconstruction%20and%20Uncertainty%20Estimation&entry.906535625=Jiahao%20Huang%20and%20Liutao%20Yang%20and%20Fanwen%20Wang%20and%20Yinzhe%20Wu%20and%20Yang%20Nan%20and%20Angelica%20I.%20Aviles-Rivero%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Daoqiang%20Zhang%20and%20Guang%20Yang&entry.1292438233=%20%20The%20recent%20Mamba%20model%20has%20shown%20remarkable%20adaptability%20for%20visual%0Arepresentation%20learning%2C%20including%20in%20medical%20imaging%20tasks.%20This%20study%0Aintroduces%20MambaMIR%2C%20a%20Mamba-based%20model%20for%20medical%20image%20reconstruction%2C%20as%0Awell%20as%20its%20Generative%20Adversarial%20Network-based%20variant%2C%20MambaMIR-GAN.%20Our%0Aproposed%20MambaMIR%20inherits%20several%20advantages%2C%20such%20as%20linear%20complexity%2C%0Aglobal%20receptive%20fields%2C%20and%20dynamic%20weights%2C%20from%20the%20original%20Mamba%20model.%0AThe%20innovated%20arbitrary-mask%20mechanism%20effectively%20adapt%20Mamba%20to%20our%20image%0Areconstruction%20task%2C%20providing%20randomness%20for%20subsequent%20Monte%20Carlo-based%0Auncertainty%20estimation.%20Experiments%20conducted%20on%20various%20medical%20image%0Areconstruction%20tasks%2C%20including%20fast%20MRI%20and%20SVCT%2C%20which%20cover%20anatomical%0Aregions%20such%20as%20the%20knee%2C%20chest%2C%20and%20abdomen%2C%20have%20demonstrated%20that%20MambaMIR%0Aand%20MambaMIR-GAN%20achieve%20comparable%20or%20superior%20reconstruction%20results%20relative%0Ato%20state-of-the-art%20methods.%20Additionally%2C%20the%20estimated%20uncertainty%20maps%20offer%0Afurther%20insights%20into%20the%20reliability%20of%20the%20reconstruction%20quality.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/ayanglab/MambaMIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18451v2&entry.124074799=Read"},
{"title": "Asynchronous Distributed Smoothing and Mapping via On-Manifold Consensus\n  ADMM", "author": "Daniel McGann and Kyle Lassak and Michael Kaess", "abstract": "  In this paper we present a fully distributed, asynchronous, and general\npurpose optimization algorithm for Consensus Simultaneous Localization and\nMapping (CSLAM). Multi-robot teams require that agents have timely and accurate\nsolutions to their state as well as the states of the other robots in the team.\nTo optimize this solution we develop a CSLAM back-end based on Consensus ADMM\ncalled MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed\nto tolerate failures of individual robots, asynchronous to tolerate\ncommunication delays and outages, and general purpose to handle any CSLAM\nproblem formulation. We demonstrate that MESA exhibits superior convergence\nrates and accuracy compare to existing state-of-the art CSLAM back-end\noptimizers.\n", "link": "http://arxiv.org/abs/2310.12320v2", "date": "2024-03-19", "relevancy": 1.6084, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5213}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Distributed%20Smoothing%20and%20Mapping%20via%20On-Manifold%20Consensus%0A%20%20ADMM&body=Title%3A%20Asynchronous%20Distributed%20Smoothing%20and%20Mapping%20via%20On-Manifold%20Consensus%0A%20%20ADMM%0AAuthor%3A%20Daniel%20McGann%20and%20Kyle%20Lassak%20and%20Michael%20Kaess%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20a%20fully%20distributed%2C%20asynchronous%2C%20and%20general%0Apurpose%20optimization%20algorithm%20for%20Consensus%20Simultaneous%20Localization%20and%0AMapping%20%28CSLAM%29.%20Multi-robot%20teams%20require%20that%20agents%20have%20timely%20and%20accurate%0Asolutions%20to%20their%20state%20as%20well%20as%20the%20states%20of%20the%20other%20robots%20in%20the%20team.%0ATo%20optimize%20this%20solution%20we%20develop%20a%20CSLAM%20back-end%20based%20on%20Consensus%20ADMM%0Acalled%20MESA%20%28Manifold%2C%20Edge-based%2C%20Separable%20ADMM%29.%20MESA%20is%20fully%20distributed%0Ato%20tolerate%20failures%20of%20individual%20robots%2C%20asynchronous%20to%20tolerate%0Acommunication%20delays%20and%20outages%2C%20and%20general%20purpose%20to%20handle%20any%20CSLAM%0Aproblem%20formulation.%20We%20demonstrate%20that%20MESA%20exhibits%20superior%20convergence%0Arates%20and%20accuracy%20compare%20to%20existing%20state-of-the%20art%20CSLAM%20back-end%0Aoptimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12320v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Distributed%20Smoothing%20and%20Mapping%20via%20On-Manifold%20Consensus%0A%20%20ADMM&entry.906535625=Daniel%20McGann%20and%20Kyle%20Lassak%20and%20Michael%20Kaess&entry.1292438233=%20%20In%20this%20paper%20we%20present%20a%20fully%20distributed%2C%20asynchronous%2C%20and%20general%0Apurpose%20optimization%20algorithm%20for%20Consensus%20Simultaneous%20Localization%20and%0AMapping%20%28CSLAM%29.%20Multi-robot%20teams%20require%20that%20agents%20have%20timely%20and%20accurate%0Asolutions%20to%20their%20state%20as%20well%20as%20the%20states%20of%20the%20other%20robots%20in%20the%20team.%0ATo%20optimize%20this%20solution%20we%20develop%20a%20CSLAM%20back-end%20based%20on%20Consensus%20ADMM%0Acalled%20MESA%20%28Manifold%2C%20Edge-based%2C%20Separable%20ADMM%29.%20MESA%20is%20fully%20distributed%0Ato%20tolerate%20failures%20of%20individual%20robots%2C%20asynchronous%20to%20tolerate%0Acommunication%20delays%20and%20outages%2C%20and%20general%20purpose%20to%20handle%20any%20CSLAM%0Aproblem%20formulation.%20We%20demonstrate%20that%20MESA%20exhibits%20superior%20convergence%0Arates%20and%20accuracy%20compare%20to%20existing%20state-of-the%20art%20CSLAM%20back-end%0Aoptimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12320v2&entry.124074799=Read"},
{"title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of\n  Language Models with Hypothesis Refinement", "author": "Linlu Qiu and Liwei Jiang and Ximing Lu and Melanie Sclar and Valentina Pyatkin and Chandra Bhagavatula and Bailin Wang and Yoon Kim and Yejin Choi and Nouha Dziri and Xiang Ren", "abstract": "  The ability to derive underlying principles from a handful of observations\nand then generalize to novel situations -- known as inductive reasoning -- is\ncentral to human intelligence. Prior work suggests that language models (LMs)\noften fall short on inductive reasoning, despite achieving impressive success\non research benchmarks. In this work, we conduct a systematic study of the\ninductive reasoning capabilities of LMs through iterative hypothesis\nrefinement, a technique that more closely mirrors the human inductive process\nthan standard input-output prompting. Iterative hypothesis refinement employs a\nthree-step process: proposing, selecting, and refining hypotheses in the form\nof textual rules. By examining the intermediate rules, we observe that LMs are\nphenomenal hypothesis proposers (i.e., generating candidate rules), and when\ncoupled with a (task-specific) symbolic interpreter that is able to\nsystematically filter the proposed set of rules, this hybrid approach achieves\nstrong results across inductive reasoning benchmarks that require inducing\ncausal relations, language-like instructions, and symbolic concepts. However,\nthey also behave as puzzling inductive reasoners, showing notable performance\ngaps between rule induction (i.e., identifying plausible rules) and rule\napplication (i.e., applying proposed rules to instances), suggesting that LMs\nare proposing hypotheses without being able to actually apply the rules.\nThrough empirical and human analyses, we further reveal several discrepancies\nbetween the inductive reasoning processes of LMs and humans, shedding light on\nboth the potentials and limitations of using LMs in inductive reasoning tasks.\n", "link": "http://arxiv.org/abs/2310.08559v3", "date": "2024-03-19", "relevancy": 0.9548, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4688}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4617}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Phenomenal%20Yet%20Puzzling%3A%20Testing%20Inductive%20Reasoning%20Capabilities%20of%0A%20%20Language%20Models%20with%20Hypothesis%20Refinement&body=Title%3A%20Phenomenal%20Yet%20Puzzling%3A%20Testing%20Inductive%20Reasoning%20Capabilities%20of%0A%20%20Language%20Models%20with%20Hypothesis%20Refinement%0AAuthor%3A%20Linlu%20Qiu%20and%20Liwei%20Jiang%20and%20Ximing%20Lu%20and%20Melanie%20Sclar%20and%20Valentina%20Pyatkin%20and%20Chandra%20Bhagavatula%20and%20Bailin%20Wang%20and%20Yoon%20Kim%20and%20Yejin%20Choi%20and%20Nouha%20Dziri%20and%20Xiang%20Ren%0AAbstract%3A%20%20%20The%20ability%20to%20derive%20underlying%20principles%20from%20a%20handful%20of%20observations%0Aand%20then%20generalize%20to%20novel%20situations%20--%20known%20as%20inductive%20reasoning%20--%20is%0Acentral%20to%20human%20intelligence.%20Prior%20work%20suggests%20that%20language%20models%20%28LMs%29%0Aoften%20fall%20short%20on%20inductive%20reasoning%2C%20despite%20achieving%20impressive%20success%0Aon%20research%20benchmarks.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20the%0Ainductive%20reasoning%20capabilities%20of%20LMs%20through%20iterative%20hypothesis%0Arefinement%2C%20a%20technique%20that%20more%20closely%20mirrors%20the%20human%20inductive%20process%0Athan%20standard%20input-output%20prompting.%20Iterative%20hypothesis%20refinement%20employs%20a%0Athree-step%20process%3A%20proposing%2C%20selecting%2C%20and%20refining%20hypotheses%20in%20the%20form%0Aof%20textual%20rules.%20By%20examining%20the%20intermediate%20rules%2C%20we%20observe%20that%20LMs%20are%0Aphenomenal%20hypothesis%20proposers%20%28i.e.%2C%20generating%20candidate%20rules%29%2C%20and%20when%0Acoupled%20with%20a%20%28task-specific%29%20symbolic%20interpreter%20that%20is%20able%20to%0Asystematically%20filter%20the%20proposed%20set%20of%20rules%2C%20this%20hybrid%20approach%20achieves%0Astrong%20results%20across%20inductive%20reasoning%20benchmarks%20that%20require%20inducing%0Acausal%20relations%2C%20language-like%20instructions%2C%20and%20symbolic%20concepts.%20However%2C%0Athey%20also%20behave%20as%20puzzling%20inductive%20reasoners%2C%20showing%20notable%20performance%0Agaps%20between%20rule%20induction%20%28i.e.%2C%20identifying%20plausible%20rules%29%20and%20rule%0Aapplication%20%28i.e.%2C%20applying%20proposed%20rules%20to%20instances%29%2C%20suggesting%20that%20LMs%0Aare%20proposing%20hypotheses%20without%20being%20able%20to%20actually%20apply%20the%20rules.%0AThrough%20empirical%20and%20human%20analyses%2C%20we%20further%20reveal%20several%20discrepancies%0Abetween%20the%20inductive%20reasoning%20processes%20of%20LMs%20and%20humans%2C%20shedding%20light%20on%0Aboth%20the%20potentials%20and%20limitations%20of%20using%20LMs%20in%20inductive%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08559v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phenomenal%20Yet%20Puzzling%3A%20Testing%20Inductive%20Reasoning%20Capabilities%20of%0A%20%20Language%20Models%20with%20Hypothesis%20Refinement&entry.906535625=Linlu%20Qiu%20and%20Liwei%20Jiang%20and%20Ximing%20Lu%20and%20Melanie%20Sclar%20and%20Valentina%20Pyatkin%20and%20Chandra%20Bhagavatula%20and%20Bailin%20Wang%20and%20Yoon%20Kim%20and%20Yejin%20Choi%20and%20Nouha%20Dziri%20and%20Xiang%20Ren&entry.1292438233=%20%20The%20ability%20to%20derive%20underlying%20principles%20from%20a%20handful%20of%20observations%0Aand%20then%20generalize%20to%20novel%20situations%20--%20known%20as%20inductive%20reasoning%20--%20is%0Acentral%20to%20human%20intelligence.%20Prior%20work%20suggests%20that%20language%20models%20%28LMs%29%0Aoften%20fall%20short%20on%20inductive%20reasoning%2C%20despite%20achieving%20impressive%20success%0Aon%20research%20benchmarks.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20study%20of%20the%0Ainductive%20reasoning%20capabilities%20of%20LMs%20through%20iterative%20hypothesis%0Arefinement%2C%20a%20technique%20that%20more%20closely%20mirrors%20the%20human%20inductive%20process%0Athan%20standard%20input-output%20prompting.%20Iterative%20hypothesis%20refinement%20employs%20a%0Athree-step%20process%3A%20proposing%2C%20selecting%2C%20and%20refining%20hypotheses%20in%20the%20form%0Aof%20textual%20rules.%20By%20examining%20the%20intermediate%20rules%2C%20we%20observe%20that%20LMs%20are%0Aphenomenal%20hypothesis%20proposers%20%28i.e.%2C%20generating%20candidate%20rules%29%2C%20and%20when%0Acoupled%20with%20a%20%28task-specific%29%20symbolic%20interpreter%20that%20is%20able%20to%0Asystematically%20filter%20the%20proposed%20set%20of%20rules%2C%20this%20hybrid%20approach%20achieves%0Astrong%20results%20across%20inductive%20reasoning%20benchmarks%20that%20require%20inducing%0Acausal%20relations%2C%20language-like%20instructions%2C%20and%20symbolic%20concepts.%20However%2C%0Athey%20also%20behave%20as%20puzzling%20inductive%20reasoners%2C%20showing%20notable%20performance%0Agaps%20between%20rule%20induction%20%28i.e.%2C%20identifying%20plausible%20rules%29%20and%20rule%0Aapplication%20%28i.e.%2C%20applying%20proposed%20rules%20to%20instances%29%2C%20suggesting%20that%20LMs%0Aare%20proposing%20hypotheses%20without%20being%20able%20to%20actually%20apply%20the%20rules.%0AThrough%20empirical%20and%20human%20analyses%2C%20we%20further%20reveal%20several%20discrepancies%0Abetween%20the%20inductive%20reasoning%20processes%20of%20LMs%20and%20humans%2C%20shedding%20light%20on%0Aboth%20the%20potentials%20and%20limitations%20of%20using%20LMs%20in%20inductive%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08559v3&entry.124074799=Read"},
{"title": "Neural Parameter Regression for Explicit Representations of PDE Solution\n  Operators", "author": "Konrad Mundinger and Max Zimmer and Sebastian Pokutta", "abstract": "  We introduce Neural Parameter Regression (NPR), a novel framework\nspecifically developed for learning solution operators in Partial Differential\nEquations (PDEs). Tailored for operator learning, this approach surpasses\ntraditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural\nNetwork (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN)\nparameters. By parametrizing each solution based on specific initial\nconditions, it effectively approximates a mapping between function spaces. Our\nmethod enhances parameter efficiency by incorporating low-rank matrices,\nthereby boosting computational efficiency and scalability. The framework shows\nremarkable adaptability to new initial and boundary conditions, allowing for\nrapid fine-tuning and inference, even in cases of out-of-distribution examples.\n", "link": "http://arxiv.org/abs/2403.12764v1", "date": "2024-03-19", "relevancy": 1.8453, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4574}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Parameter%20Regression%20for%20Explicit%20Representations%20of%20PDE%20Solution%0A%20%20Operators&body=Title%3A%20Neural%20Parameter%20Regression%20for%20Explicit%20Representations%20of%20PDE%20Solution%0A%20%20Operators%0AAuthor%3A%20Konrad%20Mundinger%20and%20Max%20Zimmer%20and%20Sebastian%20Pokutta%0AAbstract%3A%20%20%20We%20introduce%20Neural%20Parameter%20Regression%20%28NPR%29%2C%20a%20novel%20framework%0Aspecifically%20developed%20for%20learning%20solution%20operators%20in%20Partial%20Differential%0AEquations%20%28PDEs%29.%20Tailored%20for%20operator%20learning%2C%20this%20approach%20surpasses%0Atraditional%20DeepONets%20%28Lu%20et%20al.%2C%202021%29%20by%20employing%20Physics-Informed%20Neural%0ANetwork%20%28PINN%2C%20Raissi%20et%20al.%2C%202019%29%20techniques%20to%20regress%20Neural%20Network%20%28NN%29%0Aparameters.%20By%20parametrizing%20each%20solution%20based%20on%20specific%20initial%0Aconditions%2C%20it%20effectively%20approximates%20a%20mapping%20between%20function%20spaces.%20Our%0Amethod%20enhances%20parameter%20efficiency%20by%20incorporating%20low-rank%20matrices%2C%0Athereby%20boosting%20computational%20efficiency%20and%20scalability.%20The%20framework%20shows%0Aremarkable%20adaptability%20to%20new%20initial%20and%20boundary%20conditions%2C%20allowing%20for%0Arapid%20fine-tuning%20and%20inference%2C%20even%20in%20cases%20of%20out-of-distribution%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12764v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Parameter%20Regression%20for%20Explicit%20Representations%20of%20PDE%20Solution%0A%20%20Operators&entry.906535625=Konrad%20Mundinger%20and%20Max%20Zimmer%20and%20Sebastian%20Pokutta&entry.1292438233=%20%20We%20introduce%20Neural%20Parameter%20Regression%20%28NPR%29%2C%20a%20novel%20framework%0Aspecifically%20developed%20for%20learning%20solution%20operators%20in%20Partial%20Differential%0AEquations%20%28PDEs%29.%20Tailored%20for%20operator%20learning%2C%20this%20approach%20surpasses%0Atraditional%20DeepONets%20%28Lu%20et%20al.%2C%202021%29%20by%20employing%20Physics-Informed%20Neural%0ANetwork%20%28PINN%2C%20Raissi%20et%20al.%2C%202019%29%20techniques%20to%20regress%20Neural%20Network%20%28NN%29%0Aparameters.%20By%20parametrizing%20each%20solution%20based%20on%20specific%20initial%0Aconditions%2C%20it%20effectively%20approximates%20a%20mapping%20between%20function%20spaces.%20Our%0Amethod%20enhances%20parameter%20efficiency%20by%20incorporating%20low-rank%20matrices%2C%0Athereby%20boosting%20computational%20efficiency%20and%20scalability.%20The%20framework%20shows%0Aremarkable%20adaptability%20to%20new%20initial%20and%20boundary%20conditions%2C%20allowing%20for%0Arapid%20fine-tuning%20and%20inference%2C%20even%20in%20cases%20of%20out-of-distribution%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12764v1&entry.124074799=Read"},
{"title": "Looking for the Human in HRI Teaching: User-Centered Course Design for\n  Tech-Savvy Students", "author": "Tobias Doernbach", "abstract": "  Top-down, user-centered thinking is not typically a strength of all students,\nespecially tech-savvy computer science-related ones. We propose Human-Robot\nInteraction (HRI) introductory courses as a highly suitable opportunity to\nfoster these important skills since the HRI discipline includes a focus on\nhumans as users. Our HRI course therefore contains elements like scenario-based\ndesign of laboratory projects, discussing and merging ideas and other\nself-empowerment techniques. Participants describe, implement and present\neveryday scenarios using Pepper robots and our customized open-source visual\nprogramming tool. We observe that students obtain a good grasp of the taught\ntopics and improve their user-centered thinking skills.\n", "link": "http://arxiv.org/abs/2403.12607v1", "date": "2024-03-19", "relevancy": 1.4729, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5068}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Looking%20for%20the%20Human%20in%20HRI%20Teaching%3A%20User-Centered%20Course%20Design%20for%0A%20%20Tech-Savvy%20Students&body=Title%3A%20Looking%20for%20the%20Human%20in%20HRI%20Teaching%3A%20User-Centered%20Course%20Design%20for%0A%20%20Tech-Savvy%20Students%0AAuthor%3A%20Tobias%20Doernbach%0AAbstract%3A%20%20%20Top-down%2C%20user-centered%20thinking%20is%20not%20typically%20a%20strength%20of%20all%20students%2C%0Aespecially%20tech-savvy%20computer%20science-related%20ones.%20We%20propose%20Human-Robot%0AInteraction%20%28HRI%29%20introductory%20courses%20as%20a%20highly%20suitable%20opportunity%20to%0Afoster%20these%20important%20skills%20since%20the%20HRI%20discipline%20includes%20a%20focus%20on%0Ahumans%20as%20users.%20Our%20HRI%20course%20therefore%20contains%20elements%20like%20scenario-based%0Adesign%20of%20laboratory%20projects%2C%20discussing%20and%20merging%20ideas%20and%20other%0Aself-empowerment%20techniques.%20Participants%20describe%2C%20implement%20and%20present%0Aeveryday%20scenarios%20using%20Pepper%20robots%20and%20our%20customized%20open-source%20visual%0Aprogramming%20tool.%20We%20observe%20that%20students%20obtain%20a%20good%20grasp%20of%20the%20taught%0Atopics%20and%20improve%20their%20user-centered%20thinking%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12607v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20for%20the%20Human%20in%20HRI%20Teaching%3A%20User-Centered%20Course%20Design%20for%0A%20%20Tech-Savvy%20Students&entry.906535625=Tobias%20Doernbach&entry.1292438233=%20%20Top-down%2C%20user-centered%20thinking%20is%20not%20typically%20a%20strength%20of%20all%20students%2C%0Aespecially%20tech-savvy%20computer%20science-related%20ones.%20We%20propose%20Human-Robot%0AInteraction%20%28HRI%29%20introductory%20courses%20as%20a%20highly%20suitable%20opportunity%20to%0Afoster%20these%20important%20skills%20since%20the%20HRI%20discipline%20includes%20a%20focus%20on%0Ahumans%20as%20users.%20Our%20HRI%20course%20therefore%20contains%20elements%20like%20scenario-based%0Adesign%20of%20laboratory%20projects%2C%20discussing%20and%20merging%20ideas%20and%20other%0Aself-empowerment%20techniques.%20Participants%20describe%2C%20implement%20and%20present%0Aeveryday%20scenarios%20using%20Pepper%20robots%20and%20our%20customized%20open-source%20visual%0Aprogramming%20tool.%20We%20observe%20that%20students%20obtain%20a%20good%20grasp%20of%20the%20taught%0Atopics%20and%20improve%20their%20user-centered%20thinking%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12607v1&entry.124074799=Read"},
{"title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented\n  Generation and Soft-Prompting for Non-Specialist LLM Users", "author": "Jennifer Dodgson and Lin Nanzheng and Julian Peh and Akira Rafhael Janson Pattirane and Alfath Daryl Alhajir and Eko Ridho Dinarto and Joseph Lim and Syed Danyal Ahmad", "abstract": "  Research into methods for improving the performance of large language models\n(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and\nsoft-prompting has tended to focus on the use of highly technical or high-cost\ntechniques, making many of the newly discovered approaches comparatively\ninaccessible to non-technical users. In this paper we tested an unmodified\nversion of GPT 3.5, a fine-tuned version, and the same unmodified model when\ngiven access to a vectorised RAG database, both in isolation and in combination\nwith a basic, non-algorithmic soft prompt. In each case we tested the model's\nability to answer a set of 100 questions relating primarily to events that\noccurred after September 2021 (the point at which GPT 3.5's training data set\nends). We found that if commercial platforms are used and default settings are\napplied with no iteration in order to establish a baseline set of outputs, a\nfine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach\nout-performed both. The application of a soft prompt significantly improved the\nperformance of each approach.\n", "link": "http://arxiv.org/abs/2311.05903v2", "date": "2024-03-19", "relevancy": 1.8105, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4421}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Establishing%20Performance%20Baselines%20in%20Fine-Tuning%2C%20Retrieval-Augmented%0A%20%20Generation%20and%20Soft-Prompting%20for%20Non-Specialist%20LLM%20Users&body=Title%3A%20Establishing%20Performance%20Baselines%20in%20Fine-Tuning%2C%20Retrieval-Augmented%0A%20%20Generation%20and%20Soft-Prompting%20for%20Non-Specialist%20LLM%20Users%0AAuthor%3A%20Jennifer%20Dodgson%20and%20Lin%20Nanzheng%20and%20Julian%20Peh%20and%20Akira%20Rafhael%20Janson%20Pattirane%20and%20Alfath%20Daryl%20Alhajir%20and%20Eko%20Ridho%20Dinarto%20and%20Joseph%20Lim%20and%20Syed%20Danyal%20Ahmad%0AAbstract%3A%20%20%20Research%20into%20methods%20for%20improving%20the%20performance%20of%20large%20language%20models%0A%28LLMs%29%20through%20fine-tuning%2C%20retrieval-augmented%20generation%20%28RAG%29%20and%0Asoft-prompting%20has%20tended%20to%20focus%20on%20the%20use%20of%20highly%20technical%20or%20high-cost%0Atechniques%2C%20making%20many%20of%20the%20newly%20discovered%20approaches%20comparatively%0Ainaccessible%20to%20non-technical%20users.%20In%20this%20paper%20we%20tested%20an%20unmodified%0Aversion%20of%20GPT%203.5%2C%20a%20fine-tuned%20version%2C%20and%20the%20same%20unmodified%20model%20when%0Agiven%20access%20to%20a%20vectorised%20RAG%20database%2C%20both%20in%20isolation%20and%20in%20combination%0Awith%20a%20basic%2C%20non-algorithmic%20soft%20prompt.%20In%20each%20case%20we%20tested%20the%20model%27s%0Aability%20to%20answer%20a%20set%20of%20100%20questions%20relating%20primarily%20to%20events%20that%0Aoccurred%20after%20September%202021%20%28the%20point%20at%20which%20GPT%203.5%27s%20training%20data%20set%0Aends%29.%20We%20found%20that%20if%20commercial%20platforms%20are%20used%20and%20default%20settings%20are%0Aapplied%20with%20no%20iteration%20in%20order%20to%20establish%20a%20baseline%20set%20of%20outputs%2C%20a%0Afine-tuned%20model%20outperforms%20GPT%203.5%20Turbo%2C%20while%20the%20RAG%20approach%0Aout-performed%20both.%20The%20application%20of%20a%20soft%20prompt%20significantly%20improved%20the%0Aperformance%20of%20each%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05903v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Establishing%20Performance%20Baselines%20in%20Fine-Tuning%2C%20Retrieval-Augmented%0A%20%20Generation%20and%20Soft-Prompting%20for%20Non-Specialist%20LLM%20Users&entry.906535625=Jennifer%20Dodgson%20and%20Lin%20Nanzheng%20and%20Julian%20Peh%20and%20Akira%20Rafhael%20Janson%20Pattirane%20and%20Alfath%20Daryl%20Alhajir%20and%20Eko%20Ridho%20Dinarto%20and%20Joseph%20Lim%20and%20Syed%20Danyal%20Ahmad&entry.1292438233=%20%20Research%20into%20methods%20for%20improving%20the%20performance%20of%20large%20language%20models%0A%28LLMs%29%20through%20fine-tuning%2C%20retrieval-augmented%20generation%20%28RAG%29%20and%0Asoft-prompting%20has%20tended%20to%20focus%20on%20the%20use%20of%20highly%20technical%20or%20high-cost%0Atechniques%2C%20making%20many%20of%20the%20newly%20discovered%20approaches%20comparatively%0Ainaccessible%20to%20non-technical%20users.%20In%20this%20paper%20we%20tested%20an%20unmodified%0Aversion%20of%20GPT%203.5%2C%20a%20fine-tuned%20version%2C%20and%20the%20same%20unmodified%20model%20when%0Agiven%20access%20to%20a%20vectorised%20RAG%20database%2C%20both%20in%20isolation%20and%20in%20combination%0Awith%20a%20basic%2C%20non-algorithmic%20soft%20prompt.%20In%20each%20case%20we%20tested%20the%20model%27s%0Aability%20to%20answer%20a%20set%20of%20100%20questions%20relating%20primarily%20to%20events%20that%0Aoccurred%20after%20September%202021%20%28the%20point%20at%20which%20GPT%203.5%27s%20training%20data%20set%0Aends%29.%20We%20found%20that%20if%20commercial%20platforms%20are%20used%20and%20default%20settings%20are%0Aapplied%20with%20no%20iteration%20in%20order%20to%20establish%20a%20baseline%20set%20of%20outputs%2C%20a%0Afine-tuned%20model%20outperforms%20GPT%203.5%20Turbo%2C%20while%20the%20RAG%20approach%0Aout-performed%20both.%20The%20application%20of%20a%20soft%20prompt%20significantly%20improved%20the%0Aperformance%20of%20each%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05903v2&entry.124074799=Read"},
{"title": "Cross or Wait? Predicting Pedestrian Interaction Outcomes at\n  Unsignalized Crossings", "author": "Chi Zhang and Amir Hossein Kalantari and Yue Yang and Zhongjun Ni and Gustav Markkula and Natasha Merat and Christian Berger", "abstract": "  Predicting pedestrian behavior when interacting with vehicles is one of the\nmost critical challenges in the field of automated driving. Pedestrian crossing\nbehavior is influenced by various interaction factors, including time to\narrival, pedestrian waiting time, the presence of zebra crossing, and the\nproperties and personality traits of both pedestrians and drivers. However,\nthese factors have not been fully explored for use in predicting interaction\noutcomes. In this paper, we use machine learning to predict pedestrian crossing\nbehavior including pedestrian crossing decision, crossing initiation time\n(CIT), and crossing duration (CD) when interacting with vehicles at\nunsignalized crossings. Distributed simulator data are utilized for predicting\nand analyzing the interaction factors. Compared with the logistic regression\nbaseline model, our proposed neural network model improves the prediction\naccuracy and F1 score by 4.46% and 3.23%, respectively. Our model also reduces\nthe root mean squared error (RMSE) for CIT and CD by 21.56% and 30.14% compared\nwith the linear regression model. Additionally, we have analyzed the importance\nof interaction factors, and present the results of models using fewer factors.\nThis provides information for model selection in different scenarios with\nlimited input features.\n", "link": "http://arxiv.org/abs/2304.08260v2", "date": "2024-03-19", "relevancy": 1.8826, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5367}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4588}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4561}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross%20or%20Wait%3F%20Predicting%20Pedestrian%20Interaction%20Outcomes%20at%0A%20%20Unsignalized%20Crossings&body=Title%3A%20Cross%20or%20Wait%3F%20Predicting%20Pedestrian%20Interaction%20Outcomes%20at%0A%20%20Unsignalized%20Crossings%0AAuthor%3A%20Chi%20Zhang%20and%20Amir%20Hossein%20Kalantari%20and%20Yue%20Yang%20and%20Zhongjun%20Ni%20and%20Gustav%20Markkula%20and%20Natasha%20Merat%20and%20Christian%20Berger%0AAbstract%3A%20%20%20Predicting%20pedestrian%20behavior%20when%20interacting%20with%20vehicles%20is%20one%20of%20the%0Amost%20critical%20challenges%20in%20the%20field%20of%20automated%20driving.%20Pedestrian%20crossing%0Abehavior%20is%20influenced%20by%20various%20interaction%20factors%2C%20including%20time%20to%0Aarrival%2C%20pedestrian%20waiting%20time%2C%20the%20presence%20of%20zebra%20crossing%2C%20and%20the%0Aproperties%20and%20personality%20traits%20of%20both%20pedestrians%20and%20drivers.%20However%2C%0Athese%20factors%20have%20not%20been%20fully%20explored%20for%20use%20in%20predicting%20interaction%0Aoutcomes.%20In%20this%20paper%2C%20we%20use%20machine%20learning%20to%20predict%20pedestrian%20crossing%0Abehavior%20including%20pedestrian%20crossing%20decision%2C%20crossing%20initiation%20time%0A%28CIT%29%2C%20and%20crossing%20duration%20%28CD%29%20when%20interacting%20with%20vehicles%20at%0Aunsignalized%20crossings.%20Distributed%20simulator%20data%20are%20utilized%20for%20predicting%0Aand%20analyzing%20the%20interaction%20factors.%20Compared%20with%20the%20logistic%20regression%0Abaseline%20model%2C%20our%20proposed%20neural%20network%20model%20improves%20the%20prediction%0Aaccuracy%20and%20F1%20score%20by%204.46%25%20and%203.23%25%2C%20respectively.%20Our%20model%20also%20reduces%0Athe%20root%20mean%20squared%20error%20%28RMSE%29%20for%20CIT%20and%20CD%20by%2021.56%25%20and%2030.14%25%20compared%0Awith%20the%20linear%20regression%20model.%20Additionally%2C%20we%20have%20analyzed%20the%20importance%0Aof%20interaction%20factors%2C%20and%20present%20the%20results%20of%20models%20using%20fewer%20factors.%0AThis%20provides%20information%20for%20model%20selection%20in%20different%20scenarios%20with%0Alimited%20input%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08260v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross%20or%20Wait%3F%20Predicting%20Pedestrian%20Interaction%20Outcomes%20at%0A%20%20Unsignalized%20Crossings&entry.906535625=Chi%20Zhang%20and%20Amir%20Hossein%20Kalantari%20and%20Yue%20Yang%20and%20Zhongjun%20Ni%20and%20Gustav%20Markkula%20and%20Natasha%20Merat%20and%20Christian%20Berger&entry.1292438233=%20%20Predicting%20pedestrian%20behavior%20when%20interacting%20with%20vehicles%20is%20one%20of%20the%0Amost%20critical%20challenges%20in%20the%20field%20of%20automated%20driving.%20Pedestrian%20crossing%0Abehavior%20is%20influenced%20by%20various%20interaction%20factors%2C%20including%20time%20to%0Aarrival%2C%20pedestrian%20waiting%20time%2C%20the%20presence%20of%20zebra%20crossing%2C%20and%20the%0Aproperties%20and%20personality%20traits%20of%20both%20pedestrians%20and%20drivers.%20However%2C%0Athese%20factors%20have%20not%20been%20fully%20explored%20for%20use%20in%20predicting%20interaction%0Aoutcomes.%20In%20this%20paper%2C%20we%20use%20machine%20learning%20to%20predict%20pedestrian%20crossing%0Abehavior%20including%20pedestrian%20crossing%20decision%2C%20crossing%20initiation%20time%0A%28CIT%29%2C%20and%20crossing%20duration%20%28CD%29%20when%20interacting%20with%20vehicles%20at%0Aunsignalized%20crossings.%20Distributed%20simulator%20data%20are%20utilized%20for%20predicting%0Aand%20analyzing%20the%20interaction%20factors.%20Compared%20with%20the%20logistic%20regression%0Abaseline%20model%2C%20our%20proposed%20neural%20network%20model%20improves%20the%20prediction%0Aaccuracy%20and%20F1%20score%20by%204.46%25%20and%203.23%25%2C%20respectively.%20Our%20model%20also%20reduces%0Athe%20root%20mean%20squared%20error%20%28RMSE%29%20for%20CIT%20and%20CD%20by%2021.56%25%20and%2030.14%25%20compared%0Awith%20the%20linear%20regression%20model.%20Additionally%2C%20we%20have%20analyzed%20the%20importance%0Aof%20interaction%20factors%2C%20and%20present%20the%20results%20of%20models%20using%20fewer%20factors.%0AThis%20provides%20information%20for%20model%20selection%20in%20different%20scenarios%20with%0Alimited%20input%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08260v2&entry.124074799=Read"},
{"title": "Dynamic Survival Analysis for Early Event Prediction", "author": "Hugo Y\u00e8che and Manuel Burger and Dinara Veshchezerova and Gunnar R\u00e4tsch", "abstract": "  This study advances Early Event Prediction (EEP) in healthcare through\nDynamic Survival Analysis (DSA), offering a novel approach by integrating risk\nlocalization into alarm policies to enhance clinical event metrics. By adapting\nand evaluating DSA models against traditional EEP benchmarks, our research\ndemonstrates their ability to match EEP models on a time-step level and\nsignificantly improve event-level metrics through a new alarm prioritization\nscheme (up to 11% AuPRC difference). This approach represents a significant\nstep forward in predictive healthcare, providing a more nuanced and actionable\nframework for early event prediction and management.\n", "link": "http://arxiv.org/abs/2403.12818v1", "date": "2024-03-19", "relevancy": 1.7673, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4716}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.432}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.416}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Survival%20Analysis%20for%20Early%20Event%20Prediction&body=Title%3A%20Dynamic%20Survival%20Analysis%20for%20Early%20Event%20Prediction%0AAuthor%3A%20Hugo%20Y%C3%A8che%20and%20Manuel%20Burger%20and%20Dinara%20Veshchezerova%20and%20Gunnar%20R%C3%A4tsch%0AAbstract%3A%20%20%20This%20study%20advances%20Early%20Event%20Prediction%20%28EEP%29%20in%20healthcare%20through%0ADynamic%20Survival%20Analysis%20%28DSA%29%2C%20offering%20a%20novel%20approach%20by%20integrating%20risk%0Alocalization%20into%20alarm%20policies%20to%20enhance%20clinical%20event%20metrics.%20By%20adapting%0Aand%20evaluating%20DSA%20models%20against%20traditional%20EEP%20benchmarks%2C%20our%20research%0Ademonstrates%20their%20ability%20to%20match%20EEP%20models%20on%20a%20time-step%20level%20and%0Asignificantly%20improve%20event-level%20metrics%20through%20a%20new%20alarm%20prioritization%0Ascheme%20%28up%20to%2011%25%20AuPRC%20difference%29.%20This%20approach%20represents%20a%20significant%0Astep%20forward%20in%20predictive%20healthcare%2C%20providing%20a%20more%20nuanced%20and%20actionable%0Aframework%20for%20early%20event%20prediction%20and%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12818v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Survival%20Analysis%20for%20Early%20Event%20Prediction&entry.906535625=Hugo%20Y%C3%A8che%20and%20Manuel%20Burger%20and%20Dinara%20Veshchezerova%20and%20Gunnar%20R%C3%A4tsch&entry.1292438233=%20%20This%20study%20advances%20Early%20Event%20Prediction%20%28EEP%29%20in%20healthcare%20through%0ADynamic%20Survival%20Analysis%20%28DSA%29%2C%20offering%20a%20novel%20approach%20by%20integrating%20risk%0Alocalization%20into%20alarm%20policies%20to%20enhance%20clinical%20event%20metrics.%20By%20adapting%0Aand%20evaluating%20DSA%20models%20against%20traditional%20EEP%20benchmarks%2C%20our%20research%0Ademonstrates%20their%20ability%20to%20match%20EEP%20models%20on%20a%20time-step%20level%20and%0Asignificantly%20improve%20event-level%20metrics%20through%20a%20new%20alarm%20prioritization%0Ascheme%20%28up%20to%2011%25%20AuPRC%20difference%29.%20This%20approach%20represents%20a%20significant%0Astep%20forward%20in%20predictive%20healthcare%2C%20providing%20a%20more%20nuanced%20and%20actionable%0Aframework%20for%20early%20event%20prediction%20and%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12818v1&entry.124074799=Read"},
{"title": "How does promoting the minority fraction affect generalization? A\n  theoretical study of the one-hidden-layer neural network on group imbalance", "author": "Hongkang Li and Shuai Zhang and Yihua Zhang and Meng Wang and Sijia Liu and Pin-Yu Chen", "abstract": "  Group imbalance has been a known problem in empirical risk minimization\n(ERM), where the achieved high average accuracy is accompanied by low accuracy\nin a minority group. Despite algorithmic efforts to improve the minority group\naccuracy, a theoretical generalization analysis of ERM on individual groups\nremains elusive. By formulating the group imbalance problem with the Gaussian\nMixture Model, this paper quantifies the impact of individual groups on the\nsample complexity, the convergence rate, and the average and group-level\ntesting performance. Although our theoretical framework is centered on binary\nclassification using a one-hidden-layer neural network, to the best of our\nknowledge, we provide the first theoretical analysis of the group-level\ngeneralization of ERM in addition to the commonly studied average\ngeneralization performance. Sample insights of our theoretical results include\nthat when all group-level co-variance is in the medium regime and all mean are\nclose to zero, the learning performance is most desirable in the sense of a\nsmall sample complexity, a fast training rate, and a high average and\ngroup-level testing accuracy. Moreover, we show that increasing the fraction of\nthe minority group in the training data does not necessarily improve the\ngeneralization performance of the minority group. Our theoretical results are\nvalidated on both synthetic and empirical datasets, such as CelebA and CIFAR-10\nin image classification.\n", "link": "http://arxiv.org/abs/2403.07310v2", "date": "2024-03-19", "relevancy": 1.7209, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4397}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4257}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20How%20does%20promoting%20the%20minority%20fraction%20affect%20generalization%3F%20A%0A%20%20theoretical%20study%20of%20the%20one-hidden-layer%20neural%20network%20on%20group%20imbalance&body=Title%3A%20How%20does%20promoting%20the%20minority%20fraction%20affect%20generalization%3F%20A%0A%20%20theoretical%20study%20of%20the%20one-hidden-layer%20neural%20network%20on%20group%20imbalance%0AAuthor%3A%20Hongkang%20Li%20and%20Shuai%20Zhang%20and%20Yihua%20Zhang%20and%20Meng%20Wang%20and%20Sijia%20Liu%20and%20Pin-Yu%20Chen%0AAbstract%3A%20%20%20Group%20imbalance%20has%20been%20a%20known%20problem%20in%20empirical%20risk%20minimization%0A%28ERM%29%2C%20where%20the%20achieved%20high%20average%20accuracy%20is%20accompanied%20by%20low%20accuracy%0Ain%20a%20minority%20group.%20Despite%20algorithmic%20efforts%20to%20improve%20the%20minority%20group%0Aaccuracy%2C%20a%20theoretical%20generalization%20analysis%20of%20ERM%20on%20individual%20groups%0Aremains%20elusive.%20By%20formulating%20the%20group%20imbalance%20problem%20with%20the%20Gaussian%0AMixture%20Model%2C%20this%20paper%20quantifies%20the%20impact%20of%20individual%20groups%20on%20the%0Asample%20complexity%2C%20the%20convergence%20rate%2C%20and%20the%20average%20and%20group-level%0Atesting%20performance.%20Although%20our%20theoretical%20framework%20is%20centered%20on%20binary%0Aclassification%20using%20a%20one-hidden-layer%20neural%20network%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20we%20provide%20the%20first%20theoretical%20analysis%20of%20the%20group-level%0Ageneralization%20of%20ERM%20in%20addition%20to%20the%20commonly%20studied%20average%0Ageneralization%20performance.%20Sample%20insights%20of%20our%20theoretical%20results%20include%0Athat%20when%20all%20group-level%20co-variance%20is%20in%20the%20medium%20regime%20and%20all%20mean%20are%0Aclose%20to%20zero%2C%20the%20learning%20performance%20is%20most%20desirable%20in%20the%20sense%20of%20a%0Asmall%20sample%20complexity%2C%20a%20fast%20training%20rate%2C%20and%20a%20high%20average%20and%0Agroup-level%20testing%20accuracy.%20Moreover%2C%20we%20show%20that%20increasing%20the%20fraction%20of%0Athe%20minority%20group%20in%20the%20training%20data%20does%20not%20necessarily%20improve%20the%0Ageneralization%20performance%20of%20the%20minority%20group.%20Our%20theoretical%20results%20are%0Avalidated%20on%20both%20synthetic%20and%20empirical%20datasets%2C%20such%20as%20CelebA%20and%20CIFAR-10%0Ain%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07310v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20does%20promoting%20the%20minority%20fraction%20affect%20generalization%3F%20A%0A%20%20theoretical%20study%20of%20the%20one-hidden-layer%20neural%20network%20on%20group%20imbalance&entry.906535625=Hongkang%20Li%20and%20Shuai%20Zhang%20and%20Yihua%20Zhang%20and%20Meng%20Wang%20and%20Sijia%20Liu%20and%20Pin-Yu%20Chen&entry.1292438233=%20%20Group%20imbalance%20has%20been%20a%20known%20problem%20in%20empirical%20risk%20minimization%0A%28ERM%29%2C%20where%20the%20achieved%20high%20average%20accuracy%20is%20accompanied%20by%20low%20accuracy%0Ain%20a%20minority%20group.%20Despite%20algorithmic%20efforts%20to%20improve%20the%20minority%20group%0Aaccuracy%2C%20a%20theoretical%20generalization%20analysis%20of%20ERM%20on%20individual%20groups%0Aremains%20elusive.%20By%20formulating%20the%20group%20imbalance%20problem%20with%20the%20Gaussian%0AMixture%20Model%2C%20this%20paper%20quantifies%20the%20impact%20of%20individual%20groups%20on%20the%0Asample%20complexity%2C%20the%20convergence%20rate%2C%20and%20the%20average%20and%20group-level%0Atesting%20performance.%20Although%20our%20theoretical%20framework%20is%20centered%20on%20binary%0Aclassification%20using%20a%20one-hidden-layer%20neural%20network%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20we%20provide%20the%20first%20theoretical%20analysis%20of%20the%20group-level%0Ageneralization%20of%20ERM%20in%20addition%20to%20the%20commonly%20studied%20average%0Ageneralization%20performance.%20Sample%20insights%20of%20our%20theoretical%20results%20include%0Athat%20when%20all%20group-level%20co-variance%20is%20in%20the%20medium%20regime%20and%20all%20mean%20are%0Aclose%20to%20zero%2C%20the%20learning%20performance%20is%20most%20desirable%20in%20the%20sense%20of%20a%0Asmall%20sample%20complexity%2C%20a%20fast%20training%20rate%2C%20and%20a%20high%20average%20and%0Agroup-level%20testing%20accuracy.%20Moreover%2C%20we%20show%20that%20increasing%20the%20fraction%20of%0Athe%20minority%20group%20in%20the%20training%20data%20does%20not%20necessarily%20improve%20the%0Ageneralization%20performance%20of%20the%20minority%20group.%20Our%20theoretical%20results%20are%0Avalidated%20on%20both%20synthetic%20and%20empirical%20datasets%2C%20such%20as%20CelebA%20and%20CIFAR-10%0Ain%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07310v2&entry.124074799=Read"},
{"title": "Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation", "author": "Zonghai Yao and Ahmed Jaafar and Beining Wang and Zhichao Yang and Hong Yu", "abstract": "  This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.\n", "link": "http://arxiv.org/abs/2311.09684v2", "date": "2024-03-19", "relevancy": 1.1558, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4124}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3696}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20Physicians%20Know%20How%20to%20Prompt%3F%20The%20Need%20for%20Automatic%20Prompt%0A%20%20Optimization%20Help%20in%20Clinical%20Note%20Generation&body=Title%3A%20Do%20Physicians%20Know%20How%20to%20Prompt%3F%20The%20Need%20for%20Automatic%20Prompt%0A%20%20Optimization%20Help%20in%20Clinical%20Note%20Generation%0AAuthor%3A%20Zonghai%20Yao%20and%20Ahmed%20Jaafar%20and%20Beining%20Wang%20and%20Zhichao%20Yang%20and%20Hong%20Yu%0AAbstract%3A%20%20%20This%20study%20examines%20the%20effect%20of%20prompt%20engineering%20on%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20in%20clinical%20note%20generation.%20We%20introduce%20an%0AAutomatic%20Prompt%20Optimization%20%28APO%29%20framework%20to%20refine%20initial%20prompts%20and%0Acompare%20the%20outputs%20of%20medical%20experts%2C%20non-medical%20experts%2C%20and%20APO-enhanced%0AGPT3.5%20and%20GPT4.%20Results%20highlight%20GPT4%20APO%27s%20superior%20performance%20in%0Astandardizing%20prompt%20quality%20across%20clinical%20note%20sections.%20A%20human-in-the-loop%0Aapproach%20shows%20that%20experts%20maintain%20content%20quality%20post-APO%2C%20with%20a%0Apreference%20for%20their%20own%20modifications%2C%20suggesting%20the%20value%20of%20expert%0Acustomization.%20We%20recommend%20a%20two-phase%20optimization%20process%2C%20leveraging%0AAPO-GPT4%20for%20consistency%20and%20expert%20input%20for%20personalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09684v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Physicians%20Know%20How%20to%20Prompt%3F%20The%20Need%20for%20Automatic%20Prompt%0A%20%20Optimization%20Help%20in%20Clinical%20Note%20Generation&entry.906535625=Zonghai%20Yao%20and%20Ahmed%20Jaafar%20and%20Beining%20Wang%20and%20Zhichao%20Yang%20and%20Hong%20Yu&entry.1292438233=%20%20This%20study%20examines%20the%20effect%20of%20prompt%20engineering%20on%20the%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20in%20clinical%20note%20generation.%20We%20introduce%20an%0AAutomatic%20Prompt%20Optimization%20%28APO%29%20framework%20to%20refine%20initial%20prompts%20and%0Acompare%20the%20outputs%20of%20medical%20experts%2C%20non-medical%20experts%2C%20and%20APO-enhanced%0AGPT3.5%20and%20GPT4.%20Results%20highlight%20GPT4%20APO%27s%20superior%20performance%20in%0Astandardizing%20prompt%20quality%20across%20clinical%20note%20sections.%20A%20human-in-the-loop%0Aapproach%20shows%20that%20experts%20maintain%20content%20quality%20post-APO%2C%20with%20a%0Apreference%20for%20their%20own%20modifications%2C%20suggesting%20the%20value%20of%20expert%0Acustomization.%20We%20recommend%20a%20two-phase%20optimization%20process%2C%20leveraging%0AAPO-GPT4%20for%20consistency%20and%20expert%20input%20for%20personalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09684v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


