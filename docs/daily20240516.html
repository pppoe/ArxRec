<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240514.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Vision-Based Neurosurgical Guidance: Unsupervised Localization and\n  Camera-Pose Prediction", "author": "Gary Sarwin and Alessandro Carretta and Victor Staartjes and Matteo Zoli and Diego Mazzatenta and Luca Regli and Carlo Serra and Ender Konukoglu", "abstract": "  Localizing oneself during endoscopic procedures can be problematic due to the\nlack of distinguishable textures and landmarks, as well as difficulties due to\nthe endoscopic device such as a limited field of view and challenging lighting\nconditions. Expert knowledge shaped by years of experience is required for\nlocalization within the human body during endoscopic procedures. In this work,\nwe present a deep learning method based on anatomy recognition, that constructs\na surgical path in an unsupervised manner from surgical videos, modelling\nrelative location and variations due to different viewing angles. At inference\ntime, the model can map an unseen video's frames on the path and estimate the\nviewing angle, aiming to provide guidance, for instance, to reach a particular\ndestination. We test the method on a dataset consisting of surgical videos of\ntranssphenoidal adenomectomies, as well as on a synthetic dataset. An online\ntool that lets researchers upload their surgical videos to obtain anatomy\ndetections and the weights of the trained YOLOv7 model are available at:\nhttps://surgicalvision.bmic.ethz.ch.\n", "link": "http://arxiv.org/abs/2405.09355v1", "date": "2024-05-15", "relevancy": 3.011, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6219}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6025}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Based%20Neurosurgical%20Guidance%3A%20Unsupervised%20Localization%20and%0A%20%20Camera-Pose%20Prediction&body=Title%3A%20Vision-Based%20Neurosurgical%20Guidance%3A%20Unsupervised%20Localization%20and%0A%20%20Camera-Pose%20Prediction%0AAuthor%3A%20Gary%20Sarwin%20and%20Alessandro%20Carretta%20and%20Victor%20Staartjes%20and%20Matteo%20Zoli%20and%20Diego%20Mazzatenta%20and%20Luca%20Regli%20and%20Carlo%20Serra%20and%20Ender%20Konukoglu%0AAbstract%3A%20%20%20Localizing%20oneself%20during%20endoscopic%20procedures%20can%20be%20problematic%20due%20to%20the%0Alack%20of%20distinguishable%20textures%20and%20landmarks%2C%20as%20well%20as%20difficulties%20due%20to%0Athe%20endoscopic%20device%20such%20as%20a%20limited%20field%20of%20view%20and%20challenging%20lighting%0Aconditions.%20Expert%20knowledge%20shaped%20by%20years%20of%20experience%20is%20required%20for%0Alocalization%20within%20the%20human%20body%20during%20endoscopic%20procedures.%20In%20this%20work%2C%0Awe%20present%20a%20deep%20learning%20method%20based%20on%20anatomy%20recognition%2C%20that%20constructs%0Aa%20surgical%20path%20in%20an%20unsupervised%20manner%20from%20surgical%20videos%2C%20modelling%0Arelative%20location%20and%20variations%20due%20to%20different%20viewing%20angles.%20At%20inference%0Atime%2C%20the%20model%20can%20map%20an%20unseen%20video%27s%20frames%20on%20the%20path%20and%20estimate%20the%0Aviewing%20angle%2C%20aiming%20to%20provide%20guidance%2C%20for%20instance%2C%20to%20reach%20a%20particular%0Adestination.%20We%20test%20the%20method%20on%20a%20dataset%20consisting%20of%20surgical%20videos%20of%0Atranssphenoidal%20adenomectomies%2C%20as%20well%20as%20on%20a%20synthetic%20dataset.%20An%20online%0Atool%20that%20lets%20researchers%20upload%20their%20surgical%20videos%20to%20obtain%20anatomy%0Adetections%20and%20the%20weights%20of%20the%20trained%20YOLOv7%20model%20are%20available%20at%3A%0Ahttps%3A//surgicalvision.bmic.ethz.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Based%2520Neurosurgical%2520Guidance%253A%2520Unsupervised%2520Localization%2520and%250A%2520%2520Camera-Pose%2520Prediction%26entry.906535625%3DGary%2520Sarwin%2520and%2520Alessandro%2520Carretta%2520and%2520Victor%2520Staartjes%2520and%2520Matteo%2520Zoli%2520and%2520Diego%2520Mazzatenta%2520and%2520Luca%2520Regli%2520and%2520Carlo%2520Serra%2520and%2520Ender%2520Konukoglu%26entry.1292438233%3D%2520%2520Localizing%2520oneself%2520during%2520endoscopic%2520procedures%2520can%2520be%2520problematic%2520due%2520to%2520the%250Alack%2520of%2520distinguishable%2520textures%2520and%2520landmarks%252C%2520as%2520well%2520as%2520difficulties%2520due%2520to%250Athe%2520endoscopic%2520device%2520such%2520as%2520a%2520limited%2520field%2520of%2520view%2520and%2520challenging%2520lighting%250Aconditions.%2520Expert%2520knowledge%2520shaped%2520by%2520years%2520of%2520experience%2520is%2520required%2520for%250Alocalization%2520within%2520the%2520human%2520body%2520during%2520endoscopic%2520procedures.%2520In%2520this%2520work%252C%250Awe%2520present%2520a%2520deep%2520learning%2520method%2520based%2520on%2520anatomy%2520recognition%252C%2520that%2520constructs%250Aa%2520surgical%2520path%2520in%2520an%2520unsupervised%2520manner%2520from%2520surgical%2520videos%252C%2520modelling%250Arelative%2520location%2520and%2520variations%2520due%2520to%2520different%2520viewing%2520angles.%2520At%2520inference%250Atime%252C%2520the%2520model%2520can%2520map%2520an%2520unseen%2520video%2527s%2520frames%2520on%2520the%2520path%2520and%2520estimate%2520the%250Aviewing%2520angle%252C%2520aiming%2520to%2520provide%2520guidance%252C%2520for%2520instance%252C%2520to%2520reach%2520a%2520particular%250Adestination.%2520We%2520test%2520the%2520method%2520on%2520a%2520dataset%2520consisting%2520of%2520surgical%2520videos%2520of%250Atranssphenoidal%2520adenomectomies%252C%2520as%2520well%2520as%2520on%2520a%2520synthetic%2520dataset.%2520An%2520online%250Atool%2520that%2520lets%2520researchers%2520upload%2520their%2520surgical%2520videos%2520to%2520obtain%2520anatomy%250Adetections%2520and%2520the%2520weights%2520of%2520the%2520trained%2520YOLOv7%2520model%2520are%2520available%2520at%253A%250Ahttps%253A//surgicalvision.bmic.ethz.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Based%20Neurosurgical%20Guidance%3A%20Unsupervised%20Localization%20and%0A%20%20Camera-Pose%20Prediction&entry.906535625=Gary%20Sarwin%20and%20Alessandro%20Carretta%20and%20Victor%20Staartjes%20and%20Matteo%20Zoli%20and%20Diego%20Mazzatenta%20and%20Luca%20Regli%20and%20Carlo%20Serra%20and%20Ender%20Konukoglu&entry.1292438233=%20%20Localizing%20oneself%20during%20endoscopic%20procedures%20can%20be%20problematic%20due%20to%20the%0Alack%20of%20distinguishable%20textures%20and%20landmarks%2C%20as%20well%20as%20difficulties%20due%20to%0Athe%20endoscopic%20device%20such%20as%20a%20limited%20field%20of%20view%20and%20challenging%20lighting%0Aconditions.%20Expert%20knowledge%20shaped%20by%20years%20of%20experience%20is%20required%20for%0Alocalization%20within%20the%20human%20body%20during%20endoscopic%20procedures.%20In%20this%20work%2C%0Awe%20present%20a%20deep%20learning%20method%20based%20on%20anatomy%20recognition%2C%20that%20constructs%0Aa%20surgical%20path%20in%20an%20unsupervised%20manner%20from%20surgical%20videos%2C%20modelling%0Arelative%20location%20and%20variations%20due%20to%20different%20viewing%20angles.%20At%20inference%0Atime%2C%20the%20model%20can%20map%20an%20unseen%20video%27s%20frames%20on%20the%20path%20and%20estimate%20the%0Aviewing%20angle%2C%20aiming%20to%20provide%20guidance%2C%20for%20instance%2C%20to%20reach%20a%20particular%0Adestination.%20We%20test%20the%20method%20on%20a%20dataset%20consisting%20of%20surgical%20videos%20of%0Atranssphenoidal%20adenomectomies%2C%20as%20well%20as%20on%20a%20synthetic%20dataset.%20An%20online%0Atool%20that%20lets%20researchers%20upload%20their%20surgical%20videos%20to%20obtain%20anatomy%0Adetections%20and%20the%20weights%20of%20the%20trained%20YOLOv7%20model%20are%20available%20at%3A%0Ahttps%3A//surgicalvision.bmic.ethz.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09355v1&entry.124074799=Read"},
{"title": "Large coordinate kernel attention network for lightweight image\n  super-resolution", "author": "Fangwei Hao and Jiesheng Wu and Haotian Lu and Ji Du and Jing Xu", "abstract": "  The multi-scale receptive field and large kernel attention (LKA) module have\nbeen shown to significantly improve performance in the lightweight image\nsuper-resolution task. However, existing lightweight super-resolution (SR)\nmethods seldom pay attention to designing efficient building block with\nmulti-scale receptive field for local modeling, and their LKA modules face a\nquadratic increase in computational and memory footprints as the convolutional\nkernel size increases. To address the first issue, we propose the multi-scale\nblueprint separable convolutions (MBSConv) as highly efficient building block\nwith multi-scale receptive field, it can focus on the learning for the\nmulti-scale information which is a vital component of discriminative\nrepresentation. As for the second issue, we revisit the key properties of LKA\nin which we find that the adjacent direct interaction of local information and\nlong-distance dependencies is crucial to provide remarkable performance. Thus,\ntaking this into account and in order to mitigate the complexity of LKA, we\npropose a large coordinate kernel attention (LCKA) module which decomposes the\n2D convolutional kernels of the depth-wise convolutional layers in LKA into\nhorizontal and vertical 1-D kernels. LCKA enables the adjacent direct\ninteraction of local information and long-distance dependencies not only in the\nhorizontal direction but also in the vertical. Besides, LCKA allows for the\ndirect use of extremely large kernels in the depth-wise convolutional layers to\ncapture more contextual information, which helps to significantly improve the\nreconstruction performance, and it incurs lower computational complexity and\nmemory footprints. Integrating MBSConv and LCKA, we propose a large coordinate\nkernel attention network (LCAN).\n", "link": "http://arxiv.org/abs/2405.09353v1", "date": "2024-05-15", "relevancy": 2.7545, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5819}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5366}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution&body=Title%3A%20Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution%0AAuthor%3A%20Fangwei%20Hao%20and%20Jiesheng%20Wu%20and%20Haotian%20Lu%20and%20Ji%20Du%20and%20Jing%20Xu%0AAbstract%3A%20%20%20The%20multi-scale%20receptive%20field%20and%20large%20kernel%20attention%20%28LKA%29%20module%20have%0Abeen%20shown%20to%20significantly%20improve%20performance%20in%20the%20lightweight%20image%0Asuper-resolution%20task.%20However%2C%20existing%20lightweight%20super-resolution%20%28SR%29%0Amethods%20seldom%20pay%20attention%20to%20designing%20efficient%20building%20block%20with%0Amulti-scale%20receptive%20field%20for%20local%20modeling%2C%20and%20their%20LKA%20modules%20face%20a%0Aquadratic%20increase%20in%20computational%20and%20memory%20footprints%20as%20the%20convolutional%0Akernel%20size%20increases.%20To%20address%20the%20first%20issue%2C%20we%20propose%20the%20multi-scale%0Ablueprint%20separable%20convolutions%20%28MBSConv%29%20as%20highly%20efficient%20building%20block%0Awith%20multi-scale%20receptive%20field%2C%20it%20can%20focus%20on%20the%20learning%20for%20the%0Amulti-scale%20information%20which%20is%20a%20vital%20component%20of%20discriminative%0Arepresentation.%20As%20for%20the%20second%20issue%2C%20we%20revisit%20the%20key%20properties%20of%20LKA%0Ain%20which%20we%20find%20that%20the%20adjacent%20direct%20interaction%20of%20local%20information%20and%0Along-distance%20dependencies%20is%20crucial%20to%20provide%20remarkable%20performance.%20Thus%2C%0Ataking%20this%20into%20account%20and%20in%20order%20to%20mitigate%20the%20complexity%20of%20LKA%2C%20we%0Apropose%20a%20large%20coordinate%20kernel%20attention%20%28LCKA%29%20module%20which%20decomposes%20the%0A2D%20convolutional%20kernels%20of%20the%20depth-wise%20convolutional%20layers%20in%20LKA%20into%0Ahorizontal%20and%20vertical%201-D%20kernels.%20LCKA%20enables%20the%20adjacent%20direct%0Ainteraction%20of%20local%20information%20and%20long-distance%20dependencies%20not%20only%20in%20the%0Ahorizontal%20direction%20but%20also%20in%20the%20vertical.%20Besides%2C%20LCKA%20allows%20for%20the%0Adirect%20use%20of%20extremely%20large%20kernels%20in%20the%20depth-wise%20convolutional%20layers%20to%0Acapture%20more%20contextual%20information%2C%20which%20helps%20to%20significantly%20improve%20the%0Areconstruction%20performance%2C%20and%20it%20incurs%20lower%20computational%20complexity%20and%0Amemory%20footprints.%20Integrating%20MBSConv%20and%20LCKA%2C%20we%20propose%20a%20large%20coordinate%0Akernel%20attention%20network%20%28LCAN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520coordinate%2520kernel%2520attention%2520network%2520for%2520lightweight%2520image%250A%2520%2520super-resolution%26entry.906535625%3DFangwei%2520Hao%2520and%2520Jiesheng%2520Wu%2520and%2520Haotian%2520Lu%2520and%2520Ji%2520Du%2520and%2520Jing%2520Xu%26entry.1292438233%3D%2520%2520The%2520multi-scale%2520receptive%2520field%2520and%2520large%2520kernel%2520attention%2520%2528LKA%2529%2520module%2520have%250Abeen%2520shown%2520to%2520significantly%2520improve%2520performance%2520in%2520the%2520lightweight%2520image%250Asuper-resolution%2520task.%2520However%252C%2520existing%2520lightweight%2520super-resolution%2520%2528SR%2529%250Amethods%2520seldom%2520pay%2520attention%2520to%2520designing%2520efficient%2520building%2520block%2520with%250Amulti-scale%2520receptive%2520field%2520for%2520local%2520modeling%252C%2520and%2520their%2520LKA%2520modules%2520face%2520a%250Aquadratic%2520increase%2520in%2520computational%2520and%2520memory%2520footprints%2520as%2520the%2520convolutional%250Akernel%2520size%2520increases.%2520To%2520address%2520the%2520first%2520issue%252C%2520we%2520propose%2520the%2520multi-scale%250Ablueprint%2520separable%2520convolutions%2520%2528MBSConv%2529%2520as%2520highly%2520efficient%2520building%2520block%250Awith%2520multi-scale%2520receptive%2520field%252C%2520it%2520can%2520focus%2520on%2520the%2520learning%2520for%2520the%250Amulti-scale%2520information%2520which%2520is%2520a%2520vital%2520component%2520of%2520discriminative%250Arepresentation.%2520As%2520for%2520the%2520second%2520issue%252C%2520we%2520revisit%2520the%2520key%2520properties%2520of%2520LKA%250Ain%2520which%2520we%2520find%2520that%2520the%2520adjacent%2520direct%2520interaction%2520of%2520local%2520information%2520and%250Along-distance%2520dependencies%2520is%2520crucial%2520to%2520provide%2520remarkable%2520performance.%2520Thus%252C%250Ataking%2520this%2520into%2520account%2520and%2520in%2520order%2520to%2520mitigate%2520the%2520complexity%2520of%2520LKA%252C%2520we%250Apropose%2520a%2520large%2520coordinate%2520kernel%2520attention%2520%2528LCKA%2529%2520module%2520which%2520decomposes%2520the%250A2D%2520convolutional%2520kernels%2520of%2520the%2520depth-wise%2520convolutional%2520layers%2520in%2520LKA%2520into%250Ahorizontal%2520and%2520vertical%25201-D%2520kernels.%2520LCKA%2520enables%2520the%2520adjacent%2520direct%250Ainteraction%2520of%2520local%2520information%2520and%2520long-distance%2520dependencies%2520not%2520only%2520in%2520the%250Ahorizontal%2520direction%2520but%2520also%2520in%2520the%2520vertical.%2520Besides%252C%2520LCKA%2520allows%2520for%2520the%250Adirect%2520use%2520of%2520extremely%2520large%2520kernels%2520in%2520the%2520depth-wise%2520convolutional%2520layers%2520to%250Acapture%2520more%2520contextual%2520information%252C%2520which%2520helps%2520to%2520significantly%2520improve%2520the%250Areconstruction%2520performance%252C%2520and%2520it%2520incurs%2520lower%2520computational%2520complexity%2520and%250Amemory%2520footprints.%2520Integrating%2520MBSConv%2520and%2520LCKA%252C%2520we%2520propose%2520a%2520large%2520coordinate%250Akernel%2520attention%2520network%2520%2528LCAN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20coordinate%20kernel%20attention%20network%20for%20lightweight%20image%0A%20%20super-resolution&entry.906535625=Fangwei%20Hao%20and%20Jiesheng%20Wu%20and%20Haotian%20Lu%20and%20Ji%20Du%20and%20Jing%20Xu&entry.1292438233=%20%20The%20multi-scale%20receptive%20field%20and%20large%20kernel%20attention%20%28LKA%29%20module%20have%0Abeen%20shown%20to%20significantly%20improve%20performance%20in%20the%20lightweight%20image%0Asuper-resolution%20task.%20However%2C%20existing%20lightweight%20super-resolution%20%28SR%29%0Amethods%20seldom%20pay%20attention%20to%20designing%20efficient%20building%20block%20with%0Amulti-scale%20receptive%20field%20for%20local%20modeling%2C%20and%20their%20LKA%20modules%20face%20a%0Aquadratic%20increase%20in%20computational%20and%20memory%20footprints%20as%20the%20convolutional%0Akernel%20size%20increases.%20To%20address%20the%20first%20issue%2C%20we%20propose%20the%20multi-scale%0Ablueprint%20separable%20convolutions%20%28MBSConv%29%20as%20highly%20efficient%20building%20block%0Awith%20multi-scale%20receptive%20field%2C%20it%20can%20focus%20on%20the%20learning%20for%20the%0Amulti-scale%20information%20which%20is%20a%20vital%20component%20of%20discriminative%0Arepresentation.%20As%20for%20the%20second%20issue%2C%20we%20revisit%20the%20key%20properties%20of%20LKA%0Ain%20which%20we%20find%20that%20the%20adjacent%20direct%20interaction%20of%20local%20information%20and%0Along-distance%20dependencies%20is%20crucial%20to%20provide%20remarkable%20performance.%20Thus%2C%0Ataking%20this%20into%20account%20and%20in%20order%20to%20mitigate%20the%20complexity%20of%20LKA%2C%20we%0Apropose%20a%20large%20coordinate%20kernel%20attention%20%28LCKA%29%20module%20which%20decomposes%20the%0A2D%20convolutional%20kernels%20of%20the%20depth-wise%20convolutional%20layers%20in%20LKA%20into%0Ahorizontal%20and%20vertical%201-D%20kernels.%20LCKA%20enables%20the%20adjacent%20direct%0Ainteraction%20of%20local%20information%20and%20long-distance%20dependencies%20not%20only%20in%20the%0Ahorizontal%20direction%20but%20also%20in%20the%20vertical.%20Besides%2C%20LCKA%20allows%20for%20the%0Adirect%20use%20of%20extremely%20large%20kernels%20in%20the%20depth-wise%20convolutional%20layers%20to%0Acapture%20more%20contextual%20information%2C%20which%20helps%20to%20significantly%20improve%20the%0Areconstruction%20performance%2C%20and%20it%20incurs%20lower%20computational%20complexity%20and%0Amemory%20footprints.%20Integrating%20MBSConv%20and%20LCKA%2C%20we%20propose%20a%20large%20coordinate%0Akernel%20attention%20network%20%28LCAN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09353v1&entry.124074799=Read"},
{"title": "VascularPilot3D: Toward a 3D fully autonomous navigation for\n  endovascular robotics", "author": "Song Jingwei and Yang Keke and Chen Han and Liu Jiayi and Gu Yinan and Hui Qianxin and Huang Yanqi and Li Meng and Zhang Zheng and Cao Tuoyu and Ghaffari Maani", "abstract": "  This research reports VascularPilot3D, the first 3D fully autonomous\nendovascular robot navigation system. As an exploration toward autonomous\nguidewire navigation, VascularPilot3D is developed as a complete navigation\nsystem based on intra-operative imaging systems (fluoroscopic X-ray in this\nstudy) and typical endovascular robots. VascularPilot3D adopts previously\nresearched fast 3D-2D vessel registration algorithms and guidewire segmentation\nmethods as its perception modules. We additionally propose three modules: a\ntopology-constrained 2D-3D instrument end-point lifting method, a tree-based\nfast path planning algorithm, and a prior-free endovascular navigation\nstrategy. VascularPilot3D is compatible with most mainstream endovascular\nrobots. Ex-vivo experiments validate that VascularPilot3D achieves 100% success\nrate among 25 trials. It reduces the human surgeon's overall control loops by\n18.38%. VascularPilot3D is promising for general clinical autonomous\nendovascular navigations.\n", "link": "http://arxiv.org/abs/2405.09375v1", "date": "2024-05-15", "relevancy": 2.6514, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6009}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VascularPilot3D%3A%20Toward%20a%203D%20fully%20autonomous%20navigation%20for%0A%20%20endovascular%20robotics&body=Title%3A%20VascularPilot3D%3A%20Toward%20a%203D%20fully%20autonomous%20navigation%20for%0A%20%20endovascular%20robotics%0AAuthor%3A%20Song%20Jingwei%20and%20Yang%20Keke%20and%20Chen%20Han%20and%20Liu%20Jiayi%20and%20Gu%20Yinan%20and%20Hui%20Qianxin%20and%20Huang%20Yanqi%20and%20Li%20Meng%20and%20Zhang%20Zheng%20and%20Cao%20Tuoyu%20and%20Ghaffari%20Maani%0AAbstract%3A%20%20%20This%20research%20reports%20VascularPilot3D%2C%20the%20first%203D%20fully%20autonomous%0Aendovascular%20robot%20navigation%20system.%20As%20an%20exploration%20toward%20autonomous%0Aguidewire%20navigation%2C%20VascularPilot3D%20is%20developed%20as%20a%20complete%20navigation%0Asystem%20based%20on%20intra-operative%20imaging%20systems%20%28fluoroscopic%20X-ray%20in%20this%0Astudy%29%20and%20typical%20endovascular%20robots.%20VascularPilot3D%20adopts%20previously%0Aresearched%20fast%203D-2D%20vessel%20registration%20algorithms%20and%20guidewire%20segmentation%0Amethods%20as%20its%20perception%20modules.%20We%20additionally%20propose%20three%20modules%3A%20a%0Atopology-constrained%202D-3D%20instrument%20end-point%20lifting%20method%2C%20a%20tree-based%0Afast%20path%20planning%20algorithm%2C%20and%20a%20prior-free%20endovascular%20navigation%0Astrategy.%20VascularPilot3D%20is%20compatible%20with%20most%20mainstream%20endovascular%0Arobots.%20Ex-vivo%20experiments%20validate%20that%20VascularPilot3D%20achieves%20100%25%20success%0Arate%20among%2025%20trials.%20It%20reduces%20the%20human%20surgeon%27s%20overall%20control%20loops%20by%0A18.38%25.%20VascularPilot3D%20is%20promising%20for%20general%20clinical%20autonomous%0Aendovascular%20navigations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVascularPilot3D%253A%2520Toward%2520a%25203D%2520fully%2520autonomous%2520navigation%2520for%250A%2520%2520endovascular%2520robotics%26entry.906535625%3DSong%2520Jingwei%2520and%2520Yang%2520Keke%2520and%2520Chen%2520Han%2520and%2520Liu%2520Jiayi%2520and%2520Gu%2520Yinan%2520and%2520Hui%2520Qianxin%2520and%2520Huang%2520Yanqi%2520and%2520Li%2520Meng%2520and%2520Zhang%2520Zheng%2520and%2520Cao%2520Tuoyu%2520and%2520Ghaffari%2520Maani%26entry.1292438233%3D%2520%2520This%2520research%2520reports%2520VascularPilot3D%252C%2520the%2520first%25203D%2520fully%2520autonomous%250Aendovascular%2520robot%2520navigation%2520system.%2520As%2520an%2520exploration%2520toward%2520autonomous%250Aguidewire%2520navigation%252C%2520VascularPilot3D%2520is%2520developed%2520as%2520a%2520complete%2520navigation%250Asystem%2520based%2520on%2520intra-operative%2520imaging%2520systems%2520%2528fluoroscopic%2520X-ray%2520in%2520this%250Astudy%2529%2520and%2520typical%2520endovascular%2520robots.%2520VascularPilot3D%2520adopts%2520previously%250Aresearched%2520fast%25203D-2D%2520vessel%2520registration%2520algorithms%2520and%2520guidewire%2520segmentation%250Amethods%2520as%2520its%2520perception%2520modules.%2520We%2520additionally%2520propose%2520three%2520modules%253A%2520a%250Atopology-constrained%25202D-3D%2520instrument%2520end-point%2520lifting%2520method%252C%2520a%2520tree-based%250Afast%2520path%2520planning%2520algorithm%252C%2520and%2520a%2520prior-free%2520endovascular%2520navigation%250Astrategy.%2520VascularPilot3D%2520is%2520compatible%2520with%2520most%2520mainstream%2520endovascular%250Arobots.%2520Ex-vivo%2520experiments%2520validate%2520that%2520VascularPilot3D%2520achieves%2520100%2525%2520success%250Arate%2520among%252025%2520trials.%2520It%2520reduces%2520the%2520human%2520surgeon%2527s%2520overall%2520control%2520loops%2520by%250A18.38%2525.%2520VascularPilot3D%2520is%2520promising%2520for%2520general%2520clinical%2520autonomous%250Aendovascular%2520navigations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VascularPilot3D%3A%20Toward%20a%203D%20fully%20autonomous%20navigation%20for%0A%20%20endovascular%20robotics&entry.906535625=Song%20Jingwei%20and%20Yang%20Keke%20and%20Chen%20Han%20and%20Liu%20Jiayi%20and%20Gu%20Yinan%20and%20Hui%20Qianxin%20and%20Huang%20Yanqi%20and%20Li%20Meng%20and%20Zhang%20Zheng%20and%20Cao%20Tuoyu%20and%20Ghaffari%20Maani&entry.1292438233=%20%20This%20research%20reports%20VascularPilot3D%2C%20the%20first%203D%20fully%20autonomous%0Aendovascular%20robot%20navigation%20system.%20As%20an%20exploration%20toward%20autonomous%0Aguidewire%20navigation%2C%20VascularPilot3D%20is%20developed%20as%20a%20complete%20navigation%0Asystem%20based%20on%20intra-operative%20imaging%20systems%20%28fluoroscopic%20X-ray%20in%20this%0Astudy%29%20and%20typical%20endovascular%20robots.%20VascularPilot3D%20adopts%20previously%0Aresearched%20fast%203D-2D%20vessel%20registration%20algorithms%20and%20guidewire%20segmentation%0Amethods%20as%20its%20perception%20modules.%20We%20additionally%20propose%20three%20modules%3A%20a%0Atopology-constrained%202D-3D%20instrument%20end-point%20lifting%20method%2C%20a%20tree-based%0Afast%20path%20planning%20algorithm%2C%20and%20a%20prior-free%20endovascular%20navigation%0Astrategy.%20VascularPilot3D%20is%20compatible%20with%20most%20mainstream%20endovascular%0Arobots.%20Ex-vivo%20experiments%20validate%20that%20VascularPilot3D%20achieves%20100%25%20success%0Arate%20among%2025%20trials.%20It%20reduces%20the%20human%20surgeon%27s%20overall%20control%20loops%20by%0A18.38%25.%20VascularPilot3D%20is%20promising%20for%20general%20clinical%20autonomous%0Aendovascular%20navigations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09375v1&entry.124074799=Read"},
{"title": "Color Space Learning for Cross-Color Person Re-Identification", "author": "Jiahao Nie and Shan Lin and Alex C. Kot", "abstract": "  The primary color profile of the same identity is assumed to remain\nconsistent in typical Person Re-identification (Person ReID) tasks. However,\nthis assumption may be invalid in real-world situations and images hold variant\ncolor profiles, because of cross-modality cameras or identity with different\nclothing. To address this issue, we propose Color Space Learning (CSL) for\nthose Cross-Color Person ReID problems. Specifically, CSL guides the model to\nbe less color-sensitive with two modules: Image-level Color-Augmentation and\nPixel-level Color-Transformation. The first module increases the color\ndiversity of the inputs and guides the model to focus more on the non-color\ninformation. The second module projects every pixel of input images onto a new\ncolor space. In addition, we introduce a new Person ReID benchmark across RGB\nand Infrared modalities, NTU-Corridor, which is the first with privacy\nagreements from all participants. To evaluate the effectiveness and robustness\nof our proposed CSL, we evaluate it on several Cross-Color Person ReID\nbenchmarks. Our method surpasses the state-of-the-art methods consistently. The\ncode and benchmark are available at: https://github.com/niejiahao1998/CSL\n", "link": "http://arxiv.org/abs/2405.09487v1", "date": "2024-05-15", "relevancy": 2.5962, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5441}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5071}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Space%20Learning%20for%20Cross-Color%20Person%20Re-Identification&body=Title%3A%20Color%20Space%20Learning%20for%20Cross-Color%20Person%20Re-Identification%0AAuthor%3A%20Jiahao%20Nie%20and%20Shan%20Lin%20and%20Alex%20C.%20Kot%0AAbstract%3A%20%20%20The%20primary%20color%20profile%20of%20the%20same%20identity%20is%20assumed%20to%20remain%0Aconsistent%20in%20typical%20Person%20Re-identification%20%28Person%20ReID%29%20tasks.%20However%2C%0Athis%20assumption%20may%20be%20invalid%20in%20real-world%20situations%20and%20images%20hold%20variant%0Acolor%20profiles%2C%20because%20of%20cross-modality%20cameras%20or%20identity%20with%20different%0Aclothing.%20To%20address%20this%20issue%2C%20we%20propose%20Color%20Space%20Learning%20%28CSL%29%20for%0Athose%20Cross-Color%20Person%20ReID%20problems.%20Specifically%2C%20CSL%20guides%20the%20model%20to%0Abe%20less%20color-sensitive%20with%20two%20modules%3A%20Image-level%20Color-Augmentation%20and%0APixel-level%20Color-Transformation.%20The%20first%20module%20increases%20the%20color%0Adiversity%20of%20the%20inputs%20and%20guides%20the%20model%20to%20focus%20more%20on%20the%20non-color%0Ainformation.%20The%20second%20module%20projects%20every%20pixel%20of%20input%20images%20onto%20a%20new%0Acolor%20space.%20In%20addition%2C%20we%20introduce%20a%20new%20Person%20ReID%20benchmark%20across%20RGB%0Aand%20Infrared%20modalities%2C%20NTU-Corridor%2C%20which%20is%20the%20first%20with%20privacy%0Aagreements%20from%20all%20participants.%20To%20evaluate%20the%20effectiveness%20and%20robustness%0Aof%20our%20proposed%20CSL%2C%20we%20evaluate%20it%20on%20several%20Cross-Color%20Person%20ReID%0Abenchmarks.%20Our%20method%20surpasses%20the%20state-of-the-art%20methods%20consistently.%20The%0Acode%20and%20benchmark%20are%20available%20at%3A%20https%3A//github.com/niejiahao1998/CSL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Space%2520Learning%2520for%2520Cross-Color%2520Person%2520Re-Identification%26entry.906535625%3DJiahao%2520Nie%2520and%2520Shan%2520Lin%2520and%2520Alex%2520C.%2520Kot%26entry.1292438233%3D%2520%2520The%2520primary%2520color%2520profile%2520of%2520the%2520same%2520identity%2520is%2520assumed%2520to%2520remain%250Aconsistent%2520in%2520typical%2520Person%2520Re-identification%2520%2528Person%2520ReID%2529%2520tasks.%2520However%252C%250Athis%2520assumption%2520may%2520be%2520invalid%2520in%2520real-world%2520situations%2520and%2520images%2520hold%2520variant%250Acolor%2520profiles%252C%2520because%2520of%2520cross-modality%2520cameras%2520or%2520identity%2520with%2520different%250Aclothing.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Color%2520Space%2520Learning%2520%2528CSL%2529%2520for%250Athose%2520Cross-Color%2520Person%2520ReID%2520problems.%2520Specifically%252C%2520CSL%2520guides%2520the%2520model%2520to%250Abe%2520less%2520color-sensitive%2520with%2520two%2520modules%253A%2520Image-level%2520Color-Augmentation%2520and%250APixel-level%2520Color-Transformation.%2520The%2520first%2520module%2520increases%2520the%2520color%250Adiversity%2520of%2520the%2520inputs%2520and%2520guides%2520the%2520model%2520to%2520focus%2520more%2520on%2520the%2520non-color%250Ainformation.%2520The%2520second%2520module%2520projects%2520every%2520pixel%2520of%2520input%2520images%2520onto%2520a%2520new%250Acolor%2520space.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520new%2520Person%2520ReID%2520benchmark%2520across%2520RGB%250Aand%2520Infrared%2520modalities%252C%2520NTU-Corridor%252C%2520which%2520is%2520the%2520first%2520with%2520privacy%250Aagreements%2520from%2520all%2520participants.%2520To%2520evaluate%2520the%2520effectiveness%2520and%2520robustness%250Aof%2520our%2520proposed%2520CSL%252C%2520we%2520evaluate%2520it%2520on%2520several%2520Cross-Color%2520Person%2520ReID%250Abenchmarks.%2520Our%2520method%2520surpasses%2520the%2520state-of-the-art%2520methods%2520consistently.%2520The%250Acode%2520and%2520benchmark%2520are%2520available%2520at%253A%2520https%253A//github.com/niejiahao1998/CSL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Space%20Learning%20for%20Cross-Color%20Person%20Re-Identification&entry.906535625=Jiahao%20Nie%20and%20Shan%20Lin%20and%20Alex%20C.%20Kot&entry.1292438233=%20%20The%20primary%20color%20profile%20of%20the%20same%20identity%20is%20assumed%20to%20remain%0Aconsistent%20in%20typical%20Person%20Re-identification%20%28Person%20ReID%29%20tasks.%20However%2C%0Athis%20assumption%20may%20be%20invalid%20in%20real-world%20situations%20and%20images%20hold%20variant%0Acolor%20profiles%2C%20because%20of%20cross-modality%20cameras%20or%20identity%20with%20different%0Aclothing.%20To%20address%20this%20issue%2C%20we%20propose%20Color%20Space%20Learning%20%28CSL%29%20for%0Athose%20Cross-Color%20Person%20ReID%20problems.%20Specifically%2C%20CSL%20guides%20the%20model%20to%0Abe%20less%20color-sensitive%20with%20two%20modules%3A%20Image-level%20Color-Augmentation%20and%0APixel-level%20Color-Transformation.%20The%20first%20module%20increases%20the%20color%0Adiversity%20of%20the%20inputs%20and%20guides%20the%20model%20to%20focus%20more%20on%20the%20non-color%0Ainformation.%20The%20second%20module%20projects%20every%20pixel%20of%20input%20images%20onto%20a%20new%0Acolor%20space.%20In%20addition%2C%20we%20introduce%20a%20new%20Person%20ReID%20benchmark%20across%20RGB%0Aand%20Infrared%20modalities%2C%20NTU-Corridor%2C%20which%20is%20the%20first%20with%20privacy%0Aagreements%20from%20all%20participants.%20To%20evaluate%20the%20effectiveness%20and%20robustness%0Aof%20our%20proposed%20CSL%2C%20we%20evaluate%20it%20on%20several%20Cross-Color%20Person%20ReID%0Abenchmarks.%20Our%20method%20surpasses%20the%20state-of-the-art%20methods%20consistently.%20The%0Acode%20and%20benchmark%20are%20available%20at%3A%20https%3A//github.com/niejiahao1998/CSL%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09487v1&entry.124074799=Read"},
{"title": "SARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target\n  Recognition", "author": "Weijie L and Wei Yang and Yuenan Hou and Li Liu and Yongxiang Liu and Xiang Li", "abstract": "  Synthetic aperture radar (SAR) is essential in actively acquiring information\nfor Earth observation. SAR Automatic Target Recognition (ATR) focuses on\ndetecting and classifying various target categories under different image\nconditions. The current deep learning-based SAR ATR methods are typically\ndesigned for specific datasets and applications. Various target\ncharacteristics, scene background information, and sensor parameters across ATR\ndatasets challenge the generalization of those methods. This paper aims to\nachieve general SAR ATR based on a foundation model with Self-Supervised\nLearning (SSL). Our motivation is to break through the specific dataset and\ncondition limitations and obtain universal perceptual capabilities across the\ntarget, scene, and sensor. A foundation model named SARATR-X is proposed with\nthe following four aspects: pre-training dataset, model backbone, SSL, and\nevaluation task. First, we integrated 14 datasets with various target\ncategories and imaging conditions as a pre-training dataset. Second, different\nmodel backbones were discussed to find the most suitable approaches for\nremote-sensing images. Third, we applied two-stage training and SAR gradient\nfeatures to ensure the diversity and scalability of SARATR-X. Finally, SARATR-X\nhas achieved competitive and superior performance on 5 datasets with 8 task\nsettings, which shows that the foundation model can achieve universal SAR ATR.\nWe believe it is time to embrace fundamental models for SAR image\ninterpretation in the era of increasing big data.\n", "link": "http://arxiv.org/abs/2405.09365v1", "date": "2024-05-15", "relevancy": 2.5846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARATR-X%3A%20A%20Foundation%20Model%20for%20Synthetic%20Aperture%20Radar%20Images%20Target%0A%20%20Recognition&body=Title%3A%20SARATR-X%3A%20A%20Foundation%20Model%20for%20Synthetic%20Aperture%20Radar%20Images%20Target%0A%20%20Recognition%0AAuthor%3A%20Weijie%20L%20and%20Wei%20Yang%20and%20Yuenan%20Hou%20and%20Li%20Liu%20and%20Yongxiang%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Synthetic%20aperture%20radar%20%28SAR%29%20is%20essential%20in%20actively%20acquiring%20information%0Afor%20Earth%20observation.%20SAR%20Automatic%20Target%20Recognition%20%28ATR%29%20focuses%20on%0Adetecting%20and%20classifying%20various%20target%20categories%20under%20different%20image%0Aconditions.%20The%20current%20deep%20learning-based%20SAR%20ATR%20methods%20are%20typically%0Adesigned%20for%20specific%20datasets%20and%20applications.%20Various%20target%0Acharacteristics%2C%20scene%20background%20information%2C%20and%20sensor%20parameters%20across%20ATR%0Adatasets%20challenge%20the%20generalization%20of%20those%20methods.%20This%20paper%20aims%20to%0Aachieve%20general%20SAR%20ATR%20based%20on%20a%20foundation%20model%20with%20Self-Supervised%0ALearning%20%28SSL%29.%20Our%20motivation%20is%20to%20break%20through%20the%20specific%20dataset%20and%0Acondition%20limitations%20and%20obtain%20universal%20perceptual%20capabilities%20across%20the%0Atarget%2C%20scene%2C%20and%20sensor.%20A%20foundation%20model%20named%20SARATR-X%20is%20proposed%20with%0Athe%20following%20four%20aspects%3A%20pre-training%20dataset%2C%20model%20backbone%2C%20SSL%2C%20and%0Aevaluation%20task.%20First%2C%20we%20integrated%2014%20datasets%20with%20various%20target%0Acategories%20and%20imaging%20conditions%20as%20a%20pre-training%20dataset.%20Second%2C%20different%0Amodel%20backbones%20were%20discussed%20to%20find%20the%20most%20suitable%20approaches%20for%0Aremote-sensing%20images.%20Third%2C%20we%20applied%20two-stage%20training%20and%20SAR%20gradient%0Afeatures%20to%20ensure%20the%20diversity%20and%20scalability%20of%20SARATR-X.%20Finally%2C%20SARATR-X%0Ahas%20achieved%20competitive%20and%20superior%20performance%20on%205%20datasets%20with%208%20task%0Asettings%2C%20which%20shows%20that%20the%20foundation%20model%20can%20achieve%20universal%20SAR%20ATR.%0AWe%20believe%20it%20is%20time%20to%20embrace%20fundamental%20models%20for%20SAR%20image%0Ainterpretation%20in%20the%20era%20of%20increasing%20big%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARATR-X%253A%2520A%2520Foundation%2520Model%2520for%2520Synthetic%2520Aperture%2520Radar%2520Images%2520Target%250A%2520%2520Recognition%26entry.906535625%3DWeijie%2520L%2520and%2520Wei%2520Yang%2520and%2520Yuenan%2520Hou%2520and%2520Li%2520Liu%2520and%2520Yongxiang%2520Liu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520is%2520essential%2520in%2520actively%2520acquiring%2520information%250Afor%2520Earth%2520observation.%2520SAR%2520Automatic%2520Target%2520Recognition%2520%2528ATR%2529%2520focuses%2520on%250Adetecting%2520and%2520classifying%2520various%2520target%2520categories%2520under%2520different%2520image%250Aconditions.%2520The%2520current%2520deep%2520learning-based%2520SAR%2520ATR%2520methods%2520are%2520typically%250Adesigned%2520for%2520specific%2520datasets%2520and%2520applications.%2520Various%2520target%250Acharacteristics%252C%2520scene%2520background%2520information%252C%2520and%2520sensor%2520parameters%2520across%2520ATR%250Adatasets%2520challenge%2520the%2520generalization%2520of%2520those%2520methods.%2520This%2520paper%2520aims%2520to%250Aachieve%2520general%2520SAR%2520ATR%2520based%2520on%2520a%2520foundation%2520model%2520with%2520Self-Supervised%250ALearning%2520%2528SSL%2529.%2520Our%2520motivation%2520is%2520to%2520break%2520through%2520the%2520specific%2520dataset%2520and%250Acondition%2520limitations%2520and%2520obtain%2520universal%2520perceptual%2520capabilities%2520across%2520the%250Atarget%252C%2520scene%252C%2520and%2520sensor.%2520A%2520foundation%2520model%2520named%2520SARATR-X%2520is%2520proposed%2520with%250Athe%2520following%2520four%2520aspects%253A%2520pre-training%2520dataset%252C%2520model%2520backbone%252C%2520SSL%252C%2520and%250Aevaluation%2520task.%2520First%252C%2520we%2520integrated%252014%2520datasets%2520with%2520various%2520target%250Acategories%2520and%2520imaging%2520conditions%2520as%2520a%2520pre-training%2520dataset.%2520Second%252C%2520different%250Amodel%2520backbones%2520were%2520discussed%2520to%2520find%2520the%2520most%2520suitable%2520approaches%2520for%250Aremote-sensing%2520images.%2520Third%252C%2520we%2520applied%2520two-stage%2520training%2520and%2520SAR%2520gradient%250Afeatures%2520to%2520ensure%2520the%2520diversity%2520and%2520scalability%2520of%2520SARATR-X.%2520Finally%252C%2520SARATR-X%250Ahas%2520achieved%2520competitive%2520and%2520superior%2520performance%2520on%25205%2520datasets%2520with%25208%2520task%250Asettings%252C%2520which%2520shows%2520that%2520the%2520foundation%2520model%2520can%2520achieve%2520universal%2520SAR%2520ATR.%250AWe%2520believe%2520it%2520is%2520time%2520to%2520embrace%2520fundamental%2520models%2520for%2520SAR%2520image%250Ainterpretation%2520in%2520the%2520era%2520of%2520increasing%2520big%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARATR-X%3A%20A%20Foundation%20Model%20for%20Synthetic%20Aperture%20Radar%20Images%20Target%0A%20%20Recognition&entry.906535625=Weijie%20L%20and%20Wei%20Yang%20and%20Yuenan%20Hou%20and%20Li%20Liu%20and%20Yongxiang%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20Synthetic%20aperture%20radar%20%28SAR%29%20is%20essential%20in%20actively%20acquiring%20information%0Afor%20Earth%20observation.%20SAR%20Automatic%20Target%20Recognition%20%28ATR%29%20focuses%20on%0Adetecting%20and%20classifying%20various%20target%20categories%20under%20different%20image%0Aconditions.%20The%20current%20deep%20learning-based%20SAR%20ATR%20methods%20are%20typically%0Adesigned%20for%20specific%20datasets%20and%20applications.%20Various%20target%0Acharacteristics%2C%20scene%20background%20information%2C%20and%20sensor%20parameters%20across%20ATR%0Adatasets%20challenge%20the%20generalization%20of%20those%20methods.%20This%20paper%20aims%20to%0Aachieve%20general%20SAR%20ATR%20based%20on%20a%20foundation%20model%20with%20Self-Supervised%0ALearning%20%28SSL%29.%20Our%20motivation%20is%20to%20break%20through%20the%20specific%20dataset%20and%0Acondition%20limitations%20and%20obtain%20universal%20perceptual%20capabilities%20across%20the%0Atarget%2C%20scene%2C%20and%20sensor.%20A%20foundation%20model%20named%20SARATR-X%20is%20proposed%20with%0Athe%20following%20four%20aspects%3A%20pre-training%20dataset%2C%20model%20backbone%2C%20SSL%2C%20and%0Aevaluation%20task.%20First%2C%20we%20integrated%2014%20datasets%20with%20various%20target%0Acategories%20and%20imaging%20conditions%20as%20a%20pre-training%20dataset.%20Second%2C%20different%0Amodel%20backbones%20were%20discussed%20to%20find%20the%20most%20suitable%20approaches%20for%0Aremote-sensing%20images.%20Third%2C%20we%20applied%20two-stage%20training%20and%20SAR%20gradient%0Afeatures%20to%20ensure%20the%20diversity%20and%20scalability%20of%20SARATR-X.%20Finally%2C%20SARATR-X%0Ahas%20achieved%20competitive%20and%20superior%20performance%20on%205%20datasets%20with%208%20task%0Asettings%2C%20which%20shows%20that%20the%20foundation%20model%20can%20achieve%20universal%20SAR%20ATR.%0AWe%20believe%20it%20is%20time%20to%20embrace%20fundamental%20models%20for%20SAR%20image%0Ainterpretation%20in%20the%20era%20of%20increasing%20big%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09365v1&entry.124074799=Read"},
{"title": "Application of Gated Recurrent Units for CT Trajectory Optimization", "author": "Yuedong Yuan and Linda-Sophie Schneider and Andreas Maier", "abstract": "  Recent advances in computed tomography (CT) imaging, especially with\ndual-robot systems, have introduced new challenges for scan trajectory\noptimization. This paper presents a novel approach using Gated Recurrent Units\n(GRUs) to optimize CT scan trajectories. Our approach exploits the flexibility\nof robotic CT systems to select projections that enhance image quality by\nimproving resolution and contrast while reducing scan time. We focus on\ncone-beam CT and employ several projection-based metrics, including absorption,\npixel intensities, contrast-to-noise ratio, and data completeness. The GRU\nnetwork aims to minimize data redundancy and maximize completeness with a\nlimited number of projections. We validate our method using simulated data of a\ntest specimen, focusing on a specific voxel of interest. The results show that\nthe GRU-optimized scan trajectories can outperform traditional circular CT\ntrajectories in terms of image quality metrics. For the used specimen, SSIM\nimproves from 0.38 to 0.49 and CNR increases from 6.97 to 9.08. This finding\nsuggests that the application of GRU in CT scan trajectory optimization can\nlead to more efficient, cost-effective, and high-quality imaging solutions.\n", "link": "http://arxiv.org/abs/2405.09333v1", "date": "2024-05-15", "relevancy": 2.5762, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.549}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Gated%20Recurrent%20Units%20for%20CT%20Trajectory%20Optimization&body=Title%3A%20Application%20of%20Gated%20Recurrent%20Units%20for%20CT%20Trajectory%20Optimization%0AAuthor%3A%20Yuedong%20Yuan%20and%20Linda-Sophie%20Schneider%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Recent%20advances%20in%20computed%20tomography%20%28CT%29%20imaging%2C%20especially%20with%0Adual-robot%20systems%2C%20have%20introduced%20new%20challenges%20for%20scan%20trajectory%0Aoptimization.%20This%20paper%20presents%20a%20novel%20approach%20using%20Gated%20Recurrent%20Units%0A%28GRUs%29%20to%20optimize%20CT%20scan%20trajectories.%20Our%20approach%20exploits%20the%20flexibility%0Aof%20robotic%20CT%20systems%20to%20select%20projections%20that%20enhance%20image%20quality%20by%0Aimproving%20resolution%20and%20contrast%20while%20reducing%20scan%20time.%20We%20focus%20on%0Acone-beam%20CT%20and%20employ%20several%20projection-based%20metrics%2C%20including%20absorption%2C%0Apixel%20intensities%2C%20contrast-to-noise%20ratio%2C%20and%20data%20completeness.%20The%20GRU%0Anetwork%20aims%20to%20minimize%20data%20redundancy%20and%20maximize%20completeness%20with%20a%0Alimited%20number%20of%20projections.%20We%20validate%20our%20method%20using%20simulated%20data%20of%20a%0Atest%20specimen%2C%20focusing%20on%20a%20specific%20voxel%20of%20interest.%20The%20results%20show%20that%0Athe%20GRU-optimized%20scan%20trajectories%20can%20outperform%20traditional%20circular%20CT%0Atrajectories%20in%20terms%20of%20image%20quality%20metrics.%20For%20the%20used%20specimen%2C%20SSIM%0Aimproves%20from%200.38%20to%200.49%20and%20CNR%20increases%20from%206.97%20to%209.08.%20This%20finding%0Asuggests%20that%20the%20application%20of%20GRU%20in%20CT%20scan%20trajectory%20optimization%20can%0Alead%20to%20more%20efficient%2C%20cost-effective%2C%20and%20high-quality%20imaging%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Gated%2520Recurrent%2520Units%2520for%2520CT%2520Trajectory%2520Optimization%26entry.906535625%3DYuedong%2520Yuan%2520and%2520Linda-Sophie%2520Schneider%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520computed%2520tomography%2520%2528CT%2529%2520imaging%252C%2520especially%2520with%250Adual-robot%2520systems%252C%2520have%2520introduced%2520new%2520challenges%2520for%2520scan%2520trajectory%250Aoptimization.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520using%2520Gated%2520Recurrent%2520Units%250A%2528GRUs%2529%2520to%2520optimize%2520CT%2520scan%2520trajectories.%2520Our%2520approach%2520exploits%2520the%2520flexibility%250Aof%2520robotic%2520CT%2520systems%2520to%2520select%2520projections%2520that%2520enhance%2520image%2520quality%2520by%250Aimproving%2520resolution%2520and%2520contrast%2520while%2520reducing%2520scan%2520time.%2520We%2520focus%2520on%250Acone-beam%2520CT%2520and%2520employ%2520several%2520projection-based%2520metrics%252C%2520including%2520absorption%252C%250Apixel%2520intensities%252C%2520contrast-to-noise%2520ratio%252C%2520and%2520data%2520completeness.%2520The%2520GRU%250Anetwork%2520aims%2520to%2520minimize%2520data%2520redundancy%2520and%2520maximize%2520completeness%2520with%2520a%250Alimited%2520number%2520of%2520projections.%2520We%2520validate%2520our%2520method%2520using%2520simulated%2520data%2520of%2520a%250Atest%2520specimen%252C%2520focusing%2520on%2520a%2520specific%2520voxel%2520of%2520interest.%2520The%2520results%2520show%2520that%250Athe%2520GRU-optimized%2520scan%2520trajectories%2520can%2520outperform%2520traditional%2520circular%2520CT%250Atrajectories%2520in%2520terms%2520of%2520image%2520quality%2520metrics.%2520For%2520the%2520used%2520specimen%252C%2520SSIM%250Aimproves%2520from%25200.38%2520to%25200.49%2520and%2520CNR%2520increases%2520from%25206.97%2520to%25209.08.%2520This%2520finding%250Asuggests%2520that%2520the%2520application%2520of%2520GRU%2520in%2520CT%2520scan%2520trajectory%2520optimization%2520can%250Alead%2520to%2520more%2520efficient%252C%2520cost-effective%252C%2520and%2520high-quality%2520imaging%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Gated%20Recurrent%20Units%20for%20CT%20Trajectory%20Optimization&entry.906535625=Yuedong%20Yuan%20and%20Linda-Sophie%20Schneider%20and%20Andreas%20Maier&entry.1292438233=%20%20Recent%20advances%20in%20computed%20tomography%20%28CT%29%20imaging%2C%20especially%20with%0Adual-robot%20systems%2C%20have%20introduced%20new%20challenges%20for%20scan%20trajectory%0Aoptimization.%20This%20paper%20presents%20a%20novel%20approach%20using%20Gated%20Recurrent%20Units%0A%28GRUs%29%20to%20optimize%20CT%20scan%20trajectories.%20Our%20approach%20exploits%20the%20flexibility%0Aof%20robotic%20CT%20systems%20to%20select%20projections%20that%20enhance%20image%20quality%20by%0Aimproving%20resolution%20and%20contrast%20while%20reducing%20scan%20time.%20We%20focus%20on%0Acone-beam%20CT%20and%20employ%20several%20projection-based%20metrics%2C%20including%20absorption%2C%0Apixel%20intensities%2C%20contrast-to-noise%20ratio%2C%20and%20data%20completeness.%20The%20GRU%0Anetwork%20aims%20to%20minimize%20data%20redundancy%20and%20maximize%20completeness%20with%20a%0Alimited%20number%20of%20projections.%20We%20validate%20our%20method%20using%20simulated%20data%20of%20a%0Atest%20specimen%2C%20focusing%20on%20a%20specific%20voxel%20of%20interest.%20The%20results%20show%20that%0Athe%20GRU-optimized%20scan%20trajectories%20can%20outperform%20traditional%20circular%20CT%0Atrajectories%20in%20terms%20of%20image%20quality%20metrics.%20For%20the%20used%20specimen%2C%20SSIM%0Aimproves%20from%200.38%20to%200.49%20and%20CNR%20increases%20from%206.97%20to%209.08.%20This%20finding%0Asuggests%20that%20the%20application%20of%20GRU%20in%20CT%20scan%20trajectory%20optimization%20can%0Alead%20to%20more%20efficient%2C%20cost-effective%2C%20and%20high-quality%20imaging%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09333v1&entry.124074799=Read"},
{"title": "Restoring balance: principled under/oversampling of data for optimal\n  classification", "author": "Emanuele Loffredo and Mauro Pastore and Simona Cocco and R\u00e9mi Monasson", "abstract": "  Class imbalance in real-world data poses a common bottleneck for machine\nlearning tasks, since achieving good generalization on under-represented\nexamples is often challenging. Mitigation strategies, such as under or\noversampling the data depending on their abundances, are routinely proposed and\ntested empirically, but how they should adapt to the data statistics remains\npoorly understood. In this work, we determine exact analytical expressions of\nthe generalization curves in the high-dimensional regime for linear classifiers\n(Support Vector Machines). We also provide a sharp prediction of the effects of\nunder/oversampling strategies depending on class imbalance, first and second\nmoments of the data, and the metrics of performance considered. We show that\nmixed strategies involving under and oversampling of data lead to performance\nimprovement. Through numerical experiments, we show the relevance of our\ntheoretical predictions on real datasets, on deeper architectures and with\nsampling strategies based on unsupervised probabilistic models.\n", "link": "http://arxiv.org/abs/2405.09535v1", "date": "2024-05-15", "relevancy": 2.5426, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification&body=Title%3A%20Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification%0AAuthor%3A%20Emanuele%20Loffredo%20and%20Mauro%20Pastore%20and%20Simona%20Cocco%20and%20R%C3%A9mi%20Monasson%0AAbstract%3A%20%20%20Class%20imbalance%20in%20real-world%20data%20poses%20a%20common%20bottleneck%20for%20machine%0Alearning%20tasks%2C%20since%20achieving%20good%20generalization%20on%20under-represented%0Aexamples%20is%20often%20challenging.%20Mitigation%20strategies%2C%20such%20as%20under%20or%0Aoversampling%20the%20data%20depending%20on%20their%20abundances%2C%20are%20routinely%20proposed%20and%0Atested%20empirically%2C%20but%20how%20they%20should%20adapt%20to%20the%20data%20statistics%20remains%0Apoorly%20understood.%20In%20this%20work%2C%20we%20determine%20exact%20analytical%20expressions%20of%0Athe%20generalization%20curves%20in%20the%20high-dimensional%20regime%20for%20linear%20classifiers%0A%28Support%20Vector%20Machines%29.%20We%20also%20provide%20a%20sharp%20prediction%20of%20the%20effects%20of%0Aunder/oversampling%20strategies%20depending%20on%20class%20imbalance%2C%20first%20and%20second%0Amoments%20of%20the%20data%2C%20and%20the%20metrics%20of%20performance%20considered.%20We%20show%20that%0Amixed%20strategies%20involving%20under%20and%20oversampling%20of%20data%20lead%20to%20performance%0Aimprovement.%20Through%20numerical%20experiments%2C%20we%20show%20the%20relevance%20of%20our%0Atheoretical%20predictions%20on%20real%20datasets%2C%20on%20deeper%20architectures%20and%20with%0Asampling%20strategies%20based%20on%20unsupervised%20probabilistic%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestoring%2520balance%253A%2520principled%2520under/oversampling%2520of%2520data%2520for%2520optimal%250A%2520%2520classification%26entry.906535625%3DEmanuele%2520Loffredo%2520and%2520Mauro%2520Pastore%2520and%2520Simona%2520Cocco%2520and%2520R%25C3%25A9mi%2520Monasson%26entry.1292438233%3D%2520%2520Class%2520imbalance%2520in%2520real-world%2520data%2520poses%2520a%2520common%2520bottleneck%2520for%2520machine%250Alearning%2520tasks%252C%2520since%2520achieving%2520good%2520generalization%2520on%2520under-represented%250Aexamples%2520is%2520often%2520challenging.%2520Mitigation%2520strategies%252C%2520such%2520as%2520under%2520or%250Aoversampling%2520the%2520data%2520depending%2520on%2520their%2520abundances%252C%2520are%2520routinely%2520proposed%2520and%250Atested%2520empirically%252C%2520but%2520how%2520they%2520should%2520adapt%2520to%2520the%2520data%2520statistics%2520remains%250Apoorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520determine%2520exact%2520analytical%2520expressions%2520of%250Athe%2520generalization%2520curves%2520in%2520the%2520high-dimensional%2520regime%2520for%2520linear%2520classifiers%250A%2528Support%2520Vector%2520Machines%2529.%2520We%2520also%2520provide%2520a%2520sharp%2520prediction%2520of%2520the%2520effects%2520of%250Aunder/oversampling%2520strategies%2520depending%2520on%2520class%2520imbalance%252C%2520first%2520and%2520second%250Amoments%2520of%2520the%2520data%252C%2520and%2520the%2520metrics%2520of%2520performance%2520considered.%2520We%2520show%2520that%250Amixed%2520strategies%2520involving%2520under%2520and%2520oversampling%2520of%2520data%2520lead%2520to%2520performance%250Aimprovement.%2520Through%2520numerical%2520experiments%252C%2520we%2520show%2520the%2520relevance%2520of%2520our%250Atheoretical%2520predictions%2520on%2520real%2520datasets%252C%2520on%2520deeper%2520architectures%2520and%2520with%250Asampling%2520strategies%2520based%2520on%2520unsupervised%2520probabilistic%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restoring%20balance%3A%20principled%20under/oversampling%20of%20data%20for%20optimal%0A%20%20classification&entry.906535625=Emanuele%20Loffredo%20and%20Mauro%20Pastore%20and%20Simona%20Cocco%20and%20R%C3%A9mi%20Monasson&entry.1292438233=%20%20Class%20imbalance%20in%20real-world%20data%20poses%20a%20common%20bottleneck%20for%20machine%0Alearning%20tasks%2C%20since%20achieving%20good%20generalization%20on%20under-represented%0Aexamples%20is%20often%20challenging.%20Mitigation%20strategies%2C%20such%20as%20under%20or%0Aoversampling%20the%20data%20depending%20on%20their%20abundances%2C%20are%20routinely%20proposed%20and%0Atested%20empirically%2C%20but%20how%20they%20should%20adapt%20to%20the%20data%20statistics%20remains%0Apoorly%20understood.%20In%20this%20work%2C%20we%20determine%20exact%20analytical%20expressions%20of%0Athe%20generalization%20curves%20in%20the%20high-dimensional%20regime%20for%20linear%20classifiers%0A%28Support%20Vector%20Machines%29.%20We%20also%20provide%20a%20sharp%20prediction%20of%20the%20effects%20of%0Aunder/oversampling%20strategies%20depending%20on%20class%20imbalance%2C%20first%20and%20second%0Amoments%20of%20the%20data%2C%20and%20the%20metrics%20of%20performance%20considered.%20We%20show%20that%0Amixed%20strategies%20involving%20under%20and%20oversampling%20of%20data%20lead%20to%20performance%0Aimprovement.%20Through%20numerical%20experiments%2C%20we%20show%20the%20relevance%20of%20our%0Atheoretical%20predictions%20on%20real%20datasets%2C%20on%20deeper%20architectures%20and%20with%0Asampling%20strategies%20based%20on%20unsupervised%20probabilistic%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09535v1&entry.124074799=Read"},
{"title": "MGSER-SAM: Memory-Guided Soft Experience Replay with Sharpness-Aware\n  Optimization for Enhanced Continual Learning", "author": "Xingyu Li and Bo Tang", "abstract": "  Deep neural networks suffer from the catastrophic forgetting problem in the\nfield of continual learning (CL). To address this challenge, we propose\nMGSER-SAM, a novel memory replay-based algorithm specifically engineered to\nenhance the generalization capabilities of CL models. We first intergrate the\nSAM optimizer, a component designed for optimizing flatness, which seamlessly\nfits into well-known Experience Replay frameworks such as ER and DER++. Then,\nMGSER-SAM distinctively addresses the complex challenge of reconciling\nconflicts in weight perturbation directions between ongoing tasks and\npreviously stored memories, which is underexplored in the SAM optimizer. This\nis effectively accomplished by the strategic integration of soft logits and the\nalignment of memory gradient directions, where the regularization terms\nfacilitate the concurrent minimization of various training loss terms integral\nto the CL process. Through rigorous experimental analysis conducted across\nmultiple benchmarks, MGSER-SAM has demonstrated a consistent ability to\noutperform existing baselines in all three CL scenarios. Comparing to the\nrepresentative memory replay-based baselines ER and DER++, MGSER-SAM not only\nimproves the testing accuracy by $24.4\\%$ and $17.6\\%$ respectively, but also\nachieves the lowest forgetting on each benchmark.\n", "link": "http://arxiv.org/abs/2405.09492v1", "date": "2024-05-15", "relevancy": 2.4825, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGSER-SAM%3A%20Memory-Guided%20Soft%20Experience%20Replay%20with%20Sharpness-Aware%0A%20%20Optimization%20for%20Enhanced%20Continual%20Learning&body=Title%3A%20MGSER-SAM%3A%20Memory-Guided%20Soft%20Experience%20Replay%20with%20Sharpness-Aware%0A%20%20Optimization%20for%20Enhanced%20Continual%20Learning%0AAuthor%3A%20Xingyu%20Li%20and%20Bo%20Tang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20suffer%20from%20the%20catastrophic%20forgetting%20problem%20in%20the%0Afield%20of%20continual%20learning%20%28CL%29.%20To%20address%20this%20challenge%2C%20we%20propose%0AMGSER-SAM%2C%20a%20novel%20memory%20replay-based%20algorithm%20specifically%20engineered%20to%0Aenhance%20the%20generalization%20capabilities%20of%20CL%20models.%20We%20first%20intergrate%20the%0ASAM%20optimizer%2C%20a%20component%20designed%20for%20optimizing%20flatness%2C%20which%20seamlessly%0Afits%20into%20well-known%20Experience%20Replay%20frameworks%20such%20as%20ER%20and%20DER%2B%2B.%20Then%2C%0AMGSER-SAM%20distinctively%20addresses%20the%20complex%20challenge%20of%20reconciling%0Aconflicts%20in%20weight%20perturbation%20directions%20between%20ongoing%20tasks%20and%0Apreviously%20stored%20memories%2C%20which%20is%20underexplored%20in%20the%20SAM%20optimizer.%20This%0Ais%20effectively%20accomplished%20by%20the%20strategic%20integration%20of%20soft%20logits%20and%20the%0Aalignment%20of%20memory%20gradient%20directions%2C%20where%20the%20regularization%20terms%0Afacilitate%20the%20concurrent%20minimization%20of%20various%20training%20loss%20terms%20integral%0Ato%20the%20CL%20process.%20Through%20rigorous%20experimental%20analysis%20conducted%20across%0Amultiple%20benchmarks%2C%20MGSER-SAM%20has%20demonstrated%20a%20consistent%20ability%20to%0Aoutperform%20existing%20baselines%20in%20all%20three%20CL%20scenarios.%20Comparing%20to%20the%0Arepresentative%20memory%20replay-based%20baselines%20ER%20and%20DER%2B%2B%2C%20MGSER-SAM%20not%20only%0Aimproves%20the%20testing%20accuracy%20by%20%2424.4%5C%25%24%20and%20%2417.6%5C%25%24%20respectively%2C%20but%20also%0Aachieves%20the%20lowest%20forgetting%20on%20each%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGSER-SAM%253A%2520Memory-Guided%2520Soft%2520Experience%2520Replay%2520with%2520Sharpness-Aware%250A%2520%2520Optimization%2520for%2520Enhanced%2520Continual%2520Learning%26entry.906535625%3DXingyu%2520Li%2520and%2520Bo%2520Tang%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520suffer%2520from%2520the%2520catastrophic%2520forgetting%2520problem%2520in%2520the%250Afield%2520of%2520continual%2520learning%2520%2528CL%2529.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AMGSER-SAM%252C%2520a%2520novel%2520memory%2520replay-based%2520algorithm%2520specifically%2520engineered%2520to%250Aenhance%2520the%2520generalization%2520capabilities%2520of%2520CL%2520models.%2520We%2520first%2520intergrate%2520the%250ASAM%2520optimizer%252C%2520a%2520component%2520designed%2520for%2520optimizing%2520flatness%252C%2520which%2520seamlessly%250Afits%2520into%2520well-known%2520Experience%2520Replay%2520frameworks%2520such%2520as%2520ER%2520and%2520DER%252B%252B.%2520Then%252C%250AMGSER-SAM%2520distinctively%2520addresses%2520the%2520complex%2520challenge%2520of%2520reconciling%250Aconflicts%2520in%2520weight%2520perturbation%2520directions%2520between%2520ongoing%2520tasks%2520and%250Apreviously%2520stored%2520memories%252C%2520which%2520is%2520underexplored%2520in%2520the%2520SAM%2520optimizer.%2520This%250Ais%2520effectively%2520accomplished%2520by%2520the%2520strategic%2520integration%2520of%2520soft%2520logits%2520and%2520the%250Aalignment%2520of%2520memory%2520gradient%2520directions%252C%2520where%2520the%2520regularization%2520terms%250Afacilitate%2520the%2520concurrent%2520minimization%2520of%2520various%2520training%2520loss%2520terms%2520integral%250Ato%2520the%2520CL%2520process.%2520Through%2520rigorous%2520experimental%2520analysis%2520conducted%2520across%250Amultiple%2520benchmarks%252C%2520MGSER-SAM%2520has%2520demonstrated%2520a%2520consistent%2520ability%2520to%250Aoutperform%2520existing%2520baselines%2520in%2520all%2520three%2520CL%2520scenarios.%2520Comparing%2520to%2520the%250Arepresentative%2520memory%2520replay-based%2520baselines%2520ER%2520and%2520DER%252B%252B%252C%2520MGSER-SAM%2520not%2520only%250Aimproves%2520the%2520testing%2520accuracy%2520by%2520%252424.4%255C%2525%2524%2520and%2520%252417.6%255C%2525%2524%2520respectively%252C%2520but%2520also%250Aachieves%2520the%2520lowest%2520forgetting%2520on%2520each%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGSER-SAM%3A%20Memory-Guided%20Soft%20Experience%20Replay%20with%20Sharpness-Aware%0A%20%20Optimization%20for%20Enhanced%20Continual%20Learning&entry.906535625=Xingyu%20Li%20and%20Bo%20Tang&entry.1292438233=%20%20Deep%20neural%20networks%20suffer%20from%20the%20catastrophic%20forgetting%20problem%20in%20the%0Afield%20of%20continual%20learning%20%28CL%29.%20To%20address%20this%20challenge%2C%20we%20propose%0AMGSER-SAM%2C%20a%20novel%20memory%20replay-based%20algorithm%20specifically%20engineered%20to%0Aenhance%20the%20generalization%20capabilities%20of%20CL%20models.%20We%20first%20intergrate%20the%0ASAM%20optimizer%2C%20a%20component%20designed%20for%20optimizing%20flatness%2C%20which%20seamlessly%0Afits%20into%20well-known%20Experience%20Replay%20frameworks%20such%20as%20ER%20and%20DER%2B%2B.%20Then%2C%0AMGSER-SAM%20distinctively%20addresses%20the%20complex%20challenge%20of%20reconciling%0Aconflicts%20in%20weight%20perturbation%20directions%20between%20ongoing%20tasks%20and%0Apreviously%20stored%20memories%2C%20which%20is%20underexplored%20in%20the%20SAM%20optimizer.%20This%0Ais%20effectively%20accomplished%20by%20the%20strategic%20integration%20of%20soft%20logits%20and%20the%0Aalignment%20of%20memory%20gradient%20directions%2C%20where%20the%20regularization%20terms%0Afacilitate%20the%20concurrent%20minimization%20of%20various%20training%20loss%20terms%20integral%0Ato%20the%20CL%20process.%20Through%20rigorous%20experimental%20analysis%20conducted%20across%0Amultiple%20benchmarks%2C%20MGSER-SAM%20has%20demonstrated%20a%20consistent%20ability%20to%0Aoutperform%20existing%20baselines%20in%20all%20three%20CL%20scenarios.%20Comparing%20to%20the%0Arepresentative%20memory%20replay-based%20baselines%20ER%20and%20DER%2B%2B%2C%20MGSER-SAM%20not%20only%0Aimproves%20the%20testing%20accuracy%20by%20%2424.4%5C%25%24%20and%20%2417.6%5C%25%24%20respectively%2C%20but%20also%0Aachieves%20the%20lowest%20forgetting%20on%20each%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09492v1&entry.124074799=Read"},
{"title": "Hierarchical Side-Tuning for Vision Transformers", "author": "Weifeng Lin and Ziheng Wu and Wentao Yang and Mingxin Huang and Jun Huang and Lianwen Jin", "abstract": "  Fine-tuning pre-trained Vision Transformers (ViTs) has showcased significant\npromise in enhancing visual recognition tasks. Yet, the demand for\nindividualized and comprehensive fine-tuning processes for each task entails\nsubstantial computational and memory costs, posing a considerable challenge.\nRecent advancements in Parameter-Efficient Transfer Learning (PETL) have shown\npotential for achieving high performance with fewer parameter updates compared\nto full fine-tuning. However, their effectiveness is primarily observed in\nsimple tasks like image classification, while they encounter challenges with\nmore complex vision tasks like dense prediction. To address this gap, this\nstudy aims to identify an effective tuning method that caters to a wider range\nof visual tasks. In this paper, we introduce Hierarchical Side-Tuning (HST), an\ninnovative PETL method facilitating the transfer of ViT models to diverse\ndownstream tasks. Diverging from existing methods that focus solely on\nfine-tuning parameters within specific input spaces or modules, HST employs a\nlightweight Hierarchical Side Network (HSN). This network leverages\nintermediate activations from the ViT backbone to model multi-scale features,\nenhancing prediction capabilities. To evaluate HST, we conducted comprehensive\nexperiments across a range of visual tasks, including classification, object\ndetection, instance segmentation, and semantic segmentation. Remarkably, HST\nachieved state-of-the-art performance in 13 out of the 19 tasks on the VTAB-1K\nbenchmark, with the highest average Top-1 accuracy of 76.1%, while fine-tuning\na mere 0.78M parameters. When applied to object detection and semantic\nsegmentation tasks on the COCO and ADE20K testdev benchmarks, HST outperformed\nexisting PETL methods and even surpassed full fine-tuning.\n", "link": "http://arxiv.org/abs/2310.05393v4", "date": "2024-05-15", "relevancy": 2.4768, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.498}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Side-Tuning%20for%20Vision%20Transformers&body=Title%3A%20Hierarchical%20Side-Tuning%20for%20Vision%20Transformers%0AAuthor%3A%20Weifeng%20Lin%20and%20Ziheng%20Wu%20and%20Wentao%20Yang%20and%20Mingxin%20Huang%20and%20Jun%20Huang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20Vision%20Transformers%20%28ViTs%29%20has%20showcased%20significant%0Apromise%20in%20enhancing%20visual%20recognition%20tasks.%20Yet%2C%20the%20demand%20for%0Aindividualized%20and%20comprehensive%20fine-tuning%20processes%20for%20each%20task%20entails%0Asubstantial%20computational%20and%20memory%20costs%2C%20posing%20a%20considerable%20challenge.%0ARecent%20advancements%20in%20Parameter-Efficient%20Transfer%20Learning%20%28PETL%29%20have%20shown%0Apotential%20for%20achieving%20high%20performance%20with%20fewer%20parameter%20updates%20compared%0Ato%20full%20fine-tuning.%20However%2C%20their%20effectiveness%20is%20primarily%20observed%20in%0Asimple%20tasks%20like%20image%20classification%2C%20while%20they%20encounter%20challenges%20with%0Amore%20complex%20vision%20tasks%20like%20dense%20prediction.%20To%20address%20this%20gap%2C%20this%0Astudy%20aims%20to%20identify%20an%20effective%20tuning%20method%20that%20caters%20to%20a%20wider%20range%0Aof%20visual%20tasks.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%20Side-Tuning%20%28HST%29%2C%20an%0Ainnovative%20PETL%20method%20facilitating%20the%20transfer%20of%20ViT%20models%20to%20diverse%0Adownstream%20tasks.%20Diverging%20from%20existing%20methods%20that%20focus%20solely%20on%0Afine-tuning%20parameters%20within%20specific%20input%20spaces%20or%20modules%2C%20HST%20employs%20a%0Alightweight%20Hierarchical%20Side%20Network%20%28HSN%29.%20This%20network%20leverages%0Aintermediate%20activations%20from%20the%20ViT%20backbone%20to%20model%20multi-scale%20features%2C%0Aenhancing%20prediction%20capabilities.%20To%20evaluate%20HST%2C%20we%20conducted%20comprehensive%0Aexperiments%20across%20a%20range%20of%20visual%20tasks%2C%20including%20classification%2C%20object%0Adetection%2C%20instance%20segmentation%2C%20and%20semantic%20segmentation.%20Remarkably%2C%20HST%0Aachieved%20state-of-the-art%20performance%20in%2013%20out%20of%20the%2019%20tasks%20on%20the%20VTAB-1K%0Abenchmark%2C%20with%20the%20highest%20average%20Top-1%20accuracy%20of%2076.1%25%2C%20while%20fine-tuning%0Aa%20mere%200.78M%20parameters.%20When%20applied%20to%20object%20detection%20and%20semantic%0Asegmentation%20tasks%20on%20the%20COCO%20and%20ADE20K%20testdev%20benchmarks%2C%20HST%20outperformed%0Aexisting%20PETL%20methods%20and%20even%20surpassed%20full%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05393v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Side-Tuning%2520for%2520Vision%2520Transformers%26entry.906535625%3DWeifeng%2520Lin%2520and%2520Ziheng%2520Wu%2520and%2520Wentao%2520Yang%2520and%2520Mingxin%2520Huang%2520and%2520Jun%2520Huang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520Vision%2520Transformers%2520%2528ViTs%2529%2520has%2520showcased%2520significant%250Apromise%2520in%2520enhancing%2520visual%2520recognition%2520tasks.%2520Yet%252C%2520the%2520demand%2520for%250Aindividualized%2520and%2520comprehensive%2520fine-tuning%2520processes%2520for%2520each%2520task%2520entails%250Asubstantial%2520computational%2520and%2520memory%2520costs%252C%2520posing%2520a%2520considerable%2520challenge.%250ARecent%2520advancements%2520in%2520Parameter-Efficient%2520Transfer%2520Learning%2520%2528PETL%2529%2520have%2520shown%250Apotential%2520for%2520achieving%2520high%2520performance%2520with%2520fewer%2520parameter%2520updates%2520compared%250Ato%2520full%2520fine-tuning.%2520However%252C%2520their%2520effectiveness%2520is%2520primarily%2520observed%2520in%250Asimple%2520tasks%2520like%2520image%2520classification%252C%2520while%2520they%2520encounter%2520challenges%2520with%250Amore%2520complex%2520vision%2520tasks%2520like%2520dense%2520prediction.%2520To%2520address%2520this%2520gap%252C%2520this%250Astudy%2520aims%2520to%2520identify%2520an%2520effective%2520tuning%2520method%2520that%2520caters%2520to%2520a%2520wider%2520range%250Aof%2520visual%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Hierarchical%2520Side-Tuning%2520%2528HST%2529%252C%2520an%250Ainnovative%2520PETL%2520method%2520facilitating%2520the%2520transfer%2520of%2520ViT%2520models%2520to%2520diverse%250Adownstream%2520tasks.%2520Diverging%2520from%2520existing%2520methods%2520that%2520focus%2520solely%2520on%250Afine-tuning%2520parameters%2520within%2520specific%2520input%2520spaces%2520or%2520modules%252C%2520HST%2520employs%2520a%250Alightweight%2520Hierarchical%2520Side%2520Network%2520%2528HSN%2529.%2520This%2520network%2520leverages%250Aintermediate%2520activations%2520from%2520the%2520ViT%2520backbone%2520to%2520model%2520multi-scale%2520features%252C%250Aenhancing%2520prediction%2520capabilities.%2520To%2520evaluate%2520HST%252C%2520we%2520conducted%2520comprehensive%250Aexperiments%2520across%2520a%2520range%2520of%2520visual%2520tasks%252C%2520including%2520classification%252C%2520object%250Adetection%252C%2520instance%2520segmentation%252C%2520and%2520semantic%2520segmentation.%2520Remarkably%252C%2520HST%250Aachieved%2520state-of-the-art%2520performance%2520in%252013%2520out%2520of%2520the%252019%2520tasks%2520on%2520the%2520VTAB-1K%250Abenchmark%252C%2520with%2520the%2520highest%2520average%2520Top-1%2520accuracy%2520of%252076.1%2525%252C%2520while%2520fine-tuning%250Aa%2520mere%25200.78M%2520parameters.%2520When%2520applied%2520to%2520object%2520detection%2520and%2520semantic%250Asegmentation%2520tasks%2520on%2520the%2520COCO%2520and%2520ADE20K%2520testdev%2520benchmarks%252C%2520HST%2520outperformed%250Aexisting%2520PETL%2520methods%2520and%2520even%2520surpassed%2520full%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05393v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Side-Tuning%20for%20Vision%20Transformers&entry.906535625=Weifeng%20Lin%20and%20Ziheng%20Wu%20and%20Wentao%20Yang%20and%20Mingxin%20Huang%20and%20Jun%20Huang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Fine-tuning%20pre-trained%20Vision%20Transformers%20%28ViTs%29%20has%20showcased%20significant%0Apromise%20in%20enhancing%20visual%20recognition%20tasks.%20Yet%2C%20the%20demand%20for%0Aindividualized%20and%20comprehensive%20fine-tuning%20processes%20for%20each%20task%20entails%0Asubstantial%20computational%20and%20memory%20costs%2C%20posing%20a%20considerable%20challenge.%0ARecent%20advancements%20in%20Parameter-Efficient%20Transfer%20Learning%20%28PETL%29%20have%20shown%0Apotential%20for%20achieving%20high%20performance%20with%20fewer%20parameter%20updates%20compared%0Ato%20full%20fine-tuning.%20However%2C%20their%20effectiveness%20is%20primarily%20observed%20in%0Asimple%20tasks%20like%20image%20classification%2C%20while%20they%20encounter%20challenges%20with%0Amore%20complex%20vision%20tasks%20like%20dense%20prediction.%20To%20address%20this%20gap%2C%20this%0Astudy%20aims%20to%20identify%20an%20effective%20tuning%20method%20that%20caters%20to%20a%20wider%20range%0Aof%20visual%20tasks.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%20Side-Tuning%20%28HST%29%2C%20an%0Ainnovative%20PETL%20method%20facilitating%20the%20transfer%20of%20ViT%20models%20to%20diverse%0Adownstream%20tasks.%20Diverging%20from%20existing%20methods%20that%20focus%20solely%20on%0Afine-tuning%20parameters%20within%20specific%20input%20spaces%20or%20modules%2C%20HST%20employs%20a%0Alightweight%20Hierarchical%20Side%20Network%20%28HSN%29.%20This%20network%20leverages%0Aintermediate%20activations%20from%20the%20ViT%20backbone%20to%20model%20multi-scale%20features%2C%0Aenhancing%20prediction%20capabilities.%20To%20evaluate%20HST%2C%20we%20conducted%20comprehensive%0Aexperiments%20across%20a%20range%20of%20visual%20tasks%2C%20including%20classification%2C%20object%0Adetection%2C%20instance%20segmentation%2C%20and%20semantic%20segmentation.%20Remarkably%2C%20HST%0Aachieved%20state-of-the-art%20performance%20in%2013%20out%20of%20the%2019%20tasks%20on%20the%20VTAB-1K%0Abenchmark%2C%20with%20the%20highest%20average%20Top-1%20accuracy%20of%2076.1%25%2C%20while%20fine-tuning%0Aa%20mere%200.78M%20parameters.%20When%20applied%20to%20object%20detection%20and%20semantic%0Asegmentation%20tasks%20on%20the%20COCO%20and%20ADE20K%20testdev%20benchmarks%2C%20HST%20outperformed%0Aexisting%20PETL%20methods%20and%20even%20surpassed%20full%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05393v4&entry.124074799=Read"},
{"title": "GrainGrasp: Dexterous Grasp Generation with Fine-grained Contact\n  Guidance", "author": "Fuqiang Zhao and Dzmitry Tsetserukou and Qian Liu", "abstract": "  One goal of dexterous robotic grasping is to allow robots to handle objects\nwith the same level of flexibility and adaptability as humans. However, it\nremains a challenging task to generate an optimal grasping strategy for\ndexterous hands, especially when it comes to delicate manipulation and accurate\nadjustment the desired grasping poses for objects of varying shapes and sizes.\nIn this paper, we propose a novel dexterous grasp generation scheme called\n\\textbf{\\textit{GrainGrasp}} that provides fine-grained contact guidance for\neach fingertip. In particular, we employ a generative model to predict separate\ncontact maps for each fingertip on the object point cloud, effectively\ncapturing the specifics of finger-object interactions. In addition, we develop\na new dexterous grasping optimization algorithm that solely relies on the point\ncloud as input, eliminating the necessity for complete mesh information of the\nobject. By leveraging the contact maps of different fingertips, the proposed\noptimization algorithm can generate precise and determinable strategies for\nhuman-like object grasping. Experimental results confirm the efficiency of the\nproposed scheme. Our code is available at https://github.com/wmtlab/GrainGrasp\n", "link": "http://arxiv.org/abs/2405.09310v1", "date": "2024-05-15", "relevancy": 2.4447, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7323}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5249}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GrainGrasp%3A%20Dexterous%20Grasp%20Generation%20with%20Fine-grained%20Contact%0A%20%20Guidance&body=Title%3A%20GrainGrasp%3A%20Dexterous%20Grasp%20Generation%20with%20Fine-grained%20Contact%0A%20%20Guidance%0AAuthor%3A%20Fuqiang%20Zhao%20and%20Dzmitry%20Tsetserukou%20and%20Qian%20Liu%0AAbstract%3A%20%20%20One%20goal%20of%20dexterous%20robotic%20grasping%20is%20to%20allow%20robots%20to%20handle%20objects%0Awith%20the%20same%20level%20of%20flexibility%20and%20adaptability%20as%20humans.%20However%2C%20it%0Aremains%20a%20challenging%20task%20to%20generate%20an%20optimal%20grasping%20strategy%20for%0Adexterous%20hands%2C%20especially%20when%20it%20comes%20to%20delicate%20manipulation%20and%20accurate%0Aadjustment%20the%20desired%20grasping%20poses%20for%20objects%20of%20varying%20shapes%20and%20sizes.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20dexterous%20grasp%20generation%20scheme%20called%0A%5Ctextbf%7B%5Ctextit%7BGrainGrasp%7D%7D%20that%20provides%20fine-grained%20contact%20guidance%20for%0Aeach%20fingertip.%20In%20particular%2C%20we%20employ%20a%20generative%20model%20to%20predict%20separate%0Acontact%20maps%20for%20each%20fingertip%20on%20the%20object%20point%20cloud%2C%20effectively%0Acapturing%20the%20specifics%20of%20finger-object%20interactions.%20In%20addition%2C%20we%20develop%0Aa%20new%20dexterous%20grasping%20optimization%20algorithm%20that%20solely%20relies%20on%20the%20point%0Acloud%20as%20input%2C%20eliminating%20the%20necessity%20for%20complete%20mesh%20information%20of%20the%0Aobject.%20By%20leveraging%20the%20contact%20maps%20of%20different%20fingertips%2C%20the%20proposed%0Aoptimization%20algorithm%20can%20generate%20precise%20and%20determinable%20strategies%20for%0Ahuman-like%20object%20grasping.%20Experimental%20results%20confirm%20the%20efficiency%20of%20the%0Aproposed%20scheme.%20Our%20code%20is%20available%20at%20https%3A//github.com/wmtlab/GrainGrasp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrainGrasp%253A%2520Dexterous%2520Grasp%2520Generation%2520with%2520Fine-grained%2520Contact%250A%2520%2520Guidance%26entry.906535625%3DFuqiang%2520Zhao%2520and%2520Dzmitry%2520Tsetserukou%2520and%2520Qian%2520Liu%26entry.1292438233%3D%2520%2520One%2520goal%2520of%2520dexterous%2520robotic%2520grasping%2520is%2520to%2520allow%2520robots%2520to%2520handle%2520objects%250Awith%2520the%2520same%2520level%2520of%2520flexibility%2520and%2520adaptability%2520as%2520humans.%2520However%252C%2520it%250Aremains%2520a%2520challenging%2520task%2520to%2520generate%2520an%2520optimal%2520grasping%2520strategy%2520for%250Adexterous%2520hands%252C%2520especially%2520when%2520it%2520comes%2520to%2520delicate%2520manipulation%2520and%2520accurate%250Aadjustment%2520the%2520desired%2520grasping%2520poses%2520for%2520objects%2520of%2520varying%2520shapes%2520and%2520sizes.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520dexterous%2520grasp%2520generation%2520scheme%2520called%250A%255Ctextbf%257B%255Ctextit%257BGrainGrasp%257D%257D%2520that%2520provides%2520fine-grained%2520contact%2520guidance%2520for%250Aeach%2520fingertip.%2520In%2520particular%252C%2520we%2520employ%2520a%2520generative%2520model%2520to%2520predict%2520separate%250Acontact%2520maps%2520for%2520each%2520fingertip%2520on%2520the%2520object%2520point%2520cloud%252C%2520effectively%250Acapturing%2520the%2520specifics%2520of%2520finger-object%2520interactions.%2520In%2520addition%252C%2520we%2520develop%250Aa%2520new%2520dexterous%2520grasping%2520optimization%2520algorithm%2520that%2520solely%2520relies%2520on%2520the%2520point%250Acloud%2520as%2520input%252C%2520eliminating%2520the%2520necessity%2520for%2520complete%2520mesh%2520information%2520of%2520the%250Aobject.%2520By%2520leveraging%2520the%2520contact%2520maps%2520of%2520different%2520fingertips%252C%2520the%2520proposed%250Aoptimization%2520algorithm%2520can%2520generate%2520precise%2520and%2520determinable%2520strategies%2520for%250Ahuman-like%2520object%2520grasping.%2520Experimental%2520results%2520confirm%2520the%2520efficiency%2520of%2520the%250Aproposed%2520scheme.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/wmtlab/GrainGrasp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GrainGrasp%3A%20Dexterous%20Grasp%20Generation%20with%20Fine-grained%20Contact%0A%20%20Guidance&entry.906535625=Fuqiang%20Zhao%20and%20Dzmitry%20Tsetserukou%20and%20Qian%20Liu&entry.1292438233=%20%20One%20goal%20of%20dexterous%20robotic%20grasping%20is%20to%20allow%20robots%20to%20handle%20objects%0Awith%20the%20same%20level%20of%20flexibility%20and%20adaptability%20as%20humans.%20However%2C%20it%0Aremains%20a%20challenging%20task%20to%20generate%20an%20optimal%20grasping%20strategy%20for%0Adexterous%20hands%2C%20especially%20when%20it%20comes%20to%20delicate%20manipulation%20and%20accurate%0Aadjustment%20the%20desired%20grasping%20poses%20for%20objects%20of%20varying%20shapes%20and%20sizes.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20dexterous%20grasp%20generation%20scheme%20called%0A%5Ctextbf%7B%5Ctextit%7BGrainGrasp%7D%7D%20that%20provides%20fine-grained%20contact%20guidance%20for%0Aeach%20fingertip.%20In%20particular%2C%20we%20employ%20a%20generative%20model%20to%20predict%20separate%0Acontact%20maps%20for%20each%20fingertip%20on%20the%20object%20point%20cloud%2C%20effectively%0Acapturing%20the%20specifics%20of%20finger-object%20interactions.%20In%20addition%2C%20we%20develop%0Aa%20new%20dexterous%20grasping%20optimization%20algorithm%20that%20solely%20relies%20on%20the%20point%0Acloud%20as%20input%2C%20eliminating%20the%20necessity%20for%20complete%20mesh%20information%20of%20the%0Aobject.%20By%20leveraging%20the%20contact%20maps%20of%20different%20fingertips%2C%20the%20proposed%0Aoptimization%20algorithm%20can%20generate%20precise%20and%20determinable%20strategies%20for%0Ahuman-like%20object%20grasping.%20Experimental%20results%20confirm%20the%20efficiency%20of%20the%0Aproposed%20scheme.%20Our%20code%20is%20available%20at%20https%3A//github.com/wmtlab/GrainGrasp%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09310v1&entry.124074799=Read"},
{"title": "Identity Overlap Between Face Recognition Train/Test Data: Causing\n  Optimistic Bias in Accuracy Measurement", "author": "Haiyu Wu and Sicong Tian and Jacob Gutierrez and Aman Bhatta and Ka\u011fan \u00d6zt\u00fcrk and Kevin W. Bowyer", "abstract": "  A fundamental tenet of pattern recognition is that overlap between training\nand testing sets causes an optimistic accuracy estimate. Deep CNNs for face\nrecognition are trained for N-way classification of the identities in the\ntraining set. Accuracy is commonly estimated as average 10-fold classification\naccuracy on image pairs from test sets such as LFW, CALFW, CPLFW, CFP-FP and\nAgeDB-30. Because train and test sets have been independently assembled, images\nand identities in any given test set may also be present in any given training\nset. In particular, our experiments reveal a surprising degree of identity and\nimage overlap between the LFW family of test sets and the MS1MV2 training set.\nOur experiments also reveal identity label noise in MS1MV2. We compare accuracy\nachieved with same-size MS1MV2 subsets that are identity-disjoint and not\nidentity-disjoint with LFW, to reveal the size of the optimistic bias. Using\nmore challenging test sets from the LFW family, we find that the size of the\noptimistic bias is larger for more challenging test sets. Our results highlight\nthe lack of and the need for identity-disjoint train and test methodology in\nface recognition research.\n", "link": "http://arxiv.org/abs/2405.09403v1", "date": "2024-05-15", "relevancy": 2.4254, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identity%20Overlap%20Between%20Face%20Recognition%20Train/Test%20Data%3A%20Causing%0A%20%20Optimistic%20Bias%20in%20Accuracy%20Measurement&body=Title%3A%20Identity%20Overlap%20Between%20Face%20Recognition%20Train/Test%20Data%3A%20Causing%0A%20%20Optimistic%20Bias%20in%20Accuracy%20Measurement%0AAuthor%3A%20Haiyu%20Wu%20and%20Sicong%20Tian%20and%20Jacob%20Gutierrez%20and%20Aman%20Bhatta%20and%20Ka%C4%9Fan%20%C3%96zt%C3%BCrk%20and%20Kevin%20W.%20Bowyer%0AAbstract%3A%20%20%20A%20fundamental%20tenet%20of%20pattern%20recognition%20is%20that%20overlap%20between%20training%0Aand%20testing%20sets%20causes%20an%20optimistic%20accuracy%20estimate.%20Deep%20CNNs%20for%20face%0Arecognition%20are%20trained%20for%20N-way%20classification%20of%20the%20identities%20in%20the%0Atraining%20set.%20Accuracy%20is%20commonly%20estimated%20as%20average%2010-fold%20classification%0Aaccuracy%20on%20image%20pairs%20from%20test%20sets%20such%20as%20LFW%2C%20CALFW%2C%20CPLFW%2C%20CFP-FP%20and%0AAgeDB-30.%20Because%20train%20and%20test%20sets%20have%20been%20independently%20assembled%2C%20images%0Aand%20identities%20in%20any%20given%20test%20set%20may%20also%20be%20present%20in%20any%20given%20training%0Aset.%20In%20particular%2C%20our%20experiments%20reveal%20a%20surprising%20degree%20of%20identity%20and%0Aimage%20overlap%20between%20the%20LFW%20family%20of%20test%20sets%20and%20the%20MS1MV2%20training%20set.%0AOur%20experiments%20also%20reveal%20identity%20label%20noise%20in%20MS1MV2.%20We%20compare%20accuracy%0Aachieved%20with%20same-size%20MS1MV2%20subsets%20that%20are%20identity-disjoint%20and%20not%0Aidentity-disjoint%20with%20LFW%2C%20to%20reveal%20the%20size%20of%20the%20optimistic%20bias.%20Using%0Amore%20challenging%20test%20sets%20from%20the%20LFW%20family%2C%20we%20find%20that%20the%20size%20of%20the%0Aoptimistic%20bias%20is%20larger%20for%20more%20challenging%20test%20sets.%20Our%20results%20highlight%0Athe%20lack%20of%20and%20the%20need%20for%20identity-disjoint%20train%20and%20test%20methodology%20in%0Aface%20recognition%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentity%2520Overlap%2520Between%2520Face%2520Recognition%2520Train/Test%2520Data%253A%2520Causing%250A%2520%2520Optimistic%2520Bias%2520in%2520Accuracy%2520Measurement%26entry.906535625%3DHaiyu%2520Wu%2520and%2520Sicong%2520Tian%2520and%2520Jacob%2520Gutierrez%2520and%2520Aman%2520Bhatta%2520and%2520Ka%25C4%259Fan%2520%25C3%2596zt%25C3%25BCrk%2520and%2520Kevin%2520W.%2520Bowyer%26entry.1292438233%3D%2520%2520A%2520fundamental%2520tenet%2520of%2520pattern%2520recognition%2520is%2520that%2520overlap%2520between%2520training%250Aand%2520testing%2520sets%2520causes%2520an%2520optimistic%2520accuracy%2520estimate.%2520Deep%2520CNNs%2520for%2520face%250Arecognition%2520are%2520trained%2520for%2520N-way%2520classification%2520of%2520the%2520identities%2520in%2520the%250Atraining%2520set.%2520Accuracy%2520is%2520commonly%2520estimated%2520as%2520average%252010-fold%2520classification%250Aaccuracy%2520on%2520image%2520pairs%2520from%2520test%2520sets%2520such%2520as%2520LFW%252C%2520CALFW%252C%2520CPLFW%252C%2520CFP-FP%2520and%250AAgeDB-30.%2520Because%2520train%2520and%2520test%2520sets%2520have%2520been%2520independently%2520assembled%252C%2520images%250Aand%2520identities%2520in%2520any%2520given%2520test%2520set%2520may%2520also%2520be%2520present%2520in%2520any%2520given%2520training%250Aset.%2520In%2520particular%252C%2520our%2520experiments%2520reveal%2520a%2520surprising%2520degree%2520of%2520identity%2520and%250Aimage%2520overlap%2520between%2520the%2520LFW%2520family%2520of%2520test%2520sets%2520and%2520the%2520MS1MV2%2520training%2520set.%250AOur%2520experiments%2520also%2520reveal%2520identity%2520label%2520noise%2520in%2520MS1MV2.%2520We%2520compare%2520accuracy%250Aachieved%2520with%2520same-size%2520MS1MV2%2520subsets%2520that%2520are%2520identity-disjoint%2520and%2520not%250Aidentity-disjoint%2520with%2520LFW%252C%2520to%2520reveal%2520the%2520size%2520of%2520the%2520optimistic%2520bias.%2520Using%250Amore%2520challenging%2520test%2520sets%2520from%2520the%2520LFW%2520family%252C%2520we%2520find%2520that%2520the%2520size%2520of%2520the%250Aoptimistic%2520bias%2520is%2520larger%2520for%2520more%2520challenging%2520test%2520sets.%2520Our%2520results%2520highlight%250Athe%2520lack%2520of%2520and%2520the%2520need%2520for%2520identity-disjoint%2520train%2520and%2520test%2520methodology%2520in%250Aface%2520recognition%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identity%20Overlap%20Between%20Face%20Recognition%20Train/Test%20Data%3A%20Causing%0A%20%20Optimistic%20Bias%20in%20Accuracy%20Measurement&entry.906535625=Haiyu%20Wu%20and%20Sicong%20Tian%20and%20Jacob%20Gutierrez%20and%20Aman%20Bhatta%20and%20Ka%C4%9Fan%20%C3%96zt%C3%BCrk%20and%20Kevin%20W.%20Bowyer&entry.1292438233=%20%20A%20fundamental%20tenet%20of%20pattern%20recognition%20is%20that%20overlap%20between%20training%0Aand%20testing%20sets%20causes%20an%20optimistic%20accuracy%20estimate.%20Deep%20CNNs%20for%20face%0Arecognition%20are%20trained%20for%20N-way%20classification%20of%20the%20identities%20in%20the%0Atraining%20set.%20Accuracy%20is%20commonly%20estimated%20as%20average%2010-fold%20classification%0Aaccuracy%20on%20image%20pairs%20from%20test%20sets%20such%20as%20LFW%2C%20CALFW%2C%20CPLFW%2C%20CFP-FP%20and%0AAgeDB-30.%20Because%20train%20and%20test%20sets%20have%20been%20independently%20assembled%2C%20images%0Aand%20identities%20in%20any%20given%20test%20set%20may%20also%20be%20present%20in%20any%20given%20training%0Aset.%20In%20particular%2C%20our%20experiments%20reveal%20a%20surprising%20degree%20of%20identity%20and%0Aimage%20overlap%20between%20the%20LFW%20family%20of%20test%20sets%20and%20the%20MS1MV2%20training%20set.%0AOur%20experiments%20also%20reveal%20identity%20label%20noise%20in%20MS1MV2.%20We%20compare%20accuracy%0Aachieved%20with%20same-size%20MS1MV2%20subsets%20that%20are%20identity-disjoint%20and%20not%0Aidentity-disjoint%20with%20LFW%2C%20to%20reveal%20the%20size%20of%20the%20optimistic%20bias.%20Using%0Amore%20challenging%20test%20sets%20from%20the%20LFW%20family%2C%20we%20find%20that%20the%20size%20of%20the%0Aoptimistic%20bias%20is%20larger%20for%20more%20challenging%20test%20sets.%20Our%20results%20highlight%0Athe%20lack%20of%20and%20the%20need%20for%20identity-disjoint%20train%20and%20test%20methodology%20in%0Aface%20recognition%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09403v1&entry.124074799=Read"},
{"title": "Learning Coarse-Grained Dynamics on Graph", "author": "Yin Yu and John Harlim and Daning Huang and Yan Li", "abstract": "  We consider a Graph Neural Network (GNN) non-Markovian modeling framework to\nidentify coarse-grained dynamical systems on graphs. Our main idea is to\nsystematically determine the GNN architecture by inspecting how the leading\nterm of the Mori-Zwanzig memory term depends on the coarse-grained interaction\ncoefficients that encode the graph topology. Based on this analysis, we found\nthat the appropriate GNN architecture that will account for $K$-hop dynamical\ninteractions has to employ a Message Passing (MP) mechanism with at least $2K$\nsteps. We also deduce that the memory length required for an accurate closure\nmodel decreases as a function of the interaction strength under the assumption\nthat the interaction strength exhibits a power law that decays as a function of\nthe hop distance. Supporting numerical demonstrations on two examples, a\nheterogeneous Kuramoto oscillator model and a power system, suggest that the\nproposed GNN architecture can predict the coarse-grained dynamics under fixed\nand time-varying graph topologies.\n", "link": "http://arxiv.org/abs/2405.09324v1", "date": "2024-05-15", "relevancy": 2.4136, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5012}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Coarse-Grained%20Dynamics%20on%20Graph&body=Title%3A%20Learning%20Coarse-Grained%20Dynamics%20on%20Graph%0AAuthor%3A%20Yin%20Yu%20and%20John%20Harlim%20and%20Daning%20Huang%20and%20Yan%20Li%0AAbstract%3A%20%20%20We%20consider%20a%20Graph%20Neural%20Network%20%28GNN%29%20non-Markovian%20modeling%20framework%20to%0Aidentify%20coarse-grained%20dynamical%20systems%20on%20graphs.%20Our%20main%20idea%20is%20to%0Asystematically%20determine%20the%20GNN%20architecture%20by%20inspecting%20how%20the%20leading%0Aterm%20of%20the%20Mori-Zwanzig%20memory%20term%20depends%20on%20the%20coarse-grained%20interaction%0Acoefficients%20that%20encode%20the%20graph%20topology.%20Based%20on%20this%20analysis%2C%20we%20found%0Athat%20the%20appropriate%20GNN%20architecture%20that%20will%20account%20for%20%24K%24-hop%20dynamical%0Ainteractions%20has%20to%20employ%20a%20Message%20Passing%20%28MP%29%20mechanism%20with%20at%20least%20%242K%24%0Asteps.%20We%20also%20deduce%20that%20the%20memory%20length%20required%20for%20an%20accurate%20closure%0Amodel%20decreases%20as%20a%20function%20of%20the%20interaction%20strength%20under%20the%20assumption%0Athat%20the%20interaction%20strength%20exhibits%20a%20power%20law%20that%20decays%20as%20a%20function%20of%0Athe%20hop%20distance.%20Supporting%20numerical%20demonstrations%20on%20two%20examples%2C%20a%0Aheterogeneous%20Kuramoto%20oscillator%20model%20and%20a%20power%20system%2C%20suggest%20that%20the%0Aproposed%20GNN%20architecture%20can%20predict%20the%20coarse-grained%20dynamics%20under%20fixed%0Aand%20time-varying%20graph%20topologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Coarse-Grained%2520Dynamics%2520on%2520Graph%26entry.906535625%3DYin%2520Yu%2520and%2520John%2520Harlim%2520and%2520Daning%2520Huang%2520and%2520Yan%2520Li%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520non-Markovian%2520modeling%2520framework%2520to%250Aidentify%2520coarse-grained%2520dynamical%2520systems%2520on%2520graphs.%2520Our%2520main%2520idea%2520is%2520to%250Asystematically%2520determine%2520the%2520GNN%2520architecture%2520by%2520inspecting%2520how%2520the%2520leading%250Aterm%2520of%2520the%2520Mori-Zwanzig%2520memory%2520term%2520depends%2520on%2520the%2520coarse-grained%2520interaction%250Acoefficients%2520that%2520encode%2520the%2520graph%2520topology.%2520Based%2520on%2520this%2520analysis%252C%2520we%2520found%250Athat%2520the%2520appropriate%2520GNN%2520architecture%2520that%2520will%2520account%2520for%2520%2524K%2524-hop%2520dynamical%250Ainteractions%2520has%2520to%2520employ%2520a%2520Message%2520Passing%2520%2528MP%2529%2520mechanism%2520with%2520at%2520least%2520%25242K%2524%250Asteps.%2520We%2520also%2520deduce%2520that%2520the%2520memory%2520length%2520required%2520for%2520an%2520accurate%2520closure%250Amodel%2520decreases%2520as%2520a%2520function%2520of%2520the%2520interaction%2520strength%2520under%2520the%2520assumption%250Athat%2520the%2520interaction%2520strength%2520exhibits%2520a%2520power%2520law%2520that%2520decays%2520as%2520a%2520function%2520of%250Athe%2520hop%2520distance.%2520Supporting%2520numerical%2520demonstrations%2520on%2520two%2520examples%252C%2520a%250Aheterogeneous%2520Kuramoto%2520oscillator%2520model%2520and%2520a%2520power%2520system%252C%2520suggest%2520that%2520the%250Aproposed%2520GNN%2520architecture%2520can%2520predict%2520the%2520coarse-grained%2520dynamics%2520under%2520fixed%250Aand%2520time-varying%2520graph%2520topologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Coarse-Grained%20Dynamics%20on%20Graph&entry.906535625=Yin%20Yu%20and%20John%20Harlim%20and%20Daning%20Huang%20and%20Yan%20Li&entry.1292438233=%20%20We%20consider%20a%20Graph%20Neural%20Network%20%28GNN%29%20non-Markovian%20modeling%20framework%20to%0Aidentify%20coarse-grained%20dynamical%20systems%20on%20graphs.%20Our%20main%20idea%20is%20to%0Asystematically%20determine%20the%20GNN%20architecture%20by%20inspecting%20how%20the%20leading%0Aterm%20of%20the%20Mori-Zwanzig%20memory%20term%20depends%20on%20the%20coarse-grained%20interaction%0Acoefficients%20that%20encode%20the%20graph%20topology.%20Based%20on%20this%20analysis%2C%20we%20found%0Athat%20the%20appropriate%20GNN%20architecture%20that%20will%20account%20for%20%24K%24-hop%20dynamical%0Ainteractions%20has%20to%20employ%20a%20Message%20Passing%20%28MP%29%20mechanism%20with%20at%20least%20%242K%24%0Asteps.%20We%20also%20deduce%20that%20the%20memory%20length%20required%20for%20an%20accurate%20closure%0Amodel%20decreases%20as%20a%20function%20of%20the%20interaction%20strength%20under%20the%20assumption%0Athat%20the%20interaction%20strength%20exhibits%20a%20power%20law%20that%20decays%20as%20a%20function%20of%0Athe%20hop%20distance.%20Supporting%20numerical%20demonstrations%20on%20two%20examples%2C%20a%0Aheterogeneous%20Kuramoto%20oscillator%20model%20and%20a%20power%20system%2C%20suggest%20that%20the%0Aproposed%20GNN%20architecture%20can%20predict%20the%20coarse-grained%20dynamics%20under%20fixed%0Aand%20time-varying%20graph%20topologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09324v1&entry.124074799=Read"},
{"title": "Dance Any Beat: Blending Beats with Visuals in Dance Video Generation", "author": "Xuanchen Wang and Heng Wang and Dongnan Liu and Weidong Cai", "abstract": "  The task of generating dance from music is crucial, yet current methods,\nwhich mainly produce joint sequences, lead to outputs that lack intuitiveness\nand complicate data collection due to the necessity for precise joint\nannotations. We introduce a Dance Any Beat Diffusion model, namely DabFusion,\nthat employs music as a conditional input to directly create dance videos from\nstill images, utilizing conditional image-to-video generation principles. This\napproach pioneers the use of music as a conditioning factor in image-to-video\nsynthesis. Our method unfolds in two stages: training an auto-encoder to\npredict latent optical flow between reference and driving frames, eliminating\nthe need for joint annotation, and training a U-Net-based diffusion model to\nproduce these latent optical flows guided by music rhythm encoded by CLAP.\nAlthough capable of producing high-quality dance videos, the baseline model\nstruggles with rhythm alignment. We enhance the model by adding beat\ninformation, improving synchronization. We introduce a 2D motion-music\nalignment score (2D-MM Align) for quantitative assessment. Evaluated on the\nAIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align\nscore and established metrics. Video results can be found on our project page:\nhttps://DabFusion.github.io.\n", "link": "http://arxiv.org/abs/2405.09266v1", "date": "2024-05-15", "relevancy": 2.4084, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.621}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5996}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dance%20Any%20Beat%3A%20Blending%20Beats%20with%20Visuals%20in%20Dance%20Video%20Generation&body=Title%3A%20Dance%20Any%20Beat%3A%20Blending%20Beats%20with%20Visuals%20in%20Dance%20Video%20Generation%0AAuthor%3A%20Xuanchen%20Wang%20and%20Heng%20Wang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20The%20task%20of%20generating%20dance%20from%20music%20is%20crucial%2C%20yet%20current%20methods%2C%0Awhich%20mainly%20produce%20joint%20sequences%2C%20lead%20to%20outputs%20that%20lack%20intuitiveness%0Aand%20complicate%20data%20collection%20due%20to%20the%20necessity%20for%20precise%20joint%0Aannotations.%20We%20introduce%20a%20Dance%20Any%20Beat%20Diffusion%20model%2C%20namely%20DabFusion%2C%0Athat%20employs%20music%20as%20a%20conditional%20input%20to%20directly%20create%20dance%20videos%20from%0Astill%20images%2C%20utilizing%20conditional%20image-to-video%20generation%20principles.%20This%0Aapproach%20pioneers%20the%20use%20of%20music%20as%20a%20conditioning%20factor%20in%20image-to-video%0Asynthesis.%20Our%20method%20unfolds%20in%20two%20stages%3A%20training%20an%20auto-encoder%20to%0Apredict%20latent%20optical%20flow%20between%20reference%20and%20driving%20frames%2C%20eliminating%0Athe%20need%20for%20joint%20annotation%2C%20and%20training%20a%20U-Net-based%20diffusion%20model%20to%0Aproduce%20these%20latent%20optical%20flows%20guided%20by%20music%20rhythm%20encoded%20by%20CLAP.%0AAlthough%20capable%20of%20producing%20high-quality%20dance%20videos%2C%20the%20baseline%20model%0Astruggles%20with%20rhythm%20alignment.%20We%20enhance%20the%20model%20by%20adding%20beat%0Ainformation%2C%20improving%20synchronization.%20We%20introduce%20a%202D%20motion-music%0Aalignment%20score%20%282D-MM%20Align%29%20for%20quantitative%20assessment.%20Evaluated%20on%20the%0AAIST%2B%2B%20dataset%2C%20our%20enhanced%20model%20shows%20marked%20improvements%20in%202D-MM%20Align%0Ascore%20and%20established%20metrics.%20Video%20results%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//DabFusion.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDance%2520Any%2520Beat%253A%2520Blending%2520Beats%2520with%2520Visuals%2520in%2520Dance%2520Video%2520Generation%26entry.906535625%3DXuanchen%2520Wang%2520and%2520Heng%2520Wang%2520and%2520Dongnan%2520Liu%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520generating%2520dance%2520from%2520music%2520is%2520crucial%252C%2520yet%2520current%2520methods%252C%250Awhich%2520mainly%2520produce%2520joint%2520sequences%252C%2520lead%2520to%2520outputs%2520that%2520lack%2520intuitiveness%250Aand%2520complicate%2520data%2520collection%2520due%2520to%2520the%2520necessity%2520for%2520precise%2520joint%250Aannotations.%2520We%2520introduce%2520a%2520Dance%2520Any%2520Beat%2520Diffusion%2520model%252C%2520namely%2520DabFusion%252C%250Athat%2520employs%2520music%2520as%2520a%2520conditional%2520input%2520to%2520directly%2520create%2520dance%2520videos%2520from%250Astill%2520images%252C%2520utilizing%2520conditional%2520image-to-video%2520generation%2520principles.%2520This%250Aapproach%2520pioneers%2520the%2520use%2520of%2520music%2520as%2520a%2520conditioning%2520factor%2520in%2520image-to-video%250Asynthesis.%2520Our%2520method%2520unfolds%2520in%2520two%2520stages%253A%2520training%2520an%2520auto-encoder%2520to%250Apredict%2520latent%2520optical%2520flow%2520between%2520reference%2520and%2520driving%2520frames%252C%2520eliminating%250Athe%2520need%2520for%2520joint%2520annotation%252C%2520and%2520training%2520a%2520U-Net-based%2520diffusion%2520model%2520to%250Aproduce%2520these%2520latent%2520optical%2520flows%2520guided%2520by%2520music%2520rhythm%2520encoded%2520by%2520CLAP.%250AAlthough%2520capable%2520of%2520producing%2520high-quality%2520dance%2520videos%252C%2520the%2520baseline%2520model%250Astruggles%2520with%2520rhythm%2520alignment.%2520We%2520enhance%2520the%2520model%2520by%2520adding%2520beat%250Ainformation%252C%2520improving%2520synchronization.%2520We%2520introduce%2520a%25202D%2520motion-music%250Aalignment%2520score%2520%25282D-MM%2520Align%2529%2520for%2520quantitative%2520assessment.%2520Evaluated%2520on%2520the%250AAIST%252B%252B%2520dataset%252C%2520our%2520enhanced%2520model%2520shows%2520marked%2520improvements%2520in%25202D-MM%2520Align%250Ascore%2520and%2520established%2520metrics.%2520Video%2520results%2520can%2520be%2520found%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//DabFusion.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dance%20Any%20Beat%3A%20Blending%20Beats%20with%20Visuals%20in%20Dance%20Video%20Generation&entry.906535625=Xuanchen%20Wang%20and%20Heng%20Wang%20and%20Dongnan%20Liu%20and%20Weidong%20Cai&entry.1292438233=%20%20The%20task%20of%20generating%20dance%20from%20music%20is%20crucial%2C%20yet%20current%20methods%2C%0Awhich%20mainly%20produce%20joint%20sequences%2C%20lead%20to%20outputs%20that%20lack%20intuitiveness%0Aand%20complicate%20data%20collection%20due%20to%20the%20necessity%20for%20precise%20joint%0Aannotations.%20We%20introduce%20a%20Dance%20Any%20Beat%20Diffusion%20model%2C%20namely%20DabFusion%2C%0Athat%20employs%20music%20as%20a%20conditional%20input%20to%20directly%20create%20dance%20videos%20from%0Astill%20images%2C%20utilizing%20conditional%20image-to-video%20generation%20principles.%20This%0Aapproach%20pioneers%20the%20use%20of%20music%20as%20a%20conditioning%20factor%20in%20image-to-video%0Asynthesis.%20Our%20method%20unfolds%20in%20two%20stages%3A%20training%20an%20auto-encoder%20to%0Apredict%20latent%20optical%20flow%20between%20reference%20and%20driving%20frames%2C%20eliminating%0Athe%20need%20for%20joint%20annotation%2C%20and%20training%20a%20U-Net-based%20diffusion%20model%20to%0Aproduce%20these%20latent%20optical%20flows%20guided%20by%20music%20rhythm%20encoded%20by%20CLAP.%0AAlthough%20capable%20of%20producing%20high-quality%20dance%20videos%2C%20the%20baseline%20model%0Astruggles%20with%20rhythm%20alignment.%20We%20enhance%20the%20model%20by%20adding%20beat%0Ainformation%2C%20improving%20synchronization.%20We%20introduce%20a%202D%20motion-music%0Aalignment%20score%20%282D-MM%20Align%29%20for%20quantitative%20assessment.%20Evaluated%20on%20the%0AAIST%2B%2B%20dataset%2C%20our%20enhanced%20model%20shows%20marked%20improvements%20in%202D-MM%20Align%0Ascore%20and%20established%20metrics.%20Video%20results%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//DabFusion.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09266v1&entry.124074799=Read"},
{"title": "Fourier Boundary Features Network with Wider Catchers for Glass\n  Segmentation", "author": "Xiaolin Qin and Jiacen Liu and Qianlei Wang and Shaolin Zhang and Fei Zhu and Zhang Yi", "abstract": "  Glass largely blurs the boundary between the real world and the reflection.\nThe special transmittance and reflectance quality have confused the semantic\ntasks related to machine vision. Therefore, how to clear the boundary built by\nglass, and avoid over-capturing features as false positive information in deep\nstructure, matters for constraining the segmentation of reflection surface and\npenetrating glass. We proposed the Fourier Boundary Features Network with Wider\nCatchers (FBWC), which might be the first attempt to utilize sufficiently wide\nhorizontal shallow branches without vertical deepening for guiding the fine\ngranularity segmentation boundary through primary glass semantic information.\nSpecifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large\narea segmentation and reducing excessive extraction from a structural\nperspective. We embed fine-grained features by Cross Transpose Attention (CTA),\nwhich is introduced to avoid the incomplete area within the boundary caused by\nreflection noise. For excavating glass features and balancing high-low layers\ncontext, a learnable Fourier Convolution Controller (FCC) is proposed to\nregulate information integration robustly. The proposed method has been\nvalidated on three different public glass segmentation datasets. Experimental\nresults reveal that the proposed method yields better segmentation performance\ncompared with the state-of-the-art (SOTA) methods in glass image segmentation.\n", "link": "http://arxiv.org/abs/2405.09459v1", "date": "2024-05-15", "relevancy": 2.3881, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4869}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fourier%20Boundary%20Features%20Network%20with%20Wider%20Catchers%20for%20Glass%0A%20%20Segmentation&body=Title%3A%20Fourier%20Boundary%20Features%20Network%20with%20Wider%20Catchers%20for%20Glass%0A%20%20Segmentation%0AAuthor%3A%20Xiaolin%20Qin%20and%20Jiacen%20Liu%20and%20Qianlei%20Wang%20and%20Shaolin%20Zhang%20and%20Fei%20Zhu%20and%20Zhang%20Yi%0AAbstract%3A%20%20%20Glass%20largely%20blurs%20the%20boundary%20between%20the%20real%20world%20and%20the%20reflection.%0AThe%20special%20transmittance%20and%20reflectance%20quality%20have%20confused%20the%20semantic%0Atasks%20related%20to%20machine%20vision.%20Therefore%2C%20how%20to%20clear%20the%20boundary%20built%20by%0Aglass%2C%20and%20avoid%20over-capturing%20features%20as%20false%20positive%20information%20in%20deep%0Astructure%2C%20matters%20for%20constraining%20the%20segmentation%20of%20reflection%20surface%20and%0Apenetrating%20glass.%20We%20proposed%20the%20Fourier%20Boundary%20Features%20Network%20with%20Wider%0ACatchers%20%28FBWC%29%2C%20which%20might%20be%20the%20first%20attempt%20to%20utilize%20sufficiently%20wide%0Ahorizontal%20shallow%20branches%20without%20vertical%20deepening%20for%20guiding%20the%20fine%0Agranularity%20segmentation%20boundary%20through%20primary%20glass%20semantic%20information.%0ASpecifically%2C%20we%20designed%20the%20Wider%20Coarse-Catchers%20%28WCC%29%20for%20anchoring%20large%0Aarea%20segmentation%20and%20reducing%20excessive%20extraction%20from%20a%20structural%0Aperspective.%20We%20embed%20fine-grained%20features%20by%20Cross%20Transpose%20Attention%20%28CTA%29%2C%0Awhich%20is%20introduced%20to%20avoid%20the%20incomplete%20area%20within%20the%20boundary%20caused%20by%0Areflection%20noise.%20For%20excavating%20glass%20features%20and%20balancing%20high-low%20layers%0Acontext%2C%20a%20learnable%20Fourier%20Convolution%20Controller%20%28FCC%29%20is%20proposed%20to%0Aregulate%20information%20integration%20robustly.%20The%20proposed%20method%20has%20been%0Avalidated%20on%20three%20different%20public%20glass%20segmentation%20datasets.%20Experimental%0Aresults%20reveal%20that%20the%20proposed%20method%20yields%20better%20segmentation%20performance%0Acompared%20with%20the%20state-of-the-art%20%28SOTA%29%20methods%20in%20glass%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFourier%2520Boundary%2520Features%2520Network%2520with%2520Wider%2520Catchers%2520for%2520Glass%250A%2520%2520Segmentation%26entry.906535625%3DXiaolin%2520Qin%2520and%2520Jiacen%2520Liu%2520and%2520Qianlei%2520Wang%2520and%2520Shaolin%2520Zhang%2520and%2520Fei%2520Zhu%2520and%2520Zhang%2520Yi%26entry.1292438233%3D%2520%2520Glass%2520largely%2520blurs%2520the%2520boundary%2520between%2520the%2520real%2520world%2520and%2520the%2520reflection.%250AThe%2520special%2520transmittance%2520and%2520reflectance%2520quality%2520have%2520confused%2520the%2520semantic%250Atasks%2520related%2520to%2520machine%2520vision.%2520Therefore%252C%2520how%2520to%2520clear%2520the%2520boundary%2520built%2520by%250Aglass%252C%2520and%2520avoid%2520over-capturing%2520features%2520as%2520false%2520positive%2520information%2520in%2520deep%250Astructure%252C%2520matters%2520for%2520constraining%2520the%2520segmentation%2520of%2520reflection%2520surface%2520and%250Apenetrating%2520glass.%2520We%2520proposed%2520the%2520Fourier%2520Boundary%2520Features%2520Network%2520with%2520Wider%250ACatchers%2520%2528FBWC%2529%252C%2520which%2520might%2520be%2520the%2520first%2520attempt%2520to%2520utilize%2520sufficiently%2520wide%250Ahorizontal%2520shallow%2520branches%2520without%2520vertical%2520deepening%2520for%2520guiding%2520the%2520fine%250Agranularity%2520segmentation%2520boundary%2520through%2520primary%2520glass%2520semantic%2520information.%250ASpecifically%252C%2520we%2520designed%2520the%2520Wider%2520Coarse-Catchers%2520%2528WCC%2529%2520for%2520anchoring%2520large%250Aarea%2520segmentation%2520and%2520reducing%2520excessive%2520extraction%2520from%2520a%2520structural%250Aperspective.%2520We%2520embed%2520fine-grained%2520features%2520by%2520Cross%2520Transpose%2520Attention%2520%2528CTA%2529%252C%250Awhich%2520is%2520introduced%2520to%2520avoid%2520the%2520incomplete%2520area%2520within%2520the%2520boundary%2520caused%2520by%250Areflection%2520noise.%2520For%2520excavating%2520glass%2520features%2520and%2520balancing%2520high-low%2520layers%250Acontext%252C%2520a%2520learnable%2520Fourier%2520Convolution%2520Controller%2520%2528FCC%2529%2520is%2520proposed%2520to%250Aregulate%2520information%2520integration%2520robustly.%2520The%2520proposed%2520method%2520has%2520been%250Avalidated%2520on%2520three%2520different%2520public%2520glass%2520segmentation%2520datasets.%2520Experimental%250Aresults%2520reveal%2520that%2520the%2520proposed%2520method%2520yields%2520better%2520segmentation%2520performance%250Acompared%2520with%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520glass%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fourier%20Boundary%20Features%20Network%20with%20Wider%20Catchers%20for%20Glass%0A%20%20Segmentation&entry.906535625=Xiaolin%20Qin%20and%20Jiacen%20Liu%20and%20Qianlei%20Wang%20and%20Shaolin%20Zhang%20and%20Fei%20Zhu%20and%20Zhang%20Yi&entry.1292438233=%20%20Glass%20largely%20blurs%20the%20boundary%20between%20the%20real%20world%20and%20the%20reflection.%0AThe%20special%20transmittance%20and%20reflectance%20quality%20have%20confused%20the%20semantic%0Atasks%20related%20to%20machine%20vision.%20Therefore%2C%20how%20to%20clear%20the%20boundary%20built%20by%0Aglass%2C%20and%20avoid%20over-capturing%20features%20as%20false%20positive%20information%20in%20deep%0Astructure%2C%20matters%20for%20constraining%20the%20segmentation%20of%20reflection%20surface%20and%0Apenetrating%20glass.%20We%20proposed%20the%20Fourier%20Boundary%20Features%20Network%20with%20Wider%0ACatchers%20%28FBWC%29%2C%20which%20might%20be%20the%20first%20attempt%20to%20utilize%20sufficiently%20wide%0Ahorizontal%20shallow%20branches%20without%20vertical%20deepening%20for%20guiding%20the%20fine%0Agranularity%20segmentation%20boundary%20through%20primary%20glass%20semantic%20information.%0ASpecifically%2C%20we%20designed%20the%20Wider%20Coarse-Catchers%20%28WCC%29%20for%20anchoring%20large%0Aarea%20segmentation%20and%20reducing%20excessive%20extraction%20from%20a%20structural%0Aperspective.%20We%20embed%20fine-grained%20features%20by%20Cross%20Transpose%20Attention%20%28CTA%29%2C%0Awhich%20is%20introduced%20to%20avoid%20the%20incomplete%20area%20within%20the%20boundary%20caused%20by%0Areflection%20noise.%20For%20excavating%20glass%20features%20and%20balancing%20high-low%20layers%0Acontext%2C%20a%20learnable%20Fourier%20Convolution%20Controller%20%28FCC%29%20is%20proposed%20to%0Aregulate%20information%20integration%20robustly.%20The%20proposed%20method%20has%20been%0Avalidated%20on%20three%20different%20public%20glass%20segmentation%20datasets.%20Experimental%0Aresults%20reveal%20that%20the%20proposed%20method%20yields%20better%20segmentation%20performance%0Acompared%20with%20the%20state-of-the-art%20%28SOTA%29%20methods%20in%20glass%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09459v1&entry.124074799=Read"},
{"title": "A Survey On Text-to-3D Contents Generation In The Wild", "author": "Chenhan Jiang", "abstract": "  3D content creation plays a vital role in various applications, such as\ngaming, robotics simulation, and virtual reality. However, the process is\nlabor-intensive and time-consuming, requiring skilled designers to invest\nconsiderable effort in creating a single 3D asset. To address this challenge,\ntext-to-3D generation technologies have emerged as a promising solution for\nautomating 3D creation. Leveraging the success of large vision language models,\nthese techniques aim to generate 3D content based on textual descriptions.\nDespite recent advancements in this area, existing solutions still face\nsignificant limitations in terms of generation quality and efficiency. In this\nsurvey, we conduct an in-depth investigation of the latest text-to-3D creation\nmethods. We provide a comprehensive background on text-to-3D creation,\nincluding discussions on datasets employed in training and evaluation metrics\nused to assess the quality of generated 3D models. Then, we delve into the\nvarious 3D representations that serve as the foundation for the 3D generation\nprocess. Furthermore, we present a thorough comparison of the rapidly growing\nliterature on generative pipelines, categorizing them into feedforward\ngenerators, optimization-based generation, and view reconstruction approaches.\nBy examining the strengths and weaknesses of these methods, we aim to shed\nlight on their respective capabilities and limitations. Lastly, we point out\nseveral promising avenues for future research. With this survey, we hope to\ninspire researchers further to explore the potential of open-vocabulary\ntext-conditioned 3D content creation.\n", "link": "http://arxiv.org/abs/2405.09431v1", "date": "2024-05-15", "relevancy": 2.3493, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.634}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.559}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20On%20Text-to-3D%20Contents%20Generation%20In%20The%20Wild&body=Title%3A%20A%20Survey%20On%20Text-to-3D%20Contents%20Generation%20In%20The%20Wild%0AAuthor%3A%20Chenhan%20Jiang%0AAbstract%3A%20%20%203D%20content%20creation%20plays%20a%20vital%20role%20in%20various%20applications%2C%20such%20as%0Agaming%2C%20robotics%20simulation%2C%20and%20virtual%20reality.%20However%2C%20the%20process%20is%0Alabor-intensive%20and%20time-consuming%2C%20requiring%20skilled%20designers%20to%20invest%0Aconsiderable%20effort%20in%20creating%20a%20single%203D%20asset.%20To%20address%20this%20challenge%2C%0Atext-to-3D%20generation%20technologies%20have%20emerged%20as%20a%20promising%20solution%20for%0Aautomating%203D%20creation.%20Leveraging%20the%20success%20of%20large%20vision%20language%20models%2C%0Athese%20techniques%20aim%20to%20generate%203D%20content%20based%20on%20textual%20descriptions.%0ADespite%20recent%20advancements%20in%20this%20area%2C%20existing%20solutions%20still%20face%0Asignificant%20limitations%20in%20terms%20of%20generation%20quality%20and%20efficiency.%20In%20this%0Asurvey%2C%20we%20conduct%20an%20in-depth%20investigation%20of%20the%20latest%20text-to-3D%20creation%0Amethods.%20We%20provide%20a%20comprehensive%20background%20on%20text-to-3D%20creation%2C%0Aincluding%20discussions%20on%20datasets%20employed%20in%20training%20and%20evaluation%20metrics%0Aused%20to%20assess%20the%20quality%20of%20generated%203D%20models.%20Then%2C%20we%20delve%20into%20the%0Avarious%203D%20representations%20that%20serve%20as%20the%20foundation%20for%20the%203D%20generation%0Aprocess.%20Furthermore%2C%20we%20present%20a%20thorough%20comparison%20of%20the%20rapidly%20growing%0Aliterature%20on%20generative%20pipelines%2C%20categorizing%20them%20into%20feedforward%0Agenerators%2C%20optimization-based%20generation%2C%20and%20view%20reconstruction%20approaches.%0ABy%20examining%20the%20strengths%20and%20weaknesses%20of%20these%20methods%2C%20we%20aim%20to%20shed%0Alight%20on%20their%20respective%20capabilities%20and%20limitations.%20Lastly%2C%20we%20point%20out%0Aseveral%20promising%20avenues%20for%20future%20research.%20With%20this%20survey%2C%20we%20hope%20to%0Ainspire%20researchers%20further%20to%20explore%20the%20potential%20of%20open-vocabulary%0Atext-conditioned%203D%20content%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520On%2520Text-to-3D%2520Contents%2520Generation%2520In%2520The%2520Wild%26entry.906535625%3DChenhan%2520Jiang%26entry.1292438233%3D%2520%25203D%2520content%2520creation%2520plays%2520a%2520vital%2520role%2520in%2520various%2520applications%252C%2520such%2520as%250Agaming%252C%2520robotics%2520simulation%252C%2520and%2520virtual%2520reality.%2520However%252C%2520the%2520process%2520is%250Alabor-intensive%2520and%2520time-consuming%252C%2520requiring%2520skilled%2520designers%2520to%2520invest%250Aconsiderable%2520effort%2520in%2520creating%2520a%2520single%25203D%2520asset.%2520To%2520address%2520this%2520challenge%252C%250Atext-to-3D%2520generation%2520technologies%2520have%2520emerged%2520as%2520a%2520promising%2520solution%2520for%250Aautomating%25203D%2520creation.%2520Leveraging%2520the%2520success%2520of%2520large%2520vision%2520language%2520models%252C%250Athese%2520techniques%2520aim%2520to%2520generate%25203D%2520content%2520based%2520on%2520textual%2520descriptions.%250ADespite%2520recent%2520advancements%2520in%2520this%2520area%252C%2520existing%2520solutions%2520still%2520face%250Asignificant%2520limitations%2520in%2520terms%2520of%2520generation%2520quality%2520and%2520efficiency.%2520In%2520this%250Asurvey%252C%2520we%2520conduct%2520an%2520in-depth%2520investigation%2520of%2520the%2520latest%2520text-to-3D%2520creation%250Amethods.%2520We%2520provide%2520a%2520comprehensive%2520background%2520on%2520text-to-3D%2520creation%252C%250Aincluding%2520discussions%2520on%2520datasets%2520employed%2520in%2520training%2520and%2520evaluation%2520metrics%250Aused%2520to%2520assess%2520the%2520quality%2520of%2520generated%25203D%2520models.%2520Then%252C%2520we%2520delve%2520into%2520the%250Avarious%25203D%2520representations%2520that%2520serve%2520as%2520the%2520foundation%2520for%2520the%25203D%2520generation%250Aprocess.%2520Furthermore%252C%2520we%2520present%2520a%2520thorough%2520comparison%2520of%2520the%2520rapidly%2520growing%250Aliterature%2520on%2520generative%2520pipelines%252C%2520categorizing%2520them%2520into%2520feedforward%250Agenerators%252C%2520optimization-based%2520generation%252C%2520and%2520view%2520reconstruction%2520approaches.%250ABy%2520examining%2520the%2520strengths%2520and%2520weaknesses%2520of%2520these%2520methods%252C%2520we%2520aim%2520to%2520shed%250Alight%2520on%2520their%2520respective%2520capabilities%2520and%2520limitations.%2520Lastly%252C%2520we%2520point%2520out%250Aseveral%2520promising%2520avenues%2520for%2520future%2520research.%2520With%2520this%2520survey%252C%2520we%2520hope%2520to%250Ainspire%2520researchers%2520further%2520to%2520explore%2520the%2520potential%2520of%2520open-vocabulary%250Atext-conditioned%25203D%2520content%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20On%20Text-to-3D%20Contents%20Generation%20In%20The%20Wild&entry.906535625=Chenhan%20Jiang&entry.1292438233=%20%203D%20content%20creation%20plays%20a%20vital%20role%20in%20various%20applications%2C%20such%20as%0Agaming%2C%20robotics%20simulation%2C%20and%20virtual%20reality.%20However%2C%20the%20process%20is%0Alabor-intensive%20and%20time-consuming%2C%20requiring%20skilled%20designers%20to%20invest%0Aconsiderable%20effort%20in%20creating%20a%20single%203D%20asset.%20To%20address%20this%20challenge%2C%0Atext-to-3D%20generation%20technologies%20have%20emerged%20as%20a%20promising%20solution%20for%0Aautomating%203D%20creation.%20Leveraging%20the%20success%20of%20large%20vision%20language%20models%2C%0Athese%20techniques%20aim%20to%20generate%203D%20content%20based%20on%20textual%20descriptions.%0ADespite%20recent%20advancements%20in%20this%20area%2C%20existing%20solutions%20still%20face%0Asignificant%20limitations%20in%20terms%20of%20generation%20quality%20and%20efficiency.%20In%20this%0Asurvey%2C%20we%20conduct%20an%20in-depth%20investigation%20of%20the%20latest%20text-to-3D%20creation%0Amethods.%20We%20provide%20a%20comprehensive%20background%20on%20text-to-3D%20creation%2C%0Aincluding%20discussions%20on%20datasets%20employed%20in%20training%20and%20evaluation%20metrics%0Aused%20to%20assess%20the%20quality%20of%20generated%203D%20models.%20Then%2C%20we%20delve%20into%20the%0Avarious%203D%20representations%20that%20serve%20as%20the%20foundation%20for%20the%203D%20generation%0Aprocess.%20Furthermore%2C%20we%20present%20a%20thorough%20comparison%20of%20the%20rapidly%20growing%0Aliterature%20on%20generative%20pipelines%2C%20categorizing%20them%20into%20feedforward%0Agenerators%2C%20optimization-based%20generation%2C%20and%20view%20reconstruction%20approaches.%0ABy%20examining%20the%20strengths%20and%20weaknesses%20of%20these%20methods%2C%20we%20aim%20to%20shed%0Alight%20on%20their%20respective%20capabilities%20and%20limitations.%20Lastly%2C%20we%20point%20out%0Aseveral%20promising%20avenues%20for%20future%20research.%20With%20this%20survey%2C%20we%20hope%20to%0Ainspire%20researchers%20further%20to%20explore%20the%20potential%20of%20open-vocabulary%0Atext-conditioned%203D%20content%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09431v1&entry.124074799=Read"},
{"title": "Progressive Depth Decoupling and Modulating for Flexible Depth\n  Completion", "author": "Zhiwen Yang and Jiehua Zhang and Liang Li and Chenggang Yan and Yaoqi Sun and Haibing Yin", "abstract": "  Image-guided depth completion aims at generating a dense depth map from\nsparse LiDAR data and RGB image. Recent methods have shown promising\nperformance by reformulating it as a classification problem with two sub-tasks:\ndepth discretization and probability prediction. They divide the depth range\ninto several discrete depth values as depth categories, serving as priors for\nscene depth distributions. However, previous depth discretization methods are\neasy to be impacted by depth distribution variations across different scenes,\nresulting in suboptimal scene depth distribution priors. To address the above\nproblem, we propose a progressive depth decoupling and modulating network,\nwhich incrementally decouples the depth range into bins and adaptively\ngenerates multi-scale dense depth maps in multiple stages. Specifically, we\nfirst design a Bins Initializing Module (BIM) to construct the seed bins by\nexploring the depth distribution information within a sparse depth map,\nadapting variations of depth distribution. Then, we devise an incremental depth\ndecoupling branch to progressively refine the depth distribution information\nfrom global to local. Meanwhile, an adaptive depth modulating branch is\ndeveloped to progressively improve the probability representation from\ncoarse-grained to fine-grained. And the bi-directional information interactions\nare proposed to strengthen the information interaction between those two\nbranches (sub-tasks) for promoting information complementation in each branch.\nFurther, we introduce a multi-scale supervision mechanism to learn the depth\ndistribution information in latent features and enhance the adaptation\ncapability across different scenes. Experimental results on public datasets\ndemonstrate that our method outperforms the state-of-the-art methods. The code\nwill be open-sourced at [this https URL](https://github.com/Cisse-away/PDDM).\n", "link": "http://arxiv.org/abs/2405.09342v1", "date": "2024-05-15", "relevancy": 2.3292, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5899}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5814}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Depth%20Decoupling%20and%20Modulating%20for%20Flexible%20Depth%0A%20%20Completion&body=Title%3A%20Progressive%20Depth%20Decoupling%20and%20Modulating%20for%20Flexible%20Depth%0A%20%20Completion%0AAuthor%3A%20Zhiwen%20Yang%20and%20Jiehua%20Zhang%20and%20Liang%20Li%20and%20Chenggang%20Yan%20and%20Yaoqi%20Sun%20and%20Haibing%20Yin%0AAbstract%3A%20%20%20Image-guided%20depth%20completion%20aims%20at%20generating%20a%20dense%20depth%20map%20from%0Asparse%20LiDAR%20data%20and%20RGB%20image.%20Recent%20methods%20have%20shown%20promising%0Aperformance%20by%20reformulating%20it%20as%20a%20classification%20problem%20with%20two%20sub-tasks%3A%0Adepth%20discretization%20and%20probability%20prediction.%20They%20divide%20the%20depth%20range%0Ainto%20several%20discrete%20depth%20values%20as%20depth%20categories%2C%20serving%20as%20priors%20for%0Ascene%20depth%20distributions.%20However%2C%20previous%20depth%20discretization%20methods%20are%0Aeasy%20to%20be%20impacted%20by%20depth%20distribution%20variations%20across%20different%20scenes%2C%0Aresulting%20in%20suboptimal%20scene%20depth%20distribution%20priors.%20To%20address%20the%20above%0Aproblem%2C%20we%20propose%20a%20progressive%20depth%20decoupling%20and%20modulating%20network%2C%0Awhich%20incrementally%20decouples%20the%20depth%20range%20into%20bins%20and%20adaptively%0Agenerates%20multi-scale%20dense%20depth%20maps%20in%20multiple%20stages.%20Specifically%2C%20we%0Afirst%20design%20a%20Bins%20Initializing%20Module%20%28BIM%29%20to%20construct%20the%20seed%20bins%20by%0Aexploring%20the%20depth%20distribution%20information%20within%20a%20sparse%20depth%20map%2C%0Aadapting%20variations%20of%20depth%20distribution.%20Then%2C%20we%20devise%20an%20incremental%20depth%0Adecoupling%20branch%20to%20progressively%20refine%20the%20depth%20distribution%20information%0Afrom%20global%20to%20local.%20Meanwhile%2C%20an%20adaptive%20depth%20modulating%20branch%20is%0Adeveloped%20to%20progressively%20improve%20the%20probability%20representation%20from%0Acoarse-grained%20to%20fine-grained.%20And%20the%20bi-directional%20information%20interactions%0Aare%20proposed%20to%20strengthen%20the%20information%20interaction%20between%20those%20two%0Abranches%20%28sub-tasks%29%20for%20promoting%20information%20complementation%20in%20each%20branch.%0AFurther%2C%20we%20introduce%20a%20multi-scale%20supervision%20mechanism%20to%20learn%20the%20depth%0Adistribution%20information%20in%20latent%20features%20and%20enhance%20the%20adaptation%0Acapability%20across%20different%20scenes.%20Experimental%20results%20on%20public%20datasets%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%20The%20code%0Awill%20be%20open-sourced%20at%20%5Bthis%20https%20URL%5D%28https%3A//github.com/Cisse-away/PDDM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09342v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Depth%2520Decoupling%2520and%2520Modulating%2520for%2520Flexible%2520Depth%250A%2520%2520Completion%26entry.906535625%3DZhiwen%2520Yang%2520and%2520Jiehua%2520Zhang%2520and%2520Liang%2520Li%2520and%2520Chenggang%2520Yan%2520and%2520Yaoqi%2520Sun%2520and%2520Haibing%2520Yin%26entry.1292438233%3D%2520%2520Image-guided%2520depth%2520completion%2520aims%2520at%2520generating%2520a%2520dense%2520depth%2520map%2520from%250Asparse%2520LiDAR%2520data%2520and%2520RGB%2520image.%2520Recent%2520methods%2520have%2520shown%2520promising%250Aperformance%2520by%2520reformulating%2520it%2520as%2520a%2520classification%2520problem%2520with%2520two%2520sub-tasks%253A%250Adepth%2520discretization%2520and%2520probability%2520prediction.%2520They%2520divide%2520the%2520depth%2520range%250Ainto%2520several%2520discrete%2520depth%2520values%2520as%2520depth%2520categories%252C%2520serving%2520as%2520priors%2520for%250Ascene%2520depth%2520distributions.%2520However%252C%2520previous%2520depth%2520discretization%2520methods%2520are%250Aeasy%2520to%2520be%2520impacted%2520by%2520depth%2520distribution%2520variations%2520across%2520different%2520scenes%252C%250Aresulting%2520in%2520suboptimal%2520scene%2520depth%2520distribution%2520priors.%2520To%2520address%2520the%2520above%250Aproblem%252C%2520we%2520propose%2520a%2520progressive%2520depth%2520decoupling%2520and%2520modulating%2520network%252C%250Awhich%2520incrementally%2520decouples%2520the%2520depth%2520range%2520into%2520bins%2520and%2520adaptively%250Agenerates%2520multi-scale%2520dense%2520depth%2520maps%2520in%2520multiple%2520stages.%2520Specifically%252C%2520we%250Afirst%2520design%2520a%2520Bins%2520Initializing%2520Module%2520%2528BIM%2529%2520to%2520construct%2520the%2520seed%2520bins%2520by%250Aexploring%2520the%2520depth%2520distribution%2520information%2520within%2520a%2520sparse%2520depth%2520map%252C%250Aadapting%2520variations%2520of%2520depth%2520distribution.%2520Then%252C%2520we%2520devise%2520an%2520incremental%2520depth%250Adecoupling%2520branch%2520to%2520progressively%2520refine%2520the%2520depth%2520distribution%2520information%250Afrom%2520global%2520to%2520local.%2520Meanwhile%252C%2520an%2520adaptive%2520depth%2520modulating%2520branch%2520is%250Adeveloped%2520to%2520progressively%2520improve%2520the%2520probability%2520representation%2520from%250Acoarse-grained%2520to%2520fine-grained.%2520And%2520the%2520bi-directional%2520information%2520interactions%250Aare%2520proposed%2520to%2520strengthen%2520the%2520information%2520interaction%2520between%2520those%2520two%250Abranches%2520%2528sub-tasks%2529%2520for%2520promoting%2520information%2520complementation%2520in%2520each%2520branch.%250AFurther%252C%2520we%2520introduce%2520a%2520multi-scale%2520supervision%2520mechanism%2520to%2520learn%2520the%2520depth%250Adistribution%2520information%2520in%2520latent%2520features%2520and%2520enhance%2520the%2520adaptation%250Acapability%2520across%2520different%2520scenes.%2520Experimental%2520results%2520on%2520public%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods.%2520The%2520code%250Awill%2520be%2520open-sourced%2520at%2520%255Bthis%2520https%2520URL%255D%2528https%253A//github.com/Cisse-away/PDDM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09342v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Depth%20Decoupling%20and%20Modulating%20for%20Flexible%20Depth%0A%20%20Completion&entry.906535625=Zhiwen%20Yang%20and%20Jiehua%20Zhang%20and%20Liang%20Li%20and%20Chenggang%20Yan%20and%20Yaoqi%20Sun%20and%20Haibing%20Yin&entry.1292438233=%20%20Image-guided%20depth%20completion%20aims%20at%20generating%20a%20dense%20depth%20map%20from%0Asparse%20LiDAR%20data%20and%20RGB%20image.%20Recent%20methods%20have%20shown%20promising%0Aperformance%20by%20reformulating%20it%20as%20a%20classification%20problem%20with%20two%20sub-tasks%3A%0Adepth%20discretization%20and%20probability%20prediction.%20They%20divide%20the%20depth%20range%0Ainto%20several%20discrete%20depth%20values%20as%20depth%20categories%2C%20serving%20as%20priors%20for%0Ascene%20depth%20distributions.%20However%2C%20previous%20depth%20discretization%20methods%20are%0Aeasy%20to%20be%20impacted%20by%20depth%20distribution%20variations%20across%20different%20scenes%2C%0Aresulting%20in%20suboptimal%20scene%20depth%20distribution%20priors.%20To%20address%20the%20above%0Aproblem%2C%20we%20propose%20a%20progressive%20depth%20decoupling%20and%20modulating%20network%2C%0Awhich%20incrementally%20decouples%20the%20depth%20range%20into%20bins%20and%20adaptively%0Agenerates%20multi-scale%20dense%20depth%20maps%20in%20multiple%20stages.%20Specifically%2C%20we%0Afirst%20design%20a%20Bins%20Initializing%20Module%20%28BIM%29%20to%20construct%20the%20seed%20bins%20by%0Aexploring%20the%20depth%20distribution%20information%20within%20a%20sparse%20depth%20map%2C%0Aadapting%20variations%20of%20depth%20distribution.%20Then%2C%20we%20devise%20an%20incremental%20depth%0Adecoupling%20branch%20to%20progressively%20refine%20the%20depth%20distribution%20information%0Afrom%20global%20to%20local.%20Meanwhile%2C%20an%20adaptive%20depth%20modulating%20branch%20is%0Adeveloped%20to%20progressively%20improve%20the%20probability%20representation%20from%0Acoarse-grained%20to%20fine-grained.%20And%20the%20bi-directional%20information%20interactions%0Aare%20proposed%20to%20strengthen%20the%20information%20interaction%20between%20those%20two%0Abranches%20%28sub-tasks%29%20for%20promoting%20information%20complementation%20in%20each%20branch.%0AFurther%2C%20we%20introduce%20a%20multi-scale%20supervision%20mechanism%20to%20learn%20the%20depth%0Adistribution%20information%20in%20latent%20features%20and%20enhance%20the%20adaptation%0Acapability%20across%20different%20scenes.%20Experimental%20results%20on%20public%20datasets%0Ademonstrate%20that%20our%20method%20outperforms%20the%20state-of-the-art%20methods.%20The%20code%0Awill%20be%20open-sourced%20at%20%5Bthis%20https%20URL%5D%28https%3A//github.com/Cisse-away/PDDM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09342v1&entry.124074799=Read"},
{"title": "Matching domain experts by training from scratch on domain knowledge", "author": "Xiaoliang Luo and Guangzhi Sun and Bradley C. Love", "abstract": "  Recently, large language models (LLMs) have outperformed human experts in\npredicting the results of neuroscience experiments (Luo et al., 2024). What is\nthe basis for this performance? One possibility is that statistical patterns in\nthat specific scientific literature, as opposed to emergent reasoning abilities\narising from broader training, underlie LLMs' performance. To evaluate this\npossibility, we trained (next word prediction) a relatively small\n124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.\nDespite being orders of magnitude smaller than larger LLMs trained on trillions\nof tokens, small models achieved expert-level performance in predicting\nneuroscience results. Small models trained on the neuroscience literature\nsucceeded when they were trained from scratch using a tokenizer specifically\ntrained on neuroscience text or when the neuroscience literature was used to\nfinetune a pretrained GPT-2. Our results indicate that expert-level performance\nmay be attained by even small LLMs through domain-specific, auto-regressive\ntraining approaches.\n", "link": "http://arxiv.org/abs/2405.09395v1", "date": "2024-05-15", "relevancy": 2.2987, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4614}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4599}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge&body=Title%3A%20Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge%0AAuthor%3A%20Xiaoliang%20Luo%20and%20Guangzhi%20Sun%20and%20Bradley%20C.%20Love%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20outperformed%20human%20experts%20in%0Apredicting%20the%20results%20of%20neuroscience%20experiments%20%28Luo%20et%20al.%2C%202024%29.%20What%20is%0Athe%20basis%20for%20this%20performance%3F%20One%20possibility%20is%20that%20statistical%20patterns%20in%0Athat%20specific%20scientific%20literature%2C%20as%20opposed%20to%20emergent%20reasoning%20abilities%0Aarising%20from%20broader%20training%2C%20underlie%20LLMs%27%20performance.%20To%20evaluate%20this%0Apossibility%2C%20we%20trained%20%28next%20word%20prediction%29%20a%20relatively%20small%0A124M-parameter%20GPT-2%20model%20on%201.3%20billion%20tokens%20of%20domain-specific%20knowledge.%0ADespite%20being%20orders%20of%20magnitude%20smaller%20than%20larger%20LLMs%20trained%20on%20trillions%0Aof%20tokens%2C%20small%20models%20achieved%20expert-level%20performance%20in%20predicting%0Aneuroscience%20results.%20Small%20models%20trained%20on%20the%20neuroscience%20literature%0Asucceeded%20when%20they%20were%20trained%20from%20scratch%20using%20a%20tokenizer%20specifically%0Atrained%20on%20neuroscience%20text%20or%20when%20the%20neuroscience%20literature%20was%20used%20to%0Afinetune%20a%20pretrained%20GPT-2.%20Our%20results%20indicate%20that%20expert-level%20performance%0Amay%20be%20attained%20by%20even%20small%20LLMs%20through%20domain-specific%2C%20auto-regressive%0Atraining%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520domain%2520experts%2520by%2520training%2520from%2520scratch%2520on%2520domain%2520knowledge%26entry.906535625%3DXiaoliang%2520Luo%2520and%2520Guangzhi%2520Sun%2520and%2520Bradley%2520C.%2520Love%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520outperformed%2520human%2520experts%2520in%250Apredicting%2520the%2520results%2520of%2520neuroscience%2520experiments%2520%2528Luo%2520et%2520al.%252C%25202024%2529.%2520What%2520is%250Athe%2520basis%2520for%2520this%2520performance%253F%2520One%2520possibility%2520is%2520that%2520statistical%2520patterns%2520in%250Athat%2520specific%2520scientific%2520literature%252C%2520as%2520opposed%2520to%2520emergent%2520reasoning%2520abilities%250Aarising%2520from%2520broader%2520training%252C%2520underlie%2520LLMs%2527%2520performance.%2520To%2520evaluate%2520this%250Apossibility%252C%2520we%2520trained%2520%2528next%2520word%2520prediction%2529%2520a%2520relatively%2520small%250A124M-parameter%2520GPT-2%2520model%2520on%25201.3%2520billion%2520tokens%2520of%2520domain-specific%2520knowledge.%250ADespite%2520being%2520orders%2520of%2520magnitude%2520smaller%2520than%2520larger%2520LLMs%2520trained%2520on%2520trillions%250Aof%2520tokens%252C%2520small%2520models%2520achieved%2520expert-level%2520performance%2520in%2520predicting%250Aneuroscience%2520results.%2520Small%2520models%2520trained%2520on%2520the%2520neuroscience%2520literature%250Asucceeded%2520when%2520they%2520were%2520trained%2520from%2520scratch%2520using%2520a%2520tokenizer%2520specifically%250Atrained%2520on%2520neuroscience%2520text%2520or%2520when%2520the%2520neuroscience%2520literature%2520was%2520used%2520to%250Afinetune%2520a%2520pretrained%2520GPT-2.%2520Our%2520results%2520indicate%2520that%2520expert-level%2520performance%250Amay%2520be%2520attained%2520by%2520even%2520small%2520LLMs%2520through%2520domain-specific%252C%2520auto-regressive%250Atraining%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20domain%20experts%20by%20training%20from%20scratch%20on%20domain%20knowledge&entry.906535625=Xiaoliang%20Luo%20and%20Guangzhi%20Sun%20and%20Bradley%20C.%20Love&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20outperformed%20human%20experts%20in%0Apredicting%20the%20results%20of%20neuroscience%20experiments%20%28Luo%20et%20al.%2C%202024%29.%20What%20is%0Athe%20basis%20for%20this%20performance%3F%20One%20possibility%20is%20that%20statistical%20patterns%20in%0Athat%20specific%20scientific%20literature%2C%20as%20opposed%20to%20emergent%20reasoning%20abilities%0Aarising%20from%20broader%20training%2C%20underlie%20LLMs%27%20performance.%20To%20evaluate%20this%0Apossibility%2C%20we%20trained%20%28next%20word%20prediction%29%20a%20relatively%20small%0A124M-parameter%20GPT-2%20model%20on%201.3%20billion%20tokens%20of%20domain-specific%20knowledge.%0ADespite%20being%20orders%20of%20magnitude%20smaller%20than%20larger%20LLMs%20trained%20on%20trillions%0Aof%20tokens%2C%20small%20models%20achieved%20expert-level%20performance%20in%20predicting%0Aneuroscience%20results.%20Small%20models%20trained%20on%20the%20neuroscience%20literature%0Asucceeded%20when%20they%20were%20trained%20from%20scratch%20using%20a%20tokenizer%20specifically%0Atrained%20on%20neuroscience%20text%20or%20when%20the%20neuroscience%20literature%20was%20used%20to%0Afinetune%20a%20pretrained%20GPT-2.%20Our%20results%20indicate%20that%20expert-level%20performance%0Amay%20be%20attained%20by%20even%20small%20LLMs%20through%20domain-specific%2C%20auto-regressive%0Atraining%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09395v1&entry.124074799=Read"},
{"title": "Graph Neural Network based Handwritten Trajectories Recognition", "author": "Anuj Sharma and Sukhdeep Singh and S Ratna", "abstract": "  The graph neural networks has been proved to be an efficient machine learning\ntechnique in real life applications. The handwritten recognition is one of the\nuseful area in real life use where both offline and online handwriting\nrecognition are required. The chain code as feature extraction technique has\nshown significant results in literature and we have been able to use chain\ncodes with graph neural networks. To the best of our knowledge, this work\npresents first time a novel combination of handwritten trajectories features as\nchain codes and graph neural networks together. The handwritten trajectories\nfor offline handwritten text has been evaluated using recovery of drawing\norder, whereas online handwritten trajectories are directly used with chain\ncodes. Our results prove that present combination surpass previous results and\nminimize error rate in few epochs only.\n", "link": "http://arxiv.org/abs/2405.09247v1", "date": "2024-05-15", "relevancy": 2.2527, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4611}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4505}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network%20based%20Handwritten%20Trajectories%20Recognition&body=Title%3A%20Graph%20Neural%20Network%20based%20Handwritten%20Trajectories%20Recognition%0AAuthor%3A%20Anuj%20Sharma%20and%20Sukhdeep%20Singh%20and%20S%20Ratna%0AAbstract%3A%20%20%20The%20graph%20neural%20networks%20has%20been%20proved%20to%20be%20an%20efficient%20machine%20learning%0Atechnique%20in%20real%20life%20applications.%20The%20handwritten%20recognition%20is%20one%20of%20the%0Auseful%20area%20in%20real%20life%20use%20where%20both%20offline%20and%20online%20handwriting%0Arecognition%20are%20required.%20The%20chain%20code%20as%20feature%20extraction%20technique%20has%0Ashown%20significant%20results%20in%20literature%20and%20we%20have%20been%20able%20to%20use%20chain%0Acodes%20with%20graph%20neural%20networks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Apresents%20first%20time%20a%20novel%20combination%20of%20handwritten%20trajectories%20features%20as%0Achain%20codes%20and%20graph%20neural%20networks%20together.%20The%20handwritten%20trajectories%0Afor%20offline%20handwritten%20text%20has%20been%20evaluated%20using%20recovery%20of%20drawing%0Aorder%2C%20whereas%20online%20handwritten%20trajectories%20are%20directly%20used%20with%20chain%0Acodes.%20Our%20results%20prove%20that%20present%20combination%20surpass%20previous%20results%20and%0Aminimize%20error%20rate%20in%20few%20epochs%20only.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network%2520based%2520Handwritten%2520Trajectories%2520Recognition%26entry.906535625%3DAnuj%2520Sharma%2520and%2520Sukhdeep%2520Singh%2520and%2520S%2520Ratna%26entry.1292438233%3D%2520%2520The%2520graph%2520neural%2520networks%2520has%2520been%2520proved%2520to%2520be%2520an%2520efficient%2520machine%2520learning%250Atechnique%2520in%2520real%2520life%2520applications.%2520The%2520handwritten%2520recognition%2520is%2520one%2520of%2520the%250Auseful%2520area%2520in%2520real%2520life%2520use%2520where%2520both%2520offline%2520and%2520online%2520handwriting%250Arecognition%2520are%2520required.%2520The%2520chain%2520code%2520as%2520feature%2520extraction%2520technique%2520has%250Ashown%2520significant%2520results%2520in%2520literature%2520and%2520we%2520have%2520been%2520able%2520to%2520use%2520chain%250Acodes%2520with%2520graph%2520neural%2520networks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%250Apresents%2520first%2520time%2520a%2520novel%2520combination%2520of%2520handwritten%2520trajectories%2520features%2520as%250Achain%2520codes%2520and%2520graph%2520neural%2520networks%2520together.%2520The%2520handwritten%2520trajectories%250Afor%2520offline%2520handwritten%2520text%2520has%2520been%2520evaluated%2520using%2520recovery%2520of%2520drawing%250Aorder%252C%2520whereas%2520online%2520handwritten%2520trajectories%2520are%2520directly%2520used%2520with%2520chain%250Acodes.%2520Our%2520results%2520prove%2520that%2520present%2520combination%2520surpass%2520previous%2520results%2520and%250Aminimize%2520error%2520rate%2520in%2520few%2520epochs%2520only.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network%20based%20Handwritten%20Trajectories%20Recognition&entry.906535625=Anuj%20Sharma%20and%20Sukhdeep%20Singh%20and%20S%20Ratna&entry.1292438233=%20%20The%20graph%20neural%20networks%20has%20been%20proved%20to%20be%20an%20efficient%20machine%20learning%0Atechnique%20in%20real%20life%20applications.%20The%20handwritten%20recognition%20is%20one%20of%20the%0Auseful%20area%20in%20real%20life%20use%20where%20both%20offline%20and%20online%20handwriting%0Arecognition%20are%20required.%20The%20chain%20code%20as%20feature%20extraction%20technique%20has%0Ashown%20significant%20results%20in%20literature%20and%20we%20have%20been%20able%20to%20use%20chain%0Acodes%20with%20graph%20neural%20networks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Apresents%20first%20time%20a%20novel%20combination%20of%20handwritten%20trajectories%20features%20as%0Achain%20codes%20and%20graph%20neural%20networks%20together.%20The%20handwritten%20trajectories%0Afor%20offline%20handwritten%20text%20has%20been%20evaluated%20using%20recovery%20of%20drawing%0Aorder%2C%20whereas%20online%20handwritten%20trajectories%20are%20directly%20used%20with%20chain%0Acodes.%20Our%20results%20prove%20that%20present%20combination%20surpass%20previous%20results%20and%0Aminimize%20error%20rate%20in%20few%20epochs%20only.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09247v1&entry.124074799=Read"},
{"title": "3D Human Pose Perception from Egocentric Stereo Videos", "author": "Hiroyasu Akada and Jian Wang and Vladislav Golyanik and Christian Theobalt", "abstract": "  While head-mounted devices are becoming more compact, they provide egocentric\nviews with significant self-occlusions of the device user. Hence, existing\nmethods often fail to accurately estimate complex 3D poses from egocentric\nviews. In this work, we propose a new transformer-based framework to improve\negocentric stereo 3D human pose estimation, which leverages the scene\ninformation and temporal context of egocentric stereo videos. Specifically, we\nutilize 1) depth features from our 3D scene reconstruction module with\nuniformly sampled windows of egocentric stereo frames, and 2) human joint\nqueries enhanced by temporal features of the video inputs. Our method is able\nto accurately estimate human poses even in challenging scenarios, such as\ncrouching and sitting. Furthermore, we introduce two new benchmark datasets,\ni.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer a\nmuch larger number of egocentric stereo views with a wider variety of human\nmotions than the existing datasets, allowing comprehensive evaluation of\nexisting and upcoming methods. Our extensive experiments show that the proposed\napproach significantly outperforms previous methods. We will release\nUnrealEgo2, UnrealEgo-RW, and trained models on our project page.\n", "link": "http://arxiv.org/abs/2401.00889v2", "date": "2024-05-15", "relevancy": 2.2331, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5552}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Human%20Pose%20Perception%20from%20Egocentric%20Stereo%20Videos&body=Title%3A%203D%20Human%20Pose%20Perception%20from%20Egocentric%20Stereo%20Videos%0AAuthor%3A%20Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20While%20head-mounted%20devices%20are%20becoming%20more%20compact%2C%20they%20provide%20egocentric%0Aviews%20with%20significant%20self-occlusions%20of%20the%20device%20user.%20Hence%2C%20existing%0Amethods%20often%20fail%20to%20accurately%20estimate%20complex%203D%20poses%20from%20egocentric%0Aviews.%20In%20this%20work%2C%20we%20propose%20a%20new%20transformer-based%20framework%20to%20improve%0Aegocentric%20stereo%203D%20human%20pose%20estimation%2C%20which%20leverages%20the%20scene%0Ainformation%20and%20temporal%20context%20of%20egocentric%20stereo%20videos.%20Specifically%2C%20we%0Autilize%201%29%20depth%20features%20from%20our%203D%20scene%20reconstruction%20module%20with%0Auniformly%20sampled%20windows%20of%20egocentric%20stereo%20frames%2C%20and%202%29%20human%20joint%0Aqueries%20enhanced%20by%20temporal%20features%20of%20the%20video%20inputs.%20Our%20method%20is%20able%0Ato%20accurately%20estimate%20human%20poses%20even%20in%20challenging%20scenarios%2C%20such%20as%0Acrouching%20and%20sitting.%20Furthermore%2C%20we%20introduce%20two%20new%20benchmark%20datasets%2C%0Ai.e.%2C%20UnrealEgo2%20and%20UnrealEgo-RW%20%28RealWorld%29.%20The%20proposed%20datasets%20offer%20a%0Amuch%20larger%20number%20of%20egocentric%20stereo%20views%20with%20a%20wider%20variety%20of%20human%0Amotions%20than%20the%20existing%20datasets%2C%20allowing%20comprehensive%20evaluation%20of%0Aexisting%20and%20upcoming%20methods.%20Our%20extensive%20experiments%20show%20that%20the%20proposed%0Aapproach%20significantly%20outperforms%20previous%20methods.%20We%20will%20release%0AUnrealEgo2%2C%20UnrealEgo-RW%2C%20and%20trained%20models%20on%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Human%2520Pose%2520Perception%2520from%2520Egocentric%2520Stereo%2520Videos%26entry.906535625%3DHiroyasu%2520Akada%2520and%2520Jian%2520Wang%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520While%2520head-mounted%2520devices%2520are%2520becoming%2520more%2520compact%252C%2520they%2520provide%2520egocentric%250Aviews%2520with%2520significant%2520self-occlusions%2520of%2520the%2520device%2520user.%2520Hence%252C%2520existing%250Amethods%2520often%2520fail%2520to%2520accurately%2520estimate%2520complex%25203D%2520poses%2520from%2520egocentric%250Aviews.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520transformer-based%2520framework%2520to%2520improve%250Aegocentric%2520stereo%25203D%2520human%2520pose%2520estimation%252C%2520which%2520leverages%2520the%2520scene%250Ainformation%2520and%2520temporal%2520context%2520of%2520egocentric%2520stereo%2520videos.%2520Specifically%252C%2520we%250Autilize%25201%2529%2520depth%2520features%2520from%2520our%25203D%2520scene%2520reconstruction%2520module%2520with%250Auniformly%2520sampled%2520windows%2520of%2520egocentric%2520stereo%2520frames%252C%2520and%25202%2529%2520human%2520joint%250Aqueries%2520enhanced%2520by%2520temporal%2520features%2520of%2520the%2520video%2520inputs.%2520Our%2520method%2520is%2520able%250Ato%2520accurately%2520estimate%2520human%2520poses%2520even%2520in%2520challenging%2520scenarios%252C%2520such%2520as%250Acrouching%2520and%2520sitting.%2520Furthermore%252C%2520we%2520introduce%2520two%2520new%2520benchmark%2520datasets%252C%250Ai.e.%252C%2520UnrealEgo2%2520and%2520UnrealEgo-RW%2520%2528RealWorld%2529.%2520The%2520proposed%2520datasets%2520offer%2520a%250Amuch%2520larger%2520number%2520of%2520egocentric%2520stereo%2520views%2520with%2520a%2520wider%2520variety%2520of%2520human%250Amotions%2520than%2520the%2520existing%2520datasets%252C%2520allowing%2520comprehensive%2520evaluation%2520of%250Aexisting%2520and%2520upcoming%2520methods.%2520Our%2520extensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aapproach%2520significantly%2520outperforms%2520previous%2520methods.%2520We%2520will%2520release%250AUnrealEgo2%252C%2520UnrealEgo-RW%252C%2520and%2520trained%2520models%2520on%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.00889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Human%20Pose%20Perception%20from%20Egocentric%20Stereo%20Videos&entry.906535625=Hiroyasu%20Akada%20and%20Jian%20Wang%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt&entry.1292438233=%20%20While%20head-mounted%20devices%20are%20becoming%20more%20compact%2C%20they%20provide%20egocentric%0Aviews%20with%20significant%20self-occlusions%20of%20the%20device%20user.%20Hence%2C%20existing%0Amethods%20often%20fail%20to%20accurately%20estimate%20complex%203D%20poses%20from%20egocentric%0Aviews.%20In%20this%20work%2C%20we%20propose%20a%20new%20transformer-based%20framework%20to%20improve%0Aegocentric%20stereo%203D%20human%20pose%20estimation%2C%20which%20leverages%20the%20scene%0Ainformation%20and%20temporal%20context%20of%20egocentric%20stereo%20videos.%20Specifically%2C%20we%0Autilize%201%29%20depth%20features%20from%20our%203D%20scene%20reconstruction%20module%20with%0Auniformly%20sampled%20windows%20of%20egocentric%20stereo%20frames%2C%20and%202%29%20human%20joint%0Aqueries%20enhanced%20by%20temporal%20features%20of%20the%20video%20inputs.%20Our%20method%20is%20able%0Ato%20accurately%20estimate%20human%20poses%20even%20in%20challenging%20scenarios%2C%20such%20as%0Acrouching%20and%20sitting.%20Furthermore%2C%20we%20introduce%20two%20new%20benchmark%20datasets%2C%0Ai.e.%2C%20UnrealEgo2%20and%20UnrealEgo-RW%20%28RealWorld%29.%20The%20proposed%20datasets%20offer%20a%0Amuch%20larger%20number%20of%20egocentric%20stereo%20views%20with%20a%20wider%20variety%20of%20human%0Amotions%20than%20the%20existing%20datasets%2C%20allowing%20comprehensive%20evaluation%20of%0Aexisting%20and%20upcoming%20methods.%20Our%20extensive%20experiments%20show%20that%20the%20proposed%0Aapproach%20significantly%20outperforms%20previous%20methods.%20We%20will%20release%0AUnrealEgo2%2C%20UnrealEgo-RW%2C%20and%20trained%20models%20on%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00889v2&entry.124074799=Read"},
{"title": "SA-FedLora: Adaptive Parameter Allocation for Efficient Federated\n  Learning with LoRA Tuning", "author": "Yuning Yang and Xiaohong Liu and Tianrun Gao and Xiaodong Xu and Guangyu Wang", "abstract": "  Fine-tuning large-scale pre-trained models via transfer learning is an\nemerging important paradigm for a wide range of downstream tasks, with\nperformance heavily reliant on extensive data. Federated learning (FL), as a\ndistributed framework, provides a secure solution to train models on local\ndatasets while safeguarding raw sensitive data. However, FL networks encounter\nhigh communication costs due to the massive parameters of large-scale\npre-trained models, necessitating parameter-efficient methods. Notably,\nparameter efficient fine tuning, such as Low-Rank Adaptation (LoRA), has shown\nremarkable success in fine-tuning pre-trained models. However, prior research\nindicates that the fixed parameter budget may be prone to the overfitting or\nslower convergence. To address this challenge, we propose a Simulated\nAnnealing-based Federated Learning with LoRA tuning (SA-FedLoRA) approach by\nreducing trainable parameters. Specifically, SA-FedLoRA comprises two stages:\ninitiating and annealing. (1) In the initiating stage, we implement a parameter\nregularization approach during the early rounds of aggregation, aiming to\nmitigate client drift and accelerate the convergence for the subsequent tuning.\n(2) In the annealing stage, we allocate higher parameter budget during the\nearly 'heating' phase and then gradually shrink the budget until the 'cooling'\nphase. This strategy not only facilitates convergence to the global optimum but\nalso reduces communication costs. Experimental results demonstrate that\nSA-FedLoRA is an efficient FL, achieving superior performance to FedAvg and\nsignificantly reducing communication parameters by up to 93.62%.\n", "link": "http://arxiv.org/abs/2405.09394v1", "date": "2024-05-15", "relevancy": 2.2207, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4501}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4414}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SA-FedLora%3A%20Adaptive%20Parameter%20Allocation%20for%20Efficient%20Federated%0A%20%20Learning%20with%20LoRA%20Tuning&body=Title%3A%20SA-FedLora%3A%20Adaptive%20Parameter%20Allocation%20for%20Efficient%20Federated%0A%20%20Learning%20with%20LoRA%20Tuning%0AAuthor%3A%20Yuning%20Yang%20and%20Xiaohong%20Liu%20and%20Tianrun%20Gao%20and%20Xiaodong%20Xu%20and%20Guangyu%20Wang%0AAbstract%3A%20%20%20Fine-tuning%20large-scale%20pre-trained%20models%20via%20transfer%20learning%20is%20an%0Aemerging%20important%20paradigm%20for%20a%20wide%20range%20of%20downstream%20tasks%2C%20with%0Aperformance%20heavily%20reliant%20on%20extensive%20data.%20Federated%20learning%20%28FL%29%2C%20as%20a%0Adistributed%20framework%2C%20provides%20a%20secure%20solution%20to%20train%20models%20on%20local%0Adatasets%20while%20safeguarding%20raw%20sensitive%20data.%20However%2C%20FL%20networks%20encounter%0Ahigh%20communication%20costs%20due%20to%20the%20massive%20parameters%20of%20large-scale%0Apre-trained%20models%2C%20necessitating%20parameter-efficient%20methods.%20Notably%2C%0Aparameter%20efficient%20fine%20tuning%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20has%20shown%0Aremarkable%20success%20in%20fine-tuning%20pre-trained%20models.%20However%2C%20prior%20research%0Aindicates%20that%20the%20fixed%20parameter%20budget%20may%20be%20prone%20to%20the%20overfitting%20or%0Aslower%20convergence.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Simulated%0AAnnealing-based%20Federated%20Learning%20with%20LoRA%20tuning%20%28SA-FedLoRA%29%20approach%20by%0Areducing%20trainable%20parameters.%20Specifically%2C%20SA-FedLoRA%20comprises%20two%20stages%3A%0Ainitiating%20and%20annealing.%20%281%29%20In%20the%20initiating%20stage%2C%20we%20implement%20a%20parameter%0Aregularization%20approach%20during%20the%20early%20rounds%20of%20aggregation%2C%20aiming%20to%0Amitigate%20client%20drift%20and%20accelerate%20the%20convergence%20for%20the%20subsequent%20tuning.%0A%282%29%20In%20the%20annealing%20stage%2C%20we%20allocate%20higher%20parameter%20budget%20during%20the%0Aearly%20%27heating%27%20phase%20and%20then%20gradually%20shrink%20the%20budget%20until%20the%20%27cooling%27%0Aphase.%20This%20strategy%20not%20only%20facilitates%20convergence%20to%20the%20global%20optimum%20but%0Aalso%20reduces%20communication%20costs.%20Experimental%20results%20demonstrate%20that%0ASA-FedLoRA%20is%20an%20efficient%20FL%2C%20achieving%20superior%20performance%20to%20FedAvg%20and%0Asignificantly%20reducing%20communication%20parameters%20by%20up%20to%2093.62%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSA-FedLora%253A%2520Adaptive%2520Parameter%2520Allocation%2520for%2520Efficient%2520Federated%250A%2520%2520Learning%2520with%2520LoRA%2520Tuning%26entry.906535625%3DYuning%2520Yang%2520and%2520Xiaohong%2520Liu%2520and%2520Tianrun%2520Gao%2520and%2520Xiaodong%2520Xu%2520and%2520Guangyu%2520Wang%26entry.1292438233%3D%2520%2520Fine-tuning%2520large-scale%2520pre-trained%2520models%2520via%2520transfer%2520learning%2520is%2520an%250Aemerging%2520important%2520paradigm%2520for%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%252C%2520with%250Aperformance%2520heavily%2520reliant%2520on%2520extensive%2520data.%2520Federated%2520learning%2520%2528FL%2529%252C%2520as%2520a%250Adistributed%2520framework%252C%2520provides%2520a%2520secure%2520solution%2520to%2520train%2520models%2520on%2520local%250Adatasets%2520while%2520safeguarding%2520raw%2520sensitive%2520data.%2520However%252C%2520FL%2520networks%2520encounter%250Ahigh%2520communication%2520costs%2520due%2520to%2520the%2520massive%2520parameters%2520of%2520large-scale%250Apre-trained%2520models%252C%2520necessitating%2520parameter-efficient%2520methods.%2520Notably%252C%250Aparameter%2520efficient%2520fine%2520tuning%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520has%2520shown%250Aremarkable%2520success%2520in%2520fine-tuning%2520pre-trained%2520models.%2520However%252C%2520prior%2520research%250Aindicates%2520that%2520the%2520fixed%2520parameter%2520budget%2520may%2520be%2520prone%2520to%2520the%2520overfitting%2520or%250Aslower%2520convergence.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520Simulated%250AAnnealing-based%2520Federated%2520Learning%2520with%2520LoRA%2520tuning%2520%2528SA-FedLoRA%2529%2520approach%2520by%250Areducing%2520trainable%2520parameters.%2520Specifically%252C%2520SA-FedLoRA%2520comprises%2520two%2520stages%253A%250Ainitiating%2520and%2520annealing.%2520%25281%2529%2520In%2520the%2520initiating%2520stage%252C%2520we%2520implement%2520a%2520parameter%250Aregularization%2520approach%2520during%2520the%2520early%2520rounds%2520of%2520aggregation%252C%2520aiming%2520to%250Amitigate%2520client%2520drift%2520and%2520accelerate%2520the%2520convergence%2520for%2520the%2520subsequent%2520tuning.%250A%25282%2529%2520In%2520the%2520annealing%2520stage%252C%2520we%2520allocate%2520higher%2520parameter%2520budget%2520during%2520the%250Aearly%2520%2527heating%2527%2520phase%2520and%2520then%2520gradually%2520shrink%2520the%2520budget%2520until%2520the%2520%2527cooling%2527%250Aphase.%2520This%2520strategy%2520not%2520only%2520facilitates%2520convergence%2520to%2520the%2520global%2520optimum%2520but%250Aalso%2520reduces%2520communication%2520costs.%2520Experimental%2520results%2520demonstrate%2520that%250ASA-FedLoRA%2520is%2520an%2520efficient%2520FL%252C%2520achieving%2520superior%2520performance%2520to%2520FedAvg%2520and%250Asignificantly%2520reducing%2520communication%2520parameters%2520by%2520up%2520to%252093.62%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SA-FedLora%3A%20Adaptive%20Parameter%20Allocation%20for%20Efficient%20Federated%0A%20%20Learning%20with%20LoRA%20Tuning&entry.906535625=Yuning%20Yang%20and%20Xiaohong%20Liu%20and%20Tianrun%20Gao%20and%20Xiaodong%20Xu%20and%20Guangyu%20Wang&entry.1292438233=%20%20Fine-tuning%20large-scale%20pre-trained%20models%20via%20transfer%20learning%20is%20an%0Aemerging%20important%20paradigm%20for%20a%20wide%20range%20of%20downstream%20tasks%2C%20with%0Aperformance%20heavily%20reliant%20on%20extensive%20data.%20Federated%20learning%20%28FL%29%2C%20as%20a%0Adistributed%20framework%2C%20provides%20a%20secure%20solution%20to%20train%20models%20on%20local%0Adatasets%20while%20safeguarding%20raw%20sensitive%20data.%20However%2C%20FL%20networks%20encounter%0Ahigh%20communication%20costs%20due%20to%20the%20massive%20parameters%20of%20large-scale%0Apre-trained%20models%2C%20necessitating%20parameter-efficient%20methods.%20Notably%2C%0Aparameter%20efficient%20fine%20tuning%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20has%20shown%0Aremarkable%20success%20in%20fine-tuning%20pre-trained%20models.%20However%2C%20prior%20research%0Aindicates%20that%20the%20fixed%20parameter%20budget%20may%20be%20prone%20to%20the%20overfitting%20or%0Aslower%20convergence.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Simulated%0AAnnealing-based%20Federated%20Learning%20with%20LoRA%20tuning%20%28SA-FedLoRA%29%20approach%20by%0Areducing%20trainable%20parameters.%20Specifically%2C%20SA-FedLoRA%20comprises%20two%20stages%3A%0Ainitiating%20and%20annealing.%20%281%29%20In%20the%20initiating%20stage%2C%20we%20implement%20a%20parameter%0Aregularization%20approach%20during%20the%20early%20rounds%20of%20aggregation%2C%20aiming%20to%0Amitigate%20client%20drift%20and%20accelerate%20the%20convergence%20for%20the%20subsequent%20tuning.%0A%282%29%20In%20the%20annealing%20stage%2C%20we%20allocate%20higher%20parameter%20budget%20during%20the%0Aearly%20%27heating%27%20phase%20and%20then%20gradually%20shrink%20the%20budget%20until%20the%20%27cooling%27%0Aphase.%20This%20strategy%20not%20only%20facilitates%20convergence%20to%20the%20global%20optimum%20but%0Aalso%20reduces%20communication%20costs.%20Experimental%20results%20demonstrate%20that%0ASA-FedLoRA%20is%20an%20efficient%20FL%2C%20achieving%20superior%20performance%20to%20FedAvg%20and%0Asignificantly%20reducing%20communication%20parameters%20by%20up%20to%2093.62%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09394v1&entry.124074799=Read"},
{"title": "Wisdom of Committee: Distilling from Foundation Model to Specialized\n  Application Model", "author": "Zichang Liu and Qingyun Liu and Yuening Li and Liang Liu and Anshumali Shrivastava and Shuchao Bi and Lichan Hong and Ed H. Chi and Zhe Zhao", "abstract": "  Recent advancements in foundation models have yielded impressive performance\nacross a wide range of tasks. Meanwhile, for specific applications,\npractitioners have been developing specialized application models. To enjoy the\nbenefits of both kinds of models, one natural path is to transfer the knowledge\nin foundation models into specialized application models, which are generally\nmore efficient for serving. Techniques from knowledge distillation may be\napplied here, where the application model learns to mimic the foundation model.\nHowever, specialized application models and foundation models have substantial\ngaps in capacity, employing distinct architectures, using different input\nfeatures from different modalities, and being optimized on different\ndistributions. These differences in model characteristics lead to significant\nchallenges for distillation methods. In this work, we propose creating a\nteaching committee comprising both foundation model teachers and complementary\nteachers. Complementary teachers possess model characteristics akin to the\nstudent's, aiming to bridge the gap between the foundation model and\nspecialized application models for a smoother knowledge transfer. Further, to\naccommodate the dissimilarity among the teachers in the committee, we introduce\nDiverseDistill, which allows the student to understand the expertise of each\nteacher and extract task knowledge. Our evaluations demonstrate that adding\ncomplementary teachers enhances student performance. Finally, DiverseDistill\nconsistently outperforms baseline distillation methods, regardless of the\nteacher choices, resulting in significantly improved student performance.\n", "link": "http://arxiv.org/abs/2402.14035v3", "date": "2024-05-15", "relevancy": 2.2206, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4421}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wisdom%20of%20Committee%3A%20Distilling%20from%20Foundation%20Model%20to%20Specialized%0A%20%20Application%20Model&body=Title%3A%20Wisdom%20of%20Committee%3A%20Distilling%20from%20Foundation%20Model%20to%20Specialized%0A%20%20Application%20Model%0AAuthor%3A%20Zichang%20Liu%20and%20Qingyun%20Liu%20and%20Yuening%20Li%20and%20Liang%20Liu%20and%20Anshumali%20Shrivastava%20and%20Shuchao%20Bi%20and%20Lichan%20Hong%20and%20Ed%20H.%20Chi%20and%20Zhe%20Zhao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20foundation%20models%20have%20yielded%20impressive%20performance%0Aacross%20a%20wide%20range%20of%20tasks.%20Meanwhile%2C%20for%20specific%20applications%2C%0Apractitioners%20have%20been%20developing%20specialized%20application%20models.%20To%20enjoy%20the%0Abenefits%20of%20both%20kinds%20of%20models%2C%20one%20natural%20path%20is%20to%20transfer%20the%20knowledge%0Ain%20foundation%20models%20into%20specialized%20application%20models%2C%20which%20are%20generally%0Amore%20efficient%20for%20serving.%20Techniques%20from%20knowledge%20distillation%20may%20be%0Aapplied%20here%2C%20where%20the%20application%20model%20learns%20to%20mimic%20the%20foundation%20model.%0AHowever%2C%20specialized%20application%20models%20and%20foundation%20models%20have%20substantial%0Agaps%20in%20capacity%2C%20employing%20distinct%20architectures%2C%20using%20different%20input%0Afeatures%20from%20different%20modalities%2C%20and%20being%20optimized%20on%20different%0Adistributions.%20These%20differences%20in%20model%20characteristics%20lead%20to%20significant%0Achallenges%20for%20distillation%20methods.%20In%20this%20work%2C%20we%20propose%20creating%20a%0Ateaching%20committee%20comprising%20both%20foundation%20model%20teachers%20and%20complementary%0Ateachers.%20Complementary%20teachers%20possess%20model%20characteristics%20akin%20to%20the%0Astudent%27s%2C%20aiming%20to%20bridge%20the%20gap%20between%20the%20foundation%20model%20and%0Aspecialized%20application%20models%20for%20a%20smoother%20knowledge%20transfer.%20Further%2C%20to%0Aaccommodate%20the%20dissimilarity%20among%20the%20teachers%20in%20the%20committee%2C%20we%20introduce%0ADiverseDistill%2C%20which%20allows%20the%20student%20to%20understand%20the%20expertise%20of%20each%0Ateacher%20and%20extract%20task%20knowledge.%20Our%20evaluations%20demonstrate%20that%20adding%0Acomplementary%20teachers%20enhances%20student%20performance.%20Finally%2C%20DiverseDistill%0Aconsistently%20outperforms%20baseline%20distillation%20methods%2C%20regardless%20of%20the%0Ateacher%20choices%2C%20resulting%20in%20significantly%20improved%20student%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWisdom%2520of%2520Committee%253A%2520Distilling%2520from%2520Foundation%2520Model%2520to%2520Specialized%250A%2520%2520Application%2520Model%26entry.906535625%3DZichang%2520Liu%2520and%2520Qingyun%2520Liu%2520and%2520Yuening%2520Li%2520and%2520Liang%2520Liu%2520and%2520Anshumali%2520Shrivastava%2520and%2520Shuchao%2520Bi%2520and%2520Lichan%2520Hong%2520and%2520Ed%2520H.%2520Chi%2520and%2520Zhe%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520foundation%2520models%2520have%2520yielded%2520impressive%2520performance%250Aacross%2520a%2520wide%2520range%2520of%2520tasks.%2520Meanwhile%252C%2520for%2520specific%2520applications%252C%250Apractitioners%2520have%2520been%2520developing%2520specialized%2520application%2520models.%2520To%2520enjoy%2520the%250Abenefits%2520of%2520both%2520kinds%2520of%2520models%252C%2520one%2520natural%2520path%2520is%2520to%2520transfer%2520the%2520knowledge%250Ain%2520foundation%2520models%2520into%2520specialized%2520application%2520models%252C%2520which%2520are%2520generally%250Amore%2520efficient%2520for%2520serving.%2520Techniques%2520from%2520knowledge%2520distillation%2520may%2520be%250Aapplied%2520here%252C%2520where%2520the%2520application%2520model%2520learns%2520to%2520mimic%2520the%2520foundation%2520model.%250AHowever%252C%2520specialized%2520application%2520models%2520and%2520foundation%2520models%2520have%2520substantial%250Agaps%2520in%2520capacity%252C%2520employing%2520distinct%2520architectures%252C%2520using%2520different%2520input%250Afeatures%2520from%2520different%2520modalities%252C%2520and%2520being%2520optimized%2520on%2520different%250Adistributions.%2520These%2520differences%2520in%2520model%2520characteristics%2520lead%2520to%2520significant%250Achallenges%2520for%2520distillation%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%2520creating%2520a%250Ateaching%2520committee%2520comprising%2520both%2520foundation%2520model%2520teachers%2520and%2520complementary%250Ateachers.%2520Complementary%2520teachers%2520possess%2520model%2520characteristics%2520akin%2520to%2520the%250Astudent%2527s%252C%2520aiming%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520foundation%2520model%2520and%250Aspecialized%2520application%2520models%2520for%2520a%2520smoother%2520knowledge%2520transfer.%2520Further%252C%2520to%250Aaccommodate%2520the%2520dissimilarity%2520among%2520the%2520teachers%2520in%2520the%2520committee%252C%2520we%2520introduce%250ADiverseDistill%252C%2520which%2520allows%2520the%2520student%2520to%2520understand%2520the%2520expertise%2520of%2520each%250Ateacher%2520and%2520extract%2520task%2520knowledge.%2520Our%2520evaluations%2520demonstrate%2520that%2520adding%250Acomplementary%2520teachers%2520enhances%2520student%2520performance.%2520Finally%252C%2520DiverseDistill%250Aconsistently%2520outperforms%2520baseline%2520distillation%2520methods%252C%2520regardless%2520of%2520the%250Ateacher%2520choices%252C%2520resulting%2520in%2520significantly%2520improved%2520student%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wisdom%20of%20Committee%3A%20Distilling%20from%20Foundation%20Model%20to%20Specialized%0A%20%20Application%20Model&entry.906535625=Zichang%20Liu%20and%20Qingyun%20Liu%20and%20Yuening%20Li%20and%20Liang%20Liu%20and%20Anshumali%20Shrivastava%20and%20Shuchao%20Bi%20and%20Lichan%20Hong%20and%20Ed%20H.%20Chi%20and%20Zhe%20Zhao&entry.1292438233=%20%20Recent%20advancements%20in%20foundation%20models%20have%20yielded%20impressive%20performance%0Aacross%20a%20wide%20range%20of%20tasks.%20Meanwhile%2C%20for%20specific%20applications%2C%0Apractitioners%20have%20been%20developing%20specialized%20application%20models.%20To%20enjoy%20the%0Abenefits%20of%20both%20kinds%20of%20models%2C%20one%20natural%20path%20is%20to%20transfer%20the%20knowledge%0Ain%20foundation%20models%20into%20specialized%20application%20models%2C%20which%20are%20generally%0Amore%20efficient%20for%20serving.%20Techniques%20from%20knowledge%20distillation%20may%20be%0Aapplied%20here%2C%20where%20the%20application%20model%20learns%20to%20mimic%20the%20foundation%20model.%0AHowever%2C%20specialized%20application%20models%20and%20foundation%20models%20have%20substantial%0Agaps%20in%20capacity%2C%20employing%20distinct%20architectures%2C%20using%20different%20input%0Afeatures%20from%20different%20modalities%2C%20and%20being%20optimized%20on%20different%0Adistributions.%20These%20differences%20in%20model%20characteristics%20lead%20to%20significant%0Achallenges%20for%20distillation%20methods.%20In%20this%20work%2C%20we%20propose%20creating%20a%0Ateaching%20committee%20comprising%20both%20foundation%20model%20teachers%20and%20complementary%0Ateachers.%20Complementary%20teachers%20possess%20model%20characteristics%20akin%20to%20the%0Astudent%27s%2C%20aiming%20to%20bridge%20the%20gap%20between%20the%20foundation%20model%20and%0Aspecialized%20application%20models%20for%20a%20smoother%20knowledge%20transfer.%20Further%2C%20to%0Aaccommodate%20the%20dissimilarity%20among%20the%20teachers%20in%20the%20committee%2C%20we%20introduce%0ADiverseDistill%2C%20which%20allows%20the%20student%20to%20understand%20the%20expertise%20of%20each%0Ateacher%20and%20extract%20task%20knowledge.%20Our%20evaluations%20demonstrate%20that%20adding%0Acomplementary%20teachers%20enhances%20student%20performance.%20Finally%2C%20DiverseDistill%0Aconsistently%20outperforms%20baseline%20distillation%20methods%2C%20regardless%20of%20the%0Ateacher%20choices%2C%20resulting%20in%20significantly%20improved%20student%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14035v3&entry.124074799=Read"},
{"title": "Sensitivity Decouple Learning for Image Compression Artifacts Reduction", "author": "Li Ma and Yifan Zhao and Peixi Peng and Yonghong Tian", "abstract": "  With the benefit of deep learning techniques, recent researches have made\nsignificant progress in image compression artifacts reduction. Despite their\nimproved performances, prevailing methods only focus on learning a mapping from\nthe compressed image to the original one but ignore the intrinsic attributes of\nthe given compressed images, which greatly harms the performance of downstream\nparsing tasks. Different from these methods, we propose to decouple the\nintrinsic attributes into two complementary features for artifacts\nreduction,ie, the compression-insensitive features to regularize the high-level\nsemantic representations during training and the compression-sensitive features\nto be aware of the compression degree. To achieve this, we first employ\nadversarial training to regularize the compressed and original encoded features\nfor retaining high-level semantics, and we then develop the compression\nquality-aware feature encoder for compression-sensitive features. Based on\nthese dual complementary features, we propose a Dual Awareness Guidance Network\n(DAGN) to utilize these awareness features as transformation guidance during\nthe decoding phase. In our proposed DAGN, we develop a cross-feature fusion\nmodule to maintain the consistency of compression-insensitive features by\nfusing compression-insensitive features into the artifacts reduction baseline.\nOur method achieves an average 2.06 dB PSNR gains on BSD500, outperforming\nstate-of-the-art methods, and only requires 29.7 ms to process one image on\nBSD500. Besides, the experimental results on LIVE1 and LIU4K also demonstrate\nthe efficiency, effectiveness, and superiority of the proposed method in terms\nof quantitative metrics, visual quality, and downstream machine vision tasks.\n", "link": "http://arxiv.org/abs/2405.09291v1", "date": "2024-05-15", "relevancy": 2.2169, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5754}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.553}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sensitivity%20Decouple%20Learning%20for%20Image%20Compression%20Artifacts%20Reduction&body=Title%3A%20Sensitivity%20Decouple%20Learning%20for%20Image%20Compression%20Artifacts%20Reduction%0AAuthor%3A%20Li%20Ma%20and%20Yifan%20Zhao%20and%20Peixi%20Peng%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20With%20the%20benefit%20of%20deep%20learning%20techniques%2C%20recent%20researches%20have%20made%0Asignificant%20progress%20in%20image%20compression%20artifacts%20reduction.%20Despite%20their%0Aimproved%20performances%2C%20prevailing%20methods%20only%20focus%20on%20learning%20a%20mapping%20from%0Athe%20compressed%20image%20to%20the%20original%20one%20but%20ignore%20the%20intrinsic%20attributes%20of%0Athe%20given%20compressed%20images%2C%20which%20greatly%20harms%20the%20performance%20of%20downstream%0Aparsing%20tasks.%20Different%20from%20these%20methods%2C%20we%20propose%20to%20decouple%20the%0Aintrinsic%20attributes%20into%20two%20complementary%20features%20for%20artifacts%0Areduction%2Cie%2C%20the%20compression-insensitive%20features%20to%20regularize%20the%20high-level%0Asemantic%20representations%20during%20training%20and%20the%20compression-sensitive%20features%0Ato%20be%20aware%20of%20the%20compression%20degree.%20To%20achieve%20this%2C%20we%20first%20employ%0Aadversarial%20training%20to%20regularize%20the%20compressed%20and%20original%20encoded%20features%0Afor%20retaining%20high-level%20semantics%2C%20and%20we%20then%20develop%20the%20compression%0Aquality-aware%20feature%20encoder%20for%20compression-sensitive%20features.%20Based%20on%0Athese%20dual%20complementary%20features%2C%20we%20propose%20a%20Dual%20Awareness%20Guidance%20Network%0A%28DAGN%29%20to%20utilize%20these%20awareness%20features%20as%20transformation%20guidance%20during%0Athe%20decoding%20phase.%20In%20our%20proposed%20DAGN%2C%20we%20develop%20a%20cross-feature%20fusion%0Amodule%20to%20maintain%20the%20consistency%20of%20compression-insensitive%20features%20by%0Afusing%20compression-insensitive%20features%20into%20the%20artifacts%20reduction%20baseline.%0AOur%20method%20achieves%20an%20average%202.06%20dB%20PSNR%20gains%20on%20BSD500%2C%20outperforming%0Astate-of-the-art%20methods%2C%20and%20only%20requires%2029.7%20ms%20to%20process%20one%20image%20on%0ABSD500.%20Besides%2C%20the%20experimental%20results%20on%20LIVE1%20and%20LIU4K%20also%20demonstrate%0Athe%20efficiency%2C%20effectiveness%2C%20and%20superiority%20of%20the%20proposed%20method%20in%20terms%0Aof%20quantitative%20metrics%2C%20visual%20quality%2C%20and%20downstream%20machine%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSensitivity%2520Decouple%2520Learning%2520for%2520Image%2520Compression%2520Artifacts%2520Reduction%26entry.906535625%3DLi%2520Ma%2520and%2520Yifan%2520Zhao%2520and%2520Peixi%2520Peng%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520With%2520the%2520benefit%2520of%2520deep%2520learning%2520techniques%252C%2520recent%2520researches%2520have%2520made%250Asignificant%2520progress%2520in%2520image%2520compression%2520artifacts%2520reduction.%2520Despite%2520their%250Aimproved%2520performances%252C%2520prevailing%2520methods%2520only%2520focus%2520on%2520learning%2520a%2520mapping%2520from%250Athe%2520compressed%2520image%2520to%2520the%2520original%2520one%2520but%2520ignore%2520the%2520intrinsic%2520attributes%2520of%250Athe%2520given%2520compressed%2520images%252C%2520which%2520greatly%2520harms%2520the%2520performance%2520of%2520downstream%250Aparsing%2520tasks.%2520Different%2520from%2520these%2520methods%252C%2520we%2520propose%2520to%2520decouple%2520the%250Aintrinsic%2520attributes%2520into%2520two%2520complementary%2520features%2520for%2520artifacts%250Areduction%252Cie%252C%2520the%2520compression-insensitive%2520features%2520to%2520regularize%2520the%2520high-level%250Asemantic%2520representations%2520during%2520training%2520and%2520the%2520compression-sensitive%2520features%250Ato%2520be%2520aware%2520of%2520the%2520compression%2520degree.%2520To%2520achieve%2520this%252C%2520we%2520first%2520employ%250Aadversarial%2520training%2520to%2520regularize%2520the%2520compressed%2520and%2520original%2520encoded%2520features%250Afor%2520retaining%2520high-level%2520semantics%252C%2520and%2520we%2520then%2520develop%2520the%2520compression%250Aquality-aware%2520feature%2520encoder%2520for%2520compression-sensitive%2520features.%2520Based%2520on%250Athese%2520dual%2520complementary%2520features%252C%2520we%2520propose%2520a%2520Dual%2520Awareness%2520Guidance%2520Network%250A%2528DAGN%2529%2520to%2520utilize%2520these%2520awareness%2520features%2520as%2520transformation%2520guidance%2520during%250Athe%2520decoding%2520phase.%2520In%2520our%2520proposed%2520DAGN%252C%2520we%2520develop%2520a%2520cross-feature%2520fusion%250Amodule%2520to%2520maintain%2520the%2520consistency%2520of%2520compression-insensitive%2520features%2520by%250Afusing%2520compression-insensitive%2520features%2520into%2520the%2520artifacts%2520reduction%2520baseline.%250AOur%2520method%2520achieves%2520an%2520average%25202.06%2520dB%2520PSNR%2520gains%2520on%2520BSD500%252C%2520outperforming%250Astate-of-the-art%2520methods%252C%2520and%2520only%2520requires%252029.7%2520ms%2520to%2520process%2520one%2520image%2520on%250ABSD500.%2520Besides%252C%2520the%2520experimental%2520results%2520on%2520LIVE1%2520and%2520LIU4K%2520also%2520demonstrate%250Athe%2520efficiency%252C%2520effectiveness%252C%2520and%2520superiority%2520of%2520the%2520proposed%2520method%2520in%2520terms%250Aof%2520quantitative%2520metrics%252C%2520visual%2520quality%252C%2520and%2520downstream%2520machine%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sensitivity%20Decouple%20Learning%20for%20Image%20Compression%20Artifacts%20Reduction&entry.906535625=Li%20Ma%20and%20Yifan%20Zhao%20and%20Peixi%20Peng%20and%20Yonghong%20Tian&entry.1292438233=%20%20With%20the%20benefit%20of%20deep%20learning%20techniques%2C%20recent%20researches%20have%20made%0Asignificant%20progress%20in%20image%20compression%20artifacts%20reduction.%20Despite%20their%0Aimproved%20performances%2C%20prevailing%20methods%20only%20focus%20on%20learning%20a%20mapping%20from%0Athe%20compressed%20image%20to%20the%20original%20one%20but%20ignore%20the%20intrinsic%20attributes%20of%0Athe%20given%20compressed%20images%2C%20which%20greatly%20harms%20the%20performance%20of%20downstream%0Aparsing%20tasks.%20Different%20from%20these%20methods%2C%20we%20propose%20to%20decouple%20the%0Aintrinsic%20attributes%20into%20two%20complementary%20features%20for%20artifacts%0Areduction%2Cie%2C%20the%20compression-insensitive%20features%20to%20regularize%20the%20high-level%0Asemantic%20representations%20during%20training%20and%20the%20compression-sensitive%20features%0Ato%20be%20aware%20of%20the%20compression%20degree.%20To%20achieve%20this%2C%20we%20first%20employ%0Aadversarial%20training%20to%20regularize%20the%20compressed%20and%20original%20encoded%20features%0Afor%20retaining%20high-level%20semantics%2C%20and%20we%20then%20develop%20the%20compression%0Aquality-aware%20feature%20encoder%20for%20compression-sensitive%20features.%20Based%20on%0Athese%20dual%20complementary%20features%2C%20we%20propose%20a%20Dual%20Awareness%20Guidance%20Network%0A%28DAGN%29%20to%20utilize%20these%20awareness%20features%20as%20transformation%20guidance%20during%0Athe%20decoding%20phase.%20In%20our%20proposed%20DAGN%2C%20we%20develop%20a%20cross-feature%20fusion%0Amodule%20to%20maintain%20the%20consistency%20of%20compression-insensitive%20features%20by%0Afusing%20compression-insensitive%20features%20into%20the%20artifacts%20reduction%20baseline.%0AOur%20method%20achieves%20an%20average%202.06%20dB%20PSNR%20gains%20on%20BSD500%2C%20outperforming%0Astate-of-the-art%20methods%2C%20and%20only%20requires%2029.7%20ms%20to%20process%20one%20image%20on%0ABSD500.%20Besides%2C%20the%20experimental%20results%20on%20LIVE1%20and%20LIU4K%20also%20demonstrate%0Athe%20efficiency%2C%20effectiveness%2C%20and%20superiority%20of%20the%20proposed%20method%20in%20terms%0Aof%20quantitative%20metrics%2C%20visual%20quality%2C%20and%20downstream%20machine%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09291v1&entry.124074799=Read"},
{"title": "ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment\n  Simulations", "author": "Artur Grigorev and Giorgio Becherini and Michael J. Black and Otmar Hilliges and Bernhard Thomaszewski", "abstract": "  Learning-based approaches to cloth simulation have started to show their\npotential in recent years. However, handling collisions and intersections in\nneural simulations remains a largely unsolved problem. In this work, we present\n\\moniker{}, a learning-based solution for handling intersections in neural\ncloth simulations. Unlike conventional approaches that critically rely on\nintersection-free inputs, \\moniker{} robustly recovers from intersections\nintroduced through missed collisions, self-penetrating bodies, or errors in\nmanually designed multi-layer outfits. The technical core of \\moniker{} is a\nnovel intersection contour loss that penalizes interpenetrations and encourages\nrapid resolution thereof. We integrate our intersection loss with a\ncollision-avoiding repulsion objective into a neural cloth simulation method\nbased on graph neural networks (GNNs). We demonstrate our method's ability\nacross a challenging set of diverse multi-layer outfits under dynamic human\nmotions. Our extensive analysis indicates that \\moniker{} significantly\nimproves collision handling for learned simulation and produces visually\ncompelling results.\n", "link": "http://arxiv.org/abs/2405.09522v1", "date": "2024-05-15", "relevancy": 2.2037, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5813}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5468}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations&body=Title%3A%20ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations%0AAuthor%3A%20Artur%20Grigorev%20and%20Giorgio%20Becherini%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges%20and%20Bernhard%20Thomaszewski%0AAbstract%3A%20%20%20Learning-based%20approaches%20to%20cloth%20simulation%20have%20started%20to%20show%20their%0Apotential%20in%20recent%20years.%20However%2C%20handling%20collisions%20and%20intersections%20in%0Aneural%20simulations%20remains%20a%20largely%20unsolved%20problem.%20In%20this%20work%2C%20we%20present%0A%5Cmoniker%7B%7D%2C%20a%20learning-based%20solution%20for%20handling%20intersections%20in%20neural%0Acloth%20simulations.%20Unlike%20conventional%20approaches%20that%20critically%20rely%20on%0Aintersection-free%20inputs%2C%20%5Cmoniker%7B%7D%20robustly%20recovers%20from%20intersections%0Aintroduced%20through%20missed%20collisions%2C%20self-penetrating%20bodies%2C%20or%20errors%20in%0Amanually%20designed%20multi-layer%20outfits.%20The%20technical%20core%20of%20%5Cmoniker%7B%7D%20is%20a%0Anovel%20intersection%20contour%20loss%20that%20penalizes%20interpenetrations%20and%20encourages%0Arapid%20resolution%20thereof.%20We%20integrate%20our%20intersection%20loss%20with%20a%0Acollision-avoiding%20repulsion%20objective%20into%20a%20neural%20cloth%20simulation%20method%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29.%20We%20demonstrate%20our%20method%27s%20ability%0Aacross%20a%20challenging%20set%20of%20diverse%20multi-layer%20outfits%20under%20dynamic%20human%0Amotions.%20Our%20extensive%20analysis%20indicates%20that%20%5Cmoniker%7B%7D%20significantly%0Aimproves%20collision%20handling%20for%20learned%20simulation%20and%20produces%20visually%0Acompelling%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContourCraft%253A%2520Learning%2520to%2520Resolve%2520Intersections%2520in%2520Neural%2520Multi-Garment%250A%2520%2520Simulations%26entry.906535625%3DArtur%2520Grigorev%2520and%2520Giorgio%2520Becherini%2520and%2520Michael%2520J.%2520Black%2520and%2520Otmar%2520Hilliges%2520and%2520Bernhard%2520Thomaszewski%26entry.1292438233%3D%2520%2520Learning-based%2520approaches%2520to%2520cloth%2520simulation%2520have%2520started%2520to%2520show%2520their%250Apotential%2520in%2520recent%2520years.%2520However%252C%2520handling%2520collisions%2520and%2520intersections%2520in%250Aneural%2520simulations%2520remains%2520a%2520largely%2520unsolved%2520problem.%2520In%2520this%2520work%252C%2520we%2520present%250A%255Cmoniker%257B%257D%252C%2520a%2520learning-based%2520solution%2520for%2520handling%2520intersections%2520in%2520neural%250Acloth%2520simulations.%2520Unlike%2520conventional%2520approaches%2520that%2520critically%2520rely%2520on%250Aintersection-free%2520inputs%252C%2520%255Cmoniker%257B%257D%2520robustly%2520recovers%2520from%2520intersections%250Aintroduced%2520through%2520missed%2520collisions%252C%2520self-penetrating%2520bodies%252C%2520or%2520errors%2520in%250Amanually%2520designed%2520multi-layer%2520outfits.%2520The%2520technical%2520core%2520of%2520%255Cmoniker%257B%257D%2520is%2520a%250Anovel%2520intersection%2520contour%2520loss%2520that%2520penalizes%2520interpenetrations%2520and%2520encourages%250Arapid%2520resolution%2520thereof.%2520We%2520integrate%2520our%2520intersection%2520loss%2520with%2520a%250Acollision-avoiding%2520repulsion%2520objective%2520into%2520a%2520neural%2520cloth%2520simulation%2520method%250Abased%2520on%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520We%2520demonstrate%2520our%2520method%2527s%2520ability%250Aacross%2520a%2520challenging%2520set%2520of%2520diverse%2520multi-layer%2520outfits%2520under%2520dynamic%2520human%250Amotions.%2520Our%2520extensive%2520analysis%2520indicates%2520that%2520%255Cmoniker%257B%257D%2520significantly%250Aimproves%2520collision%2520handling%2520for%2520learned%2520simulation%2520and%2520produces%2520visually%250Acompelling%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContourCraft%3A%20Learning%20to%20Resolve%20Intersections%20in%20Neural%20Multi-Garment%0A%20%20Simulations&entry.906535625=Artur%20Grigorev%20and%20Giorgio%20Becherini%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges%20and%20Bernhard%20Thomaszewski&entry.1292438233=%20%20Learning-based%20approaches%20to%20cloth%20simulation%20have%20started%20to%20show%20their%0Apotential%20in%20recent%20years.%20However%2C%20handling%20collisions%20and%20intersections%20in%0Aneural%20simulations%20remains%20a%20largely%20unsolved%20problem.%20In%20this%20work%2C%20we%20present%0A%5Cmoniker%7B%7D%2C%20a%20learning-based%20solution%20for%20handling%20intersections%20in%20neural%0Acloth%20simulations.%20Unlike%20conventional%20approaches%20that%20critically%20rely%20on%0Aintersection-free%20inputs%2C%20%5Cmoniker%7B%7D%20robustly%20recovers%20from%20intersections%0Aintroduced%20through%20missed%20collisions%2C%20self-penetrating%20bodies%2C%20or%20errors%20in%0Amanually%20designed%20multi-layer%20outfits.%20The%20technical%20core%20of%20%5Cmoniker%7B%7D%20is%20a%0Anovel%20intersection%20contour%20loss%20that%20penalizes%20interpenetrations%20and%20encourages%0Arapid%20resolution%20thereof.%20We%20integrate%20our%20intersection%20loss%20with%20a%0Acollision-avoiding%20repulsion%20objective%20into%20a%20neural%20cloth%20simulation%20method%0Abased%20on%20graph%20neural%20networks%20%28GNNs%29.%20We%20demonstrate%20our%20method%27s%20ability%0Aacross%20a%20challenging%20set%20of%20diverse%20multi-layer%20outfits%20under%20dynamic%20human%0Amotions.%20Our%20extensive%20analysis%20indicates%20that%20%5Cmoniker%7B%7D%20significantly%0Aimproves%20collision%20handling%20for%20learned%20simulation%20and%20produces%20visually%0Acompelling%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09522v1&entry.124074799=Read"},
{"title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech\n  Generators", "author": "Wiebke Hutiri and Oresiti Papakyriakopoulos and Alice Xiang", "abstract": "  The rapid and wide-scale adoption of AI to generate human speech poses a\nrange of significant ethical and safety risks to society that need to be\naddressed. For example, a growing number of speech generation incidents are\nassociated with swatting attacks in the United States, where anonymous\nperpetrators create synthetic voices that call police officers to close down\nschools and hospitals, or to violently gain access to innocent citizens' homes.\nIncidents like this demonstrate that multimodal generative AI risks and harms\ndo not exist in isolation, but arise from the interactions of multiple\nstakeholders and technical AI systems. In this paper we analyse speech\ngeneration incidents to study how patterns of specific harms arise. We find\nthat specific harms can be categorised according to the exposure of affected\nindividuals, that is to say whether they are a subject of, interact with,\nsuffer due to, or are excluded from speech generation systems. Similarly,\nspecific harms are also a consequence of the motives of the creators and\ndeployers of the systems. Based on these insights we propose a conceptual\nframework for modelling pathways to ethical and safety harms of AI, which we\nuse to develop a taxonomy of harms of speech generators. Our relational\napproach captures the complexity of risks and harms in sociotechnical AI\nsystems, and yields a taxonomy that can support appropriate policy\ninterventions and decision making for the responsible development and release\nof speech generation models.\n", "link": "http://arxiv.org/abs/2402.01708v2", "date": "2024-05-15", "relevancy": 2.1975, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4636}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4412}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20My%20Voice%21%20A%20Taxonomy%20of%20Ethical%20and%20Safety%20Harms%20of%20Speech%0A%20%20Generators&body=Title%3A%20Not%20My%20Voice%21%20A%20Taxonomy%20of%20Ethical%20and%20Safety%20Harms%20of%20Speech%0A%20%20Generators%0AAuthor%3A%20Wiebke%20Hutiri%20and%20Oresiti%20Papakyriakopoulos%20and%20Alice%20Xiang%0AAbstract%3A%20%20%20The%20rapid%20and%20wide-scale%20adoption%20of%20AI%20to%20generate%20human%20speech%20poses%20a%0Arange%20of%20significant%20ethical%20and%20safety%20risks%20to%20society%20that%20need%20to%20be%0Aaddressed.%20For%20example%2C%20a%20growing%20number%20of%20speech%20generation%20incidents%20are%0Aassociated%20with%20swatting%20attacks%20in%20the%20United%20States%2C%20where%20anonymous%0Aperpetrators%20create%20synthetic%20voices%20that%20call%20police%20officers%20to%20close%20down%0Aschools%20and%20hospitals%2C%20or%20to%20violently%20gain%20access%20to%20innocent%20citizens%27%20homes.%0AIncidents%20like%20this%20demonstrate%20that%20multimodal%20generative%20AI%20risks%20and%20harms%0Ado%20not%20exist%20in%20isolation%2C%20but%20arise%20from%20the%20interactions%20of%20multiple%0Astakeholders%20and%20technical%20AI%20systems.%20In%20this%20paper%20we%20analyse%20speech%0Ageneration%20incidents%20to%20study%20how%20patterns%20of%20specific%20harms%20arise.%20We%20find%0Athat%20specific%20harms%20can%20be%20categorised%20according%20to%20the%20exposure%20of%20affected%0Aindividuals%2C%20that%20is%20to%20say%20whether%20they%20are%20a%20subject%20of%2C%20interact%20with%2C%0Asuffer%20due%20to%2C%20or%20are%20excluded%20from%20speech%20generation%20systems.%20Similarly%2C%0Aspecific%20harms%20are%20also%20a%20consequence%20of%20the%20motives%20of%20the%20creators%20and%0Adeployers%20of%20the%20systems.%20Based%20on%20these%20insights%20we%20propose%20a%20conceptual%0Aframework%20for%20modelling%20pathways%20to%20ethical%20and%20safety%20harms%20of%20AI%2C%20which%20we%0Ause%20to%20develop%20a%20taxonomy%20of%20harms%20of%20speech%20generators.%20Our%20relational%0Aapproach%20captures%20the%20complexity%20of%20risks%20and%20harms%20in%20sociotechnical%20AI%0Asystems%2C%20and%20yields%20a%20taxonomy%20that%20can%20support%20appropriate%20policy%0Ainterventions%20and%20decision%20making%20for%20the%20responsible%20development%20and%20release%0Aof%20speech%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520My%2520Voice%2521%2520A%2520Taxonomy%2520of%2520Ethical%2520and%2520Safety%2520Harms%2520of%2520Speech%250A%2520%2520Generators%26entry.906535625%3DWiebke%2520Hutiri%2520and%2520Oresiti%2520Papakyriakopoulos%2520and%2520Alice%2520Xiang%26entry.1292438233%3D%2520%2520The%2520rapid%2520and%2520wide-scale%2520adoption%2520of%2520AI%2520to%2520generate%2520human%2520speech%2520poses%2520a%250Arange%2520of%2520significant%2520ethical%2520and%2520safety%2520risks%2520to%2520society%2520that%2520need%2520to%2520be%250Aaddressed.%2520For%2520example%252C%2520a%2520growing%2520number%2520of%2520speech%2520generation%2520incidents%2520are%250Aassociated%2520with%2520swatting%2520attacks%2520in%2520the%2520United%2520States%252C%2520where%2520anonymous%250Aperpetrators%2520create%2520synthetic%2520voices%2520that%2520call%2520police%2520officers%2520to%2520close%2520down%250Aschools%2520and%2520hospitals%252C%2520or%2520to%2520violently%2520gain%2520access%2520to%2520innocent%2520citizens%2527%2520homes.%250AIncidents%2520like%2520this%2520demonstrate%2520that%2520multimodal%2520generative%2520AI%2520risks%2520and%2520harms%250Ado%2520not%2520exist%2520in%2520isolation%252C%2520but%2520arise%2520from%2520the%2520interactions%2520of%2520multiple%250Astakeholders%2520and%2520technical%2520AI%2520systems.%2520In%2520this%2520paper%2520we%2520analyse%2520speech%250Ageneration%2520incidents%2520to%2520study%2520how%2520patterns%2520of%2520specific%2520harms%2520arise.%2520We%2520find%250Athat%2520specific%2520harms%2520can%2520be%2520categorised%2520according%2520to%2520the%2520exposure%2520of%2520affected%250Aindividuals%252C%2520that%2520is%2520to%2520say%2520whether%2520they%2520are%2520a%2520subject%2520of%252C%2520interact%2520with%252C%250Asuffer%2520due%2520to%252C%2520or%2520are%2520excluded%2520from%2520speech%2520generation%2520systems.%2520Similarly%252C%250Aspecific%2520harms%2520are%2520also%2520a%2520consequence%2520of%2520the%2520motives%2520of%2520the%2520creators%2520and%250Adeployers%2520of%2520the%2520systems.%2520Based%2520on%2520these%2520insights%2520we%2520propose%2520a%2520conceptual%250Aframework%2520for%2520modelling%2520pathways%2520to%2520ethical%2520and%2520safety%2520harms%2520of%2520AI%252C%2520which%2520we%250Ause%2520to%2520develop%2520a%2520taxonomy%2520of%2520harms%2520of%2520speech%2520generators.%2520Our%2520relational%250Aapproach%2520captures%2520the%2520complexity%2520of%2520risks%2520and%2520harms%2520in%2520sociotechnical%2520AI%250Asystems%252C%2520and%2520yields%2520a%2520taxonomy%2520that%2520can%2520support%2520appropriate%2520policy%250Ainterventions%2520and%2520decision%2520making%2520for%2520the%2520responsible%2520development%2520and%2520release%250Aof%2520speech%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20My%20Voice%21%20A%20Taxonomy%20of%20Ethical%20and%20Safety%20Harms%20of%20Speech%0A%20%20Generators&entry.906535625=Wiebke%20Hutiri%20and%20Oresiti%20Papakyriakopoulos%20and%20Alice%20Xiang&entry.1292438233=%20%20The%20rapid%20and%20wide-scale%20adoption%20of%20AI%20to%20generate%20human%20speech%20poses%20a%0Arange%20of%20significant%20ethical%20and%20safety%20risks%20to%20society%20that%20need%20to%20be%0Aaddressed.%20For%20example%2C%20a%20growing%20number%20of%20speech%20generation%20incidents%20are%0Aassociated%20with%20swatting%20attacks%20in%20the%20United%20States%2C%20where%20anonymous%0Aperpetrators%20create%20synthetic%20voices%20that%20call%20police%20officers%20to%20close%20down%0Aschools%20and%20hospitals%2C%20or%20to%20violently%20gain%20access%20to%20innocent%20citizens%27%20homes.%0AIncidents%20like%20this%20demonstrate%20that%20multimodal%20generative%20AI%20risks%20and%20harms%0Ado%20not%20exist%20in%20isolation%2C%20but%20arise%20from%20the%20interactions%20of%20multiple%0Astakeholders%20and%20technical%20AI%20systems.%20In%20this%20paper%20we%20analyse%20speech%0Ageneration%20incidents%20to%20study%20how%20patterns%20of%20specific%20harms%20arise.%20We%20find%0Athat%20specific%20harms%20can%20be%20categorised%20according%20to%20the%20exposure%20of%20affected%0Aindividuals%2C%20that%20is%20to%20say%20whether%20they%20are%20a%20subject%20of%2C%20interact%20with%2C%0Asuffer%20due%20to%2C%20or%20are%20excluded%20from%20speech%20generation%20systems.%20Similarly%2C%0Aspecific%20harms%20are%20also%20a%20consequence%20of%20the%20motives%20of%20the%20creators%20and%0Adeployers%20of%20the%20systems.%20Based%20on%20these%20insights%20we%20propose%20a%20conceptual%0Aframework%20for%20modelling%20pathways%20to%20ethical%20and%20safety%20harms%20of%20AI%2C%20which%20we%0Ause%20to%20develop%20a%20taxonomy%20of%20harms%20of%20speech%20generators.%20Our%20relational%0Aapproach%20captures%20the%20complexity%20of%20risks%20and%20harms%20in%20sociotechnical%20AI%0Asystems%2C%20and%20yields%20a%20taxonomy%20that%20can%20support%20appropriate%20policy%0Ainterventions%20and%20decision%20making%20for%20the%20responsible%20development%20and%20release%0Aof%20speech%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01708v2&entry.124074799=Read"},
{"title": "ReconBoost: Boosting Can Achieve Modality Reconcilement", "author": "Cong Hua and Qianqian Xu and Shilong Bao and Zhiyong Yang and Qingming Huang", "abstract": "  This paper explores a novel multi-modal alternating learning paradigm\npursuing a reconciliation between the exploitation of uni-modal features and\nthe exploration of cross-modal interactions. This is motivated by the fact that\ncurrent paradigms of multi-modal learning tend to explore multi-modal features\nsimultaneously. The resulting gradient prohibits further exploitation of the\nfeatures in the weak modality, leading to modality competition, where the\ndominant modality overpowers the learning process. To address this issue, we\nstudy the modality-alternating learning paradigm to achieve reconcilement.\nSpecifically, we propose a new method called ReconBoost to update a fixed\nmodality each time. Herein, the learning objective is dynamically adjusted with\na reconcilement regularization against competition with the historical models.\nBy choosing a KL-based reconcilement, we show that the proposed method\nresembles Friedman's Gradient-Boosting (GB) algorithm, where the updated\nlearner can correct errors made by others and help enhance the overall\nperformance. The major difference with the classic GB is that we only preserve\nthe newest model for each modality to avoid overfitting caused by ensembling\nstrong learners. Furthermore, we propose a memory consolidation scheme and a\nglobal rectification scheme to make this strategy more effective. Experiments\nover six multi-modal benchmarks speak to the efficacy of the method. We release\nthe code at https://github.com/huacong/ReconBoost.\n", "link": "http://arxiv.org/abs/2405.09321v1", "date": "2024-05-15", "relevancy": 2.1956, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5587}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReconBoost%3A%20Boosting%20Can%20Achieve%20Modality%20Reconcilement&body=Title%3A%20ReconBoost%3A%20Boosting%20Can%20Achieve%20Modality%20Reconcilement%0AAuthor%3A%20Cong%20Hua%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20This%20paper%20explores%20a%20novel%20multi-modal%20alternating%20learning%20paradigm%0Apursuing%20a%20reconciliation%20between%20the%20exploitation%20of%20uni-modal%20features%20and%0Athe%20exploration%20of%20cross-modal%20interactions.%20This%20is%20motivated%20by%20the%20fact%20that%0Acurrent%20paradigms%20of%20multi-modal%20learning%20tend%20to%20explore%20multi-modal%20features%0Asimultaneously.%20The%20resulting%20gradient%20prohibits%20further%20exploitation%20of%20the%0Afeatures%20in%20the%20weak%20modality%2C%20leading%20to%20modality%20competition%2C%20where%20the%0Adominant%20modality%20overpowers%20the%20learning%20process.%20To%20address%20this%20issue%2C%20we%0Astudy%20the%20modality-alternating%20learning%20paradigm%20to%20achieve%20reconcilement.%0ASpecifically%2C%20we%20propose%20a%20new%20method%20called%20ReconBoost%20to%20update%20a%20fixed%0Amodality%20each%20time.%20Herein%2C%20the%20learning%20objective%20is%20dynamically%20adjusted%20with%0Aa%20reconcilement%20regularization%20against%20competition%20with%20the%20historical%20models.%0ABy%20choosing%20a%20KL-based%20reconcilement%2C%20we%20show%20that%20the%20proposed%20method%0Aresembles%20Friedman%27s%20Gradient-Boosting%20%28GB%29%20algorithm%2C%20where%20the%20updated%0Alearner%20can%20correct%20errors%20made%20by%20others%20and%20help%20enhance%20the%20overall%0Aperformance.%20The%20major%20difference%20with%20the%20classic%20GB%20is%20that%20we%20only%20preserve%0Athe%20newest%20model%20for%20each%20modality%20to%20avoid%20overfitting%20caused%20by%20ensembling%0Astrong%20learners.%20Furthermore%2C%20we%20propose%20a%20memory%20consolidation%20scheme%20and%20a%0Aglobal%20rectification%20scheme%20to%20make%20this%20strategy%20more%20effective.%20Experiments%0Aover%20six%20multi-modal%20benchmarks%20speak%20to%20the%20efficacy%20of%20the%20method.%20We%20release%0Athe%20code%20at%20https%3A//github.com/huacong/ReconBoost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconBoost%253A%2520Boosting%2520Can%2520Achieve%2520Modality%2520Reconcilement%26entry.906535625%3DCong%2520Hua%2520and%2520Qianqian%2520Xu%2520and%2520Shilong%2520Bao%2520and%2520Zhiyong%2520Yang%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520a%2520novel%2520multi-modal%2520alternating%2520learning%2520paradigm%250Apursuing%2520a%2520reconciliation%2520between%2520the%2520exploitation%2520of%2520uni-modal%2520features%2520and%250Athe%2520exploration%2520of%2520cross-modal%2520interactions.%2520This%2520is%2520motivated%2520by%2520the%2520fact%2520that%250Acurrent%2520paradigms%2520of%2520multi-modal%2520learning%2520tend%2520to%2520explore%2520multi-modal%2520features%250Asimultaneously.%2520The%2520resulting%2520gradient%2520prohibits%2520further%2520exploitation%2520of%2520the%250Afeatures%2520in%2520the%2520weak%2520modality%252C%2520leading%2520to%2520modality%2520competition%252C%2520where%2520the%250Adominant%2520modality%2520overpowers%2520the%2520learning%2520process.%2520To%2520address%2520this%2520issue%252C%2520we%250Astudy%2520the%2520modality-alternating%2520learning%2520paradigm%2520to%2520achieve%2520reconcilement.%250ASpecifically%252C%2520we%2520propose%2520a%2520new%2520method%2520called%2520ReconBoost%2520to%2520update%2520a%2520fixed%250Amodality%2520each%2520time.%2520Herein%252C%2520the%2520learning%2520objective%2520is%2520dynamically%2520adjusted%2520with%250Aa%2520reconcilement%2520regularization%2520against%2520competition%2520with%2520the%2520historical%2520models.%250ABy%2520choosing%2520a%2520KL-based%2520reconcilement%252C%2520we%2520show%2520that%2520the%2520proposed%2520method%250Aresembles%2520Friedman%2527s%2520Gradient-Boosting%2520%2528GB%2529%2520algorithm%252C%2520where%2520the%2520updated%250Alearner%2520can%2520correct%2520errors%2520made%2520by%2520others%2520and%2520help%2520enhance%2520the%2520overall%250Aperformance.%2520The%2520major%2520difference%2520with%2520the%2520classic%2520GB%2520is%2520that%2520we%2520only%2520preserve%250Athe%2520newest%2520model%2520for%2520each%2520modality%2520to%2520avoid%2520overfitting%2520caused%2520by%2520ensembling%250Astrong%2520learners.%2520Furthermore%252C%2520we%2520propose%2520a%2520memory%2520consolidation%2520scheme%2520and%2520a%250Aglobal%2520rectification%2520scheme%2520to%2520make%2520this%2520strategy%2520more%2520effective.%2520Experiments%250Aover%2520six%2520multi-modal%2520benchmarks%2520speak%2520to%2520the%2520efficacy%2520of%2520the%2520method.%2520We%2520release%250Athe%2520code%2520at%2520https%253A//github.com/huacong/ReconBoost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReconBoost%3A%20Boosting%20Can%20Achieve%20Modality%20Reconcilement&entry.906535625=Cong%20Hua%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Qingming%20Huang&entry.1292438233=%20%20This%20paper%20explores%20a%20novel%20multi-modal%20alternating%20learning%20paradigm%0Apursuing%20a%20reconciliation%20between%20the%20exploitation%20of%20uni-modal%20features%20and%0Athe%20exploration%20of%20cross-modal%20interactions.%20This%20is%20motivated%20by%20the%20fact%20that%0Acurrent%20paradigms%20of%20multi-modal%20learning%20tend%20to%20explore%20multi-modal%20features%0Asimultaneously.%20The%20resulting%20gradient%20prohibits%20further%20exploitation%20of%20the%0Afeatures%20in%20the%20weak%20modality%2C%20leading%20to%20modality%20competition%2C%20where%20the%0Adominant%20modality%20overpowers%20the%20learning%20process.%20To%20address%20this%20issue%2C%20we%0Astudy%20the%20modality-alternating%20learning%20paradigm%20to%20achieve%20reconcilement.%0ASpecifically%2C%20we%20propose%20a%20new%20method%20called%20ReconBoost%20to%20update%20a%20fixed%0Amodality%20each%20time.%20Herein%2C%20the%20learning%20objective%20is%20dynamically%20adjusted%20with%0Aa%20reconcilement%20regularization%20against%20competition%20with%20the%20historical%20models.%0ABy%20choosing%20a%20KL-based%20reconcilement%2C%20we%20show%20that%20the%20proposed%20method%0Aresembles%20Friedman%27s%20Gradient-Boosting%20%28GB%29%20algorithm%2C%20where%20the%20updated%0Alearner%20can%20correct%20errors%20made%20by%20others%20and%20help%20enhance%20the%20overall%0Aperformance.%20The%20major%20difference%20with%20the%20classic%20GB%20is%20that%20we%20only%20preserve%0Athe%20newest%20model%20for%20each%20modality%20to%20avoid%20overfitting%20caused%20by%20ensembling%0Astrong%20learners.%20Furthermore%2C%20we%20propose%20a%20memory%20consolidation%20scheme%20and%20a%0Aglobal%20rectification%20scheme%20to%20make%20this%20strategy%20more%20effective.%20Experiments%0Aover%20six%20multi-modal%20benchmarks%20speak%20to%20the%20efficacy%20of%20the%20method.%20We%20release%0Athe%20code%20at%20https%3A//github.com/huacong/ReconBoost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09321v1&entry.124074799=Read"},
{"title": "Dual-Segment Clustering Strategy for Federated Learning in Heterogeneous\n  Environments", "author": "Pengcheng Sun and Erwu Liu and Wei Ni and Kanglei Yu and Rui Wang and Abbas Jamalipour", "abstract": "  Federated learning (FL) is a distributed machine learning paradigm with high\nefficiency and low communication load, only transmitting parameters or\ngradients of network. However, the non-independent and identically distributed\n(Non-IID) data characteristic has a negative impact on this paradigm.\nFurthermore, the heterogeneity of communication quality will significantly\naffect the accuracy of parameter transmission, causing a degradation in the\nperformance of the FL system or even preventing its convergence. This letter\nproposes a dual-segment clustering (DSC) strategy, which first clusters the\nclients according to the heterogeneous communication conditions and then\nperforms a second clustering by the sample size and label distribution, so as\nto solve the problem of data and communication heterogeneity. Experimental\nresults show that the DSC strategy proposed in this letter can improve the\nconvergence rate of FL, and has superiority on accuracy in a heterogeneous\nenvironment compared with the classical algorithm of cluster.\n", "link": "http://arxiv.org/abs/2405.09276v1", "date": "2024-05-15", "relevancy": 2.1849, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Segment%20Clustering%20Strategy%20for%20Federated%20Learning%20in%20Heterogeneous%0A%20%20Environments&body=Title%3A%20Dual-Segment%20Clustering%20Strategy%20for%20Federated%20Learning%20in%20Heterogeneous%0A%20%20Environments%0AAuthor%3A%20Pengcheng%20Sun%20and%20Erwu%20Liu%20and%20Wei%20Ni%20and%20Kanglei%20Yu%20and%20Rui%20Wang%20and%20Abbas%20Jamalipour%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20paradigm%20with%20high%0Aefficiency%20and%20low%20communication%20load%2C%20only%20transmitting%20parameters%20or%0Agradients%20of%20network.%20However%2C%20the%20non-independent%20and%20identically%20distributed%0A%28Non-IID%29%20data%20characteristic%20has%20a%20negative%20impact%20on%20this%20paradigm.%0AFurthermore%2C%20the%20heterogeneity%20of%20communication%20quality%20will%20significantly%0Aaffect%20the%20accuracy%20of%20parameter%20transmission%2C%20causing%20a%20degradation%20in%20the%0Aperformance%20of%20the%20FL%20system%20or%20even%20preventing%20its%20convergence.%20This%20letter%0Aproposes%20a%20dual-segment%20clustering%20%28DSC%29%20strategy%2C%20which%20first%20clusters%20the%0Aclients%20according%20to%20the%20heterogeneous%20communication%20conditions%20and%20then%0Aperforms%20a%20second%20clustering%20by%20the%20sample%20size%20and%20label%20distribution%2C%20so%20as%0Ato%20solve%20the%20problem%20of%20data%20and%20communication%20heterogeneity.%20Experimental%0Aresults%20show%20that%20the%20DSC%20strategy%20proposed%20in%20this%20letter%20can%20improve%20the%0Aconvergence%20rate%20of%20FL%2C%20and%20has%20superiority%20on%20accuracy%20in%20a%20heterogeneous%0Aenvironment%20compared%20with%20the%20classical%20algorithm%20of%20cluster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Segment%2520Clustering%2520Strategy%2520for%2520Federated%2520Learning%2520in%2520Heterogeneous%250A%2520%2520Environments%26entry.906535625%3DPengcheng%2520Sun%2520and%2520Erwu%2520Liu%2520and%2520Wei%2520Ni%2520and%2520Kanglei%2520Yu%2520and%2520Rui%2520Wang%2520and%2520Abbas%2520Jamalipour%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520paradigm%2520with%2520high%250Aefficiency%2520and%2520low%2520communication%2520load%252C%2520only%2520transmitting%2520parameters%2520or%250Agradients%2520of%2520network.%2520However%252C%2520the%2520non-independent%2520and%2520identically%2520distributed%250A%2528Non-IID%2529%2520data%2520characteristic%2520has%2520a%2520negative%2520impact%2520on%2520this%2520paradigm.%250AFurthermore%252C%2520the%2520heterogeneity%2520of%2520communication%2520quality%2520will%2520significantly%250Aaffect%2520the%2520accuracy%2520of%2520parameter%2520transmission%252C%2520causing%2520a%2520degradation%2520in%2520the%250Aperformance%2520of%2520the%2520FL%2520system%2520or%2520even%2520preventing%2520its%2520convergence.%2520This%2520letter%250Aproposes%2520a%2520dual-segment%2520clustering%2520%2528DSC%2529%2520strategy%252C%2520which%2520first%2520clusters%2520the%250Aclients%2520according%2520to%2520the%2520heterogeneous%2520communication%2520conditions%2520and%2520then%250Aperforms%2520a%2520second%2520clustering%2520by%2520the%2520sample%2520size%2520and%2520label%2520distribution%252C%2520so%2520as%250Ato%2520solve%2520the%2520problem%2520of%2520data%2520and%2520communication%2520heterogeneity.%2520Experimental%250Aresults%2520show%2520that%2520the%2520DSC%2520strategy%2520proposed%2520in%2520this%2520letter%2520can%2520improve%2520the%250Aconvergence%2520rate%2520of%2520FL%252C%2520and%2520has%2520superiority%2520on%2520accuracy%2520in%2520a%2520heterogeneous%250Aenvironment%2520compared%2520with%2520the%2520classical%2520algorithm%2520of%2520cluster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Segment%20Clustering%20Strategy%20for%20Federated%20Learning%20in%20Heterogeneous%0A%20%20Environments&entry.906535625=Pengcheng%20Sun%20and%20Erwu%20Liu%20and%20Wei%20Ni%20and%20Kanglei%20Yu%20and%20Rui%20Wang%20and%20Abbas%20Jamalipour&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20paradigm%20with%20high%0Aefficiency%20and%20low%20communication%20load%2C%20only%20transmitting%20parameters%20or%0Agradients%20of%20network.%20However%2C%20the%20non-independent%20and%20identically%20distributed%0A%28Non-IID%29%20data%20characteristic%20has%20a%20negative%20impact%20on%20this%20paradigm.%0AFurthermore%2C%20the%20heterogeneity%20of%20communication%20quality%20will%20significantly%0Aaffect%20the%20accuracy%20of%20parameter%20transmission%2C%20causing%20a%20degradation%20in%20the%0Aperformance%20of%20the%20FL%20system%20or%20even%20preventing%20its%20convergence.%20This%20letter%0Aproposes%20a%20dual-segment%20clustering%20%28DSC%29%20strategy%2C%20which%20first%20clusters%20the%0Aclients%20according%20to%20the%20heterogeneous%20communication%20conditions%20and%20then%0Aperforms%20a%20second%20clustering%20by%20the%20sample%20size%20and%20label%20distribution%2C%20so%20as%0Ato%20solve%20the%20problem%20of%20data%20and%20communication%20heterogeneity.%20Experimental%0Aresults%20show%20that%20the%20DSC%20strategy%20proposed%20in%20this%20letter%20can%20improve%20the%0Aconvergence%20rate%20of%20FL%2C%20and%20has%20superiority%20on%20accuracy%20in%20a%20heterogeneous%0Aenvironment%20compared%20with%20the%20classical%20algorithm%20of%20cluster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09276v1&entry.124074799=Read"},
{"title": "Classifying geospatial objects from multiview aerial imagery using\n  semantic meshes", "author": "David Russell and Ben Weinstein and David Wettergreen and Derek Young", "abstract": "  Aerial imagery is increasingly used in Earth science and natural resource\nmanagement as a complement to labor-intensive ground-based surveys. Aerial\nsystems can collect overlapping images that provide multiple views of each\nlocation from different perspectives. However, most prediction approaches (e.g.\nfor tree species classification) use a single, synthesized top-down\n\"orthomosaic\" image as input that contains little to no information about the\nvertical aspects of objects and may include processing artifacts. We propose an\nalternate approach that generates predictions directly on the raw images and\naccurately maps these predictions into geospatial coordinates using semantic\nmeshes. This method$\\unicode{x2013}$released as a user-friendly open-source\ntoolkit$\\unicode{x2013}$enables analysts to use the highest quality data for\npredictions, capture information about the sides of objects, and leverage\nmultiple viewpoints of each location for added robustness. We demonstrate the\nvalue of this approach on a new benchmark dataset of four forest sites in the\nwestern U.S. that consists of drone images, photogrammetry results, predicted\ntree locations, and species classification data derived from manual surveys. We\nshow that our proposed multiview method improves classification accuracy from\n53% to 75% relative to an orthomosaic baseline on a challenging cross-site tree\nspecies classification task.\n", "link": "http://arxiv.org/abs/2405.09544v1", "date": "2024-05-15", "relevancy": 2.1803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classifying%20geospatial%20objects%20from%20multiview%20aerial%20imagery%20using%0A%20%20semantic%20meshes&body=Title%3A%20Classifying%20geospatial%20objects%20from%20multiview%20aerial%20imagery%20using%0A%20%20semantic%20meshes%0AAuthor%3A%20David%20Russell%20and%20Ben%20Weinstein%20and%20David%20Wettergreen%20and%20Derek%20Young%0AAbstract%3A%20%20%20Aerial%20imagery%20is%20increasingly%20used%20in%20Earth%20science%20and%20natural%20resource%0Amanagement%20as%20a%20complement%20to%20labor-intensive%20ground-based%20surveys.%20Aerial%0Asystems%20can%20collect%20overlapping%20images%20that%20provide%20multiple%20views%20of%20each%0Alocation%20from%20different%20perspectives.%20However%2C%20most%20prediction%20approaches%20%28e.g.%0Afor%20tree%20species%20classification%29%20use%20a%20single%2C%20synthesized%20top-down%0A%22orthomosaic%22%20image%20as%20input%20that%20contains%20little%20to%20no%20information%20about%20the%0Avertical%20aspects%20of%20objects%20and%20may%20include%20processing%20artifacts.%20We%20propose%20an%0Aalternate%20approach%20that%20generates%20predictions%20directly%20on%20the%20raw%20images%20and%0Aaccurately%20maps%20these%20predictions%20into%20geospatial%20coordinates%20using%20semantic%0Ameshes.%20This%20method%24%5Cunicode%7Bx2013%7D%24released%20as%20a%20user-friendly%20open-source%0Atoolkit%24%5Cunicode%7Bx2013%7D%24enables%20analysts%20to%20use%20the%20highest%20quality%20data%20for%0Apredictions%2C%20capture%20information%20about%20the%20sides%20of%20objects%2C%20and%20leverage%0Amultiple%20viewpoints%20of%20each%20location%20for%20added%20robustness.%20We%20demonstrate%20the%0Avalue%20of%20this%20approach%20on%20a%20new%20benchmark%20dataset%20of%20four%20forest%20sites%20in%20the%0Awestern%20U.S.%20that%20consists%20of%20drone%20images%2C%20photogrammetry%20results%2C%20predicted%0Atree%20locations%2C%20and%20species%20classification%20data%20derived%20from%20manual%20surveys.%20We%0Ashow%20that%20our%20proposed%20multiview%20method%20improves%20classification%20accuracy%20from%0A53%25%20to%2075%25%20relative%20to%20an%20orthomosaic%20baseline%20on%20a%20challenging%20cross-site%20tree%0Aspecies%20classification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassifying%2520geospatial%2520objects%2520from%2520multiview%2520aerial%2520imagery%2520using%250A%2520%2520semantic%2520meshes%26entry.906535625%3DDavid%2520Russell%2520and%2520Ben%2520Weinstein%2520and%2520David%2520Wettergreen%2520and%2520Derek%2520Young%26entry.1292438233%3D%2520%2520Aerial%2520imagery%2520is%2520increasingly%2520used%2520in%2520Earth%2520science%2520and%2520natural%2520resource%250Amanagement%2520as%2520a%2520complement%2520to%2520labor-intensive%2520ground-based%2520surveys.%2520Aerial%250Asystems%2520can%2520collect%2520overlapping%2520images%2520that%2520provide%2520multiple%2520views%2520of%2520each%250Alocation%2520from%2520different%2520perspectives.%2520However%252C%2520most%2520prediction%2520approaches%2520%2528e.g.%250Afor%2520tree%2520species%2520classification%2529%2520use%2520a%2520single%252C%2520synthesized%2520top-down%250A%2522orthomosaic%2522%2520image%2520as%2520input%2520that%2520contains%2520little%2520to%2520no%2520information%2520about%2520the%250Avertical%2520aspects%2520of%2520objects%2520and%2520may%2520include%2520processing%2520artifacts.%2520We%2520propose%2520an%250Aalternate%2520approach%2520that%2520generates%2520predictions%2520directly%2520on%2520the%2520raw%2520images%2520and%250Aaccurately%2520maps%2520these%2520predictions%2520into%2520geospatial%2520coordinates%2520using%2520semantic%250Ameshes.%2520This%2520method%2524%255Cunicode%257Bx2013%257D%2524released%2520as%2520a%2520user-friendly%2520open-source%250Atoolkit%2524%255Cunicode%257Bx2013%257D%2524enables%2520analysts%2520to%2520use%2520the%2520highest%2520quality%2520data%2520for%250Apredictions%252C%2520capture%2520information%2520about%2520the%2520sides%2520of%2520objects%252C%2520and%2520leverage%250Amultiple%2520viewpoints%2520of%2520each%2520location%2520for%2520added%2520robustness.%2520We%2520demonstrate%2520the%250Avalue%2520of%2520this%2520approach%2520on%2520a%2520new%2520benchmark%2520dataset%2520of%2520four%2520forest%2520sites%2520in%2520the%250Awestern%2520U.S.%2520that%2520consists%2520of%2520drone%2520images%252C%2520photogrammetry%2520results%252C%2520predicted%250Atree%2520locations%252C%2520and%2520species%2520classification%2520data%2520derived%2520from%2520manual%2520surveys.%2520We%250Ashow%2520that%2520our%2520proposed%2520multiview%2520method%2520improves%2520classification%2520accuracy%2520from%250A53%2525%2520to%252075%2525%2520relative%2520to%2520an%2520orthomosaic%2520baseline%2520on%2520a%2520challenging%2520cross-site%2520tree%250Aspecies%2520classification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifying%20geospatial%20objects%20from%20multiview%20aerial%20imagery%20using%0A%20%20semantic%20meshes&entry.906535625=David%20Russell%20and%20Ben%20Weinstein%20and%20David%20Wettergreen%20and%20Derek%20Young&entry.1292438233=%20%20Aerial%20imagery%20is%20increasingly%20used%20in%20Earth%20science%20and%20natural%20resource%0Amanagement%20as%20a%20complement%20to%20labor-intensive%20ground-based%20surveys.%20Aerial%0Asystems%20can%20collect%20overlapping%20images%20that%20provide%20multiple%20views%20of%20each%0Alocation%20from%20different%20perspectives.%20However%2C%20most%20prediction%20approaches%20%28e.g.%0Afor%20tree%20species%20classification%29%20use%20a%20single%2C%20synthesized%20top-down%0A%22orthomosaic%22%20image%20as%20input%20that%20contains%20little%20to%20no%20information%20about%20the%0Avertical%20aspects%20of%20objects%20and%20may%20include%20processing%20artifacts.%20We%20propose%20an%0Aalternate%20approach%20that%20generates%20predictions%20directly%20on%20the%20raw%20images%20and%0Aaccurately%20maps%20these%20predictions%20into%20geospatial%20coordinates%20using%20semantic%0Ameshes.%20This%20method%24%5Cunicode%7Bx2013%7D%24released%20as%20a%20user-friendly%20open-source%0Atoolkit%24%5Cunicode%7Bx2013%7D%24enables%20analysts%20to%20use%20the%20highest%20quality%20data%20for%0Apredictions%2C%20capture%20information%20about%20the%20sides%20of%20objects%2C%20and%20leverage%0Amultiple%20viewpoints%20of%20each%20location%20for%20added%20robustness.%20We%20demonstrate%20the%0Avalue%20of%20this%20approach%20on%20a%20new%20benchmark%20dataset%20of%20four%20forest%20sites%20in%20the%0Awestern%20U.S.%20that%20consists%20of%20drone%20images%2C%20photogrammetry%20results%2C%20predicted%0Atree%20locations%2C%20and%20species%20classification%20data%20derived%20from%20manual%20surveys.%20We%0Ashow%20that%20our%20proposed%20multiview%20method%20improves%20classification%20accuracy%20from%0A53%25%20to%2075%25%20relative%20to%20an%20orthomosaic%20baseline%20on%20a%20challenging%20cross-site%20tree%0Aspecies%20classification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09544v1&entry.124074799=Read"},
{"title": "Tight Bounds for Online Convex Optimization with Adversarial Constraints", "author": "Abhishek Sinha and Rahul Vaze", "abstract": "  A well-studied generalization of the standard online convex optimization\n(OCO) is constrained online convex optimization (COCO). In COCO, on every\nround, a convex cost function and a convex constraint function are revealed to\nthe learner after the action for that round is chosen. The objective is to\ndesign an online policy that simultaneously achieves a small regret while\nensuring small cumulative constraint violation (CCV) against an adaptive\nadversary. A long-standing open question in COCO is whether an online policy\ncan simultaneously achieve $O(\\sqrt{T})$ regret and $O(\\sqrt{T})$ CCV without\nany restrictive assumptions. For the first time, we answer this in the\naffirmative and show that an online policy can simultaneously achieve\n$O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV. We establish this result by\neffectively combining the adaptive regret bound of the AdaGrad algorithm with\nLyapunov optimization - a classic tool from control theory. Surprisingly, the\nanalysis is short and elegant.\n", "link": "http://arxiv.org/abs/2405.09296v1", "date": "2024-05-15", "relevancy": 2.1785, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4502}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4319}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20Bounds%20for%20Online%20Convex%20Optimization%20with%20Adversarial%20Constraints&body=Title%3A%20Tight%20Bounds%20for%20Online%20Convex%20Optimization%20with%20Adversarial%20Constraints%0AAuthor%3A%20Abhishek%20Sinha%20and%20Rahul%20Vaze%0AAbstract%3A%20%20%20A%20well-studied%20generalization%20of%20the%20standard%20online%20convex%20optimization%0A%28OCO%29%20is%20constrained%20online%20convex%20optimization%20%28COCO%29.%20In%20COCO%2C%20on%20every%0Around%2C%20a%20convex%20cost%20function%20and%20a%20convex%20constraint%20function%20are%20revealed%20to%0Athe%20learner%20after%20the%20action%20for%20that%20round%20is%20chosen.%20The%20objective%20is%20to%0Adesign%20an%20online%20policy%20that%20simultaneously%20achieves%20a%20small%20regret%20while%0Aensuring%20small%20cumulative%20constraint%20violation%20%28CCV%29%20against%20an%20adaptive%0Aadversary.%20A%20long-standing%20open%20question%20in%20COCO%20is%20whether%20an%20online%20policy%0Acan%20simultaneously%20achieve%20%24O%28%5Csqrt%7BT%7D%29%24%20regret%20and%20%24O%28%5Csqrt%7BT%7D%29%24%20CCV%20without%0Aany%20restrictive%20assumptions.%20For%20the%20first%20time%2C%20we%20answer%20this%20in%20the%0Aaffirmative%20and%20show%20that%20an%20online%20policy%20can%20simultaneously%20achieve%0A%24O%28%5Csqrt%7BT%7D%29%24%20regret%20and%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20CCV.%20We%20establish%20this%20result%20by%0Aeffectively%20combining%20the%20adaptive%20regret%20bound%20of%20the%20AdaGrad%20algorithm%20with%0ALyapunov%20optimization%20-%20a%20classic%20tool%20from%20control%20theory.%20Surprisingly%2C%20the%0Aanalysis%20is%20short%20and%20elegant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520Bounds%2520for%2520Online%2520Convex%2520Optimization%2520with%2520Adversarial%2520Constraints%26entry.906535625%3DAbhishek%2520Sinha%2520and%2520Rahul%2520Vaze%26entry.1292438233%3D%2520%2520A%2520well-studied%2520generalization%2520of%2520the%2520standard%2520online%2520convex%2520optimization%250A%2528OCO%2529%2520is%2520constrained%2520online%2520convex%2520optimization%2520%2528COCO%2529.%2520In%2520COCO%252C%2520on%2520every%250Around%252C%2520a%2520convex%2520cost%2520function%2520and%2520a%2520convex%2520constraint%2520function%2520are%2520revealed%2520to%250Athe%2520learner%2520after%2520the%2520action%2520for%2520that%2520round%2520is%2520chosen.%2520The%2520objective%2520is%2520to%250Adesign%2520an%2520online%2520policy%2520that%2520simultaneously%2520achieves%2520a%2520small%2520regret%2520while%250Aensuring%2520small%2520cumulative%2520constraint%2520violation%2520%2528CCV%2529%2520against%2520an%2520adaptive%250Aadversary.%2520A%2520long-standing%2520open%2520question%2520in%2520COCO%2520is%2520whether%2520an%2520online%2520policy%250Acan%2520simultaneously%2520achieve%2520%2524O%2528%255Csqrt%257BT%257D%2529%2524%2520regret%2520and%2520%2524O%2528%255Csqrt%257BT%257D%2529%2524%2520CCV%2520without%250Aany%2520restrictive%2520assumptions.%2520For%2520the%2520first%2520time%252C%2520we%2520answer%2520this%2520in%2520the%250Aaffirmative%2520and%2520show%2520that%2520an%2520online%2520policy%2520can%2520simultaneously%2520achieve%250A%2524O%2528%255Csqrt%257BT%257D%2529%2524%2520regret%2520and%2520%2524%255Ctilde%257BO%257D%2528%255Csqrt%257BT%257D%2529%2524%2520CCV.%2520We%2520establish%2520this%2520result%2520by%250Aeffectively%2520combining%2520the%2520adaptive%2520regret%2520bound%2520of%2520the%2520AdaGrad%2520algorithm%2520with%250ALyapunov%2520optimization%2520-%2520a%2520classic%2520tool%2520from%2520control%2520theory.%2520Surprisingly%252C%2520the%250Aanalysis%2520is%2520short%2520and%2520elegant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20Bounds%20for%20Online%20Convex%20Optimization%20with%20Adversarial%20Constraints&entry.906535625=Abhishek%20Sinha%20and%20Rahul%20Vaze&entry.1292438233=%20%20A%20well-studied%20generalization%20of%20the%20standard%20online%20convex%20optimization%0A%28OCO%29%20is%20constrained%20online%20convex%20optimization%20%28COCO%29.%20In%20COCO%2C%20on%20every%0Around%2C%20a%20convex%20cost%20function%20and%20a%20convex%20constraint%20function%20are%20revealed%20to%0Athe%20learner%20after%20the%20action%20for%20that%20round%20is%20chosen.%20The%20objective%20is%20to%0Adesign%20an%20online%20policy%20that%20simultaneously%20achieves%20a%20small%20regret%20while%0Aensuring%20small%20cumulative%20constraint%20violation%20%28CCV%29%20against%20an%20adaptive%0Aadversary.%20A%20long-standing%20open%20question%20in%20COCO%20is%20whether%20an%20online%20policy%0Acan%20simultaneously%20achieve%20%24O%28%5Csqrt%7BT%7D%29%24%20regret%20and%20%24O%28%5Csqrt%7BT%7D%29%24%20CCV%20without%0Aany%20restrictive%20assumptions.%20For%20the%20first%20time%2C%20we%20answer%20this%20in%20the%0Aaffirmative%20and%20show%20that%20an%20online%20policy%20can%20simultaneously%20achieve%0A%24O%28%5Csqrt%7BT%7D%29%24%20regret%20and%20%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20CCV.%20We%20establish%20this%20result%20by%0Aeffectively%20combining%20the%20adaptive%20regret%20bound%20of%20the%20AdaGrad%20algorithm%20with%0ALyapunov%20optimization%20-%20a%20classic%20tool%20from%20control%20theory.%20Surprisingly%2C%20the%0Aanalysis%20is%20short%20and%20elegant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09296v1&entry.124074799=Read"},
{"title": "LRVS-Fashion: Extending Visual Search with Referring Instructions", "author": "Simon Lepage and J\u00e9r\u00e9mie Mary and David Picard", "abstract": "  This paper introduces a new challenge for image similarity search in the\ncontext of fashion, addressing the inherent ambiguity in this domain stemming\nfrom complex images. We present Referred Visual Search (RVS), a task allowing\nusers to define more precisely the desired similarity, following recent\ninterest in the industry. We release a new large public dataset, LRVS-Fashion,\nconsisting of 272k fashion products with 842k images extracted from fashion\ncatalogs, designed explicitly for this task. However, unlike traditional visual\nsearch methods in the industry, we demonstrate that superior performance can be\nachieved by bypassing explicit object detection and adopting weakly-supervised\nconditional contrastive learning on image tuples. Our method is lightweight and\ndemonstrates robustness, reaching Recall at one superior to strong\ndetection-based baselines against 2M distractors. The dataset is available at\nhttps://huggingface.co/datasets/Slep/LAION-RVS-Fashion .\n", "link": "http://arxiv.org/abs/2306.02928v3", "date": "2024-05-15", "relevancy": 2.1425, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6106}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5281}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRVS-Fashion%3A%20Extending%20Visual%20Search%20with%20Referring%20Instructions&body=Title%3A%20LRVS-Fashion%3A%20Extending%20Visual%20Search%20with%20Referring%20Instructions%0AAuthor%3A%20Simon%20Lepage%20and%20J%C3%A9r%C3%A9mie%20Mary%20and%20David%20Picard%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20challenge%20for%20image%20similarity%20search%20in%20the%0Acontext%20of%20fashion%2C%20addressing%20the%20inherent%20ambiguity%20in%20this%20domain%20stemming%0Afrom%20complex%20images.%20We%20present%20Referred%20Visual%20Search%20%28RVS%29%2C%20a%20task%20allowing%0Ausers%20to%20define%20more%20precisely%20the%20desired%20similarity%2C%20following%20recent%0Ainterest%20in%20the%20industry.%20We%20release%20a%20new%20large%20public%20dataset%2C%20LRVS-Fashion%2C%0Aconsisting%20of%20272k%20fashion%20products%20with%20842k%20images%20extracted%20from%20fashion%0Acatalogs%2C%20designed%20explicitly%20for%20this%20task.%20However%2C%20unlike%20traditional%20visual%0Asearch%20methods%20in%20the%20industry%2C%20we%20demonstrate%20that%20superior%20performance%20can%20be%0Aachieved%20by%20bypassing%20explicit%20object%20detection%20and%20adopting%20weakly-supervised%0Aconditional%20contrastive%20learning%20on%20image%20tuples.%20Our%20method%20is%20lightweight%20and%0Ademonstrates%20robustness%2C%20reaching%20Recall%20at%20one%20superior%20to%20strong%0Adetection-based%20baselines%20against%202M%20distractors.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/Slep/LAION-RVS-Fashion%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRVS-Fashion%253A%2520Extending%2520Visual%2520Search%2520with%2520Referring%2520Instructions%26entry.906535625%3DSimon%2520Lepage%2520and%2520J%25C3%25A9r%25C3%25A9mie%2520Mary%2520and%2520David%2520Picard%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520challenge%2520for%2520image%2520similarity%2520search%2520in%2520the%250Acontext%2520of%2520fashion%252C%2520addressing%2520the%2520inherent%2520ambiguity%2520in%2520this%2520domain%2520stemming%250Afrom%2520complex%2520images.%2520We%2520present%2520Referred%2520Visual%2520Search%2520%2528RVS%2529%252C%2520a%2520task%2520allowing%250Ausers%2520to%2520define%2520more%2520precisely%2520the%2520desired%2520similarity%252C%2520following%2520recent%250Ainterest%2520in%2520the%2520industry.%2520We%2520release%2520a%2520new%2520large%2520public%2520dataset%252C%2520LRVS-Fashion%252C%250Aconsisting%2520of%2520272k%2520fashion%2520products%2520with%2520842k%2520images%2520extracted%2520from%2520fashion%250Acatalogs%252C%2520designed%2520explicitly%2520for%2520this%2520task.%2520However%252C%2520unlike%2520traditional%2520visual%250Asearch%2520methods%2520in%2520the%2520industry%252C%2520we%2520demonstrate%2520that%2520superior%2520performance%2520can%2520be%250Aachieved%2520by%2520bypassing%2520explicit%2520object%2520detection%2520and%2520adopting%2520weakly-supervised%250Aconditional%2520contrastive%2520learning%2520on%2520image%2520tuples.%2520Our%2520method%2520is%2520lightweight%2520and%250Ademonstrates%2520robustness%252C%2520reaching%2520Recall%2520at%2520one%2520superior%2520to%2520strong%250Adetection-based%2520baselines%2520against%25202M%2520distractors.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/Slep/LAION-RVS-Fashion%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRVS-Fashion%3A%20Extending%20Visual%20Search%20with%20Referring%20Instructions&entry.906535625=Simon%20Lepage%20and%20J%C3%A9r%C3%A9mie%20Mary%20and%20David%20Picard&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20challenge%20for%20image%20similarity%20search%20in%20the%0Acontext%20of%20fashion%2C%20addressing%20the%20inherent%20ambiguity%20in%20this%20domain%20stemming%0Afrom%20complex%20images.%20We%20present%20Referred%20Visual%20Search%20%28RVS%29%2C%20a%20task%20allowing%0Ausers%20to%20define%20more%20precisely%20the%20desired%20similarity%2C%20following%20recent%0Ainterest%20in%20the%20industry.%20We%20release%20a%20new%20large%20public%20dataset%2C%20LRVS-Fashion%2C%0Aconsisting%20of%20272k%20fashion%20products%20with%20842k%20images%20extracted%20from%20fashion%0Acatalogs%2C%20designed%20explicitly%20for%20this%20task.%20However%2C%20unlike%20traditional%20visual%0Asearch%20methods%20in%20the%20industry%2C%20we%20demonstrate%20that%20superior%20performance%20can%20be%0Aachieved%20by%20bypassing%20explicit%20object%20detection%20and%20adopting%20weakly-supervised%0Aconditional%20contrastive%20learning%20on%20image%20tuples.%20Our%20method%20is%20lightweight%20and%0Ademonstrates%20robustness%2C%20reaching%20Recall%20at%20one%20superior%20to%20strong%0Adetection-based%20baselines%20against%202M%20distractors.%20The%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/Slep/LAION-RVS-Fashion%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02928v3&entry.124074799=Read"},
{"title": "Three-Dimensional Path Planning: Navigating through Rough Mereology", "author": "Aleksandra Szpakowska and Piotr Artiemjew", "abstract": "  In this paper, we present an innovative technique for the path planning of\nflying robots in a 3D environment in Rough Mereology terms. The main goal was\nto construct the algorithm that would generate the mereological potential\nfields in 3-dimensional space. To avoid falling into the local minimum, we\nassist with a weighted Euclidean distance. Moreover, a searching path from the\nstart point to the target, with respect to avoiding the obstacles was applied.\nThe environment was created by connecting two cameras working in real-time. To\ndetermine the gate and elements of the world inside the map was responsible the\nPython Library OpenCV [1] which recognized shapes and colors. The main purpose\nof this paper is to apply the given results to drones.\n", "link": "http://arxiv.org/abs/2405.09282v1", "date": "2024-05-15", "relevancy": 2.1384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5351}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-Dimensional%20Path%20Planning%3A%20Navigating%20through%20Rough%20Mereology&body=Title%3A%20Three-Dimensional%20Path%20Planning%3A%20Navigating%20through%20Rough%20Mereology%0AAuthor%3A%20Aleksandra%20Szpakowska%20and%20Piotr%20Artiemjew%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20innovative%20technique%20for%20the%20path%20planning%20of%0Aflying%20robots%20in%20a%203D%20environment%20in%20Rough%20Mereology%20terms.%20The%20main%20goal%20was%0Ato%20construct%20the%20algorithm%20that%20would%20generate%20the%20mereological%20potential%0Afields%20in%203-dimensional%20space.%20To%20avoid%20falling%20into%20the%20local%20minimum%2C%20we%0Aassist%20with%20a%20weighted%20Euclidean%20distance.%20Moreover%2C%20a%20searching%20path%20from%20the%0Astart%20point%20to%20the%20target%2C%20with%20respect%20to%20avoiding%20the%20obstacles%20was%20applied.%0AThe%20environment%20was%20created%20by%20connecting%20two%20cameras%20working%20in%20real-time.%20To%0Adetermine%20the%20gate%20and%20elements%20of%20the%20world%20inside%20the%20map%20was%20responsible%20the%0APython%20Library%20OpenCV%20%5B1%5D%20which%20recognized%20shapes%20and%20colors.%20The%20main%20purpose%0Aof%20this%20paper%20is%20to%20apply%20the%20given%20results%20to%20drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-Dimensional%2520Path%2520Planning%253A%2520Navigating%2520through%2520Rough%2520Mereology%26entry.906535625%3DAleksandra%2520Szpakowska%2520and%2520Piotr%2520Artiemjew%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520innovative%2520technique%2520for%2520the%2520path%2520planning%2520of%250Aflying%2520robots%2520in%2520a%25203D%2520environment%2520in%2520Rough%2520Mereology%2520terms.%2520The%2520main%2520goal%2520was%250Ato%2520construct%2520the%2520algorithm%2520that%2520would%2520generate%2520the%2520mereological%2520potential%250Afields%2520in%25203-dimensional%2520space.%2520To%2520avoid%2520falling%2520into%2520the%2520local%2520minimum%252C%2520we%250Aassist%2520with%2520a%2520weighted%2520Euclidean%2520distance.%2520Moreover%252C%2520a%2520searching%2520path%2520from%2520the%250Astart%2520point%2520to%2520the%2520target%252C%2520with%2520respect%2520to%2520avoiding%2520the%2520obstacles%2520was%2520applied.%250AThe%2520environment%2520was%2520created%2520by%2520connecting%2520two%2520cameras%2520working%2520in%2520real-time.%2520To%250Adetermine%2520the%2520gate%2520and%2520elements%2520of%2520the%2520world%2520inside%2520the%2520map%2520was%2520responsible%2520the%250APython%2520Library%2520OpenCV%2520%255B1%255D%2520which%2520recognized%2520shapes%2520and%2520colors.%2520The%2520main%2520purpose%250Aof%2520this%2520paper%2520is%2520to%2520apply%2520the%2520given%2520results%2520to%2520drones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-Dimensional%20Path%20Planning%3A%20Navigating%20through%20Rough%20Mereology&entry.906535625=Aleksandra%20Szpakowska%20and%20Piotr%20Artiemjew&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20innovative%20technique%20for%20the%20path%20planning%20of%0Aflying%20robots%20in%20a%203D%20environment%20in%20Rough%20Mereology%20terms.%20The%20main%20goal%20was%0Ato%20construct%20the%20algorithm%20that%20would%20generate%20the%20mereological%20potential%0Afields%20in%203-dimensional%20space.%20To%20avoid%20falling%20into%20the%20local%20minimum%2C%20we%0Aassist%20with%20a%20weighted%20Euclidean%20distance.%20Moreover%2C%20a%20searching%20path%20from%20the%0Astart%20point%20to%20the%20target%2C%20with%20respect%20to%20avoiding%20the%20obstacles%20was%20applied.%0AThe%20environment%20was%20created%20by%20connecting%20two%20cameras%20working%20in%20real-time.%20To%0Adetermine%20the%20gate%20and%20elements%20of%20the%20world%20inside%20the%20map%20was%20responsible%20the%0APython%20Library%20OpenCV%20%5B1%5D%20which%20recognized%20shapes%20and%20colors.%20The%20main%20purpose%0Aof%20this%20paper%20is%20to%20apply%20the%20given%20results%20to%20drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09282v1&entry.124074799=Read"},
{"title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural\n  Networks", "author": "Zhi-Qin John Xu and Yaoyu Zhang and Tao Luo and Yanyang Xiao and Zheng Ma", "abstract": "  We study the training process of Deep Neural Networks (DNNs) from the Fourier\nanalysis perspective. We demonstrate a very universal Frequency Principle\n(F-Principle) -- DNNs often fit target functions from low to high frequencies\n-- on high-dimensional benchmark datasets such as MNIST/CIFAR10 and deep neural\nnetworks such as VGG16. This F-Principle of DNNs is opposite to the behavior of\nmost conventional iterative numerical schemes (e.g., Jacobi method), which\nexhibit faster convergence for higher frequencies for various scientific\ncomputing problems. With a simple theory, we illustrate that this F-Principle\nresults from the regularity of the commonly used activation functions. The\nF-Principle implies an implicit bias that DNNs tend to fit training data by a\nlow-frequency function. This understanding provides an explanation of good\ngeneralization of DNNs on most real datasets and bad generalization of DNNs on\nparity function or randomized dataset.\n", "link": "http://arxiv.org/abs/1901.06523v6", "date": "2024-05-15", "relevancy": 2.1354, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4418}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency%20Principle%3A%20Fourier%20Analysis%20Sheds%20Light%20on%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20Frequency%20Principle%3A%20Fourier%20Analysis%20Sheds%20Light%20on%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Zhi-Qin%20John%20Xu%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo%20and%20Yanyang%20Xiao%20and%20Zheng%20Ma%0AAbstract%3A%20%20%20We%20study%20the%20training%20process%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20from%20the%20Fourier%0Aanalysis%20perspective.%20We%20demonstrate%20a%20very%20universal%20Frequency%20Principle%0A%28F-Principle%29%20--%20DNNs%20often%20fit%20target%20functions%20from%20low%20to%20high%20frequencies%0A--%20on%20high-dimensional%20benchmark%20datasets%20such%20as%20MNIST/CIFAR10%20and%20deep%20neural%0Anetworks%20such%20as%20VGG16.%20This%20F-Principle%20of%20DNNs%20is%20opposite%20to%20the%20behavior%20of%0Amost%20conventional%20iterative%20numerical%20schemes%20%28e.g.%2C%20Jacobi%20method%29%2C%20which%0Aexhibit%20faster%20convergence%20for%20higher%20frequencies%20for%20various%20scientific%0Acomputing%20problems.%20With%20a%20simple%20theory%2C%20we%20illustrate%20that%20this%20F-Principle%0Aresults%20from%20the%20regularity%20of%20the%20commonly%20used%20activation%20functions.%20The%0AF-Principle%20implies%20an%20implicit%20bias%20that%20DNNs%20tend%20to%20fit%20training%20data%20by%20a%0Alow-frequency%20function.%20This%20understanding%20provides%20an%20explanation%20of%20good%0Ageneralization%20of%20DNNs%20on%20most%20real%20datasets%20and%20bad%20generalization%20of%20DNNs%20on%0Aparity%20function%20or%20randomized%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1901.06523v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency%2520Principle%253A%2520Fourier%2520Analysis%2520Sheds%2520Light%2520on%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DZhi-Qin%2520John%2520Xu%2520and%2520Yaoyu%2520Zhang%2520and%2520Tao%2520Luo%2520and%2520Yanyang%2520Xiao%2520and%2520Zheng%2520Ma%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520training%2520process%2520of%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520from%2520the%2520Fourier%250Aanalysis%2520perspective.%2520We%2520demonstrate%2520a%2520very%2520universal%2520Frequency%2520Principle%250A%2528F-Principle%2529%2520--%2520DNNs%2520often%2520fit%2520target%2520functions%2520from%2520low%2520to%2520high%2520frequencies%250A--%2520on%2520high-dimensional%2520benchmark%2520datasets%2520such%2520as%2520MNIST/CIFAR10%2520and%2520deep%2520neural%250Anetworks%2520such%2520as%2520VGG16.%2520This%2520F-Principle%2520of%2520DNNs%2520is%2520opposite%2520to%2520the%2520behavior%2520of%250Amost%2520conventional%2520iterative%2520numerical%2520schemes%2520%2528e.g.%252C%2520Jacobi%2520method%2529%252C%2520which%250Aexhibit%2520faster%2520convergence%2520for%2520higher%2520frequencies%2520for%2520various%2520scientific%250Acomputing%2520problems.%2520With%2520a%2520simple%2520theory%252C%2520we%2520illustrate%2520that%2520this%2520F-Principle%250Aresults%2520from%2520the%2520regularity%2520of%2520the%2520commonly%2520used%2520activation%2520functions.%2520The%250AF-Principle%2520implies%2520an%2520implicit%2520bias%2520that%2520DNNs%2520tend%2520to%2520fit%2520training%2520data%2520by%2520a%250Alow-frequency%2520function.%2520This%2520understanding%2520provides%2520an%2520explanation%2520of%2520good%250Ageneralization%2520of%2520DNNs%2520on%2520most%2520real%2520datasets%2520and%2520bad%2520generalization%2520of%2520DNNs%2520on%250Aparity%2520function%2520or%2520randomized%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1901.06523v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency%20Principle%3A%20Fourier%20Analysis%20Sheds%20Light%20on%20Deep%20Neural%0A%20%20Networks&entry.906535625=Zhi-Qin%20John%20Xu%20and%20Yaoyu%20Zhang%20and%20Tao%20Luo%20and%20Yanyang%20Xiao%20and%20Zheng%20Ma&entry.1292438233=%20%20We%20study%20the%20training%20process%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20from%20the%20Fourier%0Aanalysis%20perspective.%20We%20demonstrate%20a%20very%20universal%20Frequency%20Principle%0A%28F-Principle%29%20--%20DNNs%20often%20fit%20target%20functions%20from%20low%20to%20high%20frequencies%0A--%20on%20high-dimensional%20benchmark%20datasets%20such%20as%20MNIST/CIFAR10%20and%20deep%20neural%0Anetworks%20such%20as%20VGG16.%20This%20F-Principle%20of%20DNNs%20is%20opposite%20to%20the%20behavior%20of%0Amost%20conventional%20iterative%20numerical%20schemes%20%28e.g.%2C%20Jacobi%20method%29%2C%20which%0Aexhibit%20faster%20convergence%20for%20higher%20frequencies%20for%20various%20scientific%0Acomputing%20problems.%20With%20a%20simple%20theory%2C%20we%20illustrate%20that%20this%20F-Principle%0Aresults%20from%20the%20regularity%20of%20the%20commonly%20used%20activation%20functions.%20The%0AF-Principle%20implies%20an%20implicit%20bias%20that%20DNNs%20tend%20to%20fit%20training%20data%20by%20a%0Alow-frequency%20function.%20This%20understanding%20provides%20an%20explanation%20of%20good%0Ageneralization%20of%20DNNs%20on%20most%20real%20datasets%20and%20bad%20generalization%20of%20DNNs%20on%0Aparity%20function%20or%20randomized%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/1901.06523v6&entry.124074799=Read"},
{"title": "RaffeSDG: Random Frequency Filtering enabled Single-source Domain\n  Generalization for Medical Image Segmentation", "author": "Heng Li and Haojin Li and Jianyu Chen and Zhongxi Qiu and Huazhu Fu and Lidai Wang and Yan Hu and Jiang Liu", "abstract": "  Deep learning models often encounter challenges in making accurate inferences\nwhen there are domain shifts between the source and target data. This issue is\nparticularly pronounced in clinical settings due to the scarcity of annotated\ndata resulting from the professional and private nature of medical data.\nDespite the existence of decent solutions, many of them are hindered in\nclinical settings due to limitations in data collection and computational\ncomplexity. To tackle domain shifts in data-scarce medical scenarios, we\npropose a Random frequency filtering enabled Single-source Domain\nGeneralization algorithm (RaffeSDG), which promises robust out-of-domain\ninference with segmentation models trained on a single-source domain. A\nfilter-based data augmentation strategy is first proposed to promote domain\nvariability within a single-source domain by introducing variations in\nfrequency space and blending homologous samples. Then Gaussian filter-based\nstructural saliency is also leveraged to learn robust representations across\naugmented samples, further facilitating the training of generalizable\nsegmentation models. To validate the effectiveness of RaffeSDG, we conducted\nextensive experiments involving out-of-domain inference on segmentation tasks\nfor three human tissues imaged by four diverse modalities. Through thorough\ninvestigations and comparisons, compelling evidence was observed in these\nexperiments, demonstrating the potential and generalizability of RaffeSDG. The\ncode is available at\nhttps://github.com/liamheng/Non-IID_Medical_Image_Segmentation.\n", "link": "http://arxiv.org/abs/2405.01228v2", "date": "2024-05-15", "relevancy": 2.1097, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5469}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5377}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation&body=Title%3A%20RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Heng%20Li%20and%20Haojin%20Li%20and%20Jianyu%20Chen%20and%20Zhongxi%20Qiu%20and%20Huazhu%20Fu%20and%20Lidai%20Wang%20and%20Yan%20Hu%20and%20Jiang%20Liu%0AAbstract%3A%20%20%20Deep%20learning%20models%20often%20encounter%20challenges%20in%20making%20accurate%20inferences%0Awhen%20there%20are%20domain%20shifts%20between%20the%20source%20and%20target%20data.%20This%20issue%20is%0Aparticularly%20pronounced%20in%20clinical%20settings%20due%20to%20the%20scarcity%20of%20annotated%0Adata%20resulting%20from%20the%20professional%20and%20private%20nature%20of%20medical%20data.%0ADespite%20the%20existence%20of%20decent%20solutions%2C%20many%20of%20them%20are%20hindered%20in%0Aclinical%20settings%20due%20to%20limitations%20in%20data%20collection%20and%20computational%0Acomplexity.%20To%20tackle%20domain%20shifts%20in%20data-scarce%20medical%20scenarios%2C%20we%0Apropose%20a%20Random%20frequency%20filtering%20enabled%20Single-source%20Domain%0AGeneralization%20algorithm%20%28RaffeSDG%29%2C%20which%20promises%20robust%20out-of-domain%0Ainference%20with%20segmentation%20models%20trained%20on%20a%20single-source%20domain.%20A%0Afilter-based%20data%20augmentation%20strategy%20is%20first%20proposed%20to%20promote%20domain%0Avariability%20within%20a%20single-source%20domain%20by%20introducing%20variations%20in%0Afrequency%20space%20and%20blending%20homologous%20samples.%20Then%20Gaussian%20filter-based%0Astructural%20saliency%20is%20also%20leveraged%20to%20learn%20robust%20representations%20across%0Aaugmented%20samples%2C%20further%20facilitating%20the%20training%20of%20generalizable%0Asegmentation%20models.%20To%20validate%20the%20effectiveness%20of%20RaffeSDG%2C%20we%20conducted%0Aextensive%20experiments%20involving%20out-of-domain%20inference%20on%20segmentation%20tasks%0Afor%20three%20human%20tissues%20imaged%20by%20four%20diverse%20modalities.%20Through%20thorough%0Ainvestigations%20and%20comparisons%2C%20compelling%20evidence%20was%20observed%20in%20these%0Aexperiments%2C%20demonstrating%20the%20potential%20and%20generalizability%20of%20RaffeSDG.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaffeSDG%253A%2520Random%2520Frequency%2520Filtering%2520enabled%2520Single-source%2520Domain%250A%2520%2520Generalization%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DHeng%2520Li%2520and%2520Haojin%2520Li%2520and%2520Jianyu%2520Chen%2520and%2520Zhongxi%2520Qiu%2520and%2520Huazhu%2520Fu%2520and%2520Lidai%2520Wang%2520and%2520Yan%2520Hu%2520and%2520Jiang%2520Liu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520often%2520encounter%2520challenges%2520in%2520making%2520accurate%2520inferences%250Awhen%2520there%2520are%2520domain%2520shifts%2520between%2520the%2520source%2520and%2520target%2520data.%2520This%2520issue%2520is%250Aparticularly%2520pronounced%2520in%2520clinical%2520settings%2520due%2520to%2520the%2520scarcity%2520of%2520annotated%250Adata%2520resulting%2520from%2520the%2520professional%2520and%2520private%2520nature%2520of%2520medical%2520data.%250ADespite%2520the%2520existence%2520of%2520decent%2520solutions%252C%2520many%2520of%2520them%2520are%2520hindered%2520in%250Aclinical%2520settings%2520due%2520to%2520limitations%2520in%2520data%2520collection%2520and%2520computational%250Acomplexity.%2520To%2520tackle%2520domain%2520shifts%2520in%2520data-scarce%2520medical%2520scenarios%252C%2520we%250Apropose%2520a%2520Random%2520frequency%2520filtering%2520enabled%2520Single-source%2520Domain%250AGeneralization%2520algorithm%2520%2528RaffeSDG%2529%252C%2520which%2520promises%2520robust%2520out-of-domain%250Ainference%2520with%2520segmentation%2520models%2520trained%2520on%2520a%2520single-source%2520domain.%2520A%250Afilter-based%2520data%2520augmentation%2520strategy%2520is%2520first%2520proposed%2520to%2520promote%2520domain%250Avariability%2520within%2520a%2520single-source%2520domain%2520by%2520introducing%2520variations%2520in%250Afrequency%2520space%2520and%2520blending%2520homologous%2520samples.%2520Then%2520Gaussian%2520filter-based%250Astructural%2520saliency%2520is%2520also%2520leveraged%2520to%2520learn%2520robust%2520representations%2520across%250Aaugmented%2520samples%252C%2520further%2520facilitating%2520the%2520training%2520of%2520generalizable%250Asegmentation%2520models.%2520To%2520validate%2520the%2520effectiveness%2520of%2520RaffeSDG%252C%2520we%2520conducted%250Aextensive%2520experiments%2520involving%2520out-of-domain%2520inference%2520on%2520segmentation%2520tasks%250Afor%2520three%2520human%2520tissues%2520imaged%2520by%2520four%2520diverse%2520modalities.%2520Through%2520thorough%250Ainvestigations%2520and%2520comparisons%252C%2520compelling%2520evidence%2520was%2520observed%2520in%2520these%250Aexperiments%252C%2520demonstrating%2520the%2520potential%2520and%2520generalizability%2520of%2520RaffeSDG.%2520The%250Acode%2520is%2520available%2520at%250Ahttps%253A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaffeSDG%3A%20Random%20Frequency%20Filtering%20enabled%20Single-source%20Domain%0A%20%20Generalization%20for%20Medical%20Image%20Segmentation&entry.906535625=Heng%20Li%20and%20Haojin%20Li%20and%20Jianyu%20Chen%20and%20Zhongxi%20Qiu%20and%20Huazhu%20Fu%20and%20Lidai%20Wang%20and%20Yan%20Hu%20and%20Jiang%20Liu&entry.1292438233=%20%20Deep%20learning%20models%20often%20encounter%20challenges%20in%20making%20accurate%20inferences%0Awhen%20there%20are%20domain%20shifts%20between%20the%20source%20and%20target%20data.%20This%20issue%20is%0Aparticularly%20pronounced%20in%20clinical%20settings%20due%20to%20the%20scarcity%20of%20annotated%0Adata%20resulting%20from%20the%20professional%20and%20private%20nature%20of%20medical%20data.%0ADespite%20the%20existence%20of%20decent%20solutions%2C%20many%20of%20them%20are%20hindered%20in%0Aclinical%20settings%20due%20to%20limitations%20in%20data%20collection%20and%20computational%0Acomplexity.%20To%20tackle%20domain%20shifts%20in%20data-scarce%20medical%20scenarios%2C%20we%0Apropose%20a%20Random%20frequency%20filtering%20enabled%20Single-source%20Domain%0AGeneralization%20algorithm%20%28RaffeSDG%29%2C%20which%20promises%20robust%20out-of-domain%0Ainference%20with%20segmentation%20models%20trained%20on%20a%20single-source%20domain.%20A%0Afilter-based%20data%20augmentation%20strategy%20is%20first%20proposed%20to%20promote%20domain%0Avariability%20within%20a%20single-source%20domain%20by%20introducing%20variations%20in%0Afrequency%20space%20and%20blending%20homologous%20samples.%20Then%20Gaussian%20filter-based%0Astructural%20saliency%20is%20also%20leveraged%20to%20learn%20robust%20representations%20across%0Aaugmented%20samples%2C%20further%20facilitating%20the%20training%20of%20generalizable%0Asegmentation%20models.%20To%20validate%20the%20effectiveness%20of%20RaffeSDG%2C%20we%20conducted%0Aextensive%20experiments%20involving%20out-of-domain%20inference%20on%20segmentation%20tasks%0Afor%20three%20human%20tissues%20imaged%20by%20four%20diverse%20modalities.%20Through%20thorough%0Ainvestigations%20and%20comparisons%2C%20compelling%20evidence%20was%20observed%20in%20these%0Aexperiments%2C%20demonstrating%20the%20potential%20and%20generalizability%20of%20RaffeSDG.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/liamheng/Non-IID_Medical_Image_Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01228v2&entry.124074799=Read"},
{"title": "A multiscale and multicriteria Generative Adversarial Network to\n  synthesize 1-dimensional turbulent fields", "author": "Carlos Granero-Belinchon and Manuel Cabeza Gallucci", "abstract": "  This article introduces a new Neural Network stochastic model to generate a\n1-dimensional stochastic field with turbulent velocity statistics. Both the\nmodel architecture and training procedure ground on the Kolmogorov and Obukhov\nstatistical theories of fully developed turbulence, so guaranteeing\ndescriptions of 1) energy distribution, 2) energy cascade and 3) intermittency\nacross scales in agreement with experimental observations. The model is a\nGenerative Adversarial Network with multiple multiscale optimization criteria.\nFirst, we use three physics-based criteria: the variance, skewness and flatness\nof the increments of the generated field that retrieve respectively the\nturbulent energy distribution, energy cascade and intermittency across scales.\nSecond, the Generative Adversarial Network criterion, based on reproducing\nstatistical distributions, is used on segments of different length of the\ngenerated field. Furthermore, to mimic multiscale decompositions frequently\nused in turbulence's studies, the model architecture is fully convolutional\nwith kernel sizes varying along the multiple layers of the model. To train our\nmodel we use turbulent velocity signals from grid turbulence at Modane wind\ntunnel.\n", "link": "http://arxiv.org/abs/2307.16580v2", "date": "2024-05-15", "relevancy": 2.1045, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5931}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5129}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20multiscale%20and%20multicriteria%20Generative%20Adversarial%20Network%20to%0A%20%20synthesize%201-dimensional%20turbulent%20fields&body=Title%3A%20A%20multiscale%20and%20multicriteria%20Generative%20Adversarial%20Network%20to%0A%20%20synthesize%201-dimensional%20turbulent%20fields%0AAuthor%3A%20Carlos%20Granero-Belinchon%20and%20Manuel%20Cabeza%20Gallucci%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20new%20Neural%20Network%20stochastic%20model%20to%20generate%20a%0A1-dimensional%20stochastic%20field%20with%20turbulent%20velocity%20statistics.%20Both%20the%0Amodel%20architecture%20and%20training%20procedure%20ground%20on%20the%20Kolmogorov%20and%20Obukhov%0Astatistical%20theories%20of%20fully%20developed%20turbulence%2C%20so%20guaranteeing%0Adescriptions%20of%201%29%20energy%20distribution%2C%202%29%20energy%20cascade%20and%203%29%20intermittency%0Aacross%20scales%20in%20agreement%20with%20experimental%20observations.%20The%20model%20is%20a%0AGenerative%20Adversarial%20Network%20with%20multiple%20multiscale%20optimization%20criteria.%0AFirst%2C%20we%20use%20three%20physics-based%20criteria%3A%20the%20variance%2C%20skewness%20and%20flatness%0Aof%20the%20increments%20of%20the%20generated%20field%20that%20retrieve%20respectively%20the%0Aturbulent%20energy%20distribution%2C%20energy%20cascade%20and%20intermittency%20across%20scales.%0ASecond%2C%20the%20Generative%20Adversarial%20Network%20criterion%2C%20based%20on%20reproducing%0Astatistical%20distributions%2C%20is%20used%20on%20segments%20of%20different%20length%20of%20the%0Agenerated%20field.%20Furthermore%2C%20to%20mimic%20multiscale%20decompositions%20frequently%0Aused%20in%20turbulence%27s%20studies%2C%20the%20model%20architecture%20is%20fully%20convolutional%0Awith%20kernel%20sizes%20varying%20along%20the%20multiple%20layers%20of%20the%20model.%20To%20train%20our%0Amodel%20we%20use%20turbulent%20velocity%20signals%20from%20grid%20turbulence%20at%20Modane%20wind%0Atunnel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520multiscale%2520and%2520multicriteria%2520Generative%2520Adversarial%2520Network%2520to%250A%2520%2520synthesize%25201-dimensional%2520turbulent%2520fields%26entry.906535625%3DCarlos%2520Granero-Belinchon%2520and%2520Manuel%2520Cabeza%2520Gallucci%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520new%2520Neural%2520Network%2520stochastic%2520model%2520to%2520generate%2520a%250A1-dimensional%2520stochastic%2520field%2520with%2520turbulent%2520velocity%2520statistics.%2520Both%2520the%250Amodel%2520architecture%2520and%2520training%2520procedure%2520ground%2520on%2520the%2520Kolmogorov%2520and%2520Obukhov%250Astatistical%2520theories%2520of%2520fully%2520developed%2520turbulence%252C%2520so%2520guaranteeing%250Adescriptions%2520of%25201%2529%2520energy%2520distribution%252C%25202%2529%2520energy%2520cascade%2520and%25203%2529%2520intermittency%250Aacross%2520scales%2520in%2520agreement%2520with%2520experimental%2520observations.%2520The%2520model%2520is%2520a%250AGenerative%2520Adversarial%2520Network%2520with%2520multiple%2520multiscale%2520optimization%2520criteria.%250AFirst%252C%2520we%2520use%2520three%2520physics-based%2520criteria%253A%2520the%2520variance%252C%2520skewness%2520and%2520flatness%250Aof%2520the%2520increments%2520of%2520the%2520generated%2520field%2520that%2520retrieve%2520respectively%2520the%250Aturbulent%2520energy%2520distribution%252C%2520energy%2520cascade%2520and%2520intermittency%2520across%2520scales.%250ASecond%252C%2520the%2520Generative%2520Adversarial%2520Network%2520criterion%252C%2520based%2520on%2520reproducing%250Astatistical%2520distributions%252C%2520is%2520used%2520on%2520segments%2520of%2520different%2520length%2520of%2520the%250Agenerated%2520field.%2520Furthermore%252C%2520to%2520mimic%2520multiscale%2520decompositions%2520frequently%250Aused%2520in%2520turbulence%2527s%2520studies%252C%2520the%2520model%2520architecture%2520is%2520fully%2520convolutional%250Awith%2520kernel%2520sizes%2520varying%2520along%2520the%2520multiple%2520layers%2520of%2520the%2520model.%2520To%2520train%2520our%250Amodel%2520we%2520use%2520turbulent%2520velocity%2520signals%2520from%2520grid%2520turbulence%2520at%2520Modane%2520wind%250Atunnel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20multiscale%20and%20multicriteria%20Generative%20Adversarial%20Network%20to%0A%20%20synthesize%201-dimensional%20turbulent%20fields&entry.906535625=Carlos%20Granero-Belinchon%20and%20Manuel%20Cabeza%20Gallucci&entry.1292438233=%20%20This%20article%20introduces%20a%20new%20Neural%20Network%20stochastic%20model%20to%20generate%20a%0A1-dimensional%20stochastic%20field%20with%20turbulent%20velocity%20statistics.%20Both%20the%0Amodel%20architecture%20and%20training%20procedure%20ground%20on%20the%20Kolmogorov%20and%20Obukhov%0Astatistical%20theories%20of%20fully%20developed%20turbulence%2C%20so%20guaranteeing%0Adescriptions%20of%201%29%20energy%20distribution%2C%202%29%20energy%20cascade%20and%203%29%20intermittency%0Aacross%20scales%20in%20agreement%20with%20experimental%20observations.%20The%20model%20is%20a%0AGenerative%20Adversarial%20Network%20with%20multiple%20multiscale%20optimization%20criteria.%0AFirst%2C%20we%20use%20three%20physics-based%20criteria%3A%20the%20variance%2C%20skewness%20and%20flatness%0Aof%20the%20increments%20of%20the%20generated%20field%20that%20retrieve%20respectively%20the%0Aturbulent%20energy%20distribution%2C%20energy%20cascade%20and%20intermittency%20across%20scales.%0ASecond%2C%20the%20Generative%20Adversarial%20Network%20criterion%2C%20based%20on%20reproducing%0Astatistical%20distributions%2C%20is%20used%20on%20segments%20of%20different%20length%20of%20the%0Agenerated%20field.%20Furthermore%2C%20to%20mimic%20multiscale%20decompositions%20frequently%0Aused%20in%20turbulence%27s%20studies%2C%20the%20model%20architecture%20is%20fully%20convolutional%0Awith%20kernel%20sizes%20varying%20along%20the%20multiple%20layers%20of%20the%20model.%20To%20train%20our%0Amodel%20we%20use%20turbulent%20velocity%20signals%20from%20grid%20turbulence%20at%20Modane%20wind%0Atunnel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16580v2&entry.124074799=Read"},
{"title": "A vector quantized masked autoencoder for audiovisual speech emotion\n  recognition", "author": "Samir Sadok and Simon Leglaive and Renaud S\u00e9guier", "abstract": "  The limited availability of labeled data is a major challenge in audiovisual\nspeech emotion recognition (SER). Self-supervised learning approaches have\nrecently been proposed to mitigate the need for labeled data in various\napplications. This paper proposes the VQ-MAE-AV model, a vector quantized\nmasked autoencoder (MAE) designed for audiovisual speech self-supervised\nrepresentation learning and applied to SER. Unlike previous approaches, the\nproposed method employs a self-supervised paradigm based on discrete audio and\nvisual speech representations learned by vector quantized variational\nautoencoders. A multimodal MAE with self- or cross-attention mechanisms is\nproposed to fuse the audio and visual speech modalities and to learn local and\nglobal representations of the audiovisual speech sequence, which are then used\nfor an SER downstream task. Experimental results show that the proposed\napproach, which is pre-trained on the VoxCeleb2 database and fine-tuned on\nstandard emotional audiovisual speech datasets, outperforms the\nstate-of-the-art audiovisual SER methods. Extensive ablation experiments are\nalso provided to assess the contribution of the different model components.\n", "link": "http://arxiv.org/abs/2305.03568v2", "date": "2024-05-15", "relevancy": 2.102, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5516}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20vector%20quantized%20masked%20autoencoder%20for%20audiovisual%20speech%20emotion%0A%20%20recognition&body=Title%3A%20A%20vector%20quantized%20masked%20autoencoder%20for%20audiovisual%20speech%20emotion%0A%20%20recognition%0AAuthor%3A%20Samir%20Sadok%20and%20Simon%20Leglaive%20and%20Renaud%20S%C3%A9guier%0AAbstract%3A%20%20%20The%20limited%20availability%20of%20labeled%20data%20is%20a%20major%20challenge%20in%20audiovisual%0Aspeech%20emotion%20recognition%20%28SER%29.%20Self-supervised%20learning%20approaches%20have%0Arecently%20been%20proposed%20to%20mitigate%20the%20need%20for%20labeled%20data%20in%20various%0Aapplications.%20This%20paper%20proposes%20the%20VQ-MAE-AV%20model%2C%20a%20vector%20quantized%0Amasked%20autoencoder%20%28MAE%29%20designed%20for%20audiovisual%20speech%20self-supervised%0Arepresentation%20learning%20and%20applied%20to%20SER.%20Unlike%20previous%20approaches%2C%20the%0Aproposed%20method%20employs%20a%20self-supervised%20paradigm%20based%20on%20discrete%20audio%20and%0Avisual%20speech%20representations%20learned%20by%20vector%20quantized%20variational%0Aautoencoders.%20A%20multimodal%20MAE%20with%20self-%20or%20cross-attention%20mechanisms%20is%0Aproposed%20to%20fuse%20the%20audio%20and%20visual%20speech%20modalities%20and%20to%20learn%20local%20and%0Aglobal%20representations%20of%20the%20audiovisual%20speech%20sequence%2C%20which%20are%20then%20used%0Afor%20an%20SER%20downstream%20task.%20Experimental%20results%20show%20that%20the%20proposed%0Aapproach%2C%20which%20is%20pre-trained%20on%20the%20VoxCeleb2%20database%20and%20fine-tuned%20on%0Astandard%20emotional%20audiovisual%20speech%20datasets%2C%20outperforms%20the%0Astate-of-the-art%20audiovisual%20SER%20methods.%20Extensive%20ablation%20experiments%20are%0Aalso%20provided%20to%20assess%20the%20contribution%20of%20the%20different%20model%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.03568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520vector%2520quantized%2520masked%2520autoencoder%2520for%2520audiovisual%2520speech%2520emotion%250A%2520%2520recognition%26entry.906535625%3DSamir%2520Sadok%2520and%2520Simon%2520Leglaive%2520and%2520Renaud%2520S%25C3%25A9guier%26entry.1292438233%3D%2520%2520The%2520limited%2520availability%2520of%2520labeled%2520data%2520is%2520a%2520major%2520challenge%2520in%2520audiovisual%250Aspeech%2520emotion%2520recognition%2520%2528SER%2529.%2520Self-supervised%2520learning%2520approaches%2520have%250Arecently%2520been%2520proposed%2520to%2520mitigate%2520the%2520need%2520for%2520labeled%2520data%2520in%2520various%250Aapplications.%2520This%2520paper%2520proposes%2520the%2520VQ-MAE-AV%2520model%252C%2520a%2520vector%2520quantized%250Amasked%2520autoencoder%2520%2528MAE%2529%2520designed%2520for%2520audiovisual%2520speech%2520self-supervised%250Arepresentation%2520learning%2520and%2520applied%2520to%2520SER.%2520Unlike%2520previous%2520approaches%252C%2520the%250Aproposed%2520method%2520employs%2520a%2520self-supervised%2520paradigm%2520based%2520on%2520discrete%2520audio%2520and%250Avisual%2520speech%2520representations%2520learned%2520by%2520vector%2520quantized%2520variational%250Aautoencoders.%2520A%2520multimodal%2520MAE%2520with%2520self-%2520or%2520cross-attention%2520mechanisms%2520is%250Aproposed%2520to%2520fuse%2520the%2520audio%2520and%2520visual%2520speech%2520modalities%2520and%2520to%2520learn%2520local%2520and%250Aglobal%2520representations%2520of%2520the%2520audiovisual%2520speech%2520sequence%252C%2520which%2520are%2520then%2520used%250Afor%2520an%2520SER%2520downstream%2520task.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Aapproach%252C%2520which%2520is%2520pre-trained%2520on%2520the%2520VoxCeleb2%2520database%2520and%2520fine-tuned%2520on%250Astandard%2520emotional%2520audiovisual%2520speech%2520datasets%252C%2520outperforms%2520the%250Astate-of-the-art%2520audiovisual%2520SER%2520methods.%2520Extensive%2520ablation%2520experiments%2520are%250Aalso%2520provided%2520to%2520assess%2520the%2520contribution%2520of%2520the%2520different%2520model%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.03568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20vector%20quantized%20masked%20autoencoder%20for%20audiovisual%20speech%20emotion%0A%20%20recognition&entry.906535625=Samir%20Sadok%20and%20Simon%20Leglaive%20and%20Renaud%20S%C3%A9guier&entry.1292438233=%20%20The%20limited%20availability%20of%20labeled%20data%20is%20a%20major%20challenge%20in%20audiovisual%0Aspeech%20emotion%20recognition%20%28SER%29.%20Self-supervised%20learning%20approaches%20have%0Arecently%20been%20proposed%20to%20mitigate%20the%20need%20for%20labeled%20data%20in%20various%0Aapplications.%20This%20paper%20proposes%20the%20VQ-MAE-AV%20model%2C%20a%20vector%20quantized%0Amasked%20autoencoder%20%28MAE%29%20designed%20for%20audiovisual%20speech%20self-supervised%0Arepresentation%20learning%20and%20applied%20to%20SER.%20Unlike%20previous%20approaches%2C%20the%0Aproposed%20method%20employs%20a%20self-supervised%20paradigm%20based%20on%20discrete%20audio%20and%0Avisual%20speech%20representations%20learned%20by%20vector%20quantized%20variational%0Aautoencoders.%20A%20multimodal%20MAE%20with%20self-%20or%20cross-attention%20mechanisms%20is%0Aproposed%20to%20fuse%20the%20audio%20and%20visual%20speech%20modalities%20and%20to%20learn%20local%20and%0Aglobal%20representations%20of%20the%20audiovisual%20speech%20sequence%2C%20which%20are%20then%20used%0Afor%20an%20SER%20downstream%20task.%20Experimental%20results%20show%20that%20the%20proposed%0Aapproach%2C%20which%20is%20pre-trained%20on%20the%20VoxCeleb2%20database%20and%20fine-tuned%20on%0Astandard%20emotional%20audiovisual%20speech%20datasets%2C%20outperforms%20the%0Astate-of-the-art%20audiovisual%20SER%20methods.%20Extensive%20ablation%20experiments%20are%0Aalso%20provided%20to%20assess%20the%20contribution%20of%20the%20different%20model%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03568v2&entry.124074799=Read"},
{"title": "Cross-view Action Recognition Understanding From Exocentric to\n  Egocentric Perspective", "author": "Thanh-Dat Truong and Khoa Luu", "abstract": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2305.15699v2", "date": "2024-05-15", "relevancy": 2.1, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective&body=Title%3A%20Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective%0AAuthor%3A%20Thanh-Dat%20Truong%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Understanding%20action%20recognition%20in%20egocentric%20videos%20has%20emerged%20as%20a%20vital%0Aresearch%20topic%20with%20numerous%20practical%20applications.%20With%20the%20limitation%20in%20the%0Ascale%20of%20egocentric%20data%20collection%2C%20learning%20robust%20deep%20learning-based%20action%0Arecognition%20models%20remains%20difficult.%20Transferring%20knowledge%20learned%20from%20the%0Alarge-scale%20exocentric%20data%20to%20the%20egocentric%20data%20is%20challenging%20due%20to%20the%0Adifference%20in%20videos%20across%20views.%20Our%20work%20introduces%20a%20novel%20cross-view%0Alearning%20approach%20to%20action%20recognition%20%28CVAR%29%20that%20effectively%20transfers%0Aknowledge%20from%20the%20exocentric%20to%20the%20selfish%20view.%20First%2C%20we%20present%20a%20novel%0Ageometric-based%20constraint%20into%20the%20self-attention%20mechanism%20in%20Transformer%0Abased%20on%20analyzing%20the%20camera%20positions%20between%20two%20views.%20Then%2C%20we%20propose%20a%0Anew%20cross-view%20self-attention%20loss%20learned%20on%20unpaired%20cross-view%20data%20to%0Aenforce%20the%20self-attention%20mechanism%20learning%20to%20transfer%20knowledge%20across%0Aviews.%20Finally%2C%20to%20further%20improve%20the%20performance%20of%20our%20cross-view%20learning%0Aapproach%2C%20we%20present%20the%20metrics%20to%20measure%20the%20correlations%20in%20videos%20and%0Aattention%20maps%20effectively.%20Experimental%20results%20on%20standard%20egocentric%20action%0Arecognition%20benchmarks%2C%20i.e.%2C%20Charades-Ego%2C%20EPIC-Kitchens-55%2C%20and%0AEPIC-Kitchens-100%2C%20have%20shown%20our%20approach%27s%20effectiveness%20and%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15699v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-view%2520Action%2520Recognition%2520Understanding%2520From%2520Exocentric%2520to%250A%2520%2520Egocentric%2520Perspective%26entry.906535625%3DThanh-Dat%2520Truong%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Understanding%2520action%2520recognition%2520in%2520egocentric%2520videos%2520has%2520emerged%2520as%2520a%2520vital%250Aresearch%2520topic%2520with%2520numerous%2520practical%2520applications.%2520With%2520the%2520limitation%2520in%2520the%250Ascale%2520of%2520egocentric%2520data%2520collection%252C%2520learning%2520robust%2520deep%2520learning-based%2520action%250Arecognition%2520models%2520remains%2520difficult.%2520Transferring%2520knowledge%2520learned%2520from%2520the%250Alarge-scale%2520exocentric%2520data%2520to%2520the%2520egocentric%2520data%2520is%2520challenging%2520due%2520to%2520the%250Adifference%2520in%2520videos%2520across%2520views.%2520Our%2520work%2520introduces%2520a%2520novel%2520cross-view%250Alearning%2520approach%2520to%2520action%2520recognition%2520%2528CVAR%2529%2520that%2520effectively%2520transfers%250Aknowledge%2520from%2520the%2520exocentric%2520to%2520the%2520selfish%2520view.%2520First%252C%2520we%2520present%2520a%2520novel%250Ageometric-based%2520constraint%2520into%2520the%2520self-attention%2520mechanism%2520in%2520Transformer%250Abased%2520on%2520analyzing%2520the%2520camera%2520positions%2520between%2520two%2520views.%2520Then%252C%2520we%2520propose%2520a%250Anew%2520cross-view%2520self-attention%2520loss%2520learned%2520on%2520unpaired%2520cross-view%2520data%2520to%250Aenforce%2520the%2520self-attention%2520mechanism%2520learning%2520to%2520transfer%2520knowledge%2520across%250Aviews.%2520Finally%252C%2520to%2520further%2520improve%2520the%2520performance%2520of%2520our%2520cross-view%2520learning%250Aapproach%252C%2520we%2520present%2520the%2520metrics%2520to%2520measure%2520the%2520correlations%2520in%2520videos%2520and%250Aattention%2520maps%2520effectively.%2520Experimental%2520results%2520on%2520standard%2520egocentric%2520action%250Arecognition%2520benchmarks%252C%2520i.e.%252C%2520Charades-Ego%252C%2520EPIC-Kitchens-55%252C%2520and%250AEPIC-Kitchens-100%252C%2520have%2520shown%2520our%2520approach%2527s%2520effectiveness%2520and%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15699v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective&entry.906535625=Thanh-Dat%20Truong%20and%20Khoa%20Luu&entry.1292438233=%20%20Understanding%20action%20recognition%20in%20egocentric%20videos%20has%20emerged%20as%20a%20vital%0Aresearch%20topic%20with%20numerous%20practical%20applications.%20With%20the%20limitation%20in%20the%0Ascale%20of%20egocentric%20data%20collection%2C%20learning%20robust%20deep%20learning-based%20action%0Arecognition%20models%20remains%20difficult.%20Transferring%20knowledge%20learned%20from%20the%0Alarge-scale%20exocentric%20data%20to%20the%20egocentric%20data%20is%20challenging%20due%20to%20the%0Adifference%20in%20videos%20across%20views.%20Our%20work%20introduces%20a%20novel%20cross-view%0Alearning%20approach%20to%20action%20recognition%20%28CVAR%29%20that%20effectively%20transfers%0Aknowledge%20from%20the%20exocentric%20to%20the%20selfish%20view.%20First%2C%20we%20present%20a%20novel%0Ageometric-based%20constraint%20into%20the%20self-attention%20mechanism%20in%20Transformer%0Abased%20on%20analyzing%20the%20camera%20positions%20between%20two%20views.%20Then%2C%20we%20propose%20a%0Anew%20cross-view%20self-attention%20loss%20learned%20on%20unpaired%20cross-view%20data%20to%0Aenforce%20the%20self-attention%20mechanism%20learning%20to%20transfer%20knowledge%20across%0Aviews.%20Finally%2C%20to%20further%20improve%20the%20performance%20of%20our%20cross-view%20learning%0Aapproach%2C%20we%20present%20the%20metrics%20to%20measure%20the%20correlations%20in%20videos%20and%0Aattention%20maps%20effectively.%20Experimental%20results%20on%20standard%20egocentric%20action%0Arecognition%20benchmarks%2C%20i.e.%2C%20Charades-Ego%2C%20EPIC-Kitchens-55%2C%20and%0AEPIC-Kitchens-100%2C%20have%20shown%20our%20approach%27s%20effectiveness%20and%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15699v2&entry.124074799=Read"},
{"title": "Perception- and Fidelity-aware Reduced-Reference Super-Resolution Image\n  Quality Assessment", "author": "Xinying Lin and Xuyang Liu and Hong Yang and Xiaohai He and Honggang Chen", "abstract": "  With the advent of image super-resolution (SR) algorithms, how to evaluate\nthe quality of generated SR images has become an urgent task. Although\nfull-reference methods perform well in SR image quality assessment (SR-IQA),\ntheir reliance on high-resolution (HR) images limits their practical\napplicability. Leveraging available reconstruction information as much as\npossible for SR-IQA, such as low-resolution (LR) images and the scale factors,\nis a promising way to enhance assessment performance for SR-IQA without HR for\nreference. In this letter, we attempt to evaluate the perceptual quality and\nreconstruction fidelity of SR images considering LR images and scale factors.\nSpecifically, we propose a novel dual-branch reduced-reference SR-IQA network,\n\\ie, Perception- and Fidelity-aware SR-IQA (PFIQA). The perception-aware branch\nevaluates the perceptual quality of SR images by leveraging the merits of\nglobal modeling of Vision Transformer (ViT) and local relation of ResNet, and\nincorporating the scale factor to enable comprehensive visual perception.\nMeanwhile, the fidelity-aware branch assesses the reconstruction fidelity\nbetween LR and SR images through their visual perception. The combination of\nthe two branches substantially aligns with the human visual system, enabling a\ncomprehensive SR image evaluation. Experimental results indicate that our PFIQA\noutperforms current state-of-the-art models across three widely-used SR-IQA\nbenchmarks. Notably, PFIQA excels in assessing the quality of real-world SR\nimages.\n", "link": "http://arxiv.org/abs/2405.09472v1", "date": "2024-05-15", "relevancy": 2.0919, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5242}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception-%20and%20Fidelity-aware%20Reduced-Reference%20Super-Resolution%20Image%0A%20%20Quality%20Assessment&body=Title%3A%20Perception-%20and%20Fidelity-aware%20Reduced-Reference%20Super-Resolution%20Image%0A%20%20Quality%20Assessment%0AAuthor%3A%20Xinying%20Lin%20and%20Xuyang%20Liu%20and%20Hong%20Yang%20and%20Xiaohai%20He%20and%20Honggang%20Chen%0AAbstract%3A%20%20%20With%20the%20advent%20of%20image%20super-resolution%20%28SR%29%20algorithms%2C%20how%20to%20evaluate%0Athe%20quality%20of%20generated%20SR%20images%20has%20become%20an%20urgent%20task.%20Although%0Afull-reference%20methods%20perform%20well%20in%20SR%20image%20quality%20assessment%20%28SR-IQA%29%2C%0Atheir%20reliance%20on%20high-resolution%20%28HR%29%20images%20limits%20their%20practical%0Aapplicability.%20Leveraging%20available%20reconstruction%20information%20as%20much%20as%0Apossible%20for%20SR-IQA%2C%20such%20as%20low-resolution%20%28LR%29%20images%20and%20the%20scale%20factors%2C%0Ais%20a%20promising%20way%20to%20enhance%20assessment%20performance%20for%20SR-IQA%20without%20HR%20for%0Areference.%20In%20this%20letter%2C%20we%20attempt%20to%20evaluate%20the%20perceptual%20quality%20and%0Areconstruction%20fidelity%20of%20SR%20images%20considering%20LR%20images%20and%20scale%20factors.%0ASpecifically%2C%20we%20propose%20a%20novel%20dual-branch%20reduced-reference%20SR-IQA%20network%2C%0A%5Cie%2C%20Perception-%20and%20Fidelity-aware%20SR-IQA%20%28PFIQA%29.%20The%20perception-aware%20branch%0Aevaluates%20the%20perceptual%20quality%20of%20SR%20images%20by%20leveraging%20the%20merits%20of%0Aglobal%20modeling%20of%20Vision%20Transformer%20%28ViT%29%20and%20local%20relation%20of%20ResNet%2C%20and%0Aincorporating%20the%20scale%20factor%20to%20enable%20comprehensive%20visual%20perception.%0AMeanwhile%2C%20the%20fidelity-aware%20branch%20assesses%20the%20reconstruction%20fidelity%0Abetween%20LR%20and%20SR%20images%20through%20their%20visual%20perception.%20The%20combination%20of%0Athe%20two%20branches%20substantially%20aligns%20with%20the%20human%20visual%20system%2C%20enabling%20a%0Acomprehensive%20SR%20image%20evaluation.%20Experimental%20results%20indicate%20that%20our%20PFIQA%0Aoutperforms%20current%20state-of-the-art%20models%20across%20three%20widely-used%20SR-IQA%0Abenchmarks.%20Notably%2C%20PFIQA%20excels%20in%20assessing%20the%20quality%20of%20real-world%20SR%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception-%2520and%2520Fidelity-aware%2520Reduced-Reference%2520Super-Resolution%2520Image%250A%2520%2520Quality%2520Assessment%26entry.906535625%3DXinying%2520Lin%2520and%2520Xuyang%2520Liu%2520and%2520Hong%2520Yang%2520and%2520Xiaohai%2520He%2520and%2520Honggang%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520image%2520super-resolution%2520%2528SR%2529%2520algorithms%252C%2520how%2520to%2520evaluate%250Athe%2520quality%2520of%2520generated%2520SR%2520images%2520has%2520become%2520an%2520urgent%2520task.%2520Although%250Afull-reference%2520methods%2520perform%2520well%2520in%2520SR%2520image%2520quality%2520assessment%2520%2528SR-IQA%2529%252C%250Atheir%2520reliance%2520on%2520high-resolution%2520%2528HR%2529%2520images%2520limits%2520their%2520practical%250Aapplicability.%2520Leveraging%2520available%2520reconstruction%2520information%2520as%2520much%2520as%250Apossible%2520for%2520SR-IQA%252C%2520such%2520as%2520low-resolution%2520%2528LR%2529%2520images%2520and%2520the%2520scale%2520factors%252C%250Ais%2520a%2520promising%2520way%2520to%2520enhance%2520assessment%2520performance%2520for%2520SR-IQA%2520without%2520HR%2520for%250Areference.%2520In%2520this%2520letter%252C%2520we%2520attempt%2520to%2520evaluate%2520the%2520perceptual%2520quality%2520and%250Areconstruction%2520fidelity%2520of%2520SR%2520images%2520considering%2520LR%2520images%2520and%2520scale%2520factors.%250ASpecifically%252C%2520we%2520propose%2520a%2520novel%2520dual-branch%2520reduced-reference%2520SR-IQA%2520network%252C%250A%255Cie%252C%2520Perception-%2520and%2520Fidelity-aware%2520SR-IQA%2520%2528PFIQA%2529.%2520The%2520perception-aware%2520branch%250Aevaluates%2520the%2520perceptual%2520quality%2520of%2520SR%2520images%2520by%2520leveraging%2520the%2520merits%2520of%250Aglobal%2520modeling%2520of%2520Vision%2520Transformer%2520%2528ViT%2529%2520and%2520local%2520relation%2520of%2520ResNet%252C%2520and%250Aincorporating%2520the%2520scale%2520factor%2520to%2520enable%2520comprehensive%2520visual%2520perception.%250AMeanwhile%252C%2520the%2520fidelity-aware%2520branch%2520assesses%2520the%2520reconstruction%2520fidelity%250Abetween%2520LR%2520and%2520SR%2520images%2520through%2520their%2520visual%2520perception.%2520The%2520combination%2520of%250Athe%2520two%2520branches%2520substantially%2520aligns%2520with%2520the%2520human%2520visual%2520system%252C%2520enabling%2520a%250Acomprehensive%2520SR%2520image%2520evaluation.%2520Experimental%2520results%2520indicate%2520that%2520our%2520PFIQA%250Aoutperforms%2520current%2520state-of-the-art%2520models%2520across%2520three%2520widely-used%2520SR-IQA%250Abenchmarks.%2520Notably%252C%2520PFIQA%2520excels%2520in%2520assessing%2520the%2520quality%2520of%2520real-world%2520SR%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception-%20and%20Fidelity-aware%20Reduced-Reference%20Super-Resolution%20Image%0A%20%20Quality%20Assessment&entry.906535625=Xinying%20Lin%20and%20Xuyang%20Liu%20and%20Hong%20Yang%20and%20Xiaohai%20He%20and%20Honggang%20Chen&entry.1292438233=%20%20With%20the%20advent%20of%20image%20super-resolution%20%28SR%29%20algorithms%2C%20how%20to%20evaluate%0Athe%20quality%20of%20generated%20SR%20images%20has%20become%20an%20urgent%20task.%20Although%0Afull-reference%20methods%20perform%20well%20in%20SR%20image%20quality%20assessment%20%28SR-IQA%29%2C%0Atheir%20reliance%20on%20high-resolution%20%28HR%29%20images%20limits%20their%20practical%0Aapplicability.%20Leveraging%20available%20reconstruction%20information%20as%20much%20as%0Apossible%20for%20SR-IQA%2C%20such%20as%20low-resolution%20%28LR%29%20images%20and%20the%20scale%20factors%2C%0Ais%20a%20promising%20way%20to%20enhance%20assessment%20performance%20for%20SR-IQA%20without%20HR%20for%0Areference.%20In%20this%20letter%2C%20we%20attempt%20to%20evaluate%20the%20perceptual%20quality%20and%0Areconstruction%20fidelity%20of%20SR%20images%20considering%20LR%20images%20and%20scale%20factors.%0ASpecifically%2C%20we%20propose%20a%20novel%20dual-branch%20reduced-reference%20SR-IQA%20network%2C%0A%5Cie%2C%20Perception-%20and%20Fidelity-aware%20SR-IQA%20%28PFIQA%29.%20The%20perception-aware%20branch%0Aevaluates%20the%20perceptual%20quality%20of%20SR%20images%20by%20leveraging%20the%20merits%20of%0Aglobal%20modeling%20of%20Vision%20Transformer%20%28ViT%29%20and%20local%20relation%20of%20ResNet%2C%20and%0Aincorporating%20the%20scale%20factor%20to%20enable%20comprehensive%20visual%20perception.%0AMeanwhile%2C%20the%20fidelity-aware%20branch%20assesses%20the%20reconstruction%20fidelity%0Abetween%20LR%20and%20SR%20images%20through%20their%20visual%20perception.%20The%20combination%20of%0Athe%20two%20branches%20substantially%20aligns%20with%20the%20human%20visual%20system%2C%20enabling%20a%0Acomprehensive%20SR%20image%20evaluation.%20Experimental%20results%20indicate%20that%20our%20PFIQA%0Aoutperforms%20current%20state-of-the-art%20models%20across%20three%20widely-used%20SR-IQA%0Abenchmarks.%20Notably%2C%20PFIQA%20excels%20in%20assessing%20the%20quality%20of%20real-world%20SR%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09472v1&entry.124074799=Read"},
{"title": "Physics-Informed Neural Network for Multirotor Slung Load Systems\n  Modeling", "author": "Gil Serrano and Marcelo Jacinto and Jose Ribeiro-Gomes and Joao Pinto and Bruno J. Guerreiro and Alexandre Bernardino and Rita Cunha", "abstract": "  Recent advances in aerial robotics have enabled the use of multirotor\nvehicles for autonomous payload transportation. Resorting only to classical\nmethods to reliably model a quadrotor carrying a cable-slung load poses\nsignificant challenges. On the other hand, purely data-driven learning methods\ndo not comply by design with the problem's physical constraints, especially in\nstates that are not densely represented in training data. In this work, we\nexplore the use of physics informed neural networks to learn an end-to-end\nmodel of the multirotor-slung-load system and, at a given time, estimate a\nsequence of the future system states. An LSTM encoder decoder with an attention\nmechanism is used to capture the dynamics of the system. To guarantee the\ncohesiveness between the multiple predicted states of the system, we propose\nthe use of a physics-based term in the loss function, which includes a\ndiscretized physical model derived from first principles together with slack\nvariables that allow for a small mismatch between expected and predicted\nvalues. To train the model, a dataset using a real-world quadrotor carrying a\nslung load was curated and is made available. Prediction results are presented\nand corroborate the feasibility of the approach. The proposed method\noutperforms both the first principles physical model and a comparable neural\nnetwork model trained without the physics regularization proposed.\n", "link": "http://arxiv.org/abs/2405.09428v1", "date": "2024-05-15", "relevancy": 2.0751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6315}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Neural%20Network%20for%20Multirotor%20Slung%20Load%20Systems%0A%20%20Modeling&body=Title%3A%20Physics-Informed%20Neural%20Network%20for%20Multirotor%20Slung%20Load%20Systems%0A%20%20Modeling%0AAuthor%3A%20Gil%20Serrano%20and%20Marcelo%20Jacinto%20and%20Jose%20Ribeiro-Gomes%20and%20Joao%20Pinto%20and%20Bruno%20J.%20Guerreiro%20and%20Alexandre%20Bernardino%20and%20Rita%20Cunha%0AAbstract%3A%20%20%20Recent%20advances%20in%20aerial%20robotics%20have%20enabled%20the%20use%20of%20multirotor%0Avehicles%20for%20autonomous%20payload%20transportation.%20Resorting%20only%20to%20classical%0Amethods%20to%20reliably%20model%20a%20quadrotor%20carrying%20a%20cable-slung%20load%20poses%0Asignificant%20challenges.%20On%20the%20other%20hand%2C%20purely%20data-driven%20learning%20methods%0Ado%20not%20comply%20by%20design%20with%20the%20problem%27s%20physical%20constraints%2C%20especially%20in%0Astates%20that%20are%20not%20densely%20represented%20in%20training%20data.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20physics%20informed%20neural%20networks%20to%20learn%20an%20end-to-end%0Amodel%20of%20the%20multirotor-slung-load%20system%20and%2C%20at%20a%20given%20time%2C%20estimate%20a%0Asequence%20of%20the%20future%20system%20states.%20An%20LSTM%20encoder%20decoder%20with%20an%20attention%0Amechanism%20is%20used%20to%20capture%20the%20dynamics%20of%20the%20system.%20To%20guarantee%20the%0Acohesiveness%20between%20the%20multiple%20predicted%20states%20of%20the%20system%2C%20we%20propose%0Athe%20use%20of%20a%20physics-based%20term%20in%20the%20loss%20function%2C%20which%20includes%20a%0Adiscretized%20physical%20model%20derived%20from%20first%20principles%20together%20with%20slack%0Avariables%20that%20allow%20for%20a%20small%20mismatch%20between%20expected%20and%20predicted%0Avalues.%20To%20train%20the%20model%2C%20a%20dataset%20using%20a%20real-world%20quadrotor%20carrying%20a%0Aslung%20load%20was%20curated%20and%20is%20made%20available.%20Prediction%20results%20are%20presented%0Aand%20corroborate%20the%20feasibility%20of%20the%20approach.%20The%20proposed%20method%0Aoutperforms%20both%20the%20first%20principles%20physical%20model%20and%20a%20comparable%20neural%0Anetwork%20model%20trained%20without%20the%20physics%20regularization%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Neural%2520Network%2520for%2520Multirotor%2520Slung%2520Load%2520Systems%250A%2520%2520Modeling%26entry.906535625%3DGil%2520Serrano%2520and%2520Marcelo%2520Jacinto%2520and%2520Jose%2520Ribeiro-Gomes%2520and%2520Joao%2520Pinto%2520and%2520Bruno%2520J.%2520Guerreiro%2520and%2520Alexandre%2520Bernardino%2520and%2520Rita%2520Cunha%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520aerial%2520robotics%2520have%2520enabled%2520the%2520use%2520of%2520multirotor%250Avehicles%2520for%2520autonomous%2520payload%2520transportation.%2520Resorting%2520only%2520to%2520classical%250Amethods%2520to%2520reliably%2520model%2520a%2520quadrotor%2520carrying%2520a%2520cable-slung%2520load%2520poses%250Asignificant%2520challenges.%2520On%2520the%2520other%2520hand%252C%2520purely%2520data-driven%2520learning%2520methods%250Ado%2520not%2520comply%2520by%2520design%2520with%2520the%2520problem%2527s%2520physical%2520constraints%252C%2520especially%2520in%250Astates%2520that%2520are%2520not%2520densely%2520represented%2520in%2520training%2520data.%2520In%2520this%2520work%252C%2520we%250Aexplore%2520the%2520use%2520of%2520physics%2520informed%2520neural%2520networks%2520to%2520learn%2520an%2520end-to-end%250Amodel%2520of%2520the%2520multirotor-slung-load%2520system%2520and%252C%2520at%2520a%2520given%2520time%252C%2520estimate%2520a%250Asequence%2520of%2520the%2520future%2520system%2520states.%2520An%2520LSTM%2520encoder%2520decoder%2520with%2520an%2520attention%250Amechanism%2520is%2520used%2520to%2520capture%2520the%2520dynamics%2520of%2520the%2520system.%2520To%2520guarantee%2520the%250Acohesiveness%2520between%2520the%2520multiple%2520predicted%2520states%2520of%2520the%2520system%252C%2520we%2520propose%250Athe%2520use%2520of%2520a%2520physics-based%2520term%2520in%2520the%2520loss%2520function%252C%2520which%2520includes%2520a%250Adiscretized%2520physical%2520model%2520derived%2520from%2520first%2520principles%2520together%2520with%2520slack%250Avariables%2520that%2520allow%2520for%2520a%2520small%2520mismatch%2520between%2520expected%2520and%2520predicted%250Avalues.%2520To%2520train%2520the%2520model%252C%2520a%2520dataset%2520using%2520a%2520real-world%2520quadrotor%2520carrying%2520a%250Aslung%2520load%2520was%2520curated%2520and%2520is%2520made%2520available.%2520Prediction%2520results%2520are%2520presented%250Aand%2520corroborate%2520the%2520feasibility%2520of%2520the%2520approach.%2520The%2520proposed%2520method%250Aoutperforms%2520both%2520the%2520first%2520principles%2520physical%2520model%2520and%2520a%2520comparable%2520neural%250Anetwork%2520model%2520trained%2520without%2520the%2520physics%2520regularization%2520proposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Neural%20Network%20for%20Multirotor%20Slung%20Load%20Systems%0A%20%20Modeling&entry.906535625=Gil%20Serrano%20and%20Marcelo%20Jacinto%20and%20Jose%20Ribeiro-Gomes%20and%20Joao%20Pinto%20and%20Bruno%20J.%20Guerreiro%20and%20Alexandre%20Bernardino%20and%20Rita%20Cunha&entry.1292438233=%20%20Recent%20advances%20in%20aerial%20robotics%20have%20enabled%20the%20use%20of%20multirotor%0Avehicles%20for%20autonomous%20payload%20transportation.%20Resorting%20only%20to%20classical%0Amethods%20to%20reliably%20model%20a%20quadrotor%20carrying%20a%20cable-slung%20load%20poses%0Asignificant%20challenges.%20On%20the%20other%20hand%2C%20purely%20data-driven%20learning%20methods%0Ado%20not%20comply%20by%20design%20with%20the%20problem%27s%20physical%20constraints%2C%20especially%20in%0Astates%20that%20are%20not%20densely%20represented%20in%20training%20data.%20In%20this%20work%2C%20we%0Aexplore%20the%20use%20of%20physics%20informed%20neural%20networks%20to%20learn%20an%20end-to-end%0Amodel%20of%20the%20multirotor-slung-load%20system%20and%2C%20at%20a%20given%20time%2C%20estimate%20a%0Asequence%20of%20the%20future%20system%20states.%20An%20LSTM%20encoder%20decoder%20with%20an%20attention%0Amechanism%20is%20used%20to%20capture%20the%20dynamics%20of%20the%20system.%20To%20guarantee%20the%0Acohesiveness%20between%20the%20multiple%20predicted%20states%20of%20the%20system%2C%20we%20propose%0Athe%20use%20of%20a%20physics-based%20term%20in%20the%20loss%20function%2C%20which%20includes%20a%0Adiscretized%20physical%20model%20derived%20from%20first%20principles%20together%20with%20slack%0Avariables%20that%20allow%20for%20a%20small%20mismatch%20between%20expected%20and%20predicted%0Avalues.%20To%20train%20the%20model%2C%20a%20dataset%20using%20a%20real-world%20quadrotor%20carrying%20a%0Aslung%20load%20was%20curated%20and%20is%20made%20available.%20Prediction%20results%20are%20presented%0Aand%20corroborate%20the%20feasibility%20of%20the%20approach.%20The%20proposed%20method%0Aoutperforms%20both%20the%20first%20principles%20physical%20model%20and%20a%20comparable%20neural%0Anetwork%20model%20trained%20without%20the%20physics%20regularization%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09428v1&entry.124074799=Read"},
{"title": "Time-Equivariant Contrastive Learning for Degenerative Disease\n  Progression in Retinal OCT", "author": "Taha Emre and Arunava Chakravarty and Dmitrii Lachinov and Antoine Rivail and Ursula Schmidt-Erfurth and Hrvoje Bogunovi\u0107", "abstract": "  Contrastive pretraining provides robust representations by ensuring their\ninvariance to different image transformations while simultaneously preventing\nrepresentational collapse. Equivariant contrastive learning, on the other hand,\nprovides representations sensitive to specific image transformations while\nremaining invariant to others. By introducing equivariance to time-induced\ntransformations, such as disease-related anatomical changes in longitudinal\nimaging, the model can effectively capture such changes in the representation\nspace. In this work, we pro-pose a Time-equivariant Contrastive Learning (TC)\nmethod. First, an encoder embeds two unlabeled scans from different time points\nof the same patient into the representation space. Next, a temporal\nequivariance module is trained to predict the representation of a later visit\nbased on the representation from one of the previous visits and the\ncorresponding time interval with a novel regularization loss term while\npreserving the invariance property to irrelevant image transformations. On a\nlarge longitudinal dataset, our model clearly outperforms existing equivariant\ncontrastive methods in predicting progression from intermediate age-related\nmacular degeneration (AMD) to advanced wet-AMD within a specified time-window.\n", "link": "http://arxiv.org/abs/2405.09404v1", "date": "2024-05-15", "relevancy": 2.0709, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.529}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Equivariant%20Contrastive%20Learning%20for%20Degenerative%20Disease%0A%20%20Progression%20in%20Retinal%20OCT&body=Title%3A%20Time-Equivariant%20Contrastive%20Learning%20for%20Degenerative%20Disease%0A%20%20Progression%20in%20Retinal%20OCT%0AAuthor%3A%20Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Dmitrii%20Lachinov%20and%20Antoine%20Rivail%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20Contrastive%20pretraining%20provides%20robust%20representations%20by%20ensuring%20their%0Ainvariance%20to%20different%20image%20transformations%20while%20simultaneously%20preventing%0Arepresentational%20collapse.%20Equivariant%20contrastive%20learning%2C%20on%20the%20other%20hand%2C%0Aprovides%20representations%20sensitive%20to%20specific%20image%20transformations%20while%0Aremaining%20invariant%20to%20others.%20By%20introducing%20equivariance%20to%20time-induced%0Atransformations%2C%20such%20as%20disease-related%20anatomical%20changes%20in%20longitudinal%0Aimaging%2C%20the%20model%20can%20effectively%20capture%20such%20changes%20in%20the%20representation%0Aspace.%20In%20this%20work%2C%20we%20pro-pose%20a%20Time-equivariant%20Contrastive%20Learning%20%28TC%29%0Amethod.%20First%2C%20an%20encoder%20embeds%20two%20unlabeled%20scans%20from%20different%20time%20points%0Aof%20the%20same%20patient%20into%20the%20representation%20space.%20Next%2C%20a%20temporal%0Aequivariance%20module%20is%20trained%20to%20predict%20the%20representation%20of%20a%20later%20visit%0Abased%20on%20the%20representation%20from%20one%20of%20the%20previous%20visits%20and%20the%0Acorresponding%20time%20interval%20with%20a%20novel%20regularization%20loss%20term%20while%0Apreserving%20the%20invariance%20property%20to%20irrelevant%20image%20transformations.%20On%20a%0Alarge%20longitudinal%20dataset%2C%20our%20model%20clearly%20outperforms%20existing%20equivariant%0Acontrastive%20methods%20in%20predicting%20progression%20from%20intermediate%20age-related%0Amacular%20degeneration%20%28AMD%29%20to%20advanced%20wet-AMD%20within%20a%20specified%20time-window.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Equivariant%2520Contrastive%2520Learning%2520for%2520Degenerative%2520Disease%250A%2520%2520Progression%2520in%2520Retinal%2520OCT%26entry.906535625%3DTaha%2520Emre%2520and%2520Arunava%2520Chakravarty%2520and%2520Dmitrii%2520Lachinov%2520and%2520Antoine%2520Rivail%2520and%2520Ursula%2520Schmidt-Erfurth%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520Contrastive%2520pretraining%2520provides%2520robust%2520representations%2520by%2520ensuring%2520their%250Ainvariance%2520to%2520different%2520image%2520transformations%2520while%2520simultaneously%2520preventing%250Arepresentational%2520collapse.%2520Equivariant%2520contrastive%2520learning%252C%2520on%2520the%2520other%2520hand%252C%250Aprovides%2520representations%2520sensitive%2520to%2520specific%2520image%2520transformations%2520while%250Aremaining%2520invariant%2520to%2520others.%2520By%2520introducing%2520equivariance%2520to%2520time-induced%250Atransformations%252C%2520such%2520as%2520disease-related%2520anatomical%2520changes%2520in%2520longitudinal%250Aimaging%252C%2520the%2520model%2520can%2520effectively%2520capture%2520such%2520changes%2520in%2520the%2520representation%250Aspace.%2520In%2520this%2520work%252C%2520we%2520pro-pose%2520a%2520Time-equivariant%2520Contrastive%2520Learning%2520%2528TC%2529%250Amethod.%2520First%252C%2520an%2520encoder%2520embeds%2520two%2520unlabeled%2520scans%2520from%2520different%2520time%2520points%250Aof%2520the%2520same%2520patient%2520into%2520the%2520representation%2520space.%2520Next%252C%2520a%2520temporal%250Aequivariance%2520module%2520is%2520trained%2520to%2520predict%2520the%2520representation%2520of%2520a%2520later%2520visit%250Abased%2520on%2520the%2520representation%2520from%2520one%2520of%2520the%2520previous%2520visits%2520and%2520the%250Acorresponding%2520time%2520interval%2520with%2520a%2520novel%2520regularization%2520loss%2520term%2520while%250Apreserving%2520the%2520invariance%2520property%2520to%2520irrelevant%2520image%2520transformations.%2520On%2520a%250Alarge%2520longitudinal%2520dataset%252C%2520our%2520model%2520clearly%2520outperforms%2520existing%2520equivariant%250Acontrastive%2520methods%2520in%2520predicting%2520progression%2520from%2520intermediate%2520age-related%250Amacular%2520degeneration%2520%2528AMD%2529%2520to%2520advanced%2520wet-AMD%2520within%2520a%2520specified%2520time-window.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Equivariant%20Contrastive%20Learning%20for%20Degenerative%20Disease%0A%20%20Progression%20in%20Retinal%20OCT&entry.906535625=Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Dmitrii%20Lachinov%20and%20Antoine%20Rivail%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20Contrastive%20pretraining%20provides%20robust%20representations%20by%20ensuring%20their%0Ainvariance%20to%20different%20image%20transformations%20while%20simultaneously%20preventing%0Arepresentational%20collapse.%20Equivariant%20contrastive%20learning%2C%20on%20the%20other%20hand%2C%0Aprovides%20representations%20sensitive%20to%20specific%20image%20transformations%20while%0Aremaining%20invariant%20to%20others.%20By%20introducing%20equivariance%20to%20time-induced%0Atransformations%2C%20such%20as%20disease-related%20anatomical%20changes%20in%20longitudinal%0Aimaging%2C%20the%20model%20can%20effectively%20capture%20such%20changes%20in%20the%20representation%0Aspace.%20In%20this%20work%2C%20we%20pro-pose%20a%20Time-equivariant%20Contrastive%20Learning%20%28TC%29%0Amethod.%20First%2C%20an%20encoder%20embeds%20two%20unlabeled%20scans%20from%20different%20time%20points%0Aof%20the%20same%20patient%20into%20the%20representation%20space.%20Next%2C%20a%20temporal%0Aequivariance%20module%20is%20trained%20to%20predict%20the%20representation%20of%20a%20later%20visit%0Abased%20on%20the%20representation%20from%20one%20of%20the%20previous%20visits%20and%20the%0Acorresponding%20time%20interval%20with%20a%20novel%20regularization%20loss%20term%20while%0Apreserving%20the%20invariance%20property%20to%20irrelevant%20image%20transformations.%20On%20a%0Alarge%20longitudinal%20dataset%2C%20our%20model%20clearly%20outperforms%20existing%20equivariant%0Acontrastive%20methods%20in%20predicting%20progression%20from%20intermediate%20age-related%0Amacular%20degeneration%20%28AMD%29%20to%20advanced%20wet-AMD%20within%20a%20specified%20time-window.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09404v1&entry.124074799=Read"},
{"title": "Global-Local Image Perceptual Score (GLIPS): Evaluating Photorealistic\n  Quality of AI-Generated Images", "author": "Memoona Aziz and Umair Rehman and Muhammad Umair Danish and Katarina Grolinger", "abstract": "  This paper introduces the Global-Local Image Perceptual Score (GLIPS), an\nimage metric designed to assess the photorealistic image quality of\nAI-generated images with a high degree of alignment to human visual perception.\nTraditional metrics such as FID and KID scores do not align closely with human\nevaluations. The proposed metric incorporates advanced transformer-based\nattention mechanisms to assess local similarity and Maximum Mean Discrepancy\n(MMD) to evaluate global distributional similarity. To evaluate the performance\nof GLIPS, we conducted a human study on photorealistic image quality.\nComprehensive tests across various generative models demonstrate that GLIPS\nconsistently outperforms existing metrics like FID, SSIM, and MS-SSIM in terms\nof correlation with human scores. Additionally, we introduce the Interpolative\nBinning Scale (IBS), a refined scaling method that enhances the\ninterpretability of metric scores by aligning them more closely with human\nevaluative standards. The proposed metric and scaling approach not only\nprovides more reliable assessments of AI-generated images but also suggest\npathways for future enhancements in image generation technologies.\n", "link": "http://arxiv.org/abs/2405.09426v1", "date": "2024-05-15", "relevancy": 2.0693, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5122}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images&body=Title%3A%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images%0AAuthor%3A%20Memoona%20Aziz%20and%20Umair%20Rehman%20and%20Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%2C%20an%0Aimage%20metric%20designed%20to%20assess%20the%20photorealistic%20image%20quality%20of%0AAI-generated%20images%20with%20a%20high%20degree%20of%20alignment%20to%20human%20visual%20perception.%0ATraditional%20metrics%20such%20as%20FID%20and%20KID%20scores%20do%20not%20align%20closely%20with%20human%0Aevaluations.%20The%20proposed%20metric%20incorporates%20advanced%20transformer-based%0Aattention%20mechanisms%20to%20assess%20local%20similarity%20and%20Maximum%20Mean%20Discrepancy%0A%28MMD%29%20to%20evaluate%20global%20distributional%20similarity.%20To%20evaluate%20the%20performance%0Aof%20GLIPS%2C%20we%20conducted%20a%20human%20study%20on%20photorealistic%20image%20quality.%0AComprehensive%20tests%20across%20various%20generative%20models%20demonstrate%20that%20GLIPS%0Aconsistently%20outperforms%20existing%20metrics%20like%20FID%2C%20SSIM%2C%20and%20MS-SSIM%20in%20terms%0Aof%20correlation%20with%20human%20scores.%20Additionally%2C%20we%20introduce%20the%20Interpolative%0ABinning%20Scale%20%28IBS%29%2C%20a%20refined%20scaling%20method%20that%20enhances%20the%0Ainterpretability%20of%20metric%20scores%20by%20aligning%20them%20more%20closely%20with%20human%0Aevaluative%20standards.%20The%20proposed%20metric%20and%20scaling%20approach%20not%20only%0Aprovides%20more%20reliable%20assessments%20of%20AI-generated%20images%20but%20also%20suggest%0Apathways%20for%20future%20enhancements%20in%20image%20generation%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-Local%2520Image%2520Perceptual%2520Score%2520%2528GLIPS%2529%253A%2520Evaluating%2520Photorealistic%250A%2520%2520Quality%2520of%2520AI-Generated%2520Images%26entry.906535625%3DMemoona%2520Aziz%2520and%2520Umair%2520Rehman%2520and%2520Muhammad%2520Umair%2520Danish%2520and%2520Katarina%2520Grolinger%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Global-Local%2520Image%2520Perceptual%2520Score%2520%2528GLIPS%2529%252C%2520an%250Aimage%2520metric%2520designed%2520to%2520assess%2520the%2520photorealistic%2520image%2520quality%2520of%250AAI-generated%2520images%2520with%2520a%2520high%2520degree%2520of%2520alignment%2520to%2520human%2520visual%2520perception.%250ATraditional%2520metrics%2520such%2520as%2520FID%2520and%2520KID%2520scores%2520do%2520not%2520align%2520closely%2520with%2520human%250Aevaluations.%2520The%2520proposed%2520metric%2520incorporates%2520advanced%2520transformer-based%250Aattention%2520mechanisms%2520to%2520assess%2520local%2520similarity%2520and%2520Maximum%2520Mean%2520Discrepancy%250A%2528MMD%2529%2520to%2520evaluate%2520global%2520distributional%2520similarity.%2520To%2520evaluate%2520the%2520performance%250Aof%2520GLIPS%252C%2520we%2520conducted%2520a%2520human%2520study%2520on%2520photorealistic%2520image%2520quality.%250AComprehensive%2520tests%2520across%2520various%2520generative%2520models%2520demonstrate%2520that%2520GLIPS%250Aconsistently%2520outperforms%2520existing%2520metrics%2520like%2520FID%252C%2520SSIM%252C%2520and%2520MS-SSIM%2520in%2520terms%250Aof%2520correlation%2520with%2520human%2520scores.%2520Additionally%252C%2520we%2520introduce%2520the%2520Interpolative%250ABinning%2520Scale%2520%2528IBS%2529%252C%2520a%2520refined%2520scaling%2520method%2520that%2520enhances%2520the%250Ainterpretability%2520of%2520metric%2520scores%2520by%2520aligning%2520them%2520more%2520closely%2520with%2520human%250Aevaluative%2520standards.%2520The%2520proposed%2520metric%2520and%2520scaling%2520approach%2520not%2520only%250Aprovides%2520more%2520reliable%2520assessments%2520of%2520AI-generated%2520images%2520but%2520also%2520suggest%250Apathways%2520for%2520future%2520enhancements%2520in%2520image%2520generation%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%3A%20Evaluating%20Photorealistic%0A%20%20Quality%20of%20AI-Generated%20Images&entry.906535625=Memoona%20Aziz%20and%20Umair%20Rehman%20and%20Muhammad%20Umair%20Danish%20and%20Katarina%20Grolinger&entry.1292438233=%20%20This%20paper%20introduces%20the%20Global-Local%20Image%20Perceptual%20Score%20%28GLIPS%29%2C%20an%0Aimage%20metric%20designed%20to%20assess%20the%20photorealistic%20image%20quality%20of%0AAI-generated%20images%20with%20a%20high%20degree%20of%20alignment%20to%20human%20visual%20perception.%0ATraditional%20metrics%20such%20as%20FID%20and%20KID%20scores%20do%20not%20align%20closely%20with%20human%0Aevaluations.%20The%20proposed%20metric%20incorporates%20advanced%20transformer-based%0Aattention%20mechanisms%20to%20assess%20local%20similarity%20and%20Maximum%20Mean%20Discrepancy%0A%28MMD%29%20to%20evaluate%20global%20distributional%20similarity.%20To%20evaluate%20the%20performance%0Aof%20GLIPS%2C%20we%20conducted%20a%20human%20study%20on%20photorealistic%20image%20quality.%0AComprehensive%20tests%20across%20various%20generative%20models%20demonstrate%20that%20GLIPS%0Aconsistently%20outperforms%20existing%20metrics%20like%20FID%2C%20SSIM%2C%20and%20MS-SSIM%20in%20terms%0Aof%20correlation%20with%20human%20scores.%20Additionally%2C%20we%20introduce%20the%20Interpolative%0ABinning%20Scale%20%28IBS%29%2C%20a%20refined%20scaling%20method%20that%20enhances%20the%0Ainterpretability%20of%20metric%20scores%20by%20aligning%20them%20more%20closely%20with%20human%0Aevaluative%20standards.%20The%20proposed%20metric%20and%20scaling%20approach%20not%20only%0Aprovides%20more%20reliable%20assessments%20of%20AI-generated%20images%20but%20also%20suggest%0Apathways%20for%20future%20enhancements%20in%20image%20generation%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09426v1&entry.124074799=Read"},
{"title": "Unbiased Learning to Rank Meets Reality: Lessons from Baidu's\n  Large-Scale Search Dataset", "author": "Philipp Hager and Romain Deffayet and Jean-Michel Renders and Onno Zoeter and Maarten de Rijke", "abstract": "  Unbiased learning-to-rank (ULTR) is a well-established framework for learning\nfrom user clicks, which are often biased by the ranker collecting the data.\nWhile theoretically justified and extensively tested in simulation, ULTR\ntechniques lack empirical validation, especially on modern search engines. The\nBaidu-ULTR dataset released for the WSDM Cup 2023, collected from Baidu's\nsearch engine, offers a rare opportunity to assess the real-world performance\nof prominent ULTR techniques. Despite multiple submissions during the WSDM Cup\n2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the\nobserved improvements stem from applying ULTR or other learning techniques.\n  In this work, we revisit and extend the available experiments on the\nBaidu-ULTR dataset. We find that standard unbiased learning-to-rank techniques\nrobustly improve click predictions but struggle to consistently improve ranking\nperformance, especially considering the stark differences obtained by choice of\nranking loss and query-document features. Our experiments reveal that gains in\nclick prediction do not necessarily translate to enhanced ranking performance\non expert relevance annotations, implying that conclusions strongly depend on\nhow success is measured in this benchmark.\n", "link": "http://arxiv.org/abs/2404.02543v3", "date": "2024-05-15", "relevancy": 2.0425, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset&body=Title%3A%20Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset%0AAuthor%3A%20Philipp%20Hager%20and%20Romain%20Deffayet%20and%20Jean-Michel%20Renders%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20Unbiased%20learning-to-rank%20%28ULTR%29%20is%20a%20well-established%20framework%20for%20learning%0Afrom%20user%20clicks%2C%20which%20are%20often%20biased%20by%20the%20ranker%20collecting%20the%20data.%0AWhile%20theoretically%20justified%20and%20extensively%20tested%20in%20simulation%2C%20ULTR%0Atechniques%20lack%20empirical%20validation%2C%20especially%20on%20modern%20search%20engines.%20The%0ABaidu-ULTR%20dataset%20released%20for%20the%20WSDM%20Cup%202023%2C%20collected%20from%20Baidu%27s%0Asearch%20engine%2C%20offers%20a%20rare%20opportunity%20to%20assess%20the%20real-world%20performance%0Aof%20prominent%20ULTR%20techniques.%20Despite%20multiple%20submissions%20during%20the%20WSDM%20Cup%0A2023%20and%20the%20subsequent%20NTCIR%20ULTRE-2%20task%2C%20it%20remains%20unclear%20whether%20the%0Aobserved%20improvements%20stem%20from%20applying%20ULTR%20or%20other%20learning%20techniques.%0A%20%20In%20this%20work%2C%20we%20revisit%20and%20extend%20the%20available%20experiments%20on%20the%0ABaidu-ULTR%20dataset.%20We%20find%20that%20standard%20unbiased%20learning-to-rank%20techniques%0Arobustly%20improve%20click%20predictions%20but%20struggle%20to%20consistently%20improve%20ranking%0Aperformance%2C%20especially%20considering%20the%20stark%20differences%20obtained%20by%20choice%20of%0Aranking%20loss%20and%20query-document%20features.%20Our%20experiments%20reveal%20that%20gains%20in%0Aclick%20prediction%20do%20not%20necessarily%20translate%20to%20enhanced%20ranking%20performance%0Aon%20expert%20relevance%20annotations%2C%20implying%20that%20conclusions%20strongly%20depend%20on%0Ahow%20success%20is%20measured%20in%20this%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02543v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnbiased%2520Learning%2520to%2520Rank%2520Meets%2520Reality%253A%2520Lessons%2520from%2520Baidu%2527s%250A%2520%2520Large-Scale%2520Search%2520Dataset%26entry.906535625%3DPhilipp%2520Hager%2520and%2520Romain%2520Deffayet%2520and%2520Jean-Michel%2520Renders%2520and%2520Onno%2520Zoeter%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520Unbiased%2520learning-to-rank%2520%2528ULTR%2529%2520is%2520a%2520well-established%2520framework%2520for%2520learning%250Afrom%2520user%2520clicks%252C%2520which%2520are%2520often%2520biased%2520by%2520the%2520ranker%2520collecting%2520the%2520data.%250AWhile%2520theoretically%2520justified%2520and%2520extensively%2520tested%2520in%2520simulation%252C%2520ULTR%250Atechniques%2520lack%2520empirical%2520validation%252C%2520especially%2520on%2520modern%2520search%2520engines.%2520The%250ABaidu-ULTR%2520dataset%2520released%2520for%2520the%2520WSDM%2520Cup%25202023%252C%2520collected%2520from%2520Baidu%2527s%250Asearch%2520engine%252C%2520offers%2520a%2520rare%2520opportunity%2520to%2520assess%2520the%2520real-world%2520performance%250Aof%2520prominent%2520ULTR%2520techniques.%2520Despite%2520multiple%2520submissions%2520during%2520the%2520WSDM%2520Cup%250A2023%2520and%2520the%2520subsequent%2520NTCIR%2520ULTRE-2%2520task%252C%2520it%2520remains%2520unclear%2520whether%2520the%250Aobserved%2520improvements%2520stem%2520from%2520applying%2520ULTR%2520or%2520other%2520learning%2520techniques.%250A%2520%2520In%2520this%2520work%252C%2520we%2520revisit%2520and%2520extend%2520the%2520available%2520experiments%2520on%2520the%250ABaidu-ULTR%2520dataset.%2520We%2520find%2520that%2520standard%2520unbiased%2520learning-to-rank%2520techniques%250Arobustly%2520improve%2520click%2520predictions%2520but%2520struggle%2520to%2520consistently%2520improve%2520ranking%250Aperformance%252C%2520especially%2520considering%2520the%2520stark%2520differences%2520obtained%2520by%2520choice%2520of%250Aranking%2520loss%2520and%2520query-document%2520features.%2520Our%2520experiments%2520reveal%2520that%2520gains%2520in%250Aclick%2520prediction%2520do%2520not%2520necessarily%2520translate%2520to%2520enhanced%2520ranking%2520performance%250Aon%2520expert%2520relevance%2520annotations%252C%2520implying%2520that%2520conclusions%2520strongly%2520depend%2520on%250Ahow%2520success%2520is%2520measured%2520in%2520this%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02543v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbiased%20Learning%20to%20Rank%20Meets%20Reality%3A%20Lessons%20from%20Baidu%27s%0A%20%20Large-Scale%20Search%20Dataset&entry.906535625=Philipp%20Hager%20and%20Romain%20Deffayet%20and%20Jean-Michel%20Renders%20and%20Onno%20Zoeter%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20Unbiased%20learning-to-rank%20%28ULTR%29%20is%20a%20well-established%20framework%20for%20learning%0Afrom%20user%20clicks%2C%20which%20are%20often%20biased%20by%20the%20ranker%20collecting%20the%20data.%0AWhile%20theoretically%20justified%20and%20extensively%20tested%20in%20simulation%2C%20ULTR%0Atechniques%20lack%20empirical%20validation%2C%20especially%20on%20modern%20search%20engines.%20The%0ABaidu-ULTR%20dataset%20released%20for%20the%20WSDM%20Cup%202023%2C%20collected%20from%20Baidu%27s%0Asearch%20engine%2C%20offers%20a%20rare%20opportunity%20to%20assess%20the%20real-world%20performance%0Aof%20prominent%20ULTR%20techniques.%20Despite%20multiple%20submissions%20during%20the%20WSDM%20Cup%0A2023%20and%20the%20subsequent%20NTCIR%20ULTRE-2%20task%2C%20it%20remains%20unclear%20whether%20the%0Aobserved%20improvements%20stem%20from%20applying%20ULTR%20or%20other%20learning%20techniques.%0A%20%20In%20this%20work%2C%20we%20revisit%20and%20extend%20the%20available%20experiments%20on%20the%0ABaidu-ULTR%20dataset.%20We%20find%20that%20standard%20unbiased%20learning-to-rank%20techniques%0Arobustly%20improve%20click%20predictions%20but%20struggle%20to%20consistently%20improve%20ranking%0Aperformance%2C%20especially%20considering%20the%20stark%20differences%20obtained%20by%20choice%20of%0Aranking%20loss%20and%20query-document%20features.%20Our%20experiments%20reveal%20that%20gains%20in%0Aclick%20prediction%20do%20not%20necessarily%20translate%20to%20enhanced%20ranking%20performance%0Aon%20expert%20relevance%20annotations%2C%20implying%20that%20conclusions%20strongly%20depend%20on%0Ahow%20success%20is%20measured%20in%20this%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02543v3&entry.124074799=Read"},
{"title": "Online Self-Supervised Deep Learning for Intrusion Detection Systems", "author": "Mert Nak\u0131p and Erol Gelenbe", "abstract": "  This paper proposes a novel Self-Supervised Intrusion Detection (SSID)\nframework, which enables a fully online Deep Learning (DL) based Intrusion\nDetection System (IDS) that requires no human intervention or prior off-line\nlearning. The proposed framework analyzes and labels incoming traffic packets\nbased only on the decisions of the IDS itself using an Auto-Associative Deep\nRandom Neural Network, and on an online estimate of its statistically measured\ntrustworthiness. The SSID framework enables IDS to adapt rapidly to\ntime-varying characteristics of the network traffic, and eliminates the need\nfor offline data collection. This approach avoids human errors in data\nlabeling, and human labor and computational costs of model training and data\ncollection. The approach is experimentally evaluated on public datasets and\ncompared with well-known {machine learning and deep learning} models, showing\nthat this SSID framework is very useful and advantageous as an accurate and\nonline learning DL-based IDS for IoT systems.\n", "link": "http://arxiv.org/abs/2306.13030v2", "date": "2024-05-15", "relevancy": 2.042, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5184}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5063}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Self-Supervised%20Deep%20Learning%20for%20Intrusion%20Detection%20Systems&body=Title%3A%20Online%20Self-Supervised%20Deep%20Learning%20for%20Intrusion%20Detection%20Systems%0AAuthor%3A%20Mert%20Nak%C4%B1p%20and%20Erol%20Gelenbe%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20Self-Supervised%20Intrusion%20Detection%20%28SSID%29%0Aframework%2C%20which%20enables%20a%20fully%20online%20Deep%20Learning%20%28DL%29%20based%20Intrusion%0ADetection%20System%20%28IDS%29%20that%20requires%20no%20human%20intervention%20or%20prior%20off-line%0Alearning.%20The%20proposed%20framework%20analyzes%20and%20labels%20incoming%20traffic%20packets%0Abased%20only%20on%20the%20decisions%20of%20the%20IDS%20itself%20using%20an%20Auto-Associative%20Deep%0ARandom%20Neural%20Network%2C%20and%20on%20an%20online%20estimate%20of%20its%20statistically%20measured%0Atrustworthiness.%20The%20SSID%20framework%20enables%20IDS%20to%20adapt%20rapidly%20to%0Atime-varying%20characteristics%20of%20the%20network%20traffic%2C%20and%20eliminates%20the%20need%0Afor%20offline%20data%20collection.%20This%20approach%20avoids%20human%20errors%20in%20data%0Alabeling%2C%20and%20human%20labor%20and%20computational%20costs%20of%20model%20training%20and%20data%0Acollection.%20The%20approach%20is%20experimentally%20evaluated%20on%20public%20datasets%20and%0Acompared%20with%20well-known%20%7Bmachine%20learning%20and%20deep%20learning%7D%20models%2C%20showing%0Athat%20this%20SSID%20framework%20is%20very%20useful%20and%20advantageous%20as%20an%20accurate%20and%0Aonline%20learning%20DL-based%20IDS%20for%20IoT%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13030v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Self-Supervised%2520Deep%2520Learning%2520for%2520Intrusion%2520Detection%2520Systems%26entry.906535625%3DMert%2520Nak%25C4%25B1p%2520and%2520Erol%2520Gelenbe%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520Self-Supervised%2520Intrusion%2520Detection%2520%2528SSID%2529%250Aframework%252C%2520which%2520enables%2520a%2520fully%2520online%2520Deep%2520Learning%2520%2528DL%2529%2520based%2520Intrusion%250ADetection%2520System%2520%2528IDS%2529%2520that%2520requires%2520no%2520human%2520intervention%2520or%2520prior%2520off-line%250Alearning.%2520The%2520proposed%2520framework%2520analyzes%2520and%2520labels%2520incoming%2520traffic%2520packets%250Abased%2520only%2520on%2520the%2520decisions%2520of%2520the%2520IDS%2520itself%2520using%2520an%2520Auto-Associative%2520Deep%250ARandom%2520Neural%2520Network%252C%2520and%2520on%2520an%2520online%2520estimate%2520of%2520its%2520statistically%2520measured%250Atrustworthiness.%2520The%2520SSID%2520framework%2520enables%2520IDS%2520to%2520adapt%2520rapidly%2520to%250Atime-varying%2520characteristics%2520of%2520the%2520network%2520traffic%252C%2520and%2520eliminates%2520the%2520need%250Afor%2520offline%2520data%2520collection.%2520This%2520approach%2520avoids%2520human%2520errors%2520in%2520data%250Alabeling%252C%2520and%2520human%2520labor%2520and%2520computational%2520costs%2520of%2520model%2520training%2520and%2520data%250Acollection.%2520The%2520approach%2520is%2520experimentally%2520evaluated%2520on%2520public%2520datasets%2520and%250Acompared%2520with%2520well-known%2520%257Bmachine%2520learning%2520and%2520deep%2520learning%257D%2520models%252C%2520showing%250Athat%2520this%2520SSID%2520framework%2520is%2520very%2520useful%2520and%2520advantageous%2520as%2520an%2520accurate%2520and%250Aonline%2520learning%2520DL-based%2520IDS%2520for%2520IoT%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13030v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Self-Supervised%20Deep%20Learning%20for%20Intrusion%20Detection%20Systems&entry.906535625=Mert%20Nak%C4%B1p%20and%20Erol%20Gelenbe&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20Self-Supervised%20Intrusion%20Detection%20%28SSID%29%0Aframework%2C%20which%20enables%20a%20fully%20online%20Deep%20Learning%20%28DL%29%20based%20Intrusion%0ADetection%20System%20%28IDS%29%20that%20requires%20no%20human%20intervention%20or%20prior%20off-line%0Alearning.%20The%20proposed%20framework%20analyzes%20and%20labels%20incoming%20traffic%20packets%0Abased%20only%20on%20the%20decisions%20of%20the%20IDS%20itself%20using%20an%20Auto-Associative%20Deep%0ARandom%20Neural%20Network%2C%20and%20on%20an%20online%20estimate%20of%20its%20statistically%20measured%0Atrustworthiness.%20The%20SSID%20framework%20enables%20IDS%20to%20adapt%20rapidly%20to%0Atime-varying%20characteristics%20of%20the%20network%20traffic%2C%20and%20eliminates%20the%20need%0Afor%20offline%20data%20collection.%20This%20approach%20avoids%20human%20errors%20in%20data%0Alabeling%2C%20and%20human%20labor%20and%20computational%20costs%20of%20model%20training%20and%20data%0Acollection.%20The%20approach%20is%20experimentally%20evaluated%20on%20public%20datasets%20and%0Acompared%20with%20well-known%20%7Bmachine%20learning%20and%20deep%20learning%7D%20models%2C%20showing%0Athat%20this%20SSID%20framework%20is%20very%20useful%20and%20advantageous%20as%20an%20accurate%20and%0Aonline%20learning%20DL-based%20IDS%20for%20IoT%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13030v2&entry.124074799=Read"},
{"title": "Content-Based Image Retrieval for Multi-Class Volumetric Radiology\n  Images: A Benchmark Study", "author": "Farnaz Khun Jush and Steffen Vogler and Tuan Truong and Matthias Lenga", "abstract": "  While content-based image retrieval (CBIR) has been extensively studied in\nnatural image retrieval, its application to medical images presents ongoing\nchallenges, primarily due to the 3D nature of medical images. Recent studies\nhave shown the potential use of pre-trained vision embeddings for CBIR in the\ncontext of radiology image retrieval. However, a benchmark for the retrieval of\n3D volumetric medical images is still lacking, hindering the ability to\nobjectively evaluate and compare the efficiency of proposed CBIR approaches in\nmedical imaging. In this study, we extend previous work and establish a\nbenchmark for region-based and multi-organ retrieval using the TotalSegmentator\ndataset (TS) with detailed multi-organ annotations. We benchmark embeddings\nderived from pre-trained supervised models on medical images against embeddings\nderived from pre-trained unsupervised models on non-medical images for 29\ncoarse and 104 detailed anatomical structures in volume and region levels. We\nadopt a late interaction re-ranking method inspired by text matching for image\nretrieval, comparing it against the original method proposed for volume and\nregion retrieval achieving retrieval recall of 1.0 for diverse anatomical\nregions with a wide size range. The findings and methodologies presented in\nthis paper provide essential insights and benchmarks for the development and\nevaluation of CBIR approaches in the context of medical imaging.\n", "link": "http://arxiv.org/abs/2405.09334v1", "date": "2024-05-15", "relevancy": 2.0056, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5246}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-Based%20Image%20Retrieval%20for%20Multi-Class%20Volumetric%20Radiology%0A%20%20Images%3A%20A%20Benchmark%20Study&body=Title%3A%20Content-Based%20Image%20Retrieval%20for%20Multi-Class%20Volumetric%20Radiology%0A%20%20Images%3A%20A%20Benchmark%20Study%0AAuthor%3A%20Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Tuan%20Truong%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20While%20content-based%20image%20retrieval%20%28CBIR%29%20has%20been%20extensively%20studied%20in%0Anatural%20image%20retrieval%2C%20its%20application%20to%20medical%20images%20presents%20ongoing%0Achallenges%2C%20primarily%20due%20to%20the%203D%20nature%20of%20medical%20images.%20Recent%20studies%0Ahave%20shown%20the%20potential%20use%20of%20pre-trained%20vision%20embeddings%20for%20CBIR%20in%20the%0Acontext%20of%20radiology%20image%20retrieval.%20However%2C%20a%20benchmark%20for%20the%20retrieval%20of%0A3D%20volumetric%20medical%20images%20is%20still%20lacking%2C%20hindering%20the%20ability%20to%0Aobjectively%20evaluate%20and%20compare%20the%20efficiency%20of%20proposed%20CBIR%20approaches%20in%0Amedical%20imaging.%20In%20this%20study%2C%20we%20extend%20previous%20work%20and%20establish%20a%0Abenchmark%20for%20region-based%20and%20multi-organ%20retrieval%20using%20the%20TotalSegmentator%0Adataset%20%28TS%29%20with%20detailed%20multi-organ%20annotations.%20We%20benchmark%20embeddings%0Aderived%20from%20pre-trained%20supervised%20models%20on%20medical%20images%20against%20embeddings%0Aderived%20from%20pre-trained%20unsupervised%20models%20on%20non-medical%20images%20for%2029%0Acoarse%20and%20104%20detailed%20anatomical%20structures%20in%20volume%20and%20region%20levels.%20We%0Aadopt%20a%20late%20interaction%20re-ranking%20method%20inspired%20by%20text%20matching%20for%20image%0Aretrieval%2C%20comparing%20it%20against%20the%20original%20method%20proposed%20for%20volume%20and%0Aregion%20retrieval%20achieving%20retrieval%20recall%20of%201.0%20for%20diverse%20anatomical%0Aregions%20with%20a%20wide%20size%20range.%20The%20findings%20and%20methodologies%20presented%20in%0Athis%20paper%20provide%20essential%20insights%20and%20benchmarks%20for%20the%20development%20and%0Aevaluation%20of%20CBIR%20approaches%20in%20the%20context%20of%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-Based%2520Image%2520Retrieval%2520for%2520Multi-Class%2520Volumetric%2520Radiology%250A%2520%2520Images%253A%2520A%2520Benchmark%2520Study%26entry.906535625%3DFarnaz%2520Khun%2520Jush%2520and%2520Steffen%2520Vogler%2520and%2520Tuan%2520Truong%2520and%2520Matthias%2520Lenga%26entry.1292438233%3D%2520%2520While%2520content-based%2520image%2520retrieval%2520%2528CBIR%2529%2520has%2520been%2520extensively%2520studied%2520in%250Anatural%2520image%2520retrieval%252C%2520its%2520application%2520to%2520medical%2520images%2520presents%2520ongoing%250Achallenges%252C%2520primarily%2520due%2520to%2520the%25203D%2520nature%2520of%2520medical%2520images.%2520Recent%2520studies%250Ahave%2520shown%2520the%2520potential%2520use%2520of%2520pre-trained%2520vision%2520embeddings%2520for%2520CBIR%2520in%2520the%250Acontext%2520of%2520radiology%2520image%2520retrieval.%2520However%252C%2520a%2520benchmark%2520for%2520the%2520retrieval%2520of%250A3D%2520volumetric%2520medical%2520images%2520is%2520still%2520lacking%252C%2520hindering%2520the%2520ability%2520to%250Aobjectively%2520evaluate%2520and%2520compare%2520the%2520efficiency%2520of%2520proposed%2520CBIR%2520approaches%2520in%250Amedical%2520imaging.%2520In%2520this%2520study%252C%2520we%2520extend%2520previous%2520work%2520and%2520establish%2520a%250Abenchmark%2520for%2520region-based%2520and%2520multi-organ%2520retrieval%2520using%2520the%2520TotalSegmentator%250Adataset%2520%2528TS%2529%2520with%2520detailed%2520multi-organ%2520annotations.%2520We%2520benchmark%2520embeddings%250Aderived%2520from%2520pre-trained%2520supervised%2520models%2520on%2520medical%2520images%2520against%2520embeddings%250Aderived%2520from%2520pre-trained%2520unsupervised%2520models%2520on%2520non-medical%2520images%2520for%252029%250Acoarse%2520and%2520104%2520detailed%2520anatomical%2520structures%2520in%2520volume%2520and%2520region%2520levels.%2520We%250Aadopt%2520a%2520late%2520interaction%2520re-ranking%2520method%2520inspired%2520by%2520text%2520matching%2520for%2520image%250Aretrieval%252C%2520comparing%2520it%2520against%2520the%2520original%2520method%2520proposed%2520for%2520volume%2520and%250Aregion%2520retrieval%2520achieving%2520retrieval%2520recall%2520of%25201.0%2520for%2520diverse%2520anatomical%250Aregions%2520with%2520a%2520wide%2520size%2520range.%2520The%2520findings%2520and%2520methodologies%2520presented%2520in%250Athis%2520paper%2520provide%2520essential%2520insights%2520and%2520benchmarks%2520for%2520the%2520development%2520and%250Aevaluation%2520of%2520CBIR%2520approaches%2520in%2520the%2520context%2520of%2520medical%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-Based%20Image%20Retrieval%20for%20Multi-Class%20Volumetric%20Radiology%0A%20%20Images%3A%20A%20Benchmark%20Study&entry.906535625=Farnaz%20Khun%20Jush%20and%20Steffen%20Vogler%20and%20Tuan%20Truong%20and%20Matthias%20Lenga&entry.1292438233=%20%20While%20content-based%20image%20retrieval%20%28CBIR%29%20has%20been%20extensively%20studied%20in%0Anatural%20image%20retrieval%2C%20its%20application%20to%20medical%20images%20presents%20ongoing%0Achallenges%2C%20primarily%20due%20to%20the%203D%20nature%20of%20medical%20images.%20Recent%20studies%0Ahave%20shown%20the%20potential%20use%20of%20pre-trained%20vision%20embeddings%20for%20CBIR%20in%20the%0Acontext%20of%20radiology%20image%20retrieval.%20However%2C%20a%20benchmark%20for%20the%20retrieval%20of%0A3D%20volumetric%20medical%20images%20is%20still%20lacking%2C%20hindering%20the%20ability%20to%0Aobjectively%20evaluate%20and%20compare%20the%20efficiency%20of%20proposed%20CBIR%20approaches%20in%0Amedical%20imaging.%20In%20this%20study%2C%20we%20extend%20previous%20work%20and%20establish%20a%0Abenchmark%20for%20region-based%20and%20multi-organ%20retrieval%20using%20the%20TotalSegmentator%0Adataset%20%28TS%29%20with%20detailed%20multi-organ%20annotations.%20We%20benchmark%20embeddings%0Aderived%20from%20pre-trained%20supervised%20models%20on%20medical%20images%20against%20embeddings%0Aderived%20from%20pre-trained%20unsupervised%20models%20on%20non-medical%20images%20for%2029%0Acoarse%20and%20104%20detailed%20anatomical%20structures%20in%20volume%20and%20region%20levels.%20We%0Aadopt%20a%20late%20interaction%20re-ranking%20method%20inspired%20by%20text%20matching%20for%20image%0Aretrieval%2C%20comparing%20it%20against%20the%20original%20method%20proposed%20for%20volume%20and%0Aregion%20retrieval%20achieving%20retrieval%20recall%20of%201.0%20for%20diverse%20anatomical%0Aregions%20with%20a%20wide%20size%20range.%20The%20findings%20and%20methodologies%20presented%20in%0Athis%20paper%20provide%20essential%20insights%20and%20benchmarks%20for%20the%20development%20and%0Aevaluation%20of%20CBIR%20approaches%20in%20the%20context%20of%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09334v1&entry.124074799=Read"},
{"title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in\n  Language Models", "author": "Siwei Wang and Yifei Shen and Shi Feng and Haoran Sun and Shang-Hua Teng and Wei Chen", "abstract": "  In this paper, we present the findings of our Project ALPINE which stands for\n``Autoregressive Learning for Planning In NEtworks.\" Project ALPINE initiates a\ntheoretical investigation into the development of planning capabilities in\nTransformer-based language models through their autoregressive learning\nmechanisms, aiming to identify any potential limitations in their planning\nabilities. We abstract planning as a network path-finding task where the\nobjective is to generate a valid path from a specified source node to a\ndesignated target node. In terms of expressiveness, we show that the\nTransformer is capable of executing path-finding by embedding the adjacency and\nreachability matrices within its weights. Our theoretical analysis of the\ngradient-based learning dynamic of the Transformer reveals that the Transformer\nis capable of learning both the adjacency matrix and a limited form of the\nreachability matrix. These theoretical insights are then validated through\nexperiments, which demonstrate that the Transformer indeed learns the adjacency\nmatrix and an incomplete reachability matrix, which aligns with the predictions\nmade in our theoretical analysis. Additionally, when applying our methodology\nto a real-world planning benchmark, called Blocksworld, our observations remain\nconsistent. Our theoretical and empirical analyses further unveil a potential\nlimitation of Transformer in path-finding: it cannot identify reachability\nrelationships through transitivity, and thus would fail when path concatenation\nis needed to generate a path. In summary, our findings shed new light on how\nthe internal mechanisms of autoregressive learning enable planning in networks.\nThis study may contribute to our understanding of the general planning\ncapabilities in other related domains.\n", "link": "http://arxiv.org/abs/2405.09220v1", "date": "2024-05-15", "relevancy": 1.999, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5007}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALPINE%3A%20Unveiling%20the%20Planning%20Capability%20of%20Autoregressive%20Learning%20in%0A%20%20Language%20Models&body=Title%3A%20ALPINE%3A%20Unveiling%20the%20Planning%20Capability%20of%20Autoregressive%20Learning%20in%0A%20%20Language%20Models%0AAuthor%3A%20Siwei%20Wang%20and%20Yifei%20Shen%20and%20Shi%20Feng%20and%20Haoran%20Sun%20and%20Shang-Hua%20Teng%20and%20Wei%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20findings%20of%20our%20Project%20ALPINE%20which%20stands%20for%0A%60%60Autoregressive%20Learning%20for%20Planning%20In%20NEtworks.%22%20Project%20ALPINE%20initiates%20a%0Atheoretical%20investigation%20into%20the%20development%20of%20planning%20capabilities%20in%0ATransformer-based%20language%20models%20through%20their%20autoregressive%20learning%0Amechanisms%2C%20aiming%20to%20identify%20any%20potential%20limitations%20in%20their%20planning%0Aabilities.%20We%20abstract%20planning%20as%20a%20network%20path-finding%20task%20where%20the%0Aobjective%20is%20to%20generate%20a%20valid%20path%20from%20a%20specified%20source%20node%20to%20a%0Adesignated%20target%20node.%20In%20terms%20of%20expressiveness%2C%20we%20show%20that%20the%0ATransformer%20is%20capable%20of%20executing%20path-finding%20by%20embedding%20the%20adjacency%20and%0Areachability%20matrices%20within%20its%20weights.%20Our%20theoretical%20analysis%20of%20the%0Agradient-based%20learning%20dynamic%20of%20the%20Transformer%20reveals%20that%20the%20Transformer%0Ais%20capable%20of%20learning%20both%20the%20adjacency%20matrix%20and%20a%20limited%20form%20of%20the%0Areachability%20matrix.%20These%20theoretical%20insights%20are%20then%20validated%20through%0Aexperiments%2C%20which%20demonstrate%20that%20the%20Transformer%20indeed%20learns%20the%20adjacency%0Amatrix%20and%20an%20incomplete%20reachability%20matrix%2C%20which%20aligns%20with%20the%20predictions%0Amade%20in%20our%20theoretical%20analysis.%20Additionally%2C%20when%20applying%20our%20methodology%0Ato%20a%20real-world%20planning%20benchmark%2C%20called%20Blocksworld%2C%20our%20observations%20remain%0Aconsistent.%20Our%20theoretical%20and%20empirical%20analyses%20further%20unveil%20a%20potential%0Alimitation%20of%20Transformer%20in%20path-finding%3A%20it%20cannot%20identify%20reachability%0Arelationships%20through%20transitivity%2C%20and%20thus%20would%20fail%20when%20path%20concatenation%0Ais%20needed%20to%20generate%20a%20path.%20In%20summary%2C%20our%20findings%20shed%20new%20light%20on%20how%0Athe%20internal%20mechanisms%20of%20autoregressive%20learning%20enable%20planning%20in%20networks.%0AThis%20study%20may%20contribute%20to%20our%20understanding%20of%20the%20general%20planning%0Acapabilities%20in%20other%20related%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALPINE%253A%2520Unveiling%2520the%2520Planning%2520Capability%2520of%2520Autoregressive%2520Learning%2520in%250A%2520%2520Language%2520Models%26entry.906535625%3DSiwei%2520Wang%2520and%2520Yifei%2520Shen%2520and%2520Shi%2520Feng%2520and%2520Haoran%2520Sun%2520and%2520Shang-Hua%2520Teng%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520findings%2520of%2520our%2520Project%2520ALPINE%2520which%2520stands%2520for%250A%2560%2560Autoregressive%2520Learning%2520for%2520Planning%2520In%2520NEtworks.%2522%2520Project%2520ALPINE%2520initiates%2520a%250Atheoretical%2520investigation%2520into%2520the%2520development%2520of%2520planning%2520capabilities%2520in%250ATransformer-based%2520language%2520models%2520through%2520their%2520autoregressive%2520learning%250Amechanisms%252C%2520aiming%2520to%2520identify%2520any%2520potential%2520limitations%2520in%2520their%2520planning%250Aabilities.%2520We%2520abstract%2520planning%2520as%2520a%2520network%2520path-finding%2520task%2520where%2520the%250Aobjective%2520is%2520to%2520generate%2520a%2520valid%2520path%2520from%2520a%2520specified%2520source%2520node%2520to%2520a%250Adesignated%2520target%2520node.%2520In%2520terms%2520of%2520expressiveness%252C%2520we%2520show%2520that%2520the%250ATransformer%2520is%2520capable%2520of%2520executing%2520path-finding%2520by%2520embedding%2520the%2520adjacency%2520and%250Areachability%2520matrices%2520within%2520its%2520weights.%2520Our%2520theoretical%2520analysis%2520of%2520the%250Agradient-based%2520learning%2520dynamic%2520of%2520the%2520Transformer%2520reveals%2520that%2520the%2520Transformer%250Ais%2520capable%2520of%2520learning%2520both%2520the%2520adjacency%2520matrix%2520and%2520a%2520limited%2520form%2520of%2520the%250Areachability%2520matrix.%2520These%2520theoretical%2520insights%2520are%2520then%2520validated%2520through%250Aexperiments%252C%2520which%2520demonstrate%2520that%2520the%2520Transformer%2520indeed%2520learns%2520the%2520adjacency%250Amatrix%2520and%2520an%2520incomplete%2520reachability%2520matrix%252C%2520which%2520aligns%2520with%2520the%2520predictions%250Amade%2520in%2520our%2520theoretical%2520analysis.%2520Additionally%252C%2520when%2520applying%2520our%2520methodology%250Ato%2520a%2520real-world%2520planning%2520benchmark%252C%2520called%2520Blocksworld%252C%2520our%2520observations%2520remain%250Aconsistent.%2520Our%2520theoretical%2520and%2520empirical%2520analyses%2520further%2520unveil%2520a%2520potential%250Alimitation%2520of%2520Transformer%2520in%2520path-finding%253A%2520it%2520cannot%2520identify%2520reachability%250Arelationships%2520through%2520transitivity%252C%2520and%2520thus%2520would%2520fail%2520when%2520path%2520concatenation%250Ais%2520needed%2520to%2520generate%2520a%2520path.%2520In%2520summary%252C%2520our%2520findings%2520shed%2520new%2520light%2520on%2520how%250Athe%2520internal%2520mechanisms%2520of%2520autoregressive%2520learning%2520enable%2520planning%2520in%2520networks.%250AThis%2520study%2520may%2520contribute%2520to%2520our%2520understanding%2520of%2520the%2520general%2520planning%250Acapabilities%2520in%2520other%2520related%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALPINE%3A%20Unveiling%20the%20Planning%20Capability%20of%20Autoregressive%20Learning%20in%0A%20%20Language%20Models&entry.906535625=Siwei%20Wang%20and%20Yifei%20Shen%20and%20Shi%20Feng%20and%20Haoran%20Sun%20and%20Shang-Hua%20Teng%20and%20Wei%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20findings%20of%20our%20Project%20ALPINE%20which%20stands%20for%0A%60%60Autoregressive%20Learning%20for%20Planning%20In%20NEtworks.%22%20Project%20ALPINE%20initiates%20a%0Atheoretical%20investigation%20into%20the%20development%20of%20planning%20capabilities%20in%0ATransformer-based%20language%20models%20through%20their%20autoregressive%20learning%0Amechanisms%2C%20aiming%20to%20identify%20any%20potential%20limitations%20in%20their%20planning%0Aabilities.%20We%20abstract%20planning%20as%20a%20network%20path-finding%20task%20where%20the%0Aobjective%20is%20to%20generate%20a%20valid%20path%20from%20a%20specified%20source%20node%20to%20a%0Adesignated%20target%20node.%20In%20terms%20of%20expressiveness%2C%20we%20show%20that%20the%0ATransformer%20is%20capable%20of%20executing%20path-finding%20by%20embedding%20the%20adjacency%20and%0Areachability%20matrices%20within%20its%20weights.%20Our%20theoretical%20analysis%20of%20the%0Agradient-based%20learning%20dynamic%20of%20the%20Transformer%20reveals%20that%20the%20Transformer%0Ais%20capable%20of%20learning%20both%20the%20adjacency%20matrix%20and%20a%20limited%20form%20of%20the%0Areachability%20matrix.%20These%20theoretical%20insights%20are%20then%20validated%20through%0Aexperiments%2C%20which%20demonstrate%20that%20the%20Transformer%20indeed%20learns%20the%20adjacency%0Amatrix%20and%20an%20incomplete%20reachability%20matrix%2C%20which%20aligns%20with%20the%20predictions%0Amade%20in%20our%20theoretical%20analysis.%20Additionally%2C%20when%20applying%20our%20methodology%0Ato%20a%20real-world%20planning%20benchmark%2C%20called%20Blocksworld%2C%20our%20observations%20remain%0Aconsistent.%20Our%20theoretical%20and%20empirical%20analyses%20further%20unveil%20a%20potential%0Alimitation%20of%20Transformer%20in%20path-finding%3A%20it%20cannot%20identify%20reachability%0Arelationships%20through%20transitivity%2C%20and%20thus%20would%20fail%20when%20path%20concatenation%0Ais%20needed%20to%20generate%20a%20path.%20In%20summary%2C%20our%20findings%20shed%20new%20light%20on%20how%0Athe%20internal%20mechanisms%20of%20autoregressive%20learning%20enable%20planning%20in%20networks.%0AThis%20study%20may%20contribute%20to%20our%20understanding%20of%20the%20general%20planning%0Acapabilities%20in%20other%20related%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09220v1&entry.124074799=Read"},
{"title": "Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing\n  Knowledge Graph Task", "author": "Shurong Wang and Yufei Zhang and Xuliang Huang and Hongwei Wang", "abstract": "  Knowledge graph embedding (KGE) has caught significant interest for its\neffectiveness in knowledge graph completion (KGC), specifically link prediction\n(LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly\ngrowing literature, insufficient attention has been paid to the cooperation\nbetween humans and AI on KG. However, humans' capability to analyze graphs\nconceptually may further improve the efficacy of KGE models with semantic\ninformation. To this effect, we carefully designed a human-AI team (HAIT)\nsystem dubbed KG-HAIT, which harnesses the human insights on KG by leveraging\nfully human-designed ad-hoc dynamic programming (DP) on KG to produce human\ninsightful feature (HIF) vectors that capture the subgraph structural feature\nand semantic similarities. By integrating HIF vectors into the training of KGE\nmodels, notable improvements are observed across various benchmarks and\nmetrics, accompanied by accelerated model convergence. Our results underscore\nthe effectiveness of human-designed DP in the task of LP, emphasizing the\npivotal role of collaboration between humans and AI on KG. We open avenues for\nfurther exploration and innovation through KG-HAIT, paving the way towards more\neffective and insightful KG analysis techniques.\n", "link": "http://arxiv.org/abs/2405.09477v1", "date": "2024-05-15", "relevancy": 1.959, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5121}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4778}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20Human%20Insights%20and%20AI%20Precision%3A%20Hand%20in%20Hand%20for%20Advancing%0A%20%20Knowledge%20Graph%20Task&body=Title%3A%20Harmonizing%20Human%20Insights%20and%20AI%20Precision%3A%20Hand%20in%20Hand%20for%20Advancing%0A%20%20Knowledge%20Graph%20Task%0AAuthor%3A%20Shurong%20Wang%20and%20Yufei%20Zhang%20and%20Xuliang%20Huang%20and%20Hongwei%20Wang%0AAbstract%3A%20%20%20Knowledge%20graph%20embedding%20%28KGE%29%20has%20caught%20significant%20interest%20for%20its%0Aeffectiveness%20in%20knowledge%20graph%20completion%20%28KGC%29%2C%20specifically%20link%20prediction%0A%28LP%29%2C%20with%20recent%20KGE%20models%20cracking%20the%20LP%20benchmarks.%20Despite%20the%20rapidly%0Agrowing%20literature%2C%20insufficient%20attention%20has%20been%20paid%20to%20the%20cooperation%0Abetween%20humans%20and%20AI%20on%20KG.%20However%2C%20humans%27%20capability%20to%20analyze%20graphs%0Aconceptually%20may%20further%20improve%20the%20efficacy%20of%20KGE%20models%20with%20semantic%0Ainformation.%20To%20this%20effect%2C%20we%20carefully%20designed%20a%20human-AI%20team%20%28HAIT%29%0Asystem%20dubbed%20KG-HAIT%2C%20which%20harnesses%20the%20human%20insights%20on%20KG%20by%20leveraging%0Afully%20human-designed%20ad-hoc%20dynamic%20programming%20%28DP%29%20on%20KG%20to%20produce%20human%0Ainsightful%20feature%20%28HIF%29%20vectors%20that%20capture%20the%20subgraph%20structural%20feature%0Aand%20semantic%20similarities.%20By%20integrating%20HIF%20vectors%20into%20the%20training%20of%20KGE%0Amodels%2C%20notable%20improvements%20are%20observed%20across%20various%20benchmarks%20and%0Ametrics%2C%20accompanied%20by%20accelerated%20model%20convergence.%20Our%20results%20underscore%0Athe%20effectiveness%20of%20human-designed%20DP%20in%20the%20task%20of%20LP%2C%20emphasizing%20the%0Apivotal%20role%20of%20collaboration%20between%20humans%20and%20AI%20on%20KG.%20We%20open%20avenues%20for%0Afurther%20exploration%20and%20innovation%20through%20KG-HAIT%2C%20paving%20the%20way%20towards%20more%0Aeffective%20and%20insightful%20KG%20analysis%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520Human%2520Insights%2520and%2520AI%2520Precision%253A%2520Hand%2520in%2520Hand%2520for%2520Advancing%250A%2520%2520Knowledge%2520Graph%2520Task%26entry.906535625%3DShurong%2520Wang%2520and%2520Yufei%2520Zhang%2520and%2520Xuliang%2520Huang%2520and%2520Hongwei%2520Wang%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520embedding%2520%2528KGE%2529%2520has%2520caught%2520significant%2520interest%2520for%2520its%250Aeffectiveness%2520in%2520knowledge%2520graph%2520completion%2520%2528KGC%2529%252C%2520specifically%2520link%2520prediction%250A%2528LP%2529%252C%2520with%2520recent%2520KGE%2520models%2520cracking%2520the%2520LP%2520benchmarks.%2520Despite%2520the%2520rapidly%250Agrowing%2520literature%252C%2520insufficient%2520attention%2520has%2520been%2520paid%2520to%2520the%2520cooperation%250Abetween%2520humans%2520and%2520AI%2520on%2520KG.%2520However%252C%2520humans%2527%2520capability%2520to%2520analyze%2520graphs%250Aconceptually%2520may%2520further%2520improve%2520the%2520efficacy%2520of%2520KGE%2520models%2520with%2520semantic%250Ainformation.%2520To%2520this%2520effect%252C%2520we%2520carefully%2520designed%2520a%2520human-AI%2520team%2520%2528HAIT%2529%250Asystem%2520dubbed%2520KG-HAIT%252C%2520which%2520harnesses%2520the%2520human%2520insights%2520on%2520KG%2520by%2520leveraging%250Afully%2520human-designed%2520ad-hoc%2520dynamic%2520programming%2520%2528DP%2529%2520on%2520KG%2520to%2520produce%2520human%250Ainsightful%2520feature%2520%2528HIF%2529%2520vectors%2520that%2520capture%2520the%2520subgraph%2520structural%2520feature%250Aand%2520semantic%2520similarities.%2520By%2520integrating%2520HIF%2520vectors%2520into%2520the%2520training%2520of%2520KGE%250Amodels%252C%2520notable%2520improvements%2520are%2520observed%2520across%2520various%2520benchmarks%2520and%250Ametrics%252C%2520accompanied%2520by%2520accelerated%2520model%2520convergence.%2520Our%2520results%2520underscore%250Athe%2520effectiveness%2520of%2520human-designed%2520DP%2520in%2520the%2520task%2520of%2520LP%252C%2520emphasizing%2520the%250Apivotal%2520role%2520of%2520collaboration%2520between%2520humans%2520and%2520AI%2520on%2520KG.%2520We%2520open%2520avenues%2520for%250Afurther%2520exploration%2520and%2520innovation%2520through%2520KG-HAIT%252C%2520paving%2520the%2520way%2520towards%2520more%250Aeffective%2520and%2520insightful%2520KG%2520analysis%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20Human%20Insights%20and%20AI%20Precision%3A%20Hand%20in%20Hand%20for%20Advancing%0A%20%20Knowledge%20Graph%20Task&entry.906535625=Shurong%20Wang%20and%20Yufei%20Zhang%20and%20Xuliang%20Huang%20and%20Hongwei%20Wang&entry.1292438233=%20%20Knowledge%20graph%20embedding%20%28KGE%29%20has%20caught%20significant%20interest%20for%20its%0Aeffectiveness%20in%20knowledge%20graph%20completion%20%28KGC%29%2C%20specifically%20link%20prediction%0A%28LP%29%2C%20with%20recent%20KGE%20models%20cracking%20the%20LP%20benchmarks.%20Despite%20the%20rapidly%0Agrowing%20literature%2C%20insufficient%20attention%20has%20been%20paid%20to%20the%20cooperation%0Abetween%20humans%20and%20AI%20on%20KG.%20However%2C%20humans%27%20capability%20to%20analyze%20graphs%0Aconceptually%20may%20further%20improve%20the%20efficacy%20of%20KGE%20models%20with%20semantic%0Ainformation.%20To%20this%20effect%2C%20we%20carefully%20designed%20a%20human-AI%20team%20%28HAIT%29%0Asystem%20dubbed%20KG-HAIT%2C%20which%20harnesses%20the%20human%20insights%20on%20KG%20by%20leveraging%0Afully%20human-designed%20ad-hoc%20dynamic%20programming%20%28DP%29%20on%20KG%20to%20produce%20human%0Ainsightful%20feature%20%28HIF%29%20vectors%20that%20capture%20the%20subgraph%20structural%20feature%0Aand%20semantic%20similarities.%20By%20integrating%20HIF%20vectors%20into%20the%20training%20of%20KGE%0Amodels%2C%20notable%20improvements%20are%20observed%20across%20various%20benchmarks%20and%0Ametrics%2C%20accompanied%20by%20accelerated%20model%20convergence.%20Our%20results%20underscore%0Athe%20effectiveness%20of%20human-designed%20DP%20in%20the%20task%20of%20LP%2C%20emphasizing%20the%0Apivotal%20role%20of%20collaboration%20between%20humans%20and%20AI%20on%20KG.%20We%20open%20avenues%20for%0Afurther%20exploration%20and%20innovation%20through%20KG-HAIT%2C%20paving%20the%20way%20towards%20more%0Aeffective%20and%20insightful%20KG%20analysis%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09477v1&entry.124074799=Read"},
{"title": "Gradient Boosted Filters For Signal Processing", "author": "Jose A. Lopez and Georg Stemmer and Hector A. Cordourier", "abstract": "  Gradient boosted decision trees have achieved remarkable success in several\ndomains, particularly those that work with static tabular data. However, the\napplication of gradient boosted models to signal processing is underexplored.\nIn this work, we introduce gradient boosted filters for dynamic data, by\nemploying Hammerstein systems in place of decision trees. We discuss the\nrelationship of our approach to the Volterra series, providing the theoretical\nunderpinning for its application. We demonstrate the effective generalizability\nof our approach with examples.\n", "link": "http://arxiv.org/abs/2405.09305v1", "date": "2024-05-15", "relevancy": 1.9476, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Boosted%20Filters%20For%20Signal%20Processing&body=Title%3A%20Gradient%20Boosted%20Filters%20For%20Signal%20Processing%0AAuthor%3A%20Jose%20A.%20Lopez%20and%20Georg%20Stemmer%20and%20Hector%20A.%20Cordourier%0AAbstract%3A%20%20%20Gradient%20boosted%20decision%20trees%20have%20achieved%20remarkable%20success%20in%20several%0Adomains%2C%20particularly%20those%20that%20work%20with%20static%20tabular%20data.%20However%2C%20the%0Aapplication%20of%20gradient%20boosted%20models%20to%20signal%20processing%20is%20underexplored.%0AIn%20this%20work%2C%20we%20introduce%20gradient%20boosted%20filters%20for%20dynamic%20data%2C%20by%0Aemploying%20Hammerstein%20systems%20in%20place%20of%20decision%20trees.%20We%20discuss%20the%0Arelationship%20of%20our%20approach%20to%20the%20Volterra%20series%2C%20providing%20the%20theoretical%0Aunderpinning%20for%20its%20application.%20We%20demonstrate%20the%20effective%20generalizability%0Aof%20our%20approach%20with%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Boosted%2520Filters%2520For%2520Signal%2520Processing%26entry.906535625%3DJose%2520A.%2520Lopez%2520and%2520Georg%2520Stemmer%2520and%2520Hector%2520A.%2520Cordourier%26entry.1292438233%3D%2520%2520Gradient%2520boosted%2520decision%2520trees%2520have%2520achieved%2520remarkable%2520success%2520in%2520several%250Adomains%252C%2520particularly%2520those%2520that%2520work%2520with%2520static%2520tabular%2520data.%2520However%252C%2520the%250Aapplication%2520of%2520gradient%2520boosted%2520models%2520to%2520signal%2520processing%2520is%2520underexplored.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520gradient%2520boosted%2520filters%2520for%2520dynamic%2520data%252C%2520by%250Aemploying%2520Hammerstein%2520systems%2520in%2520place%2520of%2520decision%2520trees.%2520We%2520discuss%2520the%250Arelationship%2520of%2520our%2520approach%2520to%2520the%2520Volterra%2520series%252C%2520providing%2520the%2520theoretical%250Aunderpinning%2520for%2520its%2520application.%2520We%2520demonstrate%2520the%2520effective%2520generalizability%250Aof%2520our%2520approach%2520with%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Boosted%20Filters%20For%20Signal%20Processing&entry.906535625=Jose%20A.%20Lopez%20and%20Georg%20Stemmer%20and%20Hector%20A.%20Cordourier&entry.1292438233=%20%20Gradient%20boosted%20decision%20trees%20have%20achieved%20remarkable%20success%20in%20several%0Adomains%2C%20particularly%20those%20that%20work%20with%20static%20tabular%20data.%20However%2C%20the%0Aapplication%20of%20gradient%20boosted%20models%20to%20signal%20processing%20is%20underexplored.%0AIn%20this%20work%2C%20we%20introduce%20gradient%20boosted%20filters%20for%20dynamic%20data%2C%20by%0Aemploying%20Hammerstein%20systems%20in%20place%20of%20decision%20trees.%20We%20discuss%20the%0Arelationship%20of%20our%20approach%20to%20the%20Volterra%20series%2C%20providing%20the%20theoretical%0Aunderpinning%20for%20its%20application.%20We%20demonstrate%20the%20effective%20generalizability%0Aof%20our%20approach%20with%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09305v1&entry.124074799=Read"},
{"title": "Kuramoto Oscillators and Swarms on Manifolds for Geometry Informed\n  Machine Learning", "author": "Vladimir Jacimovic", "abstract": "  We propose the idea of using Kuramoto models (including their\nhigher-dimensional generalizations) for machine learning over non-Euclidean\ndata sets. These models are systems of matrix ODE's describing collective\nmotions (swarming dynamics) of abstract particles (generalized oscillators) on\nspheres, homogeneous spaces and Lie groups. Such models have been extensively\nstudied from the beginning of XXI century both in statistical physics and\ncontrol theory. They provide a suitable framework for encoding maps between\nvarious manifolds and are capable of learning over spherical and hyperbolic\ngeometries. In addition, they can learn coupled actions of transformation\ngroups (such as special orthogonal, unitary and Lorentz groups). Furthermore,\nwe overview families of probability distributions that provide appropriate\nstatistical models for probabilistic modeling and inference in Geometric Deep\nLearning. We argue in favor of using statistical models which arise in\ndifferent Kuramoto models in the continuum limit of particles. The most\nconvenient families of probability distributions are those which are invariant\nwith respect to actions of certain symmetry groups.\n", "link": "http://arxiv.org/abs/2405.09453v1", "date": "2024-05-15", "relevancy": 1.9446, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kuramoto%20Oscillators%20and%20Swarms%20on%20Manifolds%20for%20Geometry%20Informed%0A%20%20Machine%20Learning&body=Title%3A%20Kuramoto%20Oscillators%20and%20Swarms%20on%20Manifolds%20for%20Geometry%20Informed%0A%20%20Machine%20Learning%0AAuthor%3A%20Vladimir%20Jacimovic%0AAbstract%3A%20%20%20We%20propose%20the%20idea%20of%20using%20Kuramoto%20models%20%28including%20their%0Ahigher-dimensional%20generalizations%29%20for%20machine%20learning%20over%20non-Euclidean%0Adata%20sets.%20These%20models%20are%20systems%20of%20matrix%20ODE%27s%20describing%20collective%0Amotions%20%28swarming%20dynamics%29%20of%20abstract%20particles%20%28generalized%20oscillators%29%20on%0Aspheres%2C%20homogeneous%20spaces%20and%20Lie%20groups.%20Such%20models%20have%20been%20extensively%0Astudied%20from%20the%20beginning%20of%20XXI%20century%20both%20in%20statistical%20physics%20and%0Acontrol%20theory.%20They%20provide%20a%20suitable%20framework%20for%20encoding%20maps%20between%0Avarious%20manifolds%20and%20are%20capable%20of%20learning%20over%20spherical%20and%20hyperbolic%0Ageometries.%20In%20addition%2C%20they%20can%20learn%20coupled%20actions%20of%20transformation%0Agroups%20%28such%20as%20special%20orthogonal%2C%20unitary%20and%20Lorentz%20groups%29.%20Furthermore%2C%0Awe%20overview%20families%20of%20probability%20distributions%20that%20provide%20appropriate%0Astatistical%20models%20for%20probabilistic%20modeling%20and%20inference%20in%20Geometric%20Deep%0ALearning.%20We%20argue%20in%20favor%20of%20using%20statistical%20models%20which%20arise%20in%0Adifferent%20Kuramoto%20models%20in%20the%20continuum%20limit%20of%20particles.%20The%20most%0Aconvenient%20families%20of%20probability%20distributions%20are%20those%20which%20are%20invariant%0Awith%20respect%20to%20actions%20of%20certain%20symmetry%20groups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKuramoto%2520Oscillators%2520and%2520Swarms%2520on%2520Manifolds%2520for%2520Geometry%2520Informed%250A%2520%2520Machine%2520Learning%26entry.906535625%3DVladimir%2520Jacimovic%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520idea%2520of%2520using%2520Kuramoto%2520models%2520%2528including%2520their%250Ahigher-dimensional%2520generalizations%2529%2520for%2520machine%2520learning%2520over%2520non-Euclidean%250Adata%2520sets.%2520These%2520models%2520are%2520systems%2520of%2520matrix%2520ODE%2527s%2520describing%2520collective%250Amotions%2520%2528swarming%2520dynamics%2529%2520of%2520abstract%2520particles%2520%2528generalized%2520oscillators%2529%2520on%250Aspheres%252C%2520homogeneous%2520spaces%2520and%2520Lie%2520groups.%2520Such%2520models%2520have%2520been%2520extensively%250Astudied%2520from%2520the%2520beginning%2520of%2520XXI%2520century%2520both%2520in%2520statistical%2520physics%2520and%250Acontrol%2520theory.%2520They%2520provide%2520a%2520suitable%2520framework%2520for%2520encoding%2520maps%2520between%250Avarious%2520manifolds%2520and%2520are%2520capable%2520of%2520learning%2520over%2520spherical%2520and%2520hyperbolic%250Ageometries.%2520In%2520addition%252C%2520they%2520can%2520learn%2520coupled%2520actions%2520of%2520transformation%250Agroups%2520%2528such%2520as%2520special%2520orthogonal%252C%2520unitary%2520and%2520Lorentz%2520groups%2529.%2520Furthermore%252C%250Awe%2520overview%2520families%2520of%2520probability%2520distributions%2520that%2520provide%2520appropriate%250Astatistical%2520models%2520for%2520probabilistic%2520modeling%2520and%2520inference%2520in%2520Geometric%2520Deep%250ALearning.%2520We%2520argue%2520in%2520favor%2520of%2520using%2520statistical%2520models%2520which%2520arise%2520in%250Adifferent%2520Kuramoto%2520models%2520in%2520the%2520continuum%2520limit%2520of%2520particles.%2520The%2520most%250Aconvenient%2520families%2520of%2520probability%2520distributions%2520are%2520those%2520which%2520are%2520invariant%250Awith%2520respect%2520to%2520actions%2520of%2520certain%2520symmetry%2520groups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kuramoto%20Oscillators%20and%20Swarms%20on%20Manifolds%20for%20Geometry%20Informed%0A%20%20Machine%20Learning&entry.906535625=Vladimir%20Jacimovic&entry.1292438233=%20%20We%20propose%20the%20idea%20of%20using%20Kuramoto%20models%20%28including%20their%0Ahigher-dimensional%20generalizations%29%20for%20machine%20learning%20over%20non-Euclidean%0Adata%20sets.%20These%20models%20are%20systems%20of%20matrix%20ODE%27s%20describing%20collective%0Amotions%20%28swarming%20dynamics%29%20of%20abstract%20particles%20%28generalized%20oscillators%29%20on%0Aspheres%2C%20homogeneous%20spaces%20and%20Lie%20groups.%20Such%20models%20have%20been%20extensively%0Astudied%20from%20the%20beginning%20of%20XXI%20century%20both%20in%20statistical%20physics%20and%0Acontrol%20theory.%20They%20provide%20a%20suitable%20framework%20for%20encoding%20maps%20between%0Avarious%20manifolds%20and%20are%20capable%20of%20learning%20over%20spherical%20and%20hyperbolic%0Ageometries.%20In%20addition%2C%20they%20can%20learn%20coupled%20actions%20of%20transformation%0Agroups%20%28such%20as%20special%20orthogonal%2C%20unitary%20and%20Lorentz%20groups%29.%20Furthermore%2C%0Awe%20overview%20families%20of%20probability%20distributions%20that%20provide%20appropriate%0Astatistical%20models%20for%20probabilistic%20modeling%20and%20inference%20in%20Geometric%20Deep%0ALearning.%20We%20argue%20in%20favor%20of%20using%20statistical%20models%20which%20arise%20in%0Adifferent%20Kuramoto%20models%20in%20the%20continuum%20limit%20of%20particles.%20The%20most%0Aconvenient%20families%20of%20probability%20distributions%20are%20those%20which%20are%20invariant%0Awith%20respect%20to%20actions%20of%20certain%20symmetry%20groups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09453v1&entry.124074799=Read"},
{"title": "Learning functions on symmetric matrices and point clouds via\n  lightweight invariant features", "author": "Ben Blum-Smith and Ningyuan Huang and Marco Cuturi and Soledad Villar", "abstract": "  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we construct $O(n^2)$ invariant features derived from\ngenerators for the field of rational functions on $n\\times n$ symmetric\nmatrices that are invariant under joint permutations of rows and columns. We\nshow that these invariant features can separate all distinct orbits of\nsymmetric matrices except for a measure zero set; such features can be used to\nuniversally approximate invariant functions on almost all weighted graphs. For\npoint clouds in a fixed dimension, we prove that the number of invariant\nfeatures can be reduced, generically without losing expressivity, to $O(n)$,\nwhere $n$ is the number of points. We combine these invariant features with\nDeepSets to learn functions on symmetric matrices and point clouds with varying\nsizes. We empirically demonstrate the feasibility of our approach on molecule\nproperty regression and point cloud distance prediction.\n", "link": "http://arxiv.org/abs/2405.08097v2", "date": "2024-05-15", "relevancy": 1.9291, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4933}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4811}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20functions%20on%20symmetric%20matrices%20and%20point%20clouds%20via%0A%20%20lightweight%20invariant%20features&body=Title%3A%20Learning%20functions%20on%20symmetric%20matrices%20and%20point%20clouds%20via%0A%20%20lightweight%20invariant%20features%0AAuthor%3A%20Ben%20Blum-Smith%20and%20Ningyuan%20Huang%20and%20Marco%20Cuturi%20and%20Soledad%20Villar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20mathematical%20formulation%20for%20machine%20learning%20of%0A%281%29%20functions%20on%20symmetric%20matrices%20that%20are%20invariant%20with%20respect%20to%20the%0Aaction%20of%20permutations%20by%20conjugation%2C%20and%20%282%29%20functions%20on%20point%20clouds%20that%0Aare%20invariant%20with%20respect%20to%20rotations%2C%20reflections%2C%20and%20permutations%20of%20the%0Apoints.%20To%20achieve%20this%2C%20we%20construct%20%24O%28n%5E2%29%24%20invariant%20features%20derived%20from%0Agenerators%20for%20the%20field%20of%20rational%20functions%20on%20%24n%5Ctimes%20n%24%20symmetric%0Amatrices%20that%20are%20invariant%20under%20joint%20permutations%20of%20rows%20and%20columns.%20We%0Ashow%20that%20these%20invariant%20features%20can%20separate%20all%20distinct%20orbits%20of%0Asymmetric%20matrices%20except%20for%20a%20measure%20zero%20set%3B%20such%20features%20can%20be%20used%20to%0Auniversally%20approximate%20invariant%20functions%20on%20almost%20all%20weighted%20graphs.%20For%0Apoint%20clouds%20in%20a%20fixed%20dimension%2C%20we%20prove%20that%20the%20number%20of%20invariant%0Afeatures%20can%20be%20reduced%2C%20generically%20without%20losing%20expressivity%2C%20to%20%24O%28n%29%24%2C%0Awhere%20%24n%24%20is%20the%20number%20of%20points.%20We%20combine%20these%20invariant%20features%20with%0ADeepSets%20to%20learn%20functions%20on%20symmetric%20matrices%20and%20point%20clouds%20with%20varying%0Asizes.%20We%20empirically%20demonstrate%20the%20feasibility%20of%20our%20approach%20on%20molecule%0Aproperty%20regression%20and%20point%20cloud%20distance%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520functions%2520on%2520symmetric%2520matrices%2520and%2520point%2520clouds%2520via%250A%2520%2520lightweight%2520invariant%2520features%26entry.906535625%3DBen%2520Blum-Smith%2520and%2520Ningyuan%2520Huang%2520and%2520Marco%2520Cuturi%2520and%2520Soledad%2520Villar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520mathematical%2520formulation%2520for%2520machine%2520learning%2520of%250A%25281%2529%2520functions%2520on%2520symmetric%2520matrices%2520that%2520are%2520invariant%2520with%2520respect%2520to%2520the%250Aaction%2520of%2520permutations%2520by%2520conjugation%252C%2520and%2520%25282%2529%2520functions%2520on%2520point%2520clouds%2520that%250Aare%2520invariant%2520with%2520respect%2520to%2520rotations%252C%2520reflections%252C%2520and%2520permutations%2520of%2520the%250Apoints.%2520To%2520achieve%2520this%252C%2520we%2520construct%2520%2524O%2528n%255E2%2529%2524%2520invariant%2520features%2520derived%2520from%250Agenerators%2520for%2520the%2520field%2520of%2520rational%2520functions%2520on%2520%2524n%255Ctimes%2520n%2524%2520symmetric%250Amatrices%2520that%2520are%2520invariant%2520under%2520joint%2520permutations%2520of%2520rows%2520and%2520columns.%2520We%250Ashow%2520that%2520these%2520invariant%2520features%2520can%2520separate%2520all%2520distinct%2520orbits%2520of%250Asymmetric%2520matrices%2520except%2520for%2520a%2520measure%2520zero%2520set%253B%2520such%2520features%2520can%2520be%2520used%2520to%250Auniversally%2520approximate%2520invariant%2520functions%2520on%2520almost%2520all%2520weighted%2520graphs.%2520For%250Apoint%2520clouds%2520in%2520a%2520fixed%2520dimension%252C%2520we%2520prove%2520that%2520the%2520number%2520of%2520invariant%250Afeatures%2520can%2520be%2520reduced%252C%2520generically%2520without%2520losing%2520expressivity%252C%2520to%2520%2524O%2528n%2529%2524%252C%250Awhere%2520%2524n%2524%2520is%2520the%2520number%2520of%2520points.%2520We%2520combine%2520these%2520invariant%2520features%2520with%250ADeepSets%2520to%2520learn%2520functions%2520on%2520symmetric%2520matrices%2520and%2520point%2520clouds%2520with%2520varying%250Asizes.%2520We%2520empirically%2520demonstrate%2520the%2520feasibility%2520of%2520our%2520approach%2520on%2520molecule%250Aproperty%2520regression%2520and%2520point%2520cloud%2520distance%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20functions%20on%20symmetric%20matrices%20and%20point%20clouds%20via%0A%20%20lightweight%20invariant%20features&entry.906535625=Ben%20Blum-Smith%20and%20Ningyuan%20Huang%20and%20Marco%20Cuturi%20and%20Soledad%20Villar&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20mathematical%20formulation%20for%20machine%20learning%20of%0A%281%29%20functions%20on%20symmetric%20matrices%20that%20are%20invariant%20with%20respect%20to%20the%0Aaction%20of%20permutations%20by%20conjugation%2C%20and%20%282%29%20functions%20on%20point%20clouds%20that%0Aare%20invariant%20with%20respect%20to%20rotations%2C%20reflections%2C%20and%20permutations%20of%20the%0Apoints.%20To%20achieve%20this%2C%20we%20construct%20%24O%28n%5E2%29%24%20invariant%20features%20derived%20from%0Agenerators%20for%20the%20field%20of%20rational%20functions%20on%20%24n%5Ctimes%20n%24%20symmetric%0Amatrices%20that%20are%20invariant%20under%20joint%20permutations%20of%20rows%20and%20columns.%20We%0Ashow%20that%20these%20invariant%20features%20can%20separate%20all%20distinct%20orbits%20of%0Asymmetric%20matrices%20except%20for%20a%20measure%20zero%20set%3B%20such%20features%20can%20be%20used%20to%0Auniversally%20approximate%20invariant%20functions%20on%20almost%20all%20weighted%20graphs.%20For%0Apoint%20clouds%20in%20a%20fixed%20dimension%2C%20we%20prove%20that%20the%20number%20of%20invariant%0Afeatures%20can%20be%20reduced%2C%20generically%20without%20losing%20expressivity%2C%20to%20%24O%28n%29%24%2C%0Awhere%20%24n%24%20is%20the%20number%20of%20points.%20We%20combine%20these%20invariant%20features%20with%0ADeepSets%20to%20learn%20functions%20on%20symmetric%20matrices%20and%20point%20clouds%20with%20varying%0Asizes.%20We%20empirically%20demonstrate%20the%20feasibility%20of%20our%20approach%20on%20molecule%0Aproperty%20regression%20and%20point%20cloud%20distance%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08097v2&entry.124074799=Read"},
{"title": "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense\n  of Privacy", "author": "Jamie Hayes and Ilia Shumailov and Eleni Triantafillou and Amr Khalifa and Nicolas Papernot", "abstract": "  The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel's training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their\n``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into\n``population U-MIAs'', where the same attacker is instantiated for all\nexamples, and ``per-example U-MIAs'', where a dedicated attacker is\ninstantiated for each example. We show that the latter category, wherein the\nattacker tailors its membership prediction to each example under attack, is\nsignificantly stronger. Indeed, our results show that the commonly used U-MIAs\nin the unlearning literature overestimate the privacy protection afforded by\nexisting unlearning techniques on both vision and language models. Our\ninvestigation reveals a large variance in the vulnerability of different\nexamples to per-example U-MIAs. In fact, several unlearning algorithms lead to\na reduced vulnerability for some, but not all, examples that we wish to\nunlearn, at the expense of increasing it for other examples. Notably, we find\nthat the privacy protection for the remaining training examples may worsen as a\nconsequence of unlearning. We also discuss the fundamental difficulty of\nequally protecting all examples using existing unlearning schemes, due to the\ndifferent rates at which examples are unlearned. We demonstrate that naive\nattempts at tailoring unlearning stopping criteria to different examples fail\nto alleviate these issues.\n", "link": "http://arxiv.org/abs/2403.01218v2", "date": "2024-05-15", "relevancy": 1.9228, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4776}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy&body=Title%3A%20Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy%0AAuthor%3A%20Jamie%20Hayes%20and%20Ilia%20Shumailov%20and%20Eleni%20Triantafillou%20and%20Amr%20Khalifa%20and%20Nicolas%20Papernot%0AAbstract%3A%20%20%20The%20high%20cost%20of%20model%20training%20makes%20it%20increasingly%20desirable%20to%20develop%0Atechniques%20for%20unlearning.%20These%20techniques%20seek%20to%20remove%20the%20influence%20of%20a%0Atraining%20example%20without%20having%20to%20retrain%20the%20model%20from%20scratch.%20Intuitively%2C%0Aonce%20a%20model%20has%20unlearned%2C%20an%20adversary%20that%20interacts%20with%20the%20model%20should%0Ano%20longer%20be%20able%20to%20tell%20whether%20the%20unlearned%20example%20was%20included%20in%20the%0Amodel%27s%20training%20set%20or%20not.%20In%20the%20privacy%20literature%2C%20this%20is%20known%20as%0Amembership%20inference.%20In%20this%20work%2C%20we%20discuss%20adaptations%20of%20Membership%0AInference%20Attacks%20%28MIAs%29%20to%20the%20setting%20of%20unlearning%20%28leading%20to%20their%0A%60%60U-MIA%27%27%20counterparts%29.%20We%20propose%20a%20categorization%20of%20existing%20U-MIAs%20into%0A%60%60population%20U-MIAs%27%27%2C%20where%20the%20same%20attacker%20is%20instantiated%20for%20all%0Aexamples%2C%20and%20%60%60per-example%20U-MIAs%27%27%2C%20where%20a%20dedicated%20attacker%20is%0Ainstantiated%20for%20each%20example.%20We%20show%20that%20the%20latter%20category%2C%20wherein%20the%0Aattacker%20tailors%20its%20membership%20prediction%20to%20each%20example%20under%20attack%2C%20is%0Asignificantly%20stronger.%20Indeed%2C%20our%20results%20show%20that%20the%20commonly%20used%20U-MIAs%0Ain%20the%20unlearning%20literature%20overestimate%20the%20privacy%20protection%20afforded%20by%0Aexisting%20unlearning%20techniques%20on%20both%20vision%20and%20language%20models.%20Our%0Ainvestigation%20reveals%20a%20large%20variance%20in%20the%20vulnerability%20of%20different%0Aexamples%20to%20per-example%20U-MIAs.%20In%20fact%2C%20several%20unlearning%20algorithms%20lead%20to%0Aa%20reduced%20vulnerability%20for%20some%2C%20but%20not%20all%2C%20examples%20that%20we%20wish%20to%0Aunlearn%2C%20at%20the%20expense%20of%20increasing%20it%20for%20other%20examples.%20Notably%2C%20we%20find%0Athat%20the%20privacy%20protection%20for%20the%20remaining%20training%20examples%20may%20worsen%20as%20a%0Aconsequence%20of%20unlearning.%20We%20also%20discuss%20the%20fundamental%20difficulty%20of%0Aequally%20protecting%20all%20examples%20using%20existing%20unlearning%20schemes%2C%20due%20to%20the%0Adifferent%20rates%20at%20which%20examples%20are%20unlearned.%20We%20demonstrate%20that%20naive%0Aattempts%20at%20tailoring%20unlearning%20stopping%20criteria%20to%20different%20examples%20fail%0Ato%20alleviate%20these%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInexact%2520Unlearning%2520Needs%2520More%2520Careful%2520Evaluations%2520to%2520Avoid%2520a%2520False%2520Sense%250A%2520%2520of%2520Privacy%26entry.906535625%3DJamie%2520Hayes%2520and%2520Ilia%2520Shumailov%2520and%2520Eleni%2520Triantafillou%2520and%2520Amr%2520Khalifa%2520and%2520Nicolas%2520Papernot%26entry.1292438233%3D%2520%2520The%2520high%2520cost%2520of%2520model%2520training%2520makes%2520it%2520increasingly%2520desirable%2520to%2520develop%250Atechniques%2520for%2520unlearning.%2520These%2520techniques%2520seek%2520to%2520remove%2520the%2520influence%2520of%2520a%250Atraining%2520example%2520without%2520having%2520to%2520retrain%2520the%2520model%2520from%2520scratch.%2520Intuitively%252C%250Aonce%2520a%2520model%2520has%2520unlearned%252C%2520an%2520adversary%2520that%2520interacts%2520with%2520the%2520model%2520should%250Ano%2520longer%2520be%2520able%2520to%2520tell%2520whether%2520the%2520unlearned%2520example%2520was%2520included%2520in%2520the%250Amodel%2527s%2520training%2520set%2520or%2520not.%2520In%2520the%2520privacy%2520literature%252C%2520this%2520is%2520known%2520as%250Amembership%2520inference.%2520In%2520this%2520work%252C%2520we%2520discuss%2520adaptations%2520of%2520Membership%250AInference%2520Attacks%2520%2528MIAs%2529%2520to%2520the%2520setting%2520of%2520unlearning%2520%2528leading%2520to%2520their%250A%2560%2560U-MIA%2527%2527%2520counterparts%2529.%2520We%2520propose%2520a%2520categorization%2520of%2520existing%2520U-MIAs%2520into%250A%2560%2560population%2520U-MIAs%2527%2527%252C%2520where%2520the%2520same%2520attacker%2520is%2520instantiated%2520for%2520all%250Aexamples%252C%2520and%2520%2560%2560per-example%2520U-MIAs%2527%2527%252C%2520where%2520a%2520dedicated%2520attacker%2520is%250Ainstantiated%2520for%2520each%2520example.%2520We%2520show%2520that%2520the%2520latter%2520category%252C%2520wherein%2520the%250Aattacker%2520tailors%2520its%2520membership%2520prediction%2520to%2520each%2520example%2520under%2520attack%252C%2520is%250Asignificantly%2520stronger.%2520Indeed%252C%2520our%2520results%2520show%2520that%2520the%2520commonly%2520used%2520U-MIAs%250Ain%2520the%2520unlearning%2520literature%2520overestimate%2520the%2520privacy%2520protection%2520afforded%2520by%250Aexisting%2520unlearning%2520techniques%2520on%2520both%2520vision%2520and%2520language%2520models.%2520Our%250Ainvestigation%2520reveals%2520a%2520large%2520variance%2520in%2520the%2520vulnerability%2520of%2520different%250Aexamples%2520to%2520per-example%2520U-MIAs.%2520In%2520fact%252C%2520several%2520unlearning%2520algorithms%2520lead%2520to%250Aa%2520reduced%2520vulnerability%2520for%2520some%252C%2520but%2520not%2520all%252C%2520examples%2520that%2520we%2520wish%2520to%250Aunlearn%252C%2520at%2520the%2520expense%2520of%2520increasing%2520it%2520for%2520other%2520examples.%2520Notably%252C%2520we%2520find%250Athat%2520the%2520privacy%2520protection%2520for%2520the%2520remaining%2520training%2520examples%2520may%2520worsen%2520as%2520a%250Aconsequence%2520of%2520unlearning.%2520We%2520also%2520discuss%2520the%2520fundamental%2520difficulty%2520of%250Aequally%2520protecting%2520all%2520examples%2520using%2520existing%2520unlearning%2520schemes%252C%2520due%2520to%2520the%250Adifferent%2520rates%2520at%2520which%2520examples%2520are%2520unlearned.%2520We%2520demonstrate%2520that%2520naive%250Aattempts%2520at%2520tailoring%2520unlearning%2520stopping%2520criteria%2520to%2520different%2520examples%2520fail%250Ato%2520alleviate%2520these%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy&entry.906535625=Jamie%20Hayes%20and%20Ilia%20Shumailov%20and%20Eleni%20Triantafillou%20and%20Amr%20Khalifa%20and%20Nicolas%20Papernot&entry.1292438233=%20%20The%20high%20cost%20of%20model%20training%20makes%20it%20increasingly%20desirable%20to%20develop%0Atechniques%20for%20unlearning.%20These%20techniques%20seek%20to%20remove%20the%20influence%20of%20a%0Atraining%20example%20without%20having%20to%20retrain%20the%20model%20from%20scratch.%20Intuitively%2C%0Aonce%20a%20model%20has%20unlearned%2C%20an%20adversary%20that%20interacts%20with%20the%20model%20should%0Ano%20longer%20be%20able%20to%20tell%20whether%20the%20unlearned%20example%20was%20included%20in%20the%0Amodel%27s%20training%20set%20or%20not.%20In%20the%20privacy%20literature%2C%20this%20is%20known%20as%0Amembership%20inference.%20In%20this%20work%2C%20we%20discuss%20adaptations%20of%20Membership%0AInference%20Attacks%20%28MIAs%29%20to%20the%20setting%20of%20unlearning%20%28leading%20to%20their%0A%60%60U-MIA%27%27%20counterparts%29.%20We%20propose%20a%20categorization%20of%20existing%20U-MIAs%20into%0A%60%60population%20U-MIAs%27%27%2C%20where%20the%20same%20attacker%20is%20instantiated%20for%20all%0Aexamples%2C%20and%20%60%60per-example%20U-MIAs%27%27%2C%20where%20a%20dedicated%20attacker%20is%0Ainstantiated%20for%20each%20example.%20We%20show%20that%20the%20latter%20category%2C%20wherein%20the%0Aattacker%20tailors%20its%20membership%20prediction%20to%20each%20example%20under%20attack%2C%20is%0Asignificantly%20stronger.%20Indeed%2C%20our%20results%20show%20that%20the%20commonly%20used%20U-MIAs%0Ain%20the%20unlearning%20literature%20overestimate%20the%20privacy%20protection%20afforded%20by%0Aexisting%20unlearning%20techniques%20on%20both%20vision%20and%20language%20models.%20Our%0Ainvestigation%20reveals%20a%20large%20variance%20in%20the%20vulnerability%20of%20different%0Aexamples%20to%20per-example%20U-MIAs.%20In%20fact%2C%20several%20unlearning%20algorithms%20lead%20to%0Aa%20reduced%20vulnerability%20for%20some%2C%20but%20not%20all%2C%20examples%20that%20we%20wish%20to%0Aunlearn%2C%20at%20the%20expense%20of%20increasing%20it%20for%20other%20examples.%20Notably%2C%20we%20find%0Athat%20the%20privacy%20protection%20for%20the%20remaining%20training%20examples%20may%20worsen%20as%20a%0Aconsequence%20of%20unlearning.%20We%20also%20discuss%20the%20fundamental%20difficulty%20of%0Aequally%20protecting%20all%20examples%20using%20existing%20unlearning%20schemes%2C%20due%20to%20the%0Adifferent%20rates%20at%20which%20examples%20are%20unlearned.%20We%20demonstrate%20that%20naive%0Aattempts%20at%20tailoring%20unlearning%20stopping%20criteria%20to%20different%20examples%20fail%0Ato%20alleviate%20these%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01218v2&entry.124074799=Read"},
{"title": "The Codecfake Dataset and Countermeasures for the Universally Detection\n  of Deepfake Audio", "author": "Yuankun Xie and Yi Lu and Ruibo Fu and Zhengqi Wen and Zhiyong Wang and Jianhua Tao and Xin Qi and Xiaopeng Wang and Yukun Liu and Haonan Cheng and Long Ye and Yi Sun", "abstract": "  With the proliferation of Audio Language Model (ALM) based deepfake audio,\nthere is an urgent need for generalized detection methods. ALM-based deepfake\naudio currently exhibits widespread, high deception, and type versatility,\nposing a significant challenge to current audio deepfake detection (ADD) models\ntrained solely on vocoded data. To effectively detect ALM-based deepfake audio,\nwe focus on the mechanism of the ALM-based audio generation method, the\nconversion from neural codec to waveform. We initially construct the Codecfake\ndataset, an open-source large-scale dataset, including 2 languages, over 1M\naudio samples, and various test conditions, focus on ALM-based audio detection.\nAs countermeasure, to achieve universal detection of deepfake audio and tackle\ndomain ascent bias issue of original SAM, we propose the CSAM strategy to learn\na domain balanced and generalized minima. In our experiments, we first\ndemonstrate that ADD model training with the Codecfake dataset can effectively\ndetects ALM-based audio. Furthermore, our proposed generalization\ncountermeasure yields the lowest average Equal Error Rate (EER) of 0.616%\nacross all test conditions compared to baseline models. The dataset and\nassociated code are available online.\n", "link": "http://arxiv.org/abs/2405.04880v2", "date": "2024-05-15", "relevancy": 1.9223, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4917}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4872}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Codecfake%20Dataset%20and%20Countermeasures%20for%20the%20Universally%20Detection%0A%20%20of%20Deepfake%20Audio&body=Title%3A%20The%20Codecfake%20Dataset%20and%20Countermeasures%20for%20the%20Universally%20Detection%0A%20%20of%20Deepfake%20Audio%0AAuthor%3A%20Yuankun%20Xie%20and%20Yi%20Lu%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Zhiyong%20Wang%20and%20Jianhua%20Tao%20and%20Xin%20Qi%20and%20Xiaopeng%20Wang%20and%20Yukun%20Liu%20and%20Haonan%20Cheng%20and%20Long%20Ye%20and%20Yi%20Sun%0AAbstract%3A%20%20%20With%20the%20proliferation%20of%20Audio%20Language%20Model%20%28ALM%29%20based%20deepfake%20audio%2C%0Athere%20is%20an%20urgent%20need%20for%20generalized%20detection%20methods.%20ALM-based%20deepfake%0Aaudio%20currently%20exhibits%20widespread%2C%20high%20deception%2C%20and%20type%20versatility%2C%0Aposing%20a%20significant%20challenge%20to%20current%20audio%20deepfake%20detection%20%28ADD%29%20models%0Atrained%20solely%20on%20vocoded%20data.%20To%20effectively%20detect%20ALM-based%20deepfake%20audio%2C%0Awe%20focus%20on%20the%20mechanism%20of%20the%20ALM-based%20audio%20generation%20method%2C%20the%0Aconversion%20from%20neural%20codec%20to%20waveform.%20We%20initially%20construct%20the%20Codecfake%0Adataset%2C%20an%20open-source%20large-scale%20dataset%2C%20including%202%20languages%2C%20over%201M%0Aaudio%20samples%2C%20and%20various%20test%20conditions%2C%20focus%20on%20ALM-based%20audio%20detection.%0AAs%20countermeasure%2C%20to%20achieve%20universal%20detection%20of%20deepfake%20audio%20and%20tackle%0Adomain%20ascent%20bias%20issue%20of%20original%20SAM%2C%20we%20propose%20the%20CSAM%20strategy%20to%20learn%0Aa%20domain%20balanced%20and%20generalized%20minima.%20In%20our%20experiments%2C%20we%20first%0Ademonstrate%20that%20ADD%20model%20training%20with%20the%20Codecfake%20dataset%20can%20effectively%0Adetects%20ALM-based%20audio.%20Furthermore%2C%20our%20proposed%20generalization%0Acountermeasure%20yields%20the%20lowest%20average%20Equal%20Error%20Rate%20%28EER%29%20of%200.616%25%0Aacross%20all%20test%20conditions%20compared%20to%20baseline%20models.%20The%20dataset%20and%0Aassociated%20code%20are%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Codecfake%2520Dataset%2520and%2520Countermeasures%2520for%2520the%2520Universally%2520Detection%250A%2520%2520of%2520Deepfake%2520Audio%26entry.906535625%3DYuankun%2520Xie%2520and%2520Yi%2520Lu%2520and%2520Ruibo%2520Fu%2520and%2520Zhengqi%2520Wen%2520and%2520Zhiyong%2520Wang%2520and%2520Jianhua%2520Tao%2520and%2520Xin%2520Qi%2520and%2520Xiaopeng%2520Wang%2520and%2520Yukun%2520Liu%2520and%2520Haonan%2520Cheng%2520and%2520Long%2520Ye%2520and%2520Yi%2520Sun%26entry.1292438233%3D%2520%2520With%2520the%2520proliferation%2520of%2520Audio%2520Language%2520Model%2520%2528ALM%2529%2520based%2520deepfake%2520audio%252C%250Athere%2520is%2520an%2520urgent%2520need%2520for%2520generalized%2520detection%2520methods.%2520ALM-based%2520deepfake%250Aaudio%2520currently%2520exhibits%2520widespread%252C%2520high%2520deception%252C%2520and%2520type%2520versatility%252C%250Aposing%2520a%2520significant%2520challenge%2520to%2520current%2520audio%2520deepfake%2520detection%2520%2528ADD%2529%2520models%250Atrained%2520solely%2520on%2520vocoded%2520data.%2520To%2520effectively%2520detect%2520ALM-based%2520deepfake%2520audio%252C%250Awe%2520focus%2520on%2520the%2520mechanism%2520of%2520the%2520ALM-based%2520audio%2520generation%2520method%252C%2520the%250Aconversion%2520from%2520neural%2520codec%2520to%2520waveform.%2520We%2520initially%2520construct%2520the%2520Codecfake%250Adataset%252C%2520an%2520open-source%2520large-scale%2520dataset%252C%2520including%25202%2520languages%252C%2520over%25201M%250Aaudio%2520samples%252C%2520and%2520various%2520test%2520conditions%252C%2520focus%2520on%2520ALM-based%2520audio%2520detection.%250AAs%2520countermeasure%252C%2520to%2520achieve%2520universal%2520detection%2520of%2520deepfake%2520audio%2520and%2520tackle%250Adomain%2520ascent%2520bias%2520issue%2520of%2520original%2520SAM%252C%2520we%2520propose%2520the%2520CSAM%2520strategy%2520to%2520learn%250Aa%2520domain%2520balanced%2520and%2520generalized%2520minima.%2520In%2520our%2520experiments%252C%2520we%2520first%250Ademonstrate%2520that%2520ADD%2520model%2520training%2520with%2520the%2520Codecfake%2520dataset%2520can%2520effectively%250Adetects%2520ALM-based%2520audio.%2520Furthermore%252C%2520our%2520proposed%2520generalization%250Acountermeasure%2520yields%2520the%2520lowest%2520average%2520Equal%2520Error%2520Rate%2520%2528EER%2529%2520of%25200.616%2525%250Aacross%2520all%2520test%2520conditions%2520compared%2520to%2520baseline%2520models.%2520The%2520dataset%2520and%250Aassociated%2520code%2520are%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Codecfake%20Dataset%20and%20Countermeasures%20for%20the%20Universally%20Detection%0A%20%20of%20Deepfake%20Audio&entry.906535625=Yuankun%20Xie%20and%20Yi%20Lu%20and%20Ruibo%20Fu%20and%20Zhengqi%20Wen%20and%20Zhiyong%20Wang%20and%20Jianhua%20Tao%20and%20Xin%20Qi%20and%20Xiaopeng%20Wang%20and%20Yukun%20Liu%20and%20Haonan%20Cheng%20and%20Long%20Ye%20and%20Yi%20Sun&entry.1292438233=%20%20With%20the%20proliferation%20of%20Audio%20Language%20Model%20%28ALM%29%20based%20deepfake%20audio%2C%0Athere%20is%20an%20urgent%20need%20for%20generalized%20detection%20methods.%20ALM-based%20deepfake%0Aaudio%20currently%20exhibits%20widespread%2C%20high%20deception%2C%20and%20type%20versatility%2C%0Aposing%20a%20significant%20challenge%20to%20current%20audio%20deepfake%20detection%20%28ADD%29%20models%0Atrained%20solely%20on%20vocoded%20data.%20To%20effectively%20detect%20ALM-based%20deepfake%20audio%2C%0Awe%20focus%20on%20the%20mechanism%20of%20the%20ALM-based%20audio%20generation%20method%2C%20the%0Aconversion%20from%20neural%20codec%20to%20waveform.%20We%20initially%20construct%20the%20Codecfake%0Adataset%2C%20an%20open-source%20large-scale%20dataset%2C%20including%202%20languages%2C%20over%201M%0Aaudio%20samples%2C%20and%20various%20test%20conditions%2C%20focus%20on%20ALM-based%20audio%20detection.%0AAs%20countermeasure%2C%20to%20achieve%20universal%20detection%20of%20deepfake%20audio%20and%20tackle%0Adomain%20ascent%20bias%20issue%20of%20original%20SAM%2C%20we%20propose%20the%20CSAM%20strategy%20to%20learn%0Aa%20domain%20balanced%20and%20generalized%20minima.%20In%20our%20experiments%2C%20we%20first%0Ademonstrate%20that%20ADD%20model%20training%20with%20the%20Codecfake%20dataset%20can%20effectively%0Adetects%20ALM-based%20audio.%20Furthermore%2C%20our%20proposed%20generalization%0Acountermeasure%20yields%20the%20lowest%20average%20Equal%20Error%20Rate%20%28EER%29%20of%200.616%25%0Aacross%20all%20test%20conditions%20compared%20to%20baseline%20models.%20The%20dataset%20and%0Aassociated%20code%20are%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04880v2&entry.124074799=Read"},
{"title": "RAGFormer: Learning Semantic Attributes and Topological Structure for\n  Fraud Detection", "author": "Haolin Li and Shuyang Jiang and Lifeng Zhang and Siyuan Du and Guangnan Ye and Hongfeng Chai", "abstract": "  Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.\n", "link": "http://arxiv.org/abs/2402.17472v2", "date": "2024-05-15", "relevancy": 1.9036, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4808}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAGFormer%3A%20Learning%20Semantic%20Attributes%20and%20Topological%20Structure%20for%0A%20%20Fraud%20Detection&body=Title%3A%20RAGFormer%3A%20Learning%20Semantic%20Attributes%20and%20Topological%20Structure%20for%0A%20%20Fraud%20Detection%0AAuthor%3A%20Haolin%20Li%20and%20Shuyang%20Jiang%20and%20Lifeng%20Zhang%20and%20Siyuan%20Du%20and%20Guangnan%20Ye%20and%20Hongfeng%20Chai%0AAbstract%3A%20%20%20Fraud%20detection%20remains%20a%20challenging%20task%20due%20to%20the%20complex%20and%20deceptive%0Anature%20of%20fraudulent%20activities.%20Current%20approaches%20primarily%20concentrate%20on%0Alearning%20only%20one%20perspective%20of%20the%20graph%3A%20either%20the%20topological%20structure%20of%0Athe%20graph%20or%20the%20attributes%20of%20individual%20nodes.%20However%2C%20we%20conduct%20empirical%0Astudies%20to%20reveal%20that%20these%20two%20types%20of%20features%2C%20while%20nearly%20orthogonal%2C%0Aare%20each%20independently%20effective.%20As%20a%20result%2C%20previous%20methods%20can%20not%20fully%0Acapture%20the%20comprehensive%20characteristics%20of%20the%20fraud%20graph.%20To%20address%20this%0Adilemma%2C%20we%20present%20a%20novel%20framework%20called%20Relation-Aware%20GNN%20with%0AtransFormer~%28RAGFormer%29%20which%20simultaneously%20embeds%20both%20semantic%20and%0Atopological%20features%20into%20a%20target%20node.%20The%20simple%20yet%20effective%20network%0Aconsists%20of%20a%20semantic%20encoder%2C%20a%20topology%20encoder%2C%20and%20an%20attention%20fusion%0Amodule.%20The%20semantic%20encoder%20utilizes%20Transformer%20to%20learn%20semantic%20features%0Aand%20node%20interactions%20across%20different%20relations.%20We%20introduce%20Relation-Aware%0AGNN%20as%20the%20topology%20encoder%20to%20learn%20topological%20features%20and%20node%20interactions%0Awithin%20each%20relation.%20These%20two%20complementary%20features%20are%20interleaved%20through%0Aan%20attention%20fusion%20module%20to%20support%20prediction%20by%20both%20orthogonal%20features.%0AExtensive%20experiments%20on%20two%20popular%20public%20datasets%20demonstrate%20that%20RAGFormer%0Aachieves%20state-of-the-art%20performance.%20The%20significant%20improvement%20of%20RAGFormer%0Ain%20an%20industrial%20credit%20card%20fraud%20detection%20dataset%20further%20validates%20the%0Aapplicability%20of%20our%20method%20in%20real-world%20business%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAGFormer%253A%2520Learning%2520Semantic%2520Attributes%2520and%2520Topological%2520Structure%2520for%250A%2520%2520Fraud%2520Detection%26entry.906535625%3DHaolin%2520Li%2520and%2520Shuyang%2520Jiang%2520and%2520Lifeng%2520Zhang%2520and%2520Siyuan%2520Du%2520and%2520Guangnan%2520Ye%2520and%2520Hongfeng%2520Chai%26entry.1292438233%3D%2520%2520Fraud%2520detection%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%2520complex%2520and%2520deceptive%250Anature%2520of%2520fraudulent%2520activities.%2520Current%2520approaches%2520primarily%2520concentrate%2520on%250Alearning%2520only%2520one%2520perspective%2520of%2520the%2520graph%253A%2520either%2520the%2520topological%2520structure%2520of%250Athe%2520graph%2520or%2520the%2520attributes%2520of%2520individual%2520nodes.%2520However%252C%2520we%2520conduct%2520empirical%250Astudies%2520to%2520reveal%2520that%2520these%2520two%2520types%2520of%2520features%252C%2520while%2520nearly%2520orthogonal%252C%250Aare%2520each%2520independently%2520effective.%2520As%2520a%2520result%252C%2520previous%2520methods%2520can%2520not%2520fully%250Acapture%2520the%2520comprehensive%2520characteristics%2520of%2520the%2520fraud%2520graph.%2520To%2520address%2520this%250Adilemma%252C%2520we%2520present%2520a%2520novel%2520framework%2520called%2520Relation-Aware%2520GNN%2520with%250AtransFormer~%2528RAGFormer%2529%2520which%2520simultaneously%2520embeds%2520both%2520semantic%2520and%250Atopological%2520features%2520into%2520a%2520target%2520node.%2520The%2520simple%2520yet%2520effective%2520network%250Aconsists%2520of%2520a%2520semantic%2520encoder%252C%2520a%2520topology%2520encoder%252C%2520and%2520an%2520attention%2520fusion%250Amodule.%2520The%2520semantic%2520encoder%2520utilizes%2520Transformer%2520to%2520learn%2520semantic%2520features%250Aand%2520node%2520interactions%2520across%2520different%2520relations.%2520We%2520introduce%2520Relation-Aware%250AGNN%2520as%2520the%2520topology%2520encoder%2520to%2520learn%2520topological%2520features%2520and%2520node%2520interactions%250Awithin%2520each%2520relation.%2520These%2520two%2520complementary%2520features%2520are%2520interleaved%2520through%250Aan%2520attention%2520fusion%2520module%2520to%2520support%2520prediction%2520by%2520both%2520orthogonal%2520features.%250AExtensive%2520experiments%2520on%2520two%2520popular%2520public%2520datasets%2520demonstrate%2520that%2520RAGFormer%250Aachieves%2520state-of-the-art%2520performance.%2520The%2520significant%2520improvement%2520of%2520RAGFormer%250Ain%2520an%2520industrial%2520credit%2520card%2520fraud%2520detection%2520dataset%2520further%2520validates%2520the%250Aapplicability%2520of%2520our%2520method%2520in%2520real-world%2520business%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAGFormer%3A%20Learning%20Semantic%20Attributes%20and%20Topological%20Structure%20for%0A%20%20Fraud%20Detection&entry.906535625=Haolin%20Li%20and%20Shuyang%20Jiang%20and%20Lifeng%20Zhang%20and%20Siyuan%20Du%20and%20Guangnan%20Ye%20and%20Hongfeng%20Chai&entry.1292438233=%20%20Fraud%20detection%20remains%20a%20challenging%20task%20due%20to%20the%20complex%20and%20deceptive%0Anature%20of%20fraudulent%20activities.%20Current%20approaches%20primarily%20concentrate%20on%0Alearning%20only%20one%20perspective%20of%20the%20graph%3A%20either%20the%20topological%20structure%20of%0Athe%20graph%20or%20the%20attributes%20of%20individual%20nodes.%20However%2C%20we%20conduct%20empirical%0Astudies%20to%20reveal%20that%20these%20two%20types%20of%20features%2C%20while%20nearly%20orthogonal%2C%0Aare%20each%20independently%20effective.%20As%20a%20result%2C%20previous%20methods%20can%20not%20fully%0Acapture%20the%20comprehensive%20characteristics%20of%20the%20fraud%20graph.%20To%20address%20this%0Adilemma%2C%20we%20present%20a%20novel%20framework%20called%20Relation-Aware%20GNN%20with%0AtransFormer~%28RAGFormer%29%20which%20simultaneously%20embeds%20both%20semantic%20and%0Atopological%20features%20into%20a%20target%20node.%20The%20simple%20yet%20effective%20network%0Aconsists%20of%20a%20semantic%20encoder%2C%20a%20topology%20encoder%2C%20and%20an%20attention%20fusion%0Amodule.%20The%20semantic%20encoder%20utilizes%20Transformer%20to%20learn%20semantic%20features%0Aand%20node%20interactions%20across%20different%20relations.%20We%20introduce%20Relation-Aware%0AGNN%20as%20the%20topology%20encoder%20to%20learn%20topological%20features%20and%20node%20interactions%0Awithin%20each%20relation.%20These%20two%20complementary%20features%20are%20interleaved%20through%0Aan%20attention%20fusion%20module%20to%20support%20prediction%20by%20both%20orthogonal%20features.%0AExtensive%20experiments%20on%20two%20popular%20public%20datasets%20demonstrate%20that%20RAGFormer%0Aachieves%20state-of-the-art%20performance.%20The%20significant%20improvement%20of%20RAGFormer%0Ain%20an%20industrial%20credit%20card%20fraud%20detection%20dataset%20further%20validates%20the%0Aapplicability%20of%20our%20method%20in%20real-world%20business%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17472v2&entry.124074799=Read"},
{"title": "Gaze-DETR: Using Expert Gaze to Reduce False Positives in Vulvovaginal\n  Candidiasis Screening", "author": "Yan Kong and Sheng Wang and Jiangdong Cai and Zihao Zhao and Zhenrong Shen and Yonghao Li and Manman Fei and Qian Wang", "abstract": "  Accurate detection of vulvovaginal candidiasis is critical for women's\nhealth, yet its sparse distribution and visually ambiguous characteristics pose\nsignificant challenges for accurate identification by pathologists and neural\nnetworks alike. Our eye-tracking data reveals that areas garnering sustained\nattention - yet not marked by experts after deliberation - are often aligned\nwith false positives of neural networks. Leveraging this finding, we introduce\nGaze-DETR, a pioneering method that integrates gaze data to enhance neural\nnetwork precision by diminishing false positives. Gaze-DETR incorporates a\nuniversal gaze-guided warm-up protocol applicable across various detection\nmethods and a gaze-guided rectification strategy specifically designed for\nDETR-based models. Our comprehensive tests confirm that Gaze-DETR surpasses\nexisting leading methods, showcasing remarkable improvements in detection\naccuracy and generalizability.\n", "link": "http://arxiv.org/abs/2405.09463v1", "date": "2024-05-15", "relevancy": 1.8932, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4824}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4747}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-DETR%3A%20Using%20Expert%20Gaze%20to%20Reduce%20False%20Positives%20in%20Vulvovaginal%0A%20%20Candidiasis%20Screening&body=Title%3A%20Gaze-DETR%3A%20Using%20Expert%20Gaze%20to%20Reduce%20False%20Positives%20in%20Vulvovaginal%0A%20%20Candidiasis%20Screening%0AAuthor%3A%20Yan%20Kong%20and%20Sheng%20Wang%20and%20Jiangdong%20Cai%20and%20Zihao%20Zhao%20and%20Zhenrong%20Shen%20and%20Yonghao%20Li%20and%20Manman%20Fei%20and%20Qian%20Wang%0AAbstract%3A%20%20%20Accurate%20detection%20of%20vulvovaginal%20candidiasis%20is%20critical%20for%20women%27s%0Ahealth%2C%20yet%20its%20sparse%20distribution%20and%20visually%20ambiguous%20characteristics%20pose%0Asignificant%20challenges%20for%20accurate%20identification%20by%20pathologists%20and%20neural%0Anetworks%20alike.%20Our%20eye-tracking%20data%20reveals%20that%20areas%20garnering%20sustained%0Aattention%20-%20yet%20not%20marked%20by%20experts%20after%20deliberation%20-%20are%20often%20aligned%0Awith%20false%20positives%20of%20neural%20networks.%20Leveraging%20this%20finding%2C%20we%20introduce%0AGaze-DETR%2C%20a%20pioneering%20method%20that%20integrates%20gaze%20data%20to%20enhance%20neural%0Anetwork%20precision%20by%20diminishing%20false%20positives.%20Gaze-DETR%20incorporates%20a%0Auniversal%20gaze-guided%20warm-up%20protocol%20applicable%20across%20various%20detection%0Amethods%20and%20a%20gaze-guided%20rectification%20strategy%20specifically%20designed%20for%0ADETR-based%20models.%20Our%20comprehensive%20tests%20confirm%20that%20Gaze-DETR%20surpasses%0Aexisting%20leading%20methods%2C%20showcasing%20remarkable%20improvements%20in%20detection%0Aaccuracy%20and%20generalizability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-DETR%253A%2520Using%2520Expert%2520Gaze%2520to%2520Reduce%2520False%2520Positives%2520in%2520Vulvovaginal%250A%2520%2520Candidiasis%2520Screening%26entry.906535625%3DYan%2520Kong%2520and%2520Sheng%2520Wang%2520and%2520Jiangdong%2520Cai%2520and%2520Zihao%2520Zhao%2520and%2520Zhenrong%2520Shen%2520and%2520Yonghao%2520Li%2520and%2520Manman%2520Fei%2520and%2520Qian%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520of%2520vulvovaginal%2520candidiasis%2520is%2520critical%2520for%2520women%2527s%250Ahealth%252C%2520yet%2520its%2520sparse%2520distribution%2520and%2520visually%2520ambiguous%2520characteristics%2520pose%250Asignificant%2520challenges%2520for%2520accurate%2520identification%2520by%2520pathologists%2520and%2520neural%250Anetworks%2520alike.%2520Our%2520eye-tracking%2520data%2520reveals%2520that%2520areas%2520garnering%2520sustained%250Aattention%2520-%2520yet%2520not%2520marked%2520by%2520experts%2520after%2520deliberation%2520-%2520are%2520often%2520aligned%250Awith%2520false%2520positives%2520of%2520neural%2520networks.%2520Leveraging%2520this%2520finding%252C%2520we%2520introduce%250AGaze-DETR%252C%2520a%2520pioneering%2520method%2520that%2520integrates%2520gaze%2520data%2520to%2520enhance%2520neural%250Anetwork%2520precision%2520by%2520diminishing%2520false%2520positives.%2520Gaze-DETR%2520incorporates%2520a%250Auniversal%2520gaze-guided%2520warm-up%2520protocol%2520applicable%2520across%2520various%2520detection%250Amethods%2520and%2520a%2520gaze-guided%2520rectification%2520strategy%2520specifically%2520designed%2520for%250ADETR-based%2520models.%2520Our%2520comprehensive%2520tests%2520confirm%2520that%2520Gaze-DETR%2520surpasses%250Aexisting%2520leading%2520methods%252C%2520showcasing%2520remarkable%2520improvements%2520in%2520detection%250Aaccuracy%2520and%2520generalizability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-DETR%3A%20Using%20Expert%20Gaze%20to%20Reduce%20False%20Positives%20in%20Vulvovaginal%0A%20%20Candidiasis%20Screening&entry.906535625=Yan%20Kong%20and%20Sheng%20Wang%20and%20Jiangdong%20Cai%20and%20Zihao%20Zhao%20and%20Zhenrong%20Shen%20and%20Yonghao%20Li%20and%20Manman%20Fei%20and%20Qian%20Wang&entry.1292438233=%20%20Accurate%20detection%20of%20vulvovaginal%20candidiasis%20is%20critical%20for%20women%27s%0Ahealth%2C%20yet%20its%20sparse%20distribution%20and%20visually%20ambiguous%20characteristics%20pose%0Asignificant%20challenges%20for%20accurate%20identification%20by%20pathologists%20and%20neural%0Anetworks%20alike.%20Our%20eye-tracking%20data%20reveals%20that%20areas%20garnering%20sustained%0Aattention%20-%20yet%20not%20marked%20by%20experts%20after%20deliberation%20-%20are%20often%20aligned%0Awith%20false%20positives%20of%20neural%20networks.%20Leveraging%20this%20finding%2C%20we%20introduce%0AGaze-DETR%2C%20a%20pioneering%20method%20that%20integrates%20gaze%20data%20to%20enhance%20neural%0Anetwork%20precision%20by%20diminishing%20false%20positives.%20Gaze-DETR%20incorporates%20a%0Auniversal%20gaze-guided%20warm-up%20protocol%20applicable%20across%20various%20detection%0Amethods%20and%20a%20gaze-guided%20rectification%20strategy%20specifically%20designed%20for%0ADETR-based%20models.%20Our%20comprehensive%20tests%20confirm%20that%20Gaze-DETR%20surpasses%0Aexisting%20leading%20methods%2C%20showcasing%20remarkable%20improvements%20in%20detection%0Aaccuracy%20and%20generalizability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09463v1&entry.124074799=Read"},
{"title": "Parameter-Efficient Instruction Tuning of Large Language Models For\n  Extreme Financial Numeral Labelling", "author": "Subhendu Khatuya and Rajdeep Mukherjee and Akash Ghosh and Manjunath Hegde and Koustuv Dasgupta and Niloy Ganguly and Saptarshi Ghosh and Pawan Goyal", "abstract": "  We study the problem of automatically annotating relevant numerals (GAAP\nmetrics) occurring in the financial documents with their corresponding XBRL\ntags. Different from prior works, we investigate the feasibility of solving\nthis extreme classification problem using a generative paradigm through\ninstruction tuning of Large Language Models (LLMs). To this end, we leverage\nmetric metadata information to frame our target outputs while proposing a\nparameter efficient solution for the task using LoRA. We perform experiments on\ntwo recently released financial numeric labeling datasets. Our proposed model,\nFLAN-FinXC, achieves new state-of-the-art performances on both the datasets,\noutperforming several strong baselines. We explain the better scores of our\nproposed model by demonstrating its capability for zero-shot as well as the\nleast frequently occurring tags. Also, even when we fail to predict the XBRL\ntags correctly, our generated output has substantial overlap with the\nground-truth in majority of the cases.\n", "link": "http://arxiv.org/abs/2405.06671v2", "date": "2024-05-15", "relevancy": 1.8887, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4961}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4708}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Instruction%20Tuning%20of%20Large%20Language%20Models%20For%0A%20%20Extreme%20Financial%20Numeral%20Labelling&body=Title%3A%20Parameter-Efficient%20Instruction%20Tuning%20of%20Large%20Language%20Models%20For%0A%20%20Extreme%20Financial%20Numeral%20Labelling%0AAuthor%3A%20Subhendu%20Khatuya%20and%20Rajdeep%20Mukherjee%20and%20Akash%20Ghosh%20and%20Manjunath%20Hegde%20and%20Koustuv%20Dasgupta%20and%20Niloy%20Ganguly%20and%20Saptarshi%20Ghosh%20and%20Pawan%20Goyal%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20automatically%20annotating%20relevant%20numerals%20%28GAAP%0Ametrics%29%20occurring%20in%20the%20financial%20documents%20with%20their%20corresponding%20XBRL%0Atags.%20Different%20from%20prior%20works%2C%20we%20investigate%20the%20feasibility%20of%20solving%0Athis%20extreme%20classification%20problem%20using%20a%20generative%20paradigm%20through%0Ainstruction%20tuning%20of%20Large%20Language%20Models%20%28LLMs%29.%20To%20this%20end%2C%20we%20leverage%0Ametric%20metadata%20information%20to%20frame%20our%20target%20outputs%20while%20proposing%20a%0Aparameter%20efficient%20solution%20for%20the%20task%20using%20LoRA.%20We%20perform%20experiments%20on%0Atwo%20recently%20released%20financial%20numeric%20labeling%20datasets.%20Our%20proposed%20model%2C%0AFLAN-FinXC%2C%20achieves%20new%20state-of-the-art%20performances%20on%20both%20the%20datasets%2C%0Aoutperforming%20several%20strong%20baselines.%20We%20explain%20the%20better%20scores%20of%20our%0Aproposed%20model%20by%20demonstrating%20its%20capability%20for%20zero-shot%20as%20well%20as%20the%0Aleast%20frequently%20occurring%20tags.%20Also%2C%20even%20when%20we%20fail%20to%20predict%20the%20XBRL%0Atags%20correctly%2C%20our%20generated%20output%20has%20substantial%20overlap%20with%20the%0Aground-truth%20in%20majority%20of%20the%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06671v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Instruction%2520Tuning%2520of%2520Large%2520Language%2520Models%2520For%250A%2520%2520Extreme%2520Financial%2520Numeral%2520Labelling%26entry.906535625%3DSubhendu%2520Khatuya%2520and%2520Rajdeep%2520Mukherjee%2520and%2520Akash%2520Ghosh%2520and%2520Manjunath%2520Hegde%2520and%2520Koustuv%2520Dasgupta%2520and%2520Niloy%2520Ganguly%2520and%2520Saptarshi%2520Ghosh%2520and%2520Pawan%2520Goyal%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520automatically%2520annotating%2520relevant%2520numerals%2520%2528GAAP%250Ametrics%2529%2520occurring%2520in%2520the%2520financial%2520documents%2520with%2520their%2520corresponding%2520XBRL%250Atags.%2520Different%2520from%2520prior%2520works%252C%2520we%2520investigate%2520the%2520feasibility%2520of%2520solving%250Athis%2520extreme%2520classification%2520problem%2520using%2520a%2520generative%2520paradigm%2520through%250Ainstruction%2520tuning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520To%2520this%2520end%252C%2520we%2520leverage%250Ametric%2520metadata%2520information%2520to%2520frame%2520our%2520target%2520outputs%2520while%2520proposing%2520a%250Aparameter%2520efficient%2520solution%2520for%2520the%2520task%2520using%2520LoRA.%2520We%2520perform%2520experiments%2520on%250Atwo%2520recently%2520released%2520financial%2520numeric%2520labeling%2520datasets.%2520Our%2520proposed%2520model%252C%250AFLAN-FinXC%252C%2520achieves%2520new%2520state-of-the-art%2520performances%2520on%2520both%2520the%2520datasets%252C%250Aoutperforming%2520several%2520strong%2520baselines.%2520We%2520explain%2520the%2520better%2520scores%2520of%2520our%250Aproposed%2520model%2520by%2520demonstrating%2520its%2520capability%2520for%2520zero-shot%2520as%2520well%2520as%2520the%250Aleast%2520frequently%2520occurring%2520tags.%2520Also%252C%2520even%2520when%2520we%2520fail%2520to%2520predict%2520the%2520XBRL%250Atags%2520correctly%252C%2520our%2520generated%2520output%2520has%2520substantial%2520overlap%2520with%2520the%250Aground-truth%2520in%2520majority%2520of%2520the%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06671v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Instruction%20Tuning%20of%20Large%20Language%20Models%20For%0A%20%20Extreme%20Financial%20Numeral%20Labelling&entry.906535625=Subhendu%20Khatuya%20and%20Rajdeep%20Mukherjee%20and%20Akash%20Ghosh%20and%20Manjunath%20Hegde%20and%20Koustuv%20Dasgupta%20and%20Niloy%20Ganguly%20and%20Saptarshi%20Ghosh%20and%20Pawan%20Goyal&entry.1292438233=%20%20We%20study%20the%20problem%20of%20automatically%20annotating%20relevant%20numerals%20%28GAAP%0Ametrics%29%20occurring%20in%20the%20financial%20documents%20with%20their%20corresponding%20XBRL%0Atags.%20Different%20from%20prior%20works%2C%20we%20investigate%20the%20feasibility%20of%20solving%0Athis%20extreme%20classification%20problem%20using%20a%20generative%20paradigm%20through%0Ainstruction%20tuning%20of%20Large%20Language%20Models%20%28LLMs%29.%20To%20this%20end%2C%20we%20leverage%0Ametric%20metadata%20information%20to%20frame%20our%20target%20outputs%20while%20proposing%20a%0Aparameter%20efficient%20solution%20for%20the%20task%20using%20LoRA.%20We%20perform%20experiments%20on%0Atwo%20recently%20released%20financial%20numeric%20labeling%20datasets.%20Our%20proposed%20model%2C%0AFLAN-FinXC%2C%20achieves%20new%20state-of-the-art%20performances%20on%20both%20the%20datasets%2C%0Aoutperforming%20several%20strong%20baselines.%20We%20explain%20the%20better%20scores%20of%20our%0Aproposed%20model%20by%20demonstrating%20its%20capability%20for%20zero-shot%20as%20well%20as%20the%0Aleast%20frequently%20occurring%20tags.%20Also%2C%20even%20when%20we%20fail%20to%20predict%20the%20XBRL%0Atags%20correctly%2C%20our%20generated%20output%20has%20substantial%20overlap%20with%20the%0Aground-truth%20in%20majority%20of%20the%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06671v2&entry.124074799=Read"},
{"title": "Tailoring Instructions to Student's Learning Levels Boosts Knowledge\n  Distillation", "author": "Yuxin Ren and Zihan Zhong and Xingjian Shi and Yi Zhu and Chun Yuan and Mu Li", "abstract": "  It has been commonly observed that a teacher model with superior performance\ndoes not necessarily result in a stronger student, highlighting a discrepancy\nbetween current teacher training practices and effective knowledge transfer. In\norder to enhance the guidance of the teacher training process, we introduce the\nconcept of distillation influence to determine the impact of distillation from\neach training sample on the student's generalization ability. In this paper, we\npropose Learning Good Teacher Matters (LGTM), an efficient training technique\nfor incorporating distillation influence into the teacher's learning process.\nBy prioritizing samples that are likely to enhance the student's generalization\nability, our LGTM outperforms 10 common knowledge distillation baselines on 6\ntext classification tasks in the GLUE benchmark.\n", "link": "http://arxiv.org/abs/2305.09651v3", "date": "2024-05-15", "relevancy": 1.8824, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4623}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Instructions%20to%20Student%27s%20Learning%20Levels%20Boosts%20Knowledge%0A%20%20Distillation&body=Title%3A%20Tailoring%20Instructions%20to%20Student%27s%20Learning%20Levels%20Boosts%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Yuxin%20Ren%20and%20Zihan%20Zhong%20and%20Xingjian%20Shi%20and%20Yi%20Zhu%20and%20Chun%20Yuan%20and%20Mu%20Li%0AAbstract%3A%20%20%20It%20has%20been%20commonly%20observed%20that%20a%20teacher%20model%20with%20superior%20performance%0Adoes%20not%20necessarily%20result%20in%20a%20stronger%20student%2C%20highlighting%20a%20discrepancy%0Abetween%20current%20teacher%20training%20practices%20and%20effective%20knowledge%20transfer.%20In%0Aorder%20to%20enhance%20the%20guidance%20of%20the%20teacher%20training%20process%2C%20we%20introduce%20the%0Aconcept%20of%20distillation%20influence%20to%20determine%20the%20impact%20of%20distillation%20from%0Aeach%20training%20sample%20on%20the%20student%27s%20generalization%20ability.%20In%20this%20paper%2C%20we%0Apropose%20Learning%20Good%20Teacher%20Matters%20%28LGTM%29%2C%20an%20efficient%20training%20technique%0Afor%20incorporating%20distillation%20influence%20into%20the%20teacher%27s%20learning%20process.%0ABy%20prioritizing%20samples%20that%20are%20likely%20to%20enhance%20the%20student%27s%20generalization%0Aability%2C%20our%20LGTM%20outperforms%2010%20common%20knowledge%20distillation%20baselines%20on%206%0Atext%20classification%20tasks%20in%20the%20GLUE%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.09651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Instructions%2520to%2520Student%2527s%2520Learning%2520Levels%2520Boosts%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DYuxin%2520Ren%2520and%2520Zihan%2520Zhong%2520and%2520Xingjian%2520Shi%2520and%2520Yi%2520Zhu%2520and%2520Chun%2520Yuan%2520and%2520Mu%2520Li%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520commonly%2520observed%2520that%2520a%2520teacher%2520model%2520with%2520superior%2520performance%250Adoes%2520not%2520necessarily%2520result%2520in%2520a%2520stronger%2520student%252C%2520highlighting%2520a%2520discrepancy%250Abetween%2520current%2520teacher%2520training%2520practices%2520and%2520effective%2520knowledge%2520transfer.%2520In%250Aorder%2520to%2520enhance%2520the%2520guidance%2520of%2520the%2520teacher%2520training%2520process%252C%2520we%2520introduce%2520the%250Aconcept%2520of%2520distillation%2520influence%2520to%2520determine%2520the%2520impact%2520of%2520distillation%2520from%250Aeach%2520training%2520sample%2520on%2520the%2520student%2527s%2520generalization%2520ability.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Learning%2520Good%2520Teacher%2520Matters%2520%2528LGTM%2529%252C%2520an%2520efficient%2520training%2520technique%250Afor%2520incorporating%2520distillation%2520influence%2520into%2520the%2520teacher%2527s%2520learning%2520process.%250ABy%2520prioritizing%2520samples%2520that%2520are%2520likely%2520to%2520enhance%2520the%2520student%2527s%2520generalization%250Aability%252C%2520our%2520LGTM%2520outperforms%252010%2520common%2520knowledge%2520distillation%2520baselines%2520on%25206%250Atext%2520classification%2520tasks%2520in%2520the%2520GLUE%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.09651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Instructions%20to%20Student%27s%20Learning%20Levels%20Boosts%20Knowledge%0A%20%20Distillation&entry.906535625=Yuxin%20Ren%20and%20Zihan%20Zhong%20and%20Xingjian%20Shi%20and%20Yi%20Zhu%20and%20Chun%20Yuan%20and%20Mu%20Li&entry.1292438233=%20%20It%20has%20been%20commonly%20observed%20that%20a%20teacher%20model%20with%20superior%20performance%0Adoes%20not%20necessarily%20result%20in%20a%20stronger%20student%2C%20highlighting%20a%20discrepancy%0Abetween%20current%20teacher%20training%20practices%20and%20effective%20knowledge%20transfer.%20In%0Aorder%20to%20enhance%20the%20guidance%20of%20the%20teacher%20training%20process%2C%20we%20introduce%20the%0Aconcept%20of%20distillation%20influence%20to%20determine%20the%20impact%20of%20distillation%20from%0Aeach%20training%20sample%20on%20the%20student%27s%20generalization%20ability.%20In%20this%20paper%2C%20we%0Apropose%20Learning%20Good%20Teacher%20Matters%20%28LGTM%29%2C%20an%20efficient%20training%20technique%0Afor%20incorporating%20distillation%20influence%20into%20the%20teacher%27s%20learning%20process.%0ABy%20prioritizing%20samples%20that%20are%20likely%20to%20enhance%20the%20student%27s%20generalization%0Aability%2C%20our%20LGTM%20outperforms%2010%20common%20knowledge%20distillation%20baselines%20on%206%0Atext%20classification%20tasks%20in%20the%20GLUE%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.09651v3&entry.124074799=Read"},
{"title": "Extracting the gamma-ray source-count distribution below the Fermi-LAT\n  detection limit with deep learning", "author": "Aurelio Amerio and Alessandro Cuoco and Nicolao Fornengo", "abstract": "  We reconstruct the extra-galactic gamma-ray source-count distribution, or\n$dN/dS$, of resolved and unresolved sources by adopting machine learning\ntechniques. Specifically, we train a convolutional neural network on synthetic\n2-dimensional sky-maps, which are built by varying parameters of underlying\nsource-counts models and incorporate the Fermi-LAT instrumental response\nfunctions. The trained neural network is then applied to the Fermi-LAT data,\nfrom which we estimate the source count distribution down to flux levels a\nfactor of 50 below the Fermi-LAT threshold. We perform our analysis using 14\nyears of data collected in the $(1,10)$ GeV energy range. The results we obtain\nshow a source count distribution which, in the resolved regime, is in excellent\nagreement with the one derived from catalogued sources, and then extends as\n$dN/dS \\sim S^{-2}$ in the unresolved regime, down to fluxes of $5 \\cdot\n10^{-12}$ cm$^{-2}$ s$^{-1}$. The neural network architecture and the devised\nmethodology have the flexibility to enable future analyses to study the energy\ndependence of the source-count distribution.\n", "link": "http://arxiv.org/abs/2302.01947v2", "date": "2024-05-15", "relevancy": 1.8597, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4899}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.46}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20the%20gamma-ray%20source-count%20distribution%20below%20the%20Fermi-LAT%0A%20%20detection%20limit%20with%20deep%20learning&body=Title%3A%20Extracting%20the%20gamma-ray%20source-count%20distribution%20below%20the%20Fermi-LAT%0A%20%20detection%20limit%20with%20deep%20learning%0AAuthor%3A%20Aurelio%20Amerio%20and%20Alessandro%20Cuoco%20and%20Nicolao%20Fornengo%0AAbstract%3A%20%20%20We%20reconstruct%20the%20extra-galactic%20gamma-ray%20source-count%20distribution%2C%20or%0A%24dN/dS%24%2C%20of%20resolved%20and%20unresolved%20sources%20by%20adopting%20machine%20learning%0Atechniques.%20Specifically%2C%20we%20train%20a%20convolutional%20neural%20network%20on%20synthetic%0A2-dimensional%20sky-maps%2C%20which%20are%20built%20by%20varying%20parameters%20of%20underlying%0Asource-counts%20models%20and%20incorporate%20the%20Fermi-LAT%20instrumental%20response%0Afunctions.%20The%20trained%20neural%20network%20is%20then%20applied%20to%20the%20Fermi-LAT%20data%2C%0Afrom%20which%20we%20estimate%20the%20source%20count%20distribution%20down%20to%20flux%20levels%20a%0Afactor%20of%2050%20below%20the%20Fermi-LAT%20threshold.%20We%20perform%20our%20analysis%20using%2014%0Ayears%20of%20data%20collected%20in%20the%20%24%281%2C10%29%24%20GeV%20energy%20range.%20The%20results%20we%20obtain%0Ashow%20a%20source%20count%20distribution%20which%2C%20in%20the%20resolved%20regime%2C%20is%20in%20excellent%0Aagreement%20with%20the%20one%20derived%20from%20catalogued%20sources%2C%20and%20then%20extends%20as%0A%24dN/dS%20%5Csim%20S%5E%7B-2%7D%24%20in%20the%20unresolved%20regime%2C%20down%20to%20fluxes%20of%20%245%20%5Ccdot%0A10%5E%7B-12%7D%24%20cm%24%5E%7B-2%7D%24%20s%24%5E%7B-1%7D%24.%20The%20neural%20network%20architecture%20and%20the%20devised%0Amethodology%20have%20the%20flexibility%20to%20enable%20future%20analyses%20to%20study%20the%20energy%0Adependence%20of%20the%20source-count%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.01947v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520the%2520gamma-ray%2520source-count%2520distribution%2520below%2520the%2520Fermi-LAT%250A%2520%2520detection%2520limit%2520with%2520deep%2520learning%26entry.906535625%3DAurelio%2520Amerio%2520and%2520Alessandro%2520Cuoco%2520and%2520Nicolao%2520Fornengo%26entry.1292438233%3D%2520%2520We%2520reconstruct%2520the%2520extra-galactic%2520gamma-ray%2520source-count%2520distribution%252C%2520or%250A%2524dN/dS%2524%252C%2520of%2520resolved%2520and%2520unresolved%2520sources%2520by%2520adopting%2520machine%2520learning%250Atechniques.%2520Specifically%252C%2520we%2520train%2520a%2520convolutional%2520neural%2520network%2520on%2520synthetic%250A2-dimensional%2520sky-maps%252C%2520which%2520are%2520built%2520by%2520varying%2520parameters%2520of%2520underlying%250Asource-counts%2520models%2520and%2520incorporate%2520the%2520Fermi-LAT%2520instrumental%2520response%250Afunctions.%2520The%2520trained%2520neural%2520network%2520is%2520then%2520applied%2520to%2520the%2520Fermi-LAT%2520data%252C%250Afrom%2520which%2520we%2520estimate%2520the%2520source%2520count%2520distribution%2520down%2520to%2520flux%2520levels%2520a%250Afactor%2520of%252050%2520below%2520the%2520Fermi-LAT%2520threshold.%2520We%2520perform%2520our%2520analysis%2520using%252014%250Ayears%2520of%2520data%2520collected%2520in%2520the%2520%2524%25281%252C10%2529%2524%2520GeV%2520energy%2520range.%2520The%2520results%2520we%2520obtain%250Ashow%2520a%2520source%2520count%2520distribution%2520which%252C%2520in%2520the%2520resolved%2520regime%252C%2520is%2520in%2520excellent%250Aagreement%2520with%2520the%2520one%2520derived%2520from%2520catalogued%2520sources%252C%2520and%2520then%2520extends%2520as%250A%2524dN/dS%2520%255Csim%2520S%255E%257B-2%257D%2524%2520in%2520the%2520unresolved%2520regime%252C%2520down%2520to%2520fluxes%2520of%2520%25245%2520%255Ccdot%250A10%255E%257B-12%257D%2524%2520cm%2524%255E%257B-2%257D%2524%2520s%2524%255E%257B-1%257D%2524.%2520The%2520neural%2520network%2520architecture%2520and%2520the%2520devised%250Amethodology%2520have%2520the%2520flexibility%2520to%2520enable%2520future%2520analyses%2520to%2520study%2520the%2520energy%250Adependence%2520of%2520the%2520source-count%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.01947v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20the%20gamma-ray%20source-count%20distribution%20below%20the%20Fermi-LAT%0A%20%20detection%20limit%20with%20deep%20learning&entry.906535625=Aurelio%20Amerio%20and%20Alessandro%20Cuoco%20and%20Nicolao%20Fornengo&entry.1292438233=%20%20We%20reconstruct%20the%20extra-galactic%20gamma-ray%20source-count%20distribution%2C%20or%0A%24dN/dS%24%2C%20of%20resolved%20and%20unresolved%20sources%20by%20adopting%20machine%20learning%0Atechniques.%20Specifically%2C%20we%20train%20a%20convolutional%20neural%20network%20on%20synthetic%0A2-dimensional%20sky-maps%2C%20which%20are%20built%20by%20varying%20parameters%20of%20underlying%0Asource-counts%20models%20and%20incorporate%20the%20Fermi-LAT%20instrumental%20response%0Afunctions.%20The%20trained%20neural%20network%20is%20then%20applied%20to%20the%20Fermi-LAT%20data%2C%0Afrom%20which%20we%20estimate%20the%20source%20count%20distribution%20down%20to%20flux%20levels%20a%0Afactor%20of%2050%20below%20the%20Fermi-LAT%20threshold.%20We%20perform%20our%20analysis%20using%2014%0Ayears%20of%20data%20collected%20in%20the%20%24%281%2C10%29%24%20GeV%20energy%20range.%20The%20results%20we%20obtain%0Ashow%20a%20source%20count%20distribution%20which%2C%20in%20the%20resolved%20regime%2C%20is%20in%20excellent%0Aagreement%20with%20the%20one%20derived%20from%20catalogued%20sources%2C%20and%20then%20extends%20as%0A%24dN/dS%20%5Csim%20S%5E%7B-2%7D%24%20in%20the%20unresolved%20regime%2C%20down%20to%20fluxes%20of%20%245%20%5Ccdot%0A10%5E%7B-12%7D%24%20cm%24%5E%7B-2%7D%24%20s%24%5E%7B-1%7D%24.%20The%20neural%20network%20architecture%20and%20the%20devised%0Amethodology%20have%20the%20flexibility%20to%20enable%20future%20analyses%20to%20study%20the%20energy%0Adependence%20of%20the%20source-count%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01947v2&entry.124074799=Read"},
{"title": "TimeX++: Learning Time-Series Explanations with Information Bottleneck", "author": "Zichuan Liu and Tianchun Wang and Jimeng Shi and Xu Zheng and Zhuomin Chen and Lei Song and Wenqian Dong and Jayantha Obeysekera and Farhad Shirani and Dongsheng Luo", "abstract": "  Explaining deep learning models operating on time series data is crucial in\nvarious applications of interest which require interpretable and transparent\ninsights from time series signals. In this work, we investigate this problem\nfrom an information theoretic perspective and show that most existing measures\nof explainability may suffer from trivial solutions and distributional shift\nissues. To address these issues, we introduce a simple yet practical objective\nfunction for time series explainable learning. The design of the objective\nfunction builds upon the principle of information bottleneck (IB), and modifies\nthe IB objective function to avoid trivial solutions and distributional shift\nissues. We further present TimeX++, a novel explanation framework that\nleverages a parametric network to produce explanation-embedded instances that\nare both in-distributed and label-preserving. We evaluate TimeX++ on both\nsynthetic and real-world datasets comparing its performance against leading\nbaselines, and validate its practical efficacy through case studies in a\nreal-world environmental application. Quantitative and qualitative evaluations\nshow that TimeX++ outperforms baselines across all datasets, demonstrating a\nsubstantial improvement in explanation quality for time series data. The source\ncode is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}.\n", "link": "http://arxiv.org/abs/2405.09308v1", "date": "2024-05-15", "relevancy": 1.8555, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4775}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4672}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeX%2B%2B%3A%20Learning%20Time-Series%20Explanations%20with%20Information%20Bottleneck&body=Title%3A%20TimeX%2B%2B%3A%20Learning%20Time-Series%20Explanations%20with%20Information%20Bottleneck%0AAuthor%3A%20Zichuan%20Liu%20and%20Tianchun%20Wang%20and%20Jimeng%20Shi%20and%20Xu%20Zheng%20and%20Zhuomin%20Chen%20and%20Lei%20Song%20and%20Wenqian%20Dong%20and%20Jayantha%20Obeysekera%20and%20Farhad%20Shirani%20and%20Dongsheng%20Luo%0AAbstract%3A%20%20%20Explaining%20deep%20learning%20models%20operating%20on%20time%20series%20data%20is%20crucial%20in%0Avarious%20applications%20of%20interest%20which%20require%20interpretable%20and%20transparent%0Ainsights%20from%20time%20series%20signals.%20In%20this%20work%2C%20we%20investigate%20this%20problem%0Afrom%20an%20information%20theoretic%20perspective%20and%20show%20that%20most%20existing%20measures%0Aof%20explainability%20may%20suffer%20from%20trivial%20solutions%20and%20distributional%20shift%0Aissues.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20simple%20yet%20practical%20objective%0Afunction%20for%20time%20series%20explainable%20learning.%20The%20design%20of%20the%20objective%0Afunction%20builds%20upon%20the%20principle%20of%20information%20bottleneck%20%28IB%29%2C%20and%20modifies%0Athe%20IB%20objective%20function%20to%20avoid%20trivial%20solutions%20and%20distributional%20shift%0Aissues.%20We%20further%20present%20TimeX%2B%2B%2C%20a%20novel%20explanation%20framework%20that%0Aleverages%20a%20parametric%20network%20to%20produce%20explanation-embedded%20instances%20that%0Aare%20both%20in-distributed%20and%20label-preserving.%20We%20evaluate%20TimeX%2B%2B%20on%20both%0Asynthetic%20and%20real-world%20datasets%20comparing%20its%20performance%20against%20leading%0Abaselines%2C%20and%20validate%20its%20practical%20efficacy%20through%20case%20studies%20in%20a%0Areal-world%20environmental%20application.%20Quantitative%20and%20qualitative%20evaluations%0Ashow%20that%20TimeX%2B%2B%20outperforms%20baselines%20across%20all%20datasets%2C%20demonstrating%20a%0Asubstantial%20improvement%20in%20explanation%20quality%20for%20time%20series%20data.%20The%20source%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/zichuan-liu/TimeXplusplus%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeX%252B%252B%253A%2520Learning%2520Time-Series%2520Explanations%2520with%2520Information%2520Bottleneck%26entry.906535625%3DZichuan%2520Liu%2520and%2520Tianchun%2520Wang%2520and%2520Jimeng%2520Shi%2520and%2520Xu%2520Zheng%2520and%2520Zhuomin%2520Chen%2520and%2520Lei%2520Song%2520and%2520Wenqian%2520Dong%2520and%2520Jayantha%2520Obeysekera%2520and%2520Farhad%2520Shirani%2520and%2520Dongsheng%2520Luo%26entry.1292438233%3D%2520%2520Explaining%2520deep%2520learning%2520models%2520operating%2520on%2520time%2520series%2520data%2520is%2520crucial%2520in%250Avarious%2520applications%2520of%2520interest%2520which%2520require%2520interpretable%2520and%2520transparent%250Ainsights%2520from%2520time%2520series%2520signals.%2520In%2520this%2520work%252C%2520we%2520investigate%2520this%2520problem%250Afrom%2520an%2520information%2520theoretic%2520perspective%2520and%2520show%2520that%2520most%2520existing%2520measures%250Aof%2520explainability%2520may%2520suffer%2520from%2520trivial%2520solutions%2520and%2520distributional%2520shift%250Aissues.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520practical%2520objective%250Afunction%2520for%2520time%2520series%2520explainable%2520learning.%2520The%2520design%2520of%2520the%2520objective%250Afunction%2520builds%2520upon%2520the%2520principle%2520of%2520information%2520bottleneck%2520%2528IB%2529%252C%2520and%2520modifies%250Athe%2520IB%2520objective%2520function%2520to%2520avoid%2520trivial%2520solutions%2520and%2520distributional%2520shift%250Aissues.%2520We%2520further%2520present%2520TimeX%252B%252B%252C%2520a%2520novel%2520explanation%2520framework%2520that%250Aleverages%2520a%2520parametric%2520network%2520to%2520produce%2520explanation-embedded%2520instances%2520that%250Aare%2520both%2520in-distributed%2520and%2520label-preserving.%2520We%2520evaluate%2520TimeX%252B%252B%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets%2520comparing%2520its%2520performance%2520against%2520leading%250Abaselines%252C%2520and%2520validate%2520its%2520practical%2520efficacy%2520through%2520case%2520studies%2520in%2520a%250Areal-world%2520environmental%2520application.%2520Quantitative%2520and%2520qualitative%2520evaluations%250Ashow%2520that%2520TimeX%252B%252B%2520outperforms%2520baselines%2520across%2520all%2520datasets%252C%2520demonstrating%2520a%250Asubstantial%2520improvement%2520in%2520explanation%2520quality%2520for%2520time%2520series%2520data.%2520The%2520source%250Acode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/zichuan-liu/TimeXplusplus%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeX%2B%2B%3A%20Learning%20Time-Series%20Explanations%20with%20Information%20Bottleneck&entry.906535625=Zichuan%20Liu%20and%20Tianchun%20Wang%20and%20Jimeng%20Shi%20and%20Xu%20Zheng%20and%20Zhuomin%20Chen%20and%20Lei%20Song%20and%20Wenqian%20Dong%20and%20Jayantha%20Obeysekera%20and%20Farhad%20Shirani%20and%20Dongsheng%20Luo&entry.1292438233=%20%20Explaining%20deep%20learning%20models%20operating%20on%20time%20series%20data%20is%20crucial%20in%0Avarious%20applications%20of%20interest%20which%20require%20interpretable%20and%20transparent%0Ainsights%20from%20time%20series%20signals.%20In%20this%20work%2C%20we%20investigate%20this%20problem%0Afrom%20an%20information%20theoretic%20perspective%20and%20show%20that%20most%20existing%20measures%0Aof%20explainability%20may%20suffer%20from%20trivial%20solutions%20and%20distributional%20shift%0Aissues.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20simple%20yet%20practical%20objective%0Afunction%20for%20time%20series%20explainable%20learning.%20The%20design%20of%20the%20objective%0Afunction%20builds%20upon%20the%20principle%20of%20information%20bottleneck%20%28IB%29%2C%20and%20modifies%0Athe%20IB%20objective%20function%20to%20avoid%20trivial%20solutions%20and%20distributional%20shift%0Aissues.%20We%20further%20present%20TimeX%2B%2B%2C%20a%20novel%20explanation%20framework%20that%0Aleverages%20a%20parametric%20network%20to%20produce%20explanation-embedded%20instances%20that%0Aare%20both%20in-distributed%20and%20label-preserving.%20We%20evaluate%20TimeX%2B%2B%20on%20both%0Asynthetic%20and%20real-world%20datasets%20comparing%20its%20performance%20against%20leading%0Abaselines%2C%20and%20validate%20its%20practical%20efficacy%20through%20case%20studies%20in%20a%0Areal-world%20environmental%20application.%20Quantitative%20and%20qualitative%20evaluations%0Ashow%20that%20TimeX%2B%2B%20outperforms%20baselines%20across%20all%20datasets%2C%20demonstrating%20a%0Asubstantial%20improvement%20in%20explanation%20quality%20for%20time%20series%20data.%20The%20source%0Acode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/zichuan-liu/TimeXplusplus%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09308v1&entry.124074799=Read"},
{"title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer\n  Architectures for Cross-Language Structural Priming", "author": "Bushi Xiao and Chao Gao and Demi Zhang", "abstract": "  This study evaluates the performance of Recurrent Neural Network (RNN) and\nTransformer in replicating cross-language structural priming: a key indicator\nof abstract grammatical representations in human language processing. Focusing\non Chinese-English priming, which involves two typologically distinct\nlanguages, we examine how these models handle the robust phenomenon of\nstructural priming, where exposure to a particular sentence structure increases\nthe likelihood of selecting a similar structure subsequently. Additionally, we\nutilize large language models (LLM) to measure the cross-lingual structural\npriming effect. Our findings indicate that Transformer outperform RNN in\ngenerating primed sentence structures, challenging the conventional belief that\nhuman sentence processing primarily involves recurrent and immediate processing\nand suggesting a role for cue-based retrieval mechanisms. Overall, this work\ncontributes to our understanding of how computational models may reflect human\ncognitive processes in multilingual contexts.\n", "link": "http://arxiv.org/abs/2405.09508v1", "date": "2024-05-15", "relevancy": 1.8518, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4844}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Bilingual%20Sentence%20Processing%3A%20Evaluating%20RNN%20and%20Transformer%0A%20%20Architectures%20for%20Cross-Language%20Structural%20Priming&body=Title%3A%20Modeling%20Bilingual%20Sentence%20Processing%3A%20Evaluating%20RNN%20and%20Transformer%0A%20%20Architectures%20for%20Cross-Language%20Structural%20Priming%0AAuthor%3A%20Bushi%20Xiao%20and%20Chao%20Gao%20and%20Demi%20Zhang%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20performance%20of%20Recurrent%20Neural%20Network%20%28RNN%29%20and%0ATransformer%20in%20replicating%20cross-language%20structural%20priming%3A%20a%20key%20indicator%0Aof%20abstract%20grammatical%20representations%20in%20human%20language%20processing.%20Focusing%0Aon%20Chinese-English%20priming%2C%20which%20involves%20two%20typologically%20distinct%0Alanguages%2C%20we%20examine%20how%20these%20models%20handle%20the%20robust%20phenomenon%20of%0Astructural%20priming%2C%20where%20exposure%20to%20a%20particular%20sentence%20structure%20increases%0Athe%20likelihood%20of%20selecting%20a%20similar%20structure%20subsequently.%20Additionally%2C%20we%0Autilize%20large%20language%20models%20%28LLM%29%20to%20measure%20the%20cross-lingual%20structural%0Apriming%20effect.%20Our%20findings%20indicate%20that%20Transformer%20outperform%20RNN%20in%0Agenerating%20primed%20sentence%20structures%2C%20challenging%20the%20conventional%20belief%20that%0Ahuman%20sentence%20processing%20primarily%20involves%20recurrent%20and%20immediate%20processing%0Aand%20suggesting%20a%20role%20for%20cue-based%20retrieval%20mechanisms.%20Overall%2C%20this%20work%0Acontributes%20to%20our%20understanding%20of%20how%20computational%20models%20may%20reflect%20human%0Acognitive%20processes%20in%20multilingual%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Bilingual%2520Sentence%2520Processing%253A%2520Evaluating%2520RNN%2520and%2520Transformer%250A%2520%2520Architectures%2520for%2520Cross-Language%2520Structural%2520Priming%26entry.906535625%3DBushi%2520Xiao%2520and%2520Chao%2520Gao%2520and%2520Demi%2520Zhang%26entry.1292438233%3D%2520%2520This%2520study%2520evaluates%2520the%2520performance%2520of%2520Recurrent%2520Neural%2520Network%2520%2528RNN%2529%2520and%250ATransformer%2520in%2520replicating%2520cross-language%2520structural%2520priming%253A%2520a%2520key%2520indicator%250Aof%2520abstract%2520grammatical%2520representations%2520in%2520human%2520language%2520processing.%2520Focusing%250Aon%2520Chinese-English%2520priming%252C%2520which%2520involves%2520two%2520typologically%2520distinct%250Alanguages%252C%2520we%2520examine%2520how%2520these%2520models%2520handle%2520the%2520robust%2520phenomenon%2520of%250Astructural%2520priming%252C%2520where%2520exposure%2520to%2520a%2520particular%2520sentence%2520structure%2520increases%250Athe%2520likelihood%2520of%2520selecting%2520a%2520similar%2520structure%2520subsequently.%2520Additionally%252C%2520we%250Autilize%2520large%2520language%2520models%2520%2528LLM%2529%2520to%2520measure%2520the%2520cross-lingual%2520structural%250Apriming%2520effect.%2520Our%2520findings%2520indicate%2520that%2520Transformer%2520outperform%2520RNN%2520in%250Agenerating%2520primed%2520sentence%2520structures%252C%2520challenging%2520the%2520conventional%2520belief%2520that%250Ahuman%2520sentence%2520processing%2520primarily%2520involves%2520recurrent%2520and%2520immediate%2520processing%250Aand%2520suggesting%2520a%2520role%2520for%2520cue-based%2520retrieval%2520mechanisms.%2520Overall%252C%2520this%2520work%250Acontributes%2520to%2520our%2520understanding%2520of%2520how%2520computational%2520models%2520may%2520reflect%2520human%250Acognitive%2520processes%2520in%2520multilingual%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Bilingual%20Sentence%20Processing%3A%20Evaluating%20RNN%20and%20Transformer%0A%20%20Architectures%20for%20Cross-Language%20Structural%20Priming&entry.906535625=Bushi%20Xiao%20and%20Chao%20Gao%20and%20Demi%20Zhang&entry.1292438233=%20%20This%20study%20evaluates%20the%20performance%20of%20Recurrent%20Neural%20Network%20%28RNN%29%20and%0ATransformer%20in%20replicating%20cross-language%20structural%20priming%3A%20a%20key%20indicator%0Aof%20abstract%20grammatical%20representations%20in%20human%20language%20processing.%20Focusing%0Aon%20Chinese-English%20priming%2C%20which%20involves%20two%20typologically%20distinct%0Alanguages%2C%20we%20examine%20how%20these%20models%20handle%20the%20robust%20phenomenon%20of%0Astructural%20priming%2C%20where%20exposure%20to%20a%20particular%20sentence%20structure%20increases%0Athe%20likelihood%20of%20selecting%20a%20similar%20structure%20subsequently.%20Additionally%2C%20we%0Autilize%20large%20language%20models%20%28LLM%29%20to%20measure%20the%20cross-lingual%20structural%0Apriming%20effect.%20Our%20findings%20indicate%20that%20Transformer%20outperform%20RNN%20in%0Agenerating%20primed%20sentence%20structures%2C%20challenging%20the%20conventional%20belief%20that%0Ahuman%20sentence%20processing%20primarily%20involves%20recurrent%20and%20immediate%20processing%0Aand%20suggesting%20a%20role%20for%20cue-based%20retrieval%20mechanisms.%20Overall%2C%20this%20work%0Acontributes%20to%20our%20understanding%20of%20how%20computational%20models%20may%20reflect%20human%0Acognitive%20processes%20in%20multilingual%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09508v1&entry.124074799=Read"},
{"title": "Invariant Risk Minimization Is A Total Variation Model", "author": "Zhao-Rong Lai and Weiwen Wang", "abstract": "  Invariant risk minimization (IRM) is an arising approach to generalize\ninvariant features to different environments in machine learning. While most\nrelated works focus on new IRM settings or new application scenarios, the\nmathematical essence of IRM remains to be properly explained. We verify that\nIRM is essentially a total variation based on $L^2$ norm (TV-$\\ell_2$) of the\nlearning risk with respect to the classifier variable. Moreover, we propose a\nnovel IRM framework based on the TV-$\\ell_1$ model. It not only expands the\nclasses of functions that can be used as the learning risk, but also has robust\nperformance in denoising and invariant feature preservation based on the coarea\nformula. We also illustrate some requirements for IRM-TV-$\\ell_1$ to achieve\nout-of-distribution generalization. Experimental results show that the proposed\nframework achieves competitive performance in several benchmark machine\nlearning scenarios.\n", "link": "http://arxiv.org/abs/2405.01389v3", "date": "2024-05-15", "relevancy": 1.8494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4653}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4622}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&body=Title%3A%20Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model%0AAuthor%3A%20Zhao-Rong%20Lai%20and%20Weiwen%20Wang%0AAbstract%3A%20%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvariant%2520Risk%2520Minimization%2520Is%2520A%2520Total%2520Variation%2520Model%26entry.906535625%3DZhao-Rong%2520Lai%2520and%2520Weiwen%2520Wang%26entry.1292438233%3D%2520%2520Invariant%2520risk%2520minimization%2520%2528IRM%2529%2520is%2520an%2520arising%2520approach%2520to%2520generalize%250Ainvariant%2520features%2520to%2520different%2520environments%2520in%2520machine%2520learning.%2520While%2520most%250Arelated%2520works%2520focus%2520on%2520new%2520IRM%2520settings%2520or%2520new%2520application%2520scenarios%252C%2520the%250Amathematical%2520essence%2520of%2520IRM%2520remains%2520to%2520be%2520properly%2520explained.%2520We%2520verify%2520that%250AIRM%2520is%2520essentially%2520a%2520total%2520variation%2520based%2520on%2520%2524L%255E2%2524%2520norm%2520%2528TV-%2524%255Cell_2%2524%2529%2520of%2520the%250Alearning%2520risk%2520with%2520respect%2520to%2520the%2520classifier%2520variable.%2520Moreover%252C%2520we%2520propose%2520a%250Anovel%2520IRM%2520framework%2520based%2520on%2520the%2520TV-%2524%255Cell_1%2524%2520model.%2520It%2520not%2520only%2520expands%2520the%250Aclasses%2520of%2520functions%2520that%2520can%2520be%2520used%2520as%2520the%2520learning%2520risk%252C%2520but%2520also%2520has%2520robust%250Aperformance%2520in%2520denoising%2520and%2520invariant%2520feature%2520preservation%2520based%2520on%2520the%2520coarea%250Aformula.%2520We%2520also%2520illustrate%2520some%2520requirements%2520for%2520IRM-TV-%2524%255Cell_1%2524%2520to%2520achieve%250Aout-of-distribution%2520generalization.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%250Aframework%2520achieves%2520competitive%2520performance%2520in%2520several%2520benchmark%2520machine%250Alearning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invariant%20Risk%20Minimization%20Is%20A%20Total%20Variation%20Model&entry.906535625=Zhao-Rong%20Lai%20and%20Weiwen%20Wang&entry.1292438233=%20%20Invariant%20risk%20minimization%20%28IRM%29%20is%20an%20arising%20approach%20to%20generalize%0Ainvariant%20features%20to%20different%20environments%20in%20machine%20learning.%20While%20most%0Arelated%20works%20focus%20on%20new%20IRM%20settings%20or%20new%20application%20scenarios%2C%20the%0Amathematical%20essence%20of%20IRM%20remains%20to%20be%20properly%20explained.%20We%20verify%20that%0AIRM%20is%20essentially%20a%20total%20variation%20based%20on%20%24L%5E2%24%20norm%20%28TV-%24%5Cell_2%24%29%20of%20the%0Alearning%20risk%20with%20respect%20to%20the%20classifier%20variable.%20Moreover%2C%20we%20propose%20a%0Anovel%20IRM%20framework%20based%20on%20the%20TV-%24%5Cell_1%24%20model.%20It%20not%20only%20expands%20the%0Aclasses%20of%20functions%20that%20can%20be%20used%20as%20the%20learning%20risk%2C%20but%20also%20has%20robust%0Aperformance%20in%20denoising%20and%20invariant%20feature%20preservation%20based%20on%20the%20coarea%0Aformula.%20We%20also%20illustrate%20some%20requirements%20for%20IRM-TV-%24%5Cell_1%24%20to%20achieve%0Aout-of-distribution%20generalization.%20Experimental%20results%20show%20that%20the%20proposed%0Aframework%20achieves%20competitive%20performance%20in%20several%20benchmark%20machine%0Alearning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01389v3&entry.124074799=Read"},
{"title": "AirIMU: Learning Uncertainty Propagation for Inertial Odometry", "author": "Yuheng Qiu and Chen Wang and Can Xu and Yutian Chen and Xunfei Zhou and Youjie Xia and Sebastian Scherer", "abstract": "  Inertial odometry (IO) using strap-down inertial measurement units (IMUs) is\ncritical in many robotic applications where precise orientation and position\ntracking are essential. Prior kinematic motion model-based IO methods often use\na simplified linearized IMU noise model and thus usually encounter difficulties\nin modeling non-deterministic errors arising from environmental disturbances\nand mechanical defects. In contrast, data-driven IO methods struggle to\naccurately model the sensor motions, often leading to generalizability and\ninteroperability issues. To address these challenges, we present AirIMU, a\nhybrid approach to estimate the uncertainty, especially the non-deterministic\nerrors, by data-driven methods and increase the generalization abilities using\nmodel-based methods. We demonstrate the adaptability of AirIMU using a full\nspectrum of IMUs, from low-cost automotive grades to high-end navigation\ngrades. We also validate its effectiveness on various platforms, including\nhand-held devices, vehicles, and a helicopter that covers a trajectory of 262\nkilometers. In the ablation study, we validate the effectiveness of our learned\nuncertainty in an IMU-GPS pose graph optimization experiment, achieving a\n31.6\\% improvement in accuracy. Experiments demonstrate that jointly training\nthe IMU noise correction and uncertainty estimation synergistically benefits\nboth tasks.\n", "link": "http://arxiv.org/abs/2310.04874v4", "date": "2024-05-15", "relevancy": 1.8436, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6105}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirIMU%3A%20Learning%20Uncertainty%20Propagation%20for%20Inertial%20Odometry&body=Title%3A%20AirIMU%3A%20Learning%20Uncertainty%20Propagation%20for%20Inertial%20Odometry%0AAuthor%3A%20Yuheng%20Qiu%20and%20Chen%20Wang%20and%20Can%20Xu%20and%20Yutian%20Chen%20and%20Xunfei%20Zhou%20and%20Youjie%20Xia%20and%20Sebastian%20Scherer%0AAbstract%3A%20%20%20Inertial%20odometry%20%28IO%29%20using%20strap-down%20inertial%20measurement%20units%20%28IMUs%29%20is%0Acritical%20in%20many%20robotic%20applications%20where%20precise%20orientation%20and%20position%0Atracking%20are%20essential.%20Prior%20kinematic%20motion%20model-based%20IO%20methods%20often%20use%0Aa%20simplified%20linearized%20IMU%20noise%20model%20and%20thus%20usually%20encounter%20difficulties%0Ain%20modeling%20non-deterministic%20errors%20arising%20from%20environmental%20disturbances%0Aand%20mechanical%20defects.%20In%20contrast%2C%20data-driven%20IO%20methods%20struggle%20to%0Aaccurately%20model%20the%20sensor%20motions%2C%20often%20leading%20to%20generalizability%20and%0Ainteroperability%20issues.%20To%20address%20these%20challenges%2C%20we%20present%20AirIMU%2C%20a%0Ahybrid%20approach%20to%20estimate%20the%20uncertainty%2C%20especially%20the%20non-deterministic%0Aerrors%2C%20by%20data-driven%20methods%20and%20increase%20the%20generalization%20abilities%20using%0Amodel-based%20methods.%20We%20demonstrate%20the%20adaptability%20of%20AirIMU%20using%20a%20full%0Aspectrum%20of%20IMUs%2C%20from%20low-cost%20automotive%20grades%20to%20high-end%20navigation%0Agrades.%20We%20also%20validate%20its%20effectiveness%20on%20various%20platforms%2C%20including%0Ahand-held%20devices%2C%20vehicles%2C%20and%20a%20helicopter%20that%20covers%20a%20trajectory%20of%20262%0Akilometers.%20In%20the%20ablation%20study%2C%20we%20validate%20the%20effectiveness%20of%20our%20learned%0Auncertainty%20in%20an%20IMU-GPS%20pose%20graph%20optimization%20experiment%2C%20achieving%20a%0A31.6%5C%25%20improvement%20in%20accuracy.%20Experiments%20demonstrate%20that%20jointly%20training%0Athe%20IMU%20noise%20correction%20and%20uncertainty%20estimation%20synergistically%20benefits%0Aboth%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04874v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirIMU%253A%2520Learning%2520Uncertainty%2520Propagation%2520for%2520Inertial%2520Odometry%26entry.906535625%3DYuheng%2520Qiu%2520and%2520Chen%2520Wang%2520and%2520Can%2520Xu%2520and%2520Yutian%2520Chen%2520and%2520Xunfei%2520Zhou%2520and%2520Youjie%2520Xia%2520and%2520Sebastian%2520Scherer%26entry.1292438233%3D%2520%2520Inertial%2520odometry%2520%2528IO%2529%2520using%2520strap-down%2520inertial%2520measurement%2520units%2520%2528IMUs%2529%2520is%250Acritical%2520in%2520many%2520robotic%2520applications%2520where%2520precise%2520orientation%2520and%2520position%250Atracking%2520are%2520essential.%2520Prior%2520kinematic%2520motion%2520model-based%2520IO%2520methods%2520often%2520use%250Aa%2520simplified%2520linearized%2520IMU%2520noise%2520model%2520and%2520thus%2520usually%2520encounter%2520difficulties%250Ain%2520modeling%2520non-deterministic%2520errors%2520arising%2520from%2520environmental%2520disturbances%250Aand%2520mechanical%2520defects.%2520In%2520contrast%252C%2520data-driven%2520IO%2520methods%2520struggle%2520to%250Aaccurately%2520model%2520the%2520sensor%2520motions%252C%2520often%2520leading%2520to%2520generalizability%2520and%250Ainteroperability%2520issues.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520AirIMU%252C%2520a%250Ahybrid%2520approach%2520to%2520estimate%2520the%2520uncertainty%252C%2520especially%2520the%2520non-deterministic%250Aerrors%252C%2520by%2520data-driven%2520methods%2520and%2520increase%2520the%2520generalization%2520abilities%2520using%250Amodel-based%2520methods.%2520We%2520demonstrate%2520the%2520adaptability%2520of%2520AirIMU%2520using%2520a%2520full%250Aspectrum%2520of%2520IMUs%252C%2520from%2520low-cost%2520automotive%2520grades%2520to%2520high-end%2520navigation%250Agrades.%2520We%2520also%2520validate%2520its%2520effectiveness%2520on%2520various%2520platforms%252C%2520including%250Ahand-held%2520devices%252C%2520vehicles%252C%2520and%2520a%2520helicopter%2520that%2520covers%2520a%2520trajectory%2520of%2520262%250Akilometers.%2520In%2520the%2520ablation%2520study%252C%2520we%2520validate%2520the%2520effectiveness%2520of%2520our%2520learned%250Auncertainty%2520in%2520an%2520IMU-GPS%2520pose%2520graph%2520optimization%2520experiment%252C%2520achieving%2520a%250A31.6%255C%2525%2520improvement%2520in%2520accuracy.%2520Experiments%2520demonstrate%2520that%2520jointly%2520training%250Athe%2520IMU%2520noise%2520correction%2520and%2520uncertainty%2520estimation%2520synergistically%2520benefits%250Aboth%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04874v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirIMU%3A%20Learning%20Uncertainty%20Propagation%20for%20Inertial%20Odometry&entry.906535625=Yuheng%20Qiu%20and%20Chen%20Wang%20and%20Can%20Xu%20and%20Yutian%20Chen%20and%20Xunfei%20Zhou%20and%20Youjie%20Xia%20and%20Sebastian%20Scherer&entry.1292438233=%20%20Inertial%20odometry%20%28IO%29%20using%20strap-down%20inertial%20measurement%20units%20%28IMUs%29%20is%0Acritical%20in%20many%20robotic%20applications%20where%20precise%20orientation%20and%20position%0Atracking%20are%20essential.%20Prior%20kinematic%20motion%20model-based%20IO%20methods%20often%20use%0Aa%20simplified%20linearized%20IMU%20noise%20model%20and%20thus%20usually%20encounter%20difficulties%0Ain%20modeling%20non-deterministic%20errors%20arising%20from%20environmental%20disturbances%0Aand%20mechanical%20defects.%20In%20contrast%2C%20data-driven%20IO%20methods%20struggle%20to%0Aaccurately%20model%20the%20sensor%20motions%2C%20often%20leading%20to%20generalizability%20and%0Ainteroperability%20issues.%20To%20address%20these%20challenges%2C%20we%20present%20AirIMU%2C%20a%0Ahybrid%20approach%20to%20estimate%20the%20uncertainty%2C%20especially%20the%20non-deterministic%0Aerrors%2C%20by%20data-driven%20methods%20and%20increase%20the%20generalization%20abilities%20using%0Amodel-based%20methods.%20We%20demonstrate%20the%20adaptability%20of%20AirIMU%20using%20a%20full%0Aspectrum%20of%20IMUs%2C%20from%20low-cost%20automotive%20grades%20to%20high-end%20navigation%0Agrades.%20We%20also%20validate%20its%20effectiveness%20on%20various%20platforms%2C%20including%0Ahand-held%20devices%2C%20vehicles%2C%20and%20a%20helicopter%20that%20covers%20a%20trajectory%20of%20262%0Akilometers.%20In%20the%20ablation%20study%2C%20we%20validate%20the%20effectiveness%20of%20our%20learned%0Auncertainty%20in%20an%20IMU-GPS%20pose%20graph%20optimization%20experiment%2C%20achieving%20a%0A31.6%5C%25%20improvement%20in%20accuracy.%20Experiments%20demonstrate%20that%20jointly%20training%0Athe%20IMU%20noise%20correction%20and%20uncertainty%20estimation%20synergistically%20benefits%0Aboth%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04874v4&entry.124074799=Read"},
{"title": "Lumina-T2X: Transforming Text into Any Modality, Resolution, and\n  Duration via Flow-based Large Diffusion Transformers", "author": "Peng Gao and Le Zhuo and Dongyang Liu and Ruoyi Du and Xu Luo and Longtian Qiu and Yuhang Zhang and Chen Lin and Rongjie Huang and Shijie Geng and Renrui Zhang and Junlin Xi and Wenqi Shao and Zhengkai Jiang and Tianshuo Yang and Weicai Ye and He Tong and Jingwen He and Yu Qiao and Hongsheng Li", "abstract": "  Sora unveils the potential of scaling Diffusion Transformer for generating\nphotorealistic images and videos at arbitrary resolutions, aspect ratios, and\ndurations, yet it still lacks sufficient implementation details. In this\ntechnical report, we introduce the Lumina-T2X family - a series of Flow-based\nLarge Diffusion Transformers (Flag-DiT) equipped with zero-initialized\nattention, as a unified framework designed to transform noise into images,\nvideos, multi-view 3D objects, and audio clips conditioned on text\ninstructions. By tokenizing the latent spatial-temporal space and incorporating\nlearnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X\nseamlessly unifies the representations of different modalities across various\nspatial-temporal resolutions. This unified approach enables training within a\nsingle framework for different modalities and allows for flexible generation of\nmultimodal data at any resolution, aspect ratio, and length during inference.\nAdvanced techniques like RoPE, RMSNorm, and flow matching enhance the\nstability, flexibility, and scalability of Flag-DiT, enabling models of\nLumina-T2X to scale up to 7 billion parameters and extend the context window to\n128K tokens. This is particularly beneficial for creating ultra-high-definition\nimages with our Lumina-T2I model and long 720p videos with our Lumina-T2V\nmodel. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,\nrequires only 35% of the training computational costs of a\n600-million-parameter naive DiT. Our further comprehensive analysis underscores\nLumina-T2X's preliminary capability in resolution extrapolation,\nhigh-resolution editing, generating consistent 3D views, and synthesizing\nvideos with seamless transitions. We expect that the open-sourcing of\nLumina-T2X will further foster creativity, transparency, and diversity in the\ngenerative AI community.\n", "link": "http://arxiv.org/abs/2405.05945v2", "date": "2024-05-15", "relevancy": 1.8396, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6076}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers&body=Title%3A%20Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers%0AAuthor%3A%20Peng%20Gao%20and%20Le%20Zhuo%20and%20Dongyang%20Liu%20and%20Ruoyi%20Du%20and%20Xu%20Luo%20and%20Longtian%20Qiu%20and%20Yuhang%20Zhang%20and%20Chen%20Lin%20and%20Rongjie%20Huang%20and%20Shijie%20Geng%20and%20Renrui%20Zhang%20and%20Junlin%20Xi%20and%20Wenqi%20Shao%20and%20Zhengkai%20Jiang%20and%20Tianshuo%20Yang%20and%20Weicai%20Ye%20and%20He%20Tong%20and%20Jingwen%20He%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Sora%20unveils%20the%20potential%20of%20scaling%20Diffusion%20Transformer%20for%20generating%0Aphotorealistic%20images%20and%20videos%20at%20arbitrary%20resolutions%2C%20aspect%20ratios%2C%20and%0Adurations%2C%20yet%20it%20still%20lacks%20sufficient%20implementation%20details.%20In%20this%0Atechnical%20report%2C%20we%20introduce%20the%20Lumina-T2X%20family%20-%20a%20series%20of%20Flow-based%0ALarge%20Diffusion%20Transformers%20%28Flag-DiT%29%20equipped%20with%20zero-initialized%0Aattention%2C%20as%20a%20unified%20framework%20designed%20to%20transform%20noise%20into%20images%2C%0Avideos%2C%20multi-view%203D%20objects%2C%20and%20audio%20clips%20conditioned%20on%20text%0Ainstructions.%20By%20tokenizing%20the%20latent%20spatial-temporal%20space%20and%20incorporating%0Alearnable%20placeholders%20such%20as%20%5Bnextline%5D%20and%20%5Bnextframe%5D%20tokens%2C%20Lumina-T2X%0Aseamlessly%20unifies%20the%20representations%20of%20different%20modalities%20across%20various%0Aspatial-temporal%20resolutions.%20This%20unified%20approach%20enables%20training%20within%20a%0Asingle%20framework%20for%20different%20modalities%20and%20allows%20for%20flexible%20generation%20of%0Amultimodal%20data%20at%20any%20resolution%2C%20aspect%20ratio%2C%20and%20length%20during%20inference.%0AAdvanced%20techniques%20like%20RoPE%2C%20RMSNorm%2C%20and%20flow%20matching%20enhance%20the%0Astability%2C%20flexibility%2C%20and%20scalability%20of%20Flag-DiT%2C%20enabling%20models%20of%0ALumina-T2X%20to%20scale%20up%20to%207%20billion%20parameters%20and%20extend%20the%20context%20window%20to%0A128K%20tokens.%20This%20is%20particularly%20beneficial%20for%20creating%20ultra-high-definition%0Aimages%20with%20our%20Lumina-T2I%20model%20and%20long%20720p%20videos%20with%20our%20Lumina-T2V%0Amodel.%20Remarkably%2C%20Lumina-T2I%2C%20powered%20by%20a%205-billion-parameter%20Flag-DiT%2C%0Arequires%20only%2035%25%20of%20the%20training%20computational%20costs%20of%20a%0A600-million-parameter%20naive%20DiT.%20Our%20further%20comprehensive%20analysis%20underscores%0ALumina-T2X%27s%20preliminary%20capability%20in%20resolution%20extrapolation%2C%0Ahigh-resolution%20editing%2C%20generating%20consistent%203D%20views%2C%20and%20synthesizing%0Avideos%20with%20seamless%20transitions.%20We%20expect%20that%20the%20open-sourcing%20of%0ALumina-T2X%20will%20further%20foster%20creativity%2C%20transparency%2C%20and%20diversity%20in%20the%0Agenerative%20AI%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-T2X%253A%2520Transforming%2520Text%2520into%2520Any%2520Modality%252C%2520Resolution%252C%2520and%250A%2520%2520Duration%2520via%2520Flow-based%2520Large%2520Diffusion%2520Transformers%26entry.906535625%3DPeng%2520Gao%2520and%2520Le%2520Zhuo%2520and%2520Dongyang%2520Liu%2520and%2520Ruoyi%2520Du%2520and%2520Xu%2520Luo%2520and%2520Longtian%2520Qiu%2520and%2520Yuhang%2520Zhang%2520and%2520Chen%2520Lin%2520and%2520Rongjie%2520Huang%2520and%2520Shijie%2520Geng%2520and%2520Renrui%2520Zhang%2520and%2520Junlin%2520Xi%2520and%2520Wenqi%2520Shao%2520and%2520Zhengkai%2520Jiang%2520and%2520Tianshuo%2520Yang%2520and%2520Weicai%2520Ye%2520and%2520He%2520Tong%2520and%2520Jingwen%2520He%2520and%2520Yu%2520Qiao%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Sora%2520unveils%2520the%2520potential%2520of%2520scaling%2520Diffusion%2520Transformer%2520for%2520generating%250Aphotorealistic%2520images%2520and%2520videos%2520at%2520arbitrary%2520resolutions%252C%2520aspect%2520ratios%252C%2520and%250Adurations%252C%2520yet%2520it%2520still%2520lacks%2520sufficient%2520implementation%2520details.%2520In%2520this%250Atechnical%2520report%252C%2520we%2520introduce%2520the%2520Lumina-T2X%2520family%2520-%2520a%2520series%2520of%2520Flow-based%250ALarge%2520Diffusion%2520Transformers%2520%2528Flag-DiT%2529%2520equipped%2520with%2520zero-initialized%250Aattention%252C%2520as%2520a%2520unified%2520framework%2520designed%2520to%2520transform%2520noise%2520into%2520images%252C%250Avideos%252C%2520multi-view%25203D%2520objects%252C%2520and%2520audio%2520clips%2520conditioned%2520on%2520text%250Ainstructions.%2520By%2520tokenizing%2520the%2520latent%2520spatial-temporal%2520space%2520and%2520incorporating%250Alearnable%2520placeholders%2520such%2520as%2520%255Bnextline%255D%2520and%2520%255Bnextframe%255D%2520tokens%252C%2520Lumina-T2X%250Aseamlessly%2520unifies%2520the%2520representations%2520of%2520different%2520modalities%2520across%2520various%250Aspatial-temporal%2520resolutions.%2520This%2520unified%2520approach%2520enables%2520training%2520within%2520a%250Asingle%2520framework%2520for%2520different%2520modalities%2520and%2520allows%2520for%2520flexible%2520generation%2520of%250Amultimodal%2520data%2520at%2520any%2520resolution%252C%2520aspect%2520ratio%252C%2520and%2520length%2520during%2520inference.%250AAdvanced%2520techniques%2520like%2520RoPE%252C%2520RMSNorm%252C%2520and%2520flow%2520matching%2520enhance%2520the%250Astability%252C%2520flexibility%252C%2520and%2520scalability%2520of%2520Flag-DiT%252C%2520enabling%2520models%2520of%250ALumina-T2X%2520to%2520scale%2520up%2520to%25207%2520billion%2520parameters%2520and%2520extend%2520the%2520context%2520window%2520to%250A128K%2520tokens.%2520This%2520is%2520particularly%2520beneficial%2520for%2520creating%2520ultra-high-definition%250Aimages%2520with%2520our%2520Lumina-T2I%2520model%2520and%2520long%2520720p%2520videos%2520with%2520our%2520Lumina-T2V%250Amodel.%2520Remarkably%252C%2520Lumina-T2I%252C%2520powered%2520by%2520a%25205-billion-parameter%2520Flag-DiT%252C%250Arequires%2520only%252035%2525%2520of%2520the%2520training%2520computational%2520costs%2520of%2520a%250A600-million-parameter%2520naive%2520DiT.%2520Our%2520further%2520comprehensive%2520analysis%2520underscores%250ALumina-T2X%2527s%2520preliminary%2520capability%2520in%2520resolution%2520extrapolation%252C%250Ahigh-resolution%2520editing%252C%2520generating%2520consistent%25203D%2520views%252C%2520and%2520synthesizing%250Avideos%2520with%2520seamless%2520transitions.%2520We%2520expect%2520that%2520the%2520open-sourcing%2520of%250ALumina-T2X%2520will%2520further%2520foster%2520creativity%252C%2520transparency%252C%2520and%2520diversity%2520in%2520the%250Agenerative%2520AI%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers&entry.906535625=Peng%20Gao%20and%20Le%20Zhuo%20and%20Dongyang%20Liu%20and%20Ruoyi%20Du%20and%20Xu%20Luo%20and%20Longtian%20Qiu%20and%20Yuhang%20Zhang%20and%20Chen%20Lin%20and%20Rongjie%20Huang%20and%20Shijie%20Geng%20and%20Renrui%20Zhang%20and%20Junlin%20Xi%20and%20Wenqi%20Shao%20and%20Zhengkai%20Jiang%20and%20Tianshuo%20Yang%20and%20Weicai%20Ye%20and%20He%20Tong%20and%20Jingwen%20He%20and%20Yu%20Qiao%20and%20Hongsheng%20Li&entry.1292438233=%20%20Sora%20unveils%20the%20potential%20of%20scaling%20Diffusion%20Transformer%20for%20generating%0Aphotorealistic%20images%20and%20videos%20at%20arbitrary%20resolutions%2C%20aspect%20ratios%2C%20and%0Adurations%2C%20yet%20it%20still%20lacks%20sufficient%20implementation%20details.%20In%20this%0Atechnical%20report%2C%20we%20introduce%20the%20Lumina-T2X%20family%20-%20a%20series%20of%20Flow-based%0ALarge%20Diffusion%20Transformers%20%28Flag-DiT%29%20equipped%20with%20zero-initialized%0Aattention%2C%20as%20a%20unified%20framework%20designed%20to%20transform%20noise%20into%20images%2C%0Avideos%2C%20multi-view%203D%20objects%2C%20and%20audio%20clips%20conditioned%20on%20text%0Ainstructions.%20By%20tokenizing%20the%20latent%20spatial-temporal%20space%20and%20incorporating%0Alearnable%20placeholders%20such%20as%20%5Bnextline%5D%20and%20%5Bnextframe%5D%20tokens%2C%20Lumina-T2X%0Aseamlessly%20unifies%20the%20representations%20of%20different%20modalities%20across%20various%0Aspatial-temporal%20resolutions.%20This%20unified%20approach%20enables%20training%20within%20a%0Asingle%20framework%20for%20different%20modalities%20and%20allows%20for%20flexible%20generation%20of%0Amultimodal%20data%20at%20any%20resolution%2C%20aspect%20ratio%2C%20and%20length%20during%20inference.%0AAdvanced%20techniques%20like%20RoPE%2C%20RMSNorm%2C%20and%20flow%20matching%20enhance%20the%0Astability%2C%20flexibility%2C%20and%20scalability%20of%20Flag-DiT%2C%20enabling%20models%20of%0ALumina-T2X%20to%20scale%20up%20to%207%20billion%20parameters%20and%20extend%20the%20context%20window%20to%0A128K%20tokens.%20This%20is%20particularly%20beneficial%20for%20creating%20ultra-high-definition%0Aimages%20with%20our%20Lumina-T2I%20model%20and%20long%20720p%20videos%20with%20our%20Lumina-T2V%0Amodel.%20Remarkably%2C%20Lumina-T2I%2C%20powered%20by%20a%205-billion-parameter%20Flag-DiT%2C%0Arequires%20only%2035%25%20of%20the%20training%20computational%20costs%20of%20a%0A600-million-parameter%20naive%20DiT.%20Our%20further%20comprehensive%20analysis%20underscores%0ALumina-T2X%27s%20preliminary%20capability%20in%20resolution%20extrapolation%2C%0Ahigh-resolution%20editing%2C%20generating%20consistent%203D%20views%2C%20and%20synthesizing%0Avideos%20with%20seamless%20transitions.%20We%20expect%20that%20the%20open-sourcing%20of%0ALumina-T2X%20will%20further%20foster%20creativity%2C%20transparency%2C%20and%20diversity%20in%20the%0Agenerative%20AI%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05945v2&entry.124074799=Read"},
{"title": "Fair Generalized Linear Mixed Models", "author": "Jan Pablo Burgard and Jo\u00e3o Vitor Pamplona", "abstract": "  When using machine learning for automated prediction, it is important to\naccount for fairness in the prediction. Fairness in machine learning aims to\nensure that biases in the data and model inaccuracies do not lead to\ndiscriminatory decisions. E.g., predictions from fair machine learning models\nshould not discriminate against sensitive variables such as sexual orientation\nand ethnicity. The training data often in obtained from social surveys. In\nsocial surveys, oftentimes the data collection process is a strata sampling,\ne.g. due to cost restrictions. In strata samples, the assumption of\nindependence between the observation is not fulfilled. Hence, if the machine\nlearning models do not account for the strata correlations, the results may be\nbiased. Especially high is the bias in cases where the strata assignment is\ncorrelated to the variable of interest. We present in this paper an algorithm\nthat can handle both problems simultaneously, and we demonstrate the impact of\nstratified sampling on the quality of fair machine learning predictions in a\nreproducible simulation study.\n", "link": "http://arxiv.org/abs/2405.09273v1", "date": "2024-05-15", "relevancy": 1.8378, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4884}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4622}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Generalized%20Linear%20Mixed%20Models&body=Title%3A%20Fair%20Generalized%20Linear%20Mixed%20Models%0AAuthor%3A%20Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona%0AAbstract%3A%20%20%20When%20using%20machine%20learning%20for%20automated%20prediction%2C%20it%20is%20important%20to%0Aaccount%20for%20fairness%20in%20the%20prediction.%20Fairness%20in%20machine%20learning%20aims%20to%0Aensure%20that%20biases%20in%20the%20data%20and%20model%20inaccuracies%20do%20not%20lead%20to%0Adiscriminatory%20decisions.%20E.g.%2C%20predictions%20from%20fair%20machine%20learning%20models%0Ashould%20not%20discriminate%20against%20sensitive%20variables%20such%20as%20sexual%20orientation%0Aand%20ethnicity.%20The%20training%20data%20often%20in%20obtained%20from%20social%20surveys.%20In%0Asocial%20surveys%2C%20oftentimes%20the%20data%20collection%20process%20is%20a%20strata%20sampling%2C%0Ae.g.%20due%20to%20cost%20restrictions.%20In%20strata%20samples%2C%20the%20assumption%20of%0Aindependence%20between%20the%20observation%20is%20not%20fulfilled.%20Hence%2C%20if%20the%20machine%0Alearning%20models%20do%20not%20account%20for%20the%20strata%20correlations%2C%20the%20results%20may%20be%0Abiased.%20Especially%20high%20is%20the%20bias%20in%20cases%20where%20the%20strata%20assignment%20is%0Acorrelated%20to%20the%20variable%20of%20interest.%20We%20present%20in%20this%20paper%20an%20algorithm%0Athat%20can%20handle%20both%20problems%20simultaneously%2C%20and%20we%20demonstrate%20the%20impact%20of%0Astratified%20sampling%20on%20the%20quality%20of%20fair%20machine%20learning%20predictions%20in%20a%0Areproducible%20simulation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Generalized%2520Linear%2520Mixed%2520Models%26entry.906535625%3DJan%2520Pablo%2520Burgard%2520and%2520Jo%25C3%25A3o%2520Vitor%2520Pamplona%26entry.1292438233%3D%2520%2520When%2520using%2520machine%2520learning%2520for%2520automated%2520prediction%252C%2520it%2520is%2520important%2520to%250Aaccount%2520for%2520fairness%2520in%2520the%2520prediction.%2520Fairness%2520in%2520machine%2520learning%2520aims%2520to%250Aensure%2520that%2520biases%2520in%2520the%2520data%2520and%2520model%2520inaccuracies%2520do%2520not%2520lead%2520to%250Adiscriminatory%2520decisions.%2520E.g.%252C%2520predictions%2520from%2520fair%2520machine%2520learning%2520models%250Ashould%2520not%2520discriminate%2520against%2520sensitive%2520variables%2520such%2520as%2520sexual%2520orientation%250Aand%2520ethnicity.%2520The%2520training%2520data%2520often%2520in%2520obtained%2520from%2520social%2520surveys.%2520In%250Asocial%2520surveys%252C%2520oftentimes%2520the%2520data%2520collection%2520process%2520is%2520a%2520strata%2520sampling%252C%250Ae.g.%2520due%2520to%2520cost%2520restrictions.%2520In%2520strata%2520samples%252C%2520the%2520assumption%2520of%250Aindependence%2520between%2520the%2520observation%2520is%2520not%2520fulfilled.%2520Hence%252C%2520if%2520the%2520machine%250Alearning%2520models%2520do%2520not%2520account%2520for%2520the%2520strata%2520correlations%252C%2520the%2520results%2520may%2520be%250Abiased.%2520Especially%2520high%2520is%2520the%2520bias%2520in%2520cases%2520where%2520the%2520strata%2520assignment%2520is%250Acorrelated%2520to%2520the%2520variable%2520of%2520interest.%2520We%2520present%2520in%2520this%2520paper%2520an%2520algorithm%250Athat%2520can%2520handle%2520both%2520problems%2520simultaneously%252C%2520and%2520we%2520demonstrate%2520the%2520impact%2520of%250Astratified%2520sampling%2520on%2520the%2520quality%2520of%2520fair%2520machine%2520learning%2520predictions%2520in%2520a%250Areproducible%2520simulation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Generalized%20Linear%20Mixed%20Models&entry.906535625=Jan%20Pablo%20Burgard%20and%20Jo%C3%A3o%20Vitor%20Pamplona&entry.1292438233=%20%20When%20using%20machine%20learning%20for%20automated%20prediction%2C%20it%20is%20important%20to%0Aaccount%20for%20fairness%20in%20the%20prediction.%20Fairness%20in%20machine%20learning%20aims%20to%0Aensure%20that%20biases%20in%20the%20data%20and%20model%20inaccuracies%20do%20not%20lead%20to%0Adiscriminatory%20decisions.%20E.g.%2C%20predictions%20from%20fair%20machine%20learning%20models%0Ashould%20not%20discriminate%20against%20sensitive%20variables%20such%20as%20sexual%20orientation%0Aand%20ethnicity.%20The%20training%20data%20often%20in%20obtained%20from%20social%20surveys.%20In%0Asocial%20surveys%2C%20oftentimes%20the%20data%20collection%20process%20is%20a%20strata%20sampling%2C%0Ae.g.%20due%20to%20cost%20restrictions.%20In%20strata%20samples%2C%20the%20assumption%20of%0Aindependence%20between%20the%20observation%20is%20not%20fulfilled.%20Hence%2C%20if%20the%20machine%0Alearning%20models%20do%20not%20account%20for%20the%20strata%20correlations%2C%20the%20results%20may%20be%0Abiased.%20Especially%20high%20is%20the%20bias%20in%20cases%20where%20the%20strata%20assignment%20is%0Acorrelated%20to%20the%20variable%20of%20interest.%20We%20present%20in%20this%20paper%20an%20algorithm%0Athat%20can%20handle%20both%20problems%20simultaneously%2C%20and%20we%20demonstrate%20the%20impact%20of%0Astratified%20sampling%20on%20the%20quality%20of%20fair%20machine%20learning%20predictions%20in%20a%0Areproducible%20simulation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09273v1&entry.124074799=Read"},
{"title": "Transfer Learning in Pre-Trained Large Language Models for Malware\n  Detection Based on System Calls", "author": "Pedro Miguel S\u00e1nchez S\u00e1nchez and Alberto Huertas Celdr\u00e1n and G\u00e9r\u00f4me Bovet and Gregorio Mart\u00ednez P\u00e9rez", "abstract": "  In the current cybersecurity landscape, protecting military devices such as\ncommunication and battlefield management systems against sophisticated cyber\nattacks is crucial. Malware exploits vulnerabilities through stealth methods,\noften evading traditional detection mechanisms such as software signatures. The\napplication of ML/DL in vulnerability detection has been extensively explored\nin the literature. However, current ML/DL vulnerability detection methods\nstruggle with understanding the context and intent behind complex attacks.\nIntegrating large language models (LLMs) with system call analysis offers a\npromising approach to enhance malware detection. This work presents a novel\nframework leveraging LLMs to classify malware based on system call data. The\nframework uses transfer learning to adapt pre-trained LLMs for malware\ndetection. By retraining LLMs on a dataset of benign and malicious system\ncalls, the models are refined to detect signs of malware activity. Experiments\nwith a dataset of over 1TB of system calls demonstrate that models with larger\ncontext sizes, such as BigBird and Longformer, achieve superior accuracy and\nF1-Score of approximately 0.86. The results highlight the importance of context\nsize in improving detection rates and underscore the trade-offs between\ncomputational complexity and performance. This approach shows significant\npotential for real-time detection in high-stakes environments, offering a\nrobust solution to evolving cyber threats.\n", "link": "http://arxiv.org/abs/2405.09318v1", "date": "2024-05-15", "relevancy": 1.8274, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4687}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Learning%20in%20Pre-Trained%20Large%20Language%20Models%20for%20Malware%0A%20%20Detection%20Based%20on%20System%20Calls&body=Title%3A%20Transfer%20Learning%20in%20Pre-Trained%20Large%20Language%20Models%20for%20Malware%0A%20%20Detection%20Based%20on%20System%20Calls%0AAuthor%3A%20Pedro%20Miguel%20S%C3%A1nchez%20S%C3%A1nchez%20and%20Alberto%20Huertas%20Celdr%C3%A1n%20and%20G%C3%A9r%C3%B4me%20Bovet%20and%20Gregorio%20Mart%C3%ADnez%20P%C3%A9rez%0AAbstract%3A%20%20%20In%20the%20current%20cybersecurity%20landscape%2C%20protecting%20military%20devices%20such%20as%0Acommunication%20and%20battlefield%20management%20systems%20against%20sophisticated%20cyber%0Aattacks%20is%20crucial.%20Malware%20exploits%20vulnerabilities%20through%20stealth%20methods%2C%0Aoften%20evading%20traditional%20detection%20mechanisms%20such%20as%20software%20signatures.%20The%0Aapplication%20of%20ML/DL%20in%20vulnerability%20detection%20has%20been%20extensively%20explored%0Ain%20the%20literature.%20However%2C%20current%20ML/DL%20vulnerability%20detection%20methods%0Astruggle%20with%20understanding%20the%20context%20and%20intent%20behind%20complex%20attacks.%0AIntegrating%20large%20language%20models%20%28LLMs%29%20with%20system%20call%20analysis%20offers%20a%0Apromising%20approach%20to%20enhance%20malware%20detection.%20This%20work%20presents%20a%20novel%0Aframework%20leveraging%20LLMs%20to%20classify%20malware%20based%20on%20system%20call%20data.%20The%0Aframework%20uses%20transfer%20learning%20to%20adapt%20pre-trained%20LLMs%20for%20malware%0Adetection.%20By%20retraining%20LLMs%20on%20a%20dataset%20of%20benign%20and%20malicious%20system%0Acalls%2C%20the%20models%20are%20refined%20to%20detect%20signs%20of%20malware%20activity.%20Experiments%0Awith%20a%20dataset%20of%20over%201TB%20of%20system%20calls%20demonstrate%20that%20models%20with%20larger%0Acontext%20sizes%2C%20such%20as%20BigBird%20and%20Longformer%2C%20achieve%20superior%20accuracy%20and%0AF1-Score%20of%20approximately%200.86.%20The%20results%20highlight%20the%20importance%20of%20context%0Asize%20in%20improving%20detection%20rates%20and%20underscore%20the%20trade-offs%20between%0Acomputational%20complexity%20and%20performance.%20This%20approach%20shows%20significant%0Apotential%20for%20real-time%20detection%20in%20high-stakes%20environments%2C%20offering%20a%0Arobust%20solution%20to%20evolving%20cyber%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Learning%2520in%2520Pre-Trained%2520Large%2520Language%2520Models%2520for%2520Malware%250A%2520%2520Detection%2520Based%2520on%2520System%2520Calls%26entry.906535625%3DPedro%2520Miguel%2520S%25C3%25A1nchez%2520S%25C3%25A1nchez%2520and%2520Alberto%2520Huertas%2520Celdr%25C3%25A1n%2520and%2520G%25C3%25A9r%25C3%25B4me%2520Bovet%2520and%2520Gregorio%2520Mart%25C3%25ADnez%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520cybersecurity%2520landscape%252C%2520protecting%2520military%2520devices%2520such%2520as%250Acommunication%2520and%2520battlefield%2520management%2520systems%2520against%2520sophisticated%2520cyber%250Aattacks%2520is%2520crucial.%2520Malware%2520exploits%2520vulnerabilities%2520through%2520stealth%2520methods%252C%250Aoften%2520evading%2520traditional%2520detection%2520mechanisms%2520such%2520as%2520software%2520signatures.%2520The%250Aapplication%2520of%2520ML/DL%2520in%2520vulnerability%2520detection%2520has%2520been%2520extensively%2520explored%250Ain%2520the%2520literature.%2520However%252C%2520current%2520ML/DL%2520vulnerability%2520detection%2520methods%250Astruggle%2520with%2520understanding%2520the%2520context%2520and%2520intent%2520behind%2520complex%2520attacks.%250AIntegrating%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520system%2520call%2520analysis%2520offers%2520a%250Apromising%2520approach%2520to%2520enhance%2520malware%2520detection.%2520This%2520work%2520presents%2520a%2520novel%250Aframework%2520leveraging%2520LLMs%2520to%2520classify%2520malware%2520based%2520on%2520system%2520call%2520data.%2520The%250Aframework%2520uses%2520transfer%2520learning%2520to%2520adapt%2520pre-trained%2520LLMs%2520for%2520malware%250Adetection.%2520By%2520retraining%2520LLMs%2520on%2520a%2520dataset%2520of%2520benign%2520and%2520malicious%2520system%250Acalls%252C%2520the%2520models%2520are%2520refined%2520to%2520detect%2520signs%2520of%2520malware%2520activity.%2520Experiments%250Awith%2520a%2520dataset%2520of%2520over%25201TB%2520of%2520system%2520calls%2520demonstrate%2520that%2520models%2520with%2520larger%250Acontext%2520sizes%252C%2520such%2520as%2520BigBird%2520and%2520Longformer%252C%2520achieve%2520superior%2520accuracy%2520and%250AF1-Score%2520of%2520approximately%25200.86.%2520The%2520results%2520highlight%2520the%2520importance%2520of%2520context%250Asize%2520in%2520improving%2520detection%2520rates%2520and%2520underscore%2520the%2520trade-offs%2520between%250Acomputational%2520complexity%2520and%2520performance.%2520This%2520approach%2520shows%2520significant%250Apotential%2520for%2520real-time%2520detection%2520in%2520high-stakes%2520environments%252C%2520offering%2520a%250Arobust%2520solution%2520to%2520evolving%2520cyber%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Learning%20in%20Pre-Trained%20Large%20Language%20Models%20for%20Malware%0A%20%20Detection%20Based%20on%20System%20Calls&entry.906535625=Pedro%20Miguel%20S%C3%A1nchez%20S%C3%A1nchez%20and%20Alberto%20Huertas%20Celdr%C3%A1n%20and%20G%C3%A9r%C3%B4me%20Bovet%20and%20Gregorio%20Mart%C3%ADnez%20P%C3%A9rez&entry.1292438233=%20%20In%20the%20current%20cybersecurity%20landscape%2C%20protecting%20military%20devices%20such%20as%0Acommunication%20and%20battlefield%20management%20systems%20against%20sophisticated%20cyber%0Aattacks%20is%20crucial.%20Malware%20exploits%20vulnerabilities%20through%20stealth%20methods%2C%0Aoften%20evading%20traditional%20detection%20mechanisms%20such%20as%20software%20signatures.%20The%0Aapplication%20of%20ML/DL%20in%20vulnerability%20detection%20has%20been%20extensively%20explored%0Ain%20the%20literature.%20However%2C%20current%20ML/DL%20vulnerability%20detection%20methods%0Astruggle%20with%20understanding%20the%20context%20and%20intent%20behind%20complex%20attacks.%0AIntegrating%20large%20language%20models%20%28LLMs%29%20with%20system%20call%20analysis%20offers%20a%0Apromising%20approach%20to%20enhance%20malware%20detection.%20This%20work%20presents%20a%20novel%0Aframework%20leveraging%20LLMs%20to%20classify%20malware%20based%20on%20system%20call%20data.%20The%0Aframework%20uses%20transfer%20learning%20to%20adapt%20pre-trained%20LLMs%20for%20malware%0Adetection.%20By%20retraining%20LLMs%20on%20a%20dataset%20of%20benign%20and%20malicious%20system%0Acalls%2C%20the%20models%20are%20refined%20to%20detect%20signs%20of%20malware%20activity.%20Experiments%0Awith%20a%20dataset%20of%20over%201TB%20of%20system%20calls%20demonstrate%20that%20models%20with%20larger%0Acontext%20sizes%2C%20such%20as%20BigBird%20and%20Longformer%2C%20achieve%20superior%20accuracy%20and%0AF1-Score%20of%20approximately%200.86.%20The%20results%20highlight%20the%20importance%20of%20context%0Asize%20in%20improving%20detection%20rates%20and%20underscore%20the%20trade-offs%20between%0Acomputational%20complexity%20and%20performance.%20This%20approach%20shows%20significant%0Apotential%20for%20real-time%20detection%20in%20high-stakes%20environments%2C%20offering%20a%0Arobust%20solution%20to%20evolving%20cyber%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09318v1&entry.124074799=Read"},
{"title": "Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study", "author": "Chi Ma and Mincong Huang and Chao Wang and Yujie Wang and Lei Yu", "abstract": "  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n", "link": "http://arxiv.org/abs/2405.09274v1", "date": "2024-05-15", "relevancy": 1.8236, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4647}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Activation%20Pitfalls%20in%20LLaMA%20Models%3A%20An%20Empirical%20Study&body=Title%3A%20Dynamic%20Activation%20Pitfalls%20in%20LLaMA%20Models%3A%20An%20Empirical%20Study%0AAuthor%3A%20Chi%20Ma%20and%20Mincong%20Huang%20and%20Chao%20Wang%20and%20Yujie%20Wang%20and%20Lei%20Yu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20efficacy%20of%20dynamic%0Aactivation%20mechanisms%20within%20the%20LLaMA%20family%20of%20language%20models.%20Despite%20the%0Apotential%20of%20dynamic%20activation%20methods%20to%20reduce%20computation%20and%20increase%0Aspeed%20in%20models%20using%20the%20ReLU%20activation%20function%2C%20our%20empirical%20findings%20have%0Auncovered%20several%20inherent%20pitfalls%20in%20the%20current%20dynamic%20activation%20schemes.%0AThrough%20extensive%20experiments%20across%20various%20dynamic%20activation%20strategies%2C%20we%0Ademonstrate%20that%20LLaMA%20models%20usually%20underperform%20when%20compared%20to%20their%20ReLU%0Acounterparts%2C%20particularly%20in%20scenarios%20demanding%20high%20sparsity%20ratio.%20We%0Aattribute%20these%20deficiencies%20to%20a%20combination%20of%20factors%3A%201%29%20the%20inherent%0Acomplexity%20of%20dynamically%20predicting%20activation%20heads%20and%20neurons%3B%202%29%20the%0Ainadequate%20sparsity%20resulting%20from%20activation%20functions%3B%203%29%20the%20insufficient%0Apreservation%20of%20information%20resulting%20from%20KV%20cache%20skipping.%20Our%20analysis%20not%0Aonly%20sheds%20light%20on%20the%20limitations%20of%20dynamic%20activation%20in%20the%20context%20of%0Alarge-scale%20LLaMA%20models%20but%20also%20proposes%20roadmaps%20for%20enhancing%20the%20design%20of%0Afuture%20sparsity%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Activation%2520Pitfalls%2520in%2520LLaMA%2520Models%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DChi%2520Ma%2520and%2520Mincong%2520Huang%2520and%2520Chao%2520Wang%2520and%2520Yujie%2520Wang%2520and%2520Lei%2520Yu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520the%2520efficacy%2520of%2520dynamic%250Aactivation%2520mechanisms%2520within%2520the%2520LLaMA%2520family%2520of%2520language%2520models.%2520Despite%2520the%250Apotential%2520of%2520dynamic%2520activation%2520methods%2520to%2520reduce%2520computation%2520and%2520increase%250Aspeed%2520in%2520models%2520using%2520the%2520ReLU%2520activation%2520function%252C%2520our%2520empirical%2520findings%2520have%250Auncovered%2520several%2520inherent%2520pitfalls%2520in%2520the%2520current%2520dynamic%2520activation%2520schemes.%250AThrough%2520extensive%2520experiments%2520across%2520various%2520dynamic%2520activation%2520strategies%252C%2520we%250Ademonstrate%2520that%2520LLaMA%2520models%2520usually%2520underperform%2520when%2520compared%2520to%2520their%2520ReLU%250Acounterparts%252C%2520particularly%2520in%2520scenarios%2520demanding%2520high%2520sparsity%2520ratio.%2520We%250Aattribute%2520these%2520deficiencies%2520to%2520a%2520combination%2520of%2520factors%253A%25201%2529%2520the%2520inherent%250Acomplexity%2520of%2520dynamically%2520predicting%2520activation%2520heads%2520and%2520neurons%253B%25202%2529%2520the%250Ainadequate%2520sparsity%2520resulting%2520from%2520activation%2520functions%253B%25203%2529%2520the%2520insufficient%250Apreservation%2520of%2520information%2520resulting%2520from%2520KV%2520cache%2520skipping.%2520Our%2520analysis%2520not%250Aonly%2520sheds%2520light%2520on%2520the%2520limitations%2520of%2520dynamic%2520activation%2520in%2520the%2520context%2520of%250Alarge-scale%2520LLaMA%2520models%2520but%2520also%2520proposes%2520roadmaps%2520for%2520enhancing%2520the%2520design%2520of%250Afuture%2520sparsity%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Activation%20Pitfalls%20in%20LLaMA%20Models%3A%20An%20Empirical%20Study&entry.906535625=Chi%20Ma%20and%20Mincong%20Huang%20and%20Chao%20Wang%20and%20Yujie%20Wang%20and%20Lei%20Yu&entry.1292438233=%20%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20efficacy%20of%20dynamic%0Aactivation%20mechanisms%20within%20the%20LLaMA%20family%20of%20language%20models.%20Despite%20the%0Apotential%20of%20dynamic%20activation%20methods%20to%20reduce%20computation%20and%20increase%0Aspeed%20in%20models%20using%20the%20ReLU%20activation%20function%2C%20our%20empirical%20findings%20have%0Auncovered%20several%20inherent%20pitfalls%20in%20the%20current%20dynamic%20activation%20schemes.%0AThrough%20extensive%20experiments%20across%20various%20dynamic%20activation%20strategies%2C%20we%0Ademonstrate%20that%20LLaMA%20models%20usually%20underperform%20when%20compared%20to%20their%20ReLU%0Acounterparts%2C%20particularly%20in%20scenarios%20demanding%20high%20sparsity%20ratio.%20We%0Aattribute%20these%20deficiencies%20to%20a%20combination%20of%20factors%3A%201%29%20the%20inherent%0Acomplexity%20of%20dynamically%20predicting%20activation%20heads%20and%20neurons%3B%202%29%20the%0Ainadequate%20sparsity%20resulting%20from%20activation%20functions%3B%203%29%20the%20insufficient%0Apreservation%20of%20information%20resulting%20from%20KV%20cache%20skipping.%20Our%20analysis%20not%0Aonly%20sheds%20light%20on%20the%20limitations%20of%20dynamic%20activation%20in%20the%20context%20of%0Alarge-scale%20LLaMA%20models%20but%20also%20proposes%20roadmaps%20for%20enhancing%20the%20design%20of%0Afuture%20sparsity%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09274v1&entry.124074799=Read"},
{"title": "AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the\n  Dangers of Stochastic Pollocks", "author": "Trystan S. Goetze", "abstract": "  Since the launch of applications such as DALL-E, Midjourney, and Stable\nDiffusion, generative artificial intelligence has been controversial as a tool\nfor creating artwork. While some have presented longtermist worries about these\ntechnologies as harbingers of fully automated futures to come, more pressing is\nthe impact of generative AI on creative labour in the present. Already,\nbusiness leaders have begun replacing human artistic labour with AI-generated\nimages. In response, the artistic community has launched a protest movement,\nwhich argues that AI image generation is a kind of theft. This paper analyzes,\nsubstantiates, and critiques these arguments, concluding that AI image\ngenerators involve an unethical kind of labour theft. If correct, many other AI\napplications also rely upon theft.\n", "link": "http://arxiv.org/abs/2401.06178v2", "date": "2024-05-15", "relevancy": 1.8058, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4676}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4415}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Art%20is%20Theft%3A%20Labour%2C%20Extraction%2C%20and%20Exploitation%2C%20Or%2C%20On%20the%0A%20%20Dangers%20of%20Stochastic%20Pollocks&body=Title%3A%20AI%20Art%20is%20Theft%3A%20Labour%2C%20Extraction%2C%20and%20Exploitation%2C%20Or%2C%20On%20the%0A%20%20Dangers%20of%20Stochastic%20Pollocks%0AAuthor%3A%20Trystan%20S.%20Goetze%0AAbstract%3A%20%20%20Since%20the%20launch%20of%20applications%20such%20as%20DALL-E%2C%20Midjourney%2C%20and%20Stable%0ADiffusion%2C%20generative%20artificial%20intelligence%20has%20been%20controversial%20as%20a%20tool%0Afor%20creating%20artwork.%20While%20some%20have%20presented%20longtermist%20worries%20about%20these%0Atechnologies%20as%20harbingers%20of%20fully%20automated%20futures%20to%20come%2C%20more%20pressing%20is%0Athe%20impact%20of%20generative%20AI%20on%20creative%20labour%20in%20the%20present.%20Already%2C%0Abusiness%20leaders%20have%20begun%20replacing%20human%20artistic%20labour%20with%20AI-generated%0Aimages.%20In%20response%2C%20the%20artistic%20community%20has%20launched%20a%20protest%20movement%2C%0Awhich%20argues%20that%20AI%20image%20generation%20is%20a%20kind%20of%20theft.%20This%20paper%20analyzes%2C%0Asubstantiates%2C%20and%20critiques%20these%20arguments%2C%20concluding%20that%20AI%20image%0Agenerators%20involve%20an%20unethical%20kind%20of%20labour%20theft.%20If%20correct%2C%20many%20other%20AI%0Aapplications%20also%20rely%20upon%20theft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Art%2520is%2520Theft%253A%2520Labour%252C%2520Extraction%252C%2520and%2520Exploitation%252C%2520Or%252C%2520On%2520the%250A%2520%2520Dangers%2520of%2520Stochastic%2520Pollocks%26entry.906535625%3DTrystan%2520S.%2520Goetze%26entry.1292438233%3D%2520%2520Since%2520the%2520launch%2520of%2520applications%2520such%2520as%2520DALL-E%252C%2520Midjourney%252C%2520and%2520Stable%250ADiffusion%252C%2520generative%2520artificial%2520intelligence%2520has%2520been%2520controversial%2520as%2520a%2520tool%250Afor%2520creating%2520artwork.%2520While%2520some%2520have%2520presented%2520longtermist%2520worries%2520about%2520these%250Atechnologies%2520as%2520harbingers%2520of%2520fully%2520automated%2520futures%2520to%2520come%252C%2520more%2520pressing%2520is%250Athe%2520impact%2520of%2520generative%2520AI%2520on%2520creative%2520labour%2520in%2520the%2520present.%2520Already%252C%250Abusiness%2520leaders%2520have%2520begun%2520replacing%2520human%2520artistic%2520labour%2520with%2520AI-generated%250Aimages.%2520In%2520response%252C%2520the%2520artistic%2520community%2520has%2520launched%2520a%2520protest%2520movement%252C%250Awhich%2520argues%2520that%2520AI%2520image%2520generation%2520is%2520a%2520kind%2520of%2520theft.%2520This%2520paper%2520analyzes%252C%250Asubstantiates%252C%2520and%2520critiques%2520these%2520arguments%252C%2520concluding%2520that%2520AI%2520image%250Agenerators%2520involve%2520an%2520unethical%2520kind%2520of%2520labour%2520theft.%2520If%2520correct%252C%2520many%2520other%2520AI%250Aapplications%2520also%2520rely%2520upon%2520theft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Art%20is%20Theft%3A%20Labour%2C%20Extraction%2C%20and%20Exploitation%2C%20Or%2C%20On%20the%0A%20%20Dangers%20of%20Stochastic%20Pollocks&entry.906535625=Trystan%20S.%20Goetze&entry.1292438233=%20%20Since%20the%20launch%20of%20applications%20such%20as%20DALL-E%2C%20Midjourney%2C%20and%20Stable%0ADiffusion%2C%20generative%20artificial%20intelligence%20has%20been%20controversial%20as%20a%20tool%0Afor%20creating%20artwork.%20While%20some%20have%20presented%20longtermist%20worries%20about%20these%0Atechnologies%20as%20harbingers%20of%20fully%20automated%20futures%20to%20come%2C%20more%20pressing%20is%0Athe%20impact%20of%20generative%20AI%20on%20creative%20labour%20in%20the%20present.%20Already%2C%0Abusiness%20leaders%20have%20begun%20replacing%20human%20artistic%20labour%20with%20AI-generated%0Aimages.%20In%20response%2C%20the%20artistic%20community%20has%20launched%20a%20protest%20movement%2C%0Awhich%20argues%20that%20AI%20image%20generation%20is%20a%20kind%20of%20theft.%20This%20paper%20analyzes%2C%0Asubstantiates%2C%20and%20critiques%20these%20arguments%2C%20concluding%20that%20AI%20image%0Agenerators%20involve%20an%20unethical%20kind%20of%20labour%20theft.%20If%20correct%2C%20many%20other%20AI%0Aapplications%20also%20rely%20upon%20theft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06178v2&entry.124074799=Read"},
{"title": "Attribute reduction algorithm of rough sets based on spatial\n  optimization", "author": "Xuchang Guo and Houbiao Li", "abstract": "  Rough set is one of the important methods for rule acquisition and attribute\nreduction. The current goal of rough set attribute reduction focuses more on\nminimizing the number of reduced attributes, but ignores the spatial similarity\nbetween reduced and decision attributes, which may lead to problems such as\nincreased number of rules and limited generality. In this paper, a rough set\nattribute reduction algorithm based on spatial optimization is proposed. By\nintroducing the concept of spatial similarity, to find the reduction with the\nhighest spatial similarity, so that the spatial similarity between reduction\nand decision attributes is higher, and more concise and widespread rules are\nobtained. In addition, a comparative experiment with the traditional rough set\nattribute reduction algorithms is designed to prove the effectiveness of the\nrough set attribute reduction algorithm based on spatial optimization, which\nhas made significant improvements on many datasets.\n", "link": "http://arxiv.org/abs/2405.09292v1", "date": "2024-05-15", "relevancy": 1.7973, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribute%20reduction%20algorithm%20of%20rough%20sets%20based%20on%20spatial%0A%20%20optimization&body=Title%3A%20Attribute%20reduction%20algorithm%20of%20rough%20sets%20based%20on%20spatial%0A%20%20optimization%0AAuthor%3A%20Xuchang%20Guo%20and%20Houbiao%20Li%0AAbstract%3A%20%20%20Rough%20set%20is%20one%20of%20the%20important%20methods%20for%20rule%20acquisition%20and%20attribute%0Areduction.%20The%20current%20goal%20of%20rough%20set%20attribute%20reduction%20focuses%20more%20on%0Aminimizing%20the%20number%20of%20reduced%20attributes%2C%20but%20ignores%20the%20spatial%20similarity%0Abetween%20reduced%20and%20decision%20attributes%2C%20which%20may%20lead%20to%20problems%20such%20as%0Aincreased%20number%20of%20rules%20and%20limited%20generality.%20In%20this%20paper%2C%20a%20rough%20set%0Aattribute%20reduction%20algorithm%20based%20on%20spatial%20optimization%20is%20proposed.%20By%0Aintroducing%20the%20concept%20of%20spatial%20similarity%2C%20to%20find%20the%20reduction%20with%20the%0Ahighest%20spatial%20similarity%2C%20so%20that%20the%20spatial%20similarity%20between%20reduction%0Aand%20decision%20attributes%20is%20higher%2C%20and%20more%20concise%20and%20widespread%20rules%20are%0Aobtained.%20In%20addition%2C%20a%20comparative%20experiment%20with%20the%20traditional%20rough%20set%0Aattribute%20reduction%20algorithms%20is%20designed%20to%20prove%20the%20effectiveness%20of%20the%0Arough%20set%20attribute%20reduction%20algorithm%20based%20on%20spatial%20optimization%2C%20which%0Ahas%20made%20significant%20improvements%20on%20many%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribute%2520reduction%2520algorithm%2520of%2520rough%2520sets%2520based%2520on%2520spatial%250A%2520%2520optimization%26entry.906535625%3DXuchang%2520Guo%2520and%2520Houbiao%2520Li%26entry.1292438233%3D%2520%2520Rough%2520set%2520is%2520one%2520of%2520the%2520important%2520methods%2520for%2520rule%2520acquisition%2520and%2520attribute%250Areduction.%2520The%2520current%2520goal%2520of%2520rough%2520set%2520attribute%2520reduction%2520focuses%2520more%2520on%250Aminimizing%2520the%2520number%2520of%2520reduced%2520attributes%252C%2520but%2520ignores%2520the%2520spatial%2520similarity%250Abetween%2520reduced%2520and%2520decision%2520attributes%252C%2520which%2520may%2520lead%2520to%2520problems%2520such%2520as%250Aincreased%2520number%2520of%2520rules%2520and%2520limited%2520generality.%2520In%2520this%2520paper%252C%2520a%2520rough%2520set%250Aattribute%2520reduction%2520algorithm%2520based%2520on%2520spatial%2520optimization%2520is%2520proposed.%2520By%250Aintroducing%2520the%2520concept%2520of%2520spatial%2520similarity%252C%2520to%2520find%2520the%2520reduction%2520with%2520the%250Ahighest%2520spatial%2520similarity%252C%2520so%2520that%2520the%2520spatial%2520similarity%2520between%2520reduction%250Aand%2520decision%2520attributes%2520is%2520higher%252C%2520and%2520more%2520concise%2520and%2520widespread%2520rules%2520are%250Aobtained.%2520In%2520addition%252C%2520a%2520comparative%2520experiment%2520with%2520the%2520traditional%2520rough%2520set%250Aattribute%2520reduction%2520algorithms%2520is%2520designed%2520to%2520prove%2520the%2520effectiveness%2520of%2520the%250Arough%2520set%2520attribute%2520reduction%2520algorithm%2520based%2520on%2520spatial%2520optimization%252C%2520which%250Ahas%2520made%2520significant%2520improvements%2520on%2520many%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribute%20reduction%20algorithm%20of%20rough%20sets%20based%20on%20spatial%0A%20%20optimization&entry.906535625=Xuchang%20Guo%20and%20Houbiao%20Li&entry.1292438233=%20%20Rough%20set%20is%20one%20of%20the%20important%20methods%20for%20rule%20acquisition%20and%20attribute%0Areduction.%20The%20current%20goal%20of%20rough%20set%20attribute%20reduction%20focuses%20more%20on%0Aminimizing%20the%20number%20of%20reduced%20attributes%2C%20but%20ignores%20the%20spatial%20similarity%0Abetween%20reduced%20and%20decision%20attributes%2C%20which%20may%20lead%20to%20problems%20such%20as%0Aincreased%20number%20of%20rules%20and%20limited%20generality.%20In%20this%20paper%2C%20a%20rough%20set%0Aattribute%20reduction%20algorithm%20based%20on%20spatial%20optimization%20is%20proposed.%20By%0Aintroducing%20the%20concept%20of%20spatial%20similarity%2C%20to%20find%20the%20reduction%20with%20the%0Ahighest%20spatial%20similarity%2C%20so%20that%20the%20spatial%20similarity%20between%20reduction%0Aand%20decision%20attributes%20is%20higher%2C%20and%20more%20concise%20and%20widespread%20rules%20are%0Aobtained.%20In%20addition%2C%20a%20comparative%20experiment%20with%20the%20traditional%20rough%20set%0Aattribute%20reduction%20algorithms%20is%20designed%20to%20prove%20the%20effectiveness%20of%20the%0Arough%20set%20attribute%20reduction%20algorithm%20based%20on%20spatial%20optimization%2C%20which%0Ahas%20made%20significant%20improvements%20on%20many%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09292v1&entry.124074799=Read"},
{"title": "NeuralCMS: A deep learning approach to study Jupiter's interior", "author": "Maayan Ziv and Eli Galanti and Amir Sheffer and Saburo Howard and Tristan Guillot and Yohai Kaspi", "abstract": "  NASA's Juno mission provided exquisite measurements of Jupiter's gravity\nfield that together with the Galileo entry probe atmospheric measurements\nconstrains the interior structure of the giant planet. Inferring its interior\nstructure range remains a challenging inverse problem requiring a\ncomputationally intensive search of combinations of various planetary\nproperties, such as the cloud-level temperature, composition, and core\nfeatures, requiring the computation of ~10^9 interior models. We propose an\nefficient deep neural network (DNN) model to generate high-precision\nwide-ranged interior models based on the very accurate but computationally\ndemanding concentric MacLaurin spheroid (CMS) method. We trained a\nsharing-based DNN with a large set of CMS results for a four-layer interior\nmodel of Jupiter, including a dilute core, to accurately predict the gravity\nmoments and mass, given a combination of interior features. We evaluated the\nperformance of the trained DNN (NeuralCMS) to inspect its predictive\nlimitations. NeuralCMS shows very good performance in predicting the gravity\nmoments, with errors comparable with the uncertainty due to differential\nrotation, and a very accurate mass prediction. This allowed us to perform a\nbroad parameter space search by computing only ~10^4 actual CMS interior\nmodels, resulting in a large sample of plausible interior structures, and\nreducing the computation time by a factor of 10^5. Moreover, we used a DNN\nexplainability algorithm to analyze the impact of the parameters setting the\ninterior model on the predicted observables, providing information on their\nnonlinear relation.\n", "link": "http://arxiv.org/abs/2405.09244v1", "date": "2024-05-15", "relevancy": 1.7957, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4548}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuralCMS%3A%20A%20deep%20learning%20approach%20to%20study%20Jupiter%27s%20interior&body=Title%3A%20NeuralCMS%3A%20A%20deep%20learning%20approach%20to%20study%20Jupiter%27s%20interior%0AAuthor%3A%20Maayan%20Ziv%20and%20Eli%20Galanti%20and%20Amir%20Sheffer%20and%20Saburo%20Howard%20and%20Tristan%20Guillot%20and%20Yohai%20Kaspi%0AAbstract%3A%20%20%20NASA%27s%20Juno%20mission%20provided%20exquisite%20measurements%20of%20Jupiter%27s%20gravity%0Afield%20that%20together%20with%20the%20Galileo%20entry%20probe%20atmospheric%20measurements%0Aconstrains%20the%20interior%20structure%20of%20the%20giant%20planet.%20Inferring%20its%20interior%0Astructure%20range%20remains%20a%20challenging%20inverse%20problem%20requiring%20a%0Acomputationally%20intensive%20search%20of%20combinations%20of%20various%20planetary%0Aproperties%2C%20such%20as%20the%20cloud-level%20temperature%2C%20composition%2C%20and%20core%0Afeatures%2C%20requiring%20the%20computation%20of%20~10%5E9%20interior%20models.%20We%20propose%20an%0Aefficient%20deep%20neural%20network%20%28DNN%29%20model%20to%20generate%20high-precision%0Awide-ranged%20interior%20models%20based%20on%20the%20very%20accurate%20but%20computationally%0Ademanding%20concentric%20MacLaurin%20spheroid%20%28CMS%29%20method.%20We%20trained%20a%0Asharing-based%20DNN%20with%20a%20large%20set%20of%20CMS%20results%20for%20a%20four-layer%20interior%0Amodel%20of%20Jupiter%2C%20including%20a%20dilute%20core%2C%20to%20accurately%20predict%20the%20gravity%0Amoments%20and%20mass%2C%20given%20a%20combination%20of%20interior%20features.%20We%20evaluated%20the%0Aperformance%20of%20the%20trained%20DNN%20%28NeuralCMS%29%20to%20inspect%20its%20predictive%0Alimitations.%20NeuralCMS%20shows%20very%20good%20performance%20in%20predicting%20the%20gravity%0Amoments%2C%20with%20errors%20comparable%20with%20the%20uncertainty%20due%20to%20differential%0Arotation%2C%20and%20a%20very%20accurate%20mass%20prediction.%20This%20allowed%20us%20to%20perform%20a%0Abroad%20parameter%20space%20search%20by%20computing%20only%20~10%5E4%20actual%20CMS%20interior%0Amodels%2C%20resulting%20in%20a%20large%20sample%20of%20plausible%20interior%20structures%2C%20and%0Areducing%20the%20computation%20time%20by%20a%20factor%20of%2010%5E5.%20Moreover%2C%20we%20used%20a%20DNN%0Aexplainability%20algorithm%20to%20analyze%20the%20impact%20of%20the%20parameters%20setting%20the%0Ainterior%20model%20on%20the%20predicted%20observables%2C%20providing%20information%20on%20their%0Anonlinear%20relation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralCMS%253A%2520A%2520deep%2520learning%2520approach%2520to%2520study%2520Jupiter%2527s%2520interior%26entry.906535625%3DMaayan%2520Ziv%2520and%2520Eli%2520Galanti%2520and%2520Amir%2520Sheffer%2520and%2520Saburo%2520Howard%2520and%2520Tristan%2520Guillot%2520and%2520Yohai%2520Kaspi%26entry.1292438233%3D%2520%2520NASA%2527s%2520Juno%2520mission%2520provided%2520exquisite%2520measurements%2520of%2520Jupiter%2527s%2520gravity%250Afield%2520that%2520together%2520with%2520the%2520Galileo%2520entry%2520probe%2520atmospheric%2520measurements%250Aconstrains%2520the%2520interior%2520structure%2520of%2520the%2520giant%2520planet.%2520Inferring%2520its%2520interior%250Astructure%2520range%2520remains%2520a%2520challenging%2520inverse%2520problem%2520requiring%2520a%250Acomputationally%2520intensive%2520search%2520of%2520combinations%2520of%2520various%2520planetary%250Aproperties%252C%2520such%2520as%2520the%2520cloud-level%2520temperature%252C%2520composition%252C%2520and%2520core%250Afeatures%252C%2520requiring%2520the%2520computation%2520of%2520~10%255E9%2520interior%2520models.%2520We%2520propose%2520an%250Aefficient%2520deep%2520neural%2520network%2520%2528DNN%2529%2520model%2520to%2520generate%2520high-precision%250Awide-ranged%2520interior%2520models%2520based%2520on%2520the%2520very%2520accurate%2520but%2520computationally%250Ademanding%2520concentric%2520MacLaurin%2520spheroid%2520%2528CMS%2529%2520method.%2520We%2520trained%2520a%250Asharing-based%2520DNN%2520with%2520a%2520large%2520set%2520of%2520CMS%2520results%2520for%2520a%2520four-layer%2520interior%250Amodel%2520of%2520Jupiter%252C%2520including%2520a%2520dilute%2520core%252C%2520to%2520accurately%2520predict%2520the%2520gravity%250Amoments%2520and%2520mass%252C%2520given%2520a%2520combination%2520of%2520interior%2520features.%2520We%2520evaluated%2520the%250Aperformance%2520of%2520the%2520trained%2520DNN%2520%2528NeuralCMS%2529%2520to%2520inspect%2520its%2520predictive%250Alimitations.%2520NeuralCMS%2520shows%2520very%2520good%2520performance%2520in%2520predicting%2520the%2520gravity%250Amoments%252C%2520with%2520errors%2520comparable%2520with%2520the%2520uncertainty%2520due%2520to%2520differential%250Arotation%252C%2520and%2520a%2520very%2520accurate%2520mass%2520prediction.%2520This%2520allowed%2520us%2520to%2520perform%2520a%250Abroad%2520parameter%2520space%2520search%2520by%2520computing%2520only%2520~10%255E4%2520actual%2520CMS%2520interior%250Amodels%252C%2520resulting%2520in%2520a%2520large%2520sample%2520of%2520plausible%2520interior%2520structures%252C%2520and%250Areducing%2520the%2520computation%2520time%2520by%2520a%2520factor%2520of%252010%255E5.%2520Moreover%252C%2520we%2520used%2520a%2520DNN%250Aexplainability%2520algorithm%2520to%2520analyze%2520the%2520impact%2520of%2520the%2520parameters%2520setting%2520the%250Ainterior%2520model%2520on%2520the%2520predicted%2520observables%252C%2520providing%2520information%2520on%2520their%250Anonlinear%2520relation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuralCMS%3A%20A%20deep%20learning%20approach%20to%20study%20Jupiter%27s%20interior&entry.906535625=Maayan%20Ziv%20and%20Eli%20Galanti%20and%20Amir%20Sheffer%20and%20Saburo%20Howard%20and%20Tristan%20Guillot%20and%20Yohai%20Kaspi&entry.1292438233=%20%20NASA%27s%20Juno%20mission%20provided%20exquisite%20measurements%20of%20Jupiter%27s%20gravity%0Afield%20that%20together%20with%20the%20Galileo%20entry%20probe%20atmospheric%20measurements%0Aconstrains%20the%20interior%20structure%20of%20the%20giant%20planet.%20Inferring%20its%20interior%0Astructure%20range%20remains%20a%20challenging%20inverse%20problem%20requiring%20a%0Acomputationally%20intensive%20search%20of%20combinations%20of%20various%20planetary%0Aproperties%2C%20such%20as%20the%20cloud-level%20temperature%2C%20composition%2C%20and%20core%0Afeatures%2C%20requiring%20the%20computation%20of%20~10%5E9%20interior%20models.%20We%20propose%20an%0Aefficient%20deep%20neural%20network%20%28DNN%29%20model%20to%20generate%20high-precision%0Awide-ranged%20interior%20models%20based%20on%20the%20very%20accurate%20but%20computationally%0Ademanding%20concentric%20MacLaurin%20spheroid%20%28CMS%29%20method.%20We%20trained%20a%0Asharing-based%20DNN%20with%20a%20large%20set%20of%20CMS%20results%20for%20a%20four-layer%20interior%0Amodel%20of%20Jupiter%2C%20including%20a%20dilute%20core%2C%20to%20accurately%20predict%20the%20gravity%0Amoments%20and%20mass%2C%20given%20a%20combination%20of%20interior%20features.%20We%20evaluated%20the%0Aperformance%20of%20the%20trained%20DNN%20%28NeuralCMS%29%20to%20inspect%20its%20predictive%0Alimitations.%20NeuralCMS%20shows%20very%20good%20performance%20in%20predicting%20the%20gravity%0Amoments%2C%20with%20errors%20comparable%20with%20the%20uncertainty%20due%20to%20differential%0Arotation%2C%20and%20a%20very%20accurate%20mass%20prediction.%20This%20allowed%20us%20to%20perform%20a%0Abroad%20parameter%20space%20search%20by%20computing%20only%20~10%5E4%20actual%20CMS%20interior%0Amodels%2C%20resulting%20in%20a%20large%20sample%20of%20plausible%20interior%20structures%2C%20and%0Areducing%20the%20computation%20time%20by%20a%20factor%20of%2010%5E5.%20Moreover%2C%20we%20used%20a%20DNN%0Aexplainability%20algorithm%20to%20analyze%20the%20impact%20of%20the%20parameters%20setting%20the%0Ainterior%20model%20on%20the%20predicted%20observables%2C%20providing%20information%20on%20their%0Anonlinear%20relation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09244v1&entry.124074799=Read"},
{"title": "Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data\n  Sequences", "author": "Samuel Chun-Hei Lam and Justin Sirignano and Konstantinos Spiliopoulos", "abstract": "  Mathematical methods are developed to characterize the asymptotics of\nrecurrent neural networks (RNN) as the number of hidden units, data samples in\nthe sequence, hidden state updates, and training steps simultaneously grow to\ninfinity. In the case of an RNN with a simplified weight matrix, we prove the\nconvergence of the RNN to the solution of an infinite-dimensional ODE coupled\nwith the fixed point of a random algebraic equation. The analysis requires\naddressing several challenges which are unique to RNNs. In typical mean-field\napplications (e.g., feedforward neural networks), discrete updates are of\nmagnitude $\\mathcal{O}(\\frac{1}{N})$ and the number of updates is\n$\\mathcal{O}(N)$. Therefore, the system can be represented as an Euler\napproximation of an appropriate ODE/PDE, which it will converge to as $N\n\\rightarrow \\infty$. However, the RNN hidden layer updates are\n$\\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of\nan ODE/PDE and standard mean-field techniques cannot be applied. Instead, we\ndevelop a fixed point analysis for the evolution of the RNN memory states, with\nconvergence estimates in terms of the number of update steps and the number of\nhidden units. The RNN hidden layer is studied as a function in a Sobolev space,\nwhose evolution is governed by the data sequence (a Markov chain), the\nparameter updates, and its dependence on the RNN hidden layer at the previous\ntime step. Due to the strong correlation between updates, a Poisson equation\nmust be used to bound the fluctuations of the RNN around its limit equation.\nThese mathematical methods give rise to the neural tangent kernel (NTK) limits\nfor RNNs trained on data sequences as the number of data samples and size of\nthe neural network grow to infinity.\n", "link": "http://arxiv.org/abs/2308.14555v2", "date": "2024-05-15", "relevancy": 1.7869, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4313}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kernel%20Limit%20of%20Recurrent%20Neural%20Networks%20Trained%20on%20Ergodic%20Data%0A%20%20Sequences&body=Title%3A%20Kernel%20Limit%20of%20Recurrent%20Neural%20Networks%20Trained%20on%20Ergodic%20Data%0A%20%20Sequences%0AAuthor%3A%20Samuel%20Chun-Hei%20Lam%20and%20Justin%20Sirignano%20and%20Konstantinos%20Spiliopoulos%0AAbstract%3A%20%20%20Mathematical%20methods%20are%20developed%20to%20characterize%20the%20asymptotics%20of%0Arecurrent%20neural%20networks%20%28RNN%29%20as%20the%20number%20of%20hidden%20units%2C%20data%20samples%20in%0Athe%20sequence%2C%20hidden%20state%20updates%2C%20and%20training%20steps%20simultaneously%20grow%20to%0Ainfinity.%20In%20the%20case%20of%20an%20RNN%20with%20a%20simplified%20weight%20matrix%2C%20we%20prove%20the%0Aconvergence%20of%20the%20RNN%20to%20the%20solution%20of%20an%20infinite-dimensional%20ODE%20coupled%0Awith%20the%20fixed%20point%20of%20a%20random%20algebraic%20equation.%20The%20analysis%20requires%0Aaddressing%20several%20challenges%20which%20are%20unique%20to%20RNNs.%20In%20typical%20mean-field%0Aapplications%20%28e.g.%2C%20feedforward%20neural%20networks%29%2C%20discrete%20updates%20are%20of%0Amagnitude%20%24%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29%24%20and%20the%20number%20of%20updates%20is%0A%24%5Cmathcal%7BO%7D%28N%29%24.%20Therefore%2C%20the%20system%20can%20be%20represented%20as%20an%20Euler%0Aapproximation%20of%20an%20appropriate%20ODE/PDE%2C%20which%20it%20will%20converge%20to%20as%20%24N%0A%5Crightarrow%20%5Cinfty%24.%20However%2C%20the%20RNN%20hidden%20layer%20updates%20are%0A%24%5Cmathcal%7BO%7D%281%29%24.%20Therefore%2C%20RNNs%20cannot%20be%20represented%20as%20a%20discretization%20of%0Aan%20ODE/PDE%20and%20standard%20mean-field%20techniques%20cannot%20be%20applied.%20Instead%2C%20we%0Adevelop%20a%20fixed%20point%20analysis%20for%20the%20evolution%20of%20the%20RNN%20memory%20states%2C%20with%0Aconvergence%20estimates%20in%20terms%20of%20the%20number%20of%20update%20steps%20and%20the%20number%20of%0Ahidden%20units.%20The%20RNN%20hidden%20layer%20is%20studied%20as%20a%20function%20in%20a%20Sobolev%20space%2C%0Awhose%20evolution%20is%20governed%20by%20the%20data%20sequence%20%28a%20Markov%20chain%29%2C%20the%0Aparameter%20updates%2C%20and%20its%20dependence%20on%20the%20RNN%20hidden%20layer%20at%20the%20previous%0Atime%20step.%20Due%20to%20the%20strong%20correlation%20between%20updates%2C%20a%20Poisson%20equation%0Amust%20be%20used%20to%20bound%20the%20fluctuations%20of%20the%20RNN%20around%20its%20limit%20equation.%0AThese%20mathematical%20methods%20give%20rise%20to%20the%20neural%20tangent%20kernel%20%28NTK%29%20limits%0Afor%20RNNs%20trained%20on%20data%20sequences%20as%20the%20number%20of%20data%20samples%20and%20size%20of%0Athe%20neural%20network%20grow%20to%20infinity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKernel%2520Limit%2520of%2520Recurrent%2520Neural%2520Networks%2520Trained%2520on%2520Ergodic%2520Data%250A%2520%2520Sequences%26entry.906535625%3DSamuel%2520Chun-Hei%2520Lam%2520and%2520Justin%2520Sirignano%2520and%2520Konstantinos%2520Spiliopoulos%26entry.1292438233%3D%2520%2520Mathematical%2520methods%2520are%2520developed%2520to%2520characterize%2520the%2520asymptotics%2520of%250Arecurrent%2520neural%2520networks%2520%2528RNN%2529%2520as%2520the%2520number%2520of%2520hidden%2520units%252C%2520data%2520samples%2520in%250Athe%2520sequence%252C%2520hidden%2520state%2520updates%252C%2520and%2520training%2520steps%2520simultaneously%2520grow%2520to%250Ainfinity.%2520In%2520the%2520case%2520of%2520an%2520RNN%2520with%2520a%2520simplified%2520weight%2520matrix%252C%2520we%2520prove%2520the%250Aconvergence%2520of%2520the%2520RNN%2520to%2520the%2520solution%2520of%2520an%2520infinite-dimensional%2520ODE%2520coupled%250Awith%2520the%2520fixed%2520point%2520of%2520a%2520random%2520algebraic%2520equation.%2520The%2520analysis%2520requires%250Aaddressing%2520several%2520challenges%2520which%2520are%2520unique%2520to%2520RNNs.%2520In%2520typical%2520mean-field%250Aapplications%2520%2528e.g.%252C%2520feedforward%2520neural%2520networks%2529%252C%2520discrete%2520updates%2520are%2520of%250Amagnitude%2520%2524%255Cmathcal%257BO%257D%2528%255Cfrac%257B1%257D%257BN%257D%2529%2524%2520and%2520the%2520number%2520of%2520updates%2520is%250A%2524%255Cmathcal%257BO%257D%2528N%2529%2524.%2520Therefore%252C%2520the%2520system%2520can%2520be%2520represented%2520as%2520an%2520Euler%250Aapproximation%2520of%2520an%2520appropriate%2520ODE/PDE%252C%2520which%2520it%2520will%2520converge%2520to%2520as%2520%2524N%250A%255Crightarrow%2520%255Cinfty%2524.%2520However%252C%2520the%2520RNN%2520hidden%2520layer%2520updates%2520are%250A%2524%255Cmathcal%257BO%257D%25281%2529%2524.%2520Therefore%252C%2520RNNs%2520cannot%2520be%2520represented%2520as%2520a%2520discretization%2520of%250Aan%2520ODE/PDE%2520and%2520standard%2520mean-field%2520techniques%2520cannot%2520be%2520applied.%2520Instead%252C%2520we%250Adevelop%2520a%2520fixed%2520point%2520analysis%2520for%2520the%2520evolution%2520of%2520the%2520RNN%2520memory%2520states%252C%2520with%250Aconvergence%2520estimates%2520in%2520terms%2520of%2520the%2520number%2520of%2520update%2520steps%2520and%2520the%2520number%2520of%250Ahidden%2520units.%2520The%2520RNN%2520hidden%2520layer%2520is%2520studied%2520as%2520a%2520function%2520in%2520a%2520Sobolev%2520space%252C%250Awhose%2520evolution%2520is%2520governed%2520by%2520the%2520data%2520sequence%2520%2528a%2520Markov%2520chain%2529%252C%2520the%250Aparameter%2520updates%252C%2520and%2520its%2520dependence%2520on%2520the%2520RNN%2520hidden%2520layer%2520at%2520the%2520previous%250Atime%2520step.%2520Due%2520to%2520the%2520strong%2520correlation%2520between%2520updates%252C%2520a%2520Poisson%2520equation%250Amust%2520be%2520used%2520to%2520bound%2520the%2520fluctuations%2520of%2520the%2520RNN%2520around%2520its%2520limit%2520equation.%250AThese%2520mathematical%2520methods%2520give%2520rise%2520to%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%2520limits%250Afor%2520RNNs%2520trained%2520on%2520data%2520sequences%2520as%2520the%2520number%2520of%2520data%2520samples%2520and%2520size%2520of%250Athe%2520neural%2520network%2520grow%2520to%2520infinity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kernel%20Limit%20of%20Recurrent%20Neural%20Networks%20Trained%20on%20Ergodic%20Data%0A%20%20Sequences&entry.906535625=Samuel%20Chun-Hei%20Lam%20and%20Justin%20Sirignano%20and%20Konstantinos%20Spiliopoulos&entry.1292438233=%20%20Mathematical%20methods%20are%20developed%20to%20characterize%20the%20asymptotics%20of%0Arecurrent%20neural%20networks%20%28RNN%29%20as%20the%20number%20of%20hidden%20units%2C%20data%20samples%20in%0Athe%20sequence%2C%20hidden%20state%20updates%2C%20and%20training%20steps%20simultaneously%20grow%20to%0Ainfinity.%20In%20the%20case%20of%20an%20RNN%20with%20a%20simplified%20weight%20matrix%2C%20we%20prove%20the%0Aconvergence%20of%20the%20RNN%20to%20the%20solution%20of%20an%20infinite-dimensional%20ODE%20coupled%0Awith%20the%20fixed%20point%20of%20a%20random%20algebraic%20equation.%20The%20analysis%20requires%0Aaddressing%20several%20challenges%20which%20are%20unique%20to%20RNNs.%20In%20typical%20mean-field%0Aapplications%20%28e.g.%2C%20feedforward%20neural%20networks%29%2C%20discrete%20updates%20are%20of%0Amagnitude%20%24%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29%24%20and%20the%20number%20of%20updates%20is%0A%24%5Cmathcal%7BO%7D%28N%29%24.%20Therefore%2C%20the%20system%20can%20be%20represented%20as%20an%20Euler%0Aapproximation%20of%20an%20appropriate%20ODE/PDE%2C%20which%20it%20will%20converge%20to%20as%20%24N%0A%5Crightarrow%20%5Cinfty%24.%20However%2C%20the%20RNN%20hidden%20layer%20updates%20are%0A%24%5Cmathcal%7BO%7D%281%29%24.%20Therefore%2C%20RNNs%20cannot%20be%20represented%20as%20a%20discretization%20of%0Aan%20ODE/PDE%20and%20standard%20mean-field%20techniques%20cannot%20be%20applied.%20Instead%2C%20we%0Adevelop%20a%20fixed%20point%20analysis%20for%20the%20evolution%20of%20the%20RNN%20memory%20states%2C%20with%0Aconvergence%20estimates%20in%20terms%20of%20the%20number%20of%20update%20steps%20and%20the%20number%20of%0Ahidden%20units.%20The%20RNN%20hidden%20layer%20is%20studied%20as%20a%20function%20in%20a%20Sobolev%20space%2C%0Awhose%20evolution%20is%20governed%20by%20the%20data%20sequence%20%28a%20Markov%20chain%29%2C%20the%0Aparameter%20updates%2C%20and%20its%20dependence%20on%20the%20RNN%20hidden%20layer%20at%20the%20previous%0Atime%20step.%20Due%20to%20the%20strong%20correlation%20between%20updates%2C%20a%20Poisson%20equation%0Amust%20be%20used%20to%20bound%20the%20fluctuations%20of%20the%20RNN%20around%20its%20limit%20equation.%0AThese%20mathematical%20methods%20give%20rise%20to%20the%20neural%20tangent%20kernel%20%28NTK%29%20limits%0Afor%20RNNs%20trained%20on%20data%20sequences%20as%20the%20number%20of%20data%20samples%20and%20size%20of%0Athe%20neural%20network%20grow%20to%20infinity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14555v2&entry.124074799=Read"},
{"title": "ParaNames 1.0: Creating an Entity Name Corpus for 400+ Languages using\n  Wikidata", "author": "Jonne S\u00e4lev\u00e4 and Constantine Lignos", "abstract": "  We introduce ParaNames, a massively multilingual parallel name resource\nconsisting of 140 million names spanning over 400 languages. Names are provided\nfor 16.8 million entities, and each entity is mapped from a complex type\nhierarchy to a standard type (PER/LOC/ORG). Using Wikidata as a source, we\ncreate the largest resource of this type to date. We describe our approach to\nfiltering and standardizing the data to provide the best quality possible.\nParaNames is useful for multilingual language processing, both in defining\ntasks for name translation/transliteration and as supplementary data for tasks\nsuch as named entity recognition and linking. We demonstrate the usefulness of\nParaNames on two tasks. First, we perform canonical name translation between\nEnglish and 17 other languages. Second, we use it as a gazetteer for\nmultilingual named entity recognition, obtaining performance improvements on\nall 10 languages evaluated.\n", "link": "http://arxiv.org/abs/2405.09496v1", "date": "2024-05-15", "relevancy": 1.7746, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3729}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3497}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParaNames%201.0%3A%20Creating%20an%20Entity%20Name%20Corpus%20for%20400%2B%20Languages%20using%0A%20%20Wikidata&body=Title%3A%20ParaNames%201.0%3A%20Creating%20an%20Entity%20Name%20Corpus%20for%20400%2B%20Languages%20using%0A%20%20Wikidata%0AAuthor%3A%20Jonne%20S%C3%A4lev%C3%A4%20and%20Constantine%20Lignos%0AAbstract%3A%20%20%20We%20introduce%20ParaNames%2C%20a%20massively%20multilingual%20parallel%20name%20resource%0Aconsisting%20of%20140%20million%20names%20spanning%20over%20400%20languages.%20Names%20are%20provided%0Afor%2016.8%20million%20entities%2C%20and%20each%20entity%20is%20mapped%20from%20a%20complex%20type%0Ahierarchy%20to%20a%20standard%20type%20%28PER/LOC/ORG%29.%20Using%20Wikidata%20as%20a%20source%2C%20we%0Acreate%20the%20largest%20resource%20of%20this%20type%20to%20date.%20We%20describe%20our%20approach%20to%0Afiltering%20and%20standardizing%20the%20data%20to%20provide%20the%20best%20quality%20possible.%0AParaNames%20is%20useful%20for%20multilingual%20language%20processing%2C%20both%20in%20defining%0Atasks%20for%20name%20translation/transliteration%20and%20as%20supplementary%20data%20for%20tasks%0Asuch%20as%20named%20entity%20recognition%20and%20linking.%20We%20demonstrate%20the%20usefulness%20of%0AParaNames%20on%20two%20tasks.%20First%2C%20we%20perform%20canonical%20name%20translation%20between%0AEnglish%20and%2017%20other%20languages.%20Second%2C%20we%20use%20it%20as%20a%20gazetteer%20for%0Amultilingual%20named%20entity%20recognition%2C%20obtaining%20performance%20improvements%20on%0Aall%2010%20languages%20evaluated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParaNames%25201.0%253A%2520Creating%2520an%2520Entity%2520Name%2520Corpus%2520for%2520400%252B%2520Languages%2520using%250A%2520%2520Wikidata%26entry.906535625%3DJonne%2520S%25C3%25A4lev%25C3%25A4%2520and%2520Constantine%2520Lignos%26entry.1292438233%3D%2520%2520We%2520introduce%2520ParaNames%252C%2520a%2520massively%2520multilingual%2520parallel%2520name%2520resource%250Aconsisting%2520of%2520140%2520million%2520names%2520spanning%2520over%2520400%2520languages.%2520Names%2520are%2520provided%250Afor%252016.8%2520million%2520entities%252C%2520and%2520each%2520entity%2520is%2520mapped%2520from%2520a%2520complex%2520type%250Ahierarchy%2520to%2520a%2520standard%2520type%2520%2528PER/LOC/ORG%2529.%2520Using%2520Wikidata%2520as%2520a%2520source%252C%2520we%250Acreate%2520the%2520largest%2520resource%2520of%2520this%2520type%2520to%2520date.%2520We%2520describe%2520our%2520approach%2520to%250Afiltering%2520and%2520standardizing%2520the%2520data%2520to%2520provide%2520the%2520best%2520quality%2520possible.%250AParaNames%2520is%2520useful%2520for%2520multilingual%2520language%2520processing%252C%2520both%2520in%2520defining%250Atasks%2520for%2520name%2520translation/transliteration%2520and%2520as%2520supplementary%2520data%2520for%2520tasks%250Asuch%2520as%2520named%2520entity%2520recognition%2520and%2520linking.%2520We%2520demonstrate%2520the%2520usefulness%2520of%250AParaNames%2520on%2520two%2520tasks.%2520First%252C%2520we%2520perform%2520canonical%2520name%2520translation%2520between%250AEnglish%2520and%252017%2520other%2520languages.%2520Second%252C%2520we%2520use%2520it%2520as%2520a%2520gazetteer%2520for%250Amultilingual%2520named%2520entity%2520recognition%252C%2520obtaining%2520performance%2520improvements%2520on%250Aall%252010%2520languages%2520evaluated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParaNames%201.0%3A%20Creating%20an%20Entity%20Name%20Corpus%20for%20400%2B%20Languages%20using%0A%20%20Wikidata&entry.906535625=Jonne%20S%C3%A4lev%C3%A4%20and%20Constantine%20Lignos&entry.1292438233=%20%20We%20introduce%20ParaNames%2C%20a%20massively%20multilingual%20parallel%20name%20resource%0Aconsisting%20of%20140%20million%20names%20spanning%20over%20400%20languages.%20Names%20are%20provided%0Afor%2016.8%20million%20entities%2C%20and%20each%20entity%20is%20mapped%20from%20a%20complex%20type%0Ahierarchy%20to%20a%20standard%20type%20%28PER/LOC/ORG%29.%20Using%20Wikidata%20as%20a%20source%2C%20we%0Acreate%20the%20largest%20resource%20of%20this%20type%20to%20date.%20We%20describe%20our%20approach%20to%0Afiltering%20and%20standardizing%20the%20data%20to%20provide%20the%20best%20quality%20possible.%0AParaNames%20is%20useful%20for%20multilingual%20language%20processing%2C%20both%20in%20defining%0Atasks%20for%20name%20translation/transliteration%20and%20as%20supplementary%20data%20for%20tasks%0Asuch%20as%20named%20entity%20recognition%20and%20linking.%20We%20demonstrate%20the%20usefulness%20of%0AParaNames%20on%20two%20tasks.%20First%2C%20we%20perform%20canonical%20name%20translation%20between%0AEnglish%20and%2017%20other%20languages.%20Second%2C%20we%20use%20it%20as%20a%20gazetteer%20for%0Amultilingual%20named%20entity%20recognition%2C%20obtaining%20performance%20improvements%20on%0Aall%2010%20languages%20evaluated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09496v1&entry.124074799=Read"},
{"title": "Adversarial Consistency and the Uniqueness of the Adversarial Bayes\n  Classifier", "author": "Natalie S. Frank", "abstract": "  Adversarial training is a common technique for learning robust classifiers.\nPrior work showed that convex surrogate losses are not statistically consistent\nin the adversarial context -- or in other words, a minimizing sequence of the\nadversarial surrogate risk will not necessarily minimize the adversarial\nclassification error. We connect the consistency of adversarial surrogate\nlosses to properties of minimizers to the adversarial classification risk,\nknown as \\emph{adversarial Bayes classifiers}. Specifically, under reasonable\ndistributional assumptions, a convex loss is statistically consistent for\nadversarial learning iff the adversarial Bayes classifier satisfies a certain\nnotion of uniqueness.\n", "link": "http://arxiv.org/abs/2404.17358v2", "date": "2024-05-15", "relevancy": 1.7606, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4253}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier&body=Title%3A%20Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier%0AAuthor%3A%20Natalie%20S.%20Frank%0AAbstract%3A%20%20%20Adversarial%20training%20is%20a%20common%20technique%20for%20learning%20robust%20classifiers.%0APrior%20work%20showed%20that%20convex%20surrogate%20losses%20are%20not%20statistically%20consistent%0Ain%20the%20adversarial%20context%20--%20or%20in%20other%20words%2C%20a%20minimizing%20sequence%20of%20the%0Aadversarial%20surrogate%20risk%20will%20not%20necessarily%20minimize%20the%20adversarial%0Aclassification%20error.%20We%20connect%20the%20consistency%20of%20adversarial%20surrogate%0Alosses%20to%20properties%20of%20minimizers%20to%20the%20adversarial%20classification%20risk%2C%0Aknown%20as%20%5Cemph%7Badversarial%20Bayes%20classifiers%7D.%20Specifically%2C%20under%20reasonable%0Adistributional%20assumptions%2C%20a%20convex%20loss%20is%20statistically%20consistent%20for%0Aadversarial%20learning%20iff%20the%20adversarial%20Bayes%20classifier%20satisfies%20a%20certain%0Anotion%20of%20uniqueness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Consistency%2520and%2520the%2520Uniqueness%2520of%2520the%2520Adversarial%2520Bayes%250A%2520%2520Classifier%26entry.906535625%3DNatalie%2520S.%2520Frank%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520is%2520a%2520common%2520technique%2520for%2520learning%2520robust%2520classifiers.%250APrior%2520work%2520showed%2520that%2520convex%2520surrogate%2520losses%2520are%2520not%2520statistically%2520consistent%250Ain%2520the%2520adversarial%2520context%2520--%2520or%2520in%2520other%2520words%252C%2520a%2520minimizing%2520sequence%2520of%2520the%250Aadversarial%2520surrogate%2520risk%2520will%2520not%2520necessarily%2520minimize%2520the%2520adversarial%250Aclassification%2520error.%2520We%2520connect%2520the%2520consistency%2520of%2520adversarial%2520surrogate%250Alosses%2520to%2520properties%2520of%2520minimizers%2520to%2520the%2520adversarial%2520classification%2520risk%252C%250Aknown%2520as%2520%255Cemph%257Badversarial%2520Bayes%2520classifiers%257D.%2520Specifically%252C%2520under%2520reasonable%250Adistributional%2520assumptions%252C%2520a%2520convex%2520loss%2520is%2520statistically%2520consistent%2520for%250Aadversarial%2520learning%2520iff%2520the%2520adversarial%2520Bayes%2520classifier%2520satisfies%2520a%2520certain%250Anotion%2520of%2520uniqueness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Consistency%20and%20the%20Uniqueness%20of%20the%20Adversarial%20Bayes%0A%20%20Classifier&entry.906535625=Natalie%20S.%20Frank&entry.1292438233=%20%20Adversarial%20training%20is%20a%20common%20technique%20for%20learning%20robust%20classifiers.%0APrior%20work%20showed%20that%20convex%20surrogate%20losses%20are%20not%20statistically%20consistent%0Ain%20the%20adversarial%20context%20--%20or%20in%20other%20words%2C%20a%20minimizing%20sequence%20of%20the%0Aadversarial%20surrogate%20risk%20will%20not%20necessarily%20minimize%20the%20adversarial%0Aclassification%20error.%20We%20connect%20the%20consistency%20of%20adversarial%20surrogate%0Alosses%20to%20properties%20of%20minimizers%20to%20the%20adversarial%20classification%20risk%2C%0Aknown%20as%20%5Cemph%7Badversarial%20Bayes%20classifiers%7D.%20Specifically%2C%20under%20reasonable%0Adistributional%20assumptions%2C%20a%20convex%20loss%20is%20statistically%20consistent%20for%0Aadversarial%20learning%20iff%20the%20adversarial%20Bayes%20classifier%20satisfies%20a%20certain%0Anotion%20of%20uniqueness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17358v2&entry.124074799=Read"},
{"title": "Spectral complexity of deep neural networks", "author": "Simmaco Di Lillo and Domenico Marinucci and Michele Salvi and Stefano Vigogna", "abstract": "  It is well-known that randomly initialized, push-forward, fully-connected\nneural networks weakly converge to isotropic Gaussian processes, in the limit\nwhere the width of all layers goes to infinity. In this paper, we propose to\nuse the angular power spectrum of the limiting field to characterize the\ncomplexity of the network architecture. In particular, we define sequences of\nrandom variables associated with the angular power spectrum, and provide a full\ncharacterization of the network complexity in terms of the asymptotic\ndistribution of these sequences as the depth diverges. On this basis, we\nclassify neural networks as low-disorder, sparse, or high-disorder; we show how\nthis classification highlights a number of distinct features for standard\nactivation functions, and in particular, sparsity properties of ReLU networks.\nOur theoretical results are also validated by numerical simulations.\n", "link": "http://arxiv.org/abs/2405.09541v1", "date": "2024-05-15", "relevancy": 1.7518, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4278}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20complexity%20of%20deep%20neural%20networks&body=Title%3A%20Spectral%20complexity%20of%20deep%20neural%20networks%0AAuthor%3A%20Simmaco%20Di%20Lillo%20and%20Domenico%20Marinucci%20and%20Michele%20Salvi%20and%20Stefano%20Vigogna%0AAbstract%3A%20%20%20It%20is%20well-known%20that%20randomly%20initialized%2C%20push-forward%2C%20fully-connected%0Aneural%20networks%20weakly%20converge%20to%20isotropic%20Gaussian%20processes%2C%20in%20the%20limit%0Awhere%20the%20width%20of%20all%20layers%20goes%20to%20infinity.%20In%20this%20paper%2C%20we%20propose%20to%0Ause%20the%20angular%20power%20spectrum%20of%20the%20limiting%20field%20to%20characterize%20the%0Acomplexity%20of%20the%20network%20architecture.%20In%20particular%2C%20we%20define%20sequences%20of%0Arandom%20variables%20associated%20with%20the%20angular%20power%20spectrum%2C%20and%20provide%20a%20full%0Acharacterization%20of%20the%20network%20complexity%20in%20terms%20of%20the%20asymptotic%0Adistribution%20of%20these%20sequences%20as%20the%20depth%20diverges.%20On%20this%20basis%2C%20we%0Aclassify%20neural%20networks%20as%20low-disorder%2C%20sparse%2C%20or%20high-disorder%3B%20we%20show%20how%0Athis%20classification%20highlights%20a%20number%20of%20distinct%20features%20for%20standard%0Aactivation%20functions%2C%20and%20in%20particular%2C%20sparsity%20properties%20of%20ReLU%20networks.%0AOur%20theoretical%20results%20are%20also%20validated%20by%20numerical%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520complexity%2520of%2520deep%2520neural%2520networks%26entry.906535625%3DSimmaco%2520Di%2520Lillo%2520and%2520Domenico%2520Marinucci%2520and%2520Michele%2520Salvi%2520and%2520Stefano%2520Vigogna%26entry.1292438233%3D%2520%2520It%2520is%2520well-known%2520that%2520randomly%2520initialized%252C%2520push-forward%252C%2520fully-connected%250Aneural%2520networks%2520weakly%2520converge%2520to%2520isotropic%2520Gaussian%2520processes%252C%2520in%2520the%2520limit%250Awhere%2520the%2520width%2520of%2520all%2520layers%2520goes%2520to%2520infinity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%250Ause%2520the%2520angular%2520power%2520spectrum%2520of%2520the%2520limiting%2520field%2520to%2520characterize%2520the%250Acomplexity%2520of%2520the%2520network%2520architecture.%2520In%2520particular%252C%2520we%2520define%2520sequences%2520of%250Arandom%2520variables%2520associated%2520with%2520the%2520angular%2520power%2520spectrum%252C%2520and%2520provide%2520a%2520full%250Acharacterization%2520of%2520the%2520network%2520complexity%2520in%2520terms%2520of%2520the%2520asymptotic%250Adistribution%2520of%2520these%2520sequences%2520as%2520the%2520depth%2520diverges.%2520On%2520this%2520basis%252C%2520we%250Aclassify%2520neural%2520networks%2520as%2520low-disorder%252C%2520sparse%252C%2520or%2520high-disorder%253B%2520we%2520show%2520how%250Athis%2520classification%2520highlights%2520a%2520number%2520of%2520distinct%2520features%2520for%2520standard%250Aactivation%2520functions%252C%2520and%2520in%2520particular%252C%2520sparsity%2520properties%2520of%2520ReLU%2520networks.%250AOur%2520theoretical%2520results%2520are%2520also%2520validated%2520by%2520numerical%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20complexity%20of%20deep%20neural%20networks&entry.906535625=Simmaco%20Di%20Lillo%20and%20Domenico%20Marinucci%20and%20Michele%20Salvi%20and%20Stefano%20Vigogna&entry.1292438233=%20%20It%20is%20well-known%20that%20randomly%20initialized%2C%20push-forward%2C%20fully-connected%0Aneural%20networks%20weakly%20converge%20to%20isotropic%20Gaussian%20processes%2C%20in%20the%20limit%0Awhere%20the%20width%20of%20all%20layers%20goes%20to%20infinity.%20In%20this%20paper%2C%20we%20propose%20to%0Ause%20the%20angular%20power%20spectrum%20of%20the%20limiting%20field%20to%20characterize%20the%0Acomplexity%20of%20the%20network%20architecture.%20In%20particular%2C%20we%20define%20sequences%20of%0Arandom%20variables%20associated%20with%20the%20angular%20power%20spectrum%2C%20and%20provide%20a%20full%0Acharacterization%20of%20the%20network%20complexity%20in%20terms%20of%20the%20asymptotic%0Adistribution%20of%20these%20sequences%20as%20the%20depth%20diverges.%20On%20this%20basis%2C%20we%0Aclassify%20neural%20networks%20as%20low-disorder%2C%20sparse%2C%20or%20high-disorder%3B%20we%20show%20how%0Athis%20classification%20highlights%20a%20number%20of%20distinct%20features%20for%20standard%0Aactivation%20functions%2C%20and%20in%20particular%2C%20sparsity%20properties%20of%20ReLU%20networks.%0AOur%20theoretical%20results%20are%20also%20validated%20by%20numerical%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09541v1&entry.124074799=Read"},
{"title": "Word Alignment as Preference for Machine Translation", "author": "Qiyu Wu and Masaaki Nagata and Zhongtao Miao and Yoshimasa Tsuruoka", "abstract": "  The problem of hallucination and omission, a long-standing problem in machine\ntranslation (MT), is more pronounced when a large language model (LLM) is used\nin MT because an LLM itself is susceptible to these phenomena. In this work, we\nmitigate the problem in an LLM-based MT model by guiding it to better word\nalignment. We first study the correlation between word alignment and the\nphenomena of hallucination and omission in MT. Then we propose to utilize word\nalignment as preference to optimize the LLM-based MT model. The preference data\nare constructed by selecting chosen and rejected translations from multiple MT\ntools. Subsequently, direct preference optimization is used to optimize the\nLLM-based model towards the preference signal. Given the absence of evaluators\nspecifically designed for hallucination and omission in MT, we further propose\nselecting hard instances and utilizing GPT-4 to directly evaluate the\nperformance of the models in mitigating these issues. We verify the rationality\nof these designed evaluation methods by experiments, followed by extensive\nresults demonstrating the effectiveness of word alignment-based preference\noptimization to mitigate hallucination and omission.\n", "link": "http://arxiv.org/abs/2405.09223v1", "date": "2024-05-15", "relevancy": 1.7436, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4456}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Word%20Alignment%20as%20Preference%20for%20Machine%20Translation&body=Title%3A%20Word%20Alignment%20as%20Preference%20for%20Machine%20Translation%0AAuthor%3A%20Qiyu%20Wu%20and%20Masaaki%20Nagata%20and%20Zhongtao%20Miao%20and%20Yoshimasa%20Tsuruoka%0AAbstract%3A%20%20%20The%20problem%20of%20hallucination%20and%20omission%2C%20a%20long-standing%20problem%20in%20machine%0Atranslation%20%28MT%29%2C%20is%20more%20pronounced%20when%20a%20large%20language%20model%20%28LLM%29%20is%20used%0Ain%20MT%20because%20an%20LLM%20itself%20is%20susceptible%20to%20these%20phenomena.%20In%20this%20work%2C%20we%0Amitigate%20the%20problem%20in%20an%20LLM-based%20MT%20model%20by%20guiding%20it%20to%20better%20word%0Aalignment.%20We%20first%20study%20the%20correlation%20between%20word%20alignment%20and%20the%0Aphenomena%20of%20hallucination%20and%20omission%20in%20MT.%20Then%20we%20propose%20to%20utilize%20word%0Aalignment%20as%20preference%20to%20optimize%20the%20LLM-based%20MT%20model.%20The%20preference%20data%0Aare%20constructed%20by%20selecting%20chosen%20and%20rejected%20translations%20from%20multiple%20MT%0Atools.%20Subsequently%2C%20direct%20preference%20optimization%20is%20used%20to%20optimize%20the%0ALLM-based%20model%20towards%20the%20preference%20signal.%20Given%20the%20absence%20of%20evaluators%0Aspecifically%20designed%20for%20hallucination%20and%20omission%20in%20MT%2C%20we%20further%20propose%0Aselecting%20hard%20instances%20and%20utilizing%20GPT-4%20to%20directly%20evaluate%20the%0Aperformance%20of%20the%20models%20in%20mitigating%20these%20issues.%20We%20verify%20the%20rationality%0Aof%20these%20designed%20evaluation%20methods%20by%20experiments%2C%20followed%20by%20extensive%0Aresults%20demonstrating%20the%20effectiveness%20of%20word%20alignment-based%20preference%0Aoptimization%20to%20mitigate%20hallucination%20and%20omission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWord%2520Alignment%2520as%2520Preference%2520for%2520Machine%2520Translation%26entry.906535625%3DQiyu%2520Wu%2520and%2520Masaaki%2520Nagata%2520and%2520Zhongtao%2520Miao%2520and%2520Yoshimasa%2520Tsuruoka%26entry.1292438233%3D%2520%2520The%2520problem%2520of%2520hallucination%2520and%2520omission%252C%2520a%2520long-standing%2520problem%2520in%2520machine%250Atranslation%2520%2528MT%2529%252C%2520is%2520more%2520pronounced%2520when%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520is%2520used%250Ain%2520MT%2520because%2520an%2520LLM%2520itself%2520is%2520susceptible%2520to%2520these%2520phenomena.%2520In%2520this%2520work%252C%2520we%250Amitigate%2520the%2520problem%2520in%2520an%2520LLM-based%2520MT%2520model%2520by%2520guiding%2520it%2520to%2520better%2520word%250Aalignment.%2520We%2520first%2520study%2520the%2520correlation%2520between%2520word%2520alignment%2520and%2520the%250Aphenomena%2520of%2520hallucination%2520and%2520omission%2520in%2520MT.%2520Then%2520we%2520propose%2520to%2520utilize%2520word%250Aalignment%2520as%2520preference%2520to%2520optimize%2520the%2520LLM-based%2520MT%2520model.%2520The%2520preference%2520data%250Aare%2520constructed%2520by%2520selecting%2520chosen%2520and%2520rejected%2520translations%2520from%2520multiple%2520MT%250Atools.%2520Subsequently%252C%2520direct%2520preference%2520optimization%2520is%2520used%2520to%2520optimize%2520the%250ALLM-based%2520model%2520towards%2520the%2520preference%2520signal.%2520Given%2520the%2520absence%2520of%2520evaluators%250Aspecifically%2520designed%2520for%2520hallucination%2520and%2520omission%2520in%2520MT%252C%2520we%2520further%2520propose%250Aselecting%2520hard%2520instances%2520and%2520utilizing%2520GPT-4%2520to%2520directly%2520evaluate%2520the%250Aperformance%2520of%2520the%2520models%2520in%2520mitigating%2520these%2520issues.%2520We%2520verify%2520the%2520rationality%250Aof%2520these%2520designed%2520evaluation%2520methods%2520by%2520experiments%252C%2520followed%2520by%2520extensive%250Aresults%2520demonstrating%2520the%2520effectiveness%2520of%2520word%2520alignment-based%2520preference%250Aoptimization%2520to%2520mitigate%2520hallucination%2520and%2520omission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word%20Alignment%20as%20Preference%20for%20Machine%20Translation&entry.906535625=Qiyu%20Wu%20and%20Masaaki%20Nagata%20and%20Zhongtao%20Miao%20and%20Yoshimasa%20Tsuruoka&entry.1292438233=%20%20The%20problem%20of%20hallucination%20and%20omission%2C%20a%20long-standing%20problem%20in%20machine%0Atranslation%20%28MT%29%2C%20is%20more%20pronounced%20when%20a%20large%20language%20model%20%28LLM%29%20is%20used%0Ain%20MT%20because%20an%20LLM%20itself%20is%20susceptible%20to%20these%20phenomena.%20In%20this%20work%2C%20we%0Amitigate%20the%20problem%20in%20an%20LLM-based%20MT%20model%20by%20guiding%20it%20to%20better%20word%0Aalignment.%20We%20first%20study%20the%20correlation%20between%20word%20alignment%20and%20the%0Aphenomena%20of%20hallucination%20and%20omission%20in%20MT.%20Then%20we%20propose%20to%20utilize%20word%0Aalignment%20as%20preference%20to%20optimize%20the%20LLM-based%20MT%20model.%20The%20preference%20data%0Aare%20constructed%20by%20selecting%20chosen%20and%20rejected%20translations%20from%20multiple%20MT%0Atools.%20Subsequently%2C%20direct%20preference%20optimization%20is%20used%20to%20optimize%20the%0ALLM-based%20model%20towards%20the%20preference%20signal.%20Given%20the%20absence%20of%20evaluators%0Aspecifically%20designed%20for%20hallucination%20and%20omission%20in%20MT%2C%20we%20further%20propose%0Aselecting%20hard%20instances%20and%20utilizing%20GPT-4%20to%20directly%20evaluate%20the%0Aperformance%20of%20the%20models%20in%20mitigating%20these%20issues.%20We%20verify%20the%20rationality%0Aof%20these%20designed%20evaluation%20methods%20by%20experiments%2C%20followed%20by%20extensive%0Aresults%20demonstrating%20the%20effectiveness%20of%20word%20alignment-based%20preference%0Aoptimization%20to%20mitigate%20hallucination%20and%20omission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09223v1&entry.124074799=Read"},
{"title": "Polarimetric Light Transport Analysis for Specular Inter-reflection", "author": "Ryota Maeda and Shinsaku Hiura", "abstract": "  Polarization is well known for its ability to decompose diffuse and specular\nreflections. However, the existing decomposition methods only focus on direct\nreflection and overlook multiple reflections, especially specular\ninter-reflection. In this paper, we propose a novel decomposition method for\nhandling specular inter-reflection of metal objects by using a unique\npolarimetric feature: the rotation direction of linear polarization. This\nrotation direction serves as a discriminative factor between direct and\ninter-reflection on specular surfaces. To decompose the reflectance components,\nwe actively rotate the linear polarization of incident light and analyze the\nrotation direction of the reflected light. We evaluate our method using both\nsynthetic and real data, demonstrating its effectiveness in decomposing\nspecular inter-reflections of metal objects. Furthermore, we demonstrate that\nour method can be combined with other decomposition methods for a detailed\nanalysis of light transport. As a practical application, we show its\neffectiveness in improving the accuracy of 3D measurement against strong\nspecular inter-reflection.\n", "link": "http://arxiv.org/abs/2312.04140v2", "date": "2024-05-15", "relevancy": 1.7145, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4464}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polarimetric%20Light%20Transport%20Analysis%20for%20Specular%20Inter-reflection&body=Title%3A%20Polarimetric%20Light%20Transport%20Analysis%20for%20Specular%20Inter-reflection%0AAuthor%3A%20Ryota%20Maeda%20and%20Shinsaku%20Hiura%0AAbstract%3A%20%20%20Polarization%20is%20well%20known%20for%20its%20ability%20to%20decompose%20diffuse%20and%20specular%0Areflections.%20However%2C%20the%20existing%20decomposition%20methods%20only%20focus%20on%20direct%0Areflection%20and%20overlook%20multiple%20reflections%2C%20especially%20specular%0Ainter-reflection.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decomposition%20method%20for%0Ahandling%20specular%20inter-reflection%20of%20metal%20objects%20by%20using%20a%20unique%0Apolarimetric%20feature%3A%20the%20rotation%20direction%20of%20linear%20polarization.%20This%0Arotation%20direction%20serves%20as%20a%20discriminative%20factor%20between%20direct%20and%0Ainter-reflection%20on%20specular%20surfaces.%20To%20decompose%20the%20reflectance%20components%2C%0Awe%20actively%20rotate%20the%20linear%20polarization%20of%20incident%20light%20and%20analyze%20the%0Arotation%20direction%20of%20the%20reflected%20light.%20We%20evaluate%20our%20method%20using%20both%0Asynthetic%20and%20real%20data%2C%20demonstrating%20its%20effectiveness%20in%20decomposing%0Aspecular%20inter-reflections%20of%20metal%20objects.%20Furthermore%2C%20we%20demonstrate%20that%0Aour%20method%20can%20be%20combined%20with%20other%20decomposition%20methods%20for%20a%20detailed%0Aanalysis%20of%20light%20transport.%20As%20a%20practical%20application%2C%20we%20show%20its%0Aeffectiveness%20in%20improving%20the%20accuracy%20of%203D%20measurement%20against%20strong%0Aspecular%20inter-reflection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarimetric%2520Light%2520Transport%2520Analysis%2520for%2520Specular%2520Inter-reflection%26entry.906535625%3DRyota%2520Maeda%2520and%2520Shinsaku%2520Hiura%26entry.1292438233%3D%2520%2520Polarization%2520is%2520well%2520known%2520for%2520its%2520ability%2520to%2520decompose%2520diffuse%2520and%2520specular%250Areflections.%2520However%252C%2520the%2520existing%2520decomposition%2520methods%2520only%2520focus%2520on%2520direct%250Areflection%2520and%2520overlook%2520multiple%2520reflections%252C%2520especially%2520specular%250Ainter-reflection.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520decomposition%2520method%2520for%250Ahandling%2520specular%2520inter-reflection%2520of%2520metal%2520objects%2520by%2520using%2520a%2520unique%250Apolarimetric%2520feature%253A%2520the%2520rotation%2520direction%2520of%2520linear%2520polarization.%2520This%250Arotation%2520direction%2520serves%2520as%2520a%2520discriminative%2520factor%2520between%2520direct%2520and%250Ainter-reflection%2520on%2520specular%2520surfaces.%2520To%2520decompose%2520the%2520reflectance%2520components%252C%250Awe%2520actively%2520rotate%2520the%2520linear%2520polarization%2520of%2520incident%2520light%2520and%2520analyze%2520the%250Arotation%2520direction%2520of%2520the%2520reflected%2520light.%2520We%2520evaluate%2520our%2520method%2520using%2520both%250Asynthetic%2520and%2520real%2520data%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520decomposing%250Aspecular%2520inter-reflections%2520of%2520metal%2520objects.%2520Furthermore%252C%2520we%2520demonstrate%2520that%250Aour%2520method%2520can%2520be%2520combined%2520with%2520other%2520decomposition%2520methods%2520for%2520a%2520detailed%250Aanalysis%2520of%2520light%2520transport.%2520As%2520a%2520practical%2520application%252C%2520we%2520show%2520its%250Aeffectiveness%2520in%2520improving%2520the%2520accuracy%2520of%25203D%2520measurement%2520against%2520strong%250Aspecular%2520inter-reflection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polarimetric%20Light%20Transport%20Analysis%20for%20Specular%20Inter-reflection&entry.906535625=Ryota%20Maeda%20and%20Shinsaku%20Hiura&entry.1292438233=%20%20Polarization%20is%20well%20known%20for%20its%20ability%20to%20decompose%20diffuse%20and%20specular%0Areflections.%20However%2C%20the%20existing%20decomposition%20methods%20only%20focus%20on%20direct%0Areflection%20and%20overlook%20multiple%20reflections%2C%20especially%20specular%0Ainter-reflection.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20decomposition%20method%20for%0Ahandling%20specular%20inter-reflection%20of%20metal%20objects%20by%20using%20a%20unique%0Apolarimetric%20feature%3A%20the%20rotation%20direction%20of%20linear%20polarization.%20This%0Arotation%20direction%20serves%20as%20a%20discriminative%20factor%20between%20direct%20and%0Ainter-reflection%20on%20specular%20surfaces.%20To%20decompose%20the%20reflectance%20components%2C%0Awe%20actively%20rotate%20the%20linear%20polarization%20of%20incident%20light%20and%20analyze%20the%0Arotation%20direction%20of%20the%20reflected%20light.%20We%20evaluate%20our%20method%20using%20both%0Asynthetic%20and%20real%20data%2C%20demonstrating%20its%20effectiveness%20in%20decomposing%0Aspecular%20inter-reflections%20of%20metal%20objects.%20Furthermore%2C%20we%20demonstrate%20that%0Aour%20method%20can%20be%20combined%20with%20other%20decomposition%20methods%20for%20a%20detailed%0Aanalysis%20of%20light%20transport.%20As%20a%20practical%20application%2C%20we%20show%20its%0Aeffectiveness%20in%20improving%20the%20accuracy%20of%203D%20measurement%20against%20strong%0Aspecular%20inter-reflection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04140v2&entry.124074799=Read"},
{"title": "3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple\n  3D Representations", "author": "Yanjie Ze and Gu Zhang and Kangning Zhang and Chenyuan Hu and Muhan Wang and Huazhe Xu", "abstract": "  Imitation learning provides an efficient way to teach robots dexterous\nskills; however, learning complex skills robustly and generalizablely usually\nconsumes large amounts of human demonstrations. To tackle this challenging\nproblem, we present 3D Diffusion Policy (DP3), a novel visual imitation\nlearning approach that incorporates the power of 3D visual representations into\ndiffusion policies, a class of conditional action generative models. The core\ndesign of DP3 is the utilization of a compact 3D visual representation,\nextracted from sparse point clouds with an efficient point encoder. In our\nexperiments involving 72 simulation tasks, DP3 successfully handles most tasks\nwith just 10 demonstrations and surpasses baselines with a 24.2% relative\nimprovement. In 4 real robot tasks, DP3 demonstrates precise control with a\nhigh success rate of 85%, given only 40 demonstrations of each task, and shows\nexcellent generalization abilities in diverse aspects, including space,\nviewpoint, appearance, and instance. Interestingly, in real robot experiments,\nDP3 rarely violates safety requirements, in contrast to baseline methods which\nfrequently do, necessitating human intervention. Our extensive evaluation\nhighlights the critical importance of 3D representations in real-world robot\nlearning. Videos, code, and data are available on\nhttps://3d-diffusion-policy.github.io .\n", "link": "http://arxiv.org/abs/2403.03954v4", "date": "2024-05-15", "relevancy": 1.7071, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5909}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5714}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Diffusion%20Policy%3A%20Generalizable%20Visuomotor%20Policy%20Learning%20via%20Simple%0A%20%203D%20Representations&body=Title%3A%203D%20Diffusion%20Policy%3A%20Generalizable%20Visuomotor%20Policy%20Learning%20via%20Simple%0A%20%203D%20Representations%0AAuthor%3A%20Yanjie%20Ze%20and%20Gu%20Zhang%20and%20Kangning%20Zhang%20and%20Chenyuan%20Hu%20and%20Muhan%20Wang%20and%20Huazhe%20Xu%0AAbstract%3A%20%20%20Imitation%20learning%20provides%20an%20efficient%20way%20to%20teach%20robots%20dexterous%0Askills%3B%20however%2C%20learning%20complex%20skills%20robustly%20and%20generalizablely%20usually%0Aconsumes%20large%20amounts%20of%20human%20demonstrations.%20To%20tackle%20this%20challenging%0Aproblem%2C%20we%20present%203D%20Diffusion%20Policy%20%28DP3%29%2C%20a%20novel%20visual%20imitation%0Alearning%20approach%20that%20incorporates%20the%20power%20of%203D%20visual%20representations%20into%0Adiffusion%20policies%2C%20a%20class%20of%20conditional%20action%20generative%20models.%20The%20core%0Adesign%20of%20DP3%20is%20the%20utilization%20of%20a%20compact%203D%20visual%20representation%2C%0Aextracted%20from%20sparse%20point%20clouds%20with%20an%20efficient%20point%20encoder.%20In%20our%0Aexperiments%20involving%2072%20simulation%20tasks%2C%20DP3%20successfully%20handles%20most%20tasks%0Awith%20just%2010%20demonstrations%20and%20surpasses%20baselines%20with%20a%2024.2%25%20relative%0Aimprovement.%20In%204%20real%20robot%20tasks%2C%20DP3%20demonstrates%20precise%20control%20with%20a%0Ahigh%20success%20rate%20of%2085%25%2C%20given%20only%2040%20demonstrations%20of%20each%20task%2C%20and%20shows%0Aexcellent%20generalization%20abilities%20in%20diverse%20aspects%2C%20including%20space%2C%0Aviewpoint%2C%20appearance%2C%20and%20instance.%20Interestingly%2C%20in%20real%20robot%20experiments%2C%0ADP3%20rarely%20violates%20safety%20requirements%2C%20in%20contrast%20to%20baseline%20methods%20which%0Afrequently%20do%2C%20necessitating%20human%20intervention.%20Our%20extensive%20evaluation%0Ahighlights%20the%20critical%20importance%20of%203D%20representations%20in%20real-world%20robot%0Alearning.%20Videos%2C%20code%2C%20and%20data%20are%20available%20on%0Ahttps%3A//3d-diffusion-policy.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03954v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Diffusion%2520Policy%253A%2520Generalizable%2520Visuomotor%2520Policy%2520Learning%2520via%2520Simple%250A%2520%25203D%2520Representations%26entry.906535625%3DYanjie%2520Ze%2520and%2520Gu%2520Zhang%2520and%2520Kangning%2520Zhang%2520and%2520Chenyuan%2520Hu%2520and%2520Muhan%2520Wang%2520and%2520Huazhe%2520Xu%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520provides%2520an%2520efficient%2520way%2520to%2520teach%2520robots%2520dexterous%250Askills%253B%2520however%252C%2520learning%2520complex%2520skills%2520robustly%2520and%2520generalizablely%2520usually%250Aconsumes%2520large%2520amounts%2520of%2520human%2520demonstrations.%2520To%2520tackle%2520this%2520challenging%250Aproblem%252C%2520we%2520present%25203D%2520Diffusion%2520Policy%2520%2528DP3%2529%252C%2520a%2520novel%2520visual%2520imitation%250Alearning%2520approach%2520that%2520incorporates%2520the%2520power%2520of%25203D%2520visual%2520representations%2520into%250Adiffusion%2520policies%252C%2520a%2520class%2520of%2520conditional%2520action%2520generative%2520models.%2520The%2520core%250Adesign%2520of%2520DP3%2520is%2520the%2520utilization%2520of%2520a%2520compact%25203D%2520visual%2520representation%252C%250Aextracted%2520from%2520sparse%2520point%2520clouds%2520with%2520an%2520efficient%2520point%2520encoder.%2520In%2520our%250Aexperiments%2520involving%252072%2520simulation%2520tasks%252C%2520DP3%2520successfully%2520handles%2520most%2520tasks%250Awith%2520just%252010%2520demonstrations%2520and%2520surpasses%2520baselines%2520with%2520a%252024.2%2525%2520relative%250Aimprovement.%2520In%25204%2520real%2520robot%2520tasks%252C%2520DP3%2520demonstrates%2520precise%2520control%2520with%2520a%250Ahigh%2520success%2520rate%2520of%252085%2525%252C%2520given%2520only%252040%2520demonstrations%2520of%2520each%2520task%252C%2520and%2520shows%250Aexcellent%2520generalization%2520abilities%2520in%2520diverse%2520aspects%252C%2520including%2520space%252C%250Aviewpoint%252C%2520appearance%252C%2520and%2520instance.%2520Interestingly%252C%2520in%2520real%2520robot%2520experiments%252C%250ADP3%2520rarely%2520violates%2520safety%2520requirements%252C%2520in%2520contrast%2520to%2520baseline%2520methods%2520which%250Afrequently%2520do%252C%2520necessitating%2520human%2520intervention.%2520Our%2520extensive%2520evaluation%250Ahighlights%2520the%2520critical%2520importance%2520of%25203D%2520representations%2520in%2520real-world%2520robot%250Alearning.%2520Videos%252C%2520code%252C%2520and%2520data%2520are%2520available%2520on%250Ahttps%253A//3d-diffusion-policy.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03954v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Diffusion%20Policy%3A%20Generalizable%20Visuomotor%20Policy%20Learning%20via%20Simple%0A%20%203D%20Representations&entry.906535625=Yanjie%20Ze%20and%20Gu%20Zhang%20and%20Kangning%20Zhang%20and%20Chenyuan%20Hu%20and%20Muhan%20Wang%20and%20Huazhe%20Xu&entry.1292438233=%20%20Imitation%20learning%20provides%20an%20efficient%20way%20to%20teach%20robots%20dexterous%0Askills%3B%20however%2C%20learning%20complex%20skills%20robustly%20and%20generalizablely%20usually%0Aconsumes%20large%20amounts%20of%20human%20demonstrations.%20To%20tackle%20this%20challenging%0Aproblem%2C%20we%20present%203D%20Diffusion%20Policy%20%28DP3%29%2C%20a%20novel%20visual%20imitation%0Alearning%20approach%20that%20incorporates%20the%20power%20of%203D%20visual%20representations%20into%0Adiffusion%20policies%2C%20a%20class%20of%20conditional%20action%20generative%20models.%20The%20core%0Adesign%20of%20DP3%20is%20the%20utilization%20of%20a%20compact%203D%20visual%20representation%2C%0Aextracted%20from%20sparse%20point%20clouds%20with%20an%20efficient%20point%20encoder.%20In%20our%0Aexperiments%20involving%2072%20simulation%20tasks%2C%20DP3%20successfully%20handles%20most%20tasks%0Awith%20just%2010%20demonstrations%20and%20surpasses%20baselines%20with%20a%2024.2%25%20relative%0Aimprovement.%20In%204%20real%20robot%20tasks%2C%20DP3%20demonstrates%20precise%20control%20with%20a%0Ahigh%20success%20rate%20of%2085%25%2C%20given%20only%2040%20demonstrations%20of%20each%20task%2C%20and%20shows%0Aexcellent%20generalization%20abilities%20in%20diverse%20aspects%2C%20including%20space%2C%0Aviewpoint%2C%20appearance%2C%20and%20instance.%20Interestingly%2C%20in%20real%20robot%20experiments%2C%0ADP3%20rarely%20violates%20safety%20requirements%2C%20in%20contrast%20to%20baseline%20methods%20which%0Afrequently%20do%2C%20necessitating%20human%20intervention.%20Our%20extensive%20evaluation%0Ahighlights%20the%20critical%20importance%20of%203D%20representations%20in%20real-world%20robot%0Alearning.%20Videos%2C%20code%2C%20and%20data%20are%20available%20on%0Ahttps%3A//3d-diffusion-policy.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03954v4&entry.124074799=Read"},
{"title": "Visual Attention Based Cognitive Human-Robot Collaboration for Pedicle\n  Screw Placement in Robot-Assisted Orthopedic Surgery", "author": "Chen Chen and Qikai Zou and Yuhang Song and Shiji Song and Xiang Li", "abstract": "  Current orthopedic robotic systems largely focus on navigation, aiding\nsurgeons in positioning a guiding tube but still requiring manual drilling and\nscrew placement. The automation of this task not only demands high precision\nand safety due to the intricate physical interactions between the surgical tool\nand bone but also poses significant risks when executed without adequate human\noversight. As it involves continuous physical interaction, the robot should\ncollaborate with the surgeon, understand the human intent, and always include\nthe surgeon in the loop. To achieve this, this paper proposes a new cognitive\nhuman-robot collaboration framework, including the intuitive AR-haptic\nhuman-robot interface, the visual-attention-based surgeon model, and the shared\ninteraction control scheme for the robot. User studies on a robotic platform\nfor orthopedic surgery are presented to illustrate the performance of the\nproposed method. The results demonstrate that the proposed human-robot\ncollaboration framework outperforms full robot and full human control in terms\nof safety and ergonomics.\n", "link": "http://arxiv.org/abs/2405.09359v1", "date": "2024-05-15", "relevancy": 1.7052, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5974}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5385}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Attention%20Based%20Cognitive%20Human-Robot%20Collaboration%20for%20Pedicle%0A%20%20Screw%20Placement%20in%20Robot-Assisted%20Orthopedic%20Surgery&body=Title%3A%20Visual%20Attention%20Based%20Cognitive%20Human-Robot%20Collaboration%20for%20Pedicle%0A%20%20Screw%20Placement%20in%20Robot-Assisted%20Orthopedic%20Surgery%0AAuthor%3A%20Chen%20Chen%20and%20Qikai%20Zou%20and%20Yuhang%20Song%20and%20Shiji%20Song%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Current%20orthopedic%20robotic%20systems%20largely%20focus%20on%20navigation%2C%20aiding%0Asurgeons%20in%20positioning%20a%20guiding%20tube%20but%20still%20requiring%20manual%20drilling%20and%0Ascrew%20placement.%20The%20automation%20of%20this%20task%20not%20only%20demands%20high%20precision%0Aand%20safety%20due%20to%20the%20intricate%20physical%20interactions%20between%20the%20surgical%20tool%0Aand%20bone%20but%20also%20poses%20significant%20risks%20when%20executed%20without%20adequate%20human%0Aoversight.%20As%20it%20involves%20continuous%20physical%20interaction%2C%20the%20robot%20should%0Acollaborate%20with%20the%20surgeon%2C%20understand%20the%20human%20intent%2C%20and%20always%20include%0Athe%20surgeon%20in%20the%20loop.%20To%20achieve%20this%2C%20this%20paper%20proposes%20a%20new%20cognitive%0Ahuman-robot%20collaboration%20framework%2C%20including%20the%20intuitive%20AR-haptic%0Ahuman-robot%20interface%2C%20the%20visual-attention-based%20surgeon%20model%2C%20and%20the%20shared%0Ainteraction%20control%20scheme%20for%20the%20robot.%20User%20studies%20on%20a%20robotic%20platform%0Afor%20orthopedic%20surgery%20are%20presented%20to%20illustrate%20the%20performance%20of%20the%0Aproposed%20method.%20The%20results%20demonstrate%20that%20the%20proposed%20human-robot%0Acollaboration%20framework%20outperforms%20full%20robot%20and%20full%20human%20control%20in%20terms%0Aof%20safety%20and%20ergonomics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Attention%2520Based%2520Cognitive%2520Human-Robot%2520Collaboration%2520for%2520Pedicle%250A%2520%2520Screw%2520Placement%2520in%2520Robot-Assisted%2520Orthopedic%2520Surgery%26entry.906535625%3DChen%2520Chen%2520and%2520Qikai%2520Zou%2520and%2520Yuhang%2520Song%2520and%2520Shiji%2520Song%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Current%2520orthopedic%2520robotic%2520systems%2520largely%2520focus%2520on%2520navigation%252C%2520aiding%250Asurgeons%2520in%2520positioning%2520a%2520guiding%2520tube%2520but%2520still%2520requiring%2520manual%2520drilling%2520and%250Ascrew%2520placement.%2520The%2520automation%2520of%2520this%2520task%2520not%2520only%2520demands%2520high%2520precision%250Aand%2520safety%2520due%2520to%2520the%2520intricate%2520physical%2520interactions%2520between%2520the%2520surgical%2520tool%250Aand%2520bone%2520but%2520also%2520poses%2520significant%2520risks%2520when%2520executed%2520without%2520adequate%2520human%250Aoversight.%2520As%2520it%2520involves%2520continuous%2520physical%2520interaction%252C%2520the%2520robot%2520should%250Acollaborate%2520with%2520the%2520surgeon%252C%2520understand%2520the%2520human%2520intent%252C%2520and%2520always%2520include%250Athe%2520surgeon%2520in%2520the%2520loop.%2520To%2520achieve%2520this%252C%2520this%2520paper%2520proposes%2520a%2520new%2520cognitive%250Ahuman-robot%2520collaboration%2520framework%252C%2520including%2520the%2520intuitive%2520AR-haptic%250Ahuman-robot%2520interface%252C%2520the%2520visual-attention-based%2520surgeon%2520model%252C%2520and%2520the%2520shared%250Ainteraction%2520control%2520scheme%2520for%2520the%2520robot.%2520User%2520studies%2520on%2520a%2520robotic%2520platform%250Afor%2520orthopedic%2520surgery%2520are%2520presented%2520to%2520illustrate%2520the%2520performance%2520of%2520the%250Aproposed%2520method.%2520The%2520results%2520demonstrate%2520that%2520the%2520proposed%2520human-robot%250Acollaboration%2520framework%2520outperforms%2520full%2520robot%2520and%2520full%2520human%2520control%2520in%2520terms%250Aof%2520safety%2520and%2520ergonomics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Attention%20Based%20Cognitive%20Human-Robot%20Collaboration%20for%20Pedicle%0A%20%20Screw%20Placement%20in%20Robot-Assisted%20Orthopedic%20Surgery&entry.906535625=Chen%20Chen%20and%20Qikai%20Zou%20and%20Yuhang%20Song%20and%20Shiji%20Song%20and%20Xiang%20Li&entry.1292438233=%20%20Current%20orthopedic%20robotic%20systems%20largely%20focus%20on%20navigation%2C%20aiding%0Asurgeons%20in%20positioning%20a%20guiding%20tube%20but%20still%20requiring%20manual%20drilling%20and%0Ascrew%20placement.%20The%20automation%20of%20this%20task%20not%20only%20demands%20high%20precision%0Aand%20safety%20due%20to%20the%20intricate%20physical%20interactions%20between%20the%20surgical%20tool%0Aand%20bone%20but%20also%20poses%20significant%20risks%20when%20executed%20without%20adequate%20human%0Aoversight.%20As%20it%20involves%20continuous%20physical%20interaction%2C%20the%20robot%20should%0Acollaborate%20with%20the%20surgeon%2C%20understand%20the%20human%20intent%2C%20and%20always%20include%0Athe%20surgeon%20in%20the%20loop.%20To%20achieve%20this%2C%20this%20paper%20proposes%20a%20new%20cognitive%0Ahuman-robot%20collaboration%20framework%2C%20including%20the%20intuitive%20AR-haptic%0Ahuman-robot%20interface%2C%20the%20visual-attention-based%20surgeon%20model%2C%20and%20the%20shared%0Ainteraction%20control%20scheme%20for%20the%20robot.%20User%20studies%20on%20a%20robotic%20platform%0Afor%20orthopedic%20surgery%20are%20presented%20to%20illustrate%20the%20performance%20of%20the%0Aproposed%20method.%20The%20results%20demonstrate%20that%20the%20proposed%20human-robot%0Acollaboration%20framework%20outperforms%20full%20robot%20and%20full%20human%20control%20in%20terms%0Aof%20safety%20and%20ergonomics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09359v1&entry.124074799=Read"},
{"title": "Towards Evaluating the Robustness of Automatic Speech Recognition\n  Systems via Audio Style Transfer", "author": "Weifei Jin and Yuxin Cao and Junjie Su and Qi Shen and Kai Ye and Derui Wang and Jie Hao and Ziyao Liu", "abstract": "  In light of the widespread application of Automatic Speech Recognition (ASR)\nsystems, their security concerns have received much more attention than ever\nbefore, primarily due to the susceptibility of Deep Neural Networks. Previous\nstudies have illustrated that surreptitiously crafting adversarial\nperturbations enables the manipulation of speech recognition systems, resulting\nin the production of malicious commands. These attack methods mostly require\nadding noise perturbations under $\\ell_p$ norm constraints, inevitably leaving\nbehind artifacts of manual modifications. Recent research has alleviated this\nlimitation by manipulating style vectors to synthesize adversarial examples\nbased on Text-to-Speech (TTS) synthesis audio. However, style modifications\nbased on optimization objectives significantly reduce the controllability and\neditability of audio styles. In this paper, we propose an attack on ASR systems\nbased on user-customized style transfer. We first test the effect of Style\nTransfer Attack (STA) which combines style transfer and adversarial attack in\nsequential order. And then, as an improvement, we propose an iterative Style\nCode Attack (SCA) to maintain audio quality. Experimental results show that our\nmethod can meet the need for user-customized styles and achieve a success rate\nof 82% in attacks, while keeping sound naturalness due to our user study.\n", "link": "http://arxiv.org/abs/2405.09470v1", "date": "2024-05-15", "relevancy": 1.7029, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4502}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4255}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Evaluating%20the%20Robustness%20of%20Automatic%20Speech%20Recognition%0A%20%20Systems%20via%20Audio%20Style%20Transfer&body=Title%3A%20Towards%20Evaluating%20the%20Robustness%20of%20Automatic%20Speech%20Recognition%0A%20%20Systems%20via%20Audio%20Style%20Transfer%0AAuthor%3A%20Weifei%20Jin%20and%20Yuxin%20Cao%20and%20Junjie%20Su%20and%20Qi%20Shen%20and%20Kai%20Ye%20and%20Derui%20Wang%20and%20Jie%20Hao%20and%20Ziyao%20Liu%0AAbstract%3A%20%20%20In%20light%20of%20the%20widespread%20application%20of%20Automatic%20Speech%20Recognition%20%28ASR%29%0Asystems%2C%20their%20security%20concerns%20have%20received%20much%20more%20attention%20than%20ever%0Abefore%2C%20primarily%20due%20to%20the%20susceptibility%20of%20Deep%20Neural%20Networks.%20Previous%0Astudies%20have%20illustrated%20that%20surreptitiously%20crafting%20adversarial%0Aperturbations%20enables%20the%20manipulation%20of%20speech%20recognition%20systems%2C%20resulting%0Ain%20the%20production%20of%20malicious%20commands.%20These%20attack%20methods%20mostly%20require%0Aadding%20noise%20perturbations%20under%20%24%5Cell_p%24%20norm%20constraints%2C%20inevitably%20leaving%0Abehind%20artifacts%20of%20manual%20modifications.%20Recent%20research%20has%20alleviated%20this%0Alimitation%20by%20manipulating%20style%20vectors%20to%20synthesize%20adversarial%20examples%0Abased%20on%20Text-to-Speech%20%28TTS%29%20synthesis%20audio.%20However%2C%20style%20modifications%0Abased%20on%20optimization%20objectives%20significantly%20reduce%20the%20controllability%20and%0Aeditability%20of%20audio%20styles.%20In%20this%20paper%2C%20we%20propose%20an%20attack%20on%20ASR%20systems%0Abased%20on%20user-customized%20style%20transfer.%20We%20first%20test%20the%20effect%20of%20Style%0ATransfer%20Attack%20%28STA%29%20which%20combines%20style%20transfer%20and%20adversarial%20attack%20in%0Asequential%20order.%20And%20then%2C%20as%20an%20improvement%2C%20we%20propose%20an%20iterative%20Style%0ACode%20Attack%20%28SCA%29%20to%20maintain%20audio%20quality.%20Experimental%20results%20show%20that%20our%0Amethod%20can%20meet%20the%20need%20for%20user-customized%20styles%20and%20achieve%20a%20success%20rate%0Aof%2082%25%20in%20attacks%2C%20while%20keeping%20sound%20naturalness%20due%20to%20our%20user%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Evaluating%2520the%2520Robustness%2520of%2520Automatic%2520Speech%2520Recognition%250A%2520%2520Systems%2520via%2520Audio%2520Style%2520Transfer%26entry.906535625%3DWeifei%2520Jin%2520and%2520Yuxin%2520Cao%2520and%2520Junjie%2520Su%2520and%2520Qi%2520Shen%2520and%2520Kai%2520Ye%2520and%2520Derui%2520Wang%2520and%2520Jie%2520Hao%2520and%2520Ziyao%2520Liu%26entry.1292438233%3D%2520%2520In%2520light%2520of%2520the%2520widespread%2520application%2520of%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%250Asystems%252C%2520their%2520security%2520concerns%2520have%2520received%2520much%2520more%2520attention%2520than%2520ever%250Abefore%252C%2520primarily%2520due%2520to%2520the%2520susceptibility%2520of%2520Deep%2520Neural%2520Networks.%2520Previous%250Astudies%2520have%2520illustrated%2520that%2520surreptitiously%2520crafting%2520adversarial%250Aperturbations%2520enables%2520the%2520manipulation%2520of%2520speech%2520recognition%2520systems%252C%2520resulting%250Ain%2520the%2520production%2520of%2520malicious%2520commands.%2520These%2520attack%2520methods%2520mostly%2520require%250Aadding%2520noise%2520perturbations%2520under%2520%2524%255Cell_p%2524%2520norm%2520constraints%252C%2520inevitably%2520leaving%250Abehind%2520artifacts%2520of%2520manual%2520modifications.%2520Recent%2520research%2520has%2520alleviated%2520this%250Alimitation%2520by%2520manipulating%2520style%2520vectors%2520to%2520synthesize%2520adversarial%2520examples%250Abased%2520on%2520Text-to-Speech%2520%2528TTS%2529%2520synthesis%2520audio.%2520However%252C%2520style%2520modifications%250Abased%2520on%2520optimization%2520objectives%2520significantly%2520reduce%2520the%2520controllability%2520and%250Aeditability%2520of%2520audio%2520styles.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520attack%2520on%2520ASR%2520systems%250Abased%2520on%2520user-customized%2520style%2520transfer.%2520We%2520first%2520test%2520the%2520effect%2520of%2520Style%250ATransfer%2520Attack%2520%2528STA%2529%2520which%2520combines%2520style%2520transfer%2520and%2520adversarial%2520attack%2520in%250Asequential%2520order.%2520And%2520then%252C%2520as%2520an%2520improvement%252C%2520we%2520propose%2520an%2520iterative%2520Style%250ACode%2520Attack%2520%2528SCA%2529%2520to%2520maintain%2520audio%2520quality.%2520Experimental%2520results%2520show%2520that%2520our%250Amethod%2520can%2520meet%2520the%2520need%2520for%2520user-customized%2520styles%2520and%2520achieve%2520a%2520success%2520rate%250Aof%252082%2525%2520in%2520attacks%252C%2520while%2520keeping%2520sound%2520naturalness%2520due%2520to%2520our%2520user%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Evaluating%20the%20Robustness%20of%20Automatic%20Speech%20Recognition%0A%20%20Systems%20via%20Audio%20Style%20Transfer&entry.906535625=Weifei%20Jin%20and%20Yuxin%20Cao%20and%20Junjie%20Su%20and%20Qi%20Shen%20and%20Kai%20Ye%20and%20Derui%20Wang%20and%20Jie%20Hao%20and%20Ziyao%20Liu&entry.1292438233=%20%20In%20light%20of%20the%20widespread%20application%20of%20Automatic%20Speech%20Recognition%20%28ASR%29%0Asystems%2C%20their%20security%20concerns%20have%20received%20much%20more%20attention%20than%20ever%0Abefore%2C%20primarily%20due%20to%20the%20susceptibility%20of%20Deep%20Neural%20Networks.%20Previous%0Astudies%20have%20illustrated%20that%20surreptitiously%20crafting%20adversarial%0Aperturbations%20enables%20the%20manipulation%20of%20speech%20recognition%20systems%2C%20resulting%0Ain%20the%20production%20of%20malicious%20commands.%20These%20attack%20methods%20mostly%20require%0Aadding%20noise%20perturbations%20under%20%24%5Cell_p%24%20norm%20constraints%2C%20inevitably%20leaving%0Abehind%20artifacts%20of%20manual%20modifications.%20Recent%20research%20has%20alleviated%20this%0Alimitation%20by%20manipulating%20style%20vectors%20to%20synthesize%20adversarial%20examples%0Abased%20on%20Text-to-Speech%20%28TTS%29%20synthesis%20audio.%20However%2C%20style%20modifications%0Abased%20on%20optimization%20objectives%20significantly%20reduce%20the%20controllability%20and%0Aeditability%20of%20audio%20styles.%20In%20this%20paper%2C%20we%20propose%20an%20attack%20on%20ASR%20systems%0Abased%20on%20user-customized%20style%20transfer.%20We%20first%20test%20the%20effect%20of%20Style%0ATransfer%20Attack%20%28STA%29%20which%20combines%20style%20transfer%20and%20adversarial%20attack%20in%0Asequential%20order.%20And%20then%2C%20as%20an%20improvement%2C%20we%20propose%20an%20iterative%20Style%0ACode%20Attack%20%28SCA%29%20to%20maintain%20audio%20quality.%20Experimental%20results%20show%20that%20our%0Amethod%20can%20meet%20the%20need%20for%20user-customized%20styles%20and%20achieve%20a%20success%20rate%0Aof%2082%25%20in%20attacks%2C%20while%20keeping%20sound%20naturalness%20due%20to%20our%20user%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09470v1&entry.124074799=Read"},
{"title": "Efficient Frontier Management for Collaborative Active SLAM", "author": "Muhammad Farhan Ahmed and Matteo Maragliano and Vincent FremontCarmine and Tommaso Recchiuto and Antonio Sgorbissa", "abstract": "  In autonomous robotics, a critical challenge lies in developing robust\nsolutions for Active Collaborative SLAM, wherein multiple robots\ncollaboratively explore and map an unknown environment while intelligently\ncoordinating their movements and sensor data acquisitions. In this article, we\npresent an efficient centralized frontier sharing approach that maximizes\nexploration by taking into account information gain in the merged map,\ndistance, and reward computation among frontier candidates and encourages the\nspread of agents into the environment. Eventually, our method efficiently\nspreads the robots for maximum exploration while keeping SLAM uncertainty low.\nAdditionally, we also present two coordination approaches, synchronous and\nasynchronous to prioritize robot goal assignments by the central server. The\nproposed method is implemented in ROS and evaluated through simulation and\nexperiments on publicly available datasets and similar methods, rendering\npromising results.\n", "link": "http://arxiv.org/abs/2310.01967v4", "date": "2024-05-15", "relevancy": 1.6933, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5837}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.567}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM&body=Title%3A%20Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM%0AAuthor%3A%20Muhammad%20Farhan%20Ahmed%20and%20Matteo%20Maragliano%20and%20Vincent%20FremontCarmine%20and%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa%0AAbstract%3A%20%20%20In%20autonomous%20robotics%2C%20a%20critical%20challenge%20lies%20in%20developing%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%2C%20wherein%20multiple%20robots%0Acollaboratively%20explore%20and%20map%20an%20unknown%20environment%20while%20intelligently%0Acoordinating%20their%20movements%20and%20sensor%20data%20acquisitions.%20In%20this%20article%2C%20we%0Apresent%20an%20efficient%20centralized%20frontier%20sharing%20approach%20that%20maximizes%0Aexploration%20by%20taking%20into%20account%20information%20gain%20in%20the%20merged%20map%2C%0Adistance%2C%20and%20reward%20computation%20among%20frontier%20candidates%20and%20encourages%20the%0Aspread%20of%20agents%20into%20the%20environment.%20Eventually%2C%20our%20method%20efficiently%0Aspreads%20the%20robots%20for%20maximum%20exploration%20while%20keeping%20SLAM%20uncertainty%20low.%0AAdditionally%2C%20we%20also%20present%20two%20coordination%20approaches%2C%20synchronous%20and%0Aasynchronous%20to%20prioritize%20robot%20goal%20assignments%20by%20the%20central%20server.%20The%0Aproposed%20method%20is%20implemented%20in%20ROS%20and%20evaluated%20through%20simulation%20and%0Aexperiments%20on%20publicly%20available%20datasets%20and%20similar%20methods%2C%20rendering%0Apromising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01967v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Frontier%2520Management%2520for%2520Collaborative%2520Active%2520SLAM%26entry.906535625%3DMuhammad%2520Farhan%2520Ahmed%2520and%2520Matteo%2520Maragliano%2520and%2520Vincent%2520FremontCarmine%2520and%2520Tommaso%2520Recchiuto%2520and%2520Antonio%2520Sgorbissa%26entry.1292438233%3D%2520%2520In%2520autonomous%2520robotics%252C%2520a%2520critical%2520challenge%2520lies%2520in%2520developing%2520robust%250Asolutions%2520for%2520Active%2520Collaborative%2520SLAM%252C%2520wherein%2520multiple%2520robots%250Acollaboratively%2520explore%2520and%2520map%2520an%2520unknown%2520environment%2520while%2520intelligently%250Acoordinating%2520their%2520movements%2520and%2520sensor%2520data%2520acquisitions.%2520In%2520this%2520article%252C%2520we%250Apresent%2520an%2520efficient%2520centralized%2520frontier%2520sharing%2520approach%2520that%2520maximizes%250Aexploration%2520by%2520taking%2520into%2520account%2520information%2520gain%2520in%2520the%2520merged%2520map%252C%250Adistance%252C%2520and%2520reward%2520computation%2520among%2520frontier%2520candidates%2520and%2520encourages%2520the%250Aspread%2520of%2520agents%2520into%2520the%2520environment.%2520Eventually%252C%2520our%2520method%2520efficiently%250Aspreads%2520the%2520robots%2520for%2520maximum%2520exploration%2520while%2520keeping%2520SLAM%2520uncertainty%2520low.%250AAdditionally%252C%2520we%2520also%2520present%2520two%2520coordination%2520approaches%252C%2520synchronous%2520and%250Aasynchronous%2520to%2520prioritize%2520robot%2520goal%2520assignments%2520by%2520the%2520central%2520server.%2520The%250Aproposed%2520method%2520is%2520implemented%2520in%2520ROS%2520and%2520evaluated%2520through%2520simulation%2520and%250Aexperiments%2520on%2520publicly%2520available%2520datasets%2520and%2520similar%2520methods%252C%2520rendering%250Apromising%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01967v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Frontier%20Management%20for%20Collaborative%20Active%20SLAM&entry.906535625=Muhammad%20Farhan%20Ahmed%20and%20Matteo%20Maragliano%20and%20Vincent%20FremontCarmine%20and%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa&entry.1292438233=%20%20In%20autonomous%20robotics%2C%20a%20critical%20challenge%20lies%20in%20developing%20robust%0Asolutions%20for%20Active%20Collaborative%20SLAM%2C%20wherein%20multiple%20robots%0Acollaboratively%20explore%20and%20map%20an%20unknown%20environment%20while%20intelligently%0Acoordinating%20their%20movements%20and%20sensor%20data%20acquisitions.%20In%20this%20article%2C%20we%0Apresent%20an%20efficient%20centralized%20frontier%20sharing%20approach%20that%20maximizes%0Aexploration%20by%20taking%20into%20account%20information%20gain%20in%20the%20merged%20map%2C%0Adistance%2C%20and%20reward%20computation%20among%20frontier%20candidates%20and%20encourages%20the%0Aspread%20of%20agents%20into%20the%20environment.%20Eventually%2C%20our%20method%20efficiently%0Aspreads%20the%20robots%20for%20maximum%20exploration%20while%20keeping%20SLAM%20uncertainty%20low.%0AAdditionally%2C%20we%20also%20present%20two%20coordination%20approaches%2C%20synchronous%20and%0Aasynchronous%20to%20prioritize%20robot%20goal%20assignments%20by%20the%20central%20server.%20The%0Aproposed%20method%20is%20implemented%20in%20ROS%20and%20evaluated%20through%20simulation%20and%0Aexperiments%20on%20publicly%20available%20datasets%20and%20similar%20methods%2C%20rendering%0Apromising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01967v4&entry.124074799=Read"},
{"title": "Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A\n  Blind Assessment of Large Language Models for Psychological Support", "author": "Birger Moell", "abstract": "  Background: Rapid advancements in natural language processing have led to the\ndevelopment of large language models with the potential to revolutionize mental\nhealth care. These models have shown promise in assisting clinicians and\nproviding support to individuals experiencing various psychological challenges.\n  Objective: This study aims to compare the performance of two large language\nmodels, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts,\nto assess their potential applicability in mental health care settings.\n  Methods: A blind methodology was employed, with a clinical psychologist\nevaluating the models' responses without knowledge of their origins. The\nprompts encompassed a diverse range of mental health topics, including\ndepression, anxiety, and trauma, to ensure a comprehensive assessment.\n  Results: The results demonstrated a significant difference in performance\nbetween the two models (p > 0.05). GPT-4 achieved an average rating of 8.29 out\nof 10, while Chat-GPT received an average rating of 6.52. The clinical\npsychologist's evaluation suggested that GPT-4 was more effective at generating\nclinically relevant and empathetic responses, thereby providing better support\nand guidance to potential users.\n  Conclusions: This study contributes to the growing body of literature on the\napplicability of large language models in mental health care settings. The\nfindings underscore the importance of continued research and development in the\nfield to optimize these models for clinical use. Further investigation is\nnecessary to understand the specific factors underlying the performance\ndifferences between the two models and to explore their generalizability across\nvarious populations and mental health conditions.\n", "link": "http://arxiv.org/abs/2405.09300v1", "date": "2024-05-15", "relevancy": 1.6893, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.434}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.433}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20the%20Efficacy%20of%20GPT-4%20and%20Chat-GPT%20in%20Mental%20Health%20Care%3A%20A%0A%20%20Blind%20Assessment%20of%20Large%20Language%20Models%20for%20Psychological%20Support&body=Title%3A%20Comparing%20the%20Efficacy%20of%20GPT-4%20and%20Chat-GPT%20in%20Mental%20Health%20Care%3A%20A%0A%20%20Blind%20Assessment%20of%20Large%20Language%20Models%20for%20Psychological%20Support%0AAuthor%3A%20Birger%20Moell%0AAbstract%3A%20%20%20Background%3A%20Rapid%20advancements%20in%20natural%20language%20processing%20have%20led%20to%20the%0Adevelopment%20of%20large%20language%20models%20with%20the%20potential%20to%20revolutionize%20mental%0Ahealth%20care.%20These%20models%20have%20shown%20promise%20in%20assisting%20clinicians%20and%0Aproviding%20support%20to%20individuals%20experiencing%20various%20psychological%20challenges.%0A%20%20Objective%3A%20This%20study%20aims%20to%20compare%20the%20performance%20of%20two%20large%20language%0Amodels%2C%20GPT-4%20and%20Chat-GPT%2C%20in%20responding%20to%20a%20set%20of%2018%20psychological%20prompts%2C%0Ato%20assess%20their%20potential%20applicability%20in%20mental%20health%20care%20settings.%0A%20%20Methods%3A%20A%20blind%20methodology%20was%20employed%2C%20with%20a%20clinical%20psychologist%0Aevaluating%20the%20models%27%20responses%20without%20knowledge%20of%20their%20origins.%20The%0Aprompts%20encompassed%20a%20diverse%20range%20of%20mental%20health%20topics%2C%20including%0Adepression%2C%20anxiety%2C%20and%20trauma%2C%20to%20ensure%20a%20comprehensive%20assessment.%0A%20%20Results%3A%20The%20results%20demonstrated%20a%20significant%20difference%20in%20performance%0Abetween%20the%20two%20models%20%28p%20%3E%200.05%29.%20GPT-4%20achieved%20an%20average%20rating%20of%208.29%20out%0Aof%2010%2C%20while%20Chat-GPT%20received%20an%20average%20rating%20of%206.52.%20The%20clinical%0Apsychologist%27s%20evaluation%20suggested%20that%20GPT-4%20was%20more%20effective%20at%20generating%0Aclinically%20relevant%20and%20empathetic%20responses%2C%20thereby%20providing%20better%20support%0Aand%20guidance%20to%20potential%20users.%0A%20%20Conclusions%3A%20This%20study%20contributes%20to%20the%20growing%20body%20of%20literature%20on%20the%0Aapplicability%20of%20large%20language%20models%20in%20mental%20health%20care%20settings.%20The%0Afindings%20underscore%20the%20importance%20of%20continued%20research%20and%20development%20in%20the%0Afield%20to%20optimize%20these%20models%20for%20clinical%20use.%20Further%20investigation%20is%0Anecessary%20to%20understand%20the%20specific%20factors%20underlying%20the%20performance%0Adifferences%20between%20the%20two%20models%20and%20to%20explore%20their%20generalizability%20across%0Avarious%20populations%20and%20mental%20health%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520the%2520Efficacy%2520of%2520GPT-4%2520and%2520Chat-GPT%2520in%2520Mental%2520Health%2520Care%253A%2520A%250A%2520%2520Blind%2520Assessment%2520of%2520Large%2520Language%2520Models%2520for%2520Psychological%2520Support%26entry.906535625%3DBirger%2520Moell%26entry.1292438233%3D%2520%2520Background%253A%2520Rapid%2520advancements%2520in%2520natural%2520language%2520processing%2520have%2520led%2520to%2520the%250Adevelopment%2520of%2520large%2520language%2520models%2520with%2520the%2520potential%2520to%2520revolutionize%2520mental%250Ahealth%2520care.%2520These%2520models%2520have%2520shown%2520promise%2520in%2520assisting%2520clinicians%2520and%250Aproviding%2520support%2520to%2520individuals%2520experiencing%2520various%2520psychological%2520challenges.%250A%2520%2520Objective%253A%2520This%2520study%2520aims%2520to%2520compare%2520the%2520performance%2520of%2520two%2520large%2520language%250Amodels%252C%2520GPT-4%2520and%2520Chat-GPT%252C%2520in%2520responding%2520to%2520a%2520set%2520of%252018%2520psychological%2520prompts%252C%250Ato%2520assess%2520their%2520potential%2520applicability%2520in%2520mental%2520health%2520care%2520settings.%250A%2520%2520Methods%253A%2520A%2520blind%2520methodology%2520was%2520employed%252C%2520with%2520a%2520clinical%2520psychologist%250Aevaluating%2520the%2520models%2527%2520responses%2520without%2520knowledge%2520of%2520their%2520origins.%2520The%250Aprompts%2520encompassed%2520a%2520diverse%2520range%2520of%2520mental%2520health%2520topics%252C%2520including%250Adepression%252C%2520anxiety%252C%2520and%2520trauma%252C%2520to%2520ensure%2520a%2520comprehensive%2520assessment.%250A%2520%2520Results%253A%2520The%2520results%2520demonstrated%2520a%2520significant%2520difference%2520in%2520performance%250Abetween%2520the%2520two%2520models%2520%2528p%2520%253E%25200.05%2529.%2520GPT-4%2520achieved%2520an%2520average%2520rating%2520of%25208.29%2520out%250Aof%252010%252C%2520while%2520Chat-GPT%2520received%2520an%2520average%2520rating%2520of%25206.52.%2520The%2520clinical%250Apsychologist%2527s%2520evaluation%2520suggested%2520that%2520GPT-4%2520was%2520more%2520effective%2520at%2520generating%250Aclinically%2520relevant%2520and%2520empathetic%2520responses%252C%2520thereby%2520providing%2520better%2520support%250Aand%2520guidance%2520to%2520potential%2520users.%250A%2520%2520Conclusions%253A%2520This%2520study%2520contributes%2520to%2520the%2520growing%2520body%2520of%2520literature%2520on%2520the%250Aapplicability%2520of%2520large%2520language%2520models%2520in%2520mental%2520health%2520care%2520settings.%2520The%250Afindings%2520underscore%2520the%2520importance%2520of%2520continued%2520research%2520and%2520development%2520in%2520the%250Afield%2520to%2520optimize%2520these%2520models%2520for%2520clinical%2520use.%2520Further%2520investigation%2520is%250Anecessary%2520to%2520understand%2520the%2520specific%2520factors%2520underlying%2520the%2520performance%250Adifferences%2520between%2520the%2520two%2520models%2520and%2520to%2520explore%2520their%2520generalizability%2520across%250Avarious%2520populations%2520and%2520mental%2520health%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20the%20Efficacy%20of%20GPT-4%20and%20Chat-GPT%20in%20Mental%20Health%20Care%3A%20A%0A%20%20Blind%20Assessment%20of%20Large%20Language%20Models%20for%20Psychological%20Support&entry.906535625=Birger%20Moell&entry.1292438233=%20%20Background%3A%20Rapid%20advancements%20in%20natural%20language%20processing%20have%20led%20to%20the%0Adevelopment%20of%20large%20language%20models%20with%20the%20potential%20to%20revolutionize%20mental%0Ahealth%20care.%20These%20models%20have%20shown%20promise%20in%20assisting%20clinicians%20and%0Aproviding%20support%20to%20individuals%20experiencing%20various%20psychological%20challenges.%0A%20%20Objective%3A%20This%20study%20aims%20to%20compare%20the%20performance%20of%20two%20large%20language%0Amodels%2C%20GPT-4%20and%20Chat-GPT%2C%20in%20responding%20to%20a%20set%20of%2018%20psychological%20prompts%2C%0Ato%20assess%20their%20potential%20applicability%20in%20mental%20health%20care%20settings.%0A%20%20Methods%3A%20A%20blind%20methodology%20was%20employed%2C%20with%20a%20clinical%20psychologist%0Aevaluating%20the%20models%27%20responses%20without%20knowledge%20of%20their%20origins.%20The%0Aprompts%20encompassed%20a%20diverse%20range%20of%20mental%20health%20topics%2C%20including%0Adepression%2C%20anxiety%2C%20and%20trauma%2C%20to%20ensure%20a%20comprehensive%20assessment.%0A%20%20Results%3A%20The%20results%20demonstrated%20a%20significant%20difference%20in%20performance%0Abetween%20the%20two%20models%20%28p%20%3E%200.05%29.%20GPT-4%20achieved%20an%20average%20rating%20of%208.29%20out%0Aof%2010%2C%20while%20Chat-GPT%20received%20an%20average%20rating%20of%206.52.%20The%20clinical%0Apsychologist%27s%20evaluation%20suggested%20that%20GPT-4%20was%20more%20effective%20at%20generating%0Aclinically%20relevant%20and%20empathetic%20responses%2C%20thereby%20providing%20better%20support%0Aand%20guidance%20to%20potential%20users.%0A%20%20Conclusions%3A%20This%20study%20contributes%20to%20the%20growing%20body%20of%20literature%20on%20the%0Aapplicability%20of%20large%20language%20models%20in%20mental%20health%20care%20settings.%20The%0Afindings%20underscore%20the%20importance%20of%20continued%20research%20and%20development%20in%20the%0Afield%20to%20optimize%20these%20models%20for%20clinical%20use.%20Further%20investigation%20is%0Anecessary%20to%20understand%20the%20specific%20factors%20underlying%20the%20performance%0Adifferences%20between%20the%20two%20models%20and%20to%20explore%20their%20generalizability%20across%0Avarious%20populations%20and%20mental%20health%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09300v1&entry.124074799=Read"},
{"title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models", "author": "Yixiao Zhang and Yukara Ikemiya and Gus Xia and Naoki Murata and Marco A. Mart\u00ednez-Ram\u00edrez and Wei-Hsiang Liao and Yuki Mitsufuji and Simon Dixon", "abstract": "  Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.\n", "link": "http://arxiv.org/abs/2402.06178v2", "date": "2024-05-15", "relevancy": 1.6791, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5774}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5581}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MusicMagus%3A%20Zero-Shot%20Text-to-Music%20Editing%20via%20Diffusion%20Models&body=Title%3A%20MusicMagus%3A%20Zero-Shot%20Text-to-Music%20Editing%20via%20Diffusion%20Models%0AAuthor%3A%20Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Gus%20Xia%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-music%20generation%20models%20have%20opened%20new%20avenues%20in%0Amusical%20creativity.%20However%2C%20music%20generation%20usually%20involves%20iterative%0Arefinements%2C%20and%20how%20to%20edit%20the%20generated%20music%20remains%20a%20significant%0Achallenge.%20This%20paper%20introduces%20a%20novel%20approach%20to%20the%20editing%20of%20music%0Agenerated%20by%20such%20models%2C%20enabling%20the%20modification%20of%20specific%20attributes%2C%0Asuch%20as%20genre%2C%20mood%20and%20instrument%2C%20while%20maintaining%20other%20aspects%20unchanged.%0AOur%20method%20transforms%20text%20editing%20to%20%5Ctextit%7Blatent%20space%20manipulation%7D%20while%0Aadding%20an%20extra%20constraint%20to%20enforce%20consistency.%20It%20seamlessly%20integrates%0Awith%20existing%20pretrained%20text-to-music%20diffusion%20models%20without%20requiring%0Aadditional%20training.%20Experimental%20results%20demonstrate%20superior%20performance%20over%0Aboth%20zero-shot%20and%20certain%20supervised%20baselines%20in%20style%20and%20timbre%20transfer%0Aevaluations.%20Additionally%2C%20we%20showcase%20the%20practical%20applicability%20of%20our%0Aapproach%20in%20real-world%20music%20editing%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusicMagus%253A%2520Zero-Shot%2520Text-to-Music%2520Editing%2520via%2520Diffusion%2520Models%26entry.906535625%3DYixiao%2520Zhang%2520and%2520Yukara%2520Ikemiya%2520and%2520Gus%2520Xia%2520and%2520Naoki%2520Murata%2520and%2520Marco%2520A.%2520Mart%25C3%25ADnez-Ram%25C3%25ADrez%2520and%2520Wei-Hsiang%2520Liao%2520and%2520Yuki%2520Mitsufuji%2520and%2520Simon%2520Dixon%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-music%2520generation%2520models%2520have%2520opened%2520new%2520avenues%2520in%250Amusical%2520creativity.%2520However%252C%2520music%2520generation%2520usually%2520involves%2520iterative%250Arefinements%252C%2520and%2520how%2520to%2520edit%2520the%2520generated%2520music%2520remains%2520a%2520significant%250Achallenge.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%2520the%2520editing%2520of%2520music%250Agenerated%2520by%2520such%2520models%252C%2520enabling%2520the%2520modification%2520of%2520specific%2520attributes%252C%250Asuch%2520as%2520genre%252C%2520mood%2520and%2520instrument%252C%2520while%2520maintaining%2520other%2520aspects%2520unchanged.%250AOur%2520method%2520transforms%2520text%2520editing%2520to%2520%255Ctextit%257Blatent%2520space%2520manipulation%257D%2520while%250Aadding%2520an%2520extra%2520constraint%2520to%2520enforce%2520consistency.%2520It%2520seamlessly%2520integrates%250Awith%2520existing%2520pretrained%2520text-to-music%2520diffusion%2520models%2520without%2520requiring%250Aadditional%2520training.%2520Experimental%2520results%2520demonstrate%2520superior%2520performance%2520over%250Aboth%2520zero-shot%2520and%2520certain%2520supervised%2520baselines%2520in%2520style%2520and%2520timbre%2520transfer%250Aevaluations.%2520Additionally%252C%2520we%2520showcase%2520the%2520practical%2520applicability%2520of%2520our%250Aapproach%2520in%2520real-world%2520music%2520editing%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MusicMagus%3A%20Zero-Shot%20Text-to-Music%20Editing%20via%20Diffusion%20Models&entry.906535625=Yixiao%20Zhang%20and%20Yukara%20Ikemiya%20and%20Gus%20Xia%20and%20Naoki%20Murata%20and%20Marco%20A.%20Mart%C3%ADnez-Ram%C3%ADrez%20and%20Wei-Hsiang%20Liao%20and%20Yuki%20Mitsufuji%20and%20Simon%20Dixon&entry.1292438233=%20%20Recent%20advances%20in%20text-to-music%20generation%20models%20have%20opened%20new%20avenues%20in%0Amusical%20creativity.%20However%2C%20music%20generation%20usually%20involves%20iterative%0Arefinements%2C%20and%20how%20to%20edit%20the%20generated%20music%20remains%20a%20significant%0Achallenge.%20This%20paper%20introduces%20a%20novel%20approach%20to%20the%20editing%20of%20music%0Agenerated%20by%20such%20models%2C%20enabling%20the%20modification%20of%20specific%20attributes%2C%0Asuch%20as%20genre%2C%20mood%20and%20instrument%2C%20while%20maintaining%20other%20aspects%20unchanged.%0AOur%20method%20transforms%20text%20editing%20to%20%5Ctextit%7Blatent%20space%20manipulation%7D%20while%0Aadding%20an%20extra%20constraint%20to%20enforce%20consistency.%20It%20seamlessly%20integrates%0Awith%20existing%20pretrained%20text-to-music%20diffusion%20models%20without%20requiring%0Aadditional%20training.%20Experimental%20results%20demonstrate%20superior%20performance%20over%0Aboth%20zero-shot%20and%20certain%20supervised%20baselines%20in%20style%20and%20timbre%20transfer%0Aevaluations.%20Additionally%2C%20we%20showcase%20the%20practical%20applicability%20of%20our%0Aapproach%20in%20real-world%20music%20editing%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06178v2&entry.124074799=Read"},
{"title": "QueryNER: Segmentation of E-commerce Queries", "author": "Chester Palen-Michel and Lizzie Liang and Zhe Wu and Constantine Lignos", "abstract": "  We present QueryNER, a manually-annotated dataset and accompanying model for\ne-commerce query segmentation. Prior work in sequence labeling for e-commerce\nhas largely addressed aspect-value extraction which focuses on extracting\nportions of a product title or query for narrowly defined aspects. Our work\ninstead focuses on the goal of dividing a query into meaningful chunks with\nbroadly applicable types. We report baseline tagging results and conduct\nexperiments comparing token and entity dropping for null and low recall query\nrecovery. Challenging test sets are created using automatic transformations and\nshow how simple data augmentation techniques can make the models more robust to\nnoise. We make the QueryNER dataset publicly available.\n", "link": "http://arxiv.org/abs/2405.09507v1", "date": "2024-05-15", "relevancy": 1.6756, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4652}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.433}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueryNER%3A%20Segmentation%20of%20E-commerce%20Queries&body=Title%3A%20QueryNER%3A%20Segmentation%20of%20E-commerce%20Queries%0AAuthor%3A%20Chester%20Palen-Michel%20and%20Lizzie%20Liang%20and%20Zhe%20Wu%20and%20Constantine%20Lignos%0AAbstract%3A%20%20%20We%20present%20QueryNER%2C%20a%20manually-annotated%20dataset%20and%20accompanying%20model%20for%0Ae-commerce%20query%20segmentation.%20Prior%20work%20in%20sequence%20labeling%20for%20e-commerce%0Ahas%20largely%20addressed%20aspect-value%20extraction%20which%20focuses%20on%20extracting%0Aportions%20of%20a%20product%20title%20or%20query%20for%20narrowly%20defined%20aspects.%20Our%20work%0Ainstead%20focuses%20on%20the%20goal%20of%20dividing%20a%20query%20into%20meaningful%20chunks%20with%0Abroadly%20applicable%20types.%20We%20report%20baseline%20tagging%20results%20and%20conduct%0Aexperiments%20comparing%20token%20and%20entity%20dropping%20for%20null%20and%20low%20recall%20query%0Arecovery.%20Challenging%20test%20sets%20are%20created%20using%20automatic%20transformations%20and%0Ashow%20how%20simple%20data%20augmentation%20techniques%20can%20make%20the%20models%20more%20robust%20to%0Anoise.%20We%20make%20the%20QueryNER%20dataset%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueryNER%253A%2520Segmentation%2520of%2520E-commerce%2520Queries%26entry.906535625%3DChester%2520Palen-Michel%2520and%2520Lizzie%2520Liang%2520and%2520Zhe%2520Wu%2520and%2520Constantine%2520Lignos%26entry.1292438233%3D%2520%2520We%2520present%2520QueryNER%252C%2520a%2520manually-annotated%2520dataset%2520and%2520accompanying%2520model%2520for%250Ae-commerce%2520query%2520segmentation.%2520Prior%2520work%2520in%2520sequence%2520labeling%2520for%2520e-commerce%250Ahas%2520largely%2520addressed%2520aspect-value%2520extraction%2520which%2520focuses%2520on%2520extracting%250Aportions%2520of%2520a%2520product%2520title%2520or%2520query%2520for%2520narrowly%2520defined%2520aspects.%2520Our%2520work%250Ainstead%2520focuses%2520on%2520the%2520goal%2520of%2520dividing%2520a%2520query%2520into%2520meaningful%2520chunks%2520with%250Abroadly%2520applicable%2520types.%2520We%2520report%2520baseline%2520tagging%2520results%2520and%2520conduct%250Aexperiments%2520comparing%2520token%2520and%2520entity%2520dropping%2520for%2520null%2520and%2520low%2520recall%2520query%250Arecovery.%2520Challenging%2520test%2520sets%2520are%2520created%2520using%2520automatic%2520transformations%2520and%250Ashow%2520how%2520simple%2520data%2520augmentation%2520techniques%2520can%2520make%2520the%2520models%2520more%2520robust%2520to%250Anoise.%2520We%2520make%2520the%2520QueryNER%2520dataset%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueryNER%3A%20Segmentation%20of%20E-commerce%20Queries&entry.906535625=Chester%20Palen-Michel%20and%20Lizzie%20Liang%20and%20Zhe%20Wu%20and%20Constantine%20Lignos&entry.1292438233=%20%20We%20present%20QueryNER%2C%20a%20manually-annotated%20dataset%20and%20accompanying%20model%20for%0Ae-commerce%20query%20segmentation.%20Prior%20work%20in%20sequence%20labeling%20for%20e-commerce%0Ahas%20largely%20addressed%20aspect-value%20extraction%20which%20focuses%20on%20extracting%0Aportions%20of%20a%20product%20title%20or%20query%20for%20narrowly%20defined%20aspects.%20Our%20work%0Ainstead%20focuses%20on%20the%20goal%20of%20dividing%20a%20query%20into%20meaningful%20chunks%20with%0Abroadly%20applicable%20types.%20We%20report%20baseline%20tagging%20results%20and%20conduct%0Aexperiments%20comparing%20token%20and%20entity%20dropping%20for%20null%20and%20low%20recall%20query%0Arecovery.%20Challenging%20test%20sets%20are%20created%20using%20automatic%20transformations%20and%0Ashow%20how%20simple%20data%20augmentation%20techniques%20can%20make%20the%20models%20more%20robust%20to%0Anoise.%20We%20make%20the%20QueryNER%20dataset%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09507v1&entry.124074799=Read"},
{"title": "DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based\n  Counterfactual Explanations", "author": "Nima Fathi and Amar Kumar and Brennan Nichyporuk and Mohammad Havaei and Tal Arbel", "abstract": "  Deep learning classifiers are prone to latching onto dominant confounders\npresent in a dataset rather than on the causal markers associated with the\ntarget class, leading to poor generalization and biased predictions. Although\nexplainability via counterfactual image generation has been successful at\nexposing the problem, bias mitigation strategies that permit accurate\nexplainability in the presence of dominant and diverse artifacts remain\nunsolved. In this work, we propose the DeCoDEx framework and show how an\nexternal, pre-trained binary artifact detector can be leveraged during\ninference to guide a diffusion-based counterfactual image generator towards\naccurate explainability. Experiments on the CheXpert dataset, using both\nsynthetic artifacts and real visual artifacts (support devices), show that the\nproposed method successfully synthesizes the counterfactual images that change\nthe causal pathology markers associated with Pleural Effusion while preserving\nor ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers\nwith the DeCoDEx generated images substantially improves the results across\nunderrepresented groups that are out of distribution for each class. The code\nis made publicly available at https://github.com/NimaFathi/DeCoDEx.\n", "link": "http://arxiv.org/abs/2405.09288v1", "date": "2024-05-15", "relevancy": 1.6713, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.57}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5682}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCoDEx%3A%20Confounder%20Detector%20Guidance%20for%20Improved%20Diffusion-based%0A%20%20Counterfactual%20Explanations&body=Title%3A%20DeCoDEx%3A%20Confounder%20Detector%20Guidance%20for%20Improved%20Diffusion-based%0A%20%20Counterfactual%20Explanations%0AAuthor%3A%20Nima%20Fathi%20and%20Amar%20Kumar%20and%20Brennan%20Nichyporuk%20and%20Mohammad%20Havaei%20and%20Tal%20Arbel%0AAbstract%3A%20%20%20Deep%20learning%20classifiers%20are%20prone%20to%20latching%20onto%20dominant%20confounders%0Apresent%20in%20a%20dataset%20rather%20than%20on%20the%20causal%20markers%20associated%20with%20the%0Atarget%20class%2C%20leading%20to%20poor%20generalization%20and%20biased%20predictions.%20Although%0Aexplainability%20via%20counterfactual%20image%20generation%20has%20been%20successful%20at%0Aexposing%20the%20problem%2C%20bias%20mitigation%20strategies%20that%20permit%20accurate%0Aexplainability%20in%20the%20presence%20of%20dominant%20and%20diverse%20artifacts%20remain%0Aunsolved.%20In%20this%20work%2C%20we%20propose%20the%20DeCoDEx%20framework%20and%20show%20how%20an%0Aexternal%2C%20pre-trained%20binary%20artifact%20detector%20can%20be%20leveraged%20during%0Ainference%20to%20guide%20a%20diffusion-based%20counterfactual%20image%20generator%20towards%0Aaccurate%20explainability.%20Experiments%20on%20the%20CheXpert%20dataset%2C%20using%20both%0Asynthetic%20artifacts%20and%20real%20visual%20artifacts%20%28support%20devices%29%2C%20show%20that%20the%0Aproposed%20method%20successfully%20synthesizes%20the%20counterfactual%20images%20that%20change%0Athe%20causal%20pathology%20markers%20associated%20with%20Pleural%20Effusion%20while%20preserving%0Aor%20ignoring%20the%20visual%20artifacts.%20Augmentation%20of%20ERM%20and%20Group-DRO%20classifiers%0Awith%20the%20DeCoDEx%20generated%20images%20substantially%20improves%20the%20results%20across%0Aunderrepresented%20groups%20that%20are%20out%20of%20distribution%20for%20each%20class.%20The%20code%0Ais%20made%20publicly%20available%20at%20https%3A//github.com/NimaFathi/DeCoDEx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCoDEx%253A%2520Confounder%2520Detector%2520Guidance%2520for%2520Improved%2520Diffusion-based%250A%2520%2520Counterfactual%2520Explanations%26entry.906535625%3DNima%2520Fathi%2520and%2520Amar%2520Kumar%2520and%2520Brennan%2520Nichyporuk%2520and%2520Mohammad%2520Havaei%2520and%2520Tal%2520Arbel%26entry.1292438233%3D%2520%2520Deep%2520learning%2520classifiers%2520are%2520prone%2520to%2520latching%2520onto%2520dominant%2520confounders%250Apresent%2520in%2520a%2520dataset%2520rather%2520than%2520on%2520the%2520causal%2520markers%2520associated%2520with%2520the%250Atarget%2520class%252C%2520leading%2520to%2520poor%2520generalization%2520and%2520biased%2520predictions.%2520Although%250Aexplainability%2520via%2520counterfactual%2520image%2520generation%2520has%2520been%2520successful%2520at%250Aexposing%2520the%2520problem%252C%2520bias%2520mitigation%2520strategies%2520that%2520permit%2520accurate%250Aexplainability%2520in%2520the%2520presence%2520of%2520dominant%2520and%2520diverse%2520artifacts%2520remain%250Aunsolved.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520DeCoDEx%2520framework%2520and%2520show%2520how%2520an%250Aexternal%252C%2520pre-trained%2520binary%2520artifact%2520detector%2520can%2520be%2520leveraged%2520during%250Ainference%2520to%2520guide%2520a%2520diffusion-based%2520counterfactual%2520image%2520generator%2520towards%250Aaccurate%2520explainability.%2520Experiments%2520on%2520the%2520CheXpert%2520dataset%252C%2520using%2520both%250Asynthetic%2520artifacts%2520and%2520real%2520visual%2520artifacts%2520%2528support%2520devices%2529%252C%2520show%2520that%2520the%250Aproposed%2520method%2520successfully%2520synthesizes%2520the%2520counterfactual%2520images%2520that%2520change%250Athe%2520causal%2520pathology%2520markers%2520associated%2520with%2520Pleural%2520Effusion%2520while%2520preserving%250Aor%2520ignoring%2520the%2520visual%2520artifacts.%2520Augmentation%2520of%2520ERM%2520and%2520Group-DRO%2520classifiers%250Awith%2520the%2520DeCoDEx%2520generated%2520images%2520substantially%2520improves%2520the%2520results%2520across%250Aunderrepresented%2520groups%2520that%2520are%2520out%2520of%2520distribution%2520for%2520each%2520class.%2520The%2520code%250Ais%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/NimaFathi/DeCoDEx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCoDEx%3A%20Confounder%20Detector%20Guidance%20for%20Improved%20Diffusion-based%0A%20%20Counterfactual%20Explanations&entry.906535625=Nima%20Fathi%20and%20Amar%20Kumar%20and%20Brennan%20Nichyporuk%20and%20Mohammad%20Havaei%20and%20Tal%20Arbel&entry.1292438233=%20%20Deep%20learning%20classifiers%20are%20prone%20to%20latching%20onto%20dominant%20confounders%0Apresent%20in%20a%20dataset%20rather%20than%20on%20the%20causal%20markers%20associated%20with%20the%0Atarget%20class%2C%20leading%20to%20poor%20generalization%20and%20biased%20predictions.%20Although%0Aexplainability%20via%20counterfactual%20image%20generation%20has%20been%20successful%20at%0Aexposing%20the%20problem%2C%20bias%20mitigation%20strategies%20that%20permit%20accurate%0Aexplainability%20in%20the%20presence%20of%20dominant%20and%20diverse%20artifacts%20remain%0Aunsolved.%20In%20this%20work%2C%20we%20propose%20the%20DeCoDEx%20framework%20and%20show%20how%20an%0Aexternal%2C%20pre-trained%20binary%20artifact%20detector%20can%20be%20leveraged%20during%0Ainference%20to%20guide%20a%20diffusion-based%20counterfactual%20image%20generator%20towards%0Aaccurate%20explainability.%20Experiments%20on%20the%20CheXpert%20dataset%2C%20using%20both%0Asynthetic%20artifacts%20and%20real%20visual%20artifacts%20%28support%20devices%29%2C%20show%20that%20the%0Aproposed%20method%20successfully%20synthesizes%20the%20counterfactual%20images%20that%20change%0Athe%20causal%20pathology%20markers%20associated%20with%20Pleural%20Effusion%20while%20preserving%0Aor%20ignoring%20the%20visual%20artifacts.%20Augmentation%20of%20ERM%20and%20Group-DRO%20classifiers%0Awith%20the%20DeCoDEx%20generated%20images%20substantially%20improves%20the%20results%20across%0Aunderrepresented%20groups%20that%20are%20out%20of%20distribution%20for%20each%20class.%20The%20code%0Ais%20made%20publicly%20available%20at%20https%3A//github.com/NimaFathi/DeCoDEx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09288v1&entry.124074799=Read"},
{"title": "BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation", "author": "Yunhao Ge and Yihe Tang and Jiashu Xu and Cem Gokmen and Chengshu Li and Wensi Ai and Benjamin Jose Martinez and Arman Aydin and Mona Anvari and Ayush K Chakravarthy and Hong-Xing Yu and Josiah Wong and Sanjana Srivastava and Sharon Lee and Shengxin Zha and Laurent Itti and Yunzhu Li and Roberto Mart\u00edn-Mart\u00edn and Miao Liu and Pengchuan Zhang and Ruohan Zhang and Li Fei-Fei and Jiajun Wu", "abstract": "  The systematic evaluation and understanding of computer vision models under\nvarying conditions require large amounts of data with comprehensive and\ncustomized labels, which real-world vision datasets rarely satisfy. While\ncurrent synthetic data generators offer a promising alternative, particularly\nfor embodied AI tasks, they often fall short for computer vision tasks due to\nlow asset and rendering quality, limited diversity, and unrealistic physical\nproperties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and\nassets to generate fully customized synthetic data for systematic evaluation of\ncomputer vision models, based on the newly developed embodied AI benchmark,\nBEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene\nlevel (e.g., lighting, object placement), the object level (e.g., joint\nconfiguration, attributes such as \"filled\" and \"folded\"), and the camera level\n(e.g., field of view, focal length). Researchers can arbitrarily vary these\nparameters during data generation to perform controlled experiments. We\nshowcase three example application scenarios: systematically evaluating the\nrobustness of models across different continuous axes of domain shift,\nevaluating scene understanding models on the same set of images, and training\nand evaluating simulation-to-real transfer for a novel vision task: unary and\nbinary state prediction. Project website:\nhttps://behavior-vision-suite.github.io/\n", "link": "http://arxiv.org/abs/2405.09546v1", "date": "2024-05-15", "relevancy": 1.6687, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5907}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.551}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEHAVIOR%20Vision%20Suite%3A%20Customizable%20Dataset%20Generation%20via%20Simulation&body=Title%3A%20BEHAVIOR%20Vision%20Suite%3A%20Customizable%20Dataset%20Generation%20via%20Simulation%0AAuthor%3A%20Yunhao%20Ge%20and%20Yihe%20Tang%20and%20Jiashu%20Xu%20and%20Cem%20Gokmen%20and%20Chengshu%20Li%20and%20Wensi%20Ai%20and%20Benjamin%20Jose%20Martinez%20and%20Arman%20Aydin%20and%20Mona%20Anvari%20and%20Ayush%20K%20Chakravarthy%20and%20Hong-Xing%20Yu%20and%20Josiah%20Wong%20and%20Sanjana%20Srivastava%20and%20Sharon%20Lee%20and%20Shengxin%20Zha%20and%20Laurent%20Itti%20and%20Yunzhu%20Li%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn%20and%20Miao%20Liu%20and%20Pengchuan%20Zhang%20and%20Ruohan%20Zhang%20and%20Li%20Fei-Fei%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20The%20systematic%20evaluation%20and%20understanding%20of%20computer%20vision%20models%20under%0Avarying%20conditions%20require%20large%20amounts%20of%20data%20with%20comprehensive%20and%0Acustomized%20labels%2C%20which%20real-world%20vision%20datasets%20rarely%20satisfy.%20While%0Acurrent%20synthetic%20data%20generators%20offer%20a%20promising%20alternative%2C%20particularly%0Afor%20embodied%20AI%20tasks%2C%20they%20often%20fall%20short%20for%20computer%20vision%20tasks%20due%20to%0Alow%20asset%20and%20rendering%20quality%2C%20limited%20diversity%2C%20and%20unrealistic%20physical%0Aproperties.%20We%20introduce%20the%20BEHAVIOR%20Vision%20Suite%20%28BVS%29%2C%20a%20set%20of%20tools%20and%0Aassets%20to%20generate%20fully%20customized%20synthetic%20data%20for%20systematic%20evaluation%20of%0Acomputer%20vision%20models%2C%20based%20on%20the%20newly%20developed%20embodied%20AI%20benchmark%2C%0ABEHAVIOR-1K.%20BVS%20supports%20a%20large%20number%20of%20adjustable%20parameters%20at%20the%20scene%0Alevel%20%28e.g.%2C%20lighting%2C%20object%20placement%29%2C%20the%20object%20level%20%28e.g.%2C%20joint%0Aconfiguration%2C%20attributes%20such%20as%20%22filled%22%20and%20%22folded%22%29%2C%20and%20the%20camera%20level%0A%28e.g.%2C%20field%20of%20view%2C%20focal%20length%29.%20Researchers%20can%20arbitrarily%20vary%20these%0Aparameters%20during%20data%20generation%20to%20perform%20controlled%20experiments.%20We%0Ashowcase%20three%20example%20application%20scenarios%3A%20systematically%20evaluating%20the%0Arobustness%20of%20models%20across%20different%20continuous%20axes%20of%20domain%20shift%2C%0Aevaluating%20scene%20understanding%20models%20on%20the%20same%20set%20of%20images%2C%20and%20training%0Aand%20evaluating%20simulation-to-real%20transfer%20for%20a%20novel%20vision%20task%3A%20unary%20and%0Abinary%20state%20prediction.%20Project%20website%3A%0Ahttps%3A//behavior-vision-suite.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEHAVIOR%2520Vision%2520Suite%253A%2520Customizable%2520Dataset%2520Generation%2520via%2520Simulation%26entry.906535625%3DYunhao%2520Ge%2520and%2520Yihe%2520Tang%2520and%2520Jiashu%2520Xu%2520and%2520Cem%2520Gokmen%2520and%2520Chengshu%2520Li%2520and%2520Wensi%2520Ai%2520and%2520Benjamin%2520Jose%2520Martinez%2520and%2520Arman%2520Aydin%2520and%2520Mona%2520Anvari%2520and%2520Ayush%2520K%2520Chakravarthy%2520and%2520Hong-Xing%2520Yu%2520and%2520Josiah%2520Wong%2520and%2520Sanjana%2520Srivastava%2520and%2520Sharon%2520Lee%2520and%2520Shengxin%2520Zha%2520and%2520Laurent%2520Itti%2520and%2520Yunzhu%2520Li%2520and%2520Roberto%2520Mart%25C3%25ADn-Mart%25C3%25ADn%2520and%2520Miao%2520Liu%2520and%2520Pengchuan%2520Zhang%2520and%2520Ruohan%2520Zhang%2520and%2520Li%2520Fei-Fei%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520The%2520systematic%2520evaluation%2520and%2520understanding%2520of%2520computer%2520vision%2520models%2520under%250Avarying%2520conditions%2520require%2520large%2520amounts%2520of%2520data%2520with%2520comprehensive%2520and%250Acustomized%2520labels%252C%2520which%2520real-world%2520vision%2520datasets%2520rarely%2520satisfy.%2520While%250Acurrent%2520synthetic%2520data%2520generators%2520offer%2520a%2520promising%2520alternative%252C%2520particularly%250Afor%2520embodied%2520AI%2520tasks%252C%2520they%2520often%2520fall%2520short%2520for%2520computer%2520vision%2520tasks%2520due%2520to%250Alow%2520asset%2520and%2520rendering%2520quality%252C%2520limited%2520diversity%252C%2520and%2520unrealistic%2520physical%250Aproperties.%2520We%2520introduce%2520the%2520BEHAVIOR%2520Vision%2520Suite%2520%2528BVS%2529%252C%2520a%2520set%2520of%2520tools%2520and%250Aassets%2520to%2520generate%2520fully%2520customized%2520synthetic%2520data%2520for%2520systematic%2520evaluation%2520of%250Acomputer%2520vision%2520models%252C%2520based%2520on%2520the%2520newly%2520developed%2520embodied%2520AI%2520benchmark%252C%250ABEHAVIOR-1K.%2520BVS%2520supports%2520a%2520large%2520number%2520of%2520adjustable%2520parameters%2520at%2520the%2520scene%250Alevel%2520%2528e.g.%252C%2520lighting%252C%2520object%2520placement%2529%252C%2520the%2520object%2520level%2520%2528e.g.%252C%2520joint%250Aconfiguration%252C%2520attributes%2520such%2520as%2520%2522filled%2522%2520and%2520%2522folded%2522%2529%252C%2520and%2520the%2520camera%2520level%250A%2528e.g.%252C%2520field%2520of%2520view%252C%2520focal%2520length%2529.%2520Researchers%2520can%2520arbitrarily%2520vary%2520these%250Aparameters%2520during%2520data%2520generation%2520to%2520perform%2520controlled%2520experiments.%2520We%250Ashowcase%2520three%2520example%2520application%2520scenarios%253A%2520systematically%2520evaluating%2520the%250Arobustness%2520of%2520models%2520across%2520different%2520continuous%2520axes%2520of%2520domain%2520shift%252C%250Aevaluating%2520scene%2520understanding%2520models%2520on%2520the%2520same%2520set%2520of%2520images%252C%2520and%2520training%250Aand%2520evaluating%2520simulation-to-real%2520transfer%2520for%2520a%2520novel%2520vision%2520task%253A%2520unary%2520and%250Abinary%2520state%2520prediction.%2520Project%2520website%253A%250Ahttps%253A//behavior-vision-suite.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEHAVIOR%20Vision%20Suite%3A%20Customizable%20Dataset%20Generation%20via%20Simulation&entry.906535625=Yunhao%20Ge%20and%20Yihe%20Tang%20and%20Jiashu%20Xu%20and%20Cem%20Gokmen%20and%20Chengshu%20Li%20and%20Wensi%20Ai%20and%20Benjamin%20Jose%20Martinez%20and%20Arman%20Aydin%20and%20Mona%20Anvari%20and%20Ayush%20K%20Chakravarthy%20and%20Hong-Xing%20Yu%20and%20Josiah%20Wong%20and%20Sanjana%20Srivastava%20and%20Sharon%20Lee%20and%20Shengxin%20Zha%20and%20Laurent%20Itti%20and%20Yunzhu%20Li%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn%20and%20Miao%20Liu%20and%20Pengchuan%20Zhang%20and%20Ruohan%20Zhang%20and%20Li%20Fei-Fei%20and%20Jiajun%20Wu&entry.1292438233=%20%20The%20systematic%20evaluation%20and%20understanding%20of%20computer%20vision%20models%20under%0Avarying%20conditions%20require%20large%20amounts%20of%20data%20with%20comprehensive%20and%0Acustomized%20labels%2C%20which%20real-world%20vision%20datasets%20rarely%20satisfy.%20While%0Acurrent%20synthetic%20data%20generators%20offer%20a%20promising%20alternative%2C%20particularly%0Afor%20embodied%20AI%20tasks%2C%20they%20often%20fall%20short%20for%20computer%20vision%20tasks%20due%20to%0Alow%20asset%20and%20rendering%20quality%2C%20limited%20diversity%2C%20and%20unrealistic%20physical%0Aproperties.%20We%20introduce%20the%20BEHAVIOR%20Vision%20Suite%20%28BVS%29%2C%20a%20set%20of%20tools%20and%0Aassets%20to%20generate%20fully%20customized%20synthetic%20data%20for%20systematic%20evaluation%20of%0Acomputer%20vision%20models%2C%20based%20on%20the%20newly%20developed%20embodied%20AI%20benchmark%2C%0ABEHAVIOR-1K.%20BVS%20supports%20a%20large%20number%20of%20adjustable%20parameters%20at%20the%20scene%0Alevel%20%28e.g.%2C%20lighting%2C%20object%20placement%29%2C%20the%20object%20level%20%28e.g.%2C%20joint%0Aconfiguration%2C%20attributes%20such%20as%20%22filled%22%20and%20%22folded%22%29%2C%20and%20the%20camera%20level%0A%28e.g.%2C%20field%20of%20view%2C%20focal%20length%29.%20Researchers%20can%20arbitrarily%20vary%20these%0Aparameters%20during%20data%20generation%20to%20perform%20controlled%20experiments.%20We%0Ashowcase%20three%20example%20application%20scenarios%3A%20systematically%20evaluating%20the%0Arobustness%20of%20models%20across%20different%20continuous%20axes%20of%20domain%20shift%2C%0Aevaluating%20scene%20understanding%20models%20on%20the%20same%20set%20of%20images%2C%20and%20training%0Aand%20evaluating%20simulation-to-real%20transfer%20for%20a%20novel%20vision%20task%3A%20unary%20and%0Abinary%20state%20prediction.%20Project%20website%3A%0Ahttps%3A//behavior-vision-suite.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09546v1&entry.124074799=Read"},
{"title": "Safe Reinforcement Learning with Free-form Natural Language Constraints\n  and Pre-Trained Language Models", "author": "Xingzhou Lou and Junge Zhang and Ziyan Wang and Kaiqi Huang and Yali Du", "abstract": "  Safe reinforcement learning (RL) agents accomplish given tasks while adhering\nto specific constraints. Employing constraints expressed via\neasily-understandable human language offers considerable potential for\nreal-world applications due to its accessibility and non-reliance on domain\nexpertise. Previous safe RL methods with natural language constraints typically\nadopt a recurrent neural network, which leads to limited capabilities when\ndealing with various forms of human language input. Furthermore, these methods\noften require a ground-truth cost function, necessitating domain expertise for\nthe conversion of language constraints into a well-defined cost function that\ndetermines constraint violation. To address these issues, we proposes to use\npre-trained language models (LM) to facilitate RL agents' comprehension of\nnatural language constraints and allow them to infer costs for safe policy\nlearning. Through the use of pre-trained LMs and the elimination of the need\nfor a ground-truth cost, our method enhances safe policy learning under a\ndiverse set of human-derived free-form natural language constraints.\nExperiments on grid-world navigation and robot control show that the proposed\nmethod can achieve strong performance while adhering to given constraints. The\nusage of pre-trained LMs allows our method to comprehend complicated\nconstraints and learn safe policies without the need for ground-truth cost at\nany stage of training or evaluation. Extensive ablation studies are conducted\nto demonstrate the efficacy of each part of our method.\n", "link": "http://arxiv.org/abs/2401.07553v3", "date": "2024-05-15", "relevancy": 1.6455, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5538}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Reinforcement%20Learning%20with%20Free-form%20Natural%20Language%20Constraints%0A%20%20and%20Pre-Trained%20Language%20Models&body=Title%3A%20Safe%20Reinforcement%20Learning%20with%20Free-form%20Natural%20Language%20Constraints%0A%20%20and%20Pre-Trained%20Language%20Models%0AAuthor%3A%20Xingzhou%20Lou%20and%20Junge%20Zhang%20and%20Ziyan%20Wang%20and%20Kaiqi%20Huang%20and%20Yali%20Du%0AAbstract%3A%20%20%20Safe%20reinforcement%20learning%20%28RL%29%20agents%20accomplish%20given%20tasks%20while%20adhering%0Ato%20specific%20constraints.%20Employing%20constraints%20expressed%20via%0Aeasily-understandable%20human%20language%20offers%20considerable%20potential%20for%0Areal-world%20applications%20due%20to%20its%20accessibility%20and%20non-reliance%20on%20domain%0Aexpertise.%20Previous%20safe%20RL%20methods%20with%20natural%20language%20constraints%20typically%0Aadopt%20a%20recurrent%20neural%20network%2C%20which%20leads%20to%20limited%20capabilities%20when%0Adealing%20with%20various%20forms%20of%20human%20language%20input.%20Furthermore%2C%20these%20methods%0Aoften%20require%20a%20ground-truth%20cost%20function%2C%20necessitating%20domain%20expertise%20for%0Athe%20conversion%20of%20language%20constraints%20into%20a%20well-defined%20cost%20function%20that%0Adetermines%20constraint%20violation.%20To%20address%20these%20issues%2C%20we%20proposes%20to%20use%0Apre-trained%20language%20models%20%28LM%29%20to%20facilitate%20RL%20agents%27%20comprehension%20of%0Anatural%20language%20constraints%20and%20allow%20them%20to%20infer%20costs%20for%20safe%20policy%0Alearning.%20Through%20the%20use%20of%20pre-trained%20LMs%20and%20the%20elimination%20of%20the%20need%0Afor%20a%20ground-truth%20cost%2C%20our%20method%20enhances%20safe%20policy%20learning%20under%20a%0Adiverse%20set%20of%20human-derived%20free-form%20natural%20language%20constraints.%0AExperiments%20on%20grid-world%20navigation%20and%20robot%20control%20show%20that%20the%20proposed%0Amethod%20can%20achieve%20strong%20performance%20while%20adhering%20to%20given%20constraints.%20The%0Ausage%20of%20pre-trained%20LMs%20allows%20our%20method%20to%20comprehend%20complicated%0Aconstraints%20and%20learn%20safe%20policies%20without%20the%20need%20for%20ground-truth%20cost%20at%0Aany%20stage%20of%20training%20or%20evaluation.%20Extensive%20ablation%20studies%20are%20conducted%0Ato%20demonstrate%20the%20efficacy%20of%20each%20part%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07553v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Reinforcement%2520Learning%2520with%2520Free-form%2520Natural%2520Language%2520Constraints%250A%2520%2520and%2520Pre-Trained%2520Language%2520Models%26entry.906535625%3DXingzhou%2520Lou%2520and%2520Junge%2520Zhang%2520and%2520Ziyan%2520Wang%2520and%2520Kaiqi%2520Huang%2520and%2520Yali%2520Du%26entry.1292438233%3D%2520%2520Safe%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520accomplish%2520given%2520tasks%2520while%2520adhering%250Ato%2520specific%2520constraints.%2520Employing%2520constraints%2520expressed%2520via%250Aeasily-understandable%2520human%2520language%2520offers%2520considerable%2520potential%2520for%250Areal-world%2520applications%2520due%2520to%2520its%2520accessibility%2520and%2520non-reliance%2520on%2520domain%250Aexpertise.%2520Previous%2520safe%2520RL%2520methods%2520with%2520natural%2520language%2520constraints%2520typically%250Aadopt%2520a%2520recurrent%2520neural%2520network%252C%2520which%2520leads%2520to%2520limited%2520capabilities%2520when%250Adealing%2520with%2520various%2520forms%2520of%2520human%2520language%2520input.%2520Furthermore%252C%2520these%2520methods%250Aoften%2520require%2520a%2520ground-truth%2520cost%2520function%252C%2520necessitating%2520domain%2520expertise%2520for%250Athe%2520conversion%2520of%2520language%2520constraints%2520into%2520a%2520well-defined%2520cost%2520function%2520that%250Adetermines%2520constraint%2520violation.%2520To%2520address%2520these%2520issues%252C%2520we%2520proposes%2520to%2520use%250Apre-trained%2520language%2520models%2520%2528LM%2529%2520to%2520facilitate%2520RL%2520agents%2527%2520comprehension%2520of%250Anatural%2520language%2520constraints%2520and%2520allow%2520them%2520to%2520infer%2520costs%2520for%2520safe%2520policy%250Alearning.%2520Through%2520the%2520use%2520of%2520pre-trained%2520LMs%2520and%2520the%2520elimination%2520of%2520the%2520need%250Afor%2520a%2520ground-truth%2520cost%252C%2520our%2520method%2520enhances%2520safe%2520policy%2520learning%2520under%2520a%250Adiverse%2520set%2520of%2520human-derived%2520free-form%2520natural%2520language%2520constraints.%250AExperiments%2520on%2520grid-world%2520navigation%2520and%2520robot%2520control%2520show%2520that%2520the%2520proposed%250Amethod%2520can%2520achieve%2520strong%2520performance%2520while%2520adhering%2520to%2520given%2520constraints.%2520The%250Ausage%2520of%2520pre-trained%2520LMs%2520allows%2520our%2520method%2520to%2520comprehend%2520complicated%250Aconstraints%2520and%2520learn%2520safe%2520policies%2520without%2520the%2520need%2520for%2520ground-truth%2520cost%2520at%250Aany%2520stage%2520of%2520training%2520or%2520evaluation.%2520Extensive%2520ablation%2520studies%2520are%2520conducted%250Ato%2520demonstrate%2520the%2520efficacy%2520of%2520each%2520part%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07553v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Reinforcement%20Learning%20with%20Free-form%20Natural%20Language%20Constraints%0A%20%20and%20Pre-Trained%20Language%20Models&entry.906535625=Xingzhou%20Lou%20and%20Junge%20Zhang%20and%20Ziyan%20Wang%20and%20Kaiqi%20Huang%20and%20Yali%20Du&entry.1292438233=%20%20Safe%20reinforcement%20learning%20%28RL%29%20agents%20accomplish%20given%20tasks%20while%20adhering%0Ato%20specific%20constraints.%20Employing%20constraints%20expressed%20via%0Aeasily-understandable%20human%20language%20offers%20considerable%20potential%20for%0Areal-world%20applications%20due%20to%20its%20accessibility%20and%20non-reliance%20on%20domain%0Aexpertise.%20Previous%20safe%20RL%20methods%20with%20natural%20language%20constraints%20typically%0Aadopt%20a%20recurrent%20neural%20network%2C%20which%20leads%20to%20limited%20capabilities%20when%0Adealing%20with%20various%20forms%20of%20human%20language%20input.%20Furthermore%2C%20these%20methods%0Aoften%20require%20a%20ground-truth%20cost%20function%2C%20necessitating%20domain%20expertise%20for%0Athe%20conversion%20of%20language%20constraints%20into%20a%20well-defined%20cost%20function%20that%0Adetermines%20constraint%20violation.%20To%20address%20these%20issues%2C%20we%20proposes%20to%20use%0Apre-trained%20language%20models%20%28LM%29%20to%20facilitate%20RL%20agents%27%20comprehension%20of%0Anatural%20language%20constraints%20and%20allow%20them%20to%20infer%20costs%20for%20safe%20policy%0Alearning.%20Through%20the%20use%20of%20pre-trained%20LMs%20and%20the%20elimination%20of%20the%20need%0Afor%20a%20ground-truth%20cost%2C%20our%20method%20enhances%20safe%20policy%20learning%20under%20a%0Adiverse%20set%20of%20human-derived%20free-form%20natural%20language%20constraints.%0AExperiments%20on%20grid-world%20navigation%20and%20robot%20control%20show%20that%20the%20proposed%0Amethod%20can%20achieve%20strong%20performance%20while%20adhering%20to%20given%20constraints.%20The%0Ausage%20of%20pre-trained%20LMs%20allows%20our%20method%20to%20comprehend%20complicated%0Aconstraints%20and%20learn%20safe%20policies%20without%20the%20need%20for%20ground-truth%20cost%20at%0Aany%20stage%20of%20training%20or%20evaluation.%20Extensive%20ablation%20studies%20are%20conducted%0Ato%20demonstrate%20the%20efficacy%20of%20each%20part%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07553v3&entry.124074799=Read"},
{"title": "A Reinforcement Learning Approach to Dairy Farm Battery Management using\n  Q Learning", "author": "Nawazish Ali and Abdul Wahid and Rachael Shaw and Karl Mason", "abstract": "  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n", "link": "http://arxiv.org/abs/2403.09499v3", "date": "2024-05-15", "relevancy": 1.6352, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4286}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Reinforcement%20Learning%20Approach%20to%20Dairy%20Farm%20Battery%20Management%20using%0A%20%20Q%20Learning&body=Title%3A%20A%20Reinforcement%20Learning%20Approach%20to%20Dairy%20Farm%20Battery%20Management%20using%0A%20%20Q%20Learning%0AAuthor%3A%20Nawazish%20Ali%20and%20Abdul%20Wahid%20and%20Rachael%20Shaw%20and%20Karl%20Mason%0AAbstract%3A%20%20%20Dairy%20farming%20consumes%20a%20significant%20amount%20of%20energy%2C%20making%20it%20an%0Aenergy-intensive%20sector%20within%20agriculture.%20Integrating%20renewable%20energy%0Ageneration%20into%20dairy%20farming%20could%20help%20address%20this%20challenge.%20Effective%0Abattery%20management%20is%20important%20for%20integrating%20renewable%20energy%20generation.%0AManaging%20battery%20charging%20and%20discharging%20poses%20significant%20challenges%20because%0Aof%20fluctuations%20in%20electrical%20consumption%2C%20the%20intermittent%20nature%20of%20renewable%0Aenergy%20generation%2C%20and%20fluctuations%20in%20energy%20prices.%20Artificial%20Intelligence%0A%28AI%29%20has%20the%20potential%20to%20significantly%20improve%20the%20use%20of%20renewable%20energy%20in%0Adairy%20farming%2C%20however%2C%20there%20is%20limited%20research%20conducted%20in%20this%20particular%0Adomain.%20This%20research%20considers%20Ireland%20as%20a%20case%20study%20as%20it%20works%20towards%0Aattaining%20its%202030%20energy%20strategy%20centered%20on%20the%20utilization%20of%20renewable%0Asources.%20This%20study%20proposes%20a%20Q-learning-based%20algorithm%20for%20scheduling%0Abattery%20charging%20and%20discharging%20in%20a%20dairy%20farm%20setting.%20This%20research%20also%0Aexplores%20the%20effect%20of%20the%20proposed%20algorithm%20by%20adding%20wind%20generation%20data%0Aand%20considering%20additional%20case%20studies.%20The%20proposed%20algorithm%20reduces%20the%0Acost%20of%20imported%20electricity%20from%20the%20grid%20by%2013.41%25%2C%20peak%20demand%20by%202%25%2C%20and%0A24.49%25%20when%20utilizing%20wind%20generation.%20These%20results%20underline%20how%0Areinforcement%20learning%20is%20highly%20effective%20in%20managing%20batteries%20in%20the%20dairy%0Afarming%20sector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09499v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Reinforcement%2520Learning%2520Approach%2520to%2520Dairy%2520Farm%2520Battery%2520Management%2520using%250A%2520%2520Q%2520Learning%26entry.906535625%3DNawazish%2520Ali%2520and%2520Abdul%2520Wahid%2520and%2520Rachael%2520Shaw%2520and%2520Karl%2520Mason%26entry.1292438233%3D%2520%2520Dairy%2520farming%2520consumes%2520a%2520significant%2520amount%2520of%2520energy%252C%2520making%2520it%2520an%250Aenergy-intensive%2520sector%2520within%2520agriculture.%2520Integrating%2520renewable%2520energy%250Ageneration%2520into%2520dairy%2520farming%2520could%2520help%2520address%2520this%2520challenge.%2520Effective%250Abattery%2520management%2520is%2520important%2520for%2520integrating%2520renewable%2520energy%2520generation.%250AManaging%2520battery%2520charging%2520and%2520discharging%2520poses%2520significant%2520challenges%2520because%250Aof%2520fluctuations%2520in%2520electrical%2520consumption%252C%2520the%2520intermittent%2520nature%2520of%2520renewable%250Aenergy%2520generation%252C%2520and%2520fluctuations%2520in%2520energy%2520prices.%2520Artificial%2520Intelligence%250A%2528AI%2529%2520has%2520the%2520potential%2520to%2520significantly%2520improve%2520the%2520use%2520of%2520renewable%2520energy%2520in%250Adairy%2520farming%252C%2520however%252C%2520there%2520is%2520limited%2520research%2520conducted%2520in%2520this%2520particular%250Adomain.%2520This%2520research%2520considers%2520Ireland%2520as%2520a%2520case%2520study%2520as%2520it%2520works%2520towards%250Aattaining%2520its%25202030%2520energy%2520strategy%2520centered%2520on%2520the%2520utilization%2520of%2520renewable%250Asources.%2520This%2520study%2520proposes%2520a%2520Q-learning-based%2520algorithm%2520for%2520scheduling%250Abattery%2520charging%2520and%2520discharging%2520in%2520a%2520dairy%2520farm%2520setting.%2520This%2520research%2520also%250Aexplores%2520the%2520effect%2520of%2520the%2520proposed%2520algorithm%2520by%2520adding%2520wind%2520generation%2520data%250Aand%2520considering%2520additional%2520case%2520studies.%2520The%2520proposed%2520algorithm%2520reduces%2520the%250Acost%2520of%2520imported%2520electricity%2520from%2520the%2520grid%2520by%252013.41%2525%252C%2520peak%2520demand%2520by%25202%2525%252C%2520and%250A24.49%2525%2520when%2520utilizing%2520wind%2520generation.%2520These%2520results%2520underline%2520how%250Areinforcement%2520learning%2520is%2520highly%2520effective%2520in%2520managing%2520batteries%2520in%2520the%2520dairy%250Afarming%2520sector.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09499v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Reinforcement%20Learning%20Approach%20to%20Dairy%20Farm%20Battery%20Management%20using%0A%20%20Q%20Learning&entry.906535625=Nawazish%20Ali%20and%20Abdul%20Wahid%20and%20Rachael%20Shaw%20and%20Karl%20Mason&entry.1292438233=%20%20Dairy%20farming%20consumes%20a%20significant%20amount%20of%20energy%2C%20making%20it%20an%0Aenergy-intensive%20sector%20within%20agriculture.%20Integrating%20renewable%20energy%0Ageneration%20into%20dairy%20farming%20could%20help%20address%20this%20challenge.%20Effective%0Abattery%20management%20is%20important%20for%20integrating%20renewable%20energy%20generation.%0AManaging%20battery%20charging%20and%20discharging%20poses%20significant%20challenges%20because%0Aof%20fluctuations%20in%20electrical%20consumption%2C%20the%20intermittent%20nature%20of%20renewable%0Aenergy%20generation%2C%20and%20fluctuations%20in%20energy%20prices.%20Artificial%20Intelligence%0A%28AI%29%20has%20the%20potential%20to%20significantly%20improve%20the%20use%20of%20renewable%20energy%20in%0Adairy%20farming%2C%20however%2C%20there%20is%20limited%20research%20conducted%20in%20this%20particular%0Adomain.%20This%20research%20considers%20Ireland%20as%20a%20case%20study%20as%20it%20works%20towards%0Aattaining%20its%202030%20energy%20strategy%20centered%20on%20the%20utilization%20of%20renewable%0Asources.%20This%20study%20proposes%20a%20Q-learning-based%20algorithm%20for%20scheduling%0Abattery%20charging%20and%20discharging%20in%20a%20dairy%20farm%20setting.%20This%20research%20also%0Aexplores%20the%20effect%20of%20the%20proposed%20algorithm%20by%20adding%20wind%20generation%20data%0Aand%20considering%20additional%20case%20studies.%20The%20proposed%20algorithm%20reduces%20the%0Acost%20of%20imported%20electricity%20from%20the%20grid%20by%2013.41%25%2C%20peak%20demand%20by%202%25%2C%20and%0A24.49%25%20when%20utilizing%20wind%20generation.%20These%20results%20underline%20how%0Areinforcement%20learning%20is%20highly%20effective%20in%20managing%20batteries%20in%20the%20dairy%0Afarming%20sector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09499v3&entry.124074799=Read"},
{"title": "Stationarity without mean reversion in improper Gaussian processes", "author": "Luca Ambrogioni", "abstract": "  The behavior of a GP regression depends on the choice of covariance function.\nStationary covariance functions are preferred in machine learning applications.\nHowever, (non-periodic) stationary covariance functions are always mean\nreverting and can therefore exhibit pathological behavior when applied to data\nthat does not relax to a fixed global mean value. In this paper we show that it\nis possible to use improper GP priors with infinite variance to define\nprocesses that are stationary but not mean reverting. To this aim, we use of\nnon-positive kernels that can only be defined in this limit regime. The\nresulting posterior distributions can be computed analytically and it involves\na simple correction of the usual formulas. The main contribution of the paper\nis the introduction of a large family of smooth non-reverting covariance\nfunctions that closely resemble the kernels commonly used in the GP literature\n(e.g. squared exponential and Mat\\'ern class). By analyzing both synthetic and\nreal data, we demonstrate that these non-positive kernels solve some known\npathologies of mean reverting GP regression while retaining most of the\nfavorable properties of ordinary smooth stationary kernels.\n", "link": "http://arxiv.org/abs/2310.02877v2", "date": "2024-05-15", "relevancy": 1.6251, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4021}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stationarity%20without%20mean%20reversion%20in%20improper%20Gaussian%20processes&body=Title%3A%20Stationarity%20without%20mean%20reversion%20in%20improper%20Gaussian%20processes%0AAuthor%3A%20Luca%20Ambrogioni%0AAbstract%3A%20%20%20The%20behavior%20of%20a%20GP%20regression%20depends%20on%20the%20choice%20of%20covariance%20function.%0AStationary%20covariance%20functions%20are%20preferred%20in%20machine%20learning%20applications.%0AHowever%2C%20%28non-periodic%29%20stationary%20covariance%20functions%20are%20always%20mean%0Areverting%20and%20can%20therefore%20exhibit%20pathological%20behavior%20when%20applied%20to%20data%0Athat%20does%20not%20relax%20to%20a%20fixed%20global%20mean%20value.%20In%20this%20paper%20we%20show%20that%20it%0Ais%20possible%20to%20use%20improper%20GP%20priors%20with%20infinite%20variance%20to%20define%0Aprocesses%20that%20are%20stationary%20but%20not%20mean%20reverting.%20To%20this%20aim%2C%20we%20use%20of%0Anon-positive%20kernels%20that%20can%20only%20be%20defined%20in%20this%20limit%20regime.%20The%0Aresulting%20posterior%20distributions%20can%20be%20computed%20analytically%20and%20it%20involves%0Aa%20simple%20correction%20of%20the%20usual%20formulas.%20The%20main%20contribution%20of%20the%20paper%0Ais%20the%20introduction%20of%20a%20large%20family%20of%20smooth%20non-reverting%20covariance%0Afunctions%20that%20closely%20resemble%20the%20kernels%20commonly%20used%20in%20the%20GP%20literature%0A%28e.g.%20squared%20exponential%20and%20Mat%5C%27ern%20class%29.%20By%20analyzing%20both%20synthetic%20and%0Areal%20data%2C%20we%20demonstrate%20that%20these%20non-positive%20kernels%20solve%20some%20known%0Apathologies%20of%20mean%20reverting%20GP%20regression%20while%20retaining%20most%20of%20the%0Afavorable%20properties%20of%20ordinary%20smooth%20stationary%20kernels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStationarity%2520without%2520mean%2520reversion%2520in%2520improper%2520Gaussian%2520processes%26entry.906535625%3DLuca%2520Ambrogioni%26entry.1292438233%3D%2520%2520The%2520behavior%2520of%2520a%2520GP%2520regression%2520depends%2520on%2520the%2520choice%2520of%2520covariance%2520function.%250AStationary%2520covariance%2520functions%2520are%2520preferred%2520in%2520machine%2520learning%2520applications.%250AHowever%252C%2520%2528non-periodic%2529%2520stationary%2520covariance%2520functions%2520are%2520always%2520mean%250Areverting%2520and%2520can%2520therefore%2520exhibit%2520pathological%2520behavior%2520when%2520applied%2520to%2520data%250Athat%2520does%2520not%2520relax%2520to%2520a%2520fixed%2520global%2520mean%2520value.%2520In%2520this%2520paper%2520we%2520show%2520that%2520it%250Ais%2520possible%2520to%2520use%2520improper%2520GP%2520priors%2520with%2520infinite%2520variance%2520to%2520define%250Aprocesses%2520that%2520are%2520stationary%2520but%2520not%2520mean%2520reverting.%2520To%2520this%2520aim%252C%2520we%2520use%2520of%250Anon-positive%2520kernels%2520that%2520can%2520only%2520be%2520defined%2520in%2520this%2520limit%2520regime.%2520The%250Aresulting%2520posterior%2520distributions%2520can%2520be%2520computed%2520analytically%2520and%2520it%2520involves%250Aa%2520simple%2520correction%2520of%2520the%2520usual%2520formulas.%2520The%2520main%2520contribution%2520of%2520the%2520paper%250Ais%2520the%2520introduction%2520of%2520a%2520large%2520family%2520of%2520smooth%2520non-reverting%2520covariance%250Afunctions%2520that%2520closely%2520resemble%2520the%2520kernels%2520commonly%2520used%2520in%2520the%2520GP%2520literature%250A%2528e.g.%2520squared%2520exponential%2520and%2520Mat%255C%2527ern%2520class%2529.%2520By%2520analyzing%2520both%2520synthetic%2520and%250Areal%2520data%252C%2520we%2520demonstrate%2520that%2520these%2520non-positive%2520kernels%2520solve%2520some%2520known%250Apathologies%2520of%2520mean%2520reverting%2520GP%2520regression%2520while%2520retaining%2520most%2520of%2520the%250Afavorable%2520properties%2520of%2520ordinary%2520smooth%2520stationary%2520kernels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stationarity%20without%20mean%20reversion%20in%20improper%20Gaussian%20processes&entry.906535625=Luca%20Ambrogioni&entry.1292438233=%20%20The%20behavior%20of%20a%20GP%20regression%20depends%20on%20the%20choice%20of%20covariance%20function.%0AStationary%20covariance%20functions%20are%20preferred%20in%20machine%20learning%20applications.%0AHowever%2C%20%28non-periodic%29%20stationary%20covariance%20functions%20are%20always%20mean%0Areverting%20and%20can%20therefore%20exhibit%20pathological%20behavior%20when%20applied%20to%20data%0Athat%20does%20not%20relax%20to%20a%20fixed%20global%20mean%20value.%20In%20this%20paper%20we%20show%20that%20it%0Ais%20possible%20to%20use%20improper%20GP%20priors%20with%20infinite%20variance%20to%20define%0Aprocesses%20that%20are%20stationary%20but%20not%20mean%20reverting.%20To%20this%20aim%2C%20we%20use%20of%0Anon-positive%20kernels%20that%20can%20only%20be%20defined%20in%20this%20limit%20regime.%20The%0Aresulting%20posterior%20distributions%20can%20be%20computed%20analytically%20and%20it%20involves%0Aa%20simple%20correction%20of%20the%20usual%20formulas.%20The%20main%20contribution%20of%20the%20paper%0Ais%20the%20introduction%20of%20a%20large%20family%20of%20smooth%20non-reverting%20covariance%0Afunctions%20that%20closely%20resemble%20the%20kernels%20commonly%20used%20in%20the%20GP%20literature%0A%28e.g.%20squared%20exponential%20and%20Mat%5C%27ern%20class%29.%20By%20analyzing%20both%20synthetic%20and%0Areal%20data%2C%20we%20demonstrate%20that%20these%20non-positive%20kernels%20solve%20some%20known%0Apathologies%20of%20mean%20reverting%20GP%20regression%20while%20retaining%20most%20of%20the%0Afavorable%20properties%20of%20ordinary%20smooth%20stationary%20kernels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02877v2&entry.124074799=Read"},
{"title": "nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance", "author": "Yunxiang Li and Bowen Jing and Zihan Li and Jing Wang and You Zhang", "abstract": "  Automatic segmentation of medical images is crucial in modern clinical\nworkflows. The Segment Anything Model (SAM) has emerged as a versatile tool for\nimage segmentation without specific domain training, but it requires human\nprompts and may have limitations in specific domains. Traditional models like\nnnUNet perform automatic segmentation during inference and are effective in\nspecific domains but need extensive domain-specific training. To combine the\nstrengths of foundational and domain-specific models, we propose nnSAM,\nintegrating SAM's robust feature extraction with nnUNet's automatic\nconfiguration to enhance segmentation accuracy on small datasets. Our nnSAM\nmodel optimizes two main approaches: leveraging SAM's feature extraction and\nnnUNet's domain-specific adaptation, and incorporating a boundary shape\nsupervision loss function based on level set functions and curvature\ncalculations to learn anatomical shape priors from limited data. We evaluated\nnnSAM on four segmentation tasks: brain white matter, liver, lung, and heart\nsegmentation. Our method outperformed others, achieving the highest DICE score\nof 82.77% and the lowest ASD of 1.14 mm in brain white matter segmentation with\n20 training samples, compared to nnUNet's DICE score of 79.25% and ASD of 1.36\nmm. A sample size study highlighted nnSAM's advantage with fewer training\nsamples. Our results demonstrate significant improvements in segmentation\nperformance with nnSAM, showcasing its potential for small-sample learning in\nmedical image segmentation.\n", "link": "http://arxiv.org/abs/2309.16967v3", "date": "2024-05-15", "relevancy": 1.6226, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5514}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5458}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20nnSAM%3A%20Plug-and-play%20Segment%20Anything%20Model%20Improves%20nnUNet%20Performance&body=Title%3A%20nnSAM%3A%20Plug-and-play%20Segment%20Anything%20Model%20Improves%20nnUNet%20Performance%0AAuthor%3A%20Yunxiang%20Li%20and%20Bowen%20Jing%20and%20Zihan%20Li%20and%20Jing%20Wang%20and%20You%20Zhang%0AAbstract%3A%20%20%20Automatic%20segmentation%20of%20medical%20images%20is%20crucial%20in%20modern%20clinical%0Aworkflows.%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20emerged%20as%20a%20versatile%20tool%20for%0Aimage%20segmentation%20without%20specific%20domain%20training%2C%20but%20it%20requires%20human%0Aprompts%20and%20may%20have%20limitations%20in%20specific%20domains.%20Traditional%20models%20like%0AnnUNet%20perform%20automatic%20segmentation%20during%20inference%20and%20are%20effective%20in%0Aspecific%20domains%20but%20need%20extensive%20domain-specific%20training.%20To%20combine%20the%0Astrengths%20of%20foundational%20and%20domain-specific%20models%2C%20we%20propose%20nnSAM%2C%0Aintegrating%20SAM%27s%20robust%20feature%20extraction%20with%20nnUNet%27s%20automatic%0Aconfiguration%20to%20enhance%20segmentation%20accuracy%20on%20small%20datasets.%20Our%20nnSAM%0Amodel%20optimizes%20two%20main%20approaches%3A%20leveraging%20SAM%27s%20feature%20extraction%20and%0AnnUNet%27s%20domain-specific%20adaptation%2C%20and%20incorporating%20a%20boundary%20shape%0Asupervision%20loss%20function%20based%20on%20level%20set%20functions%20and%20curvature%0Acalculations%20to%20learn%20anatomical%20shape%20priors%20from%20limited%20data.%20We%20evaluated%0AnnSAM%20on%20four%20segmentation%20tasks%3A%20brain%20white%20matter%2C%20liver%2C%20lung%2C%20and%20heart%0Asegmentation.%20Our%20method%20outperformed%20others%2C%20achieving%20the%20highest%20DICE%20score%0Aof%2082.77%25%20and%20the%20lowest%20ASD%20of%201.14%20mm%20in%20brain%20white%20matter%20segmentation%20with%0A20%20training%20samples%2C%20compared%20to%20nnUNet%27s%20DICE%20score%20of%2079.25%25%20and%20ASD%20of%201.36%0Amm.%20A%20sample%20size%20study%20highlighted%20nnSAM%27s%20advantage%20with%20fewer%20training%0Asamples.%20Our%20results%20demonstrate%20significant%20improvements%20in%20segmentation%0Aperformance%20with%20nnSAM%2C%20showcasing%20its%20potential%20for%20small-sample%20learning%20in%0Amedical%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16967v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DnnSAM%253A%2520Plug-and-play%2520Segment%2520Anything%2520Model%2520Improves%2520nnUNet%2520Performance%26entry.906535625%3DYunxiang%2520Li%2520and%2520Bowen%2520Jing%2520and%2520Zihan%2520Li%2520and%2520Jing%2520Wang%2520and%2520You%2520Zhang%26entry.1292438233%3D%2520%2520Automatic%2520segmentation%2520of%2520medical%2520images%2520is%2520crucial%2520in%2520modern%2520clinical%250Aworkflows.%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520emerged%2520as%2520a%2520versatile%2520tool%2520for%250Aimage%2520segmentation%2520without%2520specific%2520domain%2520training%252C%2520but%2520it%2520requires%2520human%250Aprompts%2520and%2520may%2520have%2520limitations%2520in%2520specific%2520domains.%2520Traditional%2520models%2520like%250AnnUNet%2520perform%2520automatic%2520segmentation%2520during%2520inference%2520and%2520are%2520effective%2520in%250Aspecific%2520domains%2520but%2520need%2520extensive%2520domain-specific%2520training.%2520To%2520combine%2520the%250Astrengths%2520of%2520foundational%2520and%2520domain-specific%2520models%252C%2520we%2520propose%2520nnSAM%252C%250Aintegrating%2520SAM%2527s%2520robust%2520feature%2520extraction%2520with%2520nnUNet%2527s%2520automatic%250Aconfiguration%2520to%2520enhance%2520segmentation%2520accuracy%2520on%2520small%2520datasets.%2520Our%2520nnSAM%250Amodel%2520optimizes%2520two%2520main%2520approaches%253A%2520leveraging%2520SAM%2527s%2520feature%2520extraction%2520and%250AnnUNet%2527s%2520domain-specific%2520adaptation%252C%2520and%2520incorporating%2520a%2520boundary%2520shape%250Asupervision%2520loss%2520function%2520based%2520on%2520level%2520set%2520functions%2520and%2520curvature%250Acalculations%2520to%2520learn%2520anatomical%2520shape%2520priors%2520from%2520limited%2520data.%2520We%2520evaluated%250AnnSAM%2520on%2520four%2520segmentation%2520tasks%253A%2520brain%2520white%2520matter%252C%2520liver%252C%2520lung%252C%2520and%2520heart%250Asegmentation.%2520Our%2520method%2520outperformed%2520others%252C%2520achieving%2520the%2520highest%2520DICE%2520score%250Aof%252082.77%2525%2520and%2520the%2520lowest%2520ASD%2520of%25201.14%2520mm%2520in%2520brain%2520white%2520matter%2520segmentation%2520with%250A20%2520training%2520samples%252C%2520compared%2520to%2520nnUNet%2527s%2520DICE%2520score%2520of%252079.25%2525%2520and%2520ASD%2520of%25201.36%250Amm.%2520A%2520sample%2520size%2520study%2520highlighted%2520nnSAM%2527s%2520advantage%2520with%2520fewer%2520training%250Asamples.%2520Our%2520results%2520demonstrate%2520significant%2520improvements%2520in%2520segmentation%250Aperformance%2520with%2520nnSAM%252C%2520showcasing%2520its%2520potential%2520for%2520small-sample%2520learning%2520in%250Amedical%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16967v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=nnSAM%3A%20Plug-and-play%20Segment%20Anything%20Model%20Improves%20nnUNet%20Performance&entry.906535625=Yunxiang%20Li%20and%20Bowen%20Jing%20and%20Zihan%20Li%20and%20Jing%20Wang%20and%20You%20Zhang&entry.1292438233=%20%20Automatic%20segmentation%20of%20medical%20images%20is%20crucial%20in%20modern%20clinical%0Aworkflows.%20The%20Segment%20Anything%20Model%20%28SAM%29%20has%20emerged%20as%20a%20versatile%20tool%20for%0Aimage%20segmentation%20without%20specific%20domain%20training%2C%20but%20it%20requires%20human%0Aprompts%20and%20may%20have%20limitations%20in%20specific%20domains.%20Traditional%20models%20like%0AnnUNet%20perform%20automatic%20segmentation%20during%20inference%20and%20are%20effective%20in%0Aspecific%20domains%20but%20need%20extensive%20domain-specific%20training.%20To%20combine%20the%0Astrengths%20of%20foundational%20and%20domain-specific%20models%2C%20we%20propose%20nnSAM%2C%0Aintegrating%20SAM%27s%20robust%20feature%20extraction%20with%20nnUNet%27s%20automatic%0Aconfiguration%20to%20enhance%20segmentation%20accuracy%20on%20small%20datasets.%20Our%20nnSAM%0Amodel%20optimizes%20two%20main%20approaches%3A%20leveraging%20SAM%27s%20feature%20extraction%20and%0AnnUNet%27s%20domain-specific%20adaptation%2C%20and%20incorporating%20a%20boundary%20shape%0Asupervision%20loss%20function%20based%20on%20level%20set%20functions%20and%20curvature%0Acalculations%20to%20learn%20anatomical%20shape%20priors%20from%20limited%20data.%20We%20evaluated%0AnnSAM%20on%20four%20segmentation%20tasks%3A%20brain%20white%20matter%2C%20liver%2C%20lung%2C%20and%20heart%0Asegmentation.%20Our%20method%20outperformed%20others%2C%20achieving%20the%20highest%20DICE%20score%0Aof%2082.77%25%20and%20the%20lowest%20ASD%20of%201.14%20mm%20in%20brain%20white%20matter%20segmentation%20with%0A20%20training%20samples%2C%20compared%20to%20nnUNet%27s%20DICE%20score%20of%2079.25%25%20and%20ASD%20of%201.36%0Amm.%20A%20sample%20size%20study%20highlighted%20nnSAM%27s%20advantage%20with%20fewer%20training%0Asamples.%20Our%20results%20demonstrate%20significant%20improvements%20in%20segmentation%0Aperformance%20with%20nnSAM%2C%20showcasing%20its%20potential%20for%20small-sample%20learning%20in%0Amedical%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16967v3&entry.124074799=Read"},
{"title": "Towards a fully declarative neuro-symbolic language", "author": "Tilman Hinnerichs and Robin Manhaeve and Giuseppe Marra and Sebastijan Dumancic", "abstract": "  Neuro-symbolic systems (NeSy), which claim to combine the best of both\nlearning and reasoning capabilities of artificial intelligence, are missing a\ncore property of reasoning systems: Declarativeness. The lack of\ndeclarativeness is caused by the functional nature of neural predicates\ninherited from neural networks. We propose and implement a general framework\nfor fully declarative neural predicates, which hence extends to fully\ndeclarative NeSy frameworks. We first show that the declarative extension\npreserves the learning and reasoning capabilities while being able to answer\narbitrary queries while only being trained on a single query type.\n", "link": "http://arxiv.org/abs/2405.09521v1", "date": "2024-05-15", "relevancy": 1.6196, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4521}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.397}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20fully%20declarative%20neuro-symbolic%20language&body=Title%3A%20Towards%20a%20fully%20declarative%20neuro-symbolic%20language%0AAuthor%3A%20Tilman%20Hinnerichs%20and%20Robin%20Manhaeve%20and%20Giuseppe%20Marra%20and%20Sebastijan%20Dumancic%0AAbstract%3A%20%20%20Neuro-symbolic%20systems%20%28NeSy%29%2C%20which%20claim%20to%20combine%20the%20best%20of%20both%0Alearning%20and%20reasoning%20capabilities%20of%20artificial%20intelligence%2C%20are%20missing%20a%0Acore%20property%20of%20reasoning%20systems%3A%20Declarativeness.%20The%20lack%20of%0Adeclarativeness%20is%20caused%20by%20the%20functional%20nature%20of%20neural%20predicates%0Ainherited%20from%20neural%20networks.%20We%20propose%20and%20implement%20a%20general%20framework%0Afor%20fully%20declarative%20neural%20predicates%2C%20which%20hence%20extends%20to%20fully%0Adeclarative%20NeSy%20frameworks.%20We%20first%20show%20that%20the%20declarative%20extension%0Apreserves%20the%20learning%20and%20reasoning%20capabilities%20while%20being%20able%20to%20answer%0Aarbitrary%20queries%20while%20only%20being%20trained%20on%20a%20single%20query%20type.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520fully%2520declarative%2520neuro-symbolic%2520language%26entry.906535625%3DTilman%2520Hinnerichs%2520and%2520Robin%2520Manhaeve%2520and%2520Giuseppe%2520Marra%2520and%2520Sebastijan%2520Dumancic%26entry.1292438233%3D%2520%2520Neuro-symbolic%2520systems%2520%2528NeSy%2529%252C%2520which%2520claim%2520to%2520combine%2520the%2520best%2520of%2520both%250Alearning%2520and%2520reasoning%2520capabilities%2520of%2520artificial%2520intelligence%252C%2520are%2520missing%2520a%250Acore%2520property%2520of%2520reasoning%2520systems%253A%2520Declarativeness.%2520The%2520lack%2520of%250Adeclarativeness%2520is%2520caused%2520by%2520the%2520functional%2520nature%2520of%2520neural%2520predicates%250Ainherited%2520from%2520neural%2520networks.%2520We%2520propose%2520and%2520implement%2520a%2520general%2520framework%250Afor%2520fully%2520declarative%2520neural%2520predicates%252C%2520which%2520hence%2520extends%2520to%2520fully%250Adeclarative%2520NeSy%2520frameworks.%2520We%2520first%2520show%2520that%2520the%2520declarative%2520extension%250Apreserves%2520the%2520learning%2520and%2520reasoning%2520capabilities%2520while%2520being%2520able%2520to%2520answer%250Aarbitrary%2520queries%2520while%2520only%2520being%2520trained%2520on%2520a%2520single%2520query%2520type.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20fully%20declarative%20neuro-symbolic%20language&entry.906535625=Tilman%20Hinnerichs%20and%20Robin%20Manhaeve%20and%20Giuseppe%20Marra%20and%20Sebastijan%20Dumancic&entry.1292438233=%20%20Neuro-symbolic%20systems%20%28NeSy%29%2C%20which%20claim%20to%20combine%20the%20best%20of%20both%0Alearning%20and%20reasoning%20capabilities%20of%20artificial%20intelligence%2C%20are%20missing%20a%0Acore%20property%20of%20reasoning%20systems%3A%20Declarativeness.%20The%20lack%20of%0Adeclarativeness%20is%20caused%20by%20the%20functional%20nature%20of%20neural%20predicates%0Ainherited%20from%20neural%20networks.%20We%20propose%20and%20implement%20a%20general%20framework%0Afor%20fully%20declarative%20neural%20predicates%2C%20which%20hence%20extends%20to%20fully%0Adeclarative%20NeSy%20frameworks.%20We%20first%20show%20that%20the%20declarative%20extension%0Apreserves%20the%20learning%20and%20reasoning%20capabilities%20while%20being%20able%20to%20answer%0Aarbitrary%20queries%20while%20only%20being%20trained%20on%20a%20single%20query%20type.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09521v1&entry.124074799=Read"},
{"title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and\n  Prompt Engineering May Not Help You", "author": "Felix Friedrich and Katharina H\u00e4mmerl and Patrick Schramowski and Manuel Brack and Jindrich Libovicky and Kristian Kersting and Alexander Fraser", "abstract": "  Text-to-image generation models have recently achieved astonishing results in\nimage quality, flexibility, and text alignment, and are consequently employed\nin a fast-growing number of applications. Through improvements in multilingual\nabilities, a larger community now has access to this technology. However, our\nresults show that multilingual models suffer from significant gender biases\njust as monolingual models do. Furthermore, the natural expectation that\nmultilingual models will provide similar results across languages does not hold\nup. Instead, there are important differences between languages. We propose a\nnovel benchmark, MAGBIG, intended to foster research on gender bias in\nmultilingual models. We use MAGBIG to investigate the effect of multilingualism\non gender bias in T2I models. To this end, we construct multilingual prompts\nrequesting portraits of people with a certain occupation or trait. Our results\nshow that not only do models exhibit strong gender biases but they also behave\ndifferently across languages. Furthermore, we investigate prompt engineering\nstrategies, such as indirect, neutral formulations, to mitigate these biases.\nUnfortunately, these approaches have limited success and result in worse\ntext-to-image alignment. Consequently, we call for more research into diverse\nrepresentations across languages in image generators, as well as into\nsteerability to address biased model behavior.\n", "link": "http://arxiv.org/abs/2401.16092v3", "date": "2024-05-15", "relevancy": 1.6033, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5602}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5277}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Text-to-Image%20Generation%20Magnifies%20Gender%20Stereotypes%20and%0A%20%20Prompt%20Engineering%20May%20Not%20Help%20You&body=Title%3A%20Multilingual%20Text-to-Image%20Generation%20Magnifies%20Gender%20Stereotypes%20and%0A%20%20Prompt%20Engineering%20May%20Not%20Help%20You%0AAuthor%3A%20Felix%20Friedrich%20and%20Katharina%20H%C3%A4mmerl%20and%20Patrick%20Schramowski%20and%20Manuel%20Brack%20and%20Jindrich%20Libovicky%20and%20Kristian%20Kersting%20and%20Alexander%20Fraser%0AAbstract%3A%20%20%20Text-to-image%20generation%20models%20have%20recently%20achieved%20astonishing%20results%20in%0Aimage%20quality%2C%20flexibility%2C%20and%20text%20alignment%2C%20and%20are%20consequently%20employed%0Ain%20a%20fast-growing%20number%20of%20applications.%20Through%20improvements%20in%20multilingual%0Aabilities%2C%20a%20larger%20community%20now%20has%20access%20to%20this%20technology.%20However%2C%20our%0Aresults%20show%20that%20multilingual%20models%20suffer%20from%20significant%20gender%20biases%0Ajust%20as%20monolingual%20models%20do.%20Furthermore%2C%20the%20natural%20expectation%20that%0Amultilingual%20models%20will%20provide%20similar%20results%20across%20languages%20does%20not%20hold%0Aup.%20Instead%2C%20there%20are%20important%20differences%20between%20languages.%20We%20propose%20a%0Anovel%20benchmark%2C%20MAGBIG%2C%20intended%20to%20foster%20research%20on%20gender%20bias%20in%0Amultilingual%20models.%20We%20use%20MAGBIG%20to%20investigate%20the%20effect%20of%20multilingualism%0Aon%20gender%20bias%20in%20T2I%20models.%20To%20this%20end%2C%20we%20construct%20multilingual%20prompts%0Arequesting%20portraits%20of%20people%20with%20a%20certain%20occupation%20or%20trait.%20Our%20results%0Ashow%20that%20not%20only%20do%20models%20exhibit%20strong%20gender%20biases%20but%20they%20also%20behave%0Adifferently%20across%20languages.%20Furthermore%2C%20we%20investigate%20prompt%20engineering%0Astrategies%2C%20such%20as%20indirect%2C%20neutral%20formulations%2C%20to%20mitigate%20these%20biases.%0AUnfortunately%2C%20these%20approaches%20have%20limited%20success%20and%20result%20in%20worse%0Atext-to-image%20alignment.%20Consequently%2C%20we%20call%20for%20more%20research%20into%20diverse%0Arepresentations%20across%20languages%20in%20image%20generators%2C%20as%20well%20as%20into%0Asteerability%20to%20address%20biased%20model%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16092v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Text-to-Image%2520Generation%2520Magnifies%2520Gender%2520Stereotypes%2520and%250A%2520%2520Prompt%2520Engineering%2520May%2520Not%2520Help%2520You%26entry.906535625%3DFelix%2520Friedrich%2520and%2520Katharina%2520H%25C3%25A4mmerl%2520and%2520Patrick%2520Schramowski%2520and%2520Manuel%2520Brack%2520and%2520Jindrich%2520Libovicky%2520and%2520Kristian%2520Kersting%2520and%2520Alexander%2520Fraser%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520models%2520have%2520recently%2520achieved%2520astonishing%2520results%2520in%250Aimage%2520quality%252C%2520flexibility%252C%2520and%2520text%2520alignment%252C%2520and%2520are%2520consequently%2520employed%250Ain%2520a%2520fast-growing%2520number%2520of%2520applications.%2520Through%2520improvements%2520in%2520multilingual%250Aabilities%252C%2520a%2520larger%2520community%2520now%2520has%2520access%2520to%2520this%2520technology.%2520However%252C%2520our%250Aresults%2520show%2520that%2520multilingual%2520models%2520suffer%2520from%2520significant%2520gender%2520biases%250Ajust%2520as%2520monolingual%2520models%2520do.%2520Furthermore%252C%2520the%2520natural%2520expectation%2520that%250Amultilingual%2520models%2520will%2520provide%2520similar%2520results%2520across%2520languages%2520does%2520not%2520hold%250Aup.%2520Instead%252C%2520there%2520are%2520important%2520differences%2520between%2520languages.%2520We%2520propose%2520a%250Anovel%2520benchmark%252C%2520MAGBIG%252C%2520intended%2520to%2520foster%2520research%2520on%2520gender%2520bias%2520in%250Amultilingual%2520models.%2520We%2520use%2520MAGBIG%2520to%2520investigate%2520the%2520effect%2520of%2520multilingualism%250Aon%2520gender%2520bias%2520in%2520T2I%2520models.%2520To%2520this%2520end%252C%2520we%2520construct%2520multilingual%2520prompts%250Arequesting%2520portraits%2520of%2520people%2520with%2520a%2520certain%2520occupation%2520or%2520trait.%2520Our%2520results%250Ashow%2520that%2520not%2520only%2520do%2520models%2520exhibit%2520strong%2520gender%2520biases%2520but%2520they%2520also%2520behave%250Adifferently%2520across%2520languages.%2520Furthermore%252C%2520we%2520investigate%2520prompt%2520engineering%250Astrategies%252C%2520such%2520as%2520indirect%252C%2520neutral%2520formulations%252C%2520to%2520mitigate%2520these%2520biases.%250AUnfortunately%252C%2520these%2520approaches%2520have%2520limited%2520success%2520and%2520result%2520in%2520worse%250Atext-to-image%2520alignment.%2520Consequently%252C%2520we%2520call%2520for%2520more%2520research%2520into%2520diverse%250Arepresentations%2520across%2520languages%2520in%2520image%2520generators%252C%2520as%2520well%2520as%2520into%250Asteerability%2520to%2520address%2520biased%2520model%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16092v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Text-to-Image%20Generation%20Magnifies%20Gender%20Stereotypes%20and%0A%20%20Prompt%20Engineering%20May%20Not%20Help%20You&entry.906535625=Felix%20Friedrich%20and%20Katharina%20H%C3%A4mmerl%20and%20Patrick%20Schramowski%20and%20Manuel%20Brack%20and%20Jindrich%20Libovicky%20and%20Kristian%20Kersting%20and%20Alexander%20Fraser&entry.1292438233=%20%20Text-to-image%20generation%20models%20have%20recently%20achieved%20astonishing%20results%20in%0Aimage%20quality%2C%20flexibility%2C%20and%20text%20alignment%2C%20and%20are%20consequently%20employed%0Ain%20a%20fast-growing%20number%20of%20applications.%20Through%20improvements%20in%20multilingual%0Aabilities%2C%20a%20larger%20community%20now%20has%20access%20to%20this%20technology.%20However%2C%20our%0Aresults%20show%20that%20multilingual%20models%20suffer%20from%20significant%20gender%20biases%0Ajust%20as%20monolingual%20models%20do.%20Furthermore%2C%20the%20natural%20expectation%20that%0Amultilingual%20models%20will%20provide%20similar%20results%20across%20languages%20does%20not%20hold%0Aup.%20Instead%2C%20there%20are%20important%20differences%20between%20languages.%20We%20propose%20a%0Anovel%20benchmark%2C%20MAGBIG%2C%20intended%20to%20foster%20research%20on%20gender%20bias%20in%0Amultilingual%20models.%20We%20use%20MAGBIG%20to%20investigate%20the%20effect%20of%20multilingualism%0Aon%20gender%20bias%20in%20T2I%20models.%20To%20this%20end%2C%20we%20construct%20multilingual%20prompts%0Arequesting%20portraits%20of%20people%20with%20a%20certain%20occupation%20or%20trait.%20Our%20results%0Ashow%20that%20not%20only%20do%20models%20exhibit%20strong%20gender%20biases%20but%20they%20also%20behave%0Adifferently%20across%20languages.%20Furthermore%2C%20we%20investigate%20prompt%20engineering%0Astrategies%2C%20such%20as%20indirect%2C%20neutral%20formulations%2C%20to%20mitigate%20these%20biases.%0AUnfortunately%2C%20these%20approaches%20have%20limited%20success%20and%20result%20in%20worse%0Atext-to-image%20alignment.%20Consequently%2C%20we%20call%20for%20more%20research%20into%20diverse%0Arepresentations%20across%20languages%20in%20image%20generators%2C%20as%20well%20as%20into%0Asteerability%20to%20address%20biased%20model%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16092v3&entry.124074799=Read"},
{"title": "On the Saturation Effect of Kernel Ridge Regression", "author": "Yicheng Li and Haobo Zhang and Qian Lin", "abstract": "  The saturation effect refers to the phenomenon that the kernel ridge\nregression (KRR) fails to achieve the information theoretical lower bound when\nthe smoothness of the underground truth function exceeds certain level. The\nsaturation effect has been widely observed in practices and a saturation lower\nbound of KRR has been conjectured for decades. In this paper, we provide a\nproof of this long-standing conjecture.\n", "link": "http://arxiv.org/abs/2405.09362v1", "date": "2024-05-15", "relevancy": 1.5757, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4058}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3874}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Saturation%20Effect%20of%20Kernel%20Ridge%20Regression&body=Title%3A%20On%20the%20Saturation%20Effect%20of%20Kernel%20Ridge%20Regression%0AAuthor%3A%20Yicheng%20Li%20and%20Haobo%20Zhang%20and%20Qian%20Lin%0AAbstract%3A%20%20%20The%20saturation%20effect%20refers%20to%20the%20phenomenon%20that%20the%20kernel%20ridge%0Aregression%20%28KRR%29%20fails%20to%20achieve%20the%20information%20theoretical%20lower%20bound%20when%0Athe%20smoothness%20of%20the%20underground%20truth%20function%20exceeds%20certain%20level.%20The%0Asaturation%20effect%20has%20been%20widely%20observed%20in%20practices%20and%20a%20saturation%20lower%0Abound%20of%20KRR%20has%20been%20conjectured%20for%20decades.%20In%20this%20paper%2C%20we%20provide%20a%0Aproof%20of%20this%20long-standing%20conjecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Saturation%2520Effect%2520of%2520Kernel%2520Ridge%2520Regression%26entry.906535625%3DYicheng%2520Li%2520and%2520Haobo%2520Zhang%2520and%2520Qian%2520Lin%26entry.1292438233%3D%2520%2520The%2520saturation%2520effect%2520refers%2520to%2520the%2520phenomenon%2520that%2520the%2520kernel%2520ridge%250Aregression%2520%2528KRR%2529%2520fails%2520to%2520achieve%2520the%2520information%2520theoretical%2520lower%2520bound%2520when%250Athe%2520smoothness%2520of%2520the%2520underground%2520truth%2520function%2520exceeds%2520certain%2520level.%2520The%250Asaturation%2520effect%2520has%2520been%2520widely%2520observed%2520in%2520practices%2520and%2520a%2520saturation%2520lower%250Abound%2520of%2520KRR%2520has%2520been%2520conjectured%2520for%2520decades.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%250Aproof%2520of%2520this%2520long-standing%2520conjecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Saturation%20Effect%20of%20Kernel%20Ridge%20Regression&entry.906535625=Yicheng%20Li%20and%20Haobo%20Zhang%20and%20Qian%20Lin&entry.1292438233=%20%20The%20saturation%20effect%20refers%20to%20the%20phenomenon%20that%20the%20kernel%20ridge%0Aregression%20%28KRR%29%20fails%20to%20achieve%20the%20information%20theoretical%20lower%20bound%20when%0Athe%20smoothness%20of%20the%20underground%20truth%20function%20exceeds%20certain%20level.%20The%0Asaturation%20effect%20has%20been%20widely%20observed%20in%20practices%20and%20a%20saturation%20lower%0Abound%20of%20KRR%20has%20been%20conjectured%20for%20decades.%20In%20this%20paper%2C%20we%20provide%20a%0Aproof%20of%20this%20long-standing%20conjecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09362v1&entry.124074799=Read"},
{"title": "Learning Reward for Robot Skills Using Large Language Models via\n  Self-Alignment", "author": "Yuwei Zeng and Yao Mu and Lin Shao", "abstract": "  Learning reward functions remains the bottleneck to equip a robot with a\nbroad repertoire of skills. Large Language Models (LLM) contain valuable\ntask-related knowledge that can potentially aid in the learning of reward\nfunctions. However, the proposed reward function can be imprecise, thus\nineffective which requires to be further grounded with environment information.\nWe proposed a method to learn rewards more efficiently in the absence of\nhumans. Our approach consists of two components: We first use the LLM to\npropose features and parameterization of the reward, then update the parameters\nthrough an iterative self-alignment process. In particular, the process\nminimizes the ranking inconsistency between the LLM and the learnt reward\nfunctions based on the execution feedback. The method was validated on 9 tasks\nacross 2 simulation environments. It demonstrates a consistent improvement over\ntraining efficacy and efficiency, meanwhile consuming significantly fewer GPT\ntokens compared to the alternative mutation-based method.\n", "link": "http://arxiv.org/abs/2405.07162v2", "date": "2024-05-15", "relevancy": 1.5737, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5271}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5241}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Reward%20for%20Robot%20Skills%20Using%20Large%20Language%20Models%20via%0A%20%20Self-Alignment&body=Title%3A%20Learning%20Reward%20for%20Robot%20Skills%20Using%20Large%20Language%20Models%20via%0A%20%20Self-Alignment%0AAuthor%3A%20Yuwei%20Zeng%20and%20Yao%20Mu%20and%20Lin%20Shao%0AAbstract%3A%20%20%20Learning%20reward%20functions%20remains%20the%20bottleneck%20to%20equip%20a%20robot%20with%20a%0Abroad%20repertoire%20of%20skills.%20Large%20Language%20Models%20%28LLM%29%20contain%20valuable%0Atask-related%20knowledge%20that%20can%20potentially%20aid%20in%20the%20learning%20of%20reward%0Afunctions.%20However%2C%20the%20proposed%20reward%20function%20can%20be%20imprecise%2C%20thus%0Aineffective%20which%20requires%20to%20be%20further%20grounded%20with%20environment%20information.%0AWe%20proposed%20a%20method%20to%20learn%20rewards%20more%20efficiently%20in%20the%20absence%20of%0Ahumans.%20Our%20approach%20consists%20of%20two%20components%3A%20We%20first%20use%20the%20LLM%20to%0Apropose%20features%20and%20parameterization%20of%20the%20reward%2C%20then%20update%20the%20parameters%0Athrough%20an%20iterative%20self-alignment%20process.%20In%20particular%2C%20the%20process%0Aminimizes%20the%20ranking%20inconsistency%20between%20the%20LLM%20and%20the%20learnt%20reward%0Afunctions%20based%20on%20the%20execution%20feedback.%20The%20method%20was%20validated%20on%209%20tasks%0Aacross%202%20simulation%20environments.%20It%20demonstrates%20a%20consistent%20improvement%20over%0Atraining%20efficacy%20and%20efficiency%2C%20meanwhile%20consuming%20significantly%20fewer%20GPT%0Atokens%20compared%20to%20the%20alternative%20mutation-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Reward%2520for%2520Robot%2520Skills%2520Using%2520Large%2520Language%2520Models%2520via%250A%2520%2520Self-Alignment%26entry.906535625%3DYuwei%2520Zeng%2520and%2520Yao%2520Mu%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520Learning%2520reward%2520functions%2520remains%2520the%2520bottleneck%2520to%2520equip%2520a%2520robot%2520with%2520a%250Abroad%2520repertoire%2520of%2520skills.%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520contain%2520valuable%250Atask-related%2520knowledge%2520that%2520can%2520potentially%2520aid%2520in%2520the%2520learning%2520of%2520reward%250Afunctions.%2520However%252C%2520the%2520proposed%2520reward%2520function%2520can%2520be%2520imprecise%252C%2520thus%250Aineffective%2520which%2520requires%2520to%2520be%2520further%2520grounded%2520with%2520environment%2520information.%250AWe%2520proposed%2520a%2520method%2520to%2520learn%2520rewards%2520more%2520efficiently%2520in%2520the%2520absence%2520of%250Ahumans.%2520Our%2520approach%2520consists%2520of%2520two%2520components%253A%2520We%2520first%2520use%2520the%2520LLM%2520to%250Apropose%2520features%2520and%2520parameterization%2520of%2520the%2520reward%252C%2520then%2520update%2520the%2520parameters%250Athrough%2520an%2520iterative%2520self-alignment%2520process.%2520In%2520particular%252C%2520the%2520process%250Aminimizes%2520the%2520ranking%2520inconsistency%2520between%2520the%2520LLM%2520and%2520the%2520learnt%2520reward%250Afunctions%2520based%2520on%2520the%2520execution%2520feedback.%2520The%2520method%2520was%2520validated%2520on%25209%2520tasks%250Aacross%25202%2520simulation%2520environments.%2520It%2520demonstrates%2520a%2520consistent%2520improvement%2520over%250Atraining%2520efficacy%2520and%2520efficiency%252C%2520meanwhile%2520consuming%2520significantly%2520fewer%2520GPT%250Atokens%2520compared%2520to%2520the%2520alternative%2520mutation-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Reward%20for%20Robot%20Skills%20Using%20Large%20Language%20Models%20via%0A%20%20Self-Alignment&entry.906535625=Yuwei%20Zeng%20and%20Yao%20Mu%20and%20Lin%20Shao&entry.1292438233=%20%20Learning%20reward%20functions%20remains%20the%20bottleneck%20to%20equip%20a%20robot%20with%20a%0Abroad%20repertoire%20of%20skills.%20Large%20Language%20Models%20%28LLM%29%20contain%20valuable%0Atask-related%20knowledge%20that%20can%20potentially%20aid%20in%20the%20learning%20of%20reward%0Afunctions.%20However%2C%20the%20proposed%20reward%20function%20can%20be%20imprecise%2C%20thus%0Aineffective%20which%20requires%20to%20be%20further%20grounded%20with%20environment%20information.%0AWe%20proposed%20a%20method%20to%20learn%20rewards%20more%20efficiently%20in%20the%20absence%20of%0Ahumans.%20Our%20approach%20consists%20of%20two%20components%3A%20We%20first%20use%20the%20LLM%20to%0Apropose%20features%20and%20parameterization%20of%20the%20reward%2C%20then%20update%20the%20parameters%0Athrough%20an%20iterative%20self-alignment%20process.%20In%20particular%2C%20the%20process%0Aminimizes%20the%20ranking%20inconsistency%20between%20the%20LLM%20and%20the%20learnt%20reward%0Afunctions%20based%20on%20the%20execution%20feedback.%20The%20method%20was%20validated%20on%209%20tasks%0Aacross%202%20simulation%20environments.%20It%20demonstrates%20a%20consistent%20improvement%20over%0Atraining%20efficacy%20and%20efficiency%2C%20meanwhile%20consuming%20significantly%20fewer%20GPT%0Atokens%20compared%20to%20the%20alternative%20mutation-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07162v2&entry.124074799=Read"},
{"title": "Wild Berry image dataset collected in Finnish forests and peatlands\n  using drones", "author": "Luigi Riz and Sergio Povoli and Andrea Caraffa and Davide Boscaini and Mohamed Lamine Mekhalfi and Paul Chippendale and Marjut Turtiainen and Birgitta Partanen and Laura Smith Ballester and Francisco Blanes Noguera and Alessio Franchi and Elisa Castelli and Giacomo Piccinini and Luca Marchesotti and Micael Santos Couceiro and Fabio Poiesi", "abstract": "  Berry picking has long-standing traditions in Finland, yet it is challenging\nand can potentially be dangerous. The integration of drones equipped with\nadvanced imaging techniques represents a transformative leap forward,\noptimising harvests and promising sustainable practices. We propose WildBe, the\nfirst image dataset of wild berries captured in peatlands and under the canopy\nof Finnish forests using drones. Unlike previous and related datasets, WildBe\nincludes new varieties of berries, such as bilberries, cloudberries,\nlingonberries, and crowberries, captured under severe light variations and in\ncluttered environments. WildBe features 3,516 images, including a total of\n18,468 annotated bounding boxes. We carry out a comprehensive analysis of\nWildBe using six popular object detectors, assessing their effectiveness in\nberry detection across different forest regions and camera types. We will\nrelease WildBe publicly.\n", "link": "http://arxiv.org/abs/2405.07550v2", "date": "2024-05-15", "relevancy": 1.5713, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wild%20Berry%20image%20dataset%20collected%20in%20Finnish%20forests%20and%20peatlands%0A%20%20using%20drones&body=Title%3A%20Wild%20Berry%20image%20dataset%20collected%20in%20Finnish%20forests%20and%20peatlands%0A%20%20using%20drones%0AAuthor%3A%20Luigi%20Riz%20and%20Sergio%20Povoli%20and%20Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Mohamed%20Lamine%20Mekhalfi%20and%20Paul%20Chippendale%20and%20Marjut%20Turtiainen%20and%20Birgitta%20Partanen%20and%20Laura%20Smith%20Ballester%20and%20Francisco%20Blanes%20Noguera%20and%20Alessio%20Franchi%20and%20Elisa%20Castelli%20and%20Giacomo%20Piccinini%20and%20Luca%20Marchesotti%20and%20Micael%20Santos%20Couceiro%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20Berry%20picking%20has%20long-standing%20traditions%20in%20Finland%2C%20yet%20it%20is%20challenging%0Aand%20can%20potentially%20be%20dangerous.%20The%20integration%20of%20drones%20equipped%20with%0Aadvanced%20imaging%20techniques%20represents%20a%20transformative%20leap%20forward%2C%0Aoptimising%20harvests%20and%20promising%20sustainable%20practices.%20We%20propose%20WildBe%2C%20the%0Afirst%20image%20dataset%20of%20wild%20berries%20captured%20in%20peatlands%20and%20under%20the%20canopy%0Aof%20Finnish%20forests%20using%20drones.%20Unlike%20previous%20and%20related%20datasets%2C%20WildBe%0Aincludes%20new%20varieties%20of%20berries%2C%20such%20as%20bilberries%2C%20cloudberries%2C%0Alingonberries%2C%20and%20crowberries%2C%20captured%20under%20severe%20light%20variations%20and%20in%0Acluttered%20environments.%20WildBe%20features%203%2C516%20images%2C%20including%20a%20total%20of%0A18%2C468%20annotated%20bounding%20boxes.%20We%20carry%20out%20a%20comprehensive%20analysis%20of%0AWildBe%20using%20six%20popular%20object%20detectors%2C%20assessing%20their%20effectiveness%20in%0Aberry%20detection%20across%20different%20forest%20regions%20and%20camera%20types.%20We%20will%0Arelease%20WildBe%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWild%2520Berry%2520image%2520dataset%2520collected%2520in%2520Finnish%2520forests%2520and%2520peatlands%250A%2520%2520using%2520drones%26entry.906535625%3DLuigi%2520Riz%2520and%2520Sergio%2520Povoli%2520and%2520Andrea%2520Caraffa%2520and%2520Davide%2520Boscaini%2520and%2520Mohamed%2520Lamine%2520Mekhalfi%2520and%2520Paul%2520Chippendale%2520and%2520Marjut%2520Turtiainen%2520and%2520Birgitta%2520Partanen%2520and%2520Laura%2520Smith%2520Ballester%2520and%2520Francisco%2520Blanes%2520Noguera%2520and%2520Alessio%2520Franchi%2520and%2520Elisa%2520Castelli%2520and%2520Giacomo%2520Piccinini%2520and%2520Luca%2520Marchesotti%2520and%2520Micael%2520Santos%2520Couceiro%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520Berry%2520picking%2520has%2520long-standing%2520traditions%2520in%2520Finland%252C%2520yet%2520it%2520is%2520challenging%250Aand%2520can%2520potentially%2520be%2520dangerous.%2520The%2520integration%2520of%2520drones%2520equipped%2520with%250Aadvanced%2520imaging%2520techniques%2520represents%2520a%2520transformative%2520leap%2520forward%252C%250Aoptimising%2520harvests%2520and%2520promising%2520sustainable%2520practices.%2520We%2520propose%2520WildBe%252C%2520the%250Afirst%2520image%2520dataset%2520of%2520wild%2520berries%2520captured%2520in%2520peatlands%2520and%2520under%2520the%2520canopy%250Aof%2520Finnish%2520forests%2520using%2520drones.%2520Unlike%2520previous%2520and%2520related%2520datasets%252C%2520WildBe%250Aincludes%2520new%2520varieties%2520of%2520berries%252C%2520such%2520as%2520bilberries%252C%2520cloudberries%252C%250Alingonberries%252C%2520and%2520crowberries%252C%2520captured%2520under%2520severe%2520light%2520variations%2520and%2520in%250Acluttered%2520environments.%2520WildBe%2520features%25203%252C516%2520images%252C%2520including%2520a%2520total%2520of%250A18%252C468%2520annotated%2520bounding%2520boxes.%2520We%2520carry%2520out%2520a%2520comprehensive%2520analysis%2520of%250AWildBe%2520using%2520six%2520popular%2520object%2520detectors%252C%2520assessing%2520their%2520effectiveness%2520in%250Aberry%2520detection%2520across%2520different%2520forest%2520regions%2520and%2520camera%2520types.%2520We%2520will%250Arelease%2520WildBe%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wild%20Berry%20image%20dataset%20collected%20in%20Finnish%20forests%20and%20peatlands%0A%20%20using%20drones&entry.906535625=Luigi%20Riz%20and%20Sergio%20Povoli%20and%20Andrea%20Caraffa%20and%20Davide%20Boscaini%20and%20Mohamed%20Lamine%20Mekhalfi%20and%20Paul%20Chippendale%20and%20Marjut%20Turtiainen%20and%20Birgitta%20Partanen%20and%20Laura%20Smith%20Ballester%20and%20Francisco%20Blanes%20Noguera%20and%20Alessio%20Franchi%20and%20Elisa%20Castelli%20and%20Giacomo%20Piccinini%20and%20Luca%20Marchesotti%20and%20Micael%20Santos%20Couceiro%20and%20Fabio%20Poiesi&entry.1292438233=%20%20Berry%20picking%20has%20long-standing%20traditions%20in%20Finland%2C%20yet%20it%20is%20challenging%0Aand%20can%20potentially%20be%20dangerous.%20The%20integration%20of%20drones%20equipped%20with%0Aadvanced%20imaging%20techniques%20represents%20a%20transformative%20leap%20forward%2C%0Aoptimising%20harvests%20and%20promising%20sustainable%20practices.%20We%20propose%20WildBe%2C%20the%0Afirst%20image%20dataset%20of%20wild%20berries%20captured%20in%20peatlands%20and%20under%20the%20canopy%0Aof%20Finnish%20forests%20using%20drones.%20Unlike%20previous%20and%20related%20datasets%2C%20WildBe%0Aincludes%20new%20varieties%20of%20berries%2C%20such%20as%20bilberries%2C%20cloudberries%2C%0Alingonberries%2C%20and%20crowberries%2C%20captured%20under%20severe%20light%20variations%20and%20in%0Acluttered%20environments.%20WildBe%20features%203%2C516%20images%2C%20including%20a%20total%20of%0A18%2C468%20annotated%20bounding%20boxes.%20We%20carry%20out%20a%20comprehensive%20analysis%20of%0AWildBe%20using%20six%20popular%20object%20detectors%2C%20assessing%20their%20effectiveness%20in%0Aberry%20detection%20across%20different%20forest%20regions%20and%20camera%20types.%20We%20will%0Arelease%20WildBe%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07550v2&entry.124074799=Read"},
{"title": "Deep Blur Multi-Model (DeepBlurMM) -- a strategy to mitigate the impact\n  of image blur on deep learning model performance in histopathology image\n  analysis", "author": "Yujie Xiang and Bojing Liu and Mattias Rantalainen", "abstract": "  AI-based analysis of histopathology whole slide images (WSIs) is central in\ncomputational pathology. However, image quality can impact model performance.\nHere, we investigate to what extent unsharp areas of WSIs impact deep\nconvolutional neural network classification performance. We propose a\nmulti-model approach, i.e. DeepBlurMM, to alleviate the impact of unsharp image\nareas and improve the model performance. DeepBlurMM uses the sigma cut-offs to\ndetermine the most suitable model for predicting tiles with various levels of\nblurring within a single WSI, where sigma is the standard deviation of the\nGaussian distribution. Specifically, the cut-offs categorise the tiles into\nsharp or slight blur, moderate blur, and high blur. Each blur level has a\ncorresponding model to be selected for tile-level predictions. Throughout the\nsimulation study, we demonstrated the application of DeepBlurMM in a binary\nclassification task for breast cancer Nottingham Histological Grade 1 vs 3.\nPerformance, evaluated over 5-fold cross-validation, showed that DeepBlurMM\noutperformed the base model under moderate blur and mixed blur conditions.\nUnsharp image tiles (local blurriness) at prediction time reduced model\nperformance. The proposed multi-model approach improved performance under some\nconditions, with the potential to improve quality in both research and clinical\napplications.\n", "link": "http://arxiv.org/abs/2405.09298v1", "date": "2024-05-15", "relevancy": 1.5533, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.562}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Blur%20Multi-Model%20%28DeepBlurMM%29%20--%20a%20strategy%20to%20mitigate%20the%20impact%0A%20%20of%20image%20blur%20on%20deep%20learning%20model%20performance%20in%20histopathology%20image%0A%20%20analysis&body=Title%3A%20Deep%20Blur%20Multi-Model%20%28DeepBlurMM%29%20--%20a%20strategy%20to%20mitigate%20the%20impact%0A%20%20of%20image%20blur%20on%20deep%20learning%20model%20performance%20in%20histopathology%20image%0A%20%20analysis%0AAuthor%3A%20Yujie%20Xiang%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen%0AAbstract%3A%20%20%20AI-based%20analysis%20of%20histopathology%20whole%20slide%20images%20%28WSIs%29%20is%20central%20in%0Acomputational%20pathology.%20However%2C%20image%20quality%20can%20impact%20model%20performance.%0AHere%2C%20we%20investigate%20to%20what%20extent%20unsharp%20areas%20of%20WSIs%20impact%20deep%0Aconvolutional%20neural%20network%20classification%20performance.%20We%20propose%20a%0Amulti-model%20approach%2C%20i.e.%20DeepBlurMM%2C%20to%20alleviate%20the%20impact%20of%20unsharp%20image%0Aareas%20and%20improve%20the%20model%20performance.%20DeepBlurMM%20uses%20the%20sigma%20cut-offs%20to%0Adetermine%20the%20most%20suitable%20model%20for%20predicting%20tiles%20with%20various%20levels%20of%0Ablurring%20within%20a%20single%20WSI%2C%20where%20sigma%20is%20the%20standard%20deviation%20of%20the%0AGaussian%20distribution.%20Specifically%2C%20the%20cut-offs%20categorise%20the%20tiles%20into%0Asharp%20or%20slight%20blur%2C%20moderate%20blur%2C%20and%20high%20blur.%20Each%20blur%20level%20has%20a%0Acorresponding%20model%20to%20be%20selected%20for%20tile-level%20predictions.%20Throughout%20the%0Asimulation%20study%2C%20we%20demonstrated%20the%20application%20of%20DeepBlurMM%20in%20a%20binary%0Aclassification%20task%20for%20breast%20cancer%20Nottingham%20Histological%20Grade%201%20vs%203.%0APerformance%2C%20evaluated%20over%205-fold%20cross-validation%2C%20showed%20that%20DeepBlurMM%0Aoutperformed%20the%20base%20model%20under%20moderate%20blur%20and%20mixed%20blur%20conditions.%0AUnsharp%20image%20tiles%20%28local%20blurriness%29%20at%20prediction%20time%20reduced%20model%0Aperformance.%20The%20proposed%20multi-model%20approach%20improved%20performance%20under%20some%0Aconditions%2C%20with%20the%20potential%20to%20improve%20quality%20in%20both%20research%20and%20clinical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Blur%2520Multi-Model%2520%2528DeepBlurMM%2529%2520--%2520a%2520strategy%2520to%2520mitigate%2520the%2520impact%250A%2520%2520of%2520image%2520blur%2520on%2520deep%2520learning%2520model%2520performance%2520in%2520histopathology%2520image%250A%2520%2520analysis%26entry.906535625%3DYujie%2520Xiang%2520and%2520Bojing%2520Liu%2520and%2520Mattias%2520Rantalainen%26entry.1292438233%3D%2520%2520AI-based%2520analysis%2520of%2520histopathology%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520is%2520central%2520in%250Acomputational%2520pathology.%2520However%252C%2520image%2520quality%2520can%2520impact%2520model%2520performance.%250AHere%252C%2520we%2520investigate%2520to%2520what%2520extent%2520unsharp%2520areas%2520of%2520WSIs%2520impact%2520deep%250Aconvolutional%2520neural%2520network%2520classification%2520performance.%2520We%2520propose%2520a%250Amulti-model%2520approach%252C%2520i.e.%2520DeepBlurMM%252C%2520to%2520alleviate%2520the%2520impact%2520of%2520unsharp%2520image%250Aareas%2520and%2520improve%2520the%2520model%2520performance.%2520DeepBlurMM%2520uses%2520the%2520sigma%2520cut-offs%2520to%250Adetermine%2520the%2520most%2520suitable%2520model%2520for%2520predicting%2520tiles%2520with%2520various%2520levels%2520of%250Ablurring%2520within%2520a%2520single%2520WSI%252C%2520where%2520sigma%2520is%2520the%2520standard%2520deviation%2520of%2520the%250AGaussian%2520distribution.%2520Specifically%252C%2520the%2520cut-offs%2520categorise%2520the%2520tiles%2520into%250Asharp%2520or%2520slight%2520blur%252C%2520moderate%2520blur%252C%2520and%2520high%2520blur.%2520Each%2520blur%2520level%2520has%2520a%250Acorresponding%2520model%2520to%2520be%2520selected%2520for%2520tile-level%2520predictions.%2520Throughout%2520the%250Asimulation%2520study%252C%2520we%2520demonstrated%2520the%2520application%2520of%2520DeepBlurMM%2520in%2520a%2520binary%250Aclassification%2520task%2520for%2520breast%2520cancer%2520Nottingham%2520Histological%2520Grade%25201%2520vs%25203.%250APerformance%252C%2520evaluated%2520over%25205-fold%2520cross-validation%252C%2520showed%2520that%2520DeepBlurMM%250Aoutperformed%2520the%2520base%2520model%2520under%2520moderate%2520blur%2520and%2520mixed%2520blur%2520conditions.%250AUnsharp%2520image%2520tiles%2520%2528local%2520blurriness%2529%2520at%2520prediction%2520time%2520reduced%2520model%250Aperformance.%2520The%2520proposed%2520multi-model%2520approach%2520improved%2520performance%2520under%2520some%250Aconditions%252C%2520with%2520the%2520potential%2520to%2520improve%2520quality%2520in%2520both%2520research%2520and%2520clinical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Blur%20Multi-Model%20%28DeepBlurMM%29%20--%20a%20strategy%20to%20mitigate%20the%20impact%0A%20%20of%20image%20blur%20on%20deep%20learning%20model%20performance%20in%20histopathology%20image%0A%20%20analysis&entry.906535625=Yujie%20Xiang%20and%20Bojing%20Liu%20and%20Mattias%20Rantalainen&entry.1292438233=%20%20AI-based%20analysis%20of%20histopathology%20whole%20slide%20images%20%28WSIs%29%20is%20central%20in%0Acomputational%20pathology.%20However%2C%20image%20quality%20can%20impact%20model%20performance.%0AHere%2C%20we%20investigate%20to%20what%20extent%20unsharp%20areas%20of%20WSIs%20impact%20deep%0Aconvolutional%20neural%20network%20classification%20performance.%20We%20propose%20a%0Amulti-model%20approach%2C%20i.e.%20DeepBlurMM%2C%20to%20alleviate%20the%20impact%20of%20unsharp%20image%0Aareas%20and%20improve%20the%20model%20performance.%20DeepBlurMM%20uses%20the%20sigma%20cut-offs%20to%0Adetermine%20the%20most%20suitable%20model%20for%20predicting%20tiles%20with%20various%20levels%20of%0Ablurring%20within%20a%20single%20WSI%2C%20where%20sigma%20is%20the%20standard%20deviation%20of%20the%0AGaussian%20distribution.%20Specifically%2C%20the%20cut-offs%20categorise%20the%20tiles%20into%0Asharp%20or%20slight%20blur%2C%20moderate%20blur%2C%20and%20high%20blur.%20Each%20blur%20level%20has%20a%0Acorresponding%20model%20to%20be%20selected%20for%20tile-level%20predictions.%20Throughout%20the%0Asimulation%20study%2C%20we%20demonstrated%20the%20application%20of%20DeepBlurMM%20in%20a%20binary%0Aclassification%20task%20for%20breast%20cancer%20Nottingham%20Histological%20Grade%201%20vs%203.%0APerformance%2C%20evaluated%20over%205-fold%20cross-validation%2C%20showed%20that%20DeepBlurMM%0Aoutperformed%20the%20base%20model%20under%20moderate%20blur%20and%20mixed%20blur%20conditions.%0AUnsharp%20image%20tiles%20%28local%20blurriness%29%20at%20prediction%20time%20reduced%20model%0Aperformance.%20The%20proposed%20multi-model%20approach%20improved%20performance%20under%20some%0Aconditions%2C%20with%20the%20potential%20to%20improve%20quality%20in%20both%20research%20and%20clinical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09298v1&entry.124074799=Read"},
{"title": "DemOpts: Fairness corrections in COVID-19 case prediction models", "author": "Naman Awasthi and Saad Abrar and Daniel Smolyak and Vanessa Frias-Martinez", "abstract": "  COVID-19 forecasting models have been used to inform decision making around\nresource allocation and intervention decisions e.g., hospital beds or\nstay-at-home orders. State of the art deep learning models often use multimodal\ndata such as mobility or socio-demographic data to enhance COVID-19 case\nprediction models. Nevertheless, related work has revealed under-reporting bias\nin COVID-19 cases as well as sampling bias in mobility data for certain\nminority racial and ethnic groups, which could in turn affect the fairness of\nthe COVID-19 predictions along race labels. In this paper, we show that state\nof the art deep learning models output mean prediction errors that are\nsignificantly different across racial and ethnic groups; and which could, in\nturn, support unfair policy decisions. We also propose a novel de-biasing\nmethod, DemOpts, to increase the fairness of deep learning based forecasting\nmodels trained on potentially biased datasets. Our results show that DemOpts\ncan achieve better error parity that other state of the art de-biasing\napproaches, thus effectively reducing the differences in the mean error\ndistributions across more racial and ethnic groups.\n", "link": "http://arxiv.org/abs/2405.09483v1", "date": "2024-05-15", "relevancy": 1.4479, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4955}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DemOpts%3A%20Fairness%20corrections%20in%20COVID-19%20case%20prediction%20models&body=Title%3A%20DemOpts%3A%20Fairness%20corrections%20in%20COVID-19%20case%20prediction%20models%0AAuthor%3A%20Naman%20Awasthi%20and%20Saad%20Abrar%20and%20Daniel%20Smolyak%20and%20Vanessa%20Frias-Martinez%0AAbstract%3A%20%20%20COVID-19%20forecasting%20models%20have%20been%20used%20to%20inform%20decision%20making%20around%0Aresource%20allocation%20and%20intervention%20decisions%20e.g.%2C%20hospital%20beds%20or%0Astay-at-home%20orders.%20State%20of%20the%20art%20deep%20learning%20models%20often%20use%20multimodal%0Adata%20such%20as%20mobility%20or%20socio-demographic%20data%20to%20enhance%20COVID-19%20case%0Aprediction%20models.%20Nevertheless%2C%20related%20work%20has%20revealed%20under-reporting%20bias%0Ain%20COVID-19%20cases%20as%20well%20as%20sampling%20bias%20in%20mobility%20data%20for%20certain%0Aminority%20racial%20and%20ethnic%20groups%2C%20which%20could%20in%20turn%20affect%20the%20fairness%20of%0Athe%20COVID-19%20predictions%20along%20race%20labels.%20In%20this%20paper%2C%20we%20show%20that%20state%0Aof%20the%20art%20deep%20learning%20models%20output%20mean%20prediction%20errors%20that%20are%0Asignificantly%20different%20across%20racial%20and%20ethnic%20groups%3B%20and%20which%20could%2C%20in%0Aturn%2C%20support%20unfair%20policy%20decisions.%20We%20also%20propose%20a%20novel%20de-biasing%0Amethod%2C%20DemOpts%2C%20to%20increase%20the%20fairness%20of%20deep%20learning%20based%20forecasting%0Amodels%20trained%20on%20potentially%20biased%20datasets.%20Our%20results%20show%20that%20DemOpts%0Acan%20achieve%20better%20error%20parity%20that%20other%20state%20of%20the%20art%20de-biasing%0Aapproaches%2C%20thus%20effectively%20reducing%20the%20differences%20in%20the%20mean%20error%0Adistributions%20across%20more%20racial%20and%20ethnic%20groups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemOpts%253A%2520Fairness%2520corrections%2520in%2520COVID-19%2520case%2520prediction%2520models%26entry.906535625%3DNaman%2520Awasthi%2520and%2520Saad%2520Abrar%2520and%2520Daniel%2520Smolyak%2520and%2520Vanessa%2520Frias-Martinez%26entry.1292438233%3D%2520%2520COVID-19%2520forecasting%2520models%2520have%2520been%2520used%2520to%2520inform%2520decision%2520making%2520around%250Aresource%2520allocation%2520and%2520intervention%2520decisions%2520e.g.%252C%2520hospital%2520beds%2520or%250Astay-at-home%2520orders.%2520State%2520of%2520the%2520art%2520deep%2520learning%2520models%2520often%2520use%2520multimodal%250Adata%2520such%2520as%2520mobility%2520or%2520socio-demographic%2520data%2520to%2520enhance%2520COVID-19%2520case%250Aprediction%2520models.%2520Nevertheless%252C%2520related%2520work%2520has%2520revealed%2520under-reporting%2520bias%250Ain%2520COVID-19%2520cases%2520as%2520well%2520as%2520sampling%2520bias%2520in%2520mobility%2520data%2520for%2520certain%250Aminority%2520racial%2520and%2520ethnic%2520groups%252C%2520which%2520could%2520in%2520turn%2520affect%2520the%2520fairness%2520of%250Athe%2520COVID-19%2520predictions%2520along%2520race%2520labels.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520state%250Aof%2520the%2520art%2520deep%2520learning%2520models%2520output%2520mean%2520prediction%2520errors%2520that%2520are%250Asignificantly%2520different%2520across%2520racial%2520and%2520ethnic%2520groups%253B%2520and%2520which%2520could%252C%2520in%250Aturn%252C%2520support%2520unfair%2520policy%2520decisions.%2520We%2520also%2520propose%2520a%2520novel%2520de-biasing%250Amethod%252C%2520DemOpts%252C%2520to%2520increase%2520the%2520fairness%2520of%2520deep%2520learning%2520based%2520forecasting%250Amodels%2520trained%2520on%2520potentially%2520biased%2520datasets.%2520Our%2520results%2520show%2520that%2520DemOpts%250Acan%2520achieve%2520better%2520error%2520parity%2520that%2520other%2520state%2520of%2520the%2520art%2520de-biasing%250Aapproaches%252C%2520thus%2520effectively%2520reducing%2520the%2520differences%2520in%2520the%2520mean%2520error%250Adistributions%2520across%2520more%2520racial%2520and%2520ethnic%2520groups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DemOpts%3A%20Fairness%20corrections%20in%20COVID-19%20case%20prediction%20models&entry.906535625=Naman%20Awasthi%20and%20Saad%20Abrar%20and%20Daniel%20Smolyak%20and%20Vanessa%20Frias-Martinez&entry.1292438233=%20%20COVID-19%20forecasting%20models%20have%20been%20used%20to%20inform%20decision%20making%20around%0Aresource%20allocation%20and%20intervention%20decisions%20e.g.%2C%20hospital%20beds%20or%0Astay-at-home%20orders.%20State%20of%20the%20art%20deep%20learning%20models%20often%20use%20multimodal%0Adata%20such%20as%20mobility%20or%20socio-demographic%20data%20to%20enhance%20COVID-19%20case%0Aprediction%20models.%20Nevertheless%2C%20related%20work%20has%20revealed%20under-reporting%20bias%0Ain%20COVID-19%20cases%20as%20well%20as%20sampling%20bias%20in%20mobility%20data%20for%20certain%0Aminority%20racial%20and%20ethnic%20groups%2C%20which%20could%20in%20turn%20affect%20the%20fairness%20of%0Athe%20COVID-19%20predictions%20along%20race%20labels.%20In%20this%20paper%2C%20we%20show%20that%20state%0Aof%20the%20art%20deep%20learning%20models%20output%20mean%20prediction%20errors%20that%20are%0Asignificantly%20different%20across%20racial%20and%20ethnic%20groups%3B%20and%20which%20could%2C%20in%0Aturn%2C%20support%20unfair%20policy%20decisions.%20We%20also%20propose%20a%20novel%20de-biasing%0Amethod%2C%20DemOpts%2C%20to%20increase%20the%20fairness%20of%20deep%20learning%20based%20forecasting%0Amodels%20trained%20on%20potentially%20biased%20datasets.%20Our%20results%20show%20that%20DemOpts%0Acan%20achieve%20better%20error%20parity%20that%20other%20state%20of%20the%20art%20de-biasing%0Aapproaches%2C%20thus%20effectively%20reducing%20the%20differences%20in%20the%20mean%20error%0Adistributions%20across%20more%20racial%20and%20ethnic%20groups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09483v1&entry.124074799=Read"},
{"title": "SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral\n  Image Denoising", "author": "Guanyiman Fu and Fengchao Xiong and Jianfeng Lu and Jun Zhou and Yuntao Qian", "abstract": "  Denoising hyperspectral images (HSIs) is a crucial preprocessing procedure\ndue to the noise originating from intra-imaging mechanisms and environmental\nfactors. Utilizing domain-specific knowledge of HSIs, such as spectral\ncorrelation, spatial self-similarity, and spatial-spectral correlation, is\nessential for deep learning-based denoising. Existing methods are often\nconstrained by running time, space complexity, and computational complexity,\nemploying strategies that explore these priors separately. While these\nstrategies can avoid some redundant information, they inevitably overlook\nbroader and more underlying long-range spatial-spectral information that\npositively impacts image restoration. This paper proposes a Spatial-Spectral\nSelective State Space Model-based U-shaped network, termed Spatial-Spectral\nU-Mamba (SSUMamba), for hyperspectral image denoising. We can obtain complete\nglobal spatial-spectral correlation within a module thanks to the linear space\ncomplexity in State Space Model (SSM) computations. We introduce a\nSpatial-Spectral Alternating Scan (SSAS) strategy for HSIs, which helps model\nthe information flow in multiple directions in 3-D HSIs. Experimental results\ndemonstrate that our method outperforms compared methods. The source code is\navailable at https://github.com/lronkitty/SSUMamba.\n", "link": "http://arxiv.org/abs/2405.01726v4", "date": "2024-05-15", "relevancy": 1.54, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5544}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5033}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSUMamba%3A%20Spatial-Spectral%20Selective%20State%20Space%20Model%20for%20Hyperspectral%0A%20%20Image%20Denoising&body=Title%3A%20SSUMamba%3A%20Spatial-Spectral%20Selective%20State%20Space%20Model%20for%20Hyperspectral%0A%20%20Image%20Denoising%0AAuthor%3A%20Guanyiman%20Fu%20and%20Fengchao%20Xiong%20and%20Jianfeng%20Lu%20and%20Jun%20Zhou%20and%20Yuntao%20Qian%0AAbstract%3A%20%20%20Denoising%20hyperspectral%20images%20%28HSIs%29%20is%20a%20crucial%20preprocessing%20procedure%0Adue%20to%20the%20noise%20originating%20from%20intra-imaging%20mechanisms%20and%20environmental%0Afactors.%20Utilizing%20domain-specific%20knowledge%20of%20HSIs%2C%20such%20as%20spectral%0Acorrelation%2C%20spatial%20self-similarity%2C%20and%20spatial-spectral%20correlation%2C%20is%0Aessential%20for%20deep%20learning-based%20denoising.%20Existing%20methods%20are%20often%0Aconstrained%20by%20running%20time%2C%20space%20complexity%2C%20and%20computational%20complexity%2C%0Aemploying%20strategies%20that%20explore%20these%20priors%20separately.%20While%20these%0Astrategies%20can%20avoid%20some%20redundant%20information%2C%20they%20inevitably%20overlook%0Abroader%20and%20more%20underlying%20long-range%20spatial-spectral%20information%20that%0Apositively%20impacts%20image%20restoration.%20This%20paper%20proposes%20a%20Spatial-Spectral%0ASelective%20State%20Space%20Model-based%20U-shaped%20network%2C%20termed%20Spatial-Spectral%0AU-Mamba%20%28SSUMamba%29%2C%20for%20hyperspectral%20image%20denoising.%20We%20can%20obtain%20complete%0Aglobal%20spatial-spectral%20correlation%20within%20a%20module%20thanks%20to%20the%20linear%20space%0Acomplexity%20in%20State%20Space%20Model%20%28SSM%29%20computations.%20We%20introduce%20a%0ASpatial-Spectral%20Alternating%20Scan%20%28SSAS%29%20strategy%20for%20HSIs%2C%20which%20helps%20model%0Athe%20information%20flow%20in%20multiple%20directions%20in%203-D%20HSIs.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20compared%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/lronkitty/SSUMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01726v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSUMamba%253A%2520Spatial-Spectral%2520Selective%2520State%2520Space%2520Model%2520for%2520Hyperspectral%250A%2520%2520Image%2520Denoising%26entry.906535625%3DGuanyiman%2520Fu%2520and%2520Fengchao%2520Xiong%2520and%2520Jianfeng%2520Lu%2520and%2520Jun%2520Zhou%2520and%2520Yuntao%2520Qian%26entry.1292438233%3D%2520%2520Denoising%2520hyperspectral%2520images%2520%2528HSIs%2529%2520is%2520a%2520crucial%2520preprocessing%2520procedure%250Adue%2520to%2520the%2520noise%2520originating%2520from%2520intra-imaging%2520mechanisms%2520and%2520environmental%250Afactors.%2520Utilizing%2520domain-specific%2520knowledge%2520of%2520HSIs%252C%2520such%2520as%2520spectral%250Acorrelation%252C%2520spatial%2520self-similarity%252C%2520and%2520spatial-spectral%2520correlation%252C%2520is%250Aessential%2520for%2520deep%2520learning-based%2520denoising.%2520Existing%2520methods%2520are%2520often%250Aconstrained%2520by%2520running%2520time%252C%2520space%2520complexity%252C%2520and%2520computational%2520complexity%252C%250Aemploying%2520strategies%2520that%2520explore%2520these%2520priors%2520separately.%2520While%2520these%250Astrategies%2520can%2520avoid%2520some%2520redundant%2520information%252C%2520they%2520inevitably%2520overlook%250Abroader%2520and%2520more%2520underlying%2520long-range%2520spatial-spectral%2520information%2520that%250Apositively%2520impacts%2520image%2520restoration.%2520This%2520paper%2520proposes%2520a%2520Spatial-Spectral%250ASelective%2520State%2520Space%2520Model-based%2520U-shaped%2520network%252C%2520termed%2520Spatial-Spectral%250AU-Mamba%2520%2528SSUMamba%2529%252C%2520for%2520hyperspectral%2520image%2520denoising.%2520We%2520can%2520obtain%2520complete%250Aglobal%2520spatial-spectral%2520correlation%2520within%2520a%2520module%2520thanks%2520to%2520the%2520linear%2520space%250Acomplexity%2520in%2520State%2520Space%2520Model%2520%2528SSM%2529%2520computations.%2520We%2520introduce%2520a%250ASpatial-Spectral%2520Alternating%2520Scan%2520%2528SSAS%2529%2520strategy%2520for%2520HSIs%252C%2520which%2520helps%2520model%250Athe%2520information%2520flow%2520in%2520multiple%2520directions%2520in%25203-D%2520HSIs.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520compared%2520methods.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/lronkitty/SSUMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01726v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSUMamba%3A%20Spatial-Spectral%20Selective%20State%20Space%20Model%20for%20Hyperspectral%0A%20%20Image%20Denoising&entry.906535625=Guanyiman%20Fu%20and%20Fengchao%20Xiong%20and%20Jianfeng%20Lu%20and%20Jun%20Zhou%20and%20Yuntao%20Qian&entry.1292438233=%20%20Denoising%20hyperspectral%20images%20%28HSIs%29%20is%20a%20crucial%20preprocessing%20procedure%0Adue%20to%20the%20noise%20originating%20from%20intra-imaging%20mechanisms%20and%20environmental%0Afactors.%20Utilizing%20domain-specific%20knowledge%20of%20HSIs%2C%20such%20as%20spectral%0Acorrelation%2C%20spatial%20self-similarity%2C%20and%20spatial-spectral%20correlation%2C%20is%0Aessential%20for%20deep%20learning-based%20denoising.%20Existing%20methods%20are%20often%0Aconstrained%20by%20running%20time%2C%20space%20complexity%2C%20and%20computational%20complexity%2C%0Aemploying%20strategies%20that%20explore%20these%20priors%20separately.%20While%20these%0Astrategies%20can%20avoid%20some%20redundant%20information%2C%20they%20inevitably%20overlook%0Abroader%20and%20more%20underlying%20long-range%20spatial-spectral%20information%20that%0Apositively%20impacts%20image%20restoration.%20This%20paper%20proposes%20a%20Spatial-Spectral%0ASelective%20State%20Space%20Model-based%20U-shaped%20network%2C%20termed%20Spatial-Spectral%0AU-Mamba%20%28SSUMamba%29%2C%20for%20hyperspectral%20image%20denoising.%20We%20can%20obtain%20complete%0Aglobal%20spatial-spectral%20correlation%20within%20a%20module%20thanks%20to%20the%20linear%20space%0Acomplexity%20in%20State%20Space%20Model%20%28SSM%29%20computations.%20We%20introduce%20a%0ASpatial-Spectral%20Alternating%20Scan%20%28SSAS%29%20strategy%20for%20HSIs%2C%20which%20helps%20model%0Athe%20information%20flow%20in%20multiple%20directions%20in%203-D%20HSIs.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20outperforms%20compared%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/lronkitty/SSUMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01726v4&entry.124074799=Read"},
{"title": "Prospects of Privacy Advantage in Quantum Machine Learning", "author": "Jamie Heredge and Niraj Kumar and Dylan Herman and Shouvanik Chakrabarti and Romina Yalovetzky and Shree Hari Sureshbabu and Changhao Li and Marco Pistoia", "abstract": "  Ensuring data privacy in machine learning models is critical, particularly in\ndistributed settings where model gradients are typically shared among multiple\nparties to allow collaborative learning. Motivated by the increasing success of\nrecovering input data from the gradients of classical models, this study\naddresses a central question: How hard is it to recover the input data from the\ngradients of quantum machine learning models? Focusing on variational quantum\ncircuits (VQC) as learning models, we uncover the crucial role played by the\ndynamical Lie algebra (DLA) of the VQC ansatz in determining privacy\nvulnerabilities. While the DLA has previously been linked to the classical\nsimulatability and trainability of VQC models, this work, for the first time,\nestablishes its connection to the privacy of VQC models. In particular, we show\nthat properties conducive to the trainability of VQCs, such as a\npolynomial-sized DLA, also facilitate the extraction of detailed snapshots of\nthe input. We term this a weak privacy breach, as the snapshots enable training\nVQC models for distinct learning tasks without direct access to the original\ninput. Further, we investigate the conditions for a strong privacy breach where\nthe original input data can be recovered from these snapshots by classical or\nquantum-assisted polynomial time methods. We establish conditions on the\nencoding map such as classical simulatability, overlap with DLA basis, and its\nFourier frequency characteristics that enable such a privacy breach of VQC\nmodels. Our findings thus play a crucial role in detailing the prospects of\nquantum privacy advantage by guiding the requirements for designing quantum\nmachine learning models that balance trainability with robust privacy\nprotection.\n", "link": "http://arxiv.org/abs/2405.08801v2", "date": "2024-05-15", "relevancy": 1.2981, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4334}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prospects%20of%20Privacy%20Advantage%20in%20Quantum%20Machine%20Learning&body=Title%3A%20Prospects%20of%20Privacy%20Advantage%20in%20Quantum%20Machine%20Learning%0AAuthor%3A%20Jamie%20Heredge%20and%20Niraj%20Kumar%20and%20Dylan%20Herman%20and%20Shouvanik%20Chakrabarti%20and%20Romina%20Yalovetzky%20and%20Shree%20Hari%20Sureshbabu%20and%20Changhao%20Li%20and%20Marco%20Pistoia%0AAbstract%3A%20%20%20Ensuring%20data%20privacy%20in%20machine%20learning%20models%20is%20critical%2C%20particularly%20in%0Adistributed%20settings%20where%20model%20gradients%20are%20typically%20shared%20among%20multiple%0Aparties%20to%20allow%20collaborative%20learning.%20Motivated%20by%20the%20increasing%20success%20of%0Arecovering%20input%20data%20from%20the%20gradients%20of%20classical%20models%2C%20this%20study%0Aaddresses%20a%20central%20question%3A%20How%20hard%20is%20it%20to%20recover%20the%20input%20data%20from%20the%0Agradients%20of%20quantum%20machine%20learning%20models%3F%20Focusing%20on%20variational%20quantum%0Acircuits%20%28VQC%29%20as%20learning%20models%2C%20we%20uncover%20the%20crucial%20role%20played%20by%20the%0Adynamical%20Lie%20algebra%20%28DLA%29%20of%20the%20VQC%20ansatz%20in%20determining%20privacy%0Avulnerabilities.%20While%20the%20DLA%20has%20previously%20been%20linked%20to%20the%20classical%0Asimulatability%20and%20trainability%20of%20VQC%20models%2C%20this%20work%2C%20for%20the%20first%20time%2C%0Aestablishes%20its%20connection%20to%20the%20privacy%20of%20VQC%20models.%20In%20particular%2C%20we%20show%0Athat%20properties%20conducive%20to%20the%20trainability%20of%20VQCs%2C%20such%20as%20a%0Apolynomial-sized%20DLA%2C%20also%20facilitate%20the%20extraction%20of%20detailed%20snapshots%20of%0Athe%20input.%20We%20term%20this%20a%20weak%20privacy%20breach%2C%20as%20the%20snapshots%20enable%20training%0AVQC%20models%20for%20distinct%20learning%20tasks%20without%20direct%20access%20to%20the%20original%0Ainput.%20Further%2C%20we%20investigate%20the%20conditions%20for%20a%20strong%20privacy%20breach%20where%0Athe%20original%20input%20data%20can%20be%20recovered%20from%20these%20snapshots%20by%20classical%20or%0Aquantum-assisted%20polynomial%20time%20methods.%20We%20establish%20conditions%20on%20the%0Aencoding%20map%20such%20as%20classical%20simulatability%2C%20overlap%20with%20DLA%20basis%2C%20and%20its%0AFourier%20frequency%20characteristics%20that%20enable%20such%20a%20privacy%20breach%20of%20VQC%0Amodels.%20Our%20findings%20thus%20play%20a%20crucial%20role%20in%20detailing%20the%20prospects%20of%0Aquantum%20privacy%20advantage%20by%20guiding%20the%20requirements%20for%20designing%20quantum%0Amachine%20learning%20models%20that%20balance%20trainability%20with%20robust%20privacy%0Aprotection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08801v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProspects%2520of%2520Privacy%2520Advantage%2520in%2520Quantum%2520Machine%2520Learning%26entry.906535625%3DJamie%2520Heredge%2520and%2520Niraj%2520Kumar%2520and%2520Dylan%2520Herman%2520and%2520Shouvanik%2520Chakrabarti%2520and%2520Romina%2520Yalovetzky%2520and%2520Shree%2520Hari%2520Sureshbabu%2520and%2520Changhao%2520Li%2520and%2520Marco%2520Pistoia%26entry.1292438233%3D%2520%2520Ensuring%2520data%2520privacy%2520in%2520machine%2520learning%2520models%2520is%2520critical%252C%2520particularly%2520in%250Adistributed%2520settings%2520where%2520model%2520gradients%2520are%2520typically%2520shared%2520among%2520multiple%250Aparties%2520to%2520allow%2520collaborative%2520learning.%2520Motivated%2520by%2520the%2520increasing%2520success%2520of%250Arecovering%2520input%2520data%2520from%2520the%2520gradients%2520of%2520classical%2520models%252C%2520this%2520study%250Aaddresses%2520a%2520central%2520question%253A%2520How%2520hard%2520is%2520it%2520to%2520recover%2520the%2520input%2520data%2520from%2520the%250Agradients%2520of%2520quantum%2520machine%2520learning%2520models%253F%2520Focusing%2520on%2520variational%2520quantum%250Acircuits%2520%2528VQC%2529%2520as%2520learning%2520models%252C%2520we%2520uncover%2520the%2520crucial%2520role%2520played%2520by%2520the%250Adynamical%2520Lie%2520algebra%2520%2528DLA%2529%2520of%2520the%2520VQC%2520ansatz%2520in%2520determining%2520privacy%250Avulnerabilities.%2520While%2520the%2520DLA%2520has%2520previously%2520been%2520linked%2520to%2520the%2520classical%250Asimulatability%2520and%2520trainability%2520of%2520VQC%2520models%252C%2520this%2520work%252C%2520for%2520the%2520first%2520time%252C%250Aestablishes%2520its%2520connection%2520to%2520the%2520privacy%2520of%2520VQC%2520models.%2520In%2520particular%252C%2520we%2520show%250Athat%2520properties%2520conducive%2520to%2520the%2520trainability%2520of%2520VQCs%252C%2520such%2520as%2520a%250Apolynomial-sized%2520DLA%252C%2520also%2520facilitate%2520the%2520extraction%2520of%2520detailed%2520snapshots%2520of%250Athe%2520input.%2520We%2520term%2520this%2520a%2520weak%2520privacy%2520breach%252C%2520as%2520the%2520snapshots%2520enable%2520training%250AVQC%2520models%2520for%2520distinct%2520learning%2520tasks%2520without%2520direct%2520access%2520to%2520the%2520original%250Ainput.%2520Further%252C%2520we%2520investigate%2520the%2520conditions%2520for%2520a%2520strong%2520privacy%2520breach%2520where%250Athe%2520original%2520input%2520data%2520can%2520be%2520recovered%2520from%2520these%2520snapshots%2520by%2520classical%2520or%250Aquantum-assisted%2520polynomial%2520time%2520methods.%2520We%2520establish%2520conditions%2520on%2520the%250Aencoding%2520map%2520such%2520as%2520classical%2520simulatability%252C%2520overlap%2520with%2520DLA%2520basis%252C%2520and%2520its%250AFourier%2520frequency%2520characteristics%2520that%2520enable%2520such%2520a%2520privacy%2520breach%2520of%2520VQC%250Amodels.%2520Our%2520findings%2520thus%2520play%2520a%2520crucial%2520role%2520in%2520detailing%2520the%2520prospects%2520of%250Aquantum%2520privacy%2520advantage%2520by%2520guiding%2520the%2520requirements%2520for%2520designing%2520quantum%250Amachine%2520learning%2520models%2520that%2520balance%2520trainability%2520with%2520robust%2520privacy%250Aprotection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08801v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prospects%20of%20Privacy%20Advantage%20in%20Quantum%20Machine%20Learning&entry.906535625=Jamie%20Heredge%20and%20Niraj%20Kumar%20and%20Dylan%20Herman%20and%20Shouvanik%20Chakrabarti%20and%20Romina%20Yalovetzky%20and%20Shree%20Hari%20Sureshbabu%20and%20Changhao%20Li%20and%20Marco%20Pistoia&entry.1292438233=%20%20Ensuring%20data%20privacy%20in%20machine%20learning%20models%20is%20critical%2C%20particularly%20in%0Adistributed%20settings%20where%20model%20gradients%20are%20typically%20shared%20among%20multiple%0Aparties%20to%20allow%20collaborative%20learning.%20Motivated%20by%20the%20increasing%20success%20of%0Arecovering%20input%20data%20from%20the%20gradients%20of%20classical%20models%2C%20this%20study%0Aaddresses%20a%20central%20question%3A%20How%20hard%20is%20it%20to%20recover%20the%20input%20data%20from%20the%0Agradients%20of%20quantum%20machine%20learning%20models%3F%20Focusing%20on%20variational%20quantum%0Acircuits%20%28VQC%29%20as%20learning%20models%2C%20we%20uncover%20the%20crucial%20role%20played%20by%20the%0Adynamical%20Lie%20algebra%20%28DLA%29%20of%20the%20VQC%20ansatz%20in%20determining%20privacy%0Avulnerabilities.%20While%20the%20DLA%20has%20previously%20been%20linked%20to%20the%20classical%0Asimulatability%20and%20trainability%20of%20VQC%20models%2C%20this%20work%2C%20for%20the%20first%20time%2C%0Aestablishes%20its%20connection%20to%20the%20privacy%20of%20VQC%20models.%20In%20particular%2C%20we%20show%0Athat%20properties%20conducive%20to%20the%20trainability%20of%20VQCs%2C%20such%20as%20a%0Apolynomial-sized%20DLA%2C%20also%20facilitate%20the%20extraction%20of%20detailed%20snapshots%20of%0Athe%20input.%20We%20term%20this%20a%20weak%20privacy%20breach%2C%20as%20the%20snapshots%20enable%20training%0AVQC%20models%20for%20distinct%20learning%20tasks%20without%20direct%20access%20to%20the%20original%0Ainput.%20Further%2C%20we%20investigate%20the%20conditions%20for%20a%20strong%20privacy%20breach%20where%0Athe%20original%20input%20data%20can%20be%20recovered%20from%20these%20snapshots%20by%20classical%20or%0Aquantum-assisted%20polynomial%20time%20methods.%20We%20establish%20conditions%20on%20the%0Aencoding%20map%20such%20as%20classical%20simulatability%2C%20overlap%20with%20DLA%20basis%2C%20and%20its%0AFourier%20frequency%20characteristics%20that%20enable%20such%20a%20privacy%20breach%20of%20VQC%0Amodels.%20Our%20findings%20thus%20play%20a%20crucial%20role%20in%20detailing%20the%20prospects%20of%0Aquantum%20privacy%20advantage%20by%20guiding%20the%20requirements%20for%20designing%20quantum%0Amachine%20learning%20models%20that%20balance%20trainability%20with%20robust%20privacy%0Aprotection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08801v2&entry.124074799=Read"},
{"title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with\n  Communication Cost under 18 Kilobytes", "author": "Zhen Qin and Daoyuan Chen and Bingchen Qian and Bolin Ding and Yaliang Li and Shuiguang Deng", "abstract": "  Pre-trained large language models (LLMs) need fine-tuning to improve their\nresponsiveness to natural language instructions. Federated learning offers a\nway to fine-tune LLMs using the abundant data on end devices without\ncompromising data privacy. Most existing federated fine-tuning methods for LLMs\nrely on parameter-efficient fine-tuning techniques, which may not reach the\nperformance height possible with full-parameter tuning. However, federated\nfull-parameter tuning of LLMs is a non-trivial problem due to the immense\ncommunication cost. This work introduces FedKSeed that employs zeroth-order\noptimization with a finite set of random seeds. It significantly reduces\ntransmission requirements between the server and clients to just a few random\nseeds and scalar gradients, amounting to only a few thousand bytes, making\nfederated full-parameter tuning of billion-sized LLMs possible on devices.\nBuilding on it, we develop a strategy enabling probability-differentiated seed\nsampling, prioritizing perturbations with greater impact on model accuracy.\nExperiments across six scenarios with various LLMs, datasets and data\npartitions demonstrate that our approach outperforms existing federated LLM\nfine-tuning methods in both communication efficiency and new task\ngeneralization.\n", "link": "http://arxiv.org/abs/2312.06353v4", "date": "2024-05-15", "relevancy": 1.4656, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4951}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Full-Parameter%20Tuning%20of%20Billion-Sized%20Language%20Models%20with%0A%20%20Communication%20Cost%20under%2018%20Kilobytes&body=Title%3A%20Federated%20Full-Parameter%20Tuning%20of%20Billion-Sized%20Language%20Models%20with%0A%20%20Communication%20Cost%20under%2018%20Kilobytes%0AAuthor%3A%20Zhen%20Qin%20and%20Daoyuan%20Chen%20and%20Bingchen%20Qian%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20Pre-trained%20large%20language%20models%20%28LLMs%29%20need%20fine-tuning%20to%20improve%20their%0Aresponsiveness%20to%20natural%20language%20instructions.%20Federated%20learning%20offers%20a%0Away%20to%20fine-tune%20LLMs%20using%20the%20abundant%20data%20on%20end%20devices%20without%0Acompromising%20data%20privacy.%20Most%20existing%20federated%20fine-tuning%20methods%20for%20LLMs%0Arely%20on%20parameter-efficient%20fine-tuning%20techniques%2C%20which%20may%20not%20reach%20the%0Aperformance%20height%20possible%20with%20full-parameter%20tuning.%20However%2C%20federated%0Afull-parameter%20tuning%20of%20LLMs%20is%20a%20non-trivial%20problem%20due%20to%20the%20immense%0Acommunication%20cost.%20This%20work%20introduces%20FedKSeed%20that%20employs%20zeroth-order%0Aoptimization%20with%20a%20finite%20set%20of%20random%20seeds.%20It%20significantly%20reduces%0Atransmission%20requirements%20between%20the%20server%20and%20clients%20to%20just%20a%20few%20random%0Aseeds%20and%20scalar%20gradients%2C%20amounting%20to%20only%20a%20few%20thousand%20bytes%2C%20making%0Afederated%20full-parameter%20tuning%20of%20billion-sized%20LLMs%20possible%20on%20devices.%0ABuilding%20on%20it%2C%20we%20develop%20a%20strategy%20enabling%20probability-differentiated%20seed%0Asampling%2C%20prioritizing%20perturbations%20with%20greater%20impact%20on%20model%20accuracy.%0AExperiments%20across%20six%20scenarios%20with%20various%20LLMs%2C%20datasets%20and%20data%0Apartitions%20demonstrate%20that%20our%20approach%20outperforms%20existing%20federated%20LLM%0Afine-tuning%20methods%20in%20both%20communication%20efficiency%20and%20new%20task%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06353v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Full-Parameter%2520Tuning%2520of%2520Billion-Sized%2520Language%2520Models%2520with%250A%2520%2520Communication%2520Cost%2520under%252018%2520Kilobytes%26entry.906535625%3DZhen%2520Qin%2520and%2520Daoyuan%2520Chen%2520and%2520Bingchen%2520Qian%2520and%2520Bolin%2520Ding%2520and%2520Yaliang%2520Li%2520and%2520Shuiguang%2520Deng%26entry.1292438233%3D%2520%2520Pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520need%2520fine-tuning%2520to%2520improve%2520their%250Aresponsiveness%2520to%2520natural%2520language%2520instructions.%2520Federated%2520learning%2520offers%2520a%250Away%2520to%2520fine-tune%2520LLMs%2520using%2520the%2520abundant%2520data%2520on%2520end%2520devices%2520without%250Acompromising%2520data%2520privacy.%2520Most%2520existing%2520federated%2520fine-tuning%2520methods%2520for%2520LLMs%250Arely%2520on%2520parameter-efficient%2520fine-tuning%2520techniques%252C%2520which%2520may%2520not%2520reach%2520the%250Aperformance%2520height%2520possible%2520with%2520full-parameter%2520tuning.%2520However%252C%2520federated%250Afull-parameter%2520tuning%2520of%2520LLMs%2520is%2520a%2520non-trivial%2520problem%2520due%2520to%2520the%2520immense%250Acommunication%2520cost.%2520This%2520work%2520introduces%2520FedKSeed%2520that%2520employs%2520zeroth-order%250Aoptimization%2520with%2520a%2520finite%2520set%2520of%2520random%2520seeds.%2520It%2520significantly%2520reduces%250Atransmission%2520requirements%2520between%2520the%2520server%2520and%2520clients%2520to%2520just%2520a%2520few%2520random%250Aseeds%2520and%2520scalar%2520gradients%252C%2520amounting%2520to%2520only%2520a%2520few%2520thousand%2520bytes%252C%2520making%250Afederated%2520full-parameter%2520tuning%2520of%2520billion-sized%2520LLMs%2520possible%2520on%2520devices.%250ABuilding%2520on%2520it%252C%2520we%2520develop%2520a%2520strategy%2520enabling%2520probability-differentiated%2520seed%250Asampling%252C%2520prioritizing%2520perturbations%2520with%2520greater%2520impact%2520on%2520model%2520accuracy.%250AExperiments%2520across%2520six%2520scenarios%2520with%2520various%2520LLMs%252C%2520datasets%2520and%2520data%250Apartitions%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520existing%2520federated%2520LLM%250Afine-tuning%2520methods%2520in%2520both%2520communication%2520efficiency%2520and%2520new%2520task%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06353v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Full-Parameter%20Tuning%20of%20Billion-Sized%20Language%20Models%20with%0A%20%20Communication%20Cost%20under%2018%20Kilobytes&entry.906535625=Zhen%20Qin%20and%20Daoyuan%20Chen%20and%20Bingchen%20Qian%20and%20Bolin%20Ding%20and%20Yaliang%20Li%20and%20Shuiguang%20Deng&entry.1292438233=%20%20Pre-trained%20large%20language%20models%20%28LLMs%29%20need%20fine-tuning%20to%20improve%20their%0Aresponsiveness%20to%20natural%20language%20instructions.%20Federated%20learning%20offers%20a%0Away%20to%20fine-tune%20LLMs%20using%20the%20abundant%20data%20on%20end%20devices%20without%0Acompromising%20data%20privacy.%20Most%20existing%20federated%20fine-tuning%20methods%20for%20LLMs%0Arely%20on%20parameter-efficient%20fine-tuning%20techniques%2C%20which%20may%20not%20reach%20the%0Aperformance%20height%20possible%20with%20full-parameter%20tuning.%20However%2C%20federated%0Afull-parameter%20tuning%20of%20LLMs%20is%20a%20non-trivial%20problem%20due%20to%20the%20immense%0Acommunication%20cost.%20This%20work%20introduces%20FedKSeed%20that%20employs%20zeroth-order%0Aoptimization%20with%20a%20finite%20set%20of%20random%20seeds.%20It%20significantly%20reduces%0Atransmission%20requirements%20between%20the%20server%20and%20clients%20to%20just%20a%20few%20random%0Aseeds%20and%20scalar%20gradients%2C%20amounting%20to%20only%20a%20few%20thousand%20bytes%2C%20making%0Afederated%20full-parameter%20tuning%20of%20billion-sized%20LLMs%20possible%20on%20devices.%0ABuilding%20on%20it%2C%20we%20develop%20a%20strategy%20enabling%20probability-differentiated%20seed%0Asampling%2C%20prioritizing%20perturbations%20with%20greater%20impact%20on%20model%20accuracy.%0AExperiments%20across%20six%20scenarios%20with%20various%20LLMs%2C%20datasets%20and%20data%0Apartitions%20demonstrate%20that%20our%20approach%20outperforms%20existing%20federated%20LLM%0Afine-tuning%20methods%20in%20both%20communication%20efficiency%20and%20new%20task%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06353v4&entry.124074799=Read"},
{"title": "A Resource Model For Neural Scaling Law", "author": "Jinyeop Song and Ziming Liu and Max Tegmark and Jeff Gore", "abstract": "  Neural scaling laws characterize how model performance improves as the model\nsize scales up. Inspired by empirical observations, we introduce a resource\nmodel of neural scaling. A task is usually composite hence can be decomposed\ninto many subtasks, which compete for resources (measured by the number of\nneurons allocated to subtasks). On toy problems, we empirically find that: (1)\nThe loss of a subtask is inversely proportional to its allocated neurons. (2)\nWhen multiple subtasks are present in a composite task, the resources acquired\nby each subtask uniformly grow as models get larger, keeping the ratios of\nacquired resources constants. We hypothesize these findings to be generally\ntrue and build a model to predict neural scaling laws for general composite\ntasks, which successfully replicates the neural scaling law of Chinchilla\nmodels reported in arXiv:2203.15556. We believe that the notion of resource\nused in this paper will be a useful tool for characterizing and diagnosing\nneural networks.\n", "link": "http://arxiv.org/abs/2402.05164v2", "date": "2024-05-15", "relevancy": 1.3726, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5202}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Resource%20Model%20For%20Neural%20Scaling%20Law&body=Title%3A%20A%20Resource%20Model%20For%20Neural%20Scaling%20Law%0AAuthor%3A%20Jinyeop%20Song%20and%20Ziming%20Liu%20and%20Max%20Tegmark%20and%20Jeff%20Gore%0AAbstract%3A%20%20%20Neural%20scaling%20laws%20characterize%20how%20model%20performance%20improves%20as%20the%20model%0Asize%20scales%20up.%20Inspired%20by%20empirical%20observations%2C%20we%20introduce%20a%20resource%0Amodel%20of%20neural%20scaling.%20A%20task%20is%20usually%20composite%20hence%20can%20be%20decomposed%0Ainto%20many%20subtasks%2C%20which%20compete%20for%20resources%20%28measured%20by%20the%20number%20of%0Aneurons%20allocated%20to%20subtasks%29.%20On%20toy%20problems%2C%20we%20empirically%20find%20that%3A%20%281%29%0AThe%20loss%20of%20a%20subtask%20is%20inversely%20proportional%20to%20its%20allocated%20neurons.%20%282%29%0AWhen%20multiple%20subtasks%20are%20present%20in%20a%20composite%20task%2C%20the%20resources%20acquired%0Aby%20each%20subtask%20uniformly%20grow%20as%20models%20get%20larger%2C%20keeping%20the%20ratios%20of%0Aacquired%20resources%20constants.%20We%20hypothesize%20these%20findings%20to%20be%20generally%0Atrue%20and%20build%20a%20model%20to%20predict%20neural%20scaling%20laws%20for%20general%20composite%0Atasks%2C%20which%20successfully%20replicates%20the%20neural%20scaling%20law%20of%20Chinchilla%0Amodels%20reported%20in%20arXiv%3A2203.15556.%20We%20believe%20that%20the%20notion%20of%20resource%0Aused%20in%20this%20paper%20will%20be%20a%20useful%20tool%20for%20characterizing%20and%20diagnosing%0Aneural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Resource%2520Model%2520For%2520Neural%2520Scaling%2520Law%26entry.906535625%3DJinyeop%2520Song%2520and%2520Ziming%2520Liu%2520and%2520Max%2520Tegmark%2520and%2520Jeff%2520Gore%26entry.1292438233%3D%2520%2520Neural%2520scaling%2520laws%2520characterize%2520how%2520model%2520performance%2520improves%2520as%2520the%2520model%250Asize%2520scales%2520up.%2520Inspired%2520by%2520empirical%2520observations%252C%2520we%2520introduce%2520a%2520resource%250Amodel%2520of%2520neural%2520scaling.%2520A%2520task%2520is%2520usually%2520composite%2520hence%2520can%2520be%2520decomposed%250Ainto%2520many%2520subtasks%252C%2520which%2520compete%2520for%2520resources%2520%2528measured%2520by%2520the%2520number%2520of%250Aneurons%2520allocated%2520to%2520subtasks%2529.%2520On%2520toy%2520problems%252C%2520we%2520empirically%2520find%2520that%253A%2520%25281%2529%250AThe%2520loss%2520of%2520a%2520subtask%2520is%2520inversely%2520proportional%2520to%2520its%2520allocated%2520neurons.%2520%25282%2529%250AWhen%2520multiple%2520subtasks%2520are%2520present%2520in%2520a%2520composite%2520task%252C%2520the%2520resources%2520acquired%250Aby%2520each%2520subtask%2520uniformly%2520grow%2520as%2520models%2520get%2520larger%252C%2520keeping%2520the%2520ratios%2520of%250Aacquired%2520resources%2520constants.%2520We%2520hypothesize%2520these%2520findings%2520to%2520be%2520generally%250Atrue%2520and%2520build%2520a%2520model%2520to%2520predict%2520neural%2520scaling%2520laws%2520for%2520general%2520composite%250Atasks%252C%2520which%2520successfully%2520replicates%2520the%2520neural%2520scaling%2520law%2520of%2520Chinchilla%250Amodels%2520reported%2520in%2520arXiv%253A2203.15556.%2520We%2520believe%2520that%2520the%2520notion%2520of%2520resource%250Aused%2520in%2520this%2520paper%2520will%2520be%2520a%2520useful%2520tool%2520for%2520characterizing%2520and%2520diagnosing%250Aneural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Resource%20Model%20For%20Neural%20Scaling%20Law&entry.906535625=Jinyeop%20Song%20and%20Ziming%20Liu%20and%20Max%20Tegmark%20and%20Jeff%20Gore&entry.1292438233=%20%20Neural%20scaling%20laws%20characterize%20how%20model%20performance%20improves%20as%20the%20model%0Asize%20scales%20up.%20Inspired%20by%20empirical%20observations%2C%20we%20introduce%20a%20resource%0Amodel%20of%20neural%20scaling.%20A%20task%20is%20usually%20composite%20hence%20can%20be%20decomposed%0Ainto%20many%20subtasks%2C%20which%20compete%20for%20resources%20%28measured%20by%20the%20number%20of%0Aneurons%20allocated%20to%20subtasks%29.%20On%20toy%20problems%2C%20we%20empirically%20find%20that%3A%20%281%29%0AThe%20loss%20of%20a%20subtask%20is%20inversely%20proportional%20to%20its%20allocated%20neurons.%20%282%29%0AWhen%20multiple%20subtasks%20are%20present%20in%20a%20composite%20task%2C%20the%20resources%20acquired%0Aby%20each%20subtask%20uniformly%20grow%20as%20models%20get%20larger%2C%20keeping%20the%20ratios%20of%0Aacquired%20resources%20constants.%20We%20hypothesize%20these%20findings%20to%20be%20generally%0Atrue%20and%20build%20a%20model%20to%20predict%20neural%20scaling%20laws%20for%20general%20composite%0Atasks%2C%20which%20successfully%20replicates%20the%20neural%20scaling%20law%20of%20Chinchilla%0Amodels%20reported%20in%20arXiv%3A2203.15556.%20We%20believe%20that%20the%20notion%20of%20resource%0Aused%20in%20this%20paper%20will%20be%20a%20useful%20tool%20for%20characterizing%20and%20diagnosing%0Aneural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05164v2&entry.124074799=Read"},
{"title": "Bridging the gap in online hate speech detection: a comparative analysis\n  of BERT and traditional models for homophobic content identification on\n  X/Twitter", "author": "Josh McGiff and Nikola S. Nikolov", "abstract": "  Our study addresses a significant gap in online hate speech detection\nresearch by focusing on homophobia, an area often neglected in sentiment\nanalysis research. Utilising advanced sentiment analysis models, particularly\nBERT, and traditional machine learning methods, we developed a nuanced approach\nto identify homophobic content on X/Twitter. This research is pivotal due to\nthe persistent underrepresentation of homophobia in detection models. Our\nfindings reveal that while BERT outperforms traditional methods, the choice of\nvalidation technique can impact model performance. This underscores the\nimportance of contextual understanding in detecting nuanced hate speech. By\nreleasing the largest open-source labelled English dataset for homophobia\ndetection known to us, an analysis of various models' performance and our\nstrongest BERT-based model, we aim to enhance online safety and inclusivity.\nFuture work will extend to broader LGBTQIA+ hate speech detection, addressing\nthe challenges of sourcing diverse datasets. Through this endeavour, we\ncontribute to the larger effort against online hate, advocating for a more\ninclusive digital landscape. Our study not only offers insights into the\neffective detection of homophobic content by improving on previous research\nresults, but it also lays groundwork for future advancements in hate speech\nanalysis.\n", "link": "http://arxiv.org/abs/2405.09221v1", "date": "2024-05-15", "relevancy": 1.2686, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4291}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4212}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20gap%20in%20online%20hate%20speech%20detection%3A%20a%20comparative%20analysis%0A%20%20of%20BERT%20and%20traditional%20models%20for%20homophobic%20content%20identification%20on%0A%20%20X/Twitter&body=Title%3A%20Bridging%20the%20gap%20in%20online%20hate%20speech%20detection%3A%20a%20comparative%20analysis%0A%20%20of%20BERT%20and%20traditional%20models%20for%20homophobic%20content%20identification%20on%0A%20%20X/Twitter%0AAuthor%3A%20Josh%20McGiff%20and%20Nikola%20S.%20Nikolov%0AAbstract%3A%20%20%20Our%20study%20addresses%20a%20significant%20gap%20in%20online%20hate%20speech%20detection%0Aresearch%20by%20focusing%20on%20homophobia%2C%20an%20area%20often%20neglected%20in%20sentiment%0Aanalysis%20research.%20Utilising%20advanced%20sentiment%20analysis%20models%2C%20particularly%0ABERT%2C%20and%20traditional%20machine%20learning%20methods%2C%20we%20developed%20a%20nuanced%20approach%0Ato%20identify%20homophobic%20content%20on%20X/Twitter.%20This%20research%20is%20pivotal%20due%20to%0Athe%20persistent%20underrepresentation%20of%20homophobia%20in%20detection%20models.%20Our%0Afindings%20reveal%20that%20while%20BERT%20outperforms%20traditional%20methods%2C%20the%20choice%20of%0Avalidation%20technique%20can%20impact%20model%20performance.%20This%20underscores%20the%0Aimportance%20of%20contextual%20understanding%20in%20detecting%20nuanced%20hate%20speech.%20By%0Areleasing%20the%20largest%20open-source%20labelled%20English%20dataset%20for%20homophobia%0Adetection%20known%20to%20us%2C%20an%20analysis%20of%20various%20models%27%20performance%20and%20our%0Astrongest%20BERT-based%20model%2C%20we%20aim%20to%20enhance%20online%20safety%20and%20inclusivity.%0AFuture%20work%20will%20extend%20to%20broader%20LGBTQIA%2B%20hate%20speech%20detection%2C%20addressing%0Athe%20challenges%20of%20sourcing%20diverse%20datasets.%20Through%20this%20endeavour%2C%20we%0Acontribute%20to%20the%20larger%20effort%20against%20online%20hate%2C%20advocating%20for%20a%20more%0Ainclusive%20digital%20landscape.%20Our%20study%20not%20only%20offers%20insights%20into%20the%0Aeffective%20detection%20of%20homophobic%20content%20by%20improving%20on%20previous%20research%0Aresults%2C%20but%20it%20also%20lays%20groundwork%20for%20future%20advancements%20in%20hate%20speech%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520gap%2520in%2520online%2520hate%2520speech%2520detection%253A%2520a%2520comparative%2520analysis%250A%2520%2520of%2520BERT%2520and%2520traditional%2520models%2520for%2520homophobic%2520content%2520identification%2520on%250A%2520%2520X/Twitter%26entry.906535625%3DJosh%2520McGiff%2520and%2520Nikola%2520S.%2520Nikolov%26entry.1292438233%3D%2520%2520Our%2520study%2520addresses%2520a%2520significant%2520gap%2520in%2520online%2520hate%2520speech%2520detection%250Aresearch%2520by%2520focusing%2520on%2520homophobia%252C%2520an%2520area%2520often%2520neglected%2520in%2520sentiment%250Aanalysis%2520research.%2520Utilising%2520advanced%2520sentiment%2520analysis%2520models%252C%2520particularly%250ABERT%252C%2520and%2520traditional%2520machine%2520learning%2520methods%252C%2520we%2520developed%2520a%2520nuanced%2520approach%250Ato%2520identify%2520homophobic%2520content%2520on%2520X/Twitter.%2520This%2520research%2520is%2520pivotal%2520due%2520to%250Athe%2520persistent%2520underrepresentation%2520of%2520homophobia%2520in%2520detection%2520models.%2520Our%250Afindings%2520reveal%2520that%2520while%2520BERT%2520outperforms%2520traditional%2520methods%252C%2520the%2520choice%2520of%250Avalidation%2520technique%2520can%2520impact%2520model%2520performance.%2520This%2520underscores%2520the%250Aimportance%2520of%2520contextual%2520understanding%2520in%2520detecting%2520nuanced%2520hate%2520speech.%2520By%250Areleasing%2520the%2520largest%2520open-source%2520labelled%2520English%2520dataset%2520for%2520homophobia%250Adetection%2520known%2520to%2520us%252C%2520an%2520analysis%2520of%2520various%2520models%2527%2520performance%2520and%2520our%250Astrongest%2520BERT-based%2520model%252C%2520we%2520aim%2520to%2520enhance%2520online%2520safety%2520and%2520inclusivity.%250AFuture%2520work%2520will%2520extend%2520to%2520broader%2520LGBTQIA%252B%2520hate%2520speech%2520detection%252C%2520addressing%250Athe%2520challenges%2520of%2520sourcing%2520diverse%2520datasets.%2520Through%2520this%2520endeavour%252C%2520we%250Acontribute%2520to%2520the%2520larger%2520effort%2520against%2520online%2520hate%252C%2520advocating%2520for%2520a%2520more%250Ainclusive%2520digital%2520landscape.%2520Our%2520study%2520not%2520only%2520offers%2520insights%2520into%2520the%250Aeffective%2520detection%2520of%2520homophobic%2520content%2520by%2520improving%2520on%2520previous%2520research%250Aresults%252C%2520but%2520it%2520also%2520lays%2520groundwork%2520for%2520future%2520advancements%2520in%2520hate%2520speech%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20gap%20in%20online%20hate%20speech%20detection%3A%20a%20comparative%20analysis%0A%20%20of%20BERT%20and%20traditional%20models%20for%20homophobic%20content%20identification%20on%0A%20%20X/Twitter&entry.906535625=Josh%20McGiff%20and%20Nikola%20S.%20Nikolov&entry.1292438233=%20%20Our%20study%20addresses%20a%20significant%20gap%20in%20online%20hate%20speech%20detection%0Aresearch%20by%20focusing%20on%20homophobia%2C%20an%20area%20often%20neglected%20in%20sentiment%0Aanalysis%20research.%20Utilising%20advanced%20sentiment%20analysis%20models%2C%20particularly%0ABERT%2C%20and%20traditional%20machine%20learning%20methods%2C%20we%20developed%20a%20nuanced%20approach%0Ato%20identify%20homophobic%20content%20on%20X/Twitter.%20This%20research%20is%20pivotal%20due%20to%0Athe%20persistent%20underrepresentation%20of%20homophobia%20in%20detection%20models.%20Our%0Afindings%20reveal%20that%20while%20BERT%20outperforms%20traditional%20methods%2C%20the%20choice%20of%0Avalidation%20technique%20can%20impact%20model%20performance.%20This%20underscores%20the%0Aimportance%20of%20contextual%20understanding%20in%20detecting%20nuanced%20hate%20speech.%20By%0Areleasing%20the%20largest%20open-source%20labelled%20English%20dataset%20for%20homophobia%0Adetection%20known%20to%20us%2C%20an%20analysis%20of%20various%20models%27%20performance%20and%20our%0Astrongest%20BERT-based%20model%2C%20we%20aim%20to%20enhance%20online%20safety%20and%20inclusivity.%0AFuture%20work%20will%20extend%20to%20broader%20LGBTQIA%2B%20hate%20speech%20detection%2C%20addressing%0Athe%20challenges%20of%20sourcing%20diverse%20datasets.%20Through%20this%20endeavour%2C%20we%0Acontribute%20to%20the%20larger%20effort%20against%20online%20hate%2C%20advocating%20for%20a%20more%0Ainclusive%20digital%20landscape.%20Our%20study%20not%20only%20offers%20insights%20into%20the%0Aeffective%20detection%20of%20homophobic%20content%20by%20improving%20on%20previous%20research%0Aresults%2C%20but%20it%20also%20lays%20groundwork%20for%20future%20advancements%20in%20hate%20speech%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09221v1&entry.124074799=Read"},
{"title": "MMFusion: Multi-modality Diffusion Model for Lymph Node Metastasis\n  Diagnosis in Esophageal Cancer", "author": "Chengyu Wu and Chengkai Wang and Yaqi Wang and Huiyu Zhou and Yatao Zhang and Qifeng Wang and Shuai Wang", "abstract": "  Esophageal cancer is one of the most common types of cancer worldwide and\nranks sixth in cancer-related mortality. Accurate computer-assisted diagnosis\nof cancer progression can help physicians effectively customize personalized\ntreatment plans. Currently, CT-based cancer diagnosis methods have received\nmuch attention for their comprehensive ability to examine patients' conditions.\nHowever, multi-modal based methods may likely introduce information redundancy,\nleading to underperformance. In addition, efficient and effective interactions\nbetween multi-modal representations need to be further explored, lacking\ninsightful exploration of prognostic correlation in multi-modality features. In\nthis work, we introduce a multi-modal heterogeneous graph-based conditional\nfeature-guided diffusion model for lymph node metastasis diagnosis based on CT\nimages as well as clinical measurements and radiomics data. To explore the\nintricate relationships between multi-modal features, we construct a\nheterogeneous graph. Following this, a conditional feature-guided diffusion\napproach is applied to eliminate information redundancy. Moreover, we propose a\nmasked relational representation learning strategy, aiming to uncover the\nlatent prognostic correlations and priorities of primary tumor and lymph node\nimage representations. Various experimental results validate the effectiveness\nof our proposed method. The code is available at\nhttps://github.com/wuchengyu123/MMFusion.\n", "link": "http://arxiv.org/abs/2405.09539v1", "date": "2024-05-15", "relevancy": 1.0145, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMFusion%3A%20Multi-modality%20Diffusion%20Model%20for%20Lymph%20Node%20Metastasis%0A%20%20Diagnosis%20in%20Esophageal%20Cancer&body=Title%3A%20MMFusion%3A%20Multi-modality%20Diffusion%20Model%20for%20Lymph%20Node%20Metastasis%0A%20%20Diagnosis%20in%20Esophageal%20Cancer%0AAuthor%3A%20Chengyu%20Wu%20and%20Chengkai%20Wang%20and%20Yaqi%20Wang%20and%20Huiyu%20Zhou%20and%20Yatao%20Zhang%20and%20Qifeng%20Wang%20and%20Shuai%20Wang%0AAbstract%3A%20%20%20Esophageal%20cancer%20is%20one%20of%20the%20most%20common%20types%20of%20cancer%20worldwide%20and%0Aranks%20sixth%20in%20cancer-related%20mortality.%20Accurate%20computer-assisted%20diagnosis%0Aof%20cancer%20progression%20can%20help%20physicians%20effectively%20customize%20personalized%0Atreatment%20plans.%20Currently%2C%20CT-based%20cancer%20diagnosis%20methods%20have%20received%0Amuch%20attention%20for%20their%20comprehensive%20ability%20to%20examine%20patients%27%20conditions.%0AHowever%2C%20multi-modal%20based%20methods%20may%20likely%20introduce%20information%20redundancy%2C%0Aleading%20to%20underperformance.%20In%20addition%2C%20efficient%20and%20effective%20interactions%0Abetween%20multi-modal%20representations%20need%20to%20be%20further%20explored%2C%20lacking%0Ainsightful%20exploration%20of%20prognostic%20correlation%20in%20multi-modality%20features.%20In%0Athis%20work%2C%20we%20introduce%20a%20multi-modal%20heterogeneous%20graph-based%20conditional%0Afeature-guided%20diffusion%20model%20for%20lymph%20node%20metastasis%20diagnosis%20based%20on%20CT%0Aimages%20as%20well%20as%20clinical%20measurements%20and%20radiomics%20data.%20To%20explore%20the%0Aintricate%20relationships%20between%20multi-modal%20features%2C%20we%20construct%20a%0Aheterogeneous%20graph.%20Following%20this%2C%20a%20conditional%20feature-guided%20diffusion%0Aapproach%20is%20applied%20to%20eliminate%20information%20redundancy.%20Moreover%2C%20we%20propose%20a%0Amasked%20relational%20representation%20learning%20strategy%2C%20aiming%20to%20uncover%20the%0Alatent%20prognostic%20correlations%20and%20priorities%20of%20primary%20tumor%20and%20lymph%20node%0Aimage%20representations.%20Various%20experimental%20results%20validate%20the%20effectiveness%0Aof%20our%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/wuchengyu123/MMFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMFusion%253A%2520Multi-modality%2520Diffusion%2520Model%2520for%2520Lymph%2520Node%2520Metastasis%250A%2520%2520Diagnosis%2520in%2520Esophageal%2520Cancer%26entry.906535625%3DChengyu%2520Wu%2520and%2520Chengkai%2520Wang%2520and%2520Yaqi%2520Wang%2520and%2520Huiyu%2520Zhou%2520and%2520Yatao%2520Zhang%2520and%2520Qifeng%2520Wang%2520and%2520Shuai%2520Wang%26entry.1292438233%3D%2520%2520Esophageal%2520cancer%2520is%2520one%2520of%2520the%2520most%2520common%2520types%2520of%2520cancer%2520worldwide%2520and%250Aranks%2520sixth%2520in%2520cancer-related%2520mortality.%2520Accurate%2520computer-assisted%2520diagnosis%250Aof%2520cancer%2520progression%2520can%2520help%2520physicians%2520effectively%2520customize%2520personalized%250Atreatment%2520plans.%2520Currently%252C%2520CT-based%2520cancer%2520diagnosis%2520methods%2520have%2520received%250Amuch%2520attention%2520for%2520their%2520comprehensive%2520ability%2520to%2520examine%2520patients%2527%2520conditions.%250AHowever%252C%2520multi-modal%2520based%2520methods%2520may%2520likely%2520introduce%2520information%2520redundancy%252C%250Aleading%2520to%2520underperformance.%2520In%2520addition%252C%2520efficient%2520and%2520effective%2520interactions%250Abetween%2520multi-modal%2520representations%2520need%2520to%2520be%2520further%2520explored%252C%2520lacking%250Ainsightful%2520exploration%2520of%2520prognostic%2520correlation%2520in%2520multi-modality%2520features.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520multi-modal%2520heterogeneous%2520graph-based%2520conditional%250Afeature-guided%2520diffusion%2520model%2520for%2520lymph%2520node%2520metastasis%2520diagnosis%2520based%2520on%2520CT%250Aimages%2520as%2520well%2520as%2520clinical%2520measurements%2520and%2520radiomics%2520data.%2520To%2520explore%2520the%250Aintricate%2520relationships%2520between%2520multi-modal%2520features%252C%2520we%2520construct%2520a%250Aheterogeneous%2520graph.%2520Following%2520this%252C%2520a%2520conditional%2520feature-guided%2520diffusion%250Aapproach%2520is%2520applied%2520to%2520eliminate%2520information%2520redundancy.%2520Moreover%252C%2520we%2520propose%2520a%250Amasked%2520relational%2520representation%2520learning%2520strategy%252C%2520aiming%2520to%2520uncover%2520the%250Alatent%2520prognostic%2520correlations%2520and%2520priorities%2520of%2520primary%2520tumor%2520and%2520lymph%2520node%250Aimage%2520representations.%2520Various%2520experimental%2520results%2520validate%2520the%2520effectiveness%250Aof%2520our%2520proposed%2520method.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/wuchengyu123/MMFusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMFusion%3A%20Multi-modality%20Diffusion%20Model%20for%20Lymph%20Node%20Metastasis%0A%20%20Diagnosis%20in%20Esophageal%20Cancer&entry.906535625=Chengyu%20Wu%20and%20Chengkai%20Wang%20and%20Yaqi%20Wang%20and%20Huiyu%20Zhou%20and%20Yatao%20Zhang%20and%20Qifeng%20Wang%20and%20Shuai%20Wang&entry.1292438233=%20%20Esophageal%20cancer%20is%20one%20of%20the%20most%20common%20types%20of%20cancer%20worldwide%20and%0Aranks%20sixth%20in%20cancer-related%20mortality.%20Accurate%20computer-assisted%20diagnosis%0Aof%20cancer%20progression%20can%20help%20physicians%20effectively%20customize%20personalized%0Atreatment%20plans.%20Currently%2C%20CT-based%20cancer%20diagnosis%20methods%20have%20received%0Amuch%20attention%20for%20their%20comprehensive%20ability%20to%20examine%20patients%27%20conditions.%0AHowever%2C%20multi-modal%20based%20methods%20may%20likely%20introduce%20information%20redundancy%2C%0Aleading%20to%20underperformance.%20In%20addition%2C%20efficient%20and%20effective%20interactions%0Abetween%20multi-modal%20representations%20need%20to%20be%20further%20explored%2C%20lacking%0Ainsightful%20exploration%20of%20prognostic%20correlation%20in%20multi-modality%20features.%20In%0Athis%20work%2C%20we%20introduce%20a%20multi-modal%20heterogeneous%20graph-based%20conditional%0Afeature-guided%20diffusion%20model%20for%20lymph%20node%20metastasis%20diagnosis%20based%20on%20CT%0Aimages%20as%20well%20as%20clinical%20measurements%20and%20radiomics%20data.%20To%20explore%20the%0Aintricate%20relationships%20between%20multi-modal%20features%2C%20we%20construct%20a%0Aheterogeneous%20graph.%20Following%20this%2C%20a%20conditional%20feature-guided%20diffusion%0Aapproach%20is%20applied%20to%20eliminate%20information%20redundancy.%20Moreover%2C%20we%20propose%20a%0Amasked%20relational%20representation%20learning%20strategy%2C%20aiming%20to%20uncover%20the%0Alatent%20prognostic%20correlations%20and%20priorities%20of%20primary%20tumor%20and%20lymph%20node%0Aimage%20representations.%20Various%20experimental%20results%20validate%20the%20effectiveness%0Aof%20our%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/wuchengyu123/MMFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09539v1&entry.124074799=Read"},
{"title": "Improved classical shadows from local symmetries in the Schur basis", "author": "Daniel Grier and Sihan Liu and Gaurav Mahajan", "abstract": "  We study the sample complexity of the classical shadows task: what is the\nfewest number of copies of an unknown state you need to measure to predict\nexpected values with respect to some class of observables? Large joint\nmeasurements are likely required in order to minimize sample complexity, but\nprevious joint measurement protocols only work when the unknown state is pure.\nWe present the first joint measurement protocol for classical shadows whose\nsample complexity scales with the rank of the unknown state. In particular we\nprove $\\mathcal O(\\sqrt{rB}/\\epsilon^2)$ samples suffice, where $r$ is the rank\nof the state, $B$ is a bound on the squared Frobenius norm of the observables,\nand $\\epsilon$ is the target accuracy. In the low-rank regime, this is a nearly\nquadratic advantage over traditional approaches that use single-copy\nmeasurements.\n  We present several intermediate results that may be of independent interest:\na solution to a new formulation of classical shadows that captures functions of\nnon-identical input states; a generalization of a ``nice'' Schur basis used for\noptimal qubit purification and quantum majority vote; and a measurement\nstrategy that allows us to use local symmetries in the Schur basis to avoid\nintractable Weingarten calculations in the analysis.\n", "link": "http://arxiv.org/abs/2405.09525v1", "date": "2024-05-15", "relevancy": 1.5244, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4107}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.38}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20classical%20shadows%20from%20local%20symmetries%20in%20the%20Schur%20basis&body=Title%3A%20Improved%20classical%20shadows%20from%20local%20symmetries%20in%20the%20Schur%20basis%0AAuthor%3A%20Daniel%20Grier%20and%20Sihan%20Liu%20and%20Gaurav%20Mahajan%0AAbstract%3A%20%20%20We%20study%20the%20sample%20complexity%20of%20the%20classical%20shadows%20task%3A%20what%20is%20the%0Afewest%20number%20of%20copies%20of%20an%20unknown%20state%20you%20need%20to%20measure%20to%20predict%0Aexpected%20values%20with%20respect%20to%20some%20class%20of%20observables%3F%20Large%20joint%0Ameasurements%20are%20likely%20required%20in%20order%20to%20minimize%20sample%20complexity%2C%20but%0Aprevious%20joint%20measurement%20protocols%20only%20work%20when%20the%20unknown%20state%20is%20pure.%0AWe%20present%20the%20first%20joint%20measurement%20protocol%20for%20classical%20shadows%20whose%0Asample%20complexity%20scales%20with%20the%20rank%20of%20the%20unknown%20state.%20In%20particular%20we%0Aprove%20%24%5Cmathcal%20O%28%5Csqrt%7BrB%7D/%5Cepsilon%5E2%29%24%20samples%20suffice%2C%20where%20%24r%24%20is%20the%20rank%0Aof%20the%20state%2C%20%24B%24%20is%20a%20bound%20on%20the%20squared%20Frobenius%20norm%20of%20the%20observables%2C%0Aand%20%24%5Cepsilon%24%20is%20the%20target%20accuracy.%20In%20the%20low-rank%20regime%2C%20this%20is%20a%20nearly%0Aquadratic%20advantage%20over%20traditional%20approaches%20that%20use%20single-copy%0Ameasurements.%0A%20%20We%20present%20several%20intermediate%20results%20that%20may%20be%20of%20independent%20interest%3A%0Aa%20solution%20to%20a%20new%20formulation%20of%20classical%20shadows%20that%20captures%20functions%20of%0Anon-identical%20input%20states%3B%20a%20generalization%20of%20a%20%60%60nice%27%27%20Schur%20basis%20used%20for%0Aoptimal%20qubit%20purification%20and%20quantum%20majority%20vote%3B%20and%20a%20measurement%0Astrategy%20that%20allows%20us%20to%20use%20local%20symmetries%20in%20the%20Schur%20basis%20to%20avoid%0Aintractable%20Weingarten%20calculations%20in%20the%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520classical%2520shadows%2520from%2520local%2520symmetries%2520in%2520the%2520Schur%2520basis%26entry.906535625%3DDaniel%2520Grier%2520and%2520Sihan%2520Liu%2520and%2520Gaurav%2520Mahajan%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520sample%2520complexity%2520of%2520the%2520classical%2520shadows%2520task%253A%2520what%2520is%2520the%250Afewest%2520number%2520of%2520copies%2520of%2520an%2520unknown%2520state%2520you%2520need%2520to%2520measure%2520to%2520predict%250Aexpected%2520values%2520with%2520respect%2520to%2520some%2520class%2520of%2520observables%253F%2520Large%2520joint%250Ameasurements%2520are%2520likely%2520required%2520in%2520order%2520to%2520minimize%2520sample%2520complexity%252C%2520but%250Aprevious%2520joint%2520measurement%2520protocols%2520only%2520work%2520when%2520the%2520unknown%2520state%2520is%2520pure.%250AWe%2520present%2520the%2520first%2520joint%2520measurement%2520protocol%2520for%2520classical%2520shadows%2520whose%250Asample%2520complexity%2520scales%2520with%2520the%2520rank%2520of%2520the%2520unknown%2520state.%2520In%2520particular%2520we%250Aprove%2520%2524%255Cmathcal%2520O%2528%255Csqrt%257BrB%257D/%255Cepsilon%255E2%2529%2524%2520samples%2520suffice%252C%2520where%2520%2524r%2524%2520is%2520the%2520rank%250Aof%2520the%2520state%252C%2520%2524B%2524%2520is%2520a%2520bound%2520on%2520the%2520squared%2520Frobenius%2520norm%2520of%2520the%2520observables%252C%250Aand%2520%2524%255Cepsilon%2524%2520is%2520the%2520target%2520accuracy.%2520In%2520the%2520low-rank%2520regime%252C%2520this%2520is%2520a%2520nearly%250Aquadratic%2520advantage%2520over%2520traditional%2520approaches%2520that%2520use%2520single-copy%250Ameasurements.%250A%2520%2520We%2520present%2520several%2520intermediate%2520results%2520that%2520may%2520be%2520of%2520independent%2520interest%253A%250Aa%2520solution%2520to%2520a%2520new%2520formulation%2520of%2520classical%2520shadows%2520that%2520captures%2520functions%2520of%250Anon-identical%2520input%2520states%253B%2520a%2520generalization%2520of%2520a%2520%2560%2560nice%2527%2527%2520Schur%2520basis%2520used%2520for%250Aoptimal%2520qubit%2520purification%2520and%2520quantum%2520majority%2520vote%253B%2520and%2520a%2520measurement%250Astrategy%2520that%2520allows%2520us%2520to%2520use%2520local%2520symmetries%2520in%2520the%2520Schur%2520basis%2520to%2520avoid%250Aintractable%2520Weingarten%2520calculations%2520in%2520the%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20classical%20shadows%20from%20local%20symmetries%20in%20the%20Schur%20basis&entry.906535625=Daniel%20Grier%20and%20Sihan%20Liu%20and%20Gaurav%20Mahajan&entry.1292438233=%20%20We%20study%20the%20sample%20complexity%20of%20the%20classical%20shadows%20task%3A%20what%20is%20the%0Afewest%20number%20of%20copies%20of%20an%20unknown%20state%20you%20need%20to%20measure%20to%20predict%0Aexpected%20values%20with%20respect%20to%20some%20class%20of%20observables%3F%20Large%20joint%0Ameasurements%20are%20likely%20required%20in%20order%20to%20minimize%20sample%20complexity%2C%20but%0Aprevious%20joint%20measurement%20protocols%20only%20work%20when%20the%20unknown%20state%20is%20pure.%0AWe%20present%20the%20first%20joint%20measurement%20protocol%20for%20classical%20shadows%20whose%0Asample%20complexity%20scales%20with%20the%20rank%20of%20the%20unknown%20state.%20In%20particular%20we%0Aprove%20%24%5Cmathcal%20O%28%5Csqrt%7BrB%7D/%5Cepsilon%5E2%29%24%20samples%20suffice%2C%20where%20%24r%24%20is%20the%20rank%0Aof%20the%20state%2C%20%24B%24%20is%20a%20bound%20on%20the%20squared%20Frobenius%20norm%20of%20the%20observables%2C%0Aand%20%24%5Cepsilon%24%20is%20the%20target%20accuracy.%20In%20the%20low-rank%20regime%2C%20this%20is%20a%20nearly%0Aquadratic%20advantage%20over%20traditional%20approaches%20that%20use%20single-copy%0Ameasurements.%0A%20%20We%20present%20several%20intermediate%20results%20that%20may%20be%20of%20independent%20interest%3A%0Aa%20solution%20to%20a%20new%20formulation%20of%20classical%20shadows%20that%20captures%20functions%20of%0Anon-identical%20input%20states%3B%20a%20generalization%20of%20a%20%60%60nice%27%27%20Schur%20basis%20used%20for%0Aoptimal%20qubit%20purification%20and%20quantum%20majority%20vote%3B%20and%20a%20measurement%0Astrategy%20that%20allows%20us%20to%20use%20local%20symmetries%20in%20the%20Schur%20basis%20to%20avoid%0Aintractable%20Weingarten%20calculations%20in%20the%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09525v1&entry.124074799=Read"},
{"title": "A Survey of Large Language Models in Medicine: Progress, Application,\n  and Challenge", "author": "Hongjian Zhou and Fenglin Liu and Boyang Gu and Xinyu Zou and Jinfa Huang and Jinge Wu and Yiru Li and Sam S. Chen and Peilin Zhou and Junling Liu and Yining Hua and Chengfeng Mao and Chenyu You and Xian Wu and Yefeng Zheng and Lei Clifton and Zheng Li and Jiebo Luo and David A. Clifton", "abstract": "  Large language models (LLMs), such as ChatGPT, have received substantial\nattention due to their capabilities for understanding and generating human\nlanguage. While there has been a burgeoning trend in research focusing on the\nemployment of LLMs in supporting different medical tasks (e.g., enhancing\nclinical diagnostics and providing medical education), a review of these\nefforts, particularly their development, practical applications, and outcomes\nin medicine, remains scarce. Therefore, this review aims to provide a detailed\noverview of the development and deployment of LLMs in medicine, including the\nchallenges and opportunities they face. In terms of development, we provide a\ndetailed introduction to the principles of existing medical LLMs, including\ntheir basic model structures, number of parameters, and sources and scales of\ndata used for model development. It serves as a guide for practitioners in\ndeveloping medical LLMs tailored to their specific needs. In terms of\ndeployment, we offer a comparison of the performance of different LLMs across\nvarious medical tasks, and further compare them with state-of-the-art\nlightweight models, aiming to provide an understanding of the advantages and\nlimitations of LLMs in medicine. Overall, in this review, we address the\nfollowing questions: 1) What are the practices for developing medical LLMs 2)\nHow to measure the medical task performance of LLMs in a medical setting? 3)\nHow have medical LLMs been employed in real-world practice? 4) What challenges\narise from the use of medical LLMs? and 5) How to more effectively develop and\ndeploy medical LLMs? By answering these questions, this review aims to provide\ninsights into the opportunities for LLMs in medicine and serve as a practical\nresource. We also maintain a regularly updated list of practical guides on\nmedical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.\n", "link": "http://arxiv.org/abs/2311.05112v5", "date": "2024-05-15", "relevancy": 1.3882, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4857}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Large%20Language%20Models%20in%20Medicine%3A%20Progress%2C%20Application%2C%0A%20%20and%20Challenge&body=Title%3A%20A%20Survey%20of%20Large%20Language%20Models%20in%20Medicine%3A%20Progress%2C%20Application%2C%0A%20%20and%20Challenge%0AAuthor%3A%20Hongjian%20Zhou%20and%20Fenglin%20Liu%20and%20Boyang%20Gu%20and%20Xinyu%20Zou%20and%20Jinfa%20Huang%20and%20Jinge%20Wu%20and%20Yiru%20Li%20and%20Sam%20S.%20Chen%20and%20Peilin%20Zhou%20and%20Junling%20Liu%20and%20Yining%20Hua%20and%20Chengfeng%20Mao%20and%20Chenyu%20You%20and%20Xian%20Wu%20and%20Yefeng%20Zheng%20and%20Lei%20Clifton%20and%20Zheng%20Li%20and%20Jiebo%20Luo%20and%20David%20A.%20Clifton%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20have%20received%20substantial%0Aattention%20due%20to%20their%20capabilities%20for%20understanding%20and%20generating%20human%0Alanguage.%20While%20there%20has%20been%20a%20burgeoning%20trend%20in%20research%20focusing%20on%20the%0Aemployment%20of%20LLMs%20in%20supporting%20different%20medical%20tasks%20%28e.g.%2C%20enhancing%0Aclinical%20diagnostics%20and%20providing%20medical%20education%29%2C%20a%20review%20of%20these%0Aefforts%2C%20particularly%20their%20development%2C%20practical%20applications%2C%20and%20outcomes%0Ain%20medicine%2C%20remains%20scarce.%20Therefore%2C%20this%20review%20aims%20to%20provide%20a%20detailed%0Aoverview%20of%20the%20development%20and%20deployment%20of%20LLMs%20in%20medicine%2C%20including%20the%0Achallenges%20and%20opportunities%20they%20face.%20In%20terms%20of%20development%2C%20we%20provide%20a%0Adetailed%20introduction%20to%20the%20principles%20of%20existing%20medical%20LLMs%2C%20including%0Atheir%20basic%20model%20structures%2C%20number%20of%20parameters%2C%20and%20sources%20and%20scales%20of%0Adata%20used%20for%20model%20development.%20It%20serves%20as%20a%20guide%20for%20practitioners%20in%0Adeveloping%20medical%20LLMs%20tailored%20to%20their%20specific%20needs.%20In%20terms%20of%0Adeployment%2C%20we%20offer%20a%20comparison%20of%20the%20performance%20of%20different%20LLMs%20across%0Avarious%20medical%20tasks%2C%20and%20further%20compare%20them%20with%20state-of-the-art%0Alightweight%20models%2C%20aiming%20to%20provide%20an%20understanding%20of%20the%20advantages%20and%0Alimitations%20of%20LLMs%20in%20medicine.%20Overall%2C%20in%20this%20review%2C%20we%20address%20the%0Afollowing%20questions%3A%201%29%20What%20are%20the%20practices%20for%20developing%20medical%20LLMs%202%29%0AHow%20to%20measure%20the%20medical%20task%20performance%20of%20LLMs%20in%20a%20medical%20setting%3F%203%29%0AHow%20have%20medical%20LLMs%20been%20employed%20in%20real-world%20practice%3F%204%29%20What%20challenges%0Aarise%20from%20the%20use%20of%20medical%20LLMs%3F%20and%205%29%20How%20to%20more%20effectively%20develop%20and%0Adeploy%20medical%20LLMs%3F%20By%20answering%20these%20questions%2C%20this%20review%20aims%20to%20provide%0Ainsights%20into%20the%20opportunities%20for%20LLMs%20in%20medicine%20and%20serve%20as%20a%20practical%0Aresource.%20We%20also%20maintain%20a%20regularly%20updated%20list%20of%20practical%20guides%20on%0Amedical%20LLMs%20at%3A%20https%3A//github.com/AI-in-Health/MedLLMsPracticalGuide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05112v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Large%2520Language%2520Models%2520in%2520Medicine%253A%2520Progress%252C%2520Application%252C%250A%2520%2520and%2520Challenge%26entry.906535625%3DHongjian%2520Zhou%2520and%2520Fenglin%2520Liu%2520and%2520Boyang%2520Gu%2520and%2520Xinyu%2520Zou%2520and%2520Jinfa%2520Huang%2520and%2520Jinge%2520Wu%2520and%2520Yiru%2520Li%2520and%2520Sam%2520S.%2520Chen%2520and%2520Peilin%2520Zhou%2520and%2520Junling%2520Liu%2520and%2520Yining%2520Hua%2520and%2520Chengfeng%2520Mao%2520and%2520Chenyu%2520You%2520and%2520Xian%2520Wu%2520and%2520Yefeng%2520Zheng%2520and%2520Lei%2520Clifton%2520and%2520Zheng%2520Li%2520and%2520Jiebo%2520Luo%2520and%2520David%2520A.%2520Clifton%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%2520ChatGPT%252C%2520have%2520received%2520substantial%250Aattention%2520due%2520to%2520their%2520capabilities%2520for%2520understanding%2520and%2520generating%2520human%250Alanguage.%2520While%2520there%2520has%2520been%2520a%2520burgeoning%2520trend%2520in%2520research%2520focusing%2520on%2520the%250Aemployment%2520of%2520LLMs%2520in%2520supporting%2520different%2520medical%2520tasks%2520%2528e.g.%252C%2520enhancing%250Aclinical%2520diagnostics%2520and%2520providing%2520medical%2520education%2529%252C%2520a%2520review%2520of%2520these%250Aefforts%252C%2520particularly%2520their%2520development%252C%2520practical%2520applications%252C%2520and%2520outcomes%250Ain%2520medicine%252C%2520remains%2520scarce.%2520Therefore%252C%2520this%2520review%2520aims%2520to%2520provide%2520a%2520detailed%250Aoverview%2520of%2520the%2520development%2520and%2520deployment%2520of%2520LLMs%2520in%2520medicine%252C%2520including%2520the%250Achallenges%2520and%2520opportunities%2520they%2520face.%2520In%2520terms%2520of%2520development%252C%2520we%2520provide%2520a%250Adetailed%2520introduction%2520to%2520the%2520principles%2520of%2520existing%2520medical%2520LLMs%252C%2520including%250Atheir%2520basic%2520model%2520structures%252C%2520number%2520of%2520parameters%252C%2520and%2520sources%2520and%2520scales%2520of%250Adata%2520used%2520for%2520model%2520development.%2520It%2520serves%2520as%2520a%2520guide%2520for%2520practitioners%2520in%250Adeveloping%2520medical%2520LLMs%2520tailored%2520to%2520their%2520specific%2520needs.%2520In%2520terms%2520of%250Adeployment%252C%2520we%2520offer%2520a%2520comparison%2520of%2520the%2520performance%2520of%2520different%2520LLMs%2520across%250Avarious%2520medical%2520tasks%252C%2520and%2520further%2520compare%2520them%2520with%2520state-of-the-art%250Alightweight%2520models%252C%2520aiming%2520to%2520provide%2520an%2520understanding%2520of%2520the%2520advantages%2520and%250Alimitations%2520of%2520LLMs%2520in%2520medicine.%2520Overall%252C%2520in%2520this%2520review%252C%2520we%2520address%2520the%250Afollowing%2520questions%253A%25201%2529%2520What%2520are%2520the%2520practices%2520for%2520developing%2520medical%2520LLMs%25202%2529%250AHow%2520to%2520measure%2520the%2520medical%2520task%2520performance%2520of%2520LLMs%2520in%2520a%2520medical%2520setting%253F%25203%2529%250AHow%2520have%2520medical%2520LLMs%2520been%2520employed%2520in%2520real-world%2520practice%253F%25204%2529%2520What%2520challenges%250Aarise%2520from%2520the%2520use%2520of%2520medical%2520LLMs%253F%2520and%25205%2529%2520How%2520to%2520more%2520effectively%2520develop%2520and%250Adeploy%2520medical%2520LLMs%253F%2520By%2520answering%2520these%2520questions%252C%2520this%2520review%2520aims%2520to%2520provide%250Ainsights%2520into%2520the%2520opportunities%2520for%2520LLMs%2520in%2520medicine%2520and%2520serve%2520as%2520a%2520practical%250Aresource.%2520We%2520also%2520maintain%2520a%2520regularly%2520updated%2520list%2520of%2520practical%2520guides%2520on%250Amedical%2520LLMs%2520at%253A%2520https%253A//github.com/AI-in-Health/MedLLMsPracticalGuide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05112v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Large%20Language%20Models%20in%20Medicine%3A%20Progress%2C%20Application%2C%0A%20%20and%20Challenge&entry.906535625=Hongjian%20Zhou%20and%20Fenglin%20Liu%20and%20Boyang%20Gu%20and%20Xinyu%20Zou%20and%20Jinfa%20Huang%20and%20Jinge%20Wu%20and%20Yiru%20Li%20and%20Sam%20S.%20Chen%20and%20Peilin%20Zhou%20and%20Junling%20Liu%20and%20Yining%20Hua%20and%20Chengfeng%20Mao%20and%20Chenyu%20You%20and%20Xian%20Wu%20and%20Yefeng%20Zheng%20and%20Lei%20Clifton%20and%20Zheng%20Li%20and%20Jiebo%20Luo%20and%20David%20A.%20Clifton&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20have%20received%20substantial%0Aattention%20due%20to%20their%20capabilities%20for%20understanding%20and%20generating%20human%0Alanguage.%20While%20there%20has%20been%20a%20burgeoning%20trend%20in%20research%20focusing%20on%20the%0Aemployment%20of%20LLMs%20in%20supporting%20different%20medical%20tasks%20%28e.g.%2C%20enhancing%0Aclinical%20diagnostics%20and%20providing%20medical%20education%29%2C%20a%20review%20of%20these%0Aefforts%2C%20particularly%20their%20development%2C%20practical%20applications%2C%20and%20outcomes%0Ain%20medicine%2C%20remains%20scarce.%20Therefore%2C%20this%20review%20aims%20to%20provide%20a%20detailed%0Aoverview%20of%20the%20development%20and%20deployment%20of%20LLMs%20in%20medicine%2C%20including%20the%0Achallenges%20and%20opportunities%20they%20face.%20In%20terms%20of%20development%2C%20we%20provide%20a%0Adetailed%20introduction%20to%20the%20principles%20of%20existing%20medical%20LLMs%2C%20including%0Atheir%20basic%20model%20structures%2C%20number%20of%20parameters%2C%20and%20sources%20and%20scales%20of%0Adata%20used%20for%20model%20development.%20It%20serves%20as%20a%20guide%20for%20practitioners%20in%0Adeveloping%20medical%20LLMs%20tailored%20to%20their%20specific%20needs.%20In%20terms%20of%0Adeployment%2C%20we%20offer%20a%20comparison%20of%20the%20performance%20of%20different%20LLMs%20across%0Avarious%20medical%20tasks%2C%20and%20further%20compare%20them%20with%20state-of-the-art%0Alightweight%20models%2C%20aiming%20to%20provide%20an%20understanding%20of%20the%20advantages%20and%0Alimitations%20of%20LLMs%20in%20medicine.%20Overall%2C%20in%20this%20review%2C%20we%20address%20the%0Afollowing%20questions%3A%201%29%20What%20are%20the%20practices%20for%20developing%20medical%20LLMs%202%29%0AHow%20to%20measure%20the%20medical%20task%20performance%20of%20LLMs%20in%20a%20medical%20setting%3F%203%29%0AHow%20have%20medical%20LLMs%20been%20employed%20in%20real-world%20practice%3F%204%29%20What%20challenges%0Aarise%20from%20the%20use%20of%20medical%20LLMs%3F%20and%205%29%20How%20to%20more%20effectively%20develop%20and%0Adeploy%20medical%20LLMs%3F%20By%20answering%20these%20questions%2C%20this%20review%20aims%20to%20provide%0Ainsights%20into%20the%20opportunities%20for%20LLMs%20in%20medicine%20and%20serve%20as%20a%20practical%0Aresource.%20We%20also%20maintain%20a%20regularly%20updated%20list%20of%20practical%20guides%20on%0Amedical%20LLMs%20at%3A%20https%3A//github.com/AI-in-Health/MedLLMsPracticalGuide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05112v5&entry.124074799=Read"},
{"title": "Learning Decision Policies with Instrumental Variables through Double\n  Machine Learning", "author": "Daqian Shao and Ashkan Soleymani and Francesco Quinzan and Marta Kwiatkowska", "abstract": "  A common issue in learning decision-making policies in data-rich settings is\nspurious correlations in the offline dataset, which can be caused by hidden\nconfounders. Instrumental variable (IV) regression, which utilises a key\nunconfounded variable known as the instrument, is a standard technique for\nlearning causal relationships between confounded action, outcome, and context\nvariables. Most recent IV regression algorithms use a two-stage approach, where\na deep neural network (DNN) estimator learnt in the first stage is directly\nplugged into the second stage, in which another DNN is used to estimate the\ncausal effect. Naively plugging the estimator can cause heavy bias in the\nsecond stage, especially when regularisation bias is present in the first stage\nestimator. We propose DML-IV, a non-linear IV regression method that reduces\nthe bias in two-stage IV regressions and effectively learns high-performing\npolicies. We derive a novel learning objective to reduce bias and design the\nDML-IV algorithm following the double/debiased machine learning (DML)\nframework. The learnt DML-IV estimator has strong convergence rate and\n$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is\nunconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV\nregression benchmarks and learns high-performing policies in the presence of\ninstruments.\n", "link": "http://arxiv.org/abs/2405.08498v2", "date": "2024-05-15", "relevancy": 1.4247, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4735}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning&body=Title%3A%20Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning%0AAuthor%3A%20Daqian%20Shao%20and%20Ashkan%20Soleymani%20and%20Francesco%20Quinzan%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20A%20common%20issue%20in%20learning%20decision-making%20policies%20in%20data-rich%20settings%20is%0Aspurious%20correlations%20in%20the%20offline%20dataset%2C%20which%20can%20be%20caused%20by%20hidden%0Aconfounders.%20Instrumental%20variable%20%28IV%29%20regression%2C%20which%20utilises%20a%20key%0Aunconfounded%20variable%20known%20as%20the%20instrument%2C%20is%20a%20standard%20technique%20for%0Alearning%20causal%20relationships%20between%20confounded%20action%2C%20outcome%2C%20and%20context%0Avariables.%20Most%20recent%20IV%20regression%20algorithms%20use%20a%20two-stage%20approach%2C%20where%0Aa%20deep%20neural%20network%20%28DNN%29%20estimator%20learnt%20in%20the%20first%20stage%20is%20directly%0Aplugged%20into%20the%20second%20stage%2C%20in%20which%20another%20DNN%20is%20used%20to%20estimate%20the%0Acausal%20effect.%20Naively%20plugging%20the%20estimator%20can%20cause%20heavy%20bias%20in%20the%0Asecond%20stage%2C%20especially%20when%20regularisation%20bias%20is%20present%20in%20the%20first%20stage%0Aestimator.%20We%20propose%20DML-IV%2C%20a%20non-linear%20IV%20regression%20method%20that%20reduces%0Athe%20bias%20in%20two-stage%20IV%20regressions%20and%20effectively%20learns%20high-performing%0Apolicies.%20We%20derive%20a%20novel%20learning%20objective%20to%20reduce%20bias%20and%20design%20the%0ADML-IV%20algorithm%20following%20the%20double/debiased%20machine%20learning%20%28DML%29%0Aframework.%20The%20learnt%20DML-IV%20estimator%20has%20strong%20convergence%20rate%20and%0A%24O%28N%5E%7B-1/2%7D%29%24%20suboptimality%20guarantees%20that%20match%20those%20when%20the%20dataset%20is%0Aunconfounded.%20DML-IV%20outperforms%20state-of-the-art%20IV%20regression%20methods%20on%20IV%0Aregression%20benchmarks%20and%20learns%20high-performing%20policies%20in%20the%20presence%20of%0Ainstruments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Decision%2520Policies%2520with%2520Instrumental%2520Variables%2520through%2520Double%250A%2520%2520Machine%2520Learning%26entry.906535625%3DDaqian%2520Shao%2520and%2520Ashkan%2520Soleymani%2520and%2520Francesco%2520Quinzan%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520A%2520common%2520issue%2520in%2520learning%2520decision-making%2520policies%2520in%2520data-rich%2520settings%2520is%250Aspurious%2520correlations%2520in%2520the%2520offline%2520dataset%252C%2520which%2520can%2520be%2520caused%2520by%2520hidden%250Aconfounders.%2520Instrumental%2520variable%2520%2528IV%2529%2520regression%252C%2520which%2520utilises%2520a%2520key%250Aunconfounded%2520variable%2520known%2520as%2520the%2520instrument%252C%2520is%2520a%2520standard%2520technique%2520for%250Alearning%2520causal%2520relationships%2520between%2520confounded%2520action%252C%2520outcome%252C%2520and%2520context%250Avariables.%2520Most%2520recent%2520IV%2520regression%2520algorithms%2520use%2520a%2520two-stage%2520approach%252C%2520where%250Aa%2520deep%2520neural%2520network%2520%2528DNN%2529%2520estimator%2520learnt%2520in%2520the%2520first%2520stage%2520is%2520directly%250Aplugged%2520into%2520the%2520second%2520stage%252C%2520in%2520which%2520another%2520DNN%2520is%2520used%2520to%2520estimate%2520the%250Acausal%2520effect.%2520Naively%2520plugging%2520the%2520estimator%2520can%2520cause%2520heavy%2520bias%2520in%2520the%250Asecond%2520stage%252C%2520especially%2520when%2520regularisation%2520bias%2520is%2520present%2520in%2520the%2520first%2520stage%250Aestimator.%2520We%2520propose%2520DML-IV%252C%2520a%2520non-linear%2520IV%2520regression%2520method%2520that%2520reduces%250Athe%2520bias%2520in%2520two-stage%2520IV%2520regressions%2520and%2520effectively%2520learns%2520high-performing%250Apolicies.%2520We%2520derive%2520a%2520novel%2520learning%2520objective%2520to%2520reduce%2520bias%2520and%2520design%2520the%250ADML-IV%2520algorithm%2520following%2520the%2520double/debiased%2520machine%2520learning%2520%2528DML%2529%250Aframework.%2520The%2520learnt%2520DML-IV%2520estimator%2520has%2520strong%2520convergence%2520rate%2520and%250A%2524O%2528N%255E%257B-1/2%257D%2529%2524%2520suboptimality%2520guarantees%2520that%2520match%2520those%2520when%2520the%2520dataset%2520is%250Aunconfounded.%2520DML-IV%2520outperforms%2520state-of-the-art%2520IV%2520regression%2520methods%2520on%2520IV%250Aregression%2520benchmarks%2520and%2520learns%2520high-performing%2520policies%2520in%2520the%2520presence%2520of%250Ainstruments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Decision%20Policies%20with%20Instrumental%20Variables%20through%20Double%0A%20%20Machine%20Learning&entry.906535625=Daqian%20Shao%20and%20Ashkan%20Soleymani%20and%20Francesco%20Quinzan%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20A%20common%20issue%20in%20learning%20decision-making%20policies%20in%20data-rich%20settings%20is%0Aspurious%20correlations%20in%20the%20offline%20dataset%2C%20which%20can%20be%20caused%20by%20hidden%0Aconfounders.%20Instrumental%20variable%20%28IV%29%20regression%2C%20which%20utilises%20a%20key%0Aunconfounded%20variable%20known%20as%20the%20instrument%2C%20is%20a%20standard%20technique%20for%0Alearning%20causal%20relationships%20between%20confounded%20action%2C%20outcome%2C%20and%20context%0Avariables.%20Most%20recent%20IV%20regression%20algorithms%20use%20a%20two-stage%20approach%2C%20where%0Aa%20deep%20neural%20network%20%28DNN%29%20estimator%20learnt%20in%20the%20first%20stage%20is%20directly%0Aplugged%20into%20the%20second%20stage%2C%20in%20which%20another%20DNN%20is%20used%20to%20estimate%20the%0Acausal%20effect.%20Naively%20plugging%20the%20estimator%20can%20cause%20heavy%20bias%20in%20the%0Asecond%20stage%2C%20especially%20when%20regularisation%20bias%20is%20present%20in%20the%20first%20stage%0Aestimator.%20We%20propose%20DML-IV%2C%20a%20non-linear%20IV%20regression%20method%20that%20reduces%0Athe%20bias%20in%20two-stage%20IV%20regressions%20and%20effectively%20learns%20high-performing%0Apolicies.%20We%20derive%20a%20novel%20learning%20objective%20to%20reduce%20bias%20and%20design%20the%0ADML-IV%20algorithm%20following%20the%20double/debiased%20machine%20learning%20%28DML%29%0Aframework.%20The%20learnt%20DML-IV%20estimator%20has%20strong%20convergence%20rate%20and%0A%24O%28N%5E%7B-1/2%7D%29%24%20suboptimality%20guarantees%20that%20match%20those%20when%20the%20dataset%20is%0Aunconfounded.%20DML-IV%20outperforms%20state-of-the-art%20IV%20regression%20methods%20on%20IV%0Aregression%20benchmarks%20and%20learns%20high-performing%20policies%20in%20the%20presence%20of%0Ainstruments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08498v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


